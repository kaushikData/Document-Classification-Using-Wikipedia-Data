{"id": "53587502", "url": "https://en.wikipedia.org/wiki?curid=53587502", "title": "7825 (number)", "text": "7825 (number)\n\n7825 (seven thousand, eight hundred [and] twenty-five) is the natural number following 7824 and preceding 7826.\n\n"}
{"id": "14835582", "url": "https://en.wikipedia.org/wiki?curid=14835582", "title": "Algebraic combinatorics", "text": "Algebraic combinatorics\n\nAlgebraic combinatorics is an area of mathematics that employs methods of abstract algebra, notably group theory and representation theory, in various combinatorial contexts and, conversely, applies combinatorial techniques to problems in algebra.\n\nThrough the early or mid-1990s, typical combinatorial objects of interest in algebraic combinatorics either admitted a lot of symmetries (association schemes, strongly regular graphs, posets with a group action) or possessed a rich algebraic structure, frequently of representation theoretic origin (symmetric functions, Young tableaux). This period is reflected in the area 05E, \"Algebraic combinatorics\", of the AMS Mathematics Subject Classification, introduced in 1991.\n\nAlgebraic combinatorics has come to be seen more expansively as an area of mathematics where the interaction of combinatorial and algebraic methods is particularly strong and significant. Thus the combinatorial topics may be enumerative in nature or involve matroids, polytopes, partially ordered sets, or finite geometries. On the algebraic side, besides group and representation theory, lattice theory and commutative algebra are common.\n\nThe ring of symmetric functions is a specific limit of the rings of symmetric polynomials in \"n\" indeterminates, as \"n\" goes to infinity. This ring serves as universal structure in which relations between symmetric polynomials can be expressed in a way independent of the number \"n\" of indeterminates (but its elements are neither polynomials nor functions). Among other things, this ring plays an important role in the representation theory of the symmetric groups.\n\nAn association scheme is a collection of binary relations satisfying certain compatibility conditions. Association schemes provide a unified approach to many topics, for example combinatorial designs and coding theory. In algebra, association schemes generalize groups, and the theory of association schemes generalizes the character theory of linear representations of groups.\n\nA strongly regular graph is defined as follows. Let \"G\" = (\"V\",\"E\") be a regular graph with \"v\" vertices and degree \"k\". \"G\" is said to be strongly regular if there are also integers λ and μ such that:\n\n\nA graph of this kind is sometimes said to be an srg(\"v\", \"k\", λ, μ).\n\nSome authors exclude graphs which satisfy the definition trivially, namely those graphs which are the disjoint union of one or more equal-sized complete graphs, and their complements, the Turán graphs.\n\nA Young tableau (pl.: \"tableaux\") is a combinatorial object useful in representation theory and Schubert calculus. It provides a convenient way to describe the group representations of the symmetric and general linear groups and to study their properties. Young tableaux were introduced by Alfred Young, a mathematician at Cambridge University, in 1900. They were then applied to the study of the symmetric group by Georg Frobenius in 1903. Their theory was further developed by many mathematicians, including Percy MacMahon, W. V. D. Hodge, G. de B. Robinson, Gian-Carlo Rota, Alain Lascoux, Marcel-Paul Schützenberger and Richard P. Stanley.\n\nA matroid is a structure that captures and generalizes the notion of linear independence in vector spaces. There are many equivalent ways to define a matroid, the most significant being in terms of independent sets, bases, circuits, closed sets or flats, closure operators, and rank functions.\n\nMatroid theory borrows extensively from the terminology of linear algebra and graph theory, largely because it is the abstraction of various notions of central importance in these fields. Matroids have found applications in geometry, topology, combinatorial optimization, network theory and coding theory.\n\nA finite geometry is any geometric system that has only a finite number of points.\nThe familiar Euclidean geometry is not finite, because a Euclidean line contains infinitely many points. A geometry based on the graphics displayed on a computer screen, where the pixels are considered to be the points, would be a finite geometry. While there are many systems that could be called finite geometries, attention is mostly paid to the finite projective and affine spaces because of their regularity and simplicity. Other significant types of finite geometry are finite Möbius or inversive planes and Laguerre planes, which are examples of a general type called Benz planes, and their higher-dimensional analogs such as higher finite inversive geometries.\n\nFinite geometries may be constructed via linear algebra, starting from vector spaces over a finite field; the affine and projective planes so constructed are called Galois geometries. Finite geometries can also be defined purely axiomatically. Most common finite geometries are Galois geometries, since any finite projective space of dimension three or greater is isomorphic to a projective space over a finite field (that is, the projectivization of a vector space over a finite field). However, dimension two has affine and projective planes that are not isomorphic to Galois geometries, namely the non-Desarguesian planes. Similar results hold for other kinds of finite geometries.\n\n\n"}
{"id": "44807694", "url": "https://en.wikipedia.org/wiki?curid=44807694", "title": "Alice Silverberg", "text": "Alice Silverberg\n\nAlice Silverberg (born 1958) is professor of Mathematics and Computer Science at the University of California, Irvine. Her research concerns number theory and cryptography. With Karl Rubin, she introduced the CEILIDH system for torus-based cryptography in 2003.\n\nSilverberg graduated from Harvard University in 1979, and received her Ph.D. from Princeton University in 1984 under the supervision of Goro Shimura. She joined the Ohio State University faculty in the same year, and moved to Irvine in 2004.\n\nIn 2012, Silverberg became a fellow of the American Mathematical Society. She is part of the 2019 class of fellows of the Association for Women in Mathematics.\n\nIn 2017, Silverberg began a blog entitled \"Alice's Adventures in Numberland\", which humorously discusses issues surrounding sexism in academia. This is a topic which she has previously discussed in interviews, and has been quoted on.\n\n"}
{"id": "52385740", "url": "https://en.wikipedia.org/wiki?curid=52385740", "title": "Anviksiki", "text": "Anviksiki\n\nĀnvīkṣikī is a term in Sanskrit denoting roughly the \"science of inquiry\" and it should have been recognized in India as a distinct branch of learning as early as 650 BCE. However, over the centuries its meaning and import have undergone considerable variations. In the earliest period, the term was used to denote Atma-vidya, the science of the soul, in contrast to Adhyatma-vidya, the spiritual science, or Brahma-vidya, the divine science. In Manu Smriti the term Ānvīkṣikī has been used as equivalent to Atma-vidya and it has been described as a branch of the Vedas. In the fourth century BCE, Kautilya in his Arthashastra recognised it as a distinct branch of learning different from Vedas and other disciplines. Kautilya classifies all disciplines into four categories: scripture (the three Vedas, \"trayi\"), agriculture and commerce (\"varta\"), politics and public administration (\"danda-niti\"), and \"Ānvīkṣikī\", the investigative reflective science. The distinction between Atma-vidya and Ānvīkṣikī is that while the former embodied certain dogmatic assertions about the nature of the soul, the latter contained reasons supporting those assertions. Thus Ānvīkṣikī dealt with two subjects, namely, \"atma\", soul, and \"hetu\", theory of reasons. The Samkhya, Yoga, and Lokayata, in so far as they treated of reasons affirming or denying the existence of soul, were included by Kautilya in the Ānvīkṣikī. Of the two subjects studied in the ambit of Ānvīkṣikī, the study of soul later developed and matured into a separate independent study described by the term \"Darsanas\" (meaning philosophy), and the theory of reasons was developed into an independent branch of study referred to as \"Nyaya\" or logic. This bifurcation of Ānvīkṣikī into philosophy and logic must have had its beginning in around 550 BCE with the exposition of the logical side of Ānvīkṣikī by Medhatithi Gautama. However the term Ānvīkṣikī has been in use in the general sense of a science embracing both the science of soul and the theory of reasons.\n\nIt is interesting to observe that when the part of Ānvīkṣikī dealing with the theory of reasons developed into logic, the term Ānvīkṣikī began to be used to denote in this exclusive sense also. For example, Manusamhita has used this term in this special sense of logic. Gautama-dharma-sutra, Ramayana, Mahabharata all have used the term Ānvīkṣikī in this special sense. Ānvīkṣikī in this special sense has also been called by several other names, namely, \"Hetu-sastra\", \"Hetu-vidya\", \"Tarka-sastra\", \"Vada-vidya\", and also by Nyaya-sastra.\n\nThere are a few great teachers who wrote about and taught the doctrines of Ānvīkṣikī in the earliest sense of the term, that is, as a study of both philosophy and logic. Charvaka (c. 650 BCE), known for his materialistic doctrine, Kapila (c. 650–575 BCE), known for his doctrine of matter and soul, Dattatreya (c. 650 BCE), known for his parable of a tree, Punarvasu Atreya (c. 550 BCE), known for his dissertation on senses, Sulabha (c. 550 BCE), a lady ascetic known for canons of speech, Ashtavakra (c. 550–500 BCE) known as a violent debater, and Medhatithi Gautama (c. 550 BCE), known as the founder of Indian logic, are some of these great teachers.\n"}
{"id": "7207827", "url": "https://en.wikipedia.org/wiki?curid=7207827", "title": "Binary scaling", "text": "Binary scaling\n\nBinary scaling is a computer programming technique used typically in embedded C, DSP and assembler programs to implement floating point operations by using the native integer arithmetic of the processor.\n\nA representation of a floating point value using binary scaling is more precise than a floating point representation occupying the same number of bits, but cannot represent values beyond the range that it represents, thus more easily leading to arithmetic overflow during computation. Implementation of operations using integer arithmetic instructions is often (but not always) faster than the corresponding floating point instructions.\n\nA position for the 'binary point' is chosen for each variable to be represented, and binary shifts associated with arithmetic operations are adjusted accordingly.\n\nTo give an example, a common way to use integer arithmetic to simulate floating point, using 32 bit numbers, is to multiply the coefficients by 65536.\n\nUsing binary scientific notation, this will place the binary point at B16. That is to say, the most significant 16 bits represent the integer part the remainder are represent the fractional part. This means, as a signed two's complement integer B16 number can hold a highest value of formula_1 and a lowest value of −32768.0. Put another way, the B number, is the number of integer bits used to represent the number which defines its value range. Remaining low bits (i.e. the non-integer bits) are used to store fractional quantities and supply more accuracy.\n\nFor instance, to represent 1.2 and 5.6 as B16 one multiplies them by 2, giving 78643 and 367001.\n\nMultiplying these together gives\n\nTo convert it back to B16, divide it by 2.\n\nThis gives 440400B16, which when converted back to a floating point number (by dividing again by 2, but holding the result as floating point) gives 6.71999. The correct floating point result is 6.72.\n\nThe example above for a B16 multiplication is a simplified example. Re-scaling depends on both the B scale value and the word size. B16 is often used in 32 bit systems because it works simply by multiplying and dividing by 65536 (or shifting 16 bits).\n\nConsider the Binary Point in a signed 32 bit word thus:\n\nwhere S is the sign bit and X are the other bits.\n\nPlacing the binary point at\n\nWhen using different B scalings and/or word sizes the complete B scaling conversion formula must be used.\n\nConsider a 32 bit word size, and two variables, one with a B scaling of 2 and the other with a scaling of 4.\n\nNote that here the 1.4 values is very well represented with 30 fraction bits. A 32 bit floating-point number has 23 bits to store the fraction in. This is why B scaling is always more accurate than floating point of the same word size.\nThis is especially useful in integrators or repeated summing of small quantities where rounding error can be a subtle but very dangerous problem when using floating point.\n\nNow a larger number 15.2 at B4.\n\nThe number of bits to store the fraction is 28 bits.\nMultiplying these 32 bit numbers give the 64 bit result \n\nThis result is in B7 in a 64 bit word. Shifting it down by 32 bits gives the result in B7 in 32 bits.\n\nTo convert back to floating point, divide this by \n\nVarious scalings may be used. B0 for instance can be used to represent any number between -1 and 0.999999999.\n\nBinary angles are mapped using B0, with 0 as 0 degrees, 0.5 as 90° (or formula_2), −1.0 or 0.9999999 as 180° (or π) and −0.5 as 270° (or formula_3). When these binary angles are added using normal two's complement mathematics, the rotation of the angles is correct, even when crossing the sign boundary (this of course does away with checks for angle ≥ 360° when handling normal degrees).\n\nThe terms binary angular measurement (BAM) and binary angular measurement system (BAMS) as well as brads (binary radians or binary degree) refer to implementations of binary angles. They find use in robotics, navigation, computer games, and digital sensors.\n\nNo matter what bit-pattern is stored in a binary angle, when it is multiplied by 180° (or π) using standard signed fixed-point arithmetic, the result is always a valid angle in the range of −180° degrees (−π radians) to +180° degrees (+π radians).\nIn some cases, it is convenient to use unsigned multiplication (rather than signed multiplication) on a binary angle, which gives the correct angle in the range of 0 to +360° degrees (+2π radians or +1 turn).\nCompared to storing angles in a binary angle format, storing angles in any other format inevitably results in some bit patterns giving \"angles\" outside that range, requiring extra steps to range-reduce the value to the desired range, or results in some bit patterns that are not valid angles at all (NaN), or both.\n\nBinary scaling techniques were used in the 1970s and 1980s for real-time computing that was mathematically intensive, such as flight simulation. The code was often commented with the binary scalings of the intermediate results of equations.\n\nBinary scaling is still used in many DSP applications and custom made microprocessors are usually based on binary scaling techniques.\n\nBinary scaling is currently used in the DCT used to compress JPEG images in utilities such as GIMP.\n\nAlthough floating point has taken over to a large degree, where speed and extra accuracy are required, binary scaling works on simpler hardware and is more accurate when the range of values is known in advance.\n\n"}
{"id": "15894497", "url": "https://en.wikipedia.org/wiki?curid=15894497", "title": "Bing's recognition theorem", "text": "Bing's recognition theorem\n\nIn topology, a branch of mathematics, Bing's recognition theorem, named for R. H. Bing, asserts that a necessary and sufficient condition for a 3-manifold \"M\" to be homeomorphic to the 3-sphere is that every Jordan curve in \"M\" be contained within a topological ball.\n"}
{"id": "45467085", "url": "https://en.wikipedia.org/wiki?curid=45467085", "title": "Carolyn Mahoney", "text": "Carolyn Mahoney\n\nCarolyn Ray Boone Mahoney (born 1946) is an American mathematician who served as president of Lincoln University of Missouri. Her research interests include combinatorics, graph theory, and matroids.\n\nCarolyn Mahoney was born the sixth of thirteen children in 1946 in Memphis, Tennessee to Stephen and Myrtle Boone. Her grandmother cared for the children while her mother worked. Mahoney attended Catholic schools where she was encouraged in her interest in mathematics by the nuns. As a teenager, Mahoney's parents separated due to her father's drinking and gambling and the family was forced to move to a lower-class neighborhood. Mahoney and her siblings were known for being smart in their neighborhood. She graduated from Father Bertrand High School in 1964.\n\nMahoney attended Mount St. Scholastica College, a Catholic, all-female college in Kansas for three years before finishing her degree in mathematics at Siena College in Memphis, Tennessee in 1970. She then earned her master's degree in mathematics in 1972 and a doctorate in 1983, both from Ohio State University. Her doctorate involved matroid theory and enumerative combinatorics, and was supervised by Thomas Allan Dowling. Mahoney says she felt like a foreign student in graduate school because she was in a severe minority. She was only the 25th black woman to earn a Ph.D. in mathematics in the U.S.\n\nAfter earning her doctorate, Mahoney taught first at Denison University from 1984 to 1989, then at Ohio State for two years. She also served on the test development committee for the College Board from 1986 to 1989. In 1989, Mahoney was the first mathematicians to be selected for the faculty at California State University at San Marcos, and was one of twelve founding faculty of the San Marcos campus.\n\nIn 1994 and 1995, Mahoney served as a program director at the National Science Foundation, and she later worked as an administrator at Elizabeth City State University in North Carolina.\nIn 2005, Mahoney was named president of Lincoln University of Missouri. She retired in 2012.\n\nMahoney's research has focused largely on open problems in graph theory and combinatorics. As well as her thesis work on matroids, she has also published research on the Hadwiger–Nelson problem concerning the chromatic number of unit distance graphs.\n\nShe believes that she has had a hard time finding collaborators due to the fact that she is a Black female in mathematics. She is also a proponent of educational reform, especially supporting cultural diversity in university faculty. She believes that through the efforts of organizations such as the Mathematical Association of America and the Association for Women in Mathematics the environment for women in mathematics has improved.\n\nIn 1989, Mahoney was inducted into the Ohio Women's Hall of Fame.\n\nA scholarship at CSU San Marcos and a walking trail at Lincoln University have been named in her honor.\n"}
{"id": "34686184", "url": "https://en.wikipedia.org/wiki?curid=34686184", "title": "Central Council of Church Bell Ringers", "text": "Central Council of Church Bell Ringers\n\nThe Central Council of Church Bell Ringers (CCCBR) is an organisation founded in 1891 which represents ringers of church bells in the English style.\n\nIt acts as a co-ordinating body for education, publicity and codifying change ringing rules, also for advice on maintaining and restoring full-circle bells. Within England, where the vast majority of English-style rings are located, most towers are affiliated through local ringing associations.\n\nThe Central Council also publishes the bell ringers' weekly journal \"The Ringing World\".\n\nChange ringing had developed rapidly in the nineteenth century helped by the formation of the many local ringing associations which had sprung up. However, the need to have a national body with general oversight was increasingly debated, and discussions took place in 1883 about forming one. The eminent ringer, the Revd F.E. Robinson, advocated a National Association to connect the many ringing associations and collect and publish ringing information and performances, but this did not gather much support.\n\nHowever, the bell ringing aristocrat Sir Arthur P. Heywood still saw the need for standardisation of phraseology and change ringing methods and rules, in addition to representing the interests of ringers as a whole. He saw an alternative solution, which was to have a central \"advisory\" body.\n\nHeywood contrived in 1890 to organise a dinner in Birmingham for the 80th birthday of the noted ringer Henry Johnson, to which representatives of ringing associations from around the country were invited to attend as a “national gathering”. At the dinner he proposed a meeting of representatives from each association to discuss “matters of consequence”.\n\nHeywood's ideas of the aims of the prospective Council were:\n\nAt the exploratory gathering in 1890 there was strong support for the concept of a central advisory and coordinating body, and the first formal meeting of the new Council took place the following year on Easter Tuesday, 28 March 1891, at the Inns of Court Hotel, London. 74 representatives were present from 33 different societies, and Sir Arthur was elected as the Council’s first President.\n\nTwo Initial Committees were appointed; one to liaise with the Church Congress and a second for bells & fittings. The first meeting debated the definition of peals which was a strong current topic, and which has been debated at intervals ever since. Further debate took place in 1892 with general agreement on rules for ringing on 8, 10 and 12 bells but there was divided opinion on ringing on 5 & 6 bells. Such was the dissent that the subject of peal “Decisions” was dropped in 1897 and not raised again until 1911.\n\nIn 1903 the Church Press Committee was formed to compile biographies of ringers, this was a forerunner of the present-day Biographies Committee. Over the period 1899-1900 a survey was made of the condition of bells across the country. One aspect of interest in towers were experiments in tower movement measurement, another aspect of work which today rests with the Towers & Belfries Committee.\n\nAmongst the seemingly more unusual committees formed was that seeking concessionary fares upon the railways which were the only form of long distance transport to get to meetings, peals and other ringing events. Concessions were sometimes granted and bell ringers were included along with theatre companies and other groups qualifying for reductions right up until the early 1960s.\n\nAt the London meeting in 1921 the names of over 1,000 ringers who had perished in the War was read out. The Council instituted the first volume of Rolls of Honour which has been followed on to this day with much modern research.\n\nIt was the start of radio broadcasts in 1925 which prompted interest in seeing that properly considered broadcasts of ringing came across.This was followed by television and now social media all of which are part and parcel of the Public Relations Committee work. This does of course extend to public awareness and campaigns to ringing for special occasions particularly those with national importance like HM Queen’s 90th birthday.\n\nThe Central Council Library is an important collection of books on bell ringing and campanology.\n\nIn 1916 Sir A.P. Heywood died, and left his ringing books to the Cambridge University Guild which decided to donate them as the basis of a library for the Central Council in 1920.\n\nThe Rev. C.W.O. Jenkyn was the first librarian. He was succeeded as librarian by the Rev. Bernard Tyrwhitt-Drake of Walsoken, then by Wilfrid J. Hooton, and in 1953, Frederick Sharpe F.S.A. well known as a writer on historical aspects of bells and ringing. In 1958 Frank Perrens of Coventry was appointed until 1968.\n\nin 1976, when William T. Cook was elected, and with his appointment the rate of accessions increased, and at the time of his death in 1992 there were over 2,000 catalogue entries, some of which represent multiple items. Thus, for instance, a set of Guild or Association reports, perhaps over 100 in number, is represented by a single catalogue number. The present incumbent is Dr. John C. Eisel, although his title is now that of Steward of the Library.\n\nThe Council meets annually at Spring Bank Holiday weekend in various parts of the UK. Major policy decisions are discussed and the reports of the many committees are received. However,much of the Council's work is done in committee.\n\nMost of the committees are concerned with the normal minutiae of an organisation: administration, various records/archives. However, there are some highly esoteric committees such as Methods, which is concerned with defining and recording methods and principles. It lays down the criteria for accepting peals, including quarter and half peals, which was a topic of the early council meetings, and still excites debate today.\n\nThe current (2016) committees are:\n\n\n\"The Ringing World\" is a weekly journal devoted entirely to bell ringing and is the official journal of the Central Council for Church Bell Ringers. It is published in the UK as a paper periodical and an online edition. It records notable ringing performances, carries features on bells, change ringing, bell towers and ringers, it is a platform for correspondence, and advertises ringing events and publishes obituaries. It is the \"journal of record for performances\" in ringing, and peals must be published in it.\n\nIt was first published in 1911 from Guildford as a weekly periodical to report ringing news and details of peals and quarter peals rung around the world. Its founder and first editor was John Sparkes Goldsmith, who was born at Southover, Lewes, on 13 January 1878 and died on 1 June 1942. Following his death the Central Council guaranteed the publications against losses, until in 1945 it was decided to acquire it. Subsequently, from 1983 the journal would be constituted as a self-standing charitable body but still answerable to Council members.\n\nIn 2011, celebrations of the 100 year anniversary of the magazine were held nationally, with open ringing round London churches, and a service at Westminster Abbey.\n\nIn 2016 readers of the magazine wrote to insist that bell ringing was \"an art and a sport\", as demonstrated by regular \"striking competitions.\" It was suggested that classification of change ringing as a sport by Sport England could save it from becoming obsolete. But the Central Council of Church Bell Ringers opposed the move, suggesting that it would jeopardise its relationship with church bodies, since bell ringing should be seen as part of Christian worship, not exercise. The council's president, Chris Mew, said: \"Where is the glamour of the sports field and where are the David Beckhams of the belfry?\"\n\nThe members of the CCCBR are either representative, life members or co-opted. There are representatives for 65 affiliated organisations from the British Isles and territorial organisations throughout the world who serve for a three-year term. The council may itself elect both members and life members for past services to ringing.\n\nAs of May 2016 there are: life members – 6, additional members – 10, ex-officio members – 7.\n\nAs of March 2017, the following 67 societies are affiliated members of the Central Council.\n\n\n\n"}
{"id": "28119956", "url": "https://en.wikipedia.org/wiki?curid=28119956", "title": "Coarse function", "text": "Coarse function\n\nIn mathematics, coarse functions are functions that may appear to be continuous at a distance, but in reality are not necessarily continuous. Although continuous functions are usually observed on a small scale, coarse functions are usually observed on a large scale.\n\n"}
{"id": "41416740", "url": "https://en.wikipedia.org/wiki?curid=41416740", "title": "Deep belief network", "text": "Deep belief network\n\nIn machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (\"hidden units\"), with connections between the layers but not between units within each layer.\n\nWhen trained on a set of examples without supervision, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors. After this learning step, a DBN can be further trained with supervision to perform classification.\n\nDBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, where each sub-network's hidden layer serves as the visible layer for the next. An RBM is an undirected, generative energy-based model with a \"visible\" input layer and a hidden layer and connections between but not within layers. This composition leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the \"lowest\" pair of layers (the lowest visible layer is a training set).\n\nThe observation that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep learning algorithms. Overall, there are many attractive implementations and uses of DBNs in real-life applications and scenarios (e.g., electroencephalography, drug discovery ). \n\nThe training method for RBMs proposed by Geoffrey Hinton for use with training \"Product of Expert\" models is called contrastive divergence (CD). CD provides an approximation to the maximum likelihood method that would ideally be applied for learning the weights. In training a single RBM, weight updates are performed with gradient descent via the following equation: formula_1\n\nwhere, formula_2 is the probability of a visible vector, which is given by formula_3. formula_4 is the partition function (used for normalizing) and formula_5 is the energy function assigned to the state of the network. A lower energy indicates the network is in a more \"desirable\" configuration. The gradient formula_6 has the simple form formula_7 where formula_8 represent averages with respect to distribution formula_9. The issue arises in sampling formula_10 because this requires extended alternating Gibbs sampling. CD replaces this step by running alternating Gibbs sampling for formula_11 steps (values of formula_12 perform well). After formula_11 steps, the data are sampled and that sample is used in place of formula_10. The CD procedure works as follows:\nOnce an RBM is trained, another RBM is \"stacked\" atop it, taking its input from the final trained layer. The new visible layer is initialized to a training vector, and values for the units in the already-trained layers are assigned using the current weights and biases. The new RBM is then trained with the procedure above. This whole process is repeated until the desired stopping criterion is met.\n\nAlthough the approximation of CD to maximum likelihood is crude (does not follow the gradient of any function), it is empirically effective.\n\n\n"}
{"id": "393258", "url": "https://en.wikipedia.org/wiki?curid=393258", "title": "Dirichlet series", "text": "Dirichlet series\n\nIn mathematics, a Dirichlet series is any series of the form\n\nwhere \"s\" is complex, and formula_2 is a complex sequence. It is a special case of general Dirichlet series.\n\nDirichlet series play a variety of important roles in analytic number theory. The most usually seen definition of the Riemann zeta function is a Dirichlet series, as are the Dirichlet L-functions. It is conjectured that the Selberg class of series obeys the generalized Riemann hypothesis. The series is named in honor of Peter Gustav Lejeune Dirichlet.\n\nDirichlet series can be used as generating series for counting weighted sets of objects with respect to a weight which is combined multiplicatively when taking Cartesian products.\n\nSuppose that \"A\" is a set with a function \"w\": \"A\" → N assigning a weight to each of the elements of \"A\", and suppose additionally that the fibre over any natural number under that weight is a finite set. (We call such an arrangement (\"A\",\"w\") a weighted set.) Suppose additionally that \"a\" is the number of elements of \"A\" with weight \"n\". Then we define the formal Dirichlet generating series for \"A\" with respect to \"w\" as follows:\n\nNote that if \"A\" and \"B\" are disjoint subsets of some weighted set (\"U\", \"w\"), then the Dirichlet series for their (disjoint) union is equal to the sum of their Dirichlet series:\n\nMoreover, if (\"A\", \"u\") and (\"B\", \"v\") are two weighted sets, and we define a weight function \"w\": \"A\" × \"B\" → N by\n\nfor all \"a\" in \"A\" and \"b\" in \"B\", then we have the following decomposition for the Dirichlet series of the Cartesian product:\n\nThis follows ultimately from the simple fact that formula_7\n\nThe most famous of Dirichlet series is\n\nwhich is the Riemann zeta function.\n\nTreating these as formal Dirichlet series for the time being in order to be able to ignore matters of convergence, note that we have:\n\nas each natural number has a unique multiplicative decomposition into powers of primes. It is this bit of combinatorics which inspires the Euler product formula.\n\nAnother is:\n\nwhere \"μ\"(\"n\") is the Möbius function. This and many of the following series may be obtained by applying Möbius inversion and Dirichlet convolution to known series. For example, given a Dirichlet character \"χ\"(\"n\") one has\n\nwhere \"L\"(\"χ\", \"s\") is a Dirichlet L-function.\n\nOther identities include\n\nwhere formula_13(\"n\") is the totient function,\n\nwhere \"J\" is the Jordan function, and\n\nwhere σ(\"n\") is the divisor function. By specialisation to the divisor function \"d\" = \"σ\" we have\n\nThe logarithm of the zeta function is given by\n\nfor Re(\"s\") > 1. \nSimilarly, we have that\n\nHere, Λ(\"n\") is the von Mangoldt function. The logarithmic derivative is then\n\nThese last three are special cases of a more general relationship for derivatives of Dirichlet series, given below.\n\nGiven the Liouville function \"λ\"(\"n\"), one has\n\nYet another example involves Ramanujan's sum:\n\nAnother pair of examples involves the Möbius function and the prime omega function]:\n\nGiven a sequence {\"a\"} of complex numbers we try to consider the value of\n\nas a function of the complex variable \"s\". In order for this to make sense, we need to consider the convergence properties of the above infinite series:\n\nIf {\"a\"} is a bounded sequence of complex numbers, then the corresponding Dirichlet series \"f\" converges absolutely on the open half-plane of \"s\" such that Re(\"s\") > 1. In general, if \"a\" = O(\"n\"), the series converges absolutely in the half plane Re(\"s\") > \"k\" + 1.\n\nIf the set of sums \"a\" + \"a\" + ... + \"a\" is bounded for \"n\" and \"k\" ≥ 0, then the above infinite series converges on the open half-plane of \"s\" such that Re(\"s\") > 0.\n\nIn both cases \"f\" is an analytic function on the corresponding open half plane.\n\nIn general the abscissa of convergence of a Dirichlet series is the intercept on the real axis of the vertical line in the complex plane such that there is convergence to the right of it, and divergence to the left. This is the analogue for Dirichlet series of the radius of convergence for power series. The Dirichlet series case is more complicated, though: absolute convergence and uniform convergence may occur in distinct half-planes.\n\nIn many cases, the analytic function associated with a Dirichlet series has an analytic extension to a larger domain.\n\nAssume that formula_25 converges for some formula_26.\n\n\n _{\\textstyle=\\mathcal{O}(1)} \\\\[6pt]\n</math>\n\nand hence, for every formula_51 there is a formula_52 such that for formula_53 : formula_54\n\nA formal Dirichlet series over a ring \"R\" is associated to a function \"a\" from the positive integers to \"R\"\n\nwith addition and multiplication defined by\n\nwhere\n\nis the pointwise sum and\n\nis the Dirichlet convolution of \"a\" and \"b\".\n\nThe formal Dirichlet series form a ring Ω, indeed an \"R\"-algebra, with the zero function as additive zero element and the function δ defined by \"δ\"(1) = 1, \"δ\"(\"n\") = 0 for \"n\" > 1 as multiplicative identity. An element of this ring is invertible if \"a\"(1) is invertible in \"R\". If \"R\" is commutative, so is Ω; if \"R\" is an integral domain, so is Ω. The non-zero multiplicative functions form a subgroup of the group of units of Ω.\n\nThe ring of formal Dirichlet series over C is isomorphic to a ring of formal power series in countably many variables.\n\nGiven\n\nit is possible to show that\n\nassuming the right hand side converges. For a completely multiplicative function ƒ(\"n\"), and assuming the series converges for Re(\"s\") > σ, then one has that\n\nconverges for Re(\"s\") > σ. Here, Λ(\"n\") is the von Mangoldt function.\n\nSuppose\n\nand\n\nIf both \"F\"(\"s\") and \"G\"(\"s\") are absolutely convergent for \"s\" > \"a\" and \"s\" > \"b\" then we have\n\nIf \"a\" = \"b\" and \"ƒ\"(\"n\") = \"g\"(\"n\") we have\n\nThe inverse Mellin transform of a Dirichlet series, divided by s, is given by Perron's formula. \nAdditionally, if formula_67 is the (formal) ordinary generating function of the sequence of formula_68, \nthen an integral representation for the Dirichlet series of the generating function sequence, formula_69, is given by \n\nAnother class of related derivative and series-based generating function transformations on the ordinary generating function of a sequence which effectively produces the left-hand-side expansion in the previous equation are respectively defined in.\n\nThe sequence \"a\" generated by a Dirichlet series generating function corresponding to:\n\nwhere \"ζ\"(\"s\") is the Riemann zeta function, has the ordinary generating function:\n\n\n"}
{"id": "3531066", "url": "https://en.wikipedia.org/wiki?curid=3531066", "title": "Discontinuous linear map", "text": "Discontinuous linear map\n\nIn mathematics, linear maps form an important class of \"simple\" functions which preserve the algebraic structure of linear spaces and are often used as approximations to more general functions (see linear approximation). If the spaces involved are also topological spaces (that is, topological vector spaces), then it makes sense to ask whether all linear maps are continuous. It turns out that for maps defined on infinite-dimensional topological vector spaces (e.g., infinite-dimensional normed spaces), the answer is generally no: there exist discontinuous linear maps. If the domain of definition is complete, it is trickier; such maps can be proven to exist, but the proof relies on the axiom of choice and does not provide an explicit example.\n\nLet \"X\" and \"Y\" be two normed spaces and \"f\" a linear map from \"X\" to \"Y\". If \"X\" is finite-dimensional, choose a basis (\"e\", \"e\", …, \"e\") in \"X\" which may be taken to be unit vectors. Then,\nand so by the triangle inequality, \nLetting \nand using the fact that\nfor some \"C\">0 which follows from the fact that any two norms on a finite-dimensional space are equivalent, one finds \nThus, \"f\" is a bounded linear operator and so is continuous.\n\nIf \"X\" is infinite-dimensional, this proof will fail as there is no guarantee that the supremum \"M\" exists. If \"Y\" is the zero space {0}, the only map between \"X\" and \"Y\" is the zero map which is trivially continuous. In all other cases, when \"X\" is infinite-dimensional and \"Y\" is not the zero space, one can find a discontinuous map from \"X\" to \"Y\".\n\nExamples of discontinuous linear maps are easy to construct in spaces that are not complete; on any Cauchy sequence of independent vectors which does not have a limit, a linear operator may grow without bound. In a sense, the linear operators are not continuous because the space has \"holes\".\n\nFor example, consider the space \"X\" of real-valued smooth functions on the interval [0, 1] with the uniform norm, that is, \nThe \"derivative-at-a-point\" map, given by\n\ndefined on \"X\" and with real values, is linear, but not continuous. Indeed, consider the sequence\n\nfor \"n\"≥1. This sequence converges uniformly to the constantly zero function, but\n\nas \"n\"→∞ instead of formula_10 which would hold for a continuous map. Note that \"T\" is real-valued, and so is actually a linear functional on \"X\" (an element of the algebraic dual space \"X\"). The linear map \"X\" → \"X\" which assigns to each function its derivative is similarly discontinuous. Note that although the derivative operator is not continuous, it is closed.\n\nThe fact that the domain is not complete here is important. Discontinuous operators on complete spaces require a little more work.\n\nAn algebraic basis for the real numbers as a vector space over the rationals is known as a Hamel basis (note that some authors use this term in a broader sense to mean an algebraic basis of \"any\" vector space). Note that any two noncommensurable numbers, say 1 and π, are linearly independent. One may find a Hamel basis containing them, and define a map \"f\" from R to R so that \"f\"(π) = 0, \"f\" acts as the identity on the rest of the Hamel basis, and extend to all of R by linearity. Let {\"r\"} be any sequence of rationals which converges to π. Then lim \"f\"(\"r\") = π, but \"f\"(π) = 0. By construction, \"f\" is linear over Q (not over R), but not continuous. Note that \"f\" is also not measurable; an additive real function is linear if and only if it is measurable, so for every such function there is a Vitali set. The construction of \"f\" relies on the axiom of choice.\n\nThis example can be extended into a general theorem about the existence of discontinuous linear maps on any infinite-dimensional normed space (as long as the codomain is not trivial).\n\nDiscontinuous linear maps can be proven to exist more generally even if the space is complete. Let \"X\" and \"Y\" be normed spaces over the field \"K\" where \"K\" = R or \"K\" = C. Assume that \"X\" is infinite-dimensional and \"Y\" is not the zero space. We will find a discontinuous linear map \"f\" from \"X\" to \"K\", which will imply the existence of a discontinuous linear map \"g\" from \"X\" to \"Y\" given by the formula \"g\"(\"x\") = \"f\"(\"x\")\"y\" where \"y\" is an arbitrary nonzero vector in \"Y\".\n\nIf \"X\" is infinite-dimensional, to show the existence of a linear functional which is not continuous then amounts to constructing \"f\" which is not bounded. For that, consider a sequence (\"e\") (\"n\" ≥ 1) of linearly independent vectors in \"X\". Define\n\nfor each \"n\" = 1, 2, ... Complete this sequence of linearly independent vectors to a vector space basis of \"X\", and define \"T\" at the other vectors in the basis to be zero. \"T\" so defined will extend uniquely to a linear map on \"X\", and since it is clearly not bounded, it is not continuous.\n\nNotice that by using the fact that any set of linearly independent vectors can be completed to a basis, we implicitly used the axiom of choice, which was not needed for the concrete example in the previous section but one.\n\nAs noted above, the axiom of choice (AC) is used in the general existence theorem of discontinuous linear maps. In fact, there are no constructive examples of discontinuous linear maps with complete domain (for example, Banach spaces). In analysis as it is usually practiced by working mathematicians, the axiom of choice is always employed (it is an axiom of ZFC set theory); thus, to the analyst, all infinite-dimensional topological vector spaces admit discontinuous linear maps.\n\nOn the other hand, in 1970 Robert M. Solovay exhibited a model of set theory in which every set of reals is measurable. This implies that there are no discontinuous linear real functions. Clearly AC does not hold in the model.\n\nSolovay's result shows that it is not necessary to assume that all infinite-dimensional vector spaces admit discontinuous linear maps, and there are schools of analysis which adopt a more constructivist viewpoint. For example, H. G. Garnir, in searching for so-called \"dream spaces\" (topological vector spaces on which every linear map into a normed space is continuous), was led to adopt ZF + DC + BP (dependent choice is a weakened form and the Baire property is a negation of strong AC) as his axioms to prove the Garnir–Wright closed graph theorem which states, among other things, that any linear map from an F-space to a TVS is continuous. Going to the extreme of constructivism, there is Ceitin's theorem, which states that \"every\" function is continuous (this is to be understood in the terminology of constructivism, according to which only representable functions are considered to be functions). Such stances are held by only a small minority of working mathematicians.\n\nThe upshot is that the existence of discontinuous linear maps depends on AC; it is consistent with set theory without AC that there are no discontinuous linear maps on complete spaces. In particular, no concrete construction such as the derivative can succeed in defining a discontinuous linear map everywhere on a complete space.\n\nMany naturally occurring linear discontinuous operators are closed, a class of operators which share some of the features of continuous operators. It makes sense to ask which linear operators on a given space are closed. The closed graph theorem asserts that an \"everywhere-defined\" closed operator on a complete domain is continuous, so to obtain a discontinuous closed operator, one must permit operators which are not defined everywhere.\nTo be more concrete, let formula_12 be a map from formula_13 to formula_14 with domain formula_15, written formula_16. We don't lose much if we replace \"X\" by the closure of formula_15. That is, in studying operators that are not everywhere-defined, one may restrict one's attention to densely defined operators without loss of generality.\n\nIf the graph formula_18 of formula_12 is closed in \"X\" ×\"Y\", we call \"T\" \"closed\". Otherwise, consider its closure formula_20 in \"X\" ×\"Y\". If formula_20 is itself the graph of some operator formula_22, formula_12 is called \"closable\", and formula_22 is called the \"closure\" of formula_12.\n\nSo the natural question to ask about linear operators that are not everywhere-defined is whether they are closable. The answer is, \"not necessarily\"; indeed, every infinite-dimensional normed space admits linear operators that are not closable. As in the case of discontinuous operators considered above, the proof requires the axiom of choice and so is in general nonconstructive, though again, if \"X\" is not complete, there are constructible examples.\n\nIn fact, there is even an example of a linear operator whose graph has closure \"all\" of \"X\" ×\"Y\". Such an operator is not closable. Let \"X\" be the space of polynomial functions from [0,1] to R and \"Y\" the space of polynomial functions from [2,3] to R. They are subspaces of \"C\"([0,1]) and \"C\"([2,3]) respectively, and so normed spaces. Define an operator \"T\" which takes the polynomial function \"x\" ↦ \"p\"(\"x\") on [0,1] to the same function on [2,3]. As a consequence of the Stone–Weierstrass theorem, the graph of this operator is dense in \"X\"×\"Y\", so this provides a sort of maximally discontinuous linear map (confer nowhere continuous function). Note that \"X\" is not complete here, as must be the case when there is such a constructible map.\n\nThe dual space of a topological vector space is the collection of continuous linear maps from the space into the underlying field. Thus the failure of some linear maps to be continuous for infinite-dimensional normed spaces implies that for these spaces, one needs to distinguish the algebraic dual space from the continuous dual space which is then a proper subset. It illustrates the fact that an extra dose of caution is needed in doing analysis on infinite-dimensional spaces as compared to finite-dimensional ones.\n\nThe argument for the existence of discontinuous linear maps on normed spaces can be generalized to all metrisable topological vector spaces, especially to all Fréchet-spaces, but there exist infinite-dimensional locally convex topological vector spaces such that every functional is continuous. On the other hand, the Hahn–Banach theorem, which applies to all locally convex spaces, guarantees the existence of many continuous linear functionals, and so a large dual space. In fact, to every convex set, the Minkowski gauge associates a continuous linear functional. The upshot is that spaces with fewer convex sets have fewer functionals, and in the worst-case scenario, a space may have no functionals at all other than the zero functional. This is the case for the \"L\"(R,\"dx\") spaces with 0 < \"p\" < 1, from which it follows that these spaces are nonconvex. Note that here is indicated the Lebesgue measure on the real line. There are other \"L\" spaces with 0 < \"p\" < 1 which do have nontrivial dual spaces.\n\nAnother such example is the space of real-valued measurable functions on the unit interval with quasinorm given by \nThis non-locally convex space has a trivial dual space.\n\nOne can consider even more general spaces. For example, the existence of a homomorphism between complete separable metric groups can also be shown nonconstructively.\n\n"}
{"id": "21298611", "url": "https://en.wikipedia.org/wiki?curid=21298611", "title": "Elliptic divisibility sequence", "text": "Elliptic divisibility sequence\n\nIn mathematics, an elliptic divisibility sequence (EDS) is a sequence of integers satisfying a nonlinear recursion relation arising from division polynomials on elliptic curves. EDS were first defined, and their arithmetic properties studied, by Morgan Ward \nin the 1940s. They attracted only sporadic attention until around 2000, when EDS were taken up as a class of nonlinear recurrences that are more amenable to analysis than most such sequences. This tractability is due primarily to the close connection between EDS and elliptic curves. In addition to the intrinsic interest that EDS have within number theory, EDS have applications to other areas of mathematics including logic and cryptography.\n\nA (nondegenerate) \"elliptic divisibility sequence\" (EDS) is a sequence of integers \ndefined recursively by four initial values \nwith ≠ 0 and with subsequent values determined by the formulas\n\nIt can be shown that if divides each of , , and if further divides , then every term in the sequence is an integer.\n\nAn EDS is a divisibility sequence in the sense that\nIn particular, every term in an EDS is divisible by , so\nEDS are frequently \"normalized\" to have = 1 by dividing every term by the initial term.\n\nAny three integers , , \nwith divisible by lead to a normalized EDS on setting \nIt is not obvious, but can be proven, that the condition | suffices to ensure that every term\nin the sequence is an integer.\n\nA fundamental property of elliptic divisibility sequences\nis that they satisfy the general recursion relation\n\nThe \"discriminant\" of a normalized EDS is the quantity\nAn EDS is \"nonsingular\" if its discriminant is nonzero.\n\nA simple example of an EDS is the sequence of natural numbers 1, 2, 3,… . Another interesting example is 1, 3, 8, 21, 55, 144, 377, 987,… consisting of every other term in the Fibonacci sequence, starting with the second term. However, both of these sequences satisfy a linear recurrence and both are singular EDS. An example of a nonsingular EDS is \n\nA sequence is said to be \"periodic\"\nif there is a number so\nthat = for every ≥ 1.\nIf a nondegenerate EDS \nis periodic, then one of its terms vanishes. The smallest ≥ 1 with = 0 is called the \"rank of apparition\" of the EDS. A deep theorem of Mazur\nimplies that if the rank of apparition of an EDS is finite, then it satisfies ≤ 10 or = 12.\n\nWard proves that associated to any nonsingular EDS ()\nis an elliptic curve /Q and a point\nHere ψ is the \ndivision polynomial\nof ; the roots of ψ are the\nnonzero points of order on . There is\na complicated formula\nfor and in terms of , , , and .\n\nThere is an alternative definition of EDS that directly uses elliptic curves and yields a sequence which, up to sign, almost satisfies the EDS recursion. This definition starts with an elliptic curve /Q given by a Weierstrass equation and a nontorsion point ε (Q). One writes the -coordinates of the multiples of as \nThen the sequence () is also called an elliptic divisibility sequence. It is a divisibility sequence, and there exists an integer so that the subsequence ( ± ) (with an appropriate choice of signs) is an EDS in the earlier sense.\n\nLet be a nonsingular EDS\nthat is not periodic. Then the sequence grows quadratic exponentially in the sense that there is\na positive constant such that\nThe number is the canonical height of the point on \nthe elliptic curve associated to the EDS.\n\nIt is conjectured that a nonsingular EDS contains only finitely many \nprimes\nHowever, all but finitely many terms in a nonsingular EDS admit a primitive prime \ndivisor.\nThus for all but finitely many , \nthere is a prime such that divides , but does not divide for all < . This statement is an analogue of Zsigmondy's theorem.\n\nAn EDS over a finite field F, or more generally over any field, is a sequence of elements of that field satisfying the EDS recursion. An EDS over a finite field is always periodic, and thus has a rank of apparition . The period of an EDS over F then has the form , where and satisfy\nMore precisely, there are elements and in F such that\nThe values of and are related to the\nTate pairing of the point on the associated elliptic curve.\n\nBjorn Poonen\nhas applied EDS to logic. He uses the existence of primitive divisors in EDS on elliptic curves of rank one to prove the undecidability of Hilbert's tenth problem over certain rings of integers.\n\nKatherine Stange\nhas applied EDS and their higher rank generalizations called elliptic nets\nto cryptography. She shows how EDS can be used to compute the value\nof the Weil and Tate pairings on elliptic curves over finite\nfields. These pairings have numerous applications in pairing-based cryptography.\n\n\n"}
{"id": "5783949", "url": "https://en.wikipedia.org/wiki?curid=5783949", "title": "Extremally disconnected space", "text": "Extremally disconnected space\n\nIn mathematics, a topological space is termed extremally disconnected if the closure of every open set in it is open. (The term \"extremally disconnected\" is correct, even though the word \"extremally\" does not appear in most dictionaries. The term \"extremely disconnected\" is sometimes used, but it is incorrect.)\n\nAn extremally disconnected space that is also compact and Hausdorff is sometimes called a Stonean space. (Note that this is different from a Stone space, which is usually a totally disconnected compact Hausdorff space.) A theorem due to Andrew Gleason says that the projective objects of the category of compact Hausdorff spaces are exactly the extremally disconnected compact Hausdorff spaces. In the duality between Stone spaces and Boolean algebras, the Stonean spaces correspond to the complete Boolean algebras. \n\nAn extremally disconnected first-countable collectionwise Hausdorff space must be discrete. In particular, for metric spaces, the property of being extremally disconnected (the closure of every open set is open) is equivalent to the property of being discrete (every set is open).\n\n\n"}
{"id": "27046554", "url": "https://en.wikipedia.org/wiki?curid=27046554", "title": "FORM (symbolic manipulation system)", "text": "FORM (symbolic manipulation system)\n\nFORM is a symbolic manipulation system. It reads text files containing definitions of mathematical expressions as well as statements that tell it how to manipulate these expressions. Its original author is Jos Vermaseren of Nikhef, the Dutch institute for subatomic physics.\nIt is widely used in the theoretical particle physics community, but it is not restricted to applications in this specific field.\n\n\nA text file containing\n\ncodice_1\n\nwould tell FORM to create an expression named \"myexpr\", replace therein the symbol \"y\" by \"x\", and print the result on the screen. The result would be given like\n\ncodice_2\n\nFORM was started in 1984 as a successor to Schoonschip, an algebra engine developed by\nM. Veltman. It was initially coded in FORTRAN 77, but rewritten in C before the release of version 1.0 in 1989.\nVersion 2.0 was released in 1991. The version 3.0 of FORM has been publicized in 2000. It has been made open-source on August 27, 2010 under the GPL license.\n\n\n"}
{"id": "10605275", "url": "https://en.wikipedia.org/wiki?curid=10605275", "title": "Fiber derivative", "text": "Fiber derivative\n\nIn the context of Lagrangian Mechanics the fiber derivative is used to convert between the Lagrangian and Hamiltonian forms. In particular, if formula_1 is the configuration manifold then the Lagrangian formula_2 is defined on the tangent bundle formula_3 and the Hamiltonian is defined on the cotangent bundle formula_4—the fiber derivative is a map formula_5 such that\n\nwhere formula_7 and formula_8 are vectors from the same tangent space. When restricted to a particular point, the fiber derivative is a Legendre transformation.\n"}
{"id": "2266631", "url": "https://en.wikipedia.org/wiki?curid=2266631", "title": "Finger binary", "text": "Finger binary\n\nFinger binary is a system for counting and displaying binary numbers on the fingers of one or more hands. It is possible to count from 0 to 31 (2−1) using the fingers of a single hand, or from 0 through 1023 (2−1) if both hands are used.\n\nIn the binary number system, each numerical digit has two possible states (0 or 1) and each successive digit represents an increasing power of two.\n\nNote: What follows is but one of several possible schemes for assigning the values 1, 2, 4, 8, 16, etc. to fingers, not necessarily the best. (see below the illustrations.): The rightmost digit represents two to the zeroth power (i.e., it is the \"ones digit\"); the digit to its left represents two to the first power (the \"twos digit\"); the next digit to the left represents two to the second power (the \"fours digit\"); and so on. (The decimal number system is essentially the same, only that powers of ten are used: \"ones digit\", \"tens digit\" \"hundreds digit\", etc.)\n\nIt is possible to use anatomical digits to represent numerical digits by using a raised finger to represent a binary digit in the \"1\" state and a lowered finger to represent it in the \"0\" state. Each successive finger represents a higher power of two.\n\nWith palms oriented toward the counter's face, the values for when only the right hand is used are:\n\nWhen only the left hand is used:\n\nWhen both hands are used:\n\nAnd, alternately, with the palms oriented away from the counter:\nThe values of each raised finger are added together to arrive at a total number. In the one-handed version, all fingers raised is thus 31 (16 + 8 + 4 + 2 + 1), and all fingers lowered (a fist) is 0. In the two-handed system, all fingers raised is 1,023 (512 + 256 + 128 + 64 + 32 + 16 + 8 + 4 + 2 + 1) and two fists (no fingers raised) represents 0.\n\nIt is also possible to have each hand represent an independent number between 0 and 31; this can be used to represent various types of paired numbers, such as month and day, X-Y coordinates, or sports scores (such as for table tennis or baseball).\n\nWhen used in addition to the right.\n\nJust as fractional and negative numbers can be represented in binary, they can be represented in finger binary. \n\nRepresenting negative numbers is extremely simple, by using the leftmost finger as a sign bit: raised means the number is negative, in a sign-magnitude system. Anywhere between -511 and +511 can be represented this way, using two hands. Note that, in this system, both a positive and a negative zero may be represented.\n\nIf a convention were reached on palm up/palm down or fingers pointing up/down representing positive/negative, you could maintain 2 - 1 in both positive and negative numbers (-1023 to +1023, with positive and negative zero still represented).\n\nThere are multiple ways of representing fractions in finger binary.\n\nFractions can be stored natively in a binary format by having each finger represent a fractional power of two: formula_1. (These are known as dyadic fractions.)\n\nUsing the left hand only:\n\nUsing two hands:\n\nThe total is calculated by adding all the values in the same way as regular (non-fractional) finger binary, then dividing by the largest fractional power being used (32 for one-handed fractional binary, 1024 for two-handed), and simplifying the fraction as necessary.\n\nFor example, with thumb and index finger raised on the left hand and no fingers raised on the right hand, this is (512 + 256)/1024 = 768/1024 = 3/4. If using only one hand (left or right), it would be (16 + 8)/32 = 24/32 = 3/4 also.\n\nThe simplification process can itself be greatly simplified by performing a bit shift operation: all digits to the right of the rightmost raised finger (i.e., all trailing zeros) are discarded and the rightmost raised finger is treated as the ones digit. The digits are added together using their now-shifted values to determine the numerator and the rightmost finger's original value is used to determine the denominator.\n\nFor instance, if the thumb and index finger on the left hand are the only raised digits, the rightmost raised finger (the index finger) becomes \"1\". The thumb, to its immediate left, is now the 2s digit; added together, they equal 3. The index finger's original value (1/4) determines the denominator: the result is 3/4.\n\nCombined integer and fractional values (i.e., rational numbers) can be represented by setting a radix point somewhere between two fingers (for instance, between the left and right pinkies). All digits to the left of the radix point are integers; those to the right are fractional.\n\nDyadic fractions, explained above, unfortunately have limited use in a society based around decimal figures. A simple non-dyadic fraction such as 1/3 can be approximated as 341/1024 (0.3330078125), but the conversion between dyadic and decimal (0.333) or vulgar (1/3) forms is complicated. \n\nInstead, either decimal or vulgar fractions can be represented natively in finger binary. Decimal fractions can be represented by using regular integer binary methods and dividing the result by 10, 100, 1000, or some other power of ten. Numbers between 0 and 102.3, 10.23, 1.023, etc. can be represented this way, in increments of 0.1, 0.01, 0.001, etc.\n\nVulgar fractions can be represented by using one hand to represent the numerator and one hand to represent the denominator; a spectrum of rational numbers can be represented this way, ranging from 1/31 to 31/1 (as well as 0).\n\nIn theory, it is possible to use other positions of the fingers to represent more than two states (0 and 1); for instance, a ternary numeral system (base 3) could be used by having a fully raised finger represent 2, fully lowered represent 0, and \"curled\" (half-lowered) represent 1. This would make it possible to count up to 59,048 (3−1) on two hands. In practice, however, many people will find it difficult to hold all fingers independently (especially the middle and ring fingers) in more than two distinct positions.\n\n\n\n"}
{"id": "58550416", "url": "https://en.wikipedia.org/wiki?curid=58550416", "title": "Fischer's inequality", "text": "Fischer's inequality\n\nIn mathematics, Fischer's inequality gives an upper bound for the determinant of a positive-semidefinite matrix whose entries are complex numbers in terms of the determinants of its principal diagonal blocks. \nSuppose \"A\", \"C\" are respectively \"p\"×\"p\", \"q\"×\"q\" positive-semidefinite complex matrices and \"B\" is a \"p\"×\"q\" complex matrix.\nLet \nso that \"M\" is a (\"p\"+\"q\")×(\"p\"+\"q\") matrix.\n\nThen Fischer's inequality states that \nIf \"M\" is positive-definite, equality is achieved in Fischer's inequality if and only if all the entries of \"B\" are 0. Inductively one may conclude that a similar inequality holds for a block decomposition of \"M\" with multiple principal diagonal blocks. Considering 1×1 blocks, a corollary is Hadamard's inequality.\n\nAssume that \"A\" and \"C\" are positive-definite. We have formula_3 and formula_4 are positive-definite. Let \nWe note that\nApplying the AM-GM inequality to the eigenvalues of formula_7, we see\nBy multiplicativity of determinant, we have \nIn this case, equality holds if and only if \"M\" = \"D\" that is, all entries of \"B\" are 0.\n\nFor formula_10, as formula_11 and formula_12 are positive-definite, we have \n\nTaking the limit as formula_14 proves the inequality. From the inequality we note that if \"M\" is invertible, then both \"A\" and \"C\" are invertible and we get the desired equality condition.\n\nIf \"M\" can be partitioned in square blocks \"M\", then the following inequality by Thompson is valid:\n\nwhere [det(\"M\")] is the matrix whose (\"i\",\"j\") entry is det(\"M\").\n\nIn particular, if the block matrices \"B\" and \"C\" are also square matrices, then the following inequality by Everett is valid:\n\nThompson's inequality can also be generalized by an inequality in terms of the coefficients of the characteristic polynomial of the block matrices. Expressing the characteristic polynomial of the matrix \"A\" as\n\nand supposing that the blocks \"M\" are \"m\" x \"m\" matrices, the following inequality by Lin and Zhang is valid:\n\nNote that if \"r\" = \"m\", then this inequality is identical to Thompson's inequality.\n\n\n"}
{"id": "2664839", "url": "https://en.wikipedia.org/wiki?curid=2664839", "title": "Galerkin method", "text": "Galerkin method\n\nIn mathematics, in the area of numerical analysis, Galerkin methods are a class of methods for converting a continuous operator problem (such as a differential equation) to a discrete problem. In principle, it is the equivalent of applying the method of variation of parameters to a function space, by converting the equation to a weak formulation. Typically one then applies some constraints on the function space to characterize the space with a finite set of basis functions.\n\nThe approach is usually credited to Boris Galerkin but the method was discovered by Walther Ritz, to whom Galerkin refers. Often when referring to a Galerkin method, one also gives the name along with typical approximation methods used, such as Bubnov–Galerkin method (after Ivan Bubnov), Petrov–Galerkin method (after Georgii I. Petrov) or Ritz–Galerkin method (after Walther Ritz).\n\nExamples of Galerkin methods are:\n\nLet us introduce Galerkin's method with an abstract problem posed as a weak formulation on a Hilbert space formula_1, namely,\n\nHere, formula_4 is a bilinear form (the exact requirements on formula_4 will be specified later) and formula_6 is a bounded linear functional on formula_1.\n\nChoose a subspace formula_8 of dimension \"n\" and solve the projected problem:\n\nWe call this the Galerkin equation. Notice that the equation has remained unchanged and only the spaces have changed.\nReducing the problem to a finite-dimensional vector subspace allows us to numerically compute formula_11 as a finite linear combination of the basis vectors in formula_12.\n\nThe key property of the Galerkin approach is that the error is orthogonal to the chosen subspaces. Since formula_8, we can use formula_14 as a test vector in the original equation. Subtracting the two, we get the Galerkin orthogonality relation for the error, formula_15 which is the error between the solution of the original problem, formula_16, and the solution of the Galerkin equation, formula_17\n\nSince the aim of Galerkin's method is the production of a linear system of equations, we build its matrix form, which can be used to compute the solution by a computer program.\n\nLet formula_19 be a basis for formula_20. Then, it is sufficient to use these in turn for testing the Galerkin equation, i.e.: find formula_21 such that\n\nWe expand formula_17 with respect to this basis, formula_24 and insert it into the equation above, to obtain\n\nThis previous equation is actually a linear system of equations formula_26, where\n\nDue to the definition of the matrix entries, the matrix of the Galerkin equation is symmetric if and only if the bilinear form formula_4 is symmetric.\n\nHere, we will restrict ourselves to symmetric bilinear forms, that is\n\nWhile this is not really a restriction of Galerkin methods, the application of the standard theory becomes much simpler. Furthermore, a Petrov–Galerkin method may be required in the nonsymmetric case.\n\nThe analysis of these methods proceeds in two steps. First, we will show that the Galerkin equation is a well-posed problem in the sense of Hadamard and therefore admits a unique solution. In the second step, we study the quality of approximation of the Galerkin solution formula_17.\n\nThe analysis will mostly rest on two properties of the bilinear form, namely\nBy the Lax-Milgram theorem (see weak formulation), these two conditions imply well-posedness of the original problem in weak formulation. All norms in the following sections will be norms for which the above inequalities hold (these norms are often called an energy norm).\n\nSince formula_8, boundedness and ellipticity of the bilinear form apply to formula_20. Therefore, the well-posedness of the Galerkin problem is actually inherited from the well-posedness of the original problem.\n\nThe error formula_39 between the original and the Galerkin solution admits the estimate\n\nThis means, that up to the constant formula_41, the Galerkin solution formula_17\nis as close to the original solution formula_16 as any other vector in formula_20. In particular, it will be sufficient to study approximation by spaces formula_20, completely forgetting about the equation being solved.\n\nSince the proof is very simple and the basic principle behind all Galerkin methods, we include it here:\nby ellipticity and boundedness of the bilinear form (inequalities) and Galerkin orthogonality (equals sign in the middle), we have for arbitrary formula_46:\n\nDividing by formula_48 and taking the infimum over all possible formula_14 yields the lemma.\n\n\n"}
{"id": "5642452", "url": "https://en.wikipedia.org/wiki?curid=5642452", "title": "History of information theory", "text": "History of information theory\n\nThe decisive event which established the discipline of information theory, and brought it to immediate worldwide attention, was the publication of Claude E. Shannon's classic paper \"A Mathematical Theory of Communication\" in the \"Bell System Technical Journal\" in July and October 1948.\n\nIn this revolutionary and groundbreaking paper, the work for which Shannon had substantially completed at Bell Labs by the end of 1944, Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory, opening with the assertion that \n\nWith it came the ideas of \n\nSome of the oldest methods of instant telecommunications implicitly use many of the ideas that would later be quantified in information theory. Modern telegraphy, starting in the 1830s, used Morse code, in which more common letters (like \"E\", which is expressed as one \"dot\") are transmitted more quickly than less common letters (like \"J\", which is expressed by one \"dot\" followed by three \"dashes\"). The idea of encoding information in this manner is the cornerstone of lossless data compression. A hundred years later, frequency modulation illustrated that bandwidth can be considered merely another degree of freedom. The vocoder, now largely looked at as an audio engineering curiosity, was originally designed in 1939 to use less bandwidth than that of an original message, in much the same way that mobile phones now trade off voice quality with bandwidth.\n\nThe most direct antecedents of Shannon's work were two papers published in the 1920s by Harry Nyquist and Ralph Hartley, who were both still research leaders at Bell Labs when Shannon arrived in the early 1940s.\n\nNyquist's 1924 paper, \"Certain Factors Affecting Telegraph Speed\", is mostly concerned with some detailed engineering aspects of telegraph signals. But a more theoretical section discusses quantifying \"intelligence\" and the \"line speed\" at which it can be transmitted by a communication system, giving the relation\n\nwhere \"W\" is the speed of transmission of intelligence, \"m\" is the number of different voltage levels to choose from at each time step, and \"K\" is a constant.\n\nHartley's 1928 paper, called simply \"Transmission of Information\", went further by using the word \"information\" (in a technical sense), and making explicitly clear that information in this context was a measurable quantity, reflecting only the receiver's ability to distinguish that one sequence of symbols had been intended by the sender rather than any other—quite regardless of any associated meaning or other psychological or semantic aspect the symbols might represent. This amount of information he quantified as\n\nwhere \"S\" was the number of possible symbols, and \"n\" the number of symbols in a transmission. The natural unit of information was therefore the decimal digit, much later renamed the hartley in his honour as a unit or scale or measure of information. The Hartley information, \"H\", is still used as a quantity for the logarithm of the total number of possibilities.\n\nA similar unit of log probability, the \"ban\", and its derived unit the deciban (one tenth of a ban), were introduced by Alan Turing in 1940 as part of the statistical analysis of the breaking of the German second world war Enigma cyphers. The \"decibannage\" represented the reduction in (the logarithm of) the total number of possibilities (similar to the change in the Hartley information); and also the log-likelihood ratio (or change in the weight of evidence) that could be inferred for one hypothesis over another from a set of observations. The expected change in the weight of evidence is equivalent to what was later called the Kullback discrimination information.\n\nBut underlying this notion was still the idea of equal a-priori probabilities, rather than the information content of events of unequal probability; nor yet any underlying picture of questions regarding the communication of such varied outcomes.\n\nOne area where unequal probabilities were indeed well known was statistical mechanics, where Ludwig Boltzmann had, in the context of his H-theorem of 1872, first introduced the quantity\n\nas a measure of the breadth of the spread of states available to a single particle in a gas of like particles, where \"f\" represented the relative frequency distribution of each possible state. Boltzmann argued mathematically that the effect of collisions between the particles would cause the \"H\"-function to inevitably increase from any initial configuration until equilibrium was reached; and further identified it as an underlying microscopic rationale for the macroscopic thermodynamic entropy of Clausius.\n\nBoltzmann's definition was soon reworked by the American mathematical physicist J. Willard Gibbs into a general formula for statistical-mechanical entropy, no longer requiring identical and non-interacting particles, but instead based on the probability distribution \"p\" for the complete microstate \"i\" of the total system:\n\nThis (Gibbs) entropy, from statistical mechanics, can be found to directly correspond to the Clausius's classical thermodynamic definition.\n\nShannon himself was apparently not particularly aware of the close similarity between his new measure and earlier work in thermodynamics, but John von Neumann was. It is said that, when Shannon was deciding what to call his new measure and fearing the term 'information' was already over-used, von Neumann told him firmly: \"You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one really knows what entropy really is, so in a debate you will always have the advantage.\"\n\n(Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by Rolf Landauer in the 1960s, are explored further in the article \"Entropy in thermodynamics and information theory\").\n\nThe publication of Shannon's 1948 paper, \"A Mathematical Theory of Communication\", in the \"Bell System Technical Journal\" was the founding of information theory as we know it today. Many developments and applications of the theory have taken place since then, which have made many modern devices for data communication and storage such as CD-ROMs and mobile phones possible. Notable developments are listed in a timeline of information theory.\n\n"}
{"id": "386138", "url": "https://en.wikipedia.org/wiki?curid=386138", "title": "Homothetic transformation", "text": "Homothetic transformation\n\nIn mathematics, a homothety (or homothecy, or homogeneous dilation) is a transformation of an affine space determined by a point \"S\" called its \"center\" and a nonzero number \"λ\" called its \"ratio\", which sends\nin other words it fixes \"S\", and sends any \"M\" to another point \"N\" such that the segment \"SN\" is on the same line as \"SM\", but scaled by a factor \"λ\". In Euclidean geometry homotheties are the similarities that fix a point and either preserve (if ) or reverse (if ) the direction of all vectors. Together with the translations, all homotheties of an affine (or Euclidean) space form a group, the group of dilations or homothety-translations. These are precisely the affine transformations with the property that the image of every line \"L\" is a line parallel to \"L\".\n\nIn projective geometry, a homothetic transformation is a similarity transformation (i.e., fixes a given elliptic involution) that leaves the line at infinity pointwise invariant.\n\nIn Euclidean geometry, a homothety of ratio \"λ\" multiplies distances between points by |\"λ\"| and all areas by \"λ\". The first number is called the \"ratio of magnification\" or \"dilation factor\" or \"scale factor\" or \"similitude ratio\". Such a transformation can be called an enlargement if the scale factor exceeds 1. The above-mentioned fixed point \"S\" is called \"homothetic center\" or \"center of similarity\" or \"center of similitude\".\n\nIf the homothetic center \"S\" happens to coincide with the origin \"O\" of the vector space (\"S\" ≡ \"O\"), then every homothety with scale factor \"λ\" is equivalent to a uniform scaling by the same factor, which sends\n\nAs a consequence, in the specific case in which \"S\" ≡ \"O\", the homothety becomes a linear transformation, which preserves not only the collinearity of points (straight lines are mapped to straight lines), but also vector addition and scalar multiplication.\n\nThe image of a point (\"x\", \"y\") after a homothety with center (\"a\", \"b\") and scale factor \"λ\" is given by (\"a\" + \"λ\"(\"x\" − \"a\"), \"b\" + \"λ\"(\"y\" − \"b\")).\n\n\n\n"}
{"id": "693848", "url": "https://en.wikipedia.org/wiki?curid=693848", "title": "Hypergeometric identity", "text": "Hypergeometric identity\n\nIn mathematics, hypergeometric identities are equalities involving sums over hypergeometric terms, i.e. the coefficients occurring in hypergeometric series. These identities occur frequently in solutions to combinatorial problems, and also in the analysis of algorithms. \n\nThese identities were traditionally found 'by hand'. There exist now several algorithms which can find and \"prove\" all hypergeometric identities.\n\nThere are two definitions of hypergeometric terms, both used in different cases as explained below. See also hypergeometric series.\n\nA term \"t\" is a hypergeometric term if\n\nis a rational function in \"k\".\n\nA term \"F(n,k)\" is a hypergeometric term if\n\nis a rational function in \"k\".\n\nThere exist two types of sums over hypergeometric terms, the definite and indefinite sums. A definite sum is of the form\n\nThe indefinite sum is of the form\n\nAlthough in the past one has found proofs of certain identities there exist several algorithms to find and prove identities. These algorithms first find a \"simple expression\" for a sum over hypergeometric terms and then provide a certificate which anyone could use to easily check and prove the correctness of the identity.\n\nFor each of the hypergeometric sum types there exist one or more methods to find a \"simple expression\". These methods also provide a certificate to easily check the proof of an identity:\n\nA book named A = B has been written by Marko Petkovšek, Herbert Wilf and Doron Zeilberger describing the three main approaches described above.\n\n\n"}
{"id": "4995922", "url": "https://en.wikipedia.org/wiki?curid=4995922", "title": "Ideal point", "text": "Ideal point\n\nIn hyperbolic geometry, an ideal point, omega point or point at infinity is a well defined point outside the hyperbolic plane or space.\nGiven a line \"l\" and a point \"P\" not on \"l\", right- and left-limiting parallels to \"l\" through \"P\" converge to \"l\" at \"ideal points\".\n\nUnlike the projective case, ideal points form a boundary, not a submanifold. So, these lines do not \"intersect\" at an ideal point and such points, although well defined, do not belong to the hyperbolic space itself.\n\nThe ideal points together form the Cayley absolute or boundary of a hyperbolic geometry. \nFor instance, the unit circle forms the Cayley absolute of the Poincaré disk model and the Klein disk model.\nWhile the real line forms the Cayley absolute of the Poincaré half-plane model .\n\nPasch's axiom and the exterior angle theorem still hold for an omega triangle, defined by two points in hyperbolic space and an omega point.\n\n\nif all vertices of a triangle are ideal points the triangle is an ideal triangle.\n\nIdeal triangles have a number of interesting properties:\n\n\nif all vertices of a quadrilateral are ideal points the quadrilateral is an ideal quadrilateral.\n\nWhile all ideal triangles are congruent, not all quadrilaterals are, the diagonals can make different angles with each other resulting in noncongruent quadrilaterals\nhaving said this:\n\n\nThe ideal quadrilateral where the two diagonals are perpendicular to each other form an ideal square.\n\nIt was use by Ferdinand Karl Schweikart in his memorandum on what he called \"astral geometry\", one of the first publications acknowledging the possibility of hyperbolic geometry.\n\nAn ideal \"n\"-gon can be subdivided into ideal triangles, with area times the area of an ideal triangle.\n\nIn the Klein disk model and the Poincaré disk model of the hyperbolic plane. In both disk models the ideal points are on the unit circle (hyperbolic plane) or unit sphere (higher dimensions) which is the unreachable boundary of the hyperbolic plane. \nWhen projecting the same hyperbolic line to the Klein disk model and the Poincaré disk model both lines go through the same two ideal points.(the ideal points in both models are on the same spot).\n\nGiven two distinct points \"p\" and \"q\" in the open unit disk the unique straight line connecting them intersects the unit circle in two ideal points, \"a\" and \"b\", labeled so that the points are, in order, \"a\", \"p\", \"q\", \"b\" so that |aq| > |ap| and |pb| > |qb|. Then the hyperbolic distance between \"p\" and \"q\" is expressed as\n\nGiven two distinct points \"p\" and \"q\" in the open unit disk then the unique circle arc orthogonal to the boundary connecting them intersects the unit circle in two ideal points, \"a\" and \"b\", labeled so that the points are, in order, \"a\", \"p\", \"q\", \"b\" so that |aq| > |ap| and |pb| > |qb|. Then the hyperbolic distance between \"p\" and \"q\" is expressed as\n\nWhere the distances are measured along the (straight line) segments aq, ap, pb and qb.\n\nIn the Poincaré half-plane model the ideal points are the points on the boundary axis. There is also another ideal point that is not represented in the half-plane model (but rays parallel to the positive y-axis approach it).\n\nIn the hyperboloid model there are no ideal points.\n\n"}
{"id": "5795043", "url": "https://en.wikipedia.org/wiki?curid=5795043", "title": "Implicational propositional calculus", "text": "Implicational propositional calculus\n\nIn mathematical logic, the implicational propositional calculus is a version of classical propositional calculus which uses only one connective, called implication or conditional. In formulas, this binary operation is indicated by \"implies\", \"if ..., then ...\", \"→\", \"formula_1\", etc..\n\nImplication alone is not functionally complete as a logical operator because one cannot form all other two-valued truth functions from it. However, if one has a propositional formula which is known to be false and uses that as if it were a nullary connective for falsity, then one can define all other truth functions. So implication is virtually complete as an operator. If \"P\",\"Q\", and \"F\" are propositions and \"F\" is known to be false, then:\n\nMore generally, since the above operators are known to be functionally complete, it follows that any truth function can be expressed in terms of \"→\" and \"F\", if we have a proposition \"F\" which is known to be false.\n\nIt is worth noting that \"F\" is not definable from → and arbitrary sentence variables: any formula constructed from → and propositional variables must receive the value true when all of its variables are evaluated to true.\nIt follows as a corollary that {→} is not functionally complete. It cannot, for example, be used to define the two-place truth function that always returns \"false\".\n\nThe following statements are considered tautologies (irreducible and intuitively true, by definition).\nWhere in each case, \"P\", \"Q\", and \"R\" may be replaced by any formulas which contain only \"→\" as a connective. If Γ is a set of formulas and \"A\" a formula, then formula_2 means that \"A\" is derivable using the axioms and rules above and formulas from Γ as additional hypotheses.\n\nŁukasiewicz (1948) found an axiom system for the implicational calculus, which replaces the schemas 1–3 above with a single schema\nHe also argued that there is no shorter axiom system.\n\nSince all axioms and rules of the calculus are schemata, derivation is closed under substitution:\n\nwhere σ is any substitution (of formulas using only implication).\n\nThe implicational propositional calculus also satisfies the deduction theorem:\n\nAs explained in the deduction theorem article, this holds for any axiomatic extension of the system containing axiom schemas 1 and 2 above and modus ponens.\n\nThe implicational propositional calculus is semantically complete with respect to the usual two-valued semantics of classical propositional logic. That is, if Γ is a set of implicational formulas, and \"A\" is an implicational formula entailed by Γ, then formula_2.\n\nA proof of the completeness theorem is outlined below. First, using the compactness theorem and the deduction theorem, we may reduce the completeness theorem to its special case with empty Γ, i.e., we only need to show that every tautology is derivable in the system.\n\nThe proof is similar to completeness of full propositional logic, but it also uses the following idea to overcome the functional incompleteness of implication. If \"A\" and \"F\" are formulas, then is equivalent to where \"A*\" is the result of replacing in \"A\" all, some, or none of the occurrences of \"F\" by falsity. Similarly, is equivalent to So under some conditions, one can use them as substitutes for saying \"A*\" is false or \"A*\" is true respectively.\n\nWe first observe some basic facts about derivability:\n\nLet \"F\" be an arbitrary fixed formula. For any formula \"A\", we define and Let us consider only formulas in propositional variables \"p\", ..., \"p\". We claim that for every formula \"A\" in these variables and every truth assignment \"e\",\n\nWe prove () by induction on \"A\". The base case \"A\" = \"p\" is trivial. Let We distinguish three cases:\nNow let \"F\" be a tautology in variables \"p\", ..., \"p\". We will prove by reverse induction on \"k\" = \"n\"...,0 that for every assignment \"e\",\n\nThe base case \"k\" = \"n\" follows from a special case of () using\nand the fact that \"F\"→\"F\" is a theorem by the deduction theorem.\n\nAssume that () holds for \"k\" + 1, we will show it for \"k\". By applying deduction theorem to the induction hypothesis, we obtain\nby first setting \"e\"(\"p\") = 0 and second setting \"e\"(\"p\") = 1. From this we derive () using modus ponens.\n\nFor \"k\" = 0 we obtain that the tautology \"F\" is provable without assumptions. This is what was to be proved.\n\nThis proof is constructive. That is, given a tautology, one could actually follow the instructions and create a proof of it from the axioms. However, the length of such a proof increases exponentially with the number of propositional variables in the tautology, hence it is not a practical method for any but the very shortest tautologies.\n\nThe Bernays–Tarski axiom system is often used. In particular, Łukasiewicz's paper derives the Bernays–Tarski axioms from Łukasiewicz's sole axiom as a means of showing its completeness.<br>\nIt differs from the axiom schemas above by replacing axiom schema 2, (\"P\"→(\"Q\"→\"R\"))→((\"P\"→\"Q\")→(\"P\"→\"R\")), with\nwhich is called \"hypothetical syllogism\".\nThis makes derivation of the deduction meta-theorem a little more difficult, but it can still be done.\n\nWe show that from \"P\"→(\"Q\"→\"R\") and \"P\"→\"Q\" one can derive \"P\"→\"R\". This fact can be used in lieu of axiom schema 2 to get the meta-theorem.\n\nIn this case, a useful technique is to presume that the formula is not a tautology and attempt to find a valuation which makes it false. If one succeeds, then it is indeed not a tautology. If one fails, then it is a tautology.\n\nExample of a non-tautology:\n\nSuppose [(\"A\"→\"B\")→((\"C\"→\"A\")→\"E\")]→([\"F\"→((\"C\"→\"D\")→\"E\")]→[(\"A\"→\"F\")→(\"D\"→\"E\")]) is false.\n\nThen (\"A\"→\"B\")→((\"C\"→\"A\")→\"E\") is true; \"F\"→((\"C\"→\"D\")→\"E\") is true; \"A\"→\"F\" is true; \"D\" is true; and \"E\" is false.\n\nSince \"D\" is true, \"C\"→\"D\" is true. So the truth of \"F\"→((\"C\"→\"D\")→\"E\") is equivalent to the truth of \"F\"→\"E\".\n\nThen since \"E\" is false and \"F\"→\"E\" is true, we get that \"F\" is false.\n\nSince \"A\"→\"F\" is true, \"A\" is false. Thus \"A\"→\"B\" is true and (\"C\"→\"A\")→\"E\" is true.\n\n\"C\"→\"A\" is false, so \"C\" is true.\n\nThe value of \"B\" does not matter, so we can arbitrarily choose it to be true.\n\nSumming up, the valuation which sets \"B\", \"C\" and \"D\" to be true and \"A\", \"E\" and \"F\" to be false will make [(\"A\"→\"B\")→((\"C\"→\"A\")→\"E\")]→([\"F\"→((\"C\"→\"D\")→\"E\")]→[(\"A\"→\"F\")→(\"D\"→\"E\")]) false. So it is not a tautology.\n\nExample of a tautology:\n\nSuppose ((\"A\"→\"B\")→\"C\")→((\"C\"→\"A\")→(\"D\"→\"A\")) is false.\n\nThen (\"A\"→\"B\")→\"C\" is true; \"C\"→\"A\" is true; \"D\" is true; and \"A\" is false.\n\nSince \"A\" is false, \"A\"→\"B\" is true. So \"C\" is true. Thus \"A\" must be true, contradicting the fact that it is false.\n\nThus there is no valuation which makes ((\"A\"→\"B\")→\"C\")→((\"C\"→\"A\")→(\"D\"→\"A\")) false. Consequently, it is a tautology.\n\nWhat would happen if another axiom schema were added to those listed above? There are two cases: (1) it is a tautology; or (2) it is not a tautology.\n\nIf it is a tautology, then the set of theorems remains the set of tautologies as before. However, in some cases it may be possible to find significantly shorter proofs for theorems. Nevertheless, the minimum length of proofs of theorems will remain unbounded, that is, for any natural number \"n\" there will still be theorems which cannot be proved in \"n\" or fewer steps.\n\nIf the new axiom schema is not a tautology, then every formula becomes a theorem (which makes the concept of a theorem useless in this case). What is more, there is then an upper bound on the minimum length of a proof of every formula, because there is a common method for proving every formula. For example, suppose the new axiom schema were ((\"B\"→\"C\")→\"C\")→\"B\". Then ((\"A\"→(\"A\"→\"A\"))→(\"A\"→\"A\"))→\"A\" is an instance (one of the new axioms) and also not a tautology. But [((\"A\"→(\"A\"→\"A\"))→(\"A\"→\"A\"))→\"A\"]→\"A\" is a tautology and thus a theorem due to the old axioms (using the completeness result above). Applying modus ponens, we get that \"A\" is a theorem of the extended system. Then all one has to do to prove any formula is to replace \"A\" by the desired formula throughout the proof of \"A\". This proof will have the same number of steps as the proof of \"A\".\n\nThe axioms listed above primarily work through the deduction metatheorem to arrive at completeness. Here is another axiom system which aims directly at completeness without going through the deduction metatheorem.\n\nFirst we have axiom schemas which are designed to efficiently prove the subset of tautologies which contain only one propositional variable.\nThe proof of each such tautology would begin with two parts (hypothesis and conclusion) which are the same. Then insert additional hypotheses between them. Then insert additional tautological hypotheses (which are true even when the sole variable is false) into the original hypothesis. Then add more hypotheses outside (on the left). This procedure will quickly give every tautology containing only one variable. (The symbol \"ꞈ\" in each axiom schema indicates where the conclusion used in the completeness proof begins. It is merely a comment, not a part of the formula.)\n\nConsider any formula Φ which may contain \"A\", \"B\", \"C\", ..., \"C\" and ends with \"A\" as its final conclusion. Then we take\nas an axiom schema where Φ is the result of replacing \"B\" by \"A\" throughout Φ and Φ is the result of replacing \"B\" by (\"A\"→\"A\") throughout Φ. This is a schema for axiom schemas since there are two level of substitution: in the first Φ is substituted (with variations); in the second, any of the variables (including both \"A\" and \"B\") may be replaced by arbitrary formulas of the implicational propositional calculus. This schema allows one to prove tautologies with more than one variable by considering the case when \"B\" is false Φ and the case when \"B\" is true Φ.\n\nIf the variable which is the final conclusion of a formula takes the value true, then the whole formula takes the value true regardless of the values of the other variables. Consequently if \"A\" is true, then Φ, Φ, Φ and Φ→(Φ→Φ) are all true. So without loss of generality, we may assume that \"A\" is false. Notice that Φ is a tautology if and only if both Φ and Φ are tautologies. But while Φ has \"n\"+2 distinct variables, Φ and Φ both have \"n\"+1. So the question of whether a formula is a tautology has been reduced to the question of whether certain formulas with one variable each are all tautologies. Also notice that Φ→(Φ→Φ) is a tautology regardless of whether Φ is, because if Φ is false then either Φ or Φ will be false depending on whether \"B\" is false or true.\n\nExamples:\n\nDeriving Peirce's law\n\nDeriving Łukasiewicz' sole axiom\n\nUsing a truth table to verify Łukasiewicz' sole axiom would require consideration of 16=2 cases since it contains 4 distinct variables. In this derivation, we were able to restrict consideration to merely 3 cases: \"R\" is false and \"Q\" is false, \"R\" is false and \"Q\" is true, and \"R\" is true. However because we are working within the formal system of logic (instead of outside it, informally), each case required much more effort.\n\n\n"}
{"id": "12870767", "url": "https://en.wikipedia.org/wiki?curid=12870767", "title": "Insurance cycle", "text": "Insurance cycle\n\nThe tendency of the insurance industry to swing between profitable and unprofitable periods over time is commonly known as the underwriting or insurance cycle.\n\nThe underwriting cycle is the tendency of property and casualty insurance premiums, profits, and availability of coverage to rise and fall with some regularity over time. A cycle begins when insurers tighten their underwriting standards and sharply raise premiums after a period of severe underwriting losses or negative stocks to capital (e.g., investment losses). Stricter standards and higher premium rates lead to an increase in profits and accumulation of capital. The increase in underwriting capacity increases competition, which in turn drives premium rates down and relaxes underwriting standards, thereby causing underwriting losses and setting the stage for the cycle to begin again. For example, Lloyd's Franchise Performance Director Rolf Tolle stated in 2007 that \"mitigating the insurance cycle was the \"biggest challenge\" facing managing agents in the next few years\".\n\nAll industries experience cycles of growth and decline, 'boom and bust'. These cycles are particularly important in the insurance and re-insurance industry as they are especially unpredictable.\n\nLloyd's of London research in 2006 revealed, for the second year running, that Lloyd’s underwriters see managing the insurance cycle as the top challenge for the insurance industry, and nearly two-thirds believe that the industry at large is not doing enough to respond to the challenge.\n\nThe Insurance Cycle affects all areas of insurance except life insurance, where there is enough data and a large base of similar risks (i.e. people) to accurately predict claims, and therefore minimise the risk that the cycle poses to business.\n\nThe insurance cycle is a phenomenon that has been understood since at least the 1920s. Since then it has been considered an insurance 'fact of life'. Most commentators believe that underwriting cycles are inevitable, primarily \"because the uncertainty inherent in matching insurance prices to [future] losses creates an environment in which the motivations, ambitions, and fears of a complex cast of characters can play out.\" Lloyd's counters that this has become \"a self-fulfilling prophecy\".\n\nMore recently, insurers have attempted to model the cycle and base their policy pricing and risk exposure accordingly.\n\nFor the sake of argument let's start from a 'soft' period in the cycle, that is a period in which premiums are low, capital base is high and competition is high. Premiums continue to fall as naive insurers offer cover at unrealistic rates, and established businesses are forced to compete or risk losing business in the long term.\n\nThe next stage is precipitated by a catastrophe or similar significant loss, for example Hurricane Andrew or the attacks on the World Trade Center. The graph below shows the effect that these two events had on insurance premiums.\n\nAfter a major claims burst, less stable companies are driven out of the market which decreases competition. In addition to this, large claims have left even larger companies with less capital. Therefore, premiums rise rapidly. The market hardens, and underwriters are less likely to take on risks.\n\nIn turn, this lack of competition and high rates looks suddenly very profitable, and more companies join the market whilst existing business begin to lower rates to compete. This causes a market saturation and Insurance Cycle begins again.\n\nWhile many underwriters believe that the cycle is out of their hands, Lloyd’s is trying to push for more proactive management of the ups and downs of the industry. In 2006 they published their ‘Seven Steps’ to managing the insurance cycle:\n\n1. Don’t follow the herd. Insurers need to be prepared to walk away from markets when prices fall below a prudent, risk-based premium.\n\n2. Invest in the latest risk management tools. Insurers must push for continuous improvement of these tools based on the latest science around issues such as climate change, and make full use of them to communicate their pricing and coverage decisions.\n\n3. Don’t let surplus capital dictate your underwriting. An excess of capital available for underwriting can easily push an insurer to deploy the capital in unsustainable ways, rather than having that capital migrate to other uses such as hedge funds and equities, or returning it to shareholders.\n\n4. Don’t be dazzled by higher investment returns. Don’t let higher investment returns replace disciplined underwriting as base rates creep up on both sides of the Atlantic. Notionally, splitting the business into insurance and asset management operations, and monitoring each separately, is one way to achieve this.\n\n5. Don’t rely on \"the big one\" to push prices upwards. The spectacular insured loss should not be used as an excuse to raise prices in unrelated lines of business. Regulators, rating agencies, and analysts – not to mention insurance buyers – are increasingly resisting such behaviour.\n\n6. Redeploy capital from lines where margins are unsustainable. There is little that individual insurers can do to alter overall supply-and-demand conditions. But insurers can set up internal monitoring systems to ensure that they scale back in lines in which margins have become unsustainable and migrate to other lines.\n\n7. Get smarter with underwriter and manager incentives. Incentives for key staff should be structured to reward efficient deployment of capital, linking such rewards to target shareholder returns rather than volume growth.\n\nThe Lloyd’s Managing Cycle report has several problems. It focuses on the industry as a whole being able to work together to reduce the effect of market fluctuations. However, this is somewhat unrealistic, as if underwriters do not write business in a soft market (i.e. at cheap prices for the customer), it will be hard to win this business back in a hard market due to loyalty issues.\n\nRolf Tolle asserts that \"There is nothing complex about the cycle. It is about having the courage of your convictions to act with strength.\". Swiss Re argue that instead of ‘beating’ the cycle, insurers should learn to anticipate its fluctuations. \"Cycle management is essentially proper timing. Monitoring the market, predicting market trends and accurately assessing prices play an important role\".\n\nSwiss Re gave several examples of potential business strategies. One is to write risks at a roughly fixed rate. This is clearly not practical as it does not allow for the cyclical nature of the market. Another is to fail to react fast enough to changes in the market, which leaves a company even more exposed. The recommended strategy is one that relies on prediction of the business cycle and setting premiums based on models and experience.\n\nThe unpredictable nature of the insurance industry makes it very unlikely that the cycle can be eliminated. For several years Lloyd's have been urging caution in soft periods and restraint in hard periods.\n"}
{"id": "17135554", "url": "https://en.wikipedia.org/wiki?curid=17135554", "title": "Interpretation (model theory)", "text": "Interpretation (model theory)\n\nIn model theory, interpretation of a structure \"M\" in another structure \"N\" (typically of a different signature) is a technical notion that approximates the idea of representing \"M\" inside \"N\". For example every reduct or definitional expansion of a structure \"N\" has an interpretation in \"N\".\n\nMany model-theoretic properties are preserved under interpretability. For example if the theory of \"N\" is stable and \"M\" is interpretable in \"N\", then the theory of \"M\" is also stable.\n\nAn interpretation of \"M\" in \"N\" with parameters (or without parameters, respectively)\nis a pair formula_1 where\n\"n\" is a natural number and formula_2 is a surjective map from a subset of\n\"N\" onto \"M\"\nsuch that the formula_2-preimage (more precisely the formula_4-preimage) of every set \"X\" ⊆ \"M\" definable in \"M\" by a first-order formula without parameters\nis definable (in \"N\") by a first-order formula with parameters (or without parameters, respectively).\nSince the value of \"n\" for an interpretation formula_1 is often clear from context, the map formula_2 itself is also called an interpretation.\n\nTo verify that the preimage of every definable (without parameters) set in \"M\" is definable in \"N\" (with or without parameters), it is sufficient to check the preimages of the following definable sets:\n\nIn model theory the term \"definable\" often refers to definability with parameters; if this convention is used, definability without parameters is expressed by the term \"0-definable\". Similarly, an interpretation with parameters may be referred to as simply an interpretation, and an interpretation without parameters as a 0-interpretation.\n\nIf \"L, M\" and \"N\" are three structures, \"L\" is interpreted in \"M,\"\nand \"M\" is interpreted in \"N,\" then one can naturally construct a composite interpretation of \"L\" in \"N.\"\nIf two structures \"M\" and \"N\" are interpreted in each other, then by combining the interpretations in two possible ways, one obtains an interpretation of each of the two structures in itself.\nThis observation permits one to define an equivalence relation among structures, reminiscent of the homotopy equivalence among topological spaces.\n\nTwo structures \"M\" and \"N\" are bi-interpretable if there exists an interpretation of \"M\" in \"N\" and an interpretation of \"N\" in \"M\" such that the composite interpretations of \"M\" in itself and of \"N\" in itself are definable in \"M\" and in \"N\", respectively (the composite interpretations being viewed as operations on \"M\" and on \"N\").\n\nThe partial map \"f\" from Z × Z onto Q which maps (\"x\", \"y\") to \"x\"/\"y\" provides an interpretation of the field Q of rational numbers in the ring Z of integers (to be precise, the interpretation is (2, \"f\")).\nIn fact, this particular interpretation is often used to \"define\" the rational numbers.\nTo see that it is an interpretation (without parameters), one needs to check the following preimages of definable sets in Q:\n\n"}
{"id": "1595662", "url": "https://en.wikipedia.org/wiki?curid=1595662", "title": "Key signature (cryptography)", "text": "Key signature (cryptography)\n\nIn cryptography, a key signature is the result of a third-party applying a cryptographic signature to a representation of a cryptographic key. This is usually done as a form of assurance or verification: If \"Alice\" has signed \"Bob's\" key, it can serve as an assurance to another party, say \"Eve\", that the key actually belongs to Bob, and that Alice has personally checked and attested to this.\n\nThe representation of the key that is signed is usually shorter than the key itself, because most public-key signature schemes can only encrypt or sign short lengths of data. Some derivative of the public key fingerprint may be used, i.e. via hash functions.\n\n"}
{"id": "49927590", "url": "https://en.wikipedia.org/wiki?curid=49927590", "title": "Kmc-Subset137", "text": "Kmc-Subset137\n\nThe open-source Kmc-Subset137 Project implements the protocol described in \"ERTMS/ECTS; On-line Key Management FFFIS\" UNISIG SUBSET-137 ver1.0.0.\n\nIt covers the on-line distribution of cryptographic keys among the key management Centres authoritative in their respective domains. It also deals with the exchange between a key management center and its own domain KMAC entities. The open source library provides a simple C language application programming interface (API) to access and parse UNISIG SUBSET 137 messages.\n\nFor the cryptographic part of the protocol it relies on the open source GnuTLS library (or alternatively, but discouraged, on the OpenSSL library).\n\nThe library is open-source and is licensed under the GNU General Public License version 3.0.\n\n"}
{"id": "8928339", "url": "https://en.wikipedia.org/wiki?curid=8928339", "title": "Life annuity", "text": "Life annuity\n\nA life annuity is an annuity, or series of payments at fixed intervals, paid while the purchaser (or annuitant) is alive. A life annuity is an insurance product typically sold or issued by life insurance companies. Life annuities may be sold in exchange for the immediate payment of a lump sum (single-payment annuity) or a series of regular payments (flexible payment annuity), prior to the onset of the annuity.\n\nThe payment stream from the issuer to the annuitant has an unknown duration based principally upon the date of death of the annuitant. At this point the contract will terminate and the remainder of the fund accumulated is forfeited unless there are other annuitants or beneficiaries in the contract. Thus a life annuity is a form of longevity insurance, where the uncertainty of an individual's lifespan is transferred from the individual to the insurer, which reduces its own uncertainty by pooling many clients. Annuities can be purchased to provide an income during retirement, or originate from a \"structured settlement\" of a personal injury lawsuit.\n\nThe instrument's evolution has been long and continues as part of actuarial science.\n\nUlpian is credited with generating an actuarial life annuity table between AD 211 and 222.\nMedieval German and Dutch cities and monasteries raised money by the sale of life annuities, and it was recognized that pricing them was difficult. The early practice for selling this instrument did not consider the age of the nominee, thereby raising interesting concerns. These concerns got the attention of several prominent mathematicians over the years, such as Huygens, Bernoulli, de Moivre and others: even Gauss and Laplace had an interest in matters pertaining to this instrument.\n\nIt seems that Johan de Witt was the first writer to compute the value of a life annuity as the sum of expected discounted future payments, while Halley used the first mortality table drawn from experience for that calculation. Meanwhile, the Paris Hôtel-Dieu offered some fairly priced annuities that roughly fit the Deparcieux table discounted at 5%.\n\nContinuing practice is an everyday occurrence with well-known theory founded on robust mathematics, as witnessed by the hundreds of millions worldwide who receive regular remuneration via pension or the like. The modern approach to resolving the difficult problems related to a larger scope for this instrument applies many advanced mathematical approaches, such as stochastic methods, game theory, and other tools of financial mathematics.\n\nDefined benefit pension plans are a form of life annuity typically provided by employers or governments (such as Social Security in the United States). The size of payouts is usually determined based on the employee's years of service, age, and salary.\n\nIndividual annuities are insurance products marketed to individual consumers. With the complex selection of options available, consumers can find it difficult to decide rationally on the right type of annuity product for their circumstances.\n\nThere are two phases for a deferred annuity:\n\nDeferred annuities grow capital by investment in the accumulation phase (or deferral phase) and make payments during the distribution phase. A \"single premium deferred annuity\" (SPDA) allows a single deposit or premium at the issue of the annuity with only investment growth during the accumulation phase. A \"flexible premium deferred annuity\" (FPDA) allows additional payments or premiums following the initial premium during the accumulation phase.\n\nThe phases of an annuity can be combined in the fusion of a retirement savings and retirement payment plan: the annuitant makes regular contributions to the annuity until a certain date and then receives regular payments from it until death. Sometimes there is a life insurance component added so that if the annuitant dies before annuity payments begin, a beneficiary gets either a lump sum or annuity payments.\n\nAn annuity with only a distribution phase is an \"immediate annuity, single premium immediate annuity\" (SPIA), \"payout annuity\", or \"income annuity\". Such a contract is purchased with a single payment and makes payments until the death of the annuitant(s).\n\nAnnuities that make payments in fixed amounts or in amounts that increase by a fixed percentage are called fixed annuities. Variable annuities, by contrast, pay amounts that vary according to the investment performance of a specified set of investments, typically bond and equity mutual funds.\n\nVariable annuities are used for many different objectives. One common objective is deferral of the recognition of taxable gains. Money deposited in a variable annuity grows on a tax-deferred basis, so that taxes on investment gains are not due until a withdrawal is made. Variable annuities offer a variety of funds (\"subaccounts\") from various money managers. This gives investors the ability to move between subaccounts without incurring additional fees or sales charges.\n\nVariable annuities have been criticized for their high commissions, contingent deferred sale charges, tax deferred growth, high taxes on profits, and high annual costs. Sales abuses became so prevalent that in November 2007, the Securities and Exchange Commission approved FINRA Rule 2821 requiring brokers to determine specific suitability criteria when recommending the purchase or exchange (but not the surrender) of deferred variable annuities.\n\nA pure life annuity ceases to make payments on the death of the annuitant. A \"guaranteed annuity\" or \"life and certain annuity\", makes payments for at least a certain number of years (the \"period certain\"); if the annuitant outlives the specified period certain, annuity payments then continue until the annuitant's death, and if the annuitant dies before the expiration of the period certain, the annuitant's estate or beneficiary is entitled to collect the remaining payments certain. The tradeoff between the pure life annuity and the life-with-period-certain annuity is that in exchange for the reduced risk of loss, the annuity payments for the latter will be smaller.\n\n\"Joint-life\" and \"joint-survivor\" annuities make payments until the death of one or both of the annuitants respectively. For example, an annuity may be structured to make payments to a married couple, such payments ceasing on the death of the second spouse. In joint-survivor annuities, sometimes the instrument reduces the payments to the second annuitant after death of the first.\n\nThere has also been a significant growth in the development of \"impaired life\" annuities. These involve improving the terms offered due to a medical diagnosis which is severe enough to reduce life expectancy. A process of medical underwriting is involved and the range of qualifying conditions has increased substantially in recent years. Both conventional annuities and Purchase Life Annuities can qualify for impaired terms.\n\nValuation is the calculation of economic value or worth. Valuation of an annuity is calculated as the actuarial present value of the annuity, which is dependent on the probability of the annuitant living to each future payment period, as well as the interest rate and timing of future payments. Life tables provide the probabilities of survival necessary for such calculations.\n\nWith a \"single premium\" or \"immediate\" annuity, the \"annuitant\" pays for the annuity with a single lump sum. The annuity starts making regular payments to the annuitant within a year. A common use of a single premium annuity is as a destination for roll-over retirement savings upon retirement. In such a case, a retiree withdraws all of the money he/she has saved during working life in, for example, an Individual Retirement Account (IRA), and uses some or all of the money to buy an annuity whose payments will replace the retiree's wage payments for the rest of his/her life. The advantage of such an annuity is that the annuitant has a guaranteed income for life, whereas if the retiree were instead to withdraw money regularly from the retirement account (income drawdown), he/she might run out of money before death, or alternatively not have as much to spend while alive as could have been possible with an annuity purchase. Another common use for an income annuity is to pay recurring expenses, such as assisted living expenses, mortgage or insurance premiums.\n\nThe disadvantage of such an annuity is that the election is irrevocable and, because of inflation, a guaranteed income for life is not the same thing as guaranteeing a comfortable income for life.\n\nIn the United Kingdom conversion of pension income into an annuity was compulsory by the age of 75 until new legislation was introduced by the coalition government in April 2011. The new rules allow individuals to delay the decision to purchase an annuity indefinitely.\n\nIn the UK there are a large market of annuities of different types. The most common are those where the source of the funds required to buy the annuity is from a pension scheme. Examples of these types of annuity, often referred to as a Compulsory Purchase Annuity, are conventional annuities, with profit annuities and unit linked, or \"third way\" annuities. Annuities purchased from savings (i.e. not from a pension scheme) are referred to as Purchase Life Annuities and Immediate Vesting Annuities. In October 2009, the International Longevity Centre-UK published a report on Purchased Life Annuities (Time to Annuitise). \nIn the UK it has become common for life companies to base their annuity rates on an individual's location. Legal & General were the first company to do this in 2007.\n\nIn Canada the most common type of annuity is the life annuity, which is normally purchased by persons at their retirement age with tax-sheltered funds or with savings funds. The monthly payments from annuities with tax-sheltered funds are fully taxable when withdrawn as neither the capital or return thereon has been taxed in any way. Conversely income from annuities purchased with savings funds is divided between the return of capital and interest earned, with only the latter being taxable.\nAn annuity can be a single life annuity or a joint life annuity where the payments are guaranteed until the death of the second annuitant. It is regarded as ideal for retirees as it is the only income of any financial product that is fully guaranteed. In addition, while the monthly payments are for the upkeep and enjoyment of the annuitants, any guaranteed payments on non-registered annuities are continued to beneficiaries after the second death. This way the balance of the guaranteed payments supports family members and becomes a two-generation income.\n\nSome countries developed more options of value for this type of instrument than others. However, a 2005 study reported that some of the risks related to longevity are poorly managed \"practically everywhere\" due to governments backing away from defined benefit promises and insurance companies being reluctant to sell genuine life annuities because of fears that life expectancy will go up. Longevity insurance is now becoming more common in the UK and the U.S. (see Future of annuites, below) while Chile, in comparison to the U.S., has had a very large life annuity market for 20 years.\n\nIt is expected that the aging of the baby boomer generation in the US will increase the demand for this type of instrument and for it to be optimized for the annuitant. This growing market will drive improvements necessitating more research and development of instruments and increase insight into the mechanics involved on the part of the buying public. An example of increased scrutiny and discussion is that related to privatization of part of the U.S. Social Security Trust Fund.\n\nIn late 2010, discussions related to cutting Federal taxes raised anew the following concern: how much would an annuity cost a retiree if he or she had to replace his or her Social Security income? Assuming that the average benefit from Social Security is $14,000 per year, the replacement cost would be about $250,000 for a 66-year-old individual. The figures are based upon the individual receiving an inflation-adjusted stream that would pay for life and be insured.\n\nIn March 2011 a European Court of Justice ruling was made that prevents annuity providers from setting different premiums for men and women. Annuity rates for men are generally higher than those for women because they have shorter life expectancies. The change means that either annuity rates for men will fall or annuity rates for women will rise.\n\nIn the UK any annuities that are taken out after 21 December 2012 will have to comply with the ruling.\n\n\n"}
{"id": "4135185", "url": "https://en.wikipedia.org/wiki?curid=4135185", "title": "Liénard equation", "text": "Liénard equation\n\nIn mathematics, more specifically in the study of dynamical systems and differential equations, a Liénard equation is a second order differential equation, named after the French physicist Alfred-Marie Liénard.\n\nDuring the development of radio and vacuum tube technology, Liénard equations were intensely studied as they can be used to model oscillating circuits. Under certain additional assumptions Liénard's theorem guarantees the uniqueness and existence of a limit cycle for such a system.\n\nLet \"f\" and \"g\" be two continuously differentiable functions on R, with \"g\" an odd function and \"f\" an even function. Then the second order ordinary differential equation of the form\n\nis called the Liénard equation.\n\nThe equation can be transformed into an equivalent two-dimensional system of ordinary differential equations. We define \nthen\n\nis called a Liénard system.\n\nAlternatively, since Liénard equation itself is also an autonomous differential equation, the substitution formula_6 leads the Liénard equation to become a first order differential equation:\n\nwhich belongs to Abel equation of the second kind.\n\nThe Van der Pol oscillator\n\nis a Liénard equation. The solution of a Van der Pol oscillator has a limit cycle. Such cycle has a solution of a Liénard equation with negative formula_9 at small formula_10 and positive formula_9 otherwise. The Van der Pol equation has no exact, analytic solution. Such solution for a limit cycle exists if formula_9 is a constant piece-wise function.\n\nA Liénard system has a unique and stable limit cycle surrounding the origin if it satisfies the following additional properties:\n\n\n"}
{"id": "31393808", "url": "https://en.wikipedia.org/wiki?curid=31393808", "title": "Mehler kernel", "text": "Mehler kernel\n\n defined a function\n\\exp\\left(-\\frac{\\rho^2 (x^2+y^2)- 2\\rho xy}{(1-\\rho^2)}\\right)~, </math>\nand showed, in modernized notation, that it can be expanded in terms of Hermite polynomials (.) based on weight function exp(−²) as\n\nThis result is useful, in modified form, in quantum physics, probability theory, and harmonic analysis.\n\nIn physics, the fundamental solution, (Green's function), or of the Hamiltonian for the quantum harmonic oscillator is called the Mehler kernel. It provides the fundamental solution---the most general solution to \n\nThe orthonormal eigenfunctions of the operator are the Hermite functions,\nwith corresponding eigenvalues (2+1), furnishing particular solutions\n\nThe general solution is then a linear combination of these; when fitted to the initial condition , the general solution reduces to\nwhere the kernel has the separable representation\n\nUtilizing Mehler's formula then yields \n\nOn substituting this in the expression for with the value exp(−2) for , Mehler's kernel finally reads\n~\\exp\\Bigl(-\\coth(2t)~(x^2+y^2)/2 + \\text{cosech}(2t)~xy\\Bigr).</math>\n\nWhen = 0, variables and coincide, resulting in the limiting formula necessary by the initial condition,\nAs a fundamental solution, the kernel is additive,\n\nThis is further related to the symplectic rotation structure of the kernel .\n\nThe result of Mehler can also be linked to probability. For this, the variables should be rescaled as , , so as to change from the 'physicist's' Hermite polynomials (.) (with weight function exp(−²)) to \"probabilist's\" Hermite polynomials (.) (with weight function exp(−²/2)). Then, becomes\n\nThe left-hand side here is \"p(x,y)/p(x)p(y)\" where \"p(x,y)\" is the bivariate Gaussian probability density function for variables having zero means and unit variances: \nand are the corresponding probability densities of and .\n\nThere follows the usually quoted form of the result (Kibble 1945)\n\nThis expansion is most easily derived by using the two-dimensional Fourier transform of , which is\n\nThis may be expanded as\nThe Inverse Fourier transform then immediately yields the above expansion formula.\n\nThis result can be extended to the multidimensional case (Kibble 1945, Slepian 1972, Hörmander 1985 ).\n\nSince Hermite functions are orthonormal eigenfunctions of the Fourier transform,\nin harmonic analysis and signal processing, they diagonalize the Fourier operator,\n\nThus, the continuous generalization for real angle can be readily defined (Wiener, 1929; Condon, 1937), the fractional Fourier transform (FrFT), with kernel\n\nThis is a \"continuous family of linear transforms generalizing the Fourier transform\", such that, for , it reduces to the standard Fourier transform, and for to the inverse Fourier transform.\n\nThe Mehler formula, for = exp(−i), thus directly provides\nThe square root is defined such that the argument of the result lies in the interval [−\"π\" /2, \"π\" /2].\n\nIf is an integer multiple of , then the above cotangent and cosecant functions diverge. In the limit, the kernel goes to a Dirac delta function in the integrand, or , for an even or odd multiple of , respectively. Since formula_19[ ] = (−), formula_20[ ] must be simply or for an even or odd multiple of , respectively.\n\n\n"}
{"id": "6108841", "url": "https://en.wikipedia.org/wiki?curid=6108841", "title": "Microlocal analysis", "text": "Microlocal analysis\n\nIn mathematical analysis, microlocal analysis comprises techniques developed from the 1950s onwards based on Fourier transforms related to the study of variable-coefficients-linear and nonlinear partial differential equations. This includes generalized functions, pseudo-differential operators, wave front sets, Fourier integral operators, oscillatory integral operators, and paradifferential operators. \n\nThe term \"microlocal\" implies localisation not only with respect to location in the space, but also with respect to cotangent space directions at a given point. This gains in importance on manifolds of dimension greater than one. \n\n\n"}
{"id": "187446", "url": "https://en.wikipedia.org/wiki?curid=187446", "title": "Orientability", "text": "Orientability\n\nIn mathematics, orientability is a property of surfaces in Euclidean space that measures whether it is possible to make a consistent choice of surface normal vector at every point. A choice of surface normal allows one to use the right-hand rule to define a \"clockwise\" direction of loops in the surface, as needed by Stokes' theorem for instance. More generally, orientability of an abstract surface, or manifold, measures whether one can consistently choose a \"clockwise\" orientation for all loops in the manifold. Equivalently, a surface is orientable if a two-dimensional figure such as in the space cannot be moved (continuously) around the space and back to where it started so that it looks like its own mirror image .\n\nThe notion of orientability can be generalised to higher-dimensional manifolds as well. A manifold is orientable if it has a consistent choice of orientation, and a connected orientable manifold has exactly two different possible orientations. In this setting, various equivalent formulations of orientability can be given, depending on the desired application and level of generality. Formulations applicable to general topological manifolds often employ methods of homology theory, whereas for differentiable manifolds more structure is present, allowing a formulation in terms of differential forms. An important generalization of the notion of orientability of a space is that of orientability of a family of spaces parameterized by some other space (a fiber bundle) for which an orientation must be selected in each of the spaces which varies continuously with respect to changes in the parameter values.\n\nA surface \"S\" in the Euclidean space R is orientable if a two-dimensional figure (for example, ) cannot be moved around the surface and back to where it started so that it looks like its own mirror image (). Otherwise the surface is non-orientable. An abstract surface (i.e., a two-dimensional manifold) is orientable if a consistent concept of clockwise rotation can be defined on the surface in a continuous manner. That is to say that a loop going around one way on the surface can never be continuously deformed (without overlapping itself) to a loop going around the opposite way. This turns out to be equivalent to the question of whether the surface contains no subset that is homeomorphic to the Möbius strip. Thus, for surfaces, the Möbius strip may be considered the source of all non-orientability.\n\nFor an orientable surface, a consistent choice of \"clockwise\" (as opposed to counter-clockwise) is called an orientation, and the surface is called oriented. For surfaces embedded in Euclidean space, an orientation is specified by the choice of a continuously varying surface normal n at every point. If such a normal exists at all, then there are always two ways to select it: n or −n. More generally, an orientable surface admits exactly two orientations, and the distinction between an orient\"ed\" surface and an orient\"able\" surface is subtle and frequently blurred. An orientable surface is an abstract surface that admits an orientation, while an oriented surface is a surface that is abstractly orientable, and has the additional datum of a choice of one of the two possible orientations.\n\nMost surfaces we encounter in the physical world are orientable. Spheres, planes, and tori are orientable, for example. But Möbius strips, real projective planes, and Klein bottles are non-orientable. They, as visualized in 3-dimensions, all have just one side. The real projective plane and Klein bottle cannot be embedded in R, only immersed with nice intersections.\n\nNote that locally an embedded surface always has two sides, so a near-sighted ant crawling on a one-sided surface would think there is an \"other side\". The essence of one-sidedness is that the ant can crawl from one side of the surface to the \"other\" without going through the surface or flipping over an edge, but simply by crawling far enough.\n\nIn general, the property of being orientable is not equivalent to being two-sided; however, this holds when the ambient space (such as R above) is orientable. For example, a torus embedded in \n\ncan be one-sided, and a Klein bottle in the same space can be two-sided; here formula_2 refers to the Klein bottle.\n\nAny surface has a triangulation: a decomposition into triangles such that each edge on a triangle is glued to at most one other edge. Each triangle is oriented by choosing a direction around the perimeter of the triangle, associating a direction to each edge of the triangle. If this is done in such a way that, when glued together, neighboring edges are pointing in the opposite direction, then this determines an orientation of the surface. Such a choice is only possible if the surface is orientable, and in this case there are exactly two different orientations.\n\nIf the figure can be consistently positioned at all points of the surface without turning into its mirror image, then this will induce an orientation in the above sense on each of the triangles of the triangulation by selecting the direction of each of the triangles based on the order red-green-blue of colors of any of the figures in the interior of the triangle.\n\nThis approach generalizes to any \"n\"-manifold having a triangulation. However, some 4-manifolds do not have a triangulation, and in general for \"n\" > 4 some \"n\"-manifolds have triangulations that are inequivalent.\n\n\nIf \"H\"(\"S\") denotes the first homology group of a surface \"S\", then \"S\" is orientable if and only if \"H\"(\"S\") has a trivial torsion subgroup. More precisely, if \"S\" is orientable then \"H\"(\"S\") is a free abelian group, and if not then \"H\"(\"S\") = \"F\" + Z/2Z where \"F\" is free abelian, and the Z/2Z factor is generated by the middle curve in a Möbius band embedded in \"S\".\n\nLet \"M\" be a connected topological \"n\"-manifold. There are several possible definitions of what it means for \"M\" to be orientable. Some of these definitions require that \"M\" has extra structure, like being differentiable. Occasionally, must be made into a special case. When more than one of these definitions applies to \"M\", then \"M\" is orientable under one definition if and only if it is orientable under the others.\n\nThe most intuitive definitions require that \"M\" be a differentiable manifold. This means that the transition functions in the atlas of \"M\" are \"C\"-functions. Such a function admits a Jacobian determinant. When the Jacobian determinant is positive, the transition function is said to be orientation preserving. An oriented atlas on \"M\" is an atlas for which all transition functions are orientation preserving. \"M\" is orientable if it admits an oriented atlas. When , an orientation of \"M\" is a maximal oriented atlas. (When , an orientation of \"M\" is a function }.)\n\nOrientability and orientations can also be expressed in terms of the tangent bundle. The tangent bundle is a vector bundle, so it is a fiber bundle with structure group . That is, the transition functions of the manifold induce transition functions on the tangent bundle which are fiberwise linear transformations. If the structure group can be reduced to the group of positive determinant matrices, or equivalently if there exists an atlas whose transition functions determine an orientation preserving linear transformation on each tangent space, then the manifold \"M\" is orientable. Conversely, \"M\" is orientable if and only if the structure group of the tangent bundle can be reduced in this way. Similar observations can be made for the frame bundle.\n\nAnother way to define orientations on a differentiable manifold is through volume forms. A volume form is a nowhere vanishing section \"ω\" of , the top exterior power of the cotangent bundle of \"M\". For example, R has a standard volume form given by . Given a volume form on \"M\", the collection of all charts for which the standard volume form pulls back to a positive multiple of \"ω\" is an oriented atlas. The existence of a volume form is therefore equivalent to orientability of the manifold.\n\nVolume forms and tangent vectors can be combined to give yet another description of orientability. If is a basis of tangent vectors at a point \"p\", then the basis is said to be right-handed if . A transition function is orientation preserving if and only if it sends right-handed bases to right-handed bases. The existence of a volume form implies a reduction of the structure group of the tangent bundle or the frame bundle to . As before, this implies the orientability of \"M\". Conversely, if \"M\" is orientable, then local volume forms can be patched together to create a global volume form, orientability being necessary to ensure that the global form is nowhere vanishing.\n\nAt the heart of all the above definitions of orientability of a differentiable manifold is the notion of an orientation preserving transition function. This raises the question of what exactly such transition functions are preserving. They cannot be preserving an orientation of the manifold because an orientation of the manifold is an atlas, and it makes no sense to say that a transition function preserves or does not preserve an atlas of which it is a member.\n\nThis question can be resolved by defining local orientations. On a one-dimensional manifold, a local orientation around a point \"p\" corresponds to a choice of left and right near that point. On a two-dimensional manifold, it corresponds to a choice of clockwise and counter-clockwise. These two situations share the common feature that they are described in terms of top-dimensional behavior near \"p\" but not at \"p\". For the general case, let \"M\" be a topological \"n\"-manifold. A local orientation of \"M\" around a point \"p\" is a choice of generator of the group\nTo see the geometric significance of this group, choose a chart around \"p\". In that chart there is a neighborhood of \"p\" which is an open ball \"B\" around the origin \"O\". By the excision theorem, formula_4 is isomorphic to formula_5. The ball \"B\" is contractible, so its homology groups vanish except in degree zero, and the space is an -sphere, so its homology groups vanish except in degrees and . A computation with the long exact sequence in relative homology shows that the above homology group is isomorphic to formula_6. A choice of generator therefore corresponds to a decision of whether, in the given chart, a sphere around \"p\" is positive or negative. A reflection of through the origin acts by negation on formula_7, so the geometric significance of the choice of generator is that it distinguishes charts from their reflections.\n\nOn a topological manifold, a transition function is orientation preserving if, at each point \"p\" in its domain, it fixes the generators of formula_4. From here, the relevant definitions are the same as in the differentiable case. An oriented atlas is one for which all transition functions are orientation preserving, \"M\" is orientable if it admits an oriented atlas, and when , an orientation of \"M\" is a maximal oriented atlas.\n\nIntuitively, an orientation of \"M\" ought to define a unique local orientation of \"M\" at each point. This is made precise by noting that any chart in the oriented atlas around \"p\" can be used to determine a sphere around \"p\", and this sphere determines a generator of formula_4. Moreover any other chart around \"p\" is related to the first chart by an orientation preserving transition function, and this implies that the two charts yield the same generator, whence the generator is unique.\n\nPurely homological definitions are also possible. Assuming that \"M\" is closed and connected, \"M\" is orientable if and only if the \"n\"th homology group formula_10 is isomorphic to the integers Z. An orientation of \"M\" is a choice of generator of this group. This generator determines an oriented atlas by fixing a generator of the infinite cyclic group formula_11 and taking the oriented charts to be those for which pushes forward to the fixed generator. Conversely, an oriented atlas determines such a generator as compatible local orientations can be glued together to give a generator for the homology group formula_12.\n\nAround each point of \"M\" there are two local orientations. Intuitively, there is a way to move from a local orientation at a point to a local orientation at a nearby point : when the two points lie in the same coordinate chart , that coordinate chart defines compatible local orientations at and . The set of local orientations can therefore be given a topology, and this topology makes it into a manifold.\n\nMore precisely, let \"O\" be the set of all local orientations of \"M\". To topologize \"O\" we will specify a subbase for its topology. Let \"U\" be an open subset of \"M\" chosen such that formula_13 is isomorphic to Z. Assume that α is a generator of this group. For each \"p\" in \"U\", there is a pushforward function formula_14. The codomain of this group has two generators, and α maps to one of them. The topology on \"O\" is defined so that\nis open.\n\nThere is a canonical map that sends a local orientation at \"p\" to \"p\". It is clear that every point of \"M\" has precisely two preimages under . In fact, is even a local homeomorphism, because the preimages of the open sets \"U\" mentioned above are homeomorphic to the disjoint union of two copies of \"U\". If \"M\" is orientable, then \"M\" itself is one of these open sets, so \"O\" is the disjoint union of two copies of \"M\". If \"M\" is non-orientable, however, then \"O\" is connected and orientable. The manifold \"O\" is called the orientation double cover.\n\nIf \"M\" is a manifold with boundary, then an orientation of \"M\" is defined to be an orientation of its interior. Such an orientation induces an orientation of ∂\"M\". Indeed, suppose that an orientation of \"M\" is fixed. Let be a chart at a boundary point of \"M\" which, when restricted to the interior of \"M\", is in the chosen oriented atlas. The restriction of this chart to ∂\"M\" is a chart of ∂\"M\". Such charts form an oriented atlas for ∂\"M\".\n\nWhen \"M\" is smooth, at each point \"p\" of ∂\"M\", the restriction of the tangent bundle of \"M\" to ∂\"M\" is isomorphic to , where the factor of R is described by the inward pointing normal vector. The orientation of \"T\"∂\"M\" is defined by the condition that a basis of \"T\"∂\"M\" is positively oriented if and only if it, when combined with the inward pointing normal vector, defines a positively oriented basis of \"T\"\"M\".\n\nA closely related notion uses the idea of covering space. For a connected manifold \"M\" take \"M\", the set of pairs (\"x\", o) where \"x\" is a point of \"M\" and \"o\" is an orientation at \"x\"; here we assume \"M\" is either smooth so we can choose an orientation on the tangent space at a point or we use singular homology to define orientation. Then for every open, oriented subset of \"M\" we consider the corresponding set of pairs and define that to be an open set of \"M\". This gives \"M\" a topology and the projection sending (\"x\", o) to \"x\" is then a 2-to-1 covering map. This covering space is called the orientable double cover, as it is orientable. \"M\" is connected if and only if \"M\" is not orientable.\n\nAnother way to construct this cover is to divide the loops based at a basepoint into either orientation-preserving or orientation-reversing loops. The orientation preserving loops generate a subgroup of the fundamental group which is either the whole group or of index two. In the latter case (which means there is an orientation-reversing path), the subgroup corresponds to a connected double covering; this cover is orientable by construction. In the former case, one can simply take two copies of \"M\", each of which corresponds to a different orientation.\n\nA real vector bundle, which \"a priori\" has a GL(n) structure group, is called \"orientable\" when the structure group may be reduced to formula_16, the group of matrices with positive determinant. For the tangent bundle, this reduction is always possible if the underlying base manifold is orientable and in fact this provides a convenient way to define the orientability of a smooth real manifold: a smooth manifold is defined to be orientable if its tangent bundle is orientable (as a vector bundle). Note that as a manifold in its own right, the tangent bundle is \"always\" orientable, even over nonorientable manifolds.\n\nThe notion of orientability is essentially derived from the topology of the real general linear group \n\nan invertible transform of a real vector space is either orientation-preserving or orientation-reversing.\n\nThis holds not only for differentiable manifolds but for topological manifolds, as the space of self-homotopy equivalences of a sphere also has two connected components, which can be denoted the \"orientation-preserving\" and \"orientation-reversing\" maps.\n\nThe analogous notion for the symmetric group is the alternating group of even permutations.\n\nIn Lorentzian geometry, there are two kinds of orientability: space orientability and time orientability. These play a role in the causal structure of spacetime. In the context of general relativity, a spacetime manifold is space orientable if, whenever two right-handed observers head off in rocket ships starting at the same spacetime point, and then meet again at another point, they remain right-handed with respect to one another. If a spacetime is time-orientable then the two observers will always agree on the direction of time at both points of their meeting. In fact, a spacetime is time-orientable if and only if any two observers can agree which of the two meetings preceded the other.\n\nFormally, the pseudo-orthogonal group O(\"p\",\"q\") has a pair of characters: the space orientation character σ and the time orientation character σ,\nTheir product σ = σσ is the determinant, which gives the orientation character. A space-orientation of a pseudo-Riemannian manifold is identified with a section of the associated bundle\nwhere O(\"M\") is the bundle of pseudo-orthogonal frames. Similarly, a time orientation is a section of the associated bundle\n\n\n"}
{"id": "29168254", "url": "https://en.wikipedia.org/wiki?curid=29168254", "title": "Partial group algebra", "text": "Partial group algebra\n\nA partial group algebra is an associative algebra related to the partial representations of a group.\n\n\n\n"}
{"id": "15882673", "url": "https://en.wikipedia.org/wiki?curid=15882673", "title": "Plate notation", "text": "Plate notation\n\nIn Bayesian inference, plate notation is a method of representing variables that repeat in a graphical model. Instead of drawing each repeated variable individually, a plate or rectangle is used to group variables into a subgraph that repeat together, and a number is drawn on the plate to represent the number of repetitions of the subgraph in the plate. The assumptions are that the subgraph is duplicated that many times, the variables in the subgraph are indexed by the repetition number, and any links that cross a plate boundary are replicated once for each subgraph repetition.\n\nIn this example, we consider Latent Dirichlet allocation, a Bayesian network that models how documents in a corpus are topically related. There are two variables not in any plate; \"α\" is the parameter of the uniform Dirichlet prior on the per-document topic distributions, and \"β\" is the parameter of the uniform Dirichlet prior on the per-topic word distribution.\n\nThe outermost plate represents all the variables related to a specific document, including formula_1, the topic distribution for document \"i\". The \"M\" in the corner of the plate indicates that the variables inside are repeated \"M\" times, once for each document. The inner plate represents the variables associated with each of the formula_2 words in document \"i\": formula_3 is the topic for the \"j\"th word in document \"i\", and formula_4 is the actual word used.\n\nThe \"N\" in the corner represents the repetition of the variables in the inner plate formula_2 times, once for each word in document \"i\". The circle representing the individual words is shaded, indicating that each formula_4 is observable, and the other circles are empty, indicating that the other variables are latent variables. The directed edges between variables indicate dependencies between the variables: for example, each formula_4 depends on formula_3 and \"β\".\n\nA number of extensions have been created by various authors to express more information than simply the conditional relationships. However, few of these have become standard. Perhaps the most commonly used extension is to use rectangles in place of circles to indicate non-random variables—either parameters to be computed, hyperparameters given a fixed value (or computed through empirical Bayes), or variables whose values are computed deterministically from a random variable.\n\nThe diagram on the right shows a few more non-standard conventions used in some articles in Wikipedia (e.g. variational Bayes):\n\nPlate notation has been implemented in various TeX/LaTeX drawing packages, but also as part of graphical user interfaces to Bayesian statistics programs such as BUGS and BayesiaLab.\n"}
{"id": "63218", "url": "https://en.wikipedia.org/wiki?curid=63218", "title": "Present value", "text": "Present value\n\nIn economics and finance, present value (PV), also known as present discounted value, is the value of an expected income stream determined as of the date of valuation. The present value is always less than or equal to the future value because money has interest-earning potential, a characteristic referred to as the time value of money, except during times of negative interest rates, when the present value will be more than the future value. Time value can be described with the simplified phrase, \"A dollar today is worth more than a dollar tomorrow\". Here, 'worth more' means that its value is greater. A dollar today is worth more than a dollar tomorrow because the dollar can be invested and earn a day's worth of interest, making the total accumulate to a value more than a dollar by tomorrow. Interest can be compared to rent. Just as rent is paid to a landlord by a tenant, without the ownership of the asset being transferred, interest is paid to a lender by a borrower who gains access to the money for a time before paying it back. By letting the borrower have access to the money, the lender has sacrificed the exchange value of this money, and is compensated for it in the form of interest. The initial amount of the borrowed funds (the present value) is less than the total amount of money paid to the lender.\n\nPresent value calculations, and similarly future value calculations, are used to value loans, mortgages, annuities, sinking funds, perpetuities, bonds, and more. These calculations are used to make comparisons between cash flows that don’t occur at simultaneous times, since time dates must be consistent in order to make comparisons between values. When deciding between projects in which to invest, the choice can be made by comparing respective present values of such projects by means of discounting the expected income streams at the corresponding project interest rate, or rate of return. The project with the highest present value, i.e. that is most valuable today, should be chosen.\n\nThe traditional method of valuing future income streams as a present capital sum is to multiply the average expected annual cash-flow by a multiple, known as \"years' purchase\". For example, in selling to a third party a property leased to a tenant under a 99-year lease at a rent of $10,000 per annum, a deal might be struck at \"20 years' purchase\", which would value the lease at 20 * $10,000, i.e. $200,000. This equates to a present value discounted in perpetuity at 5%. For a riskier investment the purchaser would demand to pay a lower number of years' purchase. This was the method used for example by the English crown in setting re-sale prices for manors seized at the Dissolution of the Monasteries in the early 16th century. The standard usage was 20 years' purchase.\n\nIf offered a choice between $100 today or $100 in one year, and there is a positive real interest rate throughout the year, \"ceteris paribus\", a rational person will choose $100 today. This is described by economists as time preference. Time preference can be measured by auctioning off a risk free security—like a US Treasury bill. If a $100 note with a zero coupon, payable in one year, sells for $80 now, then $80 is the present value of the note that will be worth $100 a year from now. This is because money can be put in a bank account or any other (safe) investment that will return interest in the future.\n\nAn investor who has some money has two options: to spend it right now or to save it. But the financial compensation for saving it (and not spending it) is that the money value will accrue through the compound interest that he will receive from a borrower (the bank account on which he has the money deposited).\n\nTherefore, to evaluate the real value of an amount of money today after a given period of time, economic agents compound the amount of money at a given (interest) rate. Most actuarial calculations use the risk-free interest rate which corresponds to the minimum guaranteed rate provided by a bank's saving account for example, assuming no risk of default by the bank to return the money to the account holder on time. To compare the change in purchasing power, the real interest rate (nominal interest rate minus inflation rate) should be used.\n\nThe operation of evaluating a present value into the future value is called a capitalization (how much will $100 today be worth in 5 years?). The reverse operation—evaluating the present value of a future amount of money—is called a discounting (how much will $100 received in 5 years—at a lottery for example—be worth today?).\n\nIt follows that if one has to choose between receiving $100 today and $100 in one year, the rational decision is to choose the $100 today. If the money is to be received in one year and assuming the savings account interest rate is 5%, the person has to be offered at least $105 in one year so that the two options are equivalent (either receiving $100 today or receiving $105 in one year). This is because if $100 is deposited in a savings account, the value will be $105 after one year, again assuming no risk of losing the initial amount through bank default.\n\nInterest is the additional amount of money gained between the beginning and the end of a time period. Interest represents the time value of money, and can be thought of as rent that is required of a borrower in order to use money from a lender. For example, when an individual takes out a bank loan, they are charged interest. Alternatively, when an individual deposits money into a bank, their money earns interest. In this case, the bank is the borrower of the funds and is responsible for crediting interest to the account holder. Similarly, when an individual invests in a company (through corporate bonds, or through stock), the company is borrowing funds, and must pay interest to the individual (in the form of coupon payments, dividends, or stock price appreciation).\nThe interest rate is the change, expressed as a percentage, in the amount of money during one compounding period. A compounding period is the length of time that must transpire before interest is credited, or added to the total. For example, interest that is compounded annually is credited once a year, and the compounding period is one year. Interest that is compounded quarterly is credited four times a year, and the compounding period is three months. A compounding period can be any length of time, but some common periods are annually, semiannually, quarterly, monthly, daily, and even continuously.\n\nThere are several types and terms associated with interest rates:\n\nThe operation of evaluating a present sum of money some time in the future is called a capitalization (how much will 100 today be worth in 5 years?). The reverse operation—evaluating the present value of a future amount of money—is called discounting (how much will 100 received in 5 years be worth today?).\n\nSpreadsheets commonly offer functions to compute present value. In Microsoft Excel, there are present value functions for single payments - \"=NPV(...)\", and series of equal, periodic payments - \"=PV(...)\". Programs will calculate present value flexibly for any cash flow and interest rate, or for a schedule of different interest rates at different times.\n\nThe most commonly applied model of present valuation uses compound interest. The standard formula is:\n\nWhere formula_2 is the future amount of money that must be discounted, formula_3 is the number of compounding periods between the present date and the date where the sum is worth formula_2, formula_5 is the interest rate for one compounding period (the end of a compounding period is when interest is applied, for example, annually, semiannually, quarterly, monthly, daily). The interest rate, formula_5, is given as a percentage, but expressed as a decimal in this formula.\n\nOften, formula_7 is referred to as the Present Value Factor \n\nThis is also found from the formula for the future value with negative time.\n\nFor example, if you are to receive $1000 in 5 years, and the effective annual interest rate during this period is 10% (or 0.10), then the present value of this amount is\n\nThe interpretation is that for an effective annual interest rate of 10%, an individual would be indifferent to receiving $1000 in 5 years, or $620.92 today.\n\nThe purchasing power in today's money of an amount formula_2 of money, formula_3 years into the future, can be computed with the same formula, where in this case formula_5 is an assumed future inflation rate.\n\nA cash flow is an amount of money that is either paid out or received, differentiated by a negative or positive sign, at the end of a period. Conventionally, cash flows that are received are denoted with a positive sign (total cash has increased) and cash flows that are paid out are denoted with a negative sign (total cash has decreased). The cash flow for a period represents the net change in money of that period. Calculating the net present value, formula_12, of a stream of cash flows consists of discounting each cash flow to the present, using the present value factor and the appropriate number of compounding periods, and combining these values.\n\nFor example, if a stream of cash flows consists of +$100 at the end of period one, -$50 at the end of period two, and +$35 at the end of period three, and the interest rate per compounding period is 5% (0.05) then the present value of these three Cash Flows are\n\nThus the net present value would be\n\nThere are a few considerations to be made.\n\n\nHere, formula_20 is the nominal annual interest rate, compounded quarterly, and the interest rate per quarter is formula_21\n\nMany financial arrangements (including bonds, other loans, leases, salaries, membership dues, annuities including annuity-immediate and annuity-due, straight-line depreciation charges) stipulate structured payment schedules; payments of the same amount at regular time intervals. Such an arrangement is called an annuity. The expressions for the present value of such payments are summations of geometric series.\n\nThere are two types of annuities: an annuity-immediate and annuity-due. For an annuity immediate, formula_22 payments are received (or paid) at the end of each period, at times 1 through formula_22, while for an annuity due, formula_22 payments are received (or paid) at the beginning of each period, at times 0 through formula_25. This subtle difference must be accounted for when calculating the present value.\n\nAn annuity due is an annuity immediate with one more interest-earning period. Thus, the two present values differ by a factor of formula_26:\n\nThe present value of an annuity immediate is the value at time 0 of the stream of cash flows:\n\nwhere:\n\nThe above formula (1) for annuity immediate calculations offers little insight for the average user and requires the use of some form of computing machinery. There is an approximation which is less intimidating, easier to compute and offers some insight for the non-specialist. It is given by \nWhere, as above, C is annuity payment, PV is principal, n is number of payments, starting at end of first period, and i is interest rate per period. Equivalently C is the periodic loan repayment for a loan of PV extending over n periods at interest rate, i. The formula is valid (for positive n, i) for ni≤3. For completeness, for ni≥3 the approximation is formula_33.\n\nThe formula can, under some circumstances, reduce the calculation to one of mental arithmetic alone. For example, what are the (approximate) loan repayments for a loan of PV= $10,000 repaid annually for n= 10 years at 15% interest (i=0.15)? The applicable approximate formula is C ≈10,000*(1/10 + (2/3) 0.15) =10,000*(0.1+0.1) =10,000*0.2 =$2000 pa by mental arithmetic alone. The true answer is $1993, very close.\n\nThe overall approximation is accurate to within ±6% (for all n≥1) for interest rates 0≤ i≤0.20 and within ±10% for interest rates 0.20≤i≤0.40. It is, however, intended only for \"rough\" calculations.\n\nA perpetuity refers to periodic payments, receivable indefinitely, although few such instruments exist. The present value of a perpetuity can be calculated by taking the limit of the above formula as \"n\" approaches infinity.\n\nFormula (2) can also be found by subtracting from (1) the present value of a perpetuity delayed n periods, or directly by summing the present value of the payments\n\nwhich form a geometric series.\n\nAgain there is a distinction between a perpetuity immediate – when payments received at the end of the period – and a perpetuity due – payment received at the beginning of a period. And similarly to annuity calculations, a perpetuity due and a perpetuity immediate differ by a factor of formula_36:\n\nA corporation issues a bond, an interest earning debt security, to an investor to raise funds. The bond has a face value, formula_38, coupon rate, formula_39, and maturity date which in turn yields the number of periods until the debt matures and must be repaid. A bondholder will receive coupon payments semiannually (unless otherwise specified) in the amount of formula_40, until the bond matures, at which point the bondholder will receive the final coupon payment and the face value of a bond, formula_41. The present value of a bond is the purchase price. The purchase price is equal to the bond's face value if the coupon rate is equal to the current interest rate of the market, and in this case, the bond is said to be sold 'at par'. If the coupon rate is less than the market interest rate, the purchase price will be less than the bond's face value, and the bond is said to have been sold 'at a discount', or below par. Finally, if the coupon rate is greater than the market interest rate, the purchase price will be greater than the bond's face value, and the bond is said to have been sold 'at a premium', or above par. The purchase price can be computed as:\n\nPresent value is additive. The present value of a bundle of cash flows is the sum of each one's present value.\n\nIn fact, the present value of a cashflow at a constant interest rate is mathematically one point in the Laplace transform of that cashflow, evaluated with the transform variable (usually denoted \"s\") equal to the interest rate. The full Laplace transform is the curve of all present values, plotted as a function of interest rate. For discrete time, where payments are separated by large time periods, the transform reduces to a sum, but when payments are ongoing on an almost continual basis, the mathematics of continuous functions can be used as an approximation.\n\nThese calculations must be applied carefully, as there are underlying assumptions:\n\nSee time value of money for further discussion.\n\nThere are mainly two flavors of Present Value. Whenever there will be uncertainties in both timing and amount of the cash flows, the expected present value approach will often be the appropriate technique. \n\nThe interest rate used is the risk-free interest rate if there are no risks involved in the project. The rate of return from the project must equal or exceed this rate of return or it would be better to invest the capital in these risk free assets. If there are risks involved in an investment this can be reflected through the use of a risk premium. The risk premium required can be found by comparing the project with the rate of return required from other projects with similar risks. Thus it is possible for investors to take account of any uncertainty involved in various investments.\n\nAn investor, the lender of money, must decide the financial project in which to invest their money, and present value offers one method of deciding.\nA financial project requires an initial outlay of money, such as the price of stock or the price of a corporate bond. The project claims to return the initial outlay, as well as some surplus (for example, interest, or future cash flows). An investor can decide which project to invest in by calculating each projects’ present value (using the same interest rate for each calculation) and then comparing them. The project with the smallest present value – the least initial outlay – will be chosen because it offers the same return as the other projects for the least amount of money.\n\n"}
{"id": "22963939", "url": "https://en.wikipedia.org/wiki?curid=22963939", "title": "Publicationes Mathematicae Debrecen", "text": "Publicationes Mathematicae Debrecen\n\nPublicationes Mathematicae Debrecen is a Hungarian mathematical journal, edited and published in Debrecen, at the Mathematical Institute of the University of Debrecen. It was founded by Alfréd Rényi, Tibor Szele, and Ottó Varga in 1950. The current editor-in-chief is Lajos Tamássy.\n\n"}
{"id": "2696396", "url": "https://en.wikipedia.org/wiki?curid=2696396", "title": "Rational trigonometry", "text": "Rational trigonometry\n\nRational trigonometry is a proposed reformulation of metrical planar and solid geometries (which includes trigonometry) by Canadian mathematician Norman J. Wildberger, currently a professor of mathematics at the University of New South Wales. His ideas are set out in his 2005 book \"Divine Proportions: Rational Trigonometry to Universal Geometry\". According to \"New Scientist\", part of his motivation for an alternative to traditional trigonometry was to avoid some problems that he claims occur when infinite series are used in mathematics. Rational trigonometry avoids direct use of transcendental functions like sine and cosine by substituting their squared equivalents. Wildberger draws inspiration from mathematicians predating Georg Cantor's infinite set-theory, like Gauss and Euclid, who he claims were far more wary of using infinite sets than modern mathematicians. To date, rational trigonometry is largely unmentioned in mainstream mathematical literature.\n\nRational trigonometry follows an approach built on the methods of linear algebra to the topics of elementary (high school level) geometry. Distance is replaced with its squared value (quadrance) and 'angle' is replaced with the squared value of the usual sine ratio (spread) associated to either angle between two lines. (The complement of Spread, known as cross, also corresponds to a scaled form of the inner product between line segments taken as vectors). The three main laws in trigonometry – Pythagoras's theorem, the sine law and the cosine law – are given in rational (square-equivalent) form, and are augmented by two further laws – the triple quad formula (relating the quadrances of three collinear points) and the triple spread formula (relating the spreads of three concurrent lines) –, giving the five main laws of the subject.\n\nRational trigonometry is otherwise broadly based on Cartesian analytic geometry, with \"a point\" defined as an ordered pair of rational numbers\n\nand \"a line\" \n\nas a general linear equation with rational coefficients , and .\n\nBy avoiding calculations that rely on square root operations giving only \"approximate\" distances between points, or standard trigonometric functions (and their inverses), giving only truncated polynomial \"approximations\" of angles (or their projections) geometry becomes entirely algebraic. There is no assumption, in other words, of the existence of real number solutions to problems, with results instead given over the field of rational numbers, their algebraic field extensions, or finite fields. Following this, it is claimed, makes many classical results of Euclidean geometry applicable in \"rational\" form (as quadratic analogs) over any field not of characteristic two.\n\nThe book \"Divine Proportions\" shows the application of calculus using rational trigonometric functions, including three-dimensional volume calculations. It also deals with rational trigonometry's application to situations involving irrationals, such as the proof that Platonic Solids all have rational 'spreads' between their faces.\n\nRational trigonometry (\"RT\") is mentioned in only a modest number of mathematical publications besides Wildberger's own articles and book. \"Divine Proportions\" was dismissed by reviewer Paul J. Campbell, in the \"Mathematics Magazine\" of the Mathematical Association of America (MAA): \"the author claims that this new theory will take 'less than half the usual time to learn'; but I doubt it. and it would still have to be interfaced with the traditional concepts and notation.\" Reviewer William Barker, Isaac Henry Wing Professor of Mathematics at Bowdoin College, also writing for the MAA, was more approving: \"\"Divine Proportions\" is unquestionably a valuable addition to the mathematics literature. It carefully develops a thought provoking, clever, and useful alternate approach to trigonometry and Euclidean geometry. It would not be surprising if some of its methods ultimately seep into the standard development of these subjects. However, unless there is an unexpected shift in the accepted views of the foundations of mathematics, there is not a strong case for rational trigonometry to replace the classical theory\" \"New Scientist\"'s Amanda Gefter described the approach of Wildberger as an example of finitism. James Franklin in the \"Mathematical Intelligencer\" argued that the book deserved careful consideration.\n\nAn analysis by Michael Gilsdorf of the example problems given by Wildberger in an early paper disputed the claim that \"RT\" required fewer steps to solve \"most\" problems, if free selection of classical methods (such as the 'shoelace formula' for the area of a triangle from the coordinates of its vertices or applying a special case of Stewart's theorem directly to a triangle with a median) is allowed to optimize the solution of problems. Concerning pedagogy, and whether using the quadratic quantities introduced by \"RT\" offers real benefits over traditional learning, the author observed that classical trigonometry was not initially based on use of Taylor series to approximate angles at all, but rather on measurements of chord (twice the sine of an angle) and thus with a proper understanding students could reap continued advantages from use of linear measurement without the claimed \"logical\" inconsistencies when circular parametrization by angle is subsequently introduced.\n\nQuadrance and distance (as its square root) both measure separation of points in Euclidean space. Following Pythagoras's theorem, the quadrance of two points and in a plane is therefore defined as the sum of squares of differences in the formula_3 and formula_4 coordinates:\n\nThe triangle inequality formula_6 is expressed under rational trigonometry as formula_7.\n\nSpread gives one measure to the separation of two lines as a single dimensionless number in the range (from \"parallel\" to \"perpendicular\") for Euclidean geometry. It replaces the concept of (and has several differences from) angle discussed in the section below. Descriptions of spread may include:\n\n\n\n\nSuppose two lines, and , intersect at the point as shown at right. Choose a point on and let be the foot of the perpendicular from to . Then the spread is\n\nLike angle, spread depends only on the relative slopes of two lines (constant terms being eliminated) and is invariant under translation (i.e. it is preserved when lines are moved keeping parallel with themselves). So given two lines whose equations are\n\nwe may rewrite them as two lines which meet at the origin with equations\n\nIn this position the point satisfies the first equation and satisfies the second and the three points , and forming the spread will give three quadrances:\n\nThe \"cross law\" – see below – in terms of spread is\n\nwhich becomes:\n\nThis simplifies, in the numerator, to , giving:\n\nThen, using the Brahmagupta–Fibonacci identity\n\nthe standard expression for spread in terms of slopes (or directions) of two lines becomes\n\nIn this form (and in its Cartesian equivalent that follows) a spread is the ratio of the square of a determinant of two vectors (numerator) to the product of their quadrances (denominator)\n\nThis replaces with , with and the origin , as the point of intersection of two lines, with in the previous result:\n\nUnlike angle, which can define a relationship between \"rays\" emanating from a point, by an arc measurement parametrization, and where a pair of lines can be considered four pairs of rays, forming four angles, 'spread' is more fundamental in rational trigonometry, describing \"two lines\" by a single measure of a rational function (see above). Being equivalent to the \"square\" of a sine of the corresponding angle (and to the haversine of the chord-based double-angle ), the spread of both an angle and its supplementary angle are equal.\n\nSpread is not proportional, however, to the separation between lines as angle would be; with spreads of 0, , , , and 1 corresponding to unevenly spaced angles 0°, 30°, 45°, 60° and 90°.\n\nInstead, (recalling the supplementary property) two equal, co-terminal spreads determine a third spread, whose value will be a solution of the triple spread formula for a triangle (or three concurrent lines) having spreads of , and :\n\ngiving the quadratic polynomial (in ):\n\nand solutions\n\nThis is equivalent to the trigonometric identity :\n\nof the angles , and of a triangle, using\n\nto denote a \"second\" spread polynomial in .\n\nFinding the triple of a spread likewise makes use of the triple spread formula as a quadratic equation in the unknown third spread treating the known spreads and (the previous solution) as constants. This turns out (after eliminating the 'smaller' solution ) to be:\n\nFurther multiples of any basic spread of lines can either be generated by continuing use of the triple spread formula in this way, or by use of a recursion formula (see below) which applies it indirectly. Wheras any multiple of a spread that is rational will be polynomial in that spread (and therefore rational), the converse does not apply. For example, by the half-angle formula, two lines meeting at a 15° (or 165°) angle have spread of:\n\nand thus exists by algebraic extension of the rational numbers.\n\nAs seen for double and triple spreads, an th multiple of any spread, gives a polynomial in that spread, denoted , as one solution to the triple spread formula.\n\nIn the conventional language of circular functions, these th-degree \"spread polynomials\", for , can be characterized by the identity:\n\n\nFrom the definition it immediately follows that\n\nSince the triple spread formula formula_30 is an equation whose entries can be spread polynomials of the form :formula_31, formula_32 and formula_33,\n\ntaking the difference of the expressions \n\nand rearranging, gives a recursive relation:\n\nThe spread polynomials are related to the Chebyshev polynomials of the first kind, , by the identity\n\nThis implies\n\nThe second equality above follows from the identity\n\non Chebyshev polynomials.\n\nThe spread polynomials satisfy the composition identity\n\nWhen the coefficients are taken to be members of the finite field , then the sequence of spread polynomials is periodic with period . In other words, if , then , for all .\n\nWhen the coefficients are taken to be real, then for , we have\n\nFor , the integral is unless , in which case it is .\n\nThe ordinary generating function is\n\nThe exponential generating function is\n\n satisfies the second-order linear nonhomogeneous differential equation\n\nFor every integer and every prime , there is a natural number such that is divisible by precisely when divides . This number is a divisor of either or . The proof of this number theoretical property was first given in a paper by Shuxiang Goh and N. J. Wildberger. It involves considering the projective analogue to quadrance in the finite projective line .\n\nThe first several spread polynomials are as follows:\n\nWildberger states that there are five basic laws in rational trigonometry. He also states that these laws can be verified using high-school level mathematics. Some are equivalent to standard trigonometrical formulae with the variables expressed as quadrance and spread.\n\nIn the following five formulae, we have a triangle made of three points . The spreads of the angles at those points are , and , are the quadrances of the triangle sides opposite , respectively. As in classical trigonometry, if we know three of the six elements , , and these three are not the three , then we can compute the other three.\n\nThe three points are collinear if and only if:\n\nwhere represent the quadrances between respectively. It can either be proved by analytic geometry (the preferred means within rational trigonometry) or derived from Heron's formula, using the condition for collinearity that the triangle formed by the three points has zero area.\n\nThe line has the general form:\n\nwhere the (non-unique) parameters can be expressed in terms of the coordinates of points and as:\n\nso that, everywhere on the line:\n\nBut the line can also be specified by two simultaneous equations in a parameter , where at point and at point :\n\nor, in terms of the original parameters:\n\nIf the point is collinear with points and , there exists some value of (for distinct points, not equal to 0 or 1), call it , for which these two equations are simultaneously satisfied at the coordinates of the point , such that:\n\nNow, the quadrances of the three line segments are given by the squared differences of their coordinates, which can be expressed in terms of :\n\nwhere use was made of the fact that .\n\nSubstituting these quadrances into the equation to be proved:\n\nNow, if and represent distinct points, such that , we may divide both sides by :\n\nThe lines (of quadrance ) and (of quadrance ) are perpendicular (their spread is 1) if and only if:\n\nwhere is the quadrance between and .\n\nThis is equivalent to the Pythagorean theorem (and its converse).\n\nThere are many classical proofs of Pythagoras's theorem; this one is framed in the terms of rational trigonometry.\n\nThe \"spread\" of an angle is the square of its sine. Given the triangle with a spread of 1 between sides and ,\n\nwhere is the \"quadrance\", i.e. the square of the distance.\n\nConstruct a line dividing the spread of 1, with the point on line , and making a spread of 1 with and . The triangles , and are similar (have the same spreads but not the same quadrances).\n\nThis leads to two equations in ratios, based on the spreads of the sides of the triangle:\n\nNow in general, the two spreads resulting from dividing a spread into two parts, as line does for spread , do not add up to the original spread since spread is a non-linear function. So we first prove that dividing a spread of 1, results in two spreads that do add up to the original spread of 1.\n\nFor convenience, but with no loss of generality, we orient the lines intersecting with a spread of 1 to the coordinate axes, and label the dividing line with coordinates and . Then the two spreads are given by:\n\nHence\n\nso that\n\nUsing the first two ratios from the first set of equations, this can be rewritten:\n\nMultiplying both sides by :\n\nQ.E.D.\nFor any triangle with nonzero quadrances:\n\nThis is the law of sines, just squared.\n\nFor any triangle ,\n\nThis is analogous to the law of cosines. It is called the 'cross law' because , the square of the cosine of the angle, is called the 'cross'.\n\nFor any triangle ,\n\nThis relation can be derived from the formula for the sine of a compound angle: in a triangle (whose three angles sum to 180°) we have,\n\nEquivalently, it describes the relationship between the spreads of three concurrent lines, as spread (like angle) is unaffected when the sides of a triangle are moved parallel to themselves to meet in a common point.\n\nKnowing two spreads allows the third to be calculated by solving the associated quadratic formula but, since two solutions are possible, further \"triangle spread rules\" must be used to select the appropriate one. (The compexity of this method contrasts with obtaining a supplementary angle directly by subtracting.)\n\nAs the laws of rational trigonometry give algebraic (and not transcendental) relations, they apply in generality to algebraic number fields beyond the rational numbers. Specifically, any finite field which does not have characteristic 2 reproduces a form of these laws, and thus a finite field geometry. The 'plane' formed by a finite field is the cartesian product of all ordered pairs of field elements, with opposite edges identified forming the surface topologically equivalent to a discretized torus. Individual elements correspond to standard 'points' and 'lines' to sets of no more than formula_68 points related by incidence (an initial point) plus direction or slope given in lowest terms (say all points '2 over and 1 up') that 'wrap' the plane before repeating.\n\nThe figure (right) shows a \"triangle\" of three such lines in the finite field setting :\n\nEach line has its own symbol and the intersections of lines (\"vertices\") is marked by \"two\" symbols present at points:\n\nUsing \"Pythagoras's theorem\" with arithmetic modulo 13, we find these sides have quadrances of:\n\nRearranging the cross law as \ngives separate expressions for each spread, in terms of the three quadrances:\n\nIn turn we note these ratios are all equal – as per the spread law (at least in mod 13):\n\nSince first and last ratios match (making the triangle \"isosceles\") we just cross multiply, and take differences, to show equality with the middle ratio also:\n\nOtherwise, the standard Euclidean plane is taken to consist of just rational points, , omitting any non-algebraic numbers as solutions. Properties like incidence of objects, representing the solutions or 'content' of geometric theorems, therefore follow a number theoretic approach that differs and is more restrictive than one allowing real numbers. For instance, \"not all\" lines passing through a circle's centre are considered to meet the circle at its circumference. To be incident such lines must be of the form\nand necessarily meet the circle in a \"rational\" point.\n\nRational trigonometry makes nearly all problems solvable with only addition, subtraction, multiplication or division, as trigonometric functions (of angle) are purposefully avoided in favour of trigonometric ratios in quadratic form. At most, therefore, results required as distance (or angle) can be approximated from an exact-valued rational equivalent of quadrance (or spread) after these simpler operations have been carried out. To make use of this advantage however, each problem must either be given, or set up, in terms of prior quadrances and spreads, which entails additional work.\n\nThe laws of rational trigonometry, being algebraic and 'exact-valued', introduce subtleties into the solutions of problems, such as the non-additivity of quadrances of collinear points (in the case of the triple quad formula) or the spreads of concurrent lines (in the case of the triple spread formula) absent from the classical subject, where linearity is incorporated into distance and circular measure of angles, albeit 'transcendental' techniques, necessitating approximation in results.\n\n\n\n"}
{"id": "8737421", "url": "https://en.wikipedia.org/wiki?curid=8737421", "title": "Series acceleration", "text": "Series acceleration\n\nIn mathematics, series acceleration is one of a collection of sequence transformations for improving the rate of convergence of a series. Techniques for series acceleration are often applied in numerical analysis, where they are used to improve the speed of numerical integration. Series acceleration techniques may also be used, for example, to obtain a variety of identities on special functions. Thus, the Euler transform applied to the hypergeometric series gives some of the classic, well-known hypergeometric series identities.\n\nGiven a sequence \n\nhaving a limit\n\nan accelerated series is a second sequence \n\nwhich converges faster to formula_4 than the original sequence, in the sense that \n\nIf the original sequence is divergent, the sequence transformation acts as an extrapolation method to the antilimit formula_4.\n\nThe mappings from the original to the transformed series may be linear (as defined in the article sequence transformations), or non-linear. In general, the non-linear sequence transformations tend to be more powerful.\n\nTwo classical techniques for series acceleration are Euler's transformation of series and Kummer's transformation of series. A variety of much more rapidly convergent and special-case tools have been developed in the 20th century, including Richardson extrapolation, introduced by Lewis Fry Richardson in the early 20th century but also known and used by Katahiro Takebe in 1722, the Aitken delta-squared process, introduced by Alexander Aitken in 1926 but also known and used by Takakazu Seki in the 18th century, the epsilon method given by Peter Wynn in 1956, the Levin u-transform, and the Wilf-Zeilberger-Ekhad method or WZ method.\n\nFor alternating series, several powerful techniques, offering convergence rates from formula_7 all the way to formula_8 for a summation of formula_9 terms, are described by Cohen \"et al.\".\n\nA basic example of a linear sequence transformation, offering improved convergence, is Euler's transform. It is intended to be applied to an alternating series; it is given by \n\nwhere formula_11 is the forward difference operator:\n\nIf the original series, on the left hand side, is only slowly converging, the forward differences will tend to become small quite rapidly; the additional power of two further improves the rate at which the right hand side converges.\n\nA particularly efficient numerical implementation of the Euler transform is the van Wijngaarden transformation.\n\nA series\n\ncan be written as f(1), where the function f(z) is defined as\n\nThe function f(z) can have singularities in the complex plane (branch point singularities, poles or essential singularities), which limit the radius of convergence of the series. If the point z = 1 is close to or on the boundary of the disk of convergence, the series for S will converge very slowly. One can then improve the convergence of the series by means of a conformal mapping that moves the singularities such that the point that is mapped to z = 1, ends up deeper in the new disk of convergence.\n\nThe conformal transform formula_15 needs to be chosen such that formula_16, and one usually chooses a function that has a finite derivative at w = 0. One can assume that formula_17 without loss of generality, as one can always rescale w to redefine formula_18. We then consider the function\n\nSince formula_17, we have f(1) = g(1). We can obtain the series expansion of g(w) by putting formula_21 in the series expansion of f(z) because formula_16; the first n terms of the series expansion for f(z) will yield the first n terms of the series expansion for g(w) if formula_23. Putting w = 1 in that series expansion will thus yield a series such that if it converges, it will converge to the same value as the original series.\n\nExamples of such nonlinear sequence transformations are Padé approximants, the Shanks transformation, and Levin-type sequence transformations.\n\nEspecially nonlinear sequence transformations often provide powerful numerical methods for the summation of divergent series or asymptotic series that arise for instance in perturbation theory, and may be used as highly effective extrapolation methods.\n\nA simple nonlinear sequence transformation is the Aitken extrapolation or delta-squared method,\n\ndefined by \n\nThis transformation is commonly used to improve the rate of convergence of a slowly converging sequence; heuristically, it eliminates the largest part of the absolute error.\n\n"}
{"id": "29329597", "url": "https://en.wikipedia.org/wiki?curid=29329597", "title": "Seven states of randomness", "text": "Seven states of randomness\n\nThe seven states of randomness in probability theory, fractals and risk analysis are extensions of the concept of randomness as modeled by the normal distribution. These seven states were first introduced by Benoît Mandelbrot in his 1997 book \"Fractals and Scaling in Finance\", which applied fractal analysis to the study of risk and randomness. This classification builds upon the three main states of randomness: mild, slow, and wild.\n\nThe importance of seven states of randomness classification for mathematical finance is that methods such as Markowitz mean variance portfolio and Black–Scholes model may be invalidated as the tails of the distribution of returns are fattened: the former relies on finite standard deviation (volatility) and stability of correlation, while the latter is constructed upon Brownian motion.\n\nThese seven states build on earlier work of Mandelbrot in 1963: \"The variations of certain speculative prices\" and \"New methods in statistical economics\" in which he argued that most statistical models approached only a first stage of dealing with indeterminism in science, and that they ignored many aspects of real world turbulence, in particular, most cases of financial modelling. This was then presented by Mandelbrot in the International Congress for Logic (1964) in an address titled \"The Epistemology of Chance in Certain Newer Sciences\"\n\nIntuitively speaking, Mandelbrot argued that the traditional normal distribution does not properly capture empirical and \"real world\" distributions and there are other forms of randomness that can be used to model extreme changes in risk and randomness. He observed that randomness can become quite \"wild\" if the requirements regarding finite mean and variance are abandoned. Wild randomness corresponds to situations in which a single observation, or a particular outcome can impact the total in a very disproportionate way.\n\nThe classification was formally introduced in his 1997 book \"Fractals and Scaling in Finance\", as a way to bring insight into the three main states of randomness: mild, slow, and wild . Given \"N\" addends, \"portioning\" concerns the relative contribution of the addends to their sum. By \"even\" portioning, Mandelbrot meant that the addends were of same order of magnitude, otherwise he considered the portioning to be \"concentrated\". Given the moment of order \"q\" of a random variable, Mandelbrot called the root of degree \"q\" of such moment the \"scale factor\" (of order \"q\").\n\nThe seven states are:\n\n\nWild randomness has applications outside financial markets, e.g. it has been used in the analysis of turbulent situations such as wild forest fires.\n\nUsing elements of this distinction, in March 2006, a year before the Financial crisis of 2007–2010, and four years before the Flash crash of May 2010, during which the Dow Jones Industrial Average had a 1,000 point intraday swing within minutes, Mandelbrot and Nassim Taleb published an article in the \"Financial Times\" arguing that the traditional \"bell curves\" that have been in use for over a century are inadequate for measuring risk in financial markets, given that such curves disregard the possibility of sharp jumps or discontinuities. Contrasting this approach with the traditional approaches based on random walks, they stated: \n\nWe live in a world primarily driven by random jumps, and tools designed for random walks address the wrong problem.\nMandelbrot and Taleb pointed out that although one can assume that the odds of finding a person who is several miles tall are extremely low, similar excessive observations can not be excluded in other areas of application. They argued that while traditional bell curves may provide a satisfactory representation of height and weight in the population, they do not provide a suitable modeling mechanism for market risks or returns, where just ten trading days represent 63 per cent of the returns of the past 50 years.\n\nIf the probability density of formula_3 is denoted formula_4, then it can be obtained by the double convolution formula_5.\n\nWhen u is known, the conditional probability density of u' is given by the portioning ratio:\n\nIn many important cases, the maximum of p(u')p(u-u') occurs near u'=u/2, or near u'=0 and u'=u. Take the logarithm of p(u')p(u-u') and write:\n\nformula_7\n\n\nSplitting the doubling convolution in 3 parts gives:\n\np(u) is short-run concentrated in probability if it is possible to select formula_9 so that the middle interval of (formula_10) has the following two properties as u→∞:\n\n\nConsider the formula formula_12, if p(u) is the scaling distribution the integrand is maximum at 0 and ∞, on other cases the integrand may have a sharp global maximum for some value formula_13 defined by the following equation:\n\nOne must also know formula_15 in the neighborhood of formula_13. The function formula_15 often admits a \"Gaussian\" approximation given by:\n\nWhen formula_15 is well-approximated by a Gaussian density, the bulk of formula_20 originates in the \"q-interval\" defined as\nformula_21. The Gaussian q-intervals greatly overlap for all values of formula_22. The Gaussian moments are called \"delocalized\". The lognormal's q-intervals are uniformly spaced and their width is independent of q; therefore if the log-normal is sufficiently skew, the q-interval and (q+1)-interval do not overlap. The lognormal moments are called \"uniformly localized\". In other cases, neighboring q-intervals cease to overlap for sufficiently high q, such moments are called \"asymptotically localized\".\n\n\n"}
{"id": "532678", "url": "https://en.wikipedia.org/wiki?curid=532678", "title": "Sheila Greibach", "text": "Sheila Greibach\n\nSheila Adele Greibach (born 6 October 1939 in New York City) is a researcher in formal languages in computing, automata, compiler theory (in particular), and computer science. She is an Emeritus Professor of Computer Science at the University of California, Los Angeles, and has worked with Seymour Ginsburg and Michael A. Harrison in context-sensitive parsing using the stack automaton model.\n\nBesides establishing the normal form (Greibach normal form) for context-free grammars, in 1965, she also investigated properties \nof W-grammars, pushdown automata, and decidability problems.\n\nGreibach earned in 1960 her A.B. degree from Radcliffe College in linguistics and applied mathematics (summa cum laude), and her A.M. degree in 1962 also from there. In 1963, she achieved her PhD at Harvard University, advised by Anthony Oettinger. The title of her PhD thesis is \"Inverses of Phrase Structure Generators\".\n\nShe continued to work at Harvard at the Division of Engineering and Applied Physics, until 1969 when she moved to the UCLA, where she has been professor since 1970 until present (as of March 2014).\n\nAmong her students were Ronald V. Book and Michael J. Fischer.\nThe following list indicates some of her work. The top portion of the list is from the ACM Digital Library and the remainder from the FOCS Bibliography by David M. Jones.\n\n\"Jump PDA's, deterministic context-free languages, principal AFDLs and polynomial time recognition (Extended Abstract),\" Proceedings of the fifth annual ACM symposium on Theory of Computing, April 1973\n\n\"Some restrictions on W-grammars\"\nProceedings of the sixth annual ACM symposium on Theory of computing, April 1974\n\n\"An Infinite Hierarchy of Context-Free Languages,\" \"Journal of the ACM,\" Volume 16 Issue 1, January 1969\n\n\"A New Normal-Form Theorem for Context-Free Phrase Structure Grammars,\" \"JACM,\" Volume 12 Issue 1, January 1965\n\n\"The Unsolvability of the Recognition of Linear Context-Free Languages,\" \"JACM,\" Volume 13 Issue 4, October 1966\n\n\"Multitape AFA,\" co-authored with Seymour Ginsburg, \"Journal of the ACM\", Volume 19 Issue 2, April 1972\n\n\"Superdeterministic PDAs: A Subcase with a Decidable Inclusion problem\", co-authored with E. P. Friedman, \"JACM\", October 1980, Volume 27 Issue 4\n\n\"Stack automata and compiling,\" co-authored with Seymour Ginsburg and Michael A. Harrison, \"JACM\", January 1967, Volume 14 Issue 1\n\n\"Quasi-realtime languages (Extended Abstract),\" co-authored with Ronald V. Book, Proceedings of the first annual ACM symposium on Theory of Computing, May 1969\n\n\"One-way stack automata,\" co-authored with Seymour Ginsburg and Michael A. Harrison, \"JACM\", April 1967, Volume 14 Issue 2\n\n\"Tape- and time-bounded Turing acceptors and AFLs (Extended Abstract)\"\nco-authored with Ronald V. Book and Ben Wegbreit, Proceedings of the second annual ACM symposium on Theory of computing, May 1970\n\n\"Uniformly erasable AFL\", co-authored with Seymour Ginsburg and Jonathan Goldstine, Proceedings of the fourth annual ACM symposium on Theory of computing, May 1972\n\n\n\n"}
{"id": "794811", "url": "https://en.wikipedia.org/wiki?curid=794811", "title": "Shigefumi Mori", "text": "Shigefumi Mori\n\nHe generalized the classical approach to the classification of algebraic surfaces to the classification of algebraic three-folds. The classical approach used the concept of minimal models of algebraic surfaces. He found that the concept of minimal models can be applied to three-folds as well if we allow some singularities on them.\n\nThe extension of Mori’s results to dimensions higher than three is called the Mori program and, as of 2006, is an active area of algebraic geometry.\n\nHe was awarded the Fields Medal in 1990 at the International Congress of Mathematicians.\n\nHe was visiting professor at Harvard University during 1977–1980, the Institute for Advanced Study in 1981–82, Columbia University 1985–87 and the University of Utah for periods during 1987–89 and again during 1991–92. He has been a professor at Kyoto University since 1990.\n\nHe has been elected president of the International Mathematical Union, becoming the first head of the group from East Asia.\n\n\n\n"}
{"id": "6319281", "url": "https://en.wikipedia.org/wiki?curid=6319281", "title": "Steven Matheson", "text": "Steven Matheson\n\nSteven Matheson is a fictional character from the Australian Channel Seven soap opera \"Home and Away\", played by Adam Willits. Steven was created by Alan Bateman as one of the serial's original characters and he first appeared in the pilot episode. Willits received the role after being one of over three hundred actors to audition for the roles of the serial's foster children. He was a regular cast member from 1988 to 1991 and again between 1995 and 1996. He has continued to make guest appearances in from 1997 until 2008. Steven is characterised in his early years by his quiet and studious persona, he later becomes problematic as he grows older. His main storylines have focused on his early tragedies in which his family die, bullying, his first kiss with Narelle Smart (Amanda Newman-Phillips) and his on/off relationship with Selina Roberts (Tempany Deckert). Critics of the serial have favoured his appearance and have subsequently branded him a \"heart-throb\" because of his good looks. Others have commented that he grew up quickly on-screen and eventually become a problematic character.\n\nIn 1987 over three hundred actors auditioned for the roles of the Fletcher's foster children. Eventually sixteen-year-old Adam Willits was offered the part of Steven. Prior to the role Willits had gained a fair amount of experience as a child actor. In 1989 Willits confirmed in the \"Home and Away Annual\" that he was happy to stay with the serial and watch Steven develop. In 1990, Willits decided to leave the serial to pursue other projects but remained on-screen until the following year.\n\nWillits returned to \"Home and Away\" in 1995-1996, and returned for an additional stint in 1997. In 1998 Willits filmed a conclusion to Steven's storyline on location in the UK. In 2002 Willits returned alongside numerous ex-cast members to film a special storyline for the \"150th anniversary\" of Summer Bay.\n\nIn the 1989 edition of the \"Home and Away Annual\", Steven is described in his years as being \"quiet and studious, but with an inventive mind. When he came to the Flectchers, he was disturbed, and was having terrible nightmares about his parents' death.\" They also add \"Steven is liable to become a wonderfully caring and informed person, or a pompous brat.\" In the \"Home and Away Annual\" he is described as having the life of any average teenager. Willits described Steven stating: \"A nice guy, not very trendy though\". Willits has also stated it's a challenge to play Steven because he is so different from him, so much so he did not relate to him, he also branded Steven as \"not very outgoing\". After two years of playing the role, Willits said that Steven \"started off as a bit of a bookworm but as he has grown up he has found distractions more engaging than academia!\" Willits added that Steven is \"not the trendiest guy in the world, but thankfully as time passes he gets a bit more wayward.\" In another interview he states: \"I love to get out of his character when shooting is finished, and to ruffle up my hair which is all brushed like Steven's.\" While interviewed by Clive Hopwood for the book \"Home and Away Special\", Willits said that Steven's development was slow because he remained \"a bit of a dork\". He added that Steven was destined for \"exciting things\".\n\nSteven's first kiss is with fellow character Narelle Smart (Amanda Newman-Phillips), who is three years older than he is (being seventeen at the time). Of this Willits states: \"I'd go for an older woman, if Steven can, I certainly can.\" Discussing the scenes and Steven's reasons, Willits states: \"In the script he had this plan to get Narelle to kiss me, because he has done a survey and he's a pretty hot kisser. It wasn't as though they were in love though.\" He also named it as one of Steven's greatest moments during the early years. Newman-Phillips describes the filming of the storyline stating: \"Narelle seduces Steven and when it came to the kissing scene we were both very nervous. It was the first for both of us and we just couldn't get the scene right, so we did some extra rehearsing. The kiss only lasted a minute, but in the end it took us a couple of hours to get it perfect for the cameras.\" Whilst interviewed by \"TV Week\", Willits stated: \"When you consider rehearsals and retakes, we kissed a hell of a lot. It was great. She gave me all the pointers\". The serial later cast Kate Raison to play artist Jennifer Atkinson, a new love interest for Steven. In the \"Home and Away Annual Authorised Edition\", Kesta Desmond said that their romance would develop into a \"torrid affair\" and that it could \"all end in tears\".\n\nIn another storyline school secretary Joanne Brennan (Kimberley Joseph) starts up an obsessive campaign to seduce Steven. Steven does not show any interest in return. Joseph said \"She goes out of her way to wreck any other romance he has - and ends up stalking him.\" Joanne continues her \"bitchy\" campaign until she departs. Steven had a drawn out relationship with his student Selina Roberts (Tempany Deckert), their relationship takes many \"twists\" along the way. Selina leaves Steven on their wedding day. Deckert said it was a \"typical soap wedding\" with many mishappenings. The conclusion to their storyline before Steven's final departure of the nineties, was filmed in Ironbridge, Shropshire. It was the first ever episode to be filmed abroad for the serial. \n\nWhen Steven returned to the show in 2002, it was revealed that he had been living in Hong Kong with Selina and taken a job as a IT consultant.\n\nSteven was born in 1973 in Hornsby, New South Wales as the only son of Brett and Martha Matheson. One night in 1987, while Steven was staying over at his friend, Danny's house, he heard sirens belonging to fire engines and discovered that the family home was on fire and Brett and Martha were trapped. The Mathesons had put bars on the windows previously so escape was impossible. Brett and Martha died in the blaze and Steven, with no available extended family, was placed into a temporary care home until Tom (Roger Oakley) and Pippa Fletcher (Vanessa Downing) fostered him.\n\nSteven first appears on screen in the pilot when he and his foster family sit down to celebrate Tom's 40th birthday. Later, when Tom announces to the family that he is being retrenched, Steven offers to live elsewhere as he feels the Fletchers will be unable to afford looking after him. The family eventually relocate from The city to country town Summer Bay. Early into the Fletchers' arrival Steven manages to win the approval of foster brother Frank Morgan (Alex Papps), who has been suspicious of him since his arrival. Steven also manages to befriend local tomboy Bobby Simpson (Nicolle Dickson), who later joins the family. Steven asks Frank for advice, he believes no one sees him as a grown up and just as a boy, he starts shaving and trying new things to look older. Steven starts feeling an attraction for Narelle, who is three years older than he is, they later kiss, Steven sees it as a big moment as it's the first time he's kissed a girl. Steven gets his first taste of a real romance when classmate, Sandra Barlow (Catherine McColl-Jones) shows an interest him. Unfortunately, Sandra's abusive father, Sam Barlow) (Jeff Truman) takes an immediate dislike to Steven and warns him off. Steven and Sandra continue seeing each other secretly until Sam's behavior culminates in him accidentally shooting his wife and Sandra's mother, Kerry dead. Sandra later leaves the area to stay with a foster family in the city. Steven later has several run-ins with P.E. teacher Jeff Samuels (Alex Petersons) whose training regimes do not sit well with Steven, the Fletchers or many other people in Summer Bay. Steven is overjoyed when his uncle Philip Matheson (John Morris), his only living relative arrives in Summer Bay. Philip is later killed in an arson attack on the general store at the hands of Brian \"Dodge\" Forbes (Kelly Dingwall), who has a grudge against the owner Celia Stewart (Fiona Spence). Steven is devastated and begins having nightmares and flashbacks to his parents' deaths in the fire.\n\nDodge becomes Tom and Pippa's latest foster child, he strikes up a friendship with Steven and winds him up on multiple occasions about not being macho. Dodge enjoys using Steven to do his essays and he later fools Steven into turning on his former geeky friends over a money dispute. Dodge had in fact, being bullying one of them, when they all attack him, Steven decides to attempt to attack the lads. They are shocked and tell him he's a better person than that. Dodge has a grudge against Roo Stewart's (Justine Clark) boyfriend Simon Yates (Christopher Saunders), who he tries to pin the blame on over the arson attack. He manipulates Steven into attacking him on the beach and vowing revenge. Dodge steals a car and crashes it, Steven takes the blame to protect him, but Dodge reveals it was him and vows to change. Just as everyone trusts him again, he drunkenly confesses to setting the fire that killed Phillip, enraged Steven tries to attack him before he is charged with murder. Steven later leaves to go to university.\n\nIn 1995, Steven returns and gets a job at Summer Bay High as a teacher, much to Donald Fisher's (Norman Coburn) surprise. Everyone is delighted to have him back and he settles back into life with family around. When Marilyn Chambers (Emily Symons) returns they become better friends and eventually start a relationship, it proves short lived however and they decide it's best to be friends again. Later his student, Selina falls in love with Steven, he tells her they cannot be together because of his job and age difference. However Steven later gives in to his feelings and starts an affair with her, but they later decide to end things. Selina's stalker Jeremy Riggs (David Stanley) finds out about her and Steven so tries to black mail them, but he hangs himself in the school toilets and Irene Roberts (Lynne McGranger) finds out the truth, she files a complaint which sees Steven sacked from his job and everyone in town is shocked to learn about their affair.\n\nDodge is released from prison and returns to Summer Bay, wanting revenge on Steven. Dodge hides jelly beans in Steven's house and black mails him by stating he will go to the police and tell them he has illegal drugs in his property. Scared Steven becomes extremely affected by his presence, but is happy to find out they're not drugs. Steven and Kelly Watson (Katrina Hobbs) have an affair behind Travis Nash's (Nic Testoni) back, although Kelly later decides it isn't a good idea and ends things. Kelly helps him set Dodge up for theft. Whilst out on a cruise, Dodge jumps overboard and is presumed dead. The police start a murder investigation with Steven as their prime suspect. Dodge later turns up alive and kidnaps Kelly, when Steven tracks them down they start fighting on the edge of a cliff. In the end they both roll over the edge of the cliff, Steven survives but Dodge remained unfound. After alienating himself from most of the Bay's residents and annoying Pippa when he shows no interest in his foster sister Sally Fletcher's (Kate Ritchie) abduction, he decides to rethink his place in the town. Sally is found and goes to hospital and waits for Steven. Steven leaves Pippa a note and flees the town, without saying goodbye to his closest family member Sally.\n\nOne year later he returns feeling in a better place. He starts to fall for Selina again, delighted they resume their relationship. This time more people approve and they decide to get married. On their wedding day however, Selina seemingly jilts Steven. She is actually being held hostage by cult leader Saul Bennett (David Ritchie), later she escapes and Steven is relieved to have her back. She shocks him by telling him she no longer wants to marry him after her ordeal. He leaves the bay once more. After another year passes Marilyn and Irene decide to head overseas to, Ironbridge when Selina is severely ill, they arrange for Steven to come and look after her. When he arrives he proposes to Selina, who gladly accepts. They leave the bay together this time. Steven returns in 2000 for Sally's wedding to Kieran Fletcher (Spencer McLaren), he reveals that he and Selina still haven't got married. After Keiran's infedelity is discovered, Steven beats him up on the beach which Steven should have been arrested and spent time in prison for. Steven returns again later and reveals he and Selina have married, he returns again the following year and announces he and Selina broke up and he was with someone new. In 2008 he returns when Sally decides to leave Summer Bay, after Ric Dalby (Mark Furze) gets in touch with him, he reveals he and Selina got back together. Pippa and Carly also join him, He tells Sally how much she deserves her big send off and they reminisce about the old days when they first moved into the house on the caravan park. After Sally leaves, Steven is not seen on-screen again.\n\nIn his book, \"Super Aussie Soaps\", Andrew Mercado describes the moment Steven decides to marry Selina as a scandal because she was his student. In her book \"Soapbox\", Hilary Kingsley brands Steven the quietest foster child of Pippa and Tom. In the book \"Home and Away: behind the scenes\", James Oram comments on Steven's young heart-throb status, stating: \"Those who concern themselves with such matters as heart-throbs suggest he is well on his way to joining that select circle. He could be the youngest heart-throb in history, or at least since Romeo caused Juliet's heart to flutter.\" He also added that whilst the serial followed the tradition of many other soaps, through Steven and Frank they portrayed many real issues. Jan Moir writing for the \"Evening Times\" states: \"He was orphaned when his parents were killed in a fire and is fast becoming the problem kid on the block.\" The \"Birmingham Post\" observed Steven's relationship with Selina as a \"tale of twists\" becoming more \"twisted\" with time. A columnist for \"The Newcastle Herald\" chose Steven and Selina's 1998 return episode as one of their \"TV Highlights\".\n\nAnalysing Steven's early characterisation, a columnist for \"Inside Soap\" said \"once the school swat and a bit of a square, he grew up into an egghead who couldn't get a girl. Duller than one of Alf Stewart's bowling club cheese and wine parties, quiet Steven Matheson seemed destined to stay single.\" They opined that upon his return in 1995, Steven no longer had trouble finding a partner but had a \"problem\" finding one his own age. Their colleague opined that Steven was the \"brightest foster kid\" that Pippa had ever fostered. While another opined that \"there was never any doubt that the quiet and studious Steven would end up at uni - even if a passionate fling with an older woman made him think twice about going. Steven passed his HSC with flying colours.\"\n\n"}
{"id": "8830237", "url": "https://en.wikipedia.org/wiki?curid=8830237", "title": "Subspace theorem", "text": "Subspace theorem\n\nIn mathematics, the subspace theorem is a result obtained by . It states that if \"L\"...,\"L\" are linearly independent linear forms in \"n\" variables with algebraic coefficients and if ε>0 is any given real number, then\nthe non-zero integer points \"x\" with\nlie in a finite number of proper subspaces of Q.\n\nA quantitative form of the theorem, in which the number of subspaces containing all solutions, was also obtained by Schmidt, and the theorem was generalised by to allow more general absolute values on number fields.\n\nThe theorem may be used to obtain results on Diophantine equations such as Siegel's theorem on integral points and solution of the S-unit equation.\n\nThe following corollary to the subspace theorem is often itself referred to as the \"subspace theorem\".\nIf \"a\"...,\"a\" are algebraic such that 1,\"a\"...,\"a\" are linearly independent over Q and ε>0 is any given real number, then there are only finitely many rational \"n\"-tuples (\"x\"/y...,\"x\"/y) with\n\nThe specialization \"n\" = 1 gives the Thue–Siegel–Roth theorem. One may also note that the exponent 1+1/\"n\"+ε is best possible by Dirichlet's theorem on diophantine approximation.\n\n"}
{"id": "14530635", "url": "https://en.wikipedia.org/wiki?curid=14530635", "title": "Tail value at risk", "text": "Tail value at risk\n\nTail value at risk (TVaR), also known as tail conditional expectation (TCE) or conditional tail expectation (CTE), is a risk measure associated with the more general value at risk. It quantifies the expected value of the loss given that an event outside a given probability level has occurred.\n\nThere are a number of related, but subtly different, formulations for TVaR in the literature. A common case in literature is to define TVaR and average value at risk as the same measure. Under some formulations, it is only equivalent to expected shortfall when the underlying distribution function is continuous at formula_1, the value at risk of level formula_2. Under some other settings, TVaR is the conditional expectation of loss above a given value, whereas the expected shortfall is the product of this value with the probability of it occurring. The former definition may not be a coherent risk measure in general, however it is coherent if the underlying distribution is continuous. The latter definition is a coherent risk measure. TVaR accounts for the severity of the failure, not only the chance of failure. The TVaR is a measure of the expectation only in the tail of the distribution.\n\nThe canonical tail value at risk is the left-tail (large negative values) in some disciplines and the right-tail (large positive values) in other, such as actuarial science. This is usually due to the differing conventions of treating losses as large negative or positive values. Using the negative value convention, Artzner and others define the tail value at risk as:\n\nGiven a random variable formula_3 which is the payoff of a portfolio at some future time and given a parameter formula_4 then the tail value at risk is defined by\n\nwhere formula_6 is the upper formula_2-quantile given by formula_8. Typically the payoff random variable formula_3 is in some L-space where formula_10 to guarantee the existence of the expectation.\n"}
{"id": "7372292", "url": "https://en.wikipedia.org/wiki?curid=7372292", "title": "Taro Morishima", "text": "Taro Morishima\n\nGranville wrote that Morishima's proof could not be accepted. \n\n"}
{"id": "8983001", "url": "https://en.wikipedia.org/wiki?curid=8983001", "title": "The Foundations of Arithmetic", "text": "The Foundations of Arithmetic\n\nThe Foundations of Arithmetic () is a book by Gottlob Frege, published in 1884, which investigates the philosophical foundations of arithmetic. Frege refutes other theories of number and develops his own theory of numbers. The \"Grundlagen\" also helped to motivate Frege's later works in logicism. The book was not well received and was not read widely when it was published. It did, however, draw the attentions of Bertrand Russell and Ludwig Wittgenstein, who were both heavily influenced by Frege's philosophy. An English translation was published (Oxford, 1950) by J. L. Austin, with a second edition in 1960.\n\nFrege objects to any account of mathematics based on psychologism, that is the view that math and numbers are relative to the subjective thoughts of the people who think of them. According to Frege, psychological accounts appeal to what is subjective, while mathematics is purely objective: mathematics are completely independent from human thought. Mathematical entities, according to Frege, have objective properties regardless of humans thinking of them: it is not possible to think of mathematical statements as something that evolved naturally through human history and evolution. He sees a fundamental distinction between logic (and its extension, according to Frege, math) and psychology. Logic explains necessary facts, whereas psychology studies certain thought processes in individual minds.\n\nFrege greatly appreciates the work of Immanuel Kant. He criticizes him mainly on the grounds that numerical statements are not synthetic-a priori, but rather analytic-a priori.\nKant claims that 7+5=12 is an unprovable synthetic statement. No matter how much we analyze the idea of 7+5 we will not find there the idea of 12. We must arrive at the idea of 12 by application to objects in the intuition. Kant points out that this becomes all the more clear with bigger numbers. Frege, on this point precisely, argues towards the opposite direction. Kant wrongly assumes that in a proposition containing \"big\" numbers we must count points or some such thing to assert their truth value. Frege argues that without ever having any intuition toward any of the numbers in the following equation: 654,768+436,382=1,091,150 we nevertheless can assert it is true. This is provided as evidence that such a proposition is analytic. While Frege agrees that geometry is indeed synthetic a priori, arithmetic must be analytic.\n\nFrege roundly criticizes the empiricism of John Stuart Mill. He claims that Mill's idea that numbers correspond to the various ways of splitting collections of objects into subcollections is inconsistent with confidence in calculations involving large numbers. He also denies that Mill's philosophy deals adequately with the concept of zero. He goes on to argue that the operation of addition cannot be understood as referring to physical quantities, and that Mill's confusion on this point is a symptom of a larger problem of confounding the applications of arithmetic for arithmetic itself.\n\nFrege makes a distinction between particular numerical statements such as 1+1=2, and general statements such as a+b=b+a. The latter are statements true of numbers just as well as the former. Therefore, it is necessary to ask for a definition of the concept of number itself. \nFrege investigates the possibility that number is determined in external things. He demonstrates how numbers function in natural language just as adjectives. \"This desk has 5 drawers\" is similar in form to \"This desk has green drawers\". The drawers being green is an objective fact, grounded in the external world. But this is not the case with 5. Frege argues that each drawer is on its own green, but not every drawer is 5.\nFrege urges us to remember that from this it does not follow that numbers may be subjective. Indeed, numbers are similar to colors at least in that both are wholly objective. \nFrege tells us that we can convert number statements where number words appear adjectivally (e.g., 'there are four horses') into statements where number terms appear as singular terms ('the number of horses is four'). Frege recommends such translations because he takes numbers to be objects. It makes no sense to ask whether any objects fall under 4. After Frege gives some reasons for thinking that numbers are objects, he concludes that statements of numbers are assertions about concepts.\n\nFrege takes this observation to be the fundamental thought of \"Grundlagen\". For example, the sentence \"the number of horses in the barn is four\" means that four objects fall under the concept \"horse in the barn\". Frege attempts to explain our grasp of numbers through a contextual definition of the cardinality operation ('the number of...', or formula_1). He attempts to construct the content of a judgment involving numerical identity by relying on Hume's principle (which states that the number of Fs equals the number of Gs if and only if F and G are equinumerous, i.e. in one-one correspondence). He rejects this definition because it doesn't fix the truth value of identity statements when a singular term not of the form 'the number of Fs' flanks the identity sign. Frege goes on to give an explicit definition of number in terms of extensions of concepts, but expresses some hesitation.\n\nFrege argues that numbers are objects and assert something about a concept. Frege defines numbers as extensions of concepts. 'The number of F's' is defined as the extension of the concept \"G is a concept that is equinumerous to F\". The concept in question leads to an equivalence class of all concepts that have the number of F (including F). Frege defines 0 as the extension of the concept \"being non self-identical\". So, the number of this concept is the extension of the concept of all concepts that have no objects falling under them. The number 1 is the extension of being identical with 0.\n\nThe book was fundamental in the development of two main disciplines, the foundations of mathematics and philosophy. Although Bertrand Russell later found a major flaw in Frege's work (this flaw is known as Russell's paradox, which is resolved by axiomatic set theory), the book was influential in subsequent developments, such as \"Principia Mathematica\". The book can also be considered the starting point in analytic philosophy, since it revolves mainly around the analysis of language, with the goal of clarifying the concept of number. Frege's views on mathematics are also a starting point on the philosophy of mathematics, since it introduces an innovative account on the epistemology of numbers and math in general, known as logicism.\n\nThe text is divided into five chapters, which are further divided into certain headings or topics (phrased as questions or statements), and then these into 109 sections.\n\n\n\n\n"}
{"id": "41530846", "url": "https://en.wikipedia.org/wiki?curid=41530846", "title": "The Taming of Chance", "text": "The Taming of Chance\n\nThe Taming of Chance is a 1990 book about the history of probability by the philosopher Ian Hacking. It is a sequel to his earlier \"The Emergence of Probability\" (1975). The book received both positive and mixed reviews.\n\n\"The Taming of Chance\" has been described as ground-breaking. D. V. Lindley reviewed the book in \"Nature\". M. Schabas reviewed the book in \"Science\". Stephen P. Turner reviewed the book positively in the \"American Journal of Sociology\", writing that it was useful for both sociologists of science and historians of social science, and that while Hacking's arguments were open to objections, Hacking was \"too sophisticated\" to be caught by them. The historian of science Theodore M. Porter reviewed the book in \"American Scientist\".\n\nBruce Kurlick gave \"The Taming of Chance\" a mixed review in \"American Historical Review\", noting that it was a sequel to Hacking's earlier \"The Emergence of Probability\". Kurlick praised Hacking for the \"richness of his ideas\" and credited him with mastering complicated literature in several languages and \"meticulous scholarship\" superior to that of Michel Foucault, to whom Hacking was indebted. However, he considered the book a \"strain to understand\" and criticized Hacking for giving insufficient emphasis to the role of the hospital in \"acclimating the public to chance and probability\", and for his \"penchant for irrelevant anecdotes\" and poor judgment about how to write about the past. Timothy L. Alborn reviewed \"The Taming of Chance\" positively in \"Isis\", writing that Hacking had a \"vibrant writing style\" and presented a \"wealth of material\". However, he also wrote that Hacking's book left many questions unanswered.\n\n"}
{"id": "54863674", "url": "https://en.wikipedia.org/wiki?curid=54863674", "title": "Van Genuchten–Gupta model", "text": "Van Genuchten–Gupta model\n\nThe van Genuchten–Gupta model is an inverted S-curve applicable to crop yield and soil salinity relations.\n\nThe mathematical expression is:\n\nwhere Y = yield, Ym = maximum yield of the model, C = salt concentration of the soil, C = C value at 50% yield, P = an exponent to be found by optimization and maximizing the model's goodness of fit to the data.\n\nIn the figure: Ym = 3.1, C = 12.4, P = 3.75\n\nAs an alternative, the logistic S-function can be used.\n\nThe mathematical expression is:\n\nwhere:\n\nwith Y =Yield, Yn = minimum Y, Ym = maximum Y, X = salt concentration of the soil, while A, B and C are constants to be determined by optimization and maximizing the model's goodness of fit to the data.\n\nIf the minimum Yn=0 then the expression can be simplified to:\n\nIn the figure: Ym = 3.43, Yn = 0.47, A = 0.112, B = -3.16, C = 1.42.\n\nThe third degree or cubic regression also offers a useful alternative.\n\nThe equation reads:\n\nwith Y =Yield, X = salt concentration of the soil, while A, B, C and D are constants to be determined by the regression.\n\nIn the figure: A = 0.0017, B = 0.0604, C=0.3874, D = 2.3788. These values were calculated with Microsoft Excel\n\nThe curvature is more pronounced than in the other models.\n\n"}
