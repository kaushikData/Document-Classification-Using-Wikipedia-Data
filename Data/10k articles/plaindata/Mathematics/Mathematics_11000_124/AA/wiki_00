{"id": "834367", "url": "https://en.wikipedia.org/wiki?curid=834367", "title": "212 (number)", "text": "212 (number)\n\n212 (two hundred [and] twelve) is the natural number following 211 and preceding 213.\n\n\n"}
{"id": "23914319", "url": "https://en.wikipedia.org/wiki?curid=23914319", "title": "Algebraic definition", "text": "Algebraic definition\n\nIn mathematical logic, an algebraic definition is one that can be given using only equations between terms with free variables. Inequalities and quantifiers are specifically disallowed.\n\nSaying that a definition is algebraic is a stronger condition than saying it is elementary.\n\n"}
{"id": "351908", "url": "https://en.wikipedia.org/wiki?curid=351908", "title": "Almost surely", "text": "Almost surely\n\nIn probability theory, one says that an event happens almost surely (sometimes abbreviated as a.s.) if it happens with probability one. In other words, the set of possible exceptions may be non-empty, but it has probability zero. The concept is precisely the same as the concept of \"almost everywhere\" in measure theory.\n\nIn probability experiments on a finite sample space, there is often no difference between \"almost surely\" and \"surely\". However, the distinction becomes important when the sample space is an infinite set, because an infinite set can have non-empty subsets of probability zero.\n\nSome examples of the use of this concept include the strong and uniform versions of the law of large numbers, and the continuity of the paths of Brownian motion.\n\nThe terms almost certainly (a.c.) and almost always (a.a.) are also used. Almost never describes the opposite of \"almost surely\": an event that happens with probability zero happens \"almost never\".\n\nLet formula_1 be a probability space. An event formula_2 happens \"almost surely\" if formula_3. Equivalently, formula_4 happens almost surely if the probability of formula_4 not occurring is zero: formula_6. More generally, any event formula_7 (not necessarily in formula_8) happens almost surely if formula_9 is contained in a null set: a subset of some formula_10 such that The notion of almost sureness depends on the probability measure formula_11. If it is necessary to emphasize this dependence, it is customary to say that the event formula_4 occurs \"P\"-almost surely, or almost surely (\"P\").\n\nIn general, an event can happen \"almost surely\" even if the probability space in question includes outcomes which do not belong to the event, as is illustrated in the examples below.\n\nImagine throwing a dart at a unit square (i.e. a square with area 1) so that the dart always hits exactly one point of the square, and so that each point in the square is equally likely to be hit. \n\nNow, notice that since the square has area 1, the probability that the dart will hit any particular subregion of the square equals the area of that subregion. For example, the probability that the dart will hit the right half of the square is 0.5, since the right half has area 0.5.\n\nNext, consider the event that \"the dart hits a diagonal of the unit square exactly\". Since the areas of the diagonals of the square are zero, the probability that the dart lands exactly on a diagonal is zero. So, the dart will almost never land on a diagonal (i.e. it will almost surely \"not\" land on a diagonal). Nonetheless the set of points on the diagonals is not empty and a point on a diagonal is no less possible than any other point: the diagonal does contain valid outcomes of the experiment.\n\nConsider the case where a (possibly biased) coin is tossed, corresponding to the probability space formula_13, where the event formula_14 occurs if heads is flipped, and formula_15 if tails. For this particular coin, assume the probability of flipping heads is formula_16 from which it follows that the complement event, flipping tails, has formula_17.\n\nSuppose we were to conduct an experiment where the coin is tossed repeatedly, with outcomes formula_18, and it is assumed each flip's outcome is independent of all the others. That is, they are \"i.i.d.\". Define the sequence of random variables on the coin toss space, formula_19 where formula_20. \"i.e.\" each formula_21 records the outcome of the formula_22'th flip.\n\nAny infinite sequence of heads and tails is a possible outcome of the experiment. However, any particular infinite sequence of heads and tails has probability zero of being the exact outcome of the (infinite) experiment. To see why, note that the \"i.i.d.\" assumption implies that the probability of flipping all heads over formula_23 flips is simply formula_24. Letting formula_25 yields zero, since formula_26 by assumption. Note that the result is the same no matter how much we bias the coin towards heads, so long as we constrain formula_27 to be greater than 0, and less than 1.\n\nIn particular, the event \"the sequence contains at least one formula_28\" happens almost surely (i.e., with probability 1).\nHowever, if instead of an infinite number of flips we stop flipping after some finite time, say a million flips, then the all-heads sequence has non-zero probability. The all-heads sequence has probability formula_29, while the probability of getting at least one tails is formula_30 and the event is no longer almost sure.\n\nIn asymptotic analysis, one says that a property holds asymptotically almost surely (a.a.s.) if, over a sequence of sets, the probability converges to 1. For instance, a large number is asymptotically almost surely composite, by the prime number theorem; and in random graph theory, the statement \"formula_31 is connected\" (where formula_32 denotes the graphs on formula_23 vertices with edge probability formula_27) is true a.a.s. when, for any formula_35\n\nIn number theory this is referred to as \"almost all\", as in \"almost all numbers are composite\". Similarly, in graph theory, this is sometimes referred to as \"almost surely\".\n\n\n"}
{"id": "2027", "url": "https://en.wikipedia.org/wiki?curid=2027", "title": "Andrew Wiles", "text": "Andrew Wiles\n\nSir Andrew John Wiles (born 11 April 1953) is a British mathematician and a Royal Society Research Professor at the University of Oxford, specialising in number theory. He is best known for proving Fermat's Last Theorem, for which he was awarded the 2016 Abel Prize and the 2017 Copley Medal by the Royal Society. He was appointed Knight Commander of the Order of the British Empire in 2000, and in 2018 was appointed as the first Regius Professor of Mathematics at Oxford.\n\nWiles was born on 11 April 1953 in Cambridge, England, the son of Maurice Frank Wiles (1923–2005), the Regius Professor of Divinity at the University of Oxford, and Patricia Wiles (née Mowll). His father worked as the Chaplain at Ridley Hall, Cambridge, for the years 1952–55. Wiles attended King's College School, Cambridge, and The Leys School, Cambridge.\n\nWiles states that he came across Fermat's Last Theorem on his way home from school when he was 10 years old. He stopped at his local library where he found a book about the theorem. Fascinated by the existence of a theorem that was so easy to state that he, a ten-year-old, could understand it, but that no one had proven, he decided to be the first person to prove it. However, he soon realised that his knowledge was too limited, so he abandoned his childhood dream, until it was brought back to his attention at the age of 33 by Ken Ribet's 1986 proof of the epsilon conjecture, which Gerhard Frey had previously linked to Fermat's famous equation.\n\nWiles earned his bachelor's degree in mathematics in 1974 at Merton College, Oxford, and a PhD in 1980 as a graduate student of Clare College, Cambridge. After a stay at the Institute for Advanced Study in Princeton, New Jersey in 1981, Wiles became a Professor of Mathematics at Princeton University. In 1985–86, Wiles was a Guggenheim Fellow at the Institut des Hautes Études Scientifiques near Paris and at the École Normale Supérieure. From 1988 to 1990, Wiles was a Royal Society Research Professor at the University of Oxford, and then he returned to Princeton. From 1994 - 2009, Wiles was a Eugene Higgins Professor at Princeton. He rejoined Oxford in 2011 as Royal Society Research Professor. In May 2018 he was appointed Regius Professor of Mathematics at Oxford, the first in the university's history. \n\nWiles's graduate research was guided by John Coates beginning in the summer of 1975. Together these colleagues worked on the arithmetic of elliptic curves with complex multiplication by the methods of Iwasawa theory. He further worked with Barry Mazur on the main conjecture of Iwasawa theory over the rational numbers, and soon afterward, he generalised this result to totally real fields.\n\nHis biographical page at Princeton University's website states that \"Andrew has few equals in terms of his impact on modern number theory. Many of the world’s very best young number theorists received their Ph.D.’s under Andrew ... and many of these are today leaders and professors at top institutions around the world\".\n\nStarting in mid-1986, based on successive progress of the previous few years of Gerhard Frey, Jean-Pierre Serre and Ken Ribet, it became clear that Fermat's Last Theorem could be proven as a corollary of a limited form of the modularity theorem (unproven at the time and then known as the \"Taniyama–Shimura–Weil conjecture\"). The modularity theorem involved elliptic curves, which was also Wiles's own specialist area.\n\nThe conjecture was seen by contemporary mathematicians as important, but extraordinarily difficult or perhaps impossible to prove. For example, Wiles's ex-supervisor John Coates states that it seemed \"impossible to actually prove\", and Ken Ribet considered himself \"one of the vast majority of people who believed [it] was completely inaccessible\", adding that \"Andrew Wiles was probably one of the few people on earth who had the audacity to dream that you can actually go and prove [it].\"\n\nDespite this, Wiles, with his from-childhood fascination with Fermat's Last Theorem, decided to undertake the challenge of proving the conjecture, at least to the extent needed for Frey's curve. He dedicated all of his research time to this problem for over six years in near-total secrecy, covering up his efforts by releasing prior work in small segments as separate papers and confiding only in his wife.\n\nIn June 1993, he presented his proof to the public for the first time at a conference in Cambridge.\nIn August 1993, it was discovered that the proof contained a flaw in one area. Wiles tried and failed for over a year to repair his proof. According to Wiles, the crucial idea for circumventing, rather than closing this area, came to him on 19 September 1994, when he was on the verge of giving up. Together with his former student Richard Taylor, he published a second paper which circumvented the problem and thus completed the proof. Both papers were published in May 1995 in a dedicated issue of the \"Annals of Mathematics.\"\n\nWiles's proof of Fermat's Last Theorem has stood up to the scrutiny of the world's other mathematical experts. Wiles was interviewed for an episode of the BBC documentary series \"Horizon\" that focused on Fermat's Last Theorem. This was renamed \"The Proof\", and it was made an episode of the US Public Broadcasting Service's science television series \"Nova\". His work and life are also described in great detail in Simon Singh's popular book \"Fermat's Last Theorem\".\n\nWiles has been awarded a number of major prizes in mathematics and science:\n\nWiles's 1987 certificate of election to the Royal Society reads:\n"}
{"id": "642090", "url": "https://en.wikipedia.org/wiki?curid=642090", "title": "Asymptotic expansion", "text": "Asymptotic expansion\n\nIn mathematics, an asymptotic expansion, asymptotic series or Poincaré expansion (after Henri Poincaré) is a formal series of functions which has the property that truncating the series after a finite number of terms provides an approximation to a given function as the argument of the function tends towards a particular, often infinite, point. Investigations by revealed that the divergent part of an asymptotic expansion is latently meaningful, i.e. contains information about the exact value of the expanded function.\n\nThe most common type of asymptotic expansion is a power series in either positive or negative powers. Methods of generating such expansions include the Euler–Maclaurin summation formula and integral transforms such as the Laplace and Mellin transforms. Repeated integration by parts will often lead to an asymptotic expansion.\n\nSince a \"convergent\" Taylor series fits the definition of asymptotic expansion as well, the phrase \"asymptotic series\" usually implies a \"non-convergent\" series. Despite non-convergence, the asymptotic expansion is useful when truncated to a finite number of terms. The approximation may provide benefits by being more mathematically tractable than the function being expanded, or by an increase in the speed of computation of the expanded function. Typically, the best approximation is given when the series is truncated at the smallest term. This way of optimally truncating an asymptotic expansion is known as superasymptotics. The error is then typically of the form where is the expansion parameter. The error is thus beyond all orders in the expansion parameter. It is possible to improve on the superasymptotic error, e.g. by employing resummation methods such as Borel resummation to the divergent tail. Such methods are often referred to as hyperasymptotic approximations.\n\nSee asymptotic analysis and big O notation for the notation used in this article.\n\nFirst we define an asymptotic scale, and then give the formal definition of an asymptotic expansion.\n\nIf φ is a sequence of continuous functions on some domain, and if \"L\" is a limit point of the domain, then the sequence constitutes an asymptotic scale if for every \"n\",\nformula_1. (\"L\" may be taken to be infinity.) In other words, a sequence of functions is an asymptotic scale if each function in the sequence grows strictly slower (in the limit formula_2) than the preceding function.\n\nIf \"f\" is a continuous function on the domain of the asymptotic scale, then \"f\" has an asymptotic expansion of order \"N\" with respect to the scale as a formal series formula_3 if\nor\nIf one or the other holds for all \"N\", then we write\n\nIn contrast to a convergent series for formula_7, wherein the series converges for any \"fixed\" formula_8 in the limit formula_9, one can think of the asymptotic series as converging for \"fixed\" formula_10 in the limit formula_2 (with formula_12 possibly infinite).\n\n\n\n\n\nwhere is the double factorial.\n\nAsymptotic expansions often occur when an ordinary series is used in a formal expression that forces the taking of values outside of its domain of convergence. Thus, for example, one may start with the ordinary series\n\nThe expression on the left is valid on the entire complex plane formula_21, while the right hand side converges only for formula_22. Multiplying by formula_23 and integrating both sides yields\n\nafter the substitution formula_25 on the right hand side. The integral on the left hand side, understood as a Cauchy principal value, can be expressed in terms of the exponential integral. The integral on the right hand side may be recognized as the gamma function. Evaluating both, one obtains the asymptotic expansion\n\nHere, the right hand side is clearly not convergent for any non-zero value of \"t\". However, by truncating the series on the right to a finite number of terms, one may obtain a fairly good approximation to the value of formula_27 for sufficiently small \"t\". Substituting formula_28 and noting that formula_29 results in the asymptotic expansion given earlier in this article.\n\nFor a given asymptotic scale formula_30 the asymptotic expansion of function formula_31 is unique.\nThat is: the coefficients formula_32 are uniquely determined in the following way:\n\nformula_33\n\nformula_34\n\nformula_35\n\nformula_36\n\nwhere formula_12 is the limit point of this asymptotic expansion (may be formula_38).\n\nA given function formula_31 may have many asymptotic expansions (each with a different asymptotic scale).\n\nAn asymptotic expansion may be asymptotic expansion to more than one function.\n\n\n\n"}
{"id": "4842394", "url": "https://en.wikipedia.org/wiki?curid=4842394", "title": "Biconnected component", "text": "Biconnected component\n\nIn graph theory, a biconnected component (also known as a block or 2-connected component) is a maximal biconnected subgraph. Any connected graph decomposes into a tree of biconnected components called the block-cut tree of the graph. The blocks are attached to each other at shared vertices called cut vertices or articulation points. Specifically, a cut vertex is any vertex whose removal increases the number of connected components.\n\nThe classic sequential algorithm for computing biconnected components in a connected undirected graph is due to John Hopcroft and Robert Tarjan (1973). It runs in linear time, and is based on depth-first search. This algorithm is also outlined as Problem 22-2 of Introduction to Algorithms (both 2nd and 3rd editions).\n\nThe idea is to run a depth-first search while maintaining the following information:\nThe depth is standard to maintain during a depth-first search. The lowpoint of \"v\" can be computed after visiting all descendants of \"v\" (i.e., just before \"v\" gets popped off the depth-first-search stack) as the minimum of the depth of \"v\", the depth of all neighbors of \"v\" (other than the parent of \"v\" in the depth-first-search tree) and the lowpoint of all children of \"v\" in the depth-first-search tree.\n\nThe key fact is that a nonroot vertex \"v\" is a cut vertex (or articulation point) separating two biconnected components if and only if there is a child \"y\" of \"v\" such that lowpoint(\"y\") ≥ depth(\"v\"). This property can be tested once the depth-first search returned from every child of \"v\" (i.e., just before \"v\" gets popped off the depth-first-search stack), and if true, \"v\" separates the graph into different biconnected components. This can be represented by computing one biconnected component out of every such \"y\" (a component which contains \"y\" will contain the subtree of \"y\", plus \"v\"), and then erasing the subtree of \"y\" from the tree.\n\nThe root vertex must be handled separately: it is a cut vertex if and only if it has at least two children. Thus, it suffices to simply build one component out of each child subtree of the root (including the root).\n\nGetArticulationPoints(i, d)\nNote that the terms child and parent denote the relations in the DFS tree, not the original graph.\n<br>\n\nA simple alternative to the above algorithm uses chain decompositions, which are special ear decompositions depending on DFS-trees. Chain decompositions can be computed in linear time by this traversing rule. Let \"C\" be a chain decomposition of \"G\". Then \"G\" is 2-vertex-connected if and only if \"G\" has minimum degree 2 and \"C\" is the only cycle in \"C\". This gives immediately a linear-time 2-connectivity test and can be extended to list all cut vertices of \"G\" in linear time using the following statement: A vertex \"v\" in a connected graph \"G\" (with minimum degree 2) is a cut vertex if and only if \"v\" is incident to a bridge or \"v\" is the first vertex of a cycle in \"C - C\". The list of cut vertices can be used to create the block-cut tree of \"G\" in linear time.\n\nIn the online version of the problem, vertices and edges are added (but not removed) dynamically, and a data structure must maintain the biconnected components. Jeffery Westbrook and Robert Tarjan (1992) developed an efficient data structure for this problem based on disjoint-set data structures. Specifically, it processes \"n\" vertex additions and \"m\" edge additions in O(\"m\" \"α\"(\"m\", \"n\")) total time, where α is the inverse Ackermann function. This time bound is proved to be optimal.\n\nUzi Vishkin and Robert Tarjan (1985) designed a parallel algorithm on CRCW PRAM that runs in O(log \"n\") time with \"n\" + \"m\" processors. Guojing Cong and David A. Bader (2005) developed an algorithm that achieves a speedup of 5 with 12 processors on SMPs. Speedups exceeding 30 based on the original Tarjan-Vishkin algorithm were reported by James A. Edwards and Uzi Vishkin (2012).\n\nOne can define a binary relation on the edges of an arbitrary undirected graph, according to which two edges \"e\" and \"f\" are related if and only if either \"e\" = \"f\" or the graph contains a simple cycle through both \"e\" and \"f\". Every edge is related to itself, and an edge \"e\" is related to another edge \"f\" if and only if \"f\" is related in the same way to \"e\". Less obviously, this is a transitive relation: if there exists a simple cycle containing edges \"e\" and \"f\", and another simple cycle containing edges \"f\" and \"g\", then one can combine these two cycles to find a simple cycle through \"e\" and \"g\". Therefore, this is an equivalence relation, and it can be used to partition the edges into equivalence classes, subsets of edges with the property that two edges are related to each other if and only if they belong to the same equivalence class. The subgraphs formed by the edges in each equivalence class are the biconnected components of the given graph. Thus, the biconnected components partition the edges of the graph; however, they may share vertices with each other.\n\nThe block graph of a given graph \"G\" is the intersection graph of its blocks. Thus, it has one vertex for each block of \"G\", and an edge between two vertices whenever the corresponding two blocks share a vertex.\nA graph \"H\" is the block graph of another graph \"G\" exactly when all the blocks of \"H\" are complete subgraphs. The graphs \"H\" with this property are known as the block graphs.\n\nA cutpoint, cut vertex, or articulation point of a graph \"G\" is a vertex that is shared by two or more blocks. The structure of the blocks and cutpoints of a connected graph can be described by a tree called the block-cut tree or BC-tree. This tree has a vertex for each block and for each articulation point of the given graph. There is an edge in the block-cut tree for each pair of a block and an articulation point that belongs to that block.\n\n"}
{"id": "46339949", "url": "https://en.wikipedia.org/wiki?curid=46339949", "title": "Bogdanov map", "text": "Bogdanov map\n\nIn dynamical systems theory, the Bogdanov map is a chaotic 2D map related to the Bogdanov–Takens bifurcation. It is given by the transformation:\n\nThe Bogdanov map is named after Rifkat Bogdanov.\n\n\n"}
{"id": "5673873", "url": "https://en.wikipedia.org/wiki?curid=5673873", "title": "Bruce C. Berndt", "text": "Bruce C. Berndt\n\nBruce Carl Berndt (born March 13, 1939, in St. Joseph, Michigan) \nis an American mathematician. Berndt attended college at Albion College, graduating in 1961, where he also ran track. He received his master's and doctoral degrees from the University of Wisconsin–Madison. He lectured for a year at the University of Glasgow and then, in 1967, was appointed an assistant professor at the University of Illinois at Urbana-Champaign, where he has remained since. In 1973–74 he was a visiting scholar at the Institute for Advanced Study in Princeton. He is currently () Michio Suzuki Distinguished Research Professor of Mathematics at the University of Illinois.\n\nBerndt is an analytic number theorist who is probably best known for his work explicating the discoveries of Srinivasa Ramanujan. He is a coordinating editor of The Ramanujan Journal and, in 1996, received an expository Steele Prize from the American Mathematical Society for his work editing \"Ramanujan's Notebooks\". A Lester R. Ford Award was given to Berndt, with Gerd Almkvist, in 1989 and to Berndt, with S. Bhargava, in 1994.\n\nIn 2012 he became a fellow of the American Mathematical Society.\nIn December 2012 he received an honorary doctorate from SASTRA University in Kumbakonam, India.\n\n\n\n"}
{"id": "50086789", "url": "https://en.wikipedia.org/wiki?curid=50086789", "title": "Canon Sinuum (Pitiscus)", "text": "Canon Sinuum (Pitiscus)\n\nThe \"Canon Sinuum\" is the main part of Bartholomaeus Pitiscus' \" Thesaurus Mathematicus sive Canon Sinuum ad radium 1.00000.00000.00000\" published as a folio in 1613 (misprinted on two of the titles 1513, by omission of a ) in Frankfurt. It is a table of sines, originally computed by Rheticus, with the sines given every 10 seconds to 15 places, with first, second, and third differences. This table spans 270 pages.\n\nIn addition, the \"Canon Sinuum\" gives the sines to 15 places for every second of the first and last degrees of the quadrant, as well as several other tables.\n"}
{"id": "36897268", "url": "https://en.wikipedia.org/wiki?curid=36897268", "title": "Charlotte Barnum", "text": "Charlotte Barnum\n\nCharlotte Cynthia Barnum (May 17, 1860 – March 27, 1934), mathematician and social activist, was the first woman to receive a Ph.D in mathematics from Yale University.\n\nCharlotte Barnum was born in Phillipston, Massachusetts, the third of four children of the Reverend Samuel Weed Barnum (1820–1891) and Charlotte Betts (1823–1899). Education was important in her family: two uncles had received medical degrees from Yale and her father had graduated from there with a Bachelor of Arts and a Bachelor of Divinity. Her brothers Samuel and Thomas would both graduate from Yale, and her sister Clara would attend Yale graduate school after graduating from Vassar.\n\nAfter graduating from Hillhouse High School in New Haven, Connecticut Charlotte attended Vassar College, where she graduated in 1881. From 1881 to 1886 she taught at a boys’ preparatory school, Betts Academy, in Stamford, Connecticut and at Hillhouse High School. She also did computing work for the Yale Observatory 1883–1885 and worked on a revision of James Dwight Dana’s \"System of Mineralogy\". Charlotte was an editorial writer for Webster's International Dictionary from 1886 to 1890, and then taught astronomy at Smith College for the academic year 1889–90.\n\nIn 1890 Charlotte applied for graduate studies at Johns Hopkins University, but was turned down because they did not accept women. She persisted and with the support of Simon Newcomb, professor of mathematics and astronomy at the university, she won approval to attend lectures without enrollment and without charge. Two years later, she moved to New Haven to pursue her graduate studies at Yale. In 1895 she was the first woman to receive a Ph.D. in mathematics from that institution. Her thesis was titled \"Functions Having Lines or Surfaces of Discontinuity\". The identity of her adviser is unclear from the record.\n\nAfter receiving her Ph.D., Charlotte Barnum taught at Carleton College in Northfield, Minnesota for one year. She then left academia, and did civilian and governmental applied mathematics and editorial work the remainder of her career.\n\nIn 1898 she joined the American Academy of Actuaries and until 1901 worked as an actuarial computer for the Massachusetts Mutual Life Insurance Company, Springfield, Massachusetts and the Fidelity Mutual Life Insurance Company in Philadelphia, Pennsylvania.\n\nIn 1901 she moved to Washington D.C. to work as a computer for US Naval Observatory. She subsequently did the same work for the tidal division of the US Coast and Geodetic Survey until 1908 and then was editorial assistant in the biological survey section of the US Department of Agriculture through 1913.\n\nShe left government employment and returned to New Haven in 1914 where she did editorial work for Yale Peruvian Expeditions, the Yale University secretary’s office, and the Yale University Press.\n\nStarting in 1917 she worked in various organizations and academic institutions in Connecticut, New York and Massachusetts as an editor, actuary and teacher. All her life she was involved in social and charitable organizations and activities. In 1934 she died in Middletown, Connecticut of meningitis at the age of seventy-three.\n\nOne of the first women members of the American Mathematical Society\n\nFellow, American Academy of Actuaries (AAAS)\n\nFellow, American Association for the Advancement of Science\n\nAlumnae Member, Vassar College Chapter of Phi Beta Kappa\n\nWomen’s Joint Legislative Commission (for equal rights)\n\nNational Conference of Charities (now the National Conference on Social Welfare)\n\n1911: “The Girl Who Lives at Home: Two Suggestions to Trade Union Women,” (\"Life and Labor\", Volume 1, 1911) p. 346.\n\n"}
{"id": "19335333", "url": "https://en.wikipedia.org/wiki?curid=19335333", "title": "Clifford's circle theorems", "text": "Clifford's circle theorems\n\nIn geometry, Clifford's theorems, named after the English geometer William Kingdon Clifford, are a sequence of theorems relating to intersections of circles.\n\nThe first theorem considers any four circles passing through a common point \"M\" and otherwise in general position, meaning that there are six additional points where exactly two of the circles cross and that no three of these crossing points are collinear. Every set of three of these four circles has among them three crossing points, and (by the assumption of non-collinearity) there exists a circle passing through these three crossing points. The conclusion is that, like the first set of four circles, the second set of four circles defined in this way all pass through a single point \"P\" (in general not the same point as \"M\").\n\nThe second theorem considers five circles in general position passing through a single point \"M\". Each subset of four circles defines a new point \"P\" according to the first theorem. Then these five points all lie on a single circle \"C\".\n\nThe third theorem consider six circles in general position that pass through a single point \"M\". Each subset of five circles defines a new circle by the second theorem. Then these six new circles \"C\" all pass through a single point.\n\nThe sequence of theorems can be continued indefinitely.\n\n\n\n"}
{"id": "585143", "url": "https://en.wikipedia.org/wiki?curid=585143", "title": "Closed-form expression", "text": "Closed-form expression\n\nIn mathematics, a closed-form expression is a mathematical expression that can be evaluated in a finite number of operations. It may contain constants, variables, certain \"well-known\" operations (e.g., + − × ÷), and functions (e.g., \"n\"th root, exponent, logarithm, trigonometric functions, and inverse hyperbolic functions), but usually no limit. The set of operations and functions admitted in a closed-form expression may vary with author and context.\n\nThe solutions of any quadratic equation with complex coefficients can be expressed in closed form in terms of addition, subtraction, multiplication, division, and square root extraction, each of which is an elementary function. For example, the quadratic equation\n\nis tractable since its solutions can be expressed as closed-form expression, i.e. in terms of elementary functions:\n\nSimilarly solutions of cubic and quartic (third and fourth degree) equations can be expressed using arithmetic, square roots, and cube roots, or alternatively using arithmetic and trigonometric functions. However, there are quintic equations without closed-form solutions using elementary functions, such as \"x\" − \"x\" + 1 = 0.\n\nAn area of study in mathematics referred to broadly as Galois theory involves proving that no closed-form expression exists in certain contexts, based on the central example of closed-form solutions to polynomials.\n\nChanging the definition of \"well-known\" to include additional functions can change the set of equations with closed-form solutions. Many cumulative distribution functions cannot be expressed in closed form, unless one considers special functions such as the error function or gamma function to be well known. It is possible to solve the quintic equation if general hypergeometric functions are included, although the solution is far too complicated algebraically to be useful. For many practical computer applications, it is entirely reasonable to assume that the gamma function and other special functions are well-known since numerical implementations are widely available.\n\nAn analytic expression (or expression in analytic form) is a mathematical expression constructed using well-known operations that lend themselves readily to calculation. Similar to closed-form expressions, the set of well-known functions allowed can vary according to context but always includes the basic arithmetic operations (addition, subtraction, multiplication, and division), exponentiation to a real exponent (which includes extraction of the th root), logarithms, and trigonometric functions.\n\nHowever, the class of expressions considered to be analytic expressions tends to be wider than that for closed-form expressions. In particular, special functions such as the Bessel functions and the gamma function are usually allowed, and often so are infinite series and continued fractions. On the other hand, limits in general, and integrals in particular, are typically excluded.\n\nIf an analytic expression involves only the algebraic operations (addition, subtraction, multiplication, division, and exponentiation to a rational exponent) and rational constants then it is more specifically referred to as an algebraic expression.\n\nClosed-form expressions are an important sub-class of analytic expressions, which contain a bounded or an unbounded number of applications of well-known functions. Unlike the broader analytic expressions, the closed-form expressions do not include infinite series or continued fractions; neither includes integrals or limits. Indeed, by the Stone–Weierstrass theorem, any continuous function on the unit interval can be expressed as a limit of polynomials, so any class of functions containing the polynomials and closed under limits will necessarily include all continuous functions.\n\nSimilarly, an equation or system of equations is said to have a closed-form solution if, and only if, at least one solution can be expressed as a closed-form expression; and it is said to have an analytic solution if and only if at least one solution can be expressed as an analytic expression. There is a subtle distinction between a \"closed-form \"function\"\" and a \"closed-form \"number\"\" in the discussion of a \"closed-form solution\", discussed in and below. A closed-form or analytic solution is sometimes referred to as an explicit solution.\n\nThe expression:\n\nis not in closed form because the summation entails an infinite number of elementary operations. However, by summing a geometric series this expression can be expressed in the closed-form:\n\nThe integral of a closed-form expression may or may not itself be expressible as a closed-form expression. This study is referred to as differential Galois theory, by analogy with algebraic Galois theory.\n\nThe basic theorem of differential Galois theory is due to Joseph Liouville in the 1830s and 1840s and hence referred to as Liouville's theorem.\n\nA standard example of an elementary function whose antiderivative does not have a closed-form expression is:\n\nwhose antiderivative is (up to constants) the error function:\n\nEquations or systems too complex for closed-form or analytic solutions can often be analysed by mathematical modelling and computer simulation.\n\nThree subfields of the complex numbers C have been suggested as encoding the notion of a \"closed-form number\"; in increasing order of generality, these are the Liouville numbers, EL numbers and elementary numbers. The Liouville numbers, denoted L (not to be confused with Liouville numbers in the sense of rational approximation), form the smallest \"algebraically closed\" subfield of C closed under exponentiation and logarithm (formally, intersection of all such subfields)—that is, numbers which involve \"explicit\" exponentiation and logarithms, but allow explicit and \"implicit\" polynomials (roots of polynomials); this is defined in . L was originally referred to as elementary numbers, but this term is now used more broadly to refer to numbers defined explicitly or implicitly in terms of algebraic operations, exponentials, and logarithms. A narrower definition proposed in , denoted E, and referred to as EL numbers, is the smallest subfield of C closed under exponentiation and logarithm—this need not be algebraically closed, and correspond to \"explicit\" algebraic, exponential, and logarithmic operations. \"EL\" stands both for \"Exponential-Logarithmic\" and as an abbreviation for \"elementary\".\n\nWhether a number is a closed-form number is related to whether a number is transcendental. Formally, Liouville numbers and elementary numbers contain the algebraic numbers, and they include some but not all transcendental numbers. In contrast, EL numbers do not contain all algebraic numbers, but do include some transcendental numbers. Closed-form numbers can be studied via transcendental number theory, in which a major result is the Gelfond–Schneider theorem, and a major open question is Schanuel's conjecture.\n\nFor purposes of numeric computations, being in closed form is not in general necessary, as many limits and integrals can be efficiently computed.\n\nThere is software that attempts to find closed-form expressions for numerical values, including RIES, identify in Maple and SymPy, Plouffe's Inverter, and the Inverse Symbolic Calculator.\n\n\n"}
{"id": "22823111", "url": "https://en.wikipedia.org/wiki?curid=22823111", "title": "Comparison of general and generalized linear models", "text": "Comparison of general and generalized linear models\n"}
{"id": "40770350", "url": "https://en.wikipedia.org/wiki?curid=40770350", "title": "Complex-oriented cohomology theory", "text": "Complex-oriented cohomology theory\n\nIn algebraic topology, a complex-orientable cohomology theory is a multiplicative cohomology theory \"E\" such that the restriction map formula_1 is surjective. An element of formula_2 that restricts to the canonical generator of the reduced theory formula_3 is called a complex orientation. The notion is central to Quillen's work relating cohomology to formal group laws.\n\nIf E is an even-graded theory meaning formula_4, then \"E\" is complex-orientable. This follows from the Atiyah–Hirzebruch spectral sequence. \n\nExamples:\n\nA complex orientation, call it \"t\", gives rise to a formal group law as follows: let \"m\" be the multiplication\nwhere formula_7 denotes a line passing through \"x\" in the underlying vector space formula_8 of formula_9. This is the map classifying the tensor product of the universal line bundle over formula_10. Viewing\nlet formula_12 be the pullback of \"t\" along \"m\". It lives in\nand one can show, using properties of the tensor product of line bundles, it is a formal group law (e.g., satisfies associativity).\n\n\n"}
{"id": "7363", "url": "https://en.wikipedia.org/wiki?curid=7363", "title": "Complexity", "text": "Complexity\n\nComplexity characterises the behaviour of a system or model whose components interact in multiple ways and follow local rules, meaning there is no reasonable higher instruction to define the various possible interactions.\n\nThe term is generally used to characterize something with many parts where those parts interact with each other in multiple ways, culminating in a higher order of emergence greater than the sum of its parts. The study of these complex linkages at various scales is the main goal of complex systems theory.\n\nScience takes a number of approaches to characterizing complexity; Zayed \"et al.\"\nreflect many of these. Neil Johnson states that \"even among scientists, there is no unique definition of complexity – and the scientific notion has traditionally been conveyed using particular examples...\" Ultimately Johnson adopts the definition of \"complexity science\" as \"the study of the phenomena which emerge from a collection of interacting objects\".\n\nDefinitions of complexity often depend on the concept of a confidential \"system\" – a set of parts or elements that have relationships among them differentiated from relationships with other elements outside the relational regime. Many definitions tend to postulate or assume that complexity expresses a condition of numerous elements in a system and numerous forms of relationships among the elements. However, what one sees as complex and what one sees as simple is relative and changes with time.\n\nWarren Weaver posited in 1948 two forms of complexity: disorganized complexity, and organized complexity.\nPhenomena of 'disorganized complexity' are treated using probability theory and statistical mechanics, while 'organized complexity' deals with phenomena that escape such approaches and confront \"dealing simultaneously with a sizable number of factors which are interrelated into an organic whole\". Weaver's 1948 paper has influenced subsequent thinking about complexity.\n\nThe approaches that embody concepts of systems, multiple elements, multiple relational regimes, and state spaces might be summarized as implying that complexity arises from the number of distinguishable relational regimes (and their associated state spaces) in a defined system.\n\nSome definitions relate to the algorithmic basis for the expression of a complex phenomenon or model or mathematical expression, as later set out herein.\n\nOne of the problems in addressing complexity issues has been formalizing the intuitive conceptual distinction between the large number of variances in relationships extant in random collections, and the sometimes large, but smaller, number of relationships between elements in systems where constraints (related to correlation of otherwise independent elements) simultaneously reduce the variations from element independence and create distinguishable regimes of more-uniform, or correlated, relationships, or interactions.\n\nWeaver perceived and addressed this problem, in at least a preliminary way, in drawing a distinction between \"disorganized complexity\" and \"organized complexity\".\n\nIn Weaver's view, disorganized complexity results from the particular system having a very large number of parts, say millions of parts, or many more. Though the interactions of the parts in a \"disorganized complexity\" situation can be seen as largely random, the properties of the system as a whole can be understood by using probability and statistical methods.\n\nA prime example of disorganized complexity is a gas in a container, with the gas molecules as the parts. Some would suggest that a system of disorganized complexity may be compared with the (relative) simplicity of planetary orbits – the latter can be predicted by applying Newton's laws of motion. Of course, most real-world systems, including planetary orbits, eventually become theoretically unpredictable even using Newtonian dynamics; as discovered by modern chaos theory.\n\nOrganized complexity, in Weaver's view, resides in nothing else than the non-random, or correlated, interaction between the parts. These correlated relationships create a differentiated structure that can, as a system, interact with other systems. The coordinated system manifests properties not carried or dictated by individual parts. The organized aspect of this form of complexity vis-a-vis to other systems than the subject system can be said to \"emerge,\" without any \"guiding hand\".\n\nThe number of parts does not have to be very large for a particular system to have emergent properties. A system of organized complexity may be understood in its properties (behavior among the properties) through modeling and simulation, particularly modeling and simulation with computers. An example of organized complexity is a city neighborhood as a living mechanism, with the neighborhood people among the system's parts.\n\nThere are generally rules which can be invoked to explain the origin of complexity in a given system.\n\nThe source of disorganized complexity is the large number of parts in the system of interest, and the lack of correlation between elements in the system.\n\nIn the case of self-organizing living systems, usefully organized complexity comes from beneficially mutated organisms being selected to survive by their environment for their differential reproductive ability or at least success over inanimate matter or less organized complex organisms. See e.g. Robert Ulanowicz's treatment of ecosystems.\n\nComplexity of an object or system is a relative property. For instance, for many functions (problems), such a computational complexity as time of computation is smaller when multitape Turing machines are used than when Turing machines with one tape are used. Random Access Machines allow one to even more decrease time complexity (Greenlaw and Hoover 1998: 226), while inductive Turing machines can decrease even the complexity class of a function, language or set (Burgin 2005). This shows that tools of activity can be an important factor of complexity.\n\nIn several scientific fields, \"complexity\" has a precise meaning:\n\n\nOther fields introduce less precisely defined notions of complexity:\n\n\nComplexity has always been a part of our environment, and therefore many scientific fields have dealt with complex systems and phenomena. From one perspective, that which is somehow complex – displaying variation without being random – is most worthy of interest given the rewards found in the depths of exploration.\n\nThe use of the term complex is often confused with the term complicated. In today's systems, this is the difference between myriad connecting \"stovepipes\" and effective \"integrated\" solutions. This means that complex is the opposite of independent, while complicated is the opposite of simple.\n\nWhile this has led some fields to come up with specific definitions of complexity, there is a more recent movement to regroup observations from different fields to study complexity in itself, whether it appears in anthills, human brains, or stock markets, social systems. One such interdisciplinary group of fields is relational order theories.\n\nThe behavior of a complex system is often said to be due to emergence and self-organization. Chaos theory has investigated the sensitivity of systems to variations in initial conditions as one cause of complex behaviour.\n\nRecent developments around artificial life, evolutionary computation and genetic algorithms have led to an increasing emphasis on complexity and complex adaptive systems.\n\nIn social science, the study on the emergence of macro-properties from the micro-properties, also known as macro-micro view in sociology. The topic is commonly recognized as social complexity that is often related to the use of computer simulation in social science, i.e.: computational sociology.\n\nSystems theory has long been concerned with the study of complex systems (in recent times, \"complexity theory\" and \"complex systems\" have also been used as names of the field). These systems are present in the research of a variety disciplines, including biology, economics, social studies and technology. Recently, complexity has become a natural domain of interest of real world socio-cognitive systems and emerging systemics research. Complex systems tend to be high-dimensional, non-linear, and difficult to model. In specific circumstances, they may exhibit low-dimensional behaviour.\n\nIn information theory, algorithmic information theory is concerned with the complexity of strings of data.\n\nComplex strings are harder to compress. While intuition tells us that this may depend on the codec used to compress a string (a codec could be theoretically created in any arbitrary language, including one in which the very small command \"X\" could cause the computer to output a very complicated string like \"18995316\"), any two Turing-complete languages can be implemented in each other, meaning that the length of two encodings in different languages will vary by at most the length of the \"translation\" language – which will end up being negligible for sufficiently large data strings.\n\nThese algorithmic measures of complexity tend to assign high values to random noise. However, those studying complex systems would not consider randomness as complexity.\n\nInformation entropy is also sometimes used in information theory as indicative of complexity.\n\nRecent work in machine learning has examined the complexity of the data as it affects the performance of supervised classification algorithms. Ho and Basu present a set of complexity measures for binary classification problems.\n\nThe complexity measures broadly cover:\nInstance hardness is a bottom-up approach that first seeks to identify instances that are likely to be misclassified (or, in other words, which instances are the most complex). The characteristics of the instances that are likely to be misclassified are then measured based on the output from a set of hardness measures. The hardness measures are based on several supervised learning techniques such as measuring the number of disagreeing neighbors or the likelihood of the assigned class label given the input features. The information provided by the complexity measures has been examined for use in meta learning to determine for which data sets filtering (or removing suspected noisy instances from the training set) is the most beneficial and could be expanded to other areas.\n\nA recent study based on molecular simulations and compliance constants describes molecular recognition as a phenomenon of organisation.\nEven for small molecules like carbohydrates, the recognition process can not be predicted or designed even assuming that each individual hydrogen bond's strength is exactly known.\n\nComputational complexity theory is the study of the complexity of problems – that is, the difficulty of solving them. Problems can be classified by complexity class according to the time it takes for an algorithm – usually a computer program – to solve them as a function of the problem size. Some problems are difficult to solve, while others are easy. For example, some difficult problems need algorithms that take an exponential amount of time in terms of the size of the problem to solve. Take the travelling salesman problem, for example. It can be solved in time formula_1 (where \"n\" is the size of the network to visit – the number of cities the travelling salesman must visit exactly once). As the size of the network of cities grows, the time needed to find the route grows (more than) exponentially.\n\nEven though a problem may be computationally solvable in principle, in actual practice it may not be that simple. These problems might require large amounts of time or an inordinate amount of space. Computational complexity may be approached from many different aspects. Computational complexity can be investigated on the basis of time, memory or other resources used to solve the problem. Time and space are two of the most important and popular considerations when problems of complexity are analyzed.\n\nThere exist a certain class of problems that although they are solvable in principle they require so much time or space that it is not practical to attempt to solve them. These problems are called intractable.\n\nThere is another form of complexity called hierarchical complexity. It is orthogonal to the forms of complexity discussed so far, which are called horizontal complexity.\n\n"}
{"id": "6338441", "url": "https://en.wikipedia.org/wiki?curid=6338441", "title": "CryptMT", "text": "CryptMT\n\nIn cryptography, CryptMT is a stream cipher algorithm which internally uses the Mersenne twister. It was developed by Makoto Matsumoto, Hagita Mariko, Takuji Nishimura and Matsuo Saito and is patented. It has been submitted to the eSTREAM project of the eCRYPT network. \n\nIn that submission to eSTREAM, the authors also included another cipher named Fubuki, which also uses the Mersenne twister.\n\n"}
{"id": "31555837", "url": "https://en.wikipedia.org/wiki?curid=31555837", "title": "Dichromatic reflectance model", "text": "Dichromatic reflectance model\n\nIn Shafer’s dichromatic reflection model, scene radiance has two components:\n\nBody essence is an entity invariant to interface reflection, and has two degrees of freedom. The Gaussian coefficient generalizes a conventional simple thresholding scheme, and it provides detailed use of body color similarity.\n"}
{"id": "330206", "url": "https://en.wikipedia.org/wiki?curid=330206", "title": "Differentiable function", "text": "Differentiable function\n\nIn calculus (a branch of mathematics), a differentiable function of one real variable is a function whose derivative exists at each point in its domain. As a result, the graph of a differentiable function must have a (non-vertical) tangent line at each point in its domain, be relatively smooth, and cannot contain any breaks, bends, or cusps.\n\nMore generally, if \"x\" is a point in the domain of a function \"f\", then \"f\" is said to be \"differentiable at \"x\"\" if the derivative \"f\" ′(\"x\") exists. This means that the graph of \"f\" has a non-vertical tangent line at the point (\"x\", \"f\"(\"x\")). The function \"f\" may also be called \"locally linear\" at \"x\", as it can be well approximated by a linear function near this point.\n\nA function formula_1, defined on an open set formula_2, is said to be differentiable at formula_3 if any of the following equivalent conditions is satisfied:\n\n\nIf is differentiable at a point , then must also be continuous at . In particular, any differentiable function must be continuous at every point in its domain. \"The converse does not hold\": a continuous function need not be differentiable. For example, a function with a bend, cusp, or vertical tangent may be continuous, but fails to be differentiable at the location of the anomaly.\n\nMost functions that occur in practice have derivatives at all points or at almost every point. However, a result of Stefan Banach states that the set of functions that have a derivative at some point is a meager set in the space of all continuous functions. Informally, this means that differentiable functions are very atypical among continuous functions. The first known example of a function that is continuous everywhere but differentiable nowhere is the Weierstrass function.\n\nA function \"f\" is said to be \"continuously differentiable\" if the derivative \"\"(\"x\") exists and is itself a continuous function. Although the derivative of a differentiable function never has a jump discontinuity, it is possible for the derivative to have an essential discontinuity. For example, the function\nis differentiable at 0, since\nexists. However, for \"x\" ≠ 0, differentiation rules imply\nwhich has no limit as \"x\" → 0. Nevertheless, Darboux's theorem implies that the derivative of any function satisfies the conclusion of the intermediate value theorem.\n\nContinuously differentiable functions are sometimes said to be of \"class\" C. A function is of \"class\" C if the first and second derivative of the function both exist and are continuous. More generally, a function is said to be of \"class\" C if the first \"k\" derivatives '(\"x\"), '(\"x\"), ..., \"f\"(\"x\") all exist and are continuous. If derivatives \"f\" exist for all positive integers \"n\", the function is smooth or equivalently, of \"class\" C.\nA function of several real variables is said to be differentiable at a point if there exists a linear map such that\n\nIf a function is differentiable at , then all of the partial derivatives exist at , and the linear map is given by the Jacobian matrix. A similar formulation of the higher-dimensional derivative is provided by the fundamental increment lemma found in single-variable calculus.\n\nIf all the partial derivatives of a function exist in a neighborhood of a point and are continuous at the point , then the function is differentiable at that point .\n\nHowever, the existence of the partial derivatives (or even of all the directional derivatives) does not in general guarantee that a function is differentiable at a point. For example, the function defined by\n\nis not differentiable at , but all of the partial derivatives and directional derivatives exist at this point. For a continuous example, the function\n\nis not differentiable at , but again all of the partial derivatives and directional derivatives exist.\n\nIn complex analysis, complex-differentiability is defined using the same definition as single-variable real functions. This is allowed by the possibility of dividing complex numbers. So, a function formula_18 is said to be differentiable at formula_19 when\n\nAlthough this definition looks similar to the differentiability of single-variable real functions, it is however a more restrictive condition. A function formula_18, that is complex-differentiable at a point formula_19 is automatically differentiable at that point, when viewed as a function formula_23. This is because the complex-differentiability implies that\n\nHowever, a function formula_18 can be differentiable as a multi-variable function, while not being complex-differentiable. For example, formula_26 is differentiable at every point, viewed as the 2-variable real function formula_27, but it is not complex-differentiable at any point.\n\nAny function that is complex-differentiable in a neighborhood of a point is called holomorphic at that point. Such a function is necessarily infinitely differentiable, and in fact analytic.\n\nIf \"M\" is a differentiable manifold, a real or complex-valued function \"f\" on \"M\" is said to be differentiable at a point \"p\" if it is differentiable with respect to some (or any) coordinate chart defined around \"p\". More generally, if \"M\" and \"N\" are differentiable manifolds, a function \"f\": \"M\" → \"N\" is said to be differentiable at a point \"p\" if it is differentiable with respect to some (or any) coordinate charts defined around \"p\" and \"f\"(\"p\").\n\n"}
{"id": "31395400", "url": "https://en.wikipedia.org/wiki?curid=31395400", "title": "European Prize in Combinatorics", "text": "European Prize in Combinatorics\n\nThe European Prize in Combinatorics is a prize for research in combinatorics, a mathematical discipline, which is awarded biennially at Eurocomb, the European conference on combinatorics, graph theory, and applications. The prize was first awarded at Eurocomb 2003 in Prague. Recipients must not be older than 35. The last prize has been awarded at Eurocomb 2017 in Vienna.\n\n"}
{"id": "21116542", "url": "https://en.wikipedia.org/wiki?curid=21116542", "title": "Generalized map", "text": "Generalized map\n\nIn mathematics, a generalized map is a topological model which allows one to represent and to handle subdivided objects. This model was defined starting from combinatorial maps in order to represent non-orientable and open subdivisions, which is not possible with combinatorial maps. The main advantage of generalized map is the homogeneity of one-to-one mappings in any dimensions, which simplifies definitions and algorithms comparing to combinatorial maps. For this reason, generalized maps are sometimes used instead of combinatorial maps, even to represent orientable closed partitions.\n\nLike combinatorial maps, generalized maps are used as efficient data structure in image representation and processing, in geometrical modeling, they are related to simplicial set and to combinatorial topology, and this is a boundary representation model (B-rep or BREP), i.e. it represents object by its boundaries.\n\nThe definition of generalized map in any dimension is given in and:\n\nA \"n\"D generalized map (or \"n\"G-map) is an (\"n\" + 2)-tuple \"G\" = (\"D\", \"α\", ..., \"α\") such that:\n\nAn \"n\"D generalized map represents the subdivision of an open or closed orientable or not \"n\"D space.\n\n"}
{"id": "13733", "url": "https://en.wikipedia.org/wiki?curid=13733", "title": "Hilbert's basis theorem", "text": "Hilbert's basis theorem\n\nIn mathematics, specifically commutative algebra, Hilbert's basis theorem says that a polynomial ring over a Noetherian ring is Noetherian.\n\nIf formula_1 a ring, let formula_2 denote the ring of polynomials in the indeterminate formula_3 over formula_1. Hilbert proved that if formula_1 is \"not too large\", in the sense that if formula_1 is Noetherian, the same must be true for formula_2. Formally,\n\nHilbert's Basis Theorem. If formula_1 is a Noetherian ring, then formula_2 is a Noetherian ring.\n\nCorollary. If formula_1 is a Noetherian ring, then formula_11 is a Noetherian ring.\n\nThis can be translated into algebraic geometry as follows: every algebraic set over a field can be described as the set of common roots of finitely many polynomial equations. proved the theorem (for the special case of polynomial rings over a field) in the course of his proof of finite generation of rings of invariants.\n\nHilbert produced an innovative proof by contradiction using mathematical induction; his method does not give an algorithm to produce the finitely many basis polynomials for a given ideal: it only shows that they must exist. One can determine basis polynomials using the method of Gröbner bases.\n\nRemark. We will give two proofs, in both only the \"left\" case is considered, the proof for the right case is similar.\n\nSuppose formula_14 were a non-finitely generated left-ideal. Then by recursion (using the axiom of dependent choice) there is a sequence formula_15 of polynomials such that if formula_16 is the left ideal generated by formula_17 then formula_18 is of minimal degree. It is clear that formula_19 is a non-decreasing sequence of naturals. Let formula_20 be the leading coefficient of formula_21 and let formula_22 be the left ideal in formula_1 generated by formula_24. Since formula_1 is Noetherian the chain of ideals\n\nmust terminate. Thus formula_27 for some integer formula_28. So in particular,\n\nNow consider\n\nwhose leading term is equal to that of formula_31; moreover, formula_32. However, formula_33, which means that formula_34 has degree less than formula_31, contradicting the minimality.\n\nLet formula_14 be a left-ideal. Let formula_37 be the set of leading coefficients of members of formula_38. This is obviously a left-ideal over formula_1, and so is finitely generated by the leading coefficients of finitely many members of formula_38; say formula_41. Let formula_42 be the maximum of the set formula_43, and let formula_44 be the set of leading coefficients of members of formula_38, whose degree is formula_46. As before, the formula_44 are left-ideals over formula_1, and so are finitely generated by the leading coefficients of finitely many members of formula_38, say\n\nwith degrees formula_46. Now let formula_52 be the left-ideal generated by:\n\nWe have formula_54 and claim also formula_55. Suppose for the sake of contradiction this is not so. Then let formula_56 be of minimal degree, and denote its leading coefficient by formula_57.\n\nThus our claim holds, and formula_73 which is finitely generated.\n\nNote that the only reason we had to split into two cases was to ensure that the powers of formula_3 multiplying the factors, were non-negative in the constructions.\n\nLet formula_1 be a Noetherian commutative ring. Hilbert's basis theorem has some immediate corollaries.\n\n\nThe Mizar project has completely formalized and automatically checked a proof of Hilbert's basis theorem in the HILBASIS file.\n\n"}
{"id": "160990", "url": "https://en.wikipedia.org/wiki?curid=160990", "title": "Infinitesimal", "text": "Infinitesimal\n\nIn mathematics, infinitesimals are things so small that there is no way to measure them. The insight with exploiting infinitesimals was that entities could still retain certain specific properties, such as angle or slope, even though these entities were quantitatively small. The word \"infinitesimal\" comes from a 17th-century Modern Latin coinage \"infinitesimus\", which originally referred to the \"infinite-th\" item in a sequence. \nInfinitesimals are a basic ingredient in the procedures of infinitesimal calculus as developed by Leibniz, including the law of continuity and the transcendental law of homogeneity. In common speech, an infinitesimal object is an object that is smaller than any feasible measurement, but not zero in size—or, so small that it cannot be distinguished from zero by any available means. Hence, when used as an adjective, \"infinitesimal\" means \"extremely small\". To give it a meaning, it usually must be compared to another infinitesimal object in the same context (as in a derivative). Infinitely many infinitesimals are summed to produce an integral.\n\nThe concept of infinitesimals was originally introduced around 1670 by either Nicolaus Mercator or Gottfried Wilhelm Leibniz. Archimedes used what eventually came to be known as the method of indivisibles in his work \"The Method of Mechanical Theorems\" to find areas of regions and volumes of solids. In his formal published treatises, Archimedes solved the same problem using the method of exhaustion. The 15th century saw the work of Nicholas of Cusa, further developed in the 17th century by Johannes Kepler, in particular calculation of area of a circle by representing the latter as an infinite-sided polygon. Simon Stevin's work on decimal representation of all numbers in the 16th century prepared the ground for the real continuum. Bonaventura Cavalieri's method of indivisibles led to an extension of the results of the classical authors. The method of indivisibles related to geometrical figures as being composed of entities of codimension 1. John Wallis's infinitesimals differed from indivisibles in that he would decompose geometrical figures into infinitely thin building blocks of the same dimension as the figure, preparing the ground for general methods of the integral calculus. He exploited an infinitesimal denoted 1/∞ in area calculations.\n\nThe use of infinitesimals by Leibniz relied upon heuristic principles, such as the law of continuity: what succeeds for the finite numbers succeeds also for the infinite numbers and vice versa; and the transcendental law of homogeneity that specifies procedures for replacing expressions involving inassignable quantities, by expressions involving only assignable ones. The 18th century saw routine use of infinitesimals by mathematicians such as Leonhard Euler and Joseph-Louis Lagrange. Augustin-Louis Cauchy exploited infinitesimals both in defining continuity in his \"Cours d'Analyse\", and in defining an early form of a Dirac delta function. As Cantor and Dedekind were developing more abstract versions of Stevin's continuum, Paul du Bois-Reymond wrote a series of papers on infinitesimal-enriched continua based on growth rates of functions. Du Bois-Reymond's work inspired both Émile Borel and Thoralf Skolem. Borel explicitly linked du Bois-Reymond's work to Cauchy's work on rates of growth of infinitesimals. Skolem developed the first non-standard models of arithmetic in 1934. A mathematical implementation of both the law of continuity and infinitesimals was achieved by Abraham Robinson in 1961, who developed non-standard analysis based on earlier work by Edwin Hewitt in 1948 and Jerzy Łoś in 1955. The hyperreals implement an infinitesimal-enriched continuum and the transfer principle implements Leibniz's law of continuity. The standard part function implements Fermat's adequality.\n\nVladimir Arnold wrote in 1990:\n\nThe notion of infinitely small quantities was discussed by the Eleatic School. The Greek mathematician Archimedes (c.287 BC–c.212 BC), in \"The Method of Mechanical Theorems\", was the first to propose a logically rigorous definition of infinitesimals. His Archimedean property defines a number \"x\" as infinite if it satisfies the conditions |\"x\"|>1, |\"x\"|>1+1, |\"x\"|>1+1+1, ..., and infinitesimal if \"x\"≠0 and a similar set of conditions holds for \"x\" and the reciprocals of the positive integers. A number system is said to be Archimedean if it contains no infinite or infinitesimal members.\n\nThe English mathematician John Wallis introduced the expression 1/∞ in his 1655 book \"Treatise on the Conic Sections\". The symbol, which denotes the reciprocal, or inverse, of ∞, is the symbolic representation of the mathematical concept of an infinitesimal. In his \"Treatise on the Conic Sections\" Wallis also discusses the concept of a relationship between the symbolic representation of infinitesimal 1/∞ that he introduced and the concept of infinity for which he introduced the symbol ∞. The concept suggests a thought experiment of adding an infinite number of parallelograms of infinitesimal width to form a finite area. This concept was the predecessor to the modern method of integration used in integral calculus. The conceptual origins of the concept of the infinitesimal 1/∞ can be traced as far back as the Greek philosopher Zeno of Elea, whose Zeno's dichotomy paradox was the first mathematical concept to consider the relationship between a finite interval and an interval approaching that of an infinitesimal-sized interval.\n\nInfinitesimals were the subject of political and religious controversies in 17th century Europe, including a ban on infinitesimals issued by clerics in Rome in 1632.\n\nPrior to the invention of calculus mathematicians were able to calculate tangent lines using Pierre de Fermat's method of adequality and René Descartes' method of normals. There is debate among scholars as to whether the method was infinitesimal or algebraic in nature. When Newton and Leibniz invented the calculus, they made use of infinitesimals, Newton's \"fluxions\" and Leibniz' \"differential\". The use of infinitesimals was attacked as incorrect by Bishop Berkeley in his work \"The Analyst\". Mathematicians, scientists, and engineers continued to use infinitesimals to produce correct results. In the second half of the nineteenth century, the calculus was reformulated by Augustin-Louis Cauchy, Bernard Bolzano, Karl Weierstrass, Cantor, Dedekind, and others using the (ε, δ)-definition of limit and set theory. \nWhile the followers of Cantor, Dedekind, and Weierstrass sought to rid analysis of infinitesimals, and their philosophical allies like Bertrand Russell and Rudolf Carnap declared that infinitesimals are \"pseudoconcepts\", Hermann Cohen and his Marburg school of neo-Kantianism sought to develop a working logic of infinitesimals. The mathematical study of systems containing infinitesimals continued through the work of Levi-Civita, Giuseppe Veronese, Paul du Bois-Reymond, and others, throughout the late nineteenth and the twentieth centuries, as documented by Philip Ehrlich (2006). In the 20th century, it was found that infinitesimals could serve as a basis for calculus and analysis; see hyperreal number.\n\nIn extending the real numbers to include infinite and infinitesimal quantities, one typically wishes to be as conservative as possible by not changing any of their elementary properties. This guarantees that as many familiar results as possible are still available. Typically \"elementary\" means that there is no quantification over sets, but only over elements. This limitation allows statements of the form \"for any number x...\" For example, the axiom that states \"for any number \"x\", \"x\" + 0 = \"x\"\" would still apply. The same is true for quantification over several numbers, e.g., \"for any numbers \"x\" and \"y\", \"xy\" = \"yx\".\" However, statements of the form \"for any \"set\" \"S\" of numbers ...\" may not carry over. Logic with this limitation on quantification is referred to as first-order logic.\n\nThe resulting extended number system cannot agree with the reals on all properties that can be expressed by quantification over sets, because the goal is to construct a non-Archimedean system, and the Archimedean principle can be expressed by quantification over sets. One can conservatively extend any theory including reals, including set theory, to include infinitesimals, just by adding a countably infinite list of axioms that assert that a number is smaller than 1/2, 1/3, 1/4 and so on. Similarly, the completeness property cannot be expected to carry over, because the reals are the unique complete ordered field up to isomorphism.\n\nWe can distinguish three levels at which a nonarchimedean number system could have first-order properties compatible with those of the reals:\n\n\nSystems in category 1, at the weak end of the spectrum, are relatively easy to construct, but do not allow a full treatment of classical analysis using infinitesimals in the spirit of Newton and Leibniz. For example, the transcendental functions are defined in terms of infinite limiting processes, and therefore there is typically no way to define them in first-order logic. Increasing the analytic strength of the system by passing to categories 2 and 3, we find that the flavor of the treatment tends to become less constructive, and it becomes more difficult to say anything concrete about the hierarchical structure of infinities and infinitesimals.\n\nAn example from category 1 above is the field of Laurent series with a finite number of negative-power terms. For example, the Laurent series consisting only of the constant term 1 is identified with the real number 1, and the series with only the linear term \"x\" is thought of as the simplest infinitesimal, from which the other infinitesimals are constructed. Dictionary ordering is used, which is equivalent to considering higher powers of \"x\" as negligible compared to lower powers. David O. Tall refers to this system as the super-reals, not to be confused with the superreal number system of Dales and Woodin. Since a Taylor series evaluated with a Laurent series as its argument is still a Laurent series, the system can be used to do calculus on transcendental functions if they are analytic. These infinitesimals have different first-order properties than the reals because, for example, the basic infinitesimal \"x\" does not have a square root.\n\nThe Levi-Civita field is similar to the Laurent series, but is algebraically closed. For example, the basic infinitesimal x has a square root. This field is rich enough to allow a significant amount of analysis to be done, but its elements can still be represented on a computer in the same sense that real numbers can be represented in floating point.\n\nThe field of transseries is larger than the Levi-Civita field. An example of a transseries is:\n\nwhere for purposes of ordering \"x\" is considered infinite.\n\nConway's surreal numbers fall into category 2. They are a system designed to be as rich as possible in different sizes of numbers, but not necessarily for convenience in doing analysis. Certain transcendental functions can be carried over to the surreals, including logarithms and exponentials, but most, e.g., the sine function, cannot. The existence of any particular surreal number, even one that has a direct counterpart in the reals, is not known a priori, and must be proved.\n\nThe most widespread technique for handling infinitesimals is the hyperreals, developed by Abraham Robinson in the 1960s. They fall into category 3 above, having been designed that way so all of classical analysis can be carried over from the reals. This property of being able to carry over all relations in a natural way is known as the transfer principle, proved by Jerzy Łoś in 1955. For example, the transcendental function sin has a natural counterpart *sin that takes a hyperreal input and gives a hyperreal output, and similarly the set of natural numbers formula_2 has a natural counterpart formula_3, which contains both finite and infinite integers. A proposition such as formula_4 carries over to the hyperreals as formula_5 .\n\nThe superreal number system of Dales and Woodin is a generalization of the hyperreals. It is different from the super-real system defined by David Tall.\n\nIn linear algebra, the dual numbers extend the reals by adjoining one infinitesimal, the new element ε with the property ε = 0 (that is, ε is nilpotent). Every dual number has the form \"z\" = \"a\" + \"b\"ε with \"a\" and \"b\" being uniquely determined real numbers.\n\nOne application of dual numbers is automatic differentiation. This application can be generalized to polynomials in n variables, using the Exterior algebra of an n-dimensional vector space.\n\nSynthetic differential geometry or smooth infinitesimal analysis have roots in category theory. This approach departs from the classical logic used in conventional mathematics by denying the general applicability of the law of excluded middle – i.e., \"not\" (\"a\" ≠ \"b\") does not have to mean \"a\" = \"b\". A \"nilsquare\" or \"nilpotent\" infinitesimal can then be defined. This is a number \"x\" where \"x\" = 0 is true, but \"x\" = 0 need not be true at the same time. Since the background logic is intuitionistic logic, it is not immediately clear how to classify this system with regard to classes 1, 2, and 3. Intuitionistic analogues of these classes would have to be developed first.\n\nCauchy used an infinitesimal formula_6 to write down a unit impulse, infinitely tall and narrow Dirac-type delta function formula_7 satisfying formula_8 in a number of articles in 1827, see Laugwitz (1989). Cauchy defined an infinitesimal in 1821 (Cours d'Analyse) in terms of a sequence tending to zero. Namely, such a null sequence becomes an infinitesimal in Cauchy's and Lazare Carnot's terminology.\n\nModern set-theoretic approaches allow one to define infinitesimals via the ultrapower construction, where a null sequence becomes an infinitesimal in the sense of an equivalence class modulo a relation defined in terms of a suitable ultrafilter. The article by Yamashita (2007) contains a bibliography on modern Dirac delta functions in the context of an infinitesimal-enriched continuum provided by the hyperreals.\n\nThe method of constructing infinitesimals of the kind used in nonstandard analysis depends on the model and which collection of axioms are used. We consider here systems where infinitesimals can be shown to exist.\n\nIn 1936 Maltsev proved the compactness theorem. This theorem is fundamental for the existence of infinitesimals as it proves that it is possible to formalise them. A consequence of this theorem is that if there is a number system in which it is true that for any positive integer \"n\" there is a positive number \"x\" such that 0 < \"x\" < 1/\"n\", then there exists an extension of that number system in which it is true that there exists a positive number \"x\" such that for any positive integer \"n\" we have 0 < \"x\" < 1/\"n\". The possibility to switch \"for any\" and \"there exists\" is crucial. The first statement is true in the real numbers as given in ZFC set theory : for any positive integer \"n\" it is possible to find a real number between 1/\"n\" and zero, but this real number depends on \"n\". Here, one chooses \"n\" first, then one finds the corresponding \"x\". In the second expression, the statement says that there is an \"x\" (at least one), chosen first, which is between 0 and 1/\"n\" for any \"n\". In this case \"x\" is infinitesimal. This is not true in the real numbers (R) given by ZFC. Nonetheless, the theorem proves that there is a model (a number system) in which this is true. The question is: what is this model? What are its properties? Is there only one such model?\n\nThere are in fact many ways to construct such a one-dimensional linearly ordered set of numbers, but fundamentally, there are two different approaches:\n\nIn 1960, Abraham Robinson provided an answer following the first approach. The extended set is called the hyperreals and contains numbers less in absolute value than any positive real number. The method may be considered relatively complex but it does prove that infinitesimals exist in the universe of ZFC set theory. The real numbers are called standard numbers and the new non-real hyperreals are called nonstandard.\n\nIn 1977 Edward Nelson provided an answer following the second approach. The extended axioms are IST, which stands either for Internal set theory or for the initials of the three extra axioms: Idealization, Standardization, Transfer. In this system we consider that the language is extended in such a way that we can express facts about infinitesimals. The real numbers are either standard or nonstandard. An infinitesimal is a nonstandard real number that is less, in absolute value, than any positive standard real number.\n\nIn 2006 Karel Hrbacek developed an extension of Nelson's approach in which the real numbers are stratified in (infinitely) many levels; i.e., in the coarsest level there are no infinitesimals nor unlimited numbers. Infinitesimals are in a finer level and there are also infinitesimals with respect to this new level and so on.\n\nCalculus textbooks based on infinitesimals include the classic \"Calculus Made Easy\" by Silvanus P. Thompson (bearing the motto \"What one fool can do another can\") and the German text \"Mathematik fur Mittlere Technische Fachschulen der Maschinenindustrie\" by R Neuendorff. Pioneering works based on Abraham Robinson's infinitesimals include texts by Stroyan (dating from 1972) and Howard Jerome Keisler (). Students easily relate to the intuitive notion of an infinitesimal difference 1-\"0.999...\", where \"0.999...\" differs from its standard meaning as the real number 1, and is reinterpreted as an infinite terminating extended decimal that is strictly less than 1.\n\nAnother elementary calculus text that uses the theory of infinitesimals as developed by Robinson is \"Infinitesimal Calculus\" by Henle and Kleinberg, originally published in 1979. The authors introduce the language of first order logic, and demonstrate the construction of a first order model of the hyperreal numbers. The text provides an introduction to the basics of integral and differential calculus in one dimension, including sequences and series of functions. In an Appendix, they also treat the extension of their model to the \"hyperhyper\"reals, and demonstrate some applications for the extended model.\n\nIn a related but somewhat different sense, which evolved from the original definition of \"infinitesimal\" as an infinitely small quantity, the term has also been used to refer to a function tending to zero. More precisely, Loomis and Sternberg's \"Advanced Calculus\" defines the function class of infinitesimals, formula_9, as a subset of functions formula_10 between normed vector spaces by formula_11, as well as two related classes formula_12 (see Big-O notation) by formula_13, andformula_14.The set inclusions formula_15generally hold. That the inclusions are proper is demonstrated by the real-valued functions of a real variable formula_16, formula_17, and formula_18: formula_19 but formula_20 and formula_21.As an application of these definitions, a mapping formula_22 between normed vector spaces is defined to be differentiable at formula_23 if there is a formula_24 [i.e, a bounded linear map formula_25] such that formula_26in a neighborhood of formula_6. If such a map exists, it is unique; this map is called the \"differential\" and is denoted by formula_28, coinciding with the traditional notation for the classical (though logically flawed) notion of a differential as an infinitely small \"piece\" of \"F\". This definition represents a generalization of the usual definition of differentiability for vector-valued functions of (open subsets of) Euclidean spaces.\n\nLet formula_29 be a probability space and let formula_30. An array formula_31 of random variables is called infinitesimal if for every formula_32, we have:\nThe notion of infinitesimal array is essential in some central limit theorems and it is easily seen by monotonicity of the expectation operator that any array satisfying Lindeberg's condition is infinitesimal, thus playing an important role in Lindeberg's Central Limit Theorem (a generalization of the central limit theorem).\n\n"}
{"id": "18372173", "url": "https://en.wikipedia.org/wiki?curid=18372173", "title": "Information diagram", "text": "Information diagram\n\nAn information diagram is a type of Venn diagram used in information theory to illustrate relationships among Shannon's basic measures of information: entropy, joint entropy, conditional entropy and mutual information. Information diagrams are a useful pedagogical tool for teaching and learning about these basic measures of information, but using such diagrams carries some non-trivial implications. For example, Shannon's entropy in the context of an information diagram must be taken as a signed measure. (See the article \"Information theory and measure theory\" for more information.)\n"}
{"id": "15491", "url": "https://en.wikipedia.org/wiki?curid=15491", "title": "Integer factorization", "text": "Integer factorization\n\nIn number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.\n\nWhen the numbers are sufficiently large, no efficient, non-quantum integer factorization algorithm is known. An effort by several researchers, concluded in 2009, to factor a 232-digit number (RSA-768) utilizing hundreds of machines took two years and the researchers estimated that a 1024-bit RSA modulus would take about a thousand times as long. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.\n\nNot all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.\n\nMany cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.\n\nBy the fundamental theorem of arithmetic, every positive integer has a unique prime factorization. (By convention, 1 is the empty product.) If the integer is prime then it can be recognized as such in polynomial time, for example, by the AKS primality test. If composite however, the theorem gives no insight into how to obtain the factors.\n\nGiven a general algorithm for integer factorization, any integer can be factored into its constituent prime factors by repeated application of this algorithm. The situation is more complicated with special-purpose factorization algorithms, whose benefits may not be realized as well or even at all with the factors produced during decomposition. For example, if where are very large primes, trial division will quickly produce the factors 3 and 19 but will take \"p\" divisions to find the next factor. As a contrasting example, if \"N\" is the product of the primes 13729, 1372933, and 18848997161, where , Fermat's factorization method will begin with which immediately yields and hence the factors and . While these are easily recognized as composite and prime respectively, Fermat's method will take much longer to factor the composite number because the starting value of for \"a\" is nowhere near 1372933.\n\nAmong the \"b\"-bit numbers, the most difficult to factor in practice using existing algorithms are those that are products of two primes of similar size. For this reason, these are the integers used in cryptographic applications. The largest such semiprime yet factored was RSA-768, a 768-bit number with 232 decimal digits, on December 12, 2009. This factorization was a collaboration of several research institutions, spanning two years and taking the equivalent of almost 2000 years of computing on a single-core 2.2 GHz AMD Opteron. Like all recent factorization records, this factorization was completed with a highly optimized implementation of the general number field sieve run on hundreds of machines.\n\nNo algorithm has been published that can factor all integers in polynomial time, i.e., that can factor \"b\"-bit numbers in time O(\"b\") for some constant \"k\". The problem is clearly in class NP but has not been proved to be or not be NP-complete. \n\nThere are published algorithms that are faster than O((1+\"ε\")) for all positive \"ε\", i.e., sub-exponential. The best published asymptotic running time is for the general number field sieve (GNFS) algorithm, which, for a \"b\"-bit number \"n\", is:\n\nFor current computers, GNFS is the best published algorithm for large \"n\" (more than about 400 bits). For a quantum computer, however, Peter Shor discovered an algorithm in 1994 that solves it in polynomial time. This will have significant implications for cryptography if quantum computation becomes scalable. Shor's algorithm takes only time and O(\"b\") space on \"b\"-bit number inputs. In 2001, the first seven-qubit quantum computer became the first to run Shor's algorithm. It factored the number 15.\n\nWhen discussing what complexity classes the integer factorization problem falls into, it is necessary to distinguish two slightly different versions of the problem:\n\n\nFor , the decision problem is equivalent to asking if \"N\" is not prime. \n\nAn algorithm for either version provides one for the other. Repeated application of the function problem (applied to \"d\" and \"N\"/\"d\", and their factors, if needed) will eventually provide either a factor of \"N\" no larger than \"M\" or a factorization into primes all greater than \"M\". All known algorithms for the decision problem work in this way. Hence it is only of theoretical interest that, with at most log \"N\" queries using an algorithm for the decision problem, one would isolate a factor of \"N\" (or prove it prime) by binary search. \n\nIt is not known exactly which complexity classes contain the decision version of the integer factorization problem. It is known to be in both NP and co-NP. This is because both YES and NO answers can be verified in polynomial time. An answer of YES can be certified by exhibiting a factorization with . An answer of NO can be certified by exhibiting the factorization of \"N\" into distinct primes, all larger than \"M\". We can verify their primality using the AKS primality test and that their product is \"N\" by multiplication. The fundamental theorem of arithmetic guarantees that there is only one possible string that will be accepted (providing the factors are required to be listed in order), which shows that the problem is in both UP and co-UP. It is known to be in BQP because of Shor's algorithm. It is suspected to be outside of all three of the complexity classes P, NP-complete, and co-NP-complete. It is therefore a candidate for the NP-intermediate complexity class. If it could be proved that it is in either NP-Complete or co-NP-Complete, that would imply NP = co-NP. That would be a very surprising result, and therefore integer factorization is widely suspected to be outside both of those classes. Many people have tried to find classical polynomial-time algorithms for it and failed, and therefore it is widely suspected to be outside P.\n\nIn contrast, the decision problem \"is \"N\" a composite number?\" (or equivalently: \"is \"N\" a prime number?\") appears to be much easier than the problem of actually finding the factors of \"N\". Specifically, the former can be solved in polynomial time (in the number \"n\" of digits of \"N\") with the AKS primality test. In addition, there are a number of probabilistic algorithms that can test primality very quickly in practice if one is willing to accept the vanishingly small possibility of error. The ease of primality testing is a crucial part of the RSA algorithm, as it is necessary to find large prime numbers to start with.\n\nA special-purpose factoring algorithm's running time depends on the properties of the number to be factored or on one of its unknown factors: size, special form, etc. Exactly what the running time depends on varies between algorithms.\n\nAn important subclass of special-purpose factoring algorithms is the \"Category 1\" or \"First Category\" algorithms, whose running time depends on the size of smallest prime factor. Given an integer of unknown form, these methods are usually applied before general-purpose methods to remove small factors. For example, trial division is a Category 1 algorithm.\n\n\nA general-purpose factoring algorithm, also known as a \"Category 2\", \"Second Category\", or \"Kraitchik family\" algorithm (after Maurice Kraitchik), has a running time which depends solely on the size of the integer to be factored. This is the type of algorithm used to factor RSA numbers. Most general-purpose factoring algorithms are based on the congruence of squares method.\n\n\n\nIn number theory, there are many integer factoring algorithms that heuristically have expected running time\n\nin big O and L-notation.\nSome examples of those algorithms are the elliptic curve method and the quadratic sieve.\nAnother such algorithm is the class group relations method proposed by Schnorr, Seysen, and Lenstra, that is proved under the assumption of the Generalized Riemann Hypothesis (GRH).\n\nThe Schnorr-Seysen-Lenstra probabilistic algorithm has been rigorously proven by Lenstra and Pomerance to have expected running time formula_3 by replacing the GRH assumption with the use of multipliers.\nThe algorithm uses the class group of positive binary quadratic forms of discriminant Δ denoted by \"G\".\n\"G\" is the set of triples of integers (\"a\", \"b\", \"c\") in which those integers are relative prime.\n\nGiven an integer \"n\" that will be factored, where \"n\" is an odd positive integer greater than a certain constant. In this factoring algorithm the discriminant Δ is chosen as a multiple of \"n\", , where \"d\" is some positive multiplier. The algorithm expects that for one \"d\" there exist enough smooth forms in \"G\". Lenstra and Pomerance show that the choice of \"d\" can be restricted to a small set to guarantee the smoothness result.\n\nDenote by \"P\" the set of all primes \"q\" with Kronecker symbol formula_4. By constructing a set of generators of \"G\" and prime forms \"f\" of \"G\" with \"q\" in \"P\" a sequence of relations between the set of generators and \"f\" are produced.\nThe size of \"q\" can be bounded by formula_5 for some constant formula_6.\n\nThe relation that will be used is a relation between the product of powers that is equal to the neutral element of \"G\". These relations will be used to construct a so-called ambiguous form of \"G\", which is an element of \"G\" of order dividing 2. By calculating the corresponding factorization of Δ and by taking a gcd, this ambiguous form provides the complete prime factorization of \"n\". This algorithm has these main steps:\n\nLet \"n\" be the number to be factored.\n\nTo obtain an algorithm for factoring any positive integer, it is necessary to add a few steps to this algorithm such as trial division, and the Jacobi sum test.\n\nThe algorithm as stated is a probabilistic algorithm as it makes random choices. Its expected running time is at most formula_3.\n\n\n\n"}
{"id": "482331", "url": "https://en.wikipedia.org/wiki?curid=482331", "title": "Iraqi block cipher", "text": "Iraqi block cipher\n\nIn cryptography, the Iraqi block cipher was a block cipher published in C source code form by anonymous FTP upload around July 1999, and widely distributed on Usenet. It is a five round unbalanced Feistel cipher operating on a 256 bit block with a 160 bit key.\n\nA comment suggests that it is of Iraqi origin. However, like the S-1 block cipher, it is generally regarded as a hoax, although of lesser quality than S-1. Although the comment suggests that it is Iraqi in origin, all comments, variable and function names and printed strings are in English rather than Arabic; the code is fairly inefficient (including some pointless operations), and the cipher's security may be flawed (no proof).\n\nBecause it has a constant key schedule the cipher is vulnerable to a slide attack. However, it may take 2 chosen texts to create a single slid pair, which would make the attack unfeasible. It also has a large number of fixed points, although that is not\nnecessarily a problem, except possibly for hashing modes. No public attack is currently available. As with S-1, it was David Wagner who first spotted the security flaws.\n\n"}
{"id": "187750", "url": "https://en.wikipedia.org/wiki?curid=187750", "title": "Large numbers", "text": "Large numbers\n\nLarge numbers are numbers that are significantly larger than those ordinarily used in everyday life, for instance in simple counting or in monetary transactions. The term typically refers to large positive integers, or more generally, large positive real numbers, but it may also be used in other contexts.\n\nVery large numbers often occur in fields such as mathematics, cosmology, cryptography, and statistical mechanics. Sometimes people refer to numbers as being \"astronomically large\". However, it is easy to mathematically define numbers that are much larger even than those used in astronomy.\n\nScientific notation was created to handle the wide range of values that occur in scientific study. 1.0 × 10, for example, means one billion, a 1 followed by nine zeros: 1 000 000 000, and 1.0 × 10 means one billionth, or 0.000 000 001. Writing 10 instead of nine zeros saves readers the effort and hazard of counting a long series of zeros to see how large the number is.\n\nExamples of large numbers describing everyday real-world objects are:\n\nOther large numbers, as regards length and time, are found in astronomy and cosmology. For example, the current Big Bang model suggests that the universe is 13.8 billion years (4.355 × 10 seconds) old, and that the observable universe is 93 billion light years across (8.8 × 10 metres), and contains about 5 × 10 stars, organized into around 125 billion (1.25 × 10) galaxies, according to Hubble Space Telescope observations. There are about 10 atoms in the observable universe, by rough estimation.\n\nAccording to Don Page, physicist at the University of Alberta, Canada, the longest finite time that has so far been explicitly calculated by any physicist is\n\nwhich corresponds to the scale of an estimated Poincaré recurrence time for the quantum state of a hypothetical box containing a black hole with the estimated mass of the entire universe, observable or not, assuming a certain inflationary model with an inflaton whose mass is 10 Planck masses. This time assumes a statistical model subject to Poincaré recurrence. A much simplified way of thinking about this time is in a model where the universe's history repeats itself arbitrarily many times due to properties of statistical mechanics; this is the time scale when it will first be somewhat similar (for a reasonable choice of \"similar\") to its current state again.\n\nCombinatorial processes rapidly generate even larger numbers. The factorial function, which defines the number of permutations on a set of fixed objects, grows very rapidly with the number of objects. Stirling's formula gives a precise asymptotic expression for this rate of growth.\n\nCombinatorial processes generate very large numbers in statistical mechanics. These numbers are so large that they are typically only referred to using their logarithms.\n\nGödel numbers, and similar numbers used to represent bit-strings in algorithmic information theory, are very large, even for mathematical statements of reasonable length. However, some pathological numbers are even larger than the Gödel numbers of typical mathematical propositions.\n\nLogician Harvey Friedman has done work related to very large numbers, such as with Kruskal's tree theorem and the Robertson–Seymour theorem.\n\nBetween 1980 and 2000, personal computer hard disk sizes increased from about 10 megabytes (10 bytes) to over 100 gigabytes (10 bytes). A 100-gigabyte disk could store the favorite color of all of Earth's seven billion inhabitants without using data compression (storing 14 bytes times 7 billion inhabitants would equal 98 GB used). But what about a dictionary-on-disk storing all possible passwords containing up to 40 characters? Assuming each character equals one byte, there are about 2 such passwords, which is about 2 × 10. In his paper \"Computational capacity of the universe\", Seth Lloyd points out that if every particle in the universe could be used as part of a huge computer, it could store only about 10 bits, less than one millionth of the size such a dictionary would require. However, storing information on hard disk and computing it are very different functions. On the one hand storage currently has limitations as stated, but computational speed is a different matter. It is quite conceivable that the stated limitations regarding storage have no bearing on the limitations of actual computational capacity, especially if the current research into quantum computers results in a \"quantum leap\" (but see \"holographic principle\").\n\nStill, computers can easily be programmed to start creating and displaying all possible 40-character passwords one at a time. Such a program could be left to run indefinitely. Assuming a modern PC could output 1 billion strings per second, it would take one billionth of 2 × 10 seconds, or 2 × 10 seconds to complete its task, which is about 6 × 10 years. By contrast, the universe is estimated to be 13.8 billion (1.38 × 10) years old. Computers will presumably continue to get faster, but the same paper mentioned before estimates that the entire universe functioning as a giant computer could have performed no more than 10 operations since the Big Bang. This is trillions of times more computation than is required for displaying all 40-character passwords, but computing all \"50\" character passwords would outstrip the estimated computational potential of the entire universe.\n\nProblems like this grow exponentially in the number of computations they require, and they are one reason why exponentially difficult problems are called \"intractable\" in computer science: for even small numbers like the 40 or 50 characters described earlier, the number of computations required exceeds even theoretical limits on mankind's computing power. The traditional division between \"easy\" and \"hard\" problems is thus drawn between programs that do and do not require exponentially increasing resources to execute.\n\nSuch limits are an advantage in cryptography, since any cipher-breaking technique that requires more than, say, the 10 operations mentioned before will never be feasible. Such ciphers must be broken by finding efficient techniques unknown to the cipher's designer. Likewise, much of the research throughout all branches of computer science focuses on finding efficient solutions to problems that work with far fewer resources than are required by a naïve solution. For example, one way of finding the greatest common divisor between two 1000-digit numbers is to compute all their factors by trial division. This will take up to 2 × 10 division operations, far too large to contemplate. But the Euclidean algorithm, using a much more efficient technique, takes only a fraction of a second to compute the GCD for even huge numbers such as these.\n\nAs a general rule, then, PCs in 2005 can perform 2 calculations in a few minutes. A few thousand PCs working for a few years could solve a problem requiring 2 calculations, but no amount of traditional computing power will solve a problem requiring 2 operations (which is about what would be required to brute-force the encryption keys in 128-bit SSL commonly used in web browsers, assuming the underlying ciphers remain secure). Limits on computer storage are comparable. Quantum computing might allow certain problems that require an exponential amount of calculations to become feasible. However, it has practical and theoretical challenges that may never be overcome, such as the mass production of qubits, the fundamental building block of quantum computing.\n\nThe total amount of printed material in the world is roughly 1.6 × 10 bits; therefore the contents can be represented by a number somewhere in the range 0 to roughly formula_11\n\nCompare:\nThe first number is much larger than the second, due to the larger height of the power tower, and in spite of the small numbers 1.1. In comparing the magnitude of each successive exponent in the last number with formula_15, we find a difference in the magnitude of effect on the final exponent.\n\nGiven a strictly increasing integer sequence/function formula_16 (\"n\"≥1) we can produce a faster-growing sequence formula_17 (where the superscript \"n\" denotes the \"n\" functional power). This can be repeated any number of times by letting formula_18, each sequence growing much faster than the one before it. Then we could define formula_19, which grows much faster than any formula_20 for finite \"k\" (here ω is the first infinite ordinal number, representing the limit of all finite numbers k). This is the basis for the fast-growing hierarchy of functions, in which the indexing subscript is extended to ever-larger ordinals.\n\nFor example, starting with \"f\"(\"n\") = \"n\" + 1:\n\n\nA standardized way of writing very large numbers allows them to be easily sorted in increasing order, and one can get a good idea of how much larger a number is than another one.\n\nTo compare numbers in scientific notation, say 5×10 and 2×10, compare the exponents first, in this case 5 > 4, so 2×10 > 5×10. If the exponents are equal, the \"mantissa\" (or coefficient) should be compared, thus 5×10 > 2×10 because 5 > 2.\n\nTetration with base 10 gives the sequence formula_21, the power towers of numbers 10, where formula_22 denotes a functional power of the function formula_23 (the function also expressed by the suffix \"-plex\" as in googolplex, see the Googol family).\n\nThese are very round numbers, each representing an order of magnitude in a generalized sense. A crude way of specifying how large a number is, is specifying between which two numbers in this sequence it is.\n\nMore accurately, numbers in between can be expressed in the form formula_24, i.e., with a power tower of 10s and a number at the top, possibly in scientific notation, e.g. formula_25, a number between formula_26 and formula_27 (note that formula_28 if formula_29). (See also extension of tetration to real heights.)\n\nThus googolplex is formula_30\n\nAnother example:\n\nThus the \"order of magnitude\" of a number (on a larger scale than usually meant), can be characterized by the number of times (\"n\") one has to take the formula_34 to get a number between 1 and 10. Thus, the number is between formula_35 and formula_36. As explained, a more accurate description of a number also specifies the value of this number between 1 and 10, or the previous number (taking the logarithm one time less) between 10 and 10, or the next, between 0 and 1.\n\nNote that\nI.e., if a number \"x\" is too large for a representation formula_38 we can make the power tower one higher, replacing \"x\" by log\"x\", or find \"x\" from the lower-tower representation of the log of the whole number. If the power tower would contain one or more numbers different from 10, the two approaches would lead to different results, corresponding to the fact that extending the power tower with a 10 at the bottom is then not the same as extending it with a 10 at the top (but, of course, similar remarks apply if the whole power tower consists of copies of the same number, different from 10).\n\nIf the height of the tower is large, the various representations for large numbers can be applied to the height itself. If the height is given only approximately, giving a value at the top does not make sense, so we can use the double-arrow notation, e.g. formula_39. If the value after the double arrow is a very large number itself, the above can recursively be applied to that value.\n\nExamples:\n\nSimilarly to the above, if the exponent of formula_46 is not exactly given then giving a value at the right does not make sense, and we can, instead of using the power notation of formula_46, add 1 to the exponent of formula_48, so we get e.g. formula_49.\n\nIf the exponent of formula_50 is large, the various representations for large numbers can be applied to this exponent itself. If this exponent is not exactly given then, again, giving a value at the right does not make sense, and we can, instead of using the power notation of formula_50, use the triple arrow operator, e.g. formula_52.\n\nIf the right-hand argument of the triple arrow operator is large the above applies to it, so we have e.g. formula_53 (between formula_54 and formula_55). This can be done recursively, so we can have a power of the triple arrow operator.\n\nWe can proceed with operators with higher numbers of arrows, written formula_56.\n\nCompare this notation with the hyper operator and the Conway chained arrow notation:\nAn advantage of the first is that when considered as function of \"b\", there is a natural notation for powers of this function (just like when writing out the \"n\" arrows): formula_58. For example:\n\nand only in special cases the long nested chain notation is reduced; for \"b\" = 1 we get:\n\nSince the \"b\" can also be very large, in general we write a number with a sequence of powers formula_61 with decreasing values of \"n\" (with exactly given integer exponents formula_62) with at the end a number in ordinary scientific notation. Whenever a formula_62 is too large to be given exactly, the value of formula_64 is increased by 1 and everything to the right of formula_65 is rewritten.\n\nFor describing numbers approximately, deviations from the decreasing order of values of \"n\" are not needed. For example, formula_66, and formula_67. Thus we have the somewhat counterintuitive result that a number \"x\" can be so large that, in a way, \"x\" and 10 are \"almost equal\" (for arithmetic of large numbers see also below).\n\nIf the superscript of the upward arrow is large, the various representations for large numbers can be applied to this superscript itself. If this superscript is not exactly given then there is no point in raising the operator to a particular power or to adjust the value on which it acts. We can simply use a standard value at the right, say 10, and the expression reduces to formula_68 with an approximate \"n\". For such numbers the advantage of using the upward arrow notation no longer applies, and we can also use the chain notation.\n\nThe above can be applied recursively for this \"n\", so we get the notation formula_56 in the superscript of the first arrow, etc., or we have a nested chain notation, e.g.:\n\nIf the number of levels gets too large to be convenient, a notation is used where this number of levels is written down as a number (like using the superscript of the arrow instead of writing many arrows). Introducing a function formula_72 = (10 → 10 → \"n\"), these levels become functional powers of \"f\", allowing us to write a number in the form formula_73 where \"m\" is given exactly and n is an integer which may or may not be given exactly (for the example: formula_74. If \"n\" is large we can use any of the above for expressing it. The \"roundest\" of these numbers are those of the form \"f\"(1) = (10→10→\"m\"→2). For example, formula_75\n\nCompare the definition of Graham's number: it uses numbers 3 instead of 10 and has 64 arrow levels and the number 4 at the top; thus formula_76, but also formula_77.\n\nIf \"m\" in formula_73 is too large to give exactly we can use a fixed \"n\", e.g. \"n\" = 1, and apply the above recursively to \"m\", i.e., the number of levels of upward arrows is itself represented in the superscripted upward-arrow notation, etc. Using the functional power notation of \"f\" this gives multiple levels of \"f\". Introducing a function formula_79 these levels become functional powers of \"g\", allowing us to write a number in the form formula_80 where \"m\" is given exactly and n is an integer which may or may not be given exactly. We have (10→10→\"m\"→3) = \"g\"(1). If \"n\" is large we can use any of the above for expressing it. Similarly we can introduce a function \"h\", etc. If we need many such functions we can better number them instead of using a new letter every time, e.g. as a subscript, so we get numbers of the form formula_81 where \"k\" and \"m\" are given exactly and n is an integer which may or may not be given exactly. Using \"k\"=1 for the \"f\" above, \"k\"=2 for \"g\", etc., we have (10→10→\"n\"→\"k\") = formula_82. If \"n\" is large we can use any of the above for expressing it. Thus we get a nesting of forms formula_83 where going inward the \"k\" decreases, and with as inner argument a sequence of powers formula_84 with decreasing values of \"n\" (where all these numbers are exactly given integers) with at the end a number in ordinary scientific notation.\n\nWhen \"k\" is too large to be given exactly, the number concerned can be expressed as formula_85=(10→10→10→\"n\") with an approximate \"n\". Note that the process of going from the sequence formula_86=(10→\"n\") to the sequence formula_87=(10→10→\"n\") is very similar to going from the latter to the sequence formula_85=(10→10→10→\"n\"): it is the general process of adding an element 10 to the chain in the chain notation; this process can be repeated again (see also the previous section). Numbering the subsequent versions of this function a number can be described using functions formula_89, nested in lexicographical order with \"q\" the most significant number, but with decreasing order for \"q\" and for \"k\"; as inner argument we have a sequence of powers formula_84 with decreasing values of \"n\" (where all these numbers are exactly given integers) with at the end a number in ordinary scientific notation.\n\nFor a number too large to write down in the Conway chained arrow notation we can describe how large it is by the length of that chain, for example only using elements 10 in the chain; in other words, we specify its position in the sequence 10, 10→10, 10→10→10, .. If even the position in the sequence is a large number we can apply the same techniques again for that.\n\nNumbers expressible in decimal notation:\n\nNumbers expressible in scientific notation:\n\nNumbers expressible in (10 ↑) \"k\" notation:\n\nBigger numbers:\n\nThe following illustrates the effect of a base different from 10, base 100. It also illustrates representations of numbers, and the arithmetic.\n\nformula_125, with base 10 the exponent is doubled.\n\nformula_126, ditto.\n\nformula_127, the highest exponent is very little more than doubled (increased by log2).\n\n\nNote that for a number formula_146, one unit change in \"n\" changes the result by a factor 10. In a number like formula_147, with the 6.2 the result of proper rounding using significant figures, the true value of the exponent may be 50 less or 50 more. Hence the result may be a factor formula_148 too large or too small. This seems like extremely poor accuracy, but for such a large number it may be considered fair (a large error in a large number may be \"relatively small\" and therefore acceptable).\n\nIn the case of an approximation of an extremely large number, the relative error may be large, yet there may still be a sense in which we want to consider the numbers as \"close in magnitude\". For example, consider\n\nThe relative error is\n\na large relative error. However, we can also consider the relative error in the logarithms; in this case, the logarithms (to base 10) are 10 and 9, so the relative error in the logarithms is only 10%.\n\nThe point is that exponential functions magnify relative errors greatly – if \"a\" and \"b\" have a small relative error,\n\nthe relative error is larger, and\n\nwill have even larger relative error. The question then becomes: on which level of iterated logarithms do we wish to compare two numbers? There is a sense in which we may want to consider\n\nto be \"close in magnitude\". The relative error between these two numbers is large, and the relative error between their logarithms is still large; however, the relative error in their second-iterated logarithms is small:\n\nSuch comparisons of iterated logarithms are common, e.g., in analytic number theory.\n\nThere are some general rules relating to the usual arithmetic operations performed on very large numbers:\n\nHence:\n\nThe busy beaver function Σ is an example of a function which grows faster than any computable function. Its value for even relatively small input is huge. The values of Σ(\"n\") for \"n\" = 1, 2, 3, 4 are 1, 4, 6, 13 . Σ(5) is not known but is definitely ≥ 4098. Σ(6) is at least 3.5×10.\n\nAlthough all the numbers discussed above are very large, they are all still decidedly finite. Certain fields of mathematics define infinite and transfinite numbers. For example, aleph-null is the cardinality of the infinite set of natural numbers, and aleph-one is the next greatest cardinal number. formula_164 is the cardinality of the reals. The proposition that formula_165 is known as the continuum hypothesis.\n\nSome notations for extremely large numbers:\nThese notations are essentially functions of integer variables, which increase very rapidly with those integers. Ever-faster-increasing functions can easily be constructed recursively by applying these functions with large integers as argument.\n\nA function with a vertical asymptote is not helpful in defining a very large number, although the function increases very rapidly: one has to define an argument very close to the asymptote, i.e. use a very small number, and constructing that is equivalent to constructing a very large number, e.g. the reciprocal.\n"}
{"id": "5647547", "url": "https://en.wikipedia.org/wiki?curid=5647547", "title": "Latimer–MacDuffee theorem", "text": "Latimer–MacDuffee theorem\n\nThe Latimer–MacDuffee theorem is a theorem in abstract algebra, a branch of mathematics. \nIt is named after Claiborne Latimer and Cyrus Colton MacDuffee, who published it in 1933. Significant contributions to its theory were made later by Olga Taussky-Todd.\n\nLet formula_1 be a monic, irreducible polynomial of degree formula_2. The Latimer–MacDuffee theorem gives a one-to-one correspondence between formula_3-similarity classes of formula_4 matrices with characteristic polynomial formula_1 and the ideal classes in the order\n\nwhere ideals are considered equivalent if they are equal up to an overall (nonzero) rational scalar multiple. (Note that this order need not be the full ring of integers, so nonzero ideals need not be invertible.) Since an order in a number field has only finitely many ideal classes (even if it is not the maximal order, and we mean here ideals classes for all nonzero ideals, not just the invertible ones), it follows that there are only finitely many conjugacy classes of matrices over the integers with characteristic polynomial formula_7.\n"}
{"id": "2685460", "url": "https://en.wikipedia.org/wiki?curid=2685460", "title": "Level-spacing distribution", "text": "Level-spacing distribution\n\nIn mathematical physics, level spacing is the difference between consecutive elements in some set of real numbers. In particular, it is the difference between consecutive energy levels or eigenvalues of a matrix or linear operator.\n"}
{"id": "1426957", "url": "https://en.wikipedia.org/wiki?curid=1426957", "title": "Lwów–Warsaw school", "text": "Lwów–Warsaw school\n\nThe Lwów–Warsaw school () was a Polish school of thought founded by Kazimierz Twardowski in 1895 in Lemberg (now Lviv, Ukraine).\n\nAlthough the members represented a variety of disciplines, from mathematics through logic to psychology, the School is widely considered to have been a philosophical movement. Developed in the 1920s and 1930s, the school's work was interrupted by the outbreak of World War II. Despite this, the members went on and fundamentally influenced modern science, notably mathematics and logic, in the post-war period. The most prominent member of this school, Alfred Tarski, has been ranked as one of the four greatest logicians of all time—along with Aristotle, Gottlob Frege, and Kurt Gödel.\n\nIn modern Polish science, the philosopher Jan Woleński considers himself close to the heritage of the school. In 2013 Woleński was awarded by the Foundation for Polish Science for a comprehensive analysis of the work of the Lwów–Warsaw school and for placing its achievements within the international discourse of contemporary philosophy.\n\nThe Lwów–Warsaw school of logic lay at the origin of Polish logic and was closely associated with the Warsaw school of mathematics. It began as a more general philosophical school but steadily moved toward logic. In the 1930s Alfred Tarski initiated contacts with the Vienna Circle. In addition to Brentano, his pupils Anton Marty, Alexius Meinong and Edmund Husserl also considerably influenced Polish philosophy and the Lwów–Warsaw school.\n\nPrincipal topics of interest to that school included formal ontology, mereology, and universal or categorial grammar.\n\nAs mentioned above, many of the school's members worked in more than one field.\n\n\n\n"}
{"id": "2979544", "url": "https://en.wikipedia.org/wiki?curid=2979544", "title": "Minkowski–Hlawka theorem", "text": "Minkowski–Hlawka theorem\n\nIn mathematics, the Minkowski–Hlawka theorem is a result on the lattice packing of hyperspheres in dimension \"n\" > 1. It states that there is a lattice in Euclidean space of dimension \"n\", such that the corresponding best packing of hyperspheres with centres at the lattice points has density Δ satisfying\n\nwith ζ the Riemann zeta function. Here as \"n\" → ∞, ζ(\"n\") → 1. The proof of this theorem is indirect and does not give an explicit example, however, and there is still no known simple and explicit way to construct lattices with packing densities exceeding this bound for arbitrary \"n\". In principle one can find explicit examples: for example, even just picking a few \"random\" lattices will work with high probability. The problem is that testing these lattices to see if they are solutions requires finding their shortest vectors, and the number of cases to check grows very fast with the dimension, so this could take a very long time.\n\nThis result was stated without proof by and proved by . The result is related to a linear lower bound for the Hermite constant.\n\n proved the following generalization of the Minkowski–Hlawka theorem. If \"S\" is a bounded set in R with Jordan volume vol(\"S\") then the average number of nonzero lattice vectors in \"S\" is vol(\"S\")/\"D\", where the average is taken over all lattices with a fundamental domain of volume \"D\", and similarly the average number of primitive lattice vectors in \"S\" is vol(\"S\")/\"D\"ζ(\"n\").\n\nThe Minkowski–Hlawka theorem follows easily from this, using the fact that if \"S\" is a star-shaped centrally symmetric body (such as a ball) containing less than 2 primitive lattice vectors then it contains no nonzero lattice vectors.\n\n\n"}
{"id": "10451852", "url": "https://en.wikipedia.org/wiki?curid=10451852", "title": "Nahm equations", "text": "Nahm equations\n\nThe Nahm equations are a system of ordinary differential equations introduced by Werner Nahm in the context of the \"Nahm transform\" – an alternative to Ward's twistor construction of monopoles. The Nahm equations are formally analogous to the algebraic equations in the ADHM construction of instantons, where finite order matrices are replaced by differential operators. \n\nDeep study of the Nahm equations was carried out by Nigel Hitchin and Simon Donaldson. Conceptually, the equations arise in the process of infinite-dimensional hyperkähler reduction. Among their many applications we can mention: Hitchin's construction of monopoles, where this approach is critical for establishing nonsingularity of monopole solutions; Donaldson's description of the moduli space of monopoles; and the existence of hyperkähler structure on coadjoint orbits of complex semisimple Lie groups, proved by Peter Kronheimer, Olivier Biquard, and A.G. Kovalev.\n\nLet \"T\"(\"z\"),\"T\"(\"z\"), \"T\"(\"z\") be three matrix-valued meromorphic functions of a complex variable \"z\". The Nahm equations are a system of matrix differential equations\n\ntogether with certain analyticity properties, reality conditions, and boundary conditions. The three equations can be written concisely using the Levi-Civita symbol, in the form\n\nMore generally, instead of considering \"N\" by \"N\" matrices, one can consider Nahm's equations with values in a Lie algebra g.\n\nThe variable \"z\" is restricted to the open interval (0,2), and the following conditions are imposed:\n\nThere is a natural equivalence between \n\nThe Nahm equations can be written in the Lax form as follows. Set\n\nthen the system of Nahm equations is equivalent to the Lax equation\n\nAs an immediate corollary, we obtain that the spectrum of the matrix \"A\" does not depend on \"z\". Therefore, the characteristic equation \n\nwhich determines the so-called spectral curve in the twistor space \"TP\", is invariant under the flow in \"z\".\n\n\n\n"}
{"id": "1252473", "url": "https://en.wikipedia.org/wiki?curid=1252473", "title": "Nemmers Prize in Mathematics", "text": "Nemmers Prize in Mathematics\n\nThe Frederic Esser Nemmers Prize in Mathematics is awarded biennially from Northwestern University. It was initially endowed along with a companion prize, the Erwin Plein Nemmers Prize in Economics, as part of a $14 million donation from the Nemmers brothers. They envisioned creating an award that would be as prestigious as the Nobel prize. To this end, the majority of the income earned from the endowment is returned to the principal in order to increase the size of the award. Even so, it is still \"believed to be the largest monetary awards in the United States designated specifically for academic excellence in mathematics.\n\nCurrently, the award carries a $200,000 stipend and the scholar spends several weeks in residence at Northwestern University.\n\nFollowing recipients received this award:\n\n"}
{"id": "41546610", "url": "https://en.wikipedia.org/wiki?curid=41546610", "title": "Nets within Nets", "text": "Nets within Nets\n\nNets within Nets is a modelling method belonging to the family of Petri nets.\nThis method is distinguished from other sorts of Petri nets by the possibility to provide their tokens with a proper structure, which is based on Petri net modelling again. Hence, a net can contain further net items, being able to move around and fire themselves.\n\nNets within nets are well suited for the modelling of distributed systems under the particular aspects of\n\nIn a lot of publications in relation to object-oriented design is given, in order to combine the ability of Petri nets in modelling distributed computing with the modelling of objects, being able to be created and to interact.\n\nStarting from the need of practical applications, by the mid of nineties, different formalisms have been created, which fit the description of „nets within nets“. Lomazova and Schnoebelen are listing\nsome of these approaches, namely by Sibertin-Blanc, Lakos, Moldt und Wienberg as extending Coloured Petri nets, aside the Object Nets of Valk.\nThe earliest use of such hierarchic net models appeared by Rüdiger Valk in Valk and Jessen, where the so-called task-flow nets are introduced in order to model task systems in operating systems. In these models tasks are modelled by a Petri net, which represents the precedences of tasks and their state of execution.\n\nThe most important differences in semantics is given by the execution of net tokens. On the one side net tokens can be references to net items, which case is called „reference semantics“. This kind of semantic is distinguished from value semantics, where net objects may exist in different places and different internal states. In value semantics different copies can be created to model concurrent execution. The corresponding join of such a split can be defined in different ways, as for instance by „distributed token semantics“ or „history process semantics“. In connection with mobile computing hybrid versions of reference and value semantics are of importance. In distributed token semantics the important calculus of place invariants for Petri nets remains valid.\n\nThe formalism of nets within nets would be of few importance without communication between net tokens. Like in object-oriented programming communication of net tokens is introduced via predefined interfaces which are dynamically bound.\n\nIn Figure 1 a Petri net is shown containing a token Petri net in place „a“. The token net can move around from place „a“ to place „b“ and back by firing of the transitions of the outer net. The channel inscriptions at the transitions behave like a call of a method, resulting in the synchronised firing of the calling transition in the outer net [e.g. labelled by x:forth()] and the called transition [e.g. labelled by :forth()] in the token net. The variable „x“ at an arrow is bound to the token net in the place connected with this arrow. The brackets may contain parameters to be passed. This example is so simple that reference and value semantics coincide.\n\nStandard Petri net properties like reachability, boundedness and liveness show a mixed picture. A paper of Köhler-Bußmeier gives a survey on decidability results for elementary object systems.\nTo reduce the complexity of the formalism subclasses have been defined by restricting the structure of the Petri nets, as for instance to state machines. Such restrictions still allow complex modelling of distributed and mobile systems, but have polynomial complexity in model checking.\n\n"}
{"id": "55155568", "url": "https://en.wikipedia.org/wiki?curid=55155568", "title": "Ogive (statistics)", "text": "Ogive (statistics)\n\nIn statistics, an ogive is a free-hand graph showing the curve of a cumulative distribution function. The points plotted are the upper class limit and the corresponding cumulative frequency. (which, for the normal distribution, resembles one side of an Arabesque or ogival arch). The term can also be used to refer to the empirical cumulative distribution function.\n"}
{"id": "1059850", "url": "https://en.wikipedia.org/wiki?curid=1059850", "title": "Paul Gordan", "text": "Paul Gordan\n\nPaul Albert Gordan (27 April 1837 – 21 December 1912) was a Jewish-German mathematician, a student of Carl Jacobi at the University of Königsberg before obtaining his Ph.D. at the University of Breslau (1862), and a professor at the University of Erlangen-Nuremberg.\n\nHe was born in Breslau, Germany (now Wrocław, Poland), and died in Erlangen, Germany.\n\nHe was known as \"the king of invariant theory\". His most famous result is that the ring of invariants of binary forms of fixed degree is finitely generated. He and Alfred Clebsch gave their name to Clebsch–Gordan coefficients. Gordan also served as the thesis advisor for Emmy Noether.\n\nA famous quote attributed to Gordan about David Hilbert's proof of Hilbert's basis theorem, a result which vastly generalized his result on invariants, is \"This is not mathematics; this is theology.\" The proof in question was the (non-constructive) existence of a finite basis for invariants. It is not clear if Gordan really said this since the earliest reference to it is 25 years after the events and after his death, and nor is it clear whether the quote was intended as criticism, or praise, or a subtle joke. Gordan himself encouraged Hilbert and used Hilbert's results and methods, and the widespread story that he opposed Hilbert's work on invariant theory is a myth (though he did correctly point out in a referee's report that some of the reasoning in Hilbert's paper was incomplete).\n\n\n"}
{"id": "53264857", "url": "https://en.wikipedia.org/wiki?curid=53264857", "title": "Probability management", "text": "Probability management\n\nThe discipline of probability management communicates and calculates uncertainties as vector arrays of simulated or historical realizations and meta data called Stochastic Information Packets (SIPs). A set of SIPs, which preserve statistical relationships between variables is said to be coherent and is referred to as a Stochastic Library Unit with Relationships Preserved (SLURP). SIPs and SLURPs allow Stochastic Simulations to Communicate with each other. See for example, Analytica (Wikipedia), Analytica (SIP page), Oracle Crystal Ball, Frontline Solvers, and Autobox.\n\nThe first large documented application of SIPs involved the exploration portfolio of Royal Dutch Shell in 2005 as reported by Savage, Scholtes, and Zweidler, who formalized the discipline of probability management in 2006. The topic is also explored at length in.\n\nVectors of simulated realizations of probability distributions have been used to drive stochastic optimization since at least 1991. Andrew Gelman has described such arrays of realizations as Random Variable Objects in 2007.\n\nIn 2013 ProbabilityManagement.org was incorporated as a 501(c)(3) non-profit that supports this approach through education, tools, and open standards. The Executive Director, Sam Savage, is author of \"The Flaw of Averages: Why we Underestimate Risk in the Face of Uncertainty\" and Adjunct Professor at Stanford University. He is joined on the board by Harry Markowitz, Nobel laureate in Economics. The nonprofit has received financial support from Chevron Corporation, General Electric, Lockheed Martin, PG&E and Wells Fargo Bank. The open SIPmath 2.0 Standard supports XLSX, CSV and XML Formats\n"}
{"id": "411957", "url": "https://en.wikipedia.org/wiki?curid=411957", "title": "Random Fibonacci sequence", "text": "Random Fibonacci sequence\n\nIn mathematics, the random Fibonacci sequence is a stochastic analogue of the Fibonacci sequence defined by the recurrence relation \"f\" = \"f\" ± \"f\", where the signs + or − are chosen at random with equal probability 1/2, independently for different \"n\". By a theorem of Harry Kesten and Hillel Furstenberg, random recurrent sequences of this kind grow at a certain exponential rate, but it is difficult to compute the rate explicitly. In 1999, Divakar Viswanath showed that the growth rate of the random Fibonacci sequence is equal to 1.1319882487943…, a mathematical constant that was later named Viswanath's constant.\n\nThe random Fibonacci sequence is an integer random sequence {\"f\"}, where \"f\" = \"f\" = 1 and the subsequent terms are determined from the random recurrence relation\n\nA run of the random Fibonacci sequence starts with 1,1 and the value of the each subsequent term is determined by a fair coin toss: given two consecutive elements of the sequence, the next element is either their sum or their difference with probability 1/2, independently of all the choices made previously. If in the random Fibonacci sequence the plus sign is chosen at each step, the corresponding run is the Fibonacci sequence {\"F\"},\n\nIf the signs alternate in minus-plus-plus-minus-plus-plus-... pattern, the result is the sequence\n\nHowever, such patterns occur with vanishing probability in a random experiment. In a typical run, the terms will not follow a predictable pattern:\n\nSimilarly to the deterministic case, the random Fibonacci sequence may be profitably described via matrices:\n\nwhere the signs are chosen independently for different \"n\" with equal probabilities for + or −. Thus\n\nwhere {\"M\"} is a sequence of independent identically distributed random matrices taking values \"A\" or \"B\" with probability 1/2:\n\nJohannes Kepler discovered that as \"n\" increases, the ratio of the successive terms of the Fibonacci sequence {\"F\"} approaches the golden ratio formula_8 which is approximately 1.61803. In 1765, Leonhard Euler published an explicit formula, known today as the Binet formula,\n\nIt demonstrates that the Fibonacci numbers grow at an exponential rate equal to the golden ratio \"φ\".\n\nIn 1960, Hillel Furstenberg and Harry Kesten showed that for a general class of random matrix products, the norm grows as \"λ\", where \"n\" is the number of factors. Their results apply to a broad class of random sequence generating processes that includes the random Fibonacci sequence. As a consequence, the \"n\"th root of |\"f\"| converges to a constant value \"almost surely\", or with probability one:\n\nAn explicit expression for this constant was found by Divakar Viswanath in 1999. It uses Furstenberg's formula for the Lyapunov exponent of a random matrix product and integration over a certain fractal measure on the Stern–Brocot tree. Moreover, Viswanath computed the numerical value above using floating point arithmetics validated by an analysis of the rounding error.\n\nThe Embree–Trefethen constant describes the qualitative behavior of the random sequence with the recurrence relation\n\nfor different values of β.\n\n"}
{"id": "19727024", "url": "https://en.wikipedia.org/wiki?curid=19727024", "title": "Rational number", "text": "Rational number\n\nIn mathematics, a rational number is any number that can be expressed as the quotient or fraction of two integers, a numerator and a non-zero denominator . Since may be equal to 1, every integer is a rational number. The set of all rational numbers, often referred to as \"the rationals\", the field of rationals or the field of rational numbers is usually denoted by a boldface (or blackboard bold formula_1, Unicode ℚ); it was thus denoted in 1895 by Giuseppe Peano after \"quoziente\", Italian for \"quotient\".\n\nThe decimal expansion of a rational number always either terminates after a finite number of digits or begins to repeat the same finite sequence of digits over and over. Moreover, any repeating or terminating decimal represents a rational number. These statements hold true not just for base 10, but also for any other integer base (e.g. binary, hexadecimal).\n\nA real number that is not rational is called irrational. Irrational numbers include , , , and . The decimal expansion of an irrational number continues without repeating. Since the set of rational numbers is countable, and the set of real numbers is uncountable, almost all real numbers are irrational.\n\nRational numbers can be formally defined as equivalence classes of pairs of integers such that , for the equivalence relation defined by if, and only if . With this formal definition, the fraction becomes the standard notation for the equivalence class of .\n\nRational numbers together with addition and multiplication form a field which contains the integers and is contained in any field containing the integers. In other words, the field of rational numbers is a prime field, and a field has characteristic zero if and only if it contains the rational numbers as a subfield. Finite extensions of are called algebraic number fields, and the algebraic closure of is the field of algebraic numbers.\n\nIn mathematical analysis, the rational numbers form a dense subset of the real numbers. The real numbers can be constructed from the rational numbers by completion, using Cauchy sequences, Dedekind cuts, or infinite decimals.\n\nThe term \"rational\" in reference to the set Q refers to the fact that a rational number represents a \"ratio\" of two integers. In mathematics, \"rational\" is often used as a noun abbreviating \"rational number\". The adjective \"rational\" sometimes means that the coefficients are rational numbers. For example, a rational point is a point with rational coordinates (that is a point whose coordinates are rational numbers; a \"rational matrix\" is a matrix of rational numbers; a \"rational polynomial\" may be a polynomial with rational coefficients, although the term \"polynomial over the rationals\" is generally preferred, for avoiding confusion with \"rational expression\" and \"rational function\" (a polynomial is a rational expression and defines a rational function, even if its coefficients are not rational numbers). However, a rational curve \"is not\" a curve defined over the rationals, but a curve which can be parameterized by rational functions.\n\nEvery rational number may be expressed in a unique way as an irreducible fraction , where and are coprime integers, and . This is often called the canonical form.\n\nStarting from a rational number , its canonical form may be obtained by dividing and by their greatest common divisor, and, if , changing the sign of the resulting numerator and denominator.\n\nAny integer can be expressed as the rational number , which is its canonical form as a rational number.\n\nIf both fractions are in canonical form then \nIf both denominators are positive, and, in particular, if both fractions are in canonical form,\n\nIf either denominator is negative, each fraction with a negative denominator must first be converted into an equivalent form with a positive denominator by changing the signs of both its numerator and denominator.\n\nTwo fractions are added as follows:\n\nIf both fractions are in canonical form, the result is in canonical form if and only if and are coprime integers.\n\nIf both fractions are in canonical form, the result is in canonical form if and only if and are coprime integers.\n\nThe rule for multiplication is:\n\nEven if both fractions are in canonical form, the result may be a reducible fraction.\n\nEvery rational number has an additive inverse, often called its \"opposite\",\nIf is in canonical form, the same is true for its opposite.\n\nA nonzero rational number has a multiplicative inverse, also called its \"reciprocal\",\nIf is in canonical form, then the canonical form of its reciprocal is either or , depending on the sign of .\n\nIf both and are nonzero, the division rule is \n\nThus, dividing by is equivalent to multiplying by the reciprocal of :\n\nIf is a non-negative integer, then\nThe result is in canonical form if the same is true for .\nIn particular, \n\nIf , then\nIf is in canonical form, the canonical form of the result is formula_19 if either or is even. Otherwise, the canonical form of the result is formula_20\n\nA finite continued fraction is an expression such as\nwhere \"a\" are integers. Every rational number \"a\"/\"b\" can be represented as a finite continued fraction, whose coefficients \"a\" can be determined by applying the Euclidean algorithm to (\"a\",\"b\").\n\n\nare different ways to represent the same rational value.\n\nThe rational numbers may be built as equivalence classes of ordered pairs of integers. \n\nMore precisely, let be the set of the pairs of integers such . An equivalence relation is defined on this set by \n\nAddition and multiplication can be defined by the following rules:\n\nThis equivalence relation is a congruence relation, which means that it is compatible with the addition and multiplication defined above; the set of rational numbers is the defined as the quotient set by this equivalence relation, , equipped with the addition and the multiplication induced by the above operations. (This construction can be carried out with any integral domain and produces its field of fractions.) \n\nThe equivalence class of a pair is denoted formula_31 \nTwo pairs and belong to the same equivalence class (that is are equivalent) if and only if formula_32 this means that formula_33 if and only formula_32\n\nEvery equivalence class formula_35 may be represented by infinitely many pairs, since\nIt is often convenient to chose, once for all, in each equivalence class a specific element called the \"canonical representative element\". This canonical representative is the unique pair in the equivalence class such that and are coprime, and . It is called the representation in lowest terms of the rational number.\n\nThe integers may be considered to be rational numbers identifying the integer with the rational number formula_37\n\nA total order may be defined on the rational numbers, that extends the natural order of the integers. One has formula_38 if\n\nThe set Q, together with the addition and multiplication operations shown above, forms a field, the field of fractions of the integers Z.\n\nThe rationals are the smallest field with characteristic zero: every other field of characteristic zero contains a copy of Q. The rational numbers are therefore the prime field for characteristic zero.\n\nThe algebraic closure of Q, i.e. the field of roots of rational polynomials, is the algebraic numbers.\n\nThe set of all rational numbers is countable. Since the set of all real numbers is uncountable, we say that almost all real numbers are irrational, in the sense of Lebesgue measure, i.e. the set of rational numbers is a null set.\n\nThe rationals are a densely ordered set: between any two rationals, there sits another one, and, therefore, infinitely many other ones. For example, for any two fractions such that \n(where formula_41 are positive), we have\n\nAny totally ordered set which is countable, dense (in the above sense), and has no least or greatest element is order isomorphic to the rational numbers.\n\nThe rationals are a dense subset of the real numbers: every real number has rational numbers arbitrarily close to it. A related property is that rational numbers are the only numbers with finite expansions as regular continued fractions.\n\nBy virtue of their order, the rationals carry an order topology. The rational numbers, as a subspace of the real numbers, also carry a subspace topology. The rational numbers form a metric space by using the absolute difference metric and this yields a third topology on Q. All three topologies coincide and turn the rationals into a topological field. The rational numbers are an important example of a space which is not locally compact. The rationals are characterized topologically as the unique countable metrizable space without isolated points. The space is also totally disconnected. The rational numbers do not form a complete metric space; the real numbers are the completion of Q under the metric above.\n\nIn addition to the absolute value metric mentioned above, there are other metrics which turn Q into a topological field:\n\nLet \"p\" be a prime number and for any non-zero integer \"a\", let , where \"p\" is the highest power of \"p\" dividing \"a\".\n\nIn addition set For any rational number \"a\"/\"b\", we set \n\nThen defines a metric on Q.\n\nThe metric space (Q,\"d\") is not complete, and its completion is the \"p\"-adic number field Q. Ostrowski's theorem states that any non-trivial absolute value on the rational numbers Q is equivalent to either the usual real absolute value or a \"p\"-adic absolute value.\n\n\n"}
{"id": "1190979", "url": "https://en.wikipedia.org/wiki?curid=1190979", "title": "Schnorr group", "text": "Schnorr group\n\nA Schnorr group, proposed by Claus P. Schnorr, is a large prime-order subgroup of formula_1, the multiplicative group of integers modulo formula_2 for some prime formula_2. To generate such a group, generate formula_2, formula_5, formula_6 such that\n\nwith formula_2, formula_5 prime. Then choose any formula_10 in the range formula_11 until you find one such that\n\nThis value\n\nis a generator of a subgroup of formula_1 of order formula_5.\n\nSchnorr groups are useful in discrete log based cryptosystems including Schnorr signatures and DSA. In such applications, typically formula_2 is chosen to be large enough to resist index calculus and related methods of solving the discrete-log problem (perhaps 1024 to 3072 bits), while formula_5 is large enough to resist the birthday attack on discrete log problems, which works in any group (perhaps 160 to 256 bits). Because the Schnorr group is of prime order, it has no non-trivial proper subgroups, thwarting confinement attacks due to small subgroups. Implementations of protocols that use Schnorr groups must verify where appropriate that integers supplied by other parties are in fact members of the Schnorr group; formula_18 is a member of the group if formula_19 and formula_20. Any member of the group except the element formula_21 is also a generator of the group.\n"}
{"id": "22755983", "url": "https://en.wikipedia.org/wiki?curid=22755983", "title": "Seked", "text": "Seked\n\nSeked (or seqed) is an ancient Egyptian term describing the inclination of the triangular faces of a right pyramid. The system was based on the Egyptian's length measure known as the \"royal cubit\". It was subdivided into seven \"palms\", each of which was sub-divided into four \"digits\". The inclination of measured slopes was therefore expressed as the number of horizontal palms and digits relative to each royal cubit rise.\n\nThe seked is proportional to the reciprocal of our modern measure of slope or gradient, and to the cotangent of the angle of elevation. Specifically, if \"s\" is the seked, \"m\" the slope (rise over run), and formula_1 the angle of elevation from horizontal, then:\n\nThe most famous example of a seked slope is of the Great Pyramid of Giza in Egypt built around 2,550 B.C. Based on modern surveys, the faces of this monument had a seked of 5 1/2, or 5 palms and 2 digits, in modern terms equivalent to a slope of 1.27, a gradient of 127%, and an elevation of 51.84° from the horizontal (in our 360 degree system). \n\nInformation on the use of the seked in the design of pyramids has been obtained from two mathematical papyri; the Rhind Mathematical Papyrus in the British Museum and the Moscow Mathematical Papyrus in the Museum of Fine Arts. Although there is no direct evidence of its application from the archaeology of the Old Kingdom, there are a number of examples from the two mathematical papyrii, which date to the Middle Kingdom that show the use of this system for defining the slopes of the sides of pyramids, based on their height and base dimensions. The most widely quoted example is perhaps problem 56 from the Rhind Mathematical Papyrus.\n\nThe most famous of all the pyramids of Egypt is the Great Pyramid of Giza built around 2,550 B.C.. Based on the surveys of this structure that have been carried out by Flinders Petrie and others, the slopes of the faces of this monument were a seked of 5 1/2, or 5 palms and 2 digits [see figure above] which equates to a slope of 51.84° from the horizontal, using the modern 360 degree system. This slope would probably have been accurately applied during construction by way of 'A frame' shaped wooden tools with plumb bobs, marked to the correct incline, so that slopes could be measured out and checked efficiently.\n\nFurthermore, according to Petrie's survey data in \"The Pyramids and Temples of Gizeh\" the mean slope of the Great Pyramid's entrance passage is 26° 31' 23\" ± 5\". This is less than 1/20th of one degree in deviation from an ideal slope of 1 in 2, which is 26° 33' 54\". This equates to a seked of 14, and is generally considered to have been the intentional designed slope applied by the Old Kingdom builders for internal passages.\n\nThe \"seked of a pyramid\" is described by Richard Gillings in his book 'Mathematics in the Time of the Pharaohs' as follows:\n\n\"The seked of a right pyramid is the inclination of any one of the four triangular faces to the horizontal plane of its base, and is measured as so many horizontal units per one vertical unit rise. It is thus a measure equivalent to our modern cotangent of the angle of slope. In general, the seked of a pyramid is a kind of fraction, given as so many palms horizontally for each cubit of vertically, where 7 palm equal one cubit. The Egyptian word 'seked' is thus related to our modern word 'gradient'.\"\n\nMany of the smaller pyramids in Egypt have varying slopes; however, like the Great Pyramid of Giza, the pyramid at Meidum is thought to have had sides that sloped by 51.842° or 51° 50' 35\", which is a seked of 5 palms.\n\nThe Great Pyramid scholar Professor I.E.S Edwards considered this to have been the 'normal' or most typical slope choice for pyramids. Flinders Petrie also noted the similarity of the slope of this pyramid to that of the Great Pyramid at Giza, and both Egyptologists considered it to have been a deliberate choice, based on a desire to ensure that the circuit of the base of the pyramids precisely equalled the length of a circle that would be swept out if the pyramid's height were used as a radius. Petrie wrote \"...these relations of areas and of circular ratio are so systematic that we should grant that they were in the builder's design\".\n\n\n\n"}
{"id": "18062589", "url": "https://en.wikipedia.org/wiki?curid=18062589", "title": "Sierpiński arrowhead curve", "text": "Sierpiński arrowhead curve\n\nThe Sierpiński arrowhead curve is a fractal curve similar in appearance and identical in limit to the Sierpiński triangle.\n\nThe Sierpiński arrowhead curve draws an equilateral triangle with triangular holes at equal intervals. It can be described with two substituting production rules: (A → B-A-B) and (B → A+B+A). A and B recur and at the bottom do the same thing — draw a line. Plus and minus (+ and -) mean turn 60 degrees either left or right. The terminating point of the Sierpiński arrowhead curve is always the same provided you recur an even number of times and you halve the length of the line at each recursion. If you recur to an odd depth (order is odd) then you end up turned 60 degrees, at a different point in the triangle.\n\nIn code, given these drawing functions:\nvoid draw_line( double distance);\nvoid turn( int angle_in_degrees);\nThe code to draw an (approximate) Sierpiński arrowhead curve looks like this.\nvoid sierpinski_arrowhead_curve( unsigned order, double length)\n\nvoid curve( unsigned order, double length, int angle)\nThe Sierpiński arrowhead curve can be expressed by a rewrite system (L-system).\n\nHere, \"F\" means “draw forward”, + means “turn left 60°”, and \"−\" means “turn right 60°” (see turtle graphics).\n\nLike many two-dimensional fractal curves, the Sierpiński arrowhead curve can be extended to three dimensions:\n\n\n"}
{"id": "4476828", "url": "https://en.wikipedia.org/wiki?curid=4476828", "title": "Smith conjecture", "text": "Smith conjecture\n\nIn mathematics, the Smith conjecture states that if \"f\" is a diffeomorphism of the 3-sphere of finite order, then the fixed point set of \"f\" cannot be a nontrivial knot.\n\n\n"}
{"id": "7257044", "url": "https://en.wikipedia.org/wiki?curid=7257044", "title": "Spectral flux", "text": "Spectral flux\n\nSpectral flux is a measure of how quickly the power spectrum of a signal is changing, calculated by comparing the power spectrum for one frame against the power spectrum from the previous frame. More precisely, it is usually calculated as the 2-norm (also known as the Euclidean distance) between the two normalised spectra.\n\nCalculated this way, the spectral flux is not dependent upon overall power (since the spectra are normalised), nor on phase considerations (since only the magnitudes are compared).\n\nThe spectral flux can be used to determine the timbre of an audio signal, or in onset detection, among other things.\n\nSome implementations use the 1-norm rather than the 2-norm (i.e. the sum rather than the Euclidean distance).\n\nSome implementations do not normalise the spectra.\n\nFor onset detection, increases in energy are important (not decreases), so some algorithms only include values calculated from bins in which the energy is increasing.\n"}
{"id": "1605807", "url": "https://en.wikipedia.org/wiki?curid=1605807", "title": "Stackelberg competition", "text": "Stackelberg competition\n\nThe Stackelberg leadership model is a strategic game in economics in which the leader firm moves first and then the follower firms move sequentially. It is named after the German economist Heinrich Freiherr von Stackelberg who published \"Market Structure and Equilibrium (Marktform und Gleichgewicht)\" in 1934 which described the model.\n\nIn game theory terms, the players of this game are a \"leader\" and a \"follower\" and they compete on quantity. The Stackelberg leader is sometimes referred to as the Market Leader.\n\nThere are some further constraints upon the sustaining of a Stackelberg equilibrium. The leader must know \"ex ante\" that the follower observes its action. The follower must have no means of committing to a future non-Stackelberg follower action and the leader must know this. Indeed, if the 'follower' could commit to a Stackelberg leader action and the 'leader' knew this, the leader's best response would be to play a Stackelberg follower action.\n\nFirms may engage in Stackelberg competition if one has some sort of advantage enabling it to move first. More generally, the leader must have commitment power. Moving observably first is the most obvious means of commitment: once the leader has made its move, it cannot undo it - it is committed to that action. Moving first may be possible if the leader was the incumbent monopoly of the industry and the follower is a new entrant. Holding excess capacity is another means of commitment.\n\nThe Stackelberg model can be solved to find the subgame perfect Nash equilibrium or equilibria (SPNE), i.e. the strategy profile that serves best each player, given the strategies of the other player and that entails every player playing in a Nash equilibrium in every subgame.\n\nIn very general terms, let the price function for the (duopoly) industry be formula_1; price is simply a function of total (industry) output, so is formula_2 where the subscript 1 represents the leader and 2 represents the follower. Suppose firm formula_3 has the cost structure formula_4. The model is solved by backward induction. The leader considers what the best response of the follower is, i.e. how it \"will\" respond once it has observed the quantity of the leader. The leader then picks a quantity that maximises its payoff, anticipating the predicted response of the follower. The follower actually observes this and in equilibrium picks the expected quantity as a response.\n\nTo calculate the SPNE, the best response functions of the follower must first be calculated (calculation moves 'backwards' because of backward induction).\n\nThe profit of firm 2 (the follower) is revenue minus cost. Revenue is the product of price and quantity and cost is given by the firm's cost structure, so profit is:\nformula_5. The best response is to find the value of formula_6 that maximises formula_7 given formula_8, i.e. given the output of the leader (firm 1), the output that maximises the follower's profit is found. Hence, the maximum of formula_7 with respect to formula_6 is to be found. First differentiate formula_7 with respect to formula_6:\n\nSetting this to zero for maximisation:\n\nThe values of formula_6 that satisfy this equation are the best responses. Now the best response function of the leader is considered. This function is calculated by considering the follower's output as a function of the leader's output, as just computed.\n\nThe profit of firm 1 (the leader) is formula_16, where formula_17 is the follower's quantity as a function of the leader's quantity, namely the function calculated above. The best response is to find the value of formula_8 that maximises formula_19 given formula_17, i.e. given the best response function of the follower (firm 2), the output that maximises the leader's profit is found. Hence, the maximum of formula_19 with respect to formula_8 is to be found. First, differentiate formula_19 with respect to formula_8:\n\nSetting this to zero for maximisation:\n\nThe following example is very general. It assumes a generalised linear demand structure\n\nand imposes some restrictions on cost structures for simplicity's sake so the problem can be resolved.\n\nfor ease of computation.\n\nThe follower's profit is:\n\nThe maximisation problem resolves to (from the general case):\n\nConsider the leader's problem:\n\nSubstituting for formula_17 from the follower's problem:\n\nThe maximisation problem resolves to (from the general case):\n\nNow solving for formula_8 yields formula_40, the leader's optimal action:\n\nThis is the leader's best response to the reaction of the follower in equilibrium. The follower's actual can now be found by feeding this into its reaction function calculated earlier:\n\nThe Nash equilibria are all formula_44. It is clear (if marginal costs are assumed to be zero - i.e. cost is essentially ignored) that the leader has a significant advantage. Intuitively, if the leader was no better off than the follower, it would simply adopt a Cournot competition strategy.\n\nPlugging the follower's quantity formula_45, back into the leader's best response function will not yield formula_46. This is because once leader has committed to an output and observed the followers it always wants to reduce its output ex-post. However its inability to do so is what allows it to receive higher profits than under cournot.\n\nAn extensive-form representation is often used to analyze the Stackelberg leader-follower model. Also referred to as a “decision tree”, the model shows the combination of outputs and payoffs both firms have in the Stackelberg game\n\nThe image on the left depicts in extensive form a Stackelberg game. The payoffs are shown on the right. This example is fairly simple. There is a basic cost structure involving only marginal cost (there is no fixed cost). The demand function is linear and price elasticity of demand is 1. However, it illustrates the leader's advantage.\n\nThe follower wants to choose formula_6 to maximise its payoff formula_48. Taking the first order derivative and equating it to zero (for maximisation) yields \nformula_49 as the maximum value of formula_6.\n\nThe leader wants to choose formula_8 to maximise its payoff formula_52. However, in equilibrium, it knows the follower will choose formula_6 as above. So in fact the leader wants to maximise its payoff formula_54 (by substituting formula_6 for the follower's best response function). By differentiation, the maximum payoff is given by formula_56. Feeding this into the follower's best response function yields formula_57. Suppose marginal costs were equal for the firms (so the leader has no market advantage other than first move) and in particular formula_58. The leader would produce 2000 and the follower would produce 1000. This would give the leader a profit (payoff) of two million and the follower a profit of one million. Simply by moving first, the leader has accrued twice the profit of the follower. However, Cournot profits here are 1.78 million apiece (strictly, formula_59 apiece), so the leader has not gained much, but the follower has lost. However, this is example-specific. There may be cases where a Stackelberg leader has huge gains beyond Cournot profit that approach monopoly profits (for example, if the leader also had a large cost structure advantage, perhaps due to a better production function). There may also be cases where the follower actually enjoys higher profits than the leader, but only because it, say, has much lower costs. This behaviour consistently work on duopoly markets even if the firms are asymmetrical.\n\nIf, after the leader had selected its equilibrium quantity, the follower deviated from the equilibrium and chose some non-optimal quantity it would not only hurt itself, but it could also hurt the leader. If the follower chose a much larger quantity than its best response, the market price would lower and the leader's profits would be stung, perhaps below Cournot level profits. In this case, the follower could announce to the leader before the game starts that unless the leader chooses a Cournot equilibrium quantity, the follower will choose a deviant quantity that will hit the leader's profits. After all, the quantity chosen by the leader in equilibrium is only optimal if the follower also plays in equilibrium. The leader is, however, in no danger. Once the leader has chosen its equilibrium quantity, it would be irrational for the follower to deviate because it too would be hurt. Once the leader has chosen, the follower is better off by playing on the equilibrium path. Hence, such a threat by the follower would not be credible.\n\nHowever, in an (indefinitely) repeated Stackelberg game, the follower might adopt a punishment strategy where it threatens to punish the leader in the next period unless it chooses a non-optimal strategy in the current period. This threat may be credible because it could be rational for the follower to punish in the next period so that the leader chooses Cournot quantities thereafter.\n\nThe Stackelberg and Cournot models are similar because in both competition is on quantity. However, as seen, the first move gives the leader in Stackelberg a crucial advantage. There is also the important assumption of perfect information in the Stackelberg game: the follower must observe the quantity chosen by the leader, otherwise the game reduces to Cournot. With imperfect information, the threats described above can be credible. If the follower cannot observe the leader's move, it is no longer irrational for the follower to choose, say, a Cournot level of quantity (in fact, that is the equilibrium action). However, it must be that there \"is\" imperfect information and the follower \"is\" unable to observe the leader's move because it is irrational for the follower not to observe if it can once the leader has moved. If it can observe, it will so that it can make the optimal decision. Any threat by the follower claiming that it will not observe even if it can is as uncredible as those above. This is an example of too much information hurting a player. In Cournot competition, it is the simultaneity of the game (the imperfection of knowledge) that results in neither player (\"ceteris paribus\") being at a disadvantage.\n\nAs mentioned, imperfect information in a leadership game reduces to Cournot competition. However, some Cournot strategy profiles are sustained as Nash equilibria but can be eliminated as incredible threats (as described above) by applying the solution concept of subgame perfection. Indeed, it is the very thing that makes a Cournot strategy profile a Nash equilibrium in a Stackelberg game that prevents it from being subgame perfect.\n\nConsider a Stackelberg game (i.e. one which fulfills the requirements described above for sustaining a Stackelberg equilibrium) in which, for some reason, the leader believes that whatever action it takes, the follower will choose a Cournot quantity (perhaps the leader believes that the follower is irrational). If the leader played a Stackelberg action, (it believes) that the follower will play Cournot. Hence it is non-optimal for the leader to play Stackelberg. In fact, its best response (by the definition of Cournot equilibrium) is to play Cournot quantity. Once it has done this, the best response of the follower is to play Cournot.\n\nConsider the following strategy profiles: the leader plays Cournot; the follower plays Cournot if the leader plays Cournot and the follower plays non-Stackelberg if the leader plays Stackelberg and if the leader plays something else, the follower plays an arbitrary strategy (hence this actually describes several profiles). This profile is a Nash equilibrium. As argued above, on the equilibrium path play is a best response to a best response. However, playing Cournot would not have been the best response of the leader were it that the follower would play Stackelberg if it (the leader) played Stackelberg. In this case, the best response of the leader would be to play Stackelberg. Hence, what makes this profile (or rather, these profiles) a Nash equilibrium (or rather, Nash equilibria) is the fact that the follower would play non-Stackelberg if the leader were to play Stackelberg.\n\nHowever, this very fact (that the follower would play non-Stackelberg if the leader were to play Stackelberg) means that this profile is not a Nash equilibrium of the subgame starting when the leader has already played Stackelberg (a subgame off the equilibrium path). If the leader has already played Stackelberg, the best response of the follower is to play Stackelberg (and therefore it is the only action that yields a Nash equilibrium in this subgame). Hence the strategy profile - which is Cournot - is not subgame perfect.\n\nIn comparison with other oligopoly models,\n\n\nThe Stackelberg concept has been extended to dynamic Stackelberg games. See Simaan and Cruz (1973a, 1973b). With the addition of time as a dimension, phenomena not found in static games were discovered, such as violation of the principle of optimality by the leader, Simaan and Cruz (1973b). For a survey of applications of Stackelberg differential games to supply chain and marketing channels, see He et al. (2007). In recent years, Stackelberg games have contributed a lot in the security domain where it is essential for the security personnel to protect some valuable resource and search for any potential threats to it. This is where it involves the security personnel (leader) to design his/her strategy first so that irrespective of the strategy adopted by the thief (follower), the resource remains safe.\n\n\n"}
{"id": "31980740", "url": "https://en.wikipedia.org/wiki?curid=31980740", "title": "Stanley symmetric function", "text": "Stanley symmetric function\n\nIn mathematics and especially in algebraic combinatorics, the Stanley symmetric functions are a family of symmetric polynomials introduced by in his study of the symmetric group of permutations. \n\nFormally, the Stanley symmetric function \"F\"(\"x\", \"x\", ...) indexed by a permutation \"w\" is defined as a sum of certain fundamental quasisymmetric functions. Each summand corresponds to a reduced decomposition of \"w\", that is, to a way of writing \"w\" as a product of a minimal possible number of adjacent transpositions. They were introduced in the course of Stanley's enumeration of the reduced decompositions of permutations, and in particular his proof that the permutation \"w\" = \"n\"(\"n\" − 1)...21 (written here in one-line notation) has exactly\nreduced decompositions. (Here formula_2 denotes the binomial coefficient \"n\"(\"n\" − 1)/2 and ! denotes the factorial.)\n\nThe Stanley symmetric function \"F\" is homogeneous with degree equal to the number of inversions of \"w\". Unlike other nice families of symmetric functions, the Stanley symmetric functions have many linear dependencies and so do not form a basis of the ring of symmetric functions. When a Stanley symmetric function is expanded in the basis of Schur functions, the coefficients are all non-negative integers.\n\nThe Stanley symmetric functions have the property that they are the stable limit of Schubert polynomials\nwhere we treat both sides as formal power series, and take the limit coefficientwise.\n"}
{"id": "41828", "url": "https://en.wikipedia.org/wiki?curid=41828", "title": "Two-out-of-five code", "text": "Two-out-of-five code\n\nIn telecommunication, a two-out-of-five code is an m of n code that provides exactly ten possible combinations, and thus is popular for representing decimal digits using five bits. There are ways to assign weights to each bit such that the set bits sum to the desired value, with an exception for zero.\n\nAccording to Federal Standard 1037C:\n\nThe weights give a unique encoding for most digits, but allow two encodings for 3: 0+3 or 10010 and 1+2 or 01100. The former is used to encode the digit 3, and the latter is used to represent the otherwise unrepresentable zero.\n\nThe IBM 7070, IBM 7072, and IBM 7074 computers used this code to represent each of the ten decimal digits in a machine word, although they numbered the bit positions 0-1-2-3-4, rather than with weights. Each word also had a sign flag, encoded using a two-out-of-three code, that could be A Alphanumeric, − Minus, or + Plus. When copied to a digit, the three bits were placed in bit positions 0-3-4. (Thus producing the numeric values 3, 6 and 9, respectively.)\n\nA variant is the U.S. Post Office POSTNET barcode, used to represent the ZIP+4 code for automated mail sorting and routing equipment. This uses two tall bars as \"ones\" and three short bars as \"zeros\". Here, the weights assigned to the bit positions are 7-4-2-1-0. Again, zero is encoded specially, using the 7+4 combination (binary 11000) that would naturally encode 11. This method was also used in North American telephone Multi-frequency and crossbar switching systems.\n\nThe USPS Postal Alpha Numeric Encoding Technique (PLANET) uses the same weights, but with the opposite bar-height convention.\n\nThe following table represents decimal digits from 0 to 9 in various two-out-of-five code systems:\n\nThe limit on the number of bits set is similar to, but strictly stronger than, a parity check. All constant-weight codes, including the two-out-of-five code, can not only detect any single-bit error, but also detect any unidirectional errors -- any case where all errors in a codeword are of a single type (0→1 or 1→0).\n"}
{"id": "35658939", "url": "https://en.wikipedia.org/wiki?curid=35658939", "title": "Verification and validation of computer simulation models", "text": "Verification and validation of computer simulation models\n\nVerification and validation of computer simulation models is conducted during the development of a simulation model with the ultimate goal of producing an accurate and credible model. \"Simulation models are increasingly being used to solve problems and to aid in decision-making. The developers and users of these models, the decision makers using information obtained from the results of these models, and the individuals affected by decisions based on such models are all rightly concerned with whether a model and its results are \"correct\". This concern is addressed through verification and validation of the simulation model.\n\nSimulation models are approximate imitations of real-world systems and they never exactly imitate the real-world system. Due to that, a model should be verified and validated to the degree needed for the models intended purpose or application.\n\nThe verification and validation of simulation model starts after functional specifications have been documented and initial model development has been completed. Verification and validation is an iterative process that takes place throughout the development of a model.\n\nIn the context of computer simulation, verification of a model is the process of confirming that it is correctly implemented with respect to the conceptual model (it matches specifications and assumptions deemed acceptable for the given purpose of application).\nDuring verification the model is tested to find and fix errors in the implementation of the model.\nVarious processes and techniques are used to assure the model matches specifications and assumptions with respect to the model concept. \nThe objective of model verification is to ensure that the implementation of the model is correct.\n\nThere are many techniques that can be utilized to verify a model.\nIncluding, but not limited to, have the model checked by an expert, making logic flow diagrams that include each logically possible action, examining the model output for reasonableness under a variety of settings of the input parameters, and using an interactive debugger.\nMany software engineering techniques used for software verification are applicable to simulation model verification.\n\nValidation checks the accuracy of the model's representation of the real system. Model validation is defined to mean \"substantiation that a computerized model within its domain of applicability possesses a satisfactory range of accuracy consistent with the intended application of the model\". A model should be built for a specific purpose or set of objectives and its validity determined for that purpose.\n\nThere are many approaches that can be used to validate a computer model. The approaches range from subjective reviews to objective statistical tests. One approach that is commonly used is to have the model builders determine validity of the model through a series of tests.\n\nNaylor and Finger [1967] formulated a three-step approach to model validation that has been widely followed:\n\nStep 1. Build a model that has high face validity.\n\nStep 2. Validate model assumptions.\n\nStep 3. Compare the model input-output transformations to corresponding input-output transformations for the real system.\n\nA model that has face validity appears to be a reasonable imitation of a real-world system to people who are knowledgeable of the real world system. Face validity is tested by having users and people knowledgeable with the system examine model output for reasonableness and in the process identify deficiencies. An added advantage of having the users involved in validation is that the model's credibility to the users and the user's confidence in the model increases. Sensitivity to model inputs can also be used to judge face validity. For example, if a simulation of a fast food restaurant drive through was run twice with customer arrival rates of 20 per hour and 40 per hour then model outputs such as average wait time or maximum number of customers waiting would be expected to increase with the arrival rate.\n\nAssumptions made about a model generally fall into two categories: structural assumptions about how system works and data assumptions.\n\nAssumptions made about how the system operates and how it is physically arranged are structural assumptions. For example, the number of servers in a fast food drive through lane and if there is more than one how are they utilized? Do the servers work in parallel where a customer completes a transaction by visiting a single server or does one server take orders and handle payment while the other prepares and serves the order. Many structural problems in the model come from poor or incorrect assumptions. If possible the workings of the actual system should be closely observed to understand how it operates. The systems structure and operation should also be verified with users of the actual system.\n\nThere must be a sufficient amount of appropriate data available to build a conceptual model and validate a model. Lack of appropriate data is often the reason attempts to validate a model fail. Data should be verified to come from a reliable source. A typical error is assuming an inappropriate statistical distribution for the data. The assumed statistical model should be tested using goodness of fit tests and other techniques. Examples of goodness of fit tests are the Kolmogorov–Smirnov test and the chi-square test. Any outliers in the data should be checked.\n\nThe model is viewed as an input-output transformation for these tests. The validation test consists of comparing outputs from the system under consideration to model outputs for the same set of input conditions. Data recorded while observing the system must be available in order to perform this test. The model output that is of primary interest should be used as the measure of performance. For example, if system under consideration is a fast food drive through where input to model is customer arrival time and the output measure of performance is average customer time in line, then the actual arrival time and time spent in line for customers at the drive through would be recorded. The model would be run with the actual arrival times and the model average time in line would be compared with the actual average time spent in line using one or more tests.\n\nStatistical hypothesis testing using the t-test can be used as a basis to accept the model as valid or reject it as invalid.\n\nThe hypothesis to be tested is\nversus\n\nThe test is conducted for a given sample size and level of significance or α. To perform the test a number \"n\" statistically independent runs of the model are conducted and an average or expected value, E(Y), for the variable of interest is produced. Then the test statistic, \"t\" is computed for the given α, \"n\", E(Y) and the observed value for the system μ\n\nIf \nreject H, the model needs adjustment.\n\nThere are two types of error that can occur using hypothesis testing, rejecting a valid model called type I error or \"model builders risk\" and accepting an invalid model called Type II error, β, or \"model user's risk\". The level of significance or α is equal the probability of type I error. If α is small then rejecting the null hypothesis is a strong conclusion. For example, if α = 0.05 and the null hypothesis is rejected there is only a 0.05 probability of rejecting a model that is valid. Decreasing the probability of a type II error is very important. The probability of correctly detecting an invalid model is 1 - β. The probability of a type II error is dependent of the sample size and the actual difference between the sample value and the observed value. Increasing the sample size decreases the risk of a type II error.\n\nA statistical technique where the amount of model accuracy is specified as a range has recently been developed. The technique uses hypothesis testing to accept a model if the difference between a model's variable of interest and a system's variable of interest is within a specified range of accuracy. A requirement is that both the system data and model data be approximately Normally Independent and Identically Distributed (NIID). The t-test statistic is used in this technique. If the mean of the model is μ and the mean of system is μ then the difference between the model and the system is D = μ - μ. The hypothesis to be tested is if D is within the acceptable range of accuracy. Let L = the lower limit for accuracy and U = upper limit for accuracy. Then\n\nversus\n\nis to be tested.\n\nThe operating characteristic (OC) curve is the probability that the null hypothesis is accepted when it is true. The OC curve characterizes the probabilities of both type I and II errors. Risk curves for model builder's risk and model user's can be developed from the OC curves. Comparing curves with fixed sample size tradeoffs between model builder's risk and model user's risk can be seen easily in the risk curves. If model builder's risk, model user's risk, and the upper and lower limits for the range of accuracy are all specified then the sample size needed can be calculated.\n\nConfidence intervals can be used to evaluate if a model is \"close enough\" to a system for some variable of interest. The difference between the known model value, μ, and the system value, μ, is checked to see if it is less than a value small enough that the model is valid with respect that variable of interest. The value is denoted by the symbol ε. To perform the test a number, \"n\", statistically independent runs of the model are conducted and a mean or expected value, E(Y) or μ for simulation output variable of interest Y, with a standard deviation \"S\" is produced. A confidence level is selected, 100(1-α). An interval, [a,b], is constructed by\n\nwhere \nis the critical value from the t-distribution for the given level of significance and n-1 degrees of freedom.\n\nIf statistical assumptions cannot be satisfied or there is insufficient data for the system a graphical comparisons of model outputs to system outputs can be used to make a subjective decisions, however other objective tests are preferable.\n\nDocuments and standards involving verification and validation of computational modeling and simulation are developed by the American Society of Mechanical Engineers (ASME) Verification and Validation (V&V) Committee. ASME V&V 10 provides guidance in assessing and increasing the credibility of computational solid mechanics models through the processes of verification, validation, and uncertainty quantification. ASME V&V 10.1 provides a detailed example to illustrate the concepts described in ASME V&V 10. ASME V&V 20 provides a detailed methodology for validating computational simulations as applied to fluid dynamics and heat transfer. Soon to be published in 2018, ASME V&V 40 provides a framework for establishing model credibility requirements for computational modeling, and presents examples specific in the medical device industry. \n\n"}
{"id": "10460511", "url": "https://en.wikipedia.org/wiki?curid=10460511", "title": "Viviani's theorem", "text": "Viviani's theorem\n\nViviani's theorem, named after Vincenzo Viviani, states that the sum of the distances from \"any\" interior point to the sides of an equilateral triangle equals the length of the triangle's altitude.\n\nThis proof depends on the readily-proved proposition that the area of a triangle is half its base times its height—that is, half the product of one side with the altitude from that side.\n\nLet ABC be an equilateral triangle whose height is \"h\" and whose side is \"a\".\n\nLet P be any point inside the triangle, and \"u, s, t\" the distances of P from the sides. Draw a line from P to each of A, B, and C, forming three triangles PAB, PBC, and PCA.\n\nNow, the areas of these triangles are formula_1, formula_2, and formula_3. They exactly fill the enclosing triangle, so the sum of these areas is equal to the area of the enclosing triangle.\nSo we can write:\n\nand thus\n\nQ.E.D.\n\nThe converse also holds: If the sum of the distances from an interior point of a triangle to the sides is independent of the location of the point, the triangle is equilateral.\n\nViviani's theorem means that lines parallel to the sides of an equilateral triangle give coordinates for making ternary plots, such as flammability diagrams.\n\nMore generally, they allow one to give coordinates on a regular simplex in the same way.\n\nThe sum of the distances from any interior point of a parallelogram to the sides is independent of the location of the point. The converse also holds: If the sum of the distances from a point in the interior of a quadrilateral to the sides is independent of the location of the point, then the quadrilateral is a parallelogram.\n\nThe result generalizes to any 2\"n\"-gon with opposite sides parallel. Since the sum of distances between any pair of opposite parallel sides is constant, it follows that the sum of all pairwise sums between the pairs of parallel sides, is also constant. The converse in general is not true, as the result holds for an \"equilateral\" hexagon, which does not necessarily have opposite sides parallel.\n\nIf a polygon is regular (both equiangular and equilateral), the sum of the distances to the sides from an interior point is independent of the location of the point. Specifically, it equals \"n\" times the apothem, where \"n\" is the number of sides and the apothem is the distance from the center to a side. However, the converse does not hold; the non-square parallelogram is a counterexample.\n\nThe sum of the distances from an interior point to the sides of an equiangular polygon does not depend on the location of the point.\n\nA necessary and sufficient condition for a convex polygon to have a constant sum of distances from any interior point to the sides is that there exist three non-collinear interior points with equal sums of distances.\n\nThe sum of the distances from any point in the interior of a regular polyhedron to the sides is independent of the location of the point. However, the converse does not hold, not even for tetrahedra.\n\n\n"}
{"id": "56058699", "url": "https://en.wikipedia.org/wiki?curid=56058699", "title": "William of Soissons", "text": "William of Soissons\n\nWilliam of Soissons was a French logician who lived in Paris in the 12th century. He belonged to a school of logicians, called the Parvipontians.\n\nWilliam of Soissons seems to have been the first one to answer the question, \"Why is a contradiction not accepted in logic reasoning?\" by the Principle of Explosion. Exposing a contradiction was already in the ancient days of Plato a way of showing that some reasoning was wrong, but there was no explicit argument as to why contradictions were incorrect. William of Soissons gave a proof in which he showed that from a contradiction any assertion can be inferred as true. In example from: \"It is raining (P) and it is not raining (¬P)\" you may infer \"that there are trees on the moon (or whatever else)(E)\". In symbolic language: P & ¬P → E.\n\nIf a contradiction makes anything true then it makes it impossible to say anything meaningful: whatever you say, its contradiction is also true.\n\nWilliam's contemporairies compared his proof with a siege engine (12th century). In the 1800s, Clarence Irving Lewis formalized this proof as follows:\n\nProof\n\nV : or\n& : and\n→ : inference\nP : proposition\n¬ P : denial of P\nP &¬ P : contradiction.\nE : any possible assertion (Explosion).\n\nIn the 15th century this proof was rejected by a school in Cologne. They didn't accept step (6). In 19th-century classical logic, the Principle of Explosion was widely accepted as self-evident, e.g. by logicians like George Boole and Gottlob Frege, though the formalization of the Soissons proof by Lewis provided additional grounding the Principle of Explosion.\n\nThe above proof can be rejected as is shown below. \n\nTake the proof above and unpack the justification for line (6), taking it to rely on (6*):\n\nNow only if (P &¬ P) is rejected as invalid E can be inferred:\n\nFrom (5) and (8*) follows:\n\nOn this reconstruction, only by rejecting (P &¬ P) can E be concluded. So, if (P &¬ P) is not rejected E cannot be concluded. But (P &¬ P) can in this proof only be rejected if E is valid. So this proof is a vicious circle. \n"}
{"id": "56767513", "url": "https://en.wikipedia.org/wiki?curid=56767513", "title": "Zariski's finiteness theorem", "text": "Zariski's finiteness theorem\n\nIn algebra, Zariski's finiteness theorem gives a positive answer to Hilbert's 14th problem for the polynomial ring in two variables, as a special case. Precisely, it states:\n"}
