{"id": "407368", "url": "https://en.wikipedia.org/wiki?curid=407368", "title": "108 (number)", "text": "108 (number)\n\n108 (one hundred [and] eight) is the natural number following 107 and preceding 109.\n\n108 is:\n\n\nThere are 108 free polyominoes of order 7.\n\nThe equation formula_2 results in the golden ratio.\n\nThe number 108 is considered sacred by the Dharmic Religions, such as Hinduism, Buddhism, Jainism.\n\nIn Hindu tradition, the Mukhya Shivaganas (attendants of Shiva) are 108 in number and hence Shaiva religions, particularly Lingayats, use malas of 108 beads for prayer and meditation.\n\nSimilarly, in Gaudiya Vaishnavism, Lord Krishna in Brindavan had 108 followers known as gopis. Recital of their names, often accompanied by the counting of a 108-beaded mala, is often done during religious ceremonies. \nThe Sri Vaishnavite Tradition has 108 Divya Desams (temples of Vishnu) that are revered by the 12 Alvars in the \"Divya Prabandha\", a collection of 4,000 Tamil verses.\n\nIn Jainism, the total number of ways of Karma influx (Aasrav). 4 Kashays (anger, pride, conceit, greed) x 3 karanas (mind, speech, bodily action) x 3 stages of planning (planning, procurement, commencement) x 3 ways of execution (own action, getting it done, supporting or approval of action).\n\nIn Buddhism, according to Bhante Gunaratana this number is reached by multiplying the senses smell, touch, taste, hearing, sight, and consciousness by whether they are painful, pleasant or neutral, and then again by whether these are internally generated or externally occurring, and yet again by past, present and future, finally we get 108 feelings. 6 × 3 × 2 × 3 = 108.\n\nTibetan Buddhist malas or rosaries (Tib. ཕྲེང་བ Wyl. phreng ba, \"Trengwa\") are usually 108 beads; sometimes 111 including the guru bead(s), reflecting the words of the Buddha called in Tibetan the Kangyur (Wylie: Bka'-'gyur) in 108 volumes.\nZen priests wear juzu (a ring of prayer beads) around their wrists, which consists of 108 beads.\n\nThe Lankavatara Sutra has a section where the Bodhisattva Mahamati asks Buddha 108 questions and another section where Buddha lists 108 statements of negation in the form of \"A statement concerning X is not a statement concerning X.\" In a footnote, D.T. Suzuki explains that the Sanskrit word translated as \"statement\" is \"pada\" which can also mean \"foot-step\" or \"a position.\" This confusion over the word \"pada\" explains why some have mistakenly held that the reference to 108 statements in the Lankavatara refer to the 108 steps that many temples have.\n\nIn Japan, at the end of the year, a bell is chimed 108 times in Buddhist temples to finish the old year and welcome the new one. Each ring represents one of 108 earthly temptations (Bonnō) a person must overcome to achieve nirvana.\n\nIn the neo-Gnostic teachings of Samael Aun Weor, an individual has 108 chances (lifetimes) to eliminate his egos and transcend the material world before \"devolving\" and having the egos forcefully removed in the infradimensions.\n\nMany East Asian martial arts trace their roots back to Buddhism, specifically, to the Buddhist Shaolin Temple. Because of their ties to Buddhism, 108 has become an important symbolic number in a number of martial arts styles.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "654771", "url": "https://en.wikipedia.org/wiki?curid=654771", "title": "135 (number)", "text": "135 (number)\n\n135 (one hundred [and] thirty-five) is the natural number following 134 and preceding 136.\n\nThis number in base 10 can be expressed in operations using its own digits in at least two different ways. One is as a sum-product number,\n\nformula_1\n\n(1 and 144 share this property) and the other is as the sum of consecutive powers of its digits:\n\nformula_2\n\n(175, 518, and 598 also have this property).\n\n135 is a Harshad number, and a repdigit in bases 18 and 26.\n\nThere are a total of 135 primes between 1,000 and 2,000.\n\nformula_3 for formula_4. This polynomial plays an essential role in Apéry's proof that formula_5 is irrational.\n\n\n\n\n"}
{"id": "1616221", "url": "https://en.wikipedia.org/wiki?curid=1616221", "title": "141 (number)", "text": "141 (number)\n\n141 (one hundred [and] forty-one) is the natural number following 140 and preceding 142.\n\n141 is:\n\n\n\n141 is also:\n\n"}
{"id": "50642454", "url": "https://en.wikipedia.org/wiki?curid=50642454", "title": "Ailles rectangle", "text": "Ailles rectangle\n\nThe Ailles rectangle is a rectangle constructed from four right-angled triangles which is commonly used in geometry classes to find the values of trigonometric function of 15° and 75°. It is named after high school teacher Douglas S. Ailles.\n\nIn every beginning trigonometry course, one learns of the 30°–60°–90° triangle with sides of length 1, 2, and . When two such triangles are placed in the positions shown in the illustration, the smallest rectangle that can enclose them has width 1 +  and height . Drawing a line connecting the original triangles' top corners creates a 45°–45°–90° triangle between the two, with sides of lengths 2, 2 and (by the Pythagorean theorem) 2. The remaining space at the top of the rectangle is a right triangle with small angles of 15° and 75°. and sides of  − 1,  + 1 and 2.\n\nTherefore, we can conclude\n\nand\n\nAn alternative construction (also by Ailles) places a 30°–60°–90° triangle in the middle, flanked by two 45°–45°–90° triangles in the lower corners. The lower-left triangle has sides of 1, 1, and , while the lower-right triangle has sides of , , and . The 30°–60°–90° triangle is scaled up by from its usual dimensions, so its hypotenuse is 2. The 15°–75°–90° triangle is identical.\n"}
{"id": "2452868", "url": "https://en.wikipedia.org/wiki?curid=2452868", "title": "Alessandro Padoa", "text": "Alessandro Padoa\n\nAlessandro Padoa (14 October 1868 – 25 November 1937) was an Italian mathematician and logician, a contributor to the school of Giuseppe Peano. He is remembered for a method for deciding whether, given some formal theory, a new primitive notion is truly independent of the other primitive notions. There is an analogous problem in axiomatic theories, namely deciding whether a given axiom is independent of the other axioms.\n\nThe following description of Padoa's career is included in a biography of Peano:\n\nThe congresses in Paris in 1900 were particularly notable. Padoa's addresses at these congresses have been well remembered for their clear and unconfused exposition of the modern axiomatic method in mathematics. In fact, he is said to be \"the first … to get all the ideas concerning defined and undefined concepts completely straight\".\n\nAt the International Congress of Philosophy Padoa spoke on \"Logical Introduction to Any Deductive Theory\". He says\nPadoa went on to say:\n\nPadoa spoke at the 1900 International Congress of Mathematicians with his title \"A New System of Definitions for Euclidean Geometry\". At the outset he discusses the various selections of primitive notions in geometry at the time:\nPadoa completed his address by suggesting and demonstrating his own development of geometric concepts. In particular, he showed how he and Pieri define a line in terms of \ncollinear points.\n\n\nSecondary:\n"}
{"id": "31068", "url": "https://en.wikipedia.org/wiki?curid=31068", "title": "Analogy of the divided line", "text": "Analogy of the divided line\n\nThe Analogy of the Divided Line () is presented by the Greek philosopher Plato in the \"Republic\" (509d–511e). It is written as a dialogue between Glaucon and Socrates, in which the latter further elaborates upon the immediately preceding Analogy of the Sun at the former's request. Socrates asks Glaucon to not only envision this unequally bisected line but to imagine further bisecting each of the two segments. Socrates explains that the four resulting segments represent four separate 'affections' (παθήματα) of the psyche. The lower two sections are said to represent the visible while the higher two are said to represent the intelligible. These affections are described in succession as corresponding to increasing levels of reality and truth from conjecture (εἰκασία) to belief () to thought (διάνοια) and finally to understanding (). Furthermore, this analogy not only elaborates a theory of the psyche but also presents metaphysical and epistemological views.\n\nThis analogy is immediately followed by the Analogy of the Cave at 514a. Socrates returns once more to the elements of the divided line (533d-534a) as he summarizes his dialectic.\n\nIn \"The Republic\" (509d–510a), Plato describes the Divided Line this way:\n\nThus AB represents shadows and reflections of physical things, and BC the physical things themselves. These correspond to two kinds of knowledge, the illusion (εἰκασία \"eikasia\") of our ordinary, everyday experience, and belief (πίστις \"pistis\") about discrete physical objects which cast their shadows. In the \"Timaeus\", the category of illusion includes all the \"opinions of which the minds of ordinary people are full,\" while the natural sciences are included in the category of belief.\n\nAccording to some translations, the segment CE, representing the intelligible world, is divided into the same ratio as AC, giving the subdivisions CD and DE (it can be readily verified that CD must have the same length as BC:\n\nPlato describes CD, the \"lower\" of these, as involving mathematical reasoning (διάνοια \"dianoia\"), where abstract mathematical objects such as geometric lines are discussed. Such objects are outside the physical world (and are not to be confused with the \"drawings\" of those lines, which fall within the physical world BC). However, they are less important to Plato than the subjects of philosophical understanding (νόησις \"noesis\"), the \"higher\" of these two subdivisions (DE):\n\nPlato here is using the familiar relationship between ordinary objects and their shadows or reflections in order to illustrate the relationship between the physical world as a whole and the world of Ideas (Forms) as a whole. The former is made up of a series of passing reflections of the latter, which is eternal, more real and \"true.\" Moreover, the knowledge that we have of the Ideas – when indeed we do have it – is of a higher order than knowledge of the mere physical world. In particular, knowledge of the forms leads to a knowledge of the Idea (Form) of the Good.\n\nThe Allegory of the Divided Line is the cornerstone of Plato's metaphysical framework. This structure, well hidden in the middle of the \"Republic\", a complex, multi-layered dialogue, illustrates the grand picture of Plato's metaphysics, epistemology, and ethics, all in one. It is not enough for the philosopher to understand the Ideas (Forms), he must also understand the relation of Ideas to all four levels of the structure to be able to know anything at all. In the \"Republic\", the philosopher must understand the Idea of Justice to live a just life or to organize and govern a just state.\n\nThe Divided Line also serves as our guide for most past and future metaphysics. The lowest level, which represents \"the world of becoming and passing away\" (\"Republic\", 508d), is the metaphysical model for a Heraclitean philosophy of constant flux and for Protagorean philosophy of appearance and opinion. The second level, a world of fixed physical objects, also became Aristotle's metaphysical model. The third level might be a Pythagorean level of mathematics. The fourth level is Plato's ideal Parmenidean reality, the world of highest level Ideas.\n\nPlato holds a very strict notion of knowledge. For example, he does not accept expertise about a subject, nor direct perception (see \"Theaetetus\"), nor true belief about the physical world (the \"Meno\") as knowledge. It is not enough for the philosopher to understand the Ideas (Forms), he must also understand the relation of Ideas to all four levels of the structure to be able to know anything at all. For this reason, in most of the \"earlier Socratic\" dialogues, Socrates denies knowledge both to himself and others.\n\nFor the first level, \"the world of becoming and passing away,\" Plato expressly denies the possibility of knowledge. Constant change never stays the same, therefore, properties of objects must refer to different Ideas at different times. Note that for knowledge to be possible, which Plato believed, the other three levels must be unchanging. The third and fourth level, mathematics and Ideas, are already eternal and unchanging. However, to ensure that the second level, the objective, physical world, is also unchanging, Plato, in the \"Republic\", Book 4 introduces empirically derived axiomatic restrictions that prohibit both motion and shifting perspectives.\n\n\n"}
{"id": "11851019", "url": "https://en.wikipedia.org/wiki?curid=11851019", "title": "Andreas Blass", "text": "Andreas Blass\n\nAndreas Raphael Blass (born October 27, 1947 in Nuremberg) is a mathematician, currently a professor at the University of Michigan. He specializes in mathematical logic, particularly set theory, and theoretical computer science.\n\nBlass graduated from the University of Detroit, where he was a Putnam Fellow, in 1966 with a B.S. in physics. He received his Ph.D. in 1970 from Harvard University, with a thesis on \"Orderings of Ultrafilters\" written under the supervision of Frank Wattenberg. Since 1970 he has been employed by the University of Michigan, first as a \"T.H. Hildebrandt Research Instructor\" (1970–72), then assistant professor (1972–76), associate professor (1976–84) and since 1984 he has been a full professor there.\n\nIn 2014, he became a Fellow of the American Mathematical Society.\n\nIn 1984 Blass proved that the existence of a basis for every vector space is equivalent to the axiom of choice. He made important contributions in the development of the set theory of the reals and forcing.\n\nBlass was the first to point out connections between game semantics and linear logic.\n\nHe has authored more than 200 research articles in mathematical logic and theoretical computer science, including:\n\n"}
{"id": "6960628", "url": "https://en.wikipedia.org/wiki?curid=6960628", "title": "Andrew Casson", "text": "Andrew Casson\n\nAndrew John Casson FRS (born 1943) is a mathematician, studying geometric topology.\n\nCasson is the Philip Schuyler Beebe Professor of Mathematics at Yale University in the United States where he served as department chair between 2004 and 2007. His Ph.D. advisor at the University of Liverpool was C. T. C. Wall, but he never completed his doctorate; instead what would have been his Ph.D. thesis became his fellowship dissertation as a research fellow at Trinity College, Cambridge. He was Professor of Mathematics at the University of Texas at Austin between 1981 and 1986, at the University of California, Berkeley, from 1986 to 2000, and has been at Yale since 2000. \n\nIn 1991, he was awarded the Oswald Veblen Prize in Geometry by the American Mathematical Society. In 1998 he was elected to Fellowship of the Royal Society. \n\nCasson has worked in both high-dimensional manifold topology and 3- and 4-dimensional topology,\nusing both geometric and algebraic techniques. Among other discoveries, he contributed \nto the disproof of the manifold Hauptvermutung, introduced the Casson invariant, a modern invariant for 3-manifolds, and Casson handles, used in Michael Freedman's proof of the 4-dimensional Poincaré conjecture.\n\n"}
{"id": "6988121", "url": "https://en.wikipedia.org/wiki?curid=6988121", "title": "Arithmetic and geometric Frobenius", "text": "Arithmetic and geometric Frobenius\n\nIn mathematics, the Frobenius endomorphism is defined in any commutative ring \"R\" that has characteristic \"p\", where \"p\" is a prime number. Namely, the mapping φ that takes \"r\" in \"R\" to \"r\" is a ring endomorphism of \"R\".\n\nThe image of φ is then \"R\", the subring of \"R\" consisting of \"p\"-th powers. In some important cases, for example finite fields, φ is surjective. Otherwise φ is an endomorphism but not a ring \"automorphism\".\n\nThe terminology of geometric Frobenius arises by applying the spectrum of a ring construction to φ. This gives a mapping\n\nof affine schemes. Even in cases where \"R\" = \"R\" this is not the identity, unless \"R\" is the prime field. \n\nMappings created by fibre product with φ*, i.e. base changes, tend in scheme theory to be called \"geometric Frobenius\". The reason for a careful terminology is that the Frobenius automorphism in Galois groups, or defined by transport of structure, is often the inverse mapping of the geometric Frobenius. As in the case of a cyclic group in which a generator is also the inverse of a generator, there are in many situations two possible definitions of Frobenius, and without a consistent convention some problem of a minus sign may appear.\n\n"}
{"id": "15669513", "url": "https://en.wikipedia.org/wiki?curid=15669513", "title": "Avraham Trahtman", "text": "Avraham Trahtman\n\nAvraham Naumovich Trahtman (Trakhtman) (; b. 1944, USSR) is a mathematician at Bar-Ilan University (Israel). In 2007, Trahtman solved a problem in combinatorics that had been open for 37 years, the Road Coloring Conjecture posed in 1970.\n\nTrahtman's solution to the road coloring problem was accepted in 2007 and published in 2009 by the \"Israel Journal of Mathematics\". The problem arose in the subfield of symbolic dynamics, an abstract part of the field of dynamical systems. The road coloring problem was raised by R. L. Adler and L. W. Goodwyn from the United States, and the Israeli mathematician B. Weiss. The proof used results from earlier work.\n\nThe problem of estimating the length of synchronizing word has a long history and was posed independently by several authors, but it is commonly known as the Černý conjecture. In 1964 Jan Černý conjectured that formula_1 is the upper bound for the length of the shortest synchronizing word for any n-state complete DFA (a DFA with complete state transition graph). If this is true, it would be tight: in his 1964 paper, Černý exhibited a class of automata (indexed by the number n of states) for which the shortest reset words have this length. In 2011 Trahtman published a proof of upper bound formula_2, but then he found an error in it. The conjecture holds in many partial cases, see for instance, Kari and Trahtman.\n\nThe finite basis problem for semigroups of order less than six in the theory of semigroups was posed by Alfred Tarski in 1966, and repeated by Anatoly Maltsev and L. N. Shevrin. In 1983, Trahtman solved this problem by proving that all semigroups of order less than six are finitely based.\n\nIn the theory of varieties of semigroups and universal algebras the problem of existence of covering elements in the lattice of varieties was posed by Evans in 1971. The positive solution of the problem was found by Trahtman. He also found a six-element semigroup that generates a variety with a continuum of subvarieties, and varieties of semigroups having no irreducible base of identities.\n\nThe theory of locally testable automata can be based on the theory of varieties of locally testable semigroups. Trahtman found the precise estimation on the order of local testability of finite automata.\n\nThere are results in theoretical mechanics and in the promising area of extracting moisture from the air mentioned in \"New Scientist\".\n\n"}
{"id": "3326106", "url": "https://en.wikipedia.org/wiki?curid=3326106", "title": "Bondy's theorem", "text": "Bondy's theorem\n\nIn mathematics, Bondy's theorem is a bound on the number of elements needed to distinguish the sets in a family of sets from each other. It belongs to the field of combinatorics, and is named after John Adrian Bondy, who published it in 1972.\n\nThe theorem is as follows:\n\nIn other words, if we have a 0-1 matrix with \"n\" rows and \"n\" columns such that each row is distinct, we can remove one column such that the rows of the resulting \"n\" × (\"n\" − 1) matrix are distinct.\n\nConsider the 4 × 4 matrix\nwhere all rows are pairwise distinct. If we delete, for example, the first column, the resulting matrix\nno longer has this property: the first row is identical to the second row. Nevertheless, by Bondy's theorem we know that we can always find a column that can be deleted without introducing any identical rows. In this case, we can delete the third column: all rows of the 3 × 4 matrix\nare distinct. Another possibility would have been deleting the fourth column.\n\nFrom the perspective of computational learning theory, Bondy's theorem can be rephrased as follows:\n\nThis implies that every finite concept class \"C\" has its teaching dimension bounded by |\"C\"| − 1.\n"}
{"id": "3237784", "url": "https://en.wikipedia.org/wiki?curid=3237784", "title": "Boolean grammar", "text": "Boolean grammar\n\nBoolean grammars, introduced by Okhotin, are a class of formal grammars studied in formal language theory. They extend the basic type of grammars, the context-free grammars, with conjunction and negation operations. Besides these explicit operations, Boolean grammars allow implicit disjunction represented by multiple rules for a single nonterminal symbol, which is the only logical connective expressible in context-free grammars. Conjunction and negation can be used, in particular, to specify intersection and complement of languages. An intermediate class of grammars known as conjunctive grammars allows conjunction and disjunction, but not negation. \n\nThe rules of a Boolean grammar are of the form\n\nformula_1\n\nwhere formula_2 is a nonterminal, formula_3 and formula_4, ..., formula_5, formula_6, ..., formula_7 are strings formed of symbols in formula_8 and formula_9. Informally, such a rule asserts that every string formula_10 over formula_8 that satisfies each of the syntactical conditions represented by formula_4, ..., formula_5 and none of the syntactical conditions represented by formula_6, ..., formula_7 therefore satisfies the condition defined by formula_2.\n\nThere exist several formal definitions of the language generated by a Boolean grammar. They have one thing in common: if the grammar is represented as a system of language equations with union, intersection, complementation and concatenation, the languages generated by the grammar must be the solution of this system. The semantics differ in details, some define the languages using language equations, some draw upon ideas from the field of logic programming. However, these nontrivial issues of formal definition are mostly irrelevant for practical considerations, and one can construct grammars according to the given informal semantics. The practical properties of the model are similar to those of conjunctive grammars, while the descriptional capabilities are further improved. In particular, some practically useful properties inherited from context-free grammars, such as efficient parsing algorithms, are retained, see .\n\n\n\n"}
{"id": "937664", "url": "https://en.wikipedia.org/wiki?curid=937664", "title": "Chebyshev's sum inequality", "text": "Chebyshev's sum inequality\n\nIn mathematics, Chebyshev's sum inequality, named after Pafnuty Chebyshev, states that if\n\nand\n\nthen\n\nSimilarly, if\n\nand\n\nthen\n\nConsider the sum\n\nThe two sequences are non-increasing, therefore and have the same sign for any . Hence .\n\nOpening the brackets, we deduce:\n\nwhence\n\nAn alternative proof is simply obtained with the rearrangement inequality, writing that\n\nThere is also a continuous version of Chebyshev's sum inequality:\n\nIf \"f\" and \"g\" are real-valued, integrable functions over [0,1], both non-increasing or both non-decreasing, then\n\nwith the inequality reversed if one is non-increasing and the other is non-decreasing.\n"}
{"id": "21795781", "url": "https://en.wikipedia.org/wiki?curid=21795781", "title": "Checkpointing scheme", "text": "Checkpointing scheme\n\nCheckpointing schemes are scientific computing algorithms uses in solving time dependent adjoint equation, as well as reverse mode automatic differentiation.\n"}
{"id": "25350901", "url": "https://en.wikipedia.org/wiki?curid=25350901", "title": "Cis (mathematics)", "text": "Cis (mathematics)\n\nThe notation was first coined by William Rowan Hamilton in \"Elements of Quaternions\" (1866) and subsequently used by Irving Stringham in works such as \"Uniplanar Algebra\" (1893), or by James Harkness and Frank Morley in their \"Introduction to the Theory of Analytic Functions\" (1898). It connects trigonometric functions with exponential functions in the complex plane via Euler's formula.\n\nIt is mostly used as a convenient shorthand notation to simplify some expressions,\nfor example in conjunction with Fourier and Hartley transforms, or when exponential functions shouldn't be used for some reason in math education.\n\nIn information technology, the function sees dedicated support in various high-performance math libraries (such as Intel's Math Kernel Library (MKL)), available for many compilers, programming languages (including C, C++, Common Lisp, D, Fortran, Haskell, Julia), and operating systems (including Windows, Linux, macOS and HP-UX). Depending on the platform the fused operation is about twice as fast as calling the sine and cosine functions individually.\n\nThe complex exponential function can be expressed\n\nwhere .\n\nThis can also be expressed using the following notation\n\ni.e.. \" abbreviates \".\n\nThough at first glance this notation is redundant, being equivalent to , its use is rooted in several advantages, such as being directly tied to the polar form of a complex number (and being easier to grasp).\n\nThese follow directly from Euler's formula.\n\nThe identities above hold if and are any complex numbers. If and are real, then\n\nThis notation was more common in the post–World War II era, when typewriters were used to convey mathematical expressions.\n\nSuperscripts are both offset vertically and smaller than \" or \"; hence, they can be problematic even for hand-writing, for example, versus or . For many readers, is the clearest, easiest to read of the three.\n\nThe notation is sometimes used to emphasize one method of viewing and dealing with a problem over another. The mathematics of trigonometry and exponentials are related but not exactly the same; exponential notation emphasizes the whole, whereas and notations emphasize the parts. This can be rhetorically useful to mathematicians and engineers when discussing this function, and further serve as a mnemonic (for ).\n\nThe notation is convenient for math students whose knowledge of trigonometry and complex numbers permit this notation, but whose conceptual understanding does not yet permit the notation . As students learn concepts that build on prior knowledge, it is important not to force them into levels of math they are not yet prepared for: the usual proof that requires calculus, which the student may not have studied before they encountered the expression .\n\nIn 1942, inspired by the notation, Ralph V. L. Hartley introduced the (for \"cosine-and-sine\") function for the real-valued Hartley kernel, a meanwhile established shortcut in conjunction with Hartley transforms:\n\n"}
{"id": "14835049", "url": "https://en.wikipedia.org/wiki?curid=14835049", "title": "Covering relation", "text": "Covering relation\n\nIn mathematics, especially order theory, the covering relation of a partially ordered set is the binary relation which holds between comparable elements that are immediate neighbours. The covering relation is commonly used to graphically express the partial order by means of the Hasse diagram.\n\nLet formula_1 be a set with a partial order formula_2.\nAs usual, let formula_3 be the relation on formula_1 such that formula_5 if and only if formula_6 and formula_7.\n\nLet formula_8 and formula_9 be elements of formula_1.\n\nThen formula_9 covers formula_8, written formula_13,\nif formula_5 and there is no element formula_15 such that formula_16. Equivalently, formula_9 covers formula_8 if the interval formula_19 is the two-element set formula_20.\n\nWhen formula_13, it is said that formula_9 is a cover of formula_8. Some authors also use the term cover to denote any such pair formula_24 in the covering relation.\n\n\n\n"}
{"id": "830034", "url": "https://en.wikipedia.org/wiki?curid=830034", "title": "Crystal (mathematics)", "text": "Crystal (mathematics)\n\nIn mathematics, crystals are cartesian sections of certain fibered categories. They were introduced by , who named them crystals because in some sense they are \"rigid\" and \"grow\". In particular quasicoherent crystals over the crystalline site are analogous to quasicoherent modules over a scheme. \n\nThere are several variations of crystals, as follows:\n\nThe infinitesimal site Inf(\"X\"/\"S\") has as objects the infinitesimal extensions of open sets of \"X\".\nIf \"X\" is a scheme over \"S\" then the sheaf \"O\" is defined by \n\"O\"(\"T\") = coordinate ring of \"T\", where we write \"T\" as an abbreviation for \nan object \"U\" → \"T\" of Inf(\"X\"/\"S\"). Sheaves on this site grow in the sense that they can be extended from open sets to infinitesimal extensions of open sets. \n\nA crystal on the site Inf(\"X\"/\"S\") is a sheaf \"F\" of \"O\" modules that is rigid in the following sense:\nThis is similar to the definition of a quasicoherent sheaf of modules in the Zariski topology.\n\nAn example of a crystal is the sheaf \"O\".\n\nCrystals on the crystalline site are defined in a similar way.\n\nIn general, if \"E\" is a fibered category over \"F\", then a crystal is a cartesian section of the fibered category. In the special case when \"F\" is the category of infinitesimal extensions of a scheme \"X\" and \"E\" the category of quasicoherent modules over objects of \"F\", then crystals of this fibered category are the same as crystals of the infinitesimal site.\n\n"}
{"id": "54974049", "url": "https://en.wikipedia.org/wiki?curid=54974049", "title": "Cybernetics in the Soviet Union", "text": "Cybernetics in the Soviet Union\n\nCybernetics in the Soviet Union has its own particular characteristics as the study of cybernetics there interacted with the dominant ideology of Marxism-Leninism.\n\nIt first made its impact in 1953 when three things happened:\n\n"}
{"id": "43511129", "url": "https://en.wikipedia.org/wiki?curid=43511129", "title": "Darkside communication group", "text": "Darkside communication group\n\nTheir famous work is in 1996. The book is introduced by Japanese TV programs in many times. which is their monthly magazine, won the prize of \"Best titled book in Japan (日本タイトルだけ大賞)\" in 2012. Many of members write their books in handle name.\n\n\nMore than 200 titles are published. Their books are able to classify of three types; number lists, avant-garde books and scientific descriptions.\n\n\nMany types of mathematical constants are provided online.\n\n\n\n\nOver ten other members write books and sell their books in the Comiket.\n\n\n"}
{"id": "644814", "url": "https://en.wikipedia.org/wiki?curid=644814", "title": "Einstein manifold", "text": "Einstein manifold\n\nIn differential geometry and mathematical physics, an Einstein manifold is a Riemannian or pseudo-Riemannian differentiable manifold whose Ricci tensor is proportional to the metric. They are named after Albert Einstein because this condition is equivalent to saying that the metric is a solution of the vacuum Einstein field equations (with cosmological constant), although both the dimension and the signature of the metric can be arbitrary, thus not being restricted to the four-dimensional Lorentzian manifolds usually studied in general relativity.\n\nIf \"M\" is the underlying \"n\"-dimensional manifold and \"g\" is its metric tensor the Einstein condition means that\n\nfor some constant \"k\", where Ric denotes the Ricci tensor of \"g\". Einstein manifolds with are called Ricci-flat manifolds.\n\nIn local coordinates the condition that be an Einstein manifold is simply\n\nTaking the trace of both sides reveals that the constant of proportionality \"k\" for Einstein manifolds is related to the scalar curvature \"R\" by\n\nwhere \"n\" is the dimension of \"M\".\n\nIn general relativity, Einstein's equation with a cosmological constant Λ is\n\nwritten in geometrized units with . The stress–energy tensor \"T\" gives the matter and energy content of the underlying spacetime. In vacuum (a region of spacetime devoid of matter) , and Einstein's equation can be rewritten in the form (assuming that ):\nTherefore, vacuum solutions of Einstein's equation are (Lorentzian) Einstein manifolds with \"k\" proportional to the cosmological constant.\n\nSimple examples of Einstein manifolds include:\n\n\nA necessary condition for closed, oriented, 4-manifolds to be Einstein is satisfying the Hitchin–Thorpe inequality.\n\nFour dimensional Riemannian Einstein manifolds are also important in mathematical physics as gravitational instantons in quantum theories of gravity. The term \"gravitational instanton\" is usually used restricted to Einstein 4-manifolds whose Weyl tensor is self-dual, and it is usually assumed that the metric is asymptotic to the standard metric of Euclidean 4-space (and are therefore complete but non-compact). In differential geometry, self-dual Einstein 4-manifolds are also known as (4-dimensional) hyperkähler manifolds in the Ricci-flat case, and quaternion Kähler manifolds otherwise.\n\nHigher-dimensional Lorentzian Einstein manifolds are used in modern theories of gravity, such as string theory, M-theory and supergravity. Hyperkähler and quaternion Kähler manifolds (which are special kinds of Einstein manifolds) also have applications in physics as target spaces for nonlinear σ-models with supersymmetry.\n\nCompact Einstein manifolds have been much studied in differential geometry, and many examples are known, although constructing them is often challenging. Compact Ricci-flat manifolds are particularly difficult to find: in the monograph on the subject by the pseudonymous author Arthur Besse, readers are offered a meal in a starred restaurant in exchange for a new example.\n\n"}
{"id": "50621216", "url": "https://en.wikipedia.org/wiki?curid=50621216", "title": "Empirical characteristic function", "text": "Empirical characteristic function\n\nIn statistics, the empirical characteristic function is a numerically calculated approximation to the characteristic function.\n\nIf formula_1 are i.i.d. observations, then the empirical characteristic function formula_2 is defined as\n\nIt is useful in estimation theory and hypothesis testing.\n"}
{"id": "203622", "url": "https://en.wikipedia.org/wiki?curid=203622", "title": "Euler line", "text": "Euler line\n\nIn geometry, the Euler line, named after Leonhard Euler (), is a line determined from any triangle that is not equilateral. It is a central line of the triangle, and it\npasses through several important points determined from the triangle, including the orthocenter, the circumcenter, the centroid, the Exeter point and the center of the nine-point circle of the triangle.\n\nThe concept of a triangle's Euler line extends to the Euler line of other shapes, such as the quadrilateral and the tetrahedron.\n\nEuler showed in 1765 that in any triangle, the orthocenter, circumcenter and centroid are collinear. This property is also true for another triangle center, the nine-point center, although it had not been defined in Euler's time. In equilateral triangles, these four points coincide, but in any other triangle they are all distinct from each other, and the Euler line is determined by any two of them.\n\nOther notable points that lie on the Euler line include the de Longchamps point, the Schiffler point, the Exeter point, and the Gossard perspector. However, the incenter generally does not lie on the Euler line; it is on the Euler line only for isosceles triangles, for which the Euler line coincides with the symmetry axis of the triangle and contains all triangle centers.\n\nThe tangential triangle of a reference triangle is tangent to the latter's circumcircle at the reference triangle's vertices. The circumcenter of the tangential triangle lies on the Euler line of the reference triangle. The center of similitude of the orthic and tangential triangles is also on the Euler line.\n\nLet formula_1 be a triangle. A proof of the fact that the circumcenter formula_2, the centroid formula_3 and the orthocenter formula_4 are collinear relies on free vectors. We start by stating the prerequisites. First, formula_3 satisfies the relation\n\nThis follows from the fact that the absolute barycentric coordinates of formula_3 are formula_8. Further, the problem of Sylvester reads as\n\nNow, using the vector addition, we deduce that\n\nBy adding these three relations, term by term, we obtain that\n\nIn conclusion, formula_12, and so the three points formula_2, formula_3 and formula_4 (in this order) are collinear.\n\nIn Dörrie's book, the Euler line and the problem of Sylvester are put together into a single proof. However, most of the proofs of the problem of Sylvester rely on the fundamental properties of free vectors, independently of the Euler line.\n\nOn the Euler line the centroid \"G\" is between the circumcenter \"O\" and the orthocenter \"H\" and is twice as far from the orthocenter as it is from the circumcenter:\n\nThe segment \"GH\" is a diameter of the orthocentroidal circle.\n\nThe center \"N\" of the nine-point circle lies along the Euler line midway between the orthocenter and the circumcenter:\n\nThus the Euler line could be repositioned on a number line with the circumcenter \"O\" at the location 0, the centroid \"G\" at 2\"t\", the nine-point center at 3\"t\", and the orthocenter \"H\" at 6\"t\" for some scale factor \"t\".\n\nFurthermore, the squared distance between the centroid and the circumcenter along the Euler line is less than the squared circumradius \"R\" by an amount equal to one-ninth the sum of the squares of the side lengths \"a\", \"b\", and \"c\":\n\nIn addition,\n\nLet \"A\", \"B\", \"C\" denote the vertex angles of the reference triangle, and let \"x\" : \"y\" : \"z\" be a variable point in trilinear coordinates; then an equation for the Euler line is\n\nAn equation for the Euler line in barycentric coordinates formula_23 is\n\nAnother way to represent the Euler line is in terms of a parameter \"t\". Starting with the circumcenter (with trilinear coordinates formula_25) and the orthocenter (with trilinears formula_26every point on the Euler line, except the orthocenter, is given by the trilinear coordinates\nformed as a linear combination of the trilinears of these two points, for some \"t\".\n\nFor example:\n\nIn a Cartesian coordinate system, denote the slopes of the sides of a triangle as formula_36 formula_37 and formula_38 and denote the slope of its Euler line as formula_39. Then these slopes are related according to\n\nThus the slope of the Euler line (if finite) is expressible in terms of the slopes of the sides as\n\nMoreover, the Euler line is parallel to an acute triangle's side \"BC\" if and only if formula_43\n\nThe locus of the centroids of equilateral triangles inscribed in a given triangle is formed by two lines perpendicular to the given triangle's Euler line.\n\nIn a right triangle, the Euler line coincides with the median to the hypotenuse—that is, it goes through both the right-angled vertex and the midpoint of the side opposite that vertex. This is because the right triangle's orthocenter, the intersection of its altitudes, falls on the right-angled vertex while its circumcenter, the intersection of its perpendicular bisectors of sides, falls on the midpoint of the hypotenuse.\n\nThe Euler line of an isosceles triangle coincides with the axis of symmetry. In an isosceles triangle the incenter falls on the Euler line.\n\nThe Euler line of an automedian triangle (one whose medians are in the same proportions, though in the opposite order, as the sides) is perpendicular to one of the medians.\n\nConsider a triangle \"ABC\" with Fermat–Torricelli points \"F\" and \"F\". The Euler lines of the 10 triangles with vertices chosen from \"A, B, C, F\" and \"F\" are concurrent at the centroid of triangle \"ABC\".\n\nThe Euler lines of the four triangles formed by an orthocentric system (a set of four points such that each is the orthocenter of the triangle with vertices at the other three points) are concurrent at the nine-point center common to all of the triangles.\n\nIn a convex quadrilateral, the quasiorthocenter \"H\", the \"area centroid\" \"G\", and the quasicircumcenter \"O\" are collinear in this order on the Euler line, and \"HG\" = 2\"GO\".\n\nA tetrahedron is a three-dimensional object bounded by four triangular faces. Seven lines associated with a tetrahedron are concurrent at its centroid; its six midplanes intersect at its Monge point; and there is a circumsphere passing through all of the vertices, whose center is the circumcenter. These points define the \"Euler line\" of a tetrahedron analogous to that of a triangle. The centroid is the midpoint between its Monge point and circumcenter along this line. The center of the twelve-point sphere also lies on the Euler line.\n\nA simplicial polytope is a polytope whose facets are all simplices. For example, every polygon is a simplicial polytope. The Euler line associated to such a polytope is the line determined by its centroid and circumcenter of mass. This definition of an Euler line generalizes the ones above.\n\nSuppose that formula_44 is a polygon. The Euler line formula_45 is sensitive to the symmetries of formula_44 in the following ways:\n\n1. If formula_44 has a line of reflection symmetry formula_48, then formula_45 is either formula_48 or a point on formula_48.\n\n2. If formula_44 has a center of rotational symmetry formula_53, then formula_54.\n\n3. If all but one of the sides of formula_44 have equal length, then formula_45 is orthogonal to the last side.\n\nA triangle's Kiepert parabola is the unique parabola that is tangent to the sides (two of them extended) of the triangle and has the Euler line as its directrix.\n\n"}
{"id": "38907546", "url": "https://en.wikipedia.org/wiki?curid=38907546", "title": "Fourth dimension in art", "text": "Fourth dimension in art\n\nNew possibilities opened up by the concept of four-dimensional space (and difficulties involved in trying to visualize it) helped inspire many modern artists in the first half of the twentieth century. Early Cubists, Surrealists, Futurists, and abstract artists took ideas from higher-dimensional mathematics and used them to radically advance their work.\n\nFrench mathematician Maurice Princet was known as \"le mathématicien du cubisme\" (\"the mathematician of cubism\"). An associate of the School of Paris, a group of avant-gardists including Pablo Picasso, Guillaume Apollinaire, Max Jacob, Jean Metzinger, and Marcel Duchamp, Princet is credited with introducing the work of Henri Poincaré and the concept of the \"fourth dimension\" to the cubists at the Bateau-Lavoir during the first decade of the 20th century.\n\nPrincet introduced Picasso to Esprit Jouffret's \"Traité élémentaire de géométrie à quatre dimensions\" (\"Elementary Treatise on the Geometry of Four Dimensions\", 1903), a popularization of Poincaré's \"Science and Hypothesis\" in which Jouffret described hypercubes and other complex polyhedra in four dimensions and projected them onto the two-dimensional page. Picasso's \"Portrait of Daniel-Henry Kahnweiler\" in 1910 was an important work for the artist, who spent many months shaping it. The portrait bears similarities to Jouffret's work and shows a distinct movement away from the Proto-Cubist fauvism displayed in \"Les Demoiselles d'Avignon\", to a more considered analysis of space and form.\n\nEarly cubist Max Weber wrote an article entitled \"In The Fourth Dimension from a Plastic Point of View\", for Alfred Stieglitz's July 1910 issue of \"Camera Work\". In the piece, Weber states, \"In plastic art, I believe, there is a fourth dimension which may be described as the consciousness of a great and overwhelming sense of space-magnitude in all directions at one time, and is brought into existence through the three known measurements.\"\n\nAnother influence on the School of Paris was that of Jean Metzinger and Albert Gleizes, both painters and theoreticians. The first major treatise written on the subject of Cubism was their 1912 collaboration \"Du \"Cubisme\"\", which says that: \"If we wished to relate the space of the [Cubist] painters to geometry, we should have to refer it to the non-Euclidian mathematicians; we should have to study, at some length, certain of Riemann's theorems.\"\n\nThe American modernist painter and photographer Morton Livingston Schamberg wrote in 1910 two letters to Walter Pach, parts of which were published in a review of the 1913 Armory Show for \"The Philadelphia Inquirer\", about the influence of the fourth dimension on avant-garde painting; describing how the artists' employed \"harmonic use of forms\" distinguishing between the \"representation or rendering of space and the designing in space\":\nIf we still further add to design in the third dimension, a consideration of weight, pressure, resistance, movement, as distinguished from motion, we arrive at what may legitimately be called design in the fourth dimension, or the harmonic use of what may arbitrarily be called volume. It is only at this point that we can appreciate the masterly productions of such a man as Cézanne.\n\nCézanne's explorations of geometric simplification and optical phenomena inspired the Cubists to experiment with simultaneity, complex multiple views of the same subject, as observed from differing viewpoints at the same time.\n\nIn 1936 in Paris, Charles Tamkó Sirató published his \"Manifeste Dimensioniste\", which described how \n\nThe manifesto was signed by many prominent modern artists worldwide. Hans Arp, Francis Picabia, Kandinsky, Robert Delaunay and Marcel Duchamp amongst others added their names in Paris, then a short while later it was endorsed by artists abroad including László Moholy-Nagy, Joan Miró, David Kakabadze, Alexander Calder, and Ben Nicholson.\n\nIn 1953, the surrealist Salvador Dalí proclaimed his intention to paint \"an explosive, nuclear and hypercubic\" crucifixion scene. He said that, \"This picture will be the great metaphysical work of my summer\". Completed the next year, \"Crucifixion (Corpus Hypercubus)\" depicts Jesus Christ upon the net of a hypercube, also known as a tesseract. The unfolding of a tesseract into eight cubes is analogous to unfolding the sides of a cube into six squares. The Metropolitan Museum of Art describes the painting as a \"new interpretation of an oft-depicted subject. ..[showing] Christ's spiritual triumph over corporeal harm.\"\n\nSome of Piet Mondrian's (1872–1944) abstractions and his practice of Neoplasticism are said to be rooted in his view of a utopian universe, with perpendiculars visually extending into another dimension.\n\nThe fourth dimension has been the subject of numerous fictional stories.\n\n\n\n\n"}
{"id": "40458000", "url": "https://en.wikipedia.org/wiki?curid=40458000", "title": "Geometric magic square", "text": "Geometric magic square\n\nA geometric magic square, often abbreviated to geomagic square, is a generalization of magic squares invented by Lee Sallows in 2001. A traditional magic square is a square array of numbers (almost always positive integers) whose sum taken in any row, any column, or in either diagonal is the same \"target number\". A geomagic square, on the other hand, is a square array of geometrical shapes in which those appearing in each row, column, or diagonal can be fitted together to create an identical shape called the \"target shape\". As with numerical types, it is required that the entries in a geomagic square be distinct. Similarly, the eight trivial variants of any square resulting from its rotation and/or reflection, are all counted as the same square. By the \"dimension\" of a geomagic square is meant the dimension of the pieces it uses. Hitherto interest has focused mainly on 2D squares using planar pieces, but pieces of any dimension are permitted.\n\nFigure 1 above shows a 3 × 3 geomagic square. The 3 pieces occupying each row, column and diagonal pave a rectangular target, as seen at left and right, and above and below. Here the 9 pieces are all decominoes, but pieces of any shape may appear, and it is not a requirement that they be of same size. In Figure 2, for instance, the pieces are polyominoes of consecutive sizes from 1 up to 9 units. The target is a 4 by 4 square with an inner square hole.\n\nSurprisingly, computer investigations show that Figure 2 is just one among 4,370 distinct 3 × 3 geomagic squares using pieces with these same sizes and same target. Conversely, Figure 1 is one of only two solutions using similar-sized pieces and identical target. In general, repeated piece sizes imply fewer solutions. However, at present there exists no theoretical underpinning to explain these empirical findings.\n\nThe pieces in a geomagic square may also be \"disjoint\", or composed of separated islands, as seen in Figure 3. Since they can be placed so as to mutually overlap, disjoint pieces are often able to tile areas that connected pieces cannot. The rewards of this extra pliancy are often to be seen in geomagics that possess symmetries denied to numerical specimens.\n\nBesides squares using planar shapes, there exist 3D specimens, the cells of which contain solid pieces that will combine to form the same constant solid target. Figure 5 shows an example in which the target is a cube.\n\nA well-known formula due to the mathematician Édouard Lucas characterizes the structure of every 3 × 3 magic square of numbers. Sallows, already the author of original work in this area, had long speculated that the Lucas formula might contain hidden potential. This surmise was confirmed in 1997 when he published a short paper that examined squares using complex numbers, a ploy leading to a new theorem that correlated every 3 × 3 magic square with a unique parallelogram on the complex plane. Continuing in the same vein, a decisive next step was to interpret the variables in the Lucas formula as standing for geometrical forms, an outlandish idea that led directly to the concept of a geomagic square.\nIt turned out to be an unexpected consequence of this find that traditional magic squares now became revealed as one-dimensional geomagic squares.\n\nOther researchers also took notice. Charles Ashbacher, co-editor of the Journal of Recreational Mathematics, speaks of the field of magic squares being \"dramatically expanded\" Peter Cameron, winner of the London Mathematical Society's Whitehead Prize and joint winner of the Euler Medal, called geomagic squares \"a wonderful new piece of recreational maths, which will delight non-mathematicians and give mathematicians food for thought.\" Mathematics writer Alex Bellos said, \"To come up with this after thousands of years of study of magic squares is pretty amazing.\" It may be asked whether geomagic squares might have applications outside the study of puzzles. Cameron is convinced of it, saying, \"I can immediately see a lot of things I'd like to do with this.\"\n\nTrivial examples excepted, there are no known easy methods for producing geomagic squares. To date, two approaches have been explored. Where the pieces to be used are \"polyforms\", or shapes built up from repeated units, an exhaustive search by computer becomes possible.\n\nIn the case of Figure 1, for instance, a first step would be to decide on the piece sizes to be used (in this case all the same), and the shape of the desired target. An initial program would then be able to generate a list \"L\" corresponding to every possible tiling of this target shape by 3 distinct decominoes (polyominoes of size 10). Each decomino is represented by a unique integer, so that \"L\" will consist of a list of integer triads. A subsequent routine can then run through and test every combination of three different triads in turn. The test will consist in treating the candidate triads as the row entries in a 3 × 3 square, and then checking to see whether the columns and diagonals thus formed each contain 3 integers that are also in \"L\"—which is to say, are also target-tiling triads. If so, a 3 × 3 geomagic square using 9 decominoes and selected target has been identified. If this fails, alternative target shapes can be tried. An elaborated version of the same method can be used to search for larger squares, or for squares including differently-sized pieces.\n\nAn alternative method of construction begins with a trivial geomagic square showing repeated pieces, the shapes of which are then modified so as to render each distinct, but without disrupting the square's magic property. This is achieved by means of an algebraic template such as seen below, the distinct variables in which are then interpreted as different shapes to be either appended to or excised from the initial pieces, depending on their sign.\n\nFigure 4 illustrates such a geometrical interpretation of the template in which\n\"k\" is interpreted as a small square shape, while \"a\",\"b\",\"c\" and \"d\" represent the protrusions (+) and/or indentations (-) by means of which it becomes modified so as to result in 16 distinct jigsaw pieces.\n\nContrary to the impression made at first sight, it is a misunderstanding to regard the term 'geomagic square' as referring to some category of magic square. In fact the exact opposite is the case: every (additive) magic square is a particular instance of a geomagic square, but never vice versa. The point is made clear by the example below that appears in a wide-ranging article on geomagic squares by Jean-Paul Delahaye in \"\", the French version of \"Scientific American\". In this case the target \"shape\" for the geomagic square at right is simply a one dimensional line segment 15 units long, the pieces again being no more than straight line segments. As such, the latter is obviously a straightforward translation into geometrical terms of the numerical magic square at left.\n\nAs Delahaye says, \"This example shows that the geomagic square concept generalizes magic squares. The result here is hardly spectacular, but happily there are other geomagic squares that are not the result of such a translation.\"\n\nThe point being that every numerical magic square can be understood as a one-dimensional geomagic square as above. Or as Sallows himself puts it, \"Traditional magic squares featuring numbers are then revealed as that particular case of 'geomagic' squares in which the elements are all one-dimensional.\" This however does not exhaust the 1D case, because there exist 1D geomagic squares whose components are \"disconnected\" line segments, and which do not correspond to any numerical magic square. Thus, even in dimension one, the traditional types correspond to only a tiny subset of all geometric magic squares.\n\nThe richer structure of geomagic squares is reflected in the existence of specimens showing a far greater degree of 'magic' than is possible with numerical types. Thus a \"panmagic square\" is one in which every diagonal, including the so-called \"broken diagonals\", shares the same magic property as the rows and columns. However, it is easily shown that a panmagic square of size 3 × 3 is impossible to construct with numbers, whereas a geometric example can be seen in Figure 3. No comparable example using connected pieces has yet been reported.\nIn addition to being geomagic, there exist squares with auxiliary properties making them even more distinctive. In Figure 6, for example, which is magic on rows and columns only, the 16 pieces form a so-called \"Self-tiling tile set\". Such a set is defined as any set of \"n\" distinct shapes, each of which can be tiled by smaller replicas of the complete set of \"n\" shapes.\n\nA second example is Figure 4, which is a so-called 'self-interlocking' geomagic square. Here the 16 pieces are no longer contained within separate cells, but define the square cell shapes themselves, so as to mesh together to complete a square-shaped jigsaw.\n\nOn October 9, 2014 the post office of Macau issued a series of stamps based on magic squares. The stamp below, showing one of the geomagic squares created by Sallows, was chosen to be in this collection.\n\n\n"}
{"id": "49809461", "url": "https://en.wikipedia.org/wiki?curid=49809461", "title": "Giovanni Giacomo Pierantoni", "text": "Giovanni Giacomo Pierantoni\n\nGiovanni Giacomo Pierantoni (... – 17th century) was an Italian mathematician.\n"}
{"id": "1372255", "url": "https://en.wikipedia.org/wiki?curid=1372255", "title": "Highly cototient number", "text": "Highly cototient number\n\nIn number theory, a branch of mathematics, a highly cototient number is a positive integer \"k\" which is above 1 and has more solutions to the equation \n\nthan any other integer below \"k\" and above 1. Here, φ is Euler's totient function. There are infinitely many solutions to the equation for \"k\" = 1 so this value is excluded in the definition. The first few highly cototient numbers are:\n\nMany of the highly cototient numbers are odd. In fact, after 8, all the numbers listed above are odd, and after 167 all the numbers listed above are congruent to 29 modulo 30.\n\nThe concept is somewhat analogous to that of highly composite numbers. Just as there are infinitely many highly composite numbers, there are also infinitely many highly cototient numbers. Computations become harder, since integer factorization does, as the numbers get larger.\n\nThe cototient of \"x\" is defined as \"x\" – φ(\"x\"), i.e. the number of positive integers less than or equal to \"x\" that have at least one prime factor in common with \"x\". For example, the cototient of 6 is 4 since these four positive integers have a prime factor in common with 6: 2, 3, 4, 6. The cototient of 8 is also 4, this time with these integers: 2, 4, 6, 8. There are exactly two numbers, 6 and 8, which have cototient 4. There are fewer numbers which have cototient 2 and cototient 3 (one number in each case), so 4 is a highly cototient number.\n\nThe first few highly cototient numbers which are primes are \n\n"}
{"id": "37009413", "url": "https://en.wikipedia.org/wiki?curid=37009413", "title": "Inertial manifold", "text": "Inertial manifold\n\nIn mathematics, inertial manifolds are concerned with the long term behavior of the solutions of dissipative dynamical systems. Inertial manifolds are finite-dimensional, smooth, invariant manifolds that contain the global attractor and attract all solutions exponentially quickly. Since an inertial manifold is finite-dimensional even if the original system is infinite-dimensional, and because most of the dynamics for the system takes place on the inertial manifold, studying the dynamics on an inertial manifold produces a considerable simplification in the study of the dynamics of the original system.\n\nIn many physical applications, inertial manifolds express an interaction law between the small and large wavelength structures. Some say that the small wavelengths are enslaved by the large (e.g. synergetics). Inertial manifolds may also appear as slow manifolds common in meteorology, or as the center manifold in any bifurcation. Computationally, numerical schemes for partial differential equations seek to capture the long term dynamics and so such numerical schemes form an approximate inertial manifold.\n\nConsider the dynamical system in just two variables formula_1 and formula_2 and with parameter formula_3:\n\nHence the long term behavior of the original two dimensional dynamical system is given by the 'simpler' one dimensional dynamics on the inertial manifold formula_5, namely formula_13.\n\nLet formula_14 denote a solution of a dynamical system. The solution formula_14 may be an evolving vector in formula_16 or may be an evolving function in an infinite-dimensional Banach space formula_17.\n\nIn many cases of interest the evolution of formula_14 is determined as the solution of a differential equation in formula_17, say formula_20 with initial value formula_21.\nIn any case, we assume the solution of the dynamical system can be written in terms of a semigroup operator, or state transition matrix, formula_22 such that formula_23 for all times formula_24 and all initial values formula_25.\nIn some situations we might consider only discrete values of time as in the dynamics of a map.\n\nAn inertial manifold for a dynamical semigroup formula_26 is a smooth manifold formula_5 such that\n\nThe restriction of the differential equation formula_35 to the inertial manifold formula_5 is therefore a well defined finite-dimensional system called the inertial system.\nSubtly, there is a difference between a manifold being attractive, and solutions on the manifold being attractive.\nNonetheless, under appropriate conditions the inertial system possesses so-called asymptotic completeness: that is, every solution of the differential equation has a companion solution lying in formula_5 and producing the same behavior for large time; in mathematics, for all formula_25 there exists formula_39 and possibly a time shift formula_40 such that formula_41 as formula_42.\n\nResearchers in the 2000s generalized such inertial manifolds to time dependent (nonautonomous) and/or stochastic dynamical systems (e.g.)\n\nExistence results that have been proved address inertial manifolds that are expressible as a graph.\nThe governing differential equation is rewritten more specifically in the form formula_43 for unbounded self-adjoint closed operator formula_44 with domain formula_45, and nonlinear operator formula_46.\nTypically, elementary spectral theory gives an orthonormal basis of formula_17 consisting of eigenvectors formula_48: formula_49, formula_50, for ordered eigenvalues formula_51.\n\nFor some given number formula_52 of modes, formula_53 denotes the projection of formula_17 onto the space spanned by formula_55, and formula_56 denotes the orthogonal projection onto the space spanned by formula_57.\nWe look for an inertial manifold expressed as the graph formula_58.\nFor this graph to exist the most restrictive requirement is the spectral gap condition formula_59 where the constant formula_60 depends upon the system.\nThis spectral gap condition requires that the spectrum of formula_44 must contain large gaps to be guaranteed of existence.\n\nSeveral methods are proposed to construct approximations to\ninertial manifolds, including the\nso-called \"intrinsic low-dimensional manifolds\".\n\nThe most popular way to approximate follows from the\nexistence of a graph.\nDefine the formula_52 \"slow variables\" formula_63, and the 'infinite'\n\"fast variables\" formula_64.\nThen project the differential equation\nformula_43 onto both formula_66\nand formula_67 to obtain the coupled system\nformula_68 and\nformula_69.\n\nFor trajectories on the graph of an inertial\nmanifold formula_70, the fast\nvariable formula_71.\nDifferentiating and using the coupled system form gives the\ndifferential equation for the graph:\nThis differential equation is typically solved approximately\nin an asymptotic expansion in 'small' formula_73 to\ngive an invariant manifold model,\nor a nonlinear Galerkin method,\nboth of which use a global basis whereas the so-called\n\"holistic discretisation\" uses a local basis.\nSuch approaches to approximation of inertial manifolds are\nvery closely related to approximating center manifolds\nfor which a web service exists to construct approximations\nfor systems input by a\nuser.\n\n"}
{"id": "2128068", "url": "https://en.wikipedia.org/wiki?curid=2128068", "title": "Information flow (information theory)", "text": "Information flow (information theory)\n\nInformation flow in an information theoretical context is the transfer of information from a variable formula_1 to a variable formula_2 in a given process. Not all flows may be desirable; for example, a system should not leak any secret (partially or not) to public observers.\n\nSecuring the data manipulated by computing systems has been a challenge in the past years. Several methods to limit the information disclosure exist today, such as access control lists, firewalls, and cryptography. However, although these methods do impose limits on the information that is released by a system, they provide no guarantees about information \"propagation\". For example, access control lists of file systems prevent unauthorized file access, but they do not control how the data is used afterwards. Similarly, cryptography provides a means to exchange information privately across a non-secure channel, but no\nguarantees about the confidentiality of the data are given once it is decrypted.\n\nIn low level information flow analysis, each variable is usually assigned a security level. The basic model comprises two distinct levels: low and high, meaning, respectively, publicly observable information, and secret information. To ensure confidentiality, flowing information from high to low variables should not be allowed. On the other hand, to ensure integrity, flows to high variables should be restricted.\n\nMore generally, the security levels can be viewed as a lattice with information flowing only upwards in the lattice.\n\nFor example, considering two security levels formula_3 and formula_4 (low and high), if formula_5, flows from formula_3 to formula_3, from formula_4 to formula_4, and formula_3 to formula_4 would be allowed, while flows from formula_4 to formula_3 would not.\n\nThroughout this article, the following notation is used:\n\nWhere formula_3 and formula_4 are the only two security levels in the lattice being considered.\n\nInformation flows can be divided in two major categories. The simplest one is explicit flow, where some secret is explicitly leaked to a publicly observable variable. In the following example, the secret in the variable \"h\" flows into the publicly observable variable \"l\".\n\nThe other flows fall into the side channel category. For example, in the timing attack or in the power analysis attack, the system leaks information through, respectively, the time or power it takes to perform an action depending on a secret value.\n\nIn the following example, the attacker can deduce if the value of \"h\" is one or not by the time the program takes to finish:\n\nAnother side channel flow is the implicit information flow, which consists in leakage of information through the program control flow. The following program (implicitly) discloses the value of the secret variable \"h\" to the variable \"l\". In this case, since the \"h\" variable is boolean, all the bits of the variable of \"h\" is disclosed (at the end of the program, \"l\" will be 3 if \"h\" is true, and 42 otherwise).\n\nNon-interference is a policy that enforces that an attacker should not be able to distinguish two computations from their outputs if they only vary in their secret inputs.\nHowever, this policy is too strict to be usable in realistic programs. The classic example is a password checker program that, in order to be useful, needs to disclose some secret information: whether the input password is correct or not (note that the information that an attacker learns in case the program \"rejects\" the password is that the attempted password is \"not\" the valid one).\n\nA mechanism for \"information flow control\" is one that enforces information flow policies. Several methods to enforce information flow policies have been proposed. Run-time mechanisms that tag data with information flow labels have been employed at the operating system level and at the programming language level. Static program analyses have also been developed that ensure information flows within programs are in accordance with policies.\n\nBoth static and dynamic analysis for current programming languages have been developed. However, dynamic analysis techniques cannot observe all execution paths, and therefore cannot be both sound and precise. In order to guarantee noninterference, they either terminate executions that might release sensitive information or they ignore updates that might leak information.\n\nA prominent way to enforce information flow policies in a program is through a security type system: that is, a type system that enforces security properties. In such a sound type system, if a program type-checks, it meets the flow policy and therefore contains no improper information flows.\n\nIn a programming language augmented with a security type system every expression carries both a type (such as boolean, or integer) and a security label.\n\nFollowing is a simple security type system from that enforces non-interference.\nThe notation formula_18 means that the expression formula_19 has type formula_20. Similarly, formula_21 means that the command formula_22 is typable in the security context formula_23.\n\nformula_24\n\nformula_25\n\nformula_26\n\nformula_27\n\nWell-typed commands include, for example,\n\nConversely, the program\nis ill-typed, as it will disclose the value of variable formula_30 into formula_31.\n\nNote that the rule formula_32 is a subsumption rule, which means that any command that is of security type formula_33 can be also be formula_34. For example, formula_35 can be both formula_33 and formula_34. This is called polymorphism in type theory. Similarly, the type of an expression formula_19 that satisfies formula_39 can be both formula_33 and formula_34 according to formula_42 and formula_43 respectively.\n\nAs shown previously, non-interference policy is too strict for use in most real-world applications. Therefore, several approaches to allow controlled releases of information have been devised. Such approaches are called information declassification.\n\nRobust declassification requires that an active attacker may not manipulate the system in order to learn more secrets than what passive attackers already know.\n\nInformation declassification constructs can be classified in four orthogonal dimensions: \"What\" information is released, \"Who\" is authorized to access the information, \"Where\" the information is released, and \"When\" is the information released.\n\nA \"what\" declassification policy controls which information (partial or not) may be released to a publicly observable variable.\n\nThe following code example shows a declassify construct from. In this code, the value of the variable \"h\" is explicitly allowed by the programmer to flow into the publicly observable variable \"l\".\n\nA \"who\" declassification policy controls which principals (i.e., who) can access a given piece of information. This kind of policy has been implemented in the Jif compiler.\n\nThe following example allows Bob to share its secret contained in the variable \"b\" with Alice through the commonly accessible variable \"ab\".\n\nA \"where\" declassification policy regulates where the information can be released, for example, by controlling in which lines of the source code information can be released.\n\nThe following example makes use of the flow construct proposed in. This construct takes a flow policy (in this case, variables in H are allowed to flow to variables in L) and a command, which is run under the given flow policy.\n\nA \"when\" declassification policy regulates when the information can be released. Policies of this kind can be used to verify programs that implement, for example, controlled release of secret information after payment, or encrypted secrets which should not be released in a certain time given polynomial computational power.\n\nAn implicit flow occurs when code whose conditional execution is based on private information updates a public variable. This is especially problematic when multiple executions are considered since an attacker could leverage the public variable to infer private information by observing how its value changes over time or with the input.\n\nThe naïve approach consists on enforcing the confidentiality property on all variables whose value is affected by other variables. This method leads to partially leaked information due to on some instances of the application a variable is Low and in others High.\n\nNo sensitive upgrade halts the program whenever a High variable affects the value of a Low variable effectively preventing information leakage. Since it simply looks for expressions where an information leakage might happen without looking at the context it may halt a program that despite having potential information leakage it never actually leaks information.\n\nIn the following example x is High and y is Low.\n\nIn this case the program would be halted since it uses the value of a High variable to change a Low variable despite the program never leaking information.\n\nPermissive-upgrade introduces an extra security class P which will identify information leaking variables. When a High variable affects the value of a Low variable, the latter is labeled P. If a P labeled variable affects a Low variable the program would be halted. \nTo prevent the halting the Low and P variables should be converted to High using a privatization function to ensure no information leakage can occur. On subsequent instances the program will run without interruption.\n\nPrivatization inference extends permissive upgrade to automatically apply the privatization function to any variable that might leak information. \nThis method should be used during testing where it will convert most variables. Once the program moves into production the permissive-upgrade should be used to halt the program in case of an information leakage and the privatization functions can be updated to prevent subsequent leaks.\n\nBeyond applications to programming language, information flow control theories have been applied to OS, Distributed Systems and Cloud Computing.\n\n"}
{"id": "20376027", "url": "https://en.wikipedia.org/wiki?curid=20376027", "title": "Leggett–Garg inequality", "text": "Leggett–Garg inequality\n\nThe Leggett–Garg inequality, named for Anthony James Leggett and Anupam Garg, is a mathematical inequality fulfilled by all macrorealistic physical theories. Here, macrorealism (macroscopic realism) is a classical worldview defined by the conjunction of two postulates:\n\n\nIn quantum mechanics, the Leggett–Garg inequality is violated, meaning that the time evolution of a system cannot be understood classically. The situation is similar to the violation of Bell's inequalities in Bell test experiments which plays an important role in understanding the nature of the Einstein–Podolsky–Rosen paradox. Here quantum entanglement plays the central role.\n\nAs well as Einstein's famous \"God does not play dice\" objection to quantum mechanics, there was Einstein's still more fundamental objection that the Moon is still there when nobody looks. If the violation of the Leggett–Garg inequality can be demonstrated on the \"macroscopic scale\", this would challenge even this notion of realism.\n\nThe simplest form of the Leggett–Garg inequality derives from examining a system that has only two possible states. These states have corresponding measurement values formula_1. The key here is that we have measurements at two different times, and one or more times between the first and last measurement. The simplest example is where the system is measured at three successive times formula_2. Now suppose, for instance, that there is a perfect correlation formula_3 of 1 between times formula_4 and formula_5. That is to say, that for N realisations of the experiment, the temporal correlation reads\n\nWe look at this case in some detail. What can be said about what happens at time formula_7? Well, it is possible that formula_8, so that if the value at\nformula_9, then it is also formula_10 for both times \nformula_11 and formula_5. It is also quite possible that formula_13, so that the value at formula_14 is\nflipped twice, and so has the same value at formula_15 as it did at\nformula_4. So, we can have both formula_17 and \nformula_18 anti-correlated as long as we have formula_18\nand formula_20 anti-correlated. Yet another possibility is \nthat there is no correlation between formula_17 and \nformula_18. That is we could have formula_23.\nSo, although it is known that if formula_1 at formula_25\nit must also be formula_10 at formula_5, the value\nat formula_11 may as well be determined by the toss of a coin.\nWe define formula_29 as formula_30.\nIn these three cases, we have \nformula_31 and formula_32, respectively.\n\nAll that was for 100% correlation between times formula_33 \nand formula_34. In fact, for any correlation between these\ntimes formula_35. To see this, we note that\n\nIt is easily seen that for every realisation formula_37, the term in the\nparentheses must be less than or equal to unity, so that the result for the sum is also less than (or equal to) unity. If we have four distinct times rather than three, we have formula_38 and so on. These are the Leggett–Garg inequalities. They say something definite about the relation between the temporal correlations of formula_39\nand the correlations between successive times in going from the start to the end.\n\nIn the derivations above, it has been assumed that the quantity Q, representing the state of the system, always has a definite value (macrorealism per se) and that its measurement at a certain time does not change this value nor its subsequent evolution (noninvasive measurability). A violation of the Leggett–Garg inequality implies that at least one of these two assumptions fails.\n\nOne of the first proposed experiments for demonstrating a violation of macroscopic realism employs superconducting quantum interference devices. There, using Josephson junctions, one should be able to prepare macroscopic superpositions of left and right rotating macroscopically large electronic currents in a superconducting ring. Under sufficient suppression of decoherence one should be able to demonstrate a violation of the Leggett–Garg inequality. However, some criticism has been raised concerning the nature of indistinguishable electrons in a Fermi sea.\n\nA criticism of some other proposed experiments on the Leggett–Garg inequality is that they do not really show a violation of macrorealism because they are essentially about measuring spins of individual particles. In 2015 Robens \"et al.\" demonstrated an experimental violation of the Leggett–Garg inequality using superpositions of positions instead of spin with a massive particle. At that time, and so far up until today, the Cesium atoms employed in their experiment represent the largest quantum objects which have been used to experimentally test the Leggett–Garg inequality.\n\nThe experiments of Robens \"et al.\" as well as Knee \"et al.\", using ideal negative measurements, also avoid a second criticism (referred to as “clumsiness loophole” ) that has been directed to previous experiments using measurement protocols that could be interpreted as invasive, thereby conflicting with postulate 2.\n\nSeveral other experimental violations have been reported, including in 2016 with neutrino particles using the MINOS dataset.\n\nBrukner and Kofler have also demonstrated that quantum violations can be found for arbitrarily large \"macroscopic\" systems. As an alternative to quantum decoherence, Brukner and Kofler are proposing a solution of the quantum-to-classical transition in terms of \"coarse-grained\" quantum measurements under which usually no violation of the Leggett–Garg inequality can be seen anymore.\n\nExperiments proposed by Mermin and Braunstein and Mann would be better for testing macroscopic realism, but warns that the experiments may be complex enough to admit unforeseen loopholes in the analysis. A detailed discussion of the subject can be found in the review by Emary et al.\n\nThe four-term Leggett–Garg inequality can be seen to be similar to the CHSH inequality. Moreover, \"equalities\" were proposed by Jaeger \"et al.\"\n\n"}
{"id": "13875653", "url": "https://en.wikipedia.org/wiki?curid=13875653", "title": "Line field", "text": "Line field\n\nIn mathematics, a line field on a manifold is a formation of a line being tangent to a manifold at each point, i.e. a section of the line bundle over the manifold. Line fields are of particular interest in the study of complex dynamical systems, where it is conventional to modify the definition slightly.\n\nIn general, let \"M\" be a manifold. A line field on \"M\" is a function \"μ\" that assigns to each point \"p\" of \"M\" a line \"μ\"(\"p\") through the origin in the tangent space T(\"M\"). Equivalently, one may say that \"μ\"(\"p\") is an element of the projective tangent space PT(\"M\"), or that \"μ\" is a section of the projective tangent bundle PT(\"M\").\n\nIn the study of complex dynamical systems, the manifold \"M\" is taken to be a Hersee surface. A line field on a subset \"A\" of \"M\" (where \"A\" is required to have positive two-dimensional Lebesgue measure) is a line field on \"A\" in the general sense above that is defined almost everywhere in \"A\" and is also a measurable function.\n"}
{"id": "3690584", "url": "https://en.wikipedia.org/wiki?curid=3690584", "title": "List of first-order theories", "text": "List of first-order theories\n\nIn mathematical logic, a first-order theory is given by a set of axioms in some\nlanguage. This entry lists some of the more common examples used in model theory and some of their properties.\n\nFor every natural mathematical structure there is a signature σ listing the constants, functions, and relations of the theory together with their valences, so that the object is naturally a σ-structure. Given a signature σ there is a unique first-order language \"L\" that can be used to capture the first-order expressible facts about the σ-structure.\n\nThere are two common ways to specify theories:\n\nAn L theory may:\n\nThe signature of the pure identity theory is empty, with no functions, constants, or relations.\n\nPure identity theory has no (non-logical) axioms. It is decidable.\n\nOne of the few interesting properties that can be stated in the language of pure identity theory is that of being infinite.\nThis is given by an infinite set of axioms stating there are at least 2 elements, there are at least 3 elements, and so on:\nThese axioms define the theory of an infinite set.\n\nThe opposite property of being finite cannot be stated in first-order logic for any theory that has arbitrarily large finite models: in fact any such theory has infinite models by the compactness theorem. In general if a property can be stated by a finite number of sentences of first-order logic then the opposite property can also be stated in first-order logic, but if a property needs an infinite number of sentences then its opposite property cannot be stated in first-order logic.\n\nAny statement of pure identity theory is equivalent to either σ(\"N\") or to ¬σ(\"N\") for some finite subset \"N\" of the non-negative integers, where σ(\"N\") is the statement that the number of elements is in \"N\". It is even possible to describe all possible theories in this language as follows. Any theory is either the theory of all sets of cardinality in \"N\" for some \"finite\" subset \"N\" of the non-negative integers, or the theory of all sets whose cardinality is not in \"N\", for some \"finite or infinite\" subset \"N\" of the non-negative integers. (There are no theories whose models are exactly sets of cardinality \"N\" if \"N\" is an infinite subset of the integers.) The complete theories are the theories of sets of cardinality \"n\" for some finite \"n\", and the theory of infinite sets.\n\nOne special case of this is the inconsistent theory defined by the axiom ∃\"x\" ¬\"x\" = \"x\". It is a perfectly good theory with many good properties: it is complete, decidable, finitely axiomatizable, and so on. The only problem is that it has no models at all. By Gödel's completeness theorem, it is the only theory (for any given language) with no models. It is not the same as the theory of the empty set (in versions of first-order logic that allow a model to be empty): the theory of the empty set has exactly one model, which has no elements.\n\nA set of unary relations \"P\" for \"i\" in some set \"I\" is called independent if for every two disjoint finite subsets \"A\" and \"B\" of \"I\" there is some element \"x\" such that \"P\"(\"x\") is true for \"i\" in \"A\" and false for \"i\" in \"B\". Independence can be expressed by a set of first-order statements.\n\nThe theory of a countable number of independent unary relations is complete, but has no atomic models. It is also an example of a theory that is superstable but not totally transcendental.\n\nThe signature of equivalence relations has one binary infix relation symbol ~, no constants, and no functions. Equivalence relations satisfy the axioms:\n\nSome first order properties of equivalence relations are:\n\nThe theory of an equivalence relation with exactly 2 infinite equivalence classes is an easy example of a theory which is ω-categorical but not categorical for any larger cardinal.\n\nThe equivalence relation ~ should not be confused with the identity symbol '=': if \"x\"=\"y\" then \"x\"~\"y\", but the converse is not necessarily true. Theories of equivalence relations are not all that difficult or interesting, but often give easy examples or counterexamples for various statements.\n\nThe following constructions are sometimes used to produce examples of theories with certain spectra; in fact by applying them to a small number of explicit theories \"T\" one gets examples of complete countable theories with all possible uncountable spectra. If \"T\" is a theory in some language, we define a new theory 2 by adding a new binary relation to the language, and adding axioms stating that it is an equivalence relation, such that there are an infinite number of equivalence classes all of which are models of \"T\". It is possible to iterate this construction transfinitely: given an ordinal α, define a new theory by adding an equivalence relation \"E\" for each β<α, together with axioms stating that whenever β<γ then each \"E\" equivalence class is the union of infinitely many \"E\" equivalence classes, and each \"E\" equivalence class is a model of \"T\". Informally, one can visualize models of this theory as infinitely branching trees of height α with models of \"T\" attached to all leaves.\n\nThe signature of orders has no constants or functions, and one binary relation symbols ≤. (It is of course possible to use ≥, < or > instead as the basic relation, with the obvious minor changes to the axioms.)\nWe define \"x\" ≥ \"y\", \"x\" < \"y\", \"x\" > \"y\" as abbreviations for \"y\" ≤ \"x\", \"x\" ≤ \"y\" ∧¬\"y\" ≤ \"x\", \"y\" < \"x\",\n\nSome first-order properties of orders: \nThe theory DLO of \"dense linear orders without endpoints\" (i.e. no smallest or largest element) is complete, ω-categorical, but not categorical for any uncountable cardinal. There are 3 other very similar theories: the theory of dense linear orders with a:\n\nBeing well ordered (\"any non-empty subset has a minimal element\") is not a first-order property; the usual definition involves quantifying over all \"subsets\".\n\nLattices can be considered either as special sorts of partially ordered sets, with a signature consisting of one binary relation symbol ≤, or as algebraic structures with a signature consisting of two binary operations ∧ and ∨. The two approaches can be related by defining \"a\"≤ \"b\" to mean \"a\"∧\"b\"=\"a\".\n\nFor two binary operations the axioms for a lattice are:\nFor one relation ≤ the axioms are:\n\nFirst order properties include:\n\nHeyting algebras can be defined as lattices with certain extra first-order properties.\n\nCompleteness is not a first order property of lattices.\n\nThe signature of graphs has no constants or functions, and one binary relation symbol \"R\", where \"R\"(\"x\",\"y\") is read as \"there is an edge from \"x\" to \"y\"\".\n\nThe axioms for the theory of graphs are\n\nThe \"theory of random graphs\" has the following extra axioms for each positive integer \"n\":\n\nThe theory of random graphs is ω categorical, complete, and decidable, and its countable model is called the Rado graph. A statement in the language of graphs is true in this theory if and only if the probability that an \"n\"-vertex random graph models the statement tends to 1 in the limit as \"n\" goes to infinity.\n\nThere are several different signatures and conventions used for Boolean algebras: \n\nThe axioms are:\n\nTarski proved that the theory of Boolean algebras is decidable.\n\nWe write \"x\" ≤ \"y\" as an abbreviation for \"x\" ∧ \"y\" = \"x\", and atom(\"x\") as an abbreviation for ¬\"x\" = 0 ∧ ∀\"y\" \"y\"≤\"x\" → \"y\" = 0 ∨ \"y\" = \"x\", read as \"\"x\" is an atom\", in other words a non-zero element with nothing between it and 0. Here are some first-order properties of Boolean algebras:\nThe theory of atomless Boolean algebras is ω-categorical and complete.\n\nFor any Boolean algebra \"B\", there are several invariants defined as follows. \n\nThen two Boolean algebras are elementarily equivalent if and only if their invariants \"l\", \"m\", and \"n\" are the same. In other words, the values of these invariants classify the possible completions of the theory of Boolean algebras. So the possible complete theories are:\n\nThe signature of group theory has one constant 1 (the identity), one function of arity 1\n(the inverse) whose value on \"t\" is denoted by \"t\", and one function \nof arity 2, which is usually omitted from terms. For any integer \"n\". \"t\" is an abbreviation for the obvious term for the \"n\"th power of \"t\".\n\nGroups are defined by the axioms\n\nSome properties of groups that can be defined in the first-order language of groups are:\n\nThe theory of abelian groups is decidable. The theory of infinite divisible torsion-free abelian groups is complete, as is the theory of infinite abelian groups of exponent p (for \"p\" prime).\n\nThe theory of finite groups is the set of first-order statements in the language of groups that are true in all finite groups (there are plenty of infinite models of this theory). It is not completely trivial to find any such statement that is not true for all groups: one example is \n\"given two elements of order 2, either they are conjugate or there is a non-trivial element commuting with both of them\".\n\nThe properties of being finite, or free, or simple, or torsion are not first-order. More precisely, the first-order theory of all groups with one of these properties has models that do not have this property.\n\nThe signature of (unital) rings has 2 constants 0 and 1, two binary functions + and ×, and, optionally, one unary negation function −.\n\nRings\n\nAxioms: Addition makes the ring into an abelian group, multiplication is associative and has an identity 1, and multiplication is left and right distributive.\n\nCommutative rings\n\nThe axioms for rings plus ∀\"x\" ∀\"y\" \"xy\"=\"yx\".\n\nFields\n\nThe axioms for commutative rings plus ∀\"x\" ¬ \"x\"=0 → ∃\"y\" \"xy\"=1 and ¬ 1=0. Many of the examples given here have only universal, or \"algebraic\" axioms. The class of structures satisfying such a theory has the property of being closed under substructure. For example, a subset of a group closed under the group actions of multiplication and inverse is again a group. Since the signature of fields does not usually include multiplicative and additive inverse, the axioms for inverses are not universal, and therefore a substructure of a field closed under addition and multiplication is not always a field. This can be remedied by adding unary inverse functions to the language.\n\nFor any positive integer \"n\" the property that all equations of degree \"n\" have a root can be expressed by a single first-order sentence:\n\nPerfect fields\n\nThe axioms for fields, plus axioms for each prime number \"p\" stating that if p 1 = 0 (i.e. the field has characteristic \"p\"), then every field element has a \"p\"th root.\n\nAlgebraically closed fields of characteristic \"p\"\n\nThe axioms for fields, plus for every positive \"n\" the axiom that all polynomials of degree \"n\" have a root, plus axioms fixing the characteristic. The classical examples of complete theories. Categorical in all uncountable cardinals. The theory \"ACF\" has a \"universal domain property\", in the sense that every structure \"N\" satisfying the universal axioms of \"ACF\" is a substructure of a sufficiently large algebraically closed field formula_5, and additionally any two such embeddings \"N\" → \"M\" induce an automorphism of \"M\".\n\nFinite fields\n\nThe theory of finite fields is the set of all first-order statements that are true in all finite fields. Significant examples of such statements can, for example, be given by applying the Chevalley–Warning theorem, over the prime fields. The name is a little misleading as the theory has plenty of infinite models. Ax proved that the theory is decidable.\n\nFormally real fields\n\nThese are fields with the axiom \n\nReal closed fields\n\nAxioms: *∀\"x\" ∃\"y\" \"x\"=\"yy\" ∨ \"x\"+\"yy\"= 0.\n\nThe theory of real closed fields is effective and complete and therefore decidable (the Tarski–Seidenberg theorem). The addition of further function symbols (e.g., the exponential function, the sine function) may change decidability. \n\nAxioms for various systems of geometry usually use a typed language, with the different types corresponding to different geometric objects such as points, lines, circles, planes, and so on. The signature will often consist of binary incidence relations between objects of different types; for example, the relation that a point lies on a line. The signature may have more complicated relations; for example ordered geometry might have a ternary \"betweenness\" relation for 3 points, which says whether one lies between two others, or a \"congruence\" relation between 2 pairs of points.\n\nSome examples of axiomatized systems of geometry include ordered geometry, absolute geometry, affine geometry, Euclidean geometry, projective geometry, and hyperbolic geometry. For each of these geometries there are many different and inequivalent systems of axioms for various dimensions. Some of these axiom systems include \"completeness\" axioms that are not first order.\n\nAs a typical example, the axioms for projective geometry use 2 types, points and lines, and a binary incidence relation between points and lines. If point and line variables are indicated by small and capital letter, and \"a\" incident to \"A\" is written as \"aA\", then one set of axioms is\n\nEuclid did not state all the axioms for Euclidean geometry explicitly, and the first complete list was given by Hilbert in Hilbert's axioms. This is not a first order axiomatization as one of Hilbert's axioms is a second order completeness axiom. Tarski's axioms are a first order axiomatization of Euclidean geometry. Tarski showed this axiom system is complete and decidable by relating it to the complete and decidable theory of real closed fields.\n\nThe signature is that of fields (0, 1, +, −, ×) together with a unary function ∂, the derivation.\nThe axioms are those for fields together with\nFor this theory one can add the condition that the characteristic is \"p\", a prime or zero,\nto get the theory DF of differential fields of characteristic \"p\" (and similarly with the other theories below).\n\nIf \"K\" is a differential field then the field of constants formula_12\nThe theory of differentially perfect fields is the theory of differential fields together with the condition that the field of constants is perfect; in other words, for each prime \"p\" it has the axiom:\nFor technical reasons to do with quantifier elimination, it is sometimes more convenient to force the constant field to be perfect by adding a new symbol \"r\" to the signature with the axioms\n\nThe theory of the natural numbers with a successor function has signature consisting of a constant 0 and a unary function \"S\" (\"successor\": \"S\"(\"x\") is interpreted as \"x\"+1), and has axioms:\nThe last axiom (induction) can be replaced by the axioms\n\nThe theory of the natural numbers with a successor function is complete and decidable, and is κ-categorical for uncountable κ but not for countable κ.\n\nPresburger arithmetic is the theory of the natural numbers under addition, with signature consisting of a constant 0, a unary function \"S\", and a binary function +. It is complete and decidable. The axioms are \n\nMany of the first order theories described above can be extended to complete recursively enumerable consistent theories. This is no longer true for most of the following theories; they can usually encode both multiplication and addition of natural numbers, and this gives them enough power to encode themselves, which implies that Gödel's incompleteness theorem applies and the theories can no longer be both complete and recursively enumerable (unless they are inconsistent).\n\nThe signature of a theory of arithmetic has:\nSome authors take the signature to contain a constant 1 instead of the function \"S\", then define \"S\" in the obvious way as \"St\" = 1 + \"t\".\n\nRobinson arithmetic (also called Q). Axioms (1) and (2) govern the distinguished element 0. (3) assures that \"S\" is an injection. Axioms (4) and (5) are the standard recursive definition of addition; (6) and (7) do the same for multiplication. Robinson arithmetic can be thought of as Peano arithmetic without induction. Q is a weak theory for which Gödel's incompleteness theorem holds.\nAxioms:\n\nIΣ is first order Peano arithmetic with induction restricted to Σ formulas (for \"n\" = 0, 1, 2, ...). The theory IΣ is often denoted by IΔ. This is a series of more and more powerful fragments of Peano arithmetic. The case \"n\" = 1 has about the same strength as primitive recursive arithmetic (PRA).\nExponential function arithmetic (EFA) is IΣ with an axiom stating that \"x\" exists for all \"x\" and \"y\" (with the usual properties).\n\nFirst order Peano arithmetic, PA. The \"standard\" theory of arithmetic. The axioms are the axioms of Robinson arithmetic above, together with the axiom scheme of induction:\nKurt Gödel's 1931 paper proved that PA is incomplete, and has no consistent recursively enumerable completions.\n\nComplete arithmetic (also known as true arithmetic) is the theory of the standard model of arithmetic, the natural numbers N. It is complete but does not have a recursively enumerable set of axioms.\n\nFor the real numbers, the situation is slightly different: The case that includes just addition and multiplication cannot encode the integers, and hence Gödel's incompleteness theorem does not apply. Complications arise when adding further function symbols (e.g., exponentiation).\n\nSecond-order arithmetic can refer to a first order theory (in spite of the name) with two types of variables, thought of as varying over integers and subsets of the integers. (There is also a theory of arithmetic in second order logic that is called second order arithmetic. It has only one model, unlike the corresponding theory in first order logic, which is incomplete.) The signature will typically be the signature 0, \"S\", +, × of arithmetic, together with a membership relation ∈ between integers and subsets (though there are numerous minor variations). The axioms are those of Robinson arithmetic, together with axiom schemes of induction and comprehension.\n\nThere are many different subtheories of second order arithmetic that differ in which formulas are allowed in the induction and comprehension schemes. \nIn order of increasing strength, five of the most common systems\nare \nThese are defined in detail in the articles on second order arithmetic and reverse mathematics.\n\nThe usual signature of set theory has one binary relation ∈, no constants, and no functions. Some of the theories below are \"class theories\" which have two sorts of object, sets and classes. There are three common ways of handling this in first-order logic:\n\nSome first order set theories include:\n\nSome extra first order axioms that can be added to one of these (usually ZF) include:\n\n\n"}
{"id": "8864685", "url": "https://en.wikipedia.org/wiki?curid=8864685", "title": "Magnus Wenninger", "text": "Magnus Wenninger\n\nFather Magnus J. Wenninger OSB (October 31, 1919– February 17, 2017) was an American mathematician who worked on constructing polyhedron models, and wrote the first book on their construction.\n\nBorn to German immigrants in Park Falls, Wisconsin, Joseph Wenninger always knew he was going to be a priest. From an early age, it was understood that his brother Heinie would take after their father and become a baker, and that Joe, as he was then known, would go into the priesthood.\n\nWhen Wenninger was thirteen, after graduating from the parochial school in Park Falls, Wisconsin, his parents saw an advertisement in the German newspaper \"Der Wanderer\" that would help to shape the rest of his life. The ad was for a preparatory school in Collegeville, Minnesota, associated with the Benedictine St. John’s University.\n\nWhile admitting to feeling homesick at first, Wenninger quickly made friends and, after a year, knew that this was where he needed to be. He was a student in a section of the prep school that functioned as a \"minor seminary\" – later moving on into St. John’s where he studied philosophy and theology, which led into the priesthood.\n\nWhen Fr. Wenninger became a Benedictine monk, he took on his monastic name Magnus, meaning \"Great\". At the start of his career, Wenninger did not set out on a path one might expect would lead to his becoming the great polyhedronist that he is known as today. Rather, a few chance happenings and seemingly minor decisions shaped a course for Wenninger that led to his groundbreaking studies.\n\nShortly after becoming a priest, Wenninger’s Abbot informed him that their order was starting up a school in the Bahamas. It was decided that Wenninger would be assigned to teach at that school. In order to do this, it was necessary that he get a master's degree. Wenninger was sent to the University of Ottawa, in Canada, to study educational psychology. There he studied symbolic logic under Thomas Greenwood of the philosophy department. His thesis title was \"The Concept of Number According to Roger Bacon and Albert the Great\".\n\nAfter completing his degree, Wenninger went to the school in the Bahamas, where he was asked by the headmaster to choose between teaching English or math. Wenninger chose math, since it seemed to be more in line with the topic of his MA thesis. However, not having taken many math courses in college, Wenninger admits to being able to teach by staying a few pages ahead of the students. He taught Algebra, Euclidean Geometry, Trigonometry and Analytic Geometry.\n\nAfter ten years of teaching, Wenninger felt he was becoming a bit stale. At the suggestion of his headmaster, Wenninger attended the Columbia Teachers College in summer sessions over a four-year period in the late fifties. It was here that his interest in the \"New Math\" was formed and his studies of the polyhedra began.\n\nWenninger died at the age of 97, at St John's Abbey on Friday, February 17, 2017.\n\nWenninger’s first publication on the topic of polyhedra was the booklet entitled, \"Polyhedron Models for the Classroom\", which he wrote in 1966. He wrote to H. S. M. Coxeter and received a copy of \"Uniform polyhedra \"which had a complete list of all 75 uniform polyhedra.\" \"After this, he spent a great deal of time building various polyhedra. He made 65 of them and had them on display in his classroom. At this point, Wenninger decided to contact a publisher to see if there was any interest in a book. He had the models photographed and wrote the accompanying text, which he sent off to Cambridge University Press in London. The publishers indicated an interest in the book only if Wenninger built all 75 of the uniform polyhedra.\n\nWenninger did complete the models, with the help of R. Buckley of Oxford University who had done the calculations for the snub forms by computer. This allowed Wenninger to build these difficult polyhedra with the exact measurements for lengths of the edges and shapes of the faces. This was the first time that all of the uniform polyhedra had been made as paper models. This project took Wenninger nearly ten years, and the book, \"Polyhedron Models\", was published by the Cambridge University Press in 1971, largely due to the exceptional photographs taken locally in Nassau.\n\nFrom 1971 onward, Wenninger focused his attention on the projection of the uniform polyhedra onto the surface of their circumscribing spheres. This led to the publication of his second book, \"Spherical Models\" in 1979, showing how regular, or semiregular, polyhedron can be used to build geodesic domes. He also exchanged ideas with other mathematicians, Hugo Verheyen and Gilbert Fleurent.\n\nIn 1981, Wenninger left the Bahamas and returned to St. John’s Abbey. His third book, \"Dual Models\", appeared in 1983. The book is a sequel to \"Polyhedron Models\", since it includes instructions on how to make paper models of the duals of all 75 uniform polyhedra.\n\n\n\n\nComplete publications (Arranged chronologically):\n\n\n\n"}
{"id": "18644", "url": "https://en.wikipedia.org/wiki?curid=18644", "title": "Median lethal dose", "text": "Median lethal dose\n\nIn toxicology, the median lethal dose, LD (abbreviation for \"lethal dose, 50%\"), LC (lethal concentration, 50%) or LCt is a measure of the lethal dose of a toxin, radiation, or pathogen. The value of LD for a substance is the dose required to kill half the members of a tested population after a specified test duration. LD figures are frequently used as a general indicator of a substance's acute toxicity. A lower LD is indicative of increased toxicity.\n\nThe test was created by J.W. Trevan in 1927. The term semilethal dose is occasionally used in the same sense, in particular with translations of foreign language text, but can also refer to a sublethal dose. LD is usually determined by tests on animals such as laboratory mice.\nIn 2011, the U.S. Food and Drug Administration approved alternative methods to LD for testing the cosmetic drug Botox without animal tests.\n\nThe LD is usually expressed as the mass of substance administered per unit mass of test subject, typically as milligrams of substance per kilogram of body mass, sometimes also stated as nanograms (suitable for botulinum), micrograms, or grams (suitable for paracetamol) per kilogram. Stating it this way allows the relative toxicity of different substances to be compared, and normalizes for the variation in the size of the animals exposed (although toxicity does not always scale simply with body mass). For substances in the environment, such as poisonous vapors or substances in water that are toxic to fish, the concentration in the environment (per cubic metre or per litre) is used, giving a value of LC. But in this case, the exposure time is important (see below).\n\nThe choice of 50% lethality as a benchmark avoids the potential for ambiguity of making measurements in the extremes and reduces the amount of testing required. However, this also means that LD is not the lethal dose for all subjects; some may be killed by much less, while others survive doses far higher than the LD. Measures such as \"LD\" and \"LD\" (dosage required to kill 1% or 99%, respectively, of the test population) are occasionally used for specific purposes.\n\nLethal dosage often varies depending on the method of administration; for instance, many substances are less toxic when administered orally than when intravenously administered. For this reason, LD figures are often qualified with the mode of administration, e.g., \"LD i.v.\"\n\nThe related quantities LD/30 or LD/60 are used to refer to a dose that without treatment will be lethal to 50% of the population within (respectively) 30 or 60 days. These measures are used more commonly within Radiation Health Physics, as survival beyond 60 days usually results in recovery.\n\nA comparable measurement is LCt, which relates to lethal dosage from exposure, where C is concentration and t is time. It is often expressed in terms of mg-min/m. ICt is the dose that will cause incapacitation rather than death. These measures are commonly used to indicate the comparative efficacy of chemical warfare agents, and dosages are typically qualified by rates of breathing (e.g., resting = 10 l/min) for inhalation, or degree of clothing for skin penetration. The concept of Ct was first proposed by Fritz Haber and is sometimes referred to as Haber's Law, which assumes that exposure to 1 minute of 100 mg/m is equivalent to 10 minutes of 10 mg/m (1 × 100 = 100, as does 10 × 10 = 100).\n\nSome chemicals, such as hydrogen cyanide, are rapidly detoxified by the human body, and do not follow Haber's Law. So, in these cases, the lethal concentration may be given simply as LC and qualified by a duration of exposure (e.g., 10 minutes). The Material Safety Data Sheets for toxic substances frequently use this form of the term even if the substance does follow Haber's Law.\n\nFor disease-causing organisms, there is also a measure known as the median infective dose and dosage. The median infective dose (ID) is the number of organisms received by a person or test animal qualified by the route of administration (e.g., 1,200 org/man per oral). Because of the difficulties in counting actual organisms in a dose, infective doses may be expressed in terms of biological assay, such as the number of LD's to some test animal. In biological warfare infective dosage is the number of infective doses per cubic metre of air times the number of minutes of exposure (e.g., ICt is 100 medium doses - min/m).\n\nAs a measure of toxicity, LD is somewhat unreliable and results may vary greatly between testing facilities due to factors such as the genetic characteristics of the sample population, animal species tested, environmental factors and mode of administration.\n\nThere can be wide variability between species as well; what is relatively safe for rats may very well be extremely toxic for humans (\"cf.\" paracetamol toxicity), and vice versa. For example, chocolate, comparatively harmless to humans, is known to be toxic to many animals. When used to test venom from venomous creatures, such as snakes, LD results may be misleading due to the physiological differences between mice, rats, and humans. Many venomous snakes are specialized predators on mice, and their venom may be adapted specifically to incapacitate mice; and mongooses may be exceptionally resistant. While most mammals have a very similar physiology, LD results may or may not have equal bearing upon every mammal species, such as humans, etc.\n\nNote: Comparing substances (especially drugs) to each other by LD can be misleading in many cases due (in part) to differences in effective dose (ED). Therefore, it is more useful to compare such substances by therapeutic index, which is simply the ratio of LD to ED.\n\nThe following examples are listed in reference to LD values, in descending order, and accompanied by LC values, {bracketed}, when appropriate.\n\nAnimal-rights and animal-welfare groups, such as Animal Rights International, have campaigned against LD testing on animals. Several countries, including the UK, have taken steps to ban the oral LD, and the Organisation for Economic Co-operation and Development (OECD) abolished the requirement for the oral test in 2001 (see Test Guideline 401, \"Trends in Pharmacological Sciences\" Vol 22, February 22, 2001).\n\n\n\n"}
{"id": "18908", "url": "https://en.wikipedia.org/wiki?curid=18908", "title": "Mersenne prime", "text": "Mersenne prime\n\nIn mathematics, a Mersenne prime is a prime number that is one less than a power of two. That is, it is a prime number of the form for some integer . They are named after Marin Mersenne, a French Minim friar, who studied them in the early 17th century.\n\nThe exponents which give Mersenne primes are 2, 3, 5, 7, 13, 17, 19, 31, ... and the resulting Mersenne primes are 3, 7, 31, 127, 8191, 131071, 524287, 2147483647, ... .\n\nIf is a composite number then so is . ( is divisible by both and .) This definition is therefore equivalent to a definition as a prime number of the form for some prime .\n\nMore generally, numbers of the form without the primality requirement may be called Mersenne numbers. Sometimes, however, Mersenne numbers are defined to have the additional requirement that be prime.\nThe smallest composite Mersenne number with prime exponent \"n\" is .\n\nMersenne primes are also noteworthy due to their connection to perfect numbers.\n\nA new Mersenne prime was found in December 2017. , 50 are now known. The largest known prime number is a Mersenne prime. Since 1997, all newly found Mersenne primes have been discovered by the Great Internet Mersenne Prime Search (GIMPS), a distributed computing project on the Internet.\n\nMany fundamental questions about Mersenne primes remain unresolved. It is not even known whether the set of Mersenne primes is finite or infinite. The Lenstra–Pomerance–Wagstaff conjecture asserts that there are infinitely many Mersenne primes and predicts their order of growth. It is also not known whether infinitely many Mersenne numbers with prime exponents are composite, although this would follow from widely believed conjectures about prime numbers, for example, the infinitude of Sophie Germain primes congruent to 3 (mod 4). For these primes , (which is also prime) will divide , e.g., , , , , , , , and . Since for these primes , is congruent to 7 mod 8, so 2 is a quadratic residue mod , and the multiplicative order of 2 mod must divide formula_1 = . Since is a prime, it must be or 1. However, it cannot be 1 since formula_2 and 1 has no prime factors, so it must be . Hence, divides formula_3 and = cannot be prime.\n\nThe first four Mersenne primes are , , and and because the first Mersenne prime starts at , all Mersenne primes are congruent to 3 (mod 4). Other than and , all other Mersenne numbers are also congruent to 3 (mod 4). Consequently, in the prime factorization of a Mersenne number (  ) there must be at least one prime factor congruent to 3 (mod 4).\n\nA basic theorem about Mersenne numbers states that if is prime, then the exponent must also be prime. This follows from the identity\n\nThis rules out primality for Mersenne numbers with composite exponent, such as .\n\nThough the above examples might suggest that is prime for all primes , this is not the case, and the smallest counterexample is the Mersenne number\n\nThe evidence at hand suggests that a randomly selected Mersenne number is much more likely to be prime than an arbitrary randomly selected odd integer of similar size. Nonetheless, prime values of appear to grow increasingly sparse as increases. For example, eight of the first 11 primes give rise to a Mersenne prime (the correct terms on Mersenne's original list), while is prime for only 43 of the first two million prime numbers (up to 32,452,843).\n\nThe lack of any simple test to determine whether a given Mersenne number is prime makes the search for Mersenne primes a difficult task, since Mersenne numbers grow very rapidly. The Lucas–Lehmer primality test (LLT) is an efficient primality test that greatly aids this task, making it much easier to test the primality of Mersenne numbers than that of most other numbers of the same size. The search for the largest known prime has somewhat of a cult following. Consequently, a lot of computer power has been expended searching for new Mersenne primes, much of which is now done using distributed computing.\n\nMersenne primes are used in pseudorandom number generators such as the Mersenne twister, Park–Miller random number generator, Generalized Shift Register and Fibonacci RNG.\n\nMersenne primes are also noteworthy due to their connection with perfect numbers. In the 4th century BC, Euclid proved that if is prime, then ) is a perfect number. This number, also expressible as , is the th triangular number and the th hexagonal number. In the 18th century, Leonhard Euler proved that, conversely, all even perfect numbers have this form. This is known as the Euclid–Euler theorem. It is unknown whether there are any odd perfect numbers.\n\nMersenne primes take their name from the 17th-century French scholar Marin Mersenne, who compiled what was supposed to be a list of Mersenne primes with exponents up to 257. The exponents listed by Mersenne were as follows:\n\nHis list replicated the known primes of his time with exponents up to 19. His next entry, 31, was correct, but the list then became largely incorrect, as Mersenne mistakenly included and (which are composite) and omitted , , and (which are prime). Mersenne gave little indication how he came up with his list.\n\nÉdouard Lucas proved in 1876 that is indeed prime, as Mersenne claimed. This was the largest known prime number for 75 years, and the largest ever found by hand. was determined to be prime in 1883 by Ivan Mikheevich Pervushin, though Mersenne claimed it was composite, and for this reason it is sometimes called Pervushin's number. This was the second-largest known prime number, and it remained so until 1911. Lucas had shown another error in Mersenne's list in 1876. Without finding a factor, Lucas demonstrated that is actually composite. No factor was found until a famous talk by Frank Nelson Cole in 1903. Without speaking a word, he went to a blackboard and raised 2 to the 67th power, then subtracted one. On the other side of the board, he multiplied and got the same number, then returned to his seat (to applause) without speaking. He later said that the result had taken him \"three years of Sundays\" to find. A correct list of all Mersenne primes in this number range was completed and rigorously verified only about three centuries after Mersenne published his list.\n\nFast algorithms for finding Mersenne primes are available, and the seven largest known prime numbers are Mersenne primes.\n\nThe first four Mersenne primes , , and were known in antiquity. The fifth, , was discovered anonymously before 1461; the next two ( and ) were found by Pietro Cataldi in 1588. After nearly two centuries, was verified to be prime by Leonhard Euler in 1772. The next (in historical, not numerical order) was , found by Édouard Lucas in 1876, then by Ivan Mikheevich Pervushin in 1883. Two more ( and ) were found early in the 20th century, by R. E. Powers in 1911 and 1914, respectively.\n\nThe best method presently known for testing the primality of Mersenne numbers is the Lucas–Lehmer primality test. Specifically, it can be shown that for prime , is prime if and only if divides , where and for .\n\nDuring the era of manual calculation, all the exponents up to and including 257 were tested with the Lucas–Lehmer test and found to be composite. A notable contribution was made by retired Yale physics professor Horace Scudder Uhler, who did the calculations for exponents 157, 167, 193, 199, 227, and 229. Unfortunately for those investigators, the interval they were testing contains the largest known gap between Mersenne primes, in relative terms: the next Mersenne prime exponent, 521, would turn out to be more than four times larger than the previous record of 127.\nThe search for Mersenne primes was revolutionized by the introduction of the electronic digital computer. Alan Turing searched for them on the Manchester Mark 1 in 1949, but the first successful identification of a Mersenne prime, , by this means was achieved at 10:00 pm on January 30, 1952 using the U.S. National Bureau of Standards Western Automatic Computer (SWAC) at the Institute for Numerical Analysis at the University of California, Los Angeles, under the direction of Lehmer, with a computer search program written and run by Prof. R. M. Robinson. It was the first Mersenne prime to be identified in thirty-eight years; the next one, , was found by the computer a little less than two hours later. Three more — , ,  — were found by the same program in the next several months. is the first Mersenne prime that is titanic, is the first gigantic, and was the first megaprime to be discovered, being a prime with at least 1,000,000 digits. All three were the first known prime of any kind of that size. The number of digits in the decimal representation of equals , where denotes the floor function (or equivalently ).\n\nIn September 2008, mathematicians at UCLA participating in GIMPS won part of a $100,000 prize from the Electronic Frontier Foundation for their discovery of a very nearly 13-million-digit Mersenne prime. The prize, finally confirmed in October 2009, is for the first known prime with at least 10 million digits. The prime was found on a Dell OptiPlex 745 on August 23, 2008. This was the eighth Mersenne prime discovered at UCLA.\n\nOn April 12, 2009, a GIMPS server log reported that a 47th Mersenne prime had possibly been found. The find was verified on June 12, 2009. The prime is . Although it is chronologically the 47th Mersenne prime to be discovered, it is smaller than the largest known at the time, which was the 45th to be discovered.\n\nOn January 25, 2013, Curtis Cooper, a mathematician at the University of Central Missouri, discovered a 48th Mersenne prime, (a number with 17,425,170 digits), as a result of a search executed by a GIMPS server network.\n\nOn January 19, 2016, Cooper published his discovery of a 49th Mersenne prime, (a number with 22,338,618 digits), as a result of a search executed by a GIMPS server network. This was the fourth Mersenne prime discovered by Cooper and his team in the past ten years.\n\nOn January 3, 2018, it was announced that Jonathan Pace, a 51-year-old electrical engineer living in Germantown, Tennessee, had found a 50th Mersenne prime, (a number with 23,249,425 digits), as a result of a search executed by a GIMPS server network.\n\n\nThe table below lists all known Mersenne primes (sequence () and () in OEIS):\n\nAll Mersenne numbers below the 50th Mersenne prime () have been tested at least once but some have not been double-checked. Primes are not always discovered in increasing order. For example, the 29th Mersenne prime was discovered \"after\" the 30th and the 31st. Similarly, was followed by two smaller Mersenne primes, first 2 weeks later and then 8 months later. was the first discovered prime number with more than 10 million decimal digits.\n\nThe largest known Mersenne prime is also the largest known prime number.\n\nIn modern times, the largest known prime has almost always been a Mersenne prime.\n\nSince they are prime numbers, Mersenne primes are divisible only by 1 and by themselves. However, not all Mersenne numbers are Mersenne primes, and the composite Mersenne numbers may be factored nontrivially. Mersenne numbers are very good test cases for the special number field sieve algorithm, so often the largest number factorized with this algorithm has been a Mersenne number. , 2 − 1 is the record-holder, having been factored with a variant of the special number field sieve that allows the factorization of several numbers at once. See integer factorization records for links to more information. The special number field sieve can factorize numbers with more than one large factor. If a number has only one very large factor then other algorithms can factorize larger numbers by first finding small factors and then making a primality test on the cofactor. , the largest factorization with probable prime factors allowed is , where is a 2,201,714-digit probable prime. It was discovered by Oliver Kruse. , the Mersenne number \"M\" is the smallest composite Mersenne number with no known factors; it has no prime factors below 2.\n\nThe table below shows factorizations for the first 20 composite Mersenne numbers .\n\nThe number of factors for the first 500 Mersenne numbers can be found at .\n\nThe primitive part of Mersenne number is , the th cyclotomic polynomial at 2, they are\n\nBesides, if we notice those prime factors, and delete \"old prime factors\", for example, 3 divides the 2nd, 6th, 18th, 54th, 162nd, ... terms of this sequence, we only allow the 2nd term divided by 3, if we do, they are\n\nThe numbers for which is prime are\n\nThe numbers for which has an only primitive prime factor are\n\nIn the mathematical problem Tower of Hanoi, solving a puzzle with an -disc tower requires steps, assuming no mistakes are made. The number of rice grains on the whole chessboard in the wheat and chessboard problem is .\n\nThe asteroid with minor planet number 8191 is named 8191 Mersenne after Marin Mersenne, because 8191 is a Mersenne prime (3 Juno, 7 Iris, 31 Euphrosyne and 127 Johanna having been discovered and named during the 19th century).\n\nIn geometry, an integer right triangle that is primitive and has its even leg a power of 2 (  ) generates a unique right triangle such that its inradius is always a Mersenne number. For example if the even leg is then because it is primitive it constrains the odd leg to be , the hypotenuse to be and its inradius to be .\n\nA Mersenne–Fermat number is defined as , with prime, natural number, and can be written as , when , it is a Mersenne number, and when , it is a Fermat number, the only known Mersenne–Fermat prime with are\n\nIn fact, , where is the cyclotomic polynomial.\n\nThe simplest generalized Mersenne primes are prime numbers of the form , where is a low-degree polynomial with small integer coefficients. An example is , in this case, , and ; another example is , in this case, , and .\n\nIt is also natural to try to generalize primes of the form to primes of the form (for and ). However (see also theorems above), is always divisible by , so unless the latter is a unit, the former is not a prime. There are two ways to deal with that:\n\nIn the ring of integers (on real numbers), if is a unit, then is either 2 or 0. But are the usual Mersenne primes, and the formula does not lead to anything interesting (since it is always −1 for all ). Thus, we can regard a ring of \"integers\" on complex numbers instead of real numbers, like Gaussian integers and Eisenstein integers.\n\nIf we regard the ring of Gaussian integers, we get the case and , and can ask (WLOG) for what the number is a \"Gaussian prime\" which will then be called a Gaussian Mersenne prime.\n\nThis sequence is in many ways similar to the list of exponents of ordinary Mersenne primes.\n\nThe norms (i.e. squares of absolute values) of these Gaussian primes are rational primes:\n\nWe can also regard the ring of Eisenstein integers, we get the case and , and can ask for what the number is an \"Eisenstein prime\" which will then be called a Eisenstein Mersenne prime.\n\nThe norms (i.e. squares of absolute values) of these Eisenstein primes are rational primes:\n\nThe other way to deal with the fact that is always divisible by , it is to simply take out this factor and ask which values of make\nbe prime. (The integer can be either positive or negative.) If for example we take , we get values of:\nThese primes are called repunit primes. Another example is when we take , we get values of:\nIt is a conjecture that for every integer which is not a perfect power, there are infinitely many values of such that is prime. (When is a perfect power, it can be shown that there is at most one value such that is prime)\n\nLeast such that is prime are (starting with , if no such exists)\n\nFor negative bases , they are (starting with , if no such exists)\n\nLeast base such that is prime are\n\nFor negative bases , they are\n\nAnother generalized Mersenne number is\nwith , any coprime integers, and . (Since is always divisible by , the division is necessary for there to be any chance of finding prime numbers. In fact, this number is the same as the Lucas number , since and are the roots of the quadratic equation , and this number equals 1 when ) We can ask which makes this number prime. It can be shown that such must be primes themselves or equal to 4, and can be 4 if and only if and is prime. (Since . Thus, in this case the pair must be and must be prime. That is, must be in .) It is a conjecture that for any pair such that for every natural number , and are not both perfect th powers, and is not a perfect fourth power. there are infinitely many values of such that is prime. (When and are both perfect th powers for an or when is a perfect fourth power, it can be shown that there are at most two values with this property, since if so, then can be factored algebraically) However, this has not been proved for any single value of .\n\nNote: if and is even, then the numbers are not included in the corresponding OEIS sequence.\n\nA conjecture related to the generalized Mersenne primes: (the conjecture predicts where is the next generalized Mersenne prime, if the conjecture is true, then there are infinitely many primes for all such pairs)\n\nFor any integers and which satisfy the conditions:\n\nhas prime numbers of the form\n\nfor prime , the prime numbers will be distributed near the best fit line\n\nwhere\n\nand there are about\n\nprime numbers of this form less than .\n\n\nWe also have the following three properties:\n\n\nIf this conjecture is true, then for all such pairs, let be the th prime of the form , the graph of versus is almost linear. (See )\n\nWhen , it is , a difference of two consecutive perfect th powers, and if is prime, then must be , because it is divisible by .\n\nLeast such that is prime are\n\nLeast such that is prime are\n\n\n"}
{"id": "344922", "url": "https://en.wikipedia.org/wiki?curid=344922", "title": "Neuroevolution of augmenting topologies", "text": "Neuroevolution of augmenting topologies\n\nNeuroEvolution of Augmenting Topologies (NEAT) is a genetic algorithm (GA) for the generation of evolving artificial neural networks (a neuroevolution technique) developed by Ken Stanley in 2002 while at The University of Texas at Austin. It alters both the weighting parameters and structures of networks, attempting to find a balance between the fitness of evolved solutions and their diversity. It is based on applying three key techniques: tracking genes with history markers to allow crossover among topologies, applying speciation (the evolution of species) to preserve innovations, and developing topologies incrementally from simple initial structures (\"complexifying\").\n\nOn simple control tasks, the NEAT algorithm often arrives at effective networks more quickly than other contemporary neuro-evolutionary techniques and reinforcement learning methods.\n\nTraditionally a neural network topology is chosen by a human experimenter, and effective connection weight values are learned through a training procedure. This yields a situation whereby a trial and error process may be necessary in order to determine an appropriate topology. NEAT is an example of a topology and weight evolving artificial neural network (TWEAN) which attempt to simultaneously learn weight values and an appropriate topology for a neural network.\n\nIn order to encode the network into a phenotype for the GA, NEAT uses a direct encoding scheme which means every connection and neuron is explicitly represented. This is in contrast to indirect encoding schemes which define rules that allow the network to be constructed without explicitly representing every connection and neuron allowing for more compact representation.\n\nThe NEAT approach begins with a perceptron-like feed-forward network of only input neurons and output neurons. As evolution progresses through discrete steps, the complexity of the network's topology may grow, either by inserting a new neuron into a connection path, or by creating a new connection between (formerly unconnected) neurons.\n\nThe competing conventions problem arises when there is more than one way of representing information in a phenotype. For example, if a genome contains neurons A, B and C and is represented by [A B C], if this genome is crossed with an identical genome (in terms of functionality) but ordered [C B A] crossover will yield children that are missing information ([A B A] or [C B C]), in fact 1/3 of the information has been lost in this example. NEAT solves this problem by tracking the history of genes by the use of a global innovation number which increases as new genes are added. When adding a new gene the global innovation number is incremented and assigned to that gene. Thus the higher the number the more recently the gene was added. For a particular generation if an identical mutation occurs in more than one genome they are both given the same number, beyond that however the mutation number will remain unchanged indefinitely.\n\nThese innovation numbers allow NEAT to match up genes which can be crossed with each other.\n\nThe original implementation by Ken Stanley is published under the GPL. It integrates with Guile, a GNU scheme interpreter. This implementation of NEAT is considered the conventional basic starting point for implementations of the NEAT algorithm.\n\nIn 2003 Stanley devised an extension to NEAT that allows evolution to occur in real time rather than through the iteration of generations as used by most genetic algorithms. The basic idea is to put the population under constant evaluation with a \"lifetime\" timer on each individual in the population. When a network's timer expires its current fitness measure is examined to see whether it falls near the bottom of the population, and if so it is discarded and replaced by a new network bred from two high-fitness parents. A timer is set for the new network and it is placed in the population to participate in the ongoing evaluations.\n\nThe first application of rtNEAT is a video game called Neuro-Evolving Robotic Operatives, or NERO. In the first phase of the game, individual players deploy robots in a 'sandbox' and train them to some desired tactical doctrine. Once a collection of robots has been trained, a second phase of play allows players to pit their robots in a battle against robots trained by some other player, to see how well their training regimens prepared their robots for battle.\n\nAn extension of Ken Stanley's NEAT, developed by Colin Green, adds periodic pruning of the network topologies of candidate solutions during the evolution process. This addition addressed concern that unbounded automated growth would generate unnecessary structure.\n\nHyperNEAT is specialized to evolve large scale structures. It was originally based on the CPPN theory and is an active field of research.\n\nContent-Generating NEAT (cgNEAT) evolves custom video game content based on user preferences. The first video game to implement cgNEAT is Galactic Arms Race, a space-shooter game in which unique particle system weapons are evolved based on player usage statistics. Each particle system weapon in the game is controlled by an evolved CPPN, similarly to the evolution technique in the NEAT Particles interactive art program.\n\nodNEAT is an online and decentralized version of NEAT designed for multi-robot systems. odNEAT is executed onboard robots themselves during task execution to continuously optimize the parameters and the topology of the artificial neural network-based controllers. In this way, robots executing odNEAT have the potential to adapt to changing conditions and learn new behaviors as they carry out their tasks. The online evolutionary process is implemented according to a physically distributed island model. Each robot optimizes an internal population of candidate solutions (intra-island variation), and two or more robots exchange candidate solutions when they meet (inter-island migration). In this way, each robot is potentially self-sufficient and the evolutionary process capitalizes on the exchange of controllers between multiple robots for faster synthesis of effective controllers.\n\n\n\n\n"}
{"id": "295917", "url": "https://en.wikipedia.org/wiki?curid=295917", "title": "Noncommutative geometry", "text": "Noncommutative geometry\n\nNoncommutative geometry (NCG) is a branch of mathematics concerned with a geometric approach to noncommutative algebras, and with the construction of \"spaces\" that are locally presented by noncommutative algebras of functions (possibly in some generalized sense). A noncommutative algebra is an associative algebra in which the multiplication is not commutative, that is, for which formula_1 does not always equal formula_2; or more generally an algebraic structure in which one of the principal binary operations is not commutative; one also allows additional structures, e.g. topology or norm, to be possibly carried by the noncommutative algebra of functions.\n\nThe main motivation is to extend the commutative duality between spaces and functions to the noncommutative setting. In mathematics, \"spaces\", which are geometric in nature, can be related to numerical functions on them. In general, such functions will form a commutative ring. For instance, one may take the ring \"C\"(\"X\") of continuous complex-valued functions on a topological space \"X\". In many cases (\"e.g.\", if \"X\" is a compact Hausdorff space), we can recover \"X\" from \"C\"(\"X\"), and therefore it makes some sense to say that \"X\" has \"commutative topology\".\n\nMore specifically, in topology, compact Hausdorff topological spaces can be reconstructed from the Banach algebra of functions on the space (Gel'fand-Neimark). In commutative algebraic geometry, algebraic schemes are locally prime spectra of commutative unital rings (A. Grothendieck), and schemes can be reconstructed from the categories of quasicoherent sheaves of modules on them (P. Gabriel-A. Rosenberg). For Grothendieck topologies, the cohomological properties of a site are invariant of the corresponding category of sheaves of sets viewed abstractly as a topos (A. Grothendieck). In all these cases, a space is reconstructed from the algebra of functions or its categorified version—some category of sheaves on that space.\n\nFunctions on a topological space can be multiplied and added pointwise hence they form a commutative algebra; in fact these operations are local in the topology of the base space, hence the functions form a sheaf of commutative rings over the base space.\n\nThe dream of noncommutative geometry is to generalize this duality to the duality between noncommutative algebras, or sheaves of noncommutative algebras, or sheaf-like noncommutative algebraic or operator-algebraic structures, and geometric entities of certain kinds, and give an interaction between the algebraic and geometric description of those via this duality.\n\nRegarding that the commutative rings correspond to usual affine schemes, and commutative C*-algebras to usual topological spaces, the extension to noncommutative rings and algebras requires non-trivial generalization of topological spaces as \"non-commutative spaces\". For this reason some talk about non-commutative topology, though the term also has other meanings.\n\nSome applications in particle physics are described in the entries Noncommutative standard model and Noncommutative quantum field theory. The sudden rise in interest in noncommutative geometry in physics follows after the speculations of its role in M-theory made in 1997.\n\nSome of the theory developed by Alain Connes to handle noncommutative geometry at a technical level has roots in older attempts, in particular in ergodic theory. The proposal of George Mackey to create a \"virtual subgroup\" theory, with respect to which ergodic group actions would become homogeneous spaces of an extended kind, has by now been subsumed.\n\n(The formal duals of) non-commutative C*-algebras are often now called non-commutative spaces. This is by analogy with the Gelfand representation, which shows that commutative C*-algebras are dual to locally compact Hausdorff spaces. In general, one can associate to any C*-algebra \"S\" a topological space \"Ŝ\"; see spectrum of a C*-algebra.\n\nFor the duality between σ-finite measure spaces and commutative von Neumann algebras, noncommutative von Neumann algebras are called \"non-commutative measure spaces\".\n\nA smooth Riemannian manifold \"M\" is a topological space with a lot of extra structure. From its algebra of continuous functions \"C(M)\" we only recover \"M\" topologically. The algebraic invariant that recovers the Riemannian structure is a spectral triple. It is constructed from a smooth vector bundle \"E\" over \"M\", e.g. the exterior algebra bundle. The Hilbert space \"L(M,E)\" of square integrable sections of \"E\" carries a representation of \"C(M)\" by multiplication operators, and we consider an unbounded operator \"D\" in \"L(M,E)\" with compact resolvent (e.g. the signature operator), such that the commutators \"[D,f]\" are bounded whenever \"f\" is smooth. A recent deep theorem states that \"M\" as a Riemannian manifold can be recovered from this data.\n\nThis suggests that one might define a noncommutative Riemannian manifold as a spectral triple \"(A,H,D)\", consisting of a representation of a \"C*\"-algebra \"A\" on a Hilbert space \"H\", together with an unbounded operator \"D\" on \"H\", with compact resolvent, such that \"[D,a]\" is bounded for all \"a\" in some dense subalgebra of \"A\". Research in spectral triples is very active, and many examples of noncommutative manifolds have been constructed.\n\nIn analogy to the duality between affine schemes and commutative rings, we define a category of noncommutative affine schemes as the dual of the category of associative unital rings. There are certain analogues of Zariski topology in that context so that one can glue such affine schemes to more general objects.\n\nThere are also generalizations of the Cone and of the Proj of a commutative graded ring, mimicking a Serre's theorem on Proj. Namely the category of quasicoherent sheaves of O-modules on a Proj of a commutative graded algebra is equivalent to the category of graded modules over the ring localized on Serre's subcategory of graded modules of finite length; there is also analogous theorem for coherent sheaves when the algebra is Noetherian. This theorem is extended as a definition of noncommutative projective geometry by Michael Artin and J. J. Zhang, who add also some general ring-theoretic conditions (e.g. Artin-Schelter regularity).\n\nMany properties of projective schemes extend to this context. For example, there exist an analog of the celebrated Serre duality for noncommutative projective schemes of Artin and Zhang.\n\nA. L. Rosenberg has created a rather general relative concept of noncommutative quasicompact scheme (over a base category), abstracting the Grothendieck's study of morphisms of schemes and covers in terms of categories of quasicoherent sheaves and flat localization functors. There is also another interesting approach via localization theory, due to Fred Van Oystaeyen, Luc Willaert and Alain Verschoren, where the main concept is that of a schematic algebra.\n\nSome of the motivating questions of the theory are concerned with extending known topological invariants to formal duals of noncommutative (operator) algebras and other replacements and candidates for noncommutative spaces. One of the main starting points of the Alain Connes' direction in noncommutative geometry is his discovery of a new homology theory associated to noncommutative associative algebras and noncommutative operator algebras, namely the cyclic homology and its relations to the algebraic K-theory (primarily via Connes-Chern character map).\n\nThe theory of characteristic classes of smooth manifolds has been extended to spectral triples, employing the tools of operator K-theory and cyclic cohomology. Several generalizations of now classical index theorems allow for effective extraction of numerical invariants from spectral triples. The fundamental characteristic class in cyclic cohomology, the JLO cocycle, generalizes the classical Chern character.\n\n\n\n\n\n"}
{"id": "2151329", "url": "https://en.wikipedia.org/wiki?curid=2151329", "title": "Pascal's simplex", "text": "Pascal's simplex\n\nIn mathematics, Pascal's simplex is a generalisation of Pascal's triangle into arbitrary number of dimensions, based on the multinomial theorem.\n\nLet \"m\" (\"m\" > 0) be a number of terms of a polynomial and \"n\" (\"n\" ≥ 0) be a power the polynomial is raised to.\n\nLet formula_1 denote a Pascal's \"m\"-simplex. Each Pascal's \"m\"-simplex is a semi-infinite object, which consists of an infinite series of its components.\n\nLet formula_2 denote its \"n\" component, itself a finite (\"m − 1\")-simplex with the edge length \"n\", with a notational equivalent formula_3.\n\nformula_4 consists of the coefficients of multinomial expansion of a polynomial with \"m\" terms raised to the power of \"n\":\n\nwhere formula_6.\n"}
{"id": "26953472", "url": "https://en.wikipedia.org/wiki?curid=26953472", "title": "PolyL", "text": "PolyL\n\nIn computational complexity theory, polyL is the complexity class of decision problems that can be solved on a deterministic Turing machine by an algorithm whose space complexity is bounded by a polylogarithmic function in the size of the input. In other words, polyL = DSPACE((log \"n\")), where \"n\" denotes the input size, and O(1) denotes a constant.\n\nJust as L ⊆ P, polyL ⊆ QP. However, the only proven relationship between polyL and P is that polyL ≠ P; it is unknown if polyL ⊊ P, if P ⊊ polyL, or if neither is contained in the other. One proof that polyL ≠ P is that P has a complete problem under logarithmic space many-one reductions but polyL does not due to the space hierarchy theorem.\nThe space hierarchy theorem guarantees that DSPACE(log \"n\") ⊊ DSPACE(log \"n\") for all integers d > 0. If polyL had a complete problem, call it \"A\", it would be an element of DSPACE(log \"n\") for some integer k > 0. Suppose problem \"B\" is an element of DSPACE(log \"n\") \\ DSPACE(log \"n\"). The assumption that \"A\" is complete implies the following O(log \"n\") space algorithm for \"B\": reduce \"B\" to \"A\" in logarithmic space, then decide \"A\" in O(log \"n\") space. This implies that \"B\" is an element of DSPACE(log \"n\") and hence violates the space hierarchy theorem.\n"}
{"id": "3206883", "url": "https://en.wikipedia.org/wiki?curid=3206883", "title": "Predicate abstraction", "text": "Predicate abstraction\n\nIn logic, predicate abstraction is the result of creating a predicate from a sentence. If Q is any formula then the predicate abstract formed from that sentence is (λy.Q), where λ is an abstraction operator and in which every occurrence of y occurs bound by λ in (λy.Q). The resultant predicate (λx.Q(x)) is a monadic predicate capable of taking a term t as argument as in (λx.Q(x))(t), which says that the object denoted by 't' has the property of being such that Q.\n\nThe \"law of abstraction\" states ( λx.Q(x) )(t) ≡ Q(t/x) where Q(t/x) is the result of replacing all free occurrences of x in Q by t. This law is shown to fail in general in at least two cases: (i) when t is irreferential and (ii) when Q contains modal operators.\n\nIn modal logic the \"\"de re\" / \"de dicto\" distinction\" is stated as\n\n1. (DE DICTO): formula_1\n\n2. (DE RE): formula_2.\n\nIn (1) the modal operator applies to the formula A(t) and the term t is within the scope of the modal operator. In (2) t is \"not\" within the scope of the modal operator.\n\nFor the semantics and further philosophical developments of predicate abstraction see Fitting and Mendelsohn, \"First-order Modal Logic\", Springer, 1999.\n"}
{"id": "10570298", "url": "https://en.wikipedia.org/wiki?curid=10570298", "title": "Prékopa–Leindler inequality", "text": "Prékopa–Leindler inequality\n\nIn mathematics, the Prékopa–Leindler inequality is an integral inequality closely related to the reverse Young's inequality, the Brunn–Minkowski inequality and a number of other important and classical inequalities in analysis. The result is named after the Hungarian mathematicians András Prékopa and László Leindler.\n\nLet 0 < \"λ\" < 1 and let \"f\", \"g\", \"h\" : R → [0, +∞) be non-negative real-valued measurable functions defined on \"n\"-dimensional Euclidean space R. Suppose that these functions satisfy\n\nfor all \"x\" and \"y\" in R. Then\n\nRecall that the essential supremum of a measurable function \"f\" : R → R is defined by\n\nThis notation allows the following \"essential form\" of the Prékopa–Leindler inequality: let 0 < \"λ\" < 1 and let \"f\", \"g\" ∈ \"L\"(R; [0, +∞)) be non-negative absolutely integrable functions. Let\n\nThen \"s\" is measurable and\n\nThe essential supremum form was given in. Its use can change the left side of the inequality. For example, a function \"g\" that takes the value 1 at exactly one point will not usually yield a zero left side in the \"non-essential sup\" form but it will always yield a zero left side in the \"essential sup\" form.\n\nIt can be shown that the usual Prékopa–Leindler inequality implies the Brunn–Minkowski inequality in the following form: if 0 < \"λ\" < 1 and \"A\" and \"B\" are bounded, measurable subsets of R such that the Minkowski sum (1 − \"λ\")\"A\" + λ\"B\" is also measurable, then\n\nwhere \"μ\" denotes \"n\"-dimensional Lebesgue measure. Hence, the Prékopa–Leindler inequality can also be used to prove the Brunn–Minkowski inequality in its more familiar form: if 0 < \"λ\" < 1 and \"A\" and \"B\" are non-empty, bounded, measurable subsets of R such that (1 − \"λ\")\"A\" + λ\"B\" is also measurable, then\n\nThe Prékopa–Leindler inequality is useful in the theory of log-concave distributions, as it can be used to show that log-concavity is preserved by marginalization and independent summation of log-concave distributed random variables. Suppose that \"H\"(\"x\",\"y\") is a log-concave distribution for (\"x\",\"y\") ∈ R × R, so that by definition we have\n\nand let \"M\"(\"y\") denote the marginal distribution obtained by integrating over \"x\":\n\nLet \"y\", \"y\" ∈ R and 0 < \"λ\" < 1 be given. Then equation () satisfies condition () with \"h\"(\"x\") = \"H\"(\"x\",(1 − \"λ\")y + \"λy\"), \"f\"(\"x\") = \"H\"(\"x\",\"y\") and \"g\"(\"x\") = \"H\"(\"x\",\"y\"), so the Prékopa–Leindler inequality applies. It can be written in terms of \"M\" as\n\nwhich is the definition of log-concavity for \"M\".\n\nTo see how this implies the preservation of log-convexity by independent sums, suppose that \"X\" and \"Y\" are independent random variables with log-concave distribution. Since the product of two log-concave functions is log-concave, the joint distribution of (\"X\",\"Y\") is also log-concave. Log-concavity is preserved by affine changes of coordinates, so the distribution of (\"X\" + \"Y\", \"X\" − \"Y\") is log-concave as well. Since the distribution of \"X+Y\" is a marginal over the joint distribution of (\"X\" + \"Y\", \"X\" − \"Y\"), we conclude that \"X\" + \"Y\" has a log-concave distribution.\n"}
{"id": "28846642", "url": "https://en.wikipedia.org/wiki?curid=28846642", "title": "Quadruple product", "text": "Quadruple product\n\nIn mathematics, the quadruple product is a product of four vectors in three-dimensional Euclidean space. The name \"quadruple product\" is used for two different products, the scalar-valued scalar quadruple product and the vector-valued vector quadruple product or vector product of four vectors .\n\nThe scalar quadruple product is defined as the dot product of two cross products:\nwhere a, b, c, d are vectors in three-dimensional Euclidean space. It can be evaluated using the identity:\nor using the determinant:\n\nThe vector quadruple product is defined as the cross product of two cross products:\nwhere a, b, c, d are vectors in three-dimensional Euclidean space. It can be evaluated using the identity:\nThis identity can also be written using tensor notation and the Einstein summation convention as follows:\nusing the notation for the triple product:\nwhere the last two forms are determinants with formula_8 denoting unit vectors along three mutually orthogonal directions.\n\nEquivalent forms can be obtained using the identity:\n\nThe quadruple products are useful for deriving various formulas in spherical and plane geometry. For example, if four points are chosen on the unit sphere, \"A, B, C, D\", and unit vectors drawn from the center of the sphere to the four points, a, b, c, d respectively, the identity:\nin conjunction with the relation for the magnitude of the cross product:\nand the dot product:\nwhere \"a = b\" = 1 for the unit sphere, results in the identity among the angles attributed to Gauss:\nwhere \"x\" is the angle between a × b and c × d, or equivalently, between the planes defined by these vectors.\n\nJosiah Willard Gibbs's pioneering work on vector calculus provides several other examples.\n\n"}
{"id": "42116733", "url": "https://en.wikipedia.org/wiki?curid=42116733", "title": "Resolution inference", "text": "Resolution inference\n\nIn propositional logic, a resolution inference is an instance of the following rule: \n\nWe call:\n\n\nThis rule can be generalized to first-order logic to:\n\nwhere formula_9 is a most general unifier of formula_10 and formula_11 and formula_12 and formula_13 have no common variables.\n\nThe clauses formula_14 and formula_15 can apply this rule with formula_16 as unifier.\n\nHere x is a variable and b is a constant.\n\nHere we see that\n\n"}
{"id": "58836710", "url": "https://en.wikipedia.org/wiki?curid=58836710", "title": "Shiri Artstein", "text": "Shiri Artstein\n\nShiri Artstein-Avidan (, born September 28, 1978) is an Israeli mathematician who in 2015 won the Erdős Prize. She specializes in convex geometry and asymptotic geometric analysis, and is a professor of mathematics at Tel Aviv University.\n\nArtstein was born in Jerusalem, the daughter of mathematician Zvi Artstein. She graduated summa cum laude from Tel Aviv University in 2000, with a bachelor's degree in mathematics, and completed her Ph.D. at Tel Aviv University in 2004 under the supervision of Vitali Milman, with a dissertation on \"Entropy Methods\". She worked from 2004 to 2006 as a Veblen Research Instructor in Mathematics at Princeton University and as a researcher at the Institute for Advanced Study before returning to Tel Aviv as a faculty member in 2006.\n\nArtstein won the Haim Nessyahu Prize in Mathematics, an annual dissertation award of the Israel Mathematical Union, in 2006.\nIn 2008 she won the Krill Prize for Excellence in Scientific Research, from the\nWolf Foundation.\nIn 2015 she won the Anna and Lajos Erdős Prize in Mathematics. The award cited her \"solution of Shannon's long standing problem on monotonicity of entropy (with K. Ball, F. Barthe and A. Naor), profound and unexpected development of the concept of duality, Legendre and Fourier transform from axiomatic viewpoint (with V. Milman) and discovery of an astonishing link between Mahler's conjecture in convexity theory and an isoperimetric-type inequality involving symplectic capacities (with R. Karasev and Y. Ostrover)\".\n\nWith Milman and Apostolos Giannopoulos, Artstein is the co-author of the book \"Asymptotic Geometric Analysis, Part I\" (Mathematical Surveys and Monographs 202, American Mathematical Society, 2015).\n\nHer research publications include:\n\n"}
{"id": "25045664", "url": "https://en.wikipedia.org/wiki?curid=25045664", "title": "Sinc numerical methods", "text": "Sinc numerical methods\n\nIn numerical analysis and applied mathematics, sinc numerical methods are numerical techniques for finding approximate solutions of partial differential equations and integral equations based on the translates of sinc function and Cardinal function C(f,h) which is an expansion of f defined by\nwhere the step size h>0 and where the sinc function is defined by\nSinc approximation methods excel for problems whose solutions may have singularities, or infinite domains, or boundary layers.\n\nThe truncated Sinc expansion of f is defined by the following series:\n\nIndeed, Sinc are ubiquitous for approximating every operation of calculus\n\nIn the standard setup of the sinc numerical methods, the errors (in big O notation) are known to be formula_4 with some c>0, where n is the number of nodes or bases used in the methods. However, Sugihara has recently found that the errors in the Sinc numerical methods based on double exponential transformation are formula_5 with some k>0, in a setup that is also meaningful both theoretically and practically and are found to be best possible in a certain mathematical sense.\n\n"}
{"id": "24742938", "url": "https://en.wikipedia.org/wiki?curid=24742938", "title": "Sinkhorn's theorem", "text": "Sinkhorn's theorem\n\nSinkhorn's theorem states that every square matrix with positive entries can be written in a certain standard form.\n\nIf \"A\" is an \"n\" × \"n\" matrix with strictly positive elements, then there exist diagonal matrices \"D\" and \"D\" with strictly positive diagonal elements such that \"D\"\"AD\" is doubly stochastic. The matrices \"D\" and \"D\" are unique modulo multiplying the first matrix by a positive number and dividing the second one by the same number. \n\nA simple iterative method to approach the double stochastic matrix is to alternately rescale all rows and all columns of \"A\" to sum to 1. Sinkhorn and Knopp presented this algorithm and analyzed its convergence.\nThe following analogue for unitary matrices is also true: for every unitary matrix \"U\" there exist two diagonal unitary matrices \"L\" and \"R\" such that \"LUR\" has each of its columns and rows summing to 1.\n\nThe following extension to maps between matrices is also true (see Theorem 5 and also Theorem 4.7): given a Kraus operator\nwhich represents the quantum operation Φ mapping a density matrix into another,\nthat is trace preserving,\nand, in addition, whose range is in the interior of the positive definite cone (strict positivity), there exist scalings \"x\", for \"j\" in {0,1}, that are positive definite so that the rescaled Kraus operator\nis doubly stochastic. In other words, it is such that both,\nas well as for the adjoint,\nwhere I denotes the identity operator.\n"}
{"id": "45289334", "url": "https://en.wikipedia.org/wiki?curid=45289334", "title": "Symbol (number theory)", "text": "Symbol (number theory)\n\nIn number theory, a symbol is any of many different generalizations of the Legendre symbol. This article describes the relations between these various generalizations.\n\nThe symbols below are arranged roughly in order of the date they were introduced, which is usually (but not always) in order of increasing generality.\n\n\n"}
{"id": "153208", "url": "https://en.wikipedia.org/wiki?curid=153208", "title": "System dynamics", "text": "System dynamics\n\nSystem dynamics (SD) is an approach to understanding the nonlinear behaviour of complex systems over time using stocks, flows, internal feedback loops, table functions and time delays.\n\nSystem dynamics is a methodology and mathematical modeling technique to frame, understand, and discuss complex issues and problems. Originally developed in the 1950s to help corporate managers improve their understanding of industrial processes, SD is currently being used throughout the public and private sector for policy analysis and design.\n\nConvenient graphical user interface (GUI) system dynamics software developed into user friendly versions by the 1990s and have been applied to diverse systems. SD models solve the problem of simultaneity (mutual causation) by updating all variables in small time increments with positive and negative feedbacks and time delays structuring the interactions and control. The best known SD model is probably the 1972 \"The Limits to Growth\". This model forecast that exponential growth of population and capital, with finite resource sources and sinks and perception delays, would lead to economic collapse during the 21st century under a wide variety of growth scenarios.\n\nSystem dynamics is an aspect of systems theory as a method to understand the dynamic behavior of complex systems. The basis of the method is the recognition that the structure of any system, the many circular, interlocking, sometimes time-delayed relationships among its components, is often just as important in determining its behavior as the individual components themselves. Examples are chaos theory and social dynamics. It is also claimed that because there are often properties-of-the-whole which cannot be found among the properties-of-the-elements, in some cases the behavior of the whole cannot be explained in terms of the behavior of the parts.\n\nSystem dynamics was created during the mid-1950s by Professor Jay Forrester of the Massachusetts Institute of Technology. In 1956, Forrester accepted a professorship in the newly formed MIT Sloan School of Management. His initial goal was to determine how his background in science and engineering could be brought to bear, in some useful way, on the core issues that determine the success or failure of corporations. Forrester's insights into the common foundations that underlie engineering, which led to the creation of system dynamics, were triggered, to a large degree, by his involvement with managers at General Electric (GE) during the mid-1950s. At that time, the managers at GE were perplexed because employment at their appliance plants in Kentucky exhibited a significant three-year cycle. The business cycle was judged to be an insufficient explanation for the employment instability. From hand simulations (or calculations) of the stock-flow-feedback structure of the GE plants, which included the existing corporate decision-making structure for hiring and layoffs, Forrester was able to show how the instability in GE employment was due to the internal structure of the firm and not to an external force such as the business cycle. These hand simulations were the start of the field of system dynamics.\n\nDuring the late 1950s and early 1960s, Forrester and a team of graduate students moved the emerging field of system dynamics from the hand-simulation stage to the formal computer modeling stage. Richard Bennett created the first system dynamics computer modeling language called SIMPLE (Simulation of Industrial Management Problems with Lots of Equations) in the spring of 1958. In 1959, Phyllis Fox and Alexander Pugh wrote the first version of\nDYNAMO (DYNAmic MOdels), an improved version of SIMPLE, and the system dynamics language became the industry standard for over thirty years. Forrester published the first, and still classic, book in the field titled \"Industrial Dynamics\" in 1961.\n\nFrom the late 1950s to the late 1960s, system dynamics was applied almost exclusively to corporate/managerial problems. In 1968, however, an unexpected occurrence caused the field to broaden beyond corporate modeling. John F. Collins, the former mayor of Boston, was appointed a visiting professor of Urban Affairs at MIT. The result of the Collins-Forrester collaboration was a book titled \"Urban Dynamics\". The Urban Dynamics model presented in the book was the first major non-corporate application of system dynamics.\n\nThe second major noncorporate application of system dynamics came shortly after the first. In 1970, Jay Forrester was invited by the Club of Rome to a meeting in Bern, Switzerland. The Club of Rome is an organization devoted to solving what its members describe as the \"predicament of mankind\"—that is, the global crisis that may appear sometime in the future, due to the demands being placed on the Earth's carrying capacity (its sources of renewable and nonrenewable resources and its sinks for the disposal of pollutants) by the world's exponentially growing population. At the Bern meeting, Forrester was asked if system dynamics could be used to address the predicament of mankind. His answer, of course, was that it could. On the plane back from the Bern meeting, Forrester created the first draft of a system dynamics model of the world's socioeconomic system. He called this model WORLD1. Upon his return to the United States, Forrester refined WORLD1 in preparation for a visit to MIT by members of the Club of Rome. Forrester called the refined version of the model WORLD2. Forrester published WORLD2 in a book titled World Dynamics.\n\nThe elements of system dynamics diagrams are feedback, accumulation of flows into stocks and time delays.\n\nAs an illustration of the use of system dynamics, imagine an organisation that plans to introduce an innovative new durable consumer product. The organisation needs to understand the possible market dynamics in order to design marketing and production plans.\n\nIn the system dynamics methodology, a problem or a system (e.g., ecosystem, political system or mechanical system) may be represented as a causal loop diagram. A causal loop diagram is a simple map of a system with all its constituent components and their interactions. By capturing interactions and consequently the feedback loops (see figure below), a causal loop diagram reveals the structure of a system. By understanding the structure of a system, it becomes possible to ascertain a system’s behavior over a certain time period.\n\nThe causal loop diagram of the new product introduction may look as follows:\n\nThere are two feedback loops in this diagram. The positive reinforcement (labeled R) loop on the right indicates that the more people have already adopted the new product, the stronger the word-of-mouth impact. There will be more references to the product, more demonstrations, and more reviews. This positive feedback should generate sales that continue to grow.\n\nThe second feedback loop on the left is negative reinforcement (or \"balancing\" and hence labeled B). Clearly, growth cannot continue forever, because as more and more people adopt, there remain fewer and fewer potential adopters.\n\nBoth feedback loops act simultaneously, but at different times they may have different strengths. Thus one might expect growing sales in the initial years, and then declining sales in the later years. However, in general a causal loop diagram does not specify the structure of a system sufficiently to permit determination of its behavior from the visual representation alone.\n\nCausal loop diagrams aid in visualizing a system’s structure and behavior, and analyzing the system qualitatively. To perform a more detailed quantitative analysis, a causal loop diagram is transformed to a stock and flow diagram. A stock and flow model helps in studying and analyzing the system in a quantitative way; such models are usually built and simulated using computer software.\n\nA stock is the term for any entity that accumulates or depletes over time. A flow is the rate of change in a stock.\n\nIn our example, there are two stocks: Potential adopters and Adopters. There is one flow: New adopters. For every new adopter, the stock of potential adopters declines by one, and the stock of adopters increases by one.\n\nThe real power of system dynamics is utilised through simulation. Although it is possible to perform the modeling in a spreadsheet, there are a variety of software packages that have been optimised for this.\n\nThe steps involved in a simulation are:\n\n\nIn this example, the equations that change the two stocks via the flow are:\n\nList of all the equations in discrete time, in their order of execution in each year, for years 1 to 15 :\n\nThe dynamic simulation results show that the behaviour of the system would be to have growth in \"adopters\" that follows a classic s-curve shape.\nThe increase in \"adopters\" is very slow initially, then exponential growth for a period, followed ultimately by saturation.\n\n4588\n\nTo get intermediate values and better accuracy, the model can run in continuous time: we multiply the number of units of time and we proportionally divide values that change stock levels. In this example we multiply the 15 years by 4 to obtain 60 trimesters, and we divide the value of the flow by 4.<br>\nDividing the value is the simplest with the Euler method, but other methods could be employed instead, such as Runge–Kutta methods.\n\nList of the equations in continuous time for trimesters = 1 to 60 :\n formula_11\n\n formula_15\nSystem dynamics has found application in a wide range of areas, for example population, agriculture, ecological and economic systems, which usually interact strongly with each other.\n\nSystem dynamics have various \"back of the envelope\" management applications. They are a potent tool to:\n\nComputer software is used to simulate a system dynamics model of the situation being studied. Running \"what if\" simulations to test certain policies on such a model can greatly aid in understanding how the system changes over time. System dynamics is very similar to systems thinking and constructs the same causal loop diagrams of systems with feedback. However, system dynamics typically goes further and utilises simulation to study the behaviour of systems and the impact of alternative policies.\n\nSystem dynamics has been used to investigate resource dependencies, and resulting problems, in product development.\n\nA system dynamics approach to macroeconomics, known as \"Minsky\", has been developed by the economist Steve Keen. This has been used to successfully model world economic behaviour from the apparent stability of the Great Moderation to the sudden unexpected Financial crisis of 2007–08.\n\nThe figure above is a causal loop diagram of a system dynamics model created to examine forces that may be responsible for the growth or decline of life insurance companies in the United Kingdom. A number of this figure's features are worth mentioning. The first is that the model's negative feedback loops are identified by \"C's\", which stand for \"Counteracting\" loops. The second is that double slashes are used to indicate places where there is a significant delay between causes (i.e., variables at the tails of arrows) and effects (i.e., variables at the heads of arrows). This is a common causal loop diagramming convention in system dynamics. Third, is that thicker lines are used to identify the feedback loops and links that author wishes the audience to focus on. This is also a common system dynamics diagramming convention. Last, it is clear that a decision maker would find it impossible to think through the dynamic behavior inherent in the model, from inspection of the figure alone.\n\n\n\n\n\n\n"}
{"id": "375033", "url": "https://en.wikipedia.org/wiki?curid=375033", "title": "Trigonometric substitution", "text": "Trigonometric substitution\n\nIn mathematics, trigonometric substitution is the substitution of trigonometric functions for other expressions. One may use the trigonometric identities to simplify certain integrals containing radical expressions:\n\nSubstitution 1. If the integrand contains \"a\" − \"x\", let\nand use the identity\n\nSubstitution 2. If the integrand contains \"a\" + \"x\", let\nand use the identity\n\nformula_4\n\nSubstitution 3. If the integrand contains \"x\" − \"a\", let\nand use the identity\n\nIn the integral\n\nwe may use\n\nThen,\n\nThe above step requires that and ; we can choose to be the positive square root of , and we impose the restriction on by using the arcsin function.\n\nFor a definite integral, one must figure out how the bounds of integration change. For example, as goes from 0 to , then goes from 0 to 1/2, so goes from 0 to . Then,\n\nSome care is needed when picking the bounds. The integration above requires that , so going from 0 to π/6 is the only choice. Neglecting this restriction, one might have picked to go from to , which would have resulted in the negative of the actual value.\n\nIn the integral\n\nwe may write\n\nso that the integral becomes\n\nprovided .\n\nIntegrals like\n\ncan also be evaluated by partial fractions rather than trigonometric substitutions. However, the integral\n\ncannot. In this case, an appropriate substitution is:\n\nThen,\n\nWe can then solve this using the formula for the integral of secant cubed.\n\nSubstitution can be used to remove trigonometric functions. In particular, see Tangent half-angle substitution.\n\nFor instance,\n\nSubstitutions of hyperbolic functions can also be used to simplify integrals.\n\nIn the integral formula_19, make the substitution formula_20, formula_21\n\nThen, using the identities formula_22 and formula_23\n\nformula_24\n\n"}
