{"id": "60492", "url": "https://en.wikipedia.org/wiki?curid=60492", "title": "Abstract machine", "text": "Abstract machine\n\nAn abstract machine, also called an abstract computer, is a theoretical model of a computer hardware or software system used in automata theory. Abstraction of computing processes is used in both the computer science and computer engineering disciplines and usually assumes a discrete time paradigm.\n\nIn the theory of computation, abstract machines are often used in thought experiments regarding computability or to analyze the complexity of algorithms (\"see\" computational complexity theory). A typical abstract machine consists of a definition in terms of input, output, and the set of allowable operations used to turn the former into the latter. The best-known example is the Turing machine.\n\nAbstract data types can be specified in terms of their operational semantics on an abstract machine. For example, a stack can be specified in terms of operations on an abstract machine with an array of memory. Through the use of abstract machines it is possible to compute the amount of resources (time, memory, etc.) necessary to perform a particular operation without having to construct a physical system.\n\nMore complex definitions create abstract machines with full instruction sets, registers and models of memory. One popular model more similar to real modern machines is the RAM model, which allows random access to indexed memory locations. As the performance difference between different levels of cache memory grows, cache-sensitive models such as the external-memory model and cache-oblivious model are growing in importance.\n\nAn abstract machine can also refer to a microprocessor design which has yet to be (or is not intended to be) implemented as hardware. An abstract machine implemented as a software simulation, or for which an interpreter exists, is called a virtual machine.\n\n\n"}
{"id": "3118", "url": "https://en.wikipedia.org/wiki?curid=3118", "title": "Arithmetic", "text": "Arithmetic\n\nArithmetic (from the Greek ἀριθμός \"arithmos\", \"number\" and τική [τέχνη], \"tiké [téchne]\", \"art\") is a branch of mathematics that consists of the study of numbers, especially the properties of the traditional operations on them—addition, subtraction, multiplication and division. Arithmetic is an elementary part of number theory, and number theory is considered to be one of the top-level divisions of modern mathematics, along with algebra, geometry, and analysis. The terms \"arithmetic\" and \"higher arithmetic\" were used until the beginning of the 20th century as synonyms for \"number theory\" and are sometimes still used to refer to a wider part of number theory.\n\nThe prehistory of arithmetic is limited to a small number of artifacts which may indicate the conception of addition and subtraction, the best-known being the Ishango bone from central Africa, dating from somewhere between 20,000 and 18,000 BC, although its interpretation is disputed.\n\nThe earliest written records indicate the Egyptians and Babylonians used all the elementary arithmetic operations as early as 2000 BC. These artifacts do not always reveal the specific process used for solving problems, but the characteristics of the particular numeral system strongly influence the complexity of the methods. The hieroglyphic system for Egyptian numerals, like the later Roman numerals, descended from tally marks used for counting. In both cases, this origin resulted in values that used a decimal base but did not include positional notation. Complex calculations with Roman numerals required the assistance of a counting board or the Roman abacus to obtain the results.\n\nEarly number systems that included positional notation were not decimal, including the sexagesimal (base 60) system for Babylonian numerals and the vigesimal (base 20) system that defined Maya numerals. Because of this place-value concept, the ability to reuse the same digits for different values contributed to simpler and more efficient methods of calculation.\n\nThe continuous historical development of modern arithmetic starts with the Hellenistic civilization of ancient Greece, although it originated much later than the Babylonian and Egyptian examples. Prior to the works of Euclid around 300 BC, Greek studies in mathematics overlapped with philosophical and mystical beliefs. For example, Nicomachus summarized the viewpoint of the earlier Pythagorean approach to numbers, and their relationships to each other, in his \"Introduction to Arithmetic\".\n\nGreek numerals were used by Archimedes, Diophantus and others in a positional notation not very different from ours. The ancient Greeks lacked a symbol for zero until the Hellenistic period, and they used three separate sets of symbols as digits: one set for the units place, one for the tens place, and one for the hundreds. For the thousands place they would reuse the symbols for the units place, and so on. Their addition algorithm was identical to ours, and their multiplication algorithm was only very slightly different. Their long division algorithm was the same, and the digit-by-digit square root algorithm, popularly used as recently as the 20th century, was known to Archimedes, who may have invented it. He preferred it to Hero's method of successive approximation because, once computed, a digit doesn't change, and the square roots of perfect squares, such as 7485696, terminate immediately as 2736. For numbers with a fractional part, such as 546.934, they used negative powers of 60 instead of negative powers of 10 for the fractional part 0.934.\n\nThe ancient Chinese had advanced arithmetic studies dating from the Shang Dynasty and continuing through the Tang Dynasty, from basic numbers to advanced algebra. The ancient Chinese used a positional notation similar to that of the Greeks. Since they also lacked a symbol for zero, they had one set of symbols for the unit's place, and a second set for the ten's place. For the hundred's place they then reused the symbols for the unit's place, and so on. Their symbols were based on the ancient counting rods. It is a complicated question to determine exactly when the Chinese started calculating with positional representation, but it was definitely before 400 BC. The ancient Chinese were the first to meaningfully discover, understand, and apply negative numbers as explained in the \"Nine Chapters on the Mathematical Art\" (\"Jiuzhang Suanshu\"), which was written by Liu Hui.\n\nThe gradual development of the Hindu–Arabic numeral system independently devised the place-value concept and positional notation, which combined the simpler methods for computations with a decimal base and the use of a digit representing 0. This allowed the system to consistently represent both large and small integers. This approach eventually replaced all other systems. In the early the Indian mathematician Aryabhata incorporated an existing version of this system in his work, and experimented with different notations. In the 7th century, Brahmagupta established the use of 0 as a separate number and determined the results for multiplication, division, addition and subtraction of zero and all other numbers, except for the result of division by 0. His contemporary, the Syriac bishop Severus Sebokht (650 AD) said, \"Indians possess a method of calculation that no word can praise enough. Their rational system of mathematics, or of their method of calculation. I mean the system using nine symbols.\" The Arabs also learned this new method and called it \"hesab\".\nAlthough the Codex Vigilanus described an early form of Arabic numerals (omitting 0) by 976 AD, Leonardo of Pisa (Fibonacci) was primarily responsible for spreading their use throughout Europe after the publication of his book \"Liber Abaci\" in 1202. He wrote, \"The method of the Indians (Latin \"Modus Indoram\") surpasses any known method to compute. It's a marvelous method. They do their computations using nine figures and symbol zero\".\n\nIn the Middle Ages, arithmetic was one of the seven liberal arts taught in universities.\n\nThe flourishing of algebra in the medieval Islamic world and in Renaissance Europe was an outgrowth of the enormous simplification of computation through decimal notation.\n\nVarious types of tools have been invented and widely used to assist in numeric calculations. Before Renaissance, they were various types of abaci. More recent examples include slide rules, nomograms and mechanical calculators, such as Pascal's calculator. At present, they have been supplanted by electronic calculators and computers.\n\nThe basic arithmetic operations are addition, subtraction, multiplication and division, although this subject also includes more advanced operations, such as manipulations of percentages, square roots, exponentiation, logarithmic functions, and even trigonometric functions, in the same vein as logarithms (Prosthaphaeresis). Arithmetic expressions must be evaluated according to the intended sequence of operations. There are several methods to specify this, either –most common, together with infix notation– explicitly using parentheses, and relying on precedence rules, or using a pre– or postfix notation, which uniquely fix the order of execution by themselves. Any set of objects upon which all four arithmetic operations (except division by 0) can be performed, and where these four operations obey the usual laws (including distributivity), is called a field.\n\nAddition is the most basic operation of arithmetic. In its simple form, addition combines two numbers, the \"addends\" or \"terms\", into a single number, the \"sum\" of the numbers (Such as or ).\n\nAdding finitely many numbers can be viewed as repeated simple addition; this procedure is known as summation, a term also used to denote the definition for \"adding infinitely many numbers\" in an infinite series. Repeated addition of the number 1 is the most basic form of counting, the result of adding is usually called the successor of the original number.\n\nAddition is commutative and associative, so the order in which finitely many terms are added does not matter. The identity element for a binary operation is the number that, when combined with any number, yields the same number as result. According to the rules of addition, adding  to any number yields that same number, so is the additive identity. The \"inverse of a number\" with respect to a binary operation is the number that, when combined with any number, yields the identity with respect to this operation. So the inverse of a number with respect to addition (its additive inverse, or the opposite number), is the number, that yields the additive identity, , when added to the original number; it is immediate that this is the negative of the original number. For example, the additive inverse of is , since .\n\nAddition can be interpreted geometrically as in the following example:\n\nSubtraction is the inverse operation to addition. Subtraction finds the \"difference\" between two numbers, the \"minuend\" minus the \"subtrahend\": Resorting to the previously established addition, this is to say that the difference is the number that, when added to the subtrahend, results in the minuend: \n\nFor positive arguments and holds:\nIn any case, if minuend and subtrahend are equal, the difference \n\nSubtraction is neither commutative nor associative. For that reason, in modern algebra the construction of this inverse operation is often discarded in favor of introducing the concept of inverse elements, as sketched under Addition, and to look at subtraction as adding the additive inverse of the subtrahend to the minuend, that is . The immediate price of discarding the binary operation of subtraction is the introduction of the (trivial) unary operation, delivering the additive inverse for any given number, and losing the immediate access to the notion of difference, which is potentially misleading, anyhow, when negative arguments are involved.\n\nFor any representation of numbers there are methods for calculating results, some of which are particularly advantageous in exploiting procedures, existing for one operation, by small alterations also for others. For example, digital computers can reuse existing adding-circuitry and save additional circuits for implementing a subtraction by employing the method of two's complement for representing the additive inverses, which is extremely easy to implement in hardware (negation). The trade-off is the halving of the number range for a fixed word length.\n\nA formerly wide spread method to achieve a correct change amount, knowing the due and given amounts, is the \"counting up method\", which does not explicitly generate the value of the difference. Suppose an amount \"P\" is given in order to pay the required amount \"Q\", with \"P\" greater than \"Q\". Rather than explicitly performing the subtraction \"P\" − \"Q\" = \"C\" and counting out that amount \"C\" in change, money is counted out starting with the successor of \"Q\", and continuing in the steps of the currency, until \"P\" is reached. Although the amount counted out must equal the result of the subtraction \"P\" − \"Q\", the subtraction was never really done and the value of \"P\" − \"Q\" is not supplied by this method.\n\nMultiplication is the second basic operation of arithmetic. Multiplication also combines two numbers into a single number, the \"product\". The two original numbers are called the \"multiplier\" and the \"multiplicand\", mostly both are simply called \"factors\".\n\nMultiplication may be viewed as a scaling operation. If the numbers are imagined as lying in a line, multiplication by a number, say \"x\", greater than 1 is the same as stretching everything away from 0 uniformly, in such a way that the number 1 itself is stretched to where \"x\" was. Similarly, multiplying by a number less than 1 can be imagined as squeezing towards 0. (Again, in such a way that 1 goes to the multiplicand.)\n\nAnother view on multiplication of integer numbers, extendable to rationals, but not very accessible for real numbers, is by considering it as repeated addition. So corresponds to either adding times a , or times a , giving the same result. There are different opinions on the advantageousness of these paradigmata in math education. \n\nMultiplication is commutative and associative; further it is distributive over addition and subtraction. The multiplicative identity is 1, since multiplying any number by 1 yields that same number (no stretching or squeezing). The multiplicative inverse for any number except  is the reciprocal of this number, because multiplying the reciprocal of any number by the number itself yields the multiplicative identity .  is the only number without a multiplicative inverse, and the result of multiplying any number and is again One says, is not contained in the multiplicative group of the numbers.\n\nThe product of \"a\" and \"b\" is written as or . When \"a\" or \"b\" are expressions not written simply with digits, it is also written by simple juxtaposition: \"ab\". In computer programming languages and software packages in which one can only use characters normally found on a keyboard, it is often written with an asterisk: \n\nAlgorithms implementing the operation of multiplication for various representations of numbers are by far more costly and laborious than those for addition. Those accessible for manual computation either rely on breaking down the factors to single place values and apply repeated addition, or employ tables or slide rules, thereby mapping the multiplication to addition and back. These methods are outdated and replaced by mobile devices. Computers utilize diverse sophisticated and highly optimized algorithms to implement multiplication and division for the various number formats supported in their system.\n\nDivision is essentially the inverse operation to multiplication. Division finds the \"quotient\" of two numbers, the \"dividend\" divided by the \"divisor\". Any dividend divided by 0 is undefined. For distinct positive numbers, if the dividend is larger than the divisor, the quotient is greater than 1, otherwise it is less than 1 (a similar rule applies for negative numbers). The quotient multiplied by the divisor always yields the dividend.\n\nDivision is neither commutative nor associative. So as explained for subtraction, in modern algebra the construction of the division is discarded in favor of constructing the inverse elements with respect to multiplication, as introduced there. That is, division is a multiplication with the dividend and the reciprocal of the divisor as factors, that is \n\nWithin natural numbers there is also a different, but related notion, the Euclidean division, giving two results of \"dividing\" a natural (numerator) by a natural (denominator), first, a natural (quotient) and second, a natural (remainder), such that and \n\nDecimal representation refers exclusively, in common use, to the written numeral system employing arabic numerals as the digits for a radix 10 (\"decimal\") positional notation; however, any numeral system based on powers of 10, e.g., Greek, Cyrillic, Roman, or Chinese numerals may conceptually be described as \"decimal notation\" or \"decimal representation\".\n\nModern methods for four fundamental operations (addition, subtraction, multiplication and division) were first devised by Brahmagupta of India. This was known during medieval Europe as \"Modus Indoram\" or Method of the Indians. Positional notation (also known as \"place-value notation\") refers to the representation or encoding of numbers using the same symbol for the different orders of magnitude (e.g., the \"ones place\", \"tens place\", \"hundreds place\") and, with a radix point, using those same symbols to represent fractions (e.g., the \"tenths place\", \"hundredths place\"). For example, 507.36 denotes 5 hundreds (10), plus 0 tens (10), plus 7 units (10), plus 3 tenths (10) plus 6 hundredths (10).\n\nThe concept of 0 as a number comparable to the other basic digits is essential to this notation, as is the concept of 0's use as a placeholder, and as is the definition of multiplication and addition with 0. The use of 0 as a placeholder and, therefore, the use of a positional notation is first attested to in the Jain text from India entitled the \"Lokavibhâga\", dated 458 AD and it was only in the early 13th century that these concepts, transmitted via the scholarship of the Arabic world, were introduced into Europe by Fibonacci using the Hindu–Arabic numeral system.\n\nAlgorism comprises all of the rules for performing arithmetic computations using this type of written numeral. For example, addition produces the sum of two arbitrary numbers. The result is calculated by the repeated addition of single digits from each number that occupies the same position, proceeding from right to left. An addition table with ten rows and ten columns displays all possible values for each sum. If an individual sum exceeds the value 9, the result is represented with two digits. The rightmost digit is the value for the current position, and the result for the subsequent addition of the digits to the left increases by the value of the second (leftmost) digit, which is always one. This adjustment is termed a \"carry\" of the value 1.\n\nThe process for multiplying two arbitrary numbers is similar to the process for addition. A multiplication table with ten rows and ten columns lists the results for each pair of digits. If an individual product of a pair of digits exceeds 9, the \"carry\" adjustment increases the result of any subsequent multiplication from digits to the left by a value equal to the second (leftmost) digit, which is any value from (). Additional steps define the final result.\n\nSimilar techniques exist for subtraction and division.\n\nThe creation of a correct process for multiplication relies on the relationship between values of adjacent digits. The value for any single digit in a numeral depends on its position. Also, each position to the left represents a value ten times larger than the position to the right. In mathematical terms, the exponent for the radix (base) of 10 increases by 1 (to the left) or decreases by 1 (to the right). Therefore, the value for any arbitrary digit is multiplied by a value of the form 10 with integer \"n\". The list of values corresponding to all possible positions for a single digit is written \n\nRepeated multiplication of any value in this list by 10 produces another value in the list. In mathematical terminology, this characteristic is defined as closure, and the previous list is described as closed under multiplication. It is the basis for correctly finding the results of multiplication using the previous technique. This outcome is one example of the uses of number theory.\n\nCompound unit arithmetic is the application of arithmetic operations to mixed radix quantities such as feet and inches, gallons and pints, pounds shillings and pence, and so on. Prior to the use of decimal-based systems of money and units of measure, the use of compound unit arithmetic formed a significant part of commerce and industry.\n\nThe techniques used for compound unit arithmetic were developed over many centuries and are well-documented in many textbooks in many different languages. In addition to the basic arithmetic functions encountered in decimal arithmetic, compound unit arithmetic employs three more functions:\n\nKnowledge of the relationship between the various units of measure, their multiples and their submultiples forms an essential part of compound unit arithmetic.\n\nThere are two basic approaches to compound unit arithmetic:\n\nDuring the 19th and 20th centuries various aids were developed to aid the manipulation of compound units, particularly in commercial applications. The most common aids were mechanical tills which were adapted in countries such as the United Kingdom to accommodate pounds, shillings, pennies and farthings and \"Ready Reckoners\" – books aimed at traders that catalogued the results of various routine calculations such as the percentages or multiples of various sums of money. One typical booklet that ran to 150 pages tabulated multiples \"from one to ten thousand at the various prices from one farthing to one pound\".\n\nThe cumbersome nature of compound unit arithmetic has been recognized for many years – in 1586, the Flemish mathematician Simon Stevin published a small pamphlet called \"De Thiende\" (\"the tenth\") in which he declared the universal introduction of decimal coinage, measures, and weights to be merely a question of time. In the modern era, many conversion programs, such as that included in the Microsoft Windows 7 operating system calculator, display compound units in a reduced decimal format rather than using an expanded format (i.e. \"2.5 ft\" is displayed rather than ).\n\nUntil the 19th century, \"number theory\" was a synonym of \"arithmetic\". The addressed problems were directly related to the basic operations and concerned primality, divisibility, and the solution of equations in integers, such as Fermat's last theorem. It appeared that most of these problems, although very elementary to state, are very difficult and may not be solved without very deep mathematics involving concepts and methods from many other branches of mathematics. This led to new branches of number theory such as analytic number theory, algebraic number theory, Diophantine geometry and arithmetic algebraic geometry. Wiles' proof of Fermat's Last Theorem is a typical example of the necessity of sophisticated methods, which go far beyond the classical methods of arithmetic, for solving problems that can be stated in elementary arithmetic.\n\nPrimary education in mathematics often places a strong focus on algorithms for the arithmetic of natural numbers, integers, fractions, and decimals (using the decimal place-value system). This study is sometimes known as algorism.\n\nThe difficulty and unmotivated appearance of these algorithms has long led educators to question this curriculum, advocating the early teaching of more central and intuitive mathematical ideas. One notable movement in this direction was the New Math of the 1960s and 1970s, which attempted to teach arithmetic in the spirit of axiomatic development from set theory, an echo of the prevailing trend in higher mathematics.\n\nAlso, arithmetic was used by Islamic Scholars in order to teach application of the rulings related to Zakat and Irth. This was done in a book entitled \"The Best of Arithmetic\" by Abd-al-Fattah-al-Dumyati.\n\nThe book begins with the foundations of mathematics and proceeds to its application in the later chapters.\n\n\n"}
{"id": "44456093", "url": "https://en.wikipedia.org/wiki?curid=44456093", "title": "Cohomological descent", "text": "Cohomological descent\n\nIn algebraic geometry, a cohomological descent is, roughly, a \"derived\" version of a fully faithful descent in the classical descent theory. This point is made precise by the below: the following are equivalent: in an appropriate setting, given a map \"a\" from a simplicial space \"X\" to a space \"S\", \nThe map \"a\" is then said to be a morphism of cohomological descent.\n\nThe treatment in SGA uses a lot of topos theory. Conrad's notes gives a more down-to-earth exposition.\n\n\n\n"}
{"id": "36279953", "url": "https://en.wikipedia.org/wiki?curid=36279953", "title": "Convex subgraph", "text": "Convex subgraph\n\nIn metric graph theory, a convex subgraph of an undirected graph \"G\" is a subgraph that includes every shortest path in \"G\" between two of its vertices. Thus, it is analogous to the definition of a convex set in geometry, a set that contains the line segment between every pair of its points.\n\nConvex subgraphs play an important role in the theory of partial cubes and median graphs. In particular, in median graphs, the convex subgraphs have the Helly property: if a family of convex subgraphs has the property that all pairwise intersections are nonempty, then the whole family has a nonempty intersection.\n\n"}
{"id": "18739444", "url": "https://en.wikipedia.org/wiki?curid=18739444", "title": "Coxeter–James Prize", "text": "Coxeter–James Prize\n\nThe Coxeter–James Prize is presented annually by the Canadian Mathematical Society. The award is presented to young mathematicians in recognition of outstanding contributions to mathematical research. The first award was presented in 1978. The prize was named in honor of the mathematicians Donald Coxeter and Ralph James.\n\nSource: Canadian Mathematical Society \n"}
{"id": "4120077", "url": "https://en.wikipedia.org/wiki?curid=4120077", "title": "Dominating decision rule", "text": "Dominating decision rule\n\nIn decision theory, a decision rule is said to dominate another if the performance of the former is sometimes better, and never worse, than that of the latter.\n\nFormally, let formula_1 and formula_2 be two decision rules, and let formula_3 be the risk of rule formula_4 for parameter formula_5. The decision rule formula_1 is said to dominate the rule formula_2 if formula_8 for all formula_5, and the inequality is strict for some formula_5.\n\nThis defines a partial order on decision rules; the maximal elements with respect to this order are called \"admissible decision rules.\"\n"}
{"id": "1190521", "url": "https://en.wikipedia.org/wiki?curid=1190521", "title": "Domineering", "text": "Domineering\n\nDomineering (also called Stop-Gate or Crosscram) is a mathematical game played on a sheet of graph paper, with any set of designs traced out. For example, it can be played on a 6×6 square, a checkerboard, an entirely irregular polygon, or any combination thereof. Two players have a collection of dominoes which they place on the grid in turn, covering up squares. One player, Left, plays tiles vertically, while the other, Right, plays horizontally. As in most games in combinatorial game theory, the first player who cannot move loses.\n\nDomineering is a partisan game, in that players use different pieces: the impartial version of the game is Cram.\n\nOther than the empty game, where there is no grid, the simplest game is a single box.\n\nIn this game, clearly, neither player can move. Since it is a second-player win, it is therefore a zero game.\n\nThis game is a 2-by-1 grid. There is a convention of assigning the game a positive number when Left is winning and a negative one when Right is winning. In this case, Left has no moves, while Right can play a domino to cover the entire board, leaving nothing, which is clearly a zero game. Thus in surreal number notation, this game is <nowiki>{|</nowiki>0} = −1. This makes sense, as this grid is a 1-move advantage for Right.\n\nThis game is also <nowiki>{|</nowiki>0} = −1, because a single box is unplayable.\n\nThis grid is the first case of a choice. Right \"could\" play the left two boxes, leaving −1. The rightmost boxes leave −1 as well. He could also play the middle two boxes, leaving two single boxes. This option leaves 0+0 = 0. Thus this game can be expressed as <nowiki>{|</nowiki>0,−1}. This is −2. If this game is played in conjunction with other games, this is two free moves for Right.\n\nVertical columns are evaluated in the same way. If there is a row of 2\"n\" or 2\"n\"+1 boxes, it counts as −\"n\". A column of such size counts as +\"n\".\n\n<br>\nThis is a more complex game. If Left goes first, either move leaves a 1×2 grid, which is +1. Right, on the other hand, can move to −1. Thus the surreal number notation is {1|−1}. However, this is not a surreal number because 1 > −1. This is a Game but not a number. The notation for this is ±1, and it is a hot game, because each player wants to move here.\n\n<br>\nThis is a 2×3 grid, which is even more complex, but, just like any Domineering game, it can be broken down by looking at what the various moves for Left and Right are. Left can take the left column (or, equivalently, the right column) and move to ±1, but it is clearly a better idea to split the middle, leaving two separate games, each worth +1. Thus Left's best move is to +2. Right has four \"different\" moves, but they all leave the following shape in some rotation:\n\n<br>\nThis game is not a hot game (also called a cold game), because each move hurts the player making it, as we can see by examining the moves. Left can move to −1, Right can move to 0 or +1. Thus this game is {−1|0,1} = {−1|0} = −½.\n\nOur 2×3 grid, then, is {2|−½}, which can also be represented by the mean value, ¾, together with the bonus for moving (the \"temperature\"), 1¼, thus: formula_1\n\nThe Mathematical Sciences Research Institute held a Domineering tournament, with a $500 prize for the winner. This game was played on an 8×8 board. The winner was mathematician Dan Calistrate, who defeated David Wolfe in the final. The tournament was detailed in Richard J. Nowakowski's \"Games of No Chance\" (p. 85).\n\nA problem about Domineering is to compute the winning strategy for large boards, and particularly square boards. In 2000, Dennis Breuker, Jos Uiterwijk and Jaap van den Herik computed and published the solution for the 8x8 board. The 9x9 board followed soon after some improvements of their program. Then, in 2002, Nathan Bullock solved the 10x10 board, as part of his thesis on Domineering. The 11x11 board has been solved by Jos Uiterwijk in 2016.\n\nDomineering is a first-player win for the 6x6, 7x7, 8x8, 9x9, 10x10, and 11x11 square boards. The other known values for rectangular boards can be found on the site of Nathan Bullock.\n\nCram is the impartial version of Domineering. The only difference in the rules is that each player may place their dominoes in either orientation. It seems only a small variation in the rules, but it results in a completely different game, that can be analyzed with the Sprague–Grundy theorem.\n\n\n\n"}
{"id": "15845985", "url": "https://en.wikipedia.org/wiki?curid=15845985", "title": "E∞-operad", "text": "E∞-operad\n\nIn the theory of operads in algebra and algebraic topology, an E-operad is a parameter space for a multiplication map that is associative and commutative \"up to all higher homotopies\". (An operad that describes a multiplication that is associative but not necessarily commutative \"up to homotopy\" is called an A-operad.)\n\nFor the definition, it is necessary to work in the category of operads with an action of the symmetric group. An operad \"A\" is said to be an E-operad if all of its spaces \"E\"(\"n\") are contractible; some authors also require the action of the symmetric group \"S\" on \"E\"(\"n\") to be free. In other categories than topological spaces, the notion of \"contractibility\" has to be replaced by suitable analogs, such as acyclicity in the category of chain complexes.\n\nThe letter \"E\" in the terminology stands for \"everything\" (meaning associative and commutative), and the infinity symbols says that commutativity is required up to \"all\" higher homotopies. More generally, there is a weaker notion of \"E\"-operad (\"n\" ∈ N), parametrizing multiplications that are commutative only up to a certain level of homotopies. In particular,\n\n\nThe importance of \"E\"- and \"E\"-operads in topology stems from the fact that iterated loop spaces, that is, spaces of continuous maps from an \"n\"-dimensional sphere to another space \"X\" starting and ending at a fixed base point, constitute algebras over an \"E\"-operad. (One says they are \"E\"-spaces.) Conversely, any connected \"E\"-space \"X\" is an \"n\"-fold loop space on some other space (called \"BX\", the \"n\"-fold classifying space of X).\n\nThe most obvious, if not particularly useful, example of an \"E\"-operad is the \"commutative operad\" \"c\" given by \"c\"(\"n\") = *, a point, for all \"n\". Note that according to some authors, this is not really an \"E\"-operad because the \"S\"-action is not free. This operad describes strictly associative and commutative multiplications. By definition, any other \"E\"-operad has a map to \"c\" which is a homotopy equivalence.\n\nThe operad of little \"n\"-cubes or little \"n\"-disks is an example of an \"E\"-operad that acts naturally on \"n\"-fold loop spaces.\n\n"}
{"id": "239599", "url": "https://en.wikipedia.org/wiki?curid=239599", "title": "Fermat's theorem", "text": "Fermat's theorem\n\nThe works of the 17th-century mathematician Pierre de Fermat engendered many theorems. Fermat's theorem may refer to one of the following theorems:\n\n\n"}
{"id": "43480206", "url": "https://en.wikipedia.org/wiki?curid=43480206", "title": "Fernando Codá Marques", "text": "Fernando Codá Marques\n\nFernando Codá dos Santos Cavalcanti Marques (born 8 October 1979) is a Brazilian mathematician working mainly in geometry, topology, partial differential equations and Morse theory. He is a professor at Princeton University. In 2012, together with André Neves, he proved the Willmore conjecture.\n\nFernando Codá Marques was born on 8 October 1979 in São Carlos and grew up in Maceió. His parents were both professors of engineering.\n\nCodá Marques started as a student of civil engineering at the Federal University of Alagoas in 1996, but switched to mathematics after two years.\n\nHe obtained a master's degree from the Instituto Nacional de Matemática Pura e Aplicada (IMPA) in 1999. Among his teachers at the IMPA were Manfredo do Carmo and Elon Lages Lima.\n\nFollowing the advice of Manfredo do Carmo, Codá Marques went to Cornell University to learn geometric analysis from José F. Escobar, so that he could return and bring this area of research to Brazil. While still in Brazil, Codá Marques had been informed that Escobar was facing cancer and that he could maybe die before Codá Marques could complete his Ph.D with him. Despite this information, Codá Marques decided to keep the arrangement and became his student.\n\nIn 2001, Codá Marques was awarded Cornell's Battig Prize for graduate students, for \"excellence and promise in mathematics\". He obtained his Ph.D. from Cornell University in 2003, under the supervision of José F. Escobar (thesis: \"Existence and Compactness Theorems on Conformal Deformation of Metrics\").\n\nDespite the usual path being to go for a postdoctoral research, Codá Marques had in mind that his mission was to return to Brazil. The Instituto Nacional de Matemática Pura e Aplicada (IMPA) had already offered him a position of researcher, and he accepted it. But after six months in Brazil, Escobar, who was his main connection with researchers outside of Brazil, died. Codá Marques faced the difficulties of doing research in isolation, so he decided to accept an invitation to stay one year as a postdoc at Stanford University. There he was influenced by Richard Schoen's school of thought in geometry and met André Neves (who would become his main collaborator), and many other of his contacts.\n\nHe worked at the IMPA from 2003 to 2014.\n\nOn September 1, 2014, Codá Marques joined Princeton University as a full professor.\n\nSome of his best known works are the following:\n\nIn 2009, together with Richard Schoen and Marcus Khuri he did important work on the Yamabe problem. He solved Schoen's conjecture on compactness in the Yamabe problem for spin manifolds.\n\nIn April 2010, in cooperation with Simon Brendle and André Neves, Marques provided a counter-example to the rigidity conjecture of Min-Oo.\n\nCodá Marques and Neves \"Min-max theory and the Willmore conjecture\" was uploaded to arXiv on February 2012, in it they solved the Willmore conjecture, using Almgren–Pitts min-max theory, which was then \"a relatively old tool and already somewhat out of favor\". According to Harold Rosenberg, using this tool was possible because the pair discovered a connection between objects that were apparently very different: \"connecting the problem with questions about minimal surfaces on the sphere [...] a priori there would be no reason for these things to be connected. It's curious, very curious.\", the solution to the Willmore conjecture (Willmore, 1965)\n\nIn May 2012, in cooperation with Ian Agol and André Neves, Marques provided the solution to the Freedman–He–Wang conjecture (Freedman–He–Wang, 1994)\n\nIn December 2017, in cooperation with Kei Irie and André Neves, he solved Yau's conjecture (Yau, 1982) in the generic case.\n\nCodá Marques and André Neves are currently working to extend Almgren–Pitts min-max theory.\n\nHe was an invited speaker at the International Congress of Mathematicians (ICM) of 2010 in Hyderabad (on \"Scalar curvature, conformal geometry, and the Ricci flow with surgery\"), and a plenary speaker at the ICM of 2014 in Seoul (on \"Minimal surfaces – variational theory and applications\").\n\nHe received the TWAS Prize in 2012.\n\nHe was awarded the ICTP Ramanujan Prize in 2012.\n\nIn 2014 he gave the Łojasiewicz Lecture (on \"The min-max theory of minimal surfaces and applications\") at the Jagiellonian University in Kraków.\n\nHe is a full member of the Brazilian Academy of Sciences since 2014.\n\nHe shared the 2016 Oswald Veblen Prize in Geometry with André Neves.\n\nHe was elected to the 2018 class of fellows of the American Mathematical Society.\n\nHe is a Distinguished Visiting Professor of Mathematics at the Institute for Advanced Studies.\n\nHe is married to mathematician Ana Maria Menezes de Jesus. She was a student of Harold Rosenberg at IMPA, and is currently an instructor of mathematics at Princeton University. Codá Marques and Menezes have a son named Pedro.\n\n"}
{"id": "52956308", "url": "https://en.wikipedia.org/wiki?curid=52956308", "title": "Fluxion", "text": "Fluxion\n\nThe fluxion of a \"fluent\" (a time-varying quantity, or function) is its instantaneous rate of change, or gradient, at a given point. Fluxions were introduced by Isaac Newton to describe his form of a time derivative (a derivative with respect to time). Newton introduced the concept in 1665 and detailed them in his mathematical treatise, \"Method of Fluxions\". Fluxions and fluents made up Newton's early calculus.\n\nFluxions were central to the Leibniz–Newton calculus controversy, when Newton sent a letter to Gottfried Wilhelm Leibniz explaining them, but concealing his words in code due to his suspicion. He wrote:\n\nThe gibberish string was in fact an enciphered Latin phrase, meaning: \"Given an equation that consists of any number of flowing quantities, to find the fluxions: and vice versa\".\n\nIf the fluent is defined as formula_1 (where is time) the fluxion (derivative) at formula_2 is:\nHere is an infinitely small amount of time and according to Newton, we can now ignore it because of its infinite smallness. He justified the use of as a non-zero quantity by stating that fluxions were a consequence of movement by an object.\n\nBishop George Berkeley, a prominent philosopher of the time, slammed Newton's fluxions in his essay The Analyst, published in 1734. Berkeley refused to believe that they were accurate because of the use of the infinitesimal . He did not believe it could be ignored and pointed out that if it was zero, the consequence would be division by zero. Berkeley referred to them as \"ghosts of departed quantities\", a statement which unnerved mathematicians of the time and led to the eventual disuse of infinitesimals in calculus.\n\nTowards the end of his life Newton revised his interpretation of as infinitely small, preferring to define it as approaching zero, using a similar definition to the concept of limit. He believed this put fluxions back on safe ground. By this time, Leibniz's derivative (and his notation) had largely replaced Newton's fluxions and fluents and remain in use today.\n\n"}
{"id": "48416", "url": "https://en.wikipedia.org/wiki?curid=48416", "title": "Gottlob Frege", "text": "Gottlob Frege\n\nFriedrich Ludwig Gottlob Frege (; ; 8 November 1848 – 26 July 1925) was a German philosopher, logician, and mathematician. He is understood by many to be the father of analytic philosophy, concentrating on the philosophy of language and mathematics. Though largely ignored during his lifetime, Giuseppe Peano (1858–1932) and Bertrand Russell (1872–1970) introduced his work to later generations of logicians and philosophers.\n\nHis contributions include the development of modern logic in the \"Begriffsschrift\" and work in the foundations of mathematics. His book the \"Foundations of Arithmetic\" is the seminal text of the logicist project, and is cited by Michael Dummett as where to pinpoint the linguistic turn. His philosophical papers \"On Sense and Reference\" (\"Über Sinn und Bedeutung\") and \"The Thought\" (\"Der Gedanke\") are widely cited.\n\nFrege was born in 1848 in Wismar, Mecklenburg-Schwerin (today part of Mecklenburg-Vorpommern). His father Carl (Karl) Alexander Frege (1809–1866) was the co-founder and headmaster of a girls' high school until his death. After Carl's death, the school was led by Frege's mother Auguste Wilhelmine Sophie Frege (née Bialloblotzky, 12 January 1815 – 14 October 1898); her mother was Auguste Amalia Maria Ballhorn, a descendant of Philipp Melanchthon and her father was Johann Heinrich Siegfried Bialloblotzky, a descendant of a Polish noble family who left Poland in the 17th century.\n\nIn childhood, Frege encountered philosophies that would guide his future scientific career. For example, his father wrote a textbook on the German language for children aged 9–13, entitled \"Hülfsbuch zum Unterrichte in der deutschen Sprache für Kinder von 9 bis 13 Jahren\" (2nd ed., Wismar 1850; 3rd ed., Wismar and Ludwigslust: Hinstorff, 1862), the first section of which dealt with the structure and logic of language.\n\nFrege studied at a \"gymnasium\" in Wismar and graduated in 1869. His teacher Gustav Adolf Leo Sachse (5 November 1843 – 1 September 1909), who was a poet, played the most important role in determining Frege's future scientific career, encouraging him to continue his studies at the University of Jena.\n\nFrege matriculated at the University of Jena in the spring of 1869 as a citizen of the North German Confederation. In the four semesters of his studies he attended approximately twenty courses of lectures, most of them on mathematics and physics. His most important teacher was Ernst Karl Abbe (1840–1905; physicist, mathematician, and inventor). Abbe gave lectures on theory of gravity, galvanism and electrodynamics, complex analysis theory of functions of a complex variable, applications of physics, selected divisions of mechanics, and mechanics of solids. Abbe was more than a teacher to Frege: he was a trusted friend, and, as director of the optical manufacturer Carl Zeiss AG, he was in a position to advance Frege's career. After Frege's graduation, they came into closer correspondence.\n\nHis other notable university teachers were Christian Philipp Karl Snell (1806–86; subjects: use of infinitesimal analysis in geometry, analytical geometry of planes, analytical mechanics, optics, physical foundations of mechanics); Hermann Karl Julius Traugott Schaeffer (1824–1900; analytical geometry, applied physics, algebraic analysis, on the telegraph and other electronic machines); and the philosopher Kuno Fischer (1824–1907; Kantian and critical philosophy).\n\nStarting in 1871, Frege continued his studies in Göttingen, the leading university in mathematics in German-speaking territories, where he attended the lectures of Rudolf Friedrich Alfred Clebsch (1833–72; analytical geometry), Ernst Christian Julius Schering (1824–97; function theory), Wilhelm Eduard Weber (1804–91; physical studies, applied physics), Eduard Riecke (1845–1915; theory of electricity), and Hermann Lotze (1817–81; philosophy of religion). Many of the philosophical doctrines of the mature Frege have parallels in Lotze; it has been the subject of scholarly debate whether or not there was a direct influence on Frege's views arising from his attending Lotze's lectures.\n\nIn 1873, Frege attained his doctorate under Ernst Christian Julius Schering, with a dissertation under the title of \"Ueber eine geometrische Darstellung der imaginären Gebilde in der Ebene\" (\"On a Geometrical Representation of Imaginary Forms in a Plane\"), in which he aimed to solve such fundamental problems in geometry as the mathematical interpretation of projective geometry's infinitely distant (imaginary) points.\n\nFrege married Margarete Katharina Sophia Anna Lieseberg (15 February 1856 – 25 June 1904) on 14 March 1887.\n\nThough his education and early mathematical work focused primarily on geometry, Frege's work soon turned to logic. His marked a turning point in the history of logic. The \"Begriffsschrift\" broke new ground, including a rigorous treatment of the ideas of functions and variables. Frege's goal was to show that mathematics grows out of logic, and in so doing, he devised techniques that took him far beyond the Aristotelian syllogistic and Stoic propositional logic that had come down to him in the logical tradition.\n\nA frequently noted example is that Aristotle's logic is unable to represent mathematical statements like Euclid's theorem, a fundamental statement of number theory that there are an infinite number of prime numbers. Frege's \"conceptual notation\" however can represent such inferences. The analysis of logical concepts and the machinery of formalization that is essential to \"Principia Mathematica\" (3 vols., 1910–13) (by Bertrand Russell, 1872–1970, and Alfred North Whitehead, 1861–1947), to Russell's theory of descriptions, to Kurt Gödel's (1906–78) incompleteness theorems, and to Alfred Tarski's (1901–83) theory of truth, is ultimately due to Frege.\n\nOne of Frege's stated purposes was to isolate genuinely logical principles of inference, so that in the proper representation of mathematical proof, one would at no point appeal to \"intuition\". If there was an intuitive element, it was to be isolated and represented separately as an axiom: from there on, the proof was to be purely logical and without gaps. Having exhibited this possibility, Frege's larger purpose was to defend the view that arithmetic is a branch of logic, a view known as logicism: unlike geometry, arithmetic was to be shown to have no basis in \"intuition\", and no need for non-logical axioms. Already in the 1879 \"Begriffsschrift\" important preliminary theorems, for example a generalized form of law of trichotomy, were derived within what Frege understood to be pure logic.\n\nThis idea was formulated in non-symbolic terms in his \"The Foundations of Arithmetic\" (1884). Later, in his \"Basic Laws of Arithmetic\" (vol. 1, 1893; vol. 2, 1903; vol. 2 was published at his own expense), Frege attempted to derive, by use of his symbolism, all of the laws of arithmetic from axioms he asserted as logical. Most of these axioms were carried over from his \"Begriffsschrift\", though not without some significant changes. The one truly new principle was one he called the : the \"value-range\" of the function \"f\"(\"x\") is the same as the \"value-range\" of the function \"g\"(\"x\") if and only if ∀\"x\"[\"f\"(\"x\") = \"g\"(\"x\")].\n\nThe crucial case of the law may be formulated in modern notation as follows. Let {\"x\"|\"Fx\"} denote the extension of the predicate \"Fx\", i.e., the set of all Fs, and similarly for \"Gx\". Then Basic Law V says that the predicates \"Fx\" and \"Gx\" have the same extension iff ∀x[\"Fx\" ↔ \"Gx\"]. The set of Fs is the same as the set of Gs just in case every F is a G and every G is an F. (The case is special because what is here being called the extension of a predicate, or a set, is only one type of \"value-range\" of a function.)\n\nIn a famous episode, Bertrand Russell wrote to Frege, just as Vol. 2 of the \"Grundgesetze\" was about to go to press in 1903, showing that Russell's paradox could be derived from Frege's Basic Law V. It is easy to define the relation of \"membership\" of a set or extension in Frege's system; Russell then drew attention to \"the set of things \"x\" that are such that \"x\" is not a member of \"x\"\". The system of the \"Grundgesetze\" entails that the set thus characterised \"both\" is \"and\" is not a member of itself, and is thus inconsistent. Frege wrote a hasty, last-minute Appendix to Vol. 2, deriving the contradiction and proposing to eliminate it by modifying Basic Law V. Frege opened the Appendix with the exceptionally honest comment: \"Hardly anything more unfortunate can befall a scientific writer than to have one of the foundations of his edifice shaken after the work is finished. This was the position I was placed in by a letter of Mr. Bertrand Russell, just when the printing of this volume was nearing its completion.\" (This letter and Frege's reply are translated in Jean van Heijenoort 1967.)\n\nFrege's proposed remedy was subsequently shown to imply that there is but one object in the universe of discourse, and hence is worthless (indeed, this would make for a contradiction in Frege's system if he had axiomatized the idea, fundamental to his discussion, that the True and the False are distinct objects; see, for example, Dummett 1973), but recent work has shown that much of the program of the \"Grundgesetze\" might be salvaged in other ways:\n\nFrege's work in logic had little international attention until 1903 when Russell wrote an appendix to \"The Principles of Mathematics\" stating his differences with Frege. The diagrammatic notation\nthat Frege used had no antecedents (and has had no imitators since). Moreover, until Russell and Whitehead's \"Principia Mathematica\" (3 vols.) appeared in 1910–13, the dominant approach to mathematical logic was still that of George Boole (1815–64) and his intellectual descendants, especially Ernst Schröder (1841–1902). Frege's logical ideas nevertheless spread through the writings of his student Rudolf Carnap (1891–1970) and other admirers, particularly Bertrand Russell and Ludwig Wittgenstein (1889–1951).\n\nFrege is one of the founders of analytic philosophy, whose work on logic and language gave rise to the linguistic turn in philosophy. His contributions to the philosophy of language include:\n\nAs a philosopher of mathematics, Frege attacked the psychologistic appeal to mental explanations of the content of judgment of the meaning of sentences. His original purpose was very far from answering general questions about meaning; instead, he devised his logic to explore the foundations of arithmetic, undertaking to answer questions such as \"What is a number?\" or \"What objects do number-words (\"one\", \"two\", etc.) refer to?\" But in pursuing these matters, he eventually found himself analysing and explaining what meaning is, and thus came to several conclusions that proved highly consequential for the subsequent course of analytic philosophy and the philosophy of language.\n\nIt should be kept in mind that Frege was employed as a mathematician, not a philosopher, and he published his philosophical papers in scholarly journals that often were hard to access outside of the German-speaking world. He never published a philosophical monograph other than \"The Foundations of Arithmetic\", much of which was mathematical in content, and the first collections of his writings appeared only after World War II. A volume of English translations of Frege's philosophical essays first appeared in 1952, edited by students of Wittgenstein, Peter Geach (1916-2013) and Max Black (1909–88), with the bibliographic assistance of Wittgenstein (see Geach, ed. 1975, Introduction). Despite the generous praise of Russell and Wittgenstein, Frege was little known as a philosopher during his lifetime. His ideas spread chiefly through those he influenced, such as Russell, Wittgenstein, and Carnap, and through work on logic and semantics by Polish logicians.\n\nFrege's 1892 paper, \"On Sense and Reference\" (\"Über Sinn und Bedeutung\"), introduced his influential distinction between \"sense\" (\"Sinn\") and \"reference\" (\"Bedeutung\", which has also been translated as \"meaning\", or \"denotation\"). While conventional accounts of meaning took expressions to have just one feature (reference), Frege introduced the view that expressions have two different aspects of significance: their sense and their reference.\n\n\"Reference\", (or, \"Bedeutung\") applied to proper names, where a given expression (say the expression \"Tom\") simply refers to the entity bearing the name (the person named Tom). Frege also held that propositions had a referential relationship with their truth-value (in other words, a statement \"refers\" to the truth-value it takes). By contrast, the \"sense\" (or \"Sinn\") associated with a complete sentence is the thought it expresses. The sense of an expression is said to be the \"mode of presentation\" of the item referred to, and there can be multiple modes of representation for the same referent.\n\nThe distinction can be illustrated thus: In their ordinary uses, the name \"Charles Philip Arthur George Mountbatten-Windsor\", which for logical purposes is an unanalyzable whole, and the functional expression \"the Prince of Wales\", which contains the significant parts \"the prince of ξ\" and \"Wales\", have the same \"reference\", namely, the person best known as Prince Charles. But the \"sense\" of the word \"Wales\" is a part of the sense of the latter expression, but no part of the sense of the \"full name\" of Prince Charles.\n\nThese distinctions were disputed by Bertrand Russell, especially in his paper \"On Denoting\"; the controversy has continued into the present, fueled especially by Saul Kripke's famous lectures \"Naming and Necessity\".\n\nFrege's published philosophical writings were of a very technical nature and divorced from practical issues, so much so that Frege scholar Dummett expresses his \"shock to discover, while reading Frege's diary, that his hero was an anti-Semite.\" After the German Revolution of 1918–19 his political opinions became more radical. In the last year of his life, at the age of 76, his diary contains extreme right-wing political opinions, opposing the parliamentary system, democrats, liberals, Catholics, the French and Jews, who he thought ought to be deprived of political rights and, preferably, expelled from Germany. Frege confided \"that he had once thought of himself as a liberal and was an admirer of Bismarck\", but then sympathized with General Ludendorff and Adolf Hitler. Some interpretations have been written about that time. The diary contains a critique of universal suffrage and socialism. Frege had friendly relations with Jews in real life: among his students was Gershom Scholem who much valued his teacher; and he encouraged Ludwig Wittgenstein to leave for England. The 1924 diary was published posthumously in 1994. Frege apparently never spoke in public about his political viewpoints.\n\nFrege was described by his students as a highly introverted person, seldom entering into dialogue, mostly facing the blackboard while lecturing though being witty and sometimes bitterly sarcastic.\n\n\n\"Begriffsschrift: eine der arithmetischen nachgebildete Formelsprache des reinen Denkens\" (1879), Halle a. S.\n\"Die Grundlagen der Arithmetik: Eine logisch-mathematische Untersuchung über den Begriff der Zahl\" (1884), Breslau.\n\"Grundgesetze der Arithmetik\", Band I (1893); Band II (1903), Jena: Verlag Hermann Pohle (online version).\n\n\"Function and Concept\" (1891)\n\"On Sense and Reference\" (1892)\n\"Concept and Object\" (1892)\n\"What is a Function?\" (1904)\n\n\"Logical Investigations\" (1918–1923).\nFrege intended that the following three papers be published together in a book titled \"Logische Untersuchungen\" (\"Logical Investigations\"). Though the German book never appeared, the papers were published together in \"Logische Untersuchungen\", ed. G. Patzig, Vandenhoeck & Ruprecht, 1966, and English translations appeared together in \"Logical Investigations\", ed. Peter Geach, Blackwell, 1975.\n\n\n\n\n\n\n"}
{"id": "22816079", "url": "https://en.wikipedia.org/wiki?curid=22816079", "title": "Grinberg's theorem", "text": "Grinberg's theorem\n\nIn graph theory, Grinberg's theorem is a necessary condition for a planar graph to contain a Hamiltonian cycle, based on the lengths of its face cycles. The result has been widely used to construct non-Hamiltonian planar graphs with further properties, such as to give new counterexamples to Tait's conjecture (originally disproved by W.T. Tutte in 1946). This theorem was proved by Latvian mathematician Emanuel Grinberg in 1968.\n\nLet \"G\" be a finite planar graph with a Hamiltonian cycle \"C\", with a fixed planar embedding.\nDenote by \"ƒ\" and \"g\" the number of \"k\"-gonal faces of the embedding that are inside and outside of \"C\", respectively. Then\n\nThe proof is an easy consequence of Euler's formula.\n\nA corollary of this theorem is that if a planar graph can be embedded in such a way that all but one face has a number of sides that is 2 mod 3, and the remaining face has a number of sides that is not 2 mod 3, then the graph is not Hamiltonian. For instance, for the graph in the figure, all the bounded faces have 5 or 8 sides, but the unbounded face has 9 sides, so it satisfies this condition and is not Hamiltonian. For any planar graph, the faces whose number of sides is 2 mod 3 contribute 0 mod 3 to the sum in Grinberg's theorem, because of the factor of \"k\" − 2 in the sum. However, the other faces contribute a number that nonzero mod 3, regardless of whether they are inside or outside the Hamiltonian cycle. So, when there is only one face that contributes a nonzero amount, it is not possible for the total to be zero and the graph must be non-Hamiltonian.\n\nGrinberg used his theorem to find non-Hamiltonian cubic polyhedral graphs with high cyclic edge connectivity. The cyclic edge connectivity of a graph is the smallest number of edges that may be deleted in such a way that the remaining graph has more than one cyclic component. The 46-vertex Tutte graph, and the smaller cubic non-Hamiltonian polyhedral graphs derived from it, have cyclic edge connectivity three. Grinberg used his theorem to find a non-Hamiltonian cubic polyhedral graph with 44 vertices, 24 faces, and cyclic edge connectivity four, and another example (shown in the figure) with 46 vertices, 25 faces, and cyclic edge connectivity five, the maximum possible cyclic edge connectivity for a cubic planar graph other than \"K\". In the example shown, all of the bounded faces have either five or eight edges, both of which are numbers that are 2 mod 3, but the unbounded face has nine edges, unequal to 2 mod 3. Therefore, by the corollary to Grinberg's theorem, the graph cannot be Hamiltonian.\n\nGrinberg's theorem has also been used to find planar hypohamiltonian graphs, again by making all but one face have a number of edges congruent to 2 mod 3 (, ). uses the theorem in a somewhat more complicated way to find a planar cubic hypohamiltonian graph: the graph he constructs includes a 4-edge face adjacent to four 7-edge faces, and all other faces have five or eight edges. In order to satisfy Grinberg's theorem, a Hamiltonian cycle would have to separate one of the 4- or 7-edge faces from the other four, which is not possible.\n\nThere exist planar non-Hamiltonian graphs in which all faces have five or eight sides. For these graphs, Grinberg's formula taken modulo three is always satisfied by any partition of the faces into two subsets, preventing the application of his theorem to proving non-Hamiltonicity in this case .\n\nIt is not possible to use Grinberg's theorem to find counterexamples to Barnette's conjecture, that every cubic bipartite polyhedral graph is Hamiltonian. For, in such graphs, there always exists a partition of the faces into two subsets satisfying Grinberg's theorem, regardless of Hamiltonicity .\n\n\n"}
{"id": "11007822", "url": "https://en.wikipedia.org/wiki?curid=11007822", "title": "Harish-Chandra class", "text": "Harish-Chandra class\n\nIn mathematics, Harish-Chandra's class is a class of Lie groups used in representation theory. Harish-Chandra's class contains all semisimple connected linear Lie groups and is closed under natural operations, most importantly, the passage to Levi subgroups. This closure property is crucial for many inductive arguments in representation theory of Lie groups, whereas the classes of semisimple or connected semisimple Lie groups are not closed in this sense.\n\nA Lie group \"G\" with the Lie algebra \"g\" is said to be in Harish-Chandra's class if it satisfies the following conditions:\n\n"}
{"id": "420919", "url": "https://en.wikipedia.org/wiki?curid=420919", "title": "Incidence matrix", "text": "Incidence matrix\n\nIn mathematics, an incidence matrix is a matrix that shows the relationship between two classes of objects. If the first class is \"X\" and the second is \"Y\", the matrix has one row for each element of \"X\" and one column for each element of \"Y\". The entry in row \"x\" and column \"y\" is 1 if \"x\" and \"y\" are related (called \"incident\" in this context) and 0 if they are not. There are variations; see below.\n\nIncidence matrices are frequently used in graph theory.\n\nIn graph theory an undirected graph has two kinds of incidence matrices: unoriented and oriented. \n\nThe \"unoriented incidence matrix\" (or simply \"incidence matrix\") of an undirected graph is a matrix \"B\", where \"n\" and \"m\" are the numbers of vertices and edges respectively, such that if the vertex \"v\" and edge \"e\" are incident and 0 otherwise.\n\nFor example the incidence matrix of the undirected graph shown on the right is a matrix consisting of 4 rows (corresponding to the four vertices, 1–4) and 4 columns (corresponding to the four edges, e1–e4):\nIf we look at the incidence matrix, we see that the sum of each column is equal to 2. This is because each edge has a vertex connected to each end.\n\nThe \"incidence matrix\" of a directed graph is a matrix \"B\" where \"n\" and \"m\" are the number of vertices and edges respectively, such that if the edge \"e\" leaves vertex \"v\", 1 if it enters vertex \"v\" and 0 otherwise (many authors use the opposite sign convention).\n\nThe \"oriented incidence matrix\" of an undirected graph is the incidence matrix, in the sense of directed graphs, of any orientation of the graph. That is, in the column of edge \"e\", there is one 1 in the row corresponding to one vertex of \"e\" and one −1 in the row corresponding to the other vertex of \"e\", and all other rows have 0. The oriented incidence matrix is unique up to negation of any of the columns, since negating the entries of a column corresponds to reversing the orientation of an edge.\n\nThe unoriented incidence matrix of a graph \"G\" is related to the adjacency matrix of its line graph \"L\"(\"G\") by the following theorem:\n\nwhere \"A\"(\"L\"(\"G\")) is the adjacency matrix of the line graph of \"G\", \"B\"(\"G\") is the incidence matrix, and I is the identity matrix of dimension \"m\".\n\nThe discrete Laplacian (or Kirchhoff matrix) is obtained from the oriented incidence matrix \"B\"(\"G\") by the formula\n\nThe integral cycle space of a graph is equal to the null space of its oriented incidence matrix, viewed as a matrix over the integers or real or complex numbers. The binary cycle space is the null space of its oriented or unoriented incidence matrix, viewed as a matrix over the two-element field.\n\nThe incidence matrix of a signed graph is a generalization of the oriented incidence matrix. It is the incidence matrix of any bidirected graph that orients the given signed graph. The column of a positive edge has a 1 in the row corresponding to one endpoint and a −1 in the row corresponding to the other endpoint, just like an edge in an ordinary (unsigned) graph. The column of a negative edge has either a 1 or a −1 in both rows. The line graph and Kirchhoff matrix properties generalize to signed graphs.\n\nThe definitions of incidence matrix apply to graphs with loops and multiple edges. The column of an oriented incidence matrix that corresponds to a loop is all zero, unless the graph is signed and the loop is negative; then the column is all zero except for ±2 in the row of its incident vertex.\n\nBecause the edges of ordinary graphs can only have two vertices (one at each end), the column of an incidence matrix for graphs can only have two non-zero entries. By contrast, a hypergraph can have multiple vertices assigned to one edge; thus, a general matrix of non-negative integers describes a hypergraph.\n\nThe \"incidence matrix\" of an incidence structure \"C\" is a matrix \"B\" (or its transpose), where \"p\" and \"q\" are the number of \"points\" and \"lines\" respectively, such that if the point \"p\" and line \"L\" are incident and 0 otherwise. In this case, the incidence matrix is also a biadjacency matrix of the Levi graph of the structure. As there is a hypergraph for every Levi graph, and \"vice versa\", the incidence matrix of an incidence structure describes a hypergraph.\n\nAn important example is a finite geometry. For instance, in a finite plane, \"X\" is the set of points and \"Y\" is the set of lines. In a finite geometry of higher dimension, \"X\" could be the set of points and \"Y\" could be the set of subspaces of dimension one less than the dimension of the entire space (hyperplanes); or, more generally, \"X\" could be the set of all subspaces of one dimension \"d\" and \"Y\" the set of all subspaces of another dimension \"e\", with incidence defined as containment.\n\nIn a similar manner, the relationship between cells whose dimensions differ by one in a polytope, can be represented by an incidence matrix.\n\nAnother example is a block design. Here \"X\" is a finite set of \"points\" and \"Y\" is a class of subsets of \"X\", called \"blocks\", subject to rules that depend on the type of design. The incidence matrix is an important tool in the theory of block designs. For instance, it can be used to prove Fisher's inequality, a fundamental theorem of balanced incomplete 2-designs (BIBDs), that the number of blocks is at least the number of points. Considering the blocks as a system of sets, the permanent of the incidence matrix is the number of systems of distinct representatives (SDRs).\n\n"}
{"id": "406924", "url": "https://en.wikipedia.org/wiki?curid=406924", "title": "Incomplete Fermi–Dirac integral", "text": "Incomplete Fermi–Dirac integral\n\nIn mathematics, the incomplete Fermi–Dirac integral for an index \"j\" is given by\n\nThis is an alternate definition of the incomplete polylogarithm.\n\n\n"}
{"id": "2129591", "url": "https://en.wikipedia.org/wiki?curid=2129591", "title": "Inverse dynamics", "text": "Inverse dynamics\n\nInverse dynamics is an inverse problem. It commonly refers to either inverse rigid body dynamics or inverse structural dynamics. Inverse rigid-body dynamics is a method for computing forces and/or moments of force (torques) based on the kinematics (motion) of a body and the body's inertial properties (mass and moment of inertia). Typically it uses link-segment models to represent the mechanical behaviour of interconnected segments, such as the limbs of humans, animals or robots, where given the kinematics of the various parts, inverse dynamics derives the minimum forces and moments responsible for the individual movements. In practice, inverse dynamics computes these internal moments and forces from measurements of the motion of limbs and external forces such as ground reaction forces, under a special set of assumptions.\n\nThe fields of robotics and biomechanics constitute the major application areas for inverse dynamics.\n\nWithin robotics, inverse dynamics algorithms are used to calculate the torques that a robot's motors must deliver to make the robot's end-point move in the way prescribed by its current task. The \"inverse dynamics problem\" in Robotics Engineering was solved by Eduardo Bayo in 1987. This solution calculates how each of the numerous electric motors that control a robot arm must move to produce a particular action. Humans can perform very complicated and precise movements, such as controlling the tip of a fishing rod well enough to cast the bait accurately. Before the arm moves, the brain calculates the necessary movement of each muscle involved and tells the muscles what to do as the arm swings. In the case of a robot arm, the \"muscles\" are the electric motors which must turn by a given amount at a given moment. Each motor must be supplied with just the right amount of electric current, at just the right time. Researchers can predict the motion of a robot arm if they know how the motors will move. This is known as the forward dynamics problem. Until this discovery, they had not been able to work backwards to calculate the movements of the motors required to generate a particular complicated motion., Bayo's work began with the application of frequency-domain methods to the inverse dynamics of single-link flexible robots. This approach yielded non-causal exact solutions due to the right-half plane zeros in the hub-torque-to-tip transfer functions. Extending this method to the nonlinear multi-flexible-link case was of particular importance to robotics. When combined with passive joint control in a collaborative effort with a control group, Bayo's inverse dynamics approach led to exponentially stable tip-tracking control for flexible multi-link robots.\n\nSimilarly, inverse dynamics in biomechanics computes the net turning effect of all the anatomical structures across a joint, in particular the muscles and ligaments, necessary to produce the observed motions of the joint. These moments of force may then be used to compute the amount of mechanical work performed by that moment of force. Each moment of force can perform positive work to increase the speed and/or height of the body or perform negative work to decrease the speed and/or height of the body. The equations of motion necessary for these computations are based on Newtonian mechanics, specifically the Newton–Euler equations of:\n\nThese equations mathematically model the behaviour of a limb in terms of a knowledge domain-independent, link-segment model, such as idealized solids of revolution or a skeleton with fixed-length limbs and perfect pivot joints. From these equations, inverse dynamics derives the torque (moment) level at each joint based on the movement of the attached limb or limbs affected by the joint. This process used to derive the joint moments is known as inverse dynamics because it reverses the forward dynamics equations of motion, the set of differential equations which yield the position and angle trajectories of the idealized skeleton's limbs from the accelerations and forces applied.\n\nFrom joint moments, a biomechanist could infer muscle forces that would lead to those moments based on a model of bone and muscle attachments, etc., thereby estimating muscle activation from kinematic motion.\n\nCorrectly computing force (or moment) values from inverse dynamics can be challenging because external forces (e.g., ground contact forces) affect motion but are not directly observable from the kinematic motion. In addition, co-activation of muscles can lead\nto a family of solutions which are not distinguishable from the kinematic motion's characteristics. Furthermore, closed kinematic chains, such as swinging a bat or shooting a hockey puck, require the measurement of internal forces (in the bat or stick) be made before shoulder, elbow or wrist moments and forces can be derived.\n\n\n\n"}
{"id": "23174224", "url": "https://en.wikipedia.org/wiki?curid=23174224", "title": "Kernighan–Lin algorithm", "text": "Kernighan–Lin algorithm\n\nThe Kernighan–Lin algorithm is a heuristic algorithm for finding partitions of graphs.\nThe algorithm has important applications in the layout of digital circuits and components in VLSI.\n\nThe input to the algorithm is an undirected graph with vertex set , edge set , and (optionally) numerical weights on the edges in . The goal of the algorithm is to partition into two disjoint subsets and of equal (or nearly equal) size, in a way that minimizes the sum of the weights of the subset of edges that cross from to . If the graph is unweighted, then instead the goal is to minimize the number of crossing edges; this is equivalent to assigning weight one to each edge. The algorithm maintains and improves a partition, in each pass using a greedy algorithm to pair up vertices of with vertices of , so that moving the paired vertices from one side of the partition to the other will improve the partition. After matching the vertices, it then performs a subset of the pairs chosen to have the best overall effect on the solution quality .\nGiven a graph with vertices, each pass of the algorithm runs in time . \n\nIn more detail, let formula_1 be the \"internal cost\" of \"a\", that is, the sum of the costs of edges between \"a\" and other nodes in \"A\", and let formula_2 be the \"external cost\" of \"a\", that is, the sum of the costs of edges between \"a\" and nodes in \"B\". Furthermore, let \nbe the difference between the external and internal costs of \"a\". If \"a\" and \"b\" are interchanged, then the reduction in cost is\nwhere formula_5 is the cost of the possible edge between \"a\" and \"b\".\n\nThe algorithm attempts to find an optimal series of interchange operations between elements of formula_6 and formula_7 which maximizes formula_8 and then executes the operations, producing a partition of the graph to \"A\" and \"B\".\n\nSee \n\ncodice_1\n\n"}
{"id": "61213", "url": "https://en.wikipedia.org/wiki?curid=61213", "title": "Laurent series", "text": "Laurent series\n\nIn mathematics, the Laurent series of a complex function \"f\"(\"z\") is a representation of that function as a power series which includes terms of negative degree. It may be used to express complex functions in cases where\na Taylor series expansion cannot be applied. The Laurent series was named\nafter and first published by Pierre Alphonse Laurent in 1843.\nKarl Weierstrass may have discovered it first in a paper written in 1841, but it was not published until after his death.\n\nThe Laurent series for a complex function \"f\"(\"z\") about a point \"c\" is given by:\n\nwhere the \"a and c\" are constants, defined by a line integral\nwhich is a generalization of Cauchy's integral formula:\n\nThe path of integration formula_3 is counterclockwise around a Jordan curve enclosing\n\"c\" and lying in an annulus \"A\" in which formula_4 is\nholomorphic (analytic). The expansion for formula_4 will then be valid anywhere inside the annulus. The annulus is\nshown in red in the figure on the right, along with an example of a suitable\npath of integration labeled formula_3.\nIf we take formula_3 to be a circle formula_8, where formula_9, this just amounts\nto computing the complex Fourier coefficients of the restriction of formula_10 to formula_3. The fact that these\nintegrals are unchanged by a deformation of the contour formula_3 is an immediate consequence of Green's theorem. \n\nIn practice, the above integral formula may not offer the most practical method for computing the coefficients\nformula_13 for a given function formula_4; instead, one often pieces together the Laurent\nseries by combining known Taylor expansions.\nBecause the Laurent expansion of a function is unique whenever\nit exists, any expression of this form that actually equals the given function \nformula_4 in some annulus must actually be the \nLaurent expansion of formula_4.\n\nLaurent series with complex coefficients are an important tool in complex analysis, especially to investigate the behavior of functions near singularities.\n\nConsider for instance the function formula_17 with formula_18. As a real function, it is infinitely differentiable everywhere; as a complex function however it is not differentiable at \"x\" = 0. By replacing \"x\" with −1/\"x\" in the power series for the exponential function, we obtain its Laurent series which converges and is equal to \"ƒ\"(\"x\") for all complex numbers \"x\" except at the singularity \"x\" = 0. The graph opposite shows \"e\" in black and its Laurent approximations\n\nfor \"N\" = 1, 2, 3, 4, 5, 6, 7 and 50. As \"N\" → ∞, the approximation becomes exact for all (complex) numbers \"x\" except at the singularity \"x\" = 0.\n\nMore generally, Laurent series can be used to express holomorphic functions defined on an annulus, much as power series are used to express holomorphic functions defined on a disc.\n\nSuppose \n\nis a given Laurent series with complex coefficients \"a\" and a complex center \"c\". Then there exists a unique inner radius r and outer radius \"R\" such that:\n\nIt is possible that \"r\" may be zero or \"R\" may be infinite; at the other extreme, it's not necessarily true that \"r\" is less than \"R\".\nThese radii can be computed as follows:\n\nWe take \"R\" to be infinite when this latter lim sup is zero.\n\nConversely, if we start with an annulus of the form \"A\" ≡ {\"z\" : \"r\" < |\"z\" − \"c\"| < \"R\"} and a holomorphic function \"ƒ\"(\"z\") defined on \"A\", then there always exists a unique Laurent series with center \"c\" which converges (at least) on \"A\" and represents the function \"ƒ\"(\"z\").\n\nAs an example, consider the following rational function, along with its partial fraction expansion:\n\nThis function has singularities at \"z\" = 1 and \"z\" = 2\"i\", where the denominator of the expression is zero and the expression is therefore undefined.\nA Taylor series about \"z\" = 0 (which yields a power series) will only converge in a disc of radius 1, since it \"hits\" the singularity at 1.\n\nHowever, there are three possible Laurent expansions about 0, depending on the radius of \"z\":\nThis follows from the partial fraction form of the function, along with the formula for the sum of a geometric series, formula_24 for formula_25.\nHere, we use the alternative form of the geometric series summation, formula_27 for formula_28.\nThis series can be derived using geometric series as before, or by performing polynomial long division of 1 by (\"x\"−1)(\"x\"−2i), not stopping with a remainder but continuing into \"x\" terms; indeed, the \"outer\" Laurent series of a rational function is analogous to the decimal form of a fraction. (The \"inner\" Taylor series expansion can be obtained similarly, just reversing the term order in the division algorithm.)\n\nThe case \"r\" = 0; i.e., a holomorphic function \"ƒ\"(\"z\") which may be undefined at a single point \"c\", is especially important.\n\nThe coefficient \"a\" of the Laurent expansion of such a function is called the residue of \"ƒ\"(\"z\") at the singularity \"c\"; it plays a prominent role in the residue theorem.\n\nFor an example of this, consider\n\nThis function is holomorphic everywhere except at \"z\" = 0.\nTo determine the Laurent expansion about \"c\" = 0, we use our knowledge of the Taylor series of the exponential function:\n\nand we find that the residue is 2.\n\nSuppose a function ƒ(\"z\") holomorphic on the annulus \"r\" < |\"z\" − \"c\"| < \"R\" has two Laurent series:\n\nMultiply both sides with formula_33, where k is an arbitrary integer, and integrate on a path γ inside the annulus,\n\nThe series converges uniformly on formula_35, where ε is a positive number small enough for γ to be contained in the constricted closed annulus, so the integration and summation can be interchanged. Substituting the identity\n\ninto the summation yields\n\nHence the Laurent series is unique.\n\nA Laurent polynomial is a Laurent series in which only finitely many coefficients are non-zero. Laurent polynomials differ from ordinary polynomials in that they may have terms of negative degree.\n\nThe principal part of a Laurent series is the series of terms with negative degree, that is\n\nIf the principal part of \"f\" is a finite sum, then \"f\" has a pole at \"c\" of order equal to (negative) the degree of the highest term; on the other hand, if \"f\" has an essential singularity at \"c\", the principal part is an infinite sum (meaning it has infinitely many non-zero terms).\n\nIf the inner radius of convergence of the Laurent series for \"f\" is 0, then \"f\" has an essential singularity at \"c\" if and only if the principal part is an infinite sum, and has a pole otherwise.\n\nIf the inner radius of convergence is positive, \"f\" may have infinitely many negative terms but still be regular at \"c\", as in the example above, in which case it is represented by a \"different\" Laurent series in a disk about \"c\".\n\nLaurent series with only finitely many negative terms are well-behaved—they are a power series divided by formula_39, and can be analyzed similarly—while Laurent series with infinitely many negative terms have complicated behavior on the inner circle of convergence.\n\nLaurent series cannot in general be multiplied.\nAlgebraically, the expression for the terms of the product may involve infinite sums which need not converge (one cannot take the convolution of integer sequences).\nGeometrically, the two Laurent series may have non-overlapping annuli of convergence.\n\nTwo Laurent series with only \"finitely\" many negative terms can be multiplied: algebraically, the sums are all finite; geometrically, these have poles at \"c\", and inner radius of convergence 0, so they both converge on an overlapping annulus.\n\nThus when defining formal Laurent series, one requires Laurent series with only finitely many negative terms.\n\nSimilarly, the sum of two convergent Laurent series need not converge, though it is always defined formally, but the sum of two bounded below Laurent series (or any Laurent series on a punctured disk) has a non-empty annulus of convergence.\n\n\n"}
{"id": "41144612", "url": "https://en.wikipedia.org/wiki?curid=41144612", "title": "Limiting point (geometry)", "text": "Limiting point (geometry)\n\nIn geometry, the limiting points of two disjoint circles \"A\" and \"B\" in the Euclidean plane are points \"p\" that may be defined by any of the following equivalent properties:\n\nThe midpoint of the two limiting points is the point where the radical axis of \"A\" and \"B\" crosses the line through their centers. This intersection point has equal power distance to all the circles in the pencil containing \"A\" and \"B\". The limiting points themselves can be found at this distance on either side of the intersection point, on the line through the two circle centers. From this fact it is straightforward to construct the limiting points algebraically or by compass and straightedge.\nAn explicit formula expressing the limiting points as the solution to a quadratic equation in the coordinates of the circle centers and their radii is given by Weisstein.\n\nInverting one of the two limiting points through \"A\" or \"B\" produces the other limiting point. An inversion centered at one limiting point maps the other limiting point to the common center of the concentric circles.\n"}
{"id": "33900798", "url": "https://en.wikipedia.org/wiki?curid=33900798", "title": "List of formulas in elementary geometry", "text": "List of formulas in elementary geometry\n\nThis is a short list of some common mathematical shapes and figures and the formulas that describe them.\n"}
{"id": "7457346", "url": "https://en.wikipedia.org/wiki?curid=7457346", "title": "Logic optimization", "text": "Logic optimization\n\nLogic optimization, a part of logic synthesis in electronics, is the process of finding an equivalent representation of the specified logic circuit under one or more specified constraints. Generally the circuit is constrained to minimum chip area meeting a prespecified delay.\n\nWith the advent of logic synthesis, one of the biggest challenges faced by the electronic design automation (EDA) industry was to find the best netlist representation of the given design description. While two-level logic optimization had long existed in the form of the Quine–McCluskey algorithm, later followed by the Espresso heuristic logic minimizer, the rapidly improving chip densities, and the wide adoption of HDLs for circuit description, formalized the logic optimization domain as it exists today.\n\nToday, logic optimization is divided into various categories:\n\nBased on circuit representation \n\nBased on circuit characteristics\n\nBased on type of execution\n\nWhile a two-level circuit representation of circuits strictly refers to the flattened view of the circuit in terms of SOPs (sum-of-products) — which is more applicable to a PLA implementation of the design — a multi-level representation is a more generic view of the circuit in terms of arbitrarily connected SOPs, POSs (product-of-sums), factored form etc. Logic optimization algorithms generally work either on the structural (SOPs, factored form) or functional (BDDs, ADDs) representation of the circuit.\n\nIf we have two functions \"F\" and \"F\":\n\nThe above 2-level representation takes six product terms and 24 transistors in CMOS Rep.\n\nA functionally equivalent representation in multilevel can be:\n\nWhile the number of levels here is 3, the total number of product terms and literals reduce because of the sharing of the term B + C.\n\nSimilarly, we distinguish between sequential and combinational circuits, whose behavior can be described in terms of finite-state machine state tables/diagrams or by Boolean functions and relations respectively.\n\nIn Boolean algebra, circuit minimization is the problem of obtaining the smallest logic circuit (Boolean formula) that represents a given Boolean function or truth table. For the case when the boolean function is specified by a circuit (that is, we want to find an equivalent circuit of minimum size possible), the unbounded circuit minimization problem was long-conjectured to be formula_3-complete, a result finally proved in 2008, but there are effective heuristics such as Karnaugh maps and the Quine–McCluskey algorithm that facilitate the process.\n\nThe problem with having a complicated circuit (i.e. one with many elements, such as logic gates) is that each element takes up physical space in its implementation and costs time and money to produce in itself. Circuit minimization may be one form of logic optimization used to reduce the area of complex logic in integrated circuits.\n\nWhile there are many ways to minimize a circuit, this is an example that minimizes (or simplifies) a boolean function. Note that the boolean function carried out by the circuit is directly related to the algebraic expression from which the function is implemented.\nConsider the circuit used to represent formula_4. It is evident that two negations, two conjunctions, and a disjunction are used in this statement. This means that to build the circuit one would need two inverters, two AND gates, and an OR gate.\n\nWe can simplify (minimize) the circuit by applying logical identities or using intuition. Since the example states that A is true when B is false or the other way around, we can conclude that this simply means formula_5. In terms of logical gates, inequality simply means an XOR gate (exclusive or). Therefore, formula_6. Then the two circuits shown below are equivalent:\n\nYou can additionally check the correctness of the result using a truth table.\n\nGraphical minimization methods for two-level logic include:\n\n\n"}
{"id": "8288796", "url": "https://en.wikipedia.org/wiki?curid=8288796", "title": "MDS matrix", "text": "MDS matrix\n\nAn MDS matrix (Maximum Distance Separable) is a matrix representing a function with certain diffusion properties that have useful applications in cryptography. Technically, an m×n matrix A over a finite field K is an MDS matrix if it is the transformation matrix of a linear transformation f(x)=Ax from K to K such that no two different (m+n)-tuples of the form (x,f(x)) coincide in n or more components.\nEquivalently, the set of all (m+n)-tuples (x,f(x)) is an MDS code, i.e. a linear code that reaches the Singleton bound.\n\nLet formula_1 be the matrix obtained by joining the identity matrix Id to A. Then a necessary and sufficient condition for a matrix A to be MDS is that every possible n×n submatrix obtained by removing m rows from formula_2 is non-singular. This is also equivalent to the following: all the sub-determinants of the matrix A are non-zero. Then a binary matrix A (namely over the field with two elements) is never MDS unless it has only one row or only one column with all components 1.\n\nReed–Solomon codes have the MDS property and are frequently used to obtain the MDS matrices used in cryptographic algorithms.\n\nSerge Vaudenay suggested using MDS matrices in cryptographic primitives to produce what he called \"multipermutations\", not-necessarily linear functions with this same property. These functions have what he called \"perfect diffusion\": changing t of the inputs changes at least m-t+1 of the outputs. He showed how to exploit imperfect diffusion to cryptanalyze functions that are not multipermutations.\n\nMDS matrices are used for diffusion in such block ciphers as AES, SHARK, Square, Twofish, Anubis, KHAZAD, Manta, Hierocrypt, Kalyna and Camellia, and in the stream cipher MUGI and the cryptographic hash function Whirlpool.\n"}
{"id": "668449", "url": "https://en.wikipedia.org/wiki?curid=668449", "title": "Material derivative", "text": "Material derivative\n\nIn continuum mechanics, the material derivative describes the time rate of change of some physical quantity (like heat or momentum) of a material element that is subjected to a space-and-time-dependent macroscopic velocity field variations of that physical quantity. The material derivative can serve as a link between Eulerian and Lagrangian descriptions of continuum deformation.\n\nFor example, in fluid dynamics, the velocity field is the flow velocity, and the quantity of interest might be the temperature of the fluid. In which case, the material derivative then describes the temperature change of a certain fluid parcel with time, as it flows along its pathline (trajectory).\n\nThere are many other names for the material derivative, including:\n\nThe material derivative is defined for any tensor field \"y\" that is \"macroscopic\", with the sense that it depends only on position and time coordinates, :\n\nwhere ∇\"y\" is the covariant derivative of the tensor, and u(x, \"t\") is the flow velocity. Generally the convective derivative of the field u·∇\"y\", the one that contains the covariant derivative of the field, can be interpreted both as involving the streamline tensor derivative of the field u·(∇\"y\"), or as involving the streamline directional derivative of the field , leading to the same result. \nOnly this spatial term containing the flow velocity describes the transport of the field in the flow, while the other describes the intrinsic variation of the field, independent by the presence of any flow. Confusingly, sometimes the name \"convective derivative\" is used for the whole material derivative \"D/Dt\", instead for only the spatial term u·∇, which is also a redundant nomenclature. In the nonredundant nomenclature the material derivative only equals the convective derivative for absent flows. The effect of the time-independent terms in the definitions are for the scalar and tensor case respectively known as advection and convection.\n\nFor example, for a macroscopic scalar field and a macroscopic vector field the definition becomes:\n\nIn the scalar case ∇\"φ\" is simply the gradient of a scalar, while ∇A is the covariant derivative of the macroscopic vector (which can also be thought of as the Jacobian matrix of A as a function of x). \nIn particular for a scalar field in a three-dimensional Cartesian coordinate system (\"x\", \"x\", \"x\"), the components of the velocity u are \"u\", \"u\", \"u\", the convective term is then: \n\nConsider a scalar quantity \"φ\" = \"φ\"(x, \"t\"), where \"t\" is time and x is position. Here \"φ\" may be some physical variable such as temperature or chemical concentration. The physical quantity, whose scalar quantity is \"φ\", exists in a continuum, and whose macroscopic velocity is represented by the vector field u(x, \"t\").\n\nThe (total) derivative with respect to time of \"φ\" is expanded using the multivariate chain rule:\n\nIt is apparent that this derivative is dependent on the vector\n\nwhich describes a \"chosen\" path x(\"t\") in space. For example, if formula_7 is chosen, the time derivative becomes equal to the partial time derivative, which agrees with the definition of a partial derivative: a derivative taken with respect to some variable (time in this case) holding other variables constant (space in this case). This makes sense because if formula_8, then the derivative is taken at some \"constant\" position. This static position derivative is called the Eulerian derivative.\n\nAn example of this case is a swimmer standing still and sensing temperature change in a lake early in the morning: the water gradually becomes warmer due to heating from the sun. In which case the term formula_9 is sufficient to describe the rate of change of temperature.\n\nIf the sun is not warming the water, but the path x(\"t\") is not a standstill, the time derivative of \"φ\" may change due to the path. For example, imagine the swimmer is in a motionless pool of water, indoors and unaffected by the sun. One end happens to be at a constant high temperature and the other end at a constant low temperature. By swimming from one end to the other the swimmer senses a change of temperature with respect to time, even though the temperature at any given (static) point is a constant. This is because the derivative is taken at the swimmer's changing location and the second term on the right formula_10 is sufficient to describe the rate of change of temperature. A temperature sensor attached to the swimmer would show temperature varying with time, simply due to the temperature variation from one end of the pool to the other.\n\nThe material derivative finally is obtained when the path x(\"t\") is chosen to have a velocity equal to the fluid velocity\n\nThat is, the path follows the fluid current described by the fluid's velocity field u. So, the material derivative of the scalar \"φ\" is\n\nAn example of this case is a lightweight, neutrally buoyant particle swept along a flowing river and experiencing temperature changes as it does so. The temperature of the water locally may be increasing due to one portion of the river being sunny and the other in a shadow, or the water as a whole may be heating as the day progresses. The changes due to the particle's motion (itself caused by fluid motion) is called \"advection\" (or convection if a vector is being transported).\n\nThe definition above relied on the physical nature of a fluid current; however, no laws of physics were invoked (for example, it was assumed that a lightweight particle in a river will follow the velocity of the water), but it turns out that many physical concepts can be described concisely using the material derivative. The general case of advection, however, relies on conservation of mass of the fluid stream; the situation becomes slightly different if advection happens in a non-conservative medium.\n\nOnly a path was considered for the scalar above. For a vector, the gradient becomes a tensor derivative; for tensor fields we may want to take into account not only translation of the coordinate system due to the fluid movement but also its rotation and stretching. This is achieved by the upper convected time derivative.\n\nIt may be shown that, in orthogonal coordinates, the \"j\"-th component of the convection term of the material derivative is given by\n\nwhere the \"h\" are related to the metric tensors by\n\nIn the special case of a three-dimensional Cartesian coordinate system (\"x\", \"y\", \"z\") this is just\n\n\n"}
{"id": "31381761", "url": "https://en.wikipedia.org/wiki?curid=31381761", "title": "Mathematics of radio engineering", "text": "Mathematics of radio engineering\n\nThe mathematics of radio engineering is the application of electromagnetic theory to radio-frequency engineering, using conceptual tools such as vector calculus and complex analysis. Topics studied in this area include waveguides and transmission lines, the behavior of radio antennas, and the propagation of radio waves through the Earth's atmosphere. Historically, the subject played a significant role in the development of nonlinear dynamics.\n\n"}
{"id": "19589", "url": "https://en.wikipedia.org/wiki?curid=19589", "title": "Minimax", "text": "Minimax\n\nMinimax (sometimes MinMax or MM) is a decision rule used in artificial intelligence, decision theory, game theory, statistics and philosophy for \"mini\"mizing the possible loss for a worst case (\"max\"imum loss) scenario. When dealing with gains, it is referred to as \"maximin\"—to maximize the minimum gain. Originally formulated for two-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.\n\nThe maximin value of a player is the highest value that the player can be sure to get without knowing the actions of the other players; equivalently, it is the lowest value the other players can force the player to receive when they know the player's action. Its formal definition is:\n\nWhere:\n\nCalculating the maximin value of a player is done in a worst-case approach: for each possible action of the player, we check all possible actions of the other players and determine the worst possible combination of actions—the one that gives player the smallest value. Then, we determine which action player can take in order to make sure that this smallest value is the highest possible.\n\nFor example, consider the following game for two players, where the first player (\"row player\") may choose any of three moves, labelled , , or , and the second player (\"column\" player) may choose either of two moves, or . The result of the combination of both moves is expressed in a payoff table:\n(where the first number in each cell is the pay-out of the row player and the second number is the pay-out of the column player).\n\nFor the sake of example, we consider only pure strategies. Check each player in turn:\n\nIf both players play their respective maximin strategies formula_9, the payoff vector is formula_10.\n\nThe minimax value of a player is the smallest value that the other players can force the player to receive, without knowing the player's actions; equivalently, it is the largest value the player can be sure to get when they \"know\" the actions of the other players. Its formal definition is:\n\nThe definition is very similar to that of the maximin value—only the order of the maximum and minimum operators is inverse. In the above example:\n\nFor every player , the maximin is at most the minimax:\nIntuitively, in maximin the maximization comes before the minimization, so player tries to maximize their value before knowing what the others will do; in minimax the maximization comes after the minimization, so player is in a much better position—they maximize their value knowing what the others did.\n\nAnother way to understand the \"notation\" is by reading from right to left: when we write\nthe initial set of outcomes formula_16 depends on both formula_17 and formula_18. We first \"marginalize away\" formula_17 from formula_16, by maximizing over formula_17 (for every possible value of formula_18) to yield a set of marginal outcomes formula_23, which depends only on formula_18. We then minimize over formula_18 over these outcomes. (Conversely for maximin.)\n\nAlthough it is always the case that formula_26 and formula_27, the payoff vector resulting from both players playing their minimax strategies, formula_28 in the case of formula_29 or formula_30 in the case of formula_31, cannot similarly be ranked against the payoff vector formula_10 resulting from both players playing their maximin strategy.\n\nIn zero-sum games, the minimax solution is the same as the Nash equilibrium.\n\nIn the context of zero-sum games, the minimax theorem is equivalent to:\n\nFor every two-person, zero-sum game with finitely many strategies, there exists a value V and a mixed strategy for each player, such that\n\nEquivalently, Player 1's strategy guarantees them a payoff of V regardless of Player 2's strategy, and similarly Player 2 can guarantee themselves a payoff of −V. The name minimax arises because each player minimizes the maximum payoff possible for the other—since the game is zero-sum, they also minimize their own maximum loss (i.e. maximize their minimum payoff).\nSee also example of a game without a value.\n\nThe following example of a zero-sum game, where A and B make simultaneous moves, illustrates \"minimax\" solutions. Suppose each player has three choices and consider the payoff matrix for A displayed on the right. Assume the payoff matrix for B is the same matrix with the signs reversed (i.e. if the choices are A1 and B1 then B pays 3 to A). Then, the minimax choice for A is A2 since the worst possible result is then having to pay 1, while the simple minimax choice for B is B2 since the worst possible result is then no payment. However, this solution is not stable, since if B believes A will choose A2 then B will choose B1 to gain 1; then if A believes B will choose B1 then A will choose A1 to gain 3; and then B will choose B2; and eventually both players will realize the difficulty of making a choice. So a more stable strategy is needed.\n\nSome choices are \"dominated\" by others and can be eliminated: A will not choose A3 since either A1 or A2 will produce a better result, no matter what B chooses; B will not choose B3 since some mixtures of B1 and B2 will produce a better result, no matter what A chooses.\n\nA can avoid having to make an expected payment of more than 1∕3 by choosing A1 with probability 1∕6 and A2 with probability 5∕6: The expected payoff for A would be 3 × (1∕6) − 1 × (5∕6) = −1∕3 in case B chose B1 and −2 × (1∕6) + 0 × (5∕6) = −1/3 in case B chose B2. Similarly, B can ensure an expected gain of at least 1/3, no matter what A chooses, by using a randomized strategy of choosing B1 with probability 1∕3 and B2 with probability 2∕3. These mixed minimax strategies are now stable and cannot be improved.\n\nFrequently, in game theory, maximin is distinct from minimax. Minimax is used in zero-sum games to denote minimizing the opponent's maximum payoff. In a zero-sum game, this is identical to minimizing one's own maximum loss, and to maximizing one's own minimum gain.\n\n\"Maximin\" is a term commonly used for non-zero-sum games to describe the strategy which maximizes one's own minimum payoff. In non-zero-sum games, this is not generally the same as minimizing the opponent's maximum gain, nor the same as the Nash equilibrium strategy.\n\nThe minimax values are very important in the theory of repeated games. One of the central theorems in this theory, the folk theorem, relies on the minimax values.\n\nIn combinatorial game theory, there is a minimax algorithm for game solutions.\n\nA simple version of the minimax \"algorithm\", stated below, deals with games such as tic-tac-toe, where each player can win, lose, or draw.\nIf player A \"can\" win in one move, their best move is that winning move.\nIf player B knows that one move will lead to the situation where player A \"can\" win in one move, while another move will lead to the situation where player A can, at best, draw, then player B's best move is the one leading to a draw.\nLate in the game, it's easy to see what the \"best\" move is.\nThe Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to maximize the chances of A winning, while on the next turn player B is trying to minimize the chances of A winning (i.e., to maximize B's own chances of winning).\n\nA minimax algorithm is a recursive algorithm for choosing the next move in an n-player game, usually a two-player game. A value is associated with each position or state of the game. This value is computed by means of a position evaluation function and it indicates how good it would be for a player to reach that position. The player then makes the move that maximizes the minimum value of the position resulting from the opponent's possible following moves. If it is A<nowiki>'s</nowiki> turn to move, A gives a value to each of their legal moves.\n\nA possible allocation method consists in assigning a certain win for A as +1 and for B as −1. This leads to combinatorial game theory as developed by John Horton Conway. An alternative is using a rule that if the result of a move is an immediate win for A it is assigned positive infinity and, if it is an immediate win for B, negative infinity. The value to A of any other move is the minimum of the values resulting from each of B<nowiki>'s</nowiki> possible replies. For this reason, A is called the \"maximizing player\" and B is called the \"minimizing player\", hence the name \"minimax algorithm\". The above algorithm will assign a value of positive or negative infinity to any position since the value of every position will be the value of some final winning or losing position. Often this is generally only possible at the very end of complicated games such as chess or go, since it is not computationally feasible to look ahead as far as the completion of the game, except towards the end, and instead positions are given finite values as estimates of the degree of belief that they will lead to a win for one player or another.\n\nThis can be extended if we can supply a heuristic evaluation function which gives values to non-final game states without considering all possible following complete sequences. We can then limit the minimax algorithm to look only at a certain number of moves ahead. This number is called the \"look-ahead\", measured in \"plies\". For example, the chess computer Deep Blue (the first one to beat a reigning world champion, Garry Kasparov at that time) looked ahead at least 12 plies, then applied a heuristic evaluation function.\n\nThe algorithm can be thought of as exploring the nodes of a \"game tree\". The \"effective branching factor\" of the tree is the average number of children of each node (i.e., the average number of legal moves in a position). The number of nodes to be explored usually increases exponentially with the number of plies (it is less than exponential if evaluating forced moves or repeated positions). The number of nodes to be explored for the analysis of a game is therefore approximately the branching factor raised to the power of the number of plies. It is therefore impractical to completely analyze games such as chess using the minimax algorithm.\n\nThe performance of the naïve minimax algorithm may be improved dramatically, without affecting the result, by the use of alpha-beta pruning.\nOther heuristic pruning methods can also be used, but not all of them are guaranteed to give the same result as the un-pruned search.\n\nA naive minimax algorithm may be trivially modified to additionally return an entire Principal Variation along with a minimax score.\n\nThe pseudocode for the depth limited minimax algorithm is given below.\n\nThe minimax function returns a heuristic value for leaf nodes (terminal nodes and nodes at the maximum search depth).\nNon leaf nodes inherit their value from a descendant leaf node.\nThe heuristic value is a score measuring the favorability of the node for the maximizing player.\nHence nodes resulting in a favorable outcome, such as a win, for the maximizing player have higher scores than nodes more favorable for the minimizing player.\nThe heuristic value for terminal (game ending) leaf nodes are scores corresponding to win, loss, or draw, for the maximizing player.\nFor non terminal leaf nodes at the maximum search depth, an evaluation function estimates a heuristic value for the node.\nThe quality of this estimate and the search depth determine the quality and accuracy of the final minimax result.\n\nMinimax treats the two players (the maximizing player and the minimizing player) separately in its code. Based on the observation that formula_33, minimax may often be simplified into the negamax algorithm.\n\nSuppose the game being played only has a maximum of two possible moves per player each turn. The algorithm generates the tree on the right, where the circles represent the moves of the player running the algorithm (\"maximizing player\"), and squares represent the moves of the opponent (\"minimizing player\"). Because of the limitation of computation resources, as explained above, the tree is limited to a \"look-ahead\" of 4 moves.\n\nThe algorithm evaluates each \"leaf node\" using a heuristic evaluation function, obtaining the values shown. The moves where the \"maximizing player\" wins are assigned with positive infinity, while the moves that lead to a win of the \"minimizing player\" are assigned with negative infinity. At level 3, the algorithm will choose, for each node, the smallest of the \"child node\" values, and assign it to that same node (e.g. the node on the left will choose the minimum between \"10\" and \"+∞\", therefore assigning the value \"10\" to itself). The next step, in level 2, consists of choosing for each node the largest of the \"child node\" values. Once again, the values are assigned to each \"parent node\". The algorithm continues evaluating the maximum and minimum values of the child nodes alternately until it reaches the \"root node\", where it chooses the move with the largest value (represented in the figure with a blue arrow). This is the move that the player should make in order to \"minimize\" the \"maximum\" possible loss.\n\nMinimax theory has been extended to decisions where there is no other player, but where the consequences of decisions depend on unknown facts. For example, deciding to prospect for minerals entails a cost which will be wasted if the minerals are not present, but will bring major rewards if they are. One approach is to treat this as a game against \"nature\" (see move by nature), and using a similar mindset as Murphy's law or resistentialism, take an approach which minimizes the maximum expected loss, using the same techniques as in the two-person zero-sum games.\n\nIn addition, expectiminimax trees have been developed, for two-player games in which chance (for example, dice) is a factor.\n\nIn classical statistical decision theory, we have an estimator formula_34 that is used to estimate a parameter formula_35. We also assume a risk function formula_36, usually specified as the integral of a loss function. In this framework, formula_37 is called minimax if it satisfies\n\nAn alternative criterion in the decision theoretic framework is the Bayes estimator in the presence of a prior distribution formula_39. An estimator is Bayes if it minimizes the \"average\" risk\n\nA key feature of minimax decision making is being non-probabilistic: in contrast to decisions using expected value or expected utility, it makes no assumptions about the probabilities of various outcomes, just scenario analysis of what the possible outcomes are. It is thus robust to changes in the assumptions, as these other decision techniques are not. Various extensions of this non-probabilistic approach exist, notably minimax regret and Info-gap decision theory.\n\nFurther, minimax only requires ordinal measurement (that outcomes be compared and ranked), not \"interval\" measurements (that outcomes include \"how much better or worse\"), and returns ordinal data, using only the modeled outcomes: the conclusion of a minimax analysis is: \"this strategy is minimax, as the worst case is (outcome), which is less bad than any other strategy\". Compare to expected value analysis, whose conclusion is of the form: \"this strategy yields E(\"X\")=\"n.\"\" Minimax thus can be used on ordinal data, and can be more transparent.\n\nIn philosophy, the term \"maximin\" is often used in the context of John Rawls's \"A Theory of Justice,\" where he refers to it (Rawls (1971, p. 152)) in the context of The Difference Principle.\nRawls defined this principle as the rule which states that social and economic inequalities should be arranged so that \"they are to be of the greatest benefit to the least-advantaged members of society\".\n\n"}
{"id": "5539109", "url": "https://en.wikipedia.org/wiki?curid=5539109", "title": "Miroslav Fiedler", "text": "Miroslav Fiedler\n\nMiroslav Fiedler (7 April 1926 – 20 November 2015) was a Czech mathematician known for his contributions to linear algebra, graph theory and algebraic graph theory.\n\nHis article, \"Algebraic Connectivity of Graphs\", published in the \"Czechoslovak Math Journal\" in 1973, established the use of the eigenvalues of the Laplacian matrix of a graph to create tools for measuring algebraic connectivity in algebraic graph theory. Since then, this structure has become essential to large areas of research in flocking, distributed control, clustering, multi-robot applications and image segmentation.\n\n"}
{"id": "12589161", "url": "https://en.wikipedia.org/wiki?curid=12589161", "title": "Neural cryptography", "text": "Neural cryptography\n\nNeural cryptography is a branch of cryptography dedicated to analyzing the application of stochastic algorithms, especially artificial neural network algorithms, for use in encryption and cryptanalysis.\n\nArtificial neural networks are well known for their ability to selectively explore the solution space of a given problem. This feature finds a natural niche of application in the field of cryptanalysis. At the same time, neural networks offer a new approach to attack ciphering algorithms based on the principle that any function could be reproduced by a neural network, which is a powerful proven computational tool that can be used to find the inverse-function of any cryptographic algorithm.\n\nThe ideas of mutual learning, self learning, and stochastic behavior of neural networks and similar algorithms can be used for different aspects of cryptography, like public-key cryptography, solving the key distribution problem using neural network mutual synchronization, hashing or generation of pseudo-random numbers.\n\nAnother idea is the ability of a neural network to separate space in non-linear pieces using \"bias\". It gives different probabilities of activating the neural network or not. This is very useful in the case of Cryptanalysis.\n\nTwo names are used to design the same domain of research: Neuro-Cryptography and Neural Cryptography.\n\nThe first work that it is known on this topic can be traced back to 1995 in an IT Master Thesis.\n\nThere are currently no practical applications due to the recent development of the field, but it could be used specifically where the keys are continually generated and the system (both pairs and the insecure media) is in a continuously evolving mode.\nIn 1995, Sebastien Dourlens applied neural networks to cryptanalyze DES by allowing the networks to learn how to invert the S-tables of the DES. The bias in DES studied through Differential Cryptanalysis by Adi Shamir is highlighted. The experiment shows about 50% of the key bits can be found, allowing the complete key to be found in a short time. Hardware application with multi micro-controllers have been proposed due to the easy implementation of multilayer neural networks in hardware.\nOne example of a public-key protocol is given by Khalil Shihab. He describes the decryption scheme and the public key creation that are based on a backpropagation neural network. The encryption scheme and the private key creation process are based on Boolean algebra. This technique has the advantage of small time and memory complexities. A disadvantage is the property of backpropagation algorithms: because of huge training sets, the learning phase of a neural network is very long. Therefore, the use of this protocol is only theoretical so far.\n\nThe most used protocol for key exchange between two parties A and B in the practice is Diffie-Hellman protocol. Neural key exchange, which is based on the synchronization of two tree parity machines, should be a secure replacement for this method.\nSynchronizing these two machines is similar to synchronizing two chaotic oscillators in chaos communications.\n\nThe tree parity machine is a special type of multi-layer feedforward neural network.\n\nIt consists of one output neuron, K hidden neurons and K*N input neurons. Inputs to the network take 3 values: \nThe weights between input and hidden neurons take the values: \nOutput value of each hidden neuron is calculated as a sum of all multiplications of input neurons and these weights: \nSignum is a simple function, which returns -1,0 or 1: <br>\n\nIf the scalar product is 0, the output of the hidden neuron is mapped to -1 in order to ensure a binary output value. The output of neural network is then computed as the multiplication of all values produced by hidden elements: <br>\nOutput of the tree parity machine is binary.\n\nEach party (A and B) uses its own tree parity machine. Synchronization of the tree parity machines is achieved in these steps\n\nAfter the full synchronization is achieved (the weights w of both tree parity machines are same), A and B can use their weights as keys.<br>\nThis method is known as a bidirectional learning.<br> \nOne of the following learning rules can be used for the synchronization:\n\nWhere:\nAnd:\n\nIn every attack it is considered, that the attacker E can eavesdrop messages between the parties A and B, but does not have an opportunity to change them.\n\nTo provide a brute force attack, an attacker has to test all possible keys (all possible values of weights wij). By K hidden neurons, K*N input neurons and boundary of weights L, this gives (2L+1) possibilities. For example, the configuration K = 3, L = 3 and N = 100 gives us 3*10 key possibilities, making the attack impossible with today’s computer power.\n\nOne of the basic attacks can be provided by an attacker, who owns the same tree parity machine as the parties A and B. He wants to synchronize his tree parity machine with these two parties. In each step there are three situations possible:\nIt has been proven, that the synchronization of two parties is faster than learning of an attacker. It can be improved by increasing of the synaptic depth L of the neural network. That gives this protocol enough security and an attacker can find out the key only with small probability.\n\nFor conventional cryptographic systems, we can improve the security of the protocol by increasing of the key length. In the case of neural cryptography, we improve it by increasing of the synaptic depth L of the neural networks. Changing this parameter increases the cost of a successful attack exponentially, while the effort for the users grows polynomially. Therefore, breaking the security of neural key exchange belongs to the complexity class NP.\n\nAlexander Klimov, Anton Mityaguine, and Adi Shamir say that the original neural synchronization scheme can be broken by at least three different attacks—geometric, probabilistic analysis, and using genetic algorithms. Even though this particular implementation is insecure, the ideas behind chaotic synchronization could potentially lead to a secure implementation.\n\nThe permutation parity machine is a binary variant of the tree parity machine.\n\nIt consists of one input layer, one hidden layer and one output layer. The number of neurons in the output layer depends on the number of hidden units K. Each hidden neuron has N binary input neurons: \nThe weights between input and hidden neurons are also binary: \n\nOutput value of each hidden neuron is calculated as a sum of all exclusive disjunctions (exclusive or) of input neurons and these weights:\n\n(⊕ means XOR).\n\nThe function formula_18 is a threshold function, which returns 0 or 1: <br>\n\nThe output of neural network with two or more hidden neurons can be computed as the exclusive or of the values produced by hidden elements: <br>\nOther configurations of the output layer for K>2 are also possible.\n\nThis machine has proven to be robust enough against some attacks so it could be used as a cryptographic mean, but it has been shown to be vulnerable to a probabilistic attack.\n\nA quantum computer is a device that uses quantum mechanisms for computation. In this device the data are stored as qubits (quantum binary digits). That gives a quantum computer in comparison with a conventional computer the opportunity to solve complicated problems in a short time, e.g. discrete logarithm problem or factorization. Algorithms that are not based on any of these number theory problems are being searched because of this property.\n\nNeural key exchange protocol is not based on any number theory.\nIt is based on the difference between unidirectional and bidirectional synchronization of neural networks.\nTherefore, something like the neural key exchange protocol could give rise to potentially faster key exchange schemes.\n\n\n"}
{"id": "2110951", "url": "https://en.wikipedia.org/wiki?curid=2110951", "title": "New York State Mathematics League", "text": "New York State Mathematics League\n\nThe New York State Mathematics League (NYSML) competition was originally held in 1973 and has been held annually in a different location each year since. It was founded by Alfred Kalfus. The American Regions Math League competition is based on the format of the NYSML competition.\n\n"}
{"id": "21520", "url": "https://en.wikipedia.org/wiki?curid=21520", "title": "Null set", "text": "Null set\n\nIn mathematical analysis, a null set formula_1 is a set that can be covered by a countable union of intervals of arbitrarily small total length. The notion of null set in set theory anticipates the development of Lebesgue measure since a null set necessarily has measure zero. More generally, on a given measure space formula_2 a null set is a set formula_3 such that formula_4.\n\nSuppose formula_5 is a subset of the real line formula_6 such that \n\nwhere the \"U\" are intervals and |\"U\"| is the length of \"U\", then \"A\" is a null set. Also known as a set of zero-content. \n\nIn terminology of mathematical analysis, this definition requires that there be a sequence of open covers of \"A\" for which the limit of the lengths of the covers is zero.\n\nNull sets include all finite sets, all countable sets, and even some uncountable sets such as the Cantor set.\n\nThe empty set is always a null set. More generally, any countable union of null sets is null. Any measurable subset of a null set is itself a null set. Together, these facts show that the \"m\"-null sets of \"X\" form a sigma-ideal on \"X\". Similarly, the measurable \"m\"-null sets form a sigma-ideal of the sigma-algebra of measurable sets. Thus, null sets may be interpreted as negligible sets, defining a notion of almost everywhere.\n\nThe Lebesgue measure is the standard way of assigning a length, area or volume to subsets of Euclidean space.\n\nA subset \"N\" of formula_6 has null Lebesgue measure and is considered to be a null set in formula_6 if and only if:\nThis condition can be generalised to formula_11, using \"n\"-cubes instead of intervals. In fact, the idea can be made to make sense on any Riemannian manifold, even if there is no Lebesgue measure there.\n\nFor instance:\n\nIf λ is Lebesgue measure for formula_6 and π is Lebesgue measure for formula_19, then the product measure formula_20. In terms of null sets, the following equivalence has been styled a Fubini's theorem: \n\nNull sets play a key role in the definition of the Lebesgue integral: if functions \"f\" and \"g\" are equal except on a null set, then \"f\" is integrable if and only if \"g\" is, and their integrals are equal.\n\nA measure in which all subsets of null sets are measurable is \"complete\". Any non-complete measure can be completed to form a complete measure by asserting that subsets of null sets have measure zero. Lebesgue measure is an example of a complete measure; in some constructions, it is defined as the completion of a non-complete Borel measure.\n\nThe Borel measure is not complete. One simple construction is to start with the standard Cantor set \"K\", which is closed hence Borel measurable, and which has measure zero, and to find a subset \"F\" of \"K\" which is not Borel measurable. (Since the Lebesgue measure is complete, this \"F\" is of course Lebesgue measurable.)\n\nFirst, we have to know that every set of positive measure contains a nonmeasurable subset. Let \"f\" be the Cantor function, a continuous function which is locally constant on \"K\", and monotonically increasing on [0, 1], with \"f\"(0) = 0 and \"f\"(1) = 1. Obviously, \"f\"(\"K\") is countable, since it contains one point per component of \"K\". Hence \"f\"(\"K\") has measure zero, so \"f\"(\"K\") has measure one. We need a strictly monotonic function, so consider \"g\"(\"x\") = \"f\"(\"x\") + \"x\". Since \"g\"(\"x\") is strictly monotonic and continuous, it is a homeomorphism. Furthermore, \"g\"(\"K\") has measure one. Let \"E\" ⊂ \"g\"(\"K\") be non-measurable, and let \"F\" = \"g\"(\"E\"). Because \"g\" is injective, we have that \"F\" ⊂ \"K\", and so \"F\" is a null set. However, if it were Borel measurable, then \"g\"(\"F\") would also be Borel measurable (here we use the fact that the preimage of a Borel set by a continuous function is measurable; \"g\"(\"F\") = (\"g\")(\"F\") is the preimage of \"F\" through the continuous function \"h\" = \"g\".) Therefore, \"F\" is a null, but non-Borel measurable set.\n\nIn a separable Banach space (\"X\", +), the group operation moves any subset \"A\" ⊂ \"X\" to the translates \"A\" + \"x\" for any \"x\" ∈ \"X\". When there is a probability measure μ on the σ-algebra of Borel subsets of \"X\", such that for all \"x\", μ(\"A\" + \"x\") = 0, then \"A\" is a Haar null set.\n\nThe term refers to the null invariance of the measures of translates, associating it with the complete invariance found with Haar measure.\n\nSome algebraic properties of topological groups have been related to the size of subsets and Haar null sets.\nHaar null sets have been used to in Polish groups to show that when \"A\" is not a meagre set then \"A\"\"A\" contains an open neighborhood of the identity element. This property is named for Hugo Steinhaus since it is the conclusion of the Steinhaus theorem.\n\n\n"}
{"id": "9625798", "url": "https://en.wikipedia.org/wiki?curid=9625798", "title": "Quadratics", "text": "Quadratics\n\nQuadratics is a six-part Canadian instructional television series produced by TVOntario in 1993. The miniseries is part of the \"Concepts in Mathematics\" series. The program uses computer animation to demonstrate quadratic equations and their corresponding functions in the Cartesian coordinate system.\n\nEach program involves two robots, Edie and Charon, who work on an assembly line in a high-tech factory. The robots discuss their desire to learn about quadratic equations, and they are subsequently provided with lessons that further their education.\n"}
{"id": "13463844", "url": "https://en.wikipedia.org/wiki?curid=13463844", "title": "Richardson's theorem", "text": "Richardson's theorem\n\nIn mathematics, Richardson's theorem establishes a limit on the extent to which an algorithm can decide whether certain mathematical expressions are equal. It states that for a certain fairly natural class of expressions, it is undecidable whether a particular expression \"E\" satisfies the equation \"E\" = 0, and similarly undecidable whether the functions defined by expressions \"E\" and \"F\" are everywhere equal (in fact \"E\" = \"F\" if and only if \"E\" - \"F\" = 0). It was proved in 1968 by computer scientist Daniel Richardson of the University of Bath.\n\nSpecifically, the class of expressions for which the theorem holds is that generated by rational numbers, the number π, the number log 2, the variable \"x\", the operations of addition, subtraction, multiplication, composition, and the sin, exp, and abs functions.\n\nFor some classes of expressions (generated by other primitives than in Richardson's theorem) there exist algorithms that can determine whether an expression is zero.\n\nRichardson's theorem can be stated as follows: \nLet \"E\" be a set of expressions in the variable \"x\" which contains \"x\" and, as constant expressions, all rational numbers, and is such that if \"A(x)\" and \"B(x)\" are in \"E\", then \"A(x)\" + \"B(x)\", \"A(x)\" - \"B(x)\", \"A(x)B(x)\", and \"A(B(x))\" are also in \"E\". Then:\n\nAfter Hilbert's Tenth Problem was solved in 1970, B. F. Caviness observed that the use of \"e\" and log 2 could be removed.\nP. S. Wang later noted that under the same assumptions under which the question of whether there was \"x\" with \"A(x)\" < 0 was insoluble, the question of whether there was \"x\" with \"A(x)\" = 0 was also insoluble. \n\nMiklós Laczkovich removed also the need for π and reduced the use of composition. In particular, given an expression \"A(x)\" in the ring generated by the integers, \"x\", sin \"x\", and sin(\"x\" sin \"x\"), both the question of whether \"A(x)\" > 0 for some \"x\" and whether \"A(x)\" = 0 for some \"x\" are unsolvable.\n\nBy contrast, the Tarski–Seidenberg theorem says that the first-order theory of the real field is decidable, so it is not possible to remove the sine function entirely.\n\n\n"}
{"id": "9710396", "url": "https://en.wikipedia.org/wiki?curid=9710396", "title": "Riemann–Hilbert problem", "text": "Riemann–Hilbert problem\n\nIn mathematics, Riemann–Hilbert problems, named after Bernhard Riemann and David Hilbert, are a class of problems that arise in the study of differential equations in the complex plane. Several existence theorems for Riemann–Hilbert problems have been produced by Mark Krein, Israel Gohberg and others (see the book by Clancey and Gohberg (1981)).\n\nSuppose that formula_1 is a closed simple contour in the complex plane dividing the plane into two parts denoted by formula_2 (the inside) and formula_3 (the outside), determined by the index of the contour with respect to a point. The classical problem, considered in Riemann's PhD dissertation (see ), was that of finding a function\n\nanalytic inside formula_2 such that the boundary values of \"M\" along formula_1 satisfy the equation\n\nfor all formula_8, where \"a\", \"b\", and \"c\" are given real-valued functions .\n\nBy the Riemann mapping theorem, it suffices to consider the case when formula_1 is the unit circle . In this case, one may seek \"M\"(\"z\") along with its Schwarz reflection:\n\nOn the unit circle Σ, one has formula_11, and so\n\nHence the problem reduces to finding a pair of functions \"M\"(\"z\") and \"M\"(\"z\") analytic, respectively, on the inside and the outside of the unit disc, so that on the unit circle\n\nand, moreover, so that the condition at infinity holds:\n\nBecause the solution \"M\" of a Riemann–Hilbert factorization problem is unique\n(an easy application of Liouville's theorem (complex analysis)), the Sokhotski–Plemelj theorem\ngives the solution. We get \n\ni.e. formula_16\nwhich has a branch cut at contour formula_1.\n\nCheck:\nformula_18\n\ntherefore, formula_19.\n\nCAVEAT: If the problem is not scalar one cannot take logarithms. In general explicit solutions are very rare.\n\n"}
{"id": "38212767", "url": "https://en.wikipedia.org/wiki?curid=38212767", "title": "Risk cybernetics", "text": "Risk cybernetics\n\nRisk cybernetics by Finamatrix.com (Author: Lanz Chan, Ph.D.) is a risk management blockchain project comprising risk specification and risk control techniques using advanced artificial intelligence and computing technologies with circular-causal volatility-feedback in a genetic algorithm neural network (GANN) framework. More generally, risk cybernetics refers to risk management techniques which combine human and computer capabilities and functions in a circular-causal network/system. The objective of risk cybernetics is to achieve self-learning, self-enhancing and full-automation capabilities so as to reduce accidents, errors, etc. and obtain predictable and sustainable returns which can be applied to any industry including applications in market data, financial time series, cyber security measures, robotics, etc. \n"}
{"id": "799405", "url": "https://en.wikipedia.org/wiki?curid=799405", "title": "Shear mapping", "text": "Shear mapping\n\nIn plane geometry, a shear mapping is a linear map that displaces each point in fixed direction, by an amount proportional to its signed distance from a line that is parallel to that direction. This type of mapping is also called shear transformation, transvection, or just shearing.\n\nAn example is the mapping that takes any point with coordinates formula_1 to the point formula_2. In this case, the displacement is horizontal, the fixed line is the formula_3-axis, and the signed distance is the formula_4 coordinate. Note that points on opposite sides of the reference line are displaced in opposite directions.\n\nShear mappings must not be confused with rotations. Applying a shear map to a set of points of the plane will change all angles between them (except straight angles), and the length of any line segment that is not parallel to the direction of displacement. Therefore it will usually distort the shape of a geometric figure, for example turning squares into non-square parallelograms, and circles into ellipses. However a shearing does preserve the area of geometric figures and the alignment and relative distances of collinear points. A shear mapping is the main difference between the upright and slanted (or italic) styles of letters.\nThe same definition is used in three-dimensional geometry, except that the distance is measured from a fixed plane. A three-dimensional shearing transformation preserves the volume of solid figures, but changes areas of plane figures (except those that are parallel to the displacement). \nThis transformation is used to describe laminar flow of a fluid between plates, one moving in a plane above and parallel to the first.\n\nIn the general formula_5-dimensional Cartesian space formula_6, the distance is measured from a fixed hyperplane parallel to the direction of displacement. This geometric transformation is a linear transformation of formula_6 that preserves the formula_5-dimensional measure (hypervolume) of any set.\n\nIn the plane formula_9, a horizontal shear (or shear parallel to the \"x\" axis) is a function that takes a generic point with coordinates formula_1 to the point formula_11; where formula_12 is a fixed parameter, called the shear factor.\n\nThe effect of this mapping is to displace every point horizontally by an amount proportionally to its formula_4 coordinate. Any point above the formula_3-axis is displaced to the right (increasing formula_3) if formula_16, and to the left if formula_17. Points below the formula_3-axis move in the opposite direction, while points on the axis stay fixed.\n\nStraight lines parallel to the formula_3-axis remain where they are, while all other lines are turned, by various angles, about the point where they cross the formula_3-axis. Vertical lines, in particular, become oblique lines with slope formula_21. Therefore the shear factor formula_12 is the cotangent of the angle formula_23 by which the vertical lines tilt, called the shear angle.\n\nIf the coordinates of a point are written as a column vector (a 2×1 matrix), the shear mapping can be written as multiplication by a 2×2 matrix:\n\nA vertical shear (or shear parallel to the formula_4-axis) of lines is similar, except that the roles of formula_3 and formula_4 are swapped. It corresponds to multiplying the coordinate vector by the transposed matrix:\n\nThe vertical shear displaces points to the right of the formula_4-axis up or down, depending on the sign of formula_12. It leaves vertical lines invariant, but tilts all other lines about the point where they meet the formula_4-axis. Horizontal lines, in particular, get tilted by the shear angle formula_23 to become lines with slope formula_12.\n\nFor a vector space \"V\" and subspace \"W\", a shear fixing \"W\" translates all vectors parallel to \"W\".\n\nTo be more precise, if \"V\" is the direct sum of \"W\" and \"W′\", and we write vectors as\n\ncorrespondingly, the typical shear fixing \"W\" is \"L\" where\n\nwhere \"M\" is a linear mapping from \"W′\" into \"W\". Therefore in block matrix terms \"L\" can be represented as\n\nThe following applications of shear mapping were noted by William Kingdon Clifford:\n\nThe area-preserving property of a shear mapping can be used for results involving area. For instance, the Pythagorean theorem has been illustrated with shear mapping as well as the related geometric mean theorem.\n\nAn algorithm due to Alan W. Paeth uses a sequence of three shear mappings (horizontal, vertical, then horizontal again) to rotate a digital image by an arbitrary angle. The algorithm is very simple to implement, and very efficient, since each step processes only one column or one row of pixels at a time.\n\nThe oblique type can be thought of as normal text under a shear.\n\n"}
{"id": "31077099", "url": "https://en.wikipedia.org/wiki?curid=31077099", "title": "Sir Henry Percy Gordon, 2nd Baronet", "text": "Sir Henry Percy Gordon, 2nd Baronet\n\nSir Henry Percy Gordon, 2nd Baronet FRS (21 October 1806 – 29 July 1876).\n\nHe was the only son of Sir James Willoughby Gordon, 1st Baronet, succeeding to his father's title in 1851.\n\nHe entered Peterhouse, Cambridge in 1823 and was senior wrangler and 2nd Smith's prizeman in 1827. He received an M.A. in 1830.\n\nHe became a Fellow of the Royal Society in 1830.\n\nHe was admitted to Lincoln's Inn in 1828 and called to the bar in 1831. He was a Justice of the peace and deputy lieutenant for the Isle of Wight.\n\nIn 1839 he married Lady Mary Agnes Blanche Ashburnham, daughter of George Ashburnham, 3rd Earl of Ashburnham.\n\nHe had at least one daughter, Mary Charlotte Julia Gordon. At his death, the baronetcy became extinct.\n"}
{"id": "36670", "url": "https://en.wikipedia.org/wiki?curid=36670", "title": "Soma cube", "text": "Soma cube\n\nThe Soma cube is a solid dissection puzzle invented by Piet Hein in 1933 during a lecture on quantum mechanics conducted by Werner Heisenberg. Its name is alleged to be derived from the fictitious drug \"soma\" consumed as a pastime by the establishment in Aldous Huxley's dystopic novel \"Brave New World\".\n\nSeven pieces made out of unit cubes must be assembled into a 3×3×3 cube. The pieces can also be used to make a variety of other 3D shapes.\n\nThe pieces of the Soma cube consist of all possible combinations of three or four unit cubes, joined at their faces, such that at least one inside corner is formed. There is one combination of three cubes that satisfies this condition, and six combinations of four cubes that satisfy this condition, of which two are mirror images of each other (see Chirality). Thus, 3 + (6 × 4) is 27, which is exactly the number of cells in a 3×3×3 cube.\n\nThe Soma cube was analyzed in detail by John Horton Conway on September 1958 in Mathematical Games column in \"Scientific American\", and the book \"Winning Ways for your Mathematical Plays\" also contains a detailed analysis of the Soma cube problem.\n\nThere are 240 distinct solutions of the Soma cube puzzle, excluding rotations and reflections: these are easily generated by a simple recursive backtracking search computer program similar to that used for the eight queens puzzle. Current world record for the fastest time to solve a soma cube is 2.93 seconds and was set by Krishnam Raju Gadiraju, India.\n\nThe seven Soma pieces are six polycubes of order four, and one of order three:\n\nPiet Hein authorized a finely crafted rosewood version of the Soma cube manufactured by Theodor Skjøde Knudsen's company Skjøde Skjern (of Denmark). Beginning in about 1967, it was marketed in the U.S. for several years by the game manufacturer Parker Brothers. Plastic Soma cube sets were also commercially produced by Parker Brothers in several colors (blue, red, and orange) during the 1970s. The package for the Parker Brothers version claimed there were 1,105,920 possible solutions. This figure includes rotations and reflections of each solution as well as rotations of the individual pieces. The puzzle is currently sold as a logic game by ThinkFun (formerly Binary Arts) under the name Block by Block.\n\nSolving the Soma cube has been used as a task to measure individuals' performance and effort in a series of psychology experiments. In these experiments, test subjects are asked to solve a soma cube as many times as possible within a set period of time. For example, In 1969, Edward Deci, a Carnegie Mellon University graduate assistant at the time, asked his research subjects to solve a soma cube under conditions with varying incentives in his dissertation work on intrinsic and extrinsic motivation establishing the social psychological theory of crowding out.\n\nIn each of the 240 solutions to the cube puzzle, there is only one place that the \"T\" piece can be placed. Each solved cube can be rotated such that the \"T\" piece is on the bottom with its long edge along the front and the \"tongue\" of the \"T\" in the bottom center cube (this is the normalized position of the large cube). This can be proven as follows: If you consider all the possible ways that the \"T\" piece can be placed in the large cube (without regard to any of the other pieces), it will be seen that it will always fill either two corners of the large cube or zero corners. There is no way to orient the \"T\" piece such that it fills only one corner of the large cube. The \"L\" piece can be oriented such that it fills two corners, or one corner, or zero corners. Each of the other five pieces have no orientation that fills two corners; they can fill either one corner or zero corners. Therefore, if you exclude the \"T\" piece, the maximum number of corners that can be filled by the remaining six pieces is seven (one corner each for five pieces, plus two corners for the \"L\" piece). A cube has eight corners. But the \"T\" piece cannot be oriented to fill just that one remaining corner, and orienting it such that it fills zero corners will obviously not make a cube. Therefore, the \"T\" must always fill two corners, and there is only one orientation (discounting rotations and reflections) in which it does that. It also follows from this that in all solutions, five of the remaining six pieces will fill their maximum number of corners and one piece will fill one fewer than its maximum (this is called the deficient piece).\n\nSimilar to Soma cube is the 3D pentomino puzzle, which can fill boxes of 2×3×10, 2×5×6 and 3×4×5 units.\n\nThe Bedlam cube is a 4×4×4 sided cube puzzle consisting of twelve pentacubes and one tetracube. The Diabolical cube is a puzzle of six polycubes that can be assembled together to form a single 3×3×3 cube.\n\n\n"}
{"id": "35891416", "url": "https://en.wikipedia.org/wiki?curid=35891416", "title": "SpiNNaker", "text": "SpiNNaker\n\nSpiNNaker (Spiking Neural Network Architecture) is a massively parallel, manycore supercomputer architecture designed by the Advanced Processor Technologies Research Group (APT) at the School of Computer Science, University of Manchester. It is composed of 57,600 ARM9 processors (specifically ARM968), each with 18 cores and 128MB of mobile DDR SDRAM, totaling 1,036,800 cores and over 7TB of RAM. The computing platform is based on spiking neural networks, useful in simulating the human brain (see Human Brain Project).\n\nThe completed design is housed in 10 19-inch racks, with each rack holding over 100,000 cores. The cards holding the chips are held in 5 Blade enclosures, and each core emulates 1000 Neurons. In total, the goal is to simulate the behavior of aggregates of up to a billion neurons in real time. This machine requires about 100kW from a 240V supply and an air-conditioned environment.\n\nSpiNNaker is being used as one component of the neuromorphic computing platform for the Human Brain Project.\n\nOn October 14, 2018 the HBP announced that the million core milestone had been achieved.\n\n"}
{"id": "21464146", "url": "https://en.wikipedia.org/wiki?curid=21464146", "title": "Symposium on Theory of Computing", "text": "Symposium on Theory of Computing\n\nThe Annual ACM Symposium on Theory of Computing (STOC) is an academic conference in the field of theoretical computer science. STOC has been organized annually since 1969, typically in May or June; the conference is sponsored by the Association for Computing Machinery special interest group SIGACT. Acceptance rate of STOC, averaged from 1970 to 2012, is 31%, with the rate of 29% in 2012.\n\nAs writes, STOC and its annual IEEE counterpart FOCS (the Symposium on Foundations of Computer Science) are considered the two top conferences in theoretical computer science, considered broadly: they “are forums for some of the best work throughout theory of computing that promote breadth among theory of computing researchers and help to keep the community together.” includes regular attendance at STOC and FOCS as one of several defining characteristics of theoretical computer scientists.\n\nThe Gödel Prize for outstanding papers in theoretical computer science is presented alternately at STOC and at the International Colloquium on Automata, Languages and Programming (ICALP); the Knuth Prize for outstanding contributions to the foundations of computer science is presented alternately at STOC and at FOCS.\n\nSince 2003, STOC has presented one or more Best Paper Awards to recognize papers of the highest quality at the conference. In addition, the Danny Lewin Best Student Paper Award is awarded to the author(s) of the best student-authored paper in STOC. The award is named in honor of Daniel M. Lewin, an American-Israeli mathematician and entrepreneur who co-founded internet company Akamai Technologies, and was one of the first victims of the September 11 attacks.\n\nSTOC was first organised on 5–7 May 1969, in Marina del Rey, California, United States. The conference chairman was Patrick C. Fischer, and the program committee consisted of Michael A. Harrison, Robert W. Floyd, Juris Hartmanis, Richard M. Karp, Albert R. Meyer, and Jeffrey D. Ullman.\n\nEarly seminal papers in STOC include , which introduced the concept of NP-completeness (see also Cook–Levin theorem).\n\nSTOC was organised in Canada in 1992, 1994, 2002, and 2008, and in Greece in 2001; all other meetings in 1969–2009 have been held in the United States. STOC was part of the Federated Computing Research Conference (FCRC) in 1993, 1996, 1999, 2003, 2007, and 2011.\n\n\n\n\n\n\n\n\n\n"}
{"id": "47274958", "url": "https://en.wikipedia.org/wiki?curid=47274958", "title": "Ternary equivalence relation", "text": "Ternary equivalence relation\n\nIn mathematics, a ternary equivalence relation is a kind of ternary relation analogous to a binary equivalence relation. A ternary equivalence relation is symmetric, reflexive, and transitive. The classic example is the relation of collinearity among three points in Euclidean space. In an abstract set, a ternary equivalence relation determines a collection of equivalence classes or \"pencils\" that form a linear space in the sense of incidence geometry. In the same way, a binary equivalence relation on a set determines a partition.\n\nA ternary equivalence relation on a set is a relation , written , that satisfies the following axioms:\n\n"}
{"id": "30402", "url": "https://en.wikipedia.org/wiki?curid=30402", "title": "Theory of computation", "text": "Theory of computation\n\nIn theoretical computer science and mathematics, the theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches: automata theory and languages, computability theory, and computational complexity theory, which are linked by the question: \"\"What are the fundamental capabilities and limitations of computers?\".\"\n\nIn order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a model of computation. There are several models in use, but the most commonly examined is the Turing machine. Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible \"reasonable\" model of computation (see Church–Turing thesis). It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any decidable problem solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.\n\nThe theory of computation can be considered the creation of models of all kinds in the field of computer science. Therefore, mathematics and logic are used. In the last century it became an independent academic discipline and was separated from mathematics.\n\nSome pioneers of the theory of computation were Alonzo Church, Kurt Gödel, Alan Turing, Stephen Kleene, Rózsa Péter, John von Neumann and Claude Shannon.\n\nAutomata theory is the study of abstract machines (or more appropriately, abstract 'mathematical' machines or systems) and the computational problems that can be solved using these machines. These abstract machines are called automata. Automata comes from the Greek word (Αυτόματα) which means that something is doing something by itself.\nAutomata theory is also closely related to formal language theory, as the automata are often classified by the class of formal languages they are able to recognize. An automaton can be a finite representation of a formal language that may be an infinite set. Automata are used as theoretical models for computing machines, and are used for proofs about computability.\n\nLanguage theory is a branch of mathematics concerned with describing languages as a set of operations over an alphabet. It is closely linked with automata theory, as automata are used to generate and recognize formal languages. There are several classes of formal languages, each allowing more complex language specification than the one before it, i.e. Chomsky hierarchy, and each corresponding to a class of automata which recognizes it. Because automata are used as models for computation, formal languages are the preferred mode of specification for any problem that must be computed.\n\nComputability theory deals primarily with the question of the extent to which a problem is solvable on a computer. The statement that the halting problem cannot be solved by a Turing machine is one of the most important results in computability theory, as it is an example of a concrete problem that is both easy to formulate and impossible to solve using a Turing machine. Much of computability theory builds on the halting problem result.\n\nAnother important step in computability theory was Rice's theorem, which states that for all non-trivial properties of partial functions, it is undecidable whether a Turing machine computes a partial function with that property.\n\nComputability theory is closely related to the branch of mathematical logic called recursion theory, which removes the restriction of studying only models of computation which are reducible to the Turing model. Many mathematicians and computational theorists who study recursion theory will refer to it as computability theory.\n\nComplexity theory considers not only whether a problem can be solved at all on a computer, but also how efficiently the problem can be solved. Two major aspects are considered: time complexity and space complexity, which are respectively how many steps does it take to perform a computation, and how much memory is required to perform that computation.\n\nIn order to analyze how much time and space a given algorithm requires, computer scientists express the time or space required to solve the problem as a function of the size of the input problem. For example, finding a particular number in a long list of numbers becomes harder as the list of numbers grows larger. If we say there are \"n\" numbers in the list, then if the list is not sorted or indexed in any way we may have to look at every number in order to find the number we're seeking. We thus say that in order to solve this problem, the computer needs to perform a number of steps that grows linearly in the size of the problem.\n\nTo simplify this problem, computer scientists have adopted Big O notation, which allows functions to be compared in a way that ensures that particular aspects of a machine's construction do not need to be considered, but rather only the asymptotic behavior as problems become large. So in our previous example we might say that the problem requires formula_1 steps to solve.\n\nPerhaps the most important open problem in all of computer science is the question of whether a certain broad class of problems denoted NP can be solved efficiently. This is discussed further at Complexity classes P and NP, and P versus NP problem is one of the seven Millennium Prize Problems stated by the Clay Mathematics Institute in 2000. The Official Problem Description was given by Turing Award winner Stephen Cook.\n\nAside from a Turing machine, other equivalent (See: Church–Turing thesis) models of computation are in use.\n\n\n\n\n\nIn addition to the general computational models, some simpler computational models are useful for special, restricted applications. Regular expressions, for example, specify string patterns in many contexts, from office productivity software to programming languages. Another formalism mathematically equivalent to regular expressions, Finite automata are used in circuit design and in some kinds of problem-solving. Context-free grammars specify programming language syntax. Non-deterministic pushdown automata are another formalism equivalent to context-free grammars. Primitive recursive functions are a defined subclass of the recursive functions.\n\nDifferent models of computation have the ability to do different tasks. One way to measure the power of a computational model is to study the class of formal languages that the model can generate; in such a way to the Chomsky hierarchy of languages is obtained.\n\n(There are many textbooks in this area; this list is by necessity incomplete.)\n\n\n\n"}
{"id": "1904373", "url": "https://en.wikipedia.org/wiki?curid=1904373", "title": "Transformation (function)", "text": "Transformation (function)\n\nIn mathematics, particularly in semigroup theory, a transformation is a function \"f\" that maps a set \"X\" to itself, i.e. . In other areas of mathematics, a transformation may simply be any function, regardless of domain and codomain. This wider sense shall not be considered in this article; refer instead to the article on function for that sense.\n\nExamples include linear transformations and affine transformations, rotations, reflections and translations. These can be carried out in Euclidean space, particularly in (two dimensions) and (three dimensions). They are also operations that can be performed using linear algebra, and described explicitly using matrices.\n\nA translation, or translation operator, is an affine transformation of Euclidean space which moves every point by a fixed distance in the same direction. It can also be interpreted as the addition of a constant vector to every point, or as shifting the origin of the coordinate system. In other words, if v is a fixed vector, then the translation \"T\" will work as \"T\"(p) = p + v.\n\nThe two interpretations of a translation lead to two related but different coordinate transformations. To illustrate this the examples will be restricted to the two dimensional case for simplicity, but the argument holds in any dimension.\n\nLet \"P\"(\"x\", \"y\") be a point in the plane and apply the translation (\"h\", \"k\") to obtain a new point \"P\"' with coordinates (\"X\", \"Y\"). It follows from the definition that\nand\n\nNow consider a point \"P\"(\"x\", \"y\") in the plane, whose coordinates are determined with respect to a given pair of axes. Suppose the axes are shifted from their original position by (\"h\", \"k\") and the shifted axes are taken as the new reference axes. The point \"P\" now has coordinates (\"X\", \"Y\") with respect to the new reference axes. To obtain the coordinates of \"P\" with respect to the new reference axes from the coordinates of \"P\" with respect to the original reference axes, these \"formulas of translation\" are used ():\n\nand\n\nReplacing the original coordinates, that is, \"x\" and \"y\", with these expressions in an equation of an object in the original coordinates, will produce the transformed equation for the same object with respect to the new reference axes.\n\nThe relationship that holds here is that each of the coordinate transformations is the inverse function of the other.\n\nA reflection is a map that transforms an object into its mirror image with respect to a \"mirror\", which is a hyperplane of fixed points in the geometry. For example, a reflection of the small Latin letter p with respect to a vertical line would look like a \"q\". In order to reflect a planar figure one needs the \"mirror\" to be a line (\"axis of reflection\" or \"axis of symmetry\"), while for reflections in the three-dimensional space one would use a plane (the \"plane of reflection\" or \"symmetry\") for a mirror. Reflection may be considered as the limiting case of inversion as the radius of the reference circle increases without bound.\n\nReflection is considered to be an \"opposite\" motion since it changes the orientation of the figures it reflects.\n\nA glide reflection is a type of isometry of the Euclidean plane: the combination of a reflection in a line and a translation along that line. Reversing the order of combining gives the same result. Depending on context, we may consider a simple reflection (without translation) as a special case where the translation vector is the zero vector.\nA rotation is a transformation that is performed by \"spinning\" the object around a fixed point known as the center of rotation. You can rotate the object at any degree measure, but 90° and 180° are two of the most common. Rotation by a positive angle rotates the object counterclockwise, whereas rotation by a negative angle rotates the object clockwise.\n\nUniform scaling is a linear transformation that enlarges or diminishes objects; the scale factor is the same in all directions; it is also called a homothety or dilation. The result of uniform scaling is similar (in the geometric sense) to the original.\n\nMore general is scaling with a separate scale factor for each axis direction; a special case is directional scaling (in one direction). Shapes not aligned with the axes may be subject to shear (see below) as a side effect: although the angles between lines parallel to the axes are preserved, other angles are not.\n\nShear is a transform that effectively rotates one axis so that the axes are no longer perpendicular. Under shear, a rectangle becomes a parallelogram, and a circle becomes an ellipse. Even if lines parallel to the axes stay the same length, others do not. As a mapping of the plane, it lies in the class of equi-areal mappings.\n\nMore generally, a transformation in mathematics means a mathematical function (synonyms: \"map\" and \"mapping\"). A transformation can be an invertible function from a set \"X\" to itself, or from \"X\" to another set \"Y\". The choice of the term \"transformation\" may simply flag that a function's more geometric aspects are being considered (for example, with attention paid to invariants).\n\nThe notion of transformation generalized to partial functions. A partial transformation is a function \"f\": \"A\" → \"B\", where both \"A\" and \"B\" are subsets of some set \"X\".\n\nThe set of all transformations on a given base set together with function composition forms a regular semigroup.\n\nFor a finite set of cardinality \"n\", there are \"n\" transformations and (\"n\"+1) partial transformations.\n\n"}
{"id": "186023", "url": "https://en.wikipedia.org/wiki?curid=186023", "title": "Unifying theories in mathematics", "text": "Unifying theories in mathematics\n\nThere have been several attempts in history to reach a unified theory of mathematics. Some of the greatest mathematicians have expressed views that the whole subject should be fitted into one theory.\n\nThe process of unification might be seen as helping to define what constitutes mathematics as a discipline.\n\nFor example, mechanics and mathematical analysis were commonly combined into one subject during the 18th century, united by the differential equation concept; while algebra and geometry were considered largely distinct. Now we consider analysis, algebra, and geometry, but not mechanics, as parts of mathematics because they are primarily deductive formal sciences, while mechanics like physics must proceed from observation. There is no major loss of content, with analytical mechanics in the old sense now expressed in terms of symplectic topology, based on the newer theory of manifolds.\n\nThe term \"theory\" is used informally within mathematics to mean a self-consistent body of definitions, axioms, theorems, examples, and so on. (Examples include group theory, Galois theory, control theory, and K-theory.) In particular there is no connotation of \"hypothetical\". Thus the term \"unifying theory\" is more like a sociological term used to study the actions of mathematicians. It may assume nothing conjectural that would be analogous to an undiscovered scientific link. There is really no cognate within mathematics to such concepts as \"Proto-World\" in linguistics or the Gaia hypothesis.\n\nNonetheless there have been several episodes within the history of mathematics in which sets of individual theorems were found to be special cases of a single unifying result, or in which a single perspective about how to proceed when developing an area of mathematics could be applied fruitfully to multiple branches of the subject.\n\nA well-known example was the development of analytic geometry, which in the hands of mathematicians such as Descartes and Fermat showed that many theorems about curves and surfaces of special types could be stated in algebraic language (then new), each of which could then be proved using the same techniques. That is, the theorems were very similar algebraically, even if the geometrical interpretations were distinct.\n\nIn 1859 Arthur Cayley initiated a unification of metric geometries through use of the Cayley-Klein metrics. Later Felix Klein used such metrics to provide a foundation for non-Euclidean geometry.\n\nIn 1872, Felix Klein noted that the many branches of geometry which had been developed during the 19th century (affine geometry, projective geometry, hyperbolic geometry, etc.) could all be treated in a uniform way. He did this by considering the groups under which the geometric objects were invariant. This unification of geometry goes by the name of the Erlangen programme.\n\nEarly in the 20th century, many parts of mathematics began to be treated by delineating useful sets of axioms and then studying their consequences. Thus, for example, the studies of \"hypercomplex numbers\", such as considered by the Quaternion Society, were put onto an axiomatic footing as branches of ring theory (in this case, with the specific meaning of associative algebras over the field of complex numbers.) In this context, the quotient ring concept is one of the most powerful unifiers.\n\nThis was a general change of methodology, since the needs of applications had up until then meant that much of mathematics was taught by means of algorithms (or processes close to being algorithmic). Arithmetic is still taught that way. It was a parallel to the development of mathematical logic as a stand-alone branch of mathematics. By the 1930s symbolic logic itself was adequately included within mathematics.\n\nIn most cases, mathematical objects under study can be defined (albeit non-canonically) as sets or, more informally, as sets with additional structure such as an addition operation. Set theory now serves as a \"lingua franca\" for the development of mathematical themes.\n\nThe cause of axiomatic development was taken up in earnest by the Bourbaki group of mathematicians. Taken to its extreme, this attitude was thought to demand mathematics developed in its greatest generality. One started from the most general axioms, and then specialized, for example, by introducing modules over commutative rings, and limiting to vector spaces over the real numbers only when absolutely necessary. The story proceeded in this fashion, even when the specializations were the theorems of primary interest.\n\nIn particular, this perspective placed little value on fields of mathematics (such as combinatorics) whose objects of study are very often special, or found in situations which can only superficially be related to more axiomatic branches of the subject.\n\nCategory theory is a unifying theory of mathematics that was initially developed in the second half of the 20th century. In this respect it is an alternative and complement to set theory. A key theme from the \"categorical\" point of view is that mathematics requires not only certain kinds of objects (Lie groups, Banach spaces, etc.) but also mappings between them that preserve their structure.\n\nIn particular, this clarifies exactly what it means for mathematical objects to be considered to be \"the same\". (For example, are all equilateral triangles \"the same\", or does size matter?) Saunders Mac Lane proposed that any concept with enough 'ubiquity' (occurring in various branches of mathematics) deserved isolating and studying in its own right. Category theory is arguably better adapted to that end than any other current approach. The disadvantages of relying on so-called \"abstract nonsense\" are a certain blandness and abstraction in the sense of breaking away from the roots in concrete problems. Nevertheless, the methods of category theory have steadily advanced in acceptance, in numerous areas (from D-modules to categorical logic).\n\nOn a less grandiose scale, there are frequent instances in which it appears that sets of results in two different branches of mathematics are similar, and one might ask whether there is a unifying framework which clarifies the connections. We have already noted the example of analytic geometry, and more generally the field of algebraic geometry thoroughly develops the connections between geometric objects (algebraic varieties, or more generally schemes) and algebraic ones (ideals); the touchstone result here is Hilbert's Nullstellensatz which roughly speaking shows that there is a natural one-to-one correspondence between the two types of objects.\n\nOne may view other theorems in the same light. For example, the fundamental theorem of Galois theory asserts that there is a one-to-one correspondence between extensions of a field and subgroups of the field's Galois group. The Taniyama–Shimura conjecture for elliptic curves (now proven) establishes a one-to-one correspondence between curves defined as modular forms and elliptic curves defined over the rational numbers. A research area sometimes nicknamed Monstrous Moonshine developed connections between modular forms and the finite simple group known as the Monster, starting solely with the surprise observation that in each of them the rather unusual number 196884 would arise very naturally. Another field, known as the Langlands program, likewise starts with apparently haphazard similarities (in this case, between number-theoretical results and representations of certain groups) and looks for constructions from which both sets of results would be corollaries.\n\nA short list of these theories might include:\n\nA well-known example is the Taniyama–Shimura conjecture, now the modularity theorem, which proposed that each elliptic curve over the rational numbers can be translated into a modular form (in such a way as to preserve the associated L-function). There are difficulties in identifying this with an isomorphism, in any strict sense of the word. Certain curves had been known to be both elliptic curves (of genus 1) and modular curves, before the conjecture was formulated (about 1955). The surprising part of the conjecture was the extension to factors of Jacobians of modular curves of genus > 1. It had probably not seemed plausible that there would be 'enough' such rational factors, before the conjecture was enunciated; and in fact the numerical evidence was slight until around 1970, when tables began to confirm it. The case of elliptic curves with complex multiplication was proved by Shimura in 1964. This conjecture stood for decades before being proved in generality.\n\nIn fact the Langlands program (or philosophy) is much more like a web of unifying conjectures; it really does postulate that the general theory of automorphic forms is regulated by the L-groups introduced by Robert Langlands. His \"principle of functoriality\" with respect to the L-group has a very large explanatory value with respect to known types of \"lifting\" of automorphic forms (now more broadly studied as automorphic representations). While this theory is in one sense closely linked with the Taniyama–Shimura conjecture, it should be understood that the conjecture actually operates in the opposite direction. It requires the existence of an automorphic form, starting with an object that (very abstractly) lies in a category of motives.\n\nAnother significant related point is that the Langlands approach stands apart from the whole development triggered by monstrous moonshine (connections between elliptic modular functions as Fourier series, and the group representations of the Monster group and other sporadic groups). The Langlands philosophy neither foreshadowed nor was able to include this line of research.\n\nAnother case, which so far is less well-developed but covers a wide range of mathematics, is the conjectural basis of some parts of K-theory. The Baum–Connes conjecture, now a long-standing problem, has been joined by others in a group known as the isomorphism conjectures in K-theory. These include the Farrell–Jones conjecture and Bost conjecture.\n\n"}
{"id": "1591416", "url": "https://en.wikipedia.org/wiki?curid=1591416", "title": "Valentina Borok", "text": "Valentina Borok\n\nValentina Mikhailovna Borok (9 July 1931, Kharkiv, Ukraine, USSR – 4 February\n2004, Haifa, Israel) was a Soviet Ukrainian mathematician. She is mainly known for her work on partial differential equations.\n\nBorok was born in July 9, 1931 in Kharkiv in Ukraine (then USSR), into a Jewish family. Her father, Michail Borok, was a chemist, scientist and an expert in material science. Her mother, Bella Sigal, was a well-known economist. Because of her mothers' high position at the ministry of Economics, Valentina Borok had a privileged early childhood. However, because of the political situation, her mother voluntarily resigned in 1937 and took a lower position, presumably because she knew she couldn't possibly have been spared the repressions of the late 1930s. This possibly helped the Borok family survive World War II.\n\nValentina Borok had a talent for math even in her high school years. So in 1949, with the advice of her high school teachers Borok started to study Mathematics at Kiev State University. There she met Yakov Zhitomirskii, who would be her husband until her death. During her stay at Kiev State University, Borok long with her future husband started her research in the field of mathematics under the supervision of the mathematics department supervisor, Georgii Shilov. Her undergraduate thesis on distribution theory and the applications to the theory of systems of linear partial differential equations was found to be extraordinary and was published in a top Russian journal. This thesis was later selected in 1957 to be part of the first volumes of American Mathematical Society translations.\nIn 1954, Borok graduated from Kiev State University and moved to Moscow State University in order to receive her graduate degree. In 1957, she received her PhD for her thesis \"On Systems of Linear Partial Differential Equations with Constant Coefficients\". The information about the system of Linear Partial Differential equations with constant coefficient was publicized in the annals of mathematics. She later published more papers from 1954 to 1959, which contained a range of inverse theorems that allowed partial differential equations to be characterized by certain properties of their solutions. “In the same period she obtained formula that made it possible to compute in simple algebraic terms the numerical parameters that determine classes of uniqueness and well-posed of the Cauchy problem for systems of linear partial differential equations with constant coefficients\". In 1960, she moved to Kharkiv State University, where she stayed until 1994. In 1970, Borok became a full professor and from 1983 to 1994, she was the Chair of the analysis department.\n\nIn the early 1960s Borok worked on the stability for partial differential equations well-posed. Her other works at this time were on the parabolic systems degnerating at infinity and on the dependence of classes of uniqueness on the transformations of the spatial argument. most of her works during this period of time were mostly joint works with her husband Yakov Zhitomirskii.\n\nAnd during the period of the late 1960s, Borok began her series of papers that laid the foundations for the theory of local and non-local boundary value problems in infinite layers for systems of partial differential equations. The results of her studies included the construction of maximal classes of uniqueness and well-posedness, Phragmen- Lindelöf type theorems, and the study of asymptotic properties and stability of solutions of boundary-value problems in infinite layers.\n\nStarting in the early 1970s, Borok opened a school for the study of the general theory of Partial Differential Equations in Kharkiv State University. Many of her papers helped the development of the theory of local and non-local boundary value problems in infinite layers for systems of Partial differential equations. One of her earliest works includes results on the uniqueness and well-posedness of the solutions of the Cauchy problem. Most of her works were concentrated in the area of Partial differential equations along with functional-differential equations. even to this day many of her works are being cited.\n\nDuring her years of being a professor at Kharkiv State University, Borok was considered the teacher of rigorous analysis, which was a course in which many of the students got their first taste in research. Borok was known for her \"creative problems\" as well as her development of original lecture notes for many of the core and specialized courses in analysis and Partial differential Equations. She set the curriculum of the mathematics department in Kharkiv State University for more than 30 years, setting the tradition in the university.\n\nIn 1994, Borok became severely ill but because there was no necessary medical attention available in Ukraine, she had to move to Haifa, Israel. She died at the age of 72 in 2004. Both of her children, Michail Zhitomirskii and Svetlana Jitomirskaya, became research mathematicians.\n\nBorok is known for her research and contribution on the partial differentiation equation. During her lifetime she published 80 papers in top Russian and Ukrainian journals as well as supervised 16 PhDs along with many master theses.\n\nMany of her thesis development included the studies of the Cauchy problem for the linear partial differential equations, which was published in the \"Annals of Mathematics\" explaining the theory behind the linear partial differential equation.\nIn other works she has proved the theorem on uniqueness and well-posedness theorems for the initial\nvalue problem as well as the Cauchy problem for system of linear partial differential equations.\n\nIn her studies, translated from Russian, in the Cauchy problem for systems of linear partial differential equations that are functional with respect to parameter, Her summary states that she proves that for the study in Cauchy problem for≠ system of equations of the form đu(x,y,z)/đt = P(đ/đx)u(x,t,ɖy), xɛRn, tɛ[0,T],y>0,ɖ>0, ɖ≠1, uɛCn, Where P(S) is an N x N Matrix with polynomial elements. We prove the existence of solutions of the homogeneous problem which exponentially converge to zero as |x|→∞ and for each y>0. she established estimates for the solutions as |x|→∞, y→∞ or y→+0 which guarantee its uniqueness. and she found conditions for the correct solvability of the problem in the class of solutions which are polynomial with respect to y.\n"}
{"id": "7280707", "url": "https://en.wikipedia.org/wiki?curid=7280707", "title": "Variable-length code", "text": "Variable-length code\n\nIn coding theory a variable-length code is a code which maps source symbols to a \"variable\" number of bits. \n\nVariable-length codes can allow sources to be compressed and decompressed with \"zero\" error (lossless data compression) and still be read back symbol by symbol. With the right coding strategy an independent and identically-distributed source may be compressed almost arbitrarily close to its entropy. This is in contrast to fixed length coding methods, for which data compression is only possible for large blocks of data, and any compression beyond the logarithm of the total number of possibilities comes with a finite (though perhaps arbitrarily small) probability of failure. \n\nSome examples of well-known variable-length coding strategies are Huffman coding, Lempel–Ziv coding and arithmetic coding.\n\nThe extension of a code is the mapping of finite length source sequences to finite length bit strings, that is obtained by concatenating for each symbol of the source sequence the corresponding codeword produced by the original code. \n\nUsing terms from formal language theory, the precise mathematical definition is as follows: Let formula_1 and formula_2 be two finite sets, called the source and target alphabets, respectively. A code formula_3 is a total function mapping each symbol from formula_1 to a sequence of symbols over formula_2, and the extension of formula_6 to a homomorphism of formula_7 into formula_8, which naturally maps each sequence of source symbols to a sequence of target symbols, is referred to as its extension.\n\nVariable-length codes can be strictly nested in order of decreasing generality as non-singular codes, uniquely decodable codes and prefix codes. Prefix codes are always uniquely decodable, and these in turn are always non-singular:\n\nA code is non-singular if each source symbol is mapped to a different non-empty bit string, i.e. the mapping from source symbols to bit strings is injective.\n\nA code is uniquely decodable if its extension is non-singular (see above). Whether a given code is uniquely decodable can be decided with the Sardinas–Patterson algorithm. \n\nA code is a prefix code if no target bit string in the mapping is a prefix of the target bit string of a different source symbol in the same mapping. This means that symbols can be decoded instantaneously after their entire codeword is received. Other commonly used names for this concept are prefix-free code, instantaneous code, or context-free code.\n\nA special case of prefix codes are block codes. Here all codewords must have the same length. The latter are not very useful in the context of source coding, but often serve as error correcting codes in the context of channel coding.\n\nAnother special case of prefix codes are variable-length quantity codes, which encode arbitrarily large integers as a sequence of octets -- i.e., every codeword is a multiple of 8 bits.\n\nThe advantage of a variable-length code is that unlikely source symbols can be assigned longer codewords and likely source symbols can be assigned shorter codewords, thus giving a low \"expected\" codeword length. For the above example, if the probabilities of (a, b, c, d) were formula_14, the expected number of bits used to represent a source symbol using the code above would be:\nAs the entropy of this source is 1.7500 bits per symbol, this code compresses the source as much as possible so that the source can be recovered with \"zero\" error.\n\n"}
{"id": "714050", "url": "https://en.wikipedia.org/wiki?curid=714050", "title": "Vieta's formulas", "text": "Vieta's formulas\n\nIn mathematics, Vieta's formulas are formulas that relate the coefficients of a polynomial to sums and products of its roots. Named after François Viète (more commonly referred to by the Latinised form of his name, Franciscus Vieta), the formulas are used specifically in algebra.\n\nAny general polynomial of degree \"n\"\n\n(with the coefficients being real or complex numbers and \"a\" ≠ 0) is known by the fundamental theorem of algebra to have \"n\" (not necessarily distinct) complex roots \"x\", \"x\", ..., \"x\". Vieta's formulas relate the polynomial's coefficients { \"a\" } to signed sums and products of its roots { \"x\" } as follows:\n\nEquivalently stated, the (\"n\" − \"k\")th coefficient \"a\" is related to a signed sum of all possible subproducts of roots, taken \"k\"-at-a-time:\n\nfor \"k\" = 1, 2, ..., \"n\" (where we wrote the indices \"i\" in increasing order to ensure each subproduct of roots is used exactly once).\n\nThe left hand sides of Vieta's formulas are the elementary symmetric functions of the roots.\n\nVieta's formulas are frequently used with polynomials with coefficients in any integral domain \"R\". Then, the quotients formula_4 belong to the ring of fractions of \"R\" (or in \"R\" itself if formula_5 is invertible in \"R\") and the roots formula_6 are taken in an algebraically closed extension. Typically, \"R\" is the ring of the integers, the field of fractions is the field of the rational numbers and the algebraically closed field is the field of the complex numbers.\n\nVieta's formulas are then useful because they provide relations between the roots without having to compute them.\n\nFor polynomials over a commutative ring which is not an integral domain, Vieta's formulas are only valid when formula_5 is a non-zerodivisor and formula_8 factors as formula_9. For example, in the ring of the integers modulo 8, the polynomial formula_10 has four roots: 1, 3, 5, and 7. Vieta's formulas are not true if, say, formula_11 and formula_12, because formula_13. However, formula_8 does factor as formula_15 and as formula_16, and Vieta's formulas hold if we set either formula_11 and formula_18 or formula_19 and formula_20.\n\nVieta's formulas applied to quadratic and cubic polynomial:\n\nThe roots formula_21 of the quadratic polynomial formula_22 satisfy\n\nThe first of these equations can be used to find the minimum (or maximum) of ; see .\n\nThe roots formula_24 of the cubic polynomial formula_25 satisfy\n\nVieta's formulas can be proved by expanding the equality\n\n(which is true since formula_28 are all the roots of this polynomial), multiplying the factors on the right-hand side, and identifying the coefficients of each power of formula_29\n\nFormally, if one expands formula_30 the terms are precisely formula_31 where formula_32 is either 0 or 1, accordingly as whether formula_6 is included in the product or not, and \"k\" is the number of formula_6 that are excluded, so the total number of factors in the product is \"n\" (counting \"formula_35\" with multiplicity \"k\") – as there are \"n\" binary choices (include formula_6 or \"x\"), there are formula_37 terms – geometrically, these can be understood as the vertices of a hypercube. Grouping these terms by degree yields the elementary symmetric polynomials in formula_6 – for \"x,\" all distinct \"k\"-fold products of formula_39\n\nAs reflected in the name, the formulas were discovered by the 16th century French mathematician François Viète, for the case of positive roots.\n\nIn the opinion of the 18th century British mathematician Charles Hutton, as quoted by Funkhouser, the general principle (not only for positive real roots) was first understood by the 17th century French mathematician Albert Girard: \n...[Girard was] the first person who understood the general doctrine of the formation of the coefficients of the powers from the sum of the roots and their products. He was the first who discovered the rules for summing the powers of the roots of any equation.\n\n\n"}
{"id": "1869136", "url": "https://en.wikipedia.org/wiki?curid=1869136", "title": "Volume-weighted average price", "text": "Volume-weighted average price\n\nIn finance, volume-weighted average price (VWAP) is the ratio of the value traded to total volume traded over a particular time horizon (usually one day). It is a measure of the average price at which a stock is traded over the trading horizon.\n\nVWAP is often used as a trading benchmark by investors who aim to be as passive as possible in their execution. Many pension funds, and some mutual funds, fall into this category. The aim of using a VWAP trading target is to ensure that the trader executing the order does so in-line with volume on the market. It is sometimes argued that such execution reduces transaction costs by minimizing market impact costs (the additional cost due to the market impact, i.e. the adverse effect of a trader's activities on the price of a security).\n\nVWAP can be measured between any two points in time but is displayed as the one corresponding to elapsed time during the trading day by the information provider.\n\nVWAP is often used in algorithmic trading. Indeed, a broker may guarantee execution of an order at the VWAP and have a computer program enter the orders into the market in order to earn the trader's commission and create P&L. This is called a guaranteed VWAP execution. The broker can also trade in a best effort way and answer to the client the realized price. This is called a VWAP target execution; it incurs more dispersion in the answered price compared to the VWAP price for the client but a lower received/paid commission. Trading algorithms that use VWAP as a target belong to a class of algorithms known as \"volume participation algorithms\".\n\nThe first execution of the VWAP was in 1984 for the Ford Motor Company by James Elkins, then head trader at Abel Noser. \n\nVWAP is calculated using the following formula:\n\nwhere:\n\nThe VWAP can be used similar to moving averages, where prices above the VWAP reflect a bullish sentiment and prices below the VWAP reflect a bearish sentiment. Traders may initiate short positions as a stock price moves below VWAP for a given time period or initiate long position as the price moves above VWAP\n\nInstitutional buyers and algorithms will often use VWAP to plan entries and initiate larger positions without disturbing the stock price.\n\n\n"}
{"id": "32044869", "url": "https://en.wikipedia.org/wiki?curid=32044869", "title": "Weapon target assignment problem", "text": "Weapon target assignment problem\n\nThe weapon target assignment problem (WTA) is a class of combinatorial optimization problems present in the fields of optimization and operations research. It consists of finding an optimal assignment of a set of weapons of various types to a set of targets in order to maximize the total expected damage done to the opponent.\n\nThe basic problem is as follows:\n\nNotice that as opposed to the classic assignment problem or the generalized assignment problem, more than one agent (i.e., weapon) can be assigned to each \"task\" (i.e., target) and not all targets are required to have weapons assigned. Thus, we see that the WTA allows one to formulate optimal assignment problems wherein tasks require cooperation among agents. Additionally, it provides the ability to model probabilistic completion of tasks in addition to costs.\n\nBoth static and dynamic versions of WTA can be considered. In the static case, the weapons are assigned to targets once. The dynamic case involves many rounds of assignment where the state of the system after each exchange of fire (round) is considered in the next round. While the majority of work has been done on the static WTA problem, recently the dynamic WTA problem has received more attention.\n\nIn spite of the name, there are nonmilitary applications of the WTA. The main one is to search for a lost object or person by heterogeneous assets such as dogs, aircraft, walkers, etc. The problem is to assign the assets to a partition of the space in which the object is located to minimize the probability of not finding the object. The \"value\" of each element of the partition is the probability that the object is located there.\n\nThe weapon target assignment problem is often formulated as the following nonlinear integer programming problem:\n\nsubject to the constraints\n\nWhere the variable formula_10 represents the assignment of as many weapons of type formula_3 to target formula_12 and formula_13 is the probability of survival (formula_14). The first constraint requires that the number of weapons of each type assigned does not exceed the number available. The second constraint is the integral constraint.\n\nNotice that minimizing the expected survival value is the same as maximizing the expected damage.\n\nAn exact solution can be found using branch and bound techniques which utilize relaxation (approximation). Many heuristic algorithms have been proposed which provide near-optimal solutions in polynomial time.\n\nA commander has 5 tanks, 2 aircraft, and 1 sea vessel and is told to engage 3 targets with values 5, 10, and 20. Each weapon type has the following success probabilities against each target:\nOne feasible solution is to assign the sea vessel and one aircraft to the highest valued target (3). This results in an expected survival value of formula_15. One could then assign the remaining aircraft and 2 tanks to target #2, resulting in expected survival value of formula_16. Finally, the remaining 3 tanks are assigned to target #1 which has an expected survival value of formula_17. Thus, we have a total expected survival value of formula_18. Note that a better solution can be achieved by assigning 3 tanks to target #1, 2 tanks and sea vessel to target #2 and 2 aircraft to target #3, giving an expected survival value of formula_19.\n\n"}
