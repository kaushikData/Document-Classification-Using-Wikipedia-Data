{"id": "1406077", "url": "https://en.wikipedia.org/wiki?curid=1406077", "title": "AA postulate", "text": "AA postulate\n\nIn Euclidean geometry, the AA postulate states that two triangles are similar if they have two corresponding angles congruent.\n\nThe AA postulate follows from the fact that the sum of the interior angles of a triangle is always equal to 180°. By knowing two angles, such as 32° and 64° degrees, we know that the next angle is 84°, because 180-(32+64)=84. (This is sometimes referred to as the AAA Postulate—which is true in all respects, but two angles are entirely sufficient.)\n\nThe postulate can be better understood by working in reverse order. The two triangles on grids A and B are similar, by a 1.5 dilation from A to B. If they are aligned, as in grid C, it is apparent that the angle on the origin is congruent with the other (D). We also know that the pair of sides opposite the origin are parallel. We know this because the pairs of sides around them are similar, stem from the same point, and line up with each other. We can then look at the sides around the parallels as transversals, and therefore the corresponding angles are congruent. Using this reasoning we can tell that similar triangles have congruent angles.\n\n"}
{"id": "7020888", "url": "https://en.wikipedia.org/wiki?curid=7020888", "title": "Affine action", "text": "Affine action\n\nLet formula_1 be the Weyl group of a semisimple Lie algebra formula_2 (associate to fixed choice of a Cartan subalgebra formula_3). Assume that a set of simple roots in formula_4 is chosen. \n\nThe \"affine action\" (also called the \"dot action\") of the Weyl group on the space formula_4 is\n\nwhere formula_7 is the sum of all fundamental weights, or, equivalently, the half of the sum of all positive roots.\n"}
{"id": "18906055", "url": "https://en.wikipedia.org/wiki?curid=18906055", "title": "Affine focal set", "text": "Affine focal set\n\nIn mathematics, and especially affine differential geometry, the affine focal set of a smooth submanifold \"M\" embedded in a smooth manifold \"N\" is the caustic generated by the affine normal lines. It can be realised as the bifurcation set of a certain family of functions. The bifurcation set is the set of parameter values of the family which yield functions with degenerate singularities. This is not the same as the bifurcation diagram in dynamical systems.\n\nAssume that \"M\" is an \"n\"-dimensional smooth hypersurface in real (\"n\"+1)-space. Assume that \"M\" has no points where the second fundamental form is degenerate. From the article affine differential geometry, there exists a unique transverse vector field over \"M\". This is the affine normal vector field, or the Blaschke normal field. A special (i.e. det = 1) affine transformation of real (\"n\" + 1)-space will carry the affine normal vector field of \"M\" onto the affine normal vector field of the image of \"M\" under the transformation.\n\nConsider a local parametrisation of \"M\". Let formula_1 be an open neighbourhood of 0 with coordinates formula_2, and let formula_3 be a smooth parametrisation of \"M\" in a neighbourhood of one of its points.\n\nThe affine normal vector field will be denoted by formula_4. At each point of \"M\" it is transverse to the tangent space of \"M\", i.e.\n\nFor a fixed formula_6 the affine normal line to \"M\" at formula_7 may be parametrised by \"t\" where\nThe affine focal set is given geometrically as the infinitesimal intersections of the \"n\"-parameter family of affine normal lines. To calculate, choose an affine normal line, say at point \"p\"; then look at the affine normal lines at points infinitesimally close to \"p\" and see if any intersect the one at \"p\". If \"p\" is infinitesimally close to formula_9, then it may be expressed as formula_10 where formula_11 represents the infinitesimal difference. Thus formula_12 and formula_13 will be our \"p\" and its neighbour.\n\nSolve for \"t\" and formula_11.\nThis can be done by using power series expansions, and is not too difficult; it is lengthy and has thus been omitted.\n\nRecalling from the article affine differential geometry, the affine shape operator \"S\" is a type (1,1)-tensor field on \"M\", and is given by formula_16, where \"D\" is the covariant derivative on real (\"n\" + 1)-space (for those well read: it is the usual flat and torsion free connexion).\n\nThe solutions to formula_17 are when 1/\"t\" is an eigenvalue of \"S\" and that formula_11 is a corresponding eigenvector. The eigenvalues of \"S\" are not always distinct: there may be repeated roots, there may be complex roots, and \"S\" may not always be diagonalisable. For formula_19, where formula_20 denotes the greatest integer function, there will generically be (\"n\" − 2\"k\")-pieces of the affine focal set above each point \"p\". The −2\"k\" corresponds to pairs of eigenvalues becoming complex (like the solution to formula_21 as \"a\" changes from negative to positive).\n\nThe affine focal set need not be made up of smooth hypersurfaces. In fact, for a generic hypersurface \"M\", the affine focal set will have singularities. The singularities could be found by calculation, but that may be difficult, and there is no idea of what the singularity looks like up to diffeomorphism. Using singularity theory gives much more information.\n\nThe idea here is to define a family of functions over \"M\". The family will have the ambient real (\"n\" + 1)-space as its parameter space, i.e. for each choice of ambient point there is function defined over \"M\". This family is the family of affine distance functions:\n\nGiven an ambient point formula_23 and a surface point \"p\", it is possible to decompose the chord joining \"p\" to formula_23 as a tangential component and a transverse component parallel to formula_4. The value of Δ is given implicitly in the equation\n\nwhere \"Z\" is a tangent vector. We now seek the bifurcation set of the family Δ, i.e. the ambient points for which the restricted function\nhas degenerate singularity at some \"p\". A function has degenerate singularity if both the Jacobian matrix of first order partial derivatives and the Hessian matrix of second order partial derivatives have zero determinant.\n\nTo discover if the Jacobian matrix has zero determinant we differentiate the equation \"x - p = Z + ΔA\". Let \"X\" be a tangent vector to \"M\", and differentiate in that direction:\n\nwhere \"I\" is the identity. This tells us that formula_31 and formula_32. The last equality says that we have the following equation of differential one-forms formula_33. The Jacobian matrix will have zero determinant if, and only if, formula_34 is degenerate as a one-form, i.e. formula_35 for all tangent vectors \"X\".\nSince formula_33 it follows that formula_34 is degenerate if, and only if, formula_38 is degenerate. Since \"h\" is a non-degenerate two-form it follows that \"Z = 0\". Notice that since \"M\" has a non-degenerate second fundamental form it follows that \"h\" is a non-degenerate two-form. Since \"Z = 0\" the set of ambient points \"x\" for which the restricted function formula_27 has a singularity at some \"p\" is the affine normal line to \"M\" at \"p\".\n\nTo compute the Hessian matrix we consider the differential two-form formula_40. This is the two-form whose matrix representation is the Hessian matrix. We have already seen that formula_32 we see that formula_42 We have\nNow assume that Δ has a singularity at \"p\", i.e. Z = 0, then we have the two-form\nWe have also seen that formula_31, and so the two-form becomes\nThis is degenerate as a two-form if, and only if, there exists non-zero \"X\" for which it is zero for all \"Y\". Since \"h\" is non-degenerate it must be that formula_47 and formula_48. So the singularity is degenerate if, and only if, the ambient point \"x\" lies on the affine normal line to \"p\" and the reciprocal of its distance from \"p\" is an eigenvalue of \"S\", i.e. points formula_49 where 1/\"t\" is an eigenvalue of \"S\". The affine focal set!\n\nThe affine focal set can be the following:\nTo find the singular points we simply differentiate \"p + tA\" in some tangent direction \"X\":\nThe affine focal set is singular if, and only if, there exists non-zero \"X\" such that formula_52, i.e. if, and only if, \"X\" is an eigenvector of \"S\" and the derivative of \"t\" in that direction is zero. This means that the derivative of an affine principal curvature in its own affine principal direction is zero.\n\nWe can use the standard ideas in singularity theory to classify, up to local diffeomorphism, the affine focal set. If the family of affine distance functions can be shown to be a certain kind of family then the local structure is known. We want the family of affine distance functions to be a versal unfolding of the singularities which arise.\n\nThe affine focal set of a plane curve will generically consist of smooth pieces of curve and ordinary cusp points (semi-cubical palabara|semi-cubical parabolae).\n\nThe affine focal set of a surface in three-space will generically consist of smooth pieces of surface, cuspidal cylinder points (formula_53), swallowtail points (formula_54), purse points (formula_55), and pyramid points (formula_56).\nThe formula_57 and formula_58 series are as in Arnold's list.\n\nThe question of the local structure in much higher dimension is of great interest. For example, we were able to construct a discrete list of singularity types (up to local diffeomprhism). In much higher dimensions no such discrete list can be constructed, there are functional modulii.\n\n"}
{"id": "1118832", "url": "https://en.wikipedia.org/wiki?curid=1118832", "title": "Arbitrarily large", "text": "Arbitrarily large\n\nIn mathematics, the phrases arbitrarily large, arbitrarily small, and arbitrarily long are used in statements such as:\n\nwhich is shorthand for:\n\n\"Arbitrarily large\" is not equivalent to \"sufficiently large\". For instance, while it is true that prime numbers can be arbitrarily large since there are an infinite number of them, it is not true that all sufficiently large numbers are prime. \"Arbitrarily large\" does not mean \"infinitely large\" because although prime numbers can be arbitrarily large, an infinitely large prime does not exist since all prime numbers (as well as all other integers) are finite.\n\nIn some cases, phrases such as \"P(\"x\") is true for arbitrarily large \"x\"\" are used primarily for emphasis, as in \"P(\"x\") is true for all \"x\", no matter how large \"x\" is.\" In these cases, the phrase \"arbitrarily large\" does not have the meaning indicated above but is in fact logically synonymous with \"all.\"\n\nTo say that there are \"arbitrarily long arithmetic progressions of prime numbers\" does not mean that there exists any infinitely long arithmetic progression of prime numbers (there is not), nor that there exists any particular arithmetic progression of prime numbers that is in some sense \"arbitrarily long\", but rather that no matter how large a number \"n\" is, there exists some arithmetic progression of prime numbers of length at least \"n\".\n\nThe statement \"ƒ(\"x\") is non-negative for arbitrarily large \"x\".\" could be rewritten as:\n\nUsing \"sufficiently large\" instead yields:\n\n"}
{"id": "36551969", "url": "https://en.wikipedia.org/wiki?curid=36551969", "title": "Arithmetic number", "text": "Arithmetic number\n\nIn number theory, an arithmetic number is an integer for which the average of its positive divisors is also an integer. For instance, 6 is an arithmetic number because the average of its divisors is\nwhich is also an integer. However, 2 is not an arithmetic number because its only divisors are 1 and 2, and their average 3/2 is not an integer.\n\nThe first numbers in the sequence of arithmetic numbers are\n\nIt is known that the natural density of such numbers is 1: indeed, the proportion of numbers less than \"X\" which are not arithmetic is asymptotically\n\nwhere \"c\" = 2 + o(1).\n\nA number \"N\" is arithmetic if the number of divisors \"d\"(\"N\") divides the sum of divisors σ(\"N\"). It is known that the density of integers \"N\" obeying the stronger condition that \"d\"(\"N\") divides σ(\"N\") is 1/2.\n"}
{"id": "550741", "url": "https://en.wikipedia.org/wiki?curid=550741", "title": "Composition (combinatorics)", "text": "Composition (combinatorics)\n\nIn mathematics, a composition of an integer \"n\" is a way of writing \"n\" as the sum of a sequence of (strictly) positive integers. Two sequences that differ in the order of their terms define different compositions of their sum, while they are considered to define the same partition of that number. Every integer has finitely many distinct compositions. Negative numbers do not have any compositions, but 0 has one composition, the empty sequence. Each positive integer \"n\" has 2 distinct compositions.\n\nA weak composition of an integer \"n\" is similar to a composition of \"n\", but allowing terms of the sequence to be zero: it is a way of writing \"n\" as the sum of a sequence of non-negative integers. As a consequence every positive integer admits infinitely many weak compositions (if their length is not bounded). Adding a number of terms 0 to the \"end\" of a weak composition is usually not considered to define a different weak composition; in other words, weak compositions are assumed to be implicitly extended indefinitely by terms 0.\n\nTo further generalize, an \"A\"-restricted composition of an integer \"n\", for a subset \"A\" of the (nonnegative or positive) integers, is an ordered collection of one or more elements in \"A\" whose sum is \"n\".\n\nThe sixteen compositions of 5 are:\n\nCompare this with the seven partitions of 5:\n\nIt is possible to put constraints on the parts of the compositions. For example the five compositions of 5 into distinct terms are:\n\nCompare this with the three partitions of 5 into distinct terms:\n\nConventionally the empty composition is counted as the sole composition of 0, and there are no compositions of negative integers.\nThere are 2 compositions of \"n\" ≥ 1; here is a proof:\n\nPlacing either a plus sign or a comma in each of the \"n\" − 1 boxes of the array\n\nproduces a unique composition of \"n\". Conversely, every composition of \"n\" determines an assignment of pluses and commas. Since there are \"n\" − 1 binary choices, the result follows. The same argument shows that the number of compositions of \"n\" into exactly \"k\" parts is given by the binomial coefficient formula_2. Note that by summing over all possible number of parts we recover 2 as the total number of compositions of \"n\":\n\nFor weak compositions, the number is formula_4, since each \"k\"-composition of \"n\" + \"k\" corresponds to a weak one of \"n\" by the rule [\"a\" + \"b\" + ... + \"c\" = \"n\" + \"k\"] → [(\"a\" − 1) + (\"b\" − 1) + ... + (\"c\" − 1) = \"n\"]. It follows from this formula that the number of weak compositions of \"n\" into exactly \"k\" parts equals the number of weak compositions of \"k\" − 1 into exactly \"n\" + 1 parts.\n\nFor \"A\"-restricted compositions, the number of compositions of \"n\" into exactly \"k\" parts is given by the extended binomial (or polynomial) coefficient formula_5, where the square brackets indicate the extraction of the coefficient of formula_6 in the polynomial that follows it.\n\n\n"}
{"id": "26910524", "url": "https://en.wikipedia.org/wiki?curid=26910524", "title": "Computational transportation science", "text": "Computational transportation science\n\nComputational Transportation Science (CTS) is an emerging discipline that combines computer science and engineering with the modeling, planning, and economic aspects of transport. The discipline studies how to improve the safety, mobility, and sustainability of the transport system by taking advantage of information technologies and ubiquitous computing. A list of subjects encompassed by CTS can be found at include.\n\nComputational Transportation Science is an emerging discipline going beyond vehicular technology, addressing pedestrian systems on hand-held devices but also issues such as transport data mining (or movement analysis), as well as data management aspects. CTS allows for an increasing flexibility of the system as local and autonomous negotiations between transport peers, partners and supporting infrastructure are allowed. Thus, CTS provides means to study localized computing, self-organization, cooperation and simulation of transport systems.\n\nSeveral academic conferences on CTS have been held up to date:\n\nThere is also an IGERT PHD program on Computational Transportation Science at the University of Illinois at Chicago.\n\n"}
{"id": "1840214", "url": "https://en.wikipedia.org/wiki?curid=1840214", "title": "Conservative extension", "text": "Conservative extension\n\nIn mathematical logic, a conservative extension is a supertheory of a theory which is often convenient for proving theorems, but proves no new theorems about the language of the original theory. Similarly, a non-conservative extension is a supertheory which is not conservative, and can prove more theorems than the original.\n\nMore formally stated, a theory formula_1 is a (proof theoretic) conservative extension of a theory formula_2 if every theorem of formula_2 is a theorem of formula_1, and any theorem of formula_1 in the language of formula_2 is already a theorem of formula_2.\n\nMore generally, if formula_8 is a set of formulas in the common language of formula_2 and formula_1, then formula_1 is formula_8-conservative over formula_2 if every formula from formula_8 provable in formula_1 is also provable in formula_2.\n\nNote that a conservative extension of a consistent theory is consistent. [If it were not, then by the principle of explosion (\"everything follows from a contradiction\"), every theorem in the original theory \"as well as its negation\" would belong to the new theory, which then would not be a conservative extension.] Hence, conservative extensions do not bear the risk of introducing new inconsistencies. This can also be seen as a methodology for writing and structuring large theories: start with a theory, formula_17, that is known (or assumed) to be consistent, and successively build conservative extensions formula_2, formula_1, ... of it.\n\nThe theorem provers Isabelle and ACL2 adopt this methodology by providing a language for conservative extensions by definition.\n\nRecently, conservative extensions have been used for defining a notion of module for ontologies: if an ontology is formalized as a logical theory, a subtheory is a module if the whole ontology is a conservative extension of the subtheory.\n\nAn extension which is not conservative may be called a proper extension.\n\n\nWith model-theoretic means, a stronger notion is obtained: an extension formula_1 of a theory formula_2 is model-theoretically conservative if every model of formula_2 can be expanded to a model of formula_1. It is straightforward to see that each model-theoretic conservative extension also is a (proof-theoretic) conservative extension in the above sense. The model theoretic notion has the advantage over the proof theoretic one that it does not depend so much on the language at hand; on the other hand, it is usually harder to establish model theoretic conservativity.\n\n"}
{"id": "3094450", "url": "https://en.wikipedia.org/wiki?curid=3094450", "title": "Degree of a continuous mapping", "text": "Degree of a continuous mapping\n\nIn topology, the degree of a continuous mapping between two compact oriented manifolds of the same dimension is a number that represents the number of times that the domain manifold wraps around the range manifold under the mapping. The degree is always an integer, but may be positive or negative depending on the orientations.\n\nThe degree of a map was first defined by Brouwer, who showed that the degree is homotopy invariant (invariant among homotopies), and used it to prove the Brouwer fixed point theorem. In modern mathematics, the degree of a map plays an important role in topology and geometry. In physics, the degree of a continuous map (for instance a map from space to some order parameter set) is one example of a topological quantum number.\n\nThe simplest and most important case is the degree of a continuous map from the formula_1-sphere formula_2 to itself (in the case formula_3, this is called the winding number):\n\nLet formula_4 be a continuous map. Then formula_5 induces a homomorphism formula_6, where formula_7 is the formula_1th homology group. Considering the fact that formula_9, we see that formula_10 must be of the form formula_11 for some fixed formula_12.\nThis formula_13 is then called the degree of formula_5.\n\nLet \"X\" and \"Y\" be closed connected oriented \"m\"-dimensional manifolds. Orientability of a manifold implies that its top homology group is isomorphic to Z. Choosing an orientation means choosing a generator of the top homology group.\n\nA continuous map \"f\" : \"X\"→\"Y\" induces a homomorphism \"f\" from \"H\"(\"X\") to \"H\"(\"Y\"). Let [\"X\"], resp. [\"Y\"] be the chosen generator of \"H\"(\"X\"), resp. \"H\"(\"Y\") (or the fundamental class of \"X\", \"Y\"). Then the degree of \"f\" is defined to be \"f\"([\"X\"]). In other words,\n\nIf \"y\" in \"Y\" and \"f\" (\"y\") is a finite set, the degree of \"f\" can be computed by considering the \"m\"-th local homology groups of \"X\" at each point in \"f\" (\"y\").\n\nIn the language of differential topology, the degree of a smooth map can be defined as follows: If \"f\" is a smooth map whose domain is a compact manifold and \"p\" is a regular value of \"f\", consider the finite set\n\nBy \"p\" being a regular value, in a neighborhood of each \"x\" the map \"f\" is a local diffeomorphism (it is a covering map). Diffeomorphisms can be either orientation preserving or orientation reversing. Let \"r\" be the number of points \"x\" at which \"f\" is orientation preserving and \"s\" be the number at which \"f\" is orientation reversing. When the domain of \"f\" is connected, the number \"r\" − \"s\" is independent of the choice of \"p\" (though \"n\" is not!) and one defines the degree of \"f\" to be \"r\" − \"s\". This definition coincides with the algebraic topological definition above.\n\nThe same definition works for compact manifolds with boundary but then \"f\" should send the boundary of \"X\" to the boundary of \"Y\".\n\nOne can also define degree modulo 2 (deg(\"f\")) the same way as before but taking the \"fundamental class\" in Z homology. In this case deg(\"f\") is an element of Z (the field with two elements), the manifolds need not be orientable and if \"n\" is the number of preimages of \"p\" as before then deg(\"f\") is \"n\" modulo 2.\n\nIntegration of differential forms gives a pairing between (C-)singular homology and de Rham cohomology: formula_17, where formula_18 is a homology class represented by a cycle formula_18 and formula_20 a closed form representing a de Rham cohomology class. For a smooth map \"f\" : \"X\"→\"Y\" between orientable \"m\"-manifolds, one has\n\nwhere \"f\" and \"f\"* are induced maps on chains and forms respectively. Since \"f\"[\"X\"] = deg \"f\" · [\"Y\"], we have\n\nfor any \"m\"-form \"ω\" on \"Y\".\n\nIf formula_23is a bounded region, formula_24 smooth, formula_25 a regular value of formula_5 and\nformula_27, then the degree formula_28 is defined\nby the formula\nwhere formula_30 is the Jacobi matrix of formula_5 in formula_32. \nThis definition of the degree may be naturally extended for non-regular values formula_25 such that formula_34 where formula_35 is a point close to formula_25.\n\nThe degree satisfies the following properties: \n\nThese properties characterise the degree uniquely and the degree may be defined by them in an axiomatic way.\n\nIn a similar way, we could define the degree of a map between compact oriented manifolds with boundary.\n\nThe degree of a map is a homotopy invariant; moreover for continuous maps from the sphere to itself it is a \"complete\" homotopy invariant, i.e. two maps formula_54 are homotopic if and only if formula_55.\n\nIn other words, degree is an isomorphism between formula_56 and formula_57.\n\nMoreover, the Hopf theorem states that for any formula_1-dimensional closed oriented manifold \"M\", two maps formula_59 are homotopic if and only if formula_60\n\nA self-map formula_61 of the \"n\"-sphere is extendable to a map formula_62 from the \"n\"-ball to the \"n\"-sphere if and only if formula_63. (Here the function \"F\" extends \"f\" in the sense that \"f\" is the restriction of \"F\" to formula_2.)\n\n\n\n"}
{"id": "1316878", "url": "https://en.wikipedia.org/wiki?curid=1316878", "title": "Destructive dilemma", "text": "Destructive dilemma\n\nDestructive dilemma is the name of a valid rule of inference of propositional logic. It is the inference that, if \"P\" implies \"Q\" and \"R\" implies \"S\" and either \"Q\" is false or \"S\" is false, then either \"P\" or \"R\" must be false. In sum, if two conditionals are true, but one of their consequents is false, then one of their antecedents has to be false. \"Destructive dilemma\" is the disjunctive version of \"modus tollens\". The disjunctive version of \"modus ponens\" is the constructive dilemma. The rule can be stated:\n\nwhere the rule is that wherever instances of \"formula_2\", \"formula_3\", and \"formula_4\" appear on lines of a proof, \"formula_5\" can be placed on a subsequent line.\n\nThe \"destructive dilemma\" rule may be written in sequent notation:\n\nwhere formula_7 is a metalogical symbol meaning that formula_8 is a syntactic consequence of formula_2, formula_3, and formula_4 in some logical system;\n\nand expressed as a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_13, formula_14, formula_15 and formula_16 are propositions expressed in some formal system.\n\nThe validity of this argument structure can be shown by using both conditional proof (CP) and reductio ad absurdum (RAA) in the following way:\n\n\n"}
{"id": "16628246", "url": "https://en.wikipedia.org/wiki?curid=16628246", "title": "EqWorld", "text": "EqWorld\n\nEqWorld is a free online mathematics reference site that lists information about mathematical equations.\n\nIt covers ordinary differential, partial differential, integral, functional, and other mathematical equations. It also outlines some methods for solving equations, and lists many resources for solving equations, and has an equation archive which users can add to.\n\n\n"}
{"id": "30749895", "url": "https://en.wikipedia.org/wiki?curid=30749895", "title": "Erdős space", "text": "Erdős space\n\nIn mathematics, Erdős space is a topological space named after Paul Erdős. Erdős space is defined as the set \"E\" of points in the Hilbert space ℓ of square summable sequences having all coordinates rational. \n\nErdős space is a totally disconnected, one-dimensional topological space. The space \"E\" is homeomorphic to the direct product \"E\"×\"E\". Endowed with the compact-open topology, the set of all homeomorphisms of the Euclidean space R leaving the set Q of vectors with rational coordinates invariant is homeomorphic to the Erdős space for \"n\" ≥ 2.\n"}
{"id": "1233990", "url": "https://en.wikipedia.org/wiki?curid=1233990", "title": "Erich Hecke", "text": "Erich Hecke\n\nErich Hecke (20 September 1887 – 13 February 1947) was a German mathematician. He obtained his doctorate in Göttingen under the supervision of David Hilbert. Kurt Reidemeister and Heinrich Behnke were among his students.\n\nHecke was born in Buk, Posen, Germany (now Poznań, Poland), and died in Copenhagen, Denmark. His early work included establishing the functional equation for the Dedekind zeta function, with a proof based on theta functions. The method extended to the L-functions associated to a class of characters now known as Hecke characters or idele class characters; such L-functions are now known as Hecke L-functions. He devoted most of his research to the theory of modular forms, creating the general theory of cusp forms (holomorphic, for GL(2)), as it is now understood in the classical setting.\n\nHe was a Plenary Speaker of the ICM in 1936 in Oslo.\n\n\n"}
{"id": "630017", "url": "https://en.wikipedia.org/wiki?curid=630017", "title": "Feynman–Kac formula", "text": "Feynman–Kac formula\n\nThe Feynman–Kac formula named after Richard Feynman and Mark Kac, establishes a link between parabolic partial differential equations (PDEs) and stochastic processes. When Mark Kac and Richard Feynman were both Cornell faculty, Kac attended a lecture of Feynman's and remarked that the two of them were working on the same thing from different directions. The Feynman-Kac formula resulted, which proves rigorously the real case of Feynman's path integrals. The complex case, which occurs when a particle's spin is included, is still unproven. \n\nIt offers a method of solving certain partial differential equations by simulating random paths of a stochastic process. Conversely, an important class of expectations of random processes can be computed by deterministic methods.\n\nConsider the partial differential equation\n\ndefined for all formula_2 and formula_3, subject to the terminal condition\n\nwhere μ, σ, ψ, \"V\", \"f\" are known functions, \"T\" is a parameter and formula_5 is the unknown. Then the Feynman–Kac formula tells us that the solution can be written as a conditional expectation\n\nunder the probability measure Q such that \"X\" is an Itô process driven by the equation\n\nwith \"W\"(\"t\") is a Wiener process (also called Brownian motion) under \"Q\", and the initial condition for \"X\"(\"t\") is \"X\"(t) = \"x\".\n\nA proof that the above formula is a solution of the differential equation is long, difficult and not presented here. It is however reasonably straightforward to show that, \"if a solution exists\", it must have the above form. The proof of that lesser result is as follows.\n\nLet \"u\"(\"x\", \"t\") be the solution to the above partial differential equation. Applying the product rule for Itô processes to the process\n\none gets\n\nSince\n\nthe third term is formula_11 and can be dropped. We also have that\n\nApplying Itô's lemma to formula_13, it follows that\n\nThe first term contains, in parentheses, the above partial differential equation and is therefore zero. What remains is\n\nIntegrating this equation from \"t\" to \"T\", one concludes that\n\nUpon taking expectations, conditioned on \"X\" = \"x\", and observing that the right side is an Itô integral, which has expectation zero, it follows that\n\nThe desired result is obtained by observing that\n\nand finally\n\n\n\nIn quantitative finance, the Feynman–Kac formula is used to efficiently calculate solutions to the Black–Scholes equation to price options on stocks.\n\n\n"}
{"id": "53252845", "url": "https://en.wikipedia.org/wiki?curid=53252845", "title": "Glossary of calculus", "text": "Glossary of calculus\n\n\"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\"\nThis glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4081806", "url": "https://en.wikipedia.org/wiki?curid=4081806", "title": "Graffiti (program)", "text": "Graffiti (program)\n\nGraffiti is a computer program which makes conjectures in various subfields of mathematics (particularly graph theory) and chemistry, but can be adapted to other fields. It was written by Siemion Fajtlowicz at the University of Houston. Research on conjectures produced by Graffiti has led to over 60 publications by other mathematicians.\n\n"}
{"id": "2011627", "url": "https://en.wikipedia.org/wiki?curid=2011627", "title": "Ground expression", "text": "Ground expression\n\nIn mathematical logic, a ground term of a formal system is a term that does not contain any free variables.\n\nSimilarly, a ground formula is a formula that does not contain any free variables. In first-order logic with identity, the sentence  \"x\" (\"x\"=\"x\") is a ground formula.\n\nA ground expression is a ground term or ground formula.\n\nConsider the following expressions from first order logic over a signature containing a constant symbol 0 for the number 0, a unary function symbol \"s\" for the successor function and a binary function symbol + for addition.\n\nWhat follows is a formal definition for first-order languages. Let a first-order language be given, with formula_1 the set of constant symbols, formula_2 the set of (individual) variables, formula_3 the set of functional operators, and formula_4 the set of predicate symbols.\n\nGround terms are terms that contain no variables. They may be defined by logical recursion (formula-recursion):\n\nRoughly speaking, the Herbrand universe is the set of all ground terms.\n\nA ground predicate or ground atom or ground literal is an atomic formula all of whose argument terms are ground terms.\n\nIf \"p\"∈\"P\" is an \"n\"-ary predicate symbol and α, α, ..., α are ground terms, then \"p\"(α, α, ..., α) is a ground predicate or ground atom.\n\nRoughly speaking, the Herbrand base is the set of all ground atoms, while a Herbrand interpretation assigns a truth value to each ground atom in the base.\n\nA ground formula or ground clause is a formula without free variables.\n\nFormulas with free variables may be defined by syntactic recursion as follows:\n\n"}
{"id": "41910504", "url": "https://en.wikipedia.org/wiki?curid=41910504", "title": "Groupoid object", "text": "Groupoid object\n\nIn category theory, a groupoid object in a category \"C\" admitting finite fiber products is a pair of objects formula_1 together with five morphisms formula_2 satisfying the following groupoid axioms\n\nExample: A groupoid object in the category of sets is precisely a groupoid in the usual sense: a category in which every morphism is an isomorphism. Indeed, given such a category \"C\", take \"U\" to be the set of all objects in \"C\", \"R\" the set of all arrows in \"C\", the five morphisms given by formula_10, formula_11, formula_12 and formula_13.\n\nIncidentally, one can consider a notion of a semigroupoid (unital semigroup = a category with a single object); but, according to this example, that is nothing but a category; so a groupoid object is really a special case of a \"category object\", better known as a stack (or prestack).\n\nA groupoid \"S\"-scheme is a groupoid object in the category of schemes over some fixed base scheme \"S\". If formula_14, then a groupoid scheme (where formula_15 are necessarily the structure map) is the same as a group scheme. A groupoid scheme is also called an algebraic groupoid, for example in , to convey the idea it is a generalization of algebraic groups and their actions. When the term \"groupoid\" can naturally refer to a groupoid object in some particular category in mind, the term groupoid set is used to refer to a groupoid object in the category of sets.\n\nExample: Suppose an algebraic group \"G\" acts from the right on a scheme \"U\". Then take formula_16, \"s\" the projection, \"t\" the given action. This determines a groupoid scheme.\n\nGiven a groupoid object (\"R\", \"U\"), the equalizer of formula_17, if any, is a group object called the inertia group of the groupoid. The coequalizer of the same diagram, if any, is the quotient of the groupoid.\n\nEach groupoid object in a category \"C\" (if any) may be thought of as a contravariant functor from \"C\" to the category of groupoids. This way, each groupoid object determines a prestack in groupoids. This prestack is not a stack but it can be stackified to yield a stack.\n\nThe main use of the notion is that it provides an atlas for a stack. More specifically, let formula_18 be the category of formula_19-torsors. Then it is a category fibered in groupoids; in fact, (in a nice case), a Deligne–Mumford stack. Conversely, any DM stack is of this form.\n\n\n"}
{"id": "50045170", "url": "https://en.wikipedia.org/wiki?curid=50045170", "title": "Göbel's sequence", "text": "Göbel's sequence\n\nIn mathematics, Göbel's sequence is a sequence of rational numbers defined by the recurrence relation\nwith starting value\nGöbel's sequence starts with\nThe first non-integral value is \"x\".\n\nGöbel's sequence can be generalized to \"k\"th powers by\n\nThe least indices at which the \"k\"-Göbel sequences assume a non-integral value are\n\n"}
{"id": "56358777", "url": "https://en.wikipedia.org/wiki?curid=56358777", "title": "Hardware-based encryption", "text": "Hardware-based encryption\n\nHardware-based encryption is the use of computer hardware to assist software, or sometimes replace software, in the process of data encryption. Typically, this is implemented as part of the processor's instruction set. For example, the AES encryption algorithm (a modern cipher) can be implemented using the AES instruction set on the ubiquitous x86 architecture. Such instructions also exist on the ARM architecture. However, more unusual systems exist where the cryptography module is separate from the central processor, instead being implemented as a coprocessor, in particular a secure cryptoprocessor or cryptographic accelerator, of which an example is the IBM 4758, or its successor, the IBM 4764. Hardware implementations can be faster and less prone to exploitation than traditional software implementations, and furthermore can be protected against tampering. \nPrior to the use of computer hardware, cryptography could be performed through various mechanical or electro-mechanical means. An early example is the Scytale used by the Spartans. The Enigma machine was an electro-mechanical system cipher machine notably used by the Germans in World War II. After World War II, purely electronic systems were developed. In 1987 the ABYSS (A Basic Yorktown Security System) project was initiated. The aim of this project was to protect against software piracy. However, the application of computers to cryptography in general dates back to the 1940s and Bletchley Park, where the Colossus computer was used to break the encryption used by German High Command during World War II. The use of computers to \"encrypt\", however, came later. In particular, until the development of the integrated circuit, of which the first was produced in 1960, computers were impractical for encryption, since, in comparison to the portable form factor of the Enigma machine, computers of the era took the space of an entire building. It was only with the development of the microcomputer that computer encryption became feasible, outside of niche applications. The development of the World Wide Web lead to the need for consumers to have access to encryption, as online shopping became prevalent. The key concerns for consumers were security and speed. This led to the eventual inclusion of the key algorithms into processors as a way of both increasing speed and security.\n\nThe X86 architecture, as a CISC (Complex Instruction Set Computer) Architecture, typically implements complex algorithms in hardware. Cryptographic algorithms are no exception. The x86 architecture implements significant components of the AES (Advanced Encryption Standard) algorithm, which can be used by the NSA for Top Secret information. The architecture also includes support for the SHA Hashing Algorithms through the Intel SHA extensions. Whereas AES is a cipher, which is useful for encrypting documents, hashing is used for verification, such as of passwords (see PBKDF2).\n\nARM processors can optionally support Security Extensions. Although ARM is a RISC (Reduced Instruction Set Computer) architecture, there are several optional extensions specified by ARM Holdings.\n\nAdvanced Micro Devices (AMD) processors are also x86 devices, and have supported the AES instructions since the 2011 Bulldozer processor iteration. \nDue to the existence of encryption instructions on modern processors provided by both Intel and AMD, the instructions are present on most modern computers. They also exist on many tablets and smartphones due to their implementation in ARM processors.\n\nImplementing cryptography in hardware means that part of the processor is dedicated to the task. This can lead to a large increase in speed. In particular, modern processor architectures that support pipelining can often perform other instructions concurrently with the execution of the encryption instruction. Furthermore, hardware can have methods of protecting data from software. Consequently, even if the operating system is compromised, the data may still be secure (see Software Guard Extensions).\n\nIf, however, the hardware implementation is compromised, major issues arise. Malicious software can retrieve the data from the (supposedly) secure hardware – a large class of method used is the timing attack. This is far more problematic to solve than a software bug, even within the operating system. Microsoft regularly deals with security issues through Windows Update. Similarly, regular security updates are released for Mac OS X and Linux, as well as mobile operating systems like iOS, Android, and Windows Phone. However, hardware is a different issue. Sometimes, the issue will be fixable through updates to the processor's microcode (a low level type of software). However, other issues may only be resolvable through replacing the hardware, or a workaround in the operating system which mitigates the performance benefit of the hardware implementation, such as in the Spectre exploit.\n\n"}
{"id": "37040656", "url": "https://en.wikipedia.org/wiki?curid=37040656", "title": "Henneberg surface", "text": "Henneberg surface\n\nIn differential geometry, the Henneberg surface is a non-orientable minimal surface named after Lebrecht Henneberg.\n\nIt has parametric equation\nand can be expressed as an order-15 algebraic surface. It can be viewed as an immersion of a punctured projective plane. Up until 1981 it was the only known non-orientable minimal surface.\n\nThe surface contains a semicubical parabola (\"Neile's parabola\") and can be derived from solving the corresponding Björling problem.\n"}
{"id": "17182647", "url": "https://en.wikipedia.org/wiki?curid=17182647", "title": "Hunter–Saxton equation", "text": "Hunter–Saxton equation\n\nIn mathematical physics, the Hunter–Saxton equation\n\nis an integrable PDE that arises in the theoretical study of nematic liquid crystals. If the molecules in the liquid crystal are initially all aligned, and some of them are then wiggled slightly, this disturbance in orientation will propagate through the crystal, and the Hunter–Saxton equation describes certain aspects of such orientation waves.\n\nIn the models for liquid crystals considered here, it is assumed that there is no fluid flow, so that only the \"orientation\" of the molecules is of interest.\nWithin the elastic continuum theory, the orientation is described by a field of unit vectors n(\"x\",\"y\",\"z\",\"t\"). For nematic liquid crystals, there is no difference between orienting a molecule in the n direction or in the −n direction, and the vector field n is then called a \"director field\".\nThe potential energy density of a director field is usually assumed to be given by the Oseen–Frank energy functional \n\nwhere the positive coefficients formula_3, formula_4, formula_5 are known as the elastic coefficients of splay, twist, and bend, respectively. The kinetic energy is often neglected because of the high viscosity of liquid crystals.\n\nHunter and Saxton investigated the case when viscous damping is ignored and a kinetic energy term is included in the model. Then the governing equations for the dynamics of the director field are the Euler–Lagrange equations for the Lagrangian\n\nwhere formula_7 is a Lagrange multiplier corresponding to the constraint |n|=1.\nThey restricted their attention to \"splay waves\" where the director field takes the special form\n\nThis assumption reduces the Lagrangian to\n\nand then the Euler–Lagrange equation for the angle φ becomes\n\nThere are trivial constant solutions φ=φ\ncorresponding to states where the molecules in the liquid crystal are\nperfectly aligned.\nLinearization around such an equilibrium leads to the linear wave equation\nwhich allows wave propagation in both directions with speed\nformula_11,\nso the nonlinear equation can be expected to behave similarly.\nIn order to study right-moving waves for large \"t\",\none looks for asymptotic solutions of the form\n\nwhere\n\nInserting this into the equation, one finds at the order formula_14 that\n\nA simple renaming and rescaling of the variables\ntransforms this into the Hunter–Saxton equation.\n\nThe analysis was later generalized by Alì and Hunter, who allowed the director field to point in any direction, but with the spatial dependence still only in the \"x\" direction:\n\nThen the Lagrangian is\n\nThe corresponding Euler–Lagrange equations are coupled nonlinear wave equations for the angles φ and ψ, with φ corresponding to \"splay waves\" and ψ to \"twist waves\". The previous Hunter–Saxton case (pure splay waves) is recovered by taking ψ constant, but one can also consider coupled splay-twist waves where both φ and ψ vary. Asymptotic expansions similar to that above lead to a system of equations, which, after renaming and rescaling the variables, takes the form\n\nwhere \"u\" is related to φ and \"v\" to ψ.\nThis system implies that \"u\" satisfies\n\nso (rather remarkably) the Hunter–Saxton equation arises in this context too, but in a different way.\n\nThe integrability of the Hunter–Saxton equation, or, more precisely, that of its \"x\" derivative\n\nwas shown by Hunter and Zheng, who exploited that this equation is obtained from the Camassa–Holm equation\n\nin the \"high frequency limit\"\n\nApplying this limiting procedure to a Lagrangian for the Camassa–Holm equation, they obtained a Lagrangian\n\nwhich produces the Hunter–Saxton equation after elimination of \"v\" and \"w\" from the Euler–Lagrange equations for \"u\", \"v\", \"w\". Since there is also the more obvious Lagrangian\n\nthe Hunter–Saxton has two inequivalent variational structures. Hunter and Zheng also obtained a bihamiltonian formulation and a Lax pair from the corresponding structures for the Camassa–Holm equation in a similar way.\n\nThe fact that the Hunter–Saxton equation arises physically in two different ways (as shown above) was used by Alì and Hunter to explain why it has this bivariational (or bihamiltonian) structure.\n\n\n"}
{"id": "89486", "url": "https://en.wikipedia.org/wiki?curid=89486", "title": "Inequation", "text": "Inequation\n\nIn mathematics, an inequation is a statement that an inequality holds between two values. It is usually written in the form of a pair of expressions denoting the values in question, with a relational sign between them indicating the specific inequality relation. Some examples of inequations are:\nSome authors apply the term only to inequations in which the inequality relation is specifically not-equal-to (≠).\n\nA shorthand notation is used for the conjunction of several inequations involving common expressions, by chaining them together. For example, the chain\nis shorthand for\n\nSimilar to equation solving, inequation solving means finding what values (numbers, functions, sets, etc.) fulfill a condition stated in the form of an inequation or a conjunction of several inequations.\nThese expressions contain one or more \"unknowns\", which are free variables for which values are sought that cause the condition to be fulfilled. \nTo be precise, what is sought are often not necessarily actual values, but, more in general, expressions. \nA solution of the inequation is an assignment of expressions to the \"unknowns\" that satisfies the inequation(s); in other words, expressions such that, when they are substituted for the unknowns, the inequations become true propositions.\nOften, an additional objective expression is given that is to be minimized by an \"optimal\" solution.\n\nFor example, \n\nis a conjunction of inequations, partly written as chains (where formula_8 can be read as \"and\"); the set of its solutions is shown in blue in the picture (the red, green, and orange line corresponding to the 1st, 2nd, and 3rd conjunct, respectively).\nSee Linear programming#Example for a larger example.\n\nComputer support in solving inequations is described in constraint programming;\nin particular, the simplex algorithm finds optimal solutions of linear inequations. \nThe programming language Prolog III supports solving algorithms for particular classes of inequalities (and other relations) as a basic language feature, see constraint logic programming.\n\n"}
{"id": "23581664", "url": "https://en.wikipedia.org/wiki?curid=23581664", "title": "Institute of Actuaries of France", "text": "Institute of Actuaries of France\n\nThe Institute of Actuaries () is the association of actuaries in France. The Institute was created in 2001 by a merger of the Institute of Actuaries of France and the French Federation of Actuaries. The Institute is a full member of the International Actuarial Association and the Groupe Consultatif. As of 2012, it has about 3 000 full members. Current president of the Institute is Thomas Behar.\n\nIn France the education of future actuaries will be facilitated by the national education system. The actuarial profession in France, in itself, has no such responsibility any more. But accrediting the diplomas awarded to the future actuaries is the responsibility of the French actuarial profession. Note that, these diplomas which accredited by French have access to the actuarial body. \n\n"}
{"id": "34639861", "url": "https://en.wikipedia.org/wiki?curid=34639861", "title": "Inversion in a sphere", "text": "Inversion in a sphere\n\nIn geometry, inversion in a sphere is a transformation of Euclidean space that fixes the points of a sphere while sending the points inside of the sphere to the outside of the sphere, and vice versa. Intuitively, it \"swaps the inside and outside\" of the sphere while leaving the points on the sphere unchanged. Inversion is a conformal transformation, and is the basic operation of inversive geometry.\n\nInversion in a sphere is most easily described using polar coordinates. Choose a system of affine coordinates so that the centre of the sphere is at the origin and the radius of the sphere is 1. Then every point can be written in the form \"r\"v, where \"r\" is the distance from the point to the origin and v is a unit vector; moreover, for every point apart from the origin this representation is unique. Given such a representation of a point, its image under spherical inversion is defined to be the point \"r\"v. This defines a homeomorphism from formula_1 to itself. As a map from Euclidean space to itself, the spherical inversion map is not defined at the origin, but we can extend it to formula_2, the one-point compactification of formula_3, by specifying that 0 should be sent to infinity and infinity should be sent to 0. Thus, spherical inversion can be thought of as a homeomorphism of formula_2.\n\nInversion is self-inverse, and fixes the points lying on the sphere. The inverse of a line is a circle through the centre of the reference sphere, and vice versa. The inverse of a plane is a sphere through the centre of the reference sphere, and vice versa. Otherwise the inverse of a circle is a circle; the inverse of a sphere is a sphere.\n\nInversion in a sphere is a powerful transformation. One simple example is in map projection.\n\nThe usual projection of the North or South Pole is inversion from the Earth to a plane. \nIf instead of making a pole the centre, we chose a city, then Inversion could produce a map where all the shortest routes (great circles) for flying from that city would appear as straight lines, which would simplify the flight path, for passengers at least.\n\nLet the reference sphere be Σ, with centre O and radius r denoted by {O, r}. All inverses, in this paper, are in the sphere Σ.\n\nThe results in this article are dependent on three simple ideas:\n\n\n(See fig 1)\n(See fig 2)\n(See fig 3)\n\nNote 4: Generally, the inverse of a line is a circle through the centre of reference.\n\nNote 5: Generally, the inverse of a plane is a sphere through the centre of reference.\n\n\n\n(See fig 4)\n(See fig 5)\n\nNote 6: Generally the inverse of a sphere is a sphere\n\n(Cf Figs 3, 4, 5)\n\nNote 7: Generally the inverse of a circle is a circle. \n\n\n"}
{"id": "45108865", "url": "https://en.wikipedia.org/wiki?curid=45108865", "title": "John M. Sullivan (mathematician)", "text": "John M. Sullivan (mathematician)\n\nJohn Matthew Sullivan (born February 25, 1963) is an American mathematician who works in Germany as a professor at the Technical University of Berlin. His research includes work on knot theory, constant-mean-curvature surfaces, mathematical foams, scientific visualization, and mesh generation.\n\nSullivan was born in Princeton, New Jersey, and graduated summa cum laude from Harvard University in 1985. He earned a master's degree from the University of Cambridge in 1986, and a doctorate from Princeton University in 1990 under the supervision of Frederick J. Almgren, Jr. After postdoctoral studies at The Geometry Center and the Mathematical Sciences Research Institute, he joined the faculty of the University of Illinois at Urbana–Champaign in 1997. He moved to Berlin in 2003, and chaired the Berlin Mathematical School from 2012 to 2014.\n\nIn 2012, he became one of the inaugural fellows of the American Mathematical Society.\n\n"}
{"id": "13908785", "url": "https://en.wikipedia.org/wiki?curid=13908785", "title": "Law of Continuity", "text": "Law of Continuity\n\nThe law of continuity is a heuristic principle introduced by Gottfried Leibniz based on earlier work by Nicholas of Cusa and Johannes Kepler. It is the principle that \"whatever succeeds for the finite, also succeeds for the infinite\". Kepler used The Law of Continuity to calculate the area of the circle by representing the latter as an infinite-sided polygon with infinitesimal sides, and adding the areas of infinitely-many triangles with infinitesimal bases. Leibniz used the principle to extend concepts such as arithmetic operations, from ordinary numbers to infinitesimals, laying the groundwork for infinitesimal calculus. A mathematical implementation of the law of continuity is provided by the transfer principle in the context of the hyperreal numbers.\n\nA related law of continuity concerning intersection numbers in geometry was promoted by Jean-Victor Poncelet in his \"Traité des propriétés projectives des figures\". \nLeibniz expressed the law in the following terms in 1701:\n\nIn a 1702 letter to French mathematician Pierre Varignon subtitled “Justification of the Infinitesimal Calculus by that of Ordinary Algebra,\" Leibniz adequately summed up the true meaning of his law, stating that \"the rules of the finite are found to succeed in the infinite.\" \n\nThe Law of Continuity became important to Leibniz's justification and conceptualization of the infinitesimal calculus.\n\n"}
{"id": "37520883", "url": "https://en.wikipedia.org/wiki?curid=37520883", "title": "Left and right (algebra)", "text": "Left and right (algebra)\n\nIn algebra, the terms left and right denote the order of a binary operation (usually, but not always called \"multiplication\") in non-commutative algebraic structures.\nA binary operation ∗ is usually written in the infix form:\nThe argument  is placed on the left side, and the argument  is on the right side. Even if the symbol of the operation is omitted, the order of and does matter unless ∗ is commutative.\n\nA two-sided property is fulfilled on both sides. A one-sided property is related to one (unspecified) of two sides.\n\nAlthough terms are similar, left–right distinction in algebraic parlance is not related either to left and right limits in calculus, or to left and right in geometry.\n\nA binary operation  may be considered as a family of unary operators through currying\ndepending on  as a parameter. It is the family of \"right\" operations. Similarly,\ndefines the family of \"left\" operations parametrized with .\n\nIf for some , the left operation  is identical, then is called a left identity. Similarly, if , then is a right identity.\n\nIn ring theory, a subring which is invariant under \"any\" left multiplication in a ring, is called a left ideal. Similarly, a right multiplications-invariant subring is a right ideal.\n\nOver non-commutative rings, the left–right distinction is applied to modules, namely to specify the side where a scalar (module element) appear in the scalar multiplication. \n\nThe distinction is not purely syntactical because implies two different associativity rules (the lowest row in the table) which link multiplication in a module with multiplication in a ring.\n\nA bimodule is simultaneously a left and right module, with two \"different\" scalar multiplication operations, obeying an obvious associativity condition on them.\n\n\nIn category theory the usage of \"left\" is \"right\" has some algebraic resemblance, but refers to left and right sides of morphisms. See adjoint functors.\n\n\n"}
{"id": "16834417", "url": "https://en.wikipedia.org/wiki?curid=16834417", "title": "Lentoid", "text": "Lentoid\n\nLentoid is a geometric shape of a three-dimensional body, best described as a circle viewed from one direction and a convex lens viewed from every orthogonal direction. The term is most often used in describing jewelry and cellular phenomena in microbiology.\n\nSince ancient times, the lentoid shape has been used to fashion jewelry and seals for identification made from a variety of gemstones and metals. In Minoan Crete, for example, seals have been found with complex carving on lentoid stones. The lentoid shape was one of the most commonly recovered seal shapes from Minoan Knossos on Crete dating to the Bronze Age, as evidenced by the finds at that Bronze Age palace.\n\n"}
{"id": "226505", "url": "https://en.wikipedia.org/wiki?curid=226505", "title": "Limit point", "text": "Limit point\n\nIn mathematics, a limit point (or cluster point or accumulation point) of a set \"S\" in a topological space \"X\" is a point \"x\" that can be \"approximated\" by points of \"S\" in the sense that every neighbourhood of \"x\" with respect to the topology on \"X\" also contains a point of \"S\" other than \"x\" itself. A limit point of a set \"S\" does not itself have to be an element of \"S\".\n\nThis concept profitably generalizes the notion of a limit and is the underpinning of concepts such as closed set and topological closure. Indeed, a set is closed if and only if it contains all of its limit points, and the topological closure operation can be thought of as an operation that enriches a set by uniting it with its limit points.\n\nThere is also a closely related concept for sequences. A cluster point (or accumulation point) of a sequence (\"x\") in a topological space \"X\" is a point \"x\" such that, for every neighbourhood \"V\" of \"x\", there are infinitely many natural numbers \"n\" such that \"x\" ∈ \"V\". This concept generalizes to nets and filters. \n\nLet \"S\" be a subset of a topological space \"X\". \nA point \"x\" in \"X\" is a limit point (or cluster point or accumulation point) of \"S\" if every neighbourhood of \"x\" contains at least one point of \"S\" different from \"x\" itself. \n\nNote that it doesn't make a difference if we restrict the condition to open neighbourhoods only. It is often convenient to use the \"open neighbourhood\" form of the definition to show that a point is a limit point and to use the \"general neighbourhood\" form of the definition to derive facts from a known limit point. \n\nIf \"X\" is a \"T\" space (which all metric spaces are), then \"x\" ∈ \"X\" is a limit point of \"S\" if and only if every neighbourhood of \"x\" contains infinitely many points of \"S\". Indeed, \"T\" spaces are characterized by this property. \n\nIf \"X\" is a Fréchet–Urysohn space (which all metric spaces and first-countable spaces are), then \"x\" ∈ \"X\" is a limit point of \"S\" if and only if there is a sequence of points in \"S\" \\ {\"x\"} whose limit is \"x\". Indeed, Fréchet–Urysohn spaces are characterized by this property.\n\nIf every open set containing \"x\" contains infinitely many points of \"S\" then \"x\" is a specific type of limit point called an ω-accumulation point of \"S\".\n\nIf every open set containing \"x\" contains uncountably many points of \"S\" then \"x\" is a specific type of limit point called a condensation point of \"S\".\n\nIf every open set \"U\" containing \"x\" satisfies then \"x\" is a specific type of limit point called a of \"S\".\n\nIn a topological space formula_1, a point formula_2 is said to be a cluster point (or accumulation point) of a sequence formula_3 if, for every neighbourhood formula_4 of formula_5, there are infinitely many formula_6 such that formula_7. It is equivalent to say that for every neighbourhood formula_4 of formula_5 and every formula_10, there is some formula_11 such that formula_7. If formula_1 is a metric space or a first-countable space (or, more generally, a Fréchet–Urysohn space), then formula_14 is cluster point of formula_3 if and only if formula_16 is a limit of some subsequence of formula_3. \nThe set of all cluster points of a sequence is sometimes called the limit set. \n\nThe concept of a net generalizes the idea of a sequence. A net is a function formula_18, where formula_19 is a directed set and formula_20 is a topological space. A point formula_21 is said to be a cluster point (or accumulation point) of the net formula_22 if, for every neighbourhood formula_4 of formula_5 and every formula_25, there is some formula_26 such that formula_27, equivalently, if formula_28 has a subnet which converges to formula_14. Cluster points in nets encompass the idea of both condensation points and ω-accumulation points. Clustering and limit points are also defined for the related topic of filters.\n\n"}
{"id": "5540055", "url": "https://en.wikipedia.org/wiki?curid=5540055", "title": "Line–sphere intersection", "text": "Line–sphere intersection\n\nIn analytic geometry, a line and a sphere can intersect in three ways: no intersection at all, at exactly one point, or in two points. Methods for distinguishing these cases, and determining equations for the points in the latter cases, are useful in a number of circumstances. For example, this is a common calculation to perform during ray tracing (Eberly 2006:698).\n\nIn vector notation, the equations are as follows:\n\nEquation for a sphere\n\nEquation for a line starting at formula_5\n\nSearching for points that are on the line and on the sphere means combining the equations and solving for formula_7:\n\n\n\n"}
{"id": "51647821", "url": "https://en.wikipedia.org/wiki?curid=51647821", "title": "List of companies involved in quantum computing or communication", "text": "List of companies involved in quantum computing or communication\n\n! Company !! Date initiated !! Area \n!Technology!! Affiliate University or Research Institute !! Headquarters\n"}
{"id": "346167", "url": "https://en.wikipedia.org/wiki?curid=346167", "title": "List of mathematical logic topics", "text": "List of mathematical logic topics\n\nThis is a list of mathematical logic topics, by Wikipedia page.\n\nFor traditional syllogistic logic, see the list of topics in logic. See also the list of computability and complexity topics for more theory of algorithms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "634754", "url": "https://en.wikipedia.org/wiki?curid=634754", "title": "List of partial differential equation topics", "text": "List of partial differential equation topics\n\nThis is a list of partial differential equation topics.\n\n\n\n\n"}
{"id": "252176", "url": "https://en.wikipedia.org/wiki?curid=252176", "title": "Napier's bones", "text": "Napier's bones\n\nNapier's bones is a manually-operated calculating device created by John Napier of Merchiston for calculation of products and quotients of numbers. The method was based on Arab mathematics and the lattice multiplication used by Matrakci Nasuh in the Umdet-ul Hisab and Fibonacci's work in his Liber Abaci. The technique was also called Rabdology. Napier published his version in 1617 in \"Rabdology.,\" printed in Edinburgh, Scotland, dedicated to his patron Alexander Seton.\n\nUsing the multiplication tables embedded in the rods, multiplication can be reduced to addition operations and division to subtractions. More advanced use of the rods can even extract square roots. Note that Napier's bones are not the same as logarithms, with which Napier's name is also associated.\n\nThe complete device usually includes a base board with a rim; the user places Napier's rods inside the rim to conduct multiplication or division. The board's left edge is divided into 9 squares, holding the numbers 1 to 9. The Napier's rods consist of strips of wood, metal or heavy cardboard. Napier's bones are three-dimensional, square in cross section, with four different rods engraved on each one. A set of such bones might be enclosed in a convenient carrying case.\n\nA rod's surface comprises 9 squares, and each square, except for the top one, comprises two halves divided by a diagonal line. The first square of each rod holds a single digit, and the other squares hold this number's double, triple, quadruple, quintuple, and so on until the last square contains nine times the number in the top square. The digits of each product are written one to each side of the diagonal; numbers less than 10 occupy the lower triangle.\n\nTo demonstrate how to use Napier's Bones for multiplication, three examples of increasing difficulty are explained below.\n\nThe first example computes 425 x 6.\n\nStart by placing the bones corresponding to the leading number of the problem into the boards. If a 0 is used in this number, a space is left between the bones corresponding to where the 0 digit would be. In this example, the bones 4, 2, and 5 are placed in the correct order as shown below.\n\nLooking at the first column, choose the number wishing to multiply by. In this example, that number is 6. The row this number is located in is the only row needed to perform the remaining calculations and thus the rest of the board is cleared below to allow more clarity in the remaining steps.\n\nStarting at the right side of the row, evaluate the diagonal columns by adding the numbers that share the same diagonal column. Single numbers simply remain that number.\n\nOnce the diagonal columns have been evaluated, one must simply read from left to right the numbers calculated for each diagonal column. For this example, reading the results of the summations from left to right produces the final answer of 2550.\n\nWhen multiplying by larger single digits, it is common that upon adding a diagonal column, the sum of the numbers result in a number that is 10 or greater. The following example demonstrates how to properly carry over the tens place when this occurs.\n\nThe second example computes 6785 x 8. \n\nBegin just as in Example 1 above and place in the board the corresponding bones to the leading number of the problem. For this example, the bones 6, 7, 8, and 5 are placed in the proper order as shown below. (Note that row 7 in bone 8 should read 5/6, not 5/4)\n\nIn the first column, find the number wishing to multiply by. In this example, that number is 8. With only needing to use the row 8 is located in for the remaining calculations, the rest of the board below has been cleared for clarity in explaining the remaining steps.\n\nJust as before, start at the right side of the row and evaluate each diagonal column. If the sum of a diagonal column equals 10 or greater, the tens place of this sum must be carried over and added along with the numbers in the diagonal column to the immediate left as demonstrated below.\n\nAfter each diagonal column has been evaluated, the calculated numbers can be read from left to right to produce a final answer. Reading the results of the summations from left to right, in this example, produces a final answer of 54280.\n\nThe third example computes 825 x 913.\n\nBegin once again by placing the corresponding bones to the leading number into the board. For this example the bones 8, 2, and 5 are placed in the proper order as shown below.\n\nWhen the number wishing to multiply by contains multiple digits, multiple rows must be reviewed. For the sake of this example, the rows for 9, 1, and 3 have been removed from the board, as seen below, for easier evaluation.\n\nEvaluate each row individually, adding each diagonal column as explained in the previous examples. Reading these sums from left to right will produce the numbers needed for the long hand addition calculations to follow. For this example, Row 9, Row 1, and Row 3 were evaluated separately to produce the results shown below.\n\nFor the final step of the solution, begin by writing the numbers being multiplied one over the other, drawing a line under the second number. \nStarting with the right most digit of the second number, place the results from the rows in sequential order as seen from right to left under each other while utilizing a 0 for place holders. \n\nThe rows and place holders can then be summed to produce a final answer.\n\nIn this example, the final answer produced is 753225.\n\nDivision can be performed in a similar fashion. Let's divide 46785399 by 96431, the two numbers we used in the earlier example. Put the bars for the divisor (96431) on the board, as shown in the graphic below. Using the abacus, find all the products of the divisor from 1 to 9 by reading the displayed numbers. Note that the dividend has eight digits, whereas the partial products (save for the first one) all have six. So you must temporarily ignore the final two digits of 46785399, namely the '99', leaving the number 467853. Next, look for the greatest partial product that is less than the truncated dividend. In this case, it's 385724. You must mark down two things, as seen in the diagram: since 385724 is in the '4' row of the abacus, mark down a '4' as the left-most digit of the quotient; also write the partial product, left-aligned, under the original dividend, and subtract the two terms. You get the difference as 8212999. Repeat the same steps as above: truncate the number to six digits, chose the partial product immediately less than the truncated number, write the row number as the next digit of the quotient, and subtract the partial product from the difference found in the first repetition. Following the diagram should clarify this. Repeat this cycle until the result of subtraction is less than the divisor. The number left is the remainder.\n\nSo in this example, we get a quotient of 485 with a remainder of 16364. We can just stop here and\nuse the fractional form of the answer formula_1.\n\nIf you prefer, we can also find as many decimal places as we need by continuing the cycle as in\nstandard long division. Mark a decimal point after the last digit of the quotient and append a zero\nto the remainder so we now have 163640. Continue the cycle, but each time appending a zero to the\nresult after the subtraction.\n\nLet's work through a couple of digits. The first digit after the decimal point is\n1, because the biggest partial product less than 163640 is\n96431, from row 1. Subtracting 96431 from 163640, we're left with 67209.\nAppending a zero, we have 672090 to consider for the next cycle (with the partial result\n485.1)\nThe second digit after the decimal point is 6, as the biggest partial product less\nthan 672090 is 578586 from row 6. The partial result is now 485.16, and so on.\n\nExtracting the square root uses an additional bone which looks a bit\ndifferent from the others as it has three columns on it. The first\ncolumn has the first nine squares 1, 4, 9, ... 64, 81, the second\ncolumn has the even numbers 2 through 18, and the last column just has\nthe numbers 1 through 9.\n\nLet's find the square root of 46785399 with the bones.\n\nFirst, group its digits in twos starting from the right so it looks\nlike this:\n\nStart with the leftmost group 46. Pick the largest square on the\nsquare root bone less than 46, which is 36 from the sixth row.\n\nBecause we picked the sixth row, the first digit of the solution is 6.\n\nNow read the second column from the sixth row on the square root bone,\n12, and set 12 on the board.\n\nThen subtract the value in the first\ncolumn of the sixth row, 36, from 46.\n\nAppend to this the next group of\ndigits in the number 78, to get the remainder 1078.\n\nAt the end of this step, the board and intermediate calculations\nshould look like this:\nNow, \"read\" the numbers in each row, ignoring the second and third columns\nfrom the square root bone and record these. (For example, read the sixth\nrow as : → 756)\n\nFind the largest number less than the current remainder, 1078.\nYou should find that 1024 from the eighth row is the largest value\nless than 1078.\n\nAs before, append 8 to get the next digit of the square root and\nsubtract the value of the eighth row 1024 from the current remainder\n1078 to get 54. Read the second column of the eighth row on the square\nroot bone, 16, and set the number on the board as follows.\n\nThe current number on the board is 12. Add to it the first digit of\n16, and append the second digit of 16 to the result. So you should set\nthe board to\n\nThe board and intermediate calculations now look like this.\n\nOnce again, find the row with the largest value less than the current\npartial remainder 5453. This time, it is the third row with 4089.\n\nThe next digit of the square root is 3. Repeat the same steps as\nbefore and subtract 4089 from the current remainder 5453 to get 1364\nas the next remainder. When you rearrange the board, notice that the\nsecond column of the square root bone is 6, a single digit. So just\nappend 6 to the current number on the board 136\nto set 1366 on the board.\n\nRepeat these operations once more. Now the largest value on the board\nsmaller than the current remainder 136499 is 123021 from the ninth\nrow.\n\nIn practice, you often don't need to find the value of every row to\nget the answer. You may be able to guess which row has the answer by\nlooking at the number on the first few bones on the board and\ncomparing it with the first few digits of the remainder. But in these\ndiagrams, we show the values of all rows to make it easier to\nunderstand.\n\nAs usual, append a 9 to the result and subtract 123021 from the\ncurrent remainder.\n\nYou've now \"used up\" all the digits of our number, and you still have\na remainder. This means you've got the integer portion of the square\nroot but there's some fractional bit still left.\n\nNotice that if we've really got the integer part of the square root,\nthe current result squared (6839² = 46771921) must be the\nlargest perfect square smaller than 46785899. Why? The square root of\n46785399 is going to be something like 6839.xxxx... This means\n6839² is smaller than 46785399, but 6840² is\nbigger than 46785399—the same thing as saying that 6839²\nis the largest perfect square smaller than 46785399.\n\nThis idea is used later on to understand how the technique works, but\nfor now let's continue to generate more digits of the square root.\n\nSimilar to finding the fractional portion of the answer in\nlong division, append two zeros to the remainder to get\nthe new remainder 1347800. The second column of the ninth row of the\nsquare root bone is 18 and the current number on the board is 1366. So\ncompute\nto set 13678 on the board.\n\nThe board and intermediate computations now look like this.\n\nThe ninth row with 1231101 is the largest value smaller than the\nremainder, so the first digit of the fractional part of the square\nroot is 9.\n\nSubtract the value of the ninth row from the remainder and append a\ncouple more zeros to get the new remainder 11669900. The second column\non the ninth row is 18 with 13678 on the board, so compute\nand set 136798 on the board.\n\nYou can continue these steps to find as many digits as you need and\nyou stop when you have the precision you want, or if you find that the\nremainder becomes zero which means you have the exact square root.\n\nHaving found the desired number of digits, you can easily determine whether or not you need to round up; i.e., increment the last digit. You don't need to find another digit to see if it is equal to or greater than five. Simply append 25 to the root and compare that to the remainder; if it is less than or equal to the remainder, then the next digit will be at least five and round up is needed. In the example above, we see that 6839925 is less than 11669900, so we need to round up the root to 6840.0.\n\nThere's only one more trick left to describe. If you want to find the\nsquare root of a number that isn't an integer, say 54782.917.\nEverything is the same, except you start out by grouping the digits\nto the left and right of the decimal point in groups of two.\n\nThat is, group 54782.917 as\n\nand proceed to extract the square root from these groups of digits.\n\nDuring the 19th century, Napier's bones underwent a transformation to make them easier to read. The rods began to be made with an angle of about 65° so that the triangles that had to be added were aligned vertically. In this case, in each square of the rod the unit is to the right and the ten (or the zero) to the left.\n\nThe rods were made such that the vertical and horizontal lines were more visible than the line where the rods touched, making the two components of each digit of the result much easier to read. Thus, in the picture it is immediately clear that:\n\nIn 1891, Henri Genaille invented a variant of Napier's bones which became known as Genaille–Lucas rulers. By representing the carry graphically, the user can read off the results of simple multiplication problems directly, with no intermediate mental calculations.\n\nThe following example is calculating 52749 × 4 = 210996.\n\nIn addition to the previously-described \"bones\" abacus, Napier also constructed a card abacus. Both devices are reunited in a piece held by the National Archaeological Museum of Spain in Madrid.\n\nThe apparatus is a box of wood with inlays of bone. In the top section it contains the \"bones\" abacus, and in the bottom section is the card abacus. This card abacus consists of 300 stored cards in 30 drawers. One hundred of these cards are covered with numbers (referred to as the \"number cards\"). The remaining two hundred cards contain small triangular holes, which, when laid on top of the number cards, allow the user to see only certain numbers. By the capable positioning of these cards, multiplications can be made up to the limit of a number 100 digits in length, by another number 200 digits in length.\n\nIn addition, the doors of the box contain the first powers of the digits, the coefficients of the terms of the first powers of the binomial and the numeric data of the regular polyhedra.\n\nIt is not known who was the author of this piece, nor if it is of Spanish origin or came from a foreigner, although it is probable that it originally belonged to the Spanish Academy of Mathematics (which was created by Philip II) or was a gift from the Prince of Wales. The only thing that is sure is that it was conserved in the Palace, whence it was passed to the National library and later to the National Archaeological Museum, where it is still conserved.\n\nIn 1876, the Spanish government sent the apparatus to the exhibition of scientific instruments in Kensington, where it received so much attention that several societies consulted the Spanish representation about the origin and use of the apparatus.\n\n"}
{"id": "475449", "url": "https://en.wikipedia.org/wiki?curid=475449", "title": "Oscillation (mathematics)", "text": "Oscillation (mathematics)\n\nIn mathematics, the oscillation of a function or a sequence is a number that quantifies how much a sequence or function varies between its extreme values as it approaches infinity or a point. As is the case with limits there are several definitions that put the intuitive concept into a form suitable for a mathematical treatment: oscillation of a sequence of real numbers, oscillation of a real valued function at a point, and oscillation of a function on an interval (or open set).\n\nLet formula_1 be a sequence of real numbers. The oscillation formula_2 of that sequence is defined as the difference (possibly infinite) between the limit superior and limit inferior of formula_1:\n\nThe oscillation is zero if and only if the sequence converges. It is undefined if formula_5 and formula_6 are both equal to +∞ or both equal to −∞, that is, if the sequence tends to +∞ or −∞.\n\nLet formula_7 be a real-valued function of a real variable. The oscillation of formula_7 on an interval formula_9 in its domain is the difference between the supremum and infimum of formula_7:\nMore generally, if formula_12 is a function on a topological space formula_13 (such as a metric space), then the oscillation of formula_7 on an open set formula_15 is\n\nThe oscillation of a function formula_7 of a real variable at a point formula_18 is defined as the limit as formula_19 of the oscillation of formula_7 on an formula_21-neighborhood of formula_18:\nThis is the same as the difference between the limit superior and limit inferior of the function at formula_18, \"provided\" the point formula_18 is not excluded from the limits.\n\nMore generally, if formula_12 is a real-valued function on a metric space, then the oscillation is\n\n\nIn the last example the sequence is periodic, and any sequence that is periodic without being constant will have non-zero oscillation. However, non-zero oscillation does not usually indicate periodicity.\n\nGeometrically, the graph of an oscillating function on the real numbers follows some path in the \"xy\"-plane, without settling into ever-smaller regions. In well-behaved cases the path might look like a loop coming back on itself, that is, periodic behaviour; in the worst cases quite irregular movement covering a whole region.\n\nOscillation can be used to define continuity of a function, and is easily equivalent to the usual \"ε\"-\"δ\" definition (in the case of functions defined everywhere on the real line): a function ƒ is continuous at a point \"x\" if and only if the oscillation is zero; in symbols, formula_28 A benefit of this definition is that it \"quantifies\" discontinuity: the oscillation gives how \"much\" the function is discontinuous at a point.\n\nFor example, in the classification of discontinuities:\n\nThis definition is useful in descriptive set theory to study the set of discontinuities and continuous points – the continuous points are the intersection of the sets where the oscillation is less than \"ε\" (hence a G set) – and gives a very quick proof of one direction of the Lebesgue integrability condition.\n\nThe oscillation is equivalence to the \"ε\"-\"δ\" definition by a simple re-arrangement, and by using a limit (lim sup, lim inf) to define oscillation: if (at a given point) for a given \"ε\" there is no \"δ\" that satisfies the \"ε\"-\"δ\" definition, then the oscillation is at least \"ε\", and conversely if for every \"ε\" there is a desired \"δ,\" the oscillation is 0. The oscillation definition can be naturally generalized to maps from a topological space to a metric space.\n\nMore generally, if \"f\" : \"X\" → \"Y\" is a function from a topological space \"X\" into a metric space \"Y\", then the oscillation of \"f\" is defined at each \"x\" ∈ \"X\" by\n\n\n"}
{"id": "30058480", "url": "https://en.wikipedia.org/wiki?curid=30058480", "title": "Paraproduct", "text": "Paraproduct\n\nIn mathematics, a paraproduct is a non-commutative bilinear operator acting on functions that in some sense is like the product of the two functions it acts on. According to Svante Janson and Jaak Peetre, in an article from 1988, \"the name 'paraproduct' denotes an idea rather than a unique definition; several versions exist and can be used for the same purposes.\"\n\nThis said, for a given operator formula_1 to be defined as a paraproduct, it is normally required to satisfy the following properties:\n\nA paraproduct may also be required to satisfy some form of Hölder's inequality.\n\n"}
{"id": "5133142", "url": "https://en.wikipedia.org/wiki?curid=5133142", "title": "Pathwidth", "text": "Pathwidth\n\nIn graph theory, a path decomposition of a graph \"G\" is, informally, a representation of \"G\" as a \"thickened\" path graph, and the pathwidth of \"G\" is a number that measures how much the path was thickened to form \"G\". More formally, a path-decomposition is\na sequence of subsets of vertices of \"G\" such that the endpoints of each edge appear in one of the subsets and such that each vertex appears in a contiguous subsequence of the subsets, and the pathwidth is one less than the size of the largest set in such a decomposition.\nPathwidth is also known as interval thickness (one less than the maximum clique size in an interval supergraph of \"G\"), vertex separation number, or node searching number.\n\nPathwidth and path-decompositions are closely analogous to treewidth and tree decompositions. They play a key role in the theory of graph minors: the families of graphs that are closed under graph minors and do not include all forests may be characterized as having bounded pathwidth, and the \"vortices\" appearing in the general structure theory for minor-closed graph families have bounded pathwidth. Pathwidth, and graphs of bounded pathwidth, also have applications in VLSI design, graph drawing, and computational linguistics.\n\nIt is NP-hard to find the pathwidth of arbitrary graphs, or even to approximate it accurately. However, the problem is fixed-parameter tractable: testing whether a graph has pathwidth \"k\" can be solved in an amount of time that depends linearly on the size of the graph but superexponentially on \"k\". Additionally, for several special classes of graphs, such as trees, the pathwidth may be computed in polynomial time without dependence on \"k\".\nMany problems in graph algorithms may be solved efficiently on graphs of bounded pathwidth, by using dynamic programming on a path-decomposition of the graph. Path decomposition may also be used to measure the space complexity of dynamic programming algorithms on graphs of bounded treewidth.\n\nIn the first of their famous series of papers on graph minors, define a path-decomposition of a graph \"G\" to be a sequence of subsets \"X\" of vertices of \"G\", with two properties:\nThe second of these two properties is equivalent to requiring that the subsets containing any particular vertex form a contiguous subsequence of the whole sequence. In the language of the later papers in Robertson and Seymour's graph minor series, a path-decomposition is a tree decomposition (\"X\",\"T\") in which the underlying tree \"T\" of the decomposition is a path graph.\n\nThe width of a path-decomposition is defined in the same way as for tree-decompositions, as max |\"X\"| − 1, and the pathwidth of \"G\" is the minimum width of any path-decomposition of \"G\". The subtraction of one from the size of \"X\" in this definition makes little difference in most applications of pathwidth, but is used to make the pathwidth of a path graph be equal to one.\n\nAs describes, pathwidth can be characterized in many equivalent ways.\n\nA path decomposition can be described as a sequence of graphs \"G\" that are glued together by identifying pairs of vertices from consecutive graphs in the sequence, such that the result of performing all of these gluings is \"G\". The graphs \"G\" may be taken as the induced subgraphs of the sets \"X\" in the first definition of path decompositions, with two vertices in successive induced subgraphs being glued together when they are induced by the same vertex in \"G\", and in the other direction one may recover the sets \"X\" as the vertex sets of the graphs \"G\". The width of the path decomposition is then one less than the maximum number of vertices in one of the graphs \"G\".\n\nThe pathwidth of any graph \"G\" is equal to one less than the smallest clique number of an interval graph that contains \"G\" as a subgraph. That is, for every path decomposition of \"G\" one can find an interval supergraph of \"G\", and for every interval supergraph of \"G\" one can find a path decomposition of \"G\", such that the width of the decomposition is one less than the clique number of the interval graph.\n\nIn one direction, suppose a path decomposition of \"G\" is given. Then one may represent the nodes of the decomposition as points on a line (in path order) and represent each vertex \"v\" as a closed interval having these points as endpoints. In this way, the path decomposition nodes containing \"v\" correspond to the representative points in the interval for \"v\". The intersection graph of the intervals formed from the vertices of \"G\" is an interval graph that contains \"G\" as a subgraph. Its maximal cliques are given by the sets of intervals containing the representative points, and its maximum clique size is one plus the pathwidth of \"G\".\n\nIn the other direction, if \"G\" is a subgraph of an interval graph with clique number \"p\" + 1, then \"G\" has a path decomposition of width \"p\" whose nodes are given by the maximal cliques of the interval graph. For instance, the interval graph shown with its interval representation in the figure has a path decomposition with five nodes, corresponding to its five maximal cliques \"ABC\", \"ACD\", \"CDE\", \"CDF\", and \"FG\"; the maximum clique size is three and the width of this path decomposition is two.\n\nThis equivalence between pathwidth and interval thickness is closely analogous to the equivalence between treewidth and the minimum clique number (minus one) of a chordal graph of which the given graph is a subgraph. Interval graphs are a special case of chordal graphs, and chordal graphs can be represented as intersection graphs of subtrees of a common tree generalizing the way that interval graphs are intersection graphs of subpaths of a path.\n\nSuppose that the vertices of a graph \"G\" are linearly ordered. Then the vertex separation number of \"G\" is the smallest number \"s\" such that, for each vertex \"v\", at most \"s\" vertices are earlier than \"v\" in the ordering but that have \"v\" or a later vertex as a neighbor.\nThe vertex separation number of \"G\" is the minimum vertex separation number of any linear ordering of \"G\". The vertex separation number was defined by , and is equal to the pathwidth of \"G\".\nThis follows from the earlier equivalence with interval graph clique numbers: if \"G\" is a subgraph of an interval graph \"I\", represented (as in the figure) in such a way that all interval endpoints are distinct, then the ordering of the left endpoints of the intervals of \"I\" has vertex separation number one less than the clique number of \"I\". And in the other direction, from a linear ordering of \"G\" one may derive an interval representation in which the left endpoint of the interval for a vertex \"v\" is its position in the ordering and the right endpoint is the position of the neighbor of \"v\" that comes last in the ordering.\n\nThe node searching game on a graph is a form of pursuit-evasion in which a set of searchers collaborate to track down a fugitive hiding in a graph. The searchers are placed on vertices of the graph while the fugitive may be in any edge of the graph, and the fugitive's location and moves are hidden from the searchers. In each turn, some or all of the searchers may move (arbitrarily, not necessarily along edges) from one vertex to another, and then the fugitive may move along any path in the graph that does not pass through a searcher-occupied vertex. The fugitive is caught when both endpoints of his edge are occupied by searchers. The node searching number of a graph is the minimum number of searchers needed to ensure that the fugitive can be guaranteed to be caught, no matter how he moves. As show, the node searching number of a graph equals its interval thickness. The optimal strategy for the searchers is to move the searchers so that in successive turns they form the separating sets of a linear ordering with minimal vertex separation number.\n\nEvery \"n\"-vertex graph with pathwidth \"k\" has at most edges, and the maximal pathwidth-\"k\" graphs (graphs to which no more edges can be added without increasing the pathwidth) have exactly this many edges. A maximal pathwidth-\"k\" graph must be either a \"k\"-path or a \"k\"-caterpillar, two special kinds of \"k\"-tree. A \"k\"-tree is a chordal graph with exactly maximal cliques, each containing vertices; in a \"k\"-tree that is not itself a , each maximal clique either separates the graph into two or more components, or it contains a single leaf vertex, a vertex that belongs to only a single maximal clique. A \"k\"-path is a \"k\"-tree with at most two leaves, and a \"k\"-caterpillar is a \"k\"-tree that can be partitioned into a \"k\"-path and a set of \"k\"-leaves each adjacent to a separator \"k\"-clique of the \"k\"-path. In particular the maximal graphs of pathwidth one are exactly the caterpillar trees.\n\nSince path-decompositions are a special case of tree-decompositions, the pathwidth of any graph is greater than or equal to its treewidth. The pathwidth is also less than or equal to the cutwidth, the minimum number of edges that cross any cut between lower-numbered and higher-numbered vertices in an optimal linear arrangement of the vertices of a graph; this follows because the vertex separation number, the number of lower-numbered vertices with higher-numbered neighbors, can at most equal the number of cut edges. For similar reasons, the cutwidth is at most the pathwidth times the maximum degree of the vertices in a given graph.\n\nAny \"n\"-vertex forest has pathwidth O(log \"n\"). For, in a forest, one can always find a constant number of vertices the removal of which leaves a forest that can be partitioned into two smaller subforests with at most 2\"n\"/3 vertices each. A linear arrangement formed by recursively partitioning each of these two subforests, placing the separating vertices between them, has logarithmic vertex searching number. The same technique, applied to a tree-decomposition of a graph, shows that, if the treewidth of an \"n\"-vertex graph \"G\" is \"t\", then the pathwidth of \"G\" is O(\"t\" log \"n\"). Since outerplanar graphs, series-parallel graphs, and Halin graphs all have bounded treewidth, they all also have at most logarithmic pathwidth.\n\nAs well as its relations to treewidth, pathwidth is also related to clique-width and cutwidth, via line graphs; the line graph \"L\"(\"G\") of a graph \"G\" has a vertex for each edge of \"G\" and two vertices in \"L\"(\"G\") are adjacent when the corresponding two edges of \"G\" share an endpoint. Any family of graphs has bounded pathwidth if and only if its line graphs have bounded linear clique-width, where linear clique-width replaces the disjoint union operation from clique-width with the operation of adjoining a single new vertex. If a connected graph with three or more vertices has maximum degree three, then its cutwidth equals the vertex separation number of its line graph.\n\nIn any planar graph, the pathwidth is at most proportional to the square root of the number of vertices. One way to find a path-decomposition with this width is (similarly to the logarithmic-width path-decomposition of forests described above) to use the planar separator theorem to find a set of O() vertices the removal of which separates the graph into two subgraphs of at most 2\"n\"/3 vertices each, and concatenate recursively-constructed path decompositions for each of these two subgraphs. The same technique applies to any class of graphs for which a similar separator theorem holds. Since, like planar graphs, the graphs in any fixed minor-closed graph family have separators of size O(), it follows that the pathwidth of the graphs in any fixed minor-closed family is again O(). For some classes of planar graphs, the pathwidth of the graph and the pathwidth of its dual graph must be within a constant factor of each other: bounds of this form are known for biconnected outerplanar graphs and for polyhedral graphs. For 2-connected planar graphs, the pathwidth of the dual graph is less than the pathwidth of the line graph. It remains open whether the pathwidth of a planar graph and its dual are always within a constant factor of each other in the remaining cases.\n\nIn some classes of graphs, it has been proven that the pathwidth and treewidth are always equal to each other: this is true for cographs, permutation graphs, the complements of comparability graphs, and the comparability graphs of interval orders.\nIn any cubic graph, or more generally any graph with maximum vertex degree three, the pathwidth is at most \"n\"/6 + o(\"n\"), where \"n\" is the number of vertices in the graph. There exist cubic graphs with pathwidth 0.082\"n\", but it is not known how to reduce this gap between this lower bound and the \"n\"/6 upper bound.\n\nIt is NP-complete to determine whether the pathwidth of a given graph is at most \"k\", when \"k\" is a variable given as part of the input. The best known worst-case time bounds for computing the pathwidth of arbitrary \"n\"-vertex graphs are of the form O(2 \"n\") for some constant \"c\". Nevertheless, several algorithms are known to compute path-decompositions more efficiently when the pathwidth is small, when the class of input graphs is limited, or approximately.\n\nPathwidth is fixed-parameter tractable: for any constant \"k\", it is possible to test whether the pathwidth is at most \"k\", and if so to find a path-decomposition of width \"k\", in linear time. In general, these algorithms operate in two phases. In the first phase, the assumption that the graph has pathwidth \"k\" is used to find a path-decomposition or tree-decomposition that is not optimal, but whose width can be bounded as a function of \"k\". In the second phase, a dynamic programming algorithm is applied to this decomposition in order to find the optimal decomposition.\nHowever, the time bounds for known algorithms of this type are exponential in \"k\", impractical except for the smallest values of \"k\". For the case \"k\" = 2 an explicit linear-time algorithm based on a structural decomposition of pathwidth-2 graphs is given by .\n\n surveys the complexity of computing the pathwidth on various special classes of graphs. Determining whether the pathwidth of a graph \"G\" is at most \"k\" remains NP-complete when \"G\" is restricted to bounded-degree graphs, planar graphs, planar graphs of bounded degree, chordal graphs, chordal dominoes, the complements of comparability graphs,\nand bipartite distance-hereditary graphs. It follows immediately that it is also NP-complete for the graph families that contain the bipartite distance-hereditary graphs, including the bipartite graphs, chordal bipartite graphs, distance-hereditary graphs, and circle graphs.\n\nHowever, the pathwidth may be computed in linear time for trees and forests. It may also be computed in polynomial time for graphs of bounded treewidth including series-parallel graphs, outerplanar graphs, and Halin graphs, as well as for split graphs, for the complements of chordal graphs, for permutation graphs, for cographs, for circular-arc graphs, for the comparability graphs of interval orders, and of course for interval graphs themselves, since in that case the pathwidth is just one less than the maximum number of intervals covering any point in an interval representation of the graph.\n\nIt is NP-hard to approximate the pathwidth of a graph to within an additive constant.\nThe best known approximation ratio of a polynomial time approximation algorithm for pathwidth is O((log \"n\")).\nFor earlier approximation algorithms for pathwidth, see and . For approximations on restricted classes of graphs, see .\n\nA minor of a graph \"G\" is another graph formed from \"G\" by contracting edges, removing edges, and removing vertices. Graph minors have a deep theory in which several important results involve pathwidth.\n\nIf a family \"F\" of graphs is closed under taking minors (every minor of a member of \"F\" is also in \"F\"), then by the Robertson–Seymour theorem \"F\" can be characterized as the graphs that do not have any minor in \"X\", where \"X\" is a finite set of forbidden minors. For instance, Wagner's theorem states that the planar graphs are the graphs that have neither the complete graph \"K\" nor the complete bipartite graph \"K\" as minors. In many cases, the properties of \"F\" and the properties of \"X\" are closely related, and the first such result of this type was by , and relates bounded pathwidth with the existence of a forest in the family of forbidden minors. Specifically, define a family \"F\" of graphs to have \"bounded pathwidth\" if there exists a constant \"p\" such that every graph in \"F\" has pathwidth at most \"p\". Then, a minor-closed family \"F\" has bounded pathwidth if and only if the set \"X\" of forbidden minors for \"F\" includes at least one forest.\n\nIn one direction, this result is straightforward to prove: if \"X\" does not include at least one forest, then the \"X\"-minor-free graphs do not have bounded pathwidth. For, in this case, the \"X\"-minor-free graphs include all forests, and in particular they include the perfect binary trees. But a perfect binary tree with 2\"k\" + 1 levels has pathwidth \"k\", so in this case the \"X\"-minor-free-graphs have unbounded pathwidth. In the other direction, if \"X\" contains an \"n\"-vertex forest, then the \"X\"-minor-free graphs have pathwidth at most \"n\" − 2.\n\nThe property of having pathwidth at most \"p\" is, itself, closed under taking minors: if \"G\" has a path-decomposition with width at most \"p\", then the same path-decomposition remains valid if any edge is removed from \"G\", and any vertex can be removed from \"G\" and from its path-decomposition without increasing the width. Contraction of an edge, also, can be accomplished without increasing the width of the decomposition, by merging the sub-paths representing the two endpoints of the contracted edge. Therefore, the graphs of pathwidth at most \"p\" can be characterized by a set \"X\" of excluded minors.\n\nAlthough \"X\" necessarily includes at least one forest, it is not true that all graphs in \"X\" are forests: for instance, \"X\" consists of two graphs, a seven-vertex tree and the triangle \"K\". However, the set of trees in \"X\" may be precisely characterized: these trees are exactly the trees that can be formed from three trees in \"X\" by connecting a new root vertex by an edge to an arbitrarily chosen vertex in each of the three smaller trees. For instance, the seven-vertex tree in \"X\" is formed in this way from the two-vertex tree (a single edge) in \"X\". Based on this construction, the number of forbidden minors in \"X\" can be shown to be at least (\"p\"!). The complete set \"X\" of forbidden minors for pathwidth-2 graphs has been computed; it contains 110 different graphs.\n\nThe graph structure theorem for minor-closed graph families states that, for any such family \"F\", the graphs in \"F\" can be decomposed into clique-sums of graphs that can be embedded onto surfaces of bounded genus, together with a bounded number of apexes and vortices for each component of the clique-sum. An apex is a vertex that may be adjacent to any other vertex in its component, while a vortex is a graph of bounded pathwidth that is glued into one of the faces of the bounded-genus embedding of a component. The cyclic ordering of the vertices around the face into which a vortex is embedded must be compatible with the path decomposition of the vortex, in the sense that breaking the cycle to form a linear ordering must lead to an ordering with bounded vertex separation number. This theory, in which pathwidth is intimately connected to arbitrary minor-closed graph families, has important algorithmic applications.\n\nIn VLSI design, the vertex separation problem was originally studied as a way to partition circuits into smaller subsystems, with a small number of components on the boundary between the subsystems.\n\nGate matrix layout is a specific style of CMOS VLSI layout for Boolean logic circuits. In gate matrix layouts, signals are propagated along \"lines\" (vertical line segments) while each gate of the circuit is formed by a sequence of device features that lie along a horizontal line segment. Thus, the horizontal line segment for each gate must cross the vertical segments for each of the lines that form inputs or outputs of the gate. As in the layouts of , a layout of this type that minimizes the number of vertical tracks on which the lines are to be arranged can be found by computing the pathwidth of a graph that has the lines as its vertices and pairs of lines sharing a gate as its edges. The same algorithmic approach can also be used to model folding problems in programmable logic arrays.\n\nPathwidth has several applications to graph drawing:\n\nIn the compilation of high-level programming languages, pathwidth arises in the problem of reordering sequences of straight-line code (that is, code with no control flow branches or loops) in such a way that all the values computed in the code can be placed in machine registers instead of having to be spilled into main memory. In this application, one represents the code to be compiled as a directed acyclic graph in which the nodes represent the input values to the code and the values computed by the operations within the code. An edge from node \"x\" to node \"y\" in this DAG represents the fact that value \"x\" is one of the inputs to operation \"y\". A topological ordering of the vertices of this DAG represents a valid reordering of the code, and the number of registers needed to evaluate the code in a given ordering is given by the vertex separation number of the ordering.\n\nFor any fixed number \"w\" of machine registers, it is possible to determine in linear time whether a piece of straight-line code can be reordered in such a way that it can be evaluated with at most \"w\" registers. For, if the vertex separation number of a topological ordering is at most \"w\", the minimum vertex separation among all orderings can be no larger, so the undirected graph formed by ignoring the orientations of the DAG described above must have pathwith at most \"w\". It is possible to test whether this is the case, using the known fixed-parameter-tractable algorithms for pathwidth, and if so to find a path-decomposition for the undirected graph, in linear time given the assumption that \"w\" is a constant. Once a path decomposition has been found, a topological ordering of width \"w\" (if one exists) can be found using dynamic programming, again in linear time.\n\n describe an application of path-width in natural language processing. In this application, sentences are modeled as graphs, in which the vertices represent words and the edges represent relationships between words; for instance if an adjective modifies a noun in the sentence then the graph would have an edge between those two words. Due to the limited capacity of human short-term memory, Kornai and Tuza argue that this graph must have bounded pathwidth (more specifically, they argue, pathwidth at most six), for otherwise humans would not be able to parse speech correctly.\n\nMany problems in graph algorithms may be solved efficiently on graphs of low pathwidth, by using dynamic programming on a path-decomposition of the graph. For instance, if a linear ordering of the vertices of an \"n\"-vertex graph \"G\" is given, with vertex separation number \"w\", then it is possible to find the maximum independent set of \"G\" in time On graphs of bounded pathwidth, this approach leads to fixed-parameter tractable algorithms, parametrized by the pathwidth. Such results are not frequently found in the literature because they are subsumed by similar algorithms parametrized by the treewidth; however, pathwidth arises even in treewidth-based dynamic programming algorithms in measuring the space complexity of these algorithms.\n\nThe same dynamic programming method also can be applied to graphs with unbounded pathwidth, leading to algorithms that solve unparametrized graph problems in exponential time. For instance, combining this dynamic programming approach with the fact that cubic graphs have pathwidth \"n\"/6 + o(\"n\") shows that, in a cubic graph, the maximum independent set can be constructed in time O(2), faster than previous known methods. A similar approach leads to improved exponential-time algorithms for the maximum cut and minimum dominating set problems in cubic graphs, and for several other NP-hard optimization problems.\n\n\n"}
{"id": "402504", "url": "https://en.wikipedia.org/wiki?curid=402504", "title": "Paul Painlevé", "text": "Paul Painlevé\n\nPaul Painlevé (; 5 December 1863 – 29 October 1933) was a French mathematician and statesman. He served twice as Prime Minister of the Third Republic: 12 September – 13 November 1917 and 17 April – 22 November 1925. His entry into politics came in 1906 after a professorship at the Sorbonne that began in 1892.\n\nHis first term as prime minister lasted only nine weeks but dealt with weighty issues, such as the Russian Revolution, the American entry into the war, the failure of the Nivelle Offensive, quelling the French Army Mutinies and relations with the British. In the 1920s as Minister of War he was a key figure in building the Maginot Line. In his second term as prime minister he dealt with the outbreak of rebellion in Syria's Jabal Druze in July\n1925 which had excited public and parliamentary anxiety over the general crisis of France's empire.\n\nPainlevé was born in Paris.\n\nBrought up within a family of skilled artisans (his father was a draughtsman) Painlevé showed early promise across the range of elementary studies and was initially attracted by either an engineering or political career. However, he finally entered the École Normale Supérieure in 1883 to study mathematics, receiving his doctorate in 1887 following a period of study at Göttingen, Germany with Felix Klein and Hermann Amandus Schwarz. Intending an academic career he became professor at Université de Lille, returning to Paris in 1892 to teach at the Sorbonne, École Polytechnique and later at the Collège de France and the École Normale Supérieure. He was elected a member of the Académie des Sciences in 1900.\n\nHe married Marguerite Petit de Villeneuve in 1901. Marguerite died during the birth of their son Jean Painlevé in the following year.\n\nPainlevé's mathematical work on differential equations led him to encounter their application to the theory of flight and, as ever, his broad interest in engineering topics fostered an enthusiasm for the emerging field of aviation. In 1908, he became Wilbur Wright's first airplane passenger in France and in 1909 created the first university course in aeronautics.\n\nSome differential equations can be solved using elementary algebraic operations that involve the trigonometric and exponential functions (sometimes called elementary functions). Many interesting special functions arise as solutions of linear second order ordinary differential equations. Around the turn of the century, \nPainlevé, É. Picard, and B. Gambier showed that\nof the class of nonlinear second order ordinary differential equations with polynomial coefficients, those that possess a certain desirable technical property, shared by the linear equations (nowadays commonly referred to as the 'Painlevé property') can always be transformed into one of fifty canonical forms. Of these fifty equations, just six require 'new' transcendental functions for their solution. These new transcendental functions, solving the remaining six equations, are called the Painlevé transcendents, and interest in them has revived recently due to their appearance in modern geometry, integrable systems and statistical mechanics.\n\nIn 1895 he gave a series of lectures at Stockholm University on differential equations, at the end stating the Painlevé conjecture about singularities of the n-body problem.\n\nIn the 1920s, Painlevé briefly turned his attention to the new theory of gravitation, general relativity, which had recently been introduced by Albert Einstein. In 1921, Painlevé proposed the Gullstrand–Painlevé coordinates for the Schwarzschild metric. The modification in the coordinate system was the first to reveal clearly that the Schwarzschild radius is a mere coordinate singularity (with however, profound global significance: it represents the event horizon of a black hole). This essential point was not generally appreciated by physicists until around 1963. In his diary, Harry Graf Kessler recorded that during a later visit to Berlin, Painlevé discussed pacifist international politics with Einstein, but there is no reference to discussions concerning the significance of the Schwarzschild radius.\n\nBetween 1915 and 1917, Painlevé served as French Minister for Public Instruction and Inventions. In December 1915, he requested a scientific exchange agreement between France and Britain, resulting in Anglo-French collaboration that ultimately led to the parallel development by Paul Langevin in France and Robert Boyle in Britain of the first active sonar.\n\nPainlevé took his aviation interests, along with those in naval and military matters, with him when he became, in 1906, Deputy for Paris's 5th arrondissement, the so-called Latin Quarter. By 1910, he had vacated his academic posts and World War I led to his active participation in military committees, joining Aristide Briand's cabinet in 1915 as Minister for Public Instruction and Inventions.\n\nOn his appointment as War Minister in March 1917 he was immediately called upon to give his approval, albeit with some misgivings, to Robert Georges Nivelle's wildly optimistic plans for a breakthrough offensive in Champagne. Painlevé reacted to the disastrous public failure of the plan by dismissing Nivelle and controversially replacing him with Henri Philippe Pétain. He was also responsible for isolating the Russian Expeditionary Force in France in the La Courtine camp, located in a remote spot on the plateau of Millevaches.\n\nOn 7 September 1917, Prime Minister Alexandre Ribot lost the support of the Socialists and Painlevé was called upon to form a new government.\n\nPainlevé was a leading voice at the Rapallo conference that led to the establishment of the Supreme Allied Council, a consultative body of Allied powers that anticipated the unified Allied command finally established in the following year. He appointed Ferdinand Foch as French representative knowing that he was the natural Allied commander. On Painlevé's return to Paris he was defeated and resigned on 13 November 1917 to be succeeded by Georges Clemenceau. Foch was finally made commander-in-chief of all Allied armies on the Western and Italian fronts in March 1918.\n\nPainlevé then played little active role in politics until the election of November 1919 when he emerged as a leftist critic of the right-wing Bloc National. By the time the next election approached in May 1924 his collaboration with Édouard Herriot, a fellow member of Briand's 1915 cabinet, had led to the formation of the Cartel des Gauches. Winning the election, Herriot became Prime Minister in June, while Painlevé became President of the Chamber of Deputies. Though Painlevé ran for President of France in 1924 he was defeated by Gaston Doumergue. Herriot's administration publicly recognised the Soviet Union, accepted the Dawes Plan and agreed to evacuate the Ruhr. However, a financial crisis arose from the ensuing devaluation of the franc and in April 1925, Herriot fell and Painlevé became Prime Minister for a second time on 17 April. Unfortunately, he was unable to offer convincing remedies for the financial problems and was forced to resign on 21 November.\n\nFollowing Painlevé's resignation, Briand formed a new government with Painlevé as Minister for War. Though Briand was defeated by Raymond Poincaré in 1926, Painlevé continued in office. Poincaré stabilised the franc with a return to the gold standard, but ultimately acceded power to Briand. During his tenure as Minister of War, Painlevé was instrumental in the creation of the Maginot Line. This line of military fortifications along France's Eastern border was largely designed by Painlevé, yet named for André Maginot, owing to Maginot's championing of public support and funding. Painlevé remained in office as Minister for War until July 1929.\n\nThough he was proposed for President of France in 1932, Painlevé withdrew before the election. He became Minister of Air later that year, making proposals for an international treaty to ban the manufacture of bomber aircraft and to establish an international air force to enforce global peace. On the fall of the government in January 1933, his political career ended.\n\nPainlevé died in Paris in October of the same year. On 4 November, after a eulogy by Prime Minister Albert Sarraut, he was interred in the Panthéon.\n\n\n\nChanges\n\n\nChanges\n\n\n\n\n"}
{"id": "9176798", "url": "https://en.wikipedia.org/wiki?curid=9176798", "title": "Quotient of subspace theorem", "text": "Quotient of subspace theorem\n\nIn mathematics, the quotient of subspace theorem is an important property of finite-dimensional normed spaces, discovered by Vitali Milman.\n\nLet (\"X\", ||·||) be an \"N\"-dimensional normed space. There exist subspaces \"Z\" ⊂ \"Y\" ⊂ \"X\" such that the following holds:\n\nis uniformly isomorphic to Euclidean. That is, there exists a positive quadratic form (\"Euclidean structure\") \"Q\" on \"E\", such that\n\nwith \"K\" > 1 a universal constant. \n\nThe statement is relative easy to prove by induction on the dimension of \"Z\" (even for \"Y=Z\", \"X\"=\"0\", \"c=1\") with a \"K\" that depends only on \"N\"; the point of the theorem is that \"K\" is independent of \"N\".\n\nIn fact, the constant \"c\" can be made arbitrarily close to 1, at the expense of the\nconstant \"K\" becoming large. The original proof allowed \n\n"}
{"id": "2644238", "url": "https://en.wikipedia.org/wiki?curid=2644238", "title": "Rectifiable set", "text": "Rectifiable set\n\nIn mathematics, a rectifiable set is a set that is smooth in a certain measure-theoretic sense. It is an extension of the idea of a rectifiable curve to higher dimensions; loosely speaking, a rectifiable set is a rigorous formulation of a piece-wise smooth set. As such, it has many of the desirable properties of smooth manifolds, including tangent spaces that are defined almost everywhere. Rectifiable sets are the underlying object of study in geometric measure theory.\n\nA subset formula_1 of Euclidean space formula_2 is said to be formula_3-rectifiable set if there exist a countable collection formula_4 of continuously differentiable maps\n\nsuch that the formula_3-Hausdorff measure formula_7 of\n\nis zero. The backslash here denotes the set difference. Equivalently, the formula_9 may be taken to be Lipschitz continuous without altering the definition.\n\nA set formula_1 is said to be purely formula_3-unrectifiable if for \"every\" (continuous, differentiable) formula_12, one has\n\nA standard example of a purely-1-unrectifiable set in two dimensions is the cross-product of the Smith–Volterra–Cantor set times itself.\n\n gives the following terminology for \"m\"-rectifiable sets \"E\" in a general metric space \"X\".\n\nDefinition 3 with formula_32 and formula_33 comes closest to the above definition for subsets of Euclidean spaces.\n\n\n"}
{"id": "36983", "url": "https://en.wikipedia.org/wiki?curid=36983", "title": "Recursive acronym", "text": "Recursive acronym\n\nA recursive acronym is an acronym that refers to itself. The term was first used in print in 1979 in Douglas Hofstadter's book \"Gödel, Escher, Bach: An Eternal Golden Braid\", in which Hofstadter invents the acronym GOD, meaning \"GOD Over Djinn\", to help explain infinite series, and describes it as a recursive acronym. Other references followed, however the concept was used as early as 1968 in John Brunner's science fiction novel \"Stand on Zanzibar\". In the story, the acronym EPT (Education for Particular Task) later morphed into \"Eptification for Particular Task\".\n\nRecursive acronyms typically form backwardly: either an existing ordinary acronym is given a new explanation of what the letters stand for, or a name is turned into an acronym by giving the letters an explanation of what they stand for, in each case with the first letter standing recursively for the whole acronym.\n\nIn computing, an early tradition in the hacker community (especially at MIT) was to choose acronyms and abbreviations that referred humorously to themselves or to other abbreviations. Perhaps the earliest example in this context, from 1960 the backronym \"Mash Until No Good\" was created to describe Mung, and a while after it was revised to \"Mung Until No Good\". It lived on as a recursive command in the editing language TECO. In 1977 or 1978 came TINT (\"TINT Is Not TECO\"), an editor for MagicSix written (and named) by Ted Anderson. This inspired the two MIT Lisp Machine editors called EINE (\"EINE Is Not Emacs\", German for \"one\") and ZWEI (\"ZWEI Was EINE Initially\", German for \"two\"). These were followed by Richard Stallman's GNU (GNU's Not Unix). Many others also include negatives, such as denials that the thing defined is or resembles something else (which the thing defined does in fact resemble or is even derived from), to indicate that, despite the similarities, it was distinct from the program on which it was based.\n\nAn earlier example appears in a 1976 textbook on data structures, in which the pseudo-language SPARKS is used to define the algorithms discussed in the text. \"SPARKS\" is claimed to be a non-acronymic name, but \"several cute ideas have been suggested\" as expansions of the name. One of the suggestions is \"Smart Programmers Are Required to Know SPARKS\". (this example is tail recursive)\n\n\n\nSome organizations have been named or renamed in this way:\n\n\n"}
{"id": "705158", "url": "https://en.wikipedia.org/wiki?curid=705158", "title": "Ruled surface", "text": "Ruled surface\n\nIn geometry, a surface \"S\" is ruled (also called a scroll) if \nExamples include the plane, the curved surface of a cylinder or cone, a conical surface with elliptical directrix, the right conoid, the helicoid, and the tangent developable of a smooth curve in space.\n\nA ruled surface can be described as the set of points swept by a moving straight line. For example, a cone is formed by keeping one point of a line fixed whilst moving another point along a circle. A surface is \"doubly ruled\" if through every one of its points there are two distinct lines that lie on the surface. The hyperbolic paraboloid and the hyperboloid of one sheet are doubly ruled surfaces. The plane is the only surface which contains at least three distinct lines through each of its points .\n\nThe properties of being ruled or doubly ruled are preserved by projective maps, and therefore are concepts of projective geometry. In algebraic geometry ruled surfaces are sometimes considered to be surfaces in affine or projective space over a field, but they are also sometimes considered as abstract algebraic surfaces without an embedding into affine or projective space, in which case \"straight line\" is understood to mean an affine or projective line.\n\n\nA ruled surface can be described by a \"parametric representation\" of the form \nAny curve formula_2 with fixed parameter formula_3 is a generator (line) and the curve formula_4 is the \"directrix\" of the representation. The vectors formula_5 describe the directions of the generators.\n\nThe directrix may collapse to a point (in case of a cone, see example below).\n\nAlternatively the ruled surface (CR) can be described by\n\nwith the second directrix formula_7.\nAlternatively, one can start with two non intersecting curves formula_8 as directrices, and get by (CD) a ruled surface with line directions formula_9\n\nFor the generation of a ruled surface by two directrices (or one directrix and the vectors of line directions) not only the geometric shape of these curves are essential but also the special parametric representations of them influence the shape of the ruled surface (see examples a), d)).\n\nFor theoretical investigations representation (CR) is more advantageous, because the parameter formula_10 appears only once.\n\na) Right circular cylinder formula_11:\n\nwith \n\nb) Right circular cone formula_16:\n\nwith formula_19<br>\nIn this case one could have used the apex as the directrix, i.e.: formula_20 and formula_21 as the line directions.\n\nFor any cone one can choose the apex as the directrix. This case shows: \"The directrix of a ruled surface may degenerate to a point\".\nc) Helicoid:\n\nThe directrix formula_25 is the z-axis, the line directions are formula_26 and the second directrix formula_27 is a helix.\n\nThe helicoid is a special case of the ruled generalized helicoids.\n\nd) Cylinder, cone and hyperboloids:\nThe parametric representation \nhas two horizontal circles as directrices. The additional parameter formula_29 allows to vary the parametric representations of the circles. For\n\nA hyperboloid of one sheet is a \"doubly\" ruled surface.\ne) Hyperbolic paraboloid:\n\nIf the two directrices in (CD) are the lines \none gets \nwhich is the hyperbolic paraboloid that interpolates the 4 points formula_39 bilinearly.\n\nObviously the ruled surface is a \"doubly ruled surface\", because any point lies on two lines of the surface.\n\nFor the example shown in the diagram: \nThe hyperbolic paraboloid has the equation formula_41.\nf) Möbius strip: \n\nThe ruled surface\nwith \ncontains a Möbius strip.\n\nThe diagram shows the Möbius strip for formula_45.\n\nA simple calculation shows formula_46 (see next section). Hence the given realization of a Möbius strip is \"not developable\". But there exist developable Möbius strips .\n\nFor the considerations below any necessary derivative is supposed to exist..\n\nFor the determination of the normal vector at a point one needs the partial derivatives of the representation formula_47 :\nHence the normal vector is \nBecause of formula_51 (A mixed product with two equal vectors is always 0 !), vector formula_52 is a tangent vector at any point formula_53. The tangent planes along this line are all the same, if formula_54 is a muliple of formula_55 . This is possible only , if the three vectors formula_56 lie in a plane, i.e. they are linear dependent. The linear dependency of three vectors can be checked using the determinant of these vectors:\n\n\nThe importance of this determinant condition shows the following statement:\n\n\n\nThe determinant condition for developable surfaces is used to determine numerically developable connections between space curves (directrices). The diagram shows a developable connection between two ellipses contained in different planes (one horizontal, the other vertical) and its development.\n\nAn impression of the usage of developable surfaces in \"Computer Aided Design\" (CAD) is given in \"Interactive design of developable surfaces\"\n\nA \"historical\" survey on developable surfaces can be found in \"Developable Surfaces: Their History and Application\"\n\nIn algebraic geometry, ruled surfaces were originally defined as projective surfaces in projective space containing a straight line through any given point. This immediately implies that there is a projective line on the surface through any given point, and this condition is now often used as the definition of a ruled surface: ruled surfaces are defined to be abstract projective surfaces satisfying this condition that there is a projective line through any point. This is equivalent to saying that they are birational to the product of a curve and a projective line. Sometimes a ruled surface is defined to be one satisfying the stronger condition that it has a fibration over a curve with fibers that are projective lines. This excludes the projective plane, which has a projective line though every point but cannot be written as such a fibration.\n\nRuled surfaces appear in the Enriques classification of projective complex surfaces, because every algebraic surface of Kodaira dimension formula_61 is a ruled surface (or a projective plane, if one uses the restrictive definition of ruled surface). \nEvery minimal projective ruled surface other than the projective plane is the projective bundle of a 2-dimensional vector bundle over some curve. The ruled surfaces with base curve of genus 0 are the Hirzebruch surfaces.\n\nDoubly ruled surfaces are the inspiration for curved hyperboloid structures that can be built with a latticework of straight elements, namely:\n\nThe RM-81 Agena rocket engine employed straight cooling channels that were laid out in a ruled surface to form the throat of the nozzle section.\n\n\n"}
{"id": "695241", "url": "https://en.wikipedia.org/wiki?curid=695241", "title": "Scale invariance", "text": "Scale invariance\n\nIn physics, mathematics, statistics, and economics, scale invariance is a feature of objects or laws that do not change if scales of length, energy, or other variables, are multiplied by a common factor, thus represent a universality.\n\nThe technical term for this transformation is a dilatation (also known as dilation), and the dilatations can also form part of a larger conformal symmetry.\n\n\nIn mathematics, one can consider the scaling properties of a function or curve under rescalings of the variable . That is, one is interested in the shape of for some scale factor , which can be taken to be a length or size rescaling. The requirement for to be invariant under all rescalings is usually taken to be\nfor some choice of exponent , and for all dilations . This is equivalent to   being a homogeneous function of degree .\n\nExamples of scale-invariant functions are the monomials formula_2, for which , in that clearly\n\nAn example of a scale-invariant curve is the logarithmic spiral, a kind of curve that often appears in nature. In polar coordinates , the spiral can be written as\n\nAllowing for rotations of the curve, it is invariant under all rescalings ; that is, is identical to a rotated version of .\n\nThe idea of scale invariance of a monomial generalizes in higher dimensions to the idea of a homogeneous polynomial, and more generally to a homogeneous function. Homogeneous functions are the natural denizens of projective space, and homogeneous polynomials are studied as projective varieties in projective geometry. Projective geometry is a particularly rich field of mathematics; in its most abstract forms, the geometry of schemes, it has connections to various topics in string theory.\n\nIt is sometimes said that fractals are scale-invariant, although more precisely, one should say that they are self-similar. A fractal is equal to itself typically for only a discrete set of values , and even then a translation and rotation may have to be applied to match the fractal up to itself.\n\nThus, for example, the Koch curve scales with , but the scaling holds only for values of for integer . In addition, the Koch curve scales not only at the origin, but, in a certain sense, \"everywhere\": miniature copies of itself can be found all along the curve.\n\nSome fractals may have multiple scaling factors at play at once; such scaling is studied with multi-fractal analysis.\n\nPeriodic external and internal rays are invariant curves .\n\nIf is the average, expected power at frequency , then noise scales as\nwith = 0 for white noise, = −1 for pink noise, and = −2 for Brownian noise (and more generally, Brownian motion).\n\nMore precisely, scaling in stochastic systems concerns itself with the likelihood of choosing a particular configuration out of the set of all possible random configurations. This likelihood is given by the probability distribution.\n\nExamples of scale-invariant distributions are the Pareto distribution and the Zipfian distribution.\n\nTweedie distributions are a special case of exponential dispersion models, a class of statistical models used to describe error distributions for the generalized linear model and characterized by closure under additive and reproductive convolution as well as under scale transformation. These include a number of common distributions: the normal distribution, Poisson distribution and gamma distribution, as well as more unusual distributions like the compound Poisson-gamma distribution, positive stable distributions, and extreme stable distributions.\nConsequent to their inherent scale invariance Tweedie random variables \"Y\" demonstrate a variance var(\"Y\") to mean E(\"Y\") power law:\nwhere \"a\" and \"p\" are positive constants. This variance to mean power law is known in the physics literature as fluctuation scaling, and in the ecology literature as Taylor's law.\n\nRandom sequences, governed by the Tweedie distributions and evaluated by the method of expanding bins exhibit a biconditional relationship between the variance to mean power law and power law autocorrelations. The Wiener–Khinchin theorem further implies that for any sequence that exhibits a variance to mean power law under these conditions will also manifest \"1/f\" noise.\n\nThe Tweedie convergence theorem provides a hypothetical explanation for the wide manifestation of fluctuation scaling and \"1/f\" noise. It requires, in essence, that any exponential dispersion model that asymptotically manifests a variance to mean power law will be required express a variance function that comes within the domain of attraction of a Tweedie model. Almost all distribution functions with finite cumulant generating functions qualify as exponential dispersion models and most exponential dispersion models manifest variance functions of this form. Hence many probability distributions have variance functions that express this asymptotic behavior, and the Tweedie distributions become foci of convergence for a wide range of data types.\n\nMuch as the central limit theorem requires certain kinds of random variables to have as a focus of convergence the Gaussian distribution and express white noise, the Tweedie convergence theorem requires certain non-Gaussian random variables to express \"1/f\" noise and fluctuation scaling.\n\nIn physical cosmology, the power spectrum of the spatial distribution of the cosmic microwave background is near to being a scale-invariant function. Although in mathematics this means that the spectrum is a power-law, in cosmology the term \"scale-invariant\" indicates that the amplitude, , of primordial fluctuations as a function of wave number, , is approximately constant, i.e. a flat spectrum. This pattern is consistent with the proposal of cosmic inflation.\n\nClassical field theory is generically described by a field, or set of fields, \"φ\", that depend on coordinates, \"x\". Valid field configurations are then determined by solving differential equations for \"φ\", and these equations are known as field equations.\n\nFor a theory to be scale-invariant, its field equations should be invariant under a rescaling of the coordinates, combined with some specified rescaling of the fields,\n\nThe parameter \"Δ\" is known as the scaling dimension of the field, and its value depends on the theory under consideration. Scale invariance will typically hold provided that no fixed length scale appears in the theory. Conversely, the presence of a fixed length scale indicates that a theory is not scale-invariant.\n\nA consequence of scale invariance is that given a solution of a scale-invariant field equation, we can automatically find other solutions by rescaling both the coordinates and the fields appropriately. In technical terms, given a solution, \"φ\"(\"x\"), one always has other solutions of the form\n\nFor a particular field configuration, \"φ\"(\"x\"), to be scale-invariant, we require that\n\nwhere \"Δ\" is, again, the scaling dimension of the field.\n\nWe note that this condition is rather restrictive. In general, solutions even of scale-invariant field equations will not be scale-invariant, and in such cases the symmetry is said to be spontaneously broken.\n\nAn example of a scale-invariant classical field theory is electromagnetism with no charges or currents. The fields are the electric and magnetic fields, E(x,\"t\") and B(x,\"t\"), while their field equations are Maxwell's equations.\n\nWith no charges or currents, these field equations take the form of wave equations\nwhere \"c\" is the speed of light.\n\nThese field equations are invariant under the transformation\n\nMoreover, given solutions of Maxwell's equations, E(x, \"t\") and B(x, \"t\"), it holds that \nE(λx, λ\"t\") and B(λx, λ\"t\") are also solutions.\n\nAnother example of a scale-invariant classical field theory is the massless scalar field (note that the name scalar is unrelated to scale invariance). The scalar field, is a function of a set of spatial variables, x, and a time variable, .\n\nConsider first the linear theory. Like the electromagnetic field equations above, the equation of motion for this theory is also a wave equation,\nand is invariant under the transformation\n\nThe name massless refers to the absence of a term formula_18 in the field equation. Such a term is often referred to as a `mass' term, and would break the invariance under the above transformation. In relativistic field theories, a mass-scale, is physically equivalent to a fixed length scale through\nand so it should not be surprising that massive scalar field theory is \"not\" scale-invariant.\n\nThe field equations in the examples above are all linear in the fields, which has meant that the scaling dimension, , has not been so important. However, one usually requires that the scalar field action is dimensionless, and this fixes the scaling dimension of . In particular,\nwhere is the combined number of spatial and time dimensions.\n\nGiven this scaling dimension for , there are certain nonlinear modifications of massless scalar field theory which are also scale-invariant. One example is massless φ theory for =4. The field equation is\n\nWhen =4 (e.g. three spatial dimensions and one time dimension), the scalar field scaling dimension is =1. The field equation is then invariant under the transformation\n\nThe key point is that the parameter must be dimensionless, otherwise one introduces a fixed length scale into the theory: For theory, this is only the case in =4.\nNote that under these transformations the argument of the function is unchanged.\n\nThe scale-dependence of a quantum field theory (QFT) is characterised by the way its coupling parameters depend on the energy-scale of a given physical process. This energy dependence is described by the renormalization group, and is encoded in the beta-functions of the theory.\n\nFor a QFT to be scale-invariant, its coupling parameters must be independent of the energy-scale, and this is indicated by the vanishing of the beta-functions of the theory. Such theories are also known as fixed points of the corresponding renormalization group flow.\n\nA simple example of a scale-invariant QFT is the quantized electromagnetic field without charged particles. This theory actually has no coupling parameters (since photons are massless and non-interacting) and is therefore scale-invariant, much like the classical theory.\n\nHowever, in nature the electromagnetic field is coupled to charged particles, such as electrons. The QFT describing the interactions of photons and charged particles is quantum electrodynamics (QED), and this theory is not scale-invariant. We can see this from the QED beta-function. This tells us that the electric charge (which is the coupling parameter in the theory) increases with increasing energy. Therefore, while the quantized electromagnetic field without charged particles is scale-invariant, QED is not scale-invariant.\n\nFree, massless quantized scalar field theory has no coupling parameters. Therefore, like the classical version, it is scale-invariant. In the language of the renormalization group, this theory is known as the Gaussian fixed point.\n\nHowever, even though the classical massless \"φ\" theory is scale-invariant in \"D\"=4, the quantized version is not scale-invariant. We can see this from the beta-function for the coupling parameter, \"g\".\n\nEven though the quantized massless \"φ\" is not scale-invariant, there do exist scale-invariant quantized scalar field theories other than the Gaussian fixed point. One example is the Wilson-Fisher fixed point, below.\n\nScale-invariant QFTs are almost always invariant under the full conformal symmetry, and the study of such QFTs is conformal field theory (CFT). Operators in a CFT have a well-defined scaling dimension, analogous to the scaling dimension, \"∆\", of a classical field discussed above. However, the scaling dimensions of operators in a CFT typically differ from those of the fields in the corresponding classical theory. The additional contributions appearing in the CFT are known as anomalous scaling dimensions.\n\nThe φ theory example above demonstrates that the coupling parameters of a quantum field theory can be scale-dependent even if the corresponding classical field theory is scale-invariant (or conformally invariant). If this is the case, the classical scale (or conformal) invariance is said to be anomalous. A classically scale invariant field theory, where scale invariance is broken by quantum effects, provides an explication of the nearly exponential expansion of the early universe called cosmic inflation, as long as the theory can be studied through perturbation theory.\n\nIn statistical mechanics, as a system undergoes a phase transition, its fluctuations are described by a scale-invariant statistical field theory. For a system in equilibrium (i.e. time-independent) in spatial dimensions, the corresponding statistical field theory is formally similar to a -dimensional CFT. The scaling dimensions in such problems are usually referred to as critical exponents, and one can in principle compute these exponents in the appropriate CFT.\n\nAn example that links together many of the ideas in this article is the phase transition of the Ising model, a simple model of ferromagnetic substances. This is a statistical mechanics model, which also has a description in terms of conformal field theory. The system consists of an array of lattice sites, which form a -dimensional periodic lattice. Associated with each lattice site is a magnetic moment, or spin, and this spin can take either the value +1 or −1. (These states are also called up and down, respectively.)\n\nThe key point is that the Ising model has a spin-spin interaction, making it energetically favourable for two adjacent spins to be aligned. On the other hand, thermal fluctuations typically introduce a randomness into the alignment of spins. At some critical temperature, , spontaneous magnetization is said to occur. This means that below the spin-spin interaction will begin to dominate, and there is some net alignment of spins in one of the two directions.\n\nAn example of the kind of physical quantities one would like to calculate at this critical temperature is the correlation between spins separated by a distance . This has the generic behaviour:\nfor some particular value of formula_26, which is an example of a critical exponent.\n\nThe fluctuations at temperature are scale-invariant, and so the Ising model at this phase transition is expected to be described by a scale-invariant statistical field theory. In fact, this theory is the Wilson-Fisher fixed point, a particular scale-invariant scalar field theory.\n\nIn this context, is understood as a correlation function of scalar fields,\nNow we can fit together a number of the ideas seen already.\n\nFrom the above, one sees that the critical exponent, , for this phase transition, is also an anomalous dimension. This is because the classical dimension of the scalar field,\nis modified to become\nwhere is the number of dimensions of the Ising model lattice.\n\nSo this anomalous dimension in the conformal field theory is the \"same\" as a particular critical exponent of the Ising model phase transition.\n\nNote that for dimension , can be calculated approximately, using the epsilon expansion, and one finds that\n\nIn the physically interesting case of three spatial dimensions, we have =1, and so this expansion is not strictly reliable. However, a semi-quantitative prediction is that is numerically small in three dimensions.\n\nOn the other hand, in the two-dimensional case the Ising model is exactly soluble. In particular, it is equivalent to one of the minimal models, a family of well-understood CFTs, and it is possible to compute (and the other critical exponents) exactly,\n\nThe anomalous dimensions in certain two-dimensional CFTs can be related to the typical fractal dimensions of random walks, where the random walks are defined via Schramm–Loewner evolution (SLE). As we have seen above, CFTs describe the physics of phase transitions, and so one can relate the critical exponents of certain phase transitions to these fractal dimensions. Examples include the 2\"d\" critical Ising model and the more general 2\"d\" critical Potts model. Relating other 2\"d\" CFTs to SLE is an active area of research.\n\nA phenomenon known as universality is seen in a large variety of physical systems. It expresses the idea that different microscopic physics can give rise to the same scaling behaviour at a phase transition. A canonical example of universality involves the following two systems:\n\nEven though the microscopic physics of these two systems is completely different, their critical exponents turn out to be the same. Moreover, one can calculate these exponents using the same statistical field theory. The key observation is that at a phase transition or critical point, fluctuations occur at all length scales, and thus one should look for a scale-invariant statistical field theory to describe the phenomena. In a sense, universality is the observation that there are relatively few such scale-invariant theories.\n\nThe set of different microscopic theories described by the same scale-invariant theory is known as a universality class. Other examples of systems which belong to a universality class are:\n\nThe key observation is that, for all of these different systems, the behaviour resembles a phase transition, and that the language of statistical mechanics and scale-invariant statistical field theory may be applied to describe them.\n\nUnder certain circumstances, fluid mechanics is a scale-invariant classical field theory. The fields are the velocity of the fluid flow, formula_32, the fluid density, formula_33, and the fluid pressure, formula_34. These fields must satisfy both the Navier–Stokes equation and the continuity equation. For a Newtonian fluid these take the respective forms\nwhere formula_37 is the .\n\nIn order to deduce the scale invariance of these equations we specify an equation of state, relating the fluid pressure to the fluid density. The equation of state depends on the type of fluid and the conditions to which it is subjected. For example, we consider the isothermal ideal gas, which satisfies\nwhere formula_39 is the speed of sound in the fluid. Given this equation of state, Navier–Stokes and the continuity equation are invariant under the transformations\nGiven the solutions formula_32 and formula_33, we automatically have that\nformula_46 and formula_47 are also solutions.\n\nIn computer vision and biological vision, scaling transformations arise because of the perspective image mapping and because of objects having different physical size in the world. In these areas, scale invariance refers to local image descriptors or visual representations of the image data that remain invariant when the local scale in the image domain is changed. \nDetecting local maxima over scales of normalized derivative responses provides a general framework for obtaining scale invariance from image data.\nExamples of applications include blob detection, corner detection, ridge detection, and object recognition via the scale-invariant feature transform.\n\n\n"}
{"id": "1522286", "url": "https://en.wikipedia.org/wiki?curid=1522286", "title": "Schur's theorem", "text": "Schur's theorem\n\nIn discrete mathematics, Schur's theorem is any of several theorems of the mathematician Issai Schur. In differential geometry, Schur's theorem is a theorem of . In functional analysis, Schur's theorem is often called Schur's property, also due to Issai Schur.\n\nIn Ramsey theory, Schur's theorem states that for any partition of the positive integers into a finite number of parts, one of the parts contains three integers \"x\", \"y\", \"z\" with \n\nMoreover, for every positive integer \"c\", there exists a number \"S\"(\"c\"), called \"Schur's number\", such that for every partition of the integers \n\ninto \"c\" parts, one of the parts contains integers \"x\", \"y\", and \"z\" with \n\nFolkman's theorem generalizes Schur's theorem by stating that there exist arbitrarily large sets of integers all of whose nonempty sums belong to the same part.\n\nIn combinatorics, Schur's theorem tells the number of ways for expressing a given number as a (non-negative, integer) linear combination of a fixed set of relatively prime numbers. In particular, if formula_4 is a set of integers such that formula_5, the number of different tuples of non-negative integer numbers formula_6 such that formula_7 when formula_8 goes to infinity is:\n\nAs a result, for every set of relatively prime numbers formula_4 there exists a value of formula_8 such that every larger number is representable as a linear combination of formula_4 in at least one way. This consequence of the theorem can be recast in a familiar context considering the problem of changing an amount using a set of coins. If the denominations of the coins are relatively prime numbers (such as 2 and 5) then any sufficiently large amount can be changed using only these coins. (See Coin problem.)\n\nIn differential geometry, Schur's theorem compares the distance between the endpoints of a space curve formula_13 to the distance between the endpoints of a corresponding plane curve formula_14 of less curvature.\n\nSuppose formula_15 is a plane curve with curvature formula_16 which makes a convex curve when closed by the chord connecting its endpoints, and formula_17 is a curve of the same length with curvature formula_18. Let formula_19 denote the distance between the endpoints of formula_14 and formula_21 denote the distance between the endpoints of formula_13. If formula_23 then formula_24.\n\nSchur's theorem is usually stated for formula_25 curves, but John M. Sullivan has observed that Schur's theorem applies to curves of finite total curvature (the statement is slightly different).\n\nIn linear algebra Schur’s theorem is referred to as either the triangularization of a square matrix with complex entries, or of a square matrix with real entries and real eigenvalues.\n\nIn functional analysis and the study of Banach spaces, Schur's theorem, due to J. Schur, often refers to Schur's property, that for certain spaces, weak convergence implies convergence in the norm.\n\nIn number theory, Issai Schur showed in 1912 that for every nonconstant polynomial \"p\"(\"x\") with integer coefficients, if \"S\" is the set of all nonzero values formula_26, then the set of primes that divide some member of \"S\" is infinite.\n\n\n\n"}
{"id": "1276686", "url": "https://en.wikipedia.org/wiki?curid=1276686", "title": "Stephen Stigler", "text": "Stephen Stigler\n\nStephen Mack Stigler (born August 10, 1941) is Ernest DeWitt Burton Distinguished Service Professor at the Department of Statistics of the University of Chicago. He has authored several books on the history of statistics. \n\nStigler is also known for Stigler's law of eponymy which states that no scientific discovery is named after its original discoverer (whose first formulation he credits to sociologist Robert K. Merton). \n\nStigler was born in Minneapolis. He received his Ph.D. in 1967 from the University of California, Berkeley. His dissertation was on linear functions of order statistics, and his advisor was Lucien Le Cam. His research has focused on statistical theory of robust estimators and the history of statistics. He is also known for Stigler's law of eponymy.\n\nStigler taught at University of Wisconsin–Madison until 1979 when he joined the University of Chicago. In 2006 he was elected to membership of the American Philosophical Society, and is a past president (1994) of the Institute of Mathematical Statistics.\n\nHis father was the economist George Stigler, and he has recently written on Milton Friedman, who was a friend of his father.\n\n\n\n\n\n"}
{"id": "245560", "url": "https://en.wikipedia.org/wiki?curid=245560", "title": "Trigonometric integral", "text": "Trigonometric integral\n\nIn mathematics, the trigonometric integrals are a family of integrals involving trigonometric functions. A number of the basic trigonometric integrals are discussed at the list of integrals of trigonometric functions.\n\nThe different sine integral definitions are\n\nNote that the integrand is the function, and also the zeroth .\nSince is an even entire function (holomorphic over the entire complex plane), is entire, odd, and the integral in its definition can be taken along any path connecting the endpoints.\n\nBy definition, is the antiderivative of which is zero for , and is the antiderivative of which is zero for . Their difference is given by the Dirichlet integral,\n\nIn signal processing, the oscillations of the sine integral cause overshoot and ringing artifacts when using the sinc filter, and frequency domain ringing if using a truncated sinc filter as a low-pass filter.\n\nRelated is the Gibbs phenomenon: if the sine integral is considered as the convolution of the sinc function with the heaviside step function, this corresponds to truncating the Fourier series, which is the cause of the Gibbs phenomenon.\n\nThe different cosine integral definitions are\nwhere is the Euler–Mascheroni constant. Some texts use instead of .\n\nThe hyperbolic sine integral is defined as\n\nIt is related to the ordinary sine integral by\n\nThe hyperbolic cosine integral is\n\nwhere formula_11 is the Euler–Mascheroni constant.\n\nIt has the series expansion formula_12.\n\nTrigonometric integrals can be understood in terms of the so-called \"auxiliary functions\"\nUsing these functions, the trigonometric integrals may be re-expressed as \n\nThe spiral formed by parametric plot of is known as Nielsen's spiral. It is also referred to as the Euler spiral, the Cornu spiral, a clothoid, or as a linear-curvature polynomial spiral. \n\nThe spiral is also closely related to the Fresnel integrals. This spiral has applications in vision processing, road and track construction and other areas.\n\nVarious expansions can be used for evaluation of trigonometric integrals, depending on the range of the argument.\n\nThese series are asymptotic and divergent, although can be used for estimates and even precise evaluation at .\n\nThese series are convergent at any complex , although for , the series will converge slowly initially, requiring many terms for high precision.\n\nThe function\n\nis called the exponential integral. It is closely related to Si and Ci,\n\nAs each respective function is analytic except for the cut at negative values of the argument, the area of validity of the relation should be extended to (Outside this range, additional terms which are integer factors of appear in the expression.)\n\nCases of imaginary argument of the generalized integro-exponential function are\nwhich is the real part of\n\nSimilarly\n\nPadé approximants of the convergent Taylor series provide an efficient way to evaluate the functions for small arguments. The following formulae, given by Rowe et al (2015), are accurate to better than for ,\n\nformula_25\n\nThe integrals may be evaluated indirectly via auxiliary functions formula_26 and formula_27, which are defined by\n\nformula_28formula_29\n\nFor formula_30 the Padé rational functions given below approximate formula_26 and formula_27 with error less than 10:\n\nformula_33\n\n\n\n"}
{"id": "1597977", "url": "https://en.wikipedia.org/wiki?curid=1597977", "title": "Vaughan Pratt", "text": "Vaughan Pratt\n\nVaughan Pratt (born April 12, 1944) is a Professor Emeritus at Stanford University, who was an early pioneer in the field of computer science. Since 1969, Pratt has made several contributions to foundational areas such as search algorithms, sorting algorithms, and primality testing. More recently, his research has focused on formal modeling of concurrent systems and Chu spaces. A pattern of applying models from diverse areas of mathematics such as geometry, linear algebra, abstract algebra, and especially mathematical logic to computer science pervades his work.\n\nRaised in Australia and educated at Knox Grammar School, where he was dux in 1961, Pratt attended Sydney University where he completed his masters thesis in 1970, related to what is now known as natural language processing. He then went to the United States, where he completed a Ph.D. thesis at Stanford University in only 20 months under the supervision of advisor Donald Knuth. His thesis focused on analysis of the Shellsort sorting algorithm and sorting networks.\n\nPratt was an Assistant Professor at MIT (1972 to 1976) and then Associate Professor (1976 to 1982). In 1974, working in collaboration with Knuth and Morris, Pratt completed and formalized work he had begun in 1970 as a graduate student at Berkeley; the coauthored result was the Knuth–Morris–Pratt pattern matching algorithm. In 1976, he developed the system of dynamic logic, a modal logic of structured behavior.\n\nHe went on sabbatical from MIT to Stanford (1980 to 1981), and was appointed a full professor at Stanford in 1981.\n\nPratt directed the SUN workstation project at Stanford from 1980 to 1982. He contributed in various ways to the founding and early operation of Sun Microsystems, acting in the role of consultant for its first year, then, taking a leave of absence from Stanford for the next two years, becoming Director of Research, and finally resuming his role as a consultant to Sun and returning to Stanford in 1985.\n\nHe also designed the , which features four interleaved copies of the word \"sun\"; it is an ambigram.\n\nPratt became professor emeritus at Stanford in 2000.\n\nA number of well-known algorithms bear Pratt's name. Pratt certificates, short proofs of the primality of a number, demonstrated in a practical way that primality can be efficiently verified, placing the primality testing problem in the complexity class NP and providing the first strong evidence that the problem is not co-NP-complete.\nThe Knuth–Morris–Pratt algorithm, which Pratt designed in the early 1970s together with fellow Stanford professor Donald Knuth and independently from Morris, is still the most efficient general string searching algorithm known today. Along with Blum, Floyd, Rivest, and Tarjan, he described median of medians, the first worst-case optimal selection algorithm.\n\nPratt built some useful tools. In 1976, he wrote an MIT AI Lab working paper about CGOL, an alternative syntax for MACLISP that he had designed and implemented based on his paradigm for top down operator precedence parsing. His parser is sometimes called a \"Pratt parser\" and has been used in later systems, such as MACSYMA. Douglas Crockford also used it as the underlying parser for JSLint. Pratt also implemented a TECO-based text editor named \"DOC\", which was later renamed to \"ZED\".\n\nIn 1999, Pratt built the world's smallest (at the time) web server—it was the size of a matchbox.\n\nPratt was credited in a 1995 Byte magazine article for proposing that the Pentium FDIV bug might have worse consequences than either Intel or IBM was predicting at the time.\n\nToday Pratt has a wide influence. In addition to his Stanford professorship, he holds membership in at least seven professional organizations. He is a fellow of the Association for Computing Machinery and is on the editorial board of three major mathematics journals. He was also the founder, Chairman, and CTO of TIQIT Computers, Inc. for the ten years prior to when it closed its doors in 2010.\n\n"}
{"id": "297013", "url": "https://en.wikipedia.org/wiki?curid=297013", "title": "Word problem (mathematics education)", "text": "Word problem (mathematics education)\n\nIn science education, a word problem is a mathematical exercise where significant background information on the problem is presented as text rather than in mathematical notation. As word problems often involve a narrative of some sort, they are occasionally also referred to as story problems and may vary in the amount of language used.\n\nHere is a mathematical problem in mathematical notation:\n\nThe same problem might be presented in the form of a word problem as follows:\n\nThe answer to the word problem is that John is 15 years old, while the answer to the mathematical problem is that \"J\" equals 15 (and \"A\" equals 35).\n\nWord problems can be examined on three levels:\n\nLinguistic properties can include such variables as the number of words in the problem or the mean sentence length. The logico-mathematical properties can be classified in numerous ways, but one such scheme is to classify the quantities in the problem (assuming the word problem is primarily numerical) into known quantities (the values given in the text of the problem), wanted quantities (the values that need to be found) and auxiliary quantities (values that may need to be found as intermediate stages of the problem).\n\nThe most common types of word problems are distance problems, age problems, work problems, percentage problems, mixtures problems and numbers problems.\n\nWord problems commonly include mathematical modelling questions, where data and information about a certain system is given and a student is required to develop a model. For example:\n\n\nThese examples are not only intended to force the students into developing mathematical models on their own, but may also be used to promote mathematical interest and understanding by relating the subject to real-life situations. The relevance of these situations to the students is varying. The situation in the first example is well-known to most people and may be useful in helping primary school students to understand the concept of subtraction. The second example, however, does not necessarily have to be \"real-life\" to a high school student, who may find that it is easier to handle the following problem:\n\nWord problems are a common way to train and test understanding of underlying concepts within a descriptive problem, instead of solely testing the student's capability to perform algebraic manipulation or other \"mechanical\" skills.\n\nThe modern notation that enables mathematical ideas to be expressed symbolically was developed in Europe from the sixteenth century onwards. Prior to this, all mathematical problems and solutions were written out in words; the more complicated the problem, the more laborious and convoluted the verbal explanation.\n\nExamples of word problems can be found dating back to Babylonian times. Apart from a few procedure texts for finding things like square roots, most Old Babylonian problems are couched in a language of measurement of everyday objects and activities. Students had to find lengths of canals dug, weights of stones, lengths of broken reeds, areas of fields, numbers of bricks used in a construction, and so on.\n\nAncient Egyptian mathematics also has examples of word problems. The Rhind Mathematical Papyrus includes a problem that can be translated as:\n\nThere are seven houses; in each house there are seven cats; each cat kills seven mice; each mouse has eaten seven grains of barley; each grain would have produced seven hekat. What is the sum of all the enumerated things?\n\nIn more modern times the sometimes confusing and arbitrary nature of word problems has been the subject of satire. Gustave Flaubert wrote this nonsensical problem, now known as the Age of the captain:\n\nSince you are now studying geometry and trigonometry, I will give you a problem. A ship sails the ocean. It left Boston with a cargo of wool. It grosses 200 tons. It is bound for Le Havre. The mainmast is broken, the cabin boy is on deck, there are 12 passengers aboard, the wind is blowing East-North-East, the clock points to a quarter past three in the afternoon. It is the month of May. How old is the captain? \n\nWord problems have also been satirised in \"The Simpsons\", when a lengthy word problem (\"An express train traveling 60 miles per hour leaves Santa Fe bound for Phoenix, 520 miles away. At the same time, a local train traveling 30 miles an hour carrying 40 passengers leaves Phoenix bound for Santa Fe...\") trails off with a schoolboy character instead imagining that he is on the train.\n\n"}
