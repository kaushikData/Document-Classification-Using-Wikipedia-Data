{"id": "1734292", "url": "https://en.wikipedia.org/wiki?curid=1734292", "title": "Absolutely convex set", "text": "Absolutely convex set\n\nA set \"C\" in a real or complex vector space is said to be absolutely convex or disked if it is convex and balanced (circled), in which case it is called a disk.\n\nA set formula_1 is absolutely convex if and only if for any points formula_2 in formula_1 and any numbers formula_4 satisfying\nformula_5 the sum formula_6\nbelongs to formula_1.\n\nThe intersection of arbitrarily many absolutely convex sets is again absolutely convex; however, unions of absolutely convex sets need not be absolutely convex anymore.\n\nSince the intersection of any collection of absolutely convex sets is absolutely convex, one can define\nfor any subset \"A\" of a vector space its absolutely convex hull as the intersection of all absolutely convex sets containing \"A\", analogous to the well-known construction of the convex hull. \n\nMore explicitly, one can define the absolutely convex hull of the set \"A\" via\n\nformula_8\n\nwhere the λ are elements of the underlying field.\n\nThe absolutely convex hull of a bounded set in a topological vector space is again bounded, the absolutely convex hull of a closed set is again closed. The same does not hold for open sets.\n\n\n"}
{"id": "390418", "url": "https://en.wikipedia.org/wiki?curid=390418", "title": "Ad hoc polymorphism", "text": "Ad hoc polymorphism\n\nIn programming languages, ad hoc polymorphism is a kind of polymorphism in which polymorphic functions can be applied to arguments of different types, because a polymorphic function can denote a number of distinct and potentially heterogeneous implementations depending on the type of argument(s) to which it is applied. It is also known as function overloading or operator overloading. The term ad hoc in this context is not intended to be pejorative; it refers simply to the fact that this type of polymorphism is not a fundamental feature of the type system. This is in contrast to parametric polymorphism, in which polymorphic functions are written without mention of any specific type, and can thus apply a single abstract implementation to any number of types in a transparent way. This classification was introduced by Christopher Strachey in 1967.\n\nAd hoc polymorphism is a dispatch mechanism: control moving through one named function is dispatched to various other functions without having to specify the exact function being called. Overloading allows multiple functions taking different types to be defined with the same name; the compiler or interpreter automatically ensures that the right function is called. This way, functions appending lists of integers, lists of strings, lists of real numbers, and so on could be written, and all be called \"append\"—and the right \"append\" function would be called based on the type of lists being appended. This differs from parametric polymorphism, in which the function would need to be written \"generically\", to work with any kind of list. Using overloading, it is possible to have a function perform two completely different things based on the type of input passed to it; this is not possible with parametric polymorphism. Another way to look at overloading is that a routine is uniquely identified not by its name, but by the combination of its name and the number, order and types of its parameters.\n\nThis type of polymorphism is common in object-oriented programming languages, many of which allow operators to be overloaded in a manner similar to functions (see operator overloading). Some languages that are not dynamically typed and lack ad hoc polymorphism (including type classes) have longer function names such as codice_1, codice_2, etc. This can be seen as advantage (more descriptive) or a disadvantage (overly verbose) depending on one's point of view.\n\nAn advantage that is sometimes gained from overloading is the appearance of specialization, e.g., a function with the same name can be implemented in multiple different ways, each optimized for the particular data types that it operates on. This can provide a convenient interface for code that needs to be specialized to multiple situations for performance reasons. The downside is that the type system cannot guarantee the consistency of the different implementations.\n\nSince overloading is done at compile time, it is not a substitute for late binding as found in subtyping polymorphism.\n\nThe previous section notwithstanding, there are other ways in which \"ad hoc\" polymorphism can work out. Consider for example the Smalltalk language. In Smalltalk, the overloading is done at run time, as the methods (\"function implementation\") for each overloaded message (\"overloaded function\") are resolved when they are about to be executed. This happens at run time, after the program is compiled. Therefore, polymorphism is given by subtyping polymorphism as in other languages, and it is also extended in functionality by \"ad hoc\" polymorphism at run time.\n\nA closer look will also reveal that Smalltalk provides a slightly different variety of \"ad hoc\" polymorphism. Since Smalltalk has a late bound execution model, and since it provides objects the ability to handle messages that are not understood, it is possible to go ahead and implement functionality using polymorphism without explicitly overloading a particular message. This may not be generally recommended practice for everyday programming, but it can be quite useful when implementing proxies.\n\nAlso, while in general terms common class method and constructor overloading is not considered polymorphism, there are more uniform languages in which classes are regular objects. In Smalltalk, for instance, classes are regular objects. In turn, this means messages sent to classes can be overloaded, and it is also possible to create objects that behave like classes without their classes inheriting from the hierarchy of classes. These are effective techniques which can be used to take advantage of Smalltalk's powerful reflection capabilities. Similar arrangements are also possible in languages such as Self and Newspeak.\n\nImagine an operator codice_3 that may be used in the following ways:\n\nTo handle these six function calls, four different pieces of code are needed—or \"three\", if strings are considered to be lists of characters:\nThus, the name codice_3 actually refers to three or four completely different functions. This is an example of \"overloading\".\n"}
{"id": "58039179", "url": "https://en.wikipedia.org/wiki?curid=58039179", "title": "Aluthge transform", "text": "Aluthge transform\n\nIn mathematics and more precisely in functional analysis, the Aluthge transformation is an operation defined on the set of bounded operators of a Hilbert space. It was introduced by Ariyadasa Aluthge to study p-hyponormal linear operators.\n\nLet formula_1 be a Hilbert space and let formula_2 be the algebra of linear operators from formula_1 to formula_1. By the polar decomposition theorem, there exists an unique partial isometry formula_5 such that formula_6 and formula_7, where formula_8 is the square root of the operator formula_9. If formula_10 and formula_11 is its polar decomposition, the Aluthge transform of formula_12 is the operator formula_13 defined as:\n\nMore generally, for any real number formula_15, the formula_16-Aluthge transformation is defined as \n\nFor vectors formula_18, let formula_19 denote the operator defined as \n\nAn elementary calculation shows that if formula_21, then formula_22\n"}
{"id": "35131681", "url": "https://en.wikipedia.org/wiki?curid=35131681", "title": "Atiyah conjecture on configurations", "text": "Atiyah conjecture on configurations\n\nIn mathematics, the Atiyah conjecture on configurations is a conjecture introduced by stating that a certain \"n\" by \"n\" matrix depending on \"n\" points in R is always non-singular.\n\n\n"}
{"id": "207074", "url": "https://en.wikipedia.org/wiki?curid=207074", "title": "Beta distribution", "text": "Beta distribution\n\nformula_7 for \"α\", \"β\" >1\n\nany value in formula_8 for \"α\", \"β\" = 1\n\n0 for \"α\" = 1, \"β\" > 1\n\n1 for \"α\" > 1, \"β\" = 1\nIn probability theory and statistics, the beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by \"α\" and \"β\", that appear as exponents of the random variable and control the shape of the distribution.\n\nThe beta distribution has been applied to model the behavior of random variables limited to intervals of finite length in a wide variety of disciplines.\n\nIn Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions. For example, the beta distribution can be used in Bayesian analysis to describe initial knowledge concerning probability of success such as the probability that a space vehicle will successfully complete a specified mission. The beta distribution is a suitable model for the random behavior of percentages and proportions.\n\nThe usual formulation of the beta distribution is also known as the beta distribution of the first kind, whereas \"beta distribution of the second kind\" is an alternative name for the beta prime distribution.\n\nThe probability density function (pdf) of the beta distribution, for , and shape parameters \"α\", \"β\" > 0, is a power function of the variable \"x\" and of its reflection as follows:\n\nwhere Γ(\"z\") is the gamma function. The beta function, formula_18, is a normalization constant to ensure that the total probability is 1. In the above equations \"x\" is a realization—an observed value that actually occurred—of a random process \"X\".\n\nThis definition includes both ends and , which is consistent with definitions for other continuous distributions supported on a bounded interval which are special cases of the beta distribution, for example the arcsine distribution, and consistent with several authors, like N. L. Johnson and S. Kotz. However, the inclusion of and does not work for ; accordingly, several other authors, including W. Feller, choose to exclude the ends and , (so that the two ends are not actually part of the domain of the density function) and consider instead .\n\nSeveral authors, including N. L. Johnson and S. Kotz, use the symbols \"p\" and \"q\" (instead of \"α\" and \"β\") for the shape parameters of the beta distribution, reminiscent of the symbols traditionally used for the parameters of the Bernoulli distribution, because the beta distribution approaches the Bernoulli distribution in the limit when both shape parameters \"α\" and \"β\" approach the value of zero.\n\nIn the following, a random variable \"X\" beta-distributed with parameters \"α\" and \"β\" will be denoted by:\n\nOther notations for beta-distributed random variables used in the statistical literature are formula_20 and formula_21.\n\nThe cumulative distribution function is\n\nwhere formula_23 is the incomplete beta function and formula_24 is the regularized incomplete beta function.\n\nThe mode of a Beta distributed random variable \"X\" with \"α\", \"β\" > 1 is the most likely value of the distribution (corresponding to the peak in the PDF), and is given by the following expression:\n\nWhen both parameters are less than one (\"α\", \"β\" < 1), this is the anti-mode: the lowest point of the probability density curve.\n\nLetting \"α\" = \"β\", the expression for the mode simplifies to 1/2, showing that for \"α\" = \"β\" > 1 the mode (resp. anti-mode when ), is at the center of the distribution: it is symmetric in those cases. See Shapes section in this article for a full list of mode cases, for arbitrary values of \"α\" and \"β\". For several of these cases, the maximum value of the density function occurs at one or both ends. In some cases the (maximum) value of the density function occurring at the end is finite. For example, in the case of \"α\" = 2, \"β\" = 1 (or \"α\" = 1, \"β\" = 2), the density function becomes a right-triangle distribution which is finite at both ends. In several other cases there is a singularity at one end, where the value of the density function approaches infinity. For example, in the case \"α\" = \"β\" = 1/2, the Beta distribution simplifies to become the arcsine distribution. There is debate among mathematicians about some of these cases and whether the ends (\"x\" = 0, and \"x\" = 1) can be called \"modes\" or not.\n\nThe median of the beta distribution is the unique real number formula_26 for which the regularized incomplete beta function formula_27. There is no general closed-form expression for the median of the beta distribution for arbitrary values of \"α\" and \"β\". Closed-form expressions for particular values of the parameters \"α\" and \"β\" follow:\n\n\nThe following are the limits with one parameter finite (non-zero) and the other approaching these limits:\n\nA reasonable approximation of the value of the median of the beta distribution, for both α and β greater or equal to one, is given by the formula\n\nWhen α, β ≥ 1, the relative error (the absolute error divided by the median) in this approximation is less than 4% and for both α ≥ 2 and β ≥ 2 it is less than 1%. The absolute error divided by the difference between the mean and the mode is similarly small:\n\nThe expected value (mean) (\"μ\") of a Beta distribution random variable \"X\" with two parameters \"α\" and \"β\" is a function of only the ratio \"β\"/\"α\" of these parameters:\n\nLetting in the above expression one obtains , showing that for the mean is at the center of the distribution: it is symmetric. Also, the following limits can be obtained from the above expression:\n\nTherefore, for \"β\"/\"α\" → 0, or for \"α\"/\"β\" → ∞, the mean is located at the right end, . For these limit ratios, the beta distribution becomes a one-point degenerate distribution with a Dirac delta function spike at the right end, , with probability 1, and zero probability everywhere else. There is 100% probability (absolute certainty) concentrated at the right end, .\n\nSimilarly, for \"β\"/\"α\" → ∞, or for \"α\"/\"β\" → 0, the mean is located at the left end, . The beta distribution becomes a 1-point Degenerate distribution with a Dirac delta function spike at the left end, \"x\" = 0, with probability 1, and zero probability everywhere else. There is 100% probability (absolute certainty) concentrated at the left end, \"x\" = 0. Following are the limits with one parameter finite (non-zero) and the other approaching these limits:\n\nWhile for typical unimodal distributions (with centrally located modes, inflexion points at both sides of the mode, and longer tails) (with Beta(\"α\", \"β\") such that ) it is known that the sample mean (as an estimate of location) is not as robust as the sample median, the opposite is the case for uniform or \"U-shaped\" bimodal distributions (with Beta(\"α\", \"β\") such that ), with the modes located at the ends of the distribution. As Mosteller and Tukey remark ( p. 207) \"the average of the two extreme observations uses all the sample information. This illustrates how, for short-tailed distributions, the extreme observations should get more weight.\" By contrast, it follows that the median of \"U-shaped\" bimodal distributions with modes at the edge of the distribution (with Beta(\"α\", \"β\") such that ) is not robust, as the sample median drops the extreme sample observations from consideration. A practical application of this occurs for example for random walks, since the probability for the time of the last visit to the origin in a random walk is distributed as the arcsine distribution Beta(1/2, 1/2): the mean of a number of realizations of a random walk is a much more robust estimator than the median (which is an inappropriate sample measure estimate in this case).\n\nThe logarithm of the geometric mean \"G\" of a distribution with random variable \"X\" is the arithmetic mean of ln(\"X\"), or, equivalently, its expected value:\n\nFor a beta distribution, the expected value integral gives:\n\nwhere \"ψ\" is the digamma function.\n\nTherefore, the geometric mean of a beta distribution with shape parameters \"α\" and \"β\" is the exponential of the digamma functions of \"α\" and \"β\" as follows:\n\nWhile for a beta distribution with equal shape parameters α = β, it follows that skewness = 0 and mode = mean = median = 1/2, the geometric mean is less than 1/2: . The reason for this is that the logarithmic transformation strongly weights the values of \"X\" close to zero, as ln(\"X\") strongly tends towards negative infinity as \"X\" approaches zero, while ln(\"X\") flattens towards zero as .\n\nAlong a line , the following limits apply:\n\nFollowing are the limits with one parameter finite (non-zero) and the other approaching these limits:\n\nThe accompanying plot shows the difference between the mean and the geometric mean for shape parameters α and β from zero to 2. Besides the fact that the difference between them approaches zero as α and β approach infinity and that the difference becomes large for values of α and β approaching zero, one can observe an evident asymmetry of the geometric mean with respect to the shape parameters α and β. The difference between the geometric mean and the mean is larger for small values of α in relation to β than when exchanging the magnitudes of β and α.\n\nN. L.Johnson and S. Kotz suggest the logarithmic approximation to the digamma function \"ψ\"(\"α\") ≈ ln(\"α\" − 1/2) which results in the following approximation to the geometric mean:\n\nNumerical values for the relative error in this approximation follow: []; []; []; []; []; []; []; [].\n\nSimilarly, one can calculate the value of shape parameters required for the geometric mean to equal 1/2. Given the value of the parameter \"β\", what would be the value of the other parameter, \"α\", required for the geometric mean to equal 1/2?. The answer is that (for ), the value of \"α\" required tends towards as . For example, all these couples have the same geometric mean of 1/2: [], [], [], [], [], [], [].\n\nThe fundamental property of the geometric mean, which can be proven to be false for any other mean, is\n\nThis makes the geometric mean the only correct mean when averaging \"normalized\" results, that is results that are presented as ratios to reference values. This is relevant because the beta distribution is a suitable model for the random behavior of percentages and it is particularly suitable to the statistical modelling of proportions. The geometric mean plays a central role in maximum likelihood estimation, see section \"Parameter estimation, maximum likelihood.\" Actually, when performing maximum likelihood estimation, besides the geometric mean \"G\" based on the random variable X, also another geometric mean appears naturally: the geometric mean based on the linear transformation ––, the mirror-image of \"X\", denoted by \"G\":\n\nAlong a line , the following limits apply:\n\nFollowing are the limits with one parameter finite (non-zero) and the other approaching these limits:\n\nIt has the following approximate value:\n\nAlthough both \"G\" and \"G\" are asymmetric, in the case that both shape parameters are equal , the geometric means are equal: \"G\" = \"G\". This equality follows from the following symmetry displayed between both geometric means:\n\nThe inverse of the harmonic mean (\"H\") of a distribution with random variable \"X\" is the arithmetic mean of 1/\"X\", or, equivalently, its expected value. Therefore, the harmonic mean (\"H\") of a beta distribution with shape parameters \"α\" and \"β\" is:\n\nThe harmonic mean (\"H\") of a Beta distribution with \"α\" < 1 is undefined, because its defining expression is not bounded in [0, 1] for shape parameter \"α\" less than unity.\n\nLetting \"α\" = \"β\" in the above expression one obtains\n\nshowing that for \"α\" = \"β\" the harmonic mean ranges from 0, for \"α\" = \"β\" = 1, to 1/2, for \"α\" = \"β\" → ∞.\n\nFollowing are the limits with one parameter finite (non-zero) and the other approaching these limits:\n\nThe harmonic mean plays a role in maximum likelihood estimation for the four parameter case, in addition to the geometric mean. Actually, when performing maximum likelihood estimation for the four parameter case, besides the harmonic mean \"H\" based on the random variable \"X\", also another harmonic mean appears naturally: the harmonic mean based on the linear transformation (1 − \"X\"), the mirror-image of \"X\", denoted by \"H\":\n\nThe harmonic mean (\"H\") of a Beta distribution with \"β\" < 1 is undefined, because its defining expression is not bounded in [0, 1] for shape parameter \"β\" less than unity.\n\nLetting \"α\" = \"β\" in the above expression one obtains\n\nshowing that for \"α\" = \"β\" the harmonic mean ranges from 0, for \"α\" = \"β\" = 1, to 1/2, for \"α\" = \"β\" → ∞.\n\nFollowing are the limits with one parameter finite (non-zero) and the other approaching these limits:\n\nAlthough both \"H\" and \"H\" are asymmetric, in the case that both shape parameters are equal \"α\" = \"β\", the harmonic means are equal: \"H\" = \"H\". This equality follows from the following symmetry displayed between both harmonic means:\n\nThe variance (the second moment centered on the mean) of a Beta distribution random variable \"X\" with parameters α and β is:\n\nLetting α = β in the above expression one obtains\n\nshowing that for \"α\" = \"β\" the variance decreases monotonically as increases. Setting in this expression, one finds the maximum variance var(\"X\") = 1/4 which only occurs approaching the limit, at .\n\nThe beta distribution may also be parametrized in terms of its mean \"μ\" and sample size () (see section below titled \"Mean and sample size\"):\n\nUsing this parametrization, one can express the variance in terms of the mean \"μ\" and the sample size \"ν\" as follows:\n\nSince , it must follow that .\n\nFor a symmetric distribution, the mean is at the middle of the distribution, , and therefore:\n\nAlso, the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions:\n\nThe logarithm of the geometric variance, ln(var), of a distribution with random variable \"X\" is the second moment of the logarithm of \"X\" centered on the geometric mean of \"X\", ln(\"G\"):\n\nand therefore, the geometric variance is:\n\nIn the Fisher information matrix, and the curvature of the log likelihood function, the logarithm of the geometric variance of the reflected variable 1 − \"X\" and the logarithm of the geometric covariance between \"X\" and 1 − \"X\" appear:\n\nFor a beta distribution, higher order logarithmic moments can be derived by using the representation of a beta distribution as a proportion of two Gamma distributions and differentiating through the integral. They can be expressed in terms of higher order poly-gamma functions. See the section titled \"Other moments, Moments of transformed random variables, Moments of logarithmically transformed random variables\". The variance of the logarithmic variables and covariance of ln \"X\" and ln(1−\"X\") are:\n\nwhere the trigamma function, denoted ψ(α), is the second of the polygamma functions, and is defined as the derivative of the digamma function:\n\nTherefore,\n\nThe accompanying plots show the log geometric variances and log geometric covariance versus the shape parameters \"α\" and \"β\". The plots show that the log geometric variances and log geometric covariance are close to zero for shape parameters α and β greater than 2, and that the log geometric variances rapidly rise in value for shape parameter values \"α\" and \"β\" less than unity. The log geometric variances are positive for all values of the shape parameters. The log geometric covariance is negative for all values of the shape parameters, and it reaches large negative values for \"α\" and \"β\" less than unity.\n\nFollowing are the limits with one parameter finite (non-zero) and the other approaching these limits:\n\nLimits with two parameters varying:\n\nAlthough both ln(var) and ln(var) are asymmetric, when the shape parameters are equal, α = β, one has: ln(var) = ln(var). This equality follows from the following symmetry displayed between both log geometric variances:\n\nThe log geometric covariance is symmetric:\n\nThe mean absolute deviation around the mean for the beta distribution with shape parameters α and β is:\n\nThe mean absolute deviation around the mean is a more robust estimator of statistical dispersion than the standard deviation for beta distributions with tails and inflection points at each side of the mode, Beta(\"α\", \"β\") distributions with \"α\",\"β\" > 2, as it depends on the linear (absolute) deviations rather than the square deviations from the mean. Therefore, the effect of very large deviations from the mean are not as overly weighted.\n\nUsing Stirling's approximation to the Gamma function, N.L.Johnson and S.Kotz derived the following approximation for values of the shape parameters greater than unity (the relative error for this approximation is only −3.5% for \"α\" = \"β\" = 1, and it decreases to zero as \"α\" → ∞, \"β\" → ∞):\n\nAt the limit α → ∞, β → ∞, the ratio of the mean absolute deviation to the standard deviation (for the beta distribution) becomes equal to the ratio of the same measures for the normal distribution: formula_76. For α = β = 1 this ratio equals formula_77, so that from α = β = 1 to α, β → ∞ the ratio decreases by 8.5%. For α = β = 0 the standard deviation is exactly equal to the mean absolute deviation around the mean. Therefore, this ratio decreases by 15% from α = β = 0 to α = β = 1, and by 25% from α = β = 0 to α, β → ∞ . However, for skewed beta distributions such that α → 0 or β → 0, the ratio of the standard deviation to the mean absolute deviation approaches infinity (although each of them, individually, approaches zero) because the mean absolute deviation approaches zero faster than the standard deviation.\n\nUsing the parametrization in terms of mean μ and sample size ν = α + β > 0:\n\none can express the mean absolute deviation around the mean in terms of the mean μ and the sample size ν as follows:\n\nFor a symmetric distribution, the mean is at the middle of the distribution, μ = 1/2, and therefore:\n\nAlso, the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions:\n\nThe mean absolute difference for the Beta distribution is:\n\nThe Gini coefficient for the Beta distribution is half of the relative mean absolute difference:\n\nThe skewness (the third moment centered on the mean, normalized by the 3/2 power of the variance) of the beta distribution is\n\nLetting α = β in the above expression one obtains γ = 0, showing once again that for α = β the distribution is symmetric and hence the skewness is zero. Positive skew (right-tailed) for α < β, negative skew (left-tailed) for α > β.\n\nUsing the parametrization in terms of mean μ and sample size ν = α + β:\n\none can express the skewness in terms of the mean μ and the sample size ν as follows:\n\nThe skewness can also be expressed just in terms of the variance \"var\" and the mean μ as follows:\n\nThe accompanying plot of skewness as a function of variance and mean shows that maximum variance (1/4) is coupled with zero skewness and the symmetry condition (μ = 1/2), and that maximum skewness (positive or negative infinity) occurs when the mean is located at one end or the other, so that the \"mass\" of the probability distribution is concentrated at the ends (minimum variance).\n\nThe following expression for the square of the skewness, in terms of the sample size ν = α + β and the variance \"var\", is useful for the method of moments estimation of four parameters:\n\nThis expression correctly gives a skewness of zero for α = β, since in that case (see section titled \"Variance\"): formula_88.\n\nFor the symmetric case (α = β), skewness = 0 over the whole range, and the following limits apply:\n\nFor the asymmetric cases (α ≠ β) the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions:\n\nThe beta distribution has been applied in acoustic analysis to assess damage to gears, as the kurtosis of the beta distribution has been reported to be a good indicator of the condition of a gear. Kurtosis has also been used to distinguish the seismic signal generated by a person's footsteps from other signals. As persons or other targets moving on the ground generate continuous signals in the form of seismic waves, one can separate different targets based on the seismic waves they generate. Kurtosis is sensitive to impulsive signals, so it's much more sensitive to the signal generated by human footsteps than other signals generated by vehicles, winds, noise, etc. Unfortunately, the notation for kurtosis has not been standardized. Kenney and Keeping use the symbol γ for the excess kurtosis, but Abramowitz and Stegun use different terminology. To prevent confusion between kurtosis (the fourth moment centered on the mean, normalized by the square of the variance) and excess kurtosis, when using symbols, they will be spelled out as follows:\n\nLetting α = β in the above expression one obtains\n\nTherefore, for symmetric beta distributions, the excess kurtosis is negative, increasing from a minimum value of −2 at the limit as {α = β} → 0, and approaching a maximum value of zero as {α = β} → ∞. The value of −2 is the minimum value of excess kurtosis that any distribution (not just beta distributions, but any distribution of any possible kind) can ever achieve. This minimum value is reached when all the probability density is entirely concentrated at each end \"x\" = 0 and \"x\" = 1, with nothing in between: a 2-point Bernoulli distribution with equal probability 1/2 at each end (a coin toss: see section below \"Kurtosis bounded by the square of the skewness\" for further discussion). The description of kurtosis as a measure of the \"potential outliers\" (or \"potential rare, extreme values\") of the probability distribution, is correct for all distributions including the beta distribution. When rare, extreme values can occur in the beta distribution, the higher its kurtosis; otherwise, the kurtosis is lower. For α ≠ β, skewed beta distributions, the excess kurtosis can reach unlimited positive values (particularly for α → 0 for finite β, or for β → 0 for finite α) because the side away from the mode will produce occasional extreme values. Minimum kurtosis takes place when the mass density is concentrated equally at each end (and therefore the mean is at the center), and there is no probability mass density in between the ends.\n\nUsing the parametrization in terms of mean μ and sample size ν = α + β:\n\none can express the excess kurtosis in terms of the mean μ and the sample size ν as follows:\n\nThe excess kurtosis can also be expressed in terms of just the following two parameters: the variance \"var\", and the sample size ν as follows:\n\nand, in terms of the variance \"var\" and the mean μ as follows:\n\nThe plot of excess kurtosis as a function of the variance and the mean shows that the minimum value of the excess kurtosis (−2, which is the minimum possible value for excess kurtosis for any distribution) is intimately coupled with the maximum value of variance (1/4) and the symmetry condition: the mean occurring at the midpoint (μ = 1/2). This occurs for the symmetric case of α = β = 0, with zero skewness. At the limit, this is the 2 point Bernoulli distribution with equal probability 1/2 at each Dirac delta function end \"x\" = 0 and \"x\" = 1 and zero probability everywhere else. (A coin toss: one face of the coin being \"x\" = 0 and the other face being \"x\" = 1.) Variance is maximum because the distribution is bimodal with nothing in between the two modes (spikes) at each end. Excess kurtosis is minimum: the probability density \"mass\" is zero at the mean and it is concentrated at the two peaks at each end. Excess kurtosis reaches the minimum possible value (for any distribution) when the probability density function has two spikes at each end: it is bi-\"peaky\" with nothing in between them.\n\nOn the other hand, the plot shows that for extreme skewed cases, where the mean is located near one or the other end (μ = 0 or μ = 1), the variance is close to zero, and the excess kurtosis rapidly approaches infinity when the mean of the distribution approaches either end.\n\nAlternatively, the excess kurtosis can also be expressed in terms of just the following two parameters: the square of the skewness, and the sample size ν as follows:\n\nFrom this last expression, one can obtain the same limits published practically a century ago by Karl Pearson in his paper, for the beta distribution (see section below titled \"Kurtosis bounded by the square of the skewness\"). Setting α + β= ν = 0 in the above expression, one obtains Pearson's lower boundary (values for the skewness and excess kurtosis below the boundary (excess kurtosis + 2 − skewness = 0) cannot occur for any distribution, and hence Karl Pearson appropriately called the region below this boundary the \"impossible region\"). The limit of α + β = ν → ∞ determines Pearson's upper boundary.\n\ntherefore:\n\nValues of ν = α + β such that ν ranges from zero to infinity, 0 < ν < ∞, span the whole region of the beta distribution in the plane of excess kurtosis versus squared skewness.\n\nFor the symmetric case (α = β), the following limits apply:\n\nFor the unsymmetric cases (α ≠ β) the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions:\n\nThe characteristic function is the Fourier transform of the probability density function. The characteristic function of the beta distribution is Kummer's confluent hypergeometric function (of the first kind):\n\nwhere\n\nis the rising factorial, also called the \"Pochhammer symbol\". The value of the characteristic function for \"t\" = 0, is one:\n\nAlso, the real and imaginary parts of the characteristic function enjoy the following symmetries with respect to the origin of variable \"t\":\n\nThe symmetric case α = β simplifies the characteristic function of the beta distribution to a Bessel function, since in the special case α + β = 2α the confluent hypergeometric function (of the first kind) reduces to a Bessel function (the modified Bessel function of the first kind formula_107 ) using Kummer's second transformation as follows:\n\nIn the accompanying plots, the real part (Re) of the characteristic function of the beta distribution is displayed for symmetric (α = β) and skewed (α ≠ β) cases.\n\nIt also follows that the moment generating function is\n\nIn particular \"M\"(\"α\"; \"β\"; 0) = 1.\n\nUsing the moment generating function, the \"k\"-th raw moment is given by the factor\n\nmultiplying the (exponential series) term formula_111 in the series of the moment generating function\n\nwhere (\"x\") is a Pochhammer symbol representing rising factorial. It can also be written in a recursive form as\n\nSince the moment generating function formula_114 has a positive radius of convergence, the beta distribution is determined by its moments.\n\nOne can also show the following expectations for a transformed random variable, where the random variable \"X\" is Beta-distributed with parameters α and β: \"X\" ~ Beta(α, β). The expected value of the variable 1 − \"X\" is the mirror-symmetry of the expected value based on \"X\":\n\nDue to the mirror-symmetry of the probability density function of the beta distribution, the variances based on variables \"X\" and 1 − \"X\" are identical, and the covariance on \"X\"(1 − \"X\" is the negative of the variance:\n\nThese are the expected values for inverted variables, (these are related to the harmonic means, see section titled \"Harmonic mean\"):\n\nThe following transformation by dividing the variable \"X\" by its mirror-image \"X\"/(1 − \"X\") results in the expected value of the \"inverted beta distribution\" or beta prime distribution (also known as beta distribution of the second kind or Pearson's Type VI):\n\nVariances of these transformed variables can be obtained by integration, as the expected values of the second moments centered on the corresponding variables:\n\nThe following variance of the variable \"X\" divided by its mirror-image (\"X\"/(1−\"X\") results in the variance of the \"inverted beta distribution\" or beta prime distribution (also known as beta distribution of the second kind or Pearson's Type VI):\n\nThe covariances are:\n\nThese expectations and variances appear in the four-parameter Fisher information matrix (section titled \"Fisher information,\" \"four parameters\")\n\nExpected values for logarithmic transformations (useful for maximum likelihood estimates, see section titled \"Parameter estimation, Maximum likelihood\" below) are discussed in this section. The following logarithmic linear transformations are related to the geometric means \"G\" and \"G\" (see section titled \"Geometric mean\"):\n\nWhere the digamma function ψ(α) is defined as the logarithmic derivative of the gamma function:\n\nLogit transformations are interesting, as they usually transform various shapes (including J-shapes) into (usually skewed) bell-shaped densities over the logit variable, and they may remove the end singularities over the original variable:\n\nJohnson considered the distribution of the logit - transformed variable ln(\"X\"/1−\"X\"), including its moment generating function and approximations for large values of the shape parameters. This transformation extends the finite support [0, 1] based on the original variable \"X\" to infinite support in both directions of the real line (−∞, +∞).\n\nHigher order logarithmic moments can be derived by using the representation of a beta distribution as a proportion of two Gamma distributions and differentiating through the integral. They can be expressed in terms of higher order poly-gamma functions as follows:\n\ntherefore the variance of the logarithmic variables and covariance of ln(\"X\") and ln(1−\"X\") are:\n\nwhere the trigamma function, denoted ψ(α), is the second of the polygamma functions, and is defined as the derivative of the digamma function:\n\nThe variances and covariance of the logarithmically transformed variables \"X\" and (1−\"X\") are different, in general, because the logarithmic transformation destroys the mirror-symmetry of the original variables \"X\" and (1−\"X\"), as the logarithm approaches negative infinity for the variable approaching zero.\n\nThese logarithmic variances and covariance are the elements of the Fisher information matrix for the beta distribution. They are also a measure of the curvature of the log likelihood function (see section on Maximum likelihood estimation).\n\nThe variances of the log inverse variables are identical to the variances of the log variables:\n\nIt also follows that the variances of the logit transformed variables are:\n\nGiven a beta distributed random variable, \"X\" ~ Beta(\"α\", \"β\"), the differential entropy of \"X\" is(measured in nats), the expected value of the negative of the logarithm of the probability density function:\n\nwhere \"f\"(\"x\"; \"α\", \"β\") is the probability density function of the beta distribution:\n\nThe digamma function \"ψ\" appears in the formula for the differential entropy as a consequence of Euler's integral formula for the harmonic numbers which follows from the integral:\n\nThe differential entropy of the beta distribution is negative for all values of \"α\" and \"β\" greater than zero, except at \"α\" = \"β\" = 1 (for which values the beta distribution is the same as the uniform distribution), where the differential entropy reaches its maximum value of zero. It is to be expected that the maximum entropy should take place when the beta distribution becomes equal to the uniform distribution, since uncertainty is maximal when all possible events are equiprobable.\n\nFor \"α\" or \"β\" approaching zero, the differential entropy approaches its minimum value of negative infinity. For (either or both) \"α\" or \"β\" approaching zero, there is a maximum amount of order: all the probability density is concentrated at the ends, and there is zero probability density at points located between the ends. Similarly for (either or both) \"α\" or \"β\" approaching infinity, the differential entropy approaches its minimum value of negative infinity, and a maximum amount of order. If either \"α\" or \"β\" approaches infinity (and the other is finite) all the probability density is concentrated at an end, and the probability density is zero everywhere else. If both shape parameters are equal (the symmetric case), \"α\" = \"β\", and they approach infinity simultaneously, the probability density becomes a spike (Dirac delta function) concentrated at the middle \"x\" = 1/2, and hence there is 100% probability at the middle \"x\" = 1/2 and zero probability everywhere else.\n\nThe (continuous case) differential entropy was introduced by Shannon in his original paper (where he named it the \"entropy of a continuous distribution\"), as the concluding part of the same paper where he defined the discrete entropy. It is known since then that the differential entropy may differ from the infinitesimal limit of the discrete entropy by an infinite offset, therefore the differential entropy can be negative (as it is for the beta distribution). What really matters is the relative value of entropy.\n\nGiven two beta distributed random variables, \"X\" ~ Beta(\"α\", \"β\") and \"X\" ~ Beta(\"α\"′, \"β\"′), the cross entropy is (measured in nats)\n\nThe cross entropy has been used as an error metric to measure the distance between two hypotheses. Its absolute value is minimum when the two distributions are identical. It is the information measure most closely related to the log maximum likelihood (see section on \"Parameter estimation. Maximum likelihood estimation\")).\n\nThe relative entropy, or Kullback–Leibler divergence \"D\"(\"X\", \"X\"), is a measure of the inefficiency of assuming that the distribution is \"X\" ~ Beta(\"α\"′, \"β\"′) when the distribution is really \"X\" ~ Beta(\"α\", \"β\"). It is defined as follows (measured in nats).\n\nThe relative entropy, or Kullback–Leibler divergence, is always non-negative. A few numerical examples follow:\n\n\nThe Kullback–Leibler divergence is not symmetric \"D\"(\"X\", \"X\") ≠ \"D\"(\"X\", \"X\") for the case in which the individual beta distributions Beta(1, 1) and Beta(3, 3) are symmetric, but have different entropies \"h\"(\"X\") ≠ \"h\"(\"X\"). The value of the Kullback divergence depends on the direction traveled: whether going from a higher (differential) entropy to a lower (differential) entropy or the other way around. In the numerical example above, the Kullback divergence measures the inefficiency of assuming that the distribution is (bell-shaped) Beta(3, 3), rather than (uniform) Beta(1, 1). The \"h\" entropy of Beta(1, 1) is higher than the \"h\" entropy of Beta(3, 3) because the uniform distribution Beta(1, 1) has a maximum amount of disorder. The Kullback divergence is more than two times higher (0.598803 instead of 0.267864) when measured in the direction of decreasing entropy: the direction that assumes that the (uniform) Beta(1, 1) distribution is (bell-shaped) Beta(3, 3) rather than the other way around. In this restricted sense, the Kullback divergence is consistent with the second law of thermodynamics.\n\nThe Kullback–Leibler divergence is symmetric \"D\"(\"X\", \"X\") = \"D\"(\"X\", \"X\") for the skewed cases Beta(3, 0.5) and Beta(0.5, 3) that have equal differential entropy \"h\"(\"X\") = \"h\"(\"X\").\n\nThe symmetry condition:\n\nfollows from the above definitions and the mirror-symmetry \"f\"(\"x\"; \"α\", \"β\") = \"f\"(1−\"x\"; \"α\", \"β\") enjoyed by the beta distribution.\n\nIf 1 < α < β then mode ≤ median ≤ mean. Expressing the mode (only for α, β > 1), and the mean in terms of α and β:\n\nIf 1 < β < α then the order of the inequalities are reversed. For α, β > 1 the absolute distance between the mean and the median is less than 5% of the distance between the maximum and minimum values of \"x\". On the other hand, the absolute distance between the mean and the mode can reach 50% of the distance between the maximum and minimum values of \"x\", for the (pathological) case of α = 1 and β = 1 (for which values the beta distribution approaches the uniform distribution and the differential entropy approaches its maximum value, and hence maximum \"disorder\").\n\nFor example, for α = 1.0001 and β = 1.00000001:\n\nIt is known from the inequality of arithmetic and geometric means that the geometric mean is lower than the mean. Similarly, the harmonic mean is lower than the geometric mean. The accompanying plot shows that for α = β, both the mean and the median are exactly equal to 1/2, regardless of the value of α = β, and the mode is also equal to 1/2 for α = β > 1, however the geometric and harmonic means are lower than 1/2 and they only approach this value asymptotically as α = β → ∞.\n\nAs remarked by Feller, in the Pearson system the beta probability density appears as type I (any difference between the beta distribution and Pearson's type I distribution is only superficial and it makes no difference for the following discussion regarding the relationship between kurtosis and skewness). Karl Pearson showed, in Plate 1 of his paper published in 1916, a graph with the kurtosis as the vertical axis (ordinate) and the square of the skewness as the horizontal axis (abscissa), in which a number of distributions were displayed. The region occupied by the beta distribution is bounded by the following two lines in the (skewness,kurtosis) plane, or the (skewness,excess kurtosis) plane:\n\nor, equivalently,\n\n(At a time when there were no powerful digital computers), Karl Pearson accurately computed further boundaries, for example, separating the \"U-shaped\" from the \"J-shaped\" distributions. The lower boundary line (excess kurtosis + 2 − skewness = 0) is produced by skewed \"U-shaped\" beta distributions with both values of shape parameters α and β close to zero. The upper boundary line (excess kurtosis − (3/2) skewness = 0) is produced by extremely skewed distributions with very large values of one of the parameters and very small values of the other parameter. Karl Pearson showed that this upper boundary line (excess kurtosis − (3/2) skewness = 0) is also the intersection with Pearson's distribution III, which has unlimited support in one direction (towards positive infinity), and can be bell-shaped or J-shaped. His son, Egon Pearson, showed that the region (in the kurtosis/squared-skewness plane) occupied by the beta distribution (equivalently, Pearson's distribution I) as it approaches this boundary (excess kurtosis − (3/2) skewness = 0) is shared with the noncentral chi-squared distribution. Karl Pearson (Pearson 1895, pp. 357, 360, 373–376) also showed that the gamma distribution is a Pearson type III distribution. Hence this boundary line for Pearson's type III distribution is known as the gamma line. (This can be shown from the fact that the excess kurtosis of the gamma distribution is 6/\"k\" and the square of the skewness is 4/\"k\", hence (excess kurtosis − (3/2) skewness = 0) is identically satisfied by the gamma distribution regardless of the value of the parameter \"k\"). Pearson later noted that the chi-squared distribution is a special case of Pearson's type III and also shares this boundary line (as it is apparent from the fact that for the chi-squared distribution the excess kurtosis is 12/\"k\" and the square of the skewness is 8/\"k\", hence (excess kurtosis − (3/2) skewness = 0) is identically satisfied regardless of the value of the parameter \"k\"). This is to be expected, since the chi-squared distribution \"X\" ~ χ(\"k\") is a special case of the gamma distribution, with parametrization X ~ Γ(k/2, 1/2) where k is a positive integer that specifies the \"number of degrees of freedom\" of the chi-squared distribution.\n\nAn example of a beta distribution near the upper boundary (excess kurtosis − (3/2) skewness = 0) is given by α = 0.1, β = 1000, for which the ratio (excess kurtosis)/(skewness) = 1.49835 approaches the upper limit of 1.5 from below. An example of a beta distribution near the lower boundary (excess kurtosis + 2 − skewness = 0) is given by α= 0.0001, β = 0.1, for which values the expression (excess kurtosis + 2)/(skewness) = 1.01621 approaches the lower limit of 1 from above. In the infinitesimal limit for both α and β approaching zero symmetrically, the excess kurtosis reaches its minimum value at −2. This minimum value occurs at the point at which the lower boundary line intersects the vertical axis (ordinate). (However, in Pearson's original chart, the ordinate is kurtosis, instead of excess kurtosis, and it increases downwards rather than upwards).\n\nValues for the skewness and excess kurtosis below the lower boundary (excess kurtosis + 2 − skewness = 0) cannot occur for any distribution, and hence Karl Pearson appropriately called the region below this boundary the \"impossible region.\" The boundary for this \"impossible region\" is determined by (symmetric or skewed) bimodal \"U\"-shaped distributions for which parameters α and β approach zero and hence all the probability density is concentrated at the ends: \"x\" = 0, 1 with practically nothing in between them. Since for α ≈ β ≈ 0 the probability density is concentrated at the two ends \"x\" = 0 and \"x\" = 1, this \"impossible boundary\" is determined by a 2-point distribution: the probability can only take 2 values (Bernoulli distribution), one value with probability p and the other with probability \"q\" = 1−\"p\". For cases approaching this limit boundary with symmetry α = β, skewness ≈ 0, excess kurtosis ≈ −2 (this is the lowest excess kurtosis possible for any distribution), and the probabilities are \"p\" ≈ \"q\" ≈ 1/2. For cases approaching this limit boundary with skewness, excess kurtosis ≈ −2 + skewness, and the probability density is concentrated more at one end than the other end (with practically nothing in between), with probabilities formula_141 at the left end \"x\" = 0 and formula_142 at the right end \"x\" = 1.\n\nAll statements are conditional on α, β > 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor certain values of the shape parameters α and β, the probability density function has inflection points, at which the curvature changes sign. The position of these inflection points can be useful as a measure of the dispersion or spread of the distribution.\n\nDefining the following quantity:\n\nPoints of inflection occur, depending on the value of the shape parameters α and β, as follows:\n\n\n\n\n\n\n\n\nThere are no inflection points in the remaining (symmetric and skewed) regions: U-shaped: (α, β < 1) upside-down-U-shaped: (1 < α < 2, 1 < β < 2), reverse-J-shaped (α < 1, β > 2) or J-shaped: (α > 2, β < 1)\n\nThe accompanying plots show the inflection point locations (shown vertically, ranging from 0 to 1) versus α and β (the horizontal axes ranging from 0 to 5). There are large cuts at surfaces intersecting the lines α = 1, β = 1, α = 2, and β = 2 because at these values the beta distribution change from 2 modes, to 1 mode to no mode.\n\nThe beta density function can take a wide variety of different shapes depending on the values of the two parameters \"α\" and \"β\". The ability of the beta distribution to take this great diversity of shapes (using only two parameters) is partly responsible for finding wide application for modeling actual measurements:\n\n\nThe density function is skewed. An interchange of parameter values yields the mirror image (the reverse) of the initial curve, some more specific cases:\n\nTwo unknown parameters (formula_186 of a beta distribution supported in the [0,1] interval) can be estimated, using the method of moments, with the first two moments (sample mean and sample variance) as follows. Let:\n\nbe the sample mean estimate and\n\nbe the sample variance estimate. The method-of-moments estimates of the parameters are\n\nWhen the distribution is required over a known interval other than [0, 1] with random variable \"X\", say [\"a\", \"c\"] with random variable \"Y\", then replace formula_193 with formula_194 and formula_195 with formula_196 in the above couple of equations for the shape parameters (see the \"Alternative parametrizations, four parameters\" section below)., where:\n\nAll four parameters (formula_199 of a beta distribution supported in the [\"a\", \"c\"] interval -see section \"Alternative parametrizations, Four parameters\"-) can be estimated, using the method of moments developed by Karl Pearson, by equating sample and population values of the first four central moments (mean, variance, skewness and excess kurtosis). The excess kurtosis was expressed in terms of the square of the skewness, and the sample size ν = α + β, (see previous section \"Kurtosis\") as follows:\n\nOne can use this equation to solve for the sample size ν= α + β in terms of the square of the skewness and the excess kurtosis as follows:\n\nThis is the ratio (multiplied by a factor of 3) between the previously derived limit boundaries for the beta distribution in a space (as originally done by Karl Pearson) defined with coordinates of the square of the skewness in one axis and the excess kurtosis in the other axis (see previous section titled \"Kurtosis bounded by the square of the skewness\"):\n\nThe case of zero skewness, can be immediately solved because for zero skewness, α = β and hence ν = 2α = 2β, therefore α = β = ν/2\n\n(Excess kurtosis is negative for the beta distribution with zero skewness, ranging from -2 to 0, so that formula_205 -and therefore the sample shape parameters- is positive, ranging from zero when the shape parameters approach zero and the excess kurtosis approaches -2, to infinity when the shape parameters approach infinity and the excess kurtosis approaches zero).\n\nFor non-zero sample skewness one needs to solve a system of two coupled equations. Since the skewness and the excess kurtosis are independent of the parameters formula_206, the parameters formula_207 can be uniquely determined from the sample skewness and the sample excess kurtosis, by solving the coupled equations with two known variables (sample skewness and sample excess kurtosis) and two unknowns (the shape parameters):\n\nresulting in the following solution:\n\nWhere one should take the solutions as follows: formula_213 for (negative) sample skewness < 0, and formula_214 for (positive) sample skewness > 0.\n\nThe accompanying plot shows these two solutions as surfaces in a space with horizontal axes of (sample excess kurtosis) and (sample squared skewness) and the shape parameters as the vertical axis. The surfaces are constrained by the condition that the sample excess kurtosis must be bounded by the sample squared skewness as stipulated in the above equation. The two surfaces meet at the right edge defined by zero skewness. Along this right edge, both parameters are equal and the distribution is symmetric U-shaped for α = β < 1, uniform for α = β = 1, upside-down-U-shaped for 1 < α = β < 2 and bell-shaped for α = β > 2. The surfaces also meet at the front (lower) edge defined by \"the impossible boundary\" line (excess kurtosis + 2 - skewness = 0). Along this front (lower) boundary both shape parameters approach zero, and the probability density is concentrated more at one end than the other end (with practically nothing in between), with probabilities formula_215 at the left end \"x\" = 0 and formula_216 at the right end \"x\" = 1. The two surfaces become further apart towards the rear edge. At this rear edge the surface parameters are quite different from each other. As remarked, for example, by Bowman and Shenton, sampling in the neighborhood of the line (sample excess kurtosis - (3/2)(sample skewness) = 0) (the just-J-shaped portion of the rear edge where blue meets beige), \"is dangerously near to chaos\", because at that line the denominator of the expression above for the estimate ν = α + β becomes zero and hence ν approaches infinity as that line is approached. Bowman and Shenton write that \"the higher moment parameters (kurtosis and skewness) are extremely fragile (near that line). However the mean and standard deviation are fairly reliable.\" Therefore, the problem is for the case of four parameter estimation for very skewed distributions such that the excess kurtosis approaches (3/2) times the square of the skewness. This boundary line is produced by extremely skewed distributions with very large values of one of the parameters and very small values of the other parameter. See section titled \"Kurtosis bounded by the square of the skewness\" for a numerical example and further comments about this rear edge boundary line (sample excess kurtosis - (3/2)(sample skewness) = 0). As remarked by Karl Pearson himself this issue may not be of much practical importance as this trouble arises only for very skewed J-shaped (or mirror-image J-shaped) distributions with very different values of shape parameters that are unlikely to occur much in practice). The usual skewed-bell-shape distributions that occur in practice do not have this parameter estimation problem.\n\nThe remaining two parameters formula_206 can be determined using the sample mean and the sample variance using a variety of equations. One alternative is to calculate the support interval range formula_218 based on the sample variance and the sample kurtosis. For this purpose one can solve, in terms of the range formula_219, the equation expressing the excess kurtosis in terms of the sample variance, and the sample size ν (see section titled \"Kurtosis\" and \"Alternative parametrizations, four parameters\"):\n\nto obtain:\n\nAnother alternative is to calculate the support interval range formula_218 based on the sample variance and the sample skewness. For this purpose one can solve, in terms of the range formula_218, the equation expressing the squared skewness in terms of the sample variance, and the sample size ν (see section titled \"Skewness\" and \"Alternative parametrizations, four parameters\"):\n\nto obtain:\n\nThe remaining parameter can be determined from the sample mean and the previously obtained parameters: formula_226:\n\nand finally, of course, formula_228.\n\nIn the above formulas one may take, for example, as estimates of the sample moments:\n\nThe estimators \"G\" for sample skewness and \"G\" for sample kurtosis are used by DAP/SAS, PSPP/SPSS, and Excel. However, they are not used by BMDP and (according to ) they were not used by MINITAB in 1998. Actually, Joanes and Gill in their 1998 study concluded that the skewness and kurtosis estimators used in BMDP and in MINITAB (at that time) had smaller variance and mean-squared error in normal samples, but the skewness and kurtosis estimators used in DAP/SAS, PSPP/SPSS, namely \"G\" and \"G\", had smaller mean-squared error in samples from a very skewed distribution. It is for this reason that we have spelled out \"sample skewness\", etc., in the above formulas, to make it explicit that the user should choose the best estimator according to the problem at hand, as the best estimator for skewness and kurtosis depends on the amount of skewness (as shown by Joanes and Gill).\n\nAs is also the case for maximum likelihood estimates for the gamma distribution, the maximum likelihood estimates for the beta distribution do not have a general closed form solution for arbitrary values of the shape parameters. If \"X\", ..., \"X\" are independent random variables each having a beta distribution, the joint log likelihood function for \"N\" iid observations is:\n\nFinding the maximum with respect to a shape parameter involves taking the partial derivative with respect to the shape parameter and setting the expression equal to zero yielding the maximum likelihood estimator of the shape parameters:\n\nwhere:\n\nsince the digamma function denoted ψ(α) is defined as the logarithmic derivative of the gamma function:\n\nTo ensure that the values with zero tangent slope are indeed a maximum (instead of a saddle-point or a minimum) one has to also satisfy the condition that the curvature is negative. This amounts to satisfying that the second partial derivative with respect to the shape parameters is negative\n\nusing the previous equations, this is equivalent to:\n\nwhere the trigamma function, denoted \"ψ\"(\"α\"), is the second of the polygamma functions, and is defined as the derivative of the digamma function:\n\nThese conditions are equivalent to stating that the variances of the logarithmically transformed variables are positive, since:\n\nTherefore, the condition of negative curvature at a maximum is equivalent to the statements:\n\nAlternatively, the condition of negative curvature at a maximum is also equivalent to stating that the following logarithmic derivatives of the geometric means \"G\" and \"G\" are positive, since:\n\nWhile these slopes are indeed positive, the other slopes are negative:\n\nThe slopes of the mean and the median with respect to \"α\" and \"β\" display similar sign behavior.\n\nFrom the condition that at a maximum, the partial derivative with respect to the shape parameter equals zero, we obtain the following system of coupled maximum likelihood estimate equations (for the average log-likelihoods) that needs to be inverted to obtain the (unknown) shape parameter estimates formula_248 in terms of the (known) average of logarithms of the samples \"X\", ..., \"X\":\n\nwhere we recognize formula_250 as the logarithm of the sample geometric mean and formula_251 as the logarithm of the sample geometric mean based on (1 − \"X\"), the mirror-image of \"X\". For formula_252, it follows that formula_253.\n\nThese coupled equations containing digamma functions of the shape parameter estimates formula_248 must be solved by numerical methods as done, for example, by Beckman et al. Gnanadesikan et al. give numerical solutions for a few cases. N.L.Johnson and S.Kotz suggest that for \"not too small\" shape parameter estimates formula_248, the logarithmic approximation to the digamma function formula_257 may be used to obtain initial values for an iterative solution, since the equations resulting from this approximation can be solved exactly:\n\nwhich leads to the following solution for the initial values (of the estimate shape parameters in terms of the sample geometric means) for an iterative solution:\n\nAlternatively, the estimates provided by the method of moments can instead be used as initial values for an iterative solution of the maximum likelihood coupled equations in terms of the digamma functions.\n\nWhen the distribution is required over a known interval other than [0, 1] with random variable \"X\", say [\"a\", \"c\"] with random variable \"Y\", then replace ln(\"X\") in the first equation with\n\nand replace ln(1−\"X\") in the second equation with\n\n(see \"Alternative parametrizations, four parameters\" section below).\n\nIf one of the shape parameters is known, the problem is considerably simplified. The following logit transformation can be used to solve for the unknown shape parameter (for skewed cases such that formula_264, otherwise, if symmetric, both -equal- parameters are known when one is known):\n\nThis logit transformation is the logarithm of the transformation that divides the variable \"X\" by its mirror-image (\"X\"/(1 - \"X\") resulting in the \"inverted beta distribution\" or beta prime distribution (also known as beta distribution of the second kind or Pearson's Type VI) with support [0, +∞). As previously discussed in the section \"Moments of logarithmically transformed random variables,\" the logit transformation formula_266, studied by Johnson, extends the finite support [0, 1] based on the original variable \"X\" to infinite support in both directions of the real line (−∞, +∞).\n\nIf, for example, formula_267 is known, the unknown parameter formula_268 can be obtained in terms of the inverse digamma function of the right hand side of this equation:\n\nIn particular, if one of the shape parameters has a value of unity, for example for formula_271 (the power function distribution with bounded support [0,1]), using the identity ψ(\"x\" + 1) = ψ(\"x\") + 1/\"x\" in the equation formula_272, the maximum likelihood estimator for the unknown parameter formula_268 is, exactly:\n\nThe beta has support [0, 1], therefore formula_275, and hence formula_276, and therefore formula_277\n\nIn conclusion, the maximum likelihood estimates of the shape parameters of a beta distribution are (in general) a complicated function of the sample geometric mean, and of the sample geometric mean based on \"(1−X)\", the mirror-image of \"X\". One may ask, if the variance (in addition to the mean) is necessary to estimate two shape parameters with the method of moments, why is the (logarithmic or geometric) variance not necessary to estimate two shape parameters with the maximum likelihood method, for which only the geometric means suffice? The answer is because the mean does not provide as much information as the geometric mean. For a beta distribution with equal shape parameters \"α\" = \"β\", the mean is exactly 1/2, regardless of the value of the shape parameters, and therefore regardless of the value of the statistical dispersion (the variance). On the other hand, the geometric mean of a beta distribution with equal shape parameters \"α\" = \"β\", depends on the value of the shape parameters, and therefore it contains more information. Also, the geometric mean of a beta distribution does not satisfy the symmetry conditions satisfied by the mean, therefore, by employing both the geometric mean based on \"X\" and geometric mean based on (1 − \"X\"), the maximum likelihood method is able to provide best estimates for both parameters \"α\" = \"β\", without need of employing the variance.\n\nOne can express the joint log likelihood per \"N\" iid observations in terms of the \"sufficient statistics\" (the sample geometric means) as follows:\n\nWe can plot the joint log likelihood per \"N\" observations for fixed values of the sample geometric means to see the behavior of the likelihood function as a function of the shape parameters α and β. In such a plot, the shape parameter estimators formula_248 correspond to the maxima of the likelihood function. See the accompanying graph that shows that all the likelihood functions intersect at α = β = 1, which corresponds to the values of the shape parameters that give the maximum entropy (the maximum entropy occurs for shape parameters equal to unity: the uniform distribution). It is evident from the plot that the likelihood function gives sharp peaks for values of the shape parameter estimators close to zero, but that for values of the shape parameters estimators greater than one, the likelihood function becomes quite flat, with less defined peaks. Obviously, the maximum likelihood parameter estimation method for the beta distribution becomes less acceptable for larger values of the shape parameter estimators, as the uncertainty in the peak definition increases with the value of the shape parameter estimators. One can arrive at the same conclusion by noticing that the expression for the curvature of the likelihood function is in terms of the geometric variances\n\nThese variances (and therefore the curvatures) are much larger for small values of the shape parameter α and β. However, for shape parameter values α, β > 1, the variances (and therefore the curvatures) flatten out. Equivalently, this result follows from the Cramér–Rao bound, since the Fisher information matrix components for the beta distribution are these logarithmic variances. The Cramér–Rao bound states that the variance of any \"unbiased\" estimator formula_268 of α is bounded by the reciprocal of the Fisher information:\n\nso the variance of the estimators increases with increasing α and β, as the logarithmic variances decrease.\n\nAlso one can express the joint log likelihood per \"N\" iid observations in terms of the digamma function expressions for the logarithms of the sample geometric means as follows:\n\nthis expression is identical to the negative of the cross-entropy (see section on \"Quantities of information (entropy)\"). Therefore, finding the maximum of the joint log likelihood of the shape parameters, per \"N\" iid observations, is identical to finding the minimum of the cross-entropy for the beta distribution, as a function of the shape parameters.\n\nwith the cross-entropy defined as follows:\n\nThe procedure is similar to the one followed in the two unknown parameter case. If \"Y\", ..., \"Y\" are independent random variables each having a beta distribution with four parameters, the joint log likelihood function for \"N\" iid observations is:\n\nFinding the maximum with respect to a shape parameter involves taking the partial derivative with respect to the shape parameter and setting the expression equal to zero yielding the maximum likelihood estimator of the shape parameters:\n\nthese equations can be re-arranged as the following system of four coupled equations (the first two equations are geometric means and the second two equations are the harmonic means) in terms of the maximum likelihood estimates for the four parameters formula_199:\n\nwith sample geometric means:\n\nThe parameters formula_206 are embedded inside the geometric mean expressions in a nonlinear way (to the power 1/\"N\"). This precludes, in general, a closed form solution, even for an initial value approximation for iteration purposes. One alternative is to use as initial values for iteration the values obtained from the method of moments solution for the four parameter case. Furthermore, the expressions for the harmonic means are well-defined only for formula_301, which precludes a maximum likelihood solution for shape parameters less than unity in the four-parameter case. Fisher's information matrix for the four parameter case is positive-definite only for α, β > 2 (for further discussion, see section on Fisher information matrix, four parameter case), for bell-shaped (symmetric or unsymmetric) beta distributions, with inflection points located to either side of the mode. The following Fisher information components (that represent the expectations of the curvature of the log likelihood function) have singularities at the following values:\n\n(for further discussion see section on Fisher information matrix). Thus, it is not possible to strictly carry on the maximum likelihood estimation for some well known distributions belonging to the four-parameter beta distribution family, like the uniform distribution (Beta(1, 1, \"a\", \"c\")), and the arcsine distribution (Beta(1/2, 1/2, \"a\", \"c\")). N.L.Johnson and S.Kotz ignore the equations for the harmonic means and instead suggest \"If a and c are unknown, and maximum likelihood estimators of \"a\", \"c\", α and β are required, the above procedure (for the two unknown parameter case, with \"X\" transformed as \"X\" = (\"Y\" − \"a\")/(\"c\" − \"a\")) can be repeated using a succession of trial values of \"a\" and \"c\", until the pair (\"a\", \"c\") for which maximum likelihood (given \"a\" and \"c\") is as great as possible, is attained\" (where, for the purpose of clarity, their notation for the parameters has been translated into the present notation).\n\nLet a random variable X have a probability density \"f\"(\"x\";\"α\"). The partial derivative with respect to the (unknown, and to be estimated) parameter α of the log likelihood function is called the score. The second moment of the score is called the Fisher information:\n\nThe expectation of the score is zero, therefore the Fisher information is also the second moment centered on the mean of the score: the variance of the score.\n\nIf the log likelihood function is twice differentiable with respect to the parameter α, and under certain regularity conditions, then the Fisher information may also be written as follows (which is often a more convenient form for calculation purposes):\n\nThus, the Fisher information is the negative of the expectation of the second derivative with respect to the parameter α of the log likelihood function. Therefore, Fisher information is a measure of the curvature of the log likelihood function of α. A low curvature (and therefore high radius of curvature), flatter log likelihood function curve has low Fisher information; while a log likelihood function curve with large curvature (and therefore low radius of curvature) has high Fisher information. When the Fisher information matrix is computed at the evaluates of the parameters (\"the observed Fisher information matrix\") it is equivalent to the replacement of the true log likelihood surface by a Taylor's series approximation, taken as far as the quadratic terms. The word information, in the context of Fisher information, refers to information about the parameters. Information such as: estimation, sufficiency and properties of variances of estimators. The Cramér–Rao bound states that the inverse of the Fisher information is a lower bound on the variance of any estimator of a parameter α:\n\nThe precision to which one can estimate the estimator of a parameter α is limited by the Fisher Information of the log likelihood function. The Fisher information is a measure of the minimum error involved in estimating a parameter of a distribution and it can be viewed as a measure of the resolving power of an experiment needed to discriminate between two alternative hypothesis of a parameter.\n\nWhen there are \"N\" parameters\n\nthen the Fisher information takes the form of an \"N\"×\"N\" positive semidefinite symmetric matrix, the Fisher Information Matrix, with typical element:\n\nUnder certain regularity conditions, the Fisher Information Matrix may also be written in the following form, which is often more convenient for computation:\n\nWith \"X\", ..., \"X\" iid random variables, an \"N\"-dimensional \"box\" can be constructed with sides \"X\", ..., \"X\". Costa and Cover show that the (Shannon) differential entropy \"h\"(\"X\") is related to the volume of the typical set (having the sample entropy close to the true entropy), while the Fisher information is related to the surface of this typical set.\n\nFor \"X\", ..., \"X\" independent random variables each having a beta distribution parametrized with shape parameters \"α\" and \"β\", the joint log likelihood function for \"N\" iid observations is:\n\ntherefore the joint log likelihood function per \"N\" iid observations is:\n\nFor the two parameter case, the Fisher information has 4 components: 2 diagonal and 2 off-diagonal. Since the Fisher information matrix is symmetric, one of these off diagonal components is independent. Therefore, the Fisher information matrix has 3 independent components (2 diagonal and 1 off diagonal).\nAryal and Nadarajah calculated Fisher's information matrix for the four-parameter case, from which the two parameter case can be obtained as follows:\n\nSince the Fisher information matrix is symmetric\n\nThe Fisher information components are equal to the log geometric variances and log geometric covariance. Therefore, they can be expressed as trigamma functions, denoted ψ(α), the second of the polygamma functions, defined as the derivative of the digamma function:\n\nThese derivatives are also derived in the section titled \"Parameter estimation\", \"Maximum likelihood\", \"Two unknown parameters,\" and plots of the log likelihood function are also shown in that section. The section titled \"Geometric variance and covariance\" contains plots and further discussion of the Fisher information matrix components: the log geometric variances and log geometric covariance as a function of the shape parameters α and β. The section titled \"Other moments\", \"Moments of transformed random variables\", \"Moments of logarithmically transformed random variables\" contains formulas for moments of logarithmically transformed random variables. Images for the Fisher information components formula_319 and formula_320 are shown in the section titled \"Geometric variance\".\n\nThe determinant of Fisher's information matrix is of interest (for example for the calculation of Jeffreys prior probability). From the expressions for the individual components of the Fisher information matrix, it follows that the determinant of Fisher's (symmetric) information matrix for the beta distribution is:\n\nFrom Sylvester's criterion (checking whether the diagonal elements are all positive), it follows that the Fisher information matrix for the two parameter case is positive-definite (under the standard condition that the shape parameters are positive \"α\" > 0 and \"β\" > 0).\n\nIf \"Y\", ..., \"Y\" are independent random variables each having a beta distribution with four parameters: the exponents \"α\" and \"β\", and also \"a\" (the minimum of the distribution range), and \"c\" (the maximum of the distribution range) (section titled \"Alternative parametrizations\", \"Four parameters\"), with probability density function:\n\nthe joint log likelihood function per \"N\" iid observations is:\n\nFor the four parameter case, the Fisher information has 4*4=16 components. It has 12 off-diagonal components = (4×4 total − 4 diagonal). Since the Fisher information matrix is symmetric, half of these components (12/2=6) are independent. Therefore, the Fisher information matrix has 6 independent off-diagonal + 4 diagonal = 10 independent components. Aryal and Nadarajah calculated Fisher's information matrix for the four parameter case as follows:\n\nIn the above expressions, the use of \"X\" instead of \"Y\" in the expressions var[ln(\"X\")] = ln(var) is \"not an error\". The expressions in terms of the log geometric variances and log geometric covariance occur as functions of the two parameter \"X\" ~ Beta(\"α\", \"β\") parametrization because when taking the partial derivatives with respect to the exponents (\"α\", \"β\") in the four parameter case, one obtains the identical expressions as for the two parameter case: these terms of the four parameter Fisher information matrix are independent of the minimum \"a\" and maximum \"c\" of the distribution's range. The only non-zero term upon double differentiation of the log likelihood function with respect to the exponents \"α\" and \"β\" is the second derivative of the log of the beta function: ln(B(\"α\", \"β\")). This term is independent of the minimum \"a\" and maximum \"c\" of the distribution's range. Double differentiation of this term results in trigamma functions. The sections titled \"Maximum likelihood\", \"Two unknown parameters\" and \"Four unknown parameters\" also show this fact.\n\nThe Fisher information for \"N\" i.i.d. samples is \"N\" times the individual Fisher information (eq. 11.279, page 394 of Cover and Thomas). (Aryal and Nadarajah take a single observation, \"N\" = 1, to calculate the following components of the Fisher information, which leads to the same result as considering the derivatives of the log likelihood per \"N\" observations. Moreover, below the erroneous expression for formula_327 in Aryal and Nadarajah has been corrected.)\n\nThe lower two diagonal entries of the Fisher information matrix, with respect to the parameter \"a\" (the minimum of the distribution's range): formula_329, and with respect to the parameter \"c\" (the maximum of the distribution's range): formula_330 are only defined for exponents α > 2 and β > 2 respectively. The Fisher information matrix component formula_329 for the minimum \"a\" approaches infinity for exponent α approaching 2 from above, and the Fisher information matrix component formula_330 for the maximum \"c\" approaches infinity for exponent β approaching 2 from above.\n\nThe Fisher information matrix for the four parameter case does not depend on the individual values of the minimum \"a\" and the maximum \"c\", but only on the total range (\"c\"−\"a\"). Moreover, the components of the Fisher information matrix that depend on the range (\"c\"−\"a\"), depend only through its inverse (or the square of the inverse), such that the Fisher information decreases for increasing range (\"c\"−\"a\").\n\nThe accompanying images show the Fisher information components formula_329 and formula_334. Images for the Fisher information components formula_335 and formula_336 are shown in the section titled \"Geometric variance\". All these Fisher information components look like a basin, with the \"walls\" of the basin being located at low values of the parameters.\n\nThe following four-parameter-beta-distribution Fisher information components can be expressed in terms of the two-parameter: \"X\" ~ Beta(α, β) expectations of the transformed ratio ((1-\"X\")/\"X\") and of its mirror image (\"X\"/(1-\"X\")), scaled by the range (\"c\"−\"a\"), which may be helpful for interpretation:\n\nThese are also the expected values of the \"inverted beta distribution\" or beta prime distribution (also known as beta distribution of the second kind or Pearson's Type VI) and its mirror image, scaled by the range (\"c\" − \"a\").\n\nAlso, the following Fisher information components can be expressed in terms of the harmonic (1/X) variances or of variances based on the ratio transformed variables ((1-X)/X) as follows:\n\nSee section \"Moments of linearly transformed, product and inverted random variables\" for these expectations.\n\nThe determinant of Fisher's information matrix is of interest (for example for the calculation of Jeffreys prior probability). From the expressions for the individual components, it follows that the determinant of Fisher's (symmetric) information matrix for the beta distribution with four parameters is:\n\nUsing Sylvester's criterion (checking whether the diagonal elements are all positive), and since diagonal components formula_327 and formula_342 have singularities at α=2 and β=2 it follows that the Fisher information matrix for the four parameter case is positive-definite for α>2 and β>2. Since for α > 2 and β > 2 the beta distribution is (symmetric or unsymmetric) bell shaped, it follows that the Fisher information matrix is positive-definite only for bell-shaped (symmetric or unsymmetric) beta distributions, with inflection points located to either side of the mode. Thus, important well known distributions belonging to the four-parameter beta distribution family, like the parabolic distribution (Beta(2,2,a,c)) and the uniform distribution (Beta(1,1,a,c)) have Fisher information components (formula_343) that blow up (approach infinity) in the four-parameter case (although their Fisher information components are all defined for the two parameter case). The four-parameter Wigner semicircle distribution (Beta(3/2,3/2,\"a\",\"c\")) and arcsine distribution (Beta(1/2,1/2,\"a\",\"c\")) have negative Fisher information determinants for the four-parameter case.\n\nIf \"X\" and \"Y\" are independent, with formula_344 and formula_345 then\n\nSo one algorithm for generating beta variates is to generate formula_347, where \"X\" is a gamma variate with parameters (α, 1) and \"Y\" is an independent gamma variate with parameters (β, 1). In fact, here formula_348 and formula_349 are independent, and formula_350. If formula_351 and formula_352 is independent of formula_353 and formula_354, then formula_355 and formula_356 is independent of formula_348. This shows that the product of independent formula_358 and formula_359 random variables is a formula_360 random variable.\n\nAlso, the \"k\"th order statistic of \"n\" uniformly distributed variates is formula_361, so an alternative if α and β are small integers is to generate α + β − 1 uniform variates and choose the α-th smallest.\n\nAnother way to generate the Beta distribution is by Pólya urn model. According to this method, one start with an \"urn\" with α \"black\" balls and β \"white\" balls and draw uniformly with replacement. Every trial an additional ball is added according to the color of the last ball which was drawn. Asymptotically, the proportion of black and white balls will be distributed according to the Beta distribution, where each repetition of the experiment will produce a different value.\n\n\n\n\n\n\n\nThe beta distribution has an important application in the theory of order statistics. A basic result is that the distribution of the \"k\"th smallest of a sample of size \"n\" from a continuous uniform distribution has a beta distribution. This result is summarized as:\n\nFrom this, and application of the theory related to the probability integral transform, the distribution of any individual order statistic from any continuous distribution can be derived.\n\nA classic application of the beta distribution is the rule of succession, introduced in the 18th century by Pierre-Simon Laplace in the course of treating the sunrise problem. It states that, given \"s\" successes in \"n\" conditionally independent Bernoulli trials with probability \"p,\" that the estimate of the expected value in the next trial is formula_374. This estimate is the expected value of the posterior distribution over \"p,\" namely Beta(\"s\"+1, \"n\"−\"s\"+1), which is given by Bayes' rule if one assumes a uniform prior probability over \"p\" (i.e., Beta(1, 1)) and then observes that \"p\" generated \"s\" successes in \"n\" trials. Laplace's rule of succession has been criticized by prominent scientists. R. T. Cox described Laplace's application of the rule of succession to the sunrise problem ( p. 89) as \"a travesty of the proper use of the principle.\" Keynes remarks ( Ch.XXX, p. 382) \"indeed this is so foolish a theorem that to entertain it is discreditable.\" Karl Pearson showed that the probability that the next (\"n\" + 1) trials will be successes, after n successes in n trials, is only 50%, which has been considered too low by scientists like Jeffreys and unacceptable as a representation of the scientific process of experimentation to test a proposed scientific law. As pointed out by Jeffreys ( p. 128) (crediting C. D. Broad ) Laplace's rule of succession establishes a high probability of success ((n+1)/(n+2)) in the next trial, but only a moderate probability (50%) that a further sample (n+1) comparable in size will be equally successful. As pointed out by Perks, \"The rule of succession itself is hard to accept. It assigns a probability to the next trial which implies the assumption that the actual run observed is an average run and that we are always at the end of an average run. It would, one would think, be more reasonable to assume that we were in the middle of an average run. Clearly a higher value for both probabilities is necessary if they are to accord with reasonable belief.\" These problems with Laplace's rule of succession motivated Haldane, Perks, Jeffreys and others to search for other forms of prior probability (see the next section titled \"Bayesian inference\"). According to Jaynes, the main problem with the rule of succession is that it is not valid when s=0 or s=n (see rule of succession, for an analysis of its validity).\n\nThe use of Beta distributions in Bayesian inference is due to the fact that they provide a family of conjugate prior probability distributions for binomial (including Bernoulli) and geometric distributions. The domain of the beta distribution can be viewed as a probability, and in fact the beta distribution is often used to describe the distribution of a probability value \"p\":\n\nExamples of beta distributions used as prior probabilities to represent ignorance of prior parameter values in Bayesian inference are Beta(1,1), Beta(0,0) and Beta(1/2,1/2).\n\nThe beta distribution achieves maximum differential entropy for Beta(1,1): the uniform probability density, for which all values in the domain of the distribution have equal density. This uniform distribution Beta(1,1) was suggested (\"with a great deal of doubt\") by Thomas Bayes as the prior probability distribution to express ignorance about the correct prior distribution. This prior distribution was adopted (apparently, from his writings, with little sign of doubt) by Pierre-Simon Laplace, and hence it was also known as the \"Bayes-Laplace rule\" or the \"Laplace rule\" of \"inverse probability\" in publications of the first half of the 20th century. In the later part of the 19th century and early part of the 20th century, scientists realized that the assumption of uniform \"equal\" probability density depended on the actual functions (for example whether a linear or a logarithmic scale was most appropriate) and parametrizations used. In particular, the behavior near the ends of distributions with finite support (for example near \"x\" = 0, for a distribution with initial support at \"x\" = 0) required particular attention. Keynes ( Ch.XXX, p. 381) criticized the use of Bayes's uniform prior probability (Beta(1,1)) that all values between zero and one are equiprobable, as follows: \"Thus experience, if it shows anything, shows that there is a very marked clustering of statistical ratios in the neighborhoods of zero and unity, of those for positive theories and for correlations between positive qualities in the neighborhood of zero, and of those for negative theories and for correlations between negative qualities in the neighborhood of unity. \"\n\nThe Beta(0,0) distribution was proposed by J.B.S. Haldane, who suggested that the prior probability representing complete uncertainty should be proportional to \"p\"(1−\"p\"). The function \"p\"(1−\"p\") can be viewed as the limit of the numerator of the beta distribution as both shape parameters approach zero: α, β → 0. The Beta function (in the denominator of the beta distribution) approaches infinity, for both parameters approaching zero, α, β → 0. Therefore, \"p\"(1−\"p\") divided by the Beta function approaches a 2-point Bernoulli distribution with equal probability 1/2 at each Dirac delta function end, at 0 and 1, and nothing in between, as α, β → 0. A coin-toss: one face of the coin being at 0 and the other face being at 1. The Haldane prior probability distribution Beta(0,0) is an \"improper prior\" because its integration (from 0 to 1) fails to strictly converge to 1 due to the Dirac delta function singularities at each end. However, this is not an issue for computing posterior probabilities unless the sample size is very small. Furthermore, Zellner points out that on the log-odds scale, (the logit transformation ln(\"p\"/1−\"p\")), the Haldane prior is the uniformly flat prior. The fact that a uniform prior probability on the logit transformed variable ln(\"p\"/1−\"p\") (with domain (-∞, ∞)) is equivalent to the Haldane prior on the domain [0, 1] was pointed out by Harold Jeffreys in the first edition (1939) of his book Theory of Probability ( p. 123). Jeffreys writes \"Certainly if we take the Bayes-Laplace rule right up to the extremes we are led to results that do not correspond to anybody's way of thinking. The (Haldane) rule d\"x\"/(\"x\"(1−\"x\")) goes too far the other way. It would lead to the conclusion that if a sample is of one type with respect to some property there is a probability 1 that the whole population is of that type.\" The fact that \"uniform\" depends on the parametrization, led Jeffreys to seek a form of prior that would be invariant under different parametrizations.\n\nHarold Jeffreys proposed to use an uninformative prior probability measure that should be invariant under reparameterization: proportional to the square root of the determinant of Fisher's information matrix. For the Bernoulli distribution, this can be shown as follows: for a coin that is \"heads\" with probability \"p\" ∈ [0, 1] and is \"tails\" with probability 1 − \"p\", for a given (H,T) ∈ {(0,1), (1,0)} the probability is \"p\"(1 − \"p\"). Since \"T\" = 1 − \"H\", the Bernoulli distribution is \"p\"(1 − \"p\"). Considering \"p\" as the only parameter, it follows that the log likelihood for the Bernoulli distribution is\n\nThe Fisher information matrix has only one component (it is a scalar, because there is only one parameter: \"p\"), therefore:\n\nSimilarly, for the Binomial distribution with \"n\" Bernoulli trials, it can be shown that\n\nThus, for the Bernoulli, and Binomial distributions, Jeffreys prior is proportional to formula_379, which happens to be proportional to a beta distribution with domain variable \"x\" = \"p\", and shape parameters α = β = 1/2, the arcsine distribution:\n\nIt will be shown in the next section that the normalizing constant for Jeffreys prior is immaterial to the final result because the normalizing constant cancels out in Bayes theorem for the posterior probability. Hence Beta(1/2,1/2) is used as the Jeffreys prior for both Bernoulli and binomial distributions. As shown in the next section, when using this expression as a prior probability times the likelihood in Bayes theorem, the posterior probability turns out to be a beta distribution. It is important to realize, however, that Jeffreys prior is proportional to formula_379 for the Bernoulli and binomial distribution, but not for the beta distribution. Jeffreys prior for the beta distribution is given by the determinant of Fisher's information for the beta distribution, which, as shown in the section titled \"Fisher information matrix\" is a function of the trigamma function ψ of shape parameters α and β as follows:\n\nAs previously discussed, Jeffreys prior for the Bernoulli and binomial distributions is proportional to the arcsine distribution Beta(1/2,1/2), a one-dimensional \"curve\" that looks like a basin as a function of the parameter \"p\" of the Bernoulli and binomial distributions. The walls of the basin are formed by \"p\" approaching the singularities at the ends \"p\" → 0 and \"p\" → 1, where Beta(1/2,1/2) approaches infinity. Jeffreys prior for the beta distribution is a \"2-dimensional surface\" (embedded in a three-dimensional space) that looks like a basin with only two of its walls meeting at the corner α = β = 0 (and missing the other two walls) as a function of the shape parameters α and β of the beta distribution. The two adjoining walls of this 2-dimensional surface are formed by the shape parameters α and β approaching the singularities (of the trigamma function) at α, β → 0. It has no walls for α, β → ∞ because in this case the determinant of Fisher's information matrix for the beta distribution approaches zero.\n\nIt will be shown in the next section that Jeffreys prior probability results in posterior probabilities (when multiplied by the binomial likelihood function) that are intermediate between the posterior probability results of the Haldane and Bayes prior probabilities.\n\nJeffreys prior may be difficult to obtain analytically, and for some cases it just doesn't exist (even for simple distribution functions like the asymmetric triangular distribution). Berger, Bernardo and Sun, in a 2009 paper defined a reference prior probability distribution that (unlike Jeffreys prior) exists for the asymmetric triangular distribution. They cannot obtain a closed-form expression for their reference prior, but numerical calculations show it to be nearly perfectly fitted by the (proper) prior\n\nwhere θ is the vertex variable for the asymmetric triangular distribution with support [0, 1] (corresponding to the following parameter values in Wikipedia's article on the triangular distribution: vertex \"c\" = \"θ\", left end \"a\" = 0,and right end \"b\" = 1). Berger et al. also give a heuristic argument that Beta(1/2,1/2) could indeed be the exact Berger–Bernardo–Sun reference prior for the asymmetric triangular distribution. Therefore, Beta(1/2,1/2) not only is Jeffreys prior for the Bernoulli and binomial distributions, but also seems to be the Berger–Bernardo–Sun reference prior for the asymmetric triangular distribution (for which the Jeffreys prior does not exist), a distribution used in project management and PERT analysis to describe the cost and duration of project tasks.\n\nClarke and Barron prove that, among continuous positive priors, Jeffreys prior (when it exists) asymptotically maximizes Shannon's mutual information between a sample of size n and the parameter, and therefore \"Jeffreys prior is the most uninformative prior\" (measuring information as Shannon information). The proof rests on an examination of the Kullback–Leibler distance between probability density functions for iid random variables.\n\nIf samples are drawn from the population of a random variable \"X\" that result in \"s\" successes and \"f\" failures in \"n\" Bernoulli trials \"n\" = \"s\" + \"f\", then the likelihood function for parameters \"s\" and \"f\" given \"x\" = \"p\" (the notation \"x\" = \"p\" in the expressions below will emphasize that the domain \"x\" stands for the value of the parameter \"p\" in the binomial distribution), is the following binomial distribution:\n\nIf beliefs about prior probability information are reasonably well approximated by a beta distribution with parameters \"α\" Prior and \"β\" Prior, then:\n\nAccording to Bayes' theorem for a continuous event space, the posterior probability is given by the product of the prior probability and the likelihood function (given the evidence \"s\" and \"f\" = \"n\" − \"s\"), normalized so that the area under the curve equals one, as follows:\n\nThe binomial coefficient\n\nappears both in the numerator and the denominator of the posterior probability, and it does not depend on the integration variable \"x\", hence it cancels out, and it is irrelevant to the final result. Similarly the normalizing factor for the prior probability, the beta function B(αPrior,βPrior) cancels out and it is immaterial to the final result. The same posterior probability result can be obtained if one uses an un-normalized prior\n\nbecause the normalizing factors all cancel out. Several authors (including Jeffreys himself) thus use an un-normalized prior formula since the normalization constant cancels out. The numerator of the posterior probability ends up being just the (un-normalized) product of the prior probability and the likelihood function, and the denominator is its integral from zero to one. The beta function in the denominator, B(\"s\" + \"α\" Prior, \"n\" − \"s\" + \"β\" Prior), appears as a normalization constant to ensure that the total posterior probability integrates to unity.\n\nThe ratio \"s\"/\"n\" of the number of successes to the total number of trials is a sufficient statistic in the binomial case, which is relevant for the following results.\n\nFor the Bayes' prior probability (Beta(1,1)), the posterior probability is:\n\nFor the Jeffreys' prior probability (Beta(1/2,1/2)), the posterior probability is:\n\nand for the Haldane prior probability (Beta(0,0)), the posterior probability is:\n\nFrom the above expressions it follows that for \"s\"/\"n\" = 1/2) all the above three prior probabilities result in the identical location for the posterior probability mean = mode = 1/2. For \"s\"/\"n\" < 1/2, the mean of the posterior probabilities, using the following priors, are such that: mean for Bayes prior > mean for Jeffreys prior > mean for Haldane prior. For \"s\"/\"n\" > 1/2 the order of these inequalities is reversed such that the Haldane prior probability results in the largest posterior mean. The \"Haldane\" prior probability Beta(0,0) results in a posterior probability density with \"mean\" (the expected value for the probability of success in the \"next\" trial) identical to the ratio \"s\"/\"n\" of the number of successes to the total number of trials. Therefore, the Haldane prior results in a posterior probability with expected value in the next trial equal to the maximum likelihood. The \"Bayes\" prior probability Beta(1,1) results in a posterior probability density with \"mode\" identical to the ratio \"s\"/\"n\" (the maximum likelihood).\n\nIn the case that 100% of the trials have been successful \"s\" = \"n\", the \"Bayes\" prior probability Beta(1,1) results in a posterior expected value equal to the rule of succession (\"n\" + 1)/(\"n\" + 2), while the Haldane prior Beta(0,0) results in a posterior expected value of 1 (absolute certainty of success in the next trial). Jeffreys prior probability results in a posterior expected value equal to (\"n\" + 1/2)/(\"n\" + 1). Perks (p. 303) points out: \"This provides a new rule of succession and expresses a 'reasonable' position to take up, namely, that after an unbroken run of n successes we assume a probability for the next trial equivalent to the assumption that we are about half-way through an average run, i.e. that we expect a failure once in (2\"n\" + 2) trials. The Bayes–Laplace rule implies that we are about at the end of an average run or that we expect a failure once in (\"n\" + 2) trials. The comparison clearly favours the new result (what is now called Jeffreys prior) from the point of view of 'reasonableness'.\"\n\nConversely, in the case that 100% of the trials have resulted in failure (\"s\" = 0), the \"Bayes\" prior probability Beta(1,1) results in a posterior expected value for success in the next trial equal to 1/(\"n\" + 2), while the Haldane prior Beta(0,0) results in a posterior expected value of success in the next trial of 0 (absolute certainty of failure in the next trial). Jeffreys prior probability results in a posterior expected value for success in the next trial equal to (1/2)/(\"n\" + 1), which Perks (p. 303) points out: \"is a much more reasonably remote result than the Bayes-Laplace result 1/(\"n\" + 2)\".\n\nJaynes questions (for the uniform prior Beta(1,1)) the use of these formulas for the cases \"s\" = 0 or \"s\" = \"n\" because the integrals do not converge (Beta(1,1) is an improper prior for \"s\" = 0 or \"s\" = \"n\"). In practice, the conditions 0 (p. 303) shows that, for what is now known as the Jeffreys prior, this probability is ((\"n\" + 1/2)/(\"n\" + 1))((\"n\" + 3/2)/(\"n\" + 2))...(2\"n\" + 1/2)/(2\"n\" + 1), which for \"n\" = 1, 2, 3 gives 15/24, 315/480, 9009/13440; rapidly approaching a limiting value of formula_392 as n tends to infinity. Perks remarks that what is now known as the Jeffreys prior: \"is clearly more 'reasonable' than either the Bayes-Laplace result or the result on the (Haldane) alternative rule rejected by Jeffreys which gives certainty as the probability. It clearly provides a very much better correspondence with the process of induction. Whether it is 'absolutely' reasonable for the purpose, i.e. whether it is yet large enough, without the absurdity of reaching unity, is a matter for others to decide. But it must be realized that the result depends on the assumption of complete indifference and absence of knowledge prior to the sampling experiment.\"\n\nFollowing are the variances of the posterior distribution obtained with these three prior probability distributions:\n\nfor the Bayes' prior probability (Beta(1,1)), the posterior variance is:\n\nfor the Jeffreys' prior probability (Beta(1/2,1/2)), the posterior variance is:\n\nand for the Haldane prior probability (Beta(0,0)), the posterior variance is:\n\nSo, as remarked by Silvey, for large \"n\", the variance is small and hence the posterior distribution is highly concentrated, whereas the assumed prior distribution was very diffuse. This is in accord with what one would hope for, as vague prior knowledge is transformed (through Bayes theorem) into a more precise posterior knowledge by an informative experiment. For small \"n\" the Haldane Beta(0,0) prior results in the largest posterior variance while the Bayes Beta(1,1) prior results in the more concentrated posterior. Jeffreys prior Beta(1/2,1/2) results in a posterior variance in between the other two. As \"n\" increases, the variance rapidly decreases so that the posterior variance for all three priors converges to approximately the same value (approaching zero variance as \"n\" → ∞). Recalling the previous result that the \"Haldane\" prior probability Beta(0,0) results in a posterior probability density with \"mean\" (the expected value for the probability of success in the \"next\" trial) identical to the ratio s/n of the number of successes to the total number of trials, it follows from the above expression that also the \"Haldane\" prior Beta(0,0) results in a posterior with \"variance\" identical to the variance expressed in terms of the max. likelihood estimate s/n and sample size (in section titled \"Variance\"):\n\nwith the mean \"μ\" = \"s\"/\"n\" and the sample size \"ν\" = \"n\".\n\nIn Bayesian inference, using a prior distribution Beta(\"α\"Prior,\"β\"Prior) prior to a binomial distribution is equivalent to adding (\"α\"Prior − 1) pseudo-observations of \"success\" and (\"β\"Prior − 1) pseudo-observations of \"failure\" to the actual number of successes and failures observed, then estimating the parameter \"p\" of the binomial distribution by the proportion of successes over both real- and pseudo-observations. A uniform prior Beta(1,1) does not add (or subtract) any pseudo-observations since for Beta(1,1) it follows that (\"α\"Prior − 1) = 0 and (\"β\"Prior − 1) = 0. The Haldane prior Beta(0,0) subtracts one pseudo observation from each and Jeffreys prior Beta(1/2,1/2) subtracts 1/2 pseudo-observation of success and an equal number of failure. This subtraction has the effect of smoothing out the posterior distribution. If the proportion of successes is not 50% (\"s\"/\"n\" ≠ 1/2) values of \"α\"Prior and \"β\"Prior less than 1 (and therefore negative (\"α\"Prior − 1) and (\"β\"Prior − 1)) favor sparsity, i.e. distributions where the parameter \"p\" is closer to either 0 or 1. In effect, values of \"α\"Prior and \"β\"Prior between 0 and 1, when operating together, function as a concentration parameter.\n\nThe accompanying plots show the posterior probability density functions for sample sizes \"n\" ∈ {3,10,50}, successes \"s\" ∈ {\"n\"/2,\"n\"/4} and Beta(\"α\"Prior,\"β\"Prior) ∈ {Beta(0,0),Beta(1/2,1/2),Beta(1,1)}. Also shown are the cases for \"n\" = {4,12,40}, success \"s\" = {\"n\"/4} and Beta(\"α\"Prior,\"β\"Prior) ∈ {Beta(0,0),Beta(1/2,1/2),Beta(1,1)}. The first plot shows the symmetric cases, for successes \"s\" ∈ {n/2}, with mean = mode = 1/2 and the second plot shows the skewed cases \"s\" ∈ {\"n\"/4}. The images show that there is little difference between the priors for the posterior with sample size of 50 (characterized by a more pronounced peak near \"p\" = 1/2). Significant differences appear for very small sample sizes (in particular for the flatter distribution for the degenerate case of sample size = 3). Therefore, the skewed cases, with successes \"s\" = {\"n\"/4}, show a larger effect from the choice of prior, at small sample size, than the symmetric cases. For symmetric distributions, the Bayes prior Beta(1,1) results in the most \"peaky\" and highest posterior distributions and the Haldane prior Beta(0,0) results in the flattest and lowest peak distribution. The Jeffreys prior Beta(1/2,1/2) lies in between them. For nearly symmetric, not too skewed distributions the effect of the priors is similar. For very small sample size (in this case for a sample size of 3) and skewed distribution (in this example for \"s\" ∈ {\"n\"/4}) the Haldane prior can result in a reverse-J-shaped distribution with a singularity at the left end. However, this happens only in degenerate cases (in this example \"n\" = 3 and hence \"s\" = 3/4 < 1, a degenerate value because s should be greater than unity in order for the posterior of the Haldane prior to have a mode located between the ends, and because \"s\" = 3/4 is not an integer number, hence it violates the initial assumption of a binomial distribution for the likelihood) and it is not an issue in generic cases of reasonable sample size (such that the condition 1 < \"s\" < \"n\" − 1, necessary for a mode to exist between both ends, is fulfilled).\n\nIn Chapter 12 (p. 385) of his book, Jaynes asserts that the \"Haldane prior\" Beta(0,0) describes a \"prior state of knowledge of complete ignorance\", where we are not even sure whether it is physically possible for an experiment to yield either a success or a failure, while the \"Bayes (uniform) prior Beta(1,1) applies if\" one knows that \"both binary outcomes are possible\". Jaynes states: \"\"interpret the Bayes-Laplace (Beta(1,1)) prior as describing not a state of complete ignorance\", but the state of knowledge in which we have observed one success and one failure...once we have seen at least one success and one failure, then we know that the experiment is a true binary one, in the sense of physical possibility.\" Jaynes does not specifically discuss Jeffreys prior Beta(1/2,1/2) (Jaynes discussion of \"Jeffreys prior\" on pp. 181, 423 and on chapter 12 of Jaynes book refers instead to the improper, un-normalized, prior \"1/\"p\" \"dp\" introduced by Jeffreys in the 1939 edition of his book, seven years before he introduced what is now known as Jeffreys' invariant prior: the square root of the determinant of Fisher's information matrix. \"1/p\" is Jeffreys' (1946) invariant prior for the exponential distribution, not for the Bernoulli or binomial distributions\"). However, it follows from the above discussion that Jeffreys Beta(1/2,1/2) prior represents a state of knowledge in between the Haldane Beta(0,0) and Bayes Beta (1,1) prior.\n\nSimilarly, Karl Pearson in his 1892 book The Grammar of Science (p. 144 of 1900 edition) maintained that the Bayes (Beta(1,1) uniform prior was not a complete ignorance prior, and that it should be used when prior information justified to \"distribute our ignorance equally\"\". K. Pearson wrote: \"Yet the only supposition that we appear to have made is this: that, knowing nothing of nature, routine and anomy (from the Greek ανομία, namely: a- \"without\", and nomos \"law\") are to be considered as equally likely to occur. Now we were not really justified in making even this assumption, for it involves a knowledge that we do not possess regarding nature. We use our \"experience\" of the constitution and action of coins in general to assert that heads and tails are equally probable, but we have no right to assert before experience that, as we know nothing of nature, routine and breach are equally probable. In our ignorance we ought to consider before experience that nature may consist of all routines, all anomies (normlessness), or a mixture of the two in any proportion whatever, and that all such are equally probable. Which of these constitutions after experience is the most probable must clearly depend on what that experience has been like.\"\n\nIf there is sufficient sampling data, \"and the posterior probability mode is not located at one of the extremes of the domain\" (x=0 or x=1), the three priors of Bayes (Beta(1,1)), Jeffreys (Beta(1/2,1/2)) and Haldane (Beta(0,0)) should yield similar \"posterior\" probability densities. Otherwise, as Gelman et al. (p. 65) point out, \"if so few data are available that the choice of noninformative prior distribution makes a difference, one should put relevant information into the prior distribution\", or as Berger (p. 125) points out \"when different reasonable priors yield substantially different answers, can it be right to state that there \"is\" a single answer? Would it not be better to admit that there is scientific uncertainty, with the conclusion depending on prior beliefs?.\"\n\nIn standard logic, propositions are considered to be either true or false. In contradistinction, subjective logic assumes that humans cannot determine with absolute certainty whether a proposition about the real world is absolutely true or false. In subjective logic the posteriori probability estimates of binary events can be represented by beta distributions.\n\nA wavelet is a wave-like oscillation with an amplitude that starts out at zero, increases, and then decreases back to zero. It can typically be visualized as a \"brief oscillation\" that promptly decays. Wavelets can be used to extract information from many different kinds of data, including – but certainly not limited to – audio signals and images. Thus, wavelets are purposefully crafted to have specific properties that make them useful for signal processing. Wavelets are localized in both time and frequency whereas the standard Fourier transform is only localized in frequency. Therefore, standard Fourier Transforms are only applicable to stationary processes, while wavelets are applicable to non-stationary processes. Continuous wavelets can be constructed based on the beta distribution. Beta wavelets can be viewed as a soft variety of Haar wavelets whose shape is fine-tuned by two shape parameters α and β.\n\nThe beta distribution can be used to model events which are constrained to take place within an interval defined by a minimum and maximum value. For this reason, the beta distribution — along with the triangular distribution — is used extensively in PERT, critical path method (CPM), Joint Cost Schedule Modeling (JCSM) and other project management/control systems to describe the time to completion and the cost of a task. In project management, shorthand computations are widely used to estimate the mean and standard deviation of the beta distribution:\n\nwhere \"a\" is the minimum, \"c\" is the maximum, and \"b\" is the most likely value (the mode for \"α\" > 1 and \"β\" > 1).\n\nThe above estimate for the mean formula_398 is known as the PERT three-point estimation and it is exact for either of the following values of \"β\" (for arbitrary α within these ranges):\n\nor\n\nskewness = formula_402, and excess kurtosis = formula_403\n\nThe above estimate for the standard deviation \"σ\"(\"X\") = (\"c\" − \"a\")/6 is exact for either of the following values of \"α\" and \"β\":\n\nOtherwise, these can be poor approximations for beta distributions with other values of α and β, exhibiting average errors of 40% in the mean and 549% in the variance.\n\nThe beta distribution may also be reparameterized in terms of its mean \"μ\" and the addition of both shape parameters ( p. 83). Denoting by αPosterior and βPosterior the shape parameters of the posterior beta distribution resulting from applying Bayes theorem to a binomial likelihood function and a prior probability, the interpretation of the addition of both shape parameters to be sample size = \"ν\" = \"α\"·Posterior + \"β\"·Posterior is only correct for the Haldane prior probability Beta(0,0). Specifically, for the Bayes (uniform) prior Beta(1,1) the correct interpretation would be sample size = \"α\"·Posterior + \"β\" Posterior − 2, or \"ν\" = (sample size) + 2. Of course, for sample size much larger than 2, the difference between these two priors becomes negligible. (See section Bayesian inference for further details.) In the rest of this article ν = α + β will be referred to as \"sample size\", but one should remember that it is, strictly speaking, the \"sample size\" of a binomial likelihood function only when using a Haldane Beta(0,0) prior in Bayes theorem.\n\nThis parametrization may be useful in Bayesian parameter estimation. For example, one may administer a test to a number of individuals. If it is assumed that each person's score (0 ≤ \"θ\" ≤ 1) is drawn from a population-level Beta distribution, then an important statistic is the mean of this population-level distribution. The mean and sample size parameters are related to the shape parameters α and β via\n\nUnder this parametrization, one may place an uninformative prior probability over the mean, and a vague prior probability (such as an exponential or gamma distribution) over the positive reals for the sample size, if they are independent, and prior data and/or beliefs justify it.\n\nThe mode and \"concentration\" formula_408 can also be used to calculate the parameters for a beta distribution.\n\nThe Balding–Nichols model is a two-parameter parametrization of the beta distribution used in population genetics. It is a statistical description of the allele frequencies in the components of a sub-divided population:\n\nwhere formula_411 and formula_412; here \"F\" is (Wright's) genetic distance between two populations.\n\nSee the articles Balding–Nichols model, F-statistics, fixation index and coefficient of relationship, for further information.\n\nSolving the system of (coupled) equations given in the above sections as the equations for the mean and the variance of the beta distribution in terms of the original parameters \"α\" and \"β\", one can express the \"α\" and \"β\" parameters in terms of the mean (\"μ\") and the variance (var):\n\nThis parametrization of the beta distribution may lead to a more intuitive understanding than the one based on the original parameters \"α\" and \"β\". For example, by expressing the mode, skewness, excess kurtosis and differential entropy in terms of the mean and the variance:\n\nA beta distribution with the two shape parameters α and β is supported on the range [0,1] or (0,1). It is possible to alter the location and scale of the distribution by introducing two further parameters representing the minimum, \"a\", and maximum \"c\" (\"c\" > \"a\"), values of the distribution, by a linear transformation substituting the non-dimensional variable \"x\" in terms of the new variable \"y\" (with support [\"a\",\"c\"] or (\"a\",\"c\")) and the parameters \"a\" and \"c\":\n\nThe probability density function of the four parameter beta distribution is equal to the two parameter distribution, scaled by the range (\"c\"-\"a\"), (so that the total area under the density curve equals a probability of one), and with the \"y\" variable shifted and scaled as follows:\n\nThat a random variable \"Y\" is Beta-distributed with four parameters α, β, \"a\", and \"c\" will be denoted by:\n\nThe measures of central location are scaled (by (\"c\"-\"a\")) and shifted (by \"a\"), as follows:\n\nThe statistical dispersion measures are scaled (they do not need to be shifted because they are already centered on the mean) by the range (c-a), linearly for the mean deviation and nonlinearly for the variance:\n\nSince the skewness and excess kurtosis are non-dimensional quantities (as moments centered on the mean and normalized by the standard deviation), they are independent of the parameters \"a\" and \"c\", and therefore equal to the expressions given above in terms of \"X\" (with support [0,1] or (0,1)):\n\nThe first systematic modern discussion of the beta distribution is probably due to Karl Pearson FRS (27 March 1857 – 27 April 1936), an influential English mathematician who has been credited with establishing the discipline\nof mathematical statistics. In Pearson's papers the beta distribution is couched as a solution of a differential equation: Pearson's Type I distribution which it is essentially identical to except for arbitrary shifting and re-scaling (the beta and Pearson Type I distributions can always be equalized by proper choice of parameters). In fact, in several English books and journal articles in the few decades prior to World War II, it was common to refer to the beta distribution as Pearson's Type I distribution. William P. Elderton (1877–1962) in his 1906 monograph \"Frequency curves and correlation\" further analyzes the beta distribution as Pearson's Type I distribution, including a full discussion of the method of moments for the four parameter case, and diagrams of (what Elderton describes as) U-shaped, J-shaped, twisted J-shaped, \"cocked-hat\" shapes, horizontal and angled straight-line cases. Elderton wrote \"I am chiefly indebted to Professor Pearson, but the indebtedness is of a kind for which it is impossible to offer formal thanks.\" Elderton in his 1906 monograph provides an impressive amount of information on the beta distribution, including equations for the origin of the distribution chosen to be the mode, as well as for other Pearson distributions: types I through VII. Elderton also included a number of appendixes, including one appendix (\"II\") on the beta and gamma functions. In later editions, Elderton added equations for the origin of the distribution chosen to be the mean, and analysis of Pearson distributions VIII through XII.\n\nAs remarked by Bowman and Shenton \"Fisher and Pearson had a difference of opinion in the approach to (parameter) estimation, in particular relating to (Pearson's method of) moments and (Fisher's method of) maximum likelihood in the case of the Beta distribution.\" Also according to Bowman and Shenton, \"the case of a Type I (beta distribution) model being the center of the controversy was pure serendipity. A more difficult model of 4 parameters would have been hard to find.\"\nRonald Fisher (17 February 1890 – 29 July 1962) was one of the giants of statistics in the first half of the 20th century, and his long running public conflict with Karl Pearson can be followed in a number of articles in prestigious journals. For example, concerning the estimation of the four parameters for the beta distribution, and Fisher's criticism of Pearson's method of moments as being arbitrary, see Pearson's article \"Method of moments and method of maximum likelihood\" (published three years after his retirement from University College, London, where his position had been divided between Fisher and Pearson's son Egon) in which Pearson writes \"I read (Koshai's paper in the Journal of the Royal Statistical Society, 1933) which as far as I am aware is the only case at present published of the application of Professor Fisher's method. To my astonishment that method depends on first working out the constants of the frequency curve by the (Pearson) Method of Moments and then superposing on it, by what Fisher terms \"the Method of Maximum Likelihood\" a further approximation to obtain, what he holds, he will thus get, \"more efficient values\" of the curve constants.\"\n\nDavid and Edwards's treatise on the history of statistics cites the first modern treatment of the beta distribution, in 1911, using the beta designation that has become standard, due to Corrado Gini, an Italian statistician, demographer, and sociologist, who developed the Gini coefficient. N.L.Johnson and S.Kotz, in their comprehensive and very informative monograph on leading historical personalities in statistical sciences credit Corrado Gini as \"an early Bayesian...who dealt with the problem of eliciting the parameters of an initial Beta distribution, by singling out techniques which anticipated the advent of the so called empirical Bayes approach.\" Bayes, in a posthumous paper published in 1763 by Richard Price, obtained a beta distribution as the density of the probability of success in Bernoulli trials (see the section titled \"Applications, Bayesian inference\" in this article), but the paper does not analyze any of the moments of the beta distribution or discuss any of its properties.\n\n"}
{"id": "22367851", "url": "https://en.wikipedia.org/wiki?curid=22367851", "title": "Biordered set", "text": "Biordered set\n\nA biordered set (\"boset\") is a mathematical object that occurs in the description of the structure of the set of idempotents in a semigroup. The concept and the terminology were developed by K S S Nambooripad in the early 1970s.\nThe defining properties of a biordered set are expressed in terms of two quasiorders defined on the set and hence the name biordered set. Patrick Jordan, while a master's student at University of Sydney, introduced in 2002 the term boset as an abbreviation of biordered set.\n\nAccording to Mohan S. Putcha, \"The axioms defining a biordered set are quite complicated. However, considering the general nature of semigroups, it is rather surprising that such a finite axiomatization is even possible.\" Since the publication of the original definition of the biordered set by Nambooripad, several variations in the definition have been proposed. David Easdown simplified the definition and formulated the axioms in a special arrow notation invented by him.\n\nThe set of idempotents in a semigroup is a biordered set and every biordered set is the set of idempotents of some semigroup.\nA regular biordered set is a biordered set with an additional property. The set of idempotents in a regular semigroup is a regular biordered set, and every regular biordered set is the set of idempotents of some regular semigroup. \nThe formal definition of a biordered set given by Nambooripad requires some preliminaries. \n\nIf \"X\" and \"Y\" be sets and ρ⊆ \"X\" × \"Y\", let ρ ( \"y\" ) = { \"x\" ∈ \"X\" : \"x\" ρ \"y\" }. \nLet \"E\" be a set in which a partial binary operation, indicated by juxtaposition, is defined. If \"D\" is the domain of the partial binary operation on \"E\" then \"D\" is a relation on \"E\" and (\"e\",\"f\") is in \"D\" if and only if the product \"ef\" exists in \"E\". The following relations can be defined in \"E\":\n\nIf \"T\" is any statement about \"E\" involving the partial binary operation and the above relations in \"E\", one can define the left-right dual of \"T\" denoted by \"T\"*. If \"D\" is symmetric then \"T\"* is meaningful whenever \"T\" is. \n\nThe set \"E\" is called a biordered set if the following axioms and their duals hold for arbitrary elements \"e\", \"f\", \"g\", etc. in \"E\".\n\nIn \"M\" ( \"e\", \"f\" ) = ω ( \"e\" ) ∩ ω ( \"f\" ) (the \"M\"-set of \"e\" and \"f\" in that order), define a relation formula_6 by \n\nThen the set\n\nis called the sandwich set of \"e\" and \"f\" in that order. \n\nWe say that a biordered set \"E\" is an \"M\"-biordered set if \"M\" ( \"e\", \"f\" ) ≠ ∅ for all \"e\" and \"f\" in \"E\". \nAlso, \"E\" is called a regular biordered set if \"S\" ( \"e\", \"f\" ) ≠ ∅ for all \"e\" and \"f\" in \"E\".\n\nIn 2012 Roman S. Gigoń gave a simple proof that \"M\"-biordered sets arise from \"E\"-inversive semigroups.\n\nA subset \"F\" of a biordered set \"E\" is a biordered subset (subboset) of \"E\" if \"F\" is a biordered set under the partial binary operation inherited from \"E\". \n\nFor any \"e\" in \"E\" the sets ω ( \"e\" ), ω ( \"e\" ) and ω ( \"e\" ) are biordered subsets of \"E\".\n\nA mapping φ : \"E\" → \"F\" between two biordered sets \"E\" and \"F\" is a biordered set homomorphism (also called a bimorphism) if for all ( \"e\", \"f\" ) in \"D\" we have ( \"e\"φ ) ( \"f\"φ ) = ( \"ef\" )φ.\n\nLet \"V\" be a vector space and \n\nwhere \"V\" = \"A\" ⊕ \"B\" means that \"A\" and \"B\" are subspaces of \"V\" and \"V\" is the internal direct sum of \"A\" and \"B\". \nThe partial binary operation ⋆ on E defined by \n\nmakes \"E\" a biordered set. The quasiorders in \"E\" are characterised as follows: \n\nThe set \"E\" of idempotents in a semigroup \"S\" becomes a biordered set if a partial binary operation is defined in \"E\" as follows: \"ef\" is defined in \"E\" if and only if \"ef\" = \"e\" or \"ef\"= \"f\" or \"fe\" = \"e\" or \"fe\" = \"f\" holds in \"S\". If \"S\" is a regular semigroup then \"E\" is a regular biordered set.\n\nAs a concrete example, let \"S\" be the semigroup of all mappings of \"X\" = { 1, 2, 3 } into itself. Let the symbol (\"abc\") denote the map for which 1 → \"a\", 2 → \"b\", and 3 → \"c\". The set \"E\" of idempotents in \"S\" contains the following elements:\n\nThe following table (taking composition of mappings in the diagram order) describes the partial binary operation in \"E\". An X in a cell indicates that the corresponding multiplication is not defined. \n"}
{"id": "34008046", "url": "https://en.wikipedia.org/wiki?curid=34008046", "title": "Bojan Mohar", "text": "Bojan Mohar\n\nBojan Mohar is a Slovenian and Canadian mathematician, specializing in graph theory. He is a professor of mathematics at the University of Ljubljana and the holder of a Canada Research Chair in graph theory at Simon Fraser University in Vancouver, British Columbia, Canada.\n\nMohar received his Ph.D. from the University of Ljubljana in 1986, under the supervision of Tomo Pisanski.\n\nMohar's research concerns topological graph theory, algebraic graph theory, graph minors, and graph coloring.\n\nWith Carsten Thomassen he is the co-author of the book \"Graphs on Surfaces\" (Johns Hopkins University Press, 2001).\n\nMohar was a Fulbright visiting scholar at Ohio State University in 1988, and won the Boris Kidrič prize of the Socialist Republic of Slovenia in 1990. He has been a member of the Slovenian Academy of Engineering since 1999.\nHe was named a SIAM Fellow in 2018.\n\n"}
{"id": "2045930", "url": "https://en.wikipedia.org/wiki?curid=2045930", "title": "Chaff algorithm", "text": "Chaff algorithm\n\nChaff is an algorithm for solving instances of the Boolean satisfiability problem in programming. It was designed by researchers at Princeton University, United States. The algorithm is an instance of the DPLL algorithm with a number of enhancements for efficient implementation.\n\nSome available implementations of the algorithm in software are mChaff and zChaff, the latter one being the most widely known and used. zChaff was originally written by Dr. Lintao Zhang, now at Microsoft Research, hence the “z”. It is now maintained by researchers at Princeton University and available for download as both source code and binaries on Linux. zChaff is free for non-commercial use.\n\n\n"}
{"id": "17113364", "url": "https://en.wikipedia.org/wiki?curid=17113364", "title": "Clique cover", "text": "Clique cover\n\nIn graph theory, a clique cover or partition into cliques of a given undirected graph is a partition of the vertices of the graph into cliques, subsets of vertices within which every two vertices are adjacent. A minimum clique cover is a clique cover that uses as few cliques as possible. The minimum \"k\" for which a clique cover exists is called the clique cover number of the given graph.\n\nA clique cover of a graph \"G\" may be seen as a graph coloring of the complement graph of \"G\", the graph on the same vertex set that has edges between non-adjacent vertices of \"G\". Like clique covers, graph colorings are partitions of the set of vertices, but into subsets with no adjacencies (independent sets) rather than cliques. A subset of vertices is a clique in \"G\" if and only if it is an independent set in the complement of \"G\", so a partition of the vertices of \"G\" is a clique cover of \"G\" if and only if it is a coloring of the complement of \"G\".\n\nThe clique cover problem in computational complexity theory is the algorithmic problem of finding a minimum clique cover, or (rephrased as a decision problem) finding a clique cover whose number of cliques is below a given threshold. Finding a minimum clique cover is NP-hard, and its decision version is NP-complete. It was one of Richard Karp's original 21 problems shown NP-complete in his 1972 paper \"Reducibility Among Combinatorial Problems\".\n\nThe equivalence between clique covers and coloring is a reduction that can be used to prove the NP-completeness of the clique cover problem from the known NP-completeness of graph coloring.\n\nPerfect graphs are defined as the graphs in which, for every induced subgraph, the chromatic number (minimum number of colors in a coloring) equals the size of the maximum clique.\nAccording to the weak perfect graph theorem, the complement of a perfect graph is also perfect. Therefore, the perfect graphs are also the graphs in which, for every induced subgraph, the clique cover number equals the size of the maximum independent set. It is possible to compute the clique cover number in perfect graphs in polynomial time.\n\nAnother class of graphs in which the minimum clique cover can be found in polynomial time are the triangle-free graphs. In these graphs, every clique cover consists of a matching (a set of disjoint pairs of adjacent vertices) together with singleton sets for the remaining unmatched vertices. The number of cliques equals the number of vertices minus the number of matched pairs. Therefore, in triangle-free graphs, the minimum clique cover can be found by using an algorithm for maximum matching.\n\nThe optimum partition into cliques can also be found in polynomial time for graphs of bounded clique-width. These include, among other graphs, the cographs and distance-hereditary graphs, which are both also classes of perfect graphs.\n\nThe same hardness of approximation results that are known for graph coloring also apply to clique cover. Therefore, unless P = NP, there can be no polynomial time approximation algorithm for any that, on -vertex graphs, achieves an approximation ratio better than .\n\nIn graphs where every vertex has at most three neighbors, the clique cover remains NP-hard, and there is a constant such that it is NP-hard to approximate with approximation ratio or better. Nevertheless, in polynomial time it is possible to find an approximation with ratio 5/4. That is, this approximation algorithm finds a clique cover whose number of cliques is no more than 5/4 times the optimum.\n\nThe related clique edge cover problem concerns partitioning the edges of a graph, rather than the vertices, into subgraphs induced by cliques. It is also NP-complete.\n"}
{"id": "20644971", "url": "https://en.wikipedia.org/wiki?curid=20644971", "title": "Count On", "text": "Count On\n\nCount On is a major mathematics education project in the United Kingdom which was announced by education secretary David Blunkett at the end of 2000. It was the follow-on to Maths Year 2000 which was the UK's contribution to UNICEF's World Mathematical Year.\n\nCount On had two main strands:\n\nThe MathFests were run largely by MatheMagic and the University of York.\n\nThe project has now been handed over to the NCETM.\n\n\"Count On\" and \"Maths Year 2000\" were some of the first big Popularisation of Mathematics projects. Others are listed below.\n\n"}
{"id": "41007672", "url": "https://en.wikipedia.org/wiki?curid=41007672", "title": "Cours d'Analyse", "text": "Cours d'Analyse\n\nCours d'Analyse de l’École Royale Polytechnique; I.re Partie. Analyse algébrique is a seminal textbook in infinitesimal calculus published by Augustin-Louis Cauchy in 1821. The article follows the translation by Bradley and Sandifer in describing its contents.\n\nOn page 1 of the Introduction, Cauchy writes: \"In speaking of the continuity of functions, I could not dispense with a treatment of the principal properties of infinitely small quantities, properties which serve as the foundation of the infinitesimal calculus.\" The translators comment in a footnote: \"It is interesting that Cauchy does not also mention limits here.\"\n\nCauchy continues: \"As for the methods, I have sought to give them all the rigor which one demands from geometry, so that one need never rely on arguments drawn from the generality of algebra.\"\n\nOn page 6, Cauchy first discusses variable quantities and then introduces the limit notion in the following terms: \"When the values successively attributed to a particular variable indefinitely approach a fixed value in such a way as to end up by differing from it by as little as we wish, this fixed value is called the \"limit\" of all the other values.\"\n\nOn page 7, Cauchy defines an infinitesimal as follows: \"When the successive numerical values of such a variable decrease indefinitely, in such a way as to fall below any given number, this variable becomes what we call \"infinitesimal\", or an \"infinitely small quantity\".\" Cauchy adds: \"A variable of this kind has zero as its limit.\"\n\nOn page 10, Bradley and Sandifer confuse the versed cosine with the coversed sine. Cauchy originally defined the \"sinus versus\" (versine) as siv(\"θ\") = 1-cos(\"θ\") and the \"cosinus versus\" (what is now also known as coversine) as cosiv(\"θ\") = 1-sin(\"θ\"). In the translation, however, the \"cosinus versus\" (and cosiv) are incorrectly associated with the \"versed cosine\" (what is now also known as vercosine) rather than the \"coversed sine\".\n\nThe notation\n\nis introduced on page 12. The translators observe in a footnote: \"The notation “Lim.” for limit was first used by Simon Antoine Jean L'Huilier (1750–1840) in [L’Huilier 1787, p. 31]. Cauchy wrote this as “lim.” in [Cauchy 1821, p. 13]. The period had disappeared by [Cauchy 1897, p. 26].\"\n\nThis chapter has the long title \"On infinitely small and infinitely large quantities, and on the continuity of functions. Singular values of functions in various particular cases.\" On page 21, Cauchy writes: \"We say that a variable quantity becomes \"infinitely small\" when its numerical value decreases indefinitely in such a way as to converge towards the limit zero.\" On the same page, we find the only explicit example of such a variable to be found in Cauchy, namely \nOn page 22, Cauchy starts the discussion of orders of magnitude of infinitesimals as follows: \"Let formula_2 be an infinitely small quantity, that is a variable whose numerical value decreases indefinitely. When the various integer powers of formula_2, namely \nenter into the same calculation, these various powers are called, respectively, infinitely small of the \"first\", the \"second\", the \"third order\", etc. Cauchy notes that \"the general form of infinitely small quantities of order \"n\" (where \"n\" represents an integer number) will be\n\nOn pages 23-25, Cauchy presents eight theorems on properties of infinitesimals of various orders.\n\nThis is entitled \"Continuity of functions\". Cauchy writes: \"If, beginning with a value of \"x\" contained between these limits, we add to the variable \"x\" an\ninfinitely small increment formula_2, the function itself is incremented by the difference \nand states that \nCauchy goes on to provide an italicized definition of continuity in the following terms: \n\nOn page 32 Cauchy states the intermediate value theorem.\n\nIn Theorem I in section 6.1 (page 90 in the translation by Bradley and Sandifer), Cauchy presents the sum theorem in the following terms.\n\n\"When the various terms of series (1) are functions of the same variable x, continuous with respect to this variable in the neighborhood of a particular value for which the series converges, the sum s of the series is also a continuous function of x in the neighborhood of this particular value.\"\n\nHere the series (1) appears on page 86: (1) formula_11\n\n"}
{"id": "252075", "url": "https://en.wikipedia.org/wiki?curid=252075", "title": "De Branges's theorem", "text": "De Branges's theorem\n\nIn complex analysis, de Branges's theorem, or the Bieberbach conjecture, is a theorem that gives a necessary condition on a holomorphic function in order for it to map the open unit disk of the complex plane injectively to the complex plane. It was posed by and finally proven by .\n\nThe statement concerns the Taylor coefficients \"a\" of a univalent function, i.e. a one-to-one holomorphic function that maps the unit disk into the complex plane, normalized as is always possible so that \"a\" = 0 and \"a\" = 1. That is, we consider a function defined on the open unit disk which is holomorphic and injective (\"univalent\") with Taylor series of the form\n\nSuch functions are called \"schlicht\". The theorem then states that\n\nThe Koebe function (see below) is a function in which \"a\" = \"n\" for all \"n\", and it is schlicht, so we cannot find a stricter limit on the absolute value of the \"n\"th coefficient.\n\nThe normalizations\n\nmean that\n\nThis can always be obtained by an affine transformation: starting with an arbitrary injective holomorphic function \"g\" defined on the open unit disk and setting\n\nSuch functions \"g\" are of interest because they appear in the Riemann mapping theorem.\n\nA schlicht function is defined as an analytic function \"f\" that is one-to-one and satisfies \"f\"(0) = 0 and \"f\" '(0) = 1. A family of schlicht functions are the rotated Koebe functions\n\nwith α a complex number of absolute value 1. If \"f\" is a schlicht function and |\"a\"| = \"n\" for some \"n\" ≥ 2, then \"f\" is a rotated Koebe function.\n\nThe condition of de Branges' theorem is not sufficient to show the function is schlicht, as the function\nshows: it is holomorphic on the unit disc and satisfies |\"a\"|≤\"n\" for all \"n\", but it is not injective since \"f\"(−1/2 + \"z\") = \"f\"(−1/2 − \"z\").\n\nA survey of the history is given by Koepf (2007).\n\nThen Charles Loewner () proved |\"a\"| ≤ 3, using the Löwner equation. His work was used by most later attempts, and is also applied in the theory of Schramm–Loewner evolution.\n\nIf \"f\"(\"z\") = \"z\" + ... is a schlicht function then φ(\"z\") = \"f\"(\"z\") is an odd schlicht function. \n\nThe Robertson conjecture states that if\n\nis an odd schlicht function in the unit disk with \"b\"=1 then for all positive integers \"n\", \n\nRobertson observed that his conjecture is still strong enough to imply the Bieberbach conjecture, and proved it for \"n\" = 3. This conjecture introduced the key idea of bounding various quadratic functions of the coefficients rather than the coefficients themselves, which is equivalent to bounding norms of elements in certain Hilbert spaces of schlicht functions.\n\nThere were several proofs of the Bieberbach conjecture for certain higher values of \"n\", in particular proved |\"a\"| ≤ 4, and proved |\"a\"| ≤ 6, and proved |\"a\"| ≤ 5.\n\nThe Milin conjecture states that for each schlicht function on the unit disk, and for all positive integers \"n\", \n\nwhere the logarithmic coefficients γ of \"f\" are given by\n\nFinally proved |\"a\"| ≤ \"n\" for all \"n\".\n\nThe proof uses a type of Hilbert spaces of entire functions. The study of these spaces grew into a sub-field of complex analysis and the spaces have come to be called de Branges spaces. De Branges proved the stronger Milin conjecture on logarithmic coefficients. This was already known to imply the Robertson conjecture about odd univalent functions, which in turn was known to imply the Bieberbach conjecture about schlicht functions . His proof uses the Loewner equation, the Askey–Gasper inequality about Jacobi polynomials, and the Lebedev–Milin inequality on exponentiated power series.\n\nDe Branges reduced the conjecture to some inequalities for Jacobi polynomials, and verified the first few by hand. Walter Gautschi verified more of these inequalities by computer for de Branges (proving the Bieberbach conjecture for the first 30 or so coefficients) and then asked Richard Askey whether he knew of any similar inequalities. Askey pointed out that had proved the necessary inequalities eight years before, which allowed de Branges to complete his proof. The first version was very long and had some minor mistakes which caused some skepticism about it, but these were corrected with the help of members of the Leningrad seminar on Geometric Function Theory (Leningrad Department of Steklov Mathematical Institute) when de Branges visited in 1984.\n\nDe Branges proved the following result, which for ν = 0 implies the Milin conjecture (and therefore the Bieberbach conjecture). \nSuppose that ν > −3/2 and σ are real numbers for positive integers \"n\" with limit 0 and such that\nis non-negative, non-increasing, and has limit 0. Then for all Riemann mapping functions \"F\"(\"z\") = \"z\" + ... univalent in the unit disk with\nthe maximinum value of \nis achieved by the Koebe function \"z\"/(1 − \"z\").\n\nA simplified version of the proof was published in 1985 by Carl FitzGerald and Christian Pommerenke (), and an even shorter description by Jacob Korevaar ().\n\n"}
{"id": "7684340", "url": "https://en.wikipedia.org/wiki?curid=7684340", "title": "Donaldson theory", "text": "Donaldson theory\n\nDonaldson theory is the study of the topology of smooth 4-manifolds using moduli spaces of anti-self-dual instantons. It was started by Simon Donaldson (1983) who proved Donaldson's theorem restricting the possible quadratic forms on the second cohomology group of a compact simply connected 4-manifold. Important consequences of this theorem include the existence of an Exotic R4 and the failure of the smooth h-cobordism theorem in 4 dimensions. The results of Donaldson theory depend therefore on the manifold having a differential structure, and are largely false for topological 4-manifolds. \n\nMany of the theorems in Donaldson theory can now be proved more easily using Seiberg–Witten theory, though there are a number of open problems remaining in Donaldson theory, such as the Witten conjecture and the Atiyah-Floer conjecture.\n\n\n"}
{"id": "35717039", "url": "https://en.wikipedia.org/wiki?curid=35717039", "title": "Double pushout graph rewriting", "text": "Double pushout graph rewriting\n\nIn computer science, double pushout graph rewriting (or DPO graph rewriting) refers to a mathematical framework for graph rewriting. It was introduced as one of the first algebraic approaches to graph rewriting in the article \"Graph-grammars: An algebraic approach\" (1973). It has since been generalized to allow rewriting structures which are not graphs, and to handle negative application conditions, among other extensions.\n\nA DPO graph transformation system (or graph grammar) consists of a finite graph, which is the starting state, and a finite or countable set of labeled spans in the category of finite graphs and graph homomorphisms, which serve as derivation rules. The rule spans are generally taken to be composed of monomorphisms, but the details can vary.\n\nRewriting is performed in two steps: deletion and addition.\n\nAfter a match from the left hand side to formula_1 is fixed, nodes and edges that are not in the right hand side are deleted. The right hand side is then glued in.\n\nGluing graphs is in fact a pushout construction in the category of graphs, and the deletion is the same as finding a pushout complement, hence the name.\n\nDouble pushout graph rewriting allows the specification of graph transformations by specifying a pattern of fixed size and composition to be found and replaced, where part of the pattern can be preserved. The application of a rule is potentially non-deterministic: several distinct matches can be possible. These can be non-overlapping, or share only preserved items, thus showing a kind of concurrency known as parallel independence, or they may be incompatible, in which case either the applications can sometimes be executed sequentially, or one can even preclude the other.\n\nIt can be used as a language for software design and programming (usually a variant working on richer structures than graphs is chosen). Termination for DPO graph rewriting is undecidable because the Post correspondence problem can be reduced to it.\nDPO graph rewriting can be viewed as a generalization of Petri nets.\n\nAxioms have been sought to describe categories in which DPO rewriting will work. One possibility is the notion of an adhesive category, which also enjoys many closure properties. Related notions are HLR systems, quasi-adhesive categories and formula_2-adhesive categories, adhesive HLR categories.\n\nThe concepts of adhesive category and HLR system are related (an adhesive category with coproducts is a HLR system).\n\nHypergraph, typed graph and attributed graph rewriting, for example, can be handled because they can be cast as adhesive HLR systems.\n"}
{"id": "1614212", "url": "https://en.wikipedia.org/wiki?curid=1614212", "title": "Evolution and the Theory of Games", "text": "Evolution and the Theory of Games\n\nEvolution and the Theory of Games is a book by the British evolutionary biologist John Maynard Smith on evolutionary game theory. The book was initially published in December 1982 by Cambridge University Press.\n\nIn the book, John Maynard Smith summarises work on evolutionary game theory that had developed in the 1970s, to which he made several important contributions. The book is also noted for being well written and not overly mathematically challenging.\n\nThe main contribution to be had from this book is the introduction of the Evolutionarily Stable Strategy, or ESS, concept, which states that for a set of behaviours to be conserved over evolutionary time, they must be the most profitable avenue of action when common, so that no alternative behaviour can invade. So, for instance, suppose that in a population of frogs, males fight to the death over breeding ponds. This would be an ESS if any one cowardly frog that does not fight to the death always fares worse (in fitness terms, of course). A more likely scenario is one where fighting to the death is not an ESS because a frog might arise that will stop fighting if it realises that it is going to lose. This frog would then reap the benefits of fighting, but not the ultimate cost. Hence, fighting to the death would easily be invaded by a mutation that causes this sort of \"informed fighting.\" Much complexity can be built from this, and Maynard Smith is outstanding at explaining in clear prose and with simple math.\n\n\n"}
{"id": "1860368", "url": "https://en.wikipedia.org/wiki?curid=1860368", "title": "Feedback vertex set", "text": "Feedback vertex set\n\nIn the mathematical discipline of graph theory, a feedback vertex set of a graph is a set of vertices whose removal leaves a graph without cycles. In other words, each feedback vertex set contains at least one vertex of any cycle in the graph.\nThe feedback vertex set problem is an NP-complete problem in computational complexity theory. It was among the first problems shown to be NP-complete. It has wide applications in operating systems, database systems, and VLSI chip design.\n\nThe decision problem is as follows:\n\nThe graph formula_7 that remains after removing formula_6 from formula_5 is an induced forest (resp. an induced directed acyclic graph in the case of directed graphs). Thus, finding a minimum feedback vertex set in a graph is equivalent to finding a maximum induced forest (resp. maximum induced directed acyclic graph in the case of directed graphs).\n\n showed that the feedback vertex set problem for directed graphs is NP-complete. The problem remains NP-complete on directed graphs with maximum in-degree and out-degree two, and on directed planar graphs with maximum in-degree and out-degree three. Karp's reduction also implies the NP-completeness of the feedback vertex set problem on undirected graphs, where the problem stays NP-hard on graphs of maximum degree four. The feedback vertex set problem can be solved in polynomial time on graphs of maximum degree at most three.\n\nNote that the problem of deleting as few \"edges\" as possible to make the graph cycle-free is equivalent to finding a spanning tree, which can be done in polynomial time. In contrast, the problem of deleting edges from a directed graph to make it acyclic, the feedback arc set problem, is NP-complete.\n\nThe corresponding NP optimization problem of finding the size of a minimum feedback vertex set can be solved in time \"O\"(1.7347), where \"n\" is the number of vertices in the graph. This algorithm actually computes a maximum induced forest, and when such a forest is obtained, its complement is a minimum feedback vertex set. The number of minimal feedback vertex sets in a graph is bounded by \"O\"(1.8638). The directed feedback vertex set problem can still be solved in time \"O*\"(1.9977), where \"n\" is the number of vertices in the given directed graph. The parameterized versions of the directed and undirected problems are both fixed-parameter tractable.\n\nIn undirected graphs of maximum degree three, the feedback vertex set problem can be solved in polynomial time, by transforming it into an instance of the matroid parity problem for linear matroids.\n\nThe problem is APX-complete, which directly follows from the APX-completeness of the vertex cover problem, and the existence of an approximation preserving L-reduction from the vertex cover problem to it. The best known approximation algorithm on undirected graphs is by a factor of two.\n\nAccording to the Erdős–Pósa theorem, the size of a minimum feedback vertex set is within a logarithmic factor of the maximum number of vertex-disjoint cycles in the given graph.\n\nIn operating systems, feedback vertex sets play a prominent role in the study of deadlock recovery. In the wait-for graph of an operating system, each directed cycle corresponds to a deadlock situation. In order to resolve all deadlocks, some blocked processes have to be aborted. A minimum feedback vertex set in this graph corresponds to a minimum number of processes that one needs to abort.\n\nFurthermore, the feedback vertex set problem has applications in VLSI chip design.\n\n"}
{"id": "40761970", "url": "https://en.wikipedia.org/wiki?curid=40761970", "title": "Ferdinand Rudio", "text": "Ferdinand Rudio\n\nFerdinand Rudio (born 2 August 1856 in Wiesbaden, died 21 June 1929 in Zurich) was a German and Swiss mathematician and historian of mathematics.\n\nRudio's father and maternal grandfather were both public officials in the independent Duchy of Nassau, which was annexed by Prussia when Rudio was 10. He was educated at the local gymnasium and Realgymnasium in Wiesbaden, and then in 1874 began studying at ETH Zurich, then known as the Eidgenössische Polytechnikum Zürich. His initial courses in Zurich were in civil engineering, but in his second year (under the influence of Karl Geiser) he switched to mathematics and physics. Finishing at Zurich in 1877, he went on to graduate studies at the University of Berlin from 1877 to 1880, earning his Ph.D. under the joint supervision of Ernst Kummer and Karl Weierstrass.\nNext, Rudio returned to ETH Zurich, earning his habilitation in 1881 and becoming at that time a privatdozent. He became an extraordinary professor at Zurich in 1885, and a full professor in 1889.\n\nRudio was one of the organizers of the first International Congress of Mathematicians (ICM) in 1897. He served as General Secretary of the congress, and as editor of the proceedings of the congress. He was the editor of the quarterly journal of the Zürich Natural Sciences Society from 1893 until 1912, and was also president of the society.\n\nIn 1919, the University of Zurich gave Rudio an honorary doctorate. By 1928, he was in poor health, and retired from his position at Zurich. He died a year later.\n\nRudio's research ranged over group theory, abstract algebra, and geometry. His\nthesis research concerned the use of differential equations to characterize surface by the properties of their sets of centers of curvature, and he was also known for the first proof of convergence of Viète's infinite product for π. He also authored the textbook \"Die Elemente Der Analytischen Geometrie\", in analytic geometry, published in 1908.\n\nBeginning in 1883, with a speech Rudio gave at a celebration of the centennial of Leonhard Euler's death, Rudio became interested in Euler's life and works. At the first ICM and again at a celebration in 1907 of Euler's 200th birthday, Rudio urged the compilation of a set of Euler's complete works. In 1909 the Swiss Society of Natural Sciences took up the project and appointed Rudio as editor. He finished two volumes of this project, and assisted in the editing of the next three. He gave a talk \"Mitteilungen über die Eulerausgabe\" (news about the Euler edition) at the fifth ICM in Cambridge, England in August 1912. By the time he retired as general editor of the series in 1928, 20 volumes of the series had been published of what would eventually be over 80 volumes.\n\nOther work in the history of mathematics by Rudio included the book \"Der Bericht des Simplicius über die Quadraturen des Antiphon und des Hippokrates\" (1902) on the ancient problem of squaring the circle, and a collection of biographies of mathematicians including Gotthold Eisenstein.\n"}
{"id": "932515", "url": "https://en.wikipedia.org/wiki?curid=932515", "title": "Hermann Hankel", "text": "Hermann Hankel\n\nHermann Hankel (14 February 1839 – 29 August 1873) was a German mathematician who was born in Halle, Germany and died in Schramberg (Black Forest), Imperial Germany.\n\nHe studied and worked with, among others, Möbius, Riemann, Weierstrass and Kronecker.\n\nHis 1867 exposition on complex numbers and quaternions is particularly memorable. For example, Fischbein notes that he solved the problem of products of negative numbers by proving the following theorem: \"The only multiplication in R which may be considered as an extension of the usual multiplication in R by \"respecting the law of distributivity\" to the left and the right is that which conforms to the rule of signs.\"\nFurthermore, Hankel draws attention to the linear algebra that Hermann Grassmann had developed in his \"Extension Theory\" in two publications. This was the first of many references later made to Grassmann's early insights on the nature of space.\n\n\n\n\n"}
{"id": "39585389", "url": "https://en.wikipedia.org/wiki?curid=39585389", "title": "Higher-order compact finite difference scheme", "text": "Higher-order compact finite difference scheme\n\nHigh-order compact finite difference schemes are used for solving third-order differential equations created during the study of obstacle boundary value problems. They have been shown to be highly accurate and efficient. They are constructed by modifying the second-order scheme that was developed by Noor and Al-Said in 2002. The convergence rate of the high-order compact scheme is third order, the second-order scheme is fourth order.\n\nDifferential equations are essential tools in mathematical modelling. Most physical systems are described in terms of mathematical models that include convective and diffusive transport of some variables. Finite difference methods are amongst the most popular methods that have been applied most frequently in solving such differential equations. A finite difference scheme is compact in the sense that the discretised formula comprises at most nine point stencils which includes a node in the middle about which differences are taken. In addition, greater order of accuracy (more than two) justifies the terminology 'higher-order compact finite difference scheme' (HOC). This can be achieved in several ways. The higher-order compact scheme considered here is by using the original differential equation to substitute for the leading truncation error terms in the finite difference equation. Overall, the scheme is found to be robust, efficient and accurate for most computational fluid dynamics (CFD) applications discussed here further.\n\nThe simplest problem for the validation of the numerical algorithms is the Lid Driven cavity problem. Computed results in form of tables, graphs and figures for a fluid with Prandtl number = 0.71 with Rayleigh number (Ra) ranging from 10 to 10 are available in the literature. The efficacy of the scheme is proved when it very clearly captures the secondary and tertiary vortices at the sides of the cavity at high values of Ra.\n\nAnother milestone was the development of these schemes for solving two dimensional steady/unsteady convection diffusion equations. A comprehensive study of flow past an impulsively started circular cylinder was made. The problem of flow past a circular cylinder has continued to generate tremendous interest amongst researchers working in CFD mainly because it displays almost all the fluid mechanical phenomena for incompressible, viscous flows in the simplest of geometrical settings. It was able to analyze and visualize the flow patterns more accurately for Reynold's number (Re) ranging from 10 to 9500 compared to the existing numerical results. This was followed by its extension to rotating counterpart of the cylinder surface for Re ranging from 200 to 1000. More complex phenomenon that involves a circular cylinder undergoing rotational oscillations while translating in a fluid is studied for Re as high as 500. \n\nAnother benchmark in the history is its extension to multiphase flow phenomena. Natural processes such as gas bubble in oil, ice melting, wet steam are observed everywhere in nature. Such processes also play an important role with the practical applications in the area of biology, medicine, environmental remediation. The scheme has been successively implemented to solve one and two dimensional elliptic and parabolic equation with discontinuous coefficients and singular source terms. These type of problems hold importance numerically because they usually lead to non-smooth or discontinuous solutions across the interfaces. Expansion of this idea from fixed to moving interfaces with both regular and irregular geometries is currently going on , .\n"}
{"id": "55711992", "url": "https://en.wikipedia.org/wiki?curid=55711992", "title": "Hélène Barcelo", "text": "Hélène Barcelo\n\nHélène Barcelo (born 1954) is a mathematician from Québec specializing in algebraic combinatorics. Within that field, her interests include combinatorial representation theory, homotopy theory, and arrangements of hyperplanes.\nShe is a professor emeritus of mathematics at Arizona State University, and deputy director of the Mathematical Sciences Research Institute (MSRI). She was editor-in-chief of the \"Journal of Combinatorial Theory\", Series A, from 2001 to 2009.\n\nBarcelo completed her Ph.D. from the University of California, San Diego in 1988. Her dissertation, \"On the Action of the Symmetric Group on the Free Lie Algebra and on the Homology and Cohomology of the Partition Lattice\", was supervised by Adriano Garsia. She joined the Arizona State faculty after postdoctoral studies at the University of Michigan. She retired from Arizona State, becoming a professor emerita there, and became deputy director at MSRI in 2008.\n\nShe was elected to the 2018 class of fellows of the American Mathematical Society\nand the 2019 class of fellows of the Association for Women in Mathematics.\n\n"}
{"id": "8517337", "url": "https://en.wikipedia.org/wiki?curid=8517337", "title": "Incomplete LU factorization", "text": "Incomplete LU factorization\n\nIn numerical linear algebra, an incomplete LU factorization (abbreviated as ILU) of a matrix is a sparse approximation of the LU factorization often used as a preconditioner.\n\nConsider a sparse linear system formula_1. These are often solved by computing the factorization formula_2, with \"L\" lower unitriangular and \"U\" upper triangular.\nOne then solves formula_3, formula_4, which can be done efficiently because the matrices are triangular.\n\nFor a typical sparse matrix, the LU factors can be much less sparse than the original matrix — a phenomenon called \"fill-in\". \nThe memory requirements for using a direct solver can then become a bottleneck in solving linear systems. One can combat this problem by using fill-reducing reorderings of the matrix's unknowns, such as the Cuthill-McKee ordering.\n\nAn incomplete factorization instead seeks triangular matrices \"L\", \"U\" such that formula_5 rather than formula_2. Solving for formula_7 can be done quickly but does not yield the exact solution to formula_1. So, we instead use the matrix formula_9 as a preconditioner in another iterative solution algorithm such as the conjugate gradient method or GMRES.\n\nFor a given matrix formula_10 one defines the graph formula_11 as\nwhich is used to define the conditions a \"sparsity patterns\" formula_13 needs to fulfill\n\nA decomposition of the form formula_15 where the following hold\nis called an incomplete LU decomposition (w.r.t. the sparsity pattern formula_13).\n\nThe sparsity pattern of \"L\" and \"U\" is often chosen to be the same as the sparsity pattern of the original matrix \"A\". If the underlying matrix structure can be referenced by pointers instead of copied, the only extra memory required is for the entries of \"L\" and \"U\". This preconditioner is called ILU(0).\n\nConcerning the stability of the ILU the following theorem was proven by Meijerink an van der Vorst.\n\nLet formula_23 be an M-matrix, the (complete) LU decomposition given by formula_24, and the ILU by formula_25.\nThen\nholds.\nThus, the ILU is at least as stable as the (complete) LU decomposition.\n\nOne can obtain a more accurate preconditioner by allowing some level of extra fill in the factorization. A common choice is to use the sparsity pattern of \"A\" instead of \"A\"; this matrix is appreciably more dense than \"A\", but still sparse over all. This preconditioner is called ILU(1). One can then generalize this procedure; the ILU(k) preconditioner of a matrix \"A\" is the incomplete LU factorization with the sparsity pattern of the matrix \"A\".\n\nMore accurate ILU preconditioners require more memory, to such an extent that eventually the running time of the algorithm increases even though the total number of iterations decreases. Consequently, there is a cost/accuracy trade-off that users must evaluate, typically on a case-by-case basis depending on the family of linear systems to be solved.\n\nThe ILU factorization can be performed as a fixed-point iteration in a highly parallel way.\n\n\n\n"}
{"id": "22833480", "url": "https://en.wikipedia.org/wiki?curid=22833480", "title": "Independence of premise", "text": "Independence of premise\n\nIn proof theory and constructive mathematics, the principle of independence of premise states that if φ and ∃ \"x\" θ are sentences in a formal theory and is provable, then is provable. Here \"x\" cannot be a free variable of φ.\n\nThe principle is valid in classical logic. Its main application is in the study of intuitionistic logic, where the principle is not always valid.\n\nThe principle of independence of premise is valid in classical logic because of the law of the excluded middle. Assume that is provable. Then, if φ holds, there is an \"x\" satisfying φ → θ but if φ does not hold then \"any\" \"x\" satisfies φ → θ. In either case, there is some \"x\" such that φ→θ. Thus is provable.\n\nThe principle of independence of premise is not generally valid in intuitionistic logic (Avigad and Feferman 1999). This can be illustrated by the BHK interpretation, which says that in order to prove intuitionistically, one must create a function that takes a proof of φ and returns a proof of . Here the proof itself is an input to the function and may be used to construct \"x\". On the other hand, a proof of must first demonstrate a particular \"x\", and then provide a function that converts a proof of φ into a proof of θ in which \"x\" has that particular value.\n\nAs a weak counterexample, suppose θ(\"x\") is some decidable predicate of a natural number such that it is not known whether any \"x\" satisfies θ. For example, θ may say that \"x\" is a formal proof of some mathematical conjecture whose provability is not known. Let φ the formula . Then is trivially provable. However, to prove , one must demonstrate a particular value of \"x\" such that, if any value of \"x\" satisfies θ, then the one that was chosen satisfies θ. This cannot be done without already knowing whether holds, and thus is not intuitionistically provable in this situation.\n"}
{"id": "8487966", "url": "https://en.wikipedia.org/wiki?curid=8487966", "title": "John Morgan (mathematician)", "text": "John Morgan (mathematician)\n\nJohn Willard Morgan (born March 21, 1946) is an American mathematician, with contributions to topology and geometry.\n\nHe received his B.A. in 1968 and Ph.D. in 1969, both from Rice University. His Ph.D. thesis, entitled \"Stable tangential homotopy equivalences\", was written under the supervision of Morton L. Curtis. He was an instructor at Princeton University from 1969 to 1972, and an assistant professor at MIT from 1972 to 1974. He has been on the faculty at Columbia University since 1974. In July 2009, he moved to Stony Brook University to become the first director of the Simons Center for Geometry and Physics, a research center devoted to the interface between mathematics and physics.\n\nHe is an editor of the Journal of the American Mathematical Society and Geometry and Topology.\n\nHe collaborated with Gang Tian in verifying Grigori Perelman's proof of the Poincaré conjecture. The Morgan–Tian team was one of three teams formed for this purpose; the other teams were those of Huai-Dong Cao and Xi-Ping Zhu, and Bruce Kleiner and John Lott. Morgan gave a plenary lecture at the International Congress of Mathematicians in Madrid on August 24, 2006, declaring that \"in 2003, Perelman solved the Poincaré conjecture.\"\n\nIn 2008 he was awarded a Gauss Lectureship by the German Mathematical Society. In 2009 he was elected to the National Academy of Sciences. In 2012 he became a fellow of the American Mathematical Society.\n\n\n\n"}
{"id": "9042378", "url": "https://en.wikipedia.org/wiki?curid=9042378", "title": "Kleene–Brouwer order", "text": "Kleene–Brouwer order\n\nIn descriptive set theory, the Kleene–Brouwer order or Lusin–Sierpiński order is a linear order on finite sequences over some linearly ordered set formula_1, that differs from the more commonly used lexicographic order in how it handles the case when one sequence is a prefix of the other. In the Kleene–Brouwer order, the prefix is later than the longer sequence containing it, rather than earlier.\n\nThe Kleene–Brouwer order generalizes the notion of a postorder traversal from finite trees to trees that are not necessarily finite. For trees over a well-ordered set, the Kleene–Brouwer order is itself a well-ordering if and only if the tree has no infinite branch. It is named after Stephen Cole Kleene, Luitzen Egbertus Jan Brouwer, Nikolai Luzin, and Wacław Sierpiński.\n\nIf formula_2 and formula_3 are finite sequences of elements from formula_4, we say that formula_5 when there is an formula_6 such that either:\nHere, the notation formula_15 refers to the prefix of formula_2 up to but not including formula_8.\nIn simple terms, formula_5 whenever formula_3 is a prefix of formula_2 (i.e. formula_3 terminates before formula_2, and they are equal up to that point) or formula_2 is to the \"left\" of formula_3 on the first place they differ.\n\nA tree, in descriptive set theory, is defined as a set of finite sequences that is closed under prefix operations. The parent in the tree of any sequence is the shorter sequence formed by removing its final element. Thus, any set of finite sequences can be augmented to form a tree, and the Kleene–Brouwer order is a natural ordering that may be given to this tree. It is a generalization to potentially-infinite trees of the postorder traversal of a finite tree: at every node of the tree, the child subtrees are given their left to right ordering, and the node itself comes after all its children. The fact that the Kleene–Brouwer order is a linear ordering (that is, that it is transitive as well as being total) follows immediately from this, as any three sequences on which transitivity is to be tested form (with their prefixes) a finite tree on which the Kleene–Brouwer order coincides with the postorder.\n\nThe significance of the Kleene–Brouwer ordering comes from the fact that if formula_4 is well-ordered, then a tree over formula_4 is well-founded (having no infinitely long branches) if and only if the Kleene–Brouwer ordering is a well-ordering of the elements of the tree.\n\nIn recursion theory, the Kleene–Brouwer order may be applied to the computation trees of implementations of total recursive functionals. A computation tree is well-founded if and only if the computation performed by it is total recursive. Each state formula_27 in a computation tree may be assigned an ordinal number formula_28, the supremum of the ordinal numbers formula_29 where formula_30 ranges over the children of formula_27 in the tree. In this way, the total recursive functionals themselves can be classified into a hierarchy, according to the minimum value of the ordinal at the root of a computation tree, minimized over all computation trees that implement the functional. The Kleene–Brouwer order of a well-founded computation tree is itself a recursive well-ordering, and at least as large as the ordinal assigned to the tree, from which it follows that the levels of this hierarchy are indexed by recursive ordinals.\n\nThis ordering was used by , and then again by . Brouwer does not cite any references, but Moschovakis argues that he may either have seen , or have been influenced by earlier work of the same authors leading to this work. Much later, studied the same ordering, and credited it to Brouwer.\n"}
{"id": "56357", "url": "https://en.wikipedia.org/wiki?curid=56357", "title": "Linear subspace", "text": "Linear subspace\n\nIn linear algebra and related fields of mathematics, a linear subspace, also known as a vector subspace, or, in the older literature, a linear manifold, is a vector space that is a subset of some other (higher-dimension) vector space. A linear subspace is usually called simply a \"subspace\" when the context serves to distinguish it from other kinds of subspace.\n\nLet \"K\" be a field (such as the real numbers), \"V\" be a vector space over \"K\", and let \"W\" be a subset of \"V\".\nThen \"W\" is a subspace if:\n\nExample I:\nLet the field \"K\" be the set R of real numbers, and let the vector space \"V\" be the real coordinate space R.\nTake \"W\" to be the set of all vectors in \"V\" whose last component is 0.\nThen \"W\" is a subspace of \"V\".\n\n\"Proof:\"\n\nExample II:\nLet the field be R again, but now let the vector space be the Cartesian plane R.\nTake \"W\" to be the set of points (\"x\", \"y\") of R such that \"x\" = \"y\".\nThen \"W\" is a subspace of R.\n\n\"Proof:\"\n\nIn general, any subset of the real coordinate space R that is defined by a system of homogeneous linear equations will yield a subspace.\nGeometrically, these subspaces are points, lines, planes, and so on, that pass through the point 0.\n\nExample III:\nAgain take the field to be R, but now let the vector space \"V\" be the set R of all functions from R to R.\nLet C(R) be the subset consisting of continuous functions.\nThen C(R) is a subspace of R.\n\n\"Proof:\"\n\nExample IV:\nKeep the same field and vector space as before, but now consider the set Diff(R) of all differentiable functions.\nThe same sort of argument as before shows that this is a subspace too.\n\nExamples that extend these themes are common in functional analysis.\n\nA way to characterize subspaces is that they are closed under linear combinations.\nThat is, a nonempty set \"W\" is a subspace if and only if every linear combination of (finitely many) elements of \"W\" also belongs to \"W\".\nConditions 2 and 3 for a subspace are simply the most basic kinds of linear combinations.\n\nIn a topological vector space \"X\", a subspace \"W\" need not be closed in general, but a finite-dimensional subspace is always closed. The same is true for subspaces of finite codimension, i.e. determined by a finite number of continuous linear functionals.\n\nDescriptions of subspaces include the solution set to a homogeneous system of linear equations, the subset of Euclidean space described by a system of homogeneous linear parametric equations, the span of a collection of vectors, and the null space, column space, and row space of a matrix. Geometrically (especially, over the field of real numbers and its subfields), a subspace is a flat in an \"n\"-space that passes through the origin.\n\nA natural description of an 1-subspace is the scalar multiplication of one non-zero vector v to all possible scalar values. 1-subspaces specified by two vectors are equal if and only if one vector can be obtained from another with scalar multiplication:\nThis idea is generalized for higher dimensions with linear span, but criteria for equality of \"k\"-spaces specified by sets of \"k\" vectors are not so simple.\n\nA dual description is provided with linear functionals (usually implemented as linear equations). One non-zero linear functional F specifies its kernel subspace F = 0 of codimension 1. Subspaces of codimension 1 specified by two linear functionals are equal if and only if one functional can be obtained from another with scalar multiplication (in the dual space):\nIt is generalized for higher codimensions with a system of equations. The following two subsections will present this latter description in details, and the remaining four subsections further describe the idea of linear span.\n\nThe solution set to any homogeneous system of linear equations with \"n\" variables is a subspace in the coordinate space \"K\":\n\nFor example (over real or rational numbers), the set of all vectors (\"x\", \"y\", \"z\") satisfying the equations\n\nis a one-dimensional subspace. More generally, that is to say that given a set of \"n\" independent functions, the dimension of the subspace in \"K\" will be the dimension of the null set of \"A\", the composite matrix of the \"n\" functions.\n\nIn a finite-dimensional space, a homogeneous system of linear equations can be written as a single matrix equation:\n\nThe set of solutions to this equation is known as the null space of the matrix. For example, the subspace described above is the null space of the matrix\n\nEvery subspace of \"K\" can be described as the null space of some matrix (see algorithms, below).\n\nThe subset of \"K\" described by a system of homogeneous linear parametric equations is a subspace:\n\nFor example, the set of all vectors (\"x\", \"y\", \"z\") parameterized by the equations\n\nis a two-dimensional subspace of \"K\", if \"K\" is a number field (such as real or rational numbers).\n\nIn linear algebra, the system of parametric equations can be written as a single vector equation:\n\nThe expression on the right is called a linear combination of the vectors\n(2, 5, −1) and (3, −4, 2). These two vectors are said to span the resulting subspace.\n\nIn general, a linear combination of vectors v, v, ... , v is any vector of the form\n\nThe set of all possible linear combinations is called the span:\n\nIf the vectors v, ... , v have \"n\" components, then their span is a subspace of \"K\". Geometrically, the span is the flat through the origin in \"n\"-dimensional space determined by the points v, ... , v.\n\n\nA system of linear parametric equations in a finite-dimensional space can also be written as a single matrix equation:\n\nIn this case, the subspace consists of all possible values of the vector x. In linear algebra, this subspace is known as the column space (or image) of the matrix \"A\". It is precisely the subspace of \"K\" spanned by the column vectors of \"A\".\n\nThe row space of a matrix is the subspace spanned by its row vectors. The row space is interesting because it is the orthogonal complement of the null space (see below).\n\nIn general, a subspace of \"K\" determined by \"k\" parameters (or spanned by \"k\" vectors) has dimension \"k\". However, there are exceptions to this rule. For example, the subspace of \"K\" spanned by the three vectors (1, 0, 0), (0, 0, 1), and (2, 0, 3) is just the \"xz\"-plane, with each point on the plane described by infinitely many different values of .\n\nIn general, vectors v, ... , v are called linearly independent if\n\nfor\n(\"t\", \"t\", ... , \"t\") ≠ (\"u\", \"u\", ... , \"u\").\nIf are linearly independent, then the coordinates for a vector in the span are uniquely determined.\n\nA basis for a subspace \"S\" is a set of linearly independent vectors whose span is \"S\". The number of elements in a basis is always equal to the geometric dimension of the subspace. Any spanning set for a subspace can be changed into a basis by removing redundant vectors (see algorithms, below).\n\n\nThe set-theoretical inclusion binary relation specifies a partial order on the set of all subspaces (of any dimension).\n\nA subspace cannot lie in any subspace of lesser dimension. If dim \"U\" = \"k\", a finite number, and \"U\" ⊂ \"W\", then dim \"W\" = \"k\" if and only if \"U\" = \"W\".\n\nGiven subspaces \"U\" and \"W\" of a vector space \"V\", then their intersection \"U\" ∩ \"W\" := {v ∈ \"V\" : v is an element of both \"U\" and \"W\"} is also a subspace of \"V\".\n\n\"Proof:\"\n\nFor every vector space \"V\", the set {0} and \"V\" itself are subspaces of \"V\".\n\nIf \"U\" and \"W\" are subspaces, their sum is the subspace\n\nFor example, the sum of two lines is the plane that contains them both. The dimension of the sum satisfies the inequality\n\nHere the minimum only occurs if one subspace is contained in the other, while the maximum is the most general case. The dimension of the intersection and the sum are related:\n\nThe operations intersection and sum make the set of all subspaces a bounded modular lattice, where the {0} subspace, the least element, is an identity element of the sum operation, and the identical subspace \"V\", the greatest element, is an identity element of the intersection operation.\n\nIf \"V\" is an inner product space, then the orthogonal complement ⊥ of any subspace of \"V\" is again a subspace. This operation, understood as negation (¬), makes the lattice of subspaces a (possibly infinite) orthocomplemented lattice (it is not a distributive lattice).\n\nIn a pseudo-Euclidean space there are orthogonal complements too, but such operation does not form a Boolean algebra (nor a Heyting algebra) because of null subspaces, for which The same case presents the operation in symplectic vector spaces.\n\nMost algorithms for dealing with subspaces involve row reduction. This is the process of applying elementary row operations to a matrix until it reaches either row echelon form or reduced row echelon form. Row reduction has the following important properties:\n\nSee the article on row space for an example.\n\nIf we instead put the matrix \"A\" into reduced row echelon form, then the resulting basis for the row space is uniquely determined. This provides an algorithm for checking whether two row spaces are equal and, by extension, whether two subspaces of \"K\" are equal.\n\nSee the article on column space for an example.\n\nThis produces a basis for the column space that is a subset of the original column vectors. It works because the columns with pivots are a basis for the column space of the echelon form, and row reduction does not change the linear dependence relationships between the columns.\n\nIf the final column of the reduced row echelon form contains a pivot, then the input vector v does not lie in \"S\".\n\nSee the article on null space for an example.\n\nGiven two subspaces and of , a basis of the sum formula_21 and the intersection formula_22 can be calculated using the Zassenhaus algorithm\n\n\n\n\n"}
{"id": "5921376", "url": "https://en.wikipedia.org/wiki?curid=5921376", "title": "Mathematical operators and symbols in Unicode", "text": "Mathematical operators and symbols in Unicode\n\nThe Unicode Standard encodes almost all standard characters used in mathematics.\nUnicode Technical Report #25 provides comprehensive information about the character repertoire, their properties, and guidelines for implementation.\nMathematical operators and symbols are in multiple Unicode blocks. Some of these blocks are dedicated to, or primarily contain, mathematical characters while others are a mix of mathematical and non-mathematical characters. This article covers all Unicode characters with a derived property of \"Math\".\n\nThe Mathematical Operators block (U+2200–U+22FF) contains characters for mathematical, logical, and set notation.\n\nThe Supplemental Mathematical Operators block (U+2A00–U+2AFF) contains various mathematical symbols, including N-ary operators, summations and integrals, intersections and unions, logical and relational operators, and subset/superset relations.\n\nThe Mathematical Alphanumeric Symbols block (U+1D400–U+1D7FF) contains Latin and Greek letters and decimal digits that enable mathematicians to denote different notions with different letter styles. The \"holes\" in the alphabetic ranges are filled by previously defined characters in the Letter like Symbols block shown below.\n\nThe Letterlike Symbols block (U+2100–U+214F) includes variables. Most alphabetic math symbols are in the Mathematical Alphanumeric Symbols block shown above.\n\nThe math subset of this block is U+2102, U+2107, U+210A–U+2113, U+2115, U+2118–U+2119, U+2124, U+2128–U+2129, U+212C, U+212F, U+2133, U+2135, U+213C–U+2149, and U+214B.\n\nThe Miscellaneous Mathematical Symbols-A block (U+27C0–U+27EF) contains characters for mathematical, logical, and database notation.\n\nThe Miscellaneous Mathematical Symbols-B block (U+2980–U+29FF) contains miscellaneous mathematical symbols, including brackets, angles, and circle symbols.\n\nThe Miscellaneous Technical block (U+2300–U+23FF) includes braces and operators.\n\nThe math subset of this block is U+2308–U+230B, U+2320-U+2321, U+237C, U+239B-U+23B5, 23B7, U+23D0, and U+23DC-U+23E2.\n\nThe Geometric Shapes block (U+25A0–U+25FF) contains geometric shape symbols.\n\nThe math subset of this block is U+25A0–25A1, U+25AE–25B7, U+25BC–25C1, U+25C6–25C7, U+25CA–25CB, U+25CF–25D3, U+25E2, U+25E4, U+25E7–25EC, and U+25F8–25FF.\n\nThe Miscellaneous Symbols and Arrows block (U+2B00–U+2BFF Arrows) contains arrows and geometric shapes with various fills.\n\nThe math subset of this block is U+2B30–2B44 and U+2B47–2B4C.\n\nThe Arrows block (U+2190–U+21FF) contains line, curve, and semicircle arrows and arrow-like operators.\n\nThe Supplemental Arrows-A block (U+27F0–U+27FF) contains arrows and arrow-like operators.\n\nThe Supplemental Arrows-B block (U+2900–U+297F) contains arrows and arrow-like operators (arrow tails, crossing arrows, curved arrows, and harpoons).\n\nThe Combining Diacritical Marks for Symbols block contains arrows, dots, enclosures, and overlays for modifying symbol characters.\n\nThe math subset of this block is U+20D0–U+20DC, U+20E1, U+20E5–U+20E6, and U+20EB–U+20EF.\n\nThe Arabic Mathematical Alphabetic Symbols block (U+1EE00–U+1EEFF) contains characters used in Arabic mathematical expressions.\nMathematical characters also appear in other blocks. Below is a list of these characters as of Unicode version 11.0:\n\n\n"}
{"id": "1543735", "url": "https://en.wikipedia.org/wiki?curid=1543735", "title": "Matrix norm", "text": "Matrix norm\n\nIn mathematics, a matrix norm is a vector norm in a vector space whose elements (vectors) are matrices (of given dimensions).\n\nIn what follows, formula_1 will denote a field of either real or complex numbers. \n\nLet formula_2 denote the vector space of all matrices of size formula_3 (with formula_4 rows and formula_5 columns) with entries in the field formula_1.\n\nA matrix norm is a norm on the vector space formula_2. Thus, the matrix norm is a function formula_8 that must satisfy the following properties: \n\nFor all scalars formula_9 in formula_1 and for all matrices formula_11 and formula_12 in formula_2,\n\n\nAdditionally, in the case of square matrices (thus, ), some (but not all) matrix norms satisfy the following condition, which is related to the fact that matrices are more than just vectors:\n\n\nA matrix norm that satisfies this additional property is called a sub-multiplicative norm (in some books, the terminology \"matrix norm\" is used only for those norms which are sub-multiplicative). The set of all formula_23 matrices, together with such a sub-multiplicative norm, is an example of a Banach algebra.\n\nThe definition of sub-multiplicativity is sometimes extended to non-square matrices, for instance in the case of the induced \"p\"-norm, where for formula_24 and formula_25 holds that formula_26. Here formula_27 and formula_28 are the norms induced from formula_29 and formula_30, respectively, and .\n\nThere are three types of matrix norms which will be discussed below:\n\nSuppose a vector norm formula_31 on formula_32 is given (for a field). Any formula_3 matrix induces a linear operator from formula_34 to formula_35 with respect to the standard basis, and one defines the corresponding \"induced norm\" or \"operator norm\" on the space formula_2 of all formula_3 matrices as follows:\n\nIn particular, if the \"p\"-norm for vectors () is used for both spaces formula_34 and formula_35,\nthen the corresponding induced operator norm is:\n\nThese induced norms are different from the \"entrywise\" \"p\"-norms and the Schatten \"p\"-norms for matrices treated below, which are also usually denoted by formula_42 \n\nAny induced operator norm is a sub-multiplicative matrix norm: formula_54; this follows from\nand formula_56 \n\nMoreover, any induced norm satisfies the inequality\n\nwhere is the spectral radius of . For symmetric or hermitian , we have equality in () for the 2-norm, since in this case the 2-norm \"is\" precisely the spectral radius of . For an arbitrary matrix, we may not have equality for any norm; a counterexample being given by formula_57, which has vanishing spectral radius. In any case, for square matrices\nwe have the spectral radius formula:\n\nIn the special cases of formula_59 the induced matrix norms can be computed or estimated by\n\nwhich is simply the maximum absolute column sum of the matrix;\n\nwhich is simply the maximum absolute row sum of the matrix;\n\nwhere in left hand side formula_63 represents the largest singular value of matrix formula_11, and on the right hand side formula_65 is the Frobenius norm. The first inequality can be derived from the fact that the trace of a matrix is equal to the sum of its eigenvalues. The equality holds if and only if the matrix formula_11 is a rank-one matrix or a zero matrix. \n\nFor example, if the matrix formula_11 is defined by\n\nthen we have\n\nand\n\nIn the special case of formula_71 (the Euclidean norm or formula_72-norm for vectors), the induced matrix norm is the \"spectral norm\". \nThe spectral norm of a matrix formula_11 is the largest singular value of formula_11 i.e. the square root of the largest eigenvalue of the positive-semidefinite matrix formula_75:\n\nThese norms treat an formula_79 matrix as a vector of size formula_80, and \nuse one of the familiar vector norms.\n\nFor example, using the \"p\"-norm for vectors, , we get:\n\nThis is a different norm from the induced \"p\"-norm (see above) and the Schatten \"p\"-norm (see below), but the notation is the same.\n\nThe special case \"p\" = 2 is the Frobenius norm, and \"p\" = ∞ yields the maximum norm.\n\nLet formula_82 be the columns of matrix formula_11. The formula_84 norm is the sum of the Euclidean norms of the columns of the matrix:\n\nThe formula_84 norm as an error function is more robust\nsince the error for each data point (a column) is not squared. It is used in robust data analysis and sparse coding.\n\nThe formula_84 norm can be generalized to the formula_88 norm, , defined by \n\nWhen for the formula_88 norm, it is called the Frobenius norm or the Hilbert–Schmidt norm, though the latter term is used more frequently in the context of operators on (possibly infinite-dimensional) Hilbert space. This norm can be defined in various ways:\n\nwhere formula_92 are the singular values of formula_11. Recall that the trace function returns the sum of diagonal entries of a square matrix. \n\nThe Frobenius norm is the Euclidean norm on formula_94 and comes from the Frobenius inner product on the space of all matrices.\n\nThe Frobenius norm is sub-multiplicative and is very useful for numerical linear algebra. This norm is often easier to compute than induced norms and has the useful property of being invariant under rotations, that is, formula_95 for any rotation matrix formula_96. This property follows from the trace definition restricted to real matrices:\n\nand\n\nwhere we have used the orthogonal nature of formula_96 (that is, formula_100) and the cyclic nature of the trace (formula_101). More generally the norm is invariant under a unitary transformation for complex matrices.\n\nIt also satisfies \n\nand \n\nwhere formula_104 is the Frobenius inner product.\n\nThe max norm is the elementwise norm with \"p\" = \"q\" = ∞:\nThis norm is not sub-multiplicative.\n\nThe Schatten \"p\"-norms arise when applying the \"p\"-norm to the vector of singular values of a matrix. If the singular values are denoted by \"σ\", then the Schatten \"p\"-norm is defined by\nThese norms again share the notation with the induced and entrywise \"p\"-norms, but they are different.\n\nAll Schatten norms are sub-multiplicative. They are also unitarily invariant, which means that formula_107 for all matrices formula_11 and all unitary matrices formula_109 and formula_110.\n\nThe most familiar cases are \"p\" = 1, 2, ∞. The case \"p\" = 2 yields the Frobenius norm, introduced before. The case \"p\" = ∞ yields the spectral norm, which is the operator norm induced by the vector 2-norm (see above). Finally, \"p\" = 1 yields the nuclear norm (also known as the \"trace norm\", or the Ky Fan 'n'-norm), defined as\n\nA matrix norm formula_116 on formula_2 is called \"consistent\" with a vector norm formula_118 on formula_34 and a vector norm formula_120 on formula_35 if:\nfor all formula_123. All induced norms are consistent by definition.\n\nA matrix norm formula_116 on formula_94 is called \"compatible\" with a vector norm formula_118 on formula_34 if:\nfor all formula_129. Induced norms are compatible by definition.\n\nFor any two matrix norms formula_130 and formula_131, we have\n\nfor some positive numbers \"r\" and \"s\", for all matrices \"A\" in formula_2. In other words, all norms on formula_2 are \"equivalent\"; they induce the same topology on formula_2. This is true because the vector space formula_2 has the finite dimension formula_3.\n\nMoreover, for every vector norm formula_31 on formula_139, there exists a unique positive real number formula_140 such that formula_141 is a sub-multiplicative matrix norm for every formula_142.\n\nA sub-multiplicative matrix norm formula_130 is said to be \"minimal\" if there exists no other sub-multiplicative matrix norm formula_131 satisfying formula_145.\n\nLet formula_146 once again refer to the norm induced by the vector \"p\"-norm (as above in the Induced Norm section).\n\nFor matrix formula_147 of rank formula_148, the following inequalities hold:\n\n\nAnother useful inequality between matrix norms is\nwhich is a special case of Hölder's inequality.\n\n"}
{"id": "6582659", "url": "https://en.wikipedia.org/wiki?curid=6582659", "title": "Moving least squares", "text": "Moving least squares\n\nMoving least squares is a method of reconstructing continuous functions from a set of unorganized point samples via the calculation of a weighted least squares measure biased towards the region around the point at which the reconstructed value is requested.\n\nIn computer graphics, the moving least squares method is useful for reconstructing a surface from a set of points. Often it is used to create a 3D surface from a point cloud through either downsampling or upsampling.\n\nConsider a function formula_1 and a set of sample points formula_2. Then, the moving least square approximation of degree formula_3 at the point formula_4 is formula_5 where formula_6 minimizes the weighted least-square error \nover all polynomials formula_8 of degree formula_3 in formula_10. formula_11 is the weight and it tends to zero as formula_12.\n\nIn the example formula_13. The smooth interpolator of \"order 3\" is a quadratic interpolator.\n\n\n\n"}
{"id": "46897892", "url": "https://en.wikipedia.org/wiki?curid=46897892", "title": "Networks in labor economics", "text": "Networks in labor economics\n\nThe importance of social ties in job searching is known, and empirically proved for quite a while, workers often find jobs through their friends and relatives. However, the exploration of the role of social networks in labor market outcomes has just recently started. New evidence shows that social networks not only increase the productivity of job searching but partly explain wage differences, and help decreasing the information asymmetry between the employer and employee.\n\nIn economics researches, the role of social ties’ formalization in job searching often uses exogenous job networks therefore, the graph of the network is initially given. Using a similar framework, Calvo-Armegnol and Jackson were able to point out some network related labor market issues.\n\nIn their basic model, in which they attempt to formalize the transmission of job information among individuals, the agents can be either employed with some non-zero, or unemployed with zero wages. The agents can get information about a job, and when they do so, they can decide whether to keep that information for themselves or pass it to their contacts. In the other phase, employed agents can lose their job with a given probability.\n\nImportant indication of their model is that if someone who is employed has the information about a job, she will pass it to her unemployed acquaintances who will then become employed. Therefore, there is a positive correlation between labor outcomes of an individual and her contacts. On the other hand, it can also give an explanation for long term unemployment. If someone’s acquaintances are unemployed as well, she has less chance to hear of some job opportunity. They also conclude that that different initial wage and employment can cause different drop-outs rates from the labor market, thus, it can explain the existence of wage inequalities across social groups. Calvo-Armengol and Jackson prove that position in the network, and structure of the network affect the probability of being unemployed as well.\n\nThe effectiveness of job searching with personal contacts is the consequence not only the individuals’ but the employers’ behavior as well. They often choose to hire acquaintances of their current employees instead of using a bigger pool of applicants. It is due to the information asymmetry as they hardly know anything about the productivity of the applicant, and revealing it would be rather time consuming and expensive. However, employees might be aware both their contacts unobserved characteristics and the specific expectations of employers so they can enhance this imbalance. Another benefit for the firm is that due to the personal bond, present employees are motivated to choose a candidate who will perform well, since after the recommendation, their reputation is also at stake.\n\nDustman, Glitz and Schönberg showed that using personal connections in job search increases the initial wage and decreases the probability of leaving the firm.\n\nReferral based job network can function even if there is no direct link between the referee and the potential worker. In the model of Finneran and Kelly, there is a hierarchical network in which workers has the opportunity to refer their acquaintances if their employer hires. Workers are referred for a job with some increasing probability in regards to their ability, and productivity. In a hierarchical model like this, workers who take place on a lower level, far from the information, never get an offer. However, the authors have showed that there is a threshold of this referral probability over which even those skilled worker can be referred who are low in the hierarchy. So there is a critical density of referral linkages exists under which no qualified workers can be referred, however, if the density of these linkages is high enough, all qualified workers will match with a job, despite their position in the network.\n"}
{"id": "49664155", "url": "https://en.wikipedia.org/wiki?curid=49664155", "title": "Nullspace property", "text": "Nullspace property\n\nIn compressed sensing, the nullspace property gives necessary and sufficient conditions on the reconstruction of sparse signals using the techniques of formula_1-relaxation. The term \"nullspace property\" originates from Cohen, Dahmen, and DeVore. The nullspace property is often difficult to check in practice, and the restricted isometry property is a more modern condition in the field of compressed sensing.\n\nThe non-convex formula_3-minimization problem,\n\nformula_4 subject to formula_5,\n\nis a standard problem in compressed sensing. However, formula_3-minimization is known to be NP-hard in general. As such, the technique of formula_1-relaxation is sometimes employed to circumvent the difficulties of signal reconstruction using the formula_3-norm. In formula_1-relaxation, the formula_1 problem,\n\nformula_11 subject to formula_5,\n\nis solved in place of the formula_3 problem. Note that this relaxation is convex and hence amenable to the standard techniques of linear programming - a computationally desirable feature. Naturally we wish to know when formula_1-relaxation will give the same answer as the formula_3 problem. The nullspace property is one way to guarantee agreement.\n\nAn formula_16 complex matrix formula_17 has the nullspace property of order formula_18 if for all index sets formula_19 with formula_20 we have that: formula_21 for all formula_22.\n\nThe following theorem gives necessary and sufficient condition on the recoverability of a given formula_23-sparse vector in formula_24. The proof of the theorem is a standard one, and the proof supplied here is summarized from Holger Rauhut.\n\nformula_25 Let formula_17 be a formula_16 complex matrix. Then every formula_23-sparse signal formula_29 is the unique solution to the formula_1-relaxation problem with formula_31 if and only if formula_17 satisfies the nullspace property with order formula_23.\n\nformula_34 For the forwards direction notice that formula_35 and formula_36 are distinct vectors with formula_37 by the linearity of formula_17, and hence by uniqueness we must have formula_21 as desired. For the backwards direction, let formula_40 be formula_23-sparse and formula_42 another (not necessary formula_23-sparse) vector such that formula_44 and formula_45. Define the (non-zero) vector formula_46 and notice that it lies in the nullspace of formula_17. Call formula_19 the support of formula_40, and then the result follows from an elementary application of the triangle inequality: formula_50, establishing the minimality of formula_40. formula_52\n"}
{"id": "27908263", "url": "https://en.wikipedia.org/wiki?curid=27908263", "title": "OLGA (technology)", "text": "OLGA (technology)\n\nOLGA is a modelling tool for transportation of oil, natural gas and water in the same pipeline, so-called multiphase transportation. The name is short for \"oil and gas simulator\". The main challenge with multiphase fluid flow is the formation of slugs (plugs of oil and water) in the pipelines, which causes large problems at the receiving end at the platform or the onshore plant. The modelling tool makes it possible to calculate the fluid flow and safely bring the flow to the receiving destination on shore, on a platform or a production ship through the pipes.\n\nThe idea for the tool was conceived in 1979 by two researchers at IFE, Norway: Dag Malnes and Kjell Bendiksen. The first version of OLGA was financed by Statoil and was ready in 1980. The tool was developed further by IFE in collaboration with SINTEF in the 1980s.\n\nJanuary 1, 1984 a joint industry agreement was signed by Statoil, IFE and SINTEF on the continued development of OLGA. IFE had the main responsibility for developing the model, while the technical experiments were performed in SINTEF’s laboratory at Tiller.\n\nUntil 2012 the SPT Group owned the rights to OLGA. In March of 2012, Schlumberger announced an agreement with Altor Fund II for the acquisition of SPT Group. The acquisition was completed in Q2. SPT Group, founded in 1971, was headquartered in Norway employing approximately 280 people in 11 countries at the time of the acquisition. The tool has been under continuous and still ongoing development, among others in the HORIZON II project where IFE and SPT Group are partners. OLGA has a global market share of about 90%. The technology is regarded as a central success for Norwegian petroleum research.\n\nOLGA has enabled the development of oil and gas fields at deeper seas and farther from shore than would otherwise be possible without this technology, for example the fields Troll, Ormen Lange and Snøhvit.\n"}
{"id": "9156022", "url": "https://en.wikipedia.org/wiki?curid=9156022", "title": "Odd number theorem", "text": "Odd number theorem\n\nThe odd number theorem is a theorem in strong gravitational lensing which comes directly from differential topology.\n\nThe theorem states that \"the number of multiple images produced by a bounded transparent lens must be odd\".\n\nThe gravitational lensing is a thought to mapped from what's known as \"image plane\" to \"source plane\" following the formula :\n\nformula_1.\n\nIf we use direction cosines describing the bent light rays, we can write a vector field on formula_2 plane formula_3.\n\nHowever, only in some specific directions formula_4, will the bent light rays reach the observer, i.e., the images only form where formula_5. Then we can directly apply the Poincaré–Hopf theorem formula_6.\n\nThe index of sources and sinks is +1, and that of saddle points is −1. So the Euler characteristic equals the difference between the number of positive indices formula_7 and the number of negative indices formula_8. For the far field case, there is only one image, i.e., formula_9. So the total number of images is formula_10, i.e., odd. The strict proof needs Uhlenbeck’s Morse theory of null geodesics.\n\n"}
{"id": "19074048", "url": "https://en.wikipedia.org/wiki?curid=19074048", "title": "Order (mathematics)", "text": "Order (mathematics)\n\nOrder in mathematics may refer to:\n\n\n\n\n\n\n\n\n\nIn logic, model theory and type theory:\n\n\n\n"}
{"id": "34995809", "url": "https://en.wikipedia.org/wiki?curid=34995809", "title": "PCLake", "text": "PCLake\n\nPCLake is a dynamic, mathematical model used to study eutrophication effects in shallow lakes and ponds. PCLake models explicitly the most important biotic groups and their interrelations, within the general framework of nutrient cycles. PCLake is used both by scientist and water managers.\n\nTypically, shallow lakes are in one of two contrasting alternative stable states: a clear state with submerged macrophytes and piscivorous fish, or a turbid state dominated by phytoplankton and benthivorous fish. A switch from one state to the other is largely driven by the input of nutrients (phosphorus and nitrogen) to the ecosystem. If the nutrient loading exceeds a critical value, eutrophication causes a switch from the clear to the turbid state. As a result of urban water pollution and/or intensive agriculture in catchment areas, many of the world’s shallow lakes and ponds are in a eutrophic state with turbid waters and poor ecological quality. In this turbid state, the lake also becomes subject to algal blooms of toxic cyanobacteria (also called blue-green algae). Recovery of the clear state however is difficult as the critical nutrient loading for the switch back is often found to be lower than the critical loading towards the turbid state. Lowering the nutrient input thus does not automatically lead to a switch back to the clear water phase. Hence, the system shows hysteresis.\n\nPCLake is designed to study the effects of eutrophication on shallow lakes and ponds. On one hand, the model is used by scientists to study the general behavior of these ecosystems. For example, PCLake is used to understand the phenomena of alternative stable states and hysteresis, and in that light, the relative importance of lake features such as water depth or fetch length. Also the potential effects of climate warming for shallow lakes have been studied. On the other hand, PCLake is applied by lake water resource managers that consider the turbid state as undesirable. They can use the model to define the critical loadings for their specific lakes and evaluate the effectiveness of restoration measures. For this purpose also a meta-model has been developed. The meta-model can be used by water managers to derive an estimate of the critical loading values for a certain lake based on only a few important parameters, without the need of running the full dynamical model.\n\nMathematically, PCLake is composed of a set of coupled differential equations. With a large number of state variables (>100) and parameters (>300), the model may be characterized as relatively complex. The main biotic variables are phytoplankton and submerged aquatic vegetation, describing primary production. A simplified food web is made up of zooplankton, zoobenthos, young and adult whitefish and piscivorous fish. The main abiotic factors are transparency and the nutrients phosphorus (P), nitrogen (N) and silica (Si). At the base of the model are the water and nutrient budgets (in- and outflow). The model describes a completely mixed water body and comprises both the water column and the upper sediment layer. The overall nutrient cycles for N, P and Si are described as completely closed (except for in- and outflow and denitrification). Inputs to the model are: lake hydrology, nutrient loading, dimensions and sediment characteristics. The model calculates chlorophyll-a, transparency, cyanobacteria, vegetation cover and fish biomass, as well as the concentrations and fluxes of nutrients N, P and Si, and oxygen. Optionally, a wetland zone with marsh vegetation and water exchange with the lake can be included.\n\nPCLake is calibrated against nutrient, transparency, chlorophyll and vegetation data on more than 40 European (but mainly Dutch) lakes, and systematic sensitivity and uncertainty analysis have been performed.\nAlthough PCLake is primarily used for Dutch lakes, it is likely that the model is also applicable to comparable non-stratifying lakes in other regions, if parameters are adjusted or some small changes to the model are made.\n\nThe first version of PCLake (by then called PCLoos) was built in the early 1990s at the Netherlands National Institute for Public Health and the Environment (RIVM), within the framework of a research and restoration project on Lake Loosdrecht. It has been extended and improved since then. Parallel to PCLake, PCDitch was created, which is an ecosystem model for ditches and other linear water bodies. The models were further developed by dr. Jan H. Janse and colleagues at the Netherlands Environmental Assessment Agency (PBL), formerly part of the RIVM. Since 2009, the model is jointly owned by PBL and the Netherlands Institute of Ecology, where further development and application of PCLake is taking place, related to aquatic-ecological research.\n\n"}
{"id": "638889", "url": "https://en.wikipedia.org/wiki?curid=638889", "title": "Path (graph theory)", "text": "Path (graph theory)\n\nIn graph theory, a path in a graph is a finite or infinite sequence of edges which connect a sequence of vertices which, by most definitions, are all distinct from one another. In a directed graph, a directed path (sometimes called dipath) is again a sequence of edges (or arcs) which connect a sequence of vertices, but with the added restriction that the edges all be directed in the same direction.\n\nPaths are fundamental concepts of graph theory, described in the introductory sections of most graph theory texts. See e.g. Bondy and Murty (1976), Gibbons (1985), or Diestel (2005). Korte et al. (1990) cover more advanced algorithmic topics concerning paths in graphs.\n\nA path is a trail in which all vertices (except possibly the first and last) are distinct.\nA trail is a walk in which all edges are distinct.\nA walk of length formula_1 in a graph is an alternating sequence of vertices and edges, formula_2, which begins and ends with vertices. If the graph is undirected, then the endpoints of formula_3 are formula_4 and formula_5. If the graph is directed, then formula_3 is an arc from formula_4 to formula_5. An infinite path is an alternating sequence of the same type described here, but with no first or last vertex, and a semi-infinite path (also ray) has a first vertex, formula_9, but no last vertex. Most authors require that all of the edges and vertices be distinct from one another. However, some authors do not make this requirement, and instead use the term simple path to refer to a path which contains no repeated vertices.\n\nA weighted graph associates a value (\"weight\") with every edge in the graph. The \"weight of a path\" in a weighted graph is the sum of the weights of the traversed edges. Sometimes the words \"cost\" or \"length\" are used instead of weight.\n\n\nSeveral algorithms exist to find shortest and longest paths in graphs, with the important distinction that the former problem is computationally much easier than the latter.\n\nDijkstra's algorithm produces a list of shortest paths from a source vertex to every other vertex in directed and undirected graphs with non-negative edge weights (or no edge weights), whilst the Bellman–Ford algorithm can be applied to directed graphs with negative edge weights. The Floyd–Warshall algorithm can be used to find the shortest paths between all pairs of vertices in weighted directed graphs.\n\n\n"}
{"id": "23636", "url": "https://en.wikipedia.org/wiki?curid=23636", "title": "Perimeter", "text": "Perimeter\n\nA perimeter is a path that surrounds a two-dimensional shape. The term may be used either for the path or its length—it can be thought of as the length of the outline of a shape. The perimeter of a circle or ellipse is called its circumference.\n\nCalculating the perimeter has several practical applications. A calculated perimeter is the length of fence required to surround a yard or garden. The perimeter of a wheel (its circumference) describes how far it will roll in one revolution. Similarly, the amount of string wound around a spool is related to the spool's perimeter.\n\nThe perimeter is the distance around a shape. Perimeters for more general shapes can be calculated, as any path, with formula_1, where formula_2 is the length of the path and formula_3 is an infinitesimal line element. Both of these must be replaced with by algebraic forms in order to be practically calculated. If the perimeter is given as a closed piecewise smooth plane curve formula_4 with\nthen its length formula_2 can be computed as follows:\n\nA generalized notion of perimeter, which includes hypersurfaces bounding volumes in formula_8-dimensional Euclidean spaces, is described by the theory of Caccioppoli sets.\n\nPolygons are fundamental to determining perimeters, not only because they are the simplest shapes but also because the perimeters of many shapes are calculated by approximating them with sequences of polygons tending to these shapes. The first mathematician known to have used this kind of reasoning is Archimedes, who approximated the perimeter of a circle by surrounding it with regular polygons.\n\nThe perimeter of a polygon equals the sum of the lengths of its sides (edges). In particular, the perimeter of a rectangle of width formula_9 and length formula_10 equals formula_11\n\nAn equilateral polygon is a polygon which has all sides of the same length (for example, a rhombus is a 4-sided equilateral polygon). To calculate the perimeter of an equilateral polygon, one must multiply the common length of the sides by the number of sides.\n\nA regular polygon may be characterized by the number of its sides and by its circumradius, that is to say, the constant distance between its centre and each of its vertices. The length of its sides can be calculated using trigonometry. If is a regular polygon's radius and is the number of its sides, then its perimeter is \n\nA splitter of a triangle is a cevian (a segment from a vertex to the opposite side) that divides the perimeter into two equal lengths, this common length being called the semiperimeter of the triangle. The three splitters of a triangle all intersect each other at the Nagel point of the triangle.\n\nA cleaver of a triangle is a segment from the midpoint of a side of a triangle to the opposite side such that the perimeter is divided into two equal lengths. The three cleavers of a triangle all intersect each other at the triangle's Spieker center.\n\nThe perimeter of a circle, often called the circumference, is proportional to its diameter and its radius. That is to say, there exists a constant number pi, (the Greek \"p\" for perimeter), such that if is the circle's perimeter and its diameter then,\n\nIn terms of the radius of the circle, this formula becomes,\n\nTo calculate a circle's perimeter, knowledge of its radius or diameter and the number suffices. The problem is that is not rational (it cannot be expressed as the quotient of two integers), nor is it algebraic (it is not a root of a polynomial equation with rational coefficients). So, obtaining an accurate approximation of is important in the calculation. The computation of the digits of is relevant to many fields, such as mathematical analysis, algorithmics and computer science.\n\nThe perimeter and the area are two main measures of geometric figures. Confusing them is a common error, as well as believing that the greater one of them is, the greater the other must be. Indeed, a commonplace observation is that an enlargement (or a reduction) of a shape make its area grow (or decrease) as well as its perimeter. For example, if a field is drawn on a 1/ scale map, the actual field perimeter can be calculated multiplying the drawing perimeter by . The real area is times the area of the shape on the map. Nevertheless, there is no relation between the area and the perimeter of an ordinary shape. For example, the perimeter of a rectangle of width 0.001 and length 1000 is slightly above 2000, while the perimeter of a rectangle of width 0.5 and length 2 is 5. Both areas equal to 1.\n\nProclus (5th century) reported that Greek peasants \"fairly\" parted fields relying on their perimeters. However, a field's production is proportional to its area, not to its perimeter, so many naive peasants may have gotten fields with long perimeters but small areas (thus, few crops).\n\nIf one removes a piece from a figure, its area decreases but its perimeter may not. In the case of very irregular shapes, confusion between the perimeter and the convex hull may arise. The convex hull of a figure may be visualized as the shape formed by a rubber band stretched around it. In the animated picture on the left, all the figures have the same convex hull; the big, first hexagon.\n\nThe isoperimetric problem is to determine a figure with the largest area, amongst those having a given perimeter. The solution is intuitive; it is the circle. In particular, this can be used to explain why drops of fat on a broth surface are circular.\n\nThis problem may seem simple, but its mathematical proof requires some sophisticated theorems. The isoperimetric problem is sometimes simplified by restricting the type of figures to be used. In particular, to find the quadrilateral, or the triangle, or another particular figure, with the largest area amongst those with the same shape having a given perimeter. The solution to the quadrilateral isoperimetric problem is the square, and the solution to the triangle problem is the equilateral triangle. In general, the polygon with sides having the largest area and a given perimeter is the regular polygon, which is closer to being a circle than is any irregular polygon with the same number of sides.\n\nThe word comes from the Greek περίμετρος \"perimetros\" from περί \"peri\" \"around\" and μέτρον \"metron\" \"measure\".\n\n\n"}
{"id": "54747507", "url": "https://en.wikipedia.org/wiki?curid=54747507", "title": "Philco computers", "text": "Philco computers\n\nPhilco was one of the pioneers of transistorized computers. After the company developed the surface barrier transistor, which was much faster than previous point-contact types, it was awarded contracts for military and government computers. Commercialized derivatives of some of these designs became successful business and scientific computers. The TRANSAC (Transistor Automatic Computer) Model S-1000 was released as a scientific computer. The TRANSAC S-2000 mainframe computer system was first produced in 1958, and a family of compatible machines, with increasing performance, was released over the next several years.\n\nHowever, the mainframe computer market was dominated by IBM. Other companies could not deploy resources for development, customer support and marketing on the scale that IBM could afford, making competition in this segment difficult after the introduction of the IBM 360 family. Philco went bankrupt and was purchased in 1961 by Ford Motor Company, but the computer division carried on until the Philco division of Ford exited the computer business in 1963. The Ford company maintained one Philco mainframe in use until 1981.\n\nThe surface-barrier transistor developed by Philco in 1953 had a much higher frequency response than the original point-contact transistors. The transistor was made of a thin crystal of germanium, which was electrolytically etched with pits on either side forming a very thin base region, on the order of 5 micrometers. Philco's process for etching was United States patent number 2,885,571. Philco surface-barrier transistors were used in TX-0, and in early models of what would become the DEC PDP product line. Although relatively fast, the small size of the devices limited their power to circuits operating at a few tens of milliwatts. \n\nBetween 1955 and 1957, Philco built transistor computers for use in aircraft, models C-1000, C-1100, and C-1102, intended for airborne real-time applications. By 1957, the C-1102 had been used by a civilian sector customer.\nThe BASICPAC AN/TYK 6V (first delivery in 1961), COMPAC AN/TYK 4V (not completed), and LOGICPAC systems were built for the US Army as transportable computer systems for use with their Fieldata concept of integrated information management.\n\nBASICPAC was a transistorized computer with up to 28,672 words of 38-bit core memory (including sign and parity), available in several configurations from a minimum system, to a truck-borne mobile version, to a fully expanded system. Basic clock periods was 1 microsecond (which gives a clock rate of 1 MHz), with 12 microsecond memory access and a fixed-point multiplication taking 242 microseconds. Input/output was by paper tape reader and punch, or through a teletypewriter. With additional hardware, magnetic tape storage was also available, with up to seven I/O devices. The instruction set had 31 basic operation codes and nine opcodes for I/O \n\nPhilco was contracted by the US Navy to build the CXPQ computer. One model was completed and installed at the David Taylor Model Basin. This design was later adapted to become the commercial TRANSAC S-2000. Only one CXPQ was built.\n\nIn 1955, the National Security Agency through the US Navy contracted with Philco to produce a computer suitable for use as a workstation, with an architecture based on the vacuum-tube computer system called Atlas II already in use at the NSA, and similar to the commercial UNIVAC 1103. At the time, Philco was the largest producer of surface barrier transistors, which were the only type available with the speed and quantities required for a computer. The SOLO prototype was delivered in 1958, but required extensive debugging at NSA and no further instances were ordered. Difficulties were encountered with core memory and power supplies. SOLO used paper tape and teleprinter machines for input and output. SOLO cost about $1 million US, and contained 8,000 transistors. While the system was extensively used for training, testing, research and development, no additional units were ordered. SOLO was removed from active service in 1963. The design of the SOLO became commercialized as Philco's TRANSAC Model S-1000.\n\nThe TRANSAC S-1000 was a scientific computer with a 36-bit word length and 4096 words of core memory. It was packaged in a container about the size of a large office desk, and used only 1.2 kilowatts, much less than vacuum-tube-based computers of similar capacity.\nIn a 1961 survey, about 15 S-1000 computer installations had been identified.\n\nIt weighed about .\n\nThe TRANSAC S-2000 was a large mainframe system intended for both business and scientific work. It had a 48-bit word length and supported calculations in fixed point, floating point and binary-coded decimal formats. The original S-2000 \"TRANSAC\" (Transistor Automatic Computer) released in 1958 was later designated Model 210; it was used internally at Philco. Similar to the Control Data Corporation Model 1604, it was a 48-bit fully transistorized computer. Three succeeding models were released in the series, all compatible with the software of the original model. The Model 211 was introduced in 1960, using micro-alloy diffused field-effect transistors, requiring significant redesign of circuits compared to the original. \n\nThe TRANSAC S-2000/Philco 210/211 weighed about .\n\nBy 1964, 18 Model 210, and 18 Model 211 and 7 Model 212 systems had been sold.\n\nAfter Philco was purchased by Ford Motor Company, the Model 212 was introduced in 1962 and released in 1963. It had 65,535 words of 48-bit memory. Initially made with 6-microsecond core memory, it had better performance than the IBM 7094 transistor computer. It was later upgraded in 1964 to 2-microsecond core memory, which gave the machine floating-point performance greater than the IBM 7030 Stretch computer. A Model 213 was announced in 1964 but never built. By that time competition from IBM had made the Philco computer operations no longer profitable for Ford, and the division was closed down.\n\nThe Model 212 could carry out a floating-point multiplication in 22 microseconds. Each word contained two 24-bit instructions with 16 bits of address information and eight bits for the opcode. There were 225 different valid opcodes in the Model 212; invalid opcodes were detected and halted the machine. The CPU had an accumulator register of 48 bits, three general-purpose registers of 24 bits, and 32 index registers of 15 bits. Main memory size ranged from 4K words to 64K words. Only the first model had a magnetic drum memory; later editions used tape drives.\n\nThe Model 212 weighed about .\n\nSoftware for the S-2000 initially consisted of TAC (Translator-Assember-Compiler), and ALTAC, a FORTRAN II-like language with some differences from the IBM 704 FORTRAN implementation. A COBOL compiler was also available, targeted at business applications.\n\nThe Philco 2400 was the input/output system for the S-2000. Operations such as reading cards or printing were carried out through magnetic tapes, thereby offloading the S-2000 from relatively slow input/output processing. The 2400 had a 24-bit word length and could be supplied with 4K to 32K characters (1K to 8K words) of core memory, rated at 3-microsecond cycle time. The instruction set was aimed at character I/O use.\n\nThe last Philco TRANSAC S-2000 Model 212 was taken out of service in December 1981, after 19 years service at Ford.\n\n"}
{"id": "21368888", "url": "https://en.wikipedia.org/wiki?curid=21368888", "title": "Philosophy of logic", "text": "Philosophy of logic\n\nFollowing the developments in formal logic with symbolic logic in the late nineteenth century and mathematical logic in the twentieth, topics traditionally treated by logic not being part of formal logic have tended to be termed either \"philosophy of logic\" or \"philosophical logic\" if no longer simply \"logic\".\n\nCompared to the history of logic the demarcation between philosophy of logic and philosophical logic is of recent coinage and not always entirely clear. Characterisations include\n\nThis article outlines issues in philosophy of logic or provides links to relevant articles or both.\n\nThis article makes use of the following terms and concepts:\n\nAristotle said \"To say that that which is, is not or that which is not is, is a falsehood; and to say that which is, is and that which is not is not, is true\"\n\nThis apparent truism has not proved unproblematic.\n\nLogic uses such terms as true, false, inconsistent, valid, and self-contradictory. Questions arise as Strawson (1952) writes\n\nSee:\n\nSince the use, meaning, if not the meaningfulness, of the terms is part of the debate, it is possible only to give the following working definitions for the purposes of the discussion:\n\nThe concept of logical truth is intimately linked with those of validity, logical consequence and entailment (as well as self-contradiction, necessarily false etc.).\n\nIssues that arise include:\nSee also \n\nSee\n\n\n\n\n\n\nImportant figures in the philosophy of logic include (but are not limited to):\n\n\n\n\n\n\n"}
{"id": "305331", "url": "https://en.wikipedia.org/wiki?curid=305331", "title": "Quadratic irrational number", "text": "Quadratic irrational number\n\nIn mathematics, a quadratic irrational number (also known as a quadratic irrational, a quadratic irrationality or quadratic surd) is an irrational number that is the solution to some quadratic equation with rational coefficients which is irreducible over the set of rational numbers. Since fractions in the coefficients of a quadratic equation can be cleared by multiplying both sides by their common denominator, a quadratic irrational is an irrational root of some quadratic equation whose coefficients are integers. The quadratic irrational numbers, a subset of the complex numbers, are algebraic numbers of degree 2, and can therefore be expressed as\n\nfor integers ; with , and non-zero, and with square-free. When is positive, we get real quadratic irrational numbers, while a negative gives complex quadratic irrational numbers which are not real numbers. This implies that the quadratic irrationals have the same cardinality as ordered quadruples of integers, and are therefore countable.\n\nQuadratic irrationals are used in field theory to construct field extensions of the rational field . Given the square-free integer , the augmentation of by quadratic irrationals using produces a quadratic field ). For example, the inverses of elements of ) are of the same form as the above algebraic numbers:\n\nQuadratic irrationals have useful properties, especially in relation to continued fractions, where we have the result that \"all\" real quadratic irrationals, and \"only\" real quadratic irrationals, have periodic continued fraction forms. For example\n\nWe may rewrite a quadratic irrationality as follows:\n\nIt follows that every quadratic irrational number can be written in the form\n\nThis expression is not unique.\n\nFix a nonsquare, positive integer formula_6 congruent to formula_7 or formula_8 modulo formula_9, and define a set formula_10 as \n\nEvery quadratic irrationality is in some set formula_10, since the congruence conditions can be met by scaling the numerator and denominator by an appropriate factor. \n\nA matrix\n\nwith integer entries and formula_14 can be used to transform a number formula_15 in formula_10. The transformed number is \n\nIf formula_15 is in formula_10, then formula_20 is too.\n\nThe relation between formula_15 and formula_20 above is an equivalence relation. (This follows, for instance, because the above transformation gives a group action of the group of integer matrices with determinant 1 on the set formula_10.) Thus, formula_10 partitions into equivalence classes. Each equivalence class comprises a collection of quadratic irrationalities with each pair equivalent through the action of some matrix. Serret's theorem implies that the regular continued fraction expansions of equivalent quadratic irrationalities are eventually the same, that is, their sequences of partial quotients have the same tail. Thus, all numbers in an equivalence class have continued fraction expansions that are eventually periodic with the same tail.\n\nThere are finitely many equivalence classes of quadratic irrationalities in formula_10. The standard proof of this involves considering the map formula_26 from binary quadratic forms of discriminant formula_6 to formula_10 given by\n\nA computation shows that formula_26 is a bijection between that respects the matrix action on each set. The equivalence classes of quadratic irrationalities are then in bijection with the equivalence classes of binary quadratic forms, and Lagrange showed that there are finitely many equivalence classes of binary quadratic forms of given discriminant. \n\nThrough the bijection formula_26, expanding a number in formula_10 in a continued fraction corresponds to reducing the quadratic form. The eventually periodic nature of the continued fraction is then reflected in the eventually periodic nature of the orbit of a quadratic form under reduction, with reduced quadratic irrationalities (those with a purely periodic continued fraction) corresponding to reduced quadratic forms.\n\nThe definition of quadratic irrationals requires them to satisfy two conditions: they must satisfy a quadratic equation and they must be irrational. The solutions to the quadratic equation \"ax\" + \"bx\" + \"c\" = 0 are\n\nThus quadratic irrationals are precisely those real numbers in this form that are not rational. Since \"b\" and 2\"a\" are both integers, asking when the above quantity is irrational is the same as asking when the square root of an integer is irrational. The answer to this is that the square root of any natural number that is not a square number is irrational.\n\nThe square root of 2 was the first such number to be proved irrational. Theodorus of Cyrene proved the irrationality of the square roots of whole numbers up to 17 (except those few that are square numbers, such as 16), but stopped there, probably because the algebra he used could not be applied to the square root of numbers greater than 17. Euclid's Elements Book 10 is dedicated to classification of irrational magnitudes. The original proof of the irrationality of the non-square natural numbers depends on Euclid's lemma.\n\nMany proofs of the irrationality of the square roots of non-square natural numbers implicitly assume the fundamental theorem of arithmetic, which was first proven by Carl Friedrich Gauss in his Disquisitiones Arithmeticae. This asserts that every integer has a unique factorization into primes. For any rational non-integer in lowest terms there must be a prime in the denominator which does not divide into the numerator. When the numerator is squared that prime will still not divide into it because of the unique factorization. Therefore, the square of a rational non-integer is always a non-integer; by contrapositive, the square root of an integer is always either another integer, or irrational.\n\nEuclid used a restricted version of the fundamental theorem and some careful argument to prove the theorem. His proof is in Euclid's Elements Book X Proposition 9.\n\nThe fundamental theorem of arithmetic is not actually required to prove the result, however. There are self-contained proofs by Richard Dedekind, among others. The following proof was adapted by Colin Richard Hughes from a proof of the irrationality of the square root of two found by Theodor Estermann in 1975.\n\nAssume \"D\" is a non-square natural number, then there is a number \"n\" such that:\n\nso in particular\n\nAssume the square root of \"D\" is a rational number \"p\"/\"q\", assume the \"q\" here is the smallest for which this is true, hence the smallest number for which \"q\" is also an integer. Then:\n\nis also an integer. But 0 < ( − \"n\") < 1 so ( − \"n\")\"q\" < \"q\". Hence ( − \"n\")\"q\" is an integer smaller than \"q\". This is a contradiction since \"q\" was defined to be the smallest number with this property; hence cannot be rational.\n\n\n"}
{"id": "310953", "url": "https://en.wikipedia.org/wiki?curid=310953", "title": "Quotient algebra", "text": "Quotient algebra\n\nIn mathematics, a quotient algebra, (where \"algebra\" means algebraic structure in the sense of universal algebra), also called a factor algebra, is obtained by partitioning the elements of an algebra into equivalence classes given by a congruence relation, that is an equivalence relation that is additionally \"compatible\" with all the operations of the algebra, in the formal sense described below.\n\nLet \"A\" be the set of the elements of an algebra formula_1, and let \"E\" be an equivalence relation on the set \"A\". The relation \"E\" is said to be \"compatible\" with (or have the \"substitution property\" with respect to) an \"n\"-ary operation \"f\", if formula_2 for formula_3 implies formula_4 for any formula_5 with formula_3. An equivalence relation compatible with all the operations of an algebra is called a congruence with respect to this algebra.\n\nAny equivalence relation \"E\" in a set \"A\" partitions this set in equivalence classes. The set of these equivalence classes is usually called the quotient set, and denoted \"A\"/\"E\". For an algebra formula_1, it is straightforward to define the operations induced on the elements of \"A\"/\"E\" if \"E\" is a congruence. Specifically, for any operation formula_8 of arity formula_9 in formula_1 (where the superscript simply denotes that it is an operation in formula_1, and the subscript formula_12 enumerates the functions in formula_1 and their arities) define formula_14 as formula_15, where formula_16 denotes the equivalence class of formula_17 generated by \"E\" (\"\"x\" modulo \"E\"\").\n\nFor an algebra formula_18, given a congruence \"E\" on formula_1, the algebra formula_20 is called the \"quotient algebra\" (or \"factor algebra\") of formula_1 modulo \"E\". There is a natural homomorphism from formula_1 to formula_23 mapping every element to its equivalence class. In fact, every homomorphism \"h\" determines a congruence relation via the kernel of the homomorphism, formula_24.\n\nGiven an algebra formula_1, a homomorphism \"h\" thus defines two algebras homomorphic to formula_1, the image h(formula_1) and formula_28 The two are isomorphic, a result known as the \"homomorphic image theorem\" or as the first isomorphism theorem for universal algebra. Formally, let formula_29 be a surjective homomorphism. Then, there exists a unique isomorphism \"g\" from formula_28 onto formula_31 such that \"g\" composed with the natural homomorphism induced by formula_32 equals \"h\".\n\nFor every algebra formula_1 on the set \"A\", the identity relation on A, and formula_34 are trivial congruences. An algebra with no other congruences is called \"simple\".\n\nLet formula_35 be the set of congruences on the algebra formula_1. Because congruences are closed under intersection, we can define a meet operation: formula_37 by simply taking the intersection of the congruences formula_38.\n\nOn the other hand, congruences are not closed under union. However, we can define the closure of any binary relation \"E\", with respect to a fixed algebra formula_1, such that it is a congruence, in the following way: formula_40. Note that the (congruence) closure of a binary relation depends on the operations in formula_1, not just on the carrier set. Now define formula_42 as formula_43.\n\nFor every algebra formula_1, formula_45 with the two operations defined above forms a lattice, called the \"congruence lattice\" of formula_1. It's a distributive lattice.\n\nIf two congruences \"permute\" (commute) with the composition of relations as operation, i.e. formula_47, then their join (in the congruence lattice) is equal to their composition: formula_48. An algebra is called \"congruence-permutable\" if every pair of its congruences permutes; likewise a variety is said to be congruence-permutable if all its members are \ncongruence-permutable algebras.\n\nIn 1954, Anatoly Maltsev established the following characterization of congruence-permutable varieties: a variety is congruence permutable if and only if there exist a ternary term such that ; this is called a Maltsev term and varieties with this property are called Maltsev varieties. Maltsev's characterization explains a large number of similar results in groups (take ), rings, quasigroups (take , complemented lattices, Heyting algebras etc. Furthermore, every congruence-permutable algebra is congruence-modular, i.e. its lattice of congruences is modular lattice as well; the converse is not true however.\n\nAfter Maltsev's result, other researchers found characterizations based on conditions similar to that found by Maltsev but for other kinds of properties, e.g. in 1967 Bjarni Jónsson found the conditions for varieties having congruence lattices that are distributive (thus called congruence-distributive varieties). Generically, such conditions are called Maltsev conditions.\n\nThis line of research led to the Pixley–Wille algorithm for generating Maltsev conditions associated\nwith congruence identities.\n\n\n"}
{"id": "30001920", "url": "https://en.wikipedia.org/wiki?curid=30001920", "title": "Random modulation", "text": "Random modulation\n\nIn the theories of modulation and of stochastic processes, random modulation is the creation of a new signal from two other signals by the process of quadrature amplitude modulation. In particular, the two signals are considered as being random processes. For applications, the two original signals need have a limited frequency range, and these are used to modulate a third sinusoidal carrier signal whose frequency is above the range of frequencies contained in the original signals.\n\nThe random modulation procedure starts with two stochastic baseband signals, formula_1 and formula_2, whose frequency spectrum is non-zero only for formula_3. It applies quadrature modulation to combine these with a carrier frequency formula_4 (with formula_5) to form the signal formula_6 given by\nwhere formula_8 is the equivalent baseband representation of the modulated signal formula_6\n\nIn the following it is assumed that formula_1 and formula_2 are two real jointly wide sense stationary processes. It can be shown that the new signal formula_6 is wide sense stationary if and only if formula_8 is circular complex, i.e. if and only if formula_1 and formula_2 are such that\n\n"}
{"id": "852522", "url": "https://en.wikipedia.org/wiki?curid=852522", "title": "Representation of a Lie superalgebra", "text": "Representation of a Lie superalgebra\n\nIn the mathematical field of representation theory, a representation of a Lie superalgebra is an action of Lie superalgebra \"L\" on a Z-graded vector space \"V\", such that if \"A\" and \"B\" are any two pure elements of \"L\" and \"X\" and \"Y\" are any two pure elements of \"V\", then\n\nEquivalently, a representation of \"L\" is a Z-graded representation of the universal enveloping algebra of \"L\" which respects the third equation above.\n\nA Lie superalgebra is a complex Lie superalgebra equipped with an involutive antilinear map such that * respects the grading and \n\nA unitary representation of such a Lie algebra is a Z graded Hilbert space which is a representation of a Lie superalgebra as above together with the requirement that self-adjoint elements of the Lie superalgebra are represented by Hermitian transformations.\n\nThis is a major concept in the study of supersymmetry together with representation of a Lie superalgebra on an algebra. Say A is an *-algebra representation of the Lie superalgebra (together with the additional requirement that * respects the grading and L[a]=-(-1)L[a]) and H is the unitary rep and also, H is a unitary representation of A.\n\nThese three reps are all compatible if for pure elements a in A, |ψ> in H and L in the Lie superalgebra,\n\nSometimes, the Lie superalgebra is embedded within A in the sense that there is a homomorphism from the universal enveloping algebra of the Lie superalgebra to A. In that case, the equation above reduces to\n\nThis approach avoids working directly with a Lie supergroup, and hence avoids the use of auxiliary Grassmann numbers.\n\n"}
{"id": "23868049", "url": "https://en.wikipedia.org/wiki?curid=23868049", "title": "Sequential algorithm", "text": "Sequential algorithm\n\nIn computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel. The term is primarily used to contrast with \"concurrent algorithm\" or \"parallel algorithm;\" most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many distributed algorithms are both concurrent and parallel – and thus \"sequential\" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.\n\n\"Sequential algorithm\" may also refer specifically to an algorithm for decoding a convolutional code.\n\n"}
{"id": "46510274", "url": "https://en.wikipedia.org/wiki?curid=46510274", "title": "Stream (computer science)", "text": "Stream (computer science)\n\nIn type theory and functional programming, a stream is a potentially infinite analog of a list, given by the coinductive definition:\n\nGenerating and computing with streams requires lazy evaluation, either implicitly in a lazily evaluated language or by creating and forcing thunks in an eager language. In total languages they must be defined as codata and can be iterated over using (guarded) corecursion.\n\n"}
{"id": "52238391", "url": "https://en.wikipedia.org/wiki?curid=52238391", "title": "Ten rays model", "text": "Ten rays model\n\nThe ten-ray model is a model applied to the transmissions in the urban area, to generate a model of ten rays typically four rays more are added to the six rays model, these are (formula_1 and formula_2 bouncing on both sides of the wall); This incorporate paths from one to three reflections: specifically, there is the LOS (Line of sight), GR (ground reflected), SW (single-wall reflected), DW (double-wall reflected), TW (triple-wall reflected), WG (wall-ground reflected) and GW (ground-wall reflected paths). Where each one of the paths bounces on both sides of the wall.\n\nExperimentally, it has been demonstrated that the ten ray model simulates or can represent the propagation of signals through a dielectric canyon, in it which the rays that travel from a transmitter point to a receiver point bounce many times.\n\nAs example for this model it is assume: a rectilinear free space with two walls, one upper and the other lower, from which two vertical bases are positioned at their ends, these are the transmitting and receiving antennas that it’s locate in such a way that their heights don’t surpass the limits of the top wall; Achieved this the structure acts as free space for its functioning similar to that of a dielectric canyon of signals propagation, since the rays transmitted from the transmitting antenna will collide each side of the upper and lower walls infinity of times (for this example up to 3 reflections) until reaching the receiving antenna. During the course of the rays for each reflection they suffer, part of the energy of the signal is dissipated in each reflection, normally after the third reflection of said ray its resulting component which is a retro-reflected ray is insignificant with a negligible energy.\n\nFor the mathematical modeling of the propagation of ten rays, One has in account a side view and this starts off modeling the two first rays (line by sight and his respective reflection), Considering that antennas have different heights, Then formula_3, and they have a direct distance d that separates the two antennas; The first ray is formed applying Pitágoras theorem:\n\nThe second ray or the reflected ray is made in a similar way to the first, but in this case the heights of the antennas to form the right angled triangle for the reflection of the height of the transmitter are added up.\n\nIn the deduction of the third ray it is necessary find the angle between the direct distance formula_6 and the distance of line of view formula_7\n\nViewing the model with a side view, it is necessary to find a flat distance between the transmitter and receiver called formula_9.\n\nNow we deduce the remaining height of the wall from the height of the receiver called formula_11 by the similarity of triangles:\n\nBy likeness of triangles we can deduce the distance from where collides the ray to wall until the perpendicular of the receiver called formula_14, getting:\n\nThe third ray is defined as a model of two-rays, by which is:\n\nTaking a side view it is achieves to evidence the reflected ray that there in formula_18 and is find as following manner:\n\nAs exist two rays that collide once on the wall, then is find the fifth ray, equating it to the third.\n\nSimilarly, is equalized the sixth ray with the fourth ray, since they have the same characteristics.\n\nTo model the rays that collide with the wall twice, is used the Pythagoras theorem because of the direct distance formula_6 and the sum of the distances between the receiver to each wall with double of distance of the transmitter to the wall formula_23, this divides on the angle formed between the direct distance and the reflected ray.\n\nFor the eighth ray is calculate a series of variables that allow to deduce the complete equation, which is composed by distances and heights that were found by likeness of triangles.\n\nIn first instance is take the flat distance between the wall of the second shock and the receiver:\n\nIs found the flat distance between the transmitter and the wall in the first shock.\n\nFinding the distance between the height of the wall of the second shock with respect to the first shock, is obtain:\n\nDeducing also the distance between the height of the wall of the second shock with respect to the receiver:\n\nCalculating the height of the wall where occurs the first hit:\n\nCalculating the height of the wall where occurs the second shock:\n\nWith these parameters is calculate the equation for the eighth ray:\n\nFor the ninth ray, equation is the same as the seventh ray due to its characteristics:\n\nFor the tenth ray, the equation is the same as the eighth ray due to its reflected ray shape:\n\nIs considered a signal transmitted through free space to a receiver located at a distance \"d\" from the transmitter.\n\nAssuming there are no obstacles between the transmitter and the receiver, the signal propagates along a straight line between the two. The beam model associated with this transmission is denominated line of sight (LOS), and the signal received corresponding is called the LOS signal or beam.\n\nThe trajectory losses of the ten-ray model in free space is defined as:\n\n"}
{"id": "27622712", "url": "https://en.wikipedia.org/wiki?curid=27622712", "title": "Thomas Ranken Lyle Medal", "text": "Thomas Ranken Lyle Medal\n\nThe Thomas Ranken Lyle Medal is awarded at most every two years by the Australian Academy of Science to a mathematician or physicist for his or her outstanding research accomplishments. It is named after Thomas Ranken Lyle, an Irish mathematical physicist who became a professor at the University of Melbourne. The award takes the form of a bronze medal bearing the design of the head of Thomas Lyle, as sculpted by Rayner Hoff. \n\nThe medal was founded by the Australian National Research Council (ANRC) in 1932, and first awarded in 1935. When the Australian Academy of Science was established in 1954, it took over the roles of the ANRC, including administration of the medal.\n"}
{"id": "8234367", "url": "https://en.wikipedia.org/wiki?curid=8234367", "title": "Tree-graded space", "text": "Tree-graded space\n\nA geodesic metric space formula_1 is called tree-graded space, with respect to a collection of connected proper subsets called \"pieces\", if any two distinct pieces intersect by at most one point, and every non-trivial simple geodesic triangle of formula_1 is contained in one of the pieces.\n\nThus, for pieces of bounded diameter, tree-graded spaces behave like real trees in their coarse geometry (in the sense of Gromov) while allowing non-tree-like behavior within the pieces.\n\nTree-graded spaces were introduced by in their study of the asymptotic cones of hyperbolic groups.\n"}
{"id": "15890221", "url": "https://en.wikipedia.org/wiki?curid=15890221", "title": "Twistor correspondence", "text": "Twistor correspondence\n\nIn mathematical physics, the twistor correspondence is a natural isomorphism between massless Yang-Mills fields on Minkowski space and sheaf cohomology classes on a real hypersurface of CP.\n"}
{"id": "972328", "url": "https://en.wikipedia.org/wiki?curid=972328", "title": "Wedderburn–Etherington number", "text": "Wedderburn–Etherington number\n\nThe Wedderburn–Etherington numbers are an integer sequence named for Ivor Malcolm Haddon Etherington and Joseph Wedderburn that can be used to count certain kinds of binary trees. The first few numbers in the sequence are\n\nThese numbers can be used to solve several problems in combinatorial enumeration. The \"n\"th number in the sequence (starting with the number 0 for \"n\" = 0)\ncounts\n\nThe Wedderburn–Etherington numbers may be calculated using the recurrence relation\nbeginning with the base case formula_11.\n\nIn terms of the interpretation of these numbers as counting rooted binary trees with \"n\" leaves, the summation in the recurrence counts the different ways of partitioning these leaves into two subsets, and of forming a subtree having each subset as its leaves. The formula for even values of \"n\" is slightly more complicated than the formula for odd values in order to avoid double counting trees with the same number of leaves in both subtrees.\n\nThe Wedderburn–Etherington numbers grow asymptotically as\nwhere \"B\" is the generating function of the numbers and \"ρ\" is its radius of convergence, approximately 0.4027 , and where the constant given by the part of the expression in the square root is approximately 0.3188 .\n\n use the Wedderburn–Etherington numbers as part of a design for an encryption system containing a hidden backdoor. When an input to be encrypted by their system can be sufficiently compressed by Huffman coding, it is replaced by the compressed form together with additional information that leaks key data to the attacker. In this system, the shape of the Huffman coding tree is described as an Otter tree and encoded as a binary number in the interval from 0 to the Wedderburn–Etherington number for the number of symbols in the code. In this way, the encoding uses a very small number of bits, the base-2 logarithm of the Wedderburn–Etherington number.\n\n"}
