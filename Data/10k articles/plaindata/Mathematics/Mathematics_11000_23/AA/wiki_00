{"id": "31902178", "url": "https://en.wikipedia.org/wiki?curid=31902178", "title": "Alexander Merkurjev", "text": "Alexander Merkurjev\n\nAleksandr Sergeyevich Merkurjev (, born September 25, 1955) is a Russian-American mathematician, who has made major contributions to the field of algebra. Currently Merkurjev is a professor at the University of California, Los Angeles.\n\nMerkurjev's work focuses on algebraic groups, quadratic forms, Galois cohomology, algebraic K-theory and central simple algebras. In the early 1980s Merkurjev proved a fundamental result about the structure of central simple algebras of period dividing 2, which relates the 2-torsion of the Brauer group with Milnor K-theory. In subsequent work with Suslin this was extended to higher torsion as the Merkurjev–Suslin theorem. The full statement of the norm residue isomorphism theorem (also known as the Bloch-Kato conjecture) was proven by Voevodsky.\n\nIn the late 1990s Merkurjev gave the most general approach to the notion of essential dimension, introduced by Buhler and Reichstein, and made fundamental contributions to that field. In particular Merkurjev determined the essential p-dimension of central simple algebras of degree formula_1 (for a prime p) and, in joint work with Karpenko, the essential dimension of finite \"p\"-groups.\n\nMerkurjev won the Young Mathematician Prize of the Petersburg Mathematical Society for his work on algebraic K-theory. In 1986 he was an invited speaker at the International Congress of Mathematicians in Berkeley, California, and his talk was entitled \"Milnor K-theory and Galois cohomology\". In 1995 he won the Humboldt Prize, an international prize awarded to renowned scholars. Merkurjev gave a plenary talk at the 2nd European Congress of Mathematics in Budapest, Hungary in 1996.\nIn 2012 he won the Cole Prize in Algebra for his work on the essential dimension of groups.\n\nIn 2015 a special volume of \"Documenta Mathematica\" was published in honor of Merkurjev's sixtieth birthday.\n\n\n"}
{"id": "13848490", "url": "https://en.wikipedia.org/wiki?curid=13848490", "title": "Archive for Rational Mechanics and Analysis", "text": "Archive for Rational Mechanics and Analysis\n\nThe Archive for Rational Mechanics and Analysis is a scientific journal that is devoted to research in mechanics as a deductive, mathematical science. The current editors in chief of the journal are John M. Ball and Richard D. James. It was founded in 1956 by Clifford Truesdell when he moved from Indiana University to Johns Hopkins and lost control of a similar journal he had founded a few years previously, the \"Journal of Rational Mechanics and Analysis\" (now the \"Indiana University Mathematics Journal\").\n\nGianfranco Capriz writes that Truesdell's ideals of mathematical and typesetting rigor gave the new journal a high reputation:\nJames Serrin, a later editor of the Archive, adds that it became the center of a revival of mechanics as an academic discipline, and that by the time of Truesdell's retirement as editor in 1989 subscribing to it was \"necessary for every fine scientific library\".\n"}
{"id": "52612722", "url": "https://en.wikipedia.org/wiki?curid=52612722", "title": "Bayesian survival analysis", "text": "Bayesian survival analysis\n\nSurvival analysis is normally carried out using parametric models, semi-parametric models, non-parametric models to estimate the survival rate in clinical research. However recently Bayesian models are also used to estimate the survival rate due to their ability to handle design and analysis issues in clinical research.\n"}
{"id": "12923821", "url": "https://en.wikipedia.org/wiki?curid=12923821", "title": "Beth definability", "text": "Beth definability\n\nIn mathematical logic, Beth definability is a result that connects implicit definability of a property to its explicit definability, specifically the theorem states that the two senses of definability are equivalent.\n\nThe theorem states that, given any two models \"A\" and \"B\" of a first-order theory \"T\" in the language L' ⊇ L such that \"A\"|\"L\" = \"B\"|\"L\" (where \"A\"|\"L\" is the reduct of \"A\" to \"L\"), it is the case that \"A\" ⊨ φ[\"a\"] if and only if \"B\" ⊨ φ[\"a\"] (for φ a formula in L' and for all tuples a of \"A\") only if it is also the case that φ is equivalent modulo \"T\" to a formula ψ in \"L\". Less formally: a property is implicitly definable in a theory in language L (via introduction of a new symbol φ of an extended language L') only if that property is explicitly definable in that theory (by formula ψ in the original language L).\n\nClearly the converse holds as well, so that we have an equivalence between implicit and explicit definability. That is, a \"property\" is implicitly definable with respect to a theory if and only if it is explicitly definable.\n\nThe theorem does not hold if the condition is restricted to finite models. We may have \"A\" ⊨ φ[\"a\"] if and only if \"B\" ⊨ φ[\"a\"] for all pairs A,B of finite models without there being any \"L\"-formula ψ equivalent to φ modulo T.\n\nThe result was first proven by Evert Willem Beth.\n\n"}
{"id": "49590020", "url": "https://en.wikipedia.org/wiki?curid=49590020", "title": "Canon Sinuum (Bürgi)", "text": "Canon Sinuum (Bürgi)\n\nThe Canon Sinuum was a historic table of sines thought to have given the sines to 8 sexagesimal places between 0 and 90 degrees in steps of 2 arc seconds. Some authors believe that the table was only between 0 and 45 degrees. It was created by Jost Bürgi at the end of the 16th century. Such tables were essential for navigation at sea. Johannes Kepler called the \"Canon Sinuum\" the most precise known table of sines.\n\nThis table is thought to be lost.\n\nThe \"Canon Sinuum\" was computed by Bürgi's algorithms explained in his work Fundamentum Astronomiae presented to Emperor Rudolf II. in 1592. These algorithms made use of differences and were one of the early uses of difference calculus. The largest trigonometrical table actually contained in the Fundamentum Astronomiae is a table giving the sines for every minute of the quadrant and to 5 to 7 sexagesimal places.\n\nThe manuscript of Fundamentum Astronomiae is now in the collection of the Biblioteka Uniwersytecka in Wrocław, Poland.\n\n"}
{"id": "1457636", "url": "https://en.wikipedia.org/wiki?curid=1457636", "title": "Clustering coefficient", "text": "Clustering coefficient\n\nIn graph theory, a clustering coefficient is a measure of the degree to which nodes in a graph tend to cluster together. Evidence suggests that in most real-world networks, and in particular social networks, nodes tend to create tightly knit groups characterised by a relatively high density of ties; this likelihood tends to be greater than the average probability of a tie randomly established between two nodes (Holland and Leinhardt, 1971; Watts and Strogatz, 1998).\n\nTwo versions of this measure exist: the global and the local. The global version was designed to give an overall indication of the clustering in the network, whereas the local gives an indication of the embeddedness of single nodes.\n\nThe global clustering coefficient is based on triplets of nodes. A triplet is three nodes that are connected by either two (open triplet) or three (closed triplet) undirected ties. A triangle graph therefore includes three closed triplets, one centered on each of the nodes (n.b. this means the three triplets in a triangle come from overlapping selections of nodes). The global clustering coefficient is the number of closed triplets (or 3 x triangles) over the total number of triplets (both open and closed). The first attempt to measure it was made by Luce and Perry (1949). This measure gives an indication of the clustering in the whole network (global), and can be applied to both undirected and directed networks (often called transitivity, see Wasserman and Faust, 1994, page 243).\n\nThe global clustering coefficient is defined as:\n\nThe number of closed triplets has also been referred to as 3 × triangles in the literature, so:\n\nA generalisation to weighted networks was proposed by Opsahl and Panzarasa (2009), and a redefinition to two-mode networks (both binary and weighted) by Opsahl (2009).\n\n The local clustering coefficient of a vertex (node) in a graph quantifies how close its neighbours are to being a clique (complete graph). Duncan J. Watts and Steven Strogatz introduced the measure in 1998 to determine whether a graph is a small-world network.\n\nA graph formula_3 formally consists of a set of vertices formula_4 and a set of edges formula_5 between them. An edge formula_6 connects vertex formula_7 with vertex formula_8.\n\nThe neighbourhood formula_9 for a vertex formula_7 is defined as its immediately connected neighbours as follows:\n\nWe define formula_12 as the number of vertices, formula_13, in the neighbourhood, formula_14, of a vertex.\n\nThe local clustering coefficient formula_15 for a vertex formula_7 is then given by the proportion of links between the vertices within its neighbourhood divided by the number of links that could possibly exist between them. For a directed graph, formula_6 is distinct from formula_18, and therefore for each neighbourhood formula_14 there are formula_20 links that could exist among the vertices within the neighbourhood (formula_12 is the number of neighbours of a vertex). Thus, the local clustering coefficient for directed graphs is given as \n\nAn undirected graph has the property that formula_6 and formula_18 are considered identical. Therefore, if a vertex formula_7 has formula_12 neighbours, formula_27 edges could exist among the vertices within the neighbourhood. Thus, the local clustering coefficient for undirected graphs can be defined as\n\nLet formula_29 be the number of triangles on formula_30 for undirected graph formula_31. That is, formula_29 is the number of subgraphs of formula_31 with 3 edges and 3 vertices, one of which is formula_34. Let formula_35 be the number of triples on formula_36. That is, formula_35 is the number of subgraphs (not necessarily induced) with 2 edges and 3 vertices, one of which is formula_34 and such that formula_34 is incident to both edges. Then we can also define the clustering coefficient as\n\nIt is simple to show that the two preceding definitions are the same, since\n\nThese measures are 1 if every neighbour connected to formula_7 is also connected to every other vertex within the neighbourhood, and 0 if no vertex that is connected to formula_7 connects to any other vertex that is connected to formula_7.\n\nAs an alternative to the global clustering coefficient, the overall level of clustering in a network is measured by Watts and Strogatz as the average of the local clustering coefficients of all the vertices formula_45 :\n\nIt is worth noting that this metric places more weight on the low degree nodes, while the transitivity ratio places more weight on the high degree nodes. In fact, a weighted average where each local clustering score is weighted by formula_20 is identical to the global clustering coefficient.\n\nA graph is considered small-world, if the graph has a small mean-shortest path length that scales with the natural log of the number of nodes in the network,\nformula_48 . For example, a random graph is small-world, while a lattice is not, and scale-free graphs are often considered ultra-small world, as their mean-shortest path length scales with formula_49.\n\nA generalisation to weighted networks was proposed by Barrat et al. (2004), and a redefinition to bipartite graphs (also called two-mode networks) by Latapy et al. (2008) and Opsahl (2009).\n\nAlternative generalisations to weighted and directed graphs have been provided by Fagiolo (2007) and Clemente and Grassi (2018).\n\nThis formula is not, by default, defined for graphs with isolated vertices; see Kaiser (2008) and Barmpoutis et al. The networks with the largest possible average clustering coefficient are found to have a modular structure, and at the same time, they have the smallest possible average distance among the different nodes.\n"}
{"id": "2910837", "url": "https://en.wikipedia.org/wiki?curid=2910837", "title": "Computational finance", "text": "Computational finance\n\nComputational finance is a branch of applied computer science that deals with problems of practical interest in finance. Some slightly different definitions are the study of data and algorithms currently used in finance and the mathematics of computer programs that realize financial models or systems.\n\nComputational finance emphasizes practical numerical methods rather than mathematical proofs and focuses on techniques that apply directly to economic analyses. It is an interdisciplinary field between mathematical finance and numerical methods. Two major areas are efficient and accurate computation of fair values of financial securities and the modeling of stochastic price series.\n\nThe birth of computational finance as a discipline can be traced to Harry Markowitz in the early 1950s. Markowitz conceived of the portfolio selection problem as an exercise in mean-variance optimization. This required more computer power than was available at the time, so he worked on useful algorithms for approximate solutions. Mathematical finance began with the same insight, but diverged by making simplifying assumptions to express relations in simple closed forms that did not require sophisticated computer science to evaluate.\n\nIn the 1960s, hedge fund managers such as Ed Thorp and Michael Goodkin (working with Harry Markowitz, Paul Samuelson and Robert C. Merton) pioneered the use of computers in arbitrage trading. In academics, sophisticated computer processing was needed by researchers such as Eugene Fama in order to analyze large amounts of financial data in support of the efficient-market hypothesis.\n\nDuring the 1970s, the main focus of computational finance shifted to options pricing and analyzing mortgage securitizations. In the late 1970s and early 1980s, a group of young quantitative practitioners who became known as “rocket scientists” arrived on Wall Street and brought along personal computers. This led to an explosion of both the amount and variety of computational finance applications. Many of the new techniques came from signal processing and speech recognition rather than traditional fields of computational economics like optimization and time series analysis.\n\nBy the end of the 1980s, the winding down of the Cold War brought a large group of displaced physicists and applied mathematicians, many from behind the Iron Curtain, into finance. These people become known as “financial engineers” (“quant” is a term that includes both rocket scientists and financial engineers, as well as quantitative portfolio managers). This led to a second major extension of the range of computational methods used in finance, also a move away from personal computers to mainframes and supercomputers. Around this time computational finance became recognized as a distinct academic subfield. The first degree program in computational finance was offered by Carnegie Mellon University in 1994. \n\nOver the last 20 years, the field of computational finance has expanded into virtually every area of finance, and the demand for practitioners has grown dramatically. Moreover, many specialized companies have grown up to supply computational finance software and services.\n\n\n\n"}
{"id": "10433833", "url": "https://en.wikipedia.org/wiki?curid=10433833", "title": "Computational mathematics", "text": "Computational mathematics\n\n\"Computational mathematics\" may refer to two different aspect of the relation between computing and mathematics. \n\nComputational applied mathematics consists roughly of using mathematics for allowing and improving computer computation in applied mathematics. Computational mathematics may also refer to the use of computers for mathematics itself. This includes the use of computers for mathematical computations (computer algebra), the study of what can (and cannot) be computerized in mathematics (effective methods), which computations may be done with present technology (complexity theory), and which proofs can be done on computers (proof assistants).\n\nBoth aspects of computational mathematics involves mathematical research in mathematics as well as in areas of science where computing plays a central and essential role—that, is almost all sciences—, and emphasize algorithms, numerical methods, and symbolic computations. \n\nComputational mathematics emerged as a distinct part of applied mathematics by the early 1950s. Currently, computational mathematics can refer to or include:\n\n\n"}
{"id": "44022448", "url": "https://en.wikipedia.org/wiki?curid=44022448", "title": "Droz-Farny line theorem", "text": "Droz-Farny line theorem\n\nIn Euclidean geometry, the Droz-Farny line theorem is a property of two perpendicular lines through the orthocenter of an arbitrary triangle.\n\nLet formula_1 be a triangle with vertices formula_2, formula_3, and formula_4, and let formula_5 be its orthocenter (the common point of its three altitude lines. Let formula_6 and formula_7 be any two mutually perpendicular lines through formula_5. Let formula_9, formula_10, and formula_11 be the points where formula_6 intersects the side lines formula_13, formula_14, and formula_15, respectively. Similarly, let Let formula_16, formula_17, and formula_18 be the points where formula_7 intersects those side lines. The Droz-Farny line theorem says that the midpoints of the three segments formula_20, formula_21, and formula_22 are collinear.\n\nThe theorem was stated by Arnold Droz-Farny in 1899, but it is not clear whether he had a proof.\n\nA generalization of the Droz-Farny line theorem was proved in 1930 by René Goormaghtigh.\n\nAs above, let formula_1 be a triangle with vertices formula_2, formula_3, and formula_4. Let formula_27 be any point distinct from formula_2, formula_3, and formula_4, and formula_31 be any line through formula_27. Let formula_9, formula_10, and formula_11 be points on the side lines formula_13, formula_14, and formula_15, respectively, such that the lines formula_39, formula_40, and formula_41 are the images of the lines formula_42, formula_43, and formula_44, respectively, by reflection against the line formula_31. Goormaghtigh's theorem then says that the points formula_9, formula_10, and formula_11 are collinear.\n\nThe Droz-Farny line theorem is a special case of this result, when formula_27 is the orthocenter of triangle formula_1.\n\nThe theorem was further generalized by Dao Thanh Oai. The generalization as follows:\n\nFirst generalization: Let ABC be a triangle, \"P\" be a point on the plane, let three parallel segments AA', BB', CC' such that its midpoints and \"P\" are collinear. Then PA', PB', PC' meet \"BC, CA, AB\" respectively at three collinear points.\n\nSecond generalization: Let a conic S and a point P on the plane. Construct three lines d, d, d through P such that they meet the conic at A, A'; B, B' ; C, C' respectively. Let D be a point on the polar of point P with respect to (S) or D lies on the conic (S). Let DA' ∩ BC =A; DB' ∩ AC = B; DC' ∩ AB= C. Then A, B, C are collinear. \n"}
{"id": "1615479", "url": "https://en.wikipedia.org/wiki?curid=1615479", "title": "Edge space", "text": "Edge space\n\nIn the mathematical discipline of graph theory, the edge space and vertex space of an undirected graph are vector spaces defined in terms of the edge and vertex sets, respectively. These vector spaces make it possible to use techniques of linear algebra in studying the graph.\n\nLet formula_1 be a finite undirected graph. The vertex space formula_2 of \"G\" is the vector space over the finite field of two elements \nformula_3 of all functions formula_4. Every element of formula_2 naturally corresponds the subset of \"V\" which assigns a 1 to its vertices. Also every subset of \"V\" is uniquely represented in formula_2 by its characteristic function. The edge space formula_7 is the formula_8-vector space freely generated by the edge set \"E\". The dimension of the vertex space is thus the number of vertices of the graph, while the dimension of the edge space is the number of edges.\n\nThese definitions can be made more explicit. For example, we can describe the edge space as follows:\nThe singleton subsets of \"E\" form a basis for formula_7.\n\nOne can also think of formula_2 as the power set of \"V\" made into a vector space with similar vector addition and scalar multiplication as defined for formula_7.\n\nThe incidence matrix formula_17 for a graph formula_18 defines one possible linear transformation\nbetween the edge space and the vertex space of formula_18. The incidence matrix of formula_18, as a linear transformation, maps each edge to its two incident vertices. Let formula_22 be the edge between formula_23 and formula_24 then\n\nThe cycle space and the cut space are linear subspaces of the edge space.\n\n\n"}
{"id": "57921290", "url": "https://en.wikipedia.org/wiki?curid=57921290", "title": "Elias Höchheimer", "text": "Elias Höchheimer\n\nElias ben Ḥayyim Cohen Höchheimer (or Hechim) was an eighteenth century Jewish astronomer and mathematician.\n\nBorn in Hochheim, Höchheimer lived a long time in Hildburghausen and died in Amsterdam. He was the author of \"Shebile di-Reḳi'a\" (Prague, 1784), on trigonometry and astronomy, \"Sefer Yalde ha-Zeman\" (Prague, 1786), a commentary on Jedaiah Bedersi's \"Beḥinat ha-'Olam\", and two German-language textbooks on arithmetic.\n\n"}
{"id": "6822721", "url": "https://en.wikipedia.org/wiki?curid=6822721", "title": "Fictional actuaries", "text": "Fictional actuaries\n\nFictional actuaries and the appearance of actuaries in works of fiction have been the subject of a number of articles in actuarial journals.\n\n\n\n\n\n\n\n"}
{"id": "22725313", "url": "https://en.wikipedia.org/wiki?curid=22725313", "title": "Geli (software)", "text": "Geli (software)\n\ngeli is a block device-layer disk encryption system written for FreeBSD, introduced in version 6.0. It uses the GEOM disk framework. It was designed and implemented by Paweł Jakub Dawidek.\n\ngeli was initially written to protect data on a user's computer in situations of physical theft of hardware, disallowing the thief access to the protected data. This has changed over time with the introduction of optional data authentication/integrity verification.\n\ngeli allows the key to consist of several information components (a user entered passphrase, random bits from a file, etc.), permits multiple keys (a user key and a company key, for example) and can attach a provider with a random, one-time key. The user passphrase is strengthened with PKCS#5.\n\nThe geli utility is different from gbde in that it offers different features and uses a different scheme for doing cryptographic work. It supports the crypto framework within FreeBSD, allowing hardware cryptographic acceleration if available, as well as supporting more cryptographic algorithms (currently AES, Triple DES, Blowfish and Camellia) and data authentication/integrity verification via MD5, SHA1, RIPEMD160, SHA256, SHA384 or SHA512 as Hash Message Authentication Codes.\n\n"}
{"id": "20394803", "url": "https://en.wikipedia.org/wiki?curid=20394803", "title": "Grøstl", "text": "Grøstl\n\nGrøstl is a cryptographic hash function submitted to the NIST hash function competition by Praveen Gauravaram, Lars Knudsen, Krystian Matusiewicz, Florian Mendel, Christian Rechberger, Martin Schläffer, and Søren S. Thomsen. Grøstl was chosen as one of the five finalists of the competition. It uses the same S-box as AES in a custom construction. The authors claim speeds of up to 21.4 cycles per byte on an Intel Core 2 Duo.\n\nAccording to the submission document, the name \"Grøstl\" is a multilingual play-on-words, referring to an Austrian dish that is very similar to hash (food).\n\nLike other hash functions in the MD5/SHA family, Grøstl divides the input into blocks and iteratively computes \"h\" = \"f\"(\"h\", \"m\"). However, Grøstl maintains a hash state at least twice the size of the final output (512 or 1024 bits), which is only truncated at the end of hash computation.\n\nThe compression function \"f\" is based on a pair of 256- or 512-bit permutation functions \"P\" and \"Q\", and is defined as:\n\nThe permutation functions \"P\" and \"Q\" are heavily based on the Rijndael (AES) block cipher, but operate on 8×8 or 8×16 arrays of bytes, rather than 4×4. Like AES, each round consists of four operations:\n\nUnlike Rijndael, all rounds are identical and there is no final AddRoundKey operation. 10 rounds are recommended for the 512-bit permutation, and 14 rounds for the 1024-bit version.\n\nThe final double-width hash receives a final output transformation of\nand is then truncated to the desired width. This is equivalent to applying a final iteration of the compression function using an all-zero message block \"m\", followed by a (cryptographically insignificant) exclusive-or with the fixed constant \"Q\"(0).\n\nHash values of empty string.\n\nEven a small change in the message will (with overwhelming probability) result in a mostly different hash, due to the avalanche effect. For example, adding a period to the end of the sentence:\n\n"}
{"id": "24004195", "url": "https://en.wikipedia.org/wiki?curid=24004195", "title": "Hamiltonian decomposition", "text": "Hamiltonian decomposition\n\nIn graph theory, a branch of mathematics, a Hamiltonian decomposition of a given graph is a partition of the edges of the graph into Hamiltonian cycles. Hamiltonian decompositions have been studied both for undirected graphs and for directed graphs; in the undirected case,\na Hamiltonian decomposition can also be described as a 2-factorization of the graph such that each factor is connected. For such a decomposition to exist in an undirected graph, it must be connected and regular of even degree.\nA directed graph with such a decomposition must be strongly connected and all vertices must have the same in-degree and out-degree as each other, but this degree does not need to be even.\n\nIt is known that every complete graph with an odd number formula_1 of vertices has a Hamiltonian decomposition. This result, which is a special case of the Oberwolfach problem of decomposing complete graphs into isomorphic 2-factors, was attributed to Walecki by Édouard Lucas in 1892.\nWalecki's construction places formula_2 of the vertices into a regular polygon, and covers the complete graph in this subset of vertices with formula_3 Hamiltonian paths that zigzag across the polygon, with each path rotated from each other path by a multiple of formula_4.\nThe paths can then all be completed to Hamiltonian cycles by connecting their ends through the remaining vertex.\n\nThe directed case of complete graphs are tournaments. Answering a 1968 conjecture by Kelly, Daniela Kühn and Deryk Osthus proved in 2012 that every sufficiently large regular tournament has a Hamiltonian decomposition.\n\nTesting whether an arbitrary graph has a Hamiltonian decomposition is NP-complete, both in the directed and undirected cases.\n"}
{"id": "15626235", "url": "https://en.wikipedia.org/wiki?curid=15626235", "title": "Hilbert number", "text": "Hilbert number\n\nIn number theory, a branch of mathematics, a Hilbert number is a positive integer of the form (). The Hilbert numbers were named after David Hilbert.\n\nThe sequence of Hilbert numbers begins 1, 5, 9, 13, 17, ... ). A Hilbert prime is a Hilbert number that is not divisible by a smaller Hilbert number (other than 1). The sequence of Hilbert primes begins\n\nA Hilbert prime is not necessarily a prime number; for example, 21 is a composite number since . However, 21 a Hilbert prime since neither 3 nor 7 (the only factors of 21 other than 1 and itself) are Hilbert numbers. It follows from multiplication modulo 4 that a Hilbert prime is either a prime number of the form (called a Pythagorean prime), or a semiprime of the form .\n\n"}
{"id": "23632928", "url": "https://en.wikipedia.org/wiki?curid=23632928", "title": "Hosaka plot", "text": "Hosaka plot\n\nA Hosaka plot is a graphical depiction used to evaluate the quality of a compressed image. A Hosaka plot shows noise levels at a number of scales. Hosoka plots are named after K. Hosoka, who wrote about them in 1986.\n"}
{"id": "3371683", "url": "https://en.wikipedia.org/wiki?curid=3371683", "title": "ISO 31-11", "text": "ISO 31-11\n\nISO 31-11:1992 was the part of international standard ISO 31 that defines \"mathematical signs and symbols for use in physical sciences and technology\". It was superseded in 2009 by ISO 80000-2.\n\nIts definitions include the following:\n\n"}
{"id": "51434698", "url": "https://en.wikipedia.org/wiki?curid=51434698", "title": "Infinitesimal cohomology", "text": "Infinitesimal cohomology\n\nIn mathematics, infinitesimal cohomology is a cohomology theory for algebraic varieties introduced by . In characteristic 0 it is essentially the same as crystalline cohomology. In nonzero characteristic \"p\" showed that it is closely related to etale cohomology with mod \"p\" coefficients, a theory known to have undesirable properties.\n\n"}
{"id": "295844", "url": "https://en.wikipedia.org/wiki?curid=295844", "title": "Inversive geometry", "text": "Inversive geometry\n\nIn geometry, inversive geometry is the study of those properties of figures that are preserved by a generalization of a type of transformation of the Euclidean plane, called \"inversion\". These transformations preserve angles and map generalized circles into generalized circles, where a \"generalized circle\" means either a circle or a line (loosely speaking, a circle with infinite radius). Many difficult problems in geometry become much more tractable when an inversion is applied.\n\nThe concept of inversion can be generalized to higher-dimensional spaces.\n\nTo invert a number in arithmetic usually means to \"take its reciprocal\". A closely related idea in geometry is that of \"inverting\" a point. In the plane, the inverse of a point \"P\" with respect to a \"reference circle (Ø)\" with center \"O\" and radius \"r\" is a point \"P\", lying on the ray from \"O\" through \"P\" such that\n\nThis is called circle inversion or plane inversion. The inversion taking any point \"P\" (other than \"O\") to its image \"P\" also takes \"P\" back to \"P\", so the result of applying the same inversion twice is the identity transformation on all the points of the plane other than \"O\". To make inversion an involution it is necessary to introduce a point at infinity, a single point placed on all the lines, and extend the inversion, by definition, to interchange the center \"O\" and this point at infinity.\n\nIt follows from the definition that the inversion of any point inside the reference circle must lie outside it, and vice versa, with the center and the point at infinity changing positions, whilst any point on the circle is unaffected (is \"invariant\" under inversion). In summary, the nearer a point to the center, the further away its transformation, and vice versa.\n\nTo construct the inverse \"P\" of a point \"P\" outside a circle \"Ø\":\n\nTo construct the inverse \"P\" of a point \"P\" inside a circle \"Ø\":\n\nThere is a construction of the inverse point to \"A\" with respect to a circle \"P\" that is \"independent\" of whether \"A\" is inside or outside \"P\".\n\nConsider a circle \"P\" with center \"O\" and a point \"A\" which may lie inside or outside the circle \"P\".\n\nThe inversion of a set of points in the plane with respect to a circle is the set of inverses of these points. The following properties make circle inversion useful.\n\nAdditional properties include:\n\nFor a circle not passing through the center of inversion, the center of the circle being inverted and the center of its image under inversion are collinear with the center of the reference circle. This fact can be used to prove that the Euler line of the intouch triangle of a triangle coincides with its OI line. The proof roughly goes as below:\n\nInvert with respect to the incircle of triangle \"ABC\". The medial triangle of the intouch triangle is inverted into triangle \"ABC\", meaning the circumcenter of the medial triangle, that is, the nine-point center of the intouch triangle, the incenter and circumcenter of triangle \"ABC\" are collinear.\n\nAny two non-intersecting circles may be inverted into concentric circles. Then the inversive distance (usually denoted δ) is defined as the natural logarithm of the ratio of the radii of the two concentric circles.\n\nIn addition, any two non-intersecting circles may be inverted into congruent circles, using circle of inversion centered at a point on the circle of antisimilitude.\n\nThe Peaucellier–Lipkin linkage is a mechanical implementation of inversion in a circle. It provides an exact solution to the important problem of converting between linear and circular motion.\n\nIf point \"R\" is the inverse of point \"P\" then the lines perpendicular to the line \"PR\" through one of the points is the polar of the other point (the pole).\n\nPoles and polars have several useful properties:\n\n\nCircle inversion is generalizable to sphere inversion in three dimensions. The inversion of a point \"P\" in 3D with respect to a reference sphere centered at a point \"O\" with radius \"R\" is a point \"P\" ' such that formula_3 and the points \"P\" and \"P\" ' are on the same ray starting at \"O\". As with the 2D version, a sphere inverts to a sphere, except that if a sphere passes through the center \"O\" of the reference sphere, then it inverts to a plane. Any plane not passing through \"O\", inverts to a sphere touching at \"O\". A circle, that is, the intersection of a sphere with a secant plane, inverts into a circle, except that if the circle passes through \"O\" it inverts into a line. This reduces to the 2D case when the secant plane passes through \"O\", but is a true 3D phenomenon if the secant plane does not pass through \"O\".\n\nThe simplest surface (besides a plane) is the sphere. The first picture shows a non trivial inversion (the center of the sphere is not the center of inversion) of a sphere together with two orthogonal intersecting pencils of circles.\n\nThe inversion of a cylinder, cone, or torus results in a Dupin cyclyde.\n\nA spheroid is a surface of revolution and contains a pencil of circles which is mapped onto a pencil of circles (see picture). The inverse image of a spheroid is a surface of degree 4.\n\nA hyperboloid of one sheet, which is a surface of revolution contains a pencil of circles which is mapped onto a pencil of circles. A hyperboloid of one sheet contains additional two pencils of lines, which are mapped onto pencils of circles. The picture shows one such line (blue) and its inversion.\n\nA stereographic projection usually projects a sphere from a point formula_4 (north pole) of the sphere onto the tangent plane at the opposite point formula_5 (south pole). This mapping can be performed by an inversion of the sphere onto its tangent plane. If the sphere (to be projected) has the equation formula_6 (alternately written formula_7; center formula_8, radius formula_9, green in the picture), then it will be mapped by the inversion at the unit sphere (red) onto the tangent plane at point formula_10. The lines through the center of inversion (point formula_4) are mapped onto themselves. They are the projection lines of the stereographic projection.\n\nThe 6-sphere coordinates are a coordinate system for three-dimensional space obtained by inverting the Cartesian coordinates.\n\nOne of the first to consider foundations of inversive geometry was Mario Pieri in 1911 and 1912. Edward Kasner wrote his thesis on \"Invariant theory of the inversion group\".\n\nMore recently the mathematical structure of inversive geometry has been interpreted as an incidence structure where the generalized circles are called \"blocks\": In incidence geometry, any affine plane together with a single point at infinity forms a Möbius plane, also known as an \"inversive plane\". The point at infinity is added to all the lines. These Möbius planes can be described axiomatically and exist in both finite and infinite versions.\n\nA model for the Möbius plane that comes from the Euclidean plane is the Riemann sphere.\n\nThe cross-ratio between 4 points formula_12 is invariant under an inversion. In particular if O is the centre of the inversion and formula_13 and formula_14 are distances to the ends of a line L, then length of the line formula_15 will become formula_16 under an inversion with centre O. The invariant is:\n\nAccording to Coxeter, the transformation by inversion in circle was invented by L. I. Magnus in 1831. Since then this mapping has become an avenue to higher mathematics. Through some steps of application of the circle inversion map, a student of transformation geometry soon appreciates the significance of Felix Klein’s Erlangen program, an outgrowth of certain models of hyperbolic geometry\n\nThe combination of two inversions in concentric circles results in a similarity, homothetic transformation, or dilation characterized by the ratio of the circle radii.\n\nwhen formula_19 the circle transforms into the line parallel to the imaginary axis formula_20\n\nAs mentioned above, zero, the origin, requires special consideration in the circle inversion mapping. The approach is to adjoin a point at infinity designated ∞ or 1/0 . In the complex number approach, where reciprocation is the apparent operation, this procedure leads to the complex projective line, often called the Riemann sphere. It was subspaces and subgroups of this space and group of mappings that were applied to produce early models of hyperbolic geometry by Beltrami, Cayley, and Klein. Thus inversive geometry includes the ideas originated by Lobachevsky and Bolyai in their plane geometry. Furthermore, Felix Klein was so overcome by this facility of mappings to identify geometrical phenomena that he delivered a manifesto, the Erlangen program, in 1872. Since then many mathematicians reserve the term \"geometry\" for a space together with a group of mappings of that space. The significant properties of figures in the geometry are those that are invariant under this group.\n\nFor example, Smogorzhevsky develops several theorems of inversive geometry before beginning Lobachevskian geometry.\n\nIn \"n\"-dimensional space where there is a sphere of radius \"r\", inversion in the sphere is given by\n\nThe transformation by inversion in hyperplanes or hyperspheres in E can be used to generate dilations, translations, or rotations. Indeed, two concentric hyperspheres, used to produce successive inversions, result in a dilation or contraction on the hyperspheres' center. Such a mapping is called a similarity.\n\nWhen two parallel hyperplanes are used to produce successive reflections, the result is a translation. When two hyperplanes intersect in an (\"n\"–2)-flat, successive reflections produce a rotation where every point of the (\"n\"–2)-flat is a fixed point of each reflection and thus of the composition.\n\nAll of these are conformal maps, and in fact, where the space has three or more dimensions, the mappings generated by inversion are the only conformal mappings. Liouville's theorem is a classical theorem of conformal geometry.\n\nThe addition of a point at infinity to the space obviates the distinction between hyperplane and hypersphere; higher dimensional inversive geometry is frequently studied then in the presumed context of an \"n\"-sphere as the base space. The transformations of inversive geometry are often referred to as Möbius transformations. Inversive geometry has been applied to the study of colorings, or partitionings, of an \"n\"-sphere.\n\nThe circle inversion map is anticonformal, which means that at every point it preserves angles and reverses orientation (a map is called conformal if it preserves \"oriented\" angles). Algebraically, a map is anticonformal if at every point the Jacobian is a scalar times an orthogonal matrix with negative determinant: in two dimensions the Jacobian must be a scalar times a reflection at every point. This means that if \"J\" is the Jacobian, then formula_22 and formula_23 Computing the Jacobian in the case , where gives , with , and additionally det(\"J\") is negative; hence the inversive map is anticonformal.\n\nIn the complex plane, the most obvious circle inversion map (i.e., using the unit circle centered at the origin) is the complex conjugate of the complex inverse map taking \"z\" to 1/\"z\". The complex analytic inverse map is conformal and its conjugate, circle inversion, is anticonformal.\nIn this case a homography is conformal while an anti-homography is anticonformal.\n\nThe (\"n\" − 1)-sphere with equation\n\nwill have a positive radius if \"a\" + ... + \"a\" is greater than \"c\", and on inversion gives the sphere\n\nHence, it will be invariant under inversion if and only if \"c\" = 1. But this is the condition of being orthogonal to the unit sphere. Hence we are led to consider the (\"n\" − 1)-spheres with equation\n\nwhich are invariant under inversion, orthogonal to the unit sphere, and have centers outside of the sphere. These together with the subspace hyperplanes separating hemispheres are the hypersurfaces of the Poincaré disc model of hyperbolic geometry.\n\nSince inversion in the unit sphere leaves the spheres orthogonal to it invariant, the inversion maps the points inside the unit sphere to the outside and vice versa. This is therefore true in general of orthogonal spheres, and in particular inversion in one of the spheres orthogonal to the unit sphere maps the unit sphere to itself. It also maps the interior of the unit sphere to itself, with points outside the orthogonal sphere mapping inside, and vice versa; this defines the reflections of the Poincaré disc model if we also include with them the reflections through the diameters separating hemispheres of the unit sphere. These reflections generate the group of isometries of the model, which tells us that the isometries are conformal. Hence, the angle between two curves in the model is the same as the angle between two curves in the hyperbolic space.\n\n\n\n"}
{"id": "1250498", "url": "https://en.wikipedia.org/wiki?curid=1250498", "title": "Joachim Lambek", "text": "Joachim Lambek\n\nJoachim \"Jim\" Lambek (5 December 1922 – 23 June 2014) was Peter Redpath Emeritus Professor of Pure Mathematics at McGill University, where he earned his Ph.D. degree in 1950 with Hans Zassenhaus as advisor.\n\nLambek was born in Leipzig, Germany, where he attended a Gymnasium. He came to England in 1938 as a refugee on the \"Kindertransport\". From there he was interned as an enemy alien and deported to a prison work camp in New Brunswick, Canada. There, he began in his spare time a mathematical apprenticeship with Fritz Rothberger, also\ninterned, and wrote the McGill Junior Matriculation in fall of 1941. In the spring of 1942, he was released and settled in Montreal, where he entered studies at McGill University, graduating with an honours mathematics degree in 1945 and an M.Sc. a year later. In 1950, he completed his doctorate under Hans Zassenhaus (becoming McGill's first Ph.D. in mathematics) and was promoted to assistant professor. He was made a full professor in 1963.\n\nLambek spent his sabbatical year 1965–66 in at the Institute for Mathematical Research at ETH Zurich, where Beno Eckmann had gathered together a group of researchers interested in algebraic topology and category theory, including Bill Lawvere. There Lambek reoriented his research into category theory. Though he retired in 1992, Lambek's presence in the Mathematics Department was constant until the year before his death.\n\nLambek supervised 16 doctoral students, and has 51 doctoral descendants. He has over 100 publications listed in the Mathematical Reviews, including 6 books. His earlier work was mostly in module theory, especially torsion theories, non-commutative localization, and injective modules. One of his earliest papers, , proved the Lambek-Moser theorem about integer sequences. His more recent work is in pregroups and formal languages; his earliest work in this field were probably and . He is noted, among other things, for the Lambek calculus, an effort to capture mathematical aspects of natural language syntax in logical form and a work that has been very influential in computational linguistics, as well as for developing the connections between typed lambda calculus and cartesian closed categories (see Curry-Howard-Lambek correspondence). His last works were on pregroup grammar.\n\n\n\n\n"}
{"id": "5971818", "url": "https://en.wikipedia.org/wiki?curid=5971818", "title": "List of mathematicians (M)", "text": "List of mathematicians (M)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5971843", "url": "https://en.wikipedia.org/wiki?curid=5971843", "title": "List of mathematicians (Y)", "text": "List of mathematicians (Y)\n\n\n\n"}
{"id": "58989575", "url": "https://en.wikipedia.org/wiki?curid=58989575", "title": "Lynn Batten", "text": "Lynn Batten\n\nLynn Margaret Batten (born 1948) is a Canadian mathematician known for her books about finite geometry and cryptography, and for her research on the classification of malware.\n\nBatten earned her Ph.D. at the University of Waterloo in 1977.\nHer dissertation was \"D-Partition Geometries\".\n\nFormerly the Associate Dean for Academic and Industrial Research at the University of Manitoba, she holds the Deakin Chair in Mathematics at Deakin University in Australia, where she directs the information security group.\n\nDeakin is the author of:\n\n"}
{"id": "1775513", "url": "https://en.wikipedia.org/wiki?curid=1775513", "title": "Lévy's continuity theorem", "text": "Lévy's continuity theorem\n\nIn probability theory, Lévy’s continuity theorem (or Lévy's convergence theorem), named after the French mathematician Paul Lévy, connects convergence in distribution of the sequence of random variables with pointwise convergence of their characteristic functions. \nThis theorem is the basis for one approach to prove the central limit theorem and it is one of the major theorems concerning characteristic functions.\n\nSuppose we have\nIf the sequence of characteristic functions converges pointwise to some function \"formula_1\"\nthen the following statements become equivalent:\nRigorous proofs of this theorem are available.\n\n"}
{"id": "23630747", "url": "https://en.wikipedia.org/wiki?curid=23630747", "title": "Manhattan plot", "text": "Manhattan plot\n\nA Manhattan plot is a type of scatter plot, usually used to display data with a large number of data-points - many of non-zero amplitude, and with a distribution of higher-magnitude values, for instance in genome-wide association studies (GWAS). In GWAS Manhattan plots, genomic coordinates are displayed along the X-axis, with the negative logarithm of the association \"P\"-value for each single nucleotide polymorphism (SNP) displayed on the Y-axis, meaning that each dot on the Manhattan plot signifies a SNP. Because the strongest associations have the smallest \"P\"-values (e.g., 10), their negative logarithms will be the greatest (e.g., 15).\n\nIt gains its name from the similarity of such a plot to the Manhattan skyline: a profile of skyscrapers towering above the lower level \"buildings\" which vary around a lower height.\n"}
{"id": "13431536", "url": "https://en.wikipedia.org/wiki?curid=13431536", "title": "Material point method", "text": "Material point method\n\nThe material point method (MPM) is a numerical technique used to simulate the behavior of solids, liquids, gases, and any other continuum material. Especially, it is a robust spatial discretization method for simulating multi-phase (solid-fluid-gas) interactions. In the MPM, a continuum body is described by a number of small Lagrangian elements referred to as 'material points'. These material points are surrounded by a background mesh/grid that is used only to calculate gradient terms such as the deformation gradient. Unlike other mesh-based methods like the finite element method, finite volume method or finite difference method, the MPM is not a mesh based method and is instead categorized as a meshless/meshfree or continuum-based particle method, examples of which are smoothed particle hydrodynamics and peridynamics. Despite the presence of a background mesh, the MPM does not encounter the drawbacks of mesh-based methods (high deformation tangling, advection errors etc.) which makes it a promising and powerful tool in computational mechanics.\n\nThe MPM was originally proposed, as an extension of a similar method known as FLIP (a further extension of a method called PIC) to computational solid dynamics, in the early 1990 by Professors Deborah L. Sulsky, Zhen Chen and Howard L. Schreyer at University of New Mexico. After this initial development, the MPM has been further developed both in the national labs as well as the University of New Mexico, Oregon State University, University of Utah and more across the US and the world. Recently the number of institutions researching the MPM has been growing with added popularity and awareness coming from various sources such as the MPM's use in the Disney film \"Frozen\".\n\nAn MPM simulation consists of the following stages:\n\n\"(Prior to the time integration phase)\"\n\"(During the time integration phase - explicit formulation)\"\n\n\"2.\" Material point quantities are extrapolated to grid nodes.\n3. Equations of motion are solved on the grid.\n\nformula_174. Derivative terms are extrapolated back to material points.\n5.Resetting of grid.\n\nThe PIC was originally conceived to solve problems in fluid dynamics, and developed by Harlow at Los Alamos National Laboratory in 1957. One of the first PIC codes was the Fluid-Implicit Particle (FLIP) program, which was created by Brackbill in 1986 and has been constantly in development ever since. Until the 1990s, the PIC method was used principally in fluid dynamics.\n\nMotivated by the need for better simulating penetration problems in solid dynamics, Sulsky, Chen and Schreyer started in 1993 to reformulate the PIC and develop the MPM, with funding from Sandia National Laboratories. The original MPM was then further extended by Bardenhagen \"et al.\". to include frictional contact, which enabled the simulation of granular flow, and by Nairn to include explicit cracks and crack propagation (known as CRAMP).\n\nRecently, an MPM implementation based on a micro-polar Cosserat continuum has been used to simulate high-shear granular flow, such as silo discharge. MPM's uses were further extended into Geotechnical engineering with the recent development of a quasi-static, implicit MPM solver which provides numerically stable analyses of large-deformation problems in Soil mechanics.\n\nAnnual workshops on the use of MPM are held at various locations in the United States. The Fifth MPM Workshop was held at Oregon State University, in Corvallis, OR, on April 2 and 3, 2009.\n\nThe uses of the PIC or MPM method can be divided into two broad categories: firstly, there are many applications involving fluid dynamics, plasma physics, magnetohydrodynamics, and multiphase applications. The second category of applications comprises problems in solid mechanics.\n\nThe PIC method has been used to simulate a wide range of fluid-solid interactions, including sea ice dynamics, penetration of biological soft tissues, fragmentation of gas-filled canisters, dispersion of atmospheric pollutants, multiscale simulations coupling molecular dynamics with MPM, and fluid-membrane interactions. In addition, the PIC-based FLIP code has been applied in magnetohydrodynamics and plasma processing tools, and simulations in astrophysics and free-surface flow.\n\nAs a result of a joint effort between UCLA's mathematics department and Walt Disney Animation Studios, MPM was successfully used to simulate snow in the 2013 computer-animated film \"Frozen\".\n\nMPM has also been used extensively in solid mechanics, to simulate impact, penetration, collision and rebound, as well as crack propagation. MPM has also become a widely used method within the field of soil mechanics: it has been used to simulate granular flow, silo discharge, pile driving, bucket filling, and material failure; and to model soil stress distribution, compaction, and hardening. It is now being used in wood mechanics problems such as simulations of transverse compression on the cellular level including cell wall contact (this work received the George Marra Award for paper of the year from the Society of Wood Science and Technology )\n\nOne subset of numerical methods are Meshfree methods, which are defined as methods for which \"a predefined mesh is not necessary, at least in field variable interpolation\". Ideally, a meshfree method does not make use of a mesh \"throughout the process of solving the problem governed by partial differential equations, on a given arbitrary domain, subject to all kinds of boundary conditions,\" although existing methods are not ideal and fail in at least one of these respects. Meshless methods, which are also sometimes called particle methods, share a \"common feature that the history of state variables is traced at points (particles) which are not connected with any element mesh, the distortion of which is a source of numerical difficulties.\" As can be seen by these varying interpretations, some scientists consider MPM to be a meshless method, while others do not. All agree, however, that MPM is a particle method.\n\nThe Arbitrary Lagrangian Eulerian (ALE) methods form another subset of numerical methods which includes MPM. Purely \"Lagrangian\" methods employ a framework in which a space is discretised into initial subvolumes, whose flowpaths are then charted over time. Purely \"Eulerian\" methods, on the other hand, employ a framework in which the motion of material is described relative to a mesh that remains fixed in space throughout the calculation. As the name indicates, ALE methods combine Lagrangian and Eulerian frames of reference.\n\nPIC methods may be based on either the strong form collocation or a weak form discretisation of the underlying partial differential equation (PDE). Those based on the strong form are properly referred to as finite-volume PIC methods. Those based on the weak form discretisation of PDEs may be called either PIC or MPM.\n\nMPM solvers can model problems in one, two, or three spatial dimensions, and can also model axisymmetric problems. MPM can be implemented to solve either quasi-static or dynamic equations of motion, depending on the type of problem that is to be modeled.\n\nThe time-integration used for MPM may be either \"explicit\" or \"implicit\". The advantage to implicit integration is guaranteed stability, even for large timesteps. On the other hand, explicit integration runs much faster and is easier to implement.\n\nUnlike \"FEM\", MPM does not require periodical remeshing steps and remapping of state variables, and is therefore better suited to the modeling of large material deformations. In MPM, particles and not the mesh points store all the information on the state of the calculation. Therefore, no numerical error results from the mesh returning to its original position after each calculation cycle, and no remeshing algorithm is required.\n\nThe particle basis of MPM allows it to treat crack propagation and other discontinuities better than FEM, which is known to impose the mesh orientation on crack propagation in a material. Also, particle methods are better at handling history-dependent constitutive models.\n\nBecause in MPM nodes remain fixed on a regular grid, the calculation of gradients is trivial.\n\nIn simulations with two or more phases it is rather easy to detect contact between entities, as particles can interact via the grid with other particles in the same body, with other solid bodies, and with fluids.\n\nMPM is more expensive in terms of storage than other methods, as MPM makes use of mesh as well as particle data. MPM is more computationally expensive than FEM, as the grid must be reset at the end of each MPM calculation step and reinitialised at the beginning of the following step. Spurious oscillation may occur as particles cross the boundaries of the mesh in MPM, although this effect can be minimized by using generalized interpolation methods (GIMP). In MPM as in FEM, the size and orientation of the mesh can impact the results of a calculation: for example, in MPM, strain localisation is known to be particularly sensitive to mesh refinement.\n\nA commercial package based on a meshless method is MPMsim.\n\n"}
{"id": "21536099", "url": "https://en.wikipedia.org/wiki?curid=21536099", "title": "Modulus of convergence", "text": "Modulus of convergence\n\nIn real analysis, a branch of mathematics, a modulus of convergence is a function that tells how quickly a convergent sequence converges. These moduli are often employed in the study of computable analysis and constructive mathematics. \n\nIf a sequence of real numbers (\"x\") converges to a real number \"x\", then by definition for every real ε > 0 there is a natural number \"N\" such that if \"i\" > \"N\" then |\"x\" − \"x\"| < ε. A modulus of convergence is essentially a function that, given ε, returns a corresponding value of \"N\".\n\nSuppose that (\"x\") is a convergent sequence of real numbers with limit \"x\". There are two ways of defining a modulus of convergence as a function from natural numbers to natural numbers:\nThe latter definition is often employed in constructive settings, where the limit \"x\" may actually be identified with the convergent sequence. Some authors use an alternate definition that replaces 1/\"n\" with 2.\n\n\n"}
{"id": "146263", "url": "https://en.wikipedia.org/wiki?curid=146263", "title": "Monotone convergence theorem", "text": "Monotone convergence theorem\n\nIn the mathematical field of real analysis, the monotone convergence theorem is any of a number of related theorems proving the convergence of monotonic sequences (sequences that are increasing or decreasing) that are also bounded. Informally, the theorems state that if a sequence is increasing and bounded above by a supremum, then the sequence will converge to the supremum; in the same way, if a sequence is decreasing and is bounded below by an infimum, it will converge to the infimum.\n\nIf a sequence of real numbers is increasing and bounded above, then its supremum is the limit.\n\nLet formula_1 be such a sequence. By assumption, formula_1 is non-empty and bounded above. By the least-upper-bound property of real numbers, formula_3 exists and is finite. Now, for every formula_4, there exists formula_5 such that formula_6, since otherwise formula_7 is an upper bound of formula_1, which contradicts to the definition of formula_9. Then since formula_1 is increasing, and formula_9 is its upper bound, for every formula_12, we have formula_13. Hence, by definition, the limit of formula_1 is formula_15\n\nIf a sequence of real numbers is decreasing and bounded below, then its infimum is the limit.\n\nThe proof is similar to the proof for the case when the sequence is increasing and bounded above,\n\nIf formula_1 is a monotone sequence of real numbers (i.e., if \"a\" ≤ \"a\" for every \"n\" ≥ 1 or \"a\" ≥ \"a\" for every \"n\" ≥ 1), then this sequence has a finite limit if and only if the sequence is bounded.\n\n\nIf for all natural numbers \"j\" and \"k\", \"a\" is a non-negative real number and \"a\" ≤ \"a\", then\n\nThe theorem states that if you have an infinite matrix of non-negative real numbers such that\nthen the limit of the sums of the rows is equal to the sum of the series whose term \"k\" is given by the limit of column \"k\" (which is also its supremum). The series has a convergent sum if and only if the (weakly increasing) sequence of row sums is bounded and therefore convergent.\n\nAs an example, consider the infinite series of rows\n\nwhere \"n\" approaches infinity (the limit of this series is e). Here the matrix entry in row \"n\" and column \"k\" is\n\nthe columns (fixed \"k\") are indeed weakly increasing with \"n\" and bounded (by 1/\"k\"!), while the rows only have finitely many nonzero terms, so condition 2 is satisfied; the theorem now says that you can compute the limit of the row sums formula_22 by taking the sum of the column limits, namely formula_23.\n\nThe following result is due to Beppo Levi and Henri Lebesgue. In what follows, formula_24 denotes the formula_25-algebra of Borel sets on formula_26. By definition, formula_24 contains the set formula_28 and all Borel subsets of formula_29\n\nLet formula_30 be a measure space, and formula_31. Consider a pointwise non-decreasing sequence formula_32 of formula_33-measurable non-negative functions formula_34, i.e., for every formula_35 and every formula_36,\n\nSet the pointwise limit of the sequence formula_38 to be formula_39. That is, for every formula_40,\n\nThen formula_39 is formula_33-measurable and\n\nRemark 1. The integrals may be finite or infinite.\n\nRemark 2. The theorem remains true if its assumptions hold formula_45-almost everywhere. In other words, it is enough that there is a null set formula_5 such that the sequence formula_47 non-decreases for every formula_48 To see why this is true, we start with an observation that allowing the sequence formula_49 to pointwise non-decrease almost everywhere causes its pointwise limit formula_39 to be undefined on some null set formula_5. On that null set, formula_39 may then be defined arbitrarily, e.g. as zero, or in any other way that preserves measurability. To see why this will not affect the outcome of the theorem, note that since formula_53 we have, for every formula_54\n\nprovided that formula_39 is formula_33-measurable. (These equalities follow directly from the definition of Lebesgue integral for a non-negative function).\n\nRemark 3. Under assumptions of the theorem,\n(Note that the second chain of equalities follows from Remark 5).\n\nRemark 4. The proof below does not use any properties of Lebesgue integral except those established here. The theorem, thus, can be used to prove other basic properties, such as linearity, pertaining to Lebesgue integration.\n\nRemark 5 (monotonicity of Lebesgue integral). In the proof below, we apply the monotonic property of Lebesgue integral to non-negative functions only. Specifically (see Remark 4), let the functions formula_59 be formula_33-measurable.\n\n\n\nProof. Denote formula_67 the set of simple formula_68-measurable functions formula_69 such that\nformula_70 everywhere on formula_71 \n\n1. Since formula_72 we have\n\nBy definition of Lebesgue integral and the properties of supremum,\n\n2. Let formula_75 be the indicator function of the set formula_76 It can be deduced from the definition of Lebesgue integral that\n\nif we notice that, for every formula_78 formula_79 outside of formula_76 Combined with the previous property, the inequality formula_81 implies\n\nThis proof does \"not\" rely on Fatou's lemma. However, we do explain how that lemma might be used.\n\nFor those not interested in independent proof, the intermediate results below may be skipped.\nLemma 1. Let formula_30 be a measurable space. Consider a simple formula_33-measurable non-negative function formula_85. For a subset formula_86, define\nThen formula_88 is a measure on formula_89.\n\nMonotonicity follows from Remark 5. Here, we will only prove countable additivity, leaving the rest up to the reader. Let formula_90, where all the sets formula_91 are pairwise disjoint. Due to simplicity,\nfor some finite non-negative constants formula_93 and pairwise disjoint sets formula_94 such that formula_95. By definition of Lebesgue integral,\nSince all the sets formula_97 are pairwise disjoint, the countable additivity of formula_45\ngives us\nSince all the summands are non-negative, the sum of the series, whether this sum is finite or infinite, cannot change if summation order does. For that reason,\nas required.\n\nThe following property is a direct consequence of the definition of measure.\n\nLemma 2. Let formula_45 be a measure, and formula_102, where\nis a non-decreasing chain with all its sets formula_45-measurable. Then\n\nStep 1. We begin by showing that formula_39 is formula_107–measurable.\n\nNote. If we were using Fatou's lemma, the measurability would follow easily from Remark 3(a).\n\nTo do this \"without\" using Fatou's lemma, it is sufficient to show that the inverse image of an interval formula_108 under formula_39 is an element of the sigma-algebra formula_110 on formula_111, because (closed) intervals generate the Borel sigma algebra on the reals. Since formula_108 is a closed interval, and, for every formula_113, formula_114,\n\nThus,\n\nBeing the inverse image of a Borel set under a formula_33-measurable function formula_118, each set in the countable intersection is an element of formula_110. Since formula_25-algebras are, by definition, closed under countable intersections, this shows that formula_39 is formula_33-measurable, and the integral formula_123 is well defined (and possibly infinite).\n\nStep 2. We will first show that formula_124\n\nThe definition of formula_39 and monotonicity of formula_126 imply that formula_127, for every formula_113 and every formula_40. By monotonicity (or, more precisely, its narrower version established in Remark 5; see also Remark 4) of Lebesgue integral,\nand\nNote that the limit on the right exists (finite or infinite) because, due to monotonicity (see Remark 5 and Remark 4), the sequence is non-decreasing.\n\nEnd of Step 2.\n\nWe now prove the reverse inequality. We seek to show that\n\nProof using Fatou's lemma. Per Remark 3, the inequality we want to prove is equivalent to\nBut the latter follows immediately from Fatou's lemma, and the proof is complete.\n\nIndependent proof. To prove the inequality \"without\" using Fatou's lemma, we need some extra machinery. Denote formula_134 the set of simple formula_33-measurable functions formula_69 such that\nformula_137 on formula_111.\n\nStep 3. Given a simple function formula_139 and a real number formula_140, define\nThen formula_142, formula_143, and formula_144.\n\nStep 3a. To prove the first claim, let formula_145, for some finite collection of pairwise disjoint measurable sets formula_94 such that formula_147, some (finite) non-negative constants formula_148, and formula_149 denoting the indicator function of the set formula_150.\n\nFor every formula_151 formula_152 holds if and only if formula_153 Given that the sets formula_150 are pairwise disjoint,\n\nSince the pre-image formula_156 of the Borel set\nformula_157 under the measurable function formula_118 is measurable, and formula_25-algebras, by definition, are closed under finite intersection and unions, the first claim follows.\n\nStep 3b. To prove the second claim, note that, for each formula_113 and every formula_40, formula_162\n\nStep 3c. To prove the third claim, we show that formula_163.\n\nIndeed, if, to the contrary, formula_164, then an element\nexists such that formula_166, for every formula_113. Taking the limit as formula_168, we get\n\nStep 4. For every simple formula_33-measurable non-negative function formula_171,\nTo prove this, define formula_173. By Lemma 1, formula_174 is a measure on formula_89. By \"continuity from below\" (Lemma 2),\nas required.\n\nStep 5. We now prove that, for every formula_139,\n\nIndeed, using the definition of formula_179, the non-negativity of formula_118, and the monotonicity of Lebesgue integral (see Remark 5 and Remark 4), we have\nfor every formula_182. In accordance with Step 4, as formula_168, the inequality becomes\nTaking the limit as formula_185 yields\nas required.\n\nStep 6. We are now able to prove the reverse inequality, i.e.\n\nIndeed, by non-negativity, formula_188 and formula_189 For the calculation below, the non-negativity of formula_39 is essential. Applying the definition of Lebesgue integral and the inequality established in Step 5, we have\nThe proof is complete.\n\n"}
{"id": "52912473", "url": "https://en.wikipedia.org/wiki?curid=52912473", "title": "Multiverse (set theory)", "text": "Multiverse (set theory)\n\nIn mathematical set theory, the multiverse view is that there are many models of set theory, but no \"absolute\", \"canonical\" or \"true\" model. The various models are all equally valid or true, though some may be more useful or attractive than others. The opposite view is the \"universe\" view of set theory in which all sets are contained in some single ultimate model. The collection of countable transitive models of ZFC (in some universe) is called the hyperverse and is very similar to the \"multiverse\". \n\nA typical difference between the universe and multiverse views is the attitude to the continuum hypothesis. In the universe view the continuum hypothesis is a meaningful question that is either true or false though we have not yet been able to decide which. In the multiverse view it is meaningless to ask whether the continuum hypothesis is true or false before selecting a model of set theory. Another difference is that the statement \"For every transitive model of ZFC there is a larger model of ZFC in which it is countable\" is true in some versions of the multiverse view of mathematics but is false in the universe view.\n\n"}
{"id": "11934923", "url": "https://en.wikipedia.org/wiki?curid=11934923", "title": "Natural topology", "text": "Natural topology\n\nIn any domain of mathematics, a space has a natural topology if there is a topology on the space which is \"best adapted\" to its study within the domain in question. In many cases this imprecise definition means little more than the assertion that the topology in question arises \"naturally\" or \"canonically\" (see mathematical jargon) in the given context.\nNote that in some cases multiple topologies seem \"natural\". For example, if \"Y\" is a subset of a totally ordered set \"X\", then the induced order topology, i.e. the order topology of the totally ordered \"Y\", where this order is inherited from \"X\", is coarser than the subspace topology of the order topology of \"X\".\n\n\"Natural topology\" does quite often have a more specific meaning, at least given some prior contextual information: the natural topology is a topology which makes a natural map or collection of maps continuous. This is still imprecise, even once one has specified what the natural maps are, because there may be many topologies with the required property. However, there is often a finest or coarsest topology which makes the given maps continuous, in which case these are obvious candidates for \"the\" natural topology.\n\nThe simplest cases (which nevertheless cover \"many\" examples) are the initial topology and the final topology (Willard (1970)). The initial topology is the coarsest topology on a space \"X\" which makes a given collection of maps from \"X\" to topological spaces \"X\" continuous. The final topology is the finest topology on a space \"X\" which makes a given collection of maps from topological spaces \"X\" to \"X\" continuous.\n\nTwo of the simplest examples are the natural topologies of subspaces and quotient spaces.\n\nAnother example is that any metric space has a natural topology induced by its metric.\n\n\n"}
{"id": "22637", "url": "https://en.wikipedia.org/wiki?curid=22637", "title": "Object Management Group", "text": "Object Management Group\n\nThe Object Management Group (OMG) is an international, open membership, not-for-profit computer industry standards consortium with representation from government, industry and academia. OMG Task Forces develop enterprise integration standards for a wide range of technologies and an even wider range of industries. OMG modeling standards enable powerful visual design, execution and maintenance of software and other processes. \n\nOMG provides only specifications, and does not provide implementations. But before a specification can be accepted as a standard by OMG, the members of the submitter team must guarantee that they will bring a conforming product to market within a year. This is an attempt to prevent unimplemented (and unimplementable) standards.\n\nOther private companies or open source groups are encouraged to produce conforming products and OMG is attempting to develop mechanisms to enforce true interoperability.\n\nOMG hosts four technical meetings for its members and interested nonmembers. The Technical Meetings provide a neutral forum to discuss, develop and adopt standards that enable software interoperability for a wide range of industries including: business, finance, manufacturing, healthcare, robotics, software-based communications, security, government, space and more. Additionally, OMG holds four technical meetings per year. More information about these meetings can be found on the events page of their website.\n\nFounded in 1989 by eleven companies (including Hewlett-Packard, IBM, Sun Microsystems, Apple Computer, American Airlines, iGrafx, and Data General), OMG's initial focus was to create a heterogeneous distributed object standard. The founding executive team included Christopher Stone and John Slitz. As of November 2012, the leadership includes Chairman and CEO Richard Soley, President and COO Bill Hoffman and Vice President and Technical Director Larry L. Johnson. \n\nSince 2000, OMG international headquarters has been located in Needham, Massachusetts. In November 2012, the headquarters was moved from 140 Kendrick St to 109 Highland Ave.\n\nThe goal was a common portable and interoperable object model with methods and data that work using all types of development environments on all types of platforms.\n\nIn 1997, the Unified Modeling Language (UML) was added to the list of OMG adopted technologies. UML is a standardized general-purpose modeling language in the field of object-oriented software engineering.\n\nIn June 2005, the Business Process Management Initiative (BPMI.org) and OMG announced the merger of their respective Business Process Management (BPM) activities to form the Business Modeling and Integration Domain Task Force (BMI DTF).\n\nIn 2006 the Business Process Model and Notation (BPMN) was adopted as a standard by OMG.\n\nIn 2007 the Business Motivation Model (BMM) was adopted as a standard by the OMG. The BMM is a metamodel that provides a vocabulary for corporate governance and strategic planning and is particularly relevant to businesses undertaking governance, regulatory compliance, business transformation and strategic planning activities.\n\nIn 2009 OMG, together with the Software Engineering Institute at Carnegie Mellon, launched the Consortium of IT Software Quality (CISQ). CISQ brings together industry executives from Global 2000 IT organizations, system integrators, outsourcers, and package vendors to jointly address the challenge of standardizing the measurement of IT software quality and to promote a market-based ecosystem to support its deployment.\n\nIn 2011 OMG formed the Cloud Standards Customer Council. Founding sponsors included CA, IBM, Kaavo, Rackspace and Software AG. The CSCC is an OMG end user advocacy group dedicated to accelerating cloud's successful adoption, and drilling down into the standards, security and interoperability issues surrounding the transition to the cloud. The Council is not a standards organization, but will complement existing cloud standards efforts and establish a core set of client-driven requirements to ensure cloud users will have the same freedom of choice, flexibility, and openness they have with traditional IT environments. CSCC is open to all end-user organizations.\n\nIn September 2011, the OMG Board of Directors unanimously voted to adopt the Vector Signal and Image Processing Library (VSIPL) as the latest OMG specification. Work for adopting the specification was led by Mentor Graphics' Embedded Software Division, RunTime Computing Solutions, The Mitre Corporation as well as the High Performance Embedded Computing Software Initiative (HPEC-SI). VSIPL is an application programming interface (API) defined by an open standard developed by embedded signal and image processing hardware and software vendors, academia, application developers, and government labs. VSIPL and VSIPL++ contain hundreds of functions used for common signal processing kernel and other computations. These functions include basic arithmetic, trigonometric, transcendental, signal processing, linear algebra, and image processing. The VSIPL family of libraries has been implemented by multiple vendors for a range of processor architectures, including x86, PowerPC, Cell, and NVIDIA GPUs. VSIPL and VSIPL++ are designed to achieve high performance, increase programmer productivity and maintain portability across a range of processor architectures. Additionally, VSIPL++ was designed from the start to include support for parallelism.\n\nLate 2012 early 2013, The Object Management Group Board of Directors adopted the Automated Function Point (AFP) specification. The push for adoption was led by the Consortium for IT Software Quality (CISQ). AFP provides a standard for automating the popular function point measure according to the counting guidelines of the International Function Point User Group (IFPUG).\n\nOn March 27 2014, OMG announced it would be managing the newly formed Industrial Internet Consortium (IIC). An open-membership, not-for-profit, the IIC will take the lead in establishing interoperability across industrial environments for a more connected world.\n\nThe Industrial Internet Consortium® (IIC™) is the world’s leading membership program transforming business and society by accelerating the Industrial Internet of Things (IIoT). Its mission is to accelerate growth of the Industrial Internet by coordinating ecosystem initiatives to connect and integrate objects with people, processes and data using common architectures, interoperability and open standards that lead to transformational business outcomes. Launched in 2014 by AT&T, Cisco, GE, IBM and Intel, the IIC delivers a trustworthy IIoT in which the world’s systems and devices are securely connected and controlled to deliver transformational outcomes.   \n\nThe Consortium for IT Software Quality™ (CISQ™) is an IT leadership group that develops international standards for automating the measurement of software size and structural quality from the source code. The standards written by CISQ enable IT and business leaders to measure the risk IT applications pose to the business, as well as estimate the cost of ownership. CISQ was co-founded by the Object Management Group and Software Engineering Institute (SEI) at Carnegie Mellon University. More information is available on their website.\n\nOf the many standards maintained by the OMG, 11 have been ratified as ISO standards. These standards are:\nOMG offers a number of professional certifications: \n\n"}
{"id": "547426", "url": "https://en.wikipedia.org/wiki?curid=547426", "title": "Paolo Ruffini", "text": "Paolo Ruffini\n\nPaolo Ruffini (September 22, 1765 – May 10, 1822) was an Italian mathematician and philosopher.\n\nBy 1788 he had earned university degrees in philosophy, medicine/surgery, and mathematics. Among his work was an incomplete proof (Abel–Ruffini theorem) that quintic (and higher-order) equations cannot be solved by radicals (1799), and Ruffini's rule which is a quick method for polynomial division. Ruffini also made contributions to group theory in addition to probability and quadrature of the circle.\n\nHe practiced as both a professor of mathematics (University of Modena) and a medical doctor including scientific work on typhus.\n\nRuffini’s 1799 work marked a major development for group theory. Ruffini developed Joseph Louis Lagrange's work on permutation theory, following 29 years after Lagrange’s \"Réflexions sur la théorie algébrique des equations\" (1770–1771) which was largely ignored until Ruffini who established strong connections between permutations and the solvability of algebraic equations. Ruffini was the first to controversially assert the unsolvability by radicals of algebraic equations higher than quartics. This angered many members of the community such as Malfatti (1731–1807). Work in this area was later carried on by those such as Abel and Galois who succeeded in such a proof.\n\n\n"}
{"id": "4185466", "url": "https://en.wikipedia.org/wiki?curid=4185466", "title": "Parrondo's paradox", "text": "Parrondo's paradox\n\nParrondo's paradox, a paradox in game theory, has been described as: \"A combination of losing strategies becomes a winning strategy\". It is named after its creator, Juan Parrondo, who discovered the paradox in 1996. A more explanatory description is:\n\nParrondo devised the paradox in connection with his analysis of the Brownian ratchet, a thought experiment about a machine that can purportedly extract energy from random heat motions popularized by physicist Richard Feynman. However, the paradox disappears when rigorously analyzed. More recently, problems in evolutionary biology and ecology have been modeled and explained in terms of the paradox.\n\nConsider an example in which there are two points A and B having the same altitude, as shown in Figure 1. In the first case, we have a flat profile connecting them. Here, if we leave some round marbles in the middle that move back and forth in a random fashion, they will roll around randomly but towards both ends with an equal probability. Now consider the second case where we have a saw-tooth-like region between them. Here also, the marbles will roll towards either ends with equal probability (if there were a tendency to move in one direction, marbles in a ring of this shape would tend to spontaneously extract thermal energy to revolve, violating the second law of thermodynamics). Now if we tilt the whole profile towards the right, as shown in Figure 2, it is quite clear that both these cases will become biased towards B.\n\nNow consider the game in which we alternate the two profiles while judiciously choosing the time between alternating from one profile to the other.\nWhen we leave a few marbles on the first profile at point E, they distribute themselves on the plane showing preferential movements towards point B. However, if we apply the second profile when some of the marbles have crossed the point C, but none have crossed point D, we will end up having most marbles back at point E (where we started from initially) but some also in the valley towards point A given sufficient time for the marbles to roll to the valley. Then we again apply the first profile and repeat the steps (points C, D and E now shifted one step to refer to the final valley closest to A). If no marbles cross point C before the first marble crosses point D, we must apply the second profile shortly \"before\" the first marble crosses point D, to start over.\n\nIt easily follows that eventually we will have marbles at point A, but none at point B. Hence for a problem defined with having marbles at point A being a win and having marbles at point B a loss, we clearly win by playing two losing games.\n\nA second example of Parrondo's paradox is drawn from the field of gambling. Consider playing two games, Game A and Game B with the following rules. For convenience, define formula_1 to be our capital at time \"t\", immediately before we play a game.\n\nIt is clear that by playing Game A, we will almost surely lose in the long run. Harmer and Abbott show via simulation that if formula_10 and formula_11 Game B is an almost surely losing game as well. In fact, Game B is a Markov chain, and an analysis of its state transition matrix (again with M=3) shows that the steady state probability of using coin 2 is 0.3836, and that of using coin 3 is 0.6164. As coin 2 is selected nearly 40% of the time, it has a disproportionate influence on the payoff from Game B, and results in it being a losing game.\n\nHowever, when these two losing games are played in some alternating sequence - e.g. two games of A followed by two games of B (AABBAABB...), the combination of the two games is, paradoxically, a \"winning\" game. Not all alternating sequences of A and B result in winning games. For example, one game of A followed by one game of B (ABABAB...) is a losing game, while one game of A followed by two games of B (ABBABB...) is a winning game. This coin-tossing example has become the canonical illustration of Parrondo's paradox – two games, both losing when played individually, become a winning game when played in a particular alternating sequence. The apparent paradox has been explained using a number of sophisticated approaches, including Markov chains, flashing ratchets, Simulated Annealing and information theory. One way to explain the apparent paradox is as follows:\n\nThe role of formula_6 now comes into sharp focus. It serves solely to induce a dependence between Games A and B, so that a player is more likely to enter states in which Game B has a positive expectation, allowing it to overcome the losses from Game A. With this understanding, the paradox resolves itself: The individual games are losing only under a distribution that differs from that which is actually encountered when playing the compound game. In summary, Parrondo's paradox is an example of how dependence can wreak havoc with probabilistic computations made under a naive assumption of independence. A more detailed exposition of this point, along with several related examples, can be found in Philips and Feldman.\n\nFor a simpler example of how and why the paradox works, again consider two games Game A and Game B, this time with the following rules:\n\nSay you begin with $100 in your pocket. If you start playing Game A exclusively, you will obviously lose all your money in 100 rounds. Similarly, if you decide to play Game B exclusively, you will also lose all your money in 100 rounds.\n\nHowever, consider playing the games alternatively, starting with Game B, followed by A, then by B, and so on (BABABA...). It should be easy to see that you will steadily earn a total of $2 for every two games.\n\nThus, even though each game is a losing proposition if played alone, because the results of Game B are affected by Game A, the sequence in which the games are played can affect how often Game B earns you money, and subsequently the result is different from the case where either game is played by itself.\n\nParrondo's paradox is used extensively in game theory, and its application to engineering, population dynamics, financial risk, etc., are areas of active research, as demonstrated by the reading lists below. Parrondo's games are of little practical use such as for investing in stock markets as the original games require the payoff from at least one of the interacting games to depend on the player's capital. However, the games need not be restricted to their original form and work continues in generalizing the phenomenon. Similarities to volatility pumping and the two-envelope problem have been pointed out. Simple finance textbook models of security returns have been used to prove that individual investments with negative median long-term returns may be easily combined into diversified portfolios with positive median long-term returns. Similarly, a model that is often used to illustrate optimal betting rules has been used to prove that splitting bets between multiple games can turn a negative median long-term return into a positive one. In evolutionary biology, both bacterial random phase variation and the evolution of less accurate sensors have been modelled and explained in terms of the paradox. In ecology, the periodic alternation of certain organisms between nomadic and colonial behaviors has been suggested as a manifestation of the paradox.\n\nIn the early literature on Parrondo's paradox, it was debated whether the word 'paradox' is an appropriate description given that the Parrondo effect can be understood in mathematical terms. The 'paradoxical' effect can be mathematically explained in terms of a convex linear combination.\n\nHowever, Derek Abbott, a leading Parrondo's paradox researcher, provides the following answer regarding the use of the word 'paradox' in this context:\nParrondo's paradox does not seem that paradoxical if one notes that it is actually a combination of three simple games: two of which have losing probabilities and one of which has a high probability of winning. To suggest that one can create a winning strategy with three such games is neither counterintuitive nor paradoxical.\n\n\n\n"}
{"id": "3121604", "url": "https://en.wikipedia.org/wiki?curid=3121604", "title": "Planar lamina", "text": "Planar lamina\n\nIn mathematics, a planar lamina is a closed set in a plane of mass formula_1 and surface density formula_2 such that:\n\nThe center of mass of the lamina is at the point\n\nwhere formula_5 moment of the entire lamina about the x-axis and formula_6 moment of the entire lamina about the y-axis.\n\nExample 1.\n\nFind the center of mass of a lamina with edges given by the lines formula_9 formula_10 and formula_11 where the density is given as formula_12. \n\ncenter of mass is at the point\n\nPlanar laminas can be used to determine moments of inertia, or center of mass.\n"}
{"id": "27077452", "url": "https://en.wikipedia.org/wiki?curid=27077452", "title": "Primitive abundant number", "text": "Primitive abundant number\n\nIn mathematics a primitive abundant number is an abundant number whose proper divisors are all deficient numbers.\n\nFor example, 20 is a primitive abundant number because:\n\nThe first few primitive abundant numbers are:\n\nThe smallest odd primitive abundant number is 945.\n\nA variant definition is abundant numbers having no abundant proper divisor . It starts: \n\nEvery multiple of a primitive abundant number is an abundant number.\n\nEvery abundant number is a multiple of a primitive abundant number or a multiple of a perfect number.\n\nEvery primitive abundant number is either a primitive semiperfect number or a weird number.\n\nThere are an infinite number of primitive abundant numbers.\n\nThe number of primitive abundant numbers less than or equal to \"n\" is formula_1\n"}
{"id": "30864970", "url": "https://en.wikipedia.org/wiki?curid=30864970", "title": "Principle of least privilege", "text": "Principle of least privilege\n\nIn information security, computer science, and other fields, the principle of least privilege (PoLP, also known as the principle of minimal privilege or the principle of least authority) requires that in a particular abstraction layer of a computing environment, every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\n\nThe principle means giving a user account or process only those privileges which are essential to perform its intended function. For example, a user account for the sole purpose of creating backups does not need to install software: hence, it has rights only to run backup and backup-related applications. Any other privileges, such as installing new software, are blocked. The principle applies also to a personal computer user who usually does work in a normal user account, and opens a privileged, password protected account (that is, a superuser) only when the situation absolutely demands it.\n\nWhen applied to users, the terms \"least user access\" or \"least-privileged user account\" (LUA) are also used, referring to the concept that all user accounts at all times should run with as few privileges as possible, and also launch applications with as few privileges as possible.\n\nThe principle of least privilege is widely recognized as an important design consideration in enhancing the protection of data and functionality from faults (fault tolerance) and malicious behavior (computer security).\n\nBenefits of the principle include:\n\n\nIn practice, there exist multiple competing definitions of true least privilege. As program complexity increases at an exponential rate, so do the number of potential issues, rendering a predictive approach impractical. Examples include the values of variables it may process, addresses it will need, or the precise time such things will be required. Object capability systems allow, for instance, deferring granting a single-use privilege until the time when it will be used. Currently, the closest practical approach is to eliminate privileges that can be manually evaluated as unnecessary. The resulting set of privileges typically exceeds the true minimum required privileges for the process.\n\nAnother limitation is the granularity of control that the operating environment has over privileges for an individual process. In practice, it is rarely possible to control a process's access to memory, processing time, I/O device addresses or modes with the precision needed to facilitate only the precise set of privileges a process will require.\n\nThe original formulation is from Jerome Saltzer:\n\nPeter J. Denning, in his paper \"Fault Tolerant Operating Systems\", set it in a broader perspective among four fundamental principles of fault tolerance.\n\nDynamic assignments of privileges was earlier discussed by Roger Needham in 1972.\n\nHistorically, the oldest instance of least privilege is probably the source code of \"login.c\", which begins execution with super-user permissions and—the instant they are no longer necessary—dismisses them via \"setuid()\" with a non-zero argument as demonstrated in the Version 6 Unix source code.\n\nThe kernel always runs with maximum privileges since it is the operating system core and has hardware access. One of the principal responsibilities of an operating system, particularly a multi-user operating system, is management of the hardware's availability and requests to access it from running processes. When the kernel crashes, the mechanisms by which it maintains state also fail. Even if there is a way for the CPU to recover without a hard reset, the code that resumes execution is not always what it should be. Security continues to be enforced, but the operating system cannot respond to the failure properly because detection of the failure was not possible. This is because kernel execution either halted or the program counter resumed execution from somewhere in endless, and—usually—non-functional loop.\n\nIf execution picks up, after the crash, by loading and running trojan code, the author of the trojan code can usurp control of all processes. The principle of least privilege forces code to run with the lowest privilege/permission level possible so that, in the event this occurs—or even if code execution picks up from an unexpected location—what resumes the code execution would not have the ability to perform malicious or undesirable things. One method used to accomplish this can be implemented in the microprocessor hardware. In the Intel x86 architecture, the manufacturer designed four (ring 0 through ring 3) running \"modes\".\n\nAs implemented in some operating systems, processes execute with a potential privilege set and an active privilege set. Such privilege sets are inherited from the parent as determined by the semantics of \"fork()\". An executable file that performs a privileged function—thereby technically constituting a component of the TCB, and concomitantly termed a trusted program or trusted process—may also be marked with a set of privileges, a logical extension of the notions of set user ID and set group ID. The inheritance of file privileges by a process are determined by the semantics of the \"exec()\" family of system calls. The precise manner in which potential process privileges, actual process privileges, and file privileges interact can become complex. In practice, least privilege is practiced by forcing a process to run with only those privileges required by the task. Adherence to this model is quite complex as well as error-prone.\n\nThe Trusted Computer System Evaluation Criteria (TCSEC) concept of trusted computing base (TCB) minimization is a far more stringent requirement that is only applicable to the functionally strongest assurance classes, \"viz.\", B3 and A1 (which are \"evidentiarily\" different but \"functionally\" identical).\n\nLeast privilege is often associated with privilege bracketing: that is, assuming necessary privileges at the last possible moment and dismissing them as soon as no longer strictly necessary, therefore ostensibly reducing fallout from erroneous code that unintentionally exploits more privilege than is merited. Least privilege has also been interpreted in the context of distribution of discretionary access control (DAC) permissions, for example asserting that giving user U read/write access to file F violates least privilege if U can complete his authorized tasks with only read permission.\n\n\n"}
{"id": "14098258", "url": "https://en.wikipedia.org/wiki?curid=14098258", "title": "Queue automaton", "text": "Queue automaton\n\nA queue machine or queue automaton is a finite state machine with the ability to store and retrieve data from an infinite-memory queue. It is a model of computation equivalent to a Turing machine, and therefore it can process the same class of formal languages.\n\nA queue machine can be defined as a six-tuple\n\n\nA machine \"configuration\" is an ordered pair of its state and queue contents formula_8, where formula_9 denotes the Kleene closure of formula_4. The starting configuration on an input string formula_11 is defined as formula_12, and the transition formula_13 from one configuration to the next is defined as:\n\nwhere formula_15 is a symbol from the queue alphabet, formula_16 is a sequence of queue symbols (formula_17), and formula_18. Note the \"first-in-first-out\" property of the queue in the relation.\n\nThe machine accepts a string formula_19 if after a finite number of transitions the starting configuration evolves to exhaust the string (reaching a null string formula_20), or formula_21\n\nWe can prove that a queue machine is equivalent to a Turing machine by showing that a queue machine can simulate a Turing machine and vice versa.\n\nA Turing machine can be simulated by a queue machine that keeps a copy of the Turing machine's contents in its queue at all times, with two special markers: one for the TM's head position, and one for the end of the tape; its transitions simulate those of the TM by running through the whole queue, popping off each of its symbols and re-enqueing either the popped symbol, or, near the head position, the equivalent of the TM transition's effect.\n\nA queue machine can be simulated by a Turing machine, but more easily by a multi-tape Turing machine, which is known to be equivalent to a normal single-tape machine.\nThe simulating queue machine reads input on one tape and stores the queue on the second, with pushes and pops defined by simple transitions to the beginning and end symbols of the tape. A formal proof of this is often an exercise in theoretical computer science courses.\n\nQueue machines offer a simple model on which to base computer architectures, programming languages, or algorithms.\n\n"}
{"id": "50963820", "url": "https://en.wikipedia.org/wiki?curid=50963820", "title": "Rebecca Goldin", "text": "Rebecca Goldin\n\nRebecca Freja Goldin is an American mathematician who works as a professor of mathematical sciences at George Mason University and director of the Statistical Assessment Service, a nonprofit organization associated with GMU that aims to improve the use of statistics in journalism. Her mathematical research concerns symplectic geometry, including work on Hamiltonian actions and symplectic quotients.\n\nAfter graduating with honors in mathematics from Harvard University, Goldin studied in France for a year with Bernard Teissier at the École Normale Supérieure, pursuing research on toric varieties.\nShe completed her Ph.D. in 1999 at the Massachusetts Institute of Technology under the supervision of Victor Guillemin.\n\nAfter postdoctoral research at the University of Maryland, she joined the GMU faculty in 2001.\n\nShe was the inaugural winner of the Ruth I. Michler Memorial Prize of the Association for Women in Mathematics (AWM), in 2007. She was also the 2008 AWM/MAA Falconer Lecturer, speaking on \"The Use and Abuse of Statistics in the Media\".\n\nShe was included in the 2019 class of fellows of the American Mathematical Society \"for contributions to differential geometry and service to the mathematical community, particularly in support of promoting mathematical and statistical thinking to a wide audience\".\n\n"}
{"id": "10533559", "url": "https://en.wikipedia.org/wiki?curid=10533559", "title": "Sampling risk", "text": "Sampling risk\n\nSampling risk is one of the many types of risks an auditor may face when performing the necessary procedure of audit sampling. Audit sampling exists because of the impractical and costly effects of examining all or 100% of a client's records or books. As a result, a \"sample\" of a client's accounts are examined.\nDue to the negative effects produced by sampling risk, an auditor may have to perform additional procedures which in turn can impact the overall efficiency of the audit. \n\nSampling risk represents the possibility that an auditor's conclusion based on a sample is different from that reached if the entire population were subject to audit procedure. The auditor may conclude that material misstatements exist, when in fact they do not; or material misstatements do not exist but in fact they do exist. Auditors can lower the sampling risk by increasing the sampling size.\n\nAlthough there are many types of risks associated with the audit process, each type primarily has an effect on the overall audit engagement. The effects produced by sampling risk generally can increase audit risk, the risk that an entity's financial statements will contain a material misstatement, though given an unqualified ('clean') audit report. Sampling risk can also increase detection risk which suggests the possibility that an auditor will not find material misstatements relating to the financial statements through substantive tests and analysis.\n\nAuditors must often make professional judgments in assessing sampling risk. When testing samples the auditor is primarily concerned with two aspects of sampling risk: \n\nRisk of accepting incorrect data: the sample supports the conclusion that the recorded account balance is not materially misstated when it is materially misstated.\n\nRisk of incorrect rejection: the risk that the sample supports the conclusion that the recorded amount balance is materially misstated when it is not materially misstated.\n\nIn addition, the auditor is concerned with sampling risk and its relationship with controls. Two types of sample risk/control risks are:\n\nAssessing too low: the risk that the assessed level of control risk based on the sample is less than the true operating effectiveness of the control.\n\nAssessing too high: the risk that the assessed level of control risk based on the sample is greater than the true operating effectiveness of the control.\n\nWhen selecting a sampling approach there are two approaches to audit sampling: non-statistical and statistical approach. Three ways that statistical sampling can assist the auditor are: to maximize the productivity with minimum wasted effort in designing the sample, to measure the sufficiency of the evidence taken during the audit, and to analyze the results. The statistical approach allows the auditor to measure the risk that is being sampled to help in reducing it to an acceptable level. With respect to performing samples, statistical sampling involves different kinds of costs such as training the auditors, designing individual samples to meet the requirements, and choosing the items to be examined. If there is insufficient audit evidence it is the responsibility of the auditor to choose between non statistical or the statistical sampling approaches considering their effectiveness and related costs.\n\nAlthough exercising careful judgment is crucial during every step of the sampling process, it is extremely necessary when choosing the non-statistical approach. This method does not include the use of tables or statistical percentages, but rather it relies upon professional judgment on the part of the auditor as well as the policy implemented by the firm. Under this approach, it is common practice for most accounting firms to create universal guidelines for auditors in order to determine a proper sample size. For example, if a given client's control risk is high, a firm would typically require a high sample size when selecting records.\n\nIn order to successfully gather a sample, it is important to consider the collection as a whole and the relevance of the particular items. The most common successful method is to select an even number of items which accurately represents the list as a whole. Selecting only large or small numbers could distort the sample which creates risk.\n\n"}
{"id": "23709110", "url": "https://en.wikipedia.org/wiki?curid=23709110", "title": "Special functions", "text": "Special functions\n\nSpecial functions are particular mathematical functions which have more or less established names and notations due to their importance in mathematical analysis, functional analysis, physics, or other applications.\n\nThere is no general formal definition, but the list of mathematical functions contains functions which are commonly accepted as special.\nMany special functions appear as solutions of differential equations or integrals of elementary functions. Therefore, tables of integrals usually include descriptions of special functions, and tables of special functions include most important integrals; at least, the integral representation of special functions. Because symmetries of differential equations are essential to both physics and mathematics, the theory of special functions is closely related to the theory of Lie groups and Lie algebras, as well as certain topics in mathematical physics.\n\nSymbolic computation engines usually recognize the majority of special functions. Not all such systems have efficient algorithms for the evaluation, especially in the complex plane.\n\nFunctions with established international notations are sin, cos, exp, erf, and erfc.\n\nSome special functions have several notations:\n\n\nSubscripts are often used to indicate arguments, typically integers. In a few cases, the semicolon (;) or even backslash (\\) is used as a separator. In this case, the translation to algorithmic languages admits ambiguity and may lead to confusion.\n\nSuperscripts may indicate not only exponentiation, but modification of a function. Examples (particularly with trigonometric functions and hyperbolic functions) include:\n\n\nMost special functions are considered as a function of a complex variable. They are analytic; the singularities and cuts are described; the differential and integral representations are known and the expansion to the Taylor series or asymptotic series are available. In addition, sometimes there exist relations with other special functions; a complicated special function can be expressed in terms of simpler functions. Various representations can be used for the evaluation; the simplest way to evaluate a function is to expand it into a Taylor series. However, such representation may converge slowly if at all. In algorithmic languages, rational approximations are typically used, although they may behave badly in the case of complex argument(s).\n\nWhile trigonometry can be codified as was clear already to expert mathematicians of the eighteenth century (if not before) the search for a complete and unified theory of special functions has continued since the nineteenth century. The high point of special function theory in the period 1800-1900 was the theory of elliptic functions; treatises that were essentially complete, such as that of Tannery and Molk, could be written as handbooks to all the basic identities of the theory. They were based on techniques from complex analysis.\n\nFrom that time onwards it would be assumed that analytic function theory, which had already unified the trigonometric and exponential functions, was a fundamental tool. The end of the century also saw a very detailed discussion of spherical harmonics.\n\nOf course the wish for a broad theory including as many as possible of the known special functions has its intellectual appeal, but it is worth noting other motivations. For a long time, the special functions were in the particular province of applied mathematics; applications to the physical sciences and engineering determined the relative importance of functions. In the days before the electronic computer, the ultimate compliment to a special function was the computation, by hand, of extended tables of its values. This was a capital-intensive process, intended to make the function available by look-up, as for the familiar logarithm tables. The aspects of the theory that then mattered might then be two:\n\n\nIn contrast, one might say, there are approaches typical of the interests of pure mathematics: asymptotic analysis, analytic continuation and monodromy in the complex plane, and the discovery of symmetry principles and other structure behind the façade of endless formulae in rows. There is not a real conflict between these approaches, in fact.\n\nThe twentieth century saw several waves of interest in special function theory. The classic \"Whittaker and Watson\" (1902) textbook sought to unify the theory by using complex variables; the G. N. Watson tome \"A Treatise on the Theory of Bessel Functions\" pushed the techniques as far as possible for one important type that particularly admitted asymptotics to be studied.\n\nThe later Bateman Manuscript Project, under the editorship of Arthur Erdélyi, attempted to be encyclopedic, and came around the time when electronic computation was coming to the fore and tabulation ceased to be the main issue.\n\nThe modern theory of orthogonal polynomials is of a definite but limited scope. Hypergeometric series became an intricate theory, in need of later conceptual arrangement. Lie groups, and in particular their representation theory, explain what a spherical function can be in general; from 1950 onwards substantial parts of classical theory could be recast in terms of Lie groups. Further, work on algebraic combinatorics also revived interest in older parts of the theory. Conjectures of Ian G. Macdonald helped to open up large and active new fields with the typical special function flavour. Difference equations have begun to take their place besides differential equations as a source for special functions.\n\nIn number theory, certain special functions have traditionally been studied, such as particular Dirichlet series and modular forms. Almost all aspects of special function theory are reflected there, as well as some new ones, such as came out of the monstrous moonshine theory.\n\n\n"}
{"id": "25430349", "url": "https://en.wikipedia.org/wiki?curid=25430349", "title": "Steve Shnider", "text": "Steve Shnider\n\nSteve Shnider is professor of mathematics at Bar Ilan University.\nHe received a PhD in Mathematics from Harvard University in 1972, under Shlomo Sternberg.\nHis main interests are in the differential geometry of fiber bundles; algebraic methods in the theory of deformation of geometric structures; symplectic geometry; supersymmetry; operads; and Hopf algebras.\n\nA 2002 book of Markl, Shnider and Stasheff \"Operads in algebra, topology, and physics\" was the first book to provide a systematic treatment of operad theory, an area of mathematics that came to prominence in 1990s and found many applications in algebraic topology, category theory, graph cohomology, representation theory, algebraic geometry, combinatorics, knot theory, moduli spaces, and other areas. The book was the subject of a \"Featured Review\" in Mathematical Reviews by Alexander A. Voronov which stated, in particular: \"The first book whose main goal is the theory of operads per se ... a book such as this one has been long awaited by a wide scientific readership, including mathematicians and theoretical physicists ... a great piece of mathematical literature and will be helpful to anyone who needs to use operads, from graduate students to mature mathematicians and physicists.\"\n\nAccording to Mathematical Reviews, Shnider's work has been cited over 300 times by over 300 authors by 2010.\n\n\n"}
{"id": "1458060", "url": "https://en.wikipedia.org/wiki?curid=1458060", "title": "Strict", "text": "Strict\n\nIn mathematical writing, the adjective strict is used to modify technical terms which have multiple meanings. It indicates that the exclusive meaning of the term is to be understood. (More formally, one could say that this is the meaning which implies the other meanings.) The opposite is non-strict. This is often implicit but can be put explicitly for clarity. In some contexts the word \"proper\" is used as a mathematical synonym for \"strict\".\n\nThis term is commonly used in the context of inequalities — the phrase \"strictly less than\" means \"less than and not equal to\" (likewise \"strictly greater than\" means \"greater than and not equal to\"). More generally a strict partial order, strict total order and strict weak order exclude equality and equivalence\n\nA related use occurs when comparing numbers to zero — \"strictly positive\" and \"strictly negative\" mean \"positive and not equal to zero\" and \"negative and not equal to zero\", respectively. Also, in the context of functions, the adverb \"strictly\" is used to modify the terms \"monotonic\", \"increasing\", and \"decreasing\".\n\nOn the other hand, sometimes one wants to specify the inclusive meanings of terms. In the context of comparisons, one can use the phrases \"non-negative\", \"non-positive\", \"non-increasing\", and \"non-decreasing\" to make it clear that the inclusive sense of the terms is intended.\n\nUsing such terminology helps avoid possible ambiguity and confusion. For instance, upon reading the phrase \"\"x\" is positive\", it is not immediately clear whether \"x\" = 0 is possible, since some authors might use the term \"positive\" loosely, and mean that \"x\" is not less than zero. Therefore, it is prudent to write \"\"x\" is strictly positive\" for \"x\" > 0 and \"\"x\" is non-negative\" for \"x\" ≥ 0. (A precise term like \"non-negative\" is never used with the word \"negative\" in the wide, loose sense that includes zero.)\n\nThe word \"proper\" is often used in the same way as \"strict.\" For example, a \"proper subset\" of a set \"S\" is a subset that is not equal to \"S\" itself, and a \"proper class\" is a class which is not also a set. \n"}
{"id": "1127460", "url": "https://en.wikipedia.org/wiki?curid=1127460", "title": "Sylvester's law of inertia", "text": "Sylvester's law of inertia\n\nSylvester's law of inertia is a theorem in matrix algebra about certain properties of the coefficient matrix of a real quadratic form that remain invariant under a change of basis. Namely, if \"A\" is the symmetric matrix that defines the quadratic form, and \"S\" is any invertible matrix such that \"D\" = \"SAS\" is diagonal, then the number of negative elements in the diagonal of \"D\" is always the same, for all such \"S\"; and the same goes for the number of positive elements.\n\nThis property is named after James Joseph Sylvester who published its proof in 1852.\n\nLet \"A\" be a symmetric square matrix of order \"n\" with real entries. Any non-singular matrix \"S\" of the same size is said to transform \"A\" into another symmetric matrix , also of order \"n\", where \"S\" is the transpose of \"S\". It is also said that matrices \"A\" and \"B\" are congruent. If \"A\" is the coefficient matrix of some quadratic form of R, then \"B\" is the matrix for the same form after the change of basis defined by \"S\".\n\nA symmetric matrix \"A\" can always be transformed in this way into a diagonal matrix \"D\" which has only entries 0, +1 and −1 along the diagonal. Sylvester's law of inertia states that the number of diagonal entries of each kind is an invariant of \"A\", i.e. it does not depend on the matrix \"S\" used.\n\nThe number of +1s, denoted \"n\", is called the positive index of inertia of \"A\", and the number of −1s, denoted \"n\", is called the negative index of inertia. The number of 0s, denoted \"n\", is the dimension of the kernel of \"A\", and also the corank of \"A\". These numbers satisfy an obvious relation\n\nThe difference is usually called the signature of \"A\". (However, some authors use that term for the triple consisting of the corank and the positive and negative indices of inertia of \"A\"; for a non-degenerate form of a given dimension these are equivalent data, but in general the triple yields more data.)\n\nIf the matrix \"A\" has the property that every principal upper left minor \"Δ\" is non-zero then the negative index of inertia is equal to the number of sign changes in the sequence\n\nThe law can also be stated as follows: two symmetric square matrices of the same size have the same number of positive, negative and zero eigenvalues if and only if they are congruent (formula_3, formula_4 non-singular).\n\nThe positive and negative indices of a symmetric matrix \"A\" are also the number of positive and negative eigenvalues of \"A\". Any symmetric real matrix \"A\" has an eigendecomposition of the form \"QEQ\" where \"E\" is a diagonal matrix containing the eigenvalues of \"A\", and \"Q\" is an orthonormal square matrix containing the eigenvectors. The matrix \"E\" can be written \"E\" = \"WDW\" where \"D\" is diagonal with entries 0, +1, or −1, and \"W\" is diagonal with \"W\" = √|\"E\"|. The matrix \"S\" = \"QW\" transforms \"D\" to \"A\".\n\nIn the context of quadratic forms, a real quadratic form \"Q\" in \"n\" variables (or on an \"n\"-dimensional real vector space) can by a suitable change of basis (by non-singular linear transformation from x to y) be brought to the diagonal form\n\nwith each \"a\" ∈ {0, 1, −1}. Sylvester's law of inertia states that the number of coefficients of a given sign is an invariant of \"Q\", i.e., does not depend on a particular choice of diagonalizing basis. Expressed geometrically, the law of inertia says that all maximal subspaces on which the restriction of the quadratic form is positive definite (respectively, negative definite) have the same dimension. These dimensions are the positive and negative indices of inertia.\n\nSylvester's law of inertia is also valid if \"A\" and \"B\" have complex entries. In this case, it is said that \"A\" and \"B\" are *-congruent if and only if there exists a non-singular complex matrix \"S\" such that .\n\nIn the complex scenario, a way to state Sylvester's law of inertia is that if \"A\" and \"B\" are Hermitian matrices, then \"A\" and \"B\" are *-congruent if and only if they have the same inertia. A theorem due to Ikramov generalizes the law of inertia to any normal matrices \"A\" and \"B\":\n\nIf \"A\" and \"B\" are normal matrices, then \"A\" and \"B\" are congruent if and only if they have the same number of eigenvalues on each open ray from the origin in the complex plane.\n\n\n"}
{"id": "40044024", "url": "https://en.wikipedia.org/wiki?curid=40044024", "title": "Symmetry in the Quran", "text": "Symmetry in the Quran\n\nSymmetry in the Quran is the belief held by some Muslims, that the word and verse structure of the Quran exhibits such complicated symmetry that it could be neither random placements, nor the intentional work of man, but rather proof of \"the will of God\". There are many examples of such symmetry. Using a program called QuranCode and other analysis programs, scholars have found many patterns.\n\nAn example is that chapter 76, called Al-Insan, has 247 words, and three mentions of the word silver. The middle word of the chapter is word number 124, which is the word \"silver\". It is in a verse that is the middle verse of the chapter (verse 16 of 31), and it is the third of five words of the verse. \n\nSome of this symmetry is caused by the significance of the number 19 in Islam. It is claimed that if all the numbers mentioned in the Quran are added up, their sum is 162,146, which is 19 times 6534; That there are 114 (19 times 6) Surahs; that Muhammad's first revelation consists of a verse that has 19 words and 76 (4 times 19) letters.\n\nAnother aspect of the Quran's symmetry, which was discovered using linguistic computer searching algorithms (Quran Corpus, an open-source software), includes the number of times certain words and their forms are used in the whole Quran. Some of these symmetrical patterns include:\n\n\n"}
{"id": "57865301", "url": "https://en.wikipedia.org/wiki?curid=57865301", "title": "Tapered floating point", "text": "Tapered floating point\n\nIn computing, tapered floating point (TFP) refers to a format similar to floating point, but with variable-sized entries for the significand and exponent instead of the fixed-length entries found in normal floating-point formats. In addition to this, tapered floating-point formats provide a fixed-size pointer entry indicating the number of digits in the exponent entry. The number of digits of the significand entry (including the sign) results from the difference of the fixed total length minus the length of the exponent and pointer entries.\n\nThe tapered floating-point scheme was first proposed by Robert Morris of Bell Laboratories in 1971, and refined with \"leveling\" by Masao Iri and Shouichi Matsui of University of Tokyo in 1981, and by Hozumi Hamada of Hitachi, Ltd.\n\nAlan Feldstein of Arizona State University and Peter Turner of Clarkson University described a tapered scheme resembling a conventional floating-point system except for the overflow or underflow conditions.\n\nIn 2013, John Gustafson proposed the Unum number system, a variant of tapered floating-point arithmetic with an \"exact\" bit added to the representation and some interval interpretation to the non-exact values.\n\n\n"}
{"id": "16876079", "url": "https://en.wikipedia.org/wiki?curid=16876079", "title": "The American Statistician", "text": "The American Statistician\n\nThe American Statistician is a quarterly peer-reviewed scientific journal covering statistics published by Taylor & Francis on behalf of the American Statistical Association. It was established in 1947 and the editor-in-chief is Daniel R. Jeske (University of California, Riverside).\n"}
{"id": "11945140", "url": "https://en.wikipedia.org/wiki?curid=11945140", "title": "The New York Journal of Mathematics", "text": "The New York Journal of Mathematics\n\nThe New York Journal of Mathematics is a peer-reviewed journal focusing on algebra, analysis, geometry and topology. Its editorial board, , consists of 17 university-affiliated scholars in addition to the Editor-in-chief. Articles in the \"New York Journal of Mathematics\" are published entirely electronically (on the World Wide Web). The journal uses the diamond open access model—that is, its full content is available to anyone via the Internet, without a subscription or fee.\n\nThe journal was founded in 1994 by Mark Steinberger who cited a 1993 letter by John Franks as inspiration. At the time of its launch, the \"New York Journal of Mathematics\" was the \"first electronic general mathematics journal\", predating the online versions of both \"Zentralblatt MATH\" and the journals in \"Mathematical Reviews\". It was published by the State University of New York at Albany where Steinberger had been a professor since 1987.\n\nSteinberger justified the stylistic choices of the journal by writing, \"Some proponents of electronic publication have urged changes in style, citing the low price of disk space as a rationale for publishing articles more loquacious than those commonly acceptable in a print medium. We decided to eschew this route, on the grounds that the perceived quality of our publications would be reduced. We feel it is important to follow the standards of consensus in the field. If these standards change in the future, we will change\nwith them.\"\n\nWhen the \"New York Journal of Mathematics\" was first published, it was made available via FTP and Gopher for users without a web browser. The papers, typeset in TeX, were originally downloadable in the PostScript format. PDF support was added in 1996. To incorporate hyperlinks within documents, the journal leveraged software that had been developed for the arXiv preprint server.\n\nIn 1998, the journal began including links to relevant reviews on MathSciNet with its published articles. It is listed in the Journals section of The Electronic Library of Mathematics. Articles from 2010 and later are available on Web of Science.\n\nA paper on the variability hypothesis by Theodore Hill and Sergei Tabachnikov was accepted and retracted by \"The Mathematical Intelligencer\" and later the \"New York Journal of Mathematics\". There was some controversy over the mathematical model, the peer-review process, and the lack of an official retraction notice from the \"NYJM\".\n\nIn 2017, the journal had an impact factor of 0.551 and a Mathematical Citation Quotient of 0.56.\n\nIn a professional conference presentation, Renzo Piccinini said \"An example of what I consider a good electronic journal is the New York Journal of Mathematics; this is a refereed journal--with referees not in the editor's board—with high quality papers and very fast publication time; last, but not least, it is free!\"\n\n\n"}
{"id": "20029977", "url": "https://en.wikipedia.org/wiki?curid=20029977", "title": "The Story of Maths", "text": "The Story of Maths\n\nThe Story of Maths is a four-part British television series outlining aspects of the history of mathematics. It was a co-production between the Open University and the BBC and aired in October 2008 on BBC Four. The material was written and presented by University of Oxford professor Marcus du Sautoy. The consultants were the Open University academics Robin Wilson, professor Jeremy Gray and June Barrow-Green. Kim Duke is credited as series producer.\n\nThe series comprised four programmes respectively titled: \"The Language of the Universe\"; \"The Genius of the East\"; \"The Frontiers of Space\"; and\" To Infinity and Beyond\". Du Sautoy documents the development of mathematics covering subjects such as the invention of zero and the unproven Riemann hypothesis, a 150-year-old problem for whose solution the Clay Mathematics Institute has offered a $1,000,000 prize. He escorts viewers through the subject's history and geography. He examines the development of key mathematical ideas and shows how mathematical ideas underpin the world's science, technology, and culture.\n\nHe starts his journey in ancient Egypt and finishes it by looking at current mathematics. Between he travels through Babylon, Greece, India, China, and the medieval Middle East. He also looks at mathematics in Europe and then in America and takes the viewers inside the lives of many of the greatest mathematicians.\n\nIn this opening programme Marcus du Sautoy looks at how important and fundamental mathematics is to our lives before looking at the mathematics of ancient Egypt, Mesopotamia, and Greece.\n\nDu Sautoy commences in Egypt where recording the patterns of the seasons and in particular the flooding of the Nile was essential to their economy. There was a need to solve practical problems such as land area for taxation purposes. Du Sautoy discovers the use of a decimal system based on the fingers on the hands, the unusual method for multiplication and division. He examines the Rhind Papyrus, the Moscow Papyrus and explores their understanding of binary numbers, fractions and solid shapes.\n\nHe then travels to Babylon and discovered that the way we tell the time today is based on the Babylonian 60 base number system. So because of the Babylonians we have 60 seconds in a minute, and 60 minutes in an hour. He then shows how the Babylonians used quadratic equations to measure their land. He deals briefly with Plimpton 322.\n\nIn Greece, the home of ancient Greek mathematics, he looks at the contributions of some of its greatest and well known mathematicians including Pythagoras, Plato, Euclid, and Archimedes, who are some of the people who are credited with beginning the transformation of mathematics from a tool for counting into the analytical subject we know today. A controversial figure, Pythagoras’ teachings were considered suspect and his followers seen as social outcasts and a little be strange and not in the norm. There is a legend going around that one of his followers, Hippasus, was drowned when he announced his discovery of irrational numbers. As well as his work on the properties of right angled triangles, Pythagoras developed another important theory after observing musical instruments. He discovered that the intervals between harmonious musical notes are always in whole number intervals. It deals briefly with Hypatia of Alexandria.\n\nWith the decline of ancient Greece, the development of maths stagnated in Europe. However the progress of mathematics continued in the East. Du Sautoy describes both the Chinese use of maths in engineering projects and their belief in the mystical powers of numbers. He mentions Qin Jiushao.\n\nHe describes Indian mathematicians’ invention of trigonometry; their introduction of a symbol for the number zero and their contribution to the new concepts of infinity and negative numbers. It shows Gwalior Fort where zero is inscribed on its walls. It mentions the work of Brahmagupta and Bhāskara II on the subject of zero. He mentions Madhava of Sangamagrama and Aryabhata and illustrates the - historically first exact - formula for calculating the π (pi).\n\nDu Sautoy then considers the Middle East: the invention of the new language of algebra and the evolution of a solution to cubic equations. He talks about the House of Wisdom with Muhammad ibn Mūsā al-Khwārizmī and he visits University of Al-Karaouine. He mentions Omar Khayyám.\n\nFinally he examines the spread of Eastern knowledge to the West through mathematicians such as Leonardo Fibonacci, famous for the Fibonacci sequence. He mentions Niccolò Fontana Tartaglia.\n\nFrom the seventeenth century, Europe replaced the Middle East as the engine house of mathematical ideas. Du Sautoy visits Urbino to introduce perspective using mathematician and artist, Piero della Francesca's \"The Flagellation of Christ\".\n\nDu Sautoy proceeds to describes René Descartes realisation that it was possible to describe curved lines as equations and thus link algebra and geometry. He talks with Henk J. M. Bos about Descartes. He shows how one of Pierre de Fermat’s theorems is now the basis for the codes that protect credit card transactions on the internet. He describes Isaac Newton’s development of math and physics crucial to understanding the behaviour of moving objects in engineering. He covers the Leibniz and Newton calculus controversy and the Bernoulli family. He further covers Leonhard Euler, the father of topology, and Gauss' invention of a new way of handling equations, modular arithmetic. He mentions János Bolyai.\n\nThe further contribution of Gauss to our understanding of how prime numbers are distributed is covered thus providing the platform for Bernhard Riemann's theories on prime numbers. In addition Riemann worked on the properties of objects, which he saw as manifolds that could exist in multi-dimensional space.\n\nThe final episode considers the great unsolved problems that confronted mathematicians in the 20th century. On 8 August 1900 David Hilbert gave a historic talk at the International Congress of Mathematicians in Paris. Hilbert posed twenty-three then unsolved problems in mathematics which he believed were of the most immediate importance. Hilbert succeeded in setting the agenda for 20thC mathematics and the programme commenced with Hilbert's first problem.\n\nGeorg Cantor considered the infinite set of whole numbers 1, 2, 3 ... ∞ which he compared with the smaller set of numbers 10, 20, 30 ... ∞. Cantor showed that these two infinite sets of numbers actually had the same size as it was possible to pair each number up; 1 - 10, 2 - 20, 3 - 30 ... etc.\n\nIf fractions now are considered there are an infinite number of fractions between any of the two whole numbers, suggesting that the infinity of fractions is bigger than the infinity of whole numbers. Yet Cantor was still able to pair each such fraction to a whole number 1 - /; 2 - /; 3 - / ... etc. through to ∞; i.e. the infinities of both fractions and whole numbers were shown to have the same size.\n\nBut when the set of all infinite decimal numbers was considered, Cantor was able to prove that this produced a bigger infinity. This was because, no matter how one tried to construct such a list, Cantor was able to provide a new decimal number that was missing from that list. Thus he showed that there were different infinities, some bigger than others.\n\nHowever, there was a problem that Cantor was unable to solve: Is there an infinity sitting between the smaller infinity of all the fractions and the larger infinity of the decimals? Cantor believed, in what became known as the Continuum Hypothesis, that there is no such set. This would be the first problem listed by Hilbert.\n\nNext Marcus discusses Henri Poincaré's work on the discipline of 'Bendy geometry'. If two shapes can be moulded or morphed to each other's shape then they have the same topology. Poincaré was able to identify all possible two-dimensional topological surfaces; however in 1904 he came up with a topological problem, the Poincaré conjecture, that he could not solve; namely what are all the possible shapes for a 3D universe.\n\nAccording to the programme, the question was solved in 2002 by Grigori Perelman who linked the problem to a different area of mathematics. Perelman looked at the dynamics of the way things can flow over the shape. This enabled him to find all the ways that 3D space could be wrapped up in higher dimensions.\n\nThe achievements of David Hilbert were now considered. In addition to Hilbert's problems, Hilbert space, Hilbert Classification and the Hilbert Inequality, du Sautoy highlights Hilbert's early work on equations as marking him out as a mathematician able to think in new ways. Hilbert showed that, while there were an infinity of equations, these equations could be constructed from a finite number of building block like sets. Hilbert could not construct that list of sets; he simply proved that it existed. In effect Hilbert had created a new more abstract style of Mathematics.\n\nFor 30 years Hilbert believed that mathematics was a universal language powerful enough to unlock all the truths and solve each of his 23 Problems. Yet, even as Hilbert was stating \"We must know, we will know\", Kurt Gödel had shattered this belief; he had formulated the Incompleteness Theorem based on his study of Hilbert's second problem:\n\nUsing a code based on prime numbers, Gödel was able to transform the above into a pure statement of arithmetic. Logically, the above cannot be false and hence Gödel had discovered the existence of mathematical statements that were true but were incapable of being proved.\n\nIn 1950s American mathematician Paul Cohen took up the challenge of Cantor's Continuum Hypothesis which asks \"is there is or isn't there an infinite set of number bigger than the set of whole numbers but smaller than the set of all decimals\". Cohen found that there existed two equally consistent mathematical worlds. In one world the Hypothesis was true and there did not exist such a set. Yet there existed a mutually exclusive but equally consistent mathematical proof that Hypothesis was false and there was such a set. Cohen would subsequently work on Hilbert's eighth problem, the Riemann hypothesis, although without the success of his earlier work.\n\nHilbert's tenth problem asked if there was some universal method that could tell whether any equation had whole number solutions or not. The growing belief was that no so such method was possible yet the question remained, how could you prove that, no matter how ingenious you were, you would never come up with such a method. He mentions Paul Cohen. To answer this Julia Robinson, who created the Robinson Hypothesis which stated that to show that there was no such method all you had to do was cook up one equation whose solutions were a very specific set of numbers: The set of numbers needed to grow exponentially yet still be captured by the equations at the heart of Hilbert's problem. Robinson was unable to find this set. This part of the solution fell to Yuri Matiyasevich who saw how to capture the Fibonacci sequence using the equations at the heart of Hilbert's tenth.\n\nThe final section briefly covers algebraic geometry. Évariste Galois had refined a new language for mathematics. Galois believed mathematics should be the study of structure as opposed to number and shape. Galois had discovered new techniques to tell whether certain equations could have solutions or not. The symmetry of certain geometric objects was the key. Galois' work was picked up by André Weil who built algebraic geometry, a whole new language. Weil's work connected number theory, algebra, topology and geometry.\n\nFinally du Sautoy mentions Weil's part in the creation of the fictional mathematician Nicolas Bourbaki and another contributor to Bourbaki's output - Alexander Grothendieck.\n\n\n"}
{"id": "35007920", "url": "https://en.wikipedia.org/wiki?curid=35007920", "title": "Thomas Bedwell", "text": "Thomas Bedwell\n\nThomas Bedwell (died April 1595) was an English mathematician and military engineer.\n\nBedwell matriculated as a sizar of Trinity College, Cambridge in November 1562. He became a scholar in the same year; in 1566–7 he took the degree of B.A.; he was subsequently elected fellow; and in 1570 commenced M.A. He was appointed to the office of keeper of the ordnance stores in the Tower. He is said to have been the first to project 'the bringing of the waters of the Lea from Ware to London.' In conjunction with Frederico Genebelli he was employed as a military engineer in strengthening the works at Tilbury and Gravesend at the time of the Spanish Armada. He died in April 1595.\n\nThomas Bedwell was uncle of William Bedwell, the Arabic scholar, who speaks of him as 'our English Tycho.' The two are sometimes confounded, chiefly, it would appear, on account of an ambiguity on the title-page of the first of two works published by the nephew in explanation of a 'ruler' or mesolabium architectonicum which the uncle had devised to facilitate carpenters' calculations.\n\n"}
