{"id": "3056879", "url": "https://en.wikipedia.org/wiki?curid=3056879", "title": "Adaptive resonance theory", "text": "Adaptive resonance theory\n\nAdaptive resonance theory (ART) is a theory developed by Stephen Grossberg and Gail Carpenter on aspects of how the brain processes information. It describes a number of neural network models which use supervised and unsupervised learning methods, and address problems such as pattern recognition and prediction.\n\nThe primary intuition behind the ART model is that object identification and recognition generally occur as a result of the interaction of 'top-down' observer expectations with 'bottom-up' sensory information. The model postulates that 'top-down' expectations take the form of a memory template or prototype that is then compared with the actual features of an object as detected by the senses. This comparison gives rise to a measure of category belongingness. As long as this difference between sensation and expectation does not exceed a set threshold called the 'vigilance parameter', the sensed object will be considered a member of the expected class. The system thus offers a solution to the 'plasticity/stability' problem, i.e. the problem of acquiring new knowledge without disrupting existing knowledge that is also called incremental learning.\n\nThe basic ART system is an unsupervised learning model. It typically consists of a comparison field and a recognition field composed of neurons, a vigilance parameter (threshold of recognition), and a reset module. The comparison field takes an input vector (a one-dimensional array of values) and transfers it to its best match in the recognition field. Its best match is the single neuron whose set of weights (weight vector) most closely matches the input vector. Each recognition field neuron outputs a negative signal (proportional to that neuron’s quality of match to the input vector) to each of the other recognition field neurons and thus inhibits their output. In this way the recognition field exhibits lateral inhibition, allowing each neuron in it to represent a category to which input vectors are classified.\n\nAfter the input vector is classified, the reset module compares the strength of the recognition match to the vigilance parameter. If the vigilance parameter is overcome (i.e. the input vector is within the normal range seen on previous input vectors), training commences: the weights of the winning recognition neuron are adjusted towards the features of the input vector. Otherwise, if the match level is below the vigilance parameter (i.e. the input vector's match is outside the normal expected range for that neuron) the winning recognition neuron is inhibited and a search procedure is carried out. In this search procedure, recognition neurons are disabled one by one by the reset function until the vigilance parameter is overcome by a recognition match. In particular, at each cycle of the search procedure the most active recognition neuron is selected and then switched off if its activation is below the vigilance parameter (note that it thus releases the remaining recognition neurons from its inhibition). If no committed recognition neuron’s match overcomes the vigilance parameter, then an uncommitted neuron is committed and its weights are adjusted towards matching the input vector. The vigilance parameter has considerable influence on the system: higher vigilance produces highly detailed memories (many, fine-grained categories), while lower vigilance results in more general memories (fewer, more-general categories).\n\nThere are two basic methods of training ART-based neural networks: slow and fast. In the slow learning method, the degree of training of the recognition neuron’s weights towards the input vector is calculated to continuous values with differential equations and is thus dependent on the length of time the input vector is presented. With fast learning, algebraic equations are used to calculate degree of weight adjustments to be made, and binary values are used. While fast learning is effective and efficient for a variety of tasks, the slow learning method is more biologically plausible and can be used with continuous-time networks (i.e. when the input vector can vary continuously).\n\nART 1 is the simplest variety of ART networks, accepting only binary inputs.\nART 2 extends network capabilities to support continuous inputs.\nART 2-A is a streamlined form of ART-2 with a drastically accelerated runtime, and with qualitative results being only rarely inferior to the full ART-2 implementation.\nART 3 builds on ART-2 by simulating rudimentary neurotransmitter regulation of synaptic activity by incorporating simulated sodium (Na+) and calcium (Ca2+) ion concentrations into the system's equations, which results in a more physiologically realistic means of partially inhibiting categories that trigger mismatch resets.\nARTMAP also known as Predictive ART, combines two slightly modified ART-1 or ART-2 units into a supervised learning structure where the first unit takes the input data and the second unit takes the correct output data, then used to make the minimum possible adjustment of the vigilance parameter in the first unit in order to make the correct classification.\n\nFuzzy ART implements fuzzy logic into ART’s pattern recognition, thus enhancing generalizability. An optional (and very useful) feature of fuzzy ART is complement coding, a means of incorporating the absence of features into pattern classifications, which goes a long way towards preventing inefficient and unnecessary category proliferation. The applied similarity measures are based on the L1 norm. Fuzzy ART is known to be very sensitive to noise.\n\nFuzzy ARTMAP is merely ARTMAP using fuzzy ART units, resulting in a corresponding increase in efficacy.\n\nSimplified Fuzzy ARTMAP (SFAM) constitutes a strongly simplified variant of fuzzy ARTMAP dedicated to classification tasks.\n\nGaussian ART and Gaussian ARTMAP use Gaussian activation functions and computations based on probability theory. Therefore, they have some similarity with Gaussian mixture models. In comparison to fuzzy ART and fuzzy ARTMAP, they are less sensitive to noise. But the stability of learnt representations is reduced which may lead to category proliferation in open-ended learning tasks.\n\nFusion ART and related networks extend ART and ARTMAP to multiple pattern channels. They support several learning paradigms.\n\nTopoART combines fuzzy ART with topology learning networks such as the growing neural gas. Furthermore, it adds a noise reduction mechanism. There are several derived neural networks which extend TopoART to further learning paradigms.\n\nHypersphere ART and Hypersphere ARTMAP are closely related to fuzzy ART and fuzzy ARTMAP, respectively. But as they use a different type of category representation (namely hyperspheres), they do not require their input to be normalised to the interval [0, 1]. They apply similarity measures based on the L2 norm.\n\nLAPARTThe Laterally Primed Adaptive Resonance Theory (LAPART) neural networks couple two Fuzzy ART algorithms to create a mechanism for making predictions based on learned associations. The coupling of the two Fuzzy ARTs has a unique stability that allows the system to converge rapidly towards a clear solution. Additionally, it can perform logical inference and supervised learning similar to fuzzy ARTMAP.\n\nIt has been noted that results of Fuzzy ART and ART 1 depend critically upon the order in which the training data are processed. The effect can be reduced to some extent by using a slower learning rate, but is present regardless of the size of the input data set. Hence Fuzzy ART and ART 1 estimates do not possess the statistical property of consistency.\n\nWasserman, Philip D. (1989), Neural computing: theory and practice, New York: Van Nostrand Reinhold, \n\n"}
{"id": "336271", "url": "https://en.wikipedia.org/wiki?curid=336271", "title": "Approximation", "text": "Approximation\n\nAn approximation is anything that is similar but not exactly equal to something else.\n\nThe word \"approximation\" is derived from Latin \"approximatus\", from \"proximus\" meaning \"very near\" and the prefix \"ap-\" (\"ad-\" before \"p\") meaning \"to\". Words like \"approximate\", \"approximately\" and \"approximation\" are used especially in technical or scientific contexts. In everyday English, words such as \"roughly\" or \"around\" are used with a similar meaning. It is often found abbreviated as \"approx.\"\n\nThe term can be applied to various properties (e.g., value, quantity, image, description) that are nearly, but not exactly correct; similar, but not exactly the same (e.g., the approximate time was 10 o'clock).\nAlthough approximation is most often applied to numbers, it is also frequently applied to such things as mathematical functions, shapes, and physical laws.\n\nIn science, approximation can refer to using a simpler process or model when the correct model is difficult to use. An approximate model is used to make calculations easier. Approximations might also be used if incomplete information prevents use of exact representations.\n\nThe type of approximation used depends on the available information, the degree of accuracy required, the sensitivity of the problem to this data, and the savings (usually in time and effort) that can be achieved by approximation.\n\nApproximation theory is a branch of mathematics, a quantitative part of functional analysis. Diophantine approximation deals with approximations of real numbers by rational numbers. Approximation usually occurs when an exact form or an exact numerical number is unknown or difficult to obtain. However some known form may exist and may be able to represent the real form so that no significant deviation can be found. It also is used when a number is not rational, such as the number π, which often is shortened to 3.14159, or to 1.414.\n\nNumerical approximations sometimes result from using a small number of significant digits. Calculations are likely to involve rounding errors leading to approximation. Log tables, slide rules and calculators produce approximate answers to all but the simplest calculations. The results of computer calculations are normally an approximation expressed in a limited number of significant digits, although they can be programmed to produce more precise results. Approximation can occur when a decimal number cannot be expressed in a finite number of binary digits.\n\nRelated to approximation of functions is the asymptotic value of a function, i.e. the value as one or more of a function's parameters becomes arbitrarily large. For example, the sum (\"k\"/2)+(\"k\"/4)+(\"k\"/8)+...(\"k\"/2^\"n\") is asymptotically equal to \"k\". Unfortunately no consistent notation is used throughout mathematics and some texts will use ≈ to mean approximately equal and ~ to mean asymptotically equal whereas other texts use the symbols the other way around.\n\nAs another example, in order to accelerate the convergence rate of evolutionary algorithms, fitness approximation—that leads to build model of the fitness function to choose smart search steps—is a good solution. \n\nApproximation arises naturally in scientific experiments. The predictions of a scientific theory can differ from actual measurements. This can be because there are factors in the real situation that are not included in the theory. For example, simple calculations may not include the effect of air resistance. Under these circumstances, the theory is an approximation to reality. Differences may also arise because of limitations in the measuring technique. In this case, the measurement is an approximation to the actual value.\n\nThe history of science shows that earlier theories and laws can be \"approximations\" to some deeper set of laws. Under the correspondence principle, a new scientific theory should reproduce the results of older, well-established, theories in those domains where the old theories work. The old theory becomes an approximation to the new theory.\n\nSome problems in physics are too complex to solve by direct analysis, or progress could be limited by available analytical tools. Thus, even when the exact representation is known, an approximation may yield a sufficiently accurate solution while reducing the complexity of the problem significantly. Physicists often approximate the shape of the Earth as a sphere even though more accurate representations are possible, because many physical characteristics (e.g., gravity) are much easier to calculate for a sphere than for other shapes.\n\nApproximation is also used to analyze the motion of several planets orbiting a star. This is extremely difficult due to the complex interactions of the planets' gravitational effects on each other. An approximate solution is effected by performing iterations. In the first iteration, the planets' gravitational interactions are ignored, and the star is assumed to be fixed. If a more precise solution is desired, another iteration is then performed, using the positions and motions of the planets as identified in the first iteration, but adding a first-order gravity interaction from each planet on the others. This process may be repeated until a satisfactorily precise solution is obtained.\n\nThe use of perturbations to correct for the errors can yield more accurate solutions. Simulations of the motions of the planets and the star also yields more accurate solutions.\n\nThe most common versions of philosophy of science accept that empirical measurements are always \"approximations\"—they do not perfectly represent what is being measured.\n\nThe error-tolerance property of several applications (e.g., graphics applications) allows use of approximation (e.g., lowering the precision of numerical computations) to improve performance and energy efficiency. This approach of using deliberate, controlled approximation for achieving various optimizations is referred to as approximate computing.\n\nSymbols used to denote items that are approximately equal are wavy or dotted equals signs.\n"}
{"id": "42301", "url": "https://en.wikipedia.org/wiki?curid=42301", "title": "Arity", "text": "Arity\n\nIn logic, mathematics, and computer science, the arity of a function or operation is the number of arguments or operands that the function takes. The arity of a relation (or predicate) is the dimension of the domain in the corresponding Cartesian product. (A function of arity \"n\" thus has arity \"n\"+1 considered as a relation.) The term springs from words like unary, binary, ternary, etc. Unary functions or predicates may be also called \"monadic\"; similarly, binary functions may be called \"dyadic\".\n\nIn mathematics, arity may also be named \"rank\", but this word can have many other meanings in mathematics. In logic and philosophy, arity is also called adicity and degree. In linguistics, arity is usually named valency.\n\nIn computer programming, there is often a syntactical distinction between operators and functions; syntactical operators usually have arity 0, 1, or 2 (the ternary operator is also common). Functions vary widely in the number of arguments, though large numbers can become unwieldy. Some programming languages also offer support for variadic functions, i.e., functions syntactically accepting a variable number of arguments.\n\nThe term \"arity\" is rarely employed in everyday usage. For example, rather than saying \"the arity of the addition operation is 2\" or \"addition is an operation of arity 2\" one usually says \"addition is a binary operation\".\nIn general, the naming of functions or operators with a given arity follows a convention similar to the one used for \"n\"-based numeral systems such as binary and hexadecimal. One combines a Latin prefix with the -ary ending; for example:\n\n\nSometimes it is useful to consider a constant to be an operation of arity 0, and hence call it \"nullary\".\n\nAlso, in non-functional programming, a function without arguments can be meaningful and not necessarily constant (due to side effects). Often, such functions have in fact some \"hidden input\" which might be global variables, including the whole state of the system (time, free memory, ...). The latter are important examples which usually also exist in \"purely\" functional programming languages.\n\nExamples of unary operators in mathematics and in programming include the unary minus and plus, the increment and decrement operators in C-style languages (not in logical languages), and the successor, factorial, reciprocal, floor, ceiling, fractional part, sign, absolute value, square root (the principal square root), complex conjugate (unary of \"one\" complex number, that however has two parts at a lower level of abstraction), and norm functions in mathematics. The two's complement, address reference and the logical NOT operators are examples of unary operators in math and programming.\n\nAll functions in lambda calculus and in some functional programming languages (especially those descended from ML) are technically unary, but see n-ary below.\n\nAccording to Quine, the Latin distributives being \"singuli, bini, terni,\" and so forth, the term \"singulary\" is the correct adjective, rather than \"unary.\"\nAbraham Robinson follows Quine's usage.\n\nMost operators encountered in programming and mathematics are of the binary form. For both programming and mathematics these can be the multiplication operator, the radix operator, the often omitted exponentiation operator, the logarithm operator, the addition operator, the division operator. Logical predicates such as \"OR\", \"XOR\", \"AND\", \"IMP\" are typically used as binary operators with two distinct operands. In CISC architectures, it's common to have two source operands (and store result in one of them).\n\nCommon ternary operations besides generic function in mathemathics are the summatory and the productory though some other n-ary operation may be implied.\n\nThe computer programming language C and its various descendants (including C++, C#, Java, Julia, Perl, and others) provides the ternary operator codice_1, also known as the conditional operator, taking three operands. The first operand (the condition) is evaluated, and if it is true, the result of the entire expression is the value of the second operand, otherwise it is the value of the third operand. The Forth language also contains a ternary operator, codice_2, which multiplies the first two (one-cell) numbers, dividing by the third, with the intermediate result being a double cell number. This is used when the intermediate result would overflow a single cell. The Python language has a ternary conditional expression, codice_3. The Unix dc calculator has several ternary operators, such as codice_4, which will pop three values from the stack and efficiently compute formula_6 with arbitrary precision. Additionally, many (RISC) assembly language instructions are ternary (as opposed to only two operands specified in CISC); or higher, such as , which will load (MOV) into register the contents of a calculated memory location that is the sum (parenthesis) of the registers and .\nFrom a mathematical point of view, a function of \"n\" arguments can always be considered as a function of one single argument which is an element of some product space. However, it may be convenient for notation to consider \"n\"-ary functions, as for example multilinear maps (which are not linear maps on the product space, if \"n\"≠1).\n\nThe same is true for programming languages, where functions taking several arguments could always be defined as functions taking a single argument of some composite type such as a tuple, or in languages with higher-order functions, by currying.\n\nIn computer science, a function accepting a variable number of arguments is called \"variadic\". In logic and philosophy, predicates or relations accepting a variable number of arguments are called \"multigrade\", anadic, or variably polyadic.\n\nThere are Latinate names for specific arities, primarily based on Latin distributive numbers meaning \"in group of \"n\"\", though some are based on cardinal numbers or ordinal numbers. Only \"binary\" and \"ternary\" are both commonly used and derived from distributive numbers.\n\n\nSo we can use any decimal unit prefix to expand the concept to yottanary (10-ary) or googolplexanary (10-ary), but no usage has been found for this yet.\n\nAn alternative nomenclature is derived in a similar fashion from the corresponding Greek roots; for example, \"niladic\" (or \"medadic\"), \"monadic\", \"dyadic\", \"triadic\", \"polyadic\", and so on. Thence derive the alternative terms \"adicity\" and \"adinity\" for the Latin-derived \"arity\".\n\nThese words are often used to describe anything related to that number (e.g., undenary chess is a chess variant with an 11×11 board, or the Millenary Petition of 1603).\n\nA monograph available free online:\n\n"}
{"id": "650189", "url": "https://en.wikipedia.org/wiki?curid=650189", "title": "Binary clock", "text": "Binary clock\n\nA binary clock is a clock that displays the time of day in a binary format. Originally, such clocks showed \"each decimal digit\" of sexagesimal time as a binary value, but presently binary clocks also exist which display hours, minutes, and seconds as binary numbers. Most binary clocks are digital, although analog varieties exist. True binary clocks also exist, which indicate the time by successively halving the day, instead of using hours, minutes, or seconds. Similar clocks, based on Gray coded binary, also exist. \n\nMost common binary clocks use six columns of LEDs to represent zeros and ones. Each column represents a single decimal digit, a format known as binary-coded decimal (BCD). The bottom row in each column represents 1 (or 2), with each row above representing higher powers of two, up to 2 (or 8). \n\nTo read each individual digit in the time, the user adds the values that each illuminated LED represents, then reads these from left to right. The first two columns represent the hour, the next two represent the minute and the last two represent the second. Since zero digits are not illuminated, the positions of each digit must be memorized if the clock is to be usable in the dark.\n\nBinary clocks that display time in binary-coded sexagesimal also exist. Instead of representing each digit of traditional sexagesimal time with one binary number, each component of traditional sexagesimal time is represented with one binary number, that is, using up to 6 bits instead of only 4.\n\nFor 24-hour binary-coded sexagesimal clocks, there are 11 or 17 LED lights to show us the time. There are 5 LEDs to show the hours, there are 6 LEDs to show the minutes, and there are 6 LEDs to show the seconds. 6 LEDs to show the seconds are not needed in 24-hour binary-coded sexagesimal clocks with 11 LED lights.\n\nA format exists also where hours, minutes and seconds are shown on three lines instead of columns as binary numbers.\n\n\n"}
{"id": "6125", "url": "https://en.wikipedia.org/wiki?curid=6125", "title": "Carl Friedrich Gauss", "text": "Carl Friedrich Gauss\n\nJohann Carl Friedrich Gauss (; ; ; (30 April 177723 February 1855) was a German mathematician and physicist who made significant contributions to many fields in mathematics and sciences. Sometimes referred to as the \"Princeps mathematicorum\" () and \"the greatest mathematician since antiquity\", Gauss had an exceptional influence in many fields of mathematics and science, and is ranked among history's most influential mathematicians.\n\nJohann Carl Friedrich Gauss was born on 30 April 1777 in Brunswick (Braunschweig), in the Duchy of Brunswick-Wolfenbüttel (now part of Lower Saxony, Germany), to poor, working-class parents. His mother was illiterate and never recorded the date of his birth, remembering only that he had been born on a Wednesday, eight days before the Feast of the Ascension (which occurs 39 days after Easter). Gauss later solved this puzzle about his birthdate in the context of finding the date of Easter, deriving methods to compute the date in both past and future years. He was christened and confirmed in a church near the school he attended as a child.\n\nGauss was a child prodigy. In his memorial on Gauss, Wolfgang Sartorius von Waltershausen says that when Gauss was barely three years old he corrected a math error his father made; and that when he was seven, he confidently solved an arithmetic series problem faster than anyone else in his class of 100 students. Many versions of this story have been retold since that time with various details regarding what the series was - the most frequent being the classical problem of adding all the integers from 1 to 100. There are many other anecdotes about his precocity while a toddler, and he made his first groundbreaking mathematical discoveries while still a teenager. He completed his magnum opus, \"Disquisitiones Arithmeticae\", in 1798, at the age of 21—though it was not published until 1801. This work was fundamental in consolidating number theory as a discipline and has shaped the field to the present day.\n\nGauss's intellectual abilities attracted the attention of the Duke of Brunswick, who sent him to the Collegium Carolinum (now Braunschweig University of Technology), which he attended from 1792 to 1795, and to the University of Göttingen from 1795 to 1798.\nWhile at university, Gauss independently rediscovered several important theorems. His breakthrough occurred in 1796 when he showed that a regular polygon can be constructed by compass and straightedge if the number of its sides is the product of distinct Fermat primes and a power of 2. This was a major discovery in an important field of mathematics; construction problems had occupied mathematicians since the days of the Ancient Greeks, and the discovery ultimately led Gauss to choose mathematics instead of philology as a career.\nGauss was so pleased with this result that he requested that a regular heptadecagon be inscribed on his tombstone. The stonemason declined, stating that the difficult construction would essentially look like a circle.\n\nThe year 1796 was more productive for both Gauss and number theory. He discovered a construction of the heptadecagon on 30 March. He further advanced modular arithmetic, greatly simplifying manipulations in number theory. On 8 April he became the first to prove the quadratic reciprocity law. This remarkably general law allows mathematicians to determine the solvability of any quadratic equation in modular arithmetic. The prime number theorem, conjectured on 31 May, gives a good understanding of how the prime numbers are distributed among the integers.\n\nGauss also discovered that every positive integer is representable as a sum of at most three triangular numbers on 10 July and then jotted down in his diary the note: \"ΕΥΡΗΚΑ! . On 1 October he published a result on the number of solutions of polynomials with coefficients in finite fields, which 150 years later led to the Weil conjectures.\n\nGauss remained mentally active into his old age, even while suffering from gout and general unhappiness. For example, at the age of 62, he taught himself Russian.\n\nIn 1840, Gauss published his influential \"Dioptrische Untersuchungen\", in which he gave the first systematic analysis on the formation of images under a paraxial approximation (Gaussian optics). Among his results, Gauss showed that under a paraxial approximation an optical system can be characterized by its cardinal points and he derived the Gaussian lens formula.\n\nIn 1845, he became an associated member of the Royal Institute of the Netherlands; when that became the Royal Netherlands Academy of Arts and Sciences in 1851, he joined as a foreign member.\n\nIn 1854, Gauss selected the topic for Bernhard Riemann's inaugural lecture \"Über die Hypothesen, welche der Geometrie zu Grunde liegen\" (\"About the hypotheses that underlie Geometry\"). On the way home from Riemann's lecture, Weber reported that Gauss was full of praise and excitement.\n\nOn 23 February 1855, Gauss died of a heart attack in Göttingen (then Kingdom of Hanover and now Lower Saxony); he is interred in the Albani Cemetery there. Two people gave eulogies at his funeral: Gauss's son-in-law Heinrich Ewald, and Wolfgang Sartorius von Waltershausen, who was Gauss's close friend and biographer. Gauss's brain was preserved and was studied by Rudolf Wagner, who found its mass to be slightly above average, at 1,492 grams, and the cerebral area equal to 219,588 square millimeters (340.362 square inches). Highly developed convolutions were also found, which in the early 20th century were suggested as the explanation of his genius.\n\nGauss was a Lutheran Protestant, a member of the St. Albans Evangelical Lutheran church in Göttingen. Potential evidence that Gauss believed in God comes from his response after solving a problem that had previously defeated him: \"Finally, two days ago, I succeeded—not on account of my hard efforts, but by the grace of the Lord.\" One of his biographers, G. Waldo Dunnington, described Gauss's religious views as follows:\nFor him science was the means of exposing the immortal nucleus of the human soul. In the days of his full strength, it furnished him recreation and, by the prospects which it opened up to him, gave consolation. Toward the end of his life, it brought him confidence. Gauss's God was not a cold and distant figment of metaphysics, nor a distorted caricature of embittered theology. To man is not vouchsafed that fullness of knowledge which would warrant his arrogantly holding that his blurred vision is the full light and that there can be none other which might report the truth as does his. For Gauss, not he who mumbles his creed, but he who lives it, is accepted. He believed that a life worthily spent here on earth is the best, the only, preparation for heaven. Religion is not a question of literature, but of life. God's revelation is continuous, not contained in tablets of stone or sacred parchment. A book is inspired when it inspires. The unshakeable idea of personal continuance after death, the firm belief in a last regulator of things, in an eternal, just, omniscient, omnipotent God, formed the basis of his religious life, which harmonized completely with his scientific research.\n\nApart from his correspondence, there are not many known details about Gauss's personal creed. Many biographers of Gauss disagree about his religious stance, with Bühler and others considering him a deist with very unorthodox views, while Dunnington (though admitting that Gauss did not believe literally in all Christian dogmas and that it is unknown what he believed on most doctrinal and confessional questions) points out that he was, at least, a nominal Lutheran.\n\nIn connection to this, there is a record of a conversation between Rudolf Wagner and Gauss, in which they discussed William Whewell's book \"Of the Plurality of Worlds\". In this work, Whewell had discarded the possibility of existing life in other planets, on the basis of theological arguments, but this was a position with which both Wagner and Gauss disagreed. Later Wagner explained that he did not fully believe in the Bible, though he confessed that he \"envied\" those who were able to easily believe. This later led them to discuss the topic of faith, and in some other religious remarks, Gauss said that he had been more influenced by theologians like Lutheran minister Paul Gerhardt than by Moses. Other religious influences included Wilhelm Braubach, Johann Peter Süssmilch, and the New Testament.\n\nDunnington further elaborates on Gauss's religious views by writing: Gauss's religious consciousness was based on an insatiable thirst for truth and a deep feeling of justice extending to intellectual as well as material goods. He conceived spiritual life in the whole universe as a great system of law penetrated by eternal truth, and from this source he gained the firm confidence that death does not end all.\n\nGauss declared he firmly believed in the afterlife, and saw spirituality as something essentially important for human beings. He was quoted stating: \"The world would be nonsense, the whole creation an absurdity without immortality,\" and for this statement he was severely criticized by the atheist Eugen Dühring who judged him as a narrow superstitious man.\n\nThough he was not a church-goer, Gauss strongly upheld religious tolerance, believing \"that one is not justified in disturbing another's religious belief, in which they find consolation for earthly sorrows in time of trouble.\" When his son Eugene announced that he wanted to become a Christian missionary, Gauss approved of this, saying that regardless of the problems within religious organizations, missionary work was \"a highly honorable\" task.\n\nOn 9 October 1805, Gauss married Johanna Osthoff (1780–1809), and had a son and a daughter with her. Johanna died on 11 October 1809, and her most recent child, Louis, died the following year. Gauss plunged into a depression from which he never fully recovered. He then married Minna Waldeck (1788–1831) on 4 August 1810, and had three more children. Gauss was never quite the same without his first wife, so he, just like his father, grew to dominate his children. Minna Waldeck died on 12 September 1831.\n\nGauss had six children. With Johanna (1780–1809), his children were Joseph (1806–1873), Wilhelmina (1808–1846) and Louis (1809–1810). With Minna Waldeck he also had three children: Eugene (1811–1896), Wilhelm (1813–1879) and Therese (1816–1864). Eugene shared a good measure of Gauss's talent in languages and computation. After his second wife's death in 1831 Therese took over the household and cared for Gauss for the rest of his life. His mother lived in his house from 1817 until her death in 1839.\n\nGauss eventually had conflicts with his sons. He did not want any of his sons to enter mathematics or science for \"fear of lowering the family name\", as he believed none of them would surpass his own achievements. Gauss wanted Eugene to become a lawyer, but Eugene wanted to study languages. They had an argument over a party Eugene held, which Gauss refused to pay for. The son left in anger and, in about 1832, emigrated to the United States, where he was quite successful. While working for the American Fur Company in the Midwest, he learned the Sioux language. Later, he moved to Missouri and became a successful businessman. Wilhelm also moved to America in 1837 and settled in Missouri, starting as a farmer and later becoming wealthy in the shoe business in St. Louis. It took many years for Eugene's success to counteract his reputation among Gauss's friends and colleagues. See also on 3 September 1912.\n\nCarl Gauss was an ardent perfectionist and a hard worker. He was never a prolific writer, refusing to publish work which he did not consider complete and above criticism. This was in keeping with his personal motto \"pauca sed matura\" (\"few, but ripe\"). His personal diaries indicate that he had made several important mathematical discoveries years or decades before his contemporaries published them. Scottish-American mathematician and writer Eric Temple Bell said that if Gauss had published all of his discoveries in a timely manner, he would have advanced mathematics by fifty years.\n\nThough he did take in a few students, Gauss was known to dislike teaching. It is said that he attended only a single scientific conference, which was in Berlin in 1828. However, several of his students became influential mathematicians, among them Richard Dedekind and Bernhard Riemann.\n\nOn Gauss's recommendation, Friedrich Bessel was awarded an honorary doctor degree from Göttingen in March 1811. Around that time, the two men engaged in an epistolary correspondence. However, when they met in person in 1825, they quarrelled; the details are unknown.\n\nBefore she died, Sophie Germain was recommended by Gauss to receive her honorary degree; she never received it.\n\nGauss usually declined to present the intuition behind his often very elegant proofs—he preferred them to appear \"out of thin air\" and erased all traces of how he discovered them. This is justified, if unsatisfactorily, by Gauss in his \"Disquisitiones Arithmeticae\", where he states that all analysis (i.e., the paths one traveled to reach the solution of a problem) must be suppressed for sake of brevity.\n\nGauss supported the monarchy and opposed Napoleon, whom he saw as an outgrowth of revolution.\n\nGauss summarized his views on the pursuit of knowledge in a letter to Farkas Bolyai dated 2 September 1808 as follows:It is not knowledge, but the act of learning, not possession but the act of getting there, which grants the greatest enjoyment. When I have clarified and exhausted a subject, then I turn away from it, in order to go into darkness again. The never-satisfied man is so strange; if he has completed a structure, then it is not in order to dwell in it peacefully, but in order to begin another. I imagine the world conqueror must feel thus, who, after one kingdom is scarcely conquered, stretches out his arms for others.\n\nIn his 1799 doctorate in absentia, \"A new proof of the theorem that every integral rational algebraic function of one variable can be resolved into real factors of the first or second degree\", Gauss proved the fundamental theorem of algebra which states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. Mathematicians including Jean le Rond d'Alembert had produced false proofs before him, and Gauss's dissertation contains a critique of d'Alembert's work. Ironically, by today's standard, Gauss's own attempt is not acceptable, owing to the implicit use of the Jordan curve theorem. However, he subsequently produced three other proofs, the last one in 1849 being generally rigorous. His attempts clarified the concept of complex numbers considerably along the way.\n\nGauss also made important contributions to number theory with his 1801 book \"Disquisitiones Arithmeticae\" (Latin, Arithmetical Investigations), which, among other things, introduced the symbol for congruence and used it in a clean presentation of modular arithmetic, contained the first two proofs of the law of quadratic reciprocity, developed the theories of binary and ternary quadratic forms, stated the class number problem for them, and showed that a regular heptadecagon (17-sided polygon) can be constructed with straightedge and compass. It appears that Gauss already knew the class number formula in 1801.\n\nIn addition, he proved the following conjectured theorems:\n\nHe also\n\nIn the same year, Italian astronomer Giuseppe Piazzi discovered the dwarf planet Ceres. Piazzi could only track Ceres for somewhat more than a month, following it for three degrees across the night sky. Then it disappeared temporarily behind the glare of the Sun. Several months later, when Ceres should have reappeared, Piazzi could not locate it: the mathematical tools of the time were not able to extrapolate a position from such a scant amount of data—three degrees represent less than 1% of the total orbit. Gauss heard about the problem and tackled it. After three months of intense work, he predicted a position for Ceres in December 1801—just about a year after its first sighting—and this turned out to be accurate within a half-degree when it was rediscovered by Franz Xaver von Zach on 31 December at Gotha, and one day later by Heinrich Olbers in Bremen.\n\nGauss's method involved determining a conic section in space, given one focus (the Sun) and the conic's intersection with three given lines (lines of sight from the Earth, which is itself moving on an ellipse, to the planet) and given the time it takes the planet to traverse the arcs determined by these lines (from which the lengths of the arcs can be calculated by Kepler's Second Law). This problem leads to an equation of the eighth degree, of which one solution, the Earth's orbit, is known. The solution sought is then separated from the remaining six based on physical conditions. In this work, Gauss used comprehensive approximation methods which he created for that purpose.\n\nOne such method was the fast Fourier transform. While this method is traditionally attributed to a 1965 paper by J. W. Cooley and J. W. Tukey, Gauss developed it as a trigonometric interpolation method. His paper, \"Theoria Interpolationis Methodo Nova Tractata\", was only published posthumously in Volume 3 of his collected works. This paper predates the first presentation by Joseph Fourier on the subject in 1807.\n\nZach noted that \"without the intelligent work and calculations of Doctor Gauss we might not have found Ceres again\". Though Gauss had up to that point been financially supported by his stipend from the Duke, he doubted the security of this arrangement, and also did not believe pure mathematics to be important enough to deserve support. Thus he sought a position in astronomy, and in 1807 was appointed Professor of Astronomy and Director of the astronomical observatory in Göttingen, a post he held for the remainder of his life.\nThe discovery of Ceres led Gauss to his work on a theory of the motion of planetoids disturbed by large planets, eventually published in 1809 as \"Theoria motus corporum coelestium in sectionibus conicis solem ambientum\" (Theory of motion of the celestial bodies moving in conic sections around the Sun). In the process, he so streamlined the cumbersome mathematics of 18th-century orbital prediction that his work remains a cornerstone of astronomical computation. It introduced the Gaussian gravitational constant, and contained an influential treatment of the method of least squares, a procedure used in all sciences to this day to minimize the impact of measurement error.\n\nGauss proved the method under the assumption of normally distributed errors (see Gauss–Markov theorem; see also Gaussian). The method had been described earlier by Adrien-Marie Legendre in 1805, but Gauss claimed that he had been using it since 1794 or 1795. In the history of statistics, this disagreement is called the \"priority dispute over the discovery of the method of least squares.\"\n\nIn 1818 Gauss, putting his calculation skills to practical use, carried out a geodetic survey of the Kingdom of Hanover, linking up with previous Danish surveys. To aid the survey, Gauss invented the heliotrope, an instrument that uses a mirror to reflect sunlight over great distances, to measure positions.\n\nGauss also claimed to have discovered the possibility of non-Euclidean geometries but never published it. This discovery was a major paradigm shift in mathematics, as it freed mathematicians from the mistaken belief that Euclid's axioms were the only way to make geometry consistent and non-contradictory.\n\nResearch on these geometries led to, among other things, Einstein's theory of general relativity, which describes the universe as non-Euclidean. His friend Farkas Wolfgang Bolyai with whom Gauss had sworn \"brotherhood and the banner of truth\" as a student, had tried in vain for many years to prove the parallel postulate from Euclid's other axioms of geometry.\n\nBolyai's son, János Bolyai, discovered non-Euclidean geometry in 1829; his work was published in 1832. After seeing it, Gauss wrote to Farkas Bolyai: \"To praise it would amount to praising myself. For the entire content of the work ... coincides almost exactly with my own meditations which have occupied my mind for the past thirty or thirty-five years.\"\n\nThis unproved statement put a strain on his relationship with Bolyai who thought that Gauss was \"stealing\" his idea.\n\nLetters from Gauss years before 1829 reveal him obscurely discussing the problem of parallel lines. Waldo Dunnington, a biographer of Gauss, argues in \"Gauss, Titan of Science\" that Gauss was in fact in full possession of non-Euclidean geometry long before it was published by Bolyai, but that he refused to publish any of it because of his fear of controversy.\n\nThe geodetic survey of Hanover, which required Gauss to spend summers traveling on horseback for a decade, fueled Gauss's interest in differential geometry and topology, fields of mathematics dealing with curves and surfaces. Among other things, he came up with the notion of Gaussian curvature.\nThis led in 1828 to an important theorem, the Theorema Egregium (\"remarkable theorem\"), establishing an important property of the notion of curvature. Informally, the theorem says that the curvature of a surface can be determined entirely by measuring angles and distances on the surface.\n\nThat is, curvature does not depend on how the surface might be embedded in 3-dimensional space or 2-dimensional space.\n\nIn 1821, he was made a foreign member of the Royal Swedish Academy of Sciences. Gauss was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1822.\n\nIn 1831, Gauss developed a fruitful collaboration with the physics professor Wilhelm Weber, leading to new knowledge in magnetism (including finding a representation for the unit of magnetism in terms of mass, charge, and time) and the discovery of Kirchhoff's circuit laws in electricity. It was during this time that he formulated his namesake law. They constructed the first electromechanical telegraph in 1833, which connected the observatory with the institute for physics in Göttingen. Gauss ordered a magnetic observatory to be built in the garden of the observatory, and with Weber founded the \"Magnetischer Verein\" (\"magnetic association\"), which supported measurements of Earth's magnetic field in many regions of the world. He developed a method of measuring the horizontal intensity of the magnetic field which was in use well into the second half of the 20th century, and worked out the mathematical theory for separating the inner and outer (magnetospheric) sources of Earth's magnetic field.\n\nThe British mathematician Henry John Stephen Smith (1826–1883) gave the following appraisal of Gauss:\nThere are several stories of his early genius. According to one, his gifts became very apparent at the age of three when he corrected, mentally and without fault in his calculations, an error his father had made on paper while calculating finances.\n\nAnother story has it that in primary school after the young Gauss misbehaved, his teacher, J.G. Büttner, gave him a task: add a list of integers in arithmetic progression; as the story is most often told, these were the numbers from 1 to 100. The young Gauss reputedly produced the correct answer within seconds, to the astonishment of his teacher and his assistant Martin Bartels.\n\nGauss's presumed method was to realize that pairwise addition of terms from opposite ends of the list yielded identical intermediate sums: 1 + 100 = 101, 2 + 99 = 101, 3 + 98 = 101, and so on, for a total sum of 50 × 101 = 5050.\nHowever, the details of the story are at best uncertain (see for discussion of the original Wolfgang Sartorius von Waltershausen source and the changes in other versions); some authors, such as Joseph Rotman in his book \"A first course in Abstract Algebra\", question whether it ever happened.\n\nAccording to Isaac Asimov, Gauss was once interrupted in the middle of a problem and told that his wife was dying. He is purported to have said, \"Tell her to wait a moment till I'm done.\" This anecdote is briefly discussed in G. Waldo Dunnington's \"Gauss, Titan of Science\" where it is suggested that it is an apocryphal story.\n\nHe referred to mathematics as \"the queen of sciences\" and supposedly once espoused a belief in the necessity of immediately understanding Euler's identity as a benchmark pursuant to becoming a first-class mathematician.\n\nFrom 1989 through 2001, Gauss's portrait, a normal distribution curve and some prominent Göttingen buildings were featured on the German ten-mark banknote. The reverse featured the approach for Hanover. Germany has also issued three postage stamps honoring Gauss. One (no. 725) appeared in 1955 on the hundredth anniversary of his death; two others, nos. 1246 and 1811, in 1977, the 200th anniversary of his birth.\n\nDaniel Kehlmann's 2005 novel \"Die Vermessung der Welt\", translated into English as \"Measuring the World\" (2006), explores Gauss's life and work through a lens of historical fiction, contrasting them with those of the German explorer Alexander von Humboldt. A film version directed by Detlev Buck was released in 2012.\n\nIn 2007 a bust of Gauss was placed in the Walhalla temple.\n\nThe numerous things named in honor of Gauss include:\n\nIn 1929 the Polish mathematician Marian Rejewski, who helped to solve the German Enigma cipher machine in December 1932, began studying actuarial statistics at Göttingen. At the request of his Poznań University professor, Zdzisław Krygowski, on arriving at Göttingen Rejewski laid flowers on Gauss's grave.\n\nOn 30 April 2018, Google honoured Gauss in his would-be 241st birthday with a Google Doodle showcased in Europe, Russia, Israel, Japan, Taiwan, parts of Southern and Central America and the United States.\n\nCarl Friedrich Gauss, who also introduced the so called Gaussian logarithms, sometimes gets confused with (1829–1915), a German geologist, who also published some well-known logarithm tables used up into the early 1980s.\n\n\n\n\n"}
{"id": "2146848", "url": "https://en.wikipedia.org/wiki?curid=2146848", "title": "Change of variables", "text": "Change of variables\n\nIn mathematics, a change of variables is a basic technique used to simplify problems in which the original variables are replaced with functions of other variables. The intent is that when expressed in new variables, the problem may become simpler, or equivalent to a better understood problem.\n\nChange of variables is an operation that is related to substitution. However these are different operations, as can be seen when considering differentiation (chain rule) or integration (integration by substitution).\n\nA very simple example of a useful variable change can be seen in the problem of finding the roots of the sixth degree polynomial:\n\nSixth degree polynomial equations are generally impossible to solve in terms of radicals (see Abel–Ruffini theorem). This particular equation, however, may be written \n(this is a simple case of a polynomial decomposition). Thus the equation may be\nsimplified by defining a new variable \"u\" =\"x\". Substituting \"x\" by formula_3 into the polynomial gives\n\nwhich is just a quadratic equation with the two solutions:\n\nThe solutions in terms of the original variable are obtained by substituting \"x\" back in for \"u\", which gives\n\nThen, assuming that one is interested only in real solutions, the solutions of the original equation are\n\nConsider the system of equations\n\nwhere formula_10 and formula_11 are positive integers with formula_12. (Source: 1991 AIME)\n\nSolving this normally is not very difficult, but it may get a little tedious. However, we can rewrite the second equation as formula_13. Making the substitution formula_14 reduces the system to formula_15 Solving this gives formula_16 or formula_17 Back-substituting the first ordered pair gives us formula_18, which easily gives the solution formula_19 Back-substituting the second ordered pair gives us formula_20, which gives no solutions. Hence the solution that solves the system is formula_21.\n\nLet formula_22, formula_23 be smooth manifolds and let formula_24 be a formula_25-diffeomorphism between them, that is: formula_26 is a formula_27 times continuously differentiable, bijective map from formula_22 to formula_23 with formula_27 times continuously differentiable inverse from formula_23 to formula_22. Here formula_27 may be any natural number (or zero), formula_34 (smooth) or formula_35 (analytic).\n\nThe map formula_26 is called a \"regular coordinate transformation\" or \"regular variable substitution\", where \"regular\" refers to the formula_25-ness of formula_26. Usually one will write formula_39 to indicate the replacement of the variable formula_10 by the variable formula_11 by substituting the value of formula_26 in formula_11 for every occurrence of formula_10.\n\nSome systems can be more easily solved when switching to polar coordinates. Consider for example the equation\n\nThis may be a potential energy function for some physical problem. If one does not immediately see a solution, one might try the substitution\n\nNote that if formula_48 runs outside a formula_49-length interval, for example, formula_50, the map formula_26 is no longer bijective. Therefore formula_26 should be limited to, for example formula_53. Notice how formula_54 is excluded, for formula_26 is not bijective in the origin (formula_48 can take any value, the point will be mapped to (0, 0)). Then, replacing all occurrences of the original variables by the new expressions prescribed by formula_26 and using the identity formula_58, we get\n\nNow the solutions can be readily found: formula_60, so formula_61 or formula_62. Applying the inverse of formula_26 shows that this is equivalent to formula_64 while formula_65. Indeed we see that for formula_64 the function vanishes, except for the origin.\n\nNote that, had we allowed formula_54, the origin would also have been a solution, though it is not a solution to the original problem. Here the bijectivity of formula_26 is crucial. Note also that the function is always positive (for formula_69), hence the absolute values.\n\nThe chain rule is used to simplify complicated differentiation. For example, consider the problem of calculating the derivative\n\nWriting\n\nwe get\n\nDifficult integrals may often be evaluated by changing variables; this is enabled by the substitution rule and is analogous to the use of the chain rule above. Difficult integrals may also be solved by simplifying the integral using a change of variables given by the corresponding Jacobian matrix and determinant. Using the Jacobian determinant and the corresponding change of variable that it gives is the basis of coordinate systems such as polar, cylindrical, and spherical coordinate systems.\n\nVariable changes for differentiation and integration are taught in elementary calculus and the steps are rarely carried out in full.\n\nThe very broad use of variable changes is apparent when considering differential equations, where the independent variables may be changed using the chain rule or the dependent variables are changed resulting in some differentiation to be carried out. Exotic changes, such as the mingling of dependent and independent variables in point and contact transformations, can be very complicated but allow much freedom.\n\nVery often, a general form for a change is substituted into a problem and parameters picked along the way to best simplify the problem.\n\nProbably the simplest change is the scaling and shifting of variables, that is replacing them with new variables that are \"stretched\" and \"moved\" by constant amounts. This is very common in practical applications to get physical parameters out of problems. For an \"n\" order derivative, the change simply results in\n\nwhere\n\nThis may be shown readily through the chain rule and linearity of differentiation. This change is very common in practical applications to get physical parameters out of problems, for example, the boundary value problem\n\ndescribes parallel fluid flow between flat solid walls separated by a distance δ; µ is the viscosity and formula_77 the pressure gradient, both constants. By scaling the variables the problem becomes\n\nwhere\n\nScaling is useful for many reasons. It simplifies analysis both by reducing the number of parameters and by simply making the problem neater. Proper scaling may \"normalize\" variables, that is make them have a sensible unitless range such as 0 to 1. Finally, if a problem mandates numeric solution, the fewer the parameters the fewer the number of computations.\n\nConsider a system of equations\n\nfor a given function formula_81.\nThe mass can be eliminated by the (trivial) substitution formula_82.\nClearly this is a bijective map from formula_83 to formula_83. Under the substitution formula_85 the system becomes\n\nGiven a force field formula_87, Newton's equations of motion are\nLagrange examined how these equations of motion change under an arbitrary substitution of variables formula_89, formula_90\n\nHe found that the equations\nare equivalent to Newton's equations for the function formula_92,\nwhere \"T\" is the kinetic, and \"V\" the potential energy.\n\nIn fact, when the substitution is chosen well (exploiting for example symmetries and constraints of the system) these equations are much easier to solve than Newton's equations in Cartesian coordinates.\n\n"}
{"id": "2467473", "url": "https://en.wikipedia.org/wiki?curid=2467473", "title": "Ciphertext expansion", "text": "Ciphertext expansion\n\nIn cryptography, the term ciphertext expansion refers to the length increase of a message when it is encrypted. Many modern cryptosystems cause some degree of expansion during the encryption process, for instance when the resulting ciphertext must include a message-unique Initialization Vector (IV). Probabilistic encryption schemes cause ciphertext expansion, as the set of possible ciphertexts is necessarily greater than the set of input plaintexts. Certain schemes, such as Cocks Identity Based Encryption, or the Goldwasser-Micali cryptosystem result in ciphertexts hundreds or thousands of times longer than the plaintext.\n\nCiphertext expansion may be offset or increased by other processes which compress or expand the message, e.g., data compression or error correction coding.\n"}
{"id": "6978672", "url": "https://en.wikipedia.org/wiki?curid=6978672", "title": "Comparability graph", "text": "Comparability graph\n\nIn graph theory, a comparability graph is an undirected graph that connects pairs of elements that are comparable to each other in a partial order. Comparability graphs have also been called transitively orientable graphs, partially orderable graphs, and containment graphs.\nAn incomparability graph is an undirected graph that connects pairs of elements that are not comparable to each other in a partial order.\n\nFor any strict partially ordered set (\"S\",<), the comparability graph of (\"S\", <) is the graph (\"S\", ⊥) of which the vertices are the elements of \"S\" and the edges are those pairs {\"u\", \"v\"} of elements such that \"u\" < \"v\". That is, for a partially ordered set, take the directed acyclic graph, apply transitive closure, and remove orientation. \n\nEquivalently, a comparability graph is a graph that has a transitive orientation, an assignment of directions to the edges of the graph (i.e. an orientation of the graph) such that the adjacency relation of the resulting directed graph is transitive: whenever there exist directed edges (\"x\",\"y\") and (\"y\",\"z\"), there must exist an edge (\"x\",\"z\").\n\nOne can represent any partial order as a family of sets, such that \"x\" < \"y\" in the partial order whenever the set corresponding to \"x\" is a subset of the set corresponding to \"y\". In this way, comparability graphs can be shown to be equivalent to containment graphs of set families; that is, a graph with a vertex for each set in the family and an edge between two sets whenever one is a subset of the other.\n\nAlternatively, a comparability graph is a graph such that, for every \"generalized cycle\" of odd length, one can find an edge (\"x\",\"y\") connecting two vertices that are at distance two in the cycle. Such an edge is called a \"triangular chord\". In this context, a generalized cycle is defined to be a closed walk that uses each edge of the graph at most once in each direction.\n\nComparability graphs can also be characterized by a list of forbidden induced subgraphs.\n\nEvery complete graph is a comparability graph, the comparability graph of a total order. All acyclic orientations of a complete graph are transitive. Every bipartite graph is also a comparability graph. Orienting the edges of a bipartite graph from one side of the bipartition to the other results in a transitive orientation, corresponding to a partial order of height two. As observes, every comparability graph that is neither complete nor bipartite has a skew partition.\n\nThe complement of any interval graph is a comparability graph. The comparability relation is called an interval order. Interval graphs are exactly the graphs that are chordal and that have comparability graph complements.\n\nA permutation graph is a containment graph on a set of intervals. Therefore, permutation graphs are another subclass of comparability graphs.\n\nThe trivially perfect graphs are the comparability graphs of rooted trees.\nCographs can be characterized as the comparability graphs of series-parallel partial orders; thus, cographs are also comparability graphs.\n\nThreshold graphs are another special kind of comparability graph.\n\nEvery comparability graph is perfect. The perfection of comparability graphs is Mirsky's theorem, and the perfection of their complements is Dilworth's theorem; these facts, together with the perfect graph theorem can be used to prove Dilworth's theorem from Mirsky's theorem or vice versa. More specifically, comparability graphs are perfectly orderable graphs, a subclass of perfect graphs: a greedy coloring algorithm for a topological ordering of a transitive orientation of the graph will optimally color them.\n\nThe complement of every comparability graph is a string graph.\n\nA transitive orientation of a graph, if it exists, can be found in linear time. However, the algorithm for doing so will assign orientations to the edges of any graph, so to complete the task of testing whether a graph is a comparability graph, one must test whether the resulting orientation is transitive, a problem provably equivalent in complexity to matrix multiplication.\n\nBecause comparability graphs are perfect, many problems that are hard on more general classes of graphs, including graph coloring and the independent set problem, can be computed in polynomial time for comparability graphs.\n\n"}
{"id": "1752414", "url": "https://en.wikipedia.org/wiki?curid=1752414", "title": "Contractible space", "text": "Contractible space\n\nIn mathematics, a topological space \"X\" is contractible if the identity map on \"X\" is null-homotopic, i.e. if it is homotopic to some constant map. Intuitively, a contractible space is one that can be continuously shrunk to a point within that space.\n\nA contractible space is precisely one with the homotopy type of a point. It follows that all the homotopy groups of a contractible space are trivial. Therefore any space with a nontrivial homotopy group cannot be contractible. Similarly, since singular homology is a homotopy invariant, the reduced homology groups of a contractible space are all trivial.\n\nFor a topological space \"X\" the following are all equivalent:\n\nThe cone on a space \"X\" is always contractible. Therefore any space can be embedded in a contractible one (which also illustrates that subspaces of contractible spaces need not be contractible).\n\nFurthermore, \"X\" is contractible if and only if there exists a retraction from the cone of \"X\" to \"X\".\n\nEvery contractible space is path connected and simply connected. Moreover, since all the higher homotopy groups vanish, every contractible space is \"n\"-connected for all \"n\" ≥ 0.\n\nA topological space is locally contractible if every point has a local base of contractible neighborhoods. Contractible spaces are not necessarily locally contractible nor vice versa. For example, the comb space is contractible but not locally contractible (if it were, it would be locally connected which it is not). Locally contractible spaces are locally \"n\"-connected for all \"n\" ≥ 0. In particular, they are locally simply connected, locally path connected, and locally connected.\n\n"}
{"id": "58629308", "url": "https://en.wikipedia.org/wiki?curid=58629308", "title": "Cooling and heating (combinatorial game theory)", "text": "Cooling and heating (combinatorial game theory)\n\nIn combinatorial game theory, cooling, heating, and overheating are operations on hot games to make them more amenable to the traditional methods of the theory,\nwhich was originally devised for cold games in which the winner is the last player to have a legal move.\nOverheating was generalised by Berlekamp for the analysis of Blockbusting.\nChilling (or unheating) and warming are variants used in the analysis of the endgame of Go.\n\nCooling and chilling may be thought of as a tax on the player who moves, who has to pay for the privilege of doing so,\nwhile heating, warming and overheating are operations that more or less reverse cooling and chilling.\n\nThe cooled game formula_1 (\"formula_2 cooled by formula_3\") for a game formula_2 and a (surreal) number formula_3 is defined by\nThe amount formula_3 by which formula_2 is cooled is known as the \"temperature\"; the minimum formula_9 for which formula_10 is infinitesimally close to formula_11 is known as the \"temperature\" formula_12 \"of\" formula_2; formula_2 is said to \"freeze\" to formula_10; formula_11 is the \"mean value\" (or simply \"mean\") of formula_2.\n\nHeating is the inverse of cooling and is defined as the \"integral\"\nNorton multiplication is an extension of multiplication to a game formula_2 and a positive game formula_20 (the \"unit\")\ndefined by\nThe incentives formula_22 of a game formula_20 are defined as formula_24.\n\nOverheating is an extension of heating used in Berlekamp's solution of Blockbusting,\nwhere formula_2 \"overheated from\" formula_26 \"to\" formula_3 is defined for arbitrary games formula_28 with formula_29 as\n\n\"Winning Ways\" also defines overheating of a game formula_2 by a positive game formula_32, as\nChilling is a variant of cooling by formula_34 used for Go endgames and is defined by\nThis is equivalent to cooling by formula_36 when formula_2 is an \"even elementary Go position in canonical form\".\n\nWarming is a special case of overheating, namely formula_38, normally written simply as formula_39 which inverts chilling when formula_2 is an \"even elementary Go position in canonical form\".\nIn this case the previous definition simplifies to the form\n"}
{"id": "8678944", "url": "https://en.wikipedia.org/wiki?curid=8678944", "title": "Digital probabilistic physics", "text": "Digital probabilistic physics\n\nDigital probabilistic physics is a branch of digital philosophy which holds that the universe exists as a nondeterministic state machine. The notion of the universe existing as a state machine was first postulated by Konrad Zuse's book \"Rechnender Raum\". Adherents hold that the universe state machine can move between more and less probable states, with the less probable states containing more information. This theory is in contrast to digital physics, which holds that the history of the universe is computable and is deterministically unfolding from initial conditions.\n\nThe fundamental tenets of digital probabilistic physics were first explored at great length by Tom Stonier in a series of books which explore the notion of information as existing as a physical phenomenon of the universe. According to Stonier, the arrangement of atoms and molecules which make up physical objects contains information, and high-information objects such as DNA are low-probability physical structures. Within this framework, civilization itself is a low-probability construct maintaining its existence by propagating through communication. Stonier's work has been unique in considering information as existing as a physical phenomenon, being broader than as an application to the domain of telecommunications. \n\nTo distinguish the probability of the physical state of the molecules from the probability of the energy distribution of thermodynamics, the term extropy was appropriated to define the probability of the atomic configuration, as opposed to the entropy. Thus, in thermodynamics, a 'coarse-grain' set of partitions is defined which groups together similar microscopically different states and in \"digital probabilistic physics\" the specific microscopic state probability is considered alone. The extropy is defined to be the self-information of the Markov chain describing the physical system.\n\nThe extropy of a system formula_1 in bits associated with the Markov chain configuration formula_2 whose outcome has probability formula_3 is:\n\nWithin this philosophy, the probability of the physical system does not necessarily change with the deterministic flow of energy through the atomic framework, but rather moves into a lesser probability state when the system goes through a bifurcating transition. Examples of this include Bernoulli cell formation, quantum fluctuations in a gravitational field causing gravitational precipitation points, and other systems moving through unstable self-amplifying state transitions.\n\n\n\n"}
{"id": "18347128", "url": "https://en.wikipedia.org/wiki?curid=18347128", "title": "Direct Anonymous Attestation", "text": "Direct Anonymous Attestation\n\nDirect Anonymous Attestation (DAA) is a cryptographic primitive which enables remote authentication of a trusted computer whilst preserving privacy of the platform's user. The protocol has been adopted by the Trusted Computing Group (TCG) in the latest version of its Trusted Platform Module (TPM) specification to address privacy concerns (see also Loss of Internet anonymity). ISO/IEC 20008 specifies DAA, as well, and Intel's Enhanced Privacy ID (EPID) 2.0 implementation for microprocessors is available for licensing RAND-Z along with an open source SDK.\n\nIn principle the privacy issue could be resolved using any standard signature scheme (or public key encryption) and a single key pair. Manufacturers would embed the private key into every TPM produced and the public key would be published as a certificate. Signatures produced by the TPM must have originated from the private key, by the nature of the technology, and since all TPMs use the same private key they are indistinguishable ensuring the user's privacy. This rather naive solution relies upon the assumption that there exists a \"global secret\". One only needs to look at the precedent of Content Scramble System (CSS), an encryption system for DVDs, to see that this assumption is fundamentally flawed. Furthermore, this approach fails to realize a secondary goal: the ability to detect rogue TPMs. A rogue TPM is a TPM that has been compromised and had its secrets extracted.\n\nThe solution first adopted by the TCG (TPM specification v1.1) required a trusted third-party, namely a \"privacy certificate authority\" (privacy CA). Each TPM has an embedded RSA key pair called an Endorsement Key (EK) which the privacy CA is assumed to know. In order to attest the TPM generates a second RSA key pair called an Attestation Identity Key (AIK). It sends the public AIK, signed by EK, to the privacy CA who checks its validity and issues a certificate for the AIK. (For this to work, either a) the privacy CA must know the TPM's public EK \"a priori\", or b) the TPM's manufacturer must have provided an \"endorsement certificate\".) The host/TPM is now able to authenticate itself with respect to the certificate. This approach permits two possibilities to detecting rogue TPMs: firstly the privacy CA should maintain a list of TPMs identified by their EK known to be rogue and reject requests from them, secondly if a privacy CA receives too many requests from a particular TPM it may reject them and blacklist the TPMs EK. The number of permitted requests should be subject to a risk management exercise. This solution is problematic since the privacy CA must take part in every transaction and thus must provide high availability whilst remaining secure. Furthermore, privacy requirements may be violated if the privacy CA and verifier collude. Although the latter issue can probably be resolved using blind signatures, the first remains.\n\nThe EPID 2.0 solution embeds the private key in the microprocessor when it is manufactured, inherently distributes the key with the physical device shipment, and has the key provision and ready for use with 1st power-on.\n\nThe DAA protocol is based on three entities and two different steps. The entities are the DAA Member (TPM platform or EPID-enabled microprocessor), the DAA Issuer and the DAA Verifier. The issuer is charged to verify the TPM platform during the Join step and to issue DAA credential to the platform. The platform (Member) uses the DAA credential with the Verifier during the Sign step. Through a zero-knowledge proof the Verifier can verify the credential without attempting to violate the platform's privacy. The protocol also supports a blacklisting capability so that Verifiers can identify attestations from TPMs that have been compromised.\n\nThe protocol allows differing degrees of privacy. Interactions are always anonymous, but the Member/Verifier may negotiate as to whether the Verifier is able to link transactions. This would allow user profiling and/or the rejection of requests originating from a host which has made too many requests. The Member and Verifier can also elect to reveal additional information to accomplish non-anonymous interactions (just as you can choose to tell a stranger your full name, or not). Thus, known identity can be built on top of an anonymous start. (Contrast this with: if you start with known identity, you can never prove you un-know that identity to regress to anonymity.)\n\nThe first Direct Anonymous Attestation scheme is due to Brickell, Camenisch, and Chen; that scheme is insecure and requires a fix.\nBrickell, Chen, and Li improve efficiency of that first scheme using symmetric pairings, rather than RSA. \nAnd Chen, Morrissey, and Smart attempt to further improve efficiency by switching from a symmetric to an asymmetric setting; \nunfortunately, the asymmetric scheme is insecure. \nChen, Page, and Smart proposed a new elliptic curve cryptography scheme using Barreto-Naehrig curves. This scheme is implemented by both EPID 2.0 and the TPM 2.0 standard, and the TPM 2.0 standard recommends that this scheme is implemented by TPMs in general and is required for TPMs that conform to the PC client profile.\nIn addition, the Intel EPID 2.0 implementation of ISO/IEC 20008 DAA and the available open source SDK can be used for members and verifiers to do attestation. Since one of the DAA attestation methods in TPM 2.0 is identical to EPID 2.0, work is underway to make ISO/IEC 20008 DAA and TPM 2.0 DAA attestation read consistently with each other at the spec level.\n\n\n"}
{"id": "9723", "url": "https://en.wikipedia.org/wiki?curid=9723", "title": "Edward Waring", "text": "Edward Waring\n\nEdward Waring (15 August 1798) was a British mathematician. He entered Magdalene College, Cambridge as a sizar and became Senior wrangler in 1757. He was elected a Fellow of Magdalene and in 1760 Lucasian Professor of Mathematics, holding the chair until his death. He made the assertion known as Waring's problem without proof in his writings \"Meditationes Algebraicae\". Waring was elected a Fellow of the Royal Society in 1763 and awarded the Copley Medal in 1784.\n\nWaring was the eldest son of John and Elizabeth Waring, a prosperous farming couple. He received his early education in Shrewsbury School under a Mr Hotchkin and was admitted as a sizar at Magdalene College, Cambridge, on 24 March 1753, being also Millington exhibitioner. His extraordinary talent for mathematics was recognised from his early years in Cambridge. In 1757 he graduated BA as senior wrangler and on 24 April 1758 was elected to a fellowship at Magdalene. He belonged to the Hyson Club, whose members included William Paley.\n\nAt the end of 1759 Waring published the first chapter of \"Miscellanea Analytica\". On 28 January the next year he was appointed Lucasian professor of mathematics, one of the highest positions in Cambridge. William Samuel Powell, then tutor in St John's College, Cambridge opposed Waring's election and instead supported the candidacy of William Ludlam. In the polemic with Powell, Waring was backed by John Wilson. In fact Waring was very young and did not hold the MA, necessary for qualifying for the Lucasian chair, but this was granted him in 1760 by royal mandate. In 1762 he published the full \"Miscellanea Analytica\", mainly devoted to the theory of numbers and algebraic equations. In 1763 he was elected to the Royal Society. He was awarded its Copley Medal in 1784 but withdrew from the society in 1795, after he had reached sixty, 'on account of [his] age'. Waring was also a member of the academies of sciences of Göttingen and Bologna. In 1767 he took an MD degree, but his activity in medicine was quite limited. He carried out dissections with Richard Watson, professor of chemistry and later bishop of Llandaff. From about 1770 he was physician at Addenbrooke's Hospital at Cambridge, and he also practised at St Ives, Huntingdonshire, where he lived for some years after 1767. His career as a physician was not very successful since he was seriously short-sighted and a very shy man.\n\nWaring had a younger brother, Humphrey, who obtained a fellowship at Magdalene in 1775. In 1776 Waring married Mary Oswell, sister of a draper in Shrewsbury; they moved to Shrewsbury and then retired to Plealey, 8 miles out of the town, where Waring owned an estate of 215 acres in 1797\n\nWaring wrote a number of papers in the \"Philosophical Transactions of the Royal Society\", dealing with the resolution of algebraic equations, number theory, series, approximation of roots, interpolation, the geometry of conic sections, and dynamics. The \"Meditationes Algebraicae\" (1770), where many of the results published in \"Miscellanea Analytica\" were reworked and expanded, was described by Joseph-Louis Lagrange as 'a work full of excellent researches'. In this work Waring published many theorems concerning the solution of algebraic equations which attracted the attention of continental mathematicians, but his best results are in number theory. Included in this work was the so-called Goldbach conjecture (every even integer is the sum of two primes), and also the following conjecture: every odd integer is a prime or the sum of three primes. Lagrange had proved that every positive integer is the sum of not more than four squares; Waring suggested that every positive integer is either a cube or the sum of not more than nine cubes. He also advanced the hypothesis that every positive integer is either a biquadrate (fourth power) or the sum of not more than nineteen biquadrates. These hypotheses form what is known as Waring's problem. He also published a theorem, due to his friend John Wilson, concerning prime numbers; it was later proven rigorously by Lagrange.\n\nIn \"Proprietates Algebraicarum Curvarum\" (1772) Waring reissued in a much revised form the first four chapters of the second part of \"Miscellanea Analytica\". He devoted himself to the classification of higher plane curves, improving results obtained by Isaac Newton, James Stirling, Leonhard Euler, and Gabriel Cramer. In 1794 he published a few copies of a philosophical work entitled \"An Essay on the Principles of Human Knowledge\", which were circulated among his friends.\n\nWaring's mathematical style is highly analytical. In fact he criticised those British mathematicians who adhered too strictly to geometry. It is indicative that he was one of the subscribers of John Landen's \"Residual Analysis\" (1764), one of the works in which the tradition of the Newtonian fluxional calculus was more severely criticised. In the preface of \"Meditationes Analyticae\" Waring showed a good knowledge of continental mathematicians such as Alexis Clairaut, Jean le Rond d'Alembert, and Euler. He lamented the fact that in Great Britain mathematics was cultivated with less interest than on the continent, and clearly desired to be considered as highly as the great names in continental mathematics—there is no doubt that he was reading their work at a level never reached by any other eighteenth-century British mathematician. Most notably, at the end of chapter three of \"Meditationes Analyticae\" Waring presents some partial fluxional equations (partial differential equations in Leibnizian terminology); such equations are a mathematical instrument of great importance in the study of continuous bodies which was almost completely neglected in Britain before Waring's researches. One of the most interesting results in \"Meditationes Analyticae\" is a test for the convergence of series generally attributed to d'Alembert (the 'ratio test'). The theory of convergence of series (the object of which is to establish when the summation of an infinite number of terms can be said to have a finite 'sum') was not much advanced in the eighteenth century.\n\nWaring's work was known both in Britain and on the continent, but it is difficult to evaluate his impact on the development of mathematics. His work on algebraic equations contained in \"Miscellanea Analytica\" was translated into Italian by Vincenzo Riccati in 1770. Waring's style is not systematic and his exposition is often obscure. It seems that he never lectured and did not habitually correspond with other mathematicians. After Jérôme Lalande in 1796 observed, in \"Notice sur la vie de Condorcet\", that in 1764 there was not a single first-rate analyst in England, Waring's reply, published after his death as 'Original letter of Dr Waring' in the \"Monthly Magazine\", stated that he had given 'somewhere between three and four hundred new propositions of one kind or another'.\n\nDuring his last years he sank into a deep religious melancholy, and a violent cold caused his death, in Plealey, on 15 August 1798. He was buried in the churchyard at Fitz, Shropshire.\n\n"}
{"id": "14842181", "url": "https://en.wikipedia.org/wiki?curid=14842181", "title": "Finite model property", "text": "Finite model property\n\nIn logic, a logic L has the finite model property (fmp for short) if any non-theorem of L is falsified by some \"finite\" model of L. Another way of putting this is to say that L has the fmp if for every formula A of L, A is an L-theorem iff A is a theorem of the theory of finite models of L.\n\nIf L is finitely axiomatizable (and has a recursive set of recursive rules) and has the fmp, then it is decidable. However, the result does not hold if L is merely recursively axiomatizable. Even if there are only finitely many finite models to choose from (up to isomorphism) there is still the problem of checking whether the underlying frames of such models validate the logic, and this may not be decidable when the logic is not finitely axiomatizable, even when it is recursively axiomatizable. (Note that a logic is recursively enumerable if and only if it is recursively axiomatizable, a result known as Craig's theorem.)\n\nA first-order formula with one universal quantification has the fmp. A first-order formula without functional symbols, where all existential quantifications appear first in the formula, also has the fmp.\n\n\n"}
{"id": "11344", "url": "https://en.wikipedia.org/wiki?curid=11344", "title": "First-order predicate", "text": "First-order predicate\n\nIn mathematical logic, a first-order predicate is a predicate that takes only individual(s) constants or variables as argument(s). Compare second-order predicate and higher-order predicate.\n\nThis is not to be confused with a one-place predicate or monad, which is a predicate that takes only one argument. For example the expression \"is a planet\" is a one-place predicate, while the expression \"is father of\" is a two-place predicate.\n\n"}
{"id": "21138193", "url": "https://en.wikipedia.org/wiki?curid=21138193", "title": "Formulas for generating Pythagorean triples", "text": "Formulas for generating Pythagorean triples\n\nBesides Euclid's formula, many other formulas for generating Pythagorean triples have been developed.\n\nEuclid's, Pythagoras' and Plato's formulas for calculating triples have been described here: \n\nThe methods below appear in various sources, often without attribution as to their origin.\n\nLeonardo of Pisa () described this method for generating primitive triples using the sequence of consecutive odd integers formula_1 and the fact that the sum of the first formula_2 terms of this sequence is formula_3. If formula_4 is the formula_2-th member of this sequence then formula_6.\n\nChoose any odd square number formula_4 from this sequence (formula_8) and let this square be the formula_2-th term of the sequence. Also, let formula_10 be the sum of the previous formula_11 terms, and let formula_12 be the sum of all formula_2 terms. Then we have established that formula_14 and we have generated the primitive triple [\"a, b, c\"]. This method produces an infinite number of primitive triples, but not all of them.\n\nEXAMPLE:\nChoose formula_15. This odd square number is the fifth term of the sequence, because formula_16. The sum of the previous 4 terms is formula_17 and the sum of all formula_18 terms is formula_19 giving us formula_14 and the primitive triple [\"a, b, c\"] = [3, 4, 5].\n\nThe German monk and mathematician Michael Stifel published the following method in 1544.\n\nConsider the progression of whole and fractional numbers:\nformula_21\n\nThe properties of this progression are:\n(a) the whole numbers are those of the common series and have unity as their common difference; (b) the numerators of the fractions, annexed to the whole numbers, are also the natural numbers; (c) the denominators of the fractions are the odd numbers, formula_22 etc.\n\nTo calculate a Pythagorean triple select any term of this progression and reduce it to an improper fraction. For example, take the term formula_23. The improper fraction is formula_24. The numbers 7 and 24 are the sides, \"a\" and \"b\", of a right triangle, and the hypotenuse is one greater than the largest side. For example:\n\nJacques Ozanam republished Stifel's sequence in 1694 and added the similar sequence formula_26 with terms derived from formula_27. As before, to produce a triple from this sequence, select any term and reduce it to an improper fraction. The numerator and denominator are the sides, \"a\" and \"b\", of a right triangle. In this case, the hypotenuse of the triple(s) produced is 2 greater than the larger side. For example:\n\nTogether, the Stifel and Ozanam sequences produce all primitive triples of the Plato and Pythagoras families respectively. The Fermat family must be found by other means.\n\nLeonard Eugene Dickson (1920) attributes to himself the following method for generating Pythagorean triples. To find integer solutions to formula_30, find positive integers \"r\", \"s\", and \"t\" such that formula_31 is a perfect square.\n\nThen:\n\nFrom this we see that formula_33 is any even integer and that \"s\" and \"t\" are factors of formula_34.  All Pythagorean triples may be found by this method.  When \"s\" and \"t\" are coprime, the triple will be primitive. A simple proof of Dickson's method has been presented by Josef Rukavicka (2013).\n\nExample: Choose \"r\" = 6. Then formula_35.\nThe three factor-pairs of 18 are: (1, 18), (2, 9), and (3, 6). All three factor pairs will produce triples using the above equations.\n\nFor Fibonacci numbers starting with and and with each succeeding Fibonacci number being the sum of the preceding two, one can generate a sequence of Pythagorean triples starting from (\"a\", \"b\", \"c\") = (4, 3, 5) via\n\nfor \"n\" ≥ 4.\n\nA Pythagorean triple can be generated using any two positive integers by the following procedures using generalized Fibonacci sequences.\n\nFor initial positive integers \"h\" and \"h\", if and , then\n\nis a Pythagorean triple.\n\nThe following is a matrix-based approach to generating primitive triples with generalized Fibonacci sequences. Start with a 2 × 2 array and insert two coprime positive integers ( q,q' ) in the top row. Place the even integer (if any) in the column.\n\nNow apply the following \"Fibonacci rule\" to get the entries in the bottom \nrow:\n\nSuch an array may be called a \"Fibonacci Box\". Note that q', q, p, p' is a generalized Fibonacci sequence. Taking column, row, and diagonal products we obtain the sides of triangle [a, b, c], its area A, and its perimeter P, as well as the radii r of its incircle and three excircles as follows:\n\nThe half-angle tangents at the acute angles are q/p and q'/p'.\n\nEXAMPLE:\n\nUsing coprime integers 9 and 2.\n\nThe column, row, and diagonal products are: (columns: 22 and 117), (rows: 18 and 143), (diagonals: 26 and 99), so\n\nThe half-angle tangents at the acute angles are 2/11 and 9/13. Note that if the chosen integers q, q' are not coprime, the same procedure leads to a non-primitive triple.\n\nThis method of generating \"primitive Pythagorean triples\" also provides integer solutions to Descartes' Circle Equation,\n\nwhere integer curvatures \"k\" are obtained by multiplying the reciprocal of each radius by the area A. The result is \"k\" = pp', \"k\" = qp', \"k\" = q'p, \"k\" = qq'. Here, the largest circle is taken as having negative curvature with respect to the other three. The largest circle (curvature \"k\") may also be replaced by a smaller circle with positive curvature ( \"k\" = 4\"pp' − qq' \"). \n\nEXAMPLE: \n\nUsing the area and four radii obtained above for primitive triple [44, 117, 125] we obtain the following integer solutions to Descartes' Equation: \"k\" = 143, \"k\" = 99, \"k\" = 26, \"k\" = (−18), and \"k\" = 554.\n\nEach primitive Pythagorean triple corresponds uniquely to a Fibonacci Box. Conversely, each Fibonacci Box corresponds to a unique and primitive Pythagorean triple. In this section we shall use the Fibonacci Box in place of the primitive triple it represents. An infinite ternary tree containing all primitive Pythagorean triples/Fibonacci Boxes can be constructed by the following procedure.\n\nConsider a Fibonacci Box containing two, odd, coprime integers \"x\" and \"y\" in the right-hand column.\n\nIt may be seen that these integers can also be placed as follows:\n\nresulting in three more valid Fibonacci boxes containing \"x\" and \"y\". We may think of the first Box as the \"parent\" of the next three. For example, if \"x\" = 1 and \"y\" = 3 we have:\n\nMoreover, each \"child\" is itself the parent of three more children which can be obtained by the same procedure. Continuing this process at each node leads to an infinite ternary tree containing all possible Fibonacci Boxes, or equivalently, to a ternary tree containing all possible primitive triples. (The tree shown here is distinct from the classic tree described by Berggren in 1934, and has many different number-theoretic properties.) Compare: \"Classic Tree\". See also Tree of primitive Pythagorean triples.\n\nThere are several methods for defining quadratic equations for calculating each leg of a Pythagorean triple. A simple method is to modify the standard Euclid equation by adding a variable \"x\" to each \"m\" and \"n\" pair. The \"m, n\" pair is treated as a constant while the value of \"x\" is varied to produce a \"family\" of triples based on the selected triple. An arbitrary coefficient can be placed in front of the \"x\" value on either \"m\" or \"n\", which causes the resulting equation to systematically \"skip\" through the triples. For example, consider the triple [20, 21, 29] which can be calculated from the Euclid equations with a value of \"m\" = 5 and \"n\" = 2. Also, arbitrarily put the coefficient of 4 in front of the \"\"x\" in the \"m\" term.\n\nLet formula_48 and let formula_49\n\nHence, substituting the values of \"m\" and \"n\":\n\nNote that the original triple comprises the constant term in each of the respective quadratic equations. Below is a sample output from these equations. Note that the effect of these equations is to cause the \"m\" value in the Euclid equations to increment in steps of 4, while the \"n\"\" value increments by 1.\n\nLet be a primitive triple with odd. Then 3 new triples , , may be produced from using matrix multiplication and Berggren's three matrices \"A\", \"B\", \"C\". Triple is termed the \"parent\" of the three new triples (the \"children\"). Each child is itself the parent of 3 more children, and so on. If one begins with primitive triple [3, 4, 5], all primitive triples will eventually be produced by application of these matrices. The result can be graphically represented as an infinite ternary tree with at the root node. An equivalent result may be obtained using Berggrens's three linear transformations shown below.\n\nBerggren's three linear transformations are:\n\nAlternatively, one may also use 3 different matrices found by Price. These matrices \"A', B', C\"' and their corresponding linear transformations are shown below.\n\nPrice's three linear transformations are\n\nThe 3 children produced by each of the two sets of matrices are not the same, but each set separately produces all primitive triples. \nFor example, using [5, 12, 13] as the parent, we get two sets of three children:\n\nAll primitive triples with formula_56 and with \"a\" odd can be generated as follows:\n\nWade and Wade first introduced the categorization of Pythagorean triples by their height, defined as c - b, linking 3,4,5 to 5,12,13 and 7,24,25 and so on.\n\nMcCullough and Wade extended this approach, which produces all Pythagorean triples when formula_57 Write a positive integer \"h\" as pq with \"p\" square-free and \"q\" positive. Set \"d\" = 2\"pq\" if \"p\" is odd, or \"d\"= \"pq\" if \"p\" is even. For all pairs (\"h,k\") of positive integers, the triples are given by\n\nThe primitive triples occur when gcd(\"k, h\") = 1 and either \"h=q\" with \"q\" odd or \"h\"=2\"q\".\n"}
{"id": "32969421", "url": "https://en.wikipedia.org/wiki?curid=32969421", "title": "Forney algorithm", "text": "Forney algorithm\n\nIn coding theory, the Forney algorithm (or Forney's algorithm) calculates the error values at known error locations. It is used as one of the steps in decoding BCH codes and Reed–Solomon codes (a subclass of BCH codes). George David Forney, Jr. developed the algorithm.\n\nCode words look like polynomials. By design, the generator polynomial has consecutive roots α, α, ..., α.\n\nSyndromes\n\nError location polynomial\n\nThe zeros of Λ(\"x\") are \"X\", ..., \"X\". The zeros are the reciprocals of the error locations formula_2.\n\nOnce the error locations are known, the next step is to determine the error values at those locations. The error values are then used to correct the received values at those locations to recover the original codeword.\n\nIn the more general case, the error weights can be determined by solving the linear system\n\nHowever, there is a more efficient method known as the Forney algorithm, which is based on Lagrange interpolation. First calculate the error evaluator polynomial\n\nWhere is the partial syndrome polynomial:\n\nThen evaluate the error values:\nThe value is often called the \"first consecutive root\" or \"fcr\". Some codes select , so the expression simplifies to:\n\nΛ'(\"x\") is the formal derivative of the error locator polynomial Λ(\"x\"):\nIn the above expression, note that \"i\" is an integer, and λ would be an element of the finite field. The operator · represents ordinary multiplication (repeated addition in the finite field) and not the finite field's multiplication operator.\n\nLagrange interpolation\n\nDefine the erasure locator polynomial\nWhere the erasure locations are given by \"j\". Apply the procedure described above, substituting Γ for Λ.\n\nIf both errors and erasures are present, use the error-and-erasure locator polynomial\n\n\n"}
{"id": "18536812", "url": "https://en.wikipedia.org/wiki?curid=18536812", "title": "Fourier transform on finite groups", "text": "Fourier transform on finite groups\n\nIn mathematics, the Fourier transform on finite groups is a generalization of the discrete Fourier transform from cyclic to arbitrary finite groups.\n\nThe Fourier transform of a function formula_1\nat a representation formula_2 of formula_3 is\n\nFor each representation formula_5 of formula_3, formula_7 is a formula_8 matrix, where formula_9 is the degree of formula_5.\n\nThe inverse Fourier transform at an element formula_11 of formula_3 is given by\n\nThe convolution of two functions formula_14 is defined as\n\nThe Fourier transform of a convolution at any representation formula_5 of formula_3 is given by\n\nFor functions formula_14, the Plancherel formula states\n\nwhere formula_21 are the irreducible representations of formula_22\n\nIf the group \"G\" is a finite abelian group, the situation simplifies considerably:\n\n\n\nThe Fourier transform of an element formula_26 is the function formula_27 given by\n\nThe inverse Fourier transform is then given by\n\nFor formula_30, a choice of a primitive \"n\"-th root of unity formula_31 yields an isomorphism\n\ngiven by formula_33. In the literature, the common choice is formula_34, which explains the formula given in the article about the discrete Fourier transform. However, such an isomorphism is not canonical, similarly to the situation that a finite-dimensional vector space is isomorphic to its dual, but giving an isomorphism requires choosing a basis.\n\nA property that is often useful in probability is that the Fourier transform of the uniform distribution is simply formula_35 where 0 is the group identity and formula_36 is the Kronecker delta.\n\nFourier Transform can also be done on cosets of a group.\n\nThere is a direct relationship between the Fourier transform on finite groups and the representation theory of finite groups. The set of complex-valued functions on a finite group, formula_37, together with the operations of pointwise addition and convolution, form a ring that is naturally identified with the group ring of formula_37 over the complex numbers, formula_39. Modules of this ring are the same thing as representations. Maschke's theorem implies that formula_39 is a semisimple ring, so by the Artin–Wedderburn theorem it decomposes as a direct product of matrix rings. The Fourier transform on finite groups explicitly exhibits this decomposition, with a matrix ring of dimension formula_41 for each irreducible representation.\nMore specifically, the Peter-Weyl theorem (for finite groups) states that there is an isomorphism\n\ngiven by\n\nThe left hand side is the group algebra of \"G\". The direct sum is over a complete set of inequivalent irreducible \"G\"-representations formula_44.\n\nThe Fourier transform for a finite group is just this isomorphism. The product formula mentioned above is equivalent to saying that this map is a ring isomorphism.\n\nThis generalization of the discrete Fourier transform is used in numerical analysis. A circulant matrix is a matrix where every column is a cyclic shift of the previous one. Circulant matrices can be diagonalized quickly using the fast Fourier transform, and this yields a fast method for solving systems of linear equations with circulant matrices. Similarly, the Fourier transform on arbitrary groups can be used to give fast algorithms for matrices with other symmetries . These algorithms can be used for the construction of numerical methods for solving partial differential equations that preserve the symmetries of the equations .\n\n\n"}
{"id": "665027", "url": "https://en.wikipedia.org/wiki?curid=665027", "title": "Fourth power", "text": "Fourth power\n\nIn arithmetic and algebra, the fourth power of a number \"n\" is the result of multiplying four instances of \"n\" together. So:\n\nFourth powers are also formed by multiplying a number by its cube. Furthermore, they are squares of squares.\n\nThe sequence of fourth powers of integers (also known as biquadratic numbers or tesseractic numbers) is:\n\nThe last two digits of a fourth power of an integer in base 10 can be easily shown (for instance, by computing the squares of possible last two digits of square numbers) to be restricted to only \"twelve\" possibilities:\n\n\nThese twelve possibilities can be conveniently expressed as 00, \"e\"1, \"o\"6 or 25 where \"o\" is an odd digit and \"e\" an even digit.\n\nEvery positive integer can be expressed as the sum of at most 19 fourth powers; every sufficiently large integer can be expressed as the sum of at most 16 fourth powers (see Waring's problem).\n\nFermat knew that a fourth power cannot be the sum of two other fourth powers (the \"n\"=4 case of Fermat's Last Theorem; see Fermat's right triangle theorem). Euler conjectured that a fourth power cannot be written as the sum of three fourth powers, but 200 years later, in 1986, this was disproven by Elkies with:\n\nElkies showed that there are infinitely many other counterexamples for exponent four, some of which are:\n\nThat the equation \"x\" + \"y\" = \"z\" has no solutions in nonzero integers (a special case of Fermat's Last Theorem), was known by Fermat; see Fermat's right triangle theorem.\n\nFourth-degree equations, which contain a fourth degree (but no higher) polynomial are, by the Abel–Ruffini theorem, the highest degree equations having a general solution using radicals.\n\n"}
{"id": "9775880", "url": "https://en.wikipedia.org/wiki?curid=9775880", "title": "Fraňková–Helly selection theorem", "text": "Fraňková–Helly selection theorem\n\nIn mathematics, the Fraňková–Helly selection theorem is a generalisation of Helly's selection theorem for functions of bounded variation to the case of regulated functions. It was proved in 1991 by the Czech mathematician Dana Fraňková.\n\nLet \"X\" be a separable Hilbert space, and let BV([0, \"T\"]; \"X\") denote the normed vector space of all functions \"f\" : [0, \"T\"] → \"X\" with finite total variation over the interval [0, \"T\"], equipped with the total variation norm. It is well known that BV([0, \"T\"]; \"X\") satisfies the compactness theorem known as Helly's selection theorem: given any sequence of functions (\"f\") in BV([0, \"T\"]; \"X\") that is uniformly bounded in the total variation norm, there exists a subsequence\n\nand a limit function \"f\" ∈ BV([0, \"T\"]; \"X\") such that \"f\"(\"t\") converges weakly in \"X\" to \"f\"(\"t\") for every \"t\" ∈ [0, \"T\"]. That is, for every continuous linear functional \"λ\" ∈ \"X\"*,\n\nConsider now the Banach space Reg([0, \"T\"]; \"X\") of all regulated functions \"f\" : [0, \"T\"] → \"X\", equipped with the supremum norm. Helly's theorem does not hold for the space Reg([0, \"T\"]; \"X\"): a counterexample is given by the sequence\n\nOne may ask, however, if a weaker selection theorem is true, and the Fraňková–Helly selection theorem is such a result.\n\nAs before, let \"X\" be a separable Hilbert space and let Reg([0, \"T\"]; \"X\") denote the space of regulated functions \"f\" : [0, \"T\"] → \"X\", equipped with the supremum norm. Let (\"f\") be a sequence in Reg([0, \"T\"]; \"X\") satisfying the following condition: for every \"ε\" > 0, there exists some \"L\" > 0 so that each \"f\" may be approximated by a \"u\" ∈ BV([0, \"T\"]; \"X\") satisfying\n\nand\n\nwhere |-| denotes the norm in \"X\" and Var(\"u\") denotes the variation of \"u\", which is defined to be the supremum\n\nover all partitions\n\nof [0, \"T\"]. Then there exists a subsequence\n\nand a limit function \"f\" ∈ Reg([0, \"T\"]; \"X\") such that \"f\"(\"t\") converges weakly in \"X\" to \"f\"(\"t\") for every \"t\" ∈ [0, \"T\"]. That is, for every continuous linear functional \"λ\" ∈ \"X\"*,\n"}
{"id": "1359153", "url": "https://en.wikipedia.org/wiki?curid=1359153", "title": "Fröhlich Prize", "text": "Fröhlich Prize\n\nThe Fröhlich Prize of the London Mathematical Society is awarded in even numbered years in memory of Albrecht Fröhlich. The prize is awarded for original and extremely innovative work in any branch of mathematics. According to the regulations the prize is awarded \"to a mathematician who has fewer than 25 years (full time equivalent) of involvement in mathematics at post-doctoral level, allowing for breaks in continuity, or who in the opinion of the Prizes Committee is at an equivalent stage in their career.\"\n\nSource: LMS website \n\n\n"}
{"id": "1137612", "url": "https://en.wikipedia.org/wiki?curid=1137612", "title": "Generalized eigenvector", "text": "Generalized eigenvector\n\nIn linear algebra, a generalized eigenvector of an \"n\" × \"n\" matrix formula_1 is a vector which satisfies certain criteria which are more relaxed than those for an (ordinary) eigenvector.\n\nLet formula_2 be an \"n\"-dimensional vector space; let formula_3 be a linear map in , the set of all linear maps from formula_2 into itself; and let formula_1 be the matrix representation of formula_3 with respect to some ordered basis.\n\nThere may not always exist a full set of \"n\" linearly independent eigenvectors of formula_1 that form a complete basis for formula_2. That is, the matrix formula_1 may not be diagonalizable. This happens when the algebraic multiplicity of at least one eigenvalue formula_10 is greater than its geometric multiplicity (the nullity of the matrix formula_11, or the dimension of its nullspace). In this case, formula_10 is called a defective eigenvalue and formula_1 is called a defective matrix.\n\nA generalized eigenvector formula_14 corresponding to formula_10, together with the matrix formula_11 generate a Jordan chain of linearly independent generalized eigenvectors which form a basis for an invariant subspace of formula_2.\n\nUsing generalized eigenvectors, a set of linearly independent eigenvectors of formula_1 can be extended, if necessary, to a complete basis for formula_2. This basis can be used to determine an \"almost diagonal matrix\" formula_20 in Jordan normal form, similar to formula_1, which is useful in computing certain matrix functions of formula_1. The matrix formula_20 is also useful in solving the system of linear differential equations formula_24 where formula_1 need not be diagonalizable.\n\nThere are several equivalent ways to define an ordinary eigenvector. For our purposes, an eigenvector formula_26 associated with an eigenvalue formula_27 of an formula_28 × formula_28 matrix formula_1 is a nonzero vector for which formula_31, where formula_32 is the formula_28 × formula_28 identity matrix and formula_35 is the zero vector of length formula_28. That is, formula_26 is in the kernel of the transformation formula_38. If formula_1 has formula_28 linearly independent eigenvectors, then formula_1 is similar to a diagonal matrix formula_42. That is, there exists an invertible matrix formula_43 such that formula_1 is diagonalizable through the similarity transformation formula_45. The matrix formula_42 is called a spectral matrix for formula_1. The matrix formula_43 is called a modal matrix for formula_1. Diagonalizable matrices are of particular interest since matrix functions of them can be computed easily.\n\nOn the other hand, if formula_1 does not have formula_28 linearly independent eigenvectors associated with it, then formula_1 is not diagonalizable.\n\nDefinition: A vector formula_53 is a generalized eigenvector of rank \"m\" of the matrix formula_1 and corresponding to the eigenvalue formula_27 if\n\nbut\n\nClearly, a generalized eigenvector of rank 1 is an ordinary eigenvector. Every formula_28 × formula_28 matrix formula_1 has formula_28 linearly independent generalized eigenvectors associated with it and can be shown to be similar to an \"almost diagonal\" matrix formula_20 in Jordan normal form. That is, there exists an invertible matrix formula_43 such that formula_64. The matrix formula_43 in this case is called a generalized modal matrix for formula_1. If formula_27 is an eigenvalue of algebraic multiplicity formula_68, then formula_1 will have formula_68 linearly independent generalized eigenvectors corresponding to formula_27. These results, in turn, provide a straightforward method for computing certain matrix functions of formula_1.\n\nNote: For an formula_73 matrix formula_1 over a field formula_75 to be expressed in Jordan normal form, all eigenvalues of formula_1 must be in formula_75. That is, the characteristic polynomial formula_78 must factor completely into linear factors. For example, if formula_1 has real-valued elements, then it may be necessary for the eigenvalues and the components of the eigenvectors to have complex values.\n\nThe set spanned by all generalized eigenvectors for a given formula_80, forms the generalized eigenspace for formula_80.\n\nHere are some examples to illustrate the concept of generalized eigenvectors. Some of the details will be described later.\n\nThis example is simple but clearly illustrates the point. This type of matrix is used frequently in textbooks.\nSuppose\nThen there is only one eigenvalue, formula_83, and its algebraic multiplicity is \"m\" = 2.\n\nNotice that this matrix is in Jordan normal form but is not diagonal. Hence, this matrix is not diagonalizable. Since there is one superdiagonal entry, there will be one generalized eigenvector of rank greater than 1 (or one could note that the vector space formula_84 is of dimension 2, so there can be at most one generalized eigenvector of rank greater than 1). Alternatively, one could compute the dimension of the nullspace of formula_85 to be \"p\" = 1, and thus there are \"m\" – \"p\" = 1 generalized eigenvectors of rank greater than 1.\n\nThe ordinary eigenvector formula_86 is computed as usual (see the eigenvector page for examples). Using this eigenvector, we compute the generalized eigenvector formula_87 by solving\n\nWriting out the values:\nThis simplifies to\n\nThe element formula_91 has no restrictions. The generalized eigenvector of rank 2 is then formula_92, where \"a\" can have any scalar value. The choice of \"a\" = 0 is usually the simplest.\n\nNote that\n\nso that formula_87 is a generalized eigenvector,\n\nso that formula_96 is an ordinary eigenvector, and that formula_97 and formula_98 are linearly independent and hence constitute a basis for the vector space formula_84.\n\nThis example is more complex than Example 1. Unfortunately, it is a little difficult to construct an interesting example of low order.\nThe matrix\n\nhas \"eigenvalues\" formula_101 and formula_102 with \"algebraic multiplicities\" formula_103 and formula_104, but \"geometric multiplicities\" formula_105 and formula_106.\n\nThe \"generalized eigenspaces\" of formula_1 are calculated below.\nformula_108 is the ordinary eigenvector associated with formula_109.\nformula_110 is a generalized eigenvector associated with formula_109.\nformula_112 is the ordinary eigenvector associated with formula_113.\nformula_114 and formula_115 are generalized eigenvectors associated with formula_113.\n\nThis results in a basis for each of the \"generalized eigenspaces\" of formula_1.\nTogether the two \"chains\" of generalized eigenvectors span the space of all 5-dimensional column vectors.\n\nAn \"almost diagonal\" matrix formula_20 in \"Jordan normal form\", similar to formula_1 is obtained as follows:\n\nwhere formula_43 is a generalized modal matrix for formula_1, the columns of formula_43 are a canonical basis for formula_1, and formula_132.\n\nDefinition: Let formula_53 be a generalized eigenvector of rank \"m\" corresponding to the matrix formula_1 and the eigenvalue formula_27. The chain generated by formula_53 is a set of vectors formula_137 given by\n\nThus, in general,\n\nThe vector formula_138, given by (), is a generalized eigenvector of rank \"j\" corresponding to the eigenvalue formula_27. A chain is a linearly independent set of vectors.\n\nDefinition: A set of \"n\" linearly independent generalized eigenvectors is a canonical basis if it is composed entirely of Jordan chains.\n\nThus, once we have determined that a generalized eigenvector of rank \"m\" is in a canonical basis, it follows that the \"m\" − 1 vectors formula_140 that are in the Jordan chain generated by formula_141 are also in the canonical basis.\n\nLet formula_142 be an eigenvalue of formula_1 of algebraic multiplicity formula_144. First, find the ranks (matrix ranks) of the matrices formula_145. The integer formula_146 is determined to be the \"first integer\" for which formula_147 has rank formula_148 (\"n\" being the number of rows or columns of formula_1, that is, formula_1 is \"n\" × \"n\").\n\nNow define\n\nThe variable formula_152 designates the number of linearly independent generalized eigenvectors of rank \"k\" corresponding to the eigenvalue formula_142 that will appear in a canonical basis for formula_1. Note that\n\nIn the preceding sections we have seen techniques for obtaining the \"n\" linearly independent generalized eigenvectors of a canonical basis for the vector space formula_2 associated with an \"n\" × \"n\" matrix formula_1. These techniques can be combined into a procedure:\n\nThe matrix\n\nhas an eigenvalue formula_168 of algebraic multiplicity formula_169 and an eigenvalue formula_170 of algebraic multiplicity formula_171. We also have \"n\" = 4. For formula_172 we have formula_173.\n\nThe first integer formula_177 for which formula_178 has rank formula_179 is formula_180.\n\nWe now define\n\nConsequently, there will be three linearly independent generalized eigenvectors; one each of ranks 3, 2 and 1. Since formula_172 corresponds to a single chain of three linearly independent generalized eigenvectors, we know that there is a generalized eigenvector formula_185 of rank 3 corresponding to formula_172 such that\n\nbut\n\nEquations () and () represent linear systems that can be solved for formula_185. Let\n\nThen\n\nand\n\nThus, in order to satisfy the conditions () and (), we must have formula_191 and formula_192. No restrictions are placed on formula_193 and formula_194. By choosing formula_195, we obtain\n\nas a generalized eigenvector of rank 3 corresponding to formula_197. Note that it is possible to obtain infinitely many other generalized eigenvectors of rank 3 by choosing different values of formula_193, formula_194 and formula_200, with formula_192. Our first choice, however, is the simplest.\n\nNow using equations (), we obtain formula_110 and formula_108 as generalized eigenvectors of rank 2 and 1 respectively, where\n\nand\n\nThe simple eigenvalue formula_170 can be dealt with using standard techniques and has an ordinary eigenvector\n\nA canonical basis for formula_1 is\n\nformula_210 and formula_185 are generalized eigenvectors associated with formula_109. \nformula_112 is the ordinary eigenvector associated with formula_113.\n\nIt should be noted that this is a fairly simple example. In general, the numbers formula_164 of linearly independent generalized eigenvectors of rank \"k\" will not always be equal. That is, there may be several chains of different lengths corresponding to a particular eigenvalue.\n\nLet formula_1 be an \"n\" × \"n\" matrix. A generalized modal matrix formula_43 for formula_1 is an \"n\" × \"n\" matrix whose columns, considered as vectors, form a canonical basis for formula_1 and appear in formula_43 according to the following rules:\n\n\nLet formula_2 be an \"n\"-dimensional vector space; let formula_3 be a linear map in , the set of all linear maps from formula_2 into itself; and let formula_1 be the matrix representation of formula_3 with respect to some ordered basis. It can be shown that if the characteristic polynomial formula_229 of formula_1 factors into linear factors, so that formula_229 has the form\n\nwhere formula_233 are the distinct eigenvalues of formula_1, then each formula_235 is the algebraic multiplicity of its corresponding eigenvalue formula_10 and formula_1 is similar to a matrix formula_20 in Jordan normal form, where each formula_10 appears formula_235 consecutive times on the diagonal, and the entry directly above each formula_10 (that is, on the superdiagonal) is either 0 or 1: the entry above the first occurrence of each formula_10 is always 0; all other entries on the superdiagonal are 1. All other entries (that is, off the diagonal and superdiagonal) are 0. The matrix formula_20 is as close as one can come to a diagonalization of formula_1. If formula_1 is diagonalizable, then all entries above the diagonal are zero. Note that some textbooks have the ones on the subdiagonal, that is, immediately below the main diagonal instead of on the superdiagonal. The eigenvalues are still on the main diagonal.\n\nEvery \"n\" × \"n\" matrix formula_1 is similar to a matrix formula_20 in Jordan normal form, obtained through the similarity transformation formula_248, where formula_43 is a generalized modal matrix for formula_1. (See Note above.)\n\nFind a matrix in Jordan normal form that is similar to\n\nSolution: The characteristic equation of formula_1 is formula_253, hence, formula_254 is an eigenvalue of algebraic multiplicity three. Following the procedures of the previous sections, we find that\n\nand\n\nThus, formula_257 and formula_258, which implies that a canonical basis for formula_1 will contain one linearly independent generalized eigenvector of rank 2 and two linearly independent generalized eigenvectors of rank 1, or equivalently, one chain of two vectors formula_260 and one chain of one vector formula_261. Designating formula_262, we find that\n\nand\n\nwhere formula_43 is a generalized modal matrix for formula_1, the columns of formula_43 are a canonical basis for formula_1, and formula_132. Note that since generalized eigenvectors themselves are not unique, and since some of the columns of both formula_43 and formula_20 may be interchanged, it follows that both formula_43 and formula_20 are not unique.\n\nIn Example 3, we found a canonical basis of linearly independent generalized eigenvectors for a matrix formula_1. A generalized modal matrix for formula_1 is\n\nA matrix in Jordan normal form, similar to formula_1 is\n\nso that formula_132.\n\nThree of the most fundamental operations which can be performed on square matrices are matrix addition, multiplication by a scalar, and matrix multiplication. These are exactly those operations necessary for defining a polynomial function of an \"n\" × \"n\" matrix formula_1. If we recall from basic calculus that many functions can be written as a Maclaurin series, then we can define more general functions of matrices quite easily. If formula_1 is diagonalizable, that is\n\nwith\n\nthen\n\nand the evaluation of the Maclaurin series for functions of formula_1 is greatly simplified. For example, to obtain any power \"k\" of formula_1, we need only compute formula_287, premultiply formula_287 by formula_43, and postmultiply the result by formula_290.\n\nUsing generalized eigenvectors, we can obtain the Jordan normal form for formula_1 and these results can be generalized to a straightforward method for computing functions of nondiagonalizable matrices. (See Matrix function#Jordan decomposition.)\n\nConsider the problem of solving the system of linear ordinary differential equations\n\nwhere\n\nIf the matrix formula_1 is a diagonal matrix so that formula_295 for formula_296, then the system () reduces to a system of \"n\" equations which take the form\n\nIn this case, the general solution is given by\n\nIn the general case, we try to diagonalize formula_1 and reduce the system () to a system like () as follows. If formula_1 is diagonalizable, we have formula_303, where formula_43 is a modal matrix for formula_1. Substituting formula_306, equation () takes the form formula_307, or\n\nwhere\n\nThe solution of () is\n\nThe solution formula_312 of () is then obtained using the relation ().\n\nOn the other hand, if formula_1 is not diagonalizable, we choose formula_43 to be a generalized modal matrix for formula_1, such that formula_248 is the Jordan normal form of formula_1. The system formula_318 has the form\n\nwhere the formula_142 are the eigenvalues from the main diagonal of formula_20 and the formula_321 are the ones and zeros from the superdiagonal of formula_20. The system () is often more easily solved than (). We may solve the last equation in () for formula_323, obtaining formula_324. We then substitute this solution for formula_323 into the next to last equation in () and solve for formula_326. Continuing this procedure, we work through () from the last equation to the first, solving the entire system for formula_327. The solution formula_312 is then obtained using the relation ().\n\n"}
{"id": "37673979", "url": "https://en.wikipedia.org/wiki?curid=37673979", "title": "Ida Barney", "text": "Ida Barney\n\nIda Barney (November 6, 1886 – March 7, 1982) was an American astronomer, best known for her 22 volumes of astrometric measurements on 150,000 stars. She was educated at Smith College and Yale University and spent most of her career at the Yale University Observatory. She was the 1952 recipient of the Annie J. Cannon Award in Astronomy.\n\nBarney was born on 6 November 1886 in New Haven, Connecticut. Her mother was Ida Bushnell Barney and her father was Samuel Eben Barney.\nShe was an avid birder and the New Haven Bird Club President. After her retirement from Yale, she continued to live in New Haven, where she died on 7 March 1982, 95 years old.\n\nIn 1908, Barney graduated from Smith College with a Bachelor of Arts degree. There, she was a member of Phi Beta Kappa and Sigma Xi, national honor societies for students. Three years later, she received her Ph.D. in mathematics from Yale University.\n\nFrom 1911–1912, just after receiving her Ph.D., Barney was a mathematics professor at Rollins College. At the conclusion of that year, she moved to her alma mater to Smith College, where she was an instructor of mathematics. In 1917, she was hired as a professor at Lake Erie College, where she stayed until 1919. In 1920, she returned to Smith College as an assistant professor. In 1922, the Yale University Observatory appointed Barney a Research Assistant, a title she held until 1949, when she was promoted to Research Associate. The Observatory, like many other university observatories, was allocating significant resources to astrometry, thanks to the development of telescope-mounted cameras. At the beginning of her career in astronomy, Barney worked under Frank Schlesinger; she plotted the position of stars from photographic plates and worked on the calculations of their celestial coordinates from their positions on the plates. The work was tedious, which Schlesinger thought to be suitable for women incapable of theoretical research. Despite this, she developed several methods that increased both the accuracy and speed of her measurements, including the use of a machine that automatically centered the photographic plates. Her life's work, completed over 23 years, contributed to the Yale Observatory Zone Catalog, a series of star catalogs published by the Yale Observatory for 1939 to 1983, containing around 400,000 stars, and influenced the Bright Star Catalogue. In 1941, when Schlesinger retired, Barney took over full supervision of the cataloguing. Under her direction, the measurements of the photographic plates were completed at the IBM Watson Scientific Laboratory using a new electronic device that further reduced eye strain and increased accuracy. Her individual contribution to these star catalogues recorded the position, magnitude, and proper motion of approximately 150,000 stars. Due to its high accuracy, the catalogue is still used today in proper motion studies. She retired from academic life in 1955. She was succeeded by Ellen Dorrit Hoffleit.\n\nWhile a Research Associate at the Yale University Observatory, in 1952, Barney was awarded the triennial Annie J. Cannon Award in Astronomy, a prestigious award for women astronomers given by the American Astronomical Society.\n\nHer remains are interred at Grove Street Cemetery in New Haven, Connecticut.\n\nAsteroid 5655 Barney, discovered by Ingrid van Houten-Groeneveld, Cornelis Johannes van Houten and Tom Gehrels at Palomar Observatory in 1973, was named it in her memory.\n\n\n\n\n\n"}
{"id": "26280279", "url": "https://en.wikipedia.org/wiki?curid=26280279", "title": "Inclusion (logic)", "text": "Inclusion (logic)\n\nIn logic and mathematics, inclusion is the concept that all the contents of one object are also contained within a second object.\n\nFor example, if \"m\" and \"n\" are two logical matrices, then\n\nThe modern symbol for inclusion first appears in Gergonne (1816), who defines it as one idea 'containing' or being 'contained' by another, using the backward letter 'C' to express this. Peirce articulated this clearly in 1870, arguing also that inclusion was a wider concept than equality, and hence a logically simpler one. Schröder (also Frege) calls the same concept 'subordination'.\n"}
{"id": "52183761", "url": "https://en.wikipedia.org/wiki?curid=52183761", "title": "James Gow (scholar)", "text": "James Gow (scholar)\n\nJames Gow (1854–1923) was an English scholar, educator, historian, and author, widely recognized for \"A Short History of Greek Mathematics\". The history drew highly upon the work of Moritz Cantor, as well as upon pioneering works of Carl Anton Bretschneider, Hermann Hankel, and George Johnston Allman, but included material, e.g., gematria, not discussed by contemporary historians of mathematics.\n\nJames was the son of the artist James Gow (Sr.), who was a member of the Royal Society of British Artists. He married Gertrude Sydenham (the daughter of G. P. and M. A. Everett Green) with whom he had three sons. Following an education at King's College School, he received his Master of Arts a Trinity College, Cambridge. Gow was a third Classic and Chancellor's Classical medalist in 1875 at Cambridge, and became a Fellow of Trinity College and of King's College, London and in 1876. At Cambridge he earned Doctor of Letters in 1885 and served as University Extension Lecturer from 1876 to 1878. He was Barrister at Lincoln's Inn in 1875, President of the Headmaster's Association from 1900 to 1902, and Chairman of the Headmaster's Conference in 1906. Gow was also Master of the Nottingham High School from 1885 to 1901, following which, he became Headmaster of Westminster school. He also contributed articles to the 1911 \"Encyclopædia Britannica\".\n\n\n"}
{"id": "11751393", "url": "https://en.wikipedia.org/wiki?curid=11751393", "title": "John Riordan (mathematician)", "text": "John Riordan (mathematician)\n\nJohn Francis Riordan (April 22, 1903 – August 27, 1988) was an American mathematician and the author of major early works in combinatorics, particularly \"Introduction to Combinatorial Analysis\" and \"Combinatorial Identities\".\n\nRiordan was a graduate of Yale University. In his early life he wrote a number of poems and essays and a book of short-stories, \"On the Make\", published in 1929, and was Editor-in-Chief of \"Salient\" and \"The Figure in the Carpet\", literary magazines published by The New School for Social Research in New York. He married Mavis McIntosh, the well-known poet and literary agent and founder of McIntosh & Otis. The couple had two daughters: Sheila Riordan and Kathleen Riordan Speeth, and were long time residents of Hastings-on-Hudson, New York.\n\nRiordan's long professional career was at Bell Labs, which he joined in 1926 (a year after its foundation) and where he remained, publishing over a hundred scholarly papers on combinatorial analysis, until he retired in 1968. He then joined the faculty at Rockefeller University as professor emeritus. A Festschrift was published in his honor in 1978.\n\nThroughout his life Riordan led an active literary life, with many distinguished friends such as Kenneth Burke, William Carlos Williams, and A. R. Orage.\n\nFrom the \"Introduction\" by Marc Kac to the Special Issue of the \"JCTA\" in honor of John Riordan:\n\nFrom an interview with Neil Sloane published by Bell Labs:\n\n\n"}
{"id": "46187197", "url": "https://en.wikipedia.org/wiki?curid=46187197", "title": "Karl-Heinz Boseck", "text": "Karl-Heinz Boseck\n\nKarl-Heinz Boseck (born 11 December 1915) was a German mathematician.\n\nAccording to Segal (2003), Boseck was a fanatical National Socialist and a student leader.\nHe was an informer of the Gestapo since 1939.\nIn 1944, shortly after his diploma graduation he was made an Untersturmführer of the Nazi SS and established a department for numerical computation in the Sachsenhausen concentration camp\nHe was exempted from war service due to a disease.\nHe was an assistant of the German mathematician Alfred Klose at Berlin University, and had great influence in the faculty during World War II.\nAt the first mathematicians camp 1–3 July 1938 in the youth hostel of Ützdorf near Bernau, he lectured \"On the development of student science work\".\nHe was department chairman for natural science at Berlin University, and had great influence on Ludwig Bieberbach who was leader of the \"seminar\" (may be institute); with course of time even more power shifted from Bieberbach to Boseck.\n"}
{"id": "7635266", "url": "https://en.wikipedia.org/wiki?curid=7635266", "title": "Krylov–Bogolyubov theorem", "text": "Krylov–Bogolyubov theorem\n\nIn mathematics, the Krylov–Bogolyubov theorem (also known as the existence of invariant measures theorem) may refer to either of the two related fundamental theorems within the theory of dynamical systems. The theorems guarantee the existence of invariant measures for certain \"nice\" maps defined on \"nice\" spaces and were named after Russian-Ukrainian mathematicians and theoretical physicists Nikolay Krylov and Nikolay Bogolyubov who proved the theorems.\n\nTheorem (Krylov–Bogolyubov). Let (\"X\", \"T\") be a compact, metrizable topological space and \"F\" : \"X\" → \"X\" a continuous map. Then \"F\" admits an invariant Borel probability measure.\n\nThat is, if Borel(\"X\") denotes the Borel σ-algebra generated by the collection \"T\" of open subsets of \"X\", then there exists a probability measure \"μ\" : Borel(\"X\") → [0, 1] such that for any subset \"A\" ∈ Borel(\"X\"),\n\nIn terms of the push forward, this states that\n\nLet \"X\" be a Polish space and let formula_3 be the transition probabilities for a time-homogeneous Markov semigroup on \"X\", i.e.\n\nTheorem (Krylov–Bogolyubov). If there exists a point formula_5 for which the family of probability measures { \"P\"(\"x\", ·) | \"t\" > 0 } is uniformly tight and the semigroup (\"P\") satisfies the Feller property, then there exists at least one invariant measure for (\"P\"), i.e. a probability measure \"μ\" on \"X\" such that\n\n"}
{"id": "249844", "url": "https://en.wikipedia.org/wiki?curid=249844", "title": "List of Slovenian mathematicians", "text": "List of Slovenian mathematicians\n\nThis is a list of Slovenian mathematicians.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathematician, Geometer, List of Slovenians.\n"}
{"id": "234854", "url": "https://en.wikipedia.org/wiki?curid=234854", "title": "List of integrals of rational functions", "text": "List of integrals of rational functions\n\nThe following is a list of integrals (antiderivative functions) of rational functions. \nAny rational function can be integrated by partial fraction decomposition of the function into a sum of functions of the form:\nwhich can then be integrated term by term.\n\nFor other types of functions, see lists of integrals.\n\nMany of the following antiderivatives have a term of the form ln |\"ax\" + \"b\"|. Because this is undefined when \"x\" = −\"b\" / \"a\", the most general form of the antiderivative replaces the constant of integration with a locally constant function. However, it is conventional to omit this from the notation. For example,\nis usually abbreviated as\nwhere \"C\" is to be understood as notation for a locally constant function of \"x\". This convention will be adhered to in the following.\n\nFor formula_21\n<br>\n<br>\n<br>\n\n\n\n\n\n\n\n"}
{"id": "26826869", "url": "https://en.wikipedia.org/wiki?curid=26826869", "title": "Ludwig Schlesinger", "text": "Ludwig Schlesinger\n\nLudwig Schlesinger (Hungarian: Lajos Schlesinger, Slovak Ľudovít Schlesinger), (1 November 1864 – 15 December 1933) was a German mathematician known for the research in the field of linear differential equations.\n\nSchlesinger attended the high school in Bratislava and later studied physics and mathematics in Heidelberg and Berlin. In 1887 he received his PhD (Über lineare homogene Differentialgleichungen vierter Ordnung, zwischen deren Integralen homogene Relationen höheren als ersten Grades bestehen.) His thesis advisors were Lazarus Immanuel Fuchs and Leopold Kronecker. In 1889 he became an associate professor at Berlin; in 1897 an invited professor in Bonn and in the same year, a full professor at the University of Kolozsvár, Hungary (now Cluj, Romania). From 1911 he was professor at the University of Giessen, where he taught until 1930. In 1933 he was forced to retire by the Nazis. He died shortly afterwards.\n\nSchlesinger was a historian of science. He wrote an article on the function theory of Carl Friedrich Gauss and translated René Descartes' \"La Géométrie\" into German (1894). He was one of the organizers of the celebrations for the hundredth anniversary of János Bolyai and from 1904 to 1909 with R. Fuchs he collected the works of his teacher Lazarus Fuchs, who was also his father-in-law. In 1902 he became a corresponding member of the Hungarian Academy of Sciences. In 1909 he received the Lobachevsky Prize.\n\nFrom 1929 until his death he was co-editor of \"Crelle's Journal\".\n\nLike his teacher Fuchs, he worked primarily on linear ordinary differential equations. His two-volume \"Handbuch der Theorie der Linearen Differentialgleichungen\" was published from 1895 to 1898 in Teubner in Leipzig (Vol.2 in two parts). He also published \"Einführung in die Theorie der gewöhnlichen Differentialgleichungen auf funktionentheoretischer Grundlage\" (Auflage, 1922), \"Vorlesungen über lineare Differentialgleichungen\" (1908) and \"Automorphe Funktionen\" (Gruyter, 1924). In 1909 he wrote a long report for the annual report of the German Mathematical Society on the history of linear differential equations since 1865. He also studied differential geometry, and wrote a book of lectures on Einstein's general relativity theory.\n\nToday, his best known work is \"Über eine Klasse von Differentialsystemen beliebiger Ordnung mit festen kritischen Punkten\" (Crelle's Journal, 1912). There he considered the problem of isomonodromy deformations for a certain matrix Fuchsian equation; this is a special case of Hilbert's 21st Problem (existence of differential equations with prescribed monodromy). The paper introduced what are today called Schlesinger transformations and Schlesinger equations.\n\nThe article was created as a translation (by Google) of the corresponding article in German Wikipedia.\n"}
{"id": "5173642", "url": "https://en.wikipedia.org/wiki?curid=5173642", "title": "Naimark's dilation theorem", "text": "Naimark's dilation theorem\n\nIn operator theory, Naimark's dilation theorem is a result that characterizes positive operator valued measures. It can be viewed as a consequence of Stinespring's dilation theorem.\n\nIn the mathematical literature, one may also find other results that bear Naimark's name.\n\nIn the physics literature, it is common to see the spelling “Neumark” instead of “Naimark.” The latter variant is according to the romanization of Russian used in translation of Soviet journals, with diacritics omitted (originally Naĭmark). The former is according to the etymology of the surname.\n\nLet \"X\" be a compact Hausdorff space, \"H\" be a Hilbert space, and \"L(H)\" the Banach space of bounded operators on \"H\". A mapping \"E\" from the Borel σ-algebra on \"X\" to formula_1 is called an operator-valued measure if it is weakly countably additive, that is, for any disjoint sequence of Borel sets formula_2, we have\n\nfor all \"x\" and \"y\". Some terminology for describing such measures are:\n\n\nis a regular Borel measure, meaning all compact sets have finite total variation and the measure of a set can be approximated by those of open sets.\n\n\nWe will assume throughout that \"E\" is regular.\n\nLet \"C(X)\" denote the abelian C*-algebra of continuous functions on \"X\". If \"E\" is regular and bounded, it induces a map formula_8 in the obvious way:\n\nThe boundedness of \"E\" implies, for all \"h\" of unit norm\n\nThis shows formula_11 is a bounded operator for all \"f\", and formula_12 itself is a bounded linear map as well.\n\nThe properties of formula_13 are directly related to those of \"E\":\n\n\nTake \"f\" and \"g\" to be indicator functions of Borel sets and we see that formula_12 is a homomorphism if and only if \"E\" is spectral.\n\n\nThe LHS is\n\nand the RHS is\n\nSo, taking f a sequence of continuous functions increasing to the indicator function of \"B\", we get formula_23, i.e. \"E(B)\" is self adjoint.\n\n\nThe theorem reads as follows: Let \"E\" be a positive \"L(H)\"-valued measure on \"X\". There exists a Hilbert space \"K\", a bounded operator formula_25, and a self-adjoint, spectral \"L(K)\"-valued measure on \"X\", \"F\", such that\n\nWe now sketch the proof. The argument passes \"E\" to the induced map formula_13 and uses Stinespring's dilation theorem. Since \"E\" is positive, so is formula_13 as a map between C*-algebras, as explained above. Furthermore, because the domain of formula_12, \"C(X)\", is an abelian C*-algebra, we have that formula_13 is completely positive. By Stinespring's result, there exists a Hilbert space \"K\", a *-homomorphism formula_31, and operator formula_25 such that\n\nSince π is a *-homomorphism, its corresponding operator-valued measure \"F\" is spectral and self adjoint. It is easily seen that \"F\" has the desired properties.\n\nIn the finite-dimensional case, there is a somewhat more explicit formulation.\n\nSuppose now formula_34, therefore \"C\"(\"X\") is the finite-dimensional algebra formula_35, and \"H\" has finite dimension \"m\". A positive operator-valued measure \"E\" then assigns each \"i\" a positive semidefinite \"m\" × \"m\" matrix formula_36. Naimark's theorem now states that there is a projection-valued measure on \"X\" whose restriction is \"E\".\n\nOf particular interest is the special case when formula_37 where \"I\" is the identity operator. (See the article on POVM for relevant applications.) In this case, the induced map formula_13 is unital. It can be assumed with no loss of generality that each formula_36 is a rank-one projection onto some formula_40. Under such assumptions, the case formula_41 is excluded and we must have either\nFor the second possibility, the problem of finding a suitable projection-valued measure now becomes the following problem. By assumption, the non-square matrix\nis an isometry, that is formula_48. If we can find a formula_49 matrix \"N\" where\nis a \"n\" × \"n\" unitary matrix, the projection-valued measure whose elements are projections onto the column vectors of \"U\" will then have the desired properties. In principle, such a \"N\" can always be found.\n\n"}
{"id": "47077309", "url": "https://en.wikipedia.org/wiki?curid=47077309", "title": "Negative-dimensional space", "text": "Negative-dimensional space\n\nIn topology, a discipline within mathematics, a negative-dimensional space is an extension of the usual notion of space, allowing for negative dimensions.\n\nSuppose that is a compact space of Hausdorff dimension , which is an element of a scale of compact spaces embedded in each other and parametrized by (). Such scales are considered \"equivalent\" with respect to if the compact spaces constituting them coincide for . It is said that the compact space is the \"hole\" in this equivalent set of scales, and is the negative dimension of the corresponding equivalence class.\n\nBy the 1940s, the science of topology had developed and studied a thorough basic theory of topological spaces of positive dimension. Motivated by computations, and to some extent aesthetics, topologists searched\nfor mathematical frameworks that extended our notion of space to allow for negative dimensions. Such dimensions, as well as the fourth and higher dimensions, are hard to imagine since we are not able to directly observe them. It wasn’t until the 1960s that a special topological framework was constructed—the category of spectra. A spectrum is a generalization of space that allows for negative dimensions. The concept of negative-dimensional spaces is applied, for example, to analyze linguistic statistics.\n\n\n"}
{"id": "665957", "url": "https://en.wikipedia.org/wiki?curid=665957", "title": "Nondeterministic algorithm", "text": "Nondeterministic algorithm\n\nIn computer science, a nondeterministic algorithm is an algorithm that, even for the same input, can exhibit different behaviors on different runs, as opposed to a deterministic algorithm. There are several ways an algorithm may behave differently from run to run. A concurrent algorithm can perform differently on different runs due to a race condition. A probabilistic algorithm's behaviors depends on a random number generator. An algorithm that solves a problem in nondeterministic polynomial time can run in polynomial time or exponential time depending on the choices it makes during execution. The nondeterministic algorithms are often used to find an approximation to a solution, when the exact solution would be too costly to obtain using a deterministic one.\n\nThe notion was introduced by Robert W. Floyd in 1967.\n\nOften in computational theory, the term \"algorithm\" refers to a deterministic algorithm. A nondeterministic algorithm is different from its more familiar deterministic counterpart in its ability to arrive at outcomes using various routes. If a deterministic algorithm represents a single path from an input to an outcome, a nondeterministic algorithm represents a single path stemming into many paths, some of which may arrive at the same output and some of which may arrive at unique outputs. This property is captured mathematically in \"nondeterministic\" models of computation such as the nondeterministic finite automaton. In some scenarios, all possible paths are allowed to run simultaneously.\n\nIn algorithm design, nondeterministic algorithms are often used when the problem solved by the algorithm inherently allows multiple outcomes (or when there is a single outcome with multiple paths by which the outcome may be discovered, each equally preferable). Crucially, every outcome the nondeterministic algorithm produces is valid, regardless of which choices the algorithm makes while running.\n\nIn computational complexity theory, nondeterministic algorithms are ones that, at every possible step, can allow for multiple continuations (imagine a person walking down a path in a forest and, every time they step further, they must pick which fork in the road they wish to take). These algorithms do not arrive at a solution for every possible computational path; however, they are guaranteed to arrive at a correct solution for some path (i.e., the person walking through the forest may only find their cabin if they pick some combination of \"correct\" paths). The choices can be interpreted as guesses in a search process.\n\nA large number of problems can be conceptualized through nondeterministic algorithms, including the most famous unresolved question in computing theory, P vs NP.\n\nOne way to simulate a nondeterministic algorithm \"N\" using a deterministic algorithm \"D\" is to treat sets of states of \"N\" as states of \"D\". This means that \"D\" simultaneously traces all the possible execution paths of \"N\" (see powerset construction for this technique in use for finite automata).\n\nAnother is randomization, which consists of letting all choices be determined by a random number generator. The result is called a probabilistic deterministic algorithm.\n\nThe problem: given a natural number \"n\" larger than two, determine whether it is prime.\n\nA nondeterministic algorithm for this problem is the following based on Fermat's little theorem:\n\nIf this algorithm returns the answer composite then the number is certainly not prime. If the algorithm returns the answer probably prime then there is a high probability that the number is prime, but a slight chance that it is composite. This is an example of a probabilistic nondeterministic algorithm, because it will not always return the same result given a particular input.\n\n\n"}
{"id": "56478", "url": "https://en.wikipedia.org/wiki?curid=56478", "title": "North", "text": "North\n\nNorth is one of the four compass points or cardinal directions. It is the opposite of south and is perpendicular to east and west. \"North\" is a noun, adjective, or adverb indicating direction or geography.\n\nThe word \"north\" is related to the Old High German \"nord\", both descending from the Proto-Indo-European unit *\"ner-\", meaning \"left; below\" as north is to left when facing the rising sun. Similarly, the other cardinal directions are also related to the sun's position.\n\nThe Latin word \"borealis\" comes from the Greek \"boreas\" \"north wind, north\", which, according to Ovid, was personified as the son of the river-god Strymon, the father of Calais and Zetes. \"Septentrionalis\" is from \"septentriones\", \"the seven plow oxen\", a name of \"Ursa Maior\". The Greek ἀρκτικός (\"arktikós\") is named for the same constellation, and is the source of the English word \"Arctic\".\n\nOther languages have other derivations. For example, in Lezgian, \"kefer\" can mean both \"disbelief\" and \"north\", since to the north of the Muslim Lezgian homeland there are areas formerly inhabited by non-Muslim Caucasian and Turkic peoples. In many languages of Mesoamerica, \"north\" also means \"up\". In Hungarian, the word for north is \"észak\", which is derived from \"éjszaka\" (\"night\"), since above the Tropic of Cancer, the Sun never shines from the north, except inside the Arctic Circle during the summer midnight sun.\n\nThe direction north is often associated with colder climates because most of the world's land at high latitudes is located in the Northern Hemisphere. The Arctic Circle passes through the Arctic Ocean, Norway, Sweden, Finland, Russia, the United States (Alaska), Canada (Yukon, Northwest Territories and Nunavut), Denmark (Greenland) and Iceland (where it passes through the small offshore island of Grímsey).\n\nBy convention, the top side of a map is often north.\n\nTo go north using a compass for navigation, set a bearing or azimuth of 0° or 360°.\n\nNorth is specifically the direction that, in Western culture, is treated as \"the\" fundamental direction:\n\nMagnetic north is of interest because it is the direction indicated as north on a properly functioning (but uncorrected) magnetic compass. The difference between it and true north is called the magnetic declination (or simply the declination where the context is clear). For many purposes and physical circumstances, the error in direction that results from ignoring the distinction is tolerable; in others a mental or instrument compensation, based on assumed knowledge of the applicable declination, can solve all the problems. But simple generalizations on the subject should be treated as unsound, and as likely to reflect popular misconceptions about terrestrial magnetism.\n\nMaps intended for usage in orienteering by compass will clearly indicate the local declination for easy correction to true north. Maps may also indicate grid north, which is a navigational term referring to the direction northwards along the grid lines of a map projection.\n\nThe visible rotation of the night sky around the visible celestial pole provides a vivid metaphor of that direction corresponding to up. Thus the choice of the north as corresponding to up in the northern hemisphere, or of south in that role in the southern, is, prior to worldwide communication, anything but an arbitrary one. On the contrary, it is of interest that Chinese and Islamic culture even considered south as the proper top end for maps.\n\nIn Western culture:\n\nWhile the choice of north over south as prime direction reflects quite arbitrary historical factors, east and west are not nearly as natural alternatives as first glance might suggest. Their folk definitions are, respectively, \"where the sun rises\" and \"where it sets\". Except on the Equator, however, these definitions, taken together, would imply that\n\nReasonably accurate folk astronomy, such as is usually attributed to Stone Age peoples or later Celts, would arrive at east and west by noting the directions of rising and setting (preferably more than once each) and choosing as prime direction one of the two mutually opposite directions that lie halfway between those two. The true folk-astronomical definitions of east and west are \"the directions, a right angle from the prime direction, that are closest to the rising and setting, respectively, of the sun (or moon).\n\nBeing the \"default\" direction on the compass, north is referred to frequently in Western popular culture. Some examples include:\n\n\n"}
{"id": "172048", "url": "https://en.wikipedia.org/wiki?curid=172048", "title": "Partially ordered group", "text": "Partially ordered group\n\nIn abstract algebra, a partially ordered group is a group \"(G,+)\" equipped with a partial order \"≤\" that is \"translation-invariant\"; in other words, \"≤\" has the property that, for all \"a\", \"b\", and \"g\" in \"G\", if \"a\" ≤ \"b\" then \"a+g\" ≤ \"b+g\" and \"g+a\" ≤ \"g+b\".\n\nAn element \"x\" of \"G\" is called positive element if 0 ≤ \"x\". The set of elements 0 ≤ \"x\" is often denoted with \"G\", and it is called the positive cone of G. So we have \"a\" ≤ \"b\" if and only if \"-a\"+\"b\" ∈ \"G\".\n\nBy the definition, we can reduce the partial order to a monadic property: \"a\" ≤ \"b\" if and only if \"0\" ≤ \"-a\"+\"b\".\n\nFor the general group \"G\", the existence of a positive cone specifies an order on \"G\". A group \"G\" is a partially ordered group if and only if there exists a subset \"H\" (which is \"G\") of \"G\" such that:\n\nA partially ordered group \"G\" with positive cone \"G\" is said to be unperforated if \"n\" · \"g\" ∈ \"G\" for some positive integer \"n\" implies \"g\" ∈ \"G\". Being unperforated means there is no \"gap\" in the positive cone \"G\".\n\nIf the order on the group is a linear order, then it is said to be a linearly ordered group.\nIf the order on the group is a lattice order, i.e. any two elements have a least upper bound, then it is a lattice-ordered group (shortly l-group, though usually typeset with a script ell: ℓ-group).\n\nA Riesz group is an unperforated partially ordered group with a property slightly weaker than being a lattice ordered group. Namely, a Riesz group satisfies the Riesz interpolation property: if \"x\", \"x\", \"y\", \"y\" are elements of \"G\" and \"x\" ≤ \"y\", then there exists \"z\" ∈ \"G\" such that \"x\" ≤ \"z\" ≤ \"y\".\n\nIf \"G\" and \"H\" are two partially ordered groups, a map from \"G\" to \"H\" is a \"morphism of partially ordered groups\" if it is both a group homomorphism and a monotonic function. The partially ordered groups, together with this notion of morphism, form a category.\n\nPartially ordered groups are used in the definition of valuations of fields.\n\n\n\n\n"}
{"id": "13309700", "url": "https://en.wikipedia.org/wiki?curid=13309700", "title": "Pentagonal bipyramidal molecular geometry", "text": "Pentagonal bipyramidal molecular geometry\n\nIn chemistry, a pentagonal bipyramid is a molecular geometry with one atom at the centre with seven ligands at the corners of a pentagonal bipyramid. A perfect pentagonal bipyramid belongs to the molecular point group \"D\".\n\nThe pentagonal bipyramid is a case where bond angles surrounding an atom are not identical (see also trigonal bipyramidal molecular geometry). This is one of the three common shapes for heptacoordinate transition metal complexes, along with the capped octahedron and the capped trigonal prism.\n\n"}
{"id": "215889", "url": "https://en.wikipedia.org/wiki?curid=215889", "title": "Permanent (mathematics)", "text": "Permanent (mathematics)\n\nIn linear algebra, the permanent of a square matrix is a function of the matrix similar to the determinant. The permanent, as well as the determinant, is a polynomial in the entries of the matrix. Both are special cases of a more general function of a matrix called the immanant.\n\nThe permanent of an \"n\"-by-\"n\" matrix \"A\" = (\"a\") is defined as\n\nThe sum here extends over all elements σ of the symmetric group \"S\"; i.e. over all permutations of the numbers 1, 2, ..., \"n\".\n\nFor example,\n\nand\n\nThe definition of the permanent of \"A\" differs from that of the determinant of \"A\" in that the signatures of the permutations are not taken into account.\n\nThe permanent of a matrix A is denoted per \"A\", perm \"A\", or Per \"A\", sometimes with parentheses around the argument. In his monograph, uses Per(\"A\") for the permanent of rectangular matrices, and uses per(\"A\") when \"A\" is a square matrix. uses the notation formula_4\n\nThe permanent is believed to be more difficult to compute than the determinant. While the determinant can be computed in polynomial time by Gaussian elimination, Gaussian elimination cannot be used to compute the permanent. Moreover, computing the permanent of a (0,1)-matrix is #P-complete. Thus, if the permanent can be computed in polynomial time by any method, then FP = #P, which is an even stronger statement than P = NP. When the entries of \"A\" are nonnegative, however, the permanent can be computed approximately in probabilistic polynomial time, up to an error of formula_5, where formula_6 is the value of the permanent and formula_7 is arbitrary. The permanent of a certain set of positive semidefinite matrices can also be approximated in probabilistic polynomial time: the best achievable error of this approximation is formula_8 (formula_6 is again the value of the permanent).\n\nAnother way to view permanents is via multivariate generating functions. Let formula_10 be a square matrix of order \"n\". Consider the multivariate generating function:\nThe coefficient of formula_13 in formula_14 is perm(\"A\").\n\nAs a generalization, for any sequence of \"n\" non-negative integers, formula_15 define:\nMacMahon's Master Theorem relating permanents and determinants is:\nwhere \"I\" is the order \"n\" identity matrix and \"X\" is the diagonal matrix with diagonal formula_20\n\nThe permanent function can be generalized to apply to non-square matrices. Indeed, several authors make this the definition of a permanent and consider the restriction to square matrices a special case. Specifically, for an \"m\" × \"n\" matrix formula_10 with \"m\" ≤ \"n\", define\nwhere P(\"n\",\"m\") is the set of all \"m\"-permutations of the \"n\"-set {1,2...,n}.\n\nRyser's computational result for permanents also generalizes. If \"A\" is an \"m\" × \"n\" matrix with \"m\" ≤ \"n\", let formula_23 be obtained from \"A\" by deleting \"k\" columns, let formula_24 be the product of the row-sums of formula_23, and let formula_26 be the sum of the values of formula_24 over all possible formula_23. Then \n\nThe generalization of the definition of a permanent to non-square matrices allows the concept to be used in a more natural way in some applications. For instance:\n\nLet \"S\", \"S\", ..., \"S\" be subsets (not necessarily distinct) of an \"n\"-set with \"m\" ≤ \"n\". The incidence matrix of this collection of subsets is an \"m\" × \"n\" (0,1)-matrix \"A\". The number of systems of distinct representatives (SDR's) of this collection is perm(\"A\").\n\n\n\n\n"}
{"id": "2716293", "url": "https://en.wikipedia.org/wiki?curid=2716293", "title": "Primefree sequence", "text": "Primefree sequence\n\nIn mathematics, a primefree sequence is a sequence of integers that does not contain any prime numbers. More specifically, it usually means a sequence defined by the same recurrence relation as the Fibonacci numbers, but with different initial conditions causing all members of the sequence to be composite numbers that do not all have a common divisor. To put it algebraically, a sequence of this type is defined by an appropriate choice of two composite numbers \"a\" and \"a\", such that the greatest common divisor GCD(\"a\",\"a\") is equal to 1, and such that for \"n\" > 2 there are no primes in the sequence of numbers calculated from the formula\n\nPerhaps the best known primefree sequence is the one found by Herbert Wilf, with initial terms \n\nThe proof that every term of this sequence is composite relies on the periodicity of Fibonacci-like number sequences modulo the members of a finite set of primes. For each prime \"p\", the positions in the sequence where the numbers are divisible by \"p\" repeat in a periodic pattern, and different primes in the set have overlapping patterns that result in a covering set for the whole sequence.\n\nThe requirement that the initial terms of a primefree sequence be coprime is necessary for the question to be non-trivial. If we allow the initial terms to share a prime factor \"p\" (e.g., set \"a\" = \"xp\" and \"a\" = \"yp\" for some \"x\" and \"y\" both greater than 1), due to the distributive property of multiplication \"a\" = (\"x\" + \"y\")\"p\" and more generally all subsequent value in the sequence will be multiples of \"p\". In this case, all the numbers in the sequence will be composite, but for a trivial reason.\n\nThe order of the initial terms is also important. In Paul Hoffman's biography of Paul Erdős, \"The man who loved only numbers\", the Wilf sequence is cited but with the initial terms switched. The resulting sequence appears primefree for the first hundred terms or so, but term 138 is the 45-digit prime 439351292910452432574786963588089477522344721.\n\nSeveral other primefree sequences are known:\nThe sequence of this type with the smallest known initial terms has\n\n\n"}
{"id": "5442412", "url": "https://en.wikipedia.org/wiki?curid=5442412", "title": "Range criterion", "text": "Range criterion\n\nIn quantum mechanics, in particular quantum information, the Range criterion is a necessary condition that a state must satisfy in order to be separable. In other words, it is a \"separability criterion\".\n\nConsider a quantum mechanical system composed of \"n\" subsystems. The state space \"H\" of such a system is the tensor product of those of the subsystems, i.e. formula_1.\n\nFor simplicity we will assume throughout that all relevant state spaces are finite-dimensional.\n\nThe criterion reads as follows: If ρ is a separable mixed state acting on \"H\", then the range of ρ is spanned by a set of product vectors.\n\nIn general, if a matrix \"M\" is of the form formula_2, the range of \"M\", \"Ran(M)\", is contained in the linear span of formula_3. On the other hand, we can also show formula_4 lies in \"Ran(M)\", for all \"i\". Assume without loss of generality \"i = 1\". We can write\nformula_5, where \"T\" is Hermitian and positive semidefinite. There are two possibilities:\n\n1) \"span\"formula_6\"Ker(T)\". Clearly, in this case, formula_7 \"Ran(M)\".\n\n2) Notice 1) is true if and only if \"Ker(T)\"formula_8 \"span\"formula_9, where formula_10 denotes orthogonal complement. By Hermiticity of \"T\", this is the same as \"Ran(T)\"formula_11 \"span\"formula_9. So if 1) does not hold, the intersection \"Ran(T)\" formula_13 \"span\"formula_14 is nonempty, i.e. there exists some complex number α such that formula_15. So \n\nTherefore formula_17 lies in \"Ran(M)\".\n\nThus \"Ran(M)\" coincides with the linear span of formula_3. The range criterion is a special case of this fact.\n\nA density matrix ρ acting on \"H\" is separable if and only if it can be written as \n\nwhere formula_20 is a (un-normalized) pure state on the \"j\"-th subsystem. This is also\n\nBut this is exactly the same form as \"M\" from above, with the vectorial product state formula_22 replacing formula_4. It then immediately follows that the range of ρ is the linear span of these product states. This proves the criterion.\n\n"}
{"id": "2518933", "url": "https://en.wikipedia.org/wiki?curid=2518933", "title": "Star product", "text": "Star product\n\nIn mathematics, the star product is a method of combining graded posets with unique minimal and maximal elements, preserving the property that the posets are Eulerian.\n\nThe star product of two graded posets formula_1 and formula_2, where formula_3 has a unique maximal element formula_4 and formula_5 has a unique minimal element formula_6, is a poset formula_7 on the set formula_8. We define the partial order formula_9 by formula_10 if and only if:\n\nIn other words, we pluck out the top of formula_3 and the bottom of formula_5, and require that everything in formula_3 be smaller than everything in formula_5.\n\nFor example, suppose formula_3 and formula_5 are the Boolean algebra on two elements.\n\nThen formula_7 is the poset with the Hasse diagram below.\n\nThe star product of Eulerian posets is Eulerian.\n\n\n"}
{"id": "28870339", "url": "https://en.wikipedia.org/wiki?curid=28870339", "title": "Strength (mathematical logic)", "text": "Strength (mathematical logic)\n\nThe relative strength of two systems of formal logic can be defined via model theory. Specifically, a logic formula_1 is said to be as strong as a logic formula_2 if every elementary class in formula_2 is an elementary class in formula_1.\n\n\n \n"}
{"id": "684680", "url": "https://en.wikipedia.org/wiki?curid=684680", "title": "Strongly connected component", "text": "Strongly connected component\n\nIn the mathematical theory of directed graphs, a graph is said to be strongly connected or diconnected if every vertex is reachable from every other vertex. The strongly connected components or diconnected components of an arbitrary directed graph form a partition into subgraphs that are themselves strongly connected. It is possible to test the strong connectivity of a graph, or to find its strongly connected components, in linear time (that is, Θ(V+E)).\n\nA directed graph is called strongly connected if there is a path in each direction between each pair of vertices of the graph. That is, a path exists from the first vertex in the pair to the second, and another path exists from the second vertex to the first.\nIn a directed graph \"G\" that may not itself be strongly connected, a pair of vertices \"u\" and \"v\" are said to be strongly connected to each other if there is a path in each direction between them.\n\nThe binary relation of being strongly connected is an equivalence relation, and the induced subgraphs of its equivalence classes are called strongly connected components.\nEquivalently, a strongly connected component of a directed graph \"G\" is a subgraph that is strongly connected, and is maximal with this property: no additional edges or vertices from \"G\" can be included in the subgraph without breaking its property of being strongly connected. The collection of strongly connected components forms a partition of the set of vertices of \"G\".\nIf each strongly connected component is contracted to a single vertex, the resulting graph is a directed acyclic graph, the condensation of \"G\". A directed graph is acyclic if and only if it has no strongly connected subgraphs with more than one vertex, because a directed cycle is strongly connected and every nontrivial strongly connected component contains at least one directed cycle.\n\nSeveral algorithms based on depth first search compute strongly connected components in linear time.\nAlthough Kosaraju's algorithm is conceptually simple, Tarjan's and the path-based algorithm require only one depth-first search rather than two.\n\nPrevious linear-time algorithms are based on depth-first search which is generally considered hard to parallelize. Fleischer et al. in 2000 proposed a divide-and-conquer approach based on reachability queries, and such algorithms are usually called reachability-based SCC algorithms. The idea of this approach is to pick a random pivot vertex and apply forward and backward reachability queries from this vertex. The two queries partition the vertex set into 4 subsets: vertices reached by both, either one, or none of the searches. One can show that a strongly connected component has to be contained in one of the subsets. The vertex subset reached by both searches forms a strongly connected components, and the algorithm then recurses on the other 3 subsets.\n\nThe expected sequential running time of this algorithm is shown to be O(\"n\" log \"n\"), a factor of O(log \"n\") more than the classic algorithms. The parallelism comes from: (1) the reachability queries can be parallelized more easily (e.g. by a BFS, and it can be fast if the diameter of the graph is small); and (2) the independence between the subtasks in the divide-and-conquer process.\nThis algorithm performs well on real-world graphs, but does not have theoretical guarantee on the parallelism (consider if a graph has no edges, the algorithm requires O(\"n\") levels of recursions).\n\nBlelloch et al. in 2016 shows that if the reachability queries are applied in a random order, the cost bound of O(\"n\" log \"n\") still holds. Furthermore, the queries then can be batched in a prefix-doubling manner (i.e. 1, 2, 4, 8 queries) and run simultaneously in one round. The overall span of this algorithm is log \"n\" reachability queries, which is probably the optimal parallelism that can be achieved using the reachability-based approach.\n\nAlgorithms for finding strongly connected components may be used to solve 2-satisfiability problems (systems of Boolean variables with constraints on the values of pairs of variables): as showed, a 2-satisfiability instance is unsatisfiable if and only if there is a variable \"v\" such that \"v\" and its complement are both contained in the same strongly connected component of the implication graph of the instance.\n\nStrongly connected components are also used to compute the Dulmage–Mendelsohn decomposition, a classification of the edges of a bipartite graph, according to whether or not they can be part of a perfect matching in the graph.\n\nA directed graph is strongly connected if and only if it has an ear decomposition, a partition of the edges into a sequence of directed paths and cycles such that the first subgraph in the sequence is a cycle, and each subsequent subgraph is either a cycle sharing one vertex with previous subgraphs, or a path sharing its two endpoints with previous subgraphs.\n\nAccording to Robbins' theorem, an undirected graph may be oriented in such a way that it becomes strongly connected, if and only if it is 2-edge-connected. One way to prove this result is to find an ear decomposition of the underlying undirected graph and then orient each ear consistently.\n\n\n"}
{"id": "1482138", "url": "https://en.wikipedia.org/wiki?curid=1482138", "title": "Structured program theorem", "text": "Structured program theorem\n\nThe structured program theorem, also called Böhm-Jacopini theorem, is a result in programming language theory. It states that a class of control flow graphs (historically called charts in this context) can compute any computable function if it combines subprograms in only three specific ways (control structures). These are\n\nThe structured chart subject to these constraints may however use additional variables in the form of bits (stored in an extra integer variable in the original proof) in order to keep track of information that the original program represents by the program location. The construction was based on Böhm's programming language P′′.\n\nThe theorem is typically credited to a 1966 paper by Corrado Böhm and Giuseppe Jacopini. David Harel wrote in 1980 that the Böhm–Jacopini paper enjoyed \"universal popularity\", particularly with proponents of structured programming. Harel also noted that \"due to its rather technical style [the 1966 Böhm–Jacopini paper] is apparently more often cited than read in detail\" and, after reviewing a large number of papers published up to 1980, Harel argued that the contents of the Böhm–Jacopini proof were usually misrepresented as a folk theorem that essentially contains a simpler result, a result which itself can be traced to the inception of modern computing theory in the papers of von Neumann and Kleene.\n\nHarel also writes that the more generic name was proposed by H.D. Mills as \"The Structure Theorem\" in the early 1970s.\n\nThis version of the theorem replaces all the original program's control flow with a single global codice_1 loop that simulates a program counter going over all possible labels (flowchart boxes) in the original non-structured program. Harel traced the origin of this folk theorem to two papers marking the beginning of computing. One is the 1946 description of the von Neumann architecture, which explains how a program counter operates in terms of a while loop. Harel notes that the single loop used by the folk version of the structured programming theorem basically just provides operational semantics for the execution of a flowchart on a von Neumann computer. Another, even older source that Harel traced the folk version of the theorem is Stephen Kleene's normal form theorem from 1936.\n\nDonald Knuth criticized this form of the proof, which results in pseudocode like the one below, by pointing out that the structure of the original program is completely lost in this transformation. Similarly, Bruce Ian Mills wrote about this approach that \"The spirit of block structure is a style, not a language. By simulating a Von Neumann machine, we can produce the behavior of any spaghetti code within the confines of a block-structured language. This does not prevent it from being spaghetti.\"\n\nThe proof in Böhm and Jacopini's paper proceeds by induction on the structure of the flow chart. Because it employed pattern matching in graphs, the proof of Böhm and Jacopini's was not really practical as a program transformation algorithm, and thus opened the door for additional research in this direction.\n\nThe Böhm-Jacopini proof did not settle the question of whether to adopt structured programming for software development, partly because the construction was more likely to obscure a program than to improve it. On the contrary, it signalled the beginning of the debate. Edsger Dijkstra's famous letter, \"Go To Statement Considered Harmful,\" followed in 1968.\n\nSome academics took a purist approach to the Böhm-Jacopini result and argued that even instructions like codice_2 and codice_3 from the middle of loops are bad practice as they are not needed in the Böhm-Jacopini proof, and thus they advocated that all loops should have a single exit point. This purist approach is embodied in the Pascal programming language (designed in 1968–1969), which up to the mid-1990s was the preferred tool for teaching introductory programming classes in academia.\n\nEdward Yourdon notes that in the 1970s there was even philosophical opposition to transforming unstructured programs into structured ones by automated means, based on the argument that one needed to think in structured programming fashion from the get go. The pragmatic counterpoint was that such transformations benefited a large body of existing programs. Among the first proposals for an automated transformation was a 1971 paper by Edward Ashcroft and Zohar Manna.\n\nThe direct application of the Böhm-Jacopini theorem may result in additional local variables being introduced in the structured chart, and may also result in some code duplication. The latter issue is called the loop and a half problem in this context. Pascal is affected by both of these problems and according to empirical studies cited by Eric S. Roberts, student programmers had difficulty formulating correct solutions in Pascal for several simple problems, including writing a function for searching an element in an array. A 1980 study by Henry Shapiro cited by Roberts found that using only the Pascal-provided control structures, the correct solution was given by only 20% of the subjects, while no subject wrote incorrect code for this problem if allowed to write a return from the middle of a loop.\n\nIn 1973, S. Rao Kosaraju proved that it's possible to avoid adding additional variables in structured programming, as long as arbitrary-depth, multi-level breaks from loops are allowed. Furthermore, Kosaraju proved that a strict hierarchy of programs exists, nowadays called the \"Kosaraju hierarchy\", in that for every integer \"n\", there exists a program containing a multi-level break of depth \"n\" that cannot be rewritten as program with multi-level breaks of depth less than \"n\" (without introducing additional variables). Kosaraju cites the multi-level break construct to the BLISS programming language. The multi-level breaks, in the form a codice_4 keyword were actually introduced in the BLISS-11 version of that language; the original BLISS only had single-level breaks. The BLISS family of languages didn't provide an unrestricted goto. The Java programming language would later follow this approach as well.\n\nA simpler result from Kosaraju's paper is that a program is reducible to a structured program (without adding variables) if and only if it does not contain a loop with two distinct exits. Reducibility was defined by Kosaraju, loosely speaking, as computing the same function and using the same \"primitive actions\" and predicates as the original program, but possibly using different control flow structures. (This is a narrower notion of reducibility than what Böhm-Jacopini used.) Inspired by this result, in section VI of his highly-cited paper that introduced the notion of cyclomatic complexity, Thomas J. McCabe described an analogue of Kuratowski's theorem for the control flow graphs (CFG) of non-structured programs, which is to say, the minimal subgraphs that make the CFG of a program non-structured. These subgraphs have a very good description in natural language. They are:\nMcCabe actually found that these four graphs are not independent when appearing as subgraphs, meaning that a necessary and sufficient condition for a program to be non-structured is for its CFG to have as subgraph one of any subset of three of these four graphs. He also found that if a non-structured program contains one of these four sub-graphs, it must contain another distinct one from the set of four. This latter result helps explain how the control flow of non-structured program becomes entangled in what is popularly called \"spaghetti code\". McCabe also devised a numerical measure that, given an arbitrary program, quantifies how far off it is from the ideal of being a structured program; McCabe called his measure essential complexity.\n\nMcCabe's characterization of the forbidden graphs for structured programming can be considered incomplete, at least if the Dijkstra's D structures are considered the building blocks.\n\nUp to 1990 there were quite a few proposed methods for eliminating gotos from existing program, while preserving most of their structure. The various approaches to this problem also proposed several notions of equivalence, which are stricter than simply Turing equivalence, in order to avoid output like the folk theorem discussed above. The strictness of the chosen notion of equivalence dictates the minimal set of control flow structures needed. The 1988 JACM paper by Lyle Ramshaw surveys the field up to that point, as well proposing its own method. Ramshaw's algorithm was used for example in some Java decompilers because the Java virtual machine code has branch instructions with targets expressed as offsets, but the high-level Java language only has multi-level codice_2 and codice_6 statements. Ammarguellat (1992) proposed a transformation method that goes back to enforcing single-exit.\n\nIn the 1980s IBM researcher Harlan Mills oversaw the development of the COBOL Structuring Facility, which applied a structuring algorithm to COBOL code. Mills's transformation involved the following steps for each procedure.\n\n\nNote that this construction can be improved by converting some cases of the selection statement into subprocedures.\n\n\nMaterial not yet covered above:\n\n"}
{"id": "528867", "url": "https://en.wikipedia.org/wiki?curid=528867", "title": "Surface integral", "text": "Surface integral\n\nIn mathematics, a surface integral is a generalization of multiple integrals to integration over surfaces. It can be thought of as the double integral analog of the line integral. Given a surface, one may integrate over its scalar fields (that is, functions which return scalars as values), and vector fields (that is, functions which return vectors as values).\n\nSurface integrals have applications in physics, particularly with the theories of classical electromagnetism.\n\nTo find an explicit formula for the surface integral, we need to parameterize the surface of interest, \"S\", by considering a system of curvilinear coordinates on \"S\", like the latitude and longitude on a sphere. Let such a parameterization be x(\"s\", \"t\"), where (\"s\", \"t\") varies in some region \"T\" in the plane. Then, the surface integral is given by\n\nwhere the expression between bars on the right-hand side is the magnitude of the cross product of the partial derivatives of x(\"s\", \"t\"), and is known as the surface element. The surface integral can also be expressed in the equivalent form\n\nwhere \"g\" is the determinant of the first fundamental form of the surface mapping x(\"s\", \"t\").\n\nFor example, if we want to find the surface area of the graph of some scalar function, say formula_3, we have\nwhere formula_5. So that formula_6, and formula_7. So,\nwhich is the standard formula for the area of a surface described this way. One can recognize the vector in the second line above as the normal vector to the surface.\n\nNote that because of the presence of the cross product, the above formulas only work for surfaces embedded in three-dimensional space.\n\nThis can be seen as integrating a Riemannian volume form on the parameterized surface, where the metric tensor is given by the first fundamental form of the surface.\n\nConsider a vector field v on \"S\", that is, for each x in \"S\", v(x) is a vector.\n\nThe surface integral can be defined component-wise according to the definition of the surface integral of a scalar field; the result is a vector. This applies for example in the expression of the electric field at some fixed point due to an electrically charged surface, or the gravity at some fixed point due to a sheet of material.\n\nAlternatively, if we integrate the normal component of the vector field, the result is a scalar. Imagine that we have a fluid flowing through \"S\", such that v(x) determines the velocity of the fluid at x. The flux is defined as the quantity of fluid flowing through \"S\" per unit time.\n\nThis illustration implies that if the vector field is tangent to \"S\" at each point, then the flux is zero because the fluid just flows in parallel to \"S\", and neither in nor out. This also implies that if v does not just flow along \"S\", that is, if v has both a tangential and a normal component, then only the normal component contributes to the flux. Based on this reasoning, to find the flux, we need to take the dot product of v with the unit surface normal n to \"S\" at each point, which will give us a scalar field, and integrate the obtained field as above. We find the formula\n\nThe cross product on the right-hand side of this expression is a (not necessarily unital) surface normal determined by the parametrization.\n\nThis formula \"defines\" the integral on the left (note the dot and the vector notation for the surface element).\n\nWe may also interpret this as a special case of integrating 2-forms, where we identify the vector field with a 1-form, and then integrate its Hodge dual over the surface.\nThis is equivalent to integrating formula_10 over the immersed surface, where formula_11 is the induced volume form on the surface, obtained\nby interior multiplication of the Riemannian metric of the ambient space with the outward normal of the surface.\n\nLet\nbe a differential 2-form defined on the surface \"S\", and let\n\nbe an orientation preserving parametrization of \"S\" with formula_14 in \"D\". Changing coordinates from formula_15\nto formula_16, the differential forms transform as\n\nSo formula_19 transforms to formula_20, where formula_21 denotes the determinant of the Jacobian of the transition function from formula_16 to formula_23. The transformation of the other forms are similar.\n\nThen, the surface integral of \"f\" on \"S\" is given by\n\nwhere\nis the surface element normal to \"S\".\n\nLet us note that the surface integral of this 2-form is the same as the surface integral of the vector field which has as components formula_26, formula_27 and formula_28.\n\nVarious useful results for surface integrals can be derived using differential geometry and vector calculus, such as the divergence theorem, and its generalization, Stokes' theorem.\n\nLet us notice that we defined the surface integral by using a parametrization of the surface \"S\". We know that a given surface might have several parametrizations. For example, if we move the locations of the North Pole and the South Pole on a sphere, the latitude and longitude change for all the points on the sphere. A natural question is then whether the definition of the surface integral depends on the chosen parametrization. For integrals of scalar fields, the answer to this question is simple, the value of the surface integral will be the same no matter what parametrization one uses.\n\nFor integrals of vector fields, things are more complicated, because the surface normal is involved. It can be proven that given two parametrizations of the same surface, whose surface normals point in the same direction, one obtains the same value for the surface integral with both parametrizations. If, however, the normals for these parametrizations point in opposite directions, the value of the surface integral obtained using one parametrization is the negative of the one obtained via the other parametrization. It follows that given a surface, we do not need to stick to any unique parametrization; but, when integrating vector fields, we do need to decide in advance which direction the normal will point to and then choose any parametrization consistent with that direction.\n\nAnother issue is that sometimes surfaces do not have parametrizations which cover the whole surface. The obvious solution is then to split that surface into several pieces, calculate the surface integral on each piece, and then add them all up. This is indeed how things work, but when integrating vector fields, one needs to again be careful how to choose the normal-pointing vector for each piece of the surface, so that when the pieces are put back together, the results are consistent. For the cylinder, this means that if we decide that for the side region the normal will point out of the body, then for the top and bottom circular parts the normal must point out of the body too.\n\nLastly, there are surfaces which do not admit a surface normal at each point with consistent results (for example, the Möbius strip). If such a surface is split into pieces, on each piece a parametrization and corresponding surface normal is chosen, and the pieces are put back together, we will find that the normal vectors coming from different pieces cannot be reconciled. This means that at some junction between two pieces we will have normal vectors pointing in opposite directions. Such a surface is called non-orientable, and on this kind of surface, one cannot talk about integrating vector fields.\n\n\n"}
{"id": "168977", "url": "https://en.wikipedia.org/wiki?curid=168977", "title": "Trillian (character)", "text": "Trillian (character)\n\nTricia Marie McMillan, also known as Trillian Astra, is a fictional character from Douglas Adams' series \"The Hitchhiker's Guide to the Galaxy\". She is most commonly referred to simply as \"Trillian\", a modification of her birth name, which she adopted because it sounded more \"space-like\". According to the movie version, her middle name is Marie. Physically, she is described as \"a slim, darkish humanoid, with long waves of black hair, a full mouth, an odd little knob of a nose and ridiculously brown eyes,\" looking \"vaguely Arabic.\"\n\nTricia McMillan is a mathematician and astrophysicist whom Arthur Dent attempted to talk to at a party in Islington. She and Arthur next meet six months later on the spaceship Heart of Gold, shortly after the Earth has been destroyed to make way for a hyperspace bypass. The trilogy later reveals that Trillian eventually left the party with Zaphod Beeblebrox, who, according to the Quintessential Phase, is directly responsible for her nickname.\n\nIn the radio series, she is carried off and forcibly married to the President of the Algolian Chapter of the Galactic Rotary Club, and consequently does not appear in the second radio series at all. The later radio series (the Tertiary Phase and beyond) reveal this (probably) occurred only in the artificial universe within the Guide offices. In the books, which the third, fourth and fifth series follow, she saves the universe from the Krikketers, and later becomes a Sub-Etha Radio reporter under the name Trillian Astra.\n\nSome drafts of the movie's screenplay, and Robbie Stamp's \"making of\" book covering the movie, state that Trillian was to be revealed as half-human, an acknowledged divergence from Douglas Adams' original storyline. This would have been done in order to underline the loneliness of Arthur Dent, the only 100% Homo sapiens remaining in the universe, after Earth's demolition. This idea was scrapped after the \"making of\" book was written, and the scene revealing Trillian's heritage (by the mice, to Arthur, on the Earth Mark II) was re-written. An interview with actress Zooey Deschanel, included on the DVD version, has her mention that Trillian is half-human, suggesting the interview was recorded prior to the change of plan.\n\nThe movie puts a different spin on the character. The main emotional arc of the movie is a love triangle between Trillian, Zaphod, and Arthur. Trillian is initially attracted to Arthur when she meets him on Earth, but she's disappointed by his apparent lack of spontaneity. During their travels, Trillian discovers that Zaphod may be the more superficially exciting choice, but Arthur is the man who truly cares about her, Arthur commenting when he is about to have his head cut open by the mice that his feelings for Trillian are the only thing that he ever had questions about where the answer made him happy.\n\nIn the novels and radio series, Trillian does not have a romantic relationship with Arthur (although when Arthur starts seeing Fenchurch, Ford asks him what happened to Trillian). In the fifth book, Trillian is revealed as the mother of Random Dent. It is unclear for how long (if ever) Trillian had a relationship with Zaphod. They seem to travel away from each other after the third book, although in the fourth one Arthur states Trillian is with Zaphod, and in the fifth Trillian implies that she hadn't had a child with Zaphod simply because they're different species. In the sixth novel, \"And Another Thing...\", she pursues a relationship with Wowbagger, the Infinitely Prolonged; she accuses Arthur of carrying a torch for her as well.\n\nTrillian was played on radio by Susan Sheridan, on television by Sandra Dickinson (who also reprised an alternate-universe version of the role in the fifth and sixth radio series, playing both original and alternate-universe versions in the latter)), on the Original Records LP version by Cindy Oswin, and in the 2005 film by Zooey Deschanel. In \"The Illustrated Hitchhiker's Guide to the Galaxy\", she is portrayed by Tali, a model.\n\nIn the original radio series, she is portrayed with an English accent – in both the TV series and movie she is played as an American. The \"Quintessential Phase\" of the radio series features Sandra Dickinson in the role of the alternate version of Tricia McMillan as a \"blonder and more American\" Trillian – the radio series indicates that the character is otherwise identical to the first Trillian and was born in the United Kingdom. In the book \"Mostly Harmless\", it is said that both the alternate Tricia McMillan and Trillian have an English accent. In \"The Hitchhiker's Guide to the Galaxy\" novel, she is described as follows: \"She was slim, darkish, humanoid, with long waves of black hair, a full mouth, an odd little knob of a nose and ridiculously brown eyes. With her red head scarf knotted in that particular way and her long flowing silky brown dress, she looked vaguely Arabic.\" She has consistently not been portrayed as such in the television and film adaptions, although the film adaptation Trillian is closer to her appearance in the books than it was in the television series.\n\nTrillian comes closest of all female characters to appearing in the entire \"Hitchhiker's\" saga.\n\n\nThe Hitchhiker's Guide to the Galaxy\" radio series\n\nFeaturing Susan Sheridan as Trillian:\n\nFeaturing Sandra Dickinson as the alternate character Tricia McMillan:\n\nFeaturing Sandra Dickinson as Trillian and Tricia McMillan:\n\nFeaturing Cindy Oswin as Trillian\n\nFeaturing Sandra Dickinson as Trillian\n\n\nFeaturing Zooey Deschanel as Trillian\n"}
{"id": "2584449", "url": "https://en.wikipedia.org/wiki?curid=2584449", "title": "Two-element Boolean algebra", "text": "Two-element Boolean algebra\n\nIn mathematics and abstract algebra, the two-element Boolean algebra is the Boolean algebra whose \"underlying set\" (or universe or \"carrier\") \"B\" is the Boolean domain. The elements of the Boolean domain are 1 and 0 by convention, so that \"B\" = {0, 1}. Paul Halmos's name for this algebra \"2\" has some following in the literature, and will be employed here.\n\n\"B\" is a partially ordered set and the elements of \"B\" are also its bounds.\n\nAn operation of arity \"n\" is a mapping from \"B\" to \"B\". Boolean algebra consists of two binary operations and unary complementation. The binary operations have been named and notated in various ways. Here they are called 'sum' and 'product', and notated by infix '+' and '∙', respectively. Sum and product commute and associate, as in the usual algebra of real numbers. As for the order of operations, brackets are decisive if present. Otherwise '∙' precedes '+'. Hence \"A∙B + C\" is parsed as \"(A∙B) + C\" and not as \"A∙(B + C)\". Complementation is denoted by writing an overbar over its argument. The numerical analog of the complement of \"X\" is 1 − \"X\". In the language of universal algebra, a Boolean algebra is a formula_1 algebra of type formula_2.\n\nEither one-to-one correspondence between {0,1} and {\"True\",\"False\"} yields classical bivalent logic in equational form, with complementation read as NOT. If 1 is read as \"True\", '+' is read as OR, and '∙' as AND, and vice versa if 1 is read as \"False\". These two operations define a commutative semiring, known as the Boolean semiring.\n\n2 can be seen as grounded in the following trivial \"Boolean\" arithmetic:\n\nNote that:\nThis Boolean arithmetic suffices to verify any equation of 2, including the axioms, by examining every possible assignment of 0s and 1s to each variable (see decision procedure).\n\nThe following equations may now be verified:\n\nEach of '+' and '∙' distributes over the other:\nThat '∙' distributes over '+' agrees with elementary algebra, but not '+' over '∙'. For this and other reasons, a sum of products (leading to a NAND synthesis) is more commonly employed than a product of sums (leading to a NOR synthesis).\n\nEach of '+' and '∙' can be defined in terms of the other and complementation:\nWe only need one binary operation, and concatenation suffices to denote it. Hence concatenation and overbar suffice to notate 2. This notation is also that of Quine's Boolean term schemata. Letting (\"X\") denote the complement of \"X\" and \"()\" denote either 0 or 1 yields the syntax of the primary algebra.\n\nA \"basis\" for 2 is a set of equations, called axioms, from which all of the above equations (and more) can be derived. There are many known bases for all Boolean algebras and hence for 2. An elegant basis notated using only concatenation and overbar is:\n\nWhere concatenation = OR, 1 = true, and 0 = false, or concatenation = AND, 1 = false, and 0 = true. (overbar is negation in both cases.)\n\nIf 0=1, (1)-(3) are the axioms for an abelian group.\n\n(1) only serves to prove that concatenation commutes and associates. First assume that (1) associates from either the left or the right, then prove commutativity. Then prove association from the other direction. Associativity is simply association from the left and right combined.\n\nThis basis makes for an easy approach to proof, called calculation, that proceeds by simplifying expressions to 0 or 1, by invoking axioms (2)–(4), and the elementary identities formula_13, and the distributive law.\n\nDe Morgan's theorem states that if one does the following, in the given order, to any Boolean function:\nthe result is logically equivalent to what you started with. Repeated application of De Morgan's theorem to parts of a function can be used to drive all complements down to the individual variables.\n\nA powerful and nontrivial metatheorem states that any theorem of 2 holds for all Boolean algebras. Conversely, an identity that holds for an arbitrary nontrivial Boolean algebra also holds in 2. Hence all the mathematical content of Boolean algebra is captured by 2. This theorem is useful because any equation in 2 can be verified by a decision procedure. Logicians refer to this fact as \"2 is decidable\". All known decision procedures require a number of steps that is an exponential function of the number of variables \"N\" appearing in the equation to be verified. Whether there exists a decision procedure whose steps are a polynomial function of \"N\" falls under the P = NP conjecture.\n\nMany elementary texts on Boolean algebra were published in the early years of the computer era. Perhaps the best of the lot, and one still in print, is:\n\nThe following items reveal how the two-element Boolean algebra is mathematically nontrivial.\n"}
{"id": "48710592", "url": "https://en.wikipedia.org/wiki?curid=48710592", "title": "UK National Quantum Technologies Programme", "text": "UK National Quantum Technologies Programme\n\nThe UK National Quantum Technologies Programme (UKNQTP) is a programme set up by the UK government to translate academic work on quantum mechanics, and the effects of quantum superposition and quantum entanglement into new products and services. It brings UK physicists and engineers together with companies and entrepreneurs who have an interest in commercialising the technology.\n\nThe \"second quantum revolution\", or \"quantum 2.0\" is a term that is often used to describe quantum technologies based on superposition and entanglement. Originally described in a 1997 book by Gerard J. Milburn, which was then followed by a 2003 article by Jonathan P. Dowling and Gerard J. Milburn, as well as a 2003 article by David Deutsch. These technologies use equipment such as highly stabilised laser systems, magneto-optical traps, cryogenic cooled solid state devices, ion traps and vacuum systems to create, manipulate and then use quantum effects for a number of different purposes. These include: quantum information processing, such as quantum computing, quantum simulation, quantum secure communications, quantum sensing and metrology and quantum imaging, and are widely believed to offer capabilities that will out-perform existing and future classical technologies.\n\nThe vision of the UK National Quantum Technologies programme is to \"create a coherent government, industry and academic quantum technology community that gives the UK a world-leading position in the emerging multi-billion-pound new quantum technology markets, and to substantially enhance the value of some of the biggest UK-based industries.\"\n\nThe UKNQTP was initiated by a £270 million investment by the UK Chancellor of the exchequer, George Osborne in the Autumn Statement 2013. In addition to this, the UK Defence Science and Technology Laboratory (Dstl) separately announced a £30 million investment into a programme to produce demonstrator devices.\n\nThe primary focus of the UKNQTP are four 'hubs' for quantum technologies:\n\nThe UKQTP is advised by the Quantum Technologies Strategic Advisory Board, which is chaired by Professor David Delpy, it also consists of Professor Sir Peter Knight, Baroness Neville-Jones, Professor Gerald Milburn, Professor Ian Walmsley and other leading individuals from industry, academia and public sector.\n\nThe programme is delivered by several UK public bodies: UK government Department for Business, Innovation and Skills (BIS), EPSRC, Innovate UK, Dstl, NPL, CESG and the Knowledge Transfer Network.\n\nThe UKQTP has received some attention from the UK media, with an interview with Professor Miles Padgett on the BBC Radio 4 Today programme on 11 November 2015 and articles in the \"Daily Mail\", \"New Scientist\", and Nature materials\n\n"}
{"id": "321962", "url": "https://en.wikipedia.org/wiki?curid=321962", "title": "Woodall number", "text": "Woodall number\n\nIn number theory, a Woodall number (W) is any natural number of the form\n\nfor some natural number \"n\". The first few Woodall numbers are:\n\nWoodall numbers were first studied by Allan J. C. Cunningham and H. J. Woodall in 1917, inspired by James Cullen's earlier study of the similarly-defined Cullen numbers.\n\nWoodall numbers that are also prime numbers are called Woodall primes; the first few exponents \"n\" for which the corresponding Woodall numbers \"W\" are prime are 2, 3, 6, 30, 75, 81, 115, 123, 249, 362, 384, … ; the Woodall primes themselves begin with 7, 23, 383, 32212254719, … .\n\nIn 1976 Christopher Hooley showed that almost all Cullen numbers are composite. Hooley's proof was reworked by Hiromi Suyama to show that it works for any sequence of numbers \"n\" · 2 + \"b\" where \"a\" and \"b\" are integers, and in particular also for Woodall numbers. Nonetheless, it is conjectured that there are infinitely many Woodall primes. , the largest known Woodall prime is 17016602 × 2 − 1. It has 5,122,515 digits and was found by Diego Bertolotti in March 2018 in the distributed computing project PrimeGrid.\n\nStarting with W = 63 and W = 159, every sixth Woodall number is divisible by 3; thus, in order for W to be prime, the index n cannot be congruent to 4 or 5 (modulo 6). Also, for a positive integer m, the Woodall number W may be prime only if 2 + m is prime. \n\nLike Cullen numbers, Woodall numbers have many divisibility properties. For example, if \"p\" is a prime number, then \"p\" divides\n\nA generalized Woodall number base \"b\" is defined to be a number of the form \"n\" × \"b\" − 1, where \"n\" + 2 > \"b\"; if a prime can be written in this form, it is then called a generalized Woodall prime.\n\nLeast \"n\" such that \"n\" × \"b\" - 1 is prime are\n\n, the largest known generalized Woodall prime is 17016602×2 − 1.\n\n\n\n"}
