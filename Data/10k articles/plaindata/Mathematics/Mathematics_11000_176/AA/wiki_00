{"id": "410563", "url": "https://en.wikipedia.org/wiki?curid=410563", "title": "120 (number)", "text": "120 (number)\n\n120, read as one hundred [and] twenty, is the natural number following 119 and preceding 121. \n\nIn the Germanic languages, the number 120 was also formerly known as \"one hundred\". This \"hundred\" of six score is now obsolete, but is described as the long hundred or great hundred in historical contexts.\n\n120 is the factorial of 5 and one less than a square, making (5, 11) a Brown number pair. 120 is the sum of a twin prime pair (59 + 61) and the sum of four consecutive prime numbers (23 + 29 + 31 + 37), four consecutive powers of 2 (8 + 16 + 32 + 64), and four consecutive powers of 3 (3 + 9 + 27 + 81). It is highly composite, superabundant, and colossally abundant number, with its 16 divisors being more than any number lower than it has, and it is also the smallest number to have exactly that many divisors. It is also a sparsely totient number. 120 is the smallest number to appear six times in Pascal's triangle. 120 is also the smallest multiple of 6 with no adjacent prime number, being adjacent to 119 = 7 × 17 and 121 = 11.\n\nIt is the eighth hexagonal number and the fifteenth triangular number, as well as the sum of the first eight triangular numbers, making it also a tetrahedral number. 120 is divisible by the first 5 triangular numbers and the first 4 tetrahedral numbers.\n\n120 is the first multiply perfect number of order three (\"a 3-perfect\" or \"triperfect number\"). The sum of its factors (including one and itself) sum to 360; exactly three times 120. Note that perfect numbers are order two (\"2-perfect\") by the same definition.\n\n120 is divisible by the number of primes below it, 30 in this case. However, there is no integer which has 120 as the sum of its proper divisors, making 120 an untouchable number.\n\nThe sum of Euler's totient function φ(\"x\") over the first nineteen integers is 120.\n\n120 figures in Pierre de Fermat's modified Diophantine problem as the largest known integer of the sequence 1, 3, 8, 120. Fermat wanted to find another positive integer that multiplied with any of the other numbers in the sequence yields a number that is one less than a square. Leonhard Euler also searched for this number, but failed to find it, but did find a fractional number that meets the other conditions, .\n\nThe internal angles of a regular hexagon (one where all sides and all angles are equal) are all 120 degrees.\n\n120 is a Harshad number in base 10.\n\n120 is the atomic number of Unbinilium, an element yet to be discovered.\n\n\n\n120 is also:\n\n\n"}
{"id": "399091", "url": "https://en.wikipedia.org/wiki?curid=399091", "title": "46 (number)", "text": "46 (number)\n\n46 (forty-six) is the natural number following 45 and preceding 47.\n\nForty-six is a Wedderburn-Etherington number, an enneagonal number and a centered triangular number. It is the sum of the totient function for the first twelve integers. 46 is the largest even integer that cannot be expressed as a sum of two abundant numbers. It is also the sixteenth semiprime.\n\nSince it is possible to find sequences of 46+1 consecutive integers such that each inner member shares a factor with either the first or the last member, 46 is an Erdős–Woods number.\n\n\n\n\n\nForty-six is also:\n"}
{"id": "391874", "url": "https://en.wikipedia.org/wiki?curid=391874", "title": "81 (number)", "text": "81 (number)\n\n81 (eighty-one) is the natural number following 80 and preceding 82.\n\n81 is:\n\n\nThe inverse of 81 is 0. recurring, missing only the digit \"8\" from the complete set of digits. This is an example of the general rule that, in base \"b\",\nomitting only the digit \"b\"−2.\n\n\nEighty-one is also:\n\nThe Arabic characters for the numerals 8 and 1 are visible in the left palm of the human hand.\nIn China, 81 always reminds people People's Liberation Army as it was founded on August 1.\n81 is used to refer to the motor-club Hell's Angels, since H and A are, respectively, the 8th and 1st letters of the alphabet.\n\n"}
{"id": "2423078", "url": "https://en.wikipedia.org/wiki?curid=2423078", "title": "Admissible numbering", "text": "Admissible numbering\n\nIn computability theory, admissible numberings are enumerations (numberings) of the set of partial computable functions that can be converted \"to and from\" the standard numbering. These numberings are also called acceptable numberings and acceptable programming systems.\n\nRogers' equivalence theorem shows that all acceptable programming systems are equivalent to each other in the formal sense of numbering theory. \n\nThe formalization of computability theory by Kleene led to a particular universal partial computable function Ψ(\"e\", \"x\") defined using the T predicate. This function is universal in the sense that it is partial computable, and for any partial computable function \"f\" there is an \"e\" such that, for all \"x\", \"f\"(\"x\") = Ψ(\"e\",\"x\"), where the equality means that either both sides are undefined or both are defined and are equal. It is common to write ψ(\"x\") for Ψ(\"e\",\"x\"); thus the sequence ψ, ψ, ... is an enumeration of all partial computable functions. Such enumerations are formally called computable numberings of the partial computable functions.\n\nAn arbitrary numbering η of partial functions is defined to be an admissible numbering if:\nHere, the first bullet requires the numbering to be computable; the second requires that any index for the numbering η can be converted effectively to an index to the numbering ψ; and the third requires that any index for the numbering ψ can be effectively converted to an index for the numbering η.\n\nHartley Rogers, Jr. showed that a numbering η of the partial computable functions is admissible if and only if there is a total computable bijection \"p\" such that, for all η = ψ (Soare 1987:25).\n\n\n"}
{"id": "3785849", "url": "https://en.wikipedia.org/wiki?curid=3785849", "title": "Apotome (mathematics)", "text": "Apotome (mathematics)\n\nIn the historical study of mathematics, an apotome is a line segment formed from a longer line segment by breaking it into two parts, one of which is commensurable only in power to the whole; the other part is the apotome. In this definition, two line segments are said to be \"commensurable only in power\" when the ratio of their lengths is an irrational number but the ratio of their squared lengths is rational.\n\nTranslated into modern algebraic language, an apotome can be interpreted as a quadratic irrational number formed by subtracting one square root of a rational number from another.\nThis concept of the apotome appears in Euclid's Elements beginning in book X, where Euclid defines two special kinds of apotomes. In an apotome of the first kind, the whole is rational, while in an apotome of the second kind, the part subtracted from it is rational; both kinds of apotomes also satisfy an additional condition. Euclid Proposition XIII.6 states that, if a rational line segment is split into two pieces in the golden ratio, then both pieces may be represented as apotomes.\n"}
{"id": "42063759", "url": "https://en.wikipedia.org/wiki?curid=42063759", "title": "Broer–Kaup equations", "text": "Broer–Kaup equations\n\nThe Broer–Kaup equations are a set of two coupled nonlinear partial differential equations\n\n"}
{"id": "175560", "url": "https://en.wikipedia.org/wiki?curid=175560", "title": "Ciphertext", "text": "Ciphertext\n\nIn cryptography, ciphertext or cyphertext is the result of encryption performed on plaintext using an algorithm, called a cipher. Ciphertext is also known as encrypted or encoded information because it contains a form of the original plaintext that is unreadable by a human or computer without the proper cipher to decrypt it. Decryption, the inverse of encryption, is the process of turning ciphertext into readable plaintext. Ciphertext is not to be confused with codetext because the latter is a result of a code, not a cipher.\n\nLet formula_1 be the plaintext message that Alice wants to secretly transmit to Bob and let formula_2 be the encryption cipher, where formula_3 is a cryptographic key. Alice must first transform the plaintext into ciphertext, formula_4, in order to securely send the message to Bob, as follows:\n\nIn a symmetric-key system, Bob knows Alice's encryption key. Once the message is encrypted, Alice can safely transmit it to Bob (assuming no one else knows the key). In order to read Alice's message, Bob must decrypt the ciphertext using formula_6 which is known as the decryption cipher, formula_7\n\nAlternatively, in a non-symmetric key system, everyone, not just Alice and Bob, knows the encryption key; but the decryption key cannot be inferred from the encryption key. Only Bob knows the decryption key formula_9 and decryption proceeds as \n\nThe history of cryptography began thousands of years ago. Cryptography uses a variety of different types of encryption. Earlier algorithms were performed by hand and are substantially different from modern algorithms, which are generally executed by a machine.\n\nHistorical pen and paper ciphers used in the past are sometimes known as classical ciphers. They include:\n\n\nHistorical ciphers are not generally used as a standalone encryption technique because they are quite easy to crack. Many of the classical ciphers, with the exception of the one-time pad, can be cracked using brute force.\n\nModern ciphers are more secure than classical ciphers and are designed to withstand a wide range of attacks. An attacker should not be able to find the key used in a modern cipher, even if he knows any amount of plaintext and corresponding ciphertext. Modern encryption methods can be divided into the following categories:\n\n\nIn a symmetric key algorithm (e.g., DES and AES), the sender and receiver must have a shared key set up in advance and kept secret from all other parties; the sender uses this key for encryption, and the receiver uses the same key for decryption. In an asymmetric key algorithm (e.g., RSA), there are two separate keys: a \"public key\" is published and enables any sender to perform encryption, while a \"private key\" is kept secret by the receiver and enables only him to perform correct decryption.\n\nSymmetric key ciphers can be divided into block ciphers and stream ciphers. Block ciphers operate on fixed-length groups of bits, called blocks, with an unvarying transformation. Stream ciphers encrypt plaintext digits one at a time on a continuous stream of data and the transformation of successive digits varies during the encryption process.\n\nCryptanalysis is the study of methods for obtaining the meaning of encrypted information, without access to the secret information that is normally required to do so. Typically, this involves knowing how the system works and finding a secret key. Cryptanalysis is also referred to as codebreaking or cracking the code. Ciphertext is generally the easiest part of a cryptosystem to obtain and therefore is an important part of cryptanalysis. Depending on what information is available and what type of cipher is being analyzed, crypanalysts can follow one or more attack models to crack a cipher.\n\n\nThe ciphertext-only attack model is the weakest because it implies that the cryptanalyst has nothing but ciphertext. Modern ciphers rarely fail under this attack.\n\n\n\n"}
{"id": "47037875", "url": "https://en.wikipedia.org/wiki?curid=47037875", "title": "Cirquent calculus", "text": "Cirquent calculus\n\nCirquent calculus is a proof calculus which manipulates graph-style constructs termed \"cirquents\", as opposed to the traditional tree-style objects such as formulas or sequents. Cirquents come in a variety of forms, but they all share one main characteristic feature, making them different from the more traditional objects of syntactic manipulation. This feature is the ability to explicitly account for possible sharing of subcomponents between different components. For instance, it is possible to write an expression where two subexpressions \"F\" and \"E\", while neither one is a subexpression of the other, still have a common occurrence of a subexpression \"G\" (as opposed to having two different occurrences of \"G\", one in \"F\" and one in \"E\"). \n\nAmong the later-found applications of cirquent calculus was the use of it to define a semantics for purely propositional independence-friendly logic. The corresponding logic was axiomatized by W. Xu. \n\nSyntactically, cirquent calculi are deep inference systems with the unique feature of subformula-sharing. This feature has been shown to provide speedup for certain proofs. For instance, polynomial size analytic proofs for the propositional pigeonhole have been constructed. Only quasipolynomial analytic proofs have been found for this principle in other deep inference systems. In resolution or analytic Gentzen-style systems, the pigeonhole principle is known to have only exponential size proofs.\nIssue 1, Paper 12, pp. 1–16.\n"}
{"id": "492445", "url": "https://en.wikipedia.org/wiki?curid=492445", "title": "Compass (drawing tool)", "text": "Compass (drawing tool)\n\nA pair of compasses, also known simply as a bow compass, is a technical drawing instrument that can be used for inscribing circles or arcs. As dividers, they can also be used as tools to measure distances, in particular on maps. Compasses can be used for mathematics, drafting, navigation and other purposes.\n\nCompasses are usually made of metal or plastic, and consist of two parts connected by a hinge which can be adjusted to allow the changing of the radius of the circle drawn. Typically one part has a spike at its end, and the other part a pencil, or sometimes a pen.\n\nPrior to computerization, compasses and other tools for manual drafting were often packaged as a \"bow set\" with interchangeable parts. By the mid–twentieth century, circle templates supplemented the use of compasses. Today these facilities are more often provided by computer-aided design programs, so the physical tools serve mainly a didactic purpose in teaching geometry, technical drawing, etc.\n\nCompasses are usually made of metal or plastic, and consist of two parts connected by a hinge which can be adjusted to allow the changing of the radius of the circle drawn. Typically one part has a spike at its end, and the other part a pencil, or sometimes a pen. \n\nThe handle is usually about half an inch long. Users can grip it between their pointer finger and thumb.\n\nThere are two types of legs in a pair of compasses: the straight or the steady leg and the adjustable one. Each has a separate purpose; the steady leg serves as the basis or support for the needle point, while the adjustable leg can be altered in order to draw different sizes of circles.\n\nThe screw on your hinge holds the two legs in its position; the hinge can be adjusted depending on desired stiffness. The tighter the screw, the better the compass’ performance.\n\nThe needle point is located on the steady leg, and serves as the center point of circles that are drawn.\n\nThe pencil lead draws the circle on a particular paper or material. Alternatively, an ink nib or attachment with a technical pen may be used.\n\nThis holds the pencil lead or pen in place.\n\nCircles can be made by fastening one leg of the compasses into the paper with the spike, putting the pencil on the paper, and moving the pencil around while keeping the hinge on the same angle. The radius of the circle can be adjusted by changing the angle of the hinge.\n\nDistances can be measured on a map using compasses with two spikes, also called a dividing compass. The hinge is set in such a way that the distance between the spikes on the map represents a certain distance in reality, and by measuring how many times the compasses fit between two points on the map the distance between those points can be calculated.\n\nCompasses-and-straightedge constructions are used to illustrate principles of plane geometry. Although a real pair of compasses is used to draft visible illustrations, the ideal compass used in proofs is an abstract creator of perfect circles. The most rigorous definition of this abstract tool is the \"collapsing compass\"; having drawn a circle from a given point with a given radius, it disappears; it cannot simply be moved to another point and used to draw another circle of equal radius (unlike a real pair of compasses). Euclid showed in his second proposition (Book I of the \"Elements\") that such a collapsing compass could be used to transfer a distance, proving that a collapsing compass could do anything a real compass can do.\n\nA beam compass is an instrument with a wooden or brass beam and sliding sockets, or cursors, for drawing and dividing circles larger than those made by a regular pair of compasses.\n\nScribe-compasses is an instrument used by carpenters and other tradesmen. Some compasses can be used to scribe circles, bisect angles and in this case to trace a line. It is the compass in the most simple form. Both branches are crimped metal. One branch has a pencil sleeve while the other branch is crimped with a fine point protruding from the end. A wing nut on the hinge serves two purposes: first it tightens the pencil and secondly it locks in the desired distance when the wing nut is turned clockwise.\n\nLoose leg wing dividers are made of all forged steel. The pencil holder, thumb screws, brass pivot and branches are all well built. They are used for scribing circles and stepping off repetitive measurements with some accuracy.\n\nA proportional compass, also known as a military compass or sector, was an instrument used for calculation from the end of the sixteenth century until the nineteenth century. It consists of two rulers of equal length joined by a hinge. Different types of scales are inscribed on the rulers that allow for mathematical calculation.\n\nA reduction compass is used to reduce or enlarge patterns while conserving angles.\nA pair of compasses is often used as a symbol of precision and discernment. As such it finds a place in logos and symbols such as the Freemasons' Square and Compasses and in various computer icons. English poet John Donne used the compass as a conceit in \"\" (1611).\n\n\n"}
{"id": "5205299", "url": "https://en.wikipedia.org/wiki?curid=5205299", "title": "Conjugate coding", "text": "Conjugate coding\n\nConjugate coding is a cryptographic tool, introduced by Stephen Wiesner in the late 1960s. It is part of the two applications Wiesner described for quantum coding, along with a method for creating fraud-proof banking notes. The application where the concept was based from was a method of transmitting multiple messages in such a way that reading one destroys the others. This is called quantum multiplexing and it uses photons polarized in conjugate bases as \"qubits\" to pass information. Conjugate coding also is a simple extension of the random number generator. \n\nAt the behest of Charles Bennett, Wiesner published the manuscript explaining the basic idea of conjugate coding with a number of examples but it was not embraced because it was significantly ahead of its time. Because its publication has been rejected, it was developed to the world of public-key cryptography in the 1980s as Oblivious Transfer, first by Michael Rabin and then by Shimon Even. It is used in the field of quantum computing. The initial concept of quantum cryptography developed by Bennett and Gilles Brassard was also based on this concept. \n"}
{"id": "10986233", "url": "https://en.wikipedia.org/wiki?curid=10986233", "title": "Constructive Approximation", "text": "Constructive Approximation\n\nConstructive Approximation is \"an international mathematics journal dedicated to Approximations and Expansions and related research in computation, function theory, functional analysis, interpolation spaces and interpolation of operators, numerical analysis, space of functions, special functions, and applications.\"\n\n"}
{"id": "25454823", "url": "https://en.wikipedia.org/wiki?curid=25454823", "title": "Constructive non-standard analysis", "text": "Constructive non-standard analysis\n\nIn mathematics, constructive nonstandard analysis is a version of Abraham Robinson's non-standard analysis, developed by Moerdijk (1995), Palmgren (1998), Ruokolainen (2004). Ruokolainen wrote: \n\n\n"}
{"id": "14785295", "url": "https://en.wikipedia.org/wiki?curid=14785295", "title": "David Aldous", "text": "David Aldous\n\nDavid John Aldous, FRS (born 13 July 1952) is a mathematician known for his research on probability theory and its applications, in particular in topics such as exchangeability, weak convergence, Markov chain mixing times, the continuum random tree and stochastic coalescence. He entered St. John's College, Cambridge, in 1970 and received his Ph.D. at the University of Cambridge in 1977 under his advisor, D. J. H. Garling. Since 1979 Aldous has been on the faculty at University of California, Berkeley.\n\nHe was awarded the Rollo Davidson Prize in 1980, the Loève Prize in 1993, and was elected a Fellow of the Royal Society in 1994. In 2004, Aldous was elected a Fellow of the American Academy of Arts and Sciences. In 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "62844", "url": "https://en.wikipedia.org/wiki?curid=62844", "title": "Density matrix", "text": "Density matrix\n\nA density matrix is a matrix that describes the statistical state of a system in quantum mechanics. The density matrix is especially helpful for dealing with \"mixed states\", which consist of a statistical ensemble of several different quantum systems. The opposite of a mixed state is a pure state. State vectors, also called kets, describe only pure states, whereas a density matrix can describe both pure and mixed states.\n\nDescribing a quantum state by its density matrix is a fully general alternative formalism to describing a quantum state by its ket (state vector) or by its statistical ensemble of kets. However, in practice, it is often most convenient to use density matrices for calculations involving mixed states, and to use kets for calculations involving only pure states.\n\nThe density matrix is the quantum-mechanical analogue to a phase-space probability measure (probability distribution of position and momentum) in classical statistical mechanics.\n\nMixed states arise in situations where the experimenter does not know which particular states are being manipulated. Examples include a system in thermal equilibrium at a temperature above absolute zero, or a system with an uncertain or randomly varying preparation history (so one does not know which pure state the system is in). Also, if a quantum system has two or more subsystems that are entangled, then each subsystem must be treated as a mixed state even if the complete system is in a pure state. The density matrix is also a crucial tool in quantum decoherence theory.\n\nThe density matrix is a representation of a linear operator called the density operator. The density matrix is obtained from the density operator by choice of basis in the underlying space. In practice, the terms \"density matrix\" and \"density operator\" are often used interchangeably. Both matrix and operator are self-adjoint (or Hermitian), positive semi-definite, of trace one, and may\nbe infinite-dimensional.\n\nThe formalism of density operators and matrices was introduced by John von Neumann in 1927 and independently, but less systematically by Lev Landau and Felix Bloch in 1927 and 1946 respectively.\n\nIn quantum mechanics, the state of a quantum system is represented by a state vector, denoted formula_1 (and pronounced \"ket\"). A quantum system with a state vector formula_1 is called a \"pure state\". However, it is also possible for a system to be in a statistical ensemble of different state vectors: For example, there may be a 50% probability that the state vector is formula_3 and a 50% chance that the state vector is formula_4. This system would be in a \"mixed state\". The density matrix is especially useful for mixed states, because any state, pure or mixed, can be characterized by a single density matrix.\n\nA mixed state is different from a quantum superposition. The probabilities in a mixed state are classical probabilities (as in the probabilities one learns in classic probability theory / statistics), unlike the quantum probabilities in a quantum superposition. In fact, a quantum superposition of pure states is another pure state, for example formula_5. In this case, the coefficients formula_6 are not probabilities, but rather probability amplitudes.\n\nAn example of pure and mixed states is light polarization. Photons can have two helicities, corresponding to two orthogonal quantum states, formula_7 (right circular polarization) and formula_8 (left circular polarization). A photon can also be in a superposition state, such as formula_9 (vertical polarization) or formula_10 (horizontal polarization). More generally, it can be in any state formula_11 (with formula_12), corresponding to linear, circular, or elliptical polarization. If we pass formula_9 polarized light through a circular polarizer which allows either only formula_7 polarized light, or only formula_8 polarized light, intensity would be reduced by half in both cases. This may make it \"seem\" like half of the photons are in state formula_7 and the other half in state formula_8. But this is not correct: Both formula_7 and formula_8 photons are partly absorbed by a vertical linear polarizer, but the formula_9 light will pass through that polarizer with no absorption whatsoever.\n\nHowever, unpolarized light (such as the light from an incandescent light bulb) is different from any state like formula_11 (linear, circular, or elliptical polarization). Unlike linearly or elliptically polarized light, it passes through a polarizer with 50% intensity loss whatever the orientation of the polarizer; and unlike circularly polarized light, it cannot be made linearly polarized with any wave plate because randomly oriented polarization will emerge from a wave plate with random orientation. Indeed, unpolarized light cannot be described as \"any\" state of the form formula_11 in a definite sense. However, unpolarized light \"can\" be described with ensemble averages, e.g. that each photon is either formula_23 with 50% probability or formula_24 with 50% probability. The same behavior would occur if each photon was either vertically polarized with 50% probability or horizontally polarized with 50% probability.\n\nTherefore, unpolarized light cannot be described by any pure state, but can be described as a statistical ensemble of pure states in at least two ways (the ensemble of half left and half right circularly polarized, or the ensemble of half vertically and half horizontally linearly polarized). These two ensembles are completely indistinguishable experimentally, and therefore they are considered the same mixed state. One of the advantages of the density matrix is that there is just one density matrix for each mixed state, whereas there are many statistical ensembles of pure states for each mixed state. Nevertheless, the density matrix contains all the information necessary to calculate any measurable property of the mixed state.\n\nWhere do mixed states come from? To answer that, consider how to generate unpolarized light. One way is to use a system in thermal equilibrium, a statistical mixture of enormous numbers of microstates, each with a certain probability (the Boltzmann factor), switching rapidly from one to the next due to thermal fluctuations. Thermal randomness explains why an incandescent light bulb, for example, emits unpolarized light. A second way to generate unpolarized light is to introduce uncertainty in the preparation of the system, for example, passing it through a birefringent crystal with a rough surface, so that slightly different parts of the beam acquire different polarizations. A third way to generate unpolarized light uses an EPR setup: A radioactive decay can emit two photons traveling in opposite directions, in the quantum state formula_25. The two photons \"together\" are in a pure state, but if you only look at one of the photons and ignore the other, the photon behaves just like unpolarized light.\n\nMore generally, mixed states commonly arise from a statistical mixture of the starting state (such as in thermal equilibrium), from uncertainty in the preparation procedure (such as slightly different paths that a photon can travel), or from looking at a subsystem entangled with something else.\n\nFor a finite-dimensional function space, the most general density operator is of the form\n\nwhere the coefficients \"p\" are non-negative and add up to one, and formula_27 is an outer product written in bra-ket notation. This represents a mixed state, with probability \"p\" that the system is in the pure state formula_28.\n\nFor the above example of unpolarized light, the density operator is\n\nwhere formula_30 is the left-circularly-polarized photon state and formula_31 is the right-circularly-polarized photon state.\n\nAn earlier section gave an example of two statistical ensembles of pure states that have the same density operator: unpolarized light can be described as both 50% right-circular-polarized and 50% left-circular-polarized, or 50% horizontally-polarized and 50% vertically-polarized. Such equivalent ensembles or mixtures cannot be distinguished by any measurement. This equivalence can be characterized precisely. Two ensembles ψ, ψ' define the same density operator if and only if there is a Unitary operator U with\n\nThis is simply a restatement of the following fact from linear algebra: for two square matrices and , \"M M\" = \"N N\" if and only if \"M\" = \"NU\" for some unitary . (See square root of a matrix for more details.) Thus there is a unitary freedom in the ket mixture or ensemble that gives the same density operator. However, if the kets making up the mixture are restricted to be a specific orthonormal basis, then the original probabilities \"p\" are uniquely recoverable from that basis, as the eigenvalues of the density matrix.\n\nIn operator language, a density operator is a positive semidefinite, Hermitian operator of trace 1 acting on the state space. A density operator describes a pure state if it is a rank one projection. Equivalently, a density operator ρ describes a pure state if and only if\ni.e. the state is idempotent. This is true regardless of whether is finite-dimensional or not.\n\nGeometrically, when the state is not expressible as a convex combination of other states, it is a pure state. The family of mixed states is a convex set and a state is pure if it is an extremal point of that set.\n\nIt follows from the spectral theorem for compact self-adjoint operators that every mixed state is a countable convex combination of pure states. This representation is not unique. Furthermore, a theorem of Andrew Gleason states that certain functions defined on the family of projections and taking values in [0,1] (which can be regarded as quantum analogues of probability measures) are determined by unique mixed states. See quantum logic for more details.\n\nLet \"A\" be an observable of the system, and suppose the ensemble is in a mixed state such that each of the pure states formula_34 occurs with probability \"p\". Then the corresponding density operator is:\n\nThe expectation value of the measurement can be calculated by extending from the case of pure states (see Measurement in quantum mechanics):\n\nwhere formula_37 denotes trace. Thus, the familiar expression formula_38 for pure states is replaced by\nfor mixed states.\n\nMoreover, if \"A\" has spectral resolution\n\nwhere formula_41, the corresponding density operator after the measurement is given by:\n\nNote that the above density operator describes the full ensemble after measurement. The sub-ensemble for which the measurement result was the particular value \"a\" is described by the different density operator\n\nThis is true assuming that formula_44 is the only eigenket (up to phase) with eigenvalue \"a\"; more generally, \"P\" in this expression would be replaced by the projection operator into the eigen\"space\" corresponding to eigenvalue \"a\".\n\nMore generally, suppose formula_45 is a function that associates to each observable \"A\" a number formula_46, which we may think of as the \"expectation value\" of \"A\". If formula_45 satisfies some natural properties (such as giving positive values on positive operators), then there is a unique density matrix formula_48 such that\nfor all \"A\". That is to say, any reasonable \"family of expectation values\" is representable by a density matrix. This observation suggests that density matrices are the most general notion of a quantum state.\n\nThe von Neumann entropy formula_50 of a mixture can be expressed in terms of the eigenvalues of formula_48 or in terms of the trace and logarithm of the density operator formula_48. Since formula_48 is a positive semi-definite operator, it has a spectral decomposition such that formula_54, where formula_55 are orthonormal vectors, formula_56, and formula_57. Then the entropy of a quantum system with density matrix formula_48 is\n\nThis entropy can increase, but never decrease, with a projective measurement. However, generalised measurements can decrease entropy. The entropy of a pure state is zero, while that of a proper mixture is always greater than zero. Therefore, a pure state may be converted into a mixture by a measurement, but a proper mixture can \"never\" be converted into a pure state. Thus the act of measurement induces a fundamental irreversible change on the density matrix; this is analogous to the \"collapse\" of the state vector, or wavefunction collapse. Perhaps counterintuitively, the measurement actually \"decreases information\" by erasing quantum interference in the composite system, see quantum entanglement, einselection, and quantum decoherence.\n\nA subsystem of a larger system can be turned from a mixed to a pure state, but only by increasing the von Neumann entropy elsewhere in the system. This is analogous to how the entropy of an object can be lowered by putting it in a refrigerator: The air outside the refrigerator's heat exchanger warms up, gaining even more entropy than was lost by the object in the refrigerator. See second law of thermodynamics. See Entropy in thermodynamics and information theory.\n\nAnother motivation for considering density matrices comes from consideration of systems and their subsystems.\nSuppose we have two quantum systems, described by Hilbert spaces formula_60 and formula_61. The composite system is then the tensor product formula_62 of the two Hilbert spaces. Suppose now that the composite system is in a pure state formula_63. If formula_64 happens to have the special form formula_65, then we may reasonably say that the state of the first subsystem is formula_66. In this case, we say that the two systems are not entangled. In general, however, formula_64 will not decompose as a single tensor product of vectors in formula_60 and formula_61. If formula_64 cannot be decomposed as a single tensor product of states in the component systems, we say that the two systems are entangled. In that case, there is no reasonable way to associate a pure state formula_71 to the state formula_63.\n\nIf, for example, we have a wave function formula_73 describing the state of two particles, there is no natural way to construct a wave function (i.e., pure state) formula_74 that describes the states of the first particle—unless formula_73 happens to be a product of a function formula_74 and a function formula_77.\n\nThe upshot of the preceding discussion is that even if the total system is in a pure state, the various subsystems that make it up will typically be in mixed states. Thus, the use of density matrices is unavoidable.\n\nOn the other hand, whether the composite system is in a pure state or a mixed state, we can perfectly well construct a density matrix that describes the state of formula_60. Denote the density matrix of the composite system of two systems by formula_48. Then the state of, say, formula_60, is described by a reduced density operator, given by taking the \"partial trace\" of formula_48 over formula_61.\n\nIf the state of formula_62 happens to be a density matrix of the special form formula_84 where formula_85 and formula_86 are density matrices on formula_60 and formula_61, then the partial trace of formula_48 with respect to formula_61 is just formula_85. A typical formula_48 will not be of this form, however.\n\nJust as the Schrödinger equation describes how pure states evolve in time, the von Neumann equation (also known as the Liouville–von Neumann equation) describes how a density operator evolves in time (in fact, the two equations are equivalent, in the sense that either can be derived from the other.) The von Neumann equation dictates that\n\nwhere the brackets denote a commutator.\n\nNote that this equation only holds when the density operator is taken to be in the Schrödinger picture, even though this equation seems at first look to emulate the Heisenberg equation of motion in the Heisenberg picture, with a crucial sign difference:\n\nwhere formula_95 is some \"Heisenberg picture\" operator; but in this picture the density matrix is \"not time-dependent\", and the relative sign ensures that the time derivative of the expected value formula_96 comes out \"the same as in the Schrödinger picture\".\n\nTaking the density operator to be in the Schrödinger picture makes sense, since it is composed of 'Schrödinger' kets and bras evolved in time, as per the Schrödinger picture.\nIf the Hamiltonian is time-independent, this differential equation can be easily solved to yield\n\nFor a more general Hamiltonian, if formula_98 is the wavefunction propagator over some interval, then the time evolution of the density matrix over that same interval is given by\n\nHowever, the density matrix contains both classical and quantum-mechanical probabilities it is necessary to account for changes in both in the presence of external influences.\n\nThe density matrix operator may also be realized in phase space. Under the Wigner map, the density matrix transforms into the equivalent Wigner function,\nThe equation for the time evolution of the Wigner function is then the Wigner-transform of the above von Neumann equation,\nwhere \"H\"(\"q\", \"p\") is the Hamiltonian, and <nowiki></nowiki> is the Moyal bracket, the transform of the quantum commutator.\n\nThe evolution equation for the Wigner function is then analogous to that of its classical limit, the Liouville equation of classical physics. In the limit of vanishing Planck's constant \"ħ\", \"W\"(\"q\", \"p\", \"t\") reduces to the classical Liouville probability density function in phase space.\n\nThe classical Liouville equation can be solved using the method of characteristics for partial differential equations, the characteristic equations being Hamilton's equations. The Moyal equation in quantum mechanics similarly admits formal solutions in terms of quantum characteristics, predicated on the ∗−product of phase space, although, in actual practice, solution-seeking follows different methods.\n\nDensity matrices are a basic tool of quantum mechanics, and appear at least occasionally in almost any type of quantum-mechanical calculation. Some specific examples where density matrices are especially helpful and common are as follows:\n\nIt is now generally accepted that the description of quantum mechanics in which all self-adjoint operators represent observables is untenable. For this reason, observables are identified with elements of an abstract C*-algebra \"A\" (that is one without a distinguished representation as an algebra of operators) and states are positive linear functionals on \"A\". However, by using the GNS construction, we can recover Hilbert spaces which realize \"A\" as a subalgebra of operators.\n\nGeometrically, a pure state on a C*-algebra \"A\" is a state which is an extreme point of the set of all states on \"A\". By properties of the GNS construction these states correspond to irreducible representations of \"A\".\n\nThe states of the C*-algebra of compact operators \"K\"(\"H\") correspond exactly to the density operators, and therefore the pure states of \"K\"(\"H\") are exactly the pure states in the sense of quantum mechanics.\n\nThe C*-algebraic formulation can be seen to include both classical and quantum systems. When the system is classical, the algebra of observables become an abelian C*-algebra. In that case the states become probability measures, as noted in the introduction.\n"}
{"id": "7430174", "url": "https://en.wikipedia.org/wiki?curid=7430174", "title": "Discrete dipole approximation", "text": "Discrete dipole approximation\n\nThe discrete dipole approximation (DDA) is a method for computing scattering of radiation by particles of arbitrary shape and by periodic structures. Given a target of arbitrary geometry, one seeks to calculate its scattering and absorption properties. Exact solutions to Maxwell's equations are known only for special geometries such as spheres, spheroids, or cylinders, so approximate methods are in general required. However, the DDA employs no physical approximations and can produce accurate enough results, given sufficient computer power.\n\nThe basic idea of the DDA was introduced in 1964 by DeVoe\n\nwho applied it to study the optical properties of molecular aggregates; retardation effects were not included, so DeVoe's treatment was limited to aggregates that were small compared with the wavelength. The DDA, including retardation effects, was proposed in 1973 by Purcell and Pennypacker\n\nwho used it to study interstellar dust grains. Simply stated, the DDA is an approximation of the continuum target by a finite array of polarizable points. The points acquire dipole moments in response to the local electric field. The dipoles of course interact with one another via their electric fields, so the DDA is also sometimes referred to as the coupled dipole approximation.\n\nNature provides the physical inspiration for the DDA: in 1909 Lorentz\n\nshowed that the dielectric properties of a substance could be directly related to the polarizabilities of the individual atoms of which it was composed, with a particularly simple and exact relationship, the Clausius-Mossotti relation (or Lorentz-Lorenz), when the atoms are located on a cubic lattice. We may expect that, just as a continuum representation of a solid is appropriate on length scales that are large compared with the interatomic spacing, an array of polarizable points can accurately approximate the response of a continuum target on length scales that are large compared with the interdipole separation.\n\nFor a finite array of point dipoles the scattering problem may be solved exactly, so the only approximation that is present in the DDA is the replacement of the continuum target by an array of N-point dipoles. The replacement requires specification of both the geometry (location of the dipoles) and the dipole polarizabilities. For monochromatic incident waves the self-consistent solution for the oscillating dipole moments may be found; from these the absorption and scattering cross sections are computed. If DDA solutions are obtained for two independent polarizations of the incident wave, then the complete amplitude scattering matrix can be determined.\n\nAlternatively, the DDA can be derived from volume integral equation for the electric field. This highlights that the approximation of point dipoles is equivalent to that of discretizing the integral equation, and thus decreases with decreasing dipole size.\n\nWith the recognition that the polarizabilities may be tensors, the DDA can readily be applied to anisotropic materials. The extension of the DDA to treat materials with nonzero magnetic susceptibility is also straightforward, although for most applications magnetic effects are negligible.\n\nThe method was improved by Draine, Flatau, and Goodman who applied Fast Fourier Transform and conjugate gradient method to calculate convolution problem arising in the DDA methodology which allowed to calculate scattering by large targets. They distributed discrete dipole approximation open source code DDSCAT.\nThere are now several DDA implementations. There are extensions to periodic targets and particles placed on or near a plane substrate.\nA convergence theory of the DDA has been developed and comparisons with exact technique were published.\nThe validity criteria of the discrete dipole approximation have been recently revised. That work significantly extends the range of applicability of the DDA for the case of irregularly shaped particles. The DDA has been also extended to employ rectangular-cuboid dipoles,\nwhich is very efficient for highly oblate or prolate particles.\n\n"}
{"id": "8528", "url": "https://en.wikipedia.org/wiki?curid=8528", "title": "Disjunction introduction", "text": "Disjunction introduction\n\nDisjunction introduction or addition (also called or introduction) is a rule of inference of propositional logic and almost every other deduction system. The rule makes it possible to introduce disjunctions to logical proofs. It is the inference that if \"P\" is true, then \"P or Q\" must be true.\n\nAn example in English:\n\nThe rule can be expressed as:\n\nwhere the rule is that whenever instances of \"formula_2\" appear on lines of a proof, \"formula_3\" can be placed on a subsequent line. \n\nMore generally it's also a simple valid argument form, this means that if the premise is true, then the conclusion is also true as any rule of inference should be, and an immediate inference, as it has a single proposition in its premises. \n\nDisjunction introduction is not a rule in some paraconsistent logics because in combination with other rules of logic, it leads to explosion (i.e. everything becomes provable) and paraconsistent logic tries to avoid explosion and to be able to reason with contradictions. One of the solutions is to introduce disjunction with over rules. See Tradeoffs in Paraconsistent logic.\n\nThe \"disjunction introduction\" rule may be written in sequent notation:\n\nwhere formula_5 is a metalogical symbol meaning that formula_3 is a syntactic consequence of formula_2 in some logical system;\n\nand expressed as a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_2 and formula_10 are propositions expressed in some formal system.\n"}
{"id": "12065590", "url": "https://en.wikipedia.org/wiki?curid=12065590", "title": "Doxastic logic", "text": "Doxastic logic\n\nDoxastic logic is a type of logic concerned with reasoning about beliefs. The term \"doxastic\" derives from the ancient Greek δόξα, \"doxa\", which means \"belief\". Typically, a doxastic logic uses formula_1 to mean \"It is believed that formula_2 is the case\", and the set formula_3 denotes a set of beliefs. In doxastic logic, belief is treated as a modal operator.\n\nThere is complete parallelism between a person who believes propositions and a formal system that derives propositions. Using doxastic logic, one can express the epistemic counterpart of Gödel's incompleteness theorem of metalogic, as well as Löb's theorem, and other metalogical results in terms of belief.\n\nTo demonstrate the properties of sets of beliefs, Raymond Smullyan defines the following types of reasoners:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us say an accurate reasoner is faced with the task of assigning a truth value to a statement. There exists a statement which the reasoner must either remain forever undecided about or lose his or her accuracy. One solution is the statement:\n\nIf the reasoner ever believes the statement formula_61 it becomes falsified by that fact, making formula_62 an untrue belief and hence making the reasoner inaccurate in believing S.\n\nTherefore, since the reasoner is accurate, he or she will never believe formula_63 Hence the statement was true, because that is exactly what it claimed. It further follows that the reasoner will never have the false belief that formula_62 is false. And so the reasoner must remain forever undecided as to whether the statement formula_62 is true or false.\n\nThe equivalent theorem is that for any formal system F, there exists a mathematical statement which can be interpreted as \"This statement is not provable in formal system F\". If the system F is consistent, neither the statement nor its opposite will be provable in it.\n\nA reasoner of type 1 is faced with the statement \"I will never believe this sentence.\" The interesting thing now is that if the reasoner believes he or she is always accurate, then he or she will become inaccurate. Such a reasoner will reason: \"The statement in question is that I won't believe the statement, so if it's false then I will believe the statement. Because I am accurate, believing the statement means it must be true. So if the statement is false then it must be true. It's tautological that if a statement being false implies the statement, then that statement is true. Therefore the statement is true.\"\n\nAt this point the reasoner believes the statement, which makes it false. Thus the reasoner is inaccurate in believing that the statement is true. If the reasoner hadn't assumed his or her own accuracy, he or she would never have lapsed into an inaccuracy. Formally:\n\nAdditionally, the reasoner is peculiar because he or she believes that he/she doesn't believe the statement (symbolically, formula_77 which follows from formula_78 because formula_79) even though he/she actually believes it.\n\nFor systems, we define reflexivity to mean that for any formula_16 (in the language of the system) there is some formula_17 such that formula_82 is provable in the system. Löb's theorem (in a general form) is that for any reflexive system of type 4, if formula_33 is provable in the system, so is formula_11\n\nIf a consistent reflexive reasoner of type 4 believes that he or she is stable, then he or she will become unstable. Stated otherwise, if a stable reflexive reasoner of type 4 believes that he or she is stable, then he or she will become inconsistent. Why is this? Suppose that a stable reflexive reasoner of type 4 believes that he or she is stable. We will show that he or she will (sooner or later) believe every proposition formula_16 (and hence be inconsistent). Take any proposition formula_11 The reasoner believes formula_87 hence by Löb's theorem he or she will believe formula_23 (because he or she believes formula_89 where formula_90 is the proposition formula_91 and so he or she will believe formula_92 which is the proposition formula_23). Being stable, he or she will then believe formula_11\n\n\n"}
{"id": "23453065", "url": "https://en.wikipedia.org/wiki?curid=23453065", "title": "Dürer graph", "text": "Dürer graph\n\nIn the mathematical field of graph theory, the Dürer graph is an undirected graph with 12 vertices and 18 edges. It is named after Albrecht Dürer, whose 1514 engraving \"Melencolia I\" includes a depiction of Dürer's solid, a convex polyhedron having the Dürer graph as its skeleton. Dürer's solid is one of only four well-covered simple convex polyhedra.\n\nDürer's solid is combinatorially equivalent to a cube with two opposite vertices truncated, although Dürer's depiction of it is not in this form but rather as a truncated rhombohedron or triangular truncated trapezohedron. The exact geometry of the solid depicted by Dürer is a subject of some academic debate, with different hypothetical values for its acute angles ranging from 72° to 82°.\n\nThe Dürer graph is the graph formed by the vertices and edges of the Dürer solid. It is a cubic graph of girth 3 and diameter 4. As well as its construction as the skeleton of Dürer's solid, it can be obtained by applying a Y-Δ transform to the opposite vertices of a cube graph, or as the generalized Petersen graph \"G\"(6,2). As with any graph of a convex polyhedron, the Dürer graph is a 3-vertex-connected simple planar graph.\n\nThe Dürer graph is a well-covered graph, meaning that all of its maximal independent sets have the same number of vertices, four. It is one of four well-covered cubic polyhedral graphs and one of seven well-covered 3-connected cubic graphs. The only other three well-covered simple convex polyhedra are the tetrahedron, triangular prism, and pentagonal prism.\n\nThe Dürer graph is Hamiltonian, with LCF notation [-4,5,2,-4,-2,5;-]. More precisely, it has exactly six Hamiltonian cycles, each pair of which may be mapped into each other by a symmetry of the graph.\n\nThe automorphism group both of the Dürer graph and of the Dürer solid (in either the truncated cube form or the form shown by Dürer) is isomorphic to the dihedral group of order 12 : D.\n\n"}
{"id": "3291722", "url": "https://en.wikipedia.org/wiki?curid=3291722", "title": "Frölicher–Nijenhuis bracket", "text": "Frölicher–Nijenhuis bracket\n\nIn mathematics, the Frölicher–Nijenhuis bracket is an extension of the Lie bracket of vector fields to vector-valued differential forms on a differentiable manifold.\n\nIt is useful in the study of connections, notably the Ehresmann connection, as well as in the more general study of projections in the tangent bundle.\nIt was introduced by Alfred Frölicher and Albert Nijenhuis (1956) and is related to the work of Schouten (1940).\n\nIt is related to but not the same as the Nijenhuis–Richardson bracket and the Schouten–Nijenhuis bracket.\n\nLet Ω*(\"M\") be the sheaf of exterior algebras of differential forms on a smooth manifold \"M\". This is a graded algebra in which forms are graded by degree:\nA graded derivation of degree ℓ is a mapping\nwhich is linear with respect to constants and satisfies\nThus, in particular, the interior product with a vector defines a graded derivation of degree ℓ = −1, whereas the exterior derivative is a graded derivation of degree ℓ = 1.\n\nThe vector space of all derivations of degree ℓ is denoted by DerΩ*(\"M\"). The direct sum of these spaces is a graded vector space whose homogeneous components consist of all graded derivations of a given degree; it is denoted\nThis forms a graded Lie superalgebra under the anticommutator of derivations defined on homogeneous derivations \"D\" and \"D\" of degrees \"d\" and \"d\", respectively, by\n\nAny vector-valued differential form \"K\" in Ω(\"M\", T\"M\") with values in the tangent bundle of \"M\" defines a graded derivation of degree \"k\" − 1, denoted by \"i\", and called the insertion operator. For ω ∈ Ω(\"M\"),\nThe Nijenhuis–Lie derivative along \"K\" ∈ Ω(\"M\", T\"M\") is defined by\nwhere \"d\" is the exterior derivative and \"i\" is the insertion operator.\n\nThe Frölicher–Nijenhuis bracket is defined to be the unique vector-valued differential form\n\nsuch that\n\nHence,\n\nIf \"k\" = 0, so that \"K\" ∈ Ω(\"M\", T\"M\")\nis a vector field, the usual homotopy formula for the Lie derivative is recovered\n\nIf \"k\"=\"ℓ\"=1, so that \"K,L\" ∈ Ω(\"M\", T\"M\"),\none has for any vector fields \"X\" and \"Y\"\n\nIf \"k\"=0 and \"ℓ\"=1, so that \"K=Z\"∈ Ω(\"M\", T\"M\") is a vector field and \"L\" ∈ Ω(\"M\", T\"M\"), one has for any vector field \"X\"\n\nAn explicit formula for the Frölicher–Nijenhuis bracket of formula_14 and formula_15 (for forms φ and ψ and vector fields \"X\" and \"Y\") is given by\n\nEvery derivation of Ω(\"M\") can be written as \nfor unique elements \"K\" and \"L\" of Ω(\"M\", T\"M\"). The Lie bracket of these derivations is given as follows.\n\nThe Nijenhuis tensor of an almost complex structure \"J\", is the Frölicher–Nijenhuis bracket of \"J\" with itself. An almost complex structure is a complex structure if and only if the Nijenhuis tensor is zero.\n\nWith the Frölicher–Nijenhuis bracket it is possible to define the curvature and cocurvature of a vector-valued 1-form which is a projection. This generalizes the concept of the curvature of a connection.\n\nThere is a common generalization of the Schouten–Nijenhuis bracket and the Frölicher–Nijenhuis bracket; for details see the article on the Schouten–Nijenhuis bracket.\n\n"}
{"id": "57482546", "url": "https://en.wikipedia.org/wiki?curid=57482546", "title": "Fuchs relation", "text": "Fuchs relation\n\nIn mathematics, the Fuchs relation is a relation between the starting exponents of formal series solutions of certain linear differential equations, so called \"Fuchsian equations\". It is named after Lazarus Immanuel Fuchs.\n\nA linear differential equation in which every singular point, including the point at infinity, is a regular singularity is called \"Fuchsian equation\" or \"equation of Fuchsian type\". For Fuchsian equations a formal fundamental system exists at any point, due to the Fuchsian theory.\n\nLet formula_1 be the formula_2 regular singularities in the finite part of the complex plane of the linear differential equationformula_3\n\nwith meromorphic functions formula_4. For linear differential equations the singularities are exactly the singular points of the coefficients. formula_5 is a Fuchsian equation if and only if the coefficients are rational functions of the form\n\nwith the polynomial formula_7 and certain polynomials formula_8 for formula_9, such that formula_10. This means the coefficient formula_4 has poles of order at most formula_12, for formula_9.\n\nLet formula_5 be a Fuchsian equation of order formula_15 with the singularities formula_16 and the point at infinity. Let formula_17 be the roots of the indicial polynomial relative to formula_18, for formula_19. Let formula_20 be the roots of the indicial polynomial relative to formula_21, which is given by the indicial polynomial of formula_22 transformed by formula_23 at formula_24. Then the so called \"Fuchs relation\" holds:\n\nThe Fuchs relation can be rewritten as infinite sum. Let formula_26 denote the indicial polynomial relative to formula_27 of the Fuchsian equation formula_5. Define formula_29 as\n\nwhere formula_31 gives the trace of a polynomial formula_32, i. e., formula_33 denotes the sum of a polynomial's roots counted with multiplicity.\n\nThis means that formula_34 for any ordinary point formula_35, due to the fact that the indicial polynomial relative to any ordinary point is formula_36. The transformation formula_23, that is used to obtain the indicial equation relative to formula_21, motivates the changed sign in the definition of formula_39 for formula_40. The rewritten Fuchs relation is:\n\n"}
{"id": "3043978", "url": "https://en.wikipedia.org/wiki?curid=3043978", "title": "Generic point", "text": "Generic point\n\nIn algebraic geometry, a generic point \"P\" of an algebraic variety \"X\" is, roughly speaking, a point at which all generic properties are true, a generic property being a property which is true for almost every point.\n\nIn classical algebraic geometry, a generic point of an affine or projective algebraic variety of dimension \"d\" is a point such that the field generated by its coordinates has the transcendence degree \"d\" over the field generated by the coefficients of the equations of the variety.\n\nIn scheme theory, the spectrum of an integral domain has a unique generic point, which is the minimal prime ideal. As the closure of this point for the Zariski topology is the whole spectrum, the definition has been extended to general topology, where a generic point of a topological space \"X\" is a point whose closure is \"X\".\n\nA generic point of the topological space \"X\" is a point \"P\" whose closure is all of \"X\", that is, a point that is dense in \"X\".\n\nThe terminology arises from the case of the Zariski topology on the set of subvarieties of an algebraic set: the algebraic set is irreducible (that is, it is not the union of two proper algebraic subsets) if and only if the topological space of the subvarieties has a generic point.\n\n\nIn the foundational approach of André Weil, developed in his \"Foundations of Algebraic Geometry\", generic points played an important role, but were handled in a different manner. For an algebraic variety \"V\" over a field \"K\", \"generic points\" of \"V\" were a whole class of points of \"V\" taking values in a universal domain Ω, an algebraically closed field containing \"K\" but also an infinite supply of fresh indeterminates. This approach worked, without any need to deal directly with the topology of \"V\" (\"K\"-Zariski topology, that is), because the specializations could all be discussed at the field level (as in the valuation theory approach to algebraic geometry, popular in the 1930s).\n\nThis was at a cost of there being a huge collection of equally generic points. Oscar Zariski, a colleague of Weil's at São Paulo just after World War II, always insisted that generic points should be unique. (This can be put back into topologists' terms: Weil's idea fails to give a Kolmogorov space and Zariski thinks in terms of the Kolmogorov quotient.)\n\nIn the rapid foundational changes of the 1950s Weil's approach became obsolete. In scheme theory, though, from 1957, generic points returned: this time \"à la Zariski\". For example for \"R\" a discrete valuation ring, \"Spec\"(\"R\") consists of two points, a generic point (coming from the prime ideal {0}) and a closed point or special point coming from the unique maximal ideal. For morphisms to \"Spec\"(\"R\"), the fiber above the special point is the special fiber, an important concept for example in reduction modulo p, monodromy theory and other theories about degeneration. The generic fiber, equally, is the fiber above the generic point. Geometry of degeneration is largely then about the passage from generic to special fibers, or in other words how specialization of parameters affects matters. (For a discrete valuation ring the topological space in question is the Sierpinski space of topologists. Other local rings have unique generic and special points, but a more complicated spectrum, since they represent general dimensions. The discrete valuation case is much like the complex unit disk, for these purposes.)\n\n"}
{"id": "19116785", "url": "https://en.wikipedia.org/wiki?curid=19116785", "title": "GrGen", "text": "GrGen\n\nGrGen.NET is a software development tool that offers programming languages (domain specific languages) that are optimized for the processing of graph structured data.\nThe core of the languages consists of modular graph rewrite rules, which are built on declarative graph pattern matching and rewriting; they are supplemented by many of the constructs that are used in imperative and object-oriented programming,\nand are completed with language devices known from database query languages.\n\nThe Graph Rewrite GENerator compiles the languages into efficient CLI assemblies (via C#-Code in an intermediate step), which can be integrated via an API into code written in any .NET-language.\nGrGen can be executed under Windows and Linux (Mono needed) and is open source available under LGPL v3.\n\nFor rapid prototyping and debugging, an interactive shell and a (VCG-)graph viewer are included in the package.\nWith its languages and its visual and stepwise debugging, GrGen allows one to develop at the natural level of abstraction of graph-based representations, such as those employed in engineering, model transformation, computational linguistics, or compiler construction (as intermediate representation).\n\nGrGen increases productivity for those kinds of tasks far beyond what can be achieved by programming in a traditional programming language; due to many implemented performance optimizations it still allows one to achieve high-performance solutions.\nIts authors claim that the system offers the highest combined speed of development and execution available for the algorithmic processing of graph-based representations (based on their performance regarding diverse tasks posed at different editions of the Transformation Tool Contest (/GraBaTs)).\n\nBelow is an example containing a graph model and rule specifications from the GrGen.NET-solution to the AntWorld-case posed at Grabats 08.\n\nGraph model:\n\nRewrite Rules:\n\n\n\n"}
{"id": "23748418", "url": "https://en.wikipedia.org/wiki?curid=23748418", "title": "Guido Zappa", "text": "Guido Zappa\n\nGuido Zappa (7 December 1915 – 17 March 2015) was an Italian mathematician and a noted group theorist: his other main research interests were geometry and also the history of mathematics. Zappa was particularly known for some examples of algebraic curves that strongly influenced the ideas of Francesco Severi.\n\nHe was elected ordinary non-resident member of the Accademia Pontaniana on June 16, 1949.\nOn June 3, 1951 he was elected corresponding member to the class of mathematical sciences of the Società Nazionale di Scienze Lettere e Arti in Napoli: subsequently, he became ordinary member (2 June 1951) and ordinary non-resident member (15 December 1953).\nOn 14 October 1960 he was elected corresponding member of the Accademia Nazionale dei Lincei: he became national member of the same academy on March 21, 1977.\n\n\n\n\n"}
{"id": "35258187", "url": "https://en.wikipedia.org/wiki?curid=35258187", "title": "Henry Helson", "text": "Henry Helson\n\nHenry Berge Helson (June 2, 1927 – January 10, 2010) was an American mathematician at the University of California at Berkeley who worked on analysis.\n\nHelson received his bachelor's degree from Harvard University in 1947. With the support of a Harvard travelling fellowship, he spent the academic year 1947–1948 in Europe; he visited London, Paris, Prague, and Vienna, but spent most of his time in Warsaw and then from spring 1948 in Wroclaw, where he worked with Marczewski. Helson received his Ph.D. in 1950 from Harvard with supervisor Lynn Loomis and then spent the academic year 1950–1951 primarily in Uppsala working with Beurling but with frequent trips elsewhere in Europe. He became in 1951 an instructor and then an assistant professor at Yale University. He became in 1955 an assistant professor, in 1958 an associate professor, and in 1961 a full professor at the University of California, Berkeley, retiring there as professor emeritus in 1993. In 1970 he was an Invited Speaker at the ICM in Nice.\n\nIf G is an infinite, nondiscrete, locally compact group, then a Helson set is defined to be a compact set P in G such that every continuous function on P can be extended to a function in the Fourier algebra A(G) in the group G. Helson was the first to prove that there exist perfect Helson sets for the case of the group consisting of the real line.\n\nHelson founded the mathematics-specialty publishing company Berkeley Books. Upon his death he was survived by his wife, a daughter, two sons, and three grandchildren. His doctoral students include Frank Forelli and Udai Tewari. \n\n\n"}
{"id": "8748764", "url": "https://en.wikipedia.org/wiki?curid=8748764", "title": "Hexagonal pyramidal number", "text": "Hexagonal pyramidal number\n\nA hexagonal pyramidal number is a pyramidal number formed by adding the first \"n\" hexagonal numbers. The first few hexagonal pyramidal numbers are:\n\nThe \"n\"th number in this sequence, representing the sum of the first \"n\" hexagonal numbers, is given by the formula\n\n"}
{"id": "152759", "url": "https://en.wikipedia.org/wiki?curid=152759", "title": "Hilbert's second problem", "text": "Hilbert's second problem\n\nIn mathematics, Hilbert's second problem was posed by David Hilbert in 1900 as one of his 23 problems. It asks for a proof that the arithmetic is consistent – free of any internal contradictions. Hilbert stated that the axioms he considered for arithmetic were the ones given in , which include a second order completeness axiom.\n\nIn the 1930s, Kurt Gödel and Gerhard Gentzen proved results that cast new light on the problem. Some feel that Gödel's theorems give a negative solution to the problem, while others consider Gentzen's proof as a partial positive solution.\n\nIn one English translation, Hilbert asks:\n\n\"When we are engaged in investigating the foundations of a science, we must set up a system of axioms which contains an exact and complete description of the relations subsisting between the elementary ideas of that science. ... But above all I wish to designate the following as the most important among the numerous questions which can be asked with regard to the axioms: To prove that they are not contradictory, that is, that a definite number of logical steps based upon them can never lead to contradictory results. In geometry, the proof of the compatibility of the axioms can be effected by constructing a suitable field of numbers, such that analogous relations between the numbers of this field correspond to the geometrical axioms. ... On the other hand a direct method is needed for the proof of the compatibility of the arithmetical axioms.\" \n\nHilbert's statement is sometimes misunderstood, because by the \"arithmetical axioms\" he did not mean a system equivalent to Peano arithmetic, but a stronger system with a second-order completeness axiom. The system Hilbert asked for a completeness proof of is more like second-order arithmetic than first-order Peano arithmetic.\n\nAs a nowadays common interpretation, a positive solution to Hilbert's second question would in particular provide a proof that Peano arithmetic is consistent.\n\nThere are many known proofs that Peano arithmetic is consistent that can be carried out in strong systems such as Zermelo–Fraenkel set theory. These do not provide a resolution to Hilbert's second question, however, because someone who doubts the consistency of Peano arithmetic is unlikely to accept the axioms of set theory (which is much stronger) to prove its consistency. Thus a satisfactory answer to Hilbert's problem must be carried out using principles that would be acceptable to someone who does not already believe PA is consistent. Such principles are often called finitistic because they are completely constructive and do not presuppose a completed infinity of natural numbers. Gödel's incompleteness theorem places a severe limit on how weak a finitistic system can be while still proving the consistency of Peano arithmetic.\n\nGödel's second incompleteness theorem shows that it is not possible for any proof that Peano Arithmetic is consistent to be carried out within Peano arithmetic itself. This theorem shows that if the only acceptable proof procedures are those that can be formalized within arithmetic then Hilbert's call for a consistency proof cannot be answered. However, as Nagel and Newman (1958:96–99) explain, there is still room for a proof that cannot be formalized in arithmetic:\n\nIn 1936, Gentzen published a proof that Peano Arithmetic is consistent. Gentzen's result shows that a consistency proof can be obtained in a system that is much weaker than set theory.\n\nGentzen's proof proceeds by assigning to each proof in Peano arithmetic an ordinal number, based on the structure of the proof, with each of these ordinals less than ε. He then proves by transfinite induction on these ordinals that no proof can conclude in a contradiction. The method used in this proof can also be used to prove a cut elimination result for Peano arithmetic in a stronger logic than first-order logic, but the consistency proof itself can be carried out in ordinary first-order logic using the axioms of primitive recursive arithmetic and a transfinite induction principle. Tait (2005) gives a game-theoretic interpretation of Gentzen's method.\n\nGentzen's consistency proof initiated the program of ordinal analysis in proof theory. In this program, formal theories of arithmetic or set theory are assigned ordinal numbers that measure the consistency strength of the theories. A theory will be unable to prove the consistency of another theory with a higher proof theoretic ordinal.\n\nWhile the theorems of Gödel and Gentzen are now well understood by the mathematical logic community, no consensus has formed on whether (or in what way) these theorems answer Hilbert's second problem. Simpson (1988:sec. 3) argues that Gödel's incompleteness theorem shows that it is not possible to produce finitistic consistency proofs of strong theories. Kreisel (1976) states that although Gödel's results imply that no finitistic syntactic consistency proof can be obtained, semantic (in particular, second-order) arguments can be used to give convincing consistency proofs. Detlefsen (1990:p. 65) argues that Gödel's theorem does not prevent a consistency proof because its hypotheses might not apply to all the systems in which a consistency proof could be carried out. Dawson (2006:sec. 2) calls the belief that Gödel's theorem eliminates the possibility of a persuasive consistency proof \"erroneous\", citing the consistency proof given by Gentzen and a later one given by Gödel in 1958.\n\n\n\n"}
{"id": "15036", "url": "https://en.wikipedia.org/wiki?curid=15036", "title": "Information security", "text": "Information security\n\nInformation security, sometimes shortened to InfoSec, is the practice of preventing unauthorized access, use, disclosure, disruption, modification, inspection, recording or destruction of information. The information or data may take any form, e.g. electronic or physical. Information security's primary focus is the balanced protection of the confidentiality, integrity and availability of data (also known as the CIA triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity. This is largely achieved through a multi-step risk management process that identifies assets, threat sources, vulnerabilities, potential impacts, and possible controls, followed by assessment of the effectiveness of the risk management plan.\n\nTo standardize this discipline, academics and professionals collaborate and seek to set basic guidance, policies, and industry standards on password, antivirus software, firewall, encryption software, legal liability and user/administrator training standards. This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, and transferred. However, the implementation of any standards and guidance within an entity may have limited effect if a culture of continual improvement isn't adopted.\n\nAt the core of information security is information assurance, the act of maintaining the confidentiality, integrity and availability (CIA) of information, ensuring that information is not compromised in any way when critical issues arise. These issues include but are not limited to natural disasters, computer/server malfunction, and physical theft. While paper-based business operations are still prevalent, requiring their own set of information security practices, enterprise digital initiatives are increasingly being emphasized, with information assurance now typically being dealt with by information technology (IT) security specialists. These specialists apply information security to technology (most often some form of computer system). It is worthwhile to note that a computer does not necessarily mean a home desktop. A computer is any device with a processor and some memory. Such devices can range from non-networked standalone devices as simple as calculators, to networked mobile computing devices such as smartphones and tablet computers. IT security specialists are almost always found in any major enterprise/establishment due to the nature and value of the data within larger businesses. They are responsible for keeping all of the technology within the company secure from malicious cyber attacks that often attempt to acquire critical private information or gain control of the internal systems.\n\nThe field of information security has grown and evolved significantly in recent years. It offers many areas for specialization, including securing networks and allied infrastructure, securing applications and databases, security testing, information systems auditing, business continuity planning, electronic record discovery, and digital forensics. Information security professionals are very stable in their employment. more than 80 percent of professionals had no change in employer or employment over a period of a year, and the number of professionals is projected to continuously grow more than 11 percent annually from 2014 to 2019.\n\nInformation security threats come in many different forms. Some of the most common threats today are software attacks, theft of intellectual property, identity theft, theft of equipment or information, sabotage, and information extortion. Most people have experienced software attacks of some sort. Viruses, worms, phishing attacks, and Trojan horses are a few common examples of software attacks. The theft of intellectual property has also been an extensive issue for many businesses in the IT field. Identity theft is the attempt to act as someone else usually to obtain that person's personal information or to take advantage of their access to vital information. Theft of equipment or information is becoming more prevalent today due to the fact that most devices today are mobile, are prone to theft and have also become far more desirable as the amount of data capacity increases. Sabotage usually consists of the destruction of an organization's website in an attempt to cause loss of confidence on the part of its customers. Information extortion consists of theft of a company's property or information as an attempt to receive a payment in exchange for returning the information or property back to its owner, as with ransomware. There are many ways to help protect yourself from some of these attacks but one of the most functional precautions is user carefulness.\n\nGovernments, military, corporations, financial institutions, hospitals and private businesses amass a great deal of confidential information about their employees, customers, products, research and financial status. Should confidential information about a business' customers or finances or new product line fall into the hands of a competitor or a black hat hacker, a business and its customers could suffer widespread, irreparable financial loss, as well as damage to the company's reputation. From a business perspective, information security must be balanced against cost; the Gordon-Loeb Model provides a mathematical economic approach for addressing this concern.\n\nFor the individual, information security has a significant effect on privacy, which is viewed very differently in various cultures.\n\nPossible responses to a security threat or risk are:\n\nSince the early days of communication, diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering. Julius Caesar is credited with the invention of the Caesar cipher c. 50 B.C., which was created in order to prevent his secret messages from being read should a message fall into the wrong hands; however, for the most part protection was achieved through the application of procedural handling controls. Sensitive information was marked up to indicate that it should be protected and transported by trusted persons, guarded and stored in a secure environment or strong box. As postal services expanded, governments created official organizations to intercept, decipher, read and reseal letters (e.g., the U.K.'s Secret Office, founded in 1653).\n\nIn the mid-nineteenth century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity. For example, the British Government codified this, to some extent, with the publication of the Official Secrets Act in 1889. By the time of the First World War, multi-tier classification systems were used to communicate information to and from various fronts, which encouraged greater use of code making and breaking sections in diplomatic and military headquarters. Encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information. The volume of information shared by the Allied countries during the Second World War necessitated formal alignment of classification systems and procedural controls. An arcane range of markings evolved to indicate who could handle documents (usually officers rather than men) and where they should be stored as increasingly complex safes and storage facilities were developed. The Enigma Machine, which was employed by the Germans to encrypt the data of warfare and was successfully decrypted by Alan Turing, can be regarded as a striking example of creating and using secured information. Procedures evolved to ensure documents were destroyed properly, and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war (e.g., the capture of U-570).\n\nThe end of the twentieth century and the early years of the twenty-first century saw rapid advancements in telecommunications, computing hardware and software, and data encryption. The availability of smaller, more powerful and less expensive computing equipment made electronic data processing within the reach of small business and the home user. These computers quickly became interconnected through the internet.\n\nThe rapid growth and widespread use of electronic data processing and electronic business conducted through the internet, along with numerous occurrences of international terrorism, fueled the need for better methods of protecting the computers and the information they store, process and transmit. The academic disciplines of computer security and information assurance emerged along with numerous professional organizations, all sharing the common goals of ensuring the security and reliability of information systems.\n\nVarious definitions of information security are suggested below, summarized from different sources:\n\n\nThe CIA triad of confidentiality, integrity, and availability is at the heart of information security. (The members of the classic InfoSec triad—confidentiality, integrity and availability—are interchangeably referred to in the literature as security attributes, properties, security goals, fundamental aspects, information criteria, critical information characteristics and basic building blocks.) However, debate continues about whether or not this CIA triad is sufficient to address rapidly changing technology and business requirements, with recommendations to consider expanding on the intersections between availability and confidentiality, as well as the relationship between security and privacy. Other principles such as \"accountability\" have sometimes been proposed; it has been pointed out that issues such as non-repudiation do not fit well within the three core concepts.\n\nIn 1992 and revised in 2002, the OECD's \"Guidelines for the Security of Information Systems and Networks\" proposed the nine generally accepted principles: awareness, responsibility, response, ethics, democracy, risk assessment, security design and implementation, security management, and reassessment. Building upon those, in 2004 the NIST's \"Engineering Principles for Information Technology Security\" proposed 33 principles. From each of these derived guidelines and practices.\n\nIn 1998, Donn Parker proposed an alternative model for the classic CIA triad that he called the six atomic elements of information. The elements are confidentiality, possession, integrity, authenticity, availability, and utility. The merits of the Parkerian Hexad are a subject of debate amongst security professionals.\n\nIn 2011, The Open Group published the information security management standard O-ISM3. This standard proposed an operational definition of the key concepts of security, with elements called \"security objectives\", related to access control (9), availability (3), data quality (1), compliance and technical (4). In 2009, DoD Software Protection Initiative released the Three Tenets of Cybersecurity which are System Susceptibility, Access to the Flaw, and Capability to Exploit the Flaw. Neither of these models are widely adopted.\n\nIn information security, confidentiality \"is the property, that information is not made available or disclosed to unauthorized individuals, entities, or processes.\" While similar to \"privacy,\" the two words aren't interchangeable. Rather, confidentiality is a component of privacy that implements to protect our data from unauthorized viewers. Examples of confidentiality of electronic data being compromised include laptop theft, password theft, or sensitive emails being sent to the incorrect individuals.\n\nIn information security, data integrity means maintaining and assuring the accuracy and completeness of data over its entire lifecycle. This means that data cannot be modified in an unauthorized or undetected manner. This is not the same thing as referential integrity in databases, although it can be viewed as a special case of consistency as understood in the classic ACID model of transaction processing. Information security systems typically provide message integrity along side to confidentiality.\n\nFor any information system to serve its purpose, the information must be available when it is needed. This means the computing systems used to store and process the information, the security controls used to protect it, and the communication channels used to access it must be functioning correctly. High availability systems aim to remain available at all times, preventing service disruptions due to power outages, hardware failures, and system upgrades. Ensuring availability also involves preventing denial-of-service attacks, such as a flood of incoming messages to the target system, essentially forcing it to shut down.\n\nIn the realm of information security, availability can often be viewed as one of the most important parts of a successful information security program. Ultimately end-users need to be able to perform job functions; by ensuring availability an organization is able to perform to the standards that an organization's stakeholders expect. This can involve topics such as proxy configurations, outside web access, the ability to access shared drives and the ability to send emails. Executives oftentimes do not understand the technical side of information security and look at availability as an easy fix, but this often requires collaboration from many different organizational teams, such as network operations, development operations, incident response and policy/change management. A successful information security team involves many different key roles to mesh and align for the CIA triad to be provided effectively.\n\nIn law, non-repudiation implies one's intention to fulfill their obligations to a contract. It also implies that one party of a transaction cannot deny having received a transaction, nor can the other party deny having sent a transaction.\n\nIt is important to note that while technology such as cryptographic systems can assist in non-repudiation efforts, the concept is at its core a legal concept transcending the realm of technology. It is not, for instance, sufficient to show that the message matches a digital signature signed with the sender's private key, and thus only the sender could have sent the message, and nobody else could have altered it in transit (data integrity). The alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed, or allege or prove that his signing key has been compromised. The fault for these violations may or may not lie with the sender, and such assertions may or may not relieve the sender of liability, but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity. As such, the sender may repudiate the message (because authenticity and integrity are pre-requisites for non-repudiation).\n\nThe \"Certified Information Systems Auditor (CISA) Review Manual 2006\" provides the following definition of risk management: \"Risk management is the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives, and deciding what countermeasures, if any, to take in reducing risk to an acceptable level, based on the value of the information resource to the organization.\"\n\nThere are two things in this definition that may need some clarification. First, the \"process\" of risk management is an ongoing, iterative process. It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day. Second, the choice of countermeasures (controls) used to manage risks must strike a balance between productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected.\n\nRisk analysis and risk evaluation processes have their limitations since, when security incidents occur, they emerge in a context, and their rarity and uniqueness give rise to unpredictable threats. The analysis of these phenomena, which are characterized by breakdowns, surprises and side-effects, requires a theoretical approach that is able to examine and interpret subjectively the detail of each incident.\n\nRisk is the likelihood that something bad will happen that causes harm to an informational asset (or the loss of the asset). A vulnerability is a weakness that could be used to endanger or cause harm to an informational asset. A threat is anything (man-made or act of nature) that has the potential to cause harm.\n\nThe likelihood that a threat will use a vulnerability to cause harm creates a risk. When a threat does use a vulnerability to inflict harm, it has an impact. In the context of information security, the impact is a loss of availability, integrity, and confidentiality, and possibly other losses (lost income, loss of life, loss of real property). It should be pointed out that it is not possible to identify all risks, nor is it possible to eliminate all risk. The remaining risk is called \"residual risk.\"\n\nA risk assessment is carried out by a team of people who have knowledge of specific areas of the business. Membership of the team may vary over time as different parts of the business are assessed. The assessment may use a subjective qualitative analysis based on informed opinion, or where reliable dollar figures and historical information is available, the analysis may use quantitative analysis.\n\nResearch has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human. The ISO/IEC 27002:2005 Code of practice for information security management recommends the following be examined during a risk assessment:\n\nIn broad terms, the risk management process consists of:\n\nFor any given risk, management can choose to accept the risk based upon the relative low value of the asset, the relative low frequency of occurrence, and the relative low impact on the business. Or, leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk. In some cases, the risk can be transferred to another business by buying insurance or outsourcing to another business. The reality of some risks may be disputed. In such cases leadership may choose to deny the risk.\n\nSelecting and implementing proper security controls will initially help an organization bring down risk to acceptable levels. Control selection should follow and should be based on the risk assessment. Controls can vary in nature, but fundamentally they are ways of protecting the confidentiality, integrity or availability of information. ISO/IEC 27001 has defined controls in different areas. Organizations can implement additional controls according to requirement of the organization. ISO/IEC 27002 offers a guideline for organizational information security standards.\n\nAdministrative controls consist of approved written policies, procedures, standards and guidelines. Administrative controls form the framework for running the business and managing people. They inform people on how the business is to be run and how day-to-day operations are to be conducted. Laws and regulations created by government bodies are also a type of administrative control because they inform the business. Some industry sectors have policies, procedures, standards and guidelines that must be followed – the Payment Card Industry Data Security Standard (PCI DSS) required by Visa and MasterCard is such an example. Other examples of administrative controls include the corporate security policy, password policy, hiring policies, and disciplinary policies.\n\nAdministrative controls form the basis for the selection and implementation of logical and physical controls. Logical and physical controls are manifestations of administrative controls, which are of paramount importance.\n\nLogical controls (also called technical controls) use software and data to monitor and control access to information and computing systems. Passwords, network and host-based firewalls, network intrusion detection systems, access control lists, and data encryption are examples of logical controls.\n\nAn important logical control that is frequently overlooked is the principle of least privilege, which requires that an individual, program or system process not be granted any more access privileges than are necessary to perform the task. A blatant example of the failure to adhere to the principle of least privilege is logging into Windows as user Administrator to read email and surf the web. Violations of this principle can also occur when an individual collects additional access privileges over time. This happens when employees' job duties change, employees are promoted to a new position, or employees are transferred to another department. The access privileges required by their new duties are frequently added onto their already existing access privileges, which may no longer be necessary or appropriate.\n\nPhysical controls monitor and control the environment of the work place and computing facilities. They also monitor and control access to and from such facilities and include doors, locks, heating and air conditioning, smoke and fire alarms, fire suppression systems, cameras, barricades, fencing, security guards, cable locks, etc. Separating the network and workplace into functional areas are also physical controls.\n\nAn important physical control that is frequently overlooked is separation of duties, which ensures that an individual can not complete a critical task by himself. For example, an employee who submits a request for reimbursement should not also be able to authorize payment or print the check. An applications programmer should not also be the server administrator or the database administrator; these roles and responsibilities must be separated from one another.\n\nInformation security must protect information throughout its lifespan, from the initial creation of the information on through to the final disposal of the information. The information must be protected while in motion and while at rest. During its lifetime, information may pass through many different information processing systems and through many different parts of information processing systems. There are many different ways the information and information systems can be threatened. To fully protect the information during its lifetime, each component of the information processing system must have its own protection mechanisms. The building up, layering on and overlapping of security measures is called \"defense in depth.\" In contrast to a metal chain, which is famously only as strong as its weakest link, the defense in depth strategy aims at a structure where, should one defensive measure fail, other measures will continue to provide protection.\n\nRecall the earlier discussion about administrative controls, logical controls, and physical controls. The three types of controls can be used to form the basis upon which to build a defense in depth strategy. With this approach, defense in depth can be conceptualized as three distinct layers or planes laid one on top of the other. Additional insight into defense in depth can be gained by thinking of it as forming the layers of an onion, with data at the core of the onion, people the next outer layer of the onion, and network security, host-based security and application security forming the outermost layers of the onion. Both perspectives are equally valid, and each provides valuable insight into the implementation of a good defense in depth strategy.\n\nAn important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information. Not all information is equal and so not all information requires the same degree of protection. This requires information to be assigned a security classification. The first step in information classification is to identify a member of senior management as the owner of the particular information to be classified. Next, develop a classification policy. The policy should describe the different classification labels, define the criteria for information to be assigned a particular label, and list the required security controls for each classification.\n\nSome factors that influence which classification information should be assigned include how much value that information has to the organization, how old the information is and whether or not the information has become obsolete. Laws and other regulatory requirements are also important considerations when classifying information. The Information Systems Audit and Control Association (ISACA) and its \"Business Model for Information Security\" also serves as a tool for security professionals to examine security from a systems perspective, creating an environment where security can be managed holistically, allowing actual risks to be addressed.\n\nThe type of information security classification labels selected and used will depend on the nature of the organization, with examples being:\n\nAll employees in the organization, as well as business partners, must be trained on the classification schema and understand the required security controls and handling procedures for each classification. The classification of a particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures.\n\nAccess to protected information must be restricted to people who are authorized to access the information. The computer programs, and in many cases the computers that process the information, must also be authorized. This requires that mechanisms be in place to control the access to protected information. The sophistication of the access control mechanisms should be in parity with the value of the information being protected; the more sensitive or valuable the information the stronger the control mechanisms need to be. The foundation on which access control mechanisms are built start with identification and authentication.\n\nAccess control is generally considered in three steps: identification, authentication, and authorization.\n\nIdentification is an assertion of who someone is or what something is. If a person makes the statement \"Hello, my name is John Doe\" they are making a claim of who they are. However, their claim may or may not be true. Before John Doe can be granted access to protected information it will be necessary to verify that the person claiming to be John Doe really is John Doe. Typically the claim is in the form of a username. By entering that username you are claiming \"I am the person the username belongs to\".\n\nAuthentication is the act of verifying a claim of identity. When John Doe goes into a bank to make a withdrawal, he tells the bank teller he is John Doe, a claim of identity. The bank teller asks to see a photo ID, so he hands the teller his driver's license. The bank teller checks the license to make sure it has John Doe printed on it and compares the photograph on the license against the person claiming to be John Doe. If the photo and name match the person, then the teller has authenticated that John Doe is who he claimed to be. Similarly, by entering the correct password, the user is providing evidence that he/she is the person the username belongs to.\n\nThere are three different types of information that can be used for authentication:\n\nStrong authentication requires providing more than one type of authentication information (two-factor authentication). The username is the most common form of identification on computer systems today and the password is the most common form of authentication. Usernames and passwords have served their purpose, but they are increasingly inadequate. Usernames and passwords are slowly being replaced or supplemented with more sophisticated authentication mechanisms such as Time-based One-time Password algorithms.\n\nAfter a person, program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform (run, view, create, delete, or change). This is called authorization. Authorization to access information and other computing services begins with administrative policies and procedures. The policies prescribe what information and computing services can be accessed, by whom, and under what conditions. The access control mechanisms are then configured to enforce these policies. Different computing systems are equipped with different kinds of access control mechanisms. Some may even offer a choice of different access control mechanisms. The access control mechanism a system offers will be based upon one of three approaches to access control, or it may be derived from a combination of the three approaches.\n\nThe non-discretionary approach consolidates all access control under a centralized administration. The access to information and other resources is usually based on the individuals function (role) in the organization or the tasks the individual must perform. The discretionary approach gives the creator or owner of the information resource the ability to control access to those resources. In the mandatory access control approach, access is granted or denied basing upon the security classification assigned to the information resource.\n\nExamples of common access control mechanisms in use today include role-based access control, available in many advanced database management systems; simple file permissions provided in the UNIX and Windows operating systems; Group Policy Objects provided in Windows network systems; and Kerberos, RADIUS, TACACS, and the simple access lists used in many firewalls and routers.\n\nTo be effective, policies and other security controls must be enforceable and upheld. Effective policies ensure that people are held accountable for their actions. The U.S. Treasury's guidelines for systems processing sensitive or proprietary information, for example, states that all failed and successful authentication and access attempts must be logged, and all access to information must leave some type of audit trail.\n\nAlso, the need-to-know principle needs to be in effect when talking about access control. This principle gives access rights to a person to perform their job functions. This principle is used in the government when dealing with difference clearances. Even though two employees in different departments have a top-secret clearance, they must have a need-to-know in order for information to be exchanged. Within the need-to-know principle, network administrators grant the employee the least amount of privileges to prevent employees from accessing more than what they are supposed to. Need-to-know helps to enforce the confidentiality-integrity-availability triad. Need-to-know directly impacts the confidential area of the triad.\n\nInformation security uses cryptography to transform usable information into a form that renders it unusable by anyone other than an authorized user; this process is called encryption. Information that has been encrypted (rendered unusable) can be transformed back into its original usable form by an authorized user who possesses the cryptographic key, through the process of decryption. Cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit (either electronically or physically) and while information is in storage.\n\nCryptography provides information security with other useful applications as well, including improved authentication methods, message digests, digital signatures, non-repudiation, and encrypted network communications. Older, less secure applications such as Telnet and File Transfer Protocol (FTP) are slowly being replaced with more secure applications such as Secure Shell (SSH) that use encrypted network communications. Wireless communications can be encrypted using protocols such as WPA/WPA2 or the older (and less secure) WEP. Wired communications (such as ITU‑T G.hn) are secured using AES for encryption and X.1035 for authentication and key exchange. Software applications such as GnuPG or PGP can be used to encrypt data files and email.\n\nCryptography can introduce security problems when it is not implemented correctly. Cryptographic solutions need to be implemented using industry-accepted solutions that have undergone rigorous peer review by independent experts in cryptography. The length and strength of the encryption key is also an important consideration. A key that is weak or too short will produce weak encryption. The keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information. They must be protected from unauthorized disclosure and destruction and they must be available when needed. Public key infrastructure (PKI) solutions address many of the problems that surround key management.\n\nThe terms \"reasonable and prudent person,\" \"due care\" and \"due diligence\" have been used in the fields of finance, securities, and law for many years. In recent years these terms have found their way into the fields of computing and information security. U.S. Federal Sentencing Guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems.\n\nIn the business world, stockholders, customers, business partners and governments have the expectation that corporate officers will run the business in accordance with accepted business practices and in compliance with laws and other regulatory requirements. This is often described as the \"reasonable and prudent person\" rule. A prudent person takes due care to ensure that everything necessary is done to operate the business by sound business principles and in a legal ethical manner. A prudent person is also diligent (mindful, attentive, and ongoing) in their due care of the business.\n\nIn the field of information security, Harris\noffers the following definitions of due care and due diligence:\n\n\"Due care are steps that are taken to show that a company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company, its resources, and employees.\" And, <nowiki>[Due diligence are the]</nowiki> \"continual activities that make sure the protection mechanisms are continually maintained and operational.\"\nAttention should be made to two important points in these definitions. First, in due care, steps are taken to show; this means that the steps can be verified, measured, or even produce tangible artifacts. Second, in due diligence, there are continual activities; this means that people are actually doing things to monitor and maintain the protection mechanisms, and these activities are ongoing.\n\nOrganizations have a responsibility with practicing duty of care when applying information security. The Duty of Care Risk Analysis Standard (DoCRA) provides principles and practices for evaluating risk. It considers all parties that could be affected by those risks. DoCRA helps evaluate safeguards if they are appropriate in protecting others from harm while presenting a reasonable burden. With increased data breach litigation, companies must balance security controls, compliance, and its mission.\n\nThe Software Engineering Institute at Carnegie Mellon University, in a publication titled \"Governing for Enterprise Security (GES) Implementation Guide\", defines characteristics of effective security governance. These include:\n\nAn incident response plan that addresses how discovered breaches in security is also vital. It should include:\n\nChange management is a formal process for directing and controlling alterations to the information processing environment. This includes alterations to desktop computers, the network, servers and software. The objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the processing environment as changes are made. It is not the objective of change management to prevent or hinder necessary changes from being implemented.\n\nAny change to the information processing environment introduces an element of risk. Even apparently simple changes can have unexpected effects. One of management's many responsibilities is the management of risk. Change management is a tool for managing the risks introduced by changes to the information processing environment. Part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented.\n\nNot every change needs to be managed. Some kinds of changes are a part of the everyday routine of information processing and adhere to a predefined procedure, which reduces the overall level of risk to the processing environment. Creating a new user account or deploying a new desktop computer are examples of changes that do not generally require change management. However, relocating user file shares, or upgrading the Email server pose a much higher level of risk to the processing environment and are not a normal everyday activity. The critical first steps in change management are (a) defining change (and communicating that definition) and (b) defining the scope of the change system.\n\nChange management is usually overseen by a change review board composed of representatives from key business areas, security, networking, systems administrators, database administration, application developers, desktop support and the help desk. The tasks of the change review board can be facilitated with the use of automated work flow application. The responsibility of the change review board is to ensure the organization's documented change management procedures are followed. The change management process is as follows\n\n\nChange management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment. Good change management procedures improve the overall quality and success of changes as they are implemented. This is accomplished through planning, peer review, documentation and communication.\n\nISO/IEC 20000, The Visible OPS Handbook: Implementing ITIL in 4 Practical and Auditable Steps (Full book summary), and Information Technology Infrastructure Library all provide valuable guidance on implementing an efficient and effective change management program information security.\n\nBusiness continuity management (BCM) concerns arrangements aiming to protect an organization's critical business functions from interruption due to incidents, or at least minimize the effects. BCM is essential to any organization to keep technology and business in line with current threats to the continuation of business as usual. The BCM should be included in an organizations risk analysis plan to ensure that all of the necessary business functions have what they need to keep going in the event of any type of threat to any business function.\n\nIt encompasses:\n\nWhereas BCM takes a broad approach to minimizing disaster-related risks by reducing both the probability and the severity of incidents, a disaster recovery plan (DRP) focuses specifically on resuming business operations as quickly as possible after a disaster. A disaster recovery plan, invoked soon after a disaster occurs, lays out the steps necessary to recover critical information and communications technology (ICT) infrastructure. Disaster recovery planning includes establishing a planning group, performing risk assessment, establishing priorities, developing recovery strategies, preparing inventories and documentation of the plan, developing verification criteria and procedure, and lastly implementing the plan.\n\nBelow is a partial listing of governmental laws and regulations in various parts of the world that have, had, or will have, a significant effect on data processing and information security. Important industry sector regulations have also been included when they have a significant impact on information security.\n\n\nEmployee behavior can have a big impact on information security in organizations. Cultural concepts can help different segments of the organization work effectively or work against effectiveness towards information security within an organization. \"Exploring the Relationship between Organizational Culture and Information Security Culture\" provides the following definition of information security culture: \"ISC is the totality of patterns of behavior in an organization that contribute to the protection of information of all kinds.\"\n\nAndersson and Reimers (2014) found that employees often do not see themselves as part of the organization Information Security \"effort\" and often take actions that ignore organizational information security best interests. Research shows information security culture needs to be improved continuously. In \"Information Security Culture from Analysis to Change\", authors commented, \"It's a never ending process, a cycle of evaluation and change or maintenance.\" To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.\n\n\nThe International Organization for Standardization (ISO) is a consortium of national standards institutes from 157 countries, coordinated through a secretariat in Geneva, Switzerland. ISO is the world's largest developer of standards. ISO 15443: \"Information technology – Security techniques – A framework for IT security assurance\", ISO/IEC 27002: \"Information technology – Security techniques – Code of practice for information security management\", ISO-20000: \"Information technology – Service management\", and ISO/IEC 27001: \"Information technology – Security techniques – Information security management systems – Requirements\" are of particular interest to information security professionals.\n\nThe US National Institute of Standards and Technology (NIST) is a non-regulatory federal agency within the U.S. Department of Commerce. The NIST Computer Security Division\ndevelops standards, metrics, tests and validation programs as well as publishes standards and guidelines to increase secure IT planning, implementation, management and operation. NIST is also the custodian of the U.S. Federal Information Processing Standard publications (FIPS).\n\nThe Internet Society is a professional membership society with more than 100 organizations and over 20,000 individual members in over 180 countries. It provides leadership in addressing issues that confront the future of the internet and is the organizational home for the groups responsible for internet infrastructure standards, including the Internet Engineering Task Force (IETF) and the Internet Architecture Board (IAB). The ISOC hosts the Requests for Comments (RFCs) which includes the Official Internet Protocol Standards and the RFC-2196 Site Security Handbook.\n\nThe Information Security Forum is a global nonprofit organization of several hundred leading organizations in financial services, manufacturing, telecommunications, consumer goods, government, and other areas. It undertakes research into information security practices and offers advice in its biannual Standard of Good Practice and more detailed advisories for members.\n\nThe Institute of Information Security Professionals (IISP) is an independent, non-profit body governed by its members, with the principal objective of advancing the professionalism of information security practitioners and thereby the professionalism of the industry as a whole. The institute developed the IISP Skills Framework. This framework describes the range of competencies expected of information security and information assurance professionals in the effective performance of their roles. It was developed through collaboration between both private and public sector organizations and world-renowned academics and security leaders.\n\nThe German Federal Office for Information Security (in German \"Bundesamt für Sicherheit in der Informationstechnik (BSI)\") BSI-Standards 100-1 to 100-4 are a set of recommendations including \"methods, processes, procedures, approaches and measures relating to information security\". The BSI-Standard 100-2 \"IT-Grundschutz Methodology\" describes how information security management can be implemented and operated. The standard includes a very specific guide, the IT Baseline Protection Catalogs (also known as IT-Grundschutz Catalogs). Before 2005, the catalogs were formerly known as \"IT Baseline Protection Manual\". The Catalogs are a collection of documents useful for detecting and combating security-relevant weak points in the IT environment (IT cluster). The collection encompasses as of September 2013 over 4,400 pages with the introduction and catalogs. The IT-Grundschutz approach is aligned with to the ISO/IEC 2700x family.\n\nThe European Telecommunications Standards Institute standardized a catalog of information security indicators, headed by the Industrial Specification Group (ISG) ISI.\n\n\n\n"}
{"id": "24845923", "url": "https://en.wikipedia.org/wiki?curid=24845923", "title": "Integral graph", "text": "Integral graph\n\nIn the mathematical field of graph theory, an integral graph is a graph whose adjacency matrix's spectrum consists entirely of integers. In other words, a graph is an integral graph if all of the roots of the characteristic polynomial of its adjacency matrix are integers.\n\nThe notion was introduced in 1974 by Harary and Schwenk.\n\n"}
{"id": "24153642", "url": "https://en.wikipedia.org/wiki?curid=24153642", "title": "Jucys–Murphy element", "text": "Jucys–Murphy element\n\nIn mathematics, the Jucys–Murphy elements in the group algebra formula_1 of the symmetric group, named after Algimantas Adolfas Jucys and G. E. Murphy, are defined as a sum of transpositions by the formula:\n\nThey play an important role in the representation theory of the symmetric group.\n\nThey generate a commutative subalgebra of formula_3. Moreover,\n\"X\" commutes with all elements of formula_4.\n\nThe vectors constituting the basis of Young's \"seminormal representation\" are eigenvectors for the action of \"X\". For any standard Young tableau \"U\" we have:\n\nwhere \"c\"(\"U\") is the \"content\" \"b\" − \"a\" of the cell (\"a\", \"b\") occupied by \"k\" in the standard Young tableau \"U\".\n\nTheorem (Jucys): The center formula_6 of the group algebra formula_1 of the symmetric group is generated by the symmetric polynomials in the elements \"X\".\n\nTheorem (Jucys): Let \"t\" be a formal variable commuting with everything, then the following identity for polynomials in variable \"t\" with values in the group algebra formula_1 holds true:\n\nTheorem (Okounkov–Vershik): The subalgebra of formula_1 generated by the centers\n\nis exactly the subalgebra generated by the Jucys–Murphy elements \"X\".\n\n"}
{"id": "16381455", "url": "https://en.wikipedia.org/wiki?curid=16381455", "title": "Lieb's square ice constant", "text": "Lieb's square ice constant\n\nLieb's square ice constant is a mathematical constant used in the field of combinatorics to quantify the number of Eulerian orientations of grid graphs. It was introduced by Elliott H. Lieb in 1967.\n\nAn \"n\" × \"n\" grid graph (with periodic boundary conditions and \"n\" ≥ 2) has \"n\" vertices and 2\"n\" edges; it is 4-regular, meaning that each vertex has exactly four neighbors. An orientation of this graph is an assignment of a direction to each edge; it is an Eulerian orientation if it gives each vertex exactly two incoming edges and exactly two outgoing edges. Denote the number of Eulerian orientations of this graph by \"f\"(\"n\"). Then\n\nis Lieb's square ice constant.\n\nThe same constant also quantifies in the same way the number of 3-colorings of grid graphs, and the number of local flat foldings of the Miura fold.\nSome historical and physical background can be found in the article Ice-type model.\n\n"}
{"id": "1168363", "url": "https://en.wikipedia.org/wiki?curid=1168363", "title": "List of mathematical knots and links", "text": "List of mathematical knots and links\n\nThis article contains a list of mathematical knots and links. See also list of knots, list of geometric topology topics.\n\n\n\n"}
{"id": "19167177", "url": "https://en.wikipedia.org/wiki?curid=19167177", "title": "Maximum-minimums identity", "text": "Maximum-minimums identity\n\nIn mathematics, the maximum-minimums identity is a relation between the maximum element of a set \"S\" of \"n\" numbers and the minima of the 2 − 1 nonempty subsets of \"S\".\n\nLet \"S\" = {\"x\", \"x\", ..., \"x\"}. The identity states that \n\nor conversely\n\nFor a probabilistic proof, see the reference.\n\n"}
{"id": "30353558", "url": "https://en.wikipedia.org/wiki?curid=30353558", "title": "NK model", "text": "NK model\n\nThe NK model is a mathematical model described by its primary inventor Stuart Kauffman as a \"tunably rugged\" fitness landscape. \"Tunable ruggedness\" captures the intuition that both the overall size of the landscape and the number of its local \"hills and valleys\" can be adjusted via changes to its two parameters, formula_1 and formula_2, defined below. The NK model has found application in a wide variety of fields, including the theoretical study of evolutionary biology, immunology, optimisation, technological evolution, and complex systems. The model was also adopted in organizational theory, where it is used to describe the way an agent may search a landscape by manipulating various characteristics of itself. For example, an agent can be an organization, the hills and valleys represent profit (or changes thereof), and movement on the landscape necessitates organizational decisions (such as adding product lines or altering the organizational structure), which tend to interact with each other and affect profit in a complex fashion. \n\nAn early version of the model, which considered only the smoothest (formula_3) and most rugged (formula_4) landscapes, was presented in Kauffman and Levin (1987). The model as it is currently known first appeared in Kauffman and Weinberger (1989).\n\nOne of the reasons why the model has attracted wide attention in optimisation is that it is a particularly simple instance of a so-called NP-complete problem.\n\nThe NK model defines a combinatorial phase space, consisting of every string (chosen from a given alphabet) of length formula_1. For each string in this search space, a scalar value (called the \"fitness\") is defined. If a distance metric is defined between strings, the resulting structure is a \"landscape\".\n\nFitness values are defined according to the specific incarnation of the model, but the key feature of the NK model is that the fitness of a given string formula_6 is the sum of contributions from each locus formula_7 in the string:\n\nand the contribution from each locus in general depends on its state and the state of formula_2 other loci,:\n\nwhere formula_11 is the index of the formula_12th neighbor of locus formula_13. \n\nHence, the fitness function formula_14 is a mapping between strings of length \"K\" + 1 and scalars, which Weinberger's later work calls \"fitness contributions\". Such fitness contributions are often chosen randomly from some specified probability distribution.\n\nIn 1991, Weinberger published a detailed analysis of the case in which formula_15 and the fitness contributions are chosen randomly. His analytical estimate of the number of local optima was later shown to be flawed . However, numerical experiments included in Weinberger's analysis support his analytical result that the expected fitness of a string is normally distributed with a mean of approximately\n\nformula_16\n\nand a variance of approximately\n\nformula_17.\n\nFor simplicity, we will work with binary strings. Consider an NK model with \"N\" = 5, \"K\" = 1. Here, the fitness of a string is given by the sum of individual fitness contributions from each of 5 loci. Each fitness contribution depends on the local locus value and one other. We will employ the convention that formula_18, so that each locus is affected by its neighbour, and formula_19 for cyclicity. If we choose, for example, the fitness function \"f\"(0, 0) = 0; \"f\"(0, 1) = 1; \"f\"(1, 0) = 2; \"f\"(1, 1) = 0, the fitness values of two example strings are:\n\nThe value of \"K\" controls the degree of epistasis in the NK model, or how much other loci affect the fitness contribution of a given locus. With \"K\" = 0, the fitness of a given string is a simple sum of individual contributions of loci: for nontrivial fitness functions, a global optimum is present and easy to locate (the genome of all 0s if \"f\"(0) > \"f\"(1), or all 1s if \"f\"(1) > \"f\"(0)). For nonzero \"K\", the fitness of a string is a sum of fitnesses of substrings, which may interact to frustrate the system (consider how to achieve optimal fitness in the example above). Increasing \"K\" thus increases the ruggedness of the fitness landscape.\n\nThe bare NK model does not support the phenomenon of \"neutral space\" -- that is, sets of genomes connected by single mutations that have the same fitness value. Two adaptations have been proposed to include this biologically important structure. The \"NKP model\" introduces a parameter formula_22: a proportion formula_22 of the formula_24 fitness contributions is set to zero, so that the contributions of several genetic motifs are degenerate . The \"NKQ model\" introduces a parameter formula_25 and enforces a discretisation on the possible fitness contribution values so that each contribution takes one of formula_25 possible values, again introducing degeneracy in the contributions from some genetic motifs . The bare NK model corresponds to the formula_27 and formula_28 cases under these parameterisations.\n\nThe NK model has found use in many fields, including in the study of spin glasses, epistasis and pleiotropy in evolutionary biology, and combinatorial optimisation.\n"}
{"id": "1178373", "url": "https://en.wikipedia.org/wiki?curid=1178373", "title": "Non-classical analysis", "text": "Non-classical analysis\n\nIn mathematics, non-classical analysis is any system of analysis, other than classical real analysis, and complex, vector, tensor, etc., analysis based upon it.\n\nSuch systems include:\n\n\nNon-standard analysis and the calculus it involves, non-standard calculus, are considered part of classical mathematics (i.e. The concept of \"hyperreal number\" it uses, can be constructed in the framework of Zermelo–Fraenkel set theory).\n\nNon-Newtonian calculus is also a part of classical mathematics.\n"}
{"id": "4742308", "url": "https://en.wikipedia.org/wiki?curid=4742308", "title": "Phase boundary", "text": "Phase boundary\n\nIn thermal equilibrium, each phase (i.e. liquid, solid etc.) of physical matter comes to an end at a transitional point, or spatial interface, called a phase boundary, due to the immiscibility of the matter with the matter on the other side of the boundary. This immiscibility is due to at least one difference between the two substances' corresponding physical properties. The behavior of phase boundaries has been a developing subject of interest and an active research field, called interface science, in physics and mathematics for almost two centuries, due partly to phase boundaries naturally arising in many physical processes, such as the capillarity effect, the growth of grain boundaries, the physics of binary alloys, and the formation of snow flakes. \n\nOne of the oldest problems in the area dates back to Lamé and Clapeyron who studied the freezing of the ground. Their goal was to determine the thickness of solid crust generated by the cooling of a liquid at constant temperature filling the half-space. In 1889, Stefan, while working on the freezing of the ground developed these ideas further and formulated the two-phase model which came to be known as the Stefan Problem.\n\nThe proof of existence and uniqueness of a solution to the Stefan problem was done in many stages. Proving the general existence and uniqueness of the solutions in the case of formula_1 was solved by Shoshana Kamin.\n"}
{"id": "23855903", "url": "https://en.wikipedia.org/wiki?curid=23855903", "title": "Quantum ergodicity", "text": "Quantum ergodicity\n\nIn quantum chaos, a branch of mathematical physics, quantum ergodicity is a property of the quantization of classical mechanical systems that are chaotic in the sense of exponential sensitivity to initial conditions. Quantum ergodicity states, roughly, that in the high-energy limit, the probability distributions associated to energy eigenstates of a quantized ergodic Hamiltonian tend to a uniform distribution in the classical phase space. This is consistent with the intuition that the flows of ergodic systems are equidistributed in phase space. By contrast, classical completely integrable systems generally have periodic orbits in phase space, and this is exhibited in a variety of ways in the high-energy limit of the eigenstates: typically that some form of concentration or \"scarring\" occurs in the limit.\n\nThe model case of a Hamiltonian is the geodesic Hamiltonian on the cotangent bundle of a compact Riemannian manifold. The quantization of the geodesic flow is given by the fundamental solution of the Schrödinger equation\nwhere formula_2 is the square root of the Laplace-Beltrami operator. The quantum ergodicity theorem of Shnirelman, Yves Colin de Verdière, and Zelditch states that a compact Riemannian manifold whose unit tangent bundle is ergodic under the geodesic flow is also ergodic in the sense that the probability density associated to the \"n\"th eigenfunction of the Laplacian tends weakly to the uniform distribution on the unit cotangent bundle as \"n\" → ∞ in a subset of the natural numbers of natural density equal to one. Quantum ergodicity can be formulated as a non-commutative analogue of the classical ergodicity (T. Sunada).\n\n\n"}
{"id": "54452801", "url": "https://en.wikipedia.org/wiki?curid=54452801", "title": "Quantum supremacy", "text": "Quantum supremacy\n\nQuantum supremacy or quantum advantage is the potential ability of quantum computing devices to solve problems that classical computers practically cannot. In computational complexity-theoretic terms, this generally means providing a superpolynomial speedup over the best known or possible classical algorithm. The term was originally popularized by John Preskill but the concept of a quantum computational advantage, specifically for simulating quantum systems, dates back to Yuri Manin's (1980)\nand Richard Feynman's (1981) proposals of quantum computing.\n\nShor's algorithm for factoring integers, which runs in polynomial time on a quantum computer, provides such a superpolynomial speedup over the best known classical algorithm. Although it is yet to be proved, factoring is generally believed to be hard using classical resources. The difficulty of proving what cannot be done with classical computing is a common problem in definitively demonstrating quantum supremacy. It also affects the boson sampling proposal of Aaronson and Arkhipov, D-Wave's specialized frustrated cluster loop problems, and sampling the output of random quantum circuits.\n\nLike factoring integers, sampling the output distributions of random quantum circuits is believed to be hard for classical computers based on reasonable complexity assumptions. Google previously announced plans to demonstrate quantum supremacy before the end of 2017 by solving this problem with an array of 49 superconducting qubits. However, as of early January 2018, only Intel has announced such hardware. In October 2017, IBM demonstrated the simulation of 56 qubits on a conventional supercomputer, increasing the number of qubits needed for quantum supremacy. In November of 2018, Google announced a partnership with NASA that would “analyze results from quantum circuits run on Google quantum processors, and ... provide comparisons with classical simulation to both support Google in validating its hardware and establish a baseline for quantum supremacy.”\n\nComplexity arguments concern how the amount of some resource needed to solve a problem scales with the size of the input to that problem. As an extension of classical computational complexity theory, quantum complexity theory is about what a working, universal quantum computer could accomplish without necessarily accounting for the difficulty of building one or dealing with decoherence and noise. Since quantum information is a generalization of classical information, it is clear that a quantum computer can efficiently simulate any classical algorithm.\n\nBounded quantum polynomial (BQP) is the class of decision problems that can be solved in polynomial time by a universal quantum computer. It is related to important classical complexity classes by the hierarchyformula_1. Whether any of these containments is proper is still an open question.\n\nContrary to decision problems that require yes or no answers, sampling problems ask for samples from probability distributions. If there is a classical algorithm that can efficiently sample from the output of an arbitrary quantum circuit, the polynomial hierarchy would collapse to the third level, which is considered very unlikely. BosonSampling is a more specific proposal, the classical hardness of which depends upon the intractability of calculating the permanent of a large matrix with complex entries, which is a P#-complete problem. The arguments used to reach this conclusion have also been extended to IQP Sampling, where only the conjecture that the average- and worst-case complexities of the problem are the same is needed.\n\nThe following algorithms provide superpolynomial speedups over the best known classical algorithms:\n\nThis algorithm finds the prime factorization of an \"n\"-bit integer in formula_2 time whereas the best known classical algorithm requires formula_3time and the best upper bound for the complexity of this problem is formula_4. It can also provide a speedup for any problem that reduces to integer factoring, including the membership problem for matrix groups over fields of odd order.\n\nThis algorithm is important both practically and historically for quantum computing. It was the first polynomial-time quantum algorithm proposed for a problem that is believed to be hard for classical computers. This hardness belief is so strong that the security of today's most common encryption protocol, RSA, is based upon it. A quantum computer successfully and repeatably running this algorithm has the potential to break this encryption system. The need to avoid the imminence of this risk is referred to by the term Y2Q.\n\nThis computing paradigm based upon identical photons sent through a linear-optical network can solve certain sampling and search problems that, assuming a few conjectures, are intractable for classical computers. However, it has been shown that BosonSampling in a system with large enough loss and noise can be simulated efficiently.\n\nThe largest experimental implementation of BosonSampling to date had 6 modes so could handle up to 6 photons at a time. The best proposed classical algorithm for simulating BosonSampling runs in time formula_5 for a system with \"n\" photons and \"m\" output modes. BosonSampling is an open-source implementation in R. The algorithm leads to an estimate of 50 photons required to demonstrate quantum supremacy with BosonSampling.\n\nThe best known algorithm for simulating an arbitrary random quantum circuit requires an amount of time that scales exponentially with the number of qubits, leading one group to estimate that around 50 qubits could be enough to demonstrate quantum supremacy. Google had announced its intention to demonstrate quantum supremacy by the end of 2017 by constructing and running a 49-qubit chip that will be able to sample distributions inaccessible to any current classical computers in a reasonable amount of time. But the largest quantum circuit simulation completed successfully on a supercomputer now contains 56 qubits. This may require increasing the number of qubits to demonstrate quantum supremacy.\n\nQuantum computers are much more susceptible to errors than classical computers due to decoherence and noise. The threshold theorem states that a noisy quantum computer can use quantum error-correcting codes to simulate a noiseless quantum computer assuming the error introduced in each computer cycle is less than some number. Numerical simulations suggest that that number may be as high as 3%.\n\nHowever, it is not known how the resources needed for error correction will scale with the number of qubits. Skeptics point to the unknown behavior of noise in scaled-up quantum systems as potential roadblocks for successfully implementing quantum computing and demonstrating quantum supremacy.\n\n"}
{"id": "42011185", "url": "https://en.wikipedia.org/wiki?curid=42011185", "title": "Quotient type", "text": "Quotient type\n\nIn type theory, a kind of foundation of mathematics, a quotient type is an algebraic data type that represents a type whose equality relation has been redefined by a given equivalence relation such that the elements of the type are partitioned into a set of equivalence classes whose cardinality is less than or equal to that of the base type. Just as product types and sum types are analogous to the cartesian product and disjoint sum of abstract algebraic structures, quotient types reflect the concept of set-theoretic quotients, sets whose elements are surjectively partitioned into equivalence classes by a given equivalence relation on the set. Algebraic structures whose underlying set is a quotient are also termed quotients. Examples of such quotient structures include quotient sets, groups, rings, categories and, in topology, quotient spaces. For example, formula_1, the rational numbers, is the quotient ring – or \"field of fractions\" – of formula_2, the integers.\n\nIn type theories that lack quotient types, setoids – sets explicitly equipped with an equivalence relation – are often used instead.\n\n"}
{"id": "26446", "url": "https://en.wikipedia.org/wiki?curid=26446", "title": "Recreational mathematics", "text": "Recreational mathematics\n\nRecreational mathematics is mathematics carried out for recreation (entertainment) rather than as a strictly research and application-based professional activity. Although it is not necessarily limited to being an endeavor for amateurs, many topics in this field require no knowledge of advanced mathematics. Recreational mathematics involves mathematical puzzles and games, often appealing to children and untrained adults, inspiring their further study of the subject.\n\nThe Mathematical Association of America (MAA) includes Recreational Mathematics as one of its seventeen Special Interest Groups, commenting:\n\nMathematical competitions (such as those sponsored by mathematical associations) are also categorized under recreational mathematics.\n\nSome of the more well-known topics in recreational mathematics are Rubik's Cubes, magic squares, fractals, logic puzzles and mathematical chess problems, but this area of mathematics includes the aesthetics and culture of mathematics, peculiar or amusing stories and coincidences about mathematics, and the personal lives of mathematicians.\n\nMathematical games are multiplayer games whose rules, strategies, and outcomes can be studied and explained using mathematics. The players of the game may not need to use explicit mathematics in order to play mathematical games. For example, Mancala is studied in the mathematical field of combinatorial game theory, but no mathematics is necessary in order to play it.\n\nMathematical puzzles require mathematics in order to solve them. They have specific rules, as do multiplayer games, but mathematical puzzles don't usually involve competition between two or more players. Instead, in order to solve such a puzzle, the solver must find a solution that satisfies the given conditions.\n\nLogic puzzles and classical ciphers are common examples of mathematical puzzles. Cellular automata and fractals are also considered mathematical puzzles, even though the solver only interacts with them by providing a set of initial conditions.\n\nAs they often include or require game-like features or thinking, mathematical puzzles are sometimes also called mathematical games.\n\nOther curiosities and pastimes of non-trivial mathematical interest include:\n\n\n\nProminent practitioners and advocates of recreational mathematics have included:\n\n\n\n"}
{"id": "7375449", "url": "https://en.wikipedia.org/wiki?curid=7375449", "title": "Rough number", "text": "Rough number\n\nA \"k\"-rough number, as defined by Finch in 2001 and 2003, is a positive integer whose prime factors are all greater than or equal to \"k\". \"k\"-roughness has alternately been defined as requiring all prime factors to strictly exceed \"k\".\n\n\n\n\nThe On-Line Encyclopedia of Integer Sequences (OEIS)\nlists \"p\"-rough numbers for small \"p\":\n"}
{"id": "1634778", "url": "https://en.wikipedia.org/wiki?curid=1634778", "title": "Rough set", "text": "Rough set\n\nIn computer science, a rough set, first described by Polish computer scientist Zdzisław I. Pawlak, is a formal approximation of a crisp set (i.e., conventional set) in terms of a pair of sets which give the \"lower\" and the \"upper\" approximation of the original set. In the standard version of rough set theory (Pawlak 1991), the lower- and upper-approximation sets are crisp sets, but in other variations, the approximating sets may be fuzzy sets.\n\nThe following section contains an overview of the basic framework of rough set theory, as originally proposed by Zdzisław I. Pawlak, along with some of the key definitions. More formal properties and boundaries of rough sets can be found in Pawlak (1991) and cited references. The initial and basic theory of rough sets is sometimes referred to as \"Pawlak Rough Sets\" or \"classical rough sets\", as a means to distinguish from more recent extensions and generalizations.\n\nLet formula_1 be an information system (attribute-value system), where formula_2 is a non-empty, finite set of objects (the universe) and formula_3 is a non-empty, finite set of attributes such that formula_4 for every formula_5. formula_6 is the set of values that attribute formula_7 may take. The information table assigns a value formula_8 from formula_6 to each attribute formula_7 and object formula_11 in the universe formula_12.\n\nWith any formula_13 there is an associated equivalence relation formula_14:\n\nThe relation formula_14 is called a formula_17\"-indiscernibility relation\". The partition of formula_12 is a family of all equivalence classes of formula_14 and is denoted by formula_20 (or formula_21).\n\nIf formula_22, then formula_11 and formula_24 are \"indiscernible\" (or indistinguishable) by attributes from formula_17 .\n\nThe equivalence classes of the formula_17-indiscernibility relation are denoted formula_27.\n\nFor example, consider the following information table:\n\nWhen the full set of attributes formula_28 is considered, we see that we have the following seven equivalence classes:\n\nThus, the two objects within the first equivalence class, formula_30, cannot be distinguished from each other based on the available attributes, and the three objects within the second equivalence class, formula_31, cannot be distinguished from one another based on the available attributes. The remaining five objects are each discernible from all other objects.\n\nIt is apparent that different attribute subset selections will in general lead to different indiscernibility classes. For example, if attribute formula_32 alone is selected, we obtain the following, much coarser, equivalence-class structure:\n\nLet formula_34 be a target set that we wish to represent using attribute subset formula_17; that is, we are told that an arbitrary set of objects formula_36 comprises a single class, and we wish to express this class (i.e., this subset) using the equivalence classes induced by attribute subset formula_17. In general, formula_36 cannot be expressed exactly, because the set may include and exclude objects which are indistinguishable on the basis of attributes formula_17.\n\nFor example, consider the target set formula_40, and let attribute subset formula_41, the full available set of features. It will be noted that the set formula_36 cannot be expressed exactly, because in formula_43, objects formula_44 are indiscernible. Thus, there is no way to represent any set formula_36 which \"includes\" formula_46 but \"excludes\" objects formula_47 and formula_48.\n\nHowever, the target set formula_36 can be \"approximated\" using only the information contained within formula_17 by constructing the formula_17-lower and formula_17-upper approximations of formula_36:\n\nThe formula_17\"-lower approximation\", or \"positive region\", is the union of all equivalence classes in formula_27 which are contained by (i.e., are subsets of) the target set – in the example, formula_58, the union of the two equivalence classes in formula_27 which are contained in the target set. The lower approximation is the complete set of objects in formula_21 that can be \"positively\" (i.e., unambiguously) classified as belonging to target set formula_36.\n\nThe formula_17\"-upper approximation\" is the union of all equivalence classes in formula_27 which have non-empty intersection with the target set – in the example, formula_64, the union of the three equivalence classes in formula_27 that have non-empty intersection with the target set. The upper approximation is the complete set of objects that in formula_21 that \"cannot\" be positively (i.e., unambiguously) classified as belonging to the \"complement\" (formula_67) of the target set formula_36. In other words, the upper approximation is the complete set of objects that are \"possibly\" members of the target set formula_36.\n\nThe set formula_70 therefore represents the \"negative region\", containing the set of objects that can be definitely ruled out as members of the target set.\n\nThe \"boundary region\", given by set difference formula_71, consists of those objects that can neither be ruled in nor ruled out as members of the target set formula_36.\n\nIn summary, the lower approximation of a target set is a \"conservative\" approximation consisting of only those objects which can positively be identified as members of the set. (These objects have no indiscernible \"clones\" which are excluded by the target set.) The upper approximation is a \"liberal\" approximation which includes all objects that might be members of target set. (Some objects in the upper approximation may not be members of the target set.) From the perspective of formula_21, the lower approximation contains objects that are members of the target set with certainty (probability = 1), while the upper approximation contains objects that are members of the target set with non-zero probability (probability > 0).\n\nThe tuple formula_74 composed of the lower and upper approximation is called a \"rough set\"; thus, a rough set is composed of two crisp sets, one representing a \"lower boundary\" of the target set formula_36, and the other representing an \"upper boundary\" of the target set formula_36.\n\nThe \"accuracy\" of the rough-set representation of the set formula_36 can be given (Pawlak 1991) by the following:\n\nThat is, the accuracy of the rough set representation of formula_36, formula_80, formula_81, is the ratio of the number of objects which can \"positively\" be placed in formula_36 to the number of objects that can \"possibly\" be placed in formula_36 – this provides a measure of how closely the rough set is approximating the target set. Clearly, when the upper and lower approximations are equal (i.e., boundary region empty), then formula_84, and the approximation is perfect; at the other extreme, whenever the lower approximation is empty, the accuracy is zero (regardless of the size of the upper approximation).\n\nRough set theory is one of many methods that can be employed to analyse uncertain (including vague) systems, although less common than more traditional methods of probability, statistics, entropy and Dempster–Shafer theory. However a key difference, and a unique strength, of using classical rough set theory is that it provides an objective form of analysis (Pawlak et al. 1995). Unlike other methods, as those given above, classical rough set analysis requires no additional information, external parameters, models, functions, grades or subjective interpretations to determine set membership – instead it only uses the information presented within the given data (Düntsch and Gediga 1995). More recent adaptations of rough set theory, such as dominance-based, decision-theoretic and fuzzy rough sets, have introduced more subjectivity to the analysis.\n\nIn general, the upper and lower approximations are not equal; in such cases, we say that target set formula_36 is \"undefinable\" or \"roughly definable\" on attribute set formula_17. When the upper and lower approximations are equal (i.e., the boundary is empty), formula_87, then the target set formula_36 is \"definable\" on attribute set formula_17. We can distinguish the following special cases of undefinability:\n\n\nAn interesting question is whether there are attributes in the information system (attribute-value table) which are more important to the knowledge represented in the equivalence class structure than other attributes. Often, we wonder whether there is a subset of attributes which can, by itself, fully characterize the knowledge in the database; such an attribute set is called a \"reduct\".\n\nFormally, a reduct is a subset of attributes formula_110 such that\n\n\nA reduct can be thought of as a \"sufficient\" set of features – sufficient, that is, to represent the category structure. In the example table above, attribute set formula_120 is a reduct – the information system projected on just these attributes possesses the same equivalence class structure as that expressed by the full attribute set:\n\nAttribute set formula_120 is a reduct because eliminating any of these attributes causes a collapse of the equivalence-class structure, with the result that formula_123.\n\nThe reduct of an information system is \"not unique\": there may be many subsets of attributes which preserve the equivalence-class structure (i.e., the knowledge) expressed in the information system. In the example information system above, another reduct is formula_124, producing the same equivalence-class structure as formula_27.\n\nThe set of attributes which is common to all reducts is called the \"core\": the core is the set of attributes which is possessed by \"every\" reduct, and therefore consists of attributes which cannot be removed from the information system without causing collapse of the equivalence-class structure. The core may be thought of as the set of \"necessary\" attributes – necessary, that is, for the category structure to be represented. In the example, the only such attribute is formula_126; any one of the other attributes can be removed singly without damaging the equivalence-class structure, and hence these are all \"dispensable\". However, removing formula_126 by itself \"does\" change the equivalence-class structure, and thus formula_126 is the \"indispensable\" attribute of this information system, and hence the core.\n\nIt is possible for the core to be empty, which means that there is no indispensable attribute: any single attribute in such an information system can be deleted without altering the equivalence-class structure. In such cases, there is no \"essential\" or necessary attribute which is required for the class structure to be represented.\n\nOne of the most important aspects of database analysis or data acquisition is the discovery of attribute dependencies; that is, we wish to discover which variables are strongly related to which other variables. Generally, it is these strong relationships that will warrant further investigation, and that will ultimately be of use in predictive modeling.\n\nIn rough set theory, the notion of dependency is defined very simply. Let us take two (disjoint) sets of attributes, set formula_17 and set formula_130, and inquire what degree of dependency obtains between them. Each attribute set induces an (indiscernibility) equivalence class structure, the equivalence classes induced by formula_17 given by formula_27, and the equivalence classes induced by formula_130 given by formula_134.\n\nLet formula_135, where formula_136 is a given equivalence class from the equivalence-class structure induced by attribute set formula_130. Then, the \"dependency\" of attribute set formula_130 on attribute set formula_17, formula_140, is given by\n\nThat is, for each equivalence class formula_136 in formula_134, we add up the size of its lower approximation by the attributes in formula_17, i.e., formula_145. This approximation (as above, for arbitrary set formula_36) is the number of objects which on attribute set formula_17 can be positively identified as belonging to target set formula_136. Added across all equivalence classes in formula_134, the numerator above represents the total number of objects which – based on attribute set formula_17 – can be positively categorized according to the classification induced by attributes formula_130. The dependency ratio therefore expresses the proportion (within the entire universe) of such classifiable objects. The dependency formula_140 \"can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in formula_17 to determine the values of attributes in formula_130\".\n\nAnother, intuitive, way to consider dependency is to take the partition induced by Q as the target class C, and consider P as the attribute set we wish to use in order to \"re-construct\" the target class C. If P can completely reconstruct C, then Q depends totally upon P; if P results in a poor and perhaps a random reconstruction of C, then Q does not depend upon P at all.\n\nThus, this measure of dependency expresses the degree of \"functional\" (i.e., deterministic) dependency of attribute set formula_130 on attribute set formula_17; it is \"not\" symmetric. The relationship of this notion of attribute dependency to more traditional information-theoretic (i.e., entropic) notions of attribute dependence has been discussed in a number of sources (e.g., Pawlak, Wong, & Ziarko 1988; Yao & Yao 2002; Wong, Ziarko, & Ye 1986, Quafafou & Boussouf 2000).\n\nThe category representations discussed above are all \"extensional\" in nature; that is, a category or complex class is simply the sum of all its members. To represent a category is, then, just to be able to list or identify all the objects belonging to that category. However, extensional category representations have very limited practical use, because they provide no insight for deciding whether novel (never-before-seen) objects are members of the category.\n\nWhat is generally desired is an \"intentional\" description of the category, a representation of the category based on a set of \"rules\" that describe the scope of the category. The choice of such rules is not unique, and therein lies the issue of inductive bias. See Version space and Model selection for more about this issue.\n\nThere are a few rule-extraction methods. We will start from a rule-extraction procedure based on Ziarko & Shan (1995).\n\nLet us say that we wish to find the minimal set of consistent rules (logical implications) that characterize our sample system. For a set of \"condition\" attributes formula_157 and a decision attribute formula_158, these rules should have the form formula_159, or, spelled out,\n\nwhere formula_161 are legitimate values from the domains of their respective attributes. This is a form typical of association rules, and the number of items in formula_12 which match the condition/antecedent is called the \"support\" for the rule. The method for extracting such rules given in is to form a \"decision matrix\" corresponding to each individual value formula_163 of decision attribute formula_130. Informally, the decision matrix for value formula_163 of decision attribute formula_130 lists all attribute–value pairs that \"differ\" between objects having formula_167 and formula_168.\n\nThis is best explained by example (which also avoids a lot of notation). Consider the table above, and let formula_169 be the decision variable (i.e., the variable on the right side of the implications) and let formula_170 be the condition variables (on the left side of the implication). We note that the decision variable formula_169 takes on two different values, namely formula_172. We treat each case separately.\n\nFirst, we look at the case formula_173, and we divide up formula_12 into objects that have formula_173 and those that have formula_176. (Note that objects with formula_176 in this case are simply the objects that have formula_178, but in general, formula_176 would include all objects having any value for formula_169 \"other than\" formula_173, and there may be several such classes of objects (for example, those having formula_182).) In this case, the objects having formula_173 are formula_184 while the objects which have formula_176 are formula_186. The decision matrix for formula_173 lists all the differences between the objects having formula_173 and those having formula_176; that is, the decision matrix lists all the differences between formula_184 and formula_186. We put the \"positive\" objects (formula_173) as the rows, and the \"negative\" objects formula_176 as the columns.\n\nTo read this decision matrix, look, for example, at the intersection of row formula_46 and column formula_195, showing formula_196 in the cell. This means that \"with regard to\" decision value formula_173, object formula_46 differs from object formula_195 on attributes formula_200 and formula_201, and the particular values on these attributes for the positive object formula_46 are formula_203 and formula_204. This tells us that the correct classification of formula_46 as belonging to decision class formula_173 rests on attributes formula_200 and formula_201; although one or the other might be dispensable, we know that \"at least one\" of these attributes is \"in\"dispensable.\n\nNext, from each decision matrix we form a set of Boolean expressions, one expression for each row of the matrix. The items within each cell are aggregated disjunctively, and the individuals cells are then aggregated conjunctively. Thus, for the above table we have the following five Boolean expressions:\n\nEach statement here is essentially a highly specific (probably \"too\" specific) rule governing the membership in class formula_173 of the corresponding object. For example, the last statement, corresponding to object formula_48, states that all the following must be satisfied: \n\nIt is clear that there is a large amount of redundancy here, and the next step is to simplify using traditional Boolean algebra. The statement formula_221 corresponding to objects formula_30 simplifies to formula_223, which yields the implication\n\nLikewise, the statement formula_225 corresponding to objects formula_31 simplifies to formula_227. This gives us the implication\n\nThe above implications can also be written as the following rule set:\n\nIt can be noted that each of the first two rules has a \"support\" of 1 (i.e., the antecedent matches two objects), while each of the last two rules has a support of 2. To finish writing the rule set for this knowledge system, the same procedure as above (starting with writing a new decision matrix) should be followed for the case of formula_178, thus yielding a new set of implications for that decision value (i.e., a set of implications with formula_178 as the consequent). In general, the procedure will be repeated for each possible value of the decision variable.\n\nThe data system LERS (Learning from Examples based on Rough Sets) Grzymala-Busse (1997) may induce rules from inconsistent data, i.e., data with conflicting objects. Two objects are conflicting when they are characterized by the same values of all attributes, but they belong to different concepts (classes). LERS uses rough set theory to compute lower and upper approximations for concepts involved in conflicts with other concepts.\n\nRules induced from the lower approximation of the concept \"certainly\" describe the concept, hence such rules are called \"certain\". On the other hand, rules induced from the upper approximation of the concept describe the concept \"possibly\", so these rules are called \"possible\". For rule induction LERS uses three algorithms: LEM1, LEM2, and IRIM.\n\nThe LEM2 algorithm of LERS is frequently used for rule induction and is used not only in LERS but also in other systems, e.g., in RSES (Bazan et al. (2004). LEM2 explores the search space of attribute-value pairs. Its input data set is a lower or upper approximation of a concept, so its input data set is always consistent. In general, LEM2 computes a local covering and then converts it into a rule set. We will quote a few definitions to describe the LEM2 algorithm.\n\nThe LEM2 algorithm is based on an idea of an attribute-value pair block. Let formula_36 be a nonempty lower or upper approximation of a concept represented by a decision-value pair formula_233. Set formula_36 \"depends\" on a set formula_235 of attribute-value pairs formula_236 if and only if\n\nSet formula_235 is a \"minimal complex\" of formula_36 if and only if formula_36 depends on formula_235 and no proper subset formula_242 of formula_235 exists such that formula_36 depends on formula_242. Let formula_246 be a nonempty collection of nonempty sets of attribute-value pairs. Then formula_246 is a \"local covering\" of formula_36 if and only if the following three conditions are satisfied:\n\neach member formula_235 of formula_246 is a minimal complex of formula_36,\n\nFor our sample information system, LEM2 will induce the following rules:\n\nOther rule-learning methods can be found, e.g., in Pawlak (1991), Stefanowski (1998), Bazan et al. (2004), etc.\n\nRough set theory is useful for rule induction from incomplete data sets. Using this approach we can distinguish between three types of missing attribute values: \"lost values\" (the values that were recorded but currently are unavailable), \"attribute-concept values\" (these missing attribute values may be replaced by any attribute value limited to the same concept), and \"\"do not care\" conditions\" (the original values were irrelevant). A \"concept\" (\"class\") is a set of all objects classified (or diagnosed) the same way.\n\nTwo special data sets with missing attribute values were extensively studied: in the first case, all missing attribute values were lost (Stefanowski and Tsoukias, 2001), in the second case, all missing attribute values were \"do not care\" conditions (Kryszkiewicz, 1999).\n\nIn attribute-concept values interpretation of a missing attribute value, the missing attribute value may be replaced by any value of the attribute domain restricted to the concept to which the object with a missing attribute value belongs (Grzymala-Busse and Grzymala-Busse, 2007). For example, if for a patient the value of an attribute Temperature is missing, this patient is sick with flu, and all remaining patients sick with flu have values high or very-high for Temperature when using the interpretation of the missing attribute value as the attribute-concept value, we will replace the missing attribute value with high and very-high. Additionally, the \"characteristic relation\", (see, e.g., Grzymala-Busse and Grzymala-Busse, 2007) enables to process data sets with all three kind of missing attribute values at the same time: lost, \"do not care\" conditions, and attribute-concept values.\n\nRough set methods can be applied as a component of hybrid solutions in machine learning and data mining. They have been found to be particularly useful for rule induction and feature selection (semantics-preserving dimensionality reduction). Rough set-based data analysis methods have been successfully applied in bioinformatics, economics and finance, medicine, multimedia, web and text mining, signal and image processing, software engineering, robotics, and engineering (e.g. power systems and control engineering). Recently the three regions of rough sets are interpreted as regions of acceptance, rejection and deferment. This leads to three-way decision making approach with the model which can potentially lead to interesting future applications.\n\nThe idea of rough set was proposed by Pawlak (1981) as a new mathematical tool to deal with vague concepts. Comer, Grzymala-Busse, Iwinski, Nieminen, Novotny, Pawlak, Obtulowicz, and Pomykala have studied algebraic properties of rough sets. Different algebraic semantics have been developed by P. Pagliani, I. Duntsch, M. K. Chakraborty, M. Banerjee and A. Mani; these have been extended to more generalized rough sets by D. Cattaneo and A. Mani, in particular. Rough sets can be used to represent ambiguity, vagueness and general uncertainty.\n\nSince the development of rough sets, extensions and generalizations have continued to evolve. Initial developments focused on the relationship - both similarities and difference - with fuzzy sets. While some literature contends these concepts are different, other literature considers that rough sets are a generalization of fuzzy sets - as represented through either fuzzy rough sets or rough fuzzy sets. Pawlak (1995) considered that fuzzy and rough sets should be treated as being complementary to each other, addressing different aspects of uncertainty and vagueness.\n\nThree notable extensions of classical rough sets are:\n\nRough sets can be also defined, as a generalisation, by employing a rough membership function instead of objective approximation. The rough membership function expresses a conditional probability that formula_11 belongs to formula_36 given formula_260. This can be interpreted as a degree that formula_11 belongs to formula_36 in terms of information about formula_11 expressed by formula_260.\n\nRough membership primarily differs from the fuzzy membership in that the membership of union and intersection of sets cannot, in general, be computed from their constituent membership as is the case of fuzzy sets. In this, rough membership is a generalization of fuzzy membership. Furthermore, the rough membership function is grounded more in probability than the conventionally held concepts of the fuzzy membership function.\n\nSeveral generalizations of rough sets have been introduced, studied and applied to solving problems. Here are some of these generalizations:\n\n\n\n\n\n"}
{"id": "24032607", "url": "https://en.wikipedia.org/wiki?curid=24032607", "title": "Running total", "text": "Running total\n\nA running total is the summation of a sequence of numbers which is updated each time a new number is added to the sequence, by adding the value of the new number to the previous running total. Another term for it is partial sum.\n\nThe purposes of a running total are twofold. First, it allows the total to be stated at any point in time without having to sum the entire sequence each time. Second, it can save having to record the sequence itself, if the particular numbers are not individually important.\n\nConsider the sequence <   5   8   3   2   >. What is the total of this sequence?\n\nAnswer: 5 + 8 + 3 + 2 = 18. This is arrived at by simple summation of the sequence.\n\nNow we insert the number 6 at the end of the sequence to get <   5   8   3   2   6   >. What is the total of that sequence?\n\nAnswer: 5 + 8 + 3 + 2 + 6 = 24. This is arrived at by simple summation of the sequence. \"But\" if we regarded 18 as the running total, we need only add 6 to 18 to get 24. So, 18 was, and 24 now is, the running total. In fact, we would not even need to know the sequence at all, but simply add 6 to 18 to get the new running total; as each new number is added, we get a new running total.\n\nThe same method will also work with subtraction, but in that case it is not strictly speaking a total (which implies summation) but a running difference; not to be confused with a delta. This is used, for example, when scoring the game of darts. Similarly one can multiply instead of add to get the running product.\n\nWhile this concept is very simple, it is extremely common in everyday use. For example, most cash registers display a running total of the purchases so far rung in. By the end of the transaction this will, of course, be the total of all the goods. Similarly, the machine may keep a running total of all transactions made, so that at any point in time the total can be checked against the amount in the till, even though the machine has no memory of past transactions.\n\nTypically many games of all kinds use running totals for scoring; the actual values of past events in the sequence are not important, only the current score, that is to say, the running total.\n\nThe central processing unit of computers for many years had a component called the accumulator which, essentially, kept a running total (it \"accumulated\" the results of individual calculations). This term is largely obsolete with more modern computers. A betting accumulator is the running product of the outcomes of several bets in sequence.\n\n"}
{"id": "24657893", "url": "https://en.wikipedia.org/wiki?curid=24657893", "title": "Star of David theorem", "text": "Star of David theorem\n\nThe Star of David theorem is a mathematical result on arithmetic properties of binomial coefficients. It was discovered by Henry W. Gould in 1972.\n\nThe greatest common divisors of the binomial coefficients forming each of the two triangles in the Star of David shape in Pascal's triangle are equal:\n\nRows 8, 9, and 10 of Pascal's triangle are\n\nFor \"n\"=9, \"k\"=3 or \"n\"=9, \"k\"=6, the element 84 is surrounded by, in sequence, the elements 28, 56, 126, 210, 120, 36. Taking alternating values, we have gcd(28, 126, 120) = 2 = gcd(56, 210, 36).\n\nThe element 36 is surrounded by the sequence 8, 28, 84, 120, 45, 9, and taking alternating values we have gcd(8, 84, 45) = 1 = gcd(28, 120, 9).\n\nThe above greatest common divisor also equals formula_2 Thus in the above example for the element 84 (in its rightmost appearance), we also have gcd(70, 56, 28, 8) = 2. This result in turn has further generalizations.\n\nThe two sets of three numbers which the Star of David theorem says have equal greatest common divisors also have equal products. So for example, again observing that the element 84 is surrounded by, in sequence, the elements 28, 56, 126, 210, 120, 36, and again taking alternating values, we have 28×126×120 = 2×3×5×7 = 56×210×36. This result can be confirmed by writing out each binomial coefficient in factorial form, using formula_3\n\n\n\n"}
{"id": "43742835", "url": "https://en.wikipedia.org/wiki?curid=43742835", "title": "Tame abstract elementary class", "text": "Tame abstract elementary class\n\nIn model theory, a discipline within the field of mathematical logic, a tame abstract elementary class is an abstract elementary class (AEC) which satisfies a locality property for types called tameness. Even though it appears implicitly in earlier work of Shelah, tameness as a property of AEC was first isolated by Grossberg and VanDieren, who observed that tame AECs were much easier to handle than general AECs.\n\nLet \"K\" be an AEC with joint embedding, amalgamation, and no maximal models. Just like in first-order model theory, this implies \"K\" has a universal model-homogeneous monster model formula_1. Working inside formula_1, we can define a semantic notion of types by specifying that two elements \"a\" and \"b\" have the same type over some base model formula_3 if there is an automorphism of the monster model sending \"a\" to \"b\" fixing formula_3 pointwise (note that types can be defined in a similar manner without using a monster model). Such types are called Galois types.\n\nOne can ask for such types to be determined by their restriction on a small domain. This gives rise to the notion of tameness:\n\n\nTame AECs are usually also assumed to satisfy amalgamation.\n\nWhile (without the existence of large cardinals) there are examples of non-tame AECs, most of the known natural examples are tame. In addition, the following sufficient conditions for a class to be tame are known:\n\n\nMany results in the model theory of (general) AECs assume weak forms of the Generalized continuum hypothesis and rely on sophisticated combinatorial set-theoretic arguments. On the other hand, the model theory of tame AECs is much easier to develop, as evidenced by the results presented below.\n\nThe following are some important results about tame AECs.\n\n\n"}
{"id": "1009505", "url": "https://en.wikipedia.org/wiki?curid=1009505", "title": "Titanic prime", "text": "Titanic prime\n\nTitanic prime is a term coined by Samuel Yates in the 1980s, denoting a prime number of at least 1000 decimal digits. Few such primes were known then, but the required size is trivial for modern computers. \n\nThe first 30 titanic primes are of the form:\n\nfor \"n\" one of 7, 663, 2121, 2593, 3561, 4717, 5863, 9459, 11239, 14397, 17289, 18919, 19411, 21667, 25561, 26739, 27759, 28047, 28437, 28989, 35031, 41037, 41409, 41451, 43047, 43269, 43383, 50407, 51043, 52507 .\n\nApart from the early \"n\" = 7, these values are not far from the expectation based on the prime number theorem.\n\nThe first discovered titanic primes were the Mersenne primes 2−1 (with 1281 digits), and 2−1 (with 1332 digits). They were both found November 3, 1961, by Alexander Hurwitz. It is a matter of definition which one was discovered first, since the primality of 2−1 was computed first, but Hurwitz saw the computer output about 2−1 first.\n\nSamuel Yates called those who proved the primality of a titanic prime \"titans\".\n\n\n"}
{"id": "279651", "url": "https://en.wikipedia.org/wiki?curid=279651", "title": "Truncated tetrahedron", "text": "Truncated tetrahedron\n\nIn geometry, the truncated tetrahedron is an Archimedean solid. It has 4 regular hexagonal faces, 4 equilateral triangle faces, 12 vertices and 18 edges (of two types). It can be constructed by truncating all 4 vertices of a regular tetrahedron at one third of the original edge length.\n\nA deeper truncation, removing a tetrahedron of half the original edge length from each vertex, is called rectification. The rectification of a tetrahedron produces an octahedron.\n\nA \"truncated tetrahedron\" is the Goldberg polyhedron G(1,1), containing triangular and hexagonal faces.\n\nA \"truncated tetrahedron\" can be called a cantic cube, with Coxeter diagram, , having half of the vertices of the cantellated cube (rhombicuboctahedron), . There are two dual positions of this construction, and combining them creates the uniform compound of two truncated tetrahedra.\n\nThe area \"A\" and the volume \"V\" of a truncated tetrahedron of edge length \"a\" are:\n\nThe densest packing of the Archimedean truncated tetrahedron is believed to be Φ = , as reported by two independent groups using Monte Carlo methods. Although no mathematical proof exists that this is the best possible packing for the truncated tetrahedron, the high proximity to the unity and independency of the findings make it unlikely that an even denser packing is to be found. In fact, if the truncation of the corners is slightly smaller than that of an Archimedean truncated tetrahedron, this new shape can be used to completely fill space.\n\nCartesian coordinates for the 12 vertices of a truncated tetrahedron centered at the origin, with edge length √8, are all permutations of (±1,±1,±3) with an even number of minus signs:\n\nAnother simple construction exists in 4-space as cells of the truncated 16-cell, with vertices as coordinate permutation of:\n\nThe truncated tetrahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.\nA lower symmetry version of the truncated tetrahedron (a truncated tetragonal disphenoid with order 8 D symmetry) is called a Friauf polyhedron in crystals such as complex metallic alloys. This form fits 5 Friauf polyhedra around an axis, giving a 72-degree dihedral angle on a subset of 6-6 edges. It is named after J. B. Friauf and his 1927 paper \"The crystal structure of the intermetallic compound MgCu\".\n\nGiant truncated tetrahedra were used for the \"Man the Explorer\" and \"Man the Producer\" theme pavilions in Expo 67. They were made of massive girders of steel bolted together in a geometric lattice. The truncated tetrahedra were interconnected with lattice steel platforms. All of these buildings were demolished after the end of Expo 67, as they had not been built to withstand the severity of the Montreal weather over the years. Their only remnants are in the Montreal city archives, the Public Archives Of Canada and the photo collections of tourists of the times.\n\nThe Tetraminx puzzle has a truncated tetrahedral shape. This puzzle shows a dissection of a truncated tetrahedron into 4 octahedra and 6 tetrahedra. It contains 4 central planes of rotations.\n\nIn the mathematical field of graph theory, a truncated tetrahedral graph is a Archimedean graph, the graph of vertices and edges of the truncated tetrahedron, one of the Archimedean solids. It has 12 vertices and 18 edges. It is a connected cubic graph, and connected cubic transitive graph.\n\nIt is also a part of a sequence of cantic polyhedra and tilings with vertex configuration 3.6.\"n\".6. In this wythoff construction the edges between the hexagons represent degenerate digons.\nThis polyhedron is topologically related as a part of sequence of uniform truncated polyhedra with vertex configurations (3.2\"n\".2\"n\"), and [\"n\",3] Coxeter group symmetry.\n\n\n\n"}
{"id": "71630", "url": "https://en.wikipedia.org/wiki?curid=71630", "title": "Unicity distance", "text": "Unicity distance\n\nIn cryptography, unicity distance is the length of an original ciphertext needed to break the cipher by reducing the number of possible spurious keys to zero in a brute force attack. That is, after trying every possible key, there should be just one decipherment that makes sense, i.e. expected amount of ciphertext needed to determine the key completely, assuming the underlying message has redundancy.\n\nClaude Shannon defined the unicity distance in his 1949 paper \"Communication Theory of Secrecy Systems\".\n\nConsider an attack on the ciphertext string \"WNAIW\" encrypted using a Vigenère cipher with a five letter key. Conceivably, this string could be deciphered into any other string—RIVER and WATER are both possibilities for certain keys. This is a general rule of cryptanalysis: with no additional information it is impossible to decode this message.\n\nOf course, even in this case, only a certain number of five letter keys will result in English words. Trying all possible keys we will not only get RIVER and WATER, but SXOOS and KHDOP as well. The number of \"working\" keys will likely be very much smaller than the set of all possible keys. The problem is knowing which of these \"working\" keys is the right one; the rest are spurious.\n\nIn general, given particular assumptions about the size of the key and the number of possible messages, there is an average ciphertext length where there is only one key (on average) that will generate a readable message. In the example above we see only upper case English characters, so if we assume that the plaintext has this form, then there are 26 possible letters for each position in the string. Likewise if we assume five-character upper case keys, there are K=26 possible keys, of which the majority will not \"work\".\n\nA tremendous number of possible messages, N, can be generated using even this limited set of characters: N = 26, where L is the length of the message. However, only a smaller set of them is readable plaintext due to the rules of the language, perhaps M of them, where M is likely to be very much smaller than N. Moreover, M has a one-to-one relationship with the number of keys that work, so given K possible keys, only K × (M/N) of them will \"work\". One of these is the correct key, the rest are spurious.\n\nSince M/N gets arbitrarily small as the length L of the message increases, there is eventually some L that is large enough to make the number of spurious keys equal to zero. Roughly speaking, this is the L that makes KM/N=1. This L is the unicity distance.\n\nThe unicity distance can equivalently be defined as the minimum amount of ciphertext required to permit a computationally unlimited adversary to recover the unique encryption key.\n\nThe expected unicity distance can then be shown to be:\n\nwhere \"U\" is the unicity distance, \"H\"(\"k\") is the entropy of the key space (e.g. 128 for 2 equiprobable keys, rather less if the key is a memorized pass-phrase). \"D\" is defined as the plaintext redundancy in bits per character.\n\nNow an alphabet of 32 characters can carry 5 bits of information per character (as 32 = 2). In general the number of bits of information per character is , where \"N\" is the number of characters in the alphabet and is the binary logarithm. So for English each character can convey bits of information.\n\nHowever the average amount of actual information carried per character in meaningful English text is only about 1.5 bits per character. So the plain text redundancy is \"D\" = 4.7 − 1.5 = 3.2.\n\nBasically the bigger the unicity distance the better. For a one time pad of unlimited size, given the unbounded entropy of the key space, we have formula_2, which is consistent with the one-time pad being unbreakable.\n\nFor a simple substitution cipher, the number of possible keys is , the number of ways in which the alphabet can be permuted. Assuming all keys are equally likely, bits. For English text , thus .\n\nSo given 28 characters of ciphertext it should be theoretically possible to work out an English plaintext and hence the key.\n\nUnicity distance is a useful theoretical measure, but it doesn't say much about the security of a block cipher when attacked by an adversary with real-world (limited) resources. Consider a block cipher with a unicity distance of three ciphertext blocks. Although there is clearly enough information for a computationally unbounded adversary to find the right key (simple exhaustive search), this may be computationally infeasible in practice.\n\nThe unicity distance can be increased by reducing the plaintext redundancy. One way to do this is to deploy data compression techniques prior to encryption, for example by removing redundant vowels while retaining readability. This is a good idea anyway, as it reduces the amount of data to be encrypted.\n\nCiphertexts greater than the unicity distance can be assumed to have only one meaningful decryption. Ciphertexts shorter than the unicity distance may have multiple plausible decryptions. Unicity distance is not a measure of how much ciphertext is required for cryptanalysis, but how much ciphertext is required for there to be only one reasonable solution for cryptanalysis.\n\n"}
{"id": "1498076", "url": "https://en.wikipedia.org/wiki?curid=1498076", "title": "Unit in the last place", "text": "Unit in the last place\n\nIn computer science and numerical analysis, unit in the last place or unit of least precision (ULP) is the spacing between floating-point numbers, i.e., the value the least significant digit represents if it is 1. It is used as a measure of accuracy in numeric calculations.\n\nIn radix \"b\", if \"x\" has exponent \"E\", then ULP(\"x\") = machine epsilon · \"b\", but alternative definitions exist in the numerics and computing literature for \"ULP\", \"exponent\" and \"machine epsilon\", and they may give different equalities.\n\nAnother definition, suggested by John Harrison, is slightly different: ULP(\"x\") is the distance between the two closest \"straddling\" floating-point numbers \"a\" and \"b\" (i.e., those with \"a\" ≤ \"x\" ≤ \"b\" and \"a\" ≠ \"b\"), assuming that the exponent range is not upper-bounded. These definitions differ only at signed powers of the radix.\n\nThe IEEE 754 specification—followed by all modern floating-point hardware—requires that the result of an elementary arithmetic operation (addition, subtraction, multiplication, division, and square root since 1985, and FMA since 2008) be correctly rounded, which implies that in rounding to nearest, the rounded result is within 0.5 ULP of the mathematically exact result, using John Harrison's definition; conversely, this property implies that the distance between the rounded result and the mathematically exact result is minimized (but for the halfway cases, it is satisfied by two consecutive floating-point numbers). Reputable numeric libraries compute the basic transcendental functions to between 0.5 and about 1 ULP. Only a few libraries compute them within 0.5 ULP, this problem being complex due to the Table-maker's dilemma.\n\nLet \"x\" be a nonnegative floating-point number and assume that the active rounding attribute is round to nearest, ties to even, denoted RN. If ULP(\"x\") is less than or equal to 1, then . Otherwise, or , depending on the value of the least significant digit and the exponent of x. This is demonstrated in the following Haskell code typed at an interactive prompt:\n\nHere we start with 0 in 32-bit single-precision and repeatedly add 1 until the operation is idempotent. The result is equal to 2 since the significand for a single-precision number in this example contains 24 bits.\n\nThe following example in Java approximates as a floating point value by finding the two double values bracketing :\n\nThen is determined as \n\nAnother example, in Python, also typed at an interactive prompt, is:\n\nIn this case, we start with and repeatedly double it until . The result is 2, because the double-precision floating-point format uses a 53-bit significand.\n\nSince Java 1.5, the Java standard library has included and functions.\n\nThe C language library provides functions to calculate the next floating-point number in some given direction: codice_1 and codice_2 for codice_3, codice_4 and codice_5 for codice_6, codice_7 and codice_8 for codice_9, declared in codice_10.\n\nThe Boost C++ Libraries offer boost::math::float_next, boost::math::float_prior, boost::math::nextafter \nand boost::math::float_advance functions to obtain nearby (and distant) floating-point values,\n\nand boost::math::float_distance(a, b) to calculate the floating-point distance between two doubles.\n\n"}
{"id": "2578793", "url": "https://en.wikipedia.org/wiki?curid=2578793", "title": "Z User Group", "text": "Z User Group\n\nThe Z User Group exists to promote use and development of the Z notation, a formal specification language for the description of and reasoning about computer-based systems. It was formally constituted on 14 December 1992 during the ZUM'92 Z User Meeting in London, England.\n\nZUG has organised a series of Z User Meetings approximately every 18 months initially. From 2000, these became the ZB Conference (jointly with the B-Method, co-organized with APCB), and from 2008 the ABZ Conference (with Abstract State Machines as well). In 2010, the ABZ Conference also includes Alloy, a Z-like specification language with associated tool support.\n\nThe Z User Group participated at the \"FM'99 World Congress on Formal Methods\" in Toulouse, France, in 1999.\n\nSuccessive chairs have been:\n\n\nSuccessive secretaries have been:\n\n\n\n"}
