{"id": "4189047", "url": "https://en.wikipedia.org/wiki?curid=4189047", "title": "224 (number)", "text": "224 (number)\n\n224 (two hundred [and] twenty-four) is the natural number following 223 and preceding 225.\n224 is a practical number,\nand a sum of two positive cubes .\n\n224 is the smallest \"k\" with λ(\"k\") = 24, where λ(\"k\") is the Carmichael function.\n\n"}
{"id": "30625447", "url": "https://en.wikipedia.org/wiki?curid=30625447", "title": "Acta Applicandae Mathematicae", "text": "Acta Applicandae Mathematicae\n\nFounded in 1983, the journal publishes articles on applied mathematics.\nThe journal is indexed by \"Mathematical Reviews\" and Zentralblatt MATH.\nIts 2009 MCQ was 0.34, and its 2009 impact factor was 0.523.\n"}
{"id": "38609892", "url": "https://en.wikipedia.org/wiki?curid=38609892", "title": "Alessio Figalli", "text": "Alessio Figalli\n\nAlessio Figalli (; born 2 April 1984) is an Italian mathematician working primarily on calculus of variations and partial differential equations.\n\nHe was awarded the Prix and in 2012, the EMS Prize in 2012, the Stampacchia Medal in 2015, the Feltrinelli Prize in 2017 and the Fields Medal in 2018. He was an invited speaker at the International Congress of Mathematicians 2014.\nIn 2016 he was awarded a European Research Council (ERC) grant.\n\nFigalli received his master's degree from the Scuola Normale Superiore di Pisa in 2006, and earned his doctorate in 2007 under the supervision of Luigi Ambrosio at the Scuola Normale Superiore di Pisa and Cédric Villani at the École Normale Supérieure de Lyon. In 2007 he was appointed Chargé de recherche at the French National Centre for Scientific Research, in 2008 he went to the École polytechnique as Professeur Hadamard.\n\nIn 2009 he moved to the University of Texas at Austin as Associate Professor. Then he became Full Professor in 2011, and R. L. Moore Chair holder in 2013. Since 2016, he is a chaired professor at ETH Zürich.\n\nAmongst his several recognitions, Figalli has won an EMS Prize in 2012, he has been awarded the Peccot-Vimont Prize 2011 and Cours Peccot 2012 of the Collège de France and has been appointed Nachdiplom Lecturer in 2014 at ETH Zürich. He has won the 2015 edition of the Stampacchia Medal, and the 2017 edition of the Feltrinelli Prize for mathematics.\n\nIn 2018 he won the Fields Medal \"for his contributions to the theory of optimal transport, and its application to partial differential equations, metric geometry, and probability\".\n\nFigalli has worked in the theory of optimal transport, with particular emphasis on the regularity theory of optimal transport maps and its connections to Monge–Ampère equations. Amongst the results he obtained in this direction, there stand out an important higher integrability property of the second derivatives of solutions to the Monge–Ampère equation and a partial regularity result for Monge–Ampère type equations, both proved together with Guido de Philippis. He used optimal transport techniques to get improved versions of the anisotropic isoperimetric inequality, and obtained several other important results on the stability of functional and geometric inequalities. In particular, together with Francesco Maggi and Aldo Pratelli, he proved a sharp quantitative version of the anisotropic isoperimetric inequality. \n\nThen, in a joint work with Eric Carlen, he addressed the stability analysis of some Gagliardo–Nirenberg and logarithmic Hardy–Littlewood–Sobolev inequalities to obtain a quantitative rate of convergence for the critical mass Keller–Segel equation. He also worked on Hamilton–Jacobi equations and their connections to weak Kolmogorov–Arnold–Moser theory. In a paper with Gonzalo Contreras and Ludovic Rifford, he proved generic hyperbolicity of Aubry sets on compact surfaces. \n\nIn addition, he has given several contributions to the Di Perna–Lions' theory, applying it both to the understanding of semiclassical limits of the Schrödinger equation with very rough potentials, and to study the Lagrangian structure of weak solutions to the Vlasov–Poisson equation. More recently, in collaboration with Alice Guionnet, he introduced and developed new transportation techniques in the topic of random matrices to prove universality results in several-matrix models. Also, together with Joaquim Serra, he has proved the De Giorgi's conjecture for boundary reaction terms in dimension ≤ 5, and he has improved the classical results by Luis Caffarelli on the structure of singular points in the obstacle problem.\n\n"}
{"id": "864149", "url": "https://en.wikipedia.org/wiki?curid=864149", "title": "Arrangement of lines", "text": "Arrangement of lines\n\nIn geometry an arrangement of lines is the partition of the plane formed by a collection of lines. Bounds on the complexity of arrangements have been studied in discrete geometry, and computational geometers have found algorithms for the efficient construction of arrangements.\n\nFor any set \"A\" of lines in the Euclidean plane, one can define an equivalence relation on the points of the plane according to which two points \"p\" and \"q\" are equivalent if, for every line \"l\" of \"A\", either \"p\" and \"q\" are both on \"l\" or both belong to the same open half-plane bounded by \"l\". When \"A\" is finite or locally finite the equivalence classes of this relation are of three types:\nThese three types of objects link together to form a cell complex covering the plane. Two arrangements are said to be \"isomorphic\" or \"combinatorially equivalent\" if there is a one-to-one adjacency-preserving correspondence between the objects in their associated cell complexes.\n\nThe study of arrangements was begun by Jakob Steiner, who proved the first bounds on the maximum number of features of different types that an arrangement may have.\nAn arrangement with \"n\" lines has at most \"n\"(\"n\" − 1)/2 vertices, one per pair of crossing lines. This maximum is achieved for \"simple arrangements\", those in which each two lines has a distinct pair of crossing points. In any arrangement there will be \"n\" infinite-downward rays, one per line; these rays separate \"n\" + 1 cells of the arrangement that are unbounded in the downward direction. The remaining cells all have a unique bottommost vertex, and each vertex is bottommost for a unique cell, so the number of cells in an arrangement is the number of vertices plus \"n\" + 1, or at most \"n\"(\"n\" + 1)/2 + 1; see lazy caterer's sequence. The number of edges of the arrangement is at most \"n\", as may be seen either by using the Euler characteristic to calculate it from the numbers of vertices and cells, or by observing that each line is partitioned into at most \"n\" edges by the other \"n\" − 1 lines; again, this worst-case bound is achieved for simple arrangements.\nThe \"zone\" of a line \"l\" in a line arrangement is the collection of cells having edges belonging to \"l\". The zone theorem states that the total number of edges in the cells of a single zone is linear. More precisely, the total number of edges of the cells belonging to a single side of line \"l\" is at most 5\"n\" − 1, and the total number of edges of the cells belonging to both sides of \"l\" is at most formula_1. More generally, the total complexity of the cells of a line arrangement that are intersected by any convex curve is O(\"n\" α(\"n\")), where α denotes the inverse Ackermann function, as may be shown using Davenport–Schinzel sequences. Summing the complexities of all zones, one finds that the sum of squares of cell complexities in an arrangement is O(\"n\").\n\nThe \"k-level\" of an arrangement is the polygonal chain formed by the edges that have exactly \"k\" other lines directly below them, and the \"≤k-level\" is the portion of the arrangement below the \"k\"-level. Finding matching upper and lower bounds for the complexity of a \"k\"-level remains a major open problem in discrete geometry; the best upper bound is O(\"nk\"), while the best lower bound is Ω(\"n\" exp(\"c\" (log\"k\"))). In contrast, the maximum complexity of the ≤\"k\"-level is known to be Θ(\"nk\"). A \"k\"-level is a special case of a monotone path in an arrangement; that is, a sequence of edges that intersects any vertical line in a single point. However, monotone paths may be much more complicated than \"k\"-levels: there exist arrangements and monotone paths in these arrangements where the number of points at which the path changes direction is Ω(\"n\").\n\nAlthough a single cell in an arrangement may be bounded by all \"n\" lines, it is not possible in general for \"m\" different cells to all be bounded by \"n\" lines. Rather, the total complexity of \"m\" cells is at most Θ(\"m\"\"n\" + \"n\"), almost the same bound as occurs in the Szemerédi–Trotter theorem on point-line incidences in the plane. A simple proof of this follows from the crossing number inequality: if \"m\" cells have a total of \"x\" + \"n\" edges, one can form a graph with \"m\" nodes (one per cell) and \"x\" edges (one per pair of consecutive cells on the same line). The edges of this graph can be drawn as curves that do not cross within the cells corresponding to their endpoints, and then follow the lines of the arrangement; therefore, there are O(\"n\") crossings in this drawing. However, by the crossing number inequality, there are Ω(\"x\"/\"m\") crossings; in order to satisfy both bounds, \"x\" must be O(\"m\"\"n\").\n\nIt is often convenient to study line arrangements not in the Euclidean plane but in the projective plane, due to the fact that in projective geometry every pair of lines has a crossing point. In the projective plane, we may no longer define arrangements using sides of lines (a line in the projective plane does not separate the plane into two distinct sides), but we may still define the cells of an arrangement to be the connected components of the points not belonging to any line, the edges to be the connected components of sets of points belonging to a single line, and the vertices to be points where two or more lines cross. A line arrangement in the projective plane differs from its Euclidean counterpart in that the two Euclidean rays at either end of a line are replaced by a single edge in the projective plane that connects the leftmost and rightmost vertices on that line, and in that pairs of unbounded Euclidean cells are replaced in the projective plane by single cells that are crossed by the projective line at infinity.\n\nDue to projective duality, many statements about the combinatorial properties of points in the plane may be more easily understood in an equivalent dual form about arrangements of lines. For instance, the Sylvester–Gallai theorem, stating that any non-collinear set of points in the plane has an \"ordinary line\" containing exactly two points, transforms under projective duality to the statement that any arrangement of lines with more than one vertex has an \"ordinary point\", a vertex where only two lines cross. The earliest known proof of the Sylvester–Gallai theorem, by , uses the Euler characteristic to show that such a vertex must always exist.\n\nAn arrangement of lines in the projective plane is said to be \"simplicial\" if every cell of the arrangement is bounded by exactly three edges; simplicial arrangements were first studied by Melchior. Three infinite families of simplicial line arrangements are known:\nAdditionally there are many other examples of \"sporadic simplicial arrangements\" that do not fit into any known infinite family.\nAs Grünbaum writes, simplicial arrangements “appear as examples or counterexamples in many contexts of combinatorial geometry and its applications.” For instance, use simplicial arrangements to construct counterexamples to a conjecture on the relation between the degree of a set of differential equations and the number of invariant lines the equations may have. The two known counterexamples to the Dirac-Motzkin conjecture (which states that any \"n\"-line arrangement has at least \"n\"/2 ordinary points) are both simplicial.\n\nThe dual graph of a line arrangement, in which there is one node per cell and one edge linking any pair of cells that share an edge of the arrangement, is a partial cube, a graph in which the nodes can be labeled by bitvectors in such a way that the graph distance equals the Hamming distance between labels; in the case of a line arrangement, each coordinate of the labeling assigns 0 to nodes on one side of one of the lines and 1 to nodes on the other side. Dual graphs of simplicial arrangements have been used to construct infinite families of 3-regular partial cubes, isomorphic to the graphs of simple zonohedra.\n\nIt is also of interest to study the extremal numbers of triangular cells in arrangements that may not necessarily be simplicial. In any arrangement, there must be at least \"n\" triangles; every arrangement that has only \"n\" triangles must be simple. The maximum possible number of triangles in a simple arrangement is known to be upper bounded by \"n\"(\"n\" − 1)/3 and lower bounded by \"n\"(\"n\" − 3)/3; the lower bound is achieved by certain subsets of the diagonals of a regular 2\"n\"-gon. For non-simple arrangements the maximum number of triangles is similar but more tightly bounded. The closely related Kobon triangle problem asks for the maximum number of non-overlapping finite triangles (not necessarily faces) in an arrangement in the Euclidean plane; for some but not all values of \"n\", \"n\"(\"n\" − 2)/3 triangles are possible.\n\nThe dual graph of a simple line arrangement may be represented geometrically as a collection of rhombi, one per vertex of the arrangement, with sides perpendicular to the lines that meet at that vertex. These rhombi may be joined together to form a tiling of a convex polygon in the case of an arrangement of finitely many lines, or of the entire plane in the case of a locally finite arrangement with infinitely many lines. investigated special cases of this construction in which the line arrangement consists of \"k\" sets of equally spaced parallel lines. For two perpendicular families of parallel lines this construction just gives the familiar square tiling of the plane, and for three families of lines at 120-degree angles from each other (themselves forming a trihexagonal tiling) this produces the rhombille tiling. However, for more families of lines this construction produces aperiodic tilings. In particular, for five families of lines at equal angles to each other (or, as de Bruijn calls this arrangement, a \"pentagrid\") it produces a family of tilings that include the rhombic version of the Penrose tilings.\n\nThe tetrakis square tiling is an infinite arrangement of lines forming a periodic tiling that resembles a multigrid with four parallel families, but in which two of the families are more widely spaced than the other two, and in which the arrangement is simplicial rather than simple. Its dual is the truncated square tiling. Similarly, the triangular tiling is an infinite simplicial line arrangement with three parallel families, which has as its dual the hexagonal tiling, and the bisected hexagonal tiling is an infinite simplicial line arrangement with six parallel families and two line spacings, dual to the great rhombitrihexagonal tiling.\n\n\"Constructing\" an arrangement means, given as input a list of the lines in the arrangement, computing a representation of the vertices, edges, and cells of the arrangement together with the adjacencies between these objects, for instance as a doubly connected edge list. Due to the zone theorem, arrangements can be constructed efficiently by an incremental algorithm that adds one line at a time to the arrangement of the previously added lines: each new line can be added in time proportional to its zone, resulting in a total construction time of O(\"n\"). However, the memory requirements of this algorithm are high, so it may be more convenient to report all features of an arrangement by an algorithm that does not keep the entire arrangement in memory at once. This may again be done efficiently, in time O(\"n\") and space O(\"n\"), by an algorithmic technique known as \"topological sweeping\". Computing a line arrangement exactly requires a numerical precision several times greater than that of the input coordinates: if a line is specified by two points on it, the coordinates of the arrangement vertices may need four times as much precision as these input points. Therefore, computational geometers have also studied algorithms for constructing arrangements efficiently with limited numerical precision.\n\nAs well, researchers have studied efficient algorithms for constructing smaller portions of an arrangement, such as zones, \"k\"-levels, or the set of cells containing a given set of points. The problem of finding the arrangement vertex with the median \"x\"-coordinate arises (in a dual form) in robust statistics as the problem of computing the Theil–Sen estimator of a set of points.\n\nMarc van Kreveld suggested the algorithmic problem of computing shortest paths between vertices in a line arrangement, where the paths are restricted to follow the edges of the arrangement, more quickly than the quadratic time that it would take to apply a shortest path algorithm to the whole arrangement graph. An approximation algorithm is known, and the problem may be solved efficiently for lines that fall into a small number of parallel families (as is typical for urban street grids), but the general problem remains open.\n\nA pseudoline arrangement is a family of curves that share similar topological properties with a line arrangement. These can be defined most simply in the projective plane as simple closed curves any two of which meet in a single crossing point. A pseudoline arrangement is said to be \"stretchable\" if it is combinatorially equivalent to a line arrangement; it is complete for the existential theory of the reals to distinguish stretchable arrangements from non-stretchable ones. Every arrangement of finitely many pseudolines can be extended so that they become lines in a \"spread\", a type of non-Euclidean incidence geometry in which every two points of a topological plane are connected by a unique line (as in the Euclidean plane) but in which other axioms of Euclidean geometry may not apply.\n\nAnother type of non-Euclidean geometry is the hyperbolic plane, and\narrangements of hyperbolic lines in this geometry have also been studied. Any finite set of lines in the Euclidean plane has a combinatorially equivalent arrangement in the hyperbolic plane (e.g. by enclosing the vertices of the arrangement by a large circle and interpreting the interior of the circle as a Klein model of the hyperbolic plane). However, in hyperbolic line arrangements lines may avoid crossing each other without being parallel; the intersection graph of the lines in a hyperbolic arrangement is a circle graph. The corresponding concept to hyperbolic line arrangements for pseudolines is a \"weak pseudoline arrangement\", a family of curves having the same topological properties as lines such that any two curves in the family either meet in a single crossing point or have no intersection.\n\n\n\n"}
{"id": "37815827", "url": "https://en.wikipedia.org/wiki?curid=37815827", "title": "Astrostatistics", "text": "Astrostatistics\n\nAstrostatistics is a discipline which spans astrophysics, statistical analysis and data mining. It is used to process the vast amount of data produced by automated scanning of the cosmos, to characterize complex datasets, and to link astronomical data to astrophysical theory. Many branches of statistics are involved in astronomical analysis including nonparametrics, multivariate regression and multivariate classification, time series analysis, and especially Bayesian inference.\n\nPractitioners are represented by the International Astrostatistics Association affiliated with the International Statistical Institute, the International Astronomical Union Working Group in Astrostatistics and Astroinformatics, the American Astronomical Society Working Group in Astroinformatics and Astrostatistics, and the American Statistical Association Interest Group in Astrostatistics. All of these organizations participate in the Astrostatistics and Astroinformatics Portal Web site.\n"}
{"id": "2737856", "url": "https://en.wikipedia.org/wiki?curid=2737856", "title": "Cochleoid", "text": "Cochleoid\n\nA cochleoid is a snail-shaped curve similar to a strophoid which can be represented by the polar equation\nthe Cartesian equation\nor the parametric equations\n\n\n"}
{"id": "5759", "url": "https://en.wikipedia.org/wiki?curid=5759", "title": "Complex analysis", "text": "Complex analysis\n\nComplex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers. It is useful in many branches of mathematics, including algebraic geometry, number theory, analytic combinatorics, applied mathematics; as well as in physics, including the branches of hydrodynamics, thermodynamics, and particularly quantum mechanics. By extension, use of complex analysis also has applications in engineering fields such as nuclear, aerospace, mechanical and electrical engineering.\n\nAs a differentiable function of a complex variable is equal to the sum of its Taylor series (that is, it is analytic), complex analysis is particularly concerned with analytic functions of a complex variable (that is, holomorphic functions).\n\nComplex analysis is one of the classical branches in mathematics, with roots in the 18th century and just prior. Important mathematicians associated with complex numbers include Euler, Gauss, Riemann, Cauchy, Weierstrass, and many more in the 20th century. Complex analysis, in particular the theory of conformal mappings, has many physical applications and is also used throughout analytic number theory. In modern times, it has become very popular through a new boost from complex dynamics and the pictures of fractals produced by iterating holomorphic functions. Another important application of complex analysis is in string theory which studies conformal invariants in quantum field theory.\n\nA complex function is a function whose domain and range are subsets of the complex plane. This is also expressed by saying that the independent variable and the dependent variable both are complex numbers.\n\nFor any complex function, the values formula_1 from the domain and their images formula_2 in the range may be separated into real and imaginary parts:\n\nwhere formula_5 are all real-valued. \n\nIn other words, a complex function formula_6 may be decomposed into \n\ni.e., into two real-valued functions (formula_9, formula_10) of two real variables (formula_11, formula_12).\n\nThe basic concepts of complex analysis are often introduced by extending the elementary real functions (e.g., exponential functions, logarithmic functions, and trigonometric functions) into a complex domain and the corresponding complex range.\n\nComplex functions that are differentiable at every point of an open subset formula_13 of the complex plane are said to be \"holomorphic\" \"on\" formula_13. In the context of complex analysis, the derivative of formula_15 at formula_16 is defined to be formula_17.Superficially, this definition is formally analogous to that of the derivative of a real function. However, complex derivatives and differentiable functions behave in significantly different ways compared to their real counterparts. In particular, for this limit to exist, the value of the difference quotient must approach the same complex number, regardless of the manner in which we approach formula_16 in the complex plane. Consequently, complex differentiability has much stronger implications than real differentiability. For instance, holomorphic functions are infinitely differentiable, whereas the existence of the \"n\"th derivative need not imply the existence of the (\"n\" + 1)th derivative for real functions. Furthermore, all holomorphic functions satisfy the stronger condition of analyticity, meaning that the function is, at every point in its domain, locally given by a convergent power series. In essence, this means that functions holomorphic on formula_13 can be approximated arbitrarily well by polynomials in some neighborhood of every point in formula_13. This stands in sharp contrast to differentiable real functions; even infinitely differentiable real functions can be \"nowhere\" analytic.\n\nMost elementary functions, including the exponential function, the trigonometric functions, and all polynomial functions, extended appropriately to complex arguments as functions formula_21, are holomorphic over the entire complex plane, making them \"entire\" \"functions\", while rational functions formula_22, where \"p\" and \"q\" are polynomials, are holomorphic on domains that exclude points where \"q\" is zero. Such functions that are holomorphic everywhere except a set of isolated points are known as \"meromorphic functions\". On the other hand, the functions formula_23, formula_24, and formula_25 are not holomorphic anywhere on the complex plane, as can be shown by their failure to satisfy the Cauchy-Riemann conditions (see below).\n\nAn important property that characterizes holomorphic functions is the relationship between the partial derivatives of their real and imaginary components, known as the Cauchy-Riemann conditions. If formula_6, defined by formula_27, where formula_28, is holomorphic on a region formula_13, then \"formula_30\" must hold for all formula_31. Here, the differential operator formula_32 is defined as formula_33\".\" In terms of the real and imaginary parts of the function, \"u\" and \"v\", this is equivalent to the pair of equations formula_34 and formula_35, where the subscripts indicate partial differentiation. However, it is important to note that functions satisfying the Cauchy-Riemann conditions are not necessarily holomorphic, unless additional continuity conditions are met (see Looman-Menchoff Theorem for a discussion).\n\nHolomorphic functions exhibit some remarkable features. For instance, Picard's theorem asserts that the range of an entire function can only take three possible forms: formula_36, formula_37, or formula_38 for some formula_39. In other words, if two distinct complex numbers formula_1 and formula_41 are not in the range of entire function formula_15, then formula_15 is a constant function. Moreover, given a holomorphic function formula_15 defined on an open set formula_45, the analytic continuation of formula_15 to a larger open set formula_47 is unique. As a result, the value of a holomorphic function over an arbitrarily small region in fact determines the value of the function everywhere to which it can be extended as a holomorphic function.\n\n\"See also\": analytic function, coherent sheaf and vector bundles.\n\nOne of the central tools in complex analysis is the line integral. The line integral around a closed path of a function that is holomorphic everywhere inside the area bounded by the closed path is always zero, which is what the Cauchy integral theorem states. The values of such a holomorphic function inside a disk can be computed by a path integral on the disk's boundary (as shown in Cauchy's integral formula). Path integrals in the complex plane are often used to determine complicated real integrals, and here the theory of residues among others is applicable (see methods of contour integration). A \"pole\" (or isolated singularity) of a function is a point where the function's value becomes unbounded, or \"blows up\". If a function has such a pole, then one can compute the function's residue there, which can be used to compute path integrals involving the function; this is the content of the powerful residue theorem. The remarkable behavior of holomorphic functions near essential singularities is described by Picard's Theorem. Functions that have only poles but no essential singularities are called meromorphic. Laurent series are the complex-valued equivalent to Taylor series, but can be used to study the behavior of functions near singularities through infinite sums of more well understood functions, such as polynomials.\n\nA bounded function that is holomorphic in the entire complex plane must be constant; this is Liouville's theorem. It can be used to provide a natural and short proof for the fundamental theorem of algebra which states that the field of complex numbers is algebraically closed.\n\nIf a function is holomorphic throughout a connected domain then its values are fully determined by its values on any smaller subdomain. The function on the larger domain is said to be analytically continued from its values on the smaller domain. This allows the extension of the definition of functions, such as the Riemann zeta function, which are initially defined in terms of infinite sums that converge only on limited domains to almost the entire complex plane. Sometimes, as in the case of the natural logarithm, it is impossible to analytically continue a holomorphic function to a non-simply connected domain in the complex plane but it is possible to extend it to a holomorphic function on a closely related surface known as a Riemann surface.\n\nAll this refers to complex analysis in one variable. There is also a very rich theory of complex analysis in more than one complex dimension in which the analytic properties such as power series expansion carry over whereas most of the geometric properties of holomorphic functions in one complex dimension (such as conformality) do not carry over. The Riemann mapping theorem about the conformal relationship of certain domains in the complex plane, which may be the most important result in the one-dimensional theory, fails dramatically in higher dimensions.\n\n\n\n"}
{"id": "20506691", "url": "https://en.wikipedia.org/wiki?curid=20506691", "title": "Condensation lemma", "text": "Condensation lemma\n\nIn set theory, a branch of mathematics, the condensation lemma is a result about sets in the\nconstructible universe.\n\nIt states that if \"X\" is a transitive set and is an elementary submodel of some level of the constructible hierarchy L, that is, formula_1, then in fact there is some ordinal formula_2 such that formula_3.\n\nMore can be said: If \"X\" is not transitive, then its transitive collapse is equal to some formula_4, and the hypothesis of elementarity can be weakened to elementarity only for formulas which are formula_5 in the Lévy hierarchy. Also, the assumption that \"X\" be transitive automatically holds when formula_6.\n\nThe lemma was formulated and proved by Kurt Gödel in his proof that the axiom of constructibility implies GCH.\n\n"}
{"id": "29747989", "url": "https://en.wikipedia.org/wiki?curid=29747989", "title": "Convex curve", "text": "Convex curve\n\nIn geometry, a convex curve is a curve in the Euclidean plane which lies completely on one side of each and every one of its tangent lines.\n\nThe boundary of a convex set is always a convex curve.\n\nAny straight line \"L\" divides the Euclidean plane into two half-planes whose union is the entire plane and whose intersection is \"L\" . We say that a curve \"C\" \"lies on one side of \"L\"\" if it is entirely contained in one of the half-planes. A plane curve is called convex if it lies on one side of each of its tangent lines. In other words, a convex curve is a curve that has a supporting line through each of its points.\n\nA convex curve may be defined as the boundary of a convex set in the Euclidean plane. This definition is more restrictive than the definition in terms of tangent lines; in particular, with this definition, a convex curve can have no endpoints.\n\nSometimes, a looser definition is used, in which a convex curve is a curve that forms \"a subset\" of the boundary of a convex set. For this variation, a convex curve may have endpoints.\n\nA strictly convex curve is a convex curve that does not contain any line segments. Equivalently, a strictly convex curve is a curve that intersects any line in at most two points, or a simple curve in convex position, meaning that none of its points is a convex combination of any other subset of its points.\n\nEvery convex curve that is the boundary of a closed convex set has a well-defined finite length. That is, these curves are a subset of the rectifiable curves.\n\nAccording to the four-vertex theorem, every smooth convex curve that is the boundary of a closed convex set has at least four vertices, points that are local minima or local maxima of curvature.\n\nA curve \"C\" is convex if and only if there are no three different points in \"C\" such that the tangents in these points are parallel.\n\nProof:\n\n⇒ If there are three parallel tangents, then one of them, say \"L\", must be between the other two. This means that \"C\" lies on both sides of \"L\", so it cannot be convex.\n\n⇐ If \"C\" is not convex, then by definition there is point \"p\" on \"C\" such that the tangent line at \"p\" (call it \"L\") has \"C\" on both sides of it. Since \"C\" is closed, if we trace the part of \"C\" that lies on one side of \"L\" we eventually get at a point \"q1\" which is farthest from \"L\". The tangent to \"C\" at \"q1\" (call it \"L1\") must be parallel to \"L\". The same is true in the other side of \"L\" - there is a point \"q2\" and a tangent \"L2\" which is parallel to \"L\". Thus there are three different points, {\"p\",\"q1\",\"q2\"}, such that their tangents are parallel.\n\nA curve is called simple if it does not intersect itself. A closed regular plane simple curve \"C\" is convex \"if and only if\" its curvature is either always non-negative or always non-positive—i.e., if and only if the \"turning angle\" (the angle of the tangent to the curve) is a weakly monotone function of the parametrization of the curve.\n\nProof:\n\n⇐ If \"C\" is not convex, then by the parallel tangents lemma there are three points {\"p\",\"q1\",\"q2\"} such that the tangents at these points are parallel. At least two must have their signed tangents pointing in the same direction. Without loss of generality, assume that these points are \"q1\" and \"q2\". This means that the difference in the turning angle when going from \"q1\" to \"q2\" is a multiple of 2π. There are two possibilities:\nThus we have proved that if \"C\" is not convex, the turning angle cannot be a monotone function.\n\n⇒ Assume that the turning angle is not monotone. Then we can find three points on the curve, \"s1\"<\"s0\"<\"s2\", such that the turning angle at \"s1\" and \"s2\" is the same and different than the turning angle at \"s0\". In a simple closed curve, all turning angles are covered. In particular, there is a point \"s3\" in which the turning angle is minus the turning angle at \"s1\". Now we have three points, {\"s1\",\"s2\",\"s3\"}, whose turning angle differs in a multiple of π. There are two possibilities:\nThus we have proved that if the turning angle is not monotone, the curve cannot be convex.\n\nSmooth convex curves with an axis of symmetry may sometimes be called ovals. However, in finite projective geometry, ovals are instead defined as sets for which each point has a unique line disjoint from the rest of the set, a property that in Euclidean geometry is true of the smooth strictly convex closed curves.\n\n"}
{"id": "42169", "url": "https://en.wikipedia.org/wiki?curid=42169", "title": "Dual number", "text": "Dual number\n\nIn linear algebra, the dual numbers extend the real numbers by adjoining one new element ε with the property ε = 0 (ε is nilpotent). The collection of dual numbers forms a particular two-dimensional commutative unital associative algebra over the real numbers. Every dual number has the form \"z\" = \"a\" + \"b\"ε where \"a\" and \"b\" are uniquely determined real numbers. The dual numbers can also be thought of as the exterior algebra of a one-dimensional vector space; the general case of \"n\" dimensions leads to the Grassmann numbers.\n\nThe algebra of dual numbers is a ring that is a local ring since the principal ideal generated by ε is its only maximal ideal.\nDual numbers form the coefficients of dual quaternions.\n\nDual numbers were introduced in 1873 by William Clifford, and were used at the beginning of the twentieth century by the German mathematician Eduard Study, who used them to represent the dual angle which measures the relative position of two skew lines in space. Study defined a dual angle as formula_1, where formula_2 is the angle between the directions of two lines in three-dimensional space and formula_3 is a distance between them. The \"n\"-dimensional generalization, the Grassmann number, was introduced by Hermann Grassmann in the late 19th century.\n\nUsing matrices, dual numbers can be represented as\n\nThe sum and product of dual numbers are then calculated with ordinary matrix addition and matrix multiplication; both operations are commutative and associative within the algebra of dual numbers.\n\nThis correspondence is analogous to the usual matrix representation of complex numbers.\nHowever, it is \"not\" the only representation with 2 × 2 real matrices, as is shown in the profile of 2 × 2 real matrices. Like the complex plane and split-complex number plane, the dual numbers are one of the realizations of planar algebra.\n\nThe \"unit circle\" of dual numbers consists of those with \"a\" = 1 or −1 since these satisfy \"z z\"* = 1 where \"z\"* = \"a\" − \"b\"ε. However, note that\nso the exponential map applied to the ε-axis covers only half the \"circle\".\n\nLet \"z\" = \"a\" + \"b\" ε. If \"a\" ≠ 0 and \"m\" = \"b\" /\"a\", then \"z\" = \"a\"(1 + \"m\" ε) is the polar decomposition of the dual number \"z\", and the slope \"m\" is its angular part.\nThe concept of a \"rotation\" in the dual number plane is equivalent to a vertical shear mapping since (1 + \"p\" ε)(1 + \"q\" ε) = 1 + (\"p\"+\"q\") ε.\n\nIn absolute space and time the Galilean transformation\nrelates the resting coordinates system to a moving frame of reference of velocity \"v\".\nWith dual numbers \"t\" + \"x\" ε representing events along one space dimension and time,\nthe same transformation is effected with multiplication by (1 + \"v\" ε).\n\nGiven two dual numbers \"p\", and \"q\", they determine the set of \"z\" such that the difference in slopes (\"Galilean angle\") between the lines from \"z\" to \"p\" and \"q\" is constant. This set is a cycle in the dual number plane; since the equation setting the difference in slopes of the lines to a constant is a quadratic equation in the real part of \"z\", a cycle is a parabola. The \"cyclic rotation\" of the dual number plane occurs as a motion of the projective line over dual numbers. According to Yaglom (pp. 92,3), the cycle Z = {z : y = α x} is invariant under the composition of the shear\nThis composition is a cyclic rotation; the concept has been further developed by V. V. Kisil.\n\nIn abstract algebra terms, the dual numbers can be described as the quotient of the polynomial ring R[\"X\"] by the ideal generated by the polynomial \"X\",\n\nThe image of \"X\" in the quotient is the unit ε. With this description, it is clear that the dual numbers form a commutative ring with characteristic 0. The inherited multiplication gives the dual numbers the structure of a commutative and associative algebra over the reals of dimension two. The algebra is \"not\" a division algebra or field since the elements of the form are not invertible. All elements of this form are zero divisors (also see the section \"Division\"). The algebra of dual numbers is isomorphic to the exterior algebra of formula_10.\n\nThis construction can be carried out more generally: for a commutative ring \"R\" one can define the dual numbers over \"R\" as the quotient of the polynomial ring \"R\"[\"X\"] by the ideal (\"X\"): the image of \"X\" then has square equal to zero and corresponds to the element ε from above.\n\nThis ring and its generalisations play an important part in the algebraic theory of derivations and Kähler differentials (purely algebraic differential forms).\n\nOver any ring \"R\", the dual number \"a\" + \"b\"ε is a unit (i.e. multiplicatively invertible) if and only if \"a\" is a unit in \"R\". In this case, the inverse of \"a\" + \"b\"ε is \"a\" − \"ba\"ε. As a consequence, we see that the dual numbers over any field (or any commutative local ring) form a local ring, its maximal ideal being the principal ideal generated by ε.\n\nA narrower generalization is that of introducing \"n\" anti-commuting generators; these are the Grassmann numbers or \"supernumbers\", discussed below.\n\nDual numbers find applications in physics, where they constitute one of the simplest non-trivial examples of a superspace. Equivalently, they are supernumbers with just one generator; supernumbers generalize the concept to distinct generators ε, each anti-commuting, possibly taking to infinity. Superspace generalizes supernumbers slightly, by allowing multiple commuting dimensions. \n\nThe motivation for introducing dual numbers into physics follows from the Pauli exclusion principle for fermions. The direction along ε is termed the \"fermionic\" direction, and the real component is termed the \"bosonic\" direction. The fermionic direction earns this name from the fact that fermions obey the Pauli exclusion principle: under the exchange of coordinates, the quantum mechanical wave function changes sign, and thus vanishes if two coordinates are brought together; this physical idea is captured by the algebraic relation ε = 0.\n\nOne application of dual numbers is automatic differentiation. Consider the real dual numbers above. Given any real polynomial \"P\"(\"x\") = \"p\"+\"p\"\"x\"+\"p\"\"x\"+...+\"p\"\"x\", it is straightforward to extend the domain of this polynomial from the reals to the dual numbers. Then we have this result:\n\nwhere formula_12 is the derivative of formula_13.\n\nBy computing over the dual numbers, rather than over the reals, we can use this to compute derivatives of polynomials.\n\nMore generally, we can extend any (analytic) real function to the dual numbers by looking at its Taylor series: formula_14, since any terms of involving formula_15 or greater are trivially formula_16 by the definition of formula_17.<br>\nBy computing compositions of these functions over the dual numbers and examining the coefficient of ε in the result we find we have automatically computed the derivative of the composition.\n\nA similar method works for polynomials of n variables, using the exterior algebra of an n-dimensional vector space.\n\nDivision of dual numbers is defined when the real part of the denominator is non-zero. The division process is analogous to complex division in that the denominator is multiplied by its conjugate in order to cancel the non-real parts.\n\nTherefore, to divide an equation of the form:\nWe multiply the top and bottom by the conjugate of the denominator:\nWhich is defined when c is non-zero.\n\nIf, on the other hand, c is zero while d is not, then the equation\nThis means that the non-real part of the \"quotient\" is arbitrary and division is therefore not defined for purely nonreal dual numbers. Indeed, they are (trivially) zero divisors and clearly form an ideal of the associative algebra (and thus ring) of the dual numbers.\n\nThe idea of a projective line over dual numbers was advanced by Grünwald and Corrado Segre.\n\nJust as the Riemann sphere needs a north pole point at infinity to close up the complex projective line, so a line at infinity succeeds in closing up the plane of dual numbers to a cylinder. \n\nSuppose D is the ring of dual numbers \"x\" + \"y\" ε and U is the subset with \"x\" ≠ 0. Then U is the group of units of D. Let B = {(\"a,b\") in D x D : \"a\" ∈ U or \"b\" ∈ U}. A relation is defined on B as follows: (\"a,b\") ~ (\"c,d\") when there is a \"u\" in U such that \"ua\"=\"c\" and \"ub\"=\"d\". This relation is in fact an equivalence relation. The points of the projective line over D are equivalence classes in B under this relation: P(D) = B/ ~.\n\nConsider the embedding D → P(D) by \"z\" → U(\"z\",1) where U(\"z\",1) is the equivalence class of (\"z\",1). Then points U(1,\"n\"), \"n\" = 0, are in P(D) but are not the image of any point under the embedding. P(D) is projected onto a cylinder by projection: Take a cylinder tangent to the double number plane on the line {\"y\" ε: \"y\" ∈ ℝ}, ε = 0. Now take the opposite line on the cylinder for the axis of a pencil of planes. The planes intersecting the dual number plane and cylinder provide a correspondence of points between these surfaces. The plane parallel to the dual number plane corresponds to points U(1,\"n\"), \"n\" = 0 in the projective line over dual numbers.\n\n\n"}
{"id": "546415", "url": "https://en.wikipedia.org/wiki?curid=546415", "title": "Extrapolation", "text": "Extrapolation\n\nIn mathematics, extrapolation is the process of estimating, beyond the original observation range, the value of a variable on the basis of its relationship with another variable. It is similar to interpolation, which produces estimates between known observations, but extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results. Extrapolation may also mean extension of a method, assuming similar methods will be applicable. Extrapolation may also apply to human experience to project, extend, or expand known experience into an area not known or previously experienced so as to arrive at a (usually conjectural) knowledge of the unknown (e.g. a driver extrapolates road conditions beyond his sight while driving). The extrapolation method can be applied in the interior reconstruction problem.\n\nA sound choice of which extrapolation method to apply relies on \"a prior knowledge\" of the process that created the existing data points. Some experts have proposed the use of causal forces in the evaluation of extrapolation methods. Crucial questions are, for example, if the data can be assumed to be continuous, smooth, possibly periodic etc.\n\nLinear extrapolation means creating a tangent line at the end of the known data and extending it beyond that limit. Linear extrapolation will only provide good results when used to extend the graph of an approximately linear function or not too far beyond the known data.\n\nIf the two data points nearest the point formula_1 to be extrapolated are formula_2 and formula_3, linear extrapolation gives the function:\n\n(which is identical to linear interpolation if formula_5). It is possible to include more than two points, and averaging the slope of the linear interpolant, by regression-like techniques, on the data points chosen to be included. This is similar to linear prediction.\n\nA polynomial curve can be created through the entire known data or just near the end. The resulting curve can then be extended beyond the end of the known data. Polynomial extrapolation is typically done by means of Lagrange interpolation or using Newton's method of finite differences to create a Newton series that fits the data. The resulting polynomial may be used to extrapolate the data.\n\nHigh-order polynomial extrapolation must be used with due care. For the example data set and problem in the figure above, anything above order 1 (linear extrapolation) will possibly yield unusable values; an error estimate of the extrapolated value will grow with the degree of the polynomial extrapolation. This is related to Runge's phenomenon.\n\nA conic section can be created using five points near the end of the known data. If the conic section created is an ellipse or circle, when extrapolated it will loop back and rejoin itself. An extrapolated parabola or hyperbola will not rejoin itself, but may curve back relative to the X-axis. This type of extrapolation could be done with a conic sections template (on paper) or with a computer.\n\nFrench curve extrapolation is a method suitable for any distribution that has a tendency to be exponential, but with accelerating or decelerating factors. This method has been used successfully in providing forecast projections of the growth of HIV/AIDS in the UK since 1987 and variant CJD in the UK for a number of years. Another study has shown that extrapolation can produce the same quality of forecasting results as more complex forecasting strategies.\n\nTypically, the quality of a particular method of extrapolation is limited by the assumptions about the function made by the method. If the method assumes the data are smooth, then a non-smooth function will be poorly extrapolated.\n\nIn terms of complex time series, some experts have discovered that extrapolation is more accurate when performed through the decomposition of causal forces.\n\nEven for proper assumptions about the function, the extrapolation can diverge severely from the function. The classic example is truncated power series representations of sin(\"x\") and related trigonometric functions. For instance, taking only data from near the \"x\" = 0, we may estimate that the function behaves as sin(\"x\") ~ \"x\". In the neighborhood of \"x\" = 0, this is an excellent estimate. Away from \"x\" = 0 however, the extrapolation moves arbitrarily away from the \"x\"-axis while sin(\"x\") remains in the interval [−1,1]. I.e., the error increases without bound.\n\nTaking more terms in the power series of sin(\"x\") around \"x\" = 0 will produce better agreement over a larger interval near \"x\" = 0, but will produce extrapolations that eventually diverge away from the \"x\"-axis even faster than the linear approximation.\n\nThis divergence is a specific property of extrapolation methods and is only circumvented when the functional forms assumed by the extrapolation method (inadvertently or intentionally due to additional information) accurately represent the nature of the function being extrapolated. For particular problems, this additional information may be available, but in the general case, it is impossible to satisfy all possible function behaviors with a workably small set of potential behavior.\n\nIn complex analysis, a problem of extrapolation may be converted into an interpolation problem by the change of variable formula_6. This transform exchanges the part of the complex plane inside the unit circle with the part of the complex plane outside of the unit circle. In particular, the compactification point at infinity is mapped to the origin and vice versa. Care must be taken with this transform however, since the original function may have had \"features\", for example poles and other singularities, at infinity that were not evident from the sampled data.\n\nAnother problem of extrapolation is loosely related to the problem of analytic continuation, where (typically) a power series representation of a function is expanded at one of its points of convergence to produce a power series with a larger radius of convergence. In effect, a set of data from a small region is used to extrapolate a function onto a larger region.\n\nAgain, analytic continuation can be thwarted by function features that were not evident from the initial data.\n\nAlso, one may use sequence transformations like Padé approximants and Levin-type sequence transformations as extrapolation methods that lead to a summation of power series that are divergent outside the original radius of convergence. In this case, one often obtains \nrational approximants.\n\nThe extrapolated data often convolute to a kernel function. After data is extrapolated, the size of data is increased N times, here N is approximately 2–3. If this data needs to be convoluted to a known kernel function, the numerical calculations will increase Nlog(N) times even with fast Fourier transform (FFT). There exists an algorithm, it analytically calculates the contribution from the part of the extrapolated data. The calculation time can be omitted compared with the original convolution calculation. Hence with this algorithm the calculations of a convolution using the extrapolated data is nearly not increased. This is referred as the fast extrapolation. The fast extrapolation has been applied to CT image reconstruction.\n\nExtrapolation arguments are informal and unquantified arguments which assert that something is true beyond the range of values for which it is known to be true. For example, we believe in the reality of what we see through magnifying glasses because it agrees with what we see with the naked eye but extends beyond it; we believe in what we see through light microscopes because it agrees with what we see through magnifying glasses but extends beyond it; and similarly for electron microscopes.\n\nLike slippery slope arguments, extrapolation arguments may be strong or weak depending on such factors as how far the extrapolation goes beyond the known range.\n\n\n"}
{"id": "52590134", "url": "https://en.wikipedia.org/wiki?curid=52590134", "title": "Formal holomorphic function", "text": "Formal holomorphic function\n\nIn algebraic geometry, a formal holomorphic function along a subvariety \"V\" of an algebraic variety \"W\" is an algebraic analog of a holomorphic function defined in a neighborhood of \"V\". They are sometimes just called holomorphic functions when no confusion can arise. They were introduced by .\n\nThe theory of formal holomorphic functions has largely been replaced by the theory of formal schemes which generalizes it: a formal holomorphic function on a variety is essentially just a section of the structure sheaf of a related formal scheme.\n\nIf \"V\" is an affine subvariety of the affine variety \"W\" defined by an ideal \"I\" of the coordinate ring \"R\" of \"W\", then a formal holomorphic function along \"V\" is just an element of the completion of \"R\" at the ideal \"I\". \n\nIn general holomorphic functions along a subvariety \"V\" of \"W\" are defined by gluing together holomorphic functions on affine subvarieties. \n\n"}
{"id": "2168672", "url": "https://en.wikipedia.org/wiki?curid=2168672", "title": "Gertrude Mary Cox", "text": "Gertrude Mary Cox\n\nGertrude Mary Cox (January 13, 1900 – October 17, 1978) was an American statistician and founder of the department of Experimental Statistics at North Carolina State University. She was later appointed director of both the Institute of Statistics of the Consolidated University of North Carolina and the Statistics Research Division of North Carolina State University. Her most important and influential research dealt with experimental design; she wrote an important book on the subject with W. G. Cochran. In 1949 Cox became the first female elected into the International Statistical Institute and in 1956 was President of the American Statistical Association.\n\nGertrude Cox was born in Dayton, Iowa on January 13, 1900. She studied at Perry High School in Perry, Iowa, graduating in 1918. At this time she decided to become a deaconess in the Methodist Church and worked towards that end. However, in 1925, she decided to continue her education at Iowa State College in Ames where she studied mathematics and statistics and was awarded a B.S. in 1929 and a Master's degree in statistics in 1931.\n\nFrom 1931 to 1933 Cox undertook graduate studies in psychological statistics at the University of California at Berkeley, then returned to Iowa State College to assist in establishing the new Statistical Laboratory. Here she worked on the design of experiments.\n\nIn 1939 Cox was appointed assistant professor of statistics at Iowa State College.\nIn 1940 Cox was appointed professor of statistics at North Carolina State College (now North Carolina State University) at Raleigh. There she headed the new department of Experimental Statistics, the first female head of any department at this institution. In 1945 she became director of the Institute of Statistics of the Consolidated University of North Carolina, and the Statistics Research Division of the North Carolina State College which was run by William Gemmell Cochran. In the same year of 1945 Cox became the editor of \"Biometrics Bulletin\" and of \"Biometrics\" and she held this editorship for 10 years. In 1947 she was a founder member of the International Biometric Society.\n\nIn 1960 she took up her final post as Director of Statistics at the Research Triangle Institute in Durham, North Carolina. She held this post until she retired in 1965. After retirement, then worked as a consultant to promote the development of statistical programs in Egypt and Thailand. \n\nIn 1950 she published a joint work with William Cochran, \"Experimental Design\", which became the major reference work on the design of experiments for statisticians for years afterwards.\n\nCox received many honors. In 1949 she became the first woman elected into the International Statistical Institute. In 1956 she was elected President of the American Statistical Association while in 1975 she was elected to the National Academy of Sciences.\nShe was also a Fellow of the Institute of Mathematical Statistics. \n\nThe University of North Carolina system named her an O. Max Gardner Award recipient in 1959. North Carolina State University honored Cox by naming Cox Hall in her honor in 1970, and awarding her a Watauga Medal in 1977. The Caucus of Women in Statistics also established a Gertrude M. Cox Scholarship fund in recognition of her work in 1986. \n\n"}
{"id": "51376302", "url": "https://en.wikipedia.org/wiki?curid=51376302", "title": "Glivenko's theorem (probability theory)", "text": "Glivenko's theorem (probability theory)\n\nIn probability theory, Glivenko's theorem states that if formula_1, formula_2 are the characteristic functions of some probability distributions formula_3 respectively and formula_4 almost everywhere, then formula_5 in the sense of probability distributions.\n"}
{"id": "12942590", "url": "https://en.wikipedia.org/wiki?curid=12942590", "title": "Gromov product", "text": "Gromov product\n\nIn mathematics, the Gromov product is a concept in the theory of metric spaces named after the mathematician Mikhail Gromov. The Gromov product can also be used to define \"δ\"-hyperbolic metric spaces in the sense of Gromov.\n\nLet (\"X\", \"d\") be a metric space and let \"x\", \"y\", \"z\" ∈ \"X\". Then the Gromov product of \"y\" and \"z\" at \"x\", denoted (\"y\", \"z\"), is defined by\n\nGiven three points \"x\", \"y\", \"z\" in the metric space \"X\", by the triangle inequality there exist non-negative numbers \"a\", \"b\", \"c\" such that formula_2. Then the Gromov products are formula_3. In the case that the points \"x\", \"y\", \"z\" are the outer nodes of a tripod then these Gromov products are the lengths of the edges.\n\nIn the hyperbolic, spherical or euclidean plane, the Gromov product (\"A\", \"B\") equals the distance \"p\" between \"C\" and the point where the incircle of the geodesic triangle \"ABC\" touches the edge \"CB\" or \"CA\". Indeed from the diagram , so that . Thus for any metric space, a geometric interpretation of (\"A\", \"B\") is obtained by isometrically embedding (A, B, C) into the euclidean plane.\n\n\nConsider hyperbolic space H. Fix a base point \"p\" and let formula_8 and formula_9 be two distinct points at infinity. Then the limit \nexists and is finite, and therefore can be considered as a generalized Gromov product. It is actually given by the formula \nwhere formula_12 is the angle between the geodesic rays formula_13 and formula_14.\n\nThe Gromov product can be used to define \"δ\"-hyperbolic spaces in the sense of Gromov.: (\"X\", \"d\") is said to be \"δ\"-hyperbolic if, for all \"p\", \"x\", \"y\" and \"z\" in \"X\",\n\nIn this case. Gromov product measures how long geodesics remain close together. Namely, if \"x\", \"y\" and \"z\" are three points of a \"δ\"-hyperbolic metric space then the initial segments of length (\"y\", \"z\") of geodesics from \"x\" to \"y\" and \"x\" to \"z\" are no further than 2\"δ\" apart (in the sense of the Hausdorff distance between closed sets).\n\n"}
{"id": "54651193", "url": "https://en.wikipedia.org/wiki?curid=54651193", "title": "Group-stack", "text": "Group-stack\n\nIn algebraic geometry, a group-stack is an algebraic stack whose categories of points have group structures or even groupoid structures in a compatible way. It generalizes a group scheme, which is a scheme whose sets of points have group structures in a compatible way.\n\n\nThe definition of a group action of a group-stack is a bit tricky. First, given an algebraic stack \"X\" and a group scheme \"G\" on a base scheme \"S\", a right action of \"G\" on \"X\" consists of\nthat satisfy the typical compatibility conditions.\n\nIf, more generally, \"G\" is a group-stack, one then extends the above using local presentations.\n"}
{"id": "250835", "url": "https://en.wikipedia.org/wiki?curid=250835", "title": "Henry Martyn", "text": "Henry Martyn\n\nHenry Martyn (18 February 1781 – 16 October 1812) was an Anglican priest and missionary to the peoples of India and Persia. Born in Truro, Cornwall, he was educated at Truro Grammar School and St John's College, Cambridge. A chance encounter with Charles Simeon led him to become a missionary. He was ordained a priest in the Church of England and became a chaplain for the British East India Company.\n\nMartyn arrived in India in April 1806, where he preached and occupied himself in the study of linguistics. He translated the whole of the New Testament into Urdu, Persian and Judaeo-Persic. He also translated the Psalms into Persian and the Book of Common Prayer into Urdu. From India, he set out for Bushire, Shiraz, Isfahan, and Tabriz.\n\nMartyn was seized with fever, and, though the plague was raging at Tokat, he was forced to stop there, unable to continue. On 16 October 1812 he died. He was remembered for his courage, selflessness and his religious devotion. In parts of the Anglican Communion he is celebrated with a Lesser Festival on 19 October.\n\nMartyn was born in Truro, Cornwall. His father, John Martyn, was a \"captain\" or mine-agent at Gwennap. As a boy, he was educated at Truro grammar school under Dr. Cardew and he entered St John's College, Cambridge, in the autumn of 1797, and was senior wrangler and first Smith's prizeman in 1801. In 1802, he was chosen as a fellow of his college.\n\nHe had intended to go to the bar, but in the October term of 1802 he chanced to hear Charles Simeon speaking of the good done in India by a single missionary, William Carey, and some time afterwards he read the life of David Brainerd, a missionary to the Native Americans. He resolved, accordingly, to become a missionary himself. On 22 October 1803, he was ordained deacon at Ely, and afterwards priest, and served as Simeon's curate at the Church of Holy Trinity, taking charge of the neighbouring parish of Lolworth.\n\nMartyn wanted to offer his services to the Church Missionary Society, when a financial disaster in Cornwall deprived him and his unmarried sister of the income their father had left for them. It was necessary for Martyn to earn an income that would support his sister as well as himself. He accordingly obtained a chaplaincy under the British East India Company and left for India on 5 July 1805. On his voyage to the East, Martyn happened to be present at the British conquest of the Cape Colony on 8 January 1806. He spent that day tending to the dying soldiers and was distressed by seeing the horrors of war. He would come away feeling that it was Britain's destiny to convert, not colonize, the world. He wrote in his diary:\n\nMartyn arrived in India in April 1806, and for some months he was stationed at Aldeen, near Serampur. In October 1806, he proceeded to Dinapur, where he was soon able to conduct worship among the locals in the vernacular, and established schools. In April 1809, he was transferred to Cawnpore, where he preached to British and Indians in his own compound, in spite of interruptions and threats from local non-Christians.\n\nHe occupied himself in linguistic study, and had already, during his residence at Dinapur, been engaged in revising the sheets of his Hindustani version of the New Testament. He now translated the whole of the New Testament into Urdu also, and into Persian twice. He translated the Psalms into Persian, the Gospels into Judaeo-Persic, and the Book of Common Prayer into Urdu, in spite of ill-health and \"the pride, pedantry and fury of his chief munshi Sabat.\" Ordered by the doctors to take a sea voyage, he obtained leave to go to Persia and correct his Persian New Testament. From there, he wanted to go to Arabia, and there compose an Arabic version. On 1 October 1810, having seen his work at Cawnpore rewarded on the previous day by the opening of a church, he left for Calcutta, from where he sailed on 7 January 1811 for Bombay. The ship reached port on his thirtieth birthday.\n\nFrom Bombay he set out for Bushire, bearing letters from Sir John Malcolm to men of position there, as also at Shiraz and Isfahan. After an exhausting journey from the coast he reached Shiraz, and was soon plunged into discussion with the disputants of all classes, \"Sufi, Muslim, Jew, and Jewish Muslim, even Armenian, all anxious to test their powers of argument with the first English priest who had visited them.\" He next traveled to Tabriz to attempt to present the Shah with his translation of the New Testament, which proved unsuccessful. Sir Gore Ouseley, the British ambassador to the Shah, was unable to bring about a meeting, but did deliver the manuscript. Although Martyn could not present the Bible in person, the Shah later wrote him a letter:\n\nAt this time, he was seized with fever, and after a temporary recovery, had to seek a change of climate. He set off for Constantinople, where he intended to return on furlough to England to regain his strength and recruit help for the missions in India. On 12 September 1812, he started with two Armenian servants and crossed the Aras River. Urged on from place to place by their Tatar guide, they rode from Tabriz to Erivan, from Erivan to Kars, and from Kars to Erzurum. They departed Erzurum and though the plague was raging at Tokat, he was forced to stop there, unable to continue. He wrote his final journal entry on 6 October. It read, in part:\n\nOn 16 October 1812 he died and was given a Christian burial by Armenian clergy.\n\nHe was heard to say, \"Let me burn out for God\". An indication of his zeal for the things of God.\n\nHis devotion to his tasks won him much admiration in Great Britain and he was the hero of a number of literary publications. Thomas Babington Macaulay's \"Epitaph\", composed early in 1813, testified to the impression made by his career:\n\nAn institution was established in his name in India, called the Henry Martyn Institute: An Interfaith Centre for Reconciliation and Research, Hyderabad, India. John McManners wrote in his \"Oxford Illustrated History of Christianity\" that Martyn was a man remembered for his courage, selflessness and his religious devotion. In parts of the Anglican Communion he is celebrated with a Lesser Festival on 19 October.\n\nThe Henry Martyn Trust based in Cambridge, England can trace its history back to 1897, at a time of great enthusiasm in Cambridge for overseas missions, when an appeal was launched for a 'Proposed Missionary Library for Cambridge University', to be housed in the Henry Martyn Hall, erected ten years previously.\n\nThe Henry Martyn Library opened in the Hall in 1898, and there it remained as a small collection of missionary biographies and other books until 1995. The evolution of the Henry Martyn Library into the present Henry Martyn Centre began in 1992, when Canon Graham Kings was appointed as the first Henry Martyn Lecturer in Missiology in the Cambridge Theological Federation.\n\nIn 1999 the Centre became an Associate Institute of the Cambridge Theological Federation, one of the largest providers of theological education in the United Kingdom. The Library and the Henry Martyn Centre are now housed at Westminster College. Today, the Centre continues to seek to promote the study of mission and world Christianity, developing strong links with mission study centres around the world and fulfilling the same aim that was stated by the founders of the Library in 1897.\n\n\n\n"}
{"id": "3948758", "url": "https://en.wikipedia.org/wiki?curid=3948758", "title": "Heteroclinic orbit", "text": "Heteroclinic orbit\n\nIn mathematics, in the phase portrait of a dynamical system, a heteroclinic orbit (sometimes called a heteroclinic connection) is a path in phase space which joins two different equilibrium points. If the equilibrium points at the start and end of the orbit are the same, the orbit is a homoclinic orbit.\n\nConsider the continuous dynamical system described by the ODE\nSuppose there are equilibria at formula_2 and formula_3, then a solution formula_4 is a heteroclinic orbit from formula_5 to formula_6 if\nand\n\nThis implies that the orbit is contained in the stable manifold of formula_6 and the unstable manifold of formula_5.\n\nBy using the Markov partition, the long-time behaviour of hyperbolic system can be studied using the techniques of symbolic dynamics. In this case, a heteroclinic orbit has a particularly simple and clear representation. Suppose that formula_11 is a finite set of \"M\" symbols. The dynamics of a point \"x\" is then represented by a bi-infinite string of symbols \n\nA periodic point of the system is simply a recurring sequence of letters. A heteroclinic orbit is then the joining of two distinct periodic orbits. It may be written as\n\nwhere formula_14 is a sequence of symbols of length \"k\", (of course, formula_15), and formula_16 is another sequence of symbols, of length \"m\" (likewise, formula_17). The notation formula_18 simply denotes the repetition of \"p\" an infinite number of times. Thus, a heteroclinic orbit can be understood as the transition from one periodic orbit to another. By contrast, a homoclinic orbit can be written as \n\nwith the intermediate sequence formula_20 being non-empty, and, of course, not being \"p\", as otherwise, the orbit would simply be formula_18.\n\n\n"}
{"id": "18808748", "url": "https://en.wikipedia.org/wiki?curid=18808748", "title": "Jacobi's four-square theorem", "text": "Jacobi's four-square theorem\n\nJacobi's four-square theorem gives a formula for the number of ways that a given positive integer \"n\" can be represented as the sum of four squares.\n\nThe theorem was proved in 1834 by Carl Gustav Jakob Jacobi.\n\nTwo representations are considered different if their terms are in different order or if the integer being squared (not just the square) is different; to illustrate, these are three of the eight different ways to represent 1:\n\nThe number of ways to represent n as the sum of four squares is eight times the sum of the divisors of \"n\" if \"n\" is odd and 24 times the sum of the odd divisors of \"n\" if \"n\" is even (see divisor function), i.e.\n\nEquivalently, it is eight times the sum of all its divisors which are not divisible by 4, i.e.\n\nWe may also write this as\n\nwhere the second term is to be taken as zero if \"n\" is not divisible by 4. In particular, for a prime number \"p\" we have the explicit formula \"r\"(\"p\") = 8(\"p\" + 1).\n\nSome values of \"r\"(\"n\") occur infinitely often as \"r\"(\"n\") = \"r\"(2\"n\") whenever \"n\" is even. The values of \"r\"(\"n\")/\"n\" can be arbitrarily large: indeed, \"r\"(\"n\")/\"n\" is infinitely often larger than 8.\n\nThe theorem can be proved by elementary means starting with the Jacobi triple product.\n\nThe proof shows that the Theta series for the lattice Z is a modular form of a certain level, and hence equals a linear combination of Eisenstein series.\n\n\n"}
{"id": "40816534", "url": "https://en.wikipedia.org/wiki?curid=40816534", "title": "Julia F. Knight", "text": "Julia F. Knight\n\nJulia Frandsen Knight is an American mathematician, specializing in model theory and computability theory. She is the Charles L. Huisking Professor of Mathematics at the University of Notre Dame and director of the graduate program in mathematics there.\n\nKnight did her undergraduate studies at Utah State University, graduating in 1964, and earned her Ph.D. from the University of California, Berkeley in 1972 under the supervision of Robert Lawson Vaught.\n\nIn 2012 she became a fellow of the American Mathematical Society.\n"}
{"id": "23950557", "url": "https://en.wikipedia.org/wiki?curid=23950557", "title": "Law of total covariance", "text": "Law of total covariance\n\nIn probability theory, the law of total covariance, covariance decomposition formula, or conditional covariance formula states that if \"X\", \"Y\", and \"Z\" are random variables on the same probability space, and the covariance of \"X\" and \"Y\" is finite, then\n\nThe nomenclature in this article's title parallels the phrase \"law of total variance\". Some writers on probability call this the \"conditional covariance formula\" or use other names.\n\nThe law of total covariance can be proved using the law of total expectation: First,\n\nfrom the definition of covariance. Then we apply the law of total expectation by conditioning on the random variable \"Z\":\n\nNow we rewrite the term inside the first expectation using the definition of covariance:\n\nSince expectation of a sum is the sum of expectations, we can regroup the terms:\n\nFinally, we recognize the final two terms as the covariance of the conditional expectations E[\"X\"|\"Z\"] and E[\"Y\"|\"Z\"]:\n\n"}
{"id": "19234105", "url": "https://en.wikipedia.org/wiki?curid=19234105", "title": "Lawrence–Krammer representation", "text": "Lawrence–Krammer representation\n\nIn mathematics the Lawrence–Krammer representation is a representation of the braid groups. It fits into a family of representations called the Lawrence representations. The first Lawrence representation is the Burau representation and the second is the Lawrence–Krammer representation.\n\nThe Lawrence–Krammer representation is named after Ruth Lawrence and Daan Krammer.\n\nConsider the braid group formula_1 to be the mapping class group of a disc with \"n\" marked points, formula_2. The Lawrence–Krammer representation is defined as the action of formula_1 on the homology of a certain covering space of the configuration space formula_4. Specifically, the first integral homology group of formula_4 is isomorphic to formula_6, and the subgroup of formula_7 invariant under the action of formula_1 is primitive, free abelian, and of rank 2. Generators for this invariant subgroup are denoted by formula_9.\n\nThe covering space of formula_4 corresponding to the kernel of the projection map\n\nis called the Lawrence–Krammer cover and is denoted formula_12. Diffeomorphisms offormula_2 act on formula_2, thus also on formula_4, moreover they lift uniquely to diffeomorphisms of formula_12 which restrict to the identity on the co-dimension two boundary stratum (where both points are on the boundary circle). The action of formula_1 on\n\nthought of as a\n\nis the Lawrence–Krammer representation. The group formula_20 is known to be a free formula_19-module, of rank formula_22.\n\nUsing Bigelow's conventions for the Lawrence–Krammer representation, generators for the group formula_20 are denoted formula_24 for formula_25. Letting formula_26 denote the standard Artin generators of the braid group, we obtain the expression:\n\nformula_27\n\nStephen Bigelow and Daan Krammer have given independent proofs that the Lawrence–Krammer representation is faithful.\n\nThe Lawrence–Krammer representation preserves a non-degenerate sesquilinear form which is known to be negative-definite Hermitian provided formula_9 are specialized to suitable unit complex numbers (\"q\" near 1 and \"t\" near \"i\"). Thus the braid group is a subgroup of the unitary group of square matrices of size formula_29. Recently it has been shown that the image of the Lawrence–Krammer representation is a dense subgroup of the unitary group in this case.\n\nThe sesquilinear form has the explicit description:\n\n\\left\\{\n-q^2t^2(q-1) & i=k<j<l \\text{ or } i<k<j=l \\\\\n-(q-1) & k=i<l<j \\text{ or } k\n\n"}
{"id": "152111", "url": "https://en.wikipedia.org/wiki?curid=152111", "title": "Linear function", "text": "Linear function\n\nIn mathematics, the term linear function refers to two distinct but related notions:\n\nIn calculus, analytic geometry and related areas, a linear function is a polynomial of degree one or less, including the zero polynomial (the latter not being considered to have degree zero).\n\nWhen the function is of only one variable, it is of the form\nwhere and are constants, often real numbers. The graph of such a function of one variable is a nonvertical line. is frequently referred to as the slope of the line, and as the intercept.\n\nFor a function formula_2 of any finite number of independent variables, the general formula is\nand the graph is a hyperplane of dimension .\n\nA constant function is also considered linear in this context, as it is a polynomial of degree zero or is the zero polynomial. Its graph, when there is only one independent variable, is a horizontal line.\n\nIn this context, the other meaning (a linear map) may be referred to as a homogeneous linear function or a linear form. In the context of linear algebra, this meaning (polynomial functions of degree 0 or 1) is a special kind of affine map.\n\nIn linear algebra, a linear function is a map \"f\" between two vector spaces that preserves vector addition and scalar multiplication:\nHere denotes a constant belonging to some field of scalars (for example, the real numbers) and and are elements of a vector space, which might be itself.\n\nSome authors use \"linear function\" only for linear maps that take values in the scalar field; these are also called linear functionals.\n\nThe \"linear functions\" of calculus qualify as \"linear maps\" when (and only when) formula_6, or, equivalently, when the constant formula_7. Geometrically, the graph of the function must pass through the origin.\n\n\n"}
{"id": "44390320", "url": "https://en.wikipedia.org/wiki?curid=44390320", "title": "List of arbitrary-precision arithmetic software", "text": "List of arbitrary-precision arithmetic software\n\nThis article lists libraries, applications, and other software which enable or support arbitrary-precision arithmetic.\n\nSoftware that supports arbitrary precision computations:\n\n\nProgramming languages that supports arbitrary precision computations, either built-in, or in the standard library of the language:\n\n\nFor once-off calculations. Runs on server or in browser. No installation or compilation required.\n"}
{"id": "43258526", "url": "https://en.wikipedia.org/wiki?curid=43258526", "title": "Mean of a function", "text": "Mean of a function\n\nIn calculus, and especially multivariable calculus, the mean of a function is loosely defined as the average value of the function over its domain. In one variable, the mean of a function \"f\"(\"x\") over the interval (\"a,b\") is defined by\n\nRecall that a defining property of the average value formula_2 of finitely many numbers formula_3\nis that formula_4. In other words, formula_2 is the \"constant\" value which when\n\"added\" to itself formula_6 times equals the result of adding the formula_6 terms of formula_8. By analogy, a\ndefining property of the average value formula_9 of a function over the interval formula_10 is that\n\nIn other words, formula_9 is the \"constant\" value which when \"integrated\" over formula_10 equals the result of\nintegrating formula_14 over formula_10. But by the second fundamental theorem of calculus, the integral of a constant\nformula_9 is just\n\nSee also the first mean value theorem for integration, which guarantees\nthat if formula_18 is continuous then there exists a point formula_19 such that\n\nThe point formula_21 is called the mean value of formula_14 on formula_10. So we write\nformula_24 and rearrange the preceding equation to get the above definition.\n\nIn several variables, the mean over a relatively compact domain \"U\" in a Euclidean space is defined by\n\nThis generalizes the arithmetic mean. On the other hand, it is also possible to generalize the geometric mean to functions by defining the geometric mean of \"f\" to be\n\nMore generally, in measure theory and probability theory, either sort of mean plays an important role. In this context, Jensen's inequality places sharp estimates on the relationship between these two different notions of the mean of a function.\n\nThere is also a \"harmonic average\" of functions and a \"quadratic average\" (or \"root mean square\") of functions.\n\n"}
{"id": "55000798", "url": "https://en.wikipedia.org/wiki?curid=55000798", "title": "Minimal algebra", "text": "Minimal algebra\n\nA minimal algebra is a finite algebra with more than one element, in which every non-constant unary polynomial is a permutation on its domain. A minimal algebra formula_1 falls into one of the following types \n"}
{"id": "24065513", "url": "https://en.wikipedia.org/wiki?curid=24065513", "title": "Mixed mating model", "text": "Mixed mating model\n\nThe mixed mating model is a mathematical model that describes the mating system of a plant population in terms of the degree of self-fertilisation present. It is a fairly simplistic model, employing several simplifying assumptions, most notably the assumption that every fertilisation event may be classed as either self-fertilisation, or outcrossing with a completely random mate. Thus the only model parameter to be estimated is the probability of self-fertilisation.\n\nThe mixed mating model originated in the 1910s, with plant breeders who were seeking evidence of outcrossing contamination of self-pollinating crops, but a formal description of the model and its parameter estimation was not published until 1951. The model is still in common use today, though a number of more complex models are also now in use. For example, a weakness of the model lies in its assumption that inbreeding occurs only as a result of self-fertilisation; in reality, inbreeding may also occur through outcrossing between closely related individuals. The effective selfing model relaxes this assumption by seeking also to estimate the degree of shared ancestry of outcrossing mates.\n"}
{"id": "41276205", "url": "https://en.wikipedia.org/wiki?curid=41276205", "title": "Module spectrum", "text": "Module spectrum\n\nIn algebra, a module spectrum is a spectrum with an action of a ring spectrum; it generalizes a module in abstract algebra.\n\nThe ∞-category of (say right) module spectra is stable; hence, it can be considered as either analog or generalization of the derived category of modules over a ring.\n\nLurie defines the K-theory of a ring spectrum \"R\" to be the K-theory of the ∞-category of perfect modules over \"R\" (a perfect module being defined as a compact object in the ∞-category of module spectra.)\n\n\n"}
{"id": "9755509", "url": "https://en.wikipedia.org/wiki?curid=9755509", "title": "Mosco convergence", "text": "Mosco convergence\n\nIn mathematical analysis, Mosco convergence is a notion of convergence for functionals that is used in nonlinear analysis and set-valued analysis. It is a particular case of Γ-convergence. Mosco convergence is sometimes phrased as “weak Γ-liminf and strong Γ-limsup” convergence since it uses both the weak and strong topologies on a topological vector space \"X\". In finite dimensional spaces, Mosco convergence coincides with epi-convergence.\n\n\"Mosco convergence\" is named after Italian mathematician Umberto Mosco, a current Harold J. Gay professor of mathematics at Worcester Polytechnic Institute.\n\nLet \"X\" be a topological vector space and let \"X\" denote the dual space of continuous linear functionals on \"X\". Let \"F\" : \"X\" → [0, +∞] be functionals on \"X\" for each \"n\" = 1, 2, ... The sequence (or, more generally, net) (\"F\") is said to Mosco converge to another functional \"F\" : \"X\" → [0, +∞] if the following two conditions hold:\n\n\n\nSince lower and upper bound inequalities of this type are used in the definition of Γ-convergence, Mosco convergence is sometimes phrased as “weak Γ-liminf and strong Γ-limsup” convergence. Mosco convergence is sometimes abbreviated to M-convergence and denoted by\n\n"}
{"id": "40547030", "url": "https://en.wikipedia.org/wiki?curid=40547030", "title": "Multibody simulation", "text": "Multibody simulation\n\nMultibody simulation (MBS) is a method of numerical simulation in which multibody systems are composed of various rigid or elastic bodies. Connections between the bodies can be modeled with kinematic constraints (such as joints) or force elements (such as spring dampers). Unilateral constraints and Coulomb-friction can also be used to model frictional contacts between bodies.\nMultibody simulation is a useful tool for conducting motion analysis. It is often used during product development to evaluate characteristics of comfort, safety, and performance. For example, multibody simulation has been widely used since the 1990s as a component of automotive suspension design. It can also be used to study issues of biomechanics, with applications including sports medicine, osteopathy, and human-machine interaction.\n\nThe heart of any multibody simulation software program is the solver. The solver is a set of computation algorithms that solve equations of motion. Types of components that can be studied through multibody simulation range from electronic control systems to noise, vibration and harshness. Complex models such as engines are composed of individually designed components, e.g. pistons/crankshafts.\n\nThe MBS process often can be divided in 5 main activities. The first activity of the MBS process chain is the” 3D CAD master model”, in which product developers, designers and engineers are using the CAD system to generate a CAD model and its assembly structure related to given specifications. This 3D CAD master model is converted during the activity “Data transfer” to the MBS input data formats i.e. STEP. The “MBS Modeling” is the most complex activity in the process chain. Following rules and experiences, the 3D model in MBS format, multiple boundaries, kinematics, forces, moments or degrees of freedom are used as input to generate the MBS model. Engineers have to use MBS software and their knowledge and skills in the field of engineering mechanics and machine dynamics to build the MBS model including joints and links. The generated MBS model is used during the next activity “Simulation”. Simulations, which are specified by time increments and boundaries like starting conditions are run by MBS Software i.e. MSC ADAMS. The last activity is the “Analysis and evaluation”. Engineers use case-dependent directives to analyze and evaluate moving paths, speeds, accelerations, forces or moments. The results are used to enable releases or to improve the MBS model, in case the results are insufficient. One of the most important benefits of the MBS process chain is the usability of the results to optimize the 3D CAD master model components. Due to the fact that the process chain enables the optimization of component design, the resulting loops can be used to achieve a high level of design and MBS model optimization in an iterative process.\n"}
{"id": "22210", "url": "https://en.wikipedia.org/wiki?curid=22210", "title": "One-time pad", "text": "One-time pad\n\nIn cryptography, the one-time pad (OTP) is an encryption technique that cannot be cracked, but requires the use of a one-time pre-shared key the same size as, or longer than, the message being sent. In this technique, a plaintext is paired with a random secret key (also referred to as \"a one-time pad\"). Then, each bit or character of the plaintext is encrypted by combining it with the corresponding bit or character from the pad using modular addition. If the key is truly random, is at least as long as the plaintext, is never reused in whole or in part, and is kept completely secret, then the resulting ciphertext will be impossible to decrypt or break. It has also been proven that any cipher with the perfect secrecy property must use keys with effectively the same requirements as OTP keys. Digital versions of one-time pad ciphers have been used by nations for some critical diplomatic and military communication, but the problems of secure key distribution have made them impractical for most applications.\n\nFirst described by Frank Miller in 1882, the one-time pad was re-invented in 1917. On July 22, 1919, U.S. Patent 1,310,719 was issued to Gilbert S. Vernam for the XOR operation used for the encryption of a one-time pad. Derived from his \"Vernam cipher\", the system was a cipher that combined a message with a key read from a punched tape. In its original form, Vernam's system was vulnerable because the key tape was a loop, which was reused whenever the loop made a full cycle. One-time use came later, when Joseph Mauborgne recognized that if the key tape were totally random, then cryptanalysis would be impossible.\n\nThe \"pad\" part of the name comes from early implementations where the key material was distributed as a pad of paper, so that the top sheet could be easily torn off and destroyed after use. For ease of concealment, the pad was sometimes reduced to such a small size that a powerful magnifying glass was required to use it. The KGB used pads of such size that they could fit in the palm of a hand, or in a walnut shell. To increase security, one-time pads were sometimes printed onto sheets of highly flammable nitrocellulose, so that they could be quickly burned after use.\n\nThere is some ambiguity to the term \"Vernam cipher\" because some sources use \"Vernam cipher\" and \"one-time pad\" synonymously, while others refer to any additive stream cipher as a \"Vernam cipher\", including those based on a cryptographically secure pseudorandom number generator (CSPRNG).\n\nFrank Miller in 1882 was the first to describe the one-time pad system for securing telegraphy.\n\nThe next one-time pad system was electrical. In 1917, Gilbert Vernam (of AT&T Corporation) invented and later patented in 1919 () a cipher based on teleprinter technology. Each character in a message was electrically combined with a character on a punched paper tape key. Joseph Mauborgne (then a captain in the U.S. Army and later chief of the Signal Corps) recognized that the character sequence on the key tape could be completely random and that, if so, cryptanalysis would be more difficult. Together they invented the first one-time tape system.\n\nThe next development was the paper pad system. Diplomats had long used codes and ciphers for confidentiality and to minimize telegraph costs. For the codes, words and phrases were converted to groups of numbers (typically 4 or 5 digits) using a dictionary-like codebook. For added security, secret numbers could be combined with (usually modular addition) each code group before transmission, with the secret numbers being changed periodically (this was called superencryption). In the early 1920s, three German cryptographers (Werner Kunze, Rudolf Schauffler and Erich Langlotz), who were involved in breaking such systems, realized that they could never be broken if a separate randomly chosen additive number was used for every code group. They had duplicate paper pads printed with lines of random number groups. Each page had a serial number and eight lines. Each line had six 5-digit numbers. A page would be used as a work sheet to encode a message and then destroyed. The serial number of the page would be sent with the encoded message. The recipient would reverse the procedure and then destroy his copy of the page. The German foreign office put this system into operation by 1923.\n\nA separate notion was the use of a one-time pad of letters to encode plaintext directly as in the example below. Leo Marks describes inventing such a system for the British Special Operations Executive during World War II, though he suspected at the time that it was already known in the highly compartmentalized world of cryptography, as for instance at Bletchley Park.\n\nThe final discovery was made by information theorist Claude Shannon in the 1940s who recognized and proved the theoretical significance of the one-time pad system. Shannon delivered his results in a classified report in 1945, and published them openly in 1949. At the same time, Soviet information theorist Vladimir Kotelnikov had independently proved absolute security of the one-time pad; his results were delivered in 1941 in a report that apparently remains classified.\n\nSuppose Alice wishes to send the message \"HELLO\" to Bob. Assume two pads of paper containing identical random sequences of letters were somehow previously produced and securely issued to both. Alice chooses the appropriate unused page from the pad. The way to do this is normally arranged for in advance, as for instance 'use the 12th sheet on 1 May', or 'use the next available sheet for the next message'.\n\nThe material on the selected sheet is the \"key\" for this message. Each letter from the pad will be combined in a predetermined way with one letter of the message. (It is common, but not required, to assign each letter a numerical value, e.g., \"A\" is 0, \"B\" is 1, and so on.)\n\nIn this example, the technique is to combine the key and the message using modular addition. The numerical values of corresponding message and key letters are added together, modulo 26. So, if key material begins with \"XMCKL\" and the message is \"HELLO\", then the coding would be done as follows:\n\nIf a number is larger than 26, then the remainder after subtraction of 26 is taken in modular arithmetic fashion. This simply means that if the computations \"go past\" Z, the sequence starts again at A.\n\nThe ciphertext to be sent to Bob is thus \"EQNVZ\". Bob uses the matching key page and the same process, but in reverse, to obtain the plaintext. Here the key is \"subtracted\" from the ciphertext, again using modular arithmetic:\n\nSimilar to the above, if a number is negative then 26 is added to make the number zero or higher.\n\nThus Bob recovers Alice's plaintext, the message \"HELLO\". Both Alice and Bob destroy the key sheet immediately after use, thus preventing reuse and an attack against the cipher. The KGB often issued its agents one-time pads printed on tiny sheets of \"flash paper\"—paper chemically converted to nitrocellulose, which burns almost instantly and leaves no ash.\n\nThe classical one-time pad of espionage used actual pads of minuscule, easily concealed paper, a sharp pencil, and some mental arithmetic. The method can be implemented now as a software program, using data files as input (plaintext), output (ciphertext) and key material (the required random sequence). The XOR operation is often used to combine the plaintext and the key elements, and is especially attractive on computers since it is usually a native machine instruction and is therefore very fast. It is, however, difficult to ensure that the key material is actually random, is used only once, never becomes known to the opposition, and is completely destroyed after use. The auxiliary parts of a software one-time pad implementation present real challenges: secure handling/transmission of plaintext, truly random keys, and one-time-only use of the key.\n\nTo continue the example from above, suppose Eve intercepts Alice's ciphertext: \"EQNVZ\". If Eve had infinite time, she would find that the key \"XMCKL\" would produce the plaintext \"HELLO\", but she would also find that the key \"TQURI\" would produce the plaintext \"LATER\", an equally plausible message:\nIn fact, it is possible to \"decrypt\" out of the ciphertext any message whatsoever with the same number of characters, simply by using a different key, and there is no information in the ciphertext that will allow Eve to choose among the various possible readings of the ciphertext.\n\nOne-time pads are \"information-theoretically secure\" in that the encrypted message (i.e., the ciphertext) provides no information about the original message to a cryptanalyst (except the maximum possible length of the message). This is a very strong notion of security first developed during WWII by Claude Shannon and proved, mathematically, to be true for the one-time pad by Shannon about the same time. His result was published in the \"Bell Labs Technical Journal\" in 1949. Properly used, one-time pads are secure in this sense even against adversaries with infinite computational power.\n\nClaude Shannon proved, using information theory considerations, that the one-time pad has a property he termed \"perfect secrecy\"; that is, the ciphertext \"C\" gives absolutely no additional information about the plaintext. This is because, given a truly random key that is used only once, a ciphertext can be translated into \"any\" plaintext of the same length, and all are equally likely. Thus, the \"a priori\" probability of a plaintext message \"M\" is the same as the \"a posteriori\" probability of a plaintext message \"M\" given the corresponding ciphertext. \n\nMathematically, this is expressed as formula_1, where formula_2 is the information entropy of the plaintext and formula_3 is the conditional entropy of the plaintext given the ciphertext \"C\". (Here, \"'Η\"' is the capital greek letter eta.) This implies that for every message \"M\" and corresponding ciphertext \"C\", there must be at least one key \"K\" that binds them as a one-time pad. Mathematically speaking, this means formula_4, where formula_5 denotes the length of message \"M\". In other words, if you need to be able to go from any plaintext in message space \"M\" to any cipher in cipher-space \"C\" (encryption) and from any cipher in cipher-space \"C\" to a plain text in message space \"M\" (decryption), you need at least formula_6 keys (all keys used with equal probability of formula_7 to ensure perfect secrecy).\n\nAnother way of stating perfect secrecy is based on the idea that for all messages formula_8 in message space \"M\", and for all ciphers \"c\" in cipher space \"C\", we have formula_9, where formula_10 represents the probabilities, taken over a choice of formula_11 in key space formula_12 over the coin tosses of a probabilistic algorithm, formula_13. Perfect secrecy is a strong notion of cryptanalytic difficulty.\n\nConventional symmetric encryption algorithms use complex patterns of substitution and transpositions. For the best of these currently in use, it is not known whether there can be a cryptanalytic procedure that can reverse (or, usefully, partially reverse) these transformations without knowing the key used during encryption. Asymmetric encryption algorithms depend on mathematical problems that are thought to be difficult to solve, such as integer factorization and discrete logarithms. However, there is no proof that these problems are hard, and a mathematical breakthrough could make existing systems vulnerable to attack.\n\nGiven perfect secrecy, in contrast to conventional symmetric encryption, OTP is immune even to brute-force attacks. Trying all keys simply yields all plaintexts, all equally likely to be the actual plaintext. Even with known plaintext, like part of the message being known, brute-force attacks cannot be used, since an attacker is unable to gain any information about the parts of the key needed to decrypt the rest of the message. The parts that are known will reveal \"only\" the parts of the key corresponding to them, and they correspond on a strictly one-to-one basis; no part of the key is dependent on any other part.\n\nDespite Shannon's proof of its security, the one-time pad has serious drawbacks in practice because it requires:\n\n\nOne-time pads solve few current practical problems in cryptography. High quality ciphers are widely available and their security is not considered a major worry at present. Such ciphers are almost always easier to employ than one-time pads; the amount of key material that must be properly generated and securely distributed is far smaller, and public key cryptography overcomes this problem. \n\nQuantum computers have been shown by Peter Shor and others to be much faster at solving some of the difficult problems that grant asymmetric encryption its security. If quantum computers are built with enough qubits, and overcoming some limitations to error-correction; traditional public key cryptography will become obsolete. One-time pads, however, will remain secure. See quantum cryptography and post-quantum cryptography for further discussion of the ramifications of quantum computers to information security.\n\nHigh-quality random numbers are difficult to generate. The random number generation functions in most programming language libraries are not suitable for cryptographic use. Even those generators that are suitable for normal cryptographic use, including /dev/random and many hardware random number generators, may make some use of cryptographic functions whose security has not been proven. An example of how true randomness can be achieved is by measuring radioactive emissions.\n\nIn particular, one-time use is absolutely necessary. If a one-time pad is used just twice, simple mathematical operations can reduce it to a running key cipher. If both plaintexts are in a natural language (e.g., English or Russian) then, even though both are secret, each stands a very high chance of being recovered by heuristic cryptanalysis, with possibly a few ambiguities. Of course, a longer message can only be broken for the portion that overlaps a shorter message, plus perhaps a little more by completing a word or phrase. The most famous exploit of this vulnerability occurred with the Venona project.\n\nBecause the pad, like all shared secrets, must be passed and kept secure, and the pad has to be at least as long as the message, there is often no point in using one-time padding, as one can simply send the plain text instead of the pad (as both can be the same size and have to be sent securely). However, once a very long pad has been securely sent (e.g., a computer disk full of random data), it can be used for numerous future messages, until the sum of their sizes equals the size of the pad. Quantum key distribution also proposes a solution to this problem, assuming fault-tolerant quantum computers.\n\nDistributing very long one-time pad keys is inconvenient and usually poses a significant security risk. The pad is essentially the encryption key, but unlike keys for modern ciphers, it must be extremely long and is much too difficult for humans to remember. Storage media such as thumb drives, DVD-Rs or personal digital audio players can be used to carry a very large one-time-pad from place to place in a non-suspicious way, but even so the need to transport the pad physically is a burden compared to the key negotiation protocols of a modern public-key cryptosystem, and such media cannot reliably be erased securely by any means short of physical destruction (e.g., incineration). A 4.7 GB DVD-R full of one-time-pad data, if shredded into particles 1 in size, leaves over 4 megabits of (admittedly hard to recover, but not impossibly so) data on each particle. In addition, the risk of compromise during transit (for example, a pickpocket swiping, copying and replacing the pad) is likely to be much greater in practice than the likelihood of compromise for a cipher such as AES. Finally, the effort needed to manage one-time pad key material scales very badly for large networks of communicants—the number of pads required goes up as the square of the number of users freely exchanging messages. For communication between only two persons, or a star network topology, this is less of a problem.\n\nThe key material must be securely disposed of after use, to ensure the key material is never reused and to protect the messages sent. Because the key material must be transported from one endpoint to another, and persist until the message is sent or received, it can be more vulnerable to forensic recovery than the transient plaintext it protects (see data remanence).\n\nAs traditionally used, one-time pads provide no message authentication, the lack of which can pose a security threat in real-world systems. For example, an attacker who knows that the message contains \"meet jane and me tomorrow at three thirty pm\" can derive the corresponding codes of the pad directly from the two known elements (the encrypted text and the known plaintext). The attacker can then replace that text by any other text of exactly the same length, such as \"three thirty meeting is canceled, stay home.\" The attacker's knowledge of the one-time pad is limited to this byte length, which must be maintained for any other content of the message to remain valid. This is a little different from malleability where it is not taken necessarily that the plaintext is known. \"See also\" stream cipher attack.\n\nStandard techniques to prevent this, such as the use of a message authentication code can be used along with a one-time pad system to prevent such attacks, as can classical methods such as variable length padding and Russian copulation, but they all lack the perfect security the OTP itself has. Universal hashing provides a way to authenticate messages up to an arbitrary security bound (i.e., for any formula_14, a large enough hash ensures that even a computationally unbounded attacker's likelihood of successful forgery is less than \"p\"), but this uses additional random data from the pad, and removes the possibility of implementing the system without a computer.\n\nDespite its problems, the one-time-pad retains some practical interest. In some hypothetical espionage situations, the one-time pad might be useful because it can be computed by hand with only pencil and paper. Indeed, nearly all other high quality ciphers are entirely impractical without computers. Spies can receive their pads in person from their \"handlers.\" In the modern world, however, computers (such as those embedded in personal electronic devices such as mobile phones) are so ubiquitous that possessing a computer suitable for performing conventional encryption (for example, a phone that can run concealed cryptographic software) will usually not attract suspicion.\n\n\nOne-time pads have been used in special circumstances since the early 1900s. In 1923, it was employed for diplomatic communications by the German diplomatic establishment. The Weimar Republic Diplomatic Service began using the method in about 1920. The breaking of poor Soviet cryptography by the British, with messages made public for political reasons in two instances in the 1920s (ARCOS case), appear to have induced the U.S.S.R. to adopt one-time pads for some purposes by around 1930. KGB spies are also known to have used pencil and paper one-time pads more recently. Examples include Colonel Rudolf Abel, who was arrested and convicted in New York City in the 1950s, and the 'Krogers' (i.e., Morris and Lona Cohen), who were arrested and convicted of espionage in the United Kingdom in the early 1960s. Both were found with physical one-time pads in their possession.\n\nA number of nations have used one-time pad systems for their sensitive traffic. Leo Marks reports that the British Special Operations Executive used one-time pads in World War II to encode traffic between its offices. One-time pads for use with its overseas agents were introduced late in the war. A few British one-time tape cipher machines include the Rockex and Noreen. The German Stasi Sprach Machine was also capable of using one time tape that East Germany, Russia, and even Cuba used to send encrypted messages to their agents.\n\nThe World War II voice scrambler SIGSALY was also a form of one-time system. It added noise to the signal at one end and removed it at the other end. The noise was distributed to the channel ends in the form of large shellac records that were manufactured in unique pairs. There were both starting synchronization and longer-term phase drift problems that arose and were solved before the system could be used.\n\nThe hotline between Moscow and Washington D.C., established in 1963 after the Cuban missile crisis, used teleprinters protected by a commercial one-time tape system. Each country prepared the keying tapes used to encode its messages and delivered them via their embassy in the other country. A unique advantage of the OTP in this case was that neither country had to reveal more sensitive encryption methods to the other.\n\nU.S. Army Special Forces used one-time pads in Vietnam. By using Morse code with one-time pads and continuous wave radio transmission (the carrier for Morse code), they achieved both secrecy and reliable communications.\n\nDuring the 1983 Invasion of Grenada, U.S. forces found a supply of pairs of one-time pad books in a Cuban warehouse.\n\nStarting in 1988, the African National Congress (ANC) used disk-based one-time pads as part of a secure communication system between ANC leaders outside South Africa and in-country operatives as part of Operation Vula, a successful effort to build a resistance network inside South Africa. Random numbers on the disk were erased after use. A Belgian airline stewardess acted as courier to bring in the pad disks. A regular resupply of new disks was needed as they were used up fairly quickly. One problem with the system was that it could not be used for secure data storage. Later Vula added a stream cipher keyed by book codes to solve this problem.\n\nA related notion is the one-time code—a signal, used only once, e.g., \"Alpha\" for \"mission completed\", \"Bravo\" for \"mission failed\" or even \"Torch\" for \"Allied invasion of French Northern Africa\" cannot be \"decrypted\" in any reasonable sense of the word. Understanding the message will require additional information, often 'depth' of repetition, or some traffic analysis. However, such strategies (though often used by real operatives, and baseball coaches) are not a cryptographic one-time pad in any significant sense.\n\nAt least into the 1970s, the U.S. National Security Agency (NSA) produced a variety of manual one-time pads, both general purpose and specialized, with 86,000 one-time pads produced in fiscal year 1972. Special purpose pads were produced for what NSA called \"pro forma\" systems, where “the basic framework, form or format of every message text is identical or nearly so; the same kind of information, message after message, is to be presented in the same order, and only specific values, like numbers, change with each message.” Examples included nuclear launch messages and radio direction finding reports (COMUS).\n\nGeneral purpose pads were produced in several formats, a simple list of random letters (DIANA) or just numbers (CALYPSO), tiny pads for covert agents (MICKEY MOUSE), and pads designed for more rapid encoding of short messages, at the cost of lower density. One example, ORION, had 50 rows of plaintext alphabets on one side and the corresponding random cipher text letters on the other side. By placing a sheet on top of a piece of carbon paper with the carbon face up, one could circle one letter in each row on one side and the corresponding letter one the other side would be circled by the carbon paper. Thus one ORION sheet could quickly encode or decode a message up to 50 characters long. Production of ORION pads required printing both sides in exact registration, a difficult process, so NSA switched to another pad format, MEDEA, with 25 rows of paired alphabets and random characters. (\"See\" for illustrations.)\nThe NSA also built automated systems for the “centralized headquarters of CIA and Special Forces units so that they can efficiently process the many separate one-time pad messages to and from individual pad holders in the field.”\n\nDuring World War II and into the 1950s, the U.S. made extensive use of one-time tape systems. In addition to providing confidentiality, circuits secured by one-time tape ran continually, even when there was no traffic, thus protecting against traffic analysis. In 1955, NSA produced some 1,660,000 rolls of one time tape. Each roll was 8 inches in diameter, contained 100,000 characters, lasted 166 minutes and cost $4.55 to produce. By 1972, only 55,000 rolls were produced, as one-time tapes were replaced by rotor machines such as SIGTOT, and later by electronic devices based on shift registers. The NSA describes one-time tape systems like 5-UCO and SIGTOT as being used for intelligence traffic until the introduction of the electronic cipher based KW-26 in 1957.\n\nWhile one-time pads provide perfect secrecy if generated and used properly, small mistakes can lead to successful cryptanalysis:\n\n"}
{"id": "270062", "url": "https://en.wikipedia.org/wiki?curid=270062", "title": "Operational semantics", "text": "Operational semantics\n\nOperational semantics is a category of formal programming language semantics in which certain desired properties of a program, such as correctness, safety or security, are verified by constructing proofs from logical statements about its execution and procedures, rather than by attaching mathematical meanings to its terms (denotational semantics). Operational semantics are classified in two categories: structural operational semantics (or small-step semantics) formally describe how the \"individual steps\" of a computation take place in a computer-based system; by opposition natural semantics (or big-step semantics) describe how the \"overall results\" of the executions are obtained. Other approaches to providing a formal semantics of programming languages include axiomatic semantics and denotational semantics.\n\nThe operational semantics for a programming language describes how a valid program is interpreted as sequences of computational steps.\nThese sequences then \"are\" the meaning of the program.\nIn the context of functional programs, the final step in a terminating\nsequence returns the value of the program. (In general there can be many return values for a single program,\nbecause the program could be nondeterministic, and even for a deterministic program there can be many computation sequences since the semantics may not specify exactly what sequence of operations arrives at that value.)\n\nPerhaps the first formal incarnation of operational semantics was the use of the lambda calculus to define the semantics of LISP. Abstract machines in the tradition of the SECD machine are also closely related.\n\nThe concept of operational semantics was used for the first time in defining the semantics of Algol 68.\nThe following statement is a quote from the revised ALGOL 68 report:\nThe meaning of a program in the strict language is explained in terms of a hypothetical computer\nwhich performs the set of actions which constitute the elaboration of that program. (Algol68, Section 2)\nThe first use of the term \"operational semantics\" in its present meaning is attributed to\nDana Scott (Plotkin04).\nWhat follows is a quote from Scott's seminal paper on formal semantics,\nin which he mentions the \"operational\" aspects of semantics.\nIt is all very well to aim for a more ‘abstract’ and a ‘cleaner’ approach to\nsemantics, but if the plan is to be any good, the operational aspects cannot\nbe completely ignored. (Scott70)\nGordon Plotkin introduced the structural operational semantics, Robert Hieb and Matthias Felleisen the reduction contexts, and Gilles Kahn the natural semantics.\n\nStructural operational semantics (also called structured operational semantics or small-step semantics) was introduced by Gordon Plotkin in (Plotkin81) as a logical means to define operational semantics. The basic idea behind SOS is to define the behavior of a program in terms of the behavior of its parts, thus providing a structural, i.e., syntax-oriented and inductive, view on operational semantics. An SOS specification defines the behavior of a program in terms of a (set of) transition relation(s). SOS specifications take the form of a set of inference rules that define the valid transitions of a composite piece of syntax in terms of the transitions of its components.\n\nFor a simple example, we consider part of the semantics of a simple programming language; proper illustrations are given in Plotkin81 and Hennessy90, and other textbooks. Let formula_1 range over programs of the language, and let formula_2 range over states (e.g. functions from memory locations to values). If we have expressions (ranged over by formula_3), values and locations (formula_4), then a memory update command would have semantics:\n\nformula_5\nInformally, the rule says that \"if the expression formula_3 in state formula_2 reduces to value formula_8, then the program formula_9 will update the state formula_2 with the assignment formula_11\".\n\nThe semantics of sequencing can be given by the following three rules:\n\nformula_12\n\nInformally, the first rule says that,\nif program formula_13 in state formula_2 finishes in state formula_15, then the program formula_16 in state formula_2 will reduce to the program formula_18 in state formula_15.\n(You can think of this as formalizing \"You can run formula_13, and then run formula_18\nusing the resulting memory store.)\nThe second rule says that\nif the program formula_13 in state formula_2 can reduce to the program formula_24 with state formula_15, then the program formula_16 in state formula_2 will reduce to the program formula_28 in state formula_15.\n(You can think of this as formalizing the principle for an optimizing compiler:\n\"You are allowed to transform formula_13 as if it were stand-alone, even if it is just the\nfirst part of a program.\")\nThe semantics is structural, because the meaning of the sequential program formula_16, is defined by the meaning of formula_13 and the meaning of formula_18.\n\nIf we also have Boolean expressions over the state, ranged over by formula_34, then we can define the semantics of the while command:\nformula_35\n\nSuch a definition allows formal analysis of the behavior of programs, permitting the study of relations between programs. Important relations include simulation preorders and bisimulation.\nThese are especially useful in the context of concurrency theory.\n\nThanks to its intuitive look and easy-to-follow structure,\nSOS has gained great popularity and has become a de facto standard in defining\noperational semantics. As a sign of success, the original report (so-called Aarhus\nreport) on SOS (Plotkin81) has attracted more than 1000 citations according to the CiteSeer ,\nmaking it one of the most cited technical reports in Computer Science.\n\nReduction semantics are an alternative presentation of operational semantics using so-called reduction contexts. The method was introduced by Robert Hieb and Matthias Felleisen in 1992 as a technique for formalizing an equational theory for control and state. For example, the grammar of a simple call-by-value lambda calculus and its contexts can be given as:\n\nformula_36\n\nThe contexts formula_37 include a hole formula_38 where a term can be plugged in.\nThe shape of the contexts indicate where reduction can occur (i.e., a term can be plugged into a term).\nTo describe a semantics for this language, axioms or reduction rules are provided:\n\nformula_39\n\nThis single axiom is the beta rule from the lambda calculus. The reduction contexts show how this rule composes\nwith more complicated terms. In particular, this rule can trigger for the argument position of an\napplication like formula_40 because there is a context formula_41\nthat matches the term. In this case, the contexts uniquely decompose terms so that only one reduction is possible\nat any given step. Extending the axiom to match the reduction contexts gives the \"compatible closure\". Taking the\nreflexive, transitive closure of this relation gives the \"reduction relation\" for this language.\n\nThe technique is useful for the ease in which reduction contexts can model state or control constructs (e.g., continuations). In addition, reduction semantics have been used to model object-oriented languages, contract systems, and other language features.\n\nBig-step structural operational semantics is also known under the names natural semantics, relational semantics and evaluation semantics. Big-step operational semantics was introduced under the name \"natural semantics\" by Gilles Kahn when presenting Mini-ML, a pure dialect of the ML language.\n\nOne can view big-step definitions as definitions of functions, or more generally of relations, interpreting each language construct in an appropriate domain. Its intuitiveness makes it a popular choice for semantics specification in programming languages, but it has some drawbacks that make it inconvenient or impossible to use in many situations, such as languages with control-intensive features or concurrency.\n\nA big-step semantics describes in a divide-and-conquer manner how final evaluation results of language constructs can be obtained by combining the evaluation results of their syntactic counterparts (subexpressions, substatements, etc.).\n\nThere are a number of distinctions between small-step and big-step semantics that influence whether one or the other forms a more suitable basis for specifying the semantics of a programming language.\n\nBig-step semantics have the advantage of often being simpler (needing fewer inference rules) and often directly correspond to an efficient implementation of an interpreter for the language (hence Kahn calling them \"natural\".) Both can lead to simpler proofs, for example when proving the preservation of correctness under some program transformation.\n\nThe main disadvantage of big-step semantics is that non-terminating (diverging) computations do not have an inference tree, making it impossible to state and prove properties about such computations.\n\nSmall-step semantics give more control of the details and order of evaluation. In the case of instrumented operational semantics, this allows the operational semantics to track and the semanticist to state and prove more accurate theorems about the run-time behaviour of the language. These properties make small-step semantics more convenient when proving type soundness of a type system against an operational semantics.\n\n\n"}
{"id": "17359307", "url": "https://en.wikipedia.org/wiki?curid=17359307", "title": "Paratopological group", "text": "Paratopological group\n\nIn mathematics, a paratopological group is a topological semigroup that is algebraically a group. In other words, it is a group \"G\" with a topology such that the group's product operation is a continuous function from \"G\" × \"G\" to \"G\". This differs from the definition of a topological group in that the group inverse is not required to be continuous.\n\nAs with topological groups, some authors require the topology to be Hausdorff.\n\nCompact paratopological groups are automatically topological groups.\n"}
{"id": "437052", "url": "https://en.wikipedia.org/wiki?curid=437052", "title": "Positional notation", "text": "Positional notation\n\nPositional notation or place-value notation is a method of representing or encoding numbers. Positional notation is distinguished from other notations (such as Roman numerals) for its use of the same symbol for the different orders of magnitude (for example, the \"ones place\", \"tens place\", \"hundreds place\"). This greatly simplified arithmetic, leading to the rapid spread of the notation across the world.\n\nWith the use of a radix point (decimal point in base-10), the notation can be extended to include fractions and the numeric expansions of real numbers.\n\nThe Babylonian numeral system, base-60, was the first positional system developed, and its influence is present today in the way time and angles are counted in tallies related to 60, like 60 minutes in an hour, 360 degrees in a circle. The Hindu–Arabic numeral system, base-10, is the most commonly used system in the world today for most calculations. The binary numeral system, base-2, is straightforwardly implemented in digital electronic circuitry and used by almost all computer systems and electronics for calculations and representations.\n\nToday, the base-10 (decimal) system, which is likely motivated by counting with the ten fingers, is ubiquitous. Other bases have been used in the past, and some continue to be used today. For example, the Babylonian numeral system, credited as the first positional numeral system, was base-60, but it lacked a real 0 value. Zero was indicated by a space between sexagesimal numerals. By 300 BC, a punctuation symbol (two slanted wedges) was co-opted as a placeholder in the same system. In a tablet unearthed at Kish (dating from about 700 BC), the scribe Bêl-bân-aplu wrote his zeros with three hooks, rather than two slanted wedges. The Babylonian placeholder was not a true zero because it was not used alone. Nor was it used at the end of a number. Thus numbers like 2 and 120 (2×60), 3 and 180 (3×60), 4 and 240 (4×60), looked the same because the larger numbers lacked a final sexagesimal placeholder. Only context could differentiate them.\n\nThe polymath Archimedes (ca. 287–212 BC) invented a decimal positional system in his Sand Reckoner which was based on 10 and later led the German mathematician Carl Friedrich Gauss to lament what heights science would have already reached in his days if Archimedes had fully realized the potential of his ingenious discovery.\n\nBefore positional notation became standard, simple additive systems (sign-value notation) such as Roman numerals were used, and accountants in ancient Rome and during the Middle Ages used the abacus or stone counters to do arithmetic.\nCounting rods and most abacuses have been used to represent numbers in a positional numeral system. With counting rods or abacus to perform arithmetic operations, the writing of the starting, intermediate and final values of a calculation could easily be done with a simple additive system in each position or column. This approach required no memorization of tables (as does positional notation) and could produce practical results quickly. For four centuries (from the 13th to the 16th) there was strong disagreement between those who believed in adopting the positional system in writing numbers and those who wanted to stay with the additive-system-plus-abacus. Although electronic calculators have largely replaced the abacus, the latter continues to be used in Japan and other Asian countries.\n\nAfter the French Revolution (1789-1799), the new French government promoted the extension of the decimal system.\nSome of those pro-decimal efforts—such as decimal time and the decimal calendar—were unsuccessful.\nOther French pro-decimal efforts—currency decimalisation and the metrication of weights and measures—spread widely out of France to almost the whole world.\n\nJ. Lennart Berggren notes that positional decimal fractions were used for the first time by Arab mathematician Abu'l-Hasan al-Uqlidisi as early as the 10th century. The Jewish mathematician Immanuel Bonfils used decimal fractions around 1350, anticipating Simon Stevin, but did not develop any notation to represent them. The Persian mathematician Jamshīd al-Kāshī claimed to have discovered decimal fractions himself in the 15th century.Al Khwarizmi introduced fractions to Islamic countries in the early 9th century; his fraction presentation was an exact copy of traditional Chinese mathematical fractions from Sunzi Suanjing. This form of fraction with numerator on top and denominator at bottom without a horizontal bar was also used by 10th century Abu'l-Hasan al-Uqlidisi and 15th century Jamshīd al-Kāshī's work \"Arithmetic Key\".\n\nA forerunner of modern European decimal notation was introduced by Simon Stevin in the 16th century.\n\nA key argument against the positional system was its susceptibility to easy fraud by simply putting a number at the beginning or end of a quantity, thereby changing (e.g.) 100 into 5100, or 100 into 1000. Modern cheques require a natural language spelling of an amount, as well as the decimal amount itself, to prevent such fraud. For the same reason the Chinese also use natural language numerals, for example 100 is written as 壹佰, which can never be forged into 壹仟(1000) or 伍仟壹佰(5100).\n\nMany of the advantages claimed for the metric system could be realized by any consistent positional notation.\nDozenal advocates say dozenal has several advantages over decimal, although the switching cost appears to be high.\n\nIn mathematical numeral systems the base or radix is usually the number of unique digits, including zero, that a positional numeral system uses to represent numbers. For example, for the decimal system the radix is 10, because it uses the 10 digits from 0 through 9. When a number \"hits\" 9, the next number will not be another different symbol, but a \"1\" followed by a \"0\". In binary, the radix is 2, since after it hits \"1\", instead of \"2\" or another written symbol, it jumps straight to \"10\", followed by \"11\" and \"100\".\n\nThe highest symbol of a positional numeral system usually has the value one less than the value of the base of that numeral system. The standard positional numeral systems differ from one another only in the base they use.\n\nThe base is an integer that is greater than 1 (or less than negative 1), since a radix of zero would not have any digits, and a radix of 1 would only have the zero digit. Negative bases are rarely used. In a system with a negative radix, numbers may have many different possible representations.\n\nIn base-10 (decimal) positional notation, there are 10 decimal digits and the number\nIn base-16 (hexadecimal), there are 16 hexadecimal digits (0–9 and A–F) and the number\n\nIn general, in base-\"b\", there are \"b\" digits and the number\n\nWhen describing base in mathematical notation, the letter \"b\" is generally used as a symbol for this concept, so, for a binary system, \"b\" equals 2. Another common way of expressing the base is writing it as a decimal subscript after the number that is being represented (this notation is used in this article). 1111011 implies that the number 1111011 is a base-2 number, equal to 123 (a decimal notation representation), 173 (octal) and 7B (hexadecimal). In books and articles, when using initially the written abbreviations of number bases, the base is not subsequently printed: it is assumed that binary 1111011 is the same as 1111011.\n\nThe base \"b\" may also be indicated by the phrase \"base-\"b\"\". So binary numbers are \"base-2\"; octal numbers are \"base-8\"; decimal numbers are \"base-10\"; and so on.\n\nTo a given radix \"b\" the set of digits {0, 1, ..., \"b\"−2, \"b\"−1} is called the standard set of digits. Thus, binary numbers have digits {0, 1}; decimal numbers have digits and so on. Therefore, the following are notational errors: 52, 2, 1A. (In all cases, one or more digits is not in the set of allowed digits for the given base.)\n\nPositional numeral systems work using exponentiation of the base. A digit's value is the digit multiplied by the value of its place. Place values are the number of the base raised to the \"n\"th power, where \"n\" is the number of other digits between a given digit and the radix point. If a given digit is on the left hand side of the radix point (i.e. its value is an integer) then \"n\" is positive or zero; if the digit is on the right hand side of the radix point (i.e., its value is fractional) then \"n\" is negative.\n\nAs an example of usage, the number 465 in its respective base \"b\" (which must be at least base 7 because the highest digit in it is 6) is equal to:\n\nIf the number 465 was in base-10, then it would equal:\n\nIf however, the number were in base 7, then it would equal:\n\n10 = \"b\" for any base \"b\", since 10 = 1×\"b\" + 0×\"b\". For example, 10 = 2; 10 = 3; 10 = 16. Note that the last \"16\" is indicated to be in base 10. The base makes no difference for one-digit numerals.\n\nThis concept can be demonstrated using a diagram. One object represents one unit. When the number of objects is equal to or greater than the base \"b\", then a group of objects is created with \"b\" objects. When the number of these groups exceeds \"b\", then a group of these groups of objects is created with \"b\" groups of \"b\" objects; and so on. Thus the same number in different bases will have different values:\n\nThe notation can be further augmented by allowing a leading minus sign. This allows the representation of negative numbers. For a given base, every representation corresponds to exactly one real number and every real number has at least one representation. The representations of rational numbers are those representations that are finite, use the bar notation, or end with an infinitely repeating cycle of digits.\n\nA \"digit\" is what is used as a position in place-value notation, and a \"numeral\" is one or more digits. Today's most common digits are the decimal digits \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", and \"9\". The distinction between a digit and a numeral is most pronounced in the context of a number base.\n\nA non-zero \"numeral\" with more than one digit position will mean a different number in a different number base, but in general, the \"digits\" will mean the same. The base-8 numeral 23 contains two digits, \"2\" and \"3\", and with a base number (subscripted) \"8\", means 19. In our notation here, the subscript \"\" of the numeral 23 is part of the numeral, but this may not always be the case. Imagine the numeral \"23\" as having an ambiguous base number. Then \"23\" could likely be any base, base-4 through base-60. In base-4 \"23\" means 11, and in base-60 it means the number 123. The numeral \"23\" then, in this case, corresponds to the set of numbers {11, 13, 15, 17, 19, 21, 23, ..., 121, 123} while its digits \"2\" and \"3\" always retain their original meaning: the \"2\" means \"two of\", and the \"3\" three.\n\nIn certain applications when a numeral with a fixed number of positions needs to represent a greater number, a higher number-base with more digits per position can be used. A three-digit, decimal numeral can represent only up to 999. But if the number-base is increased to 11, say, by adding the digit \"A\", then the same three positions, maximized to \"AAA\", can represent a number as great as 1330. We could increase the number base again and assign \"B\" to 11, and so on (but there is also a possible encryption between number and digit in the number-digit-numeral hierarchy). A three-digit numeral \"ZZZ\" in base-60 could mean . If we use the entire collection of our alphanumerics we could ultimately serve a base-\"62\" numeral system, but we remove two digits, uppercase \"I\" and uppercase \"O\", to reduce confusion with digits \"1\" and \"0\".\nWe are left with a base-60, or sexagesimal numeral system utilizing 60 of the 62 standard alphanumerics. (But see \"Sexagesimal system\" below.) In general, the number of possible values that can be represented by a formula_8 digit number in base formula_9 is formula_10.\n\nThe common numeral systems in computer science are binary (radix 2), octal (radix 8), and hexadecimal (radix 16). In binary only digits \"0\" and \"1\" are in the numerals. In the octal numerals, are the eight digits 0–7. Hex is 0–9 A–F, where the ten numerics retain their usual meaning, and the alphabetics correspond to values 10–15, for a total of sixteen digits. The numeral \"10\" is binary numeral \"2\", octal numeral \"8\", or hexadecimal numeral \"16\".\n\nThe notation can be extended into the negative exponents of the base \"b\". Thereby the so-called radix point, mostly ».«, is used as separator of the positions with non-negative from those with negative exponent.\n\nNumbers that are not integers use places beyond the radix point. For every position behind this point (and thus after the units digit), the exponent \"n\" of the power \"b\" decreases by 1 and the power approaches 0. For example, the number 2.35 is equal to:\n\nIf the base and all the digits in the set of digits are non-negative, negative numbers cannot be expressed. To overcome this, a minus sign, here »-«, is added to the numeral system. In the usual notation it is prepended to the string of digits representing the otherwise non-negative number.\n\nThe conversion to a base formula_12 of an integer represented in base formula_13 can be done by a succession of Euclidean divisions by formula_14 the right-most digit in base formula_12 is the remainder of the division of by formula_16 the second right-most digit is the remainder of the division of the quotient by formula_17 and so on. More precisely, the th digit from the right is the remainder of the division by formula_12 of the th quotient.\n\nFor example: converting A10B to decimal (41227):\n\nWhen converting to a larger base (such as from binary to decimal), the remainder represents formula_12 as a single digit, using digits from formula_13. For example: converting 0b11111001 (binary) to 249 (decimal):\n\nThe numbers which have a finite representation form the semiring\nMore explicitly, if formula_22 is a factorization of formula_23 into the primes formula_24 with exponents then with the non-empty set of denominators formula_25\nwe have\nwhere formula_27 is the group generated by the formula_28 and formula_29 is the so-called localization of formula_30 with respect to \n\nThe denominator of an element of formula_31 contains if reduced to lowest terms only prime factors out of formula_32.\nThis ring of all terminating fractions to base formula_23 is dense in the field of rational numbers formula_34. Its completion for the usual (Archimedean) metric is the same as for formula_34, namely the real numbers formula_36. So, if formula_37 then formula_38 has not to be confused with formula_39, the discrete valuation ring for the prime formula_40, which is equal to formula_41 with formula_42.\n\nIf formula_23 divides formula_44, we have formula_45\n\nThe representation of non-integers can be extended to allow an infinite string of digits beyond the point. For example, 1.12112111211112 ... base-3 represents the sum of the infinite series:\n\nSince a complete infinite string of digits cannot be explicitly written, the trailing ellipsis (...) designates the omitted digits, which may or may not follow a pattern of some kind. One common pattern is when a finite sequence of digits repeats infinitely. This is designated by drawing a vinculum across the repeating block:\n\nFor base-10 it is called a recurring decimal or repeating decimal.\n\nAn irrational number has an infinite non-repeating representation in all integer bases. Whether a rational number has a finite representation or requires an infinite repeating representation depends on the base. For example, one third can be represented by:\n\nFor integers \"p\" and \"q\" with \"gcd\"(\"p\", \"q\") = 1, the fraction \"p\"/\"q\" has a finite representation in base \"b\" if and only if each prime factor of \"q\" is also a prime factor of \"b\".\n\nFor a given base, any number that can be represented by a finite number of digits (without using the bar notation) will have multiple representations, including one or two infinite representations:\n\nIn the decimal (base-10) Hindu–Arabic numeral system, each position starting from the right is a higher power of 10. The first position represents 10 (1), the second position 10 (10), the third position 10 ( or 100), the fourth position 10 ( or 1000), and so on.\n\nFractional values are indicated by a separator, which can vary in different locations. Usually this separator is a period or full stop, or a comma. Digits to the right of it are multiplied by 10 raised to a negative power or exponent. The first position to the right of the separator indicates 10 (0.1), the second position 10 (0.01), and so on for each successive position.\n\nAs an example, the number 2674 in a base-10 numeral system is:\n\nor\n\nThe sexagesimal or base-60 system was used for the integral and fractional portions of Babylonian numerals and other mesopotamian systems, by Hellenistic astronomers using Greek numerals for the fractional portion only, and is still used for modern time and angles, but only for minutes and seconds. However, not all of these uses were positional.\n\nModern time separates each position by a colon or point. For example, the time might be 10:25:59 (10 hours 25 minutes 59 seconds). Angles use similar notation. For example, an angle might be 10°25'59\" (10 degrees 25 minutes 59 seconds). In both cases, only minutes and seconds use sexagesimal notation—angular degrees can be larger than 59 (one rotation around a circle is 360°, two rotations are 720°, etc.), and both time and angles use decimal fractions of a second. This contrasts with the numbers used by Hellenistic and Renaissance astronomers, who used thirds, fourths, etc. for finer increments. Where we might write 10°25'59.392\", they would have written 10°25′59″23‴31⁗12′′′′′ or 10°2559233112.\n\nUsing a digit set of digits with upper and lowercase letters allows short notation for sexagesimal numbers, e.g. 10:25:59 becomes 'ARz' (by omitting I and O, but not i and o), which is useful for use in URLs, etc., but it is not very intelligible to humans.\n\nIn the 1930s, Otto Neugebauer introduced a modern notational system for Babylonian and Hellenistic numbers that substitutes modern decimal notation from 0 to 59 in each position, while using a semicolon (;) to separate the integral and fractional portions of the number and using a comma (,) to separate the positions within each portion. For example, the mean synodic month used by both Babylonian and Hellenistic astronomers and still used in the Hebrew calendar is 29;31,50,8,20 days, and the angle used in the example above would be written 10;25,59,23,31,12 degrees.\n\nIn computing, the binary (base-2), octal (base-8) and hexadecimal (base-16) bases are most commonly used. Computers, at the most basic level, deal only with sequences of conventional zeroes and ones, thus it is easier in this sense to deal with powers of two. The hexadecimal system is used as \"shorthand\" for binary—every 4 binary digits (bits) relate to one and only one hexadecimal digit. In hexadecimal, the six digits after 9 are denoted by A, B, C, D, E, and F (and sometimes a, b, c, d, e, and f).\n\nThe octal numbering system is also used as another way to represent binary numbers. In this case the base is 8 and therefore only digits 0, 1, 2, 3, 4, 5, 6, and 7 are used. When converting from binary to octal every 3 bits relate to one and only one octal digit.\n\nHexadecimal, decimal, octal, and a wide variety of other bases have been used for binary-to-text encoding, implementations of arbitrary-precision arithmetic, and other applications.\n\n\"For a list of bases and their applications, see list of numeral systems.\"\n\nBase-12 systems (duodecimal or dozenal) have been popular because multiplication and division are easier than in base-10, with addition and subtraction being just as easy. Twelve is a useful base because it has many factors. It is the smallest common multiple of one, two, three, four and six. There is still a special word for \"dozen\" in English, and by analogy with the word for 10, \"hundred\", commerce developed a word for 12, \"gross\". The standard 12-hour clock and common use of 12 in English units emphasize the utility of the base. In addition, prior to its conversion to decimal, the old British currency Pound Sterling (GBP) \"partially\" used base-12; there were 12 pence (d) in a shilling (s), 20 shillings in a pound (£), and therefore 240 pence in a pound. Hence the term LSD or, more properly, £sd.\n\nThe Maya civilization and other civilizations of pre-Columbian Mesoamerica used base-20 (vigesimal), as did several North American tribes (two being in southern California). Evidence of base-20 counting systems is also found in the languages of central and western Africa.\n\nRemnants of a Gaulish base-20 system also exist in French, as seen today in the names of the numbers from 60 through 99. For example, sixty-five is \"soixante-cinq\" (literally, \"sixty [and] five\"), while seventy-five is \"soixante-quinze\" (literally, \"sixty [and] fifteen\"). Furthermore, for any number between 80 and 99, the \"tens-column\" number is expressed as a multiple of twenty. For example, eighty-two is \"quatre-vingt-deux\" (literally, four twenty[s] [and] two), while ninety-two is \"quatre-vingt-douze\" (literally, four twenty[s] [and] twelve). In Old French, forty was expressed as two twenties and sixty was three twenties, so that fifty-three was expressed as two twenties [and] thirteen, and so on.\n\nIn English the same base-20 counting appears in the use of \"scores\". Although mostly historical it is occasionally used colloquially. Verse 10 of Pslam 90 in the King James Version of the Bible starts: \"The days of our years are threescore years and ten; and if by reason of strength they be fourscore years, yet is their strength labour and sorrow\". The Gettysburg Address starts: \"Four score and seven years ago\".\n\nThe Irish language also used base-20 in the past, twenty being \"fichid\", forty \"dhá fhichid\", sixty \"trí fhichid\" and eighty \"ceithre fhichid\". A remnant of this system may be seen in the modern word for 40, \"daoichead\".\n\nThe Welsh language continues to use a base-20 counting system, particularly for the age of people, dates and in common phrases. 15 is also important, with 16–19 being \"one on 15\", \"two on 15\" etc. 18 is normally \"two nines\". A decimal system is commonly used.\n\nDanish numerals display a similar base-20 structure.\n\nThe Maori language of New Zealand also has evidence of an underlying base-20 system as seen in the terms \"Te Hokowhitu a Tu\" referring to a war party (literally \"the seven 20s of Tu\") and \"Tama-hokotahi\", referring to a great warrior (\"the one man equal to 20\").\n\nThe binary system was used in the Egyptian Old Kingdom, 3000 BC to 2050 BC. It was cursive by rounding off rational numbers smaller than 1 to , with a 1/64 term thrown away (the system was called the Eye of Horus).\n\nA number of Australian Aboriginal languages employ binary or binary-like counting systems. For example, in Kala Lagaw Ya, the numbers one through six are \"urapon\", \"ukasar\", \"ukasar-urapon\", \"ukasar-ukasar\", \"ukasar-ukasar-urapon\", \"ukasar-ukasar-ukasar\".\n\nNorth and Central American natives used base-4 (quaternary) to represent the four cardinal directions. Mesoamericans tended to add a second base-5 system to create a modified base-20 system.\n\nA base-5 system (quinary) has been used in many cultures for counting. Plainly it is based on the number of digits on a human hand. It may also be regarded as a sub-base of other bases, such as base-10, base-20, and base-60.\n\nA base-8 system (octal) was devised by the Yuki tribe of Northern California, who used the spaces between the fingers to count, corresponding to the digits one through eight. There is also linguistic evidence which suggests that the Bronze Age Proto-Indo Europeans (from whom most European and Indic languages descend) might have replaced a base-8 system (or a system which could only count up to 8) with a base-10 system. The evidence is that the word for 9, \"newm\", is suggested by some to derive from the word for \"new\", \"newo-\", suggesting that the number 9 had been recently invented and called the \"new number\".\n\nMany ancient counting systems use five as a primary base, almost surely coming from the number of fingers on a person's hand. Often these systems are supplemented with a secondary base, sometimes ten, sometimes twenty. In some African languages the word for five is the same as \"hand\" or \"fist\" (Dyola language of Guinea-Bissau, Banda language of Central Africa). Counting continues by adding 1, 2, 3, or 4 to combinations of 5, until the secondary base is reached. In the case of twenty, this word often means \"man complete\". This system is referred to as \"quinquavigesimal\". It is found in many languages of the Sudan region.\n\nThe Telefol language, spoken in Papua New Guinea, is notable for possessing a base-27 numeral system.\n\nInteresting properties exist when the base is not fixed or positive and when the digit symbol sets denote negative values. There are many more variations. These systems are of practical and theoretic value to computer scientists.\n\nBalanced ternary uses a base of 3 but the digit set is instead of {0,1,2}. The \"\" has an equivalent value of −1. The negation of a number is easily formed by switching the on the 1s. This system can be used to solve the balance problem, which requires finding a minimal set of known counter-weights to determine an unknown weight. Weights of 1, 3, 9, ... 3 known units can be used to determine any unknown weight up to 1 + 3 + ... + 3 units. A weight can be used on either side of the balance or not at all. Weights used on the balance pan with the unknown weight are designated with , with 1 if used on the empty pan, and with 0 if not used. If an unknown weight \"W\" is balanced with 3 (3) on its pan and 1 and 27 (3 and 3) on the other, then its weight in decimal is 25 or 101 in balanced base-3. \n\nThe factorial number system uses a varying radix, giving factorials as place values; they are related to Chinese remainder theorem and residue number system enumerations. This system effectively enumerates permutations. A derivative of this uses the Towers of Hanoi puzzle configuration as a counting system. The configuration of the towers can be put into 1-to-1 correspondence with the decimal count of the step at which the configuration occurs and vice versa.\n\nEach position does not need to be positional itself. Babylonian sexagesimal numerals were positional, but in each position were groups of two kinds of wedges representing ones and tens (a narrow vertical wedge ( | ) and an open left pointing wedge (<))—up to 14 symbols per position (5 tens (««<) and 9 ones ( ||||||||| ) grouped into one or two near squares containing up to three tiers of symbols, or a place holder (\\\\) for the lack of a position). Hellenistic astronomers used one or two alphabetic Greek numerals for each position (one chosen from 5 letters representing 10–50 and/or one chosen from 9 letters representing 1–9, or a zero symbol).\n\nExamples:\n\nRelated topics:\n\n\n"}
{"id": "35975525", "url": "https://en.wikipedia.org/wiki?curid=35975525", "title": "Quantifier rank", "text": "Quantifier rank\n\nIn mathematical logic, the quantifier rank of a formula is the depth of nesting of its quantifiers. It plays an essential role in model theory.\n\nNotice that the quantifier rank is a property of the formula itself (i.e. the expression in a language). Thus two logically equivalent formulae can have different quantifier ranks, when they express the same thing in different ways.\n\nQuantifier Rank of a Formula in First-order language (FO)\n\nLet φ be a FO formula. The quantifier rank of φ, written qr(φ), is defined as\n\nRemarks\n\nQuantifier Rank of a higher order Formula\n\n\n\n\n"}
{"id": "34304924", "url": "https://en.wikipedia.org/wiki?curid=34304924", "title": "Quasiregular map", "text": "Quasiregular map\n\nIn the mathematical field of analysis, quasiregular maps are a class of continuous maps between Euclidean spaces R of the same dimension or, more generally, between Riemannian manifolds of the same dimension, which share some of the basic properties with holomorphic functions of one complex variable.\n\nThe theory of holomorphic (=analytic) functions of one complex variable is one of the most beautiful and most useful parts of the whole mathematics.\n\nOne drawback of this theory is that it deals only with maps between two-dimensional spaces (Riemann surfaces). The theory of functions\nof several complex variables has a different character, mainly because analytic functions of several variables are not conformal. Conformal maps can be defined between Euclidean spaces of arbitrary dimension, but when the dimension is greater than 2, this class of maps is very small: it consists of Möbius transformations only.\nThis is a theorem of Joseph Liouville; relaxing the smoothness assumptions does not help, as proved by Yurii Reshetnyak.\n\nThis suggests the search of a generalization of the property of conformality which would give a rich and interesting class of maps in higher dimension.\n\nA differentiable map \"f\" of a region \"D\" in R to R is called \"K\"-quasiregular if the following inequality holds at all points in \"D\":\n\nHere \"K\" ≥ 1 is a constant, \"J\" is the Jacobian determinant, \"Df\" is the derivative, that is the linear map defined by the Jacobi matrix, and ||·|| is the usual (Euclidean) norm of the matrix.\n\nThe development of the theory of such maps showed that it is unreasonable to restrict oneself to differentiable maps in the classical sense, and that the \"correct\" class of maps consists of continuous maps in the Sobolev space \"W\" whose partial derivatives in the sense of distributions have locally summable \"n\"-th power, and such that the above inequality is satisfied almost everywhere. This is a formal definition of a \"K\"-quasiregular map. A map is called quasiregular if it is \"K\"-quasiregular with some \"K\". Constant maps are excluded from the class of quasiregular maps.\n\nThe fundamental theorem about quasiregular maps was proved by Reshetnyak:\n\nThis means that the images of open sets are open and that preimages of points consist of isolated points. In dimension 2, these two properties give a topological characterization of the class of non-constant analytic functions:\nevery continuous open and discrete map of a plane domain to the plane can be pre-composed with a homeomorphism, so that the result is an analytic function. This is a theorem of Simion Stoilov.\n\nReshetnyak's theorem implies that all pure topological results about analytic functions (such that the Maximum Modulus Principle, Rouché's theorem etc.) extend to quasiregular maps.\n\nInjective quasiregular maps are called quasiconformal. A simple example of non-injective quasiregular map is given in cylindrical coordinates in 3-space by the formula\n\nThis map is 2-quasiregular. It is smooth everywhere except the \"z\"-axis. A remarkable fact is that all smooth quasiregular maps are local homeomorphisms. Even more remarkable is that every quasiregular local homeomorphism R → R, where \"n\" ≥ 3, is a homeomorphism (this is a theorem of Vladimir Zorich).\n\nThis explains why in the definition of quasiregular maps it is not reasonable to restrict oneself to smooth maps: all smooth quasiregular maps of R to itself are quasiconformal.\n\nMany theorems about geometric properties of holomorphic functions of one complex variable have been extended to quasiregular maps. These extensions are usually highly non-trivial.\n\nPerhaps the most famous result of this sort is the extension of Picard's theorem which is due to Seppo Rickman:\n\nWhen \"n\" = 2, this omitted set can contain at most two points (this is a simple extension of Picard's theorem). But when \"n\" > 2, the omitted set can contain more than two points, and its cardinality can be estimated from above in terms of \"n\" and \"K\". In fact, any finite set\ncan be omitted, as shown by David Drasin and Pekka Pankka.\n\nIf \"f\" is an analytic function, then log \"|f|\" is subharmonic, and harmonic away from the zeros of \"f\". The corresponding fact for quasiregular maps is that log \"|f|\" satisfies a certain non-linear partial differential equation of elliptic type.\nThis discovery of Reshetnyak stimulated the development of non-linear potential theory, which treats this kind of equations\nas the usual potential theory treats harmonic and subharmonic functions.\n\n"}
{"id": "232423", "url": "https://en.wikipedia.org/wiki?curid=232423", "title": "Reification (computer science)", "text": "Reification (computer science)\n\nReification is the process by which an abstract idea about a computer program is turned into an explicit data model or other object created in a programming language. A computable/addressable object — a resource — is created in a system as a proxy for a non computable/addressable object. By means of reification, something that was previously implicit, unexpressed, and possibly inexpressible is explicitly formulated and made available to conceptual (logical or computational) manipulation. Informally, reification is often referred to as \"making something a first-class citizen\" within the scope of a particular system. Some aspect of a system can be reified at \"language design time\", which is related to reflection in programming languages. It can be applied as a stepwise refinement at \"system design time\". Reification is one of the most frequently used techniques of conceptual analysis and knowledge representation.\n\nIn the context of programming languages, reification is the process by which a user program or any aspect of a programming language that was implicit in the translated program and the run-time system, are expressed in the language itself. This process makes it available to the program, which can inspect all these aspects as ordinary data. In reflective languages, reification data is causally connected to the related reified aspect such that a modification to one of them affects the other. Therefore, the reification data is always a faithful representation of the related reified aspect . Reification data is often said to be made a first class object. Reification, at least partially, has been experienced in many languages to date: in early Lisp dialects and in current Prolog dialects, programs have been treated as data, although the causal connection has often been left to the responsibility of the programmer. In Smalltalk-80, the compiler from the source text to bytecode has been part of the run-time system since the very first implementations of the language.\n\n\nData reification (stepwise refinement) involves finding a more concrete representation of the abstract data types used in a formal specification.\n\nData reification is the terminology of the Vienna Development Method (VDM) that most other people would call data refinement. An example is taking a step towards an implementation by replacing a data representation without a counterpart in the intended implementation language, such as sets, by one that does have a counterpart (such as maps with fixed domains that can be implemented by arrays), or at least one that is closer to having a counterpart, such as sequences. The VDM community prefers the word \"reification\" over \"refinement\", as the process has more to do with concretising an idea than with refining it.\n\nFor similar usages, see Reification (linguistics).\n\nReification is widely used in conceptual modeling. Reifying a relationship means viewing it as an entity. The purpose of reifying a relationship is to make it explicit, when additional information needs to be added to it. Consider the relationship type \"codice_4\". An instance of \"codice_5\" is a relationship that represents the fact that a person is a member of a committee. The figure below shows an example population of \"codice_5\" relationship in tabular form. Person \"P1\" is a member of committees \"C1\" and \"C2\". Person \"P2\" is a member of committee \"C1\" only. \n\nThe same fact, however, could also be viewed as an entity. Viewing a relationship as an entity, one can say that the entity reifies the relationship. This is called reification of a relationship. Like any other entity, it must be an instance of an entity type. In the present example, the entity type has been named codice_7. For each instance of \"codice_5\", there is one and only one instance of \"codice_7\", and vice versa. Now, it becomes possible to add more information to the original relationship. As an example, we can express the fact that \"person p1 was nominated to be the member of committee c1 by person p2\". Reified relationship \"codice_7\" can be used as the source of a new relationship \"codice_11\".\n\nFor related usages see Reification (knowledge representation).\n\n UML provides an \"association class\" construct for defining reified relationship types. The association class is a single model element that is both a kind of association and a kind of a class. The association and the entity type that reifies are both the same model element. Note that attributes cannot be reified.\n\nIn Semantic Web languages, such as Resource Description Framework (RDF) and Web Ontology Language (OWL), a statement is a binary relation. It is used to link two individuals or an individual and a value. Applications sometimes need to describe other RDF statements, for instance, to record information like when statements were made, or who made them, which is sometimes called \"provenance\" information. As an example, we may want to represent properties of a relation, such as our certainty about it, severity or strength of a relation, relevance of a relation, and so on.\n\nThe example from the conceptual modeling section describes a particular person with codice_12, who is a member of the codice_13. The RDF triple from that description is\n\nConsider to store two further facts: (i) to record who nominated this particular person to this committee (a statement about the membership itself), and (ii) to record who added the fact to the database (a statement about the statement).\n\nThe first case is a case of classical reification like above in UML: reify the membership and store its attributes and roles etc.:\n\nAdditionally, RDF provides a built-in vocabulary intended for describing RDF statements. A description of a statement using this vocabulary is called a reification of the statement. The RDF reification vocabulary consists of the type codice_14, and the properties codice_15, codice_16, and codice_17.\n\nUsing the reification vocabulary, a reification of the statement about the person's membership would be given by assigning the statement a URIref such as codice_18 so that describing statements can be written as follows:\n\nThese statements say that the resource identified by the codice_19 is an RDF statement, that the subject of the statement refers to the resource identified by codice_20, the predicate of the statement refers to the resource identified by codice_21, and the object of the statement refers to the resource codice_13. Assuming that the original statement is actually identified by codice_18, it should be clear by comparing the original statement with the reification that the reification actually does describe it. The conventional use of the RDF reification vocabulary always involves describing a statement using four statements in this pattern. Therefore, they are sometimes referred to as the \"reification quad\".\n\nUsing reification according to this convention, we could record the fact that codice_24 added the statement to the\ndatabase by\n\nIt is important to note that in the conventional use of reification, the subject of the reification triples is assumed to identify a particular instance of a triple in a particular RDF document, rather than some arbitrary triple having the same subject, predicate, and object. This particular convention is used because reification is intended for expressing properties such as dates of composition and source information, as in the examples given already, and these properties need to be applied to specific instances of triples. \nNote that the described triple codice_25 itself is not implied by such a reification quad (and it is not necessary that it actually exists in the database). This allows also to use this mechanism to express which triples do \"not\" hold.\n\nThe power of the reification vocabulary in RDF is restricted by the lack of a built-in means for assigning URIrefs to statements, so in order to express \"provenance\" information of this kind in RDF, one has to use some mechanism (outside of RDF) to assign URIs to individual RDF statements, then make further statements about those individual statements, using their URIs to identify them.\n\nIn an XML Topic Map (XTM), only a topic can have a name or play a role in an association. One may use an association to make an assertion about a topic, but one cannot directly make assertions about that assertion. However, it is possible to create a topic that reifies a non-topic construct in a map, thus enabling the association to be named and treated as a topic itself.\n\nIn Semantic Web languages, such as RDF and OWL, a property is a binary relation used to link two individuals or an individual and a value. However, in some cases, the natural and convenient way to represent certain concepts is to use relations to link an individual to more than just one individual or value. These relations are called n-ary relations. Examples are representing relations among multiple individuals, such as a committee, a person who is a committee member and another person who has nominated the first person to become the committee member, or a buyer, a seller, and an object that was bought when describing a purchase of a book.\n\nA more general approach to reification is to create an explicit new class and n new properties to represent an n-ary relation, making an instance of the relation linking the n individuals an instance of this class. This approach can also be used to represent provenance information and other properties for an individual relation instance.\nIt is also important to note that the reification described here is not the same as \"quotation\" found in other languages. Instead, the reification describes the relationship between a particular instance of a triple and the resources the triple refers to. The reification can be read intuitively as saying \"this RDF triple talks about these things\", rather than (as in quotation) \"this RDF triple has this form.\" For instance, in the reification example used in this section, the triple:\n\ndescribing the codice_15 of the original statement says that the subject of the statement is the resource (the person) identified by the URIref codice_20. It does not state that the subject of the statement is the URIref itself (i.e., a string beginning with certain characters), as quotation would.\n\n"}
{"id": "28396062", "url": "https://en.wikipedia.org/wiki?curid=28396062", "title": "Rotation map", "text": "Rotation map\n\nIn mathematics, a rotation map is a function that represents an undirected edge-labeled graph, where each vertex enumerates its outgoing neighbors. Rotation maps were first introduced by Reingold, Vadhan and Wigderson (“Entropy waves, the zig-zag graph product, and new constant-degree expanders”, 2002) in order to conveniently define the zig-zag product and prove its properties.\nGiven a vertex formula_1 and an edge label formula_2, the rotation map returns the formula_2'th neighbor of formula_1 and the edge label that would lead back to formula_1.\n\nFor a \"D\"-regular graph \"G\", the rotation map formula_6 is defined as follows: formula_7 if the \"i\" th edge leaving \"v\" leads to \"w\", and the \"j\" th edge leaving \"w\" leads to \"v\".\n\nFrom the definition we see that formula_8 is a permutation, and moreover formula_9 is the identity map (formula_8 is an involution).\n\n\n"}
{"id": "4252019", "url": "https://en.wikipedia.org/wiki?curid=4252019", "title": "Scale factor (computer science)", "text": "Scale factor (computer science)\n\nA scale factor is used in computer science when a real world set of numbers needs to be represented on a different scale in order to fit a specific number format. For instance, a 16 bit unsigned integer (\"uint16\") can only hold a value as large as 65,535. If uint16's are to be used to represent values from 0 to 131,070, then a scale factor of 1/2 would be introduced. Notice that while the scale factor extends the range, it also decreases the precision. In this example, for instance, the number 3 could not be represented because a stored \"1\" represents a real world 2, and a stored \"2\" represents a real world 4.\n\nCertain number formats may be chosen for an application for convenience in programming, or because of certain advantages offered by the hardware for that number format. For instance, early processors did not natively support the IEEE floating point standard for representing fractional values, so integers were used to store representations of the real world values by applying a scale factor to the real value. By necessity, this was done in software, since the hardware did not support fractional value.\n\nOnce the scaled representation of a real value is stored, the scaling can often be ignored until the value needs to come back into the \"real world\". For instance, adding two scaled values is just as valid as unscaling the values, adding the real values, and then scaling the result, and the former is much easier and faster. For other operations, however, the scaling is very important.\n\nMultiplication, for instance, needs to take account of the fact that both numbers are scaled. As an example, consider two real world values \"A\" and \"B\". The real world multiplication of these real world values is:\n\nNow suppose we're storing the values with a scale factor of \"Z\". If we simply multiply the stored representations we'll get the following:\n\nNote how \"AZ\" is the scaled real world value of \"A\" or simply the product of \"A * Z\", and likewise, \"BZ\" is the scaled representation of \"B\". Also note that we didn't write \"PZ\" as the answer, the reason is simple: PZ is \"not\" the answer. You can see this by rearranging the statement, where each line in the following is equivalent:\n\nNote how we substituted \"P\" for \"A * B\" on line 4. You can now see that the result of \"AZ * BZ\" (which is \"Q\") is NOT \"PZ\", it's \"PZ * Z\". If \"PZ\" were the answer, we could simply store it directly, since it has the scale factor built in, as is the case with addition and subtraction. For multiplication, however, you can see that the product of two scaled values has an extra scaling built in. As long as this is taken into account, there's still no need to convert \"AZ\" and \"BZ\" into \"A\" and \"B\" before performing the operation, you just need to divide the result by \"Z\" before storing it back. You will then have \"PZ\" stored as the result of the multiplication, which is fine because you weren't storing the result of \"AZ * BZ\", you were storing the scaled representation of the result of \"A * B\".\n\nAs already mentioned, many older processors (and possibly some current ones) do not natively support fractional mathematics. In this case, fractional values can be scaled into integers by multiplying them by ten to the power of whatever decimal precision you want to retain. In other words, if you want to preserve \"n\" digits to the right of the decimal point, you need to multiply the entire number by 10. (Or if you're working in binary and you want to save \"m\" digits to the right of the binary point, then you would multiply the number by 2, or alternately, bit shift the value \"m\" places to the left). For example, consider the following set of real world fractional values:\n\nNotice how they all have \"3\" digits to the right of the decimal place. If we want to save all of that information (in other words, not lose any precision), we need to multiply these numbers by 10, or 1,000, giving us integer values of:\n\nCertain processors, particularly DSPs common in the embedded system industry, have built in support for the fixed point arithmetic, such as Q and IQ formats.\n\nSince the fractional part of a number takes up some bits in the field, the range of values possible in a fixed point value is less than the same number of bits would provide to an integer. For instance, in an 8 bit field, an unsigned integer can store values from <nowiki>[0, 255]</nowiki> but an unsigned fixed point with 5 fractional bits only has 3 bits left over for the integer value, and so can only store integer values from <nowiki>[0, 7]</nowiki> (note that the \"number\" of values that the two fields can store is the same, 2 = 256, because the fixed point field can also store 32 fractional values for each integer value). It is therefore common that a scaling factor is used to store real world values that may be larger than the maximum value of the fixed point format.\n\nAs an example, assume we are using an unsigned 8 bit fixed point format with 4 fractional bits, and 4 integer bits. As mentioned, the highest integer value it can store is 15, and the highest mixed value it can store is 15.9375 (0xF.F or 1111.1111). If the real world values we want to manipulate are in the range [0,160], we need to scale these values in order to get them into fixed point. Note that we \"can not\" use a scale factor of 1/10 here because scaling 160 by 1/10 gives us 16, which is greater than the greatest value we can store in our fixed point format. 1/11 will work as a scale factor, however, because 160/11 = 14.5454... which fits in our range. Let's use this scale factor to convert the following real world values into scaled representations:\n\nScaling these with the scale factor (1/11) gives us the following values:\n\nNote however, that many of these values have been truncated because they contain repeating fractions. When we try to store these in our fixed point format, we're going to lose some of our precision (which didn't seem all that precise when they were just integers). This is an interesting problem because we said we could fit 256 different values into our 8 bit format, and we're only trying to store values from a range with 161 possible values (0 through 160). As it turns out, the problem was our scale factor, 11, which introduced unnecessary precision requirements. The resolution of the problem is to find a better scaling factor. For more information, read on.\n\nAn example above illustrated how certain scale factors can cause unnecessary precision loss. We will revisit this example to further explore the situation.\n\nWe're storing representations of real data in 8 bit unsigned fixed point fields with 4 integer bits and 4 fractional bits. This gives us a range of <nowiki>[0, 15.9375]</nowiki> in decimal, or <nowiki>[0x0.0, 0xF.F]</nowiki> in hex. Our real world data is all integers and in the range [0, 160] in decimal. Note that there are only 161 unique values that we may want to store, so our 8 bit field should be plenty, since 8 bits can have 256 unique configurations.\n\nIn the example given above, we picked a scale factor of 11 so that all the numbers would be small enough to fit in the range. However, when we began scaling the following real world data:\n\nWe discovered that the precision of these fractions is going to be a problem. The following box illustrates this showing the original data, its scaled decimal values, and the binary equivalent of the scaled value.\n\nNotice how several of the binary fractions require more than the 4 fractional bits provided by our fixed point format. To fit them into our fields, we would simply truncate the remaining bits, giving us the following stored representations:\n\nOr in decimal:\n\nAnd when we need to bring them back into the real world, we need to divide by our scale factor, 1/11, giving the following \"real world\" values:\n\nNotice how they've changed? For one thing, they aren't all integers anymore, immediately indicating that an error was introduced in the storage, due to a poor choice of scaling factor.\n\nMost data sets will not have a perfect scale factor; you will probably always get some error introduced by the scaling process. However it certainly may be possible to pick a better scaling factor. For one thing, note that dividing a number by a power of two is the same as shifting all the bits to the right once for each power of two. (It's the same thing in decimal, when you divide by 10, you shift all the decimal digits one place to the right, when you divide by 100, you shift them all two places to the right). The pattern of bits doesn't change, it just moves. On the other hand, when you divide by a number that is NOT an integer power of 2, you are changing the bit pattern. This is likely to produce a bit pattern with even more bits to the right of the binary point, artificially introducing required precision. Therefore, it is almost always preferable to use a scale factor that is a power of two. You may still lose bits that get shifted right off the end of the field, but at least you won't be introducing \"new\" bits that will be shifted off the end.\n\nTo illustrate the use of powers of two in the scale factor, let's use a factor of 1/16 with the above data set. The binary value for our original data set is given below:\n\nAs we already knew, they all fit in 8 bits. Scaling these by 1/16 is the same as dividing by 16, which is the same as shifting the bits 4 places to the right. All that really means is inserting a binary point between the first four and last four bits of each number. Conveniently, that's the exact format of our fixed point fields. So just as we suspected, since all these numbers don't require more than 8 bits to represent them as integers, it doesn't have to take more than 8 bits to scale them down and fit them in a fixed point format.\n\n 1. Fixed-Point Arithmetic: An Introduction, Randy Yates, July 7, 2009 -- www.digitalsignallabs.com \n"}
{"id": "25283233", "url": "https://en.wikipedia.org/wiki?curid=25283233", "title": "Self-financing portfolio", "text": "Self-financing portfolio\n\nIn financial mathematics, a self-financing portfolio is a portfolio having the feature that, if there is no exogenous infusion or withdrawal of money, the purchase of a new asset must be financed by the sale of an old one.\n\nLet formula_1 denote the number of shares of stock number 'i' in the portfolio at time formula_2, and formula_3 the price of stock number 'i' in a frictionless market with trading in continuous time. Let\n\nThen the portfolio formula_5 is self-financing if\n\nAssume we are given a discrete filtered probability space formula_7, and let formula_8 be the solvency cone (with or without transaction costs) at time \"t\" for the market. Denote by formula_9. Then a portfolio formula_10 (in physical units, i.e. the number of each stock) is self-financing (with trading on a finite set of times only) if \n\nIf we are only concerned with the set that the portfolio can be at some future time then we can say that formula_14.\n\nIf there are transaction costs then only discrete trading should be considered, and in continuous time then the above calculations should be taken to the limit such that formula_15.\n\n"}
{"id": "147854", "url": "https://en.wikipedia.org/wiki?curid=147854", "title": "Straightedge", "text": "Straightedge\n\nA straightedge or straight edge is a tool used for drawing straight lines, or checking their straightness. If it has equally spaced markings along its length, it is usually called a ruler.\n\nStraightedges are used in the automotive service and machining industry to check the flatness of machined mating surfaces.\n\nTrue straightness can in some cases be checked by using a laser line level as an optical straightedge: it can illuminate an accurately straight line on a flat surface such as the edge of a plank or shelf.\n\nA pair of straightedges called winding sticks are used in woodworking to amplify twist (wind) in pieces of wood.\n\nAn idealized straightedge is used in compass-and-straightedge constructions in plane geometry. \nIt may be used:\nIt may not be marked or used together with the compass so as to transfer the length of one segment to another.\n\nIt is possible to do all compass and straightedge constructions without the straightedge. That is, it is possible, using only a compass, to find the intersection of two lines given two points on each, and to find the tangent points to circles. It is not, however, possible to do all constructions using only a straightedge. It \"is\" possible to do them with straightedge alone given one circle and its center.\n\n\n\n"}
{"id": "39469026", "url": "https://en.wikipedia.org/wiki?curid=39469026", "title": "Strategic Network Formation", "text": "Strategic Network Formation\n\nStrategic Network Formation defines how and why networks take particular forms. In many networks, the relation between nodes is determined by the choice of the participating players involved, not by an arbitrary rule. A “strategic” modeling of network requires defining a network’s costs and benefits and predicts how individual preferences become outcomes.\n\nA strategic network formation requires that individuals create relations that are beneficial and drop those that are not. One of the most well-known examples in this context is the marriage network of sixteen families in Florence, which showed how the Medici family gained power and took control of Florence by creating a high number of inter-marriages with the other families. “Thus, decisions about profitable relations are not a situation of choice, but a situation of strategic interaction – an aspect that is best covered by Game Theory”. In these kinds of settings, the nodes are usually called players, where formula_1{1, 2,… formula_2} is a set of players that have formed links in a network. Social Networks have diverse settings, however the simplest ones can be described by an undirected graph whereas more complicated situations are represented by directed graphs. There are fundamental differences in the way these games are modeled depending on their graph structure. If a link exists between player formula_3 and player formula_4 it is noted as formula_5. In cases of undirected networks, formula_5 is considered equal to formula_7. A network formula_8 represents a list of all the links between players. In a more formal setting, a network formula_8 is defined as a set of unordered pairs {formula_10}, with formula_11 element of formula_12.The set of all possible graphs on the set of players formula_13 is denoted with formula_14. The benefits that they receive from the network are represented by utility functions. That is, the payoff to a player i is represented by a function formula_15 : formula_16 formula_17 formula_18, where formula_15 (formula_8) represents the net benefit that i receives if network formula_8 is in place. To model strategic network formation the notion of network games is used. A network game is a set of linked players and their utility functions.\n\nNetwork games can be modeled in different ways. Some of the modeling methods that separate the utility allocation from the network formation process are extensive form games, simultaneous move games, Pariwise stability concept etc.\n\nIf a network is modeled according to the extensive form game concept then the players of the network first propose to create links one after the other and afterwards they make decisions to create a link or not. In such settings, a couple of players decide to either form a link or not by being aware of all the previous players’ decisions and by making predictions for the decisions of the following players.\n\nIn simultaneous move game settings, all the players declare at the same time to whom they want to link. Even though these sorts of games are easy to understand and analyze, their drawback is that they have multiple Nash Equilibria.\n\nIn social networks, a link between two players is formed only if both of them decide to do so, however either of them can make the decision to delete a link without the other player’s approval. The concept of Nash equilibrium has a drawback in this case since it does not take into consideration the fact that the players can discuss their decisions. To model such a situation a stability concept that takes this fact into account is required. A useful stability concept in this case is Pairwise Stability, which accounts for the mutual approval of both players. A network is considered pairwise stable if:\n\n(i) for all formula_5 ∈ formula_8, formula_15(formula_8) formula_26 formula_15(formula_8-formula_5) and formula_30(formula_8) formula_26 formula_30 (formula_8-formula_5), and\n\nTherefore, a network in which there are no two players that want to create a link and where neither one of them wants to delete a link is pairwise stable. Some of the drawbacks that make the pariwise stability concept weak, are the fact that is does not consider changes of multiple links at a time, but it only looks at changes that happen between single links. The fact that it considers movements for only a couple of players at a given time can be considered as an additional weakness.\n\nThere is a difference between networks that maximize the social welfare and networks that are based on personal incentives. In strategic network formation it is important to look at the overall social benefit and to see if networks that players create manage to be efficient for the society in general. A network formula_8 is efficient relative to a profile of utility functions (formula_47..., formula_48) if formula_49 for all formula_50.\n\nPareto Efficiency is another efficiency concept used by economists to study the overall social welfare. \nA network formula_8 is Pareto efficient relative to (formula_47... formula_48) if there does not exist any formula_54 such that formula_15(formula_56) formula_26 formula_15(formula_8) for all formula_3 with strict inequality for some formula_3. The Pareto efficiency notion is more reasonable in settings in which allocation rules are fixed. A network can Pareto dominate another network if it has strictly larger benefits for one individual and weakly larger benefits for all individuals. If there exists a network which is not Pareto dominated by another network then it is a Pareto efficient network. In the figure \"An Example of Effcient, Pareto Effcient, and Pairwise Stable Networks in a Four Person Society\" an example with four players is given, where the payoffs of the players are noted by the numbers next to the nodes. If an arrow is pointing away from a network, it means that the network is not stable, since there would be benefit from deleting a link from a player or by creating a new link from two players of the network. The network in red is Efficient and Pareto efficient, since all the other link combinations offer lower payoffs to some of the players. The network in green is Pareto efficient since the payoffs are higher but it is not Pairwise Stable because the players that have created only one link would also benefit by adding links to one another. The only Pairwise Stable network in the figure is the dark blue colored one since none of the players involved would benefit by deleting or creating a link.\n\nJackson and Wolinsky showed that for homogeneous connection cost, the efficient network can only take one of three forms: a complete graph, a star or an empty graph depending on connection cost and benefits. Finding general analytical solutions for the efficient networks with heterogeneous costs can be intractable. However, for particular cost structures, such as Island-connection and Separable Heterogeneous connection costs, the efficient network can identified based on the heterogeneous connection costs and benefits. For the latter the efficient network has \"generalized star\" structure.\n\nThe utility the players receive does not just come from the direct links that they form with each other, but also from their indirect relations. The benefit function formula_62: {1,… formula_63} formula_17 formula_18 measures the indirect benefit that players obtain from being close to other players in the network. When we consider distance, the utility function takes the form\n\nformula_66, where formula_67 represents the shortest path length between player formula_3 and player formula_4.\n\nThe distance-based utility assumes that all players’ utility functions are alike and it takes into account only the benefits from indirect links that depend on minimum path length. These two features are considered as drawbacks of the distance-based utility.\n\nExternalities show that players’ benefits depend heavily on other players’ commitment decisions. The distance-based utility showed that the payoffs of players do not just depend on the direct links that they form, but also on the links that other players have created in the network. Players may confront positive or negative externalities in networks. The distance-based utility model is an example of positive externalities, since players can only get more benefits when other players increase their number of connections. On the other hand, a model that faces players with negative externalities is the so-called “Co-Author model” presented by Jackson and Wolinsky in the paper of 1996. Given that working on a research paper requires time and devotion, two researchers can benefit more if are only working with each other at a given period of time and not with many other people. Therefore, in the “Co-Author model” researchers benefit more if their other colleagues have fewer links. In this model, if a player’s neighbors have many links, it will bring negative externalities to them. In different models, positive or negative externalities lead to inefficiency.\n"}
{"id": "15991830", "url": "https://en.wikipedia.org/wiki?curid=15991830", "title": "Størmer number", "text": "Størmer number\n\nIn mathematics, a Størmer number or arc-cotangent irreducible number, named after Carl Størmer, is a positive integer \"n\" for which the greatest prime factor of \"n\" + 1 is greater than or equal to 2\"n\".\n\nThe first few Størmer numbers are:\n\nJohn Todd proved that this sequence is neither finite nor cofinite.\nMore precisely, the natural density of the Størmer numbers lies between 0.5324 and 0.905.\nIt has been conjectured that their natural density is the natural logarithm of 2, approximately 0.693, but this remains unproven.\nBecause the Stormer numbers have positive density, the Stormer numbers form a large set.\nA number of the form 2x for x>1 cannot be a Stormer number. This is because (2x)+1 = 4x+1 = (2x-2x+1)(2x+2x+1).\n\nThe Størmer numbers arise in connection with the problem of representing the Gregory numbers (arctangents of rational numbers) formula_1 as sums of Gregory numbers for integers (arctangents of unit fractions). The Gregory number formula_2 may be decomposed by repeatedly multiplying the Gaussian integer formula_3 by numbers of the form formula_4, in order to cancel prime factors \"p\" from the imaginary part; here formula_5 is chosen to be a Størmer number such that formula_6 is divisible by formula_7.\n"}
{"id": "1759187", "url": "https://en.wikipedia.org/wiki?curid=1759187", "title": "Superabundant number", "text": "Superabundant number\n\nIn mathematics, a superabundant number (sometimes abbreviated as SA) is a certain kind of natural number. A natural number \"n\" is called superabundant precisely when, for all \"m\" < \"n\"\n\nwhere \"σ\" denotes the sum-of-divisors function (i.e., the sum of all positive divisors of \"n\", including \"n\" itself). The first few superabundant numbers are 1, 2, 4, 6, 12, 24, 36, 48, 60, 120, ... . For example, the number 5 is not a superabundant number because for 1, 2, 3, 4, and 5, the sigma is 1, 3, 4, 7, 6, and 7/4 > 6/5.\n\nSuperabundant numbers were defined by . Unknown to Alaoglu and Erdős, about 30 pages of Ramanujan's 1915 paper \"Highly Composite Numbers\" were suppressed. Those pages were finally published in The Ramanujan Journal 1 (1997), 119–153. In section 59 of that paper, Ramanujan defines generalized highly composite numbers, which include the superabundant numbers.\n\n proved that if \"n\" is superabundant, then there exist a \"k\" and \"a\", \"a\", ..., \"a\" such that\n\nwhere \"p\" is the \"i\"-th prime number, and\n\nThat is, they proved that if \"n\" is superabundant, the prime decomposition of \"n\" has non-increasing exponents (the exponent of a larger prime is never more than that a smaller prime) and that all primes up to formula_4 are factors of \"n\". Then in particular any superabundant number is an even integer, and it is a multiple of the \"k\"-th primorial formula_5\n\nIn fact, the last exponent \"a\" is equal to 1 except when n is 4 or 36.\n\nSuperabundant numbers are closely related to highly composite numbers. Not all superabundant numbers are highly composite numbers. In fact, only 449 superabundant and highly composite numbers are the same . For instance, 7560 is highly composite but not superabundant. Conversely, 1163962800 is superabundant but not highly composite.\n\nAlaoglu and Erdős observed that all superabundant numbers are highly abundant. \n\nNot all superabundant numbers are Harshad numbers. The first exception is the 105th SA number, 149602080797769600. The digit sum is 81, but 81 does not divide evenly into this SA number.\n\nSuperabundant numbers are also of interest in connection with the Riemann hypothesis, and with Robin's theorem that the Riemann hypothesis is equivalent to the statement that\nfor all \"n\" greater than the largest known exception, the superabundant number 5040. If this inequality has a larger counterexample, proving the Riemann hypothesis to be false, the smallest such counterexample must be a superabundant number .\n\nNot all superabundant numbers are colossally abundant.\n\nThe generalized formula_7-super abundant numbers are those such that formula_8 for all formula_9, where formula_10 is the sum of the formula_7-th powers of the divisors of formula_12.\n\n1-super abundant numbers are superabundant numbers. 0-super abundant numbers are highly composite numbers.\n\nFor example, generalized 2-super abundant numbers are 1, 2, 4, 6, 12, 24, 48, 60, 120, 240, … (A208767 in OEIS)\n\n\n"}
{"id": "12196164", "url": "https://en.wikipedia.org/wiki?curid=12196164", "title": "Tail sequence", "text": "Tail sequence\n\nIn mathematics, specifically set theory, a tail sequence is an unbounded sequence of contiguous ordinals. Formally, let β be a limit ordinal. Then a γ-sequence formula_1 is a tail sequence in β if there exists an ε < β such that \"s\" is a normal sequence assuming all values in formula_2\n"}
{"id": "19373832", "url": "https://en.wikipedia.org/wiki?curid=19373832", "title": "Timeline of number theory", "text": "Timeline of number theory\n\nA timeline of number theory.\n\n\n\n\n\n\n\n\n"}
{"id": "552624", "url": "https://en.wikipedia.org/wiki?curid=552624", "title": "United Kingdom Mathematics Trust", "text": "United Kingdom Mathematics Trust\n\nThe United Kingdom Mathematics Trust (UKMT) is a charity founded in 1996 to help with the education of children in mathematics within the UK.\n\nThe national mathematics competitions existed prior to the formation of the UKMT, but the foundation of the UKMT in the summer of 1996 enabled them to be run collectively. The Senior Mathematical Challenge was formerly the National Mathematics Contest. Founded in 1961, it was run by the Mathematical Association from 1975 until its adoption by the UKMT in 1996. The Junior and Intermediate Mathematical Challenges were the initiative of Dr Tony Gardiner in 1987 and were run by him under the name of the United Kingdom Mathematics Foundation until 1996. The popularity of the UK national mathematics competitions is largely due to the publicising efforts of Dr Gardiner in the years 1987-1995. Hence, in 1995, he advertised for the formation of a committee and for a host institution that would lead to the establishment of the UKMT, enabling the challenges to be run effectively together under one organisation.\n\nThe UKMT run a series of mathematics challenges to encourage children's interest in mathematics and develop their skills:\n\nIn the Junior and Intermediate Challenges the top scoring 40% of the entrants receive bronze, silver or gold certificates based on their mark in the paper. In the Senior Mathematical Challenge these certificates are awarded to top scoring 60% of the entries. In each case bronze, silver and gold certificates are awarded in the ratio 3 : 2 : 1.\nSo in the Junior and Intermediate Challenges\nFor the Senior Challenge these percentages are 10%, 20% and 30%, respectively.\n\nThe Junior Mathematical Challenge (JMC) is an introductory challenge for pupils in Years 8 or below (aged 13) or below. This takes the form of twenty-five multiple choice questions to be sat in exam conditions, to be completed within one hour. The first fifteen questions are designed to be easier, and a pupil will gain 5 marks for getting a question in this section correct. Questions 16-20 are more difficult and are worth 6 marks, with a penalty of 1 point for a wrong answer which tries to stop pupils guessing. The last five questions are intended to be the most challenging and so are also 6 marks, but with a 2 point penalty for an incorrectly answered question. Questions to which no answer is entered will gain (and lose) 0 marks.\n\nThe top 40% of students get a certificate of varying levels (Gold, Silver or Bronze) based on their score. The highest 1200 scorers are also invited to take part in the Junior Mathematical Olympiad (JMO). Like the JMC, the JMO is sat in schools. This is also divided into two sections. Part A is composed of ten questions in which the candidate gives just the answer (not multiple choice), worth 10 marks (each question 1 mark). Part B consists of 6 questions and encourages students to write out full solutions. Each B question is marked out of 10 and students are encouraged to write complete answers to 2-4 questions rather than hurry through incomplete answers to all 6. If the solution is judged to be incomplete, it is marked on a 0+ basis, maximum 3 marks. If it has an evident logical strategy, it is marked on a 10- basis. The total mark is out of 70. Everyone who participates in this challenge will gain a certificate (Participation 75%, Distinction 25%); the top 200 or so gaining medals (Gold, Silver, Bronze); with the top fifty winning a book prize.\n\nThe Intermediate Mathematical Challenge (IMC) is aimed at school years equivalent to English Years 9-11. Following the same structure as the JMC, this paper presents the student with twenty-five multiple choice questions to be done under exam conditions in one hour. The first fifteen questions are designed to be easier, and a pupil will gain 5 marks for getting a question in this section correct. Questions 16-20 are more difficult and are worth 6 marks, with a penalty of 1 point for a wrong answer which tries to stop pupils guessing. The last five questions are intended to be the most challenging and so are also 6 marks, but with a 2 point penalty for an incorrectly answered question. Questions to which no answer is entered will gain (and lose) 0 marks.\n\nAgain, the top 40% of students taking this challenge get a certificate. There are two follow-on rounds to this competition: The European Kangaroo and the Intermediate Mathematical Olympiad.\n\nTo prevent this getting confused with the International Mathematical Olympiad, this is often abbreviated to the \"IMOK Olympiad\" (IMOK = Intermediate Mathematical Olympiad and Kangaroo).\n\nThe IMOK is sat by the top 500 scorers from each school year in the Intermediate Maths Challenge and consists of three papers, 'Cayley', 'Maclaurin' and 'Hamilton' named after famous mathematicians. The paper the student will undertake depends on the year group that student is in (Cayley for those in year 9 and below, Hamilton for year 10 and Maclaurin for year 11).\n\nEach paper contains six questions. Each solution is marked out of 10 on a 0+ and 10- scale; that is to say, if an answer is judged incomplete or unfinished, it is awarded a few marks for progress and relevant observations, whereas if it is presented as complete and correct, marks are deducted for faults, poor reasoning, or unproven assumptions. As a result, it is quite uncommon for an answer to score a middling mark (e.g. 4–6). This makes the maximum mark out of 60. For a student to get two questions fully correct is considered \"very good\". All people taking part in this challenge will get a certificate (participation for the bottom 50%, merit for the next 25% and distinction for the top 25%). The mark boundaries for these certificates change every year, but normally around 30 marks will gain a Distinction. Those scoring highly (the top 50) will gain a book prize; again, this changes every year, with 44 marks required in the Maclaurin paper in 2006. Also, the top 100 candidates will receive a medal; bronze for Cayley, silver for Hamilton and gold for Maclaurin.\n\nIn addition to the book prize, each year approximately two x forty students are chosen to go to a National Mathematics Summer School in July (two separate summer schools each of 1 week). At this summer school the students are stretched, with daily lectures going beyond the normal GCSE syllabus and exploring some of the wider (and more appealing) aspects of mathematics.\n\nThe European Kangaroo is a competition which follows the same structure as the AMC (Australian Mathematics Competition). There are twenty-five multiple-choice questions and no penalty marking. This paper is taken throughout Europe by over 3 million pupils from more than 37 countries. Two different Kangaroo papers follow on from the Intermediate Maths Challenge and the next 5500 highest scorers below the Olympiad threshold are invited to take part (both papers are by invitation only). The \"Grey Kangaroo\" is sat by students in year 9 and below and the \"Pink Kangaroo\" is sat by those in years 10 and 11. The top 25% of scorers in each paper receive a certificate of merit and the rest receive a certificate of participation. All those who sit either \"Kangaroo\" also receive a keyfob containing a different mathematical puzzle each year. (The puzzles along with solutions)\n\nThe Senior Mathematical Challenge (SMC) is open to students who are in Year 13 (aged 18) or below. The paper has twenty-five multiple choice questions. A correct answer is worth 4 marks, while 1 mark is deducted from a starting total of 25 for an incorrect answer. This gives a score between 0 and 125 marks.\n\nUnlike the JMC and IMC, the top 60% get a certificate, the 1000 (approx.) highest scorers are invited to compete in the British Mathematical Olympiad and the next 2000 (approx.) highest scorers are invited to sit the Senior Kangaroo. Mathematics teachers may also, on payment of a fee, enter students who did not score quite well enough in the SMC, but who might cope well in the next round.\n\nRound 1 of the Olympiad is a three-and-a-half hour examination including six more difficult, long answer questions, which serve to test entrants' puzzle-solving skills. As of 2005, a more accessible first question was added to the paper; before this, it only consisted of 5 questions. Around one hundred high scoring entrants from BMO1 are invited to sit the second round, with the same time limit, in which 4 questions are posed. The twenty top scoring students from the second round are subsequently invited to a training camp at Trinity College, Cambridge for the first stage of the International Mathematical Olympiad UK team selection.\n\nThe Senior Kangaroo is a one-hour examination to which the next 1500 (approx.) highest scorers below the Olympiad threshold are invited and unlike the Olympiad, a fee cannot be paid for entry. The paper consists of twenty questions, each of which require three digit answers (leading zeros are used if the answer is less than 100, since the paper is marked by machine). The top 25% of candidates receive a certificate of merit and the rest receive a certificate of participation.\n\nThe UKMT Team Maths Challenge is an annual event. One team from each participating school, comprising four pupils selected from year 8 and 9 (ages 12–14), competes in a regional round. No more than 2 pupils on a team may be from Year 9. There are over 60 regional competitions in the UK, held between February and May. The winning team in each regional round, as well as a few high-scoring runners-up from throughout the country, are then invited to the National Final in London, usually in late June.\n\nThere are 4 rounds: \n\nIn the National Final however an additional 'Poster Round' is added at the beginning. The poster round is a separate competition, however, since 2018 it is worth up to six marks towards the main event. Four schools have won the Junior Maths Team competition at least twice: Queen Mary's Grammar School in Walsall, City of London School, St Olave's Grammar School, and Westminster Under School.\n\nA pilot event for a competition similar to the Team Challenge, aimed at 16- to 18-year-olds, was launched in the Autumn of 2007. The format is much the same, with a limitation of 2 year 13 (upper sixth-form) pupils per team. There were 19 regional heats held in November, with the winning team from each heat going to a national final held in London on 7 February 2008, with the winners being Torquay Boys' Grammar School. The 2009 final was held in February, with the winners this time being Westminster School. The 2010 final was held in February, and Westminster School retained their title.\n\nIn 2011 Harrow School won the 2011 final, after scoring 178/180 in the main competition itself.\n\nThe 2013 National Final concluded on 5 February at the Camden Centre in London. 62 teams were invited to the final which was won by Westminster School for the third time (2009, 2010, 2013). There was a three-way tie for second place between City of London School, Eton College and Magdalen College School.\n\n\"For more information see British Mathematical Olympiad Subtrust.\"\n\nThe \"British Mathematical Olympiad Subtrust\" is run by the UKMT, it runs the British Mathematical Olympiad as well as the UK Mathematical Olympiad for Girls, several training camps throughout the year such as a winter camp in Hungary, an Easter camp at Trinity College, Cambridge, and other training and selection of the IMO team.\n\n\n"}
{"id": "47412478", "url": "https://en.wikipedia.org/wiki?curid=47412478", "title": "Universality probability", "text": "Universality probability\n\nA Turing machine is a basic model of computation. Some Turing machines might be specific to doing particular calculations. For example, a Turing machine might take input which comprises two numbers and then produce output which is the product of their multiplication. Another Turing machine might take input which is a list of numbers and then give output which is those numbers sorted in order.\n\nA Turing machine which has the ability to simulate any other Turing machine is called universal - in other words, a Turing machine (TM) is said to be a universal Turing machine (or UTM) if, given any other TM, there is a some input (or \"header\") such that the first TM given that input \"header\" will forever after behave like the second TM.\n\nAn interesting mathematical and philosophical question then arises. If a universal Turing machine is given random input (for suitable definition of random), how probable is it that it remains universal forever?\n\nGiven a prefix-free Turing machine, the universality probability of it is the probability that it remains universal even when every input of it (as a binary string) is prefixed by a random binary string. More formally, it is the probability measure of reals (infinite binary sequences) which have the property that every initial segment of them preserves the universality of the given Turing machine. This notion was introduced by the computer scientist Chris Wallace and was first explicitly discussed in print in an article by Dowe (and a subsequent article). However, relevant discussions also appear in an earlier article by Wallace and Dowe.\n\nAlthough the universality probability of a UTM (UTM) was originally suspected to be zero, relatively simple proofs exist that the supremum of the set of universality probabilities is equal to 1, such as a proof based on random walks and a proof in Barmpalias and Dowe (2012).\nOnce one has one prefix-free UTM with a non-zero universality probability, it immediately follows that all prefix-free UTMs have non-zero universality probability.\nFurther, because the supremum of the set of universality probabilities is 1 and because the set \nis dense in the interval [0, 1],\nsuitable constructions of UTMs\n(e.g., if \"U\" is a UTM, define a\nUTM \"U\" by \"U\"(0\"s\") halts for all strings \"s\",\nU(1\"s\") = \"U\"(\"s\") for all s) gives that the set of universality probabilities is\ndense in the open interval (0, 1).\n\nUniversality probability was thoroughly studied and characterized by Barmpalias and Dowe in 2012.\nSeen as real numbers, these probabilities were completely characterized in terms of notions in computability theory\nand algorithmic information theory.\nIt was shown that when the underlying machine is universal, these numbers are highly algorithmically random. More specifically, it is Martin-Löf random relative to the third iteration of the halting problem. In other words, they are random relative to null sets that can be defined with four quantifiers in Peano arithmetic. Vice versa, given such a highly random number (with appropriate approximation properties) there is a Turing machine with universality probability that number.\n\nUniversality probabilities are very related to the Chaitin constant, which is the halting probability of a universal prefix-free machine. In a sense, they are complementary to the halting probabilities of universal machines relative to the third iteration of the halting problem. In particular, the universality probability can be seen as the non-halting probability of a machine with oracle the third iteration of the halting problem. Vice versa, the non-halting probability of any prefix-free machine with this highly non-computable oracle is the universality probability of some prefix-free machine.\n\nUniversality probability provides a concrete and somewhat natural example of a highly random number (in the sense of algorithmic information theory). In the same sense, Chaitin's constant provides a concrete example of a random number (but for a much weaker notion of algorithmic randomness).\n\n\n\n"}
{"id": "43939832", "url": "https://en.wikipedia.org/wiki?curid=43939832", "title": "Wayne Snyder", "text": "Wayne Snyder\n\nWayne Snyder is an associate professor at Boston University known for his work in E-unification theory.\n\nHe was raised in Yardley, Pennsylvania, worked in his father's aircraft shop, attended the Berklee School of Music, and obtained an MA in Augustan poetry at Tufts University.\nHe then studied computer science, and earned his Ph.D. at the University of Pennsylvania in 1988.\nIn 1987 he came to Boston University, teaching introductory computer science, and researching on automated reasoning, and, more particularly, E-unification.\n\n"}
{"id": "1461534", "url": "https://en.wikipedia.org/wiki?curid=1461534", "title": "Wirtinger's inequality for functions", "text": "Wirtinger's inequality for functions\n\nIn mathematics, historically Wirtinger's inequality for real functions was an inequality used in Fourier analysis. It was named after Wilhelm Wirtinger. It was used in 1904 to prove the isoperimetric inequality. A variety of closely related results are today known as Wirtinger's inequality.\n\nLet formula_1 be a periodic function of period 2π, which is continuous and has a continuous derivative throughout R, and such that\n\nThen\n\nwith equality if and only if \"f\"(\"x\") = \"a\" sin(\"x\") + \"b\" cos(\"x\") for some \"a\" and \"b\" (or equivalently \"f\"(\"x\") = \"c\" sin (\"x\" + \"d\") for some \"c\" and \"d\").\n\nThis version of the Wirtinger inequality is the one-dimensional Poincaré inequality, with optimal constant.\n\nThe following related inequality is also called Wirtinger's inequality :\n\nwhenever \"f\" is a C function such that \"f\"(0) = \"f\"(\"a\") = 0. In this form, Wirtinger's inequality is seen as the one-dimensional version of Friedrichs' inequality.\n\nThe proof of the two versions are similar. Here is a proof of the first version of the inequality. Since Dirichlet's conditions are met, we can write\n\nand moreover \"a\" = 0 since the integral of \"f\" vanishes. By Parseval's identity,\n\nand\n\nand since the summands are all ≥ 0, we get the desired inequality, with equality if and only if \"a\" = \"b\" = 0 for all \"n\" ≥ 2.\n\n\n"}
