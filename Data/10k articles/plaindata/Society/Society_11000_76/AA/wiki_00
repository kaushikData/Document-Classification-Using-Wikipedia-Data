{"id": "7380545", "url": "https://en.wikipedia.org/wiki?curid=7380545", "title": "Agraharam", "text": "Agraharam\n\nAn Agraharam or Agrahara was a grant of land and royal income from it, typically by a king or a noble family, to religious purposes, particularly to Brahmins to maintain temples in that land or a pilgrimage site and to sustain their families. Agraharams were also known as Chaturvedimangalams in ancient times. They were also known as ghatoka, and boya.\n\nThe name originates from the fact that the agraharams have lines of houses on either side of the road and the temple to the village god at the centre, thus resembling a garland around the temple. According to the traditional Hindu practice of architecture and town-planning, an agraharam is held to be two rows of houses running north-south on either side of a road at one end of which would be a temple to Shiva and at the other end, a temple to Vishnu. An example is Vadiveeswaram in Tamil Nadu.\n\nWith Brahmins taking up professions in urban areas and some migrating abroad agraharams are vanishing fast. Many of the traditional houses are giving way to concrete structures and commercial buildings.\n\nThe earliest existing description of an \"agraharam\" has been found in a 3rd-century AD Sangam Age work called \"Perumpāṇāṟṟuppaṭai\".\n\nThere are a number of places in Andhra pradesh named \"agraharam\". These places may have originated as Bramhin-populated villages. Examples of such settlements include:\n\n\nThere are a number of places in Southern Karnataka named \"agrahara\". These places might have, probably, originated as Brahmin villages.\n\n\n\n"}
{"id": "212373", "url": "https://en.wikipedia.org/wiki?curid=212373", "title": "Barthélemy Prosper Enfantin", "text": "Barthélemy Prosper Enfantin\n\nBarthélemy Prosper Enfantin (8 February 17961 September 1864) was a French social reformer, one of the founders of Saint-Simonianism. He was also a proponent of a Suez canal.\n\nEnfantin was born in Paris, the son of a banker of Dauphiné. After receiving his early education at a lyceum, he was sent in 1813 to the École polytechnique. In March 1814 he was one of the band of students who, on the heights of Montmartre and Saint-Chaumont, attempted resistance to the armies of the Sixth Coalition which had engaged in the invasion of Paris. In consequence of this outbreak of patriotic enthusiasm, the school was soon after closed by Louis XVIII, and the young student was compelled to seek another career.\n\nInitially, he began working for a country wine merchant, travelling to Germany, Russia and the United Kingdom of the Netherlands. In 1821 he entered a banking-house newly established at Saint Petersburg, but returned two years later to Paris, where he was appointed cashier to the \"Caisse Hypothécaire\". At the same time, he became a member of the secret society of the \"Carbonari\".\n\nIn 1825 a new turn was given to his thoughts and his life by the friendship which he formed with Olinde Rodriguez, who introduced him to the Comte de Saint-Simon. He affiliated to Saint-Simon's version of utopian socialism, and, by 1829, he had become one of the acknowledged heads of the sect.\n\nAfter the July Revolution of 1830 Enfantin resigned his office of cashier, and devoted all his energy to the cause. Besides contributing to \"Le Globe\", he made appeals to the people by systematic preaching, and organized centres of action in some of the main cities of France.\n\nThe headquarters in Paris were moved from the modest rooms in the Rue Taranne to the large halls near the Boulevard des Italiens. Enfantin and Amand Bazard were proclaimed \"Pères Suprêmes\" (\"Supreme Fathers\") – a union which was, however, only nominal, as a divergence was already manifest. Bazard, who concentrated on organizing the group, had devoted himself to political reform, while Enfantin, who favoured teaching and preaching, dedicated his time to social and moral change. The antagonism was widened by Enfantin's announcement of his theory of the relation of man and woman, which would substitute a system of \"free love\" for the \"tyranny of marriage\".\n\nBazard and his disciples broke with Enfantin's group. The latter became sole \"father\", leading a chiefly religiously-oriented movement, joined by new converts (according to Enfantin's estimate, the total number of followers would have reached 40,000). He wore on his breast a badge with his title of \"Père\", was referred to by his preachers as \"the living law\", declared himself to be the chosen of God, and sent out emissaries in a quest of a woman predestined to be the \"female Messiah,\" and the mother of a new Saviour (the latter quest was very costly and altogether fruitless).\n\nMeanwhile, the new religion gathered believers in all parts of Europe. His extravagances and success at length brought him to the attention of authorities, who argued that he was endangering public morality - Enfantin had announced that the gulf between the sexes was too wide and this social inequality would impede rapid growth of society. Enfantin called for the abolition of prostitution and for the ability for women to divorce and obtain legal rights. This was considered radical for the time. In May 1832 the halls of the new sect were closed by the government, and the \"Père\", with some of his followers, appeared before the tribunals. He then retired to his estate at Menilmontant, near Paris, where with forty disciples, all of them men, he continued to carry out his socialist views. In August of the same year he was again arrested, and on his appearance in court he desired his defence to be undertaken by two women who were with him, alleging that the matter was of special concern to women; the request was promptly refused. The trial occupied two days and resulted in a verdict of guilty, and a sentence of imprisonment for a year with a small fine.\n\nThis prosecution discredited the new society. Enfantin was released in a few months. \n\nThen, accompanied by twenty of his followers, many of whom were also engineers from the Ecole Polytechnique, including , also known as Lambert-Bey, and some women, he went first to Constantinople. Enfantin had declared 1933 the Year of the Mother, and upon arrival in Constantinople the group began to strongly preach their views about gender relations and New Christianity. The Ottoman Empire told them to leave to avoid prison.\n\nEnfantin and his group then arrived in Egypt, where he planned to penetrate the feminine Orient with the masculine Occident in a consumation of progression - build a canal connecting the Mediterranean Sea with the Red Sea. In Egypt at that time, Muhammad Ali, the Egyptian Viceroy, was at odds with the Ottoman Sultan in Constantinople, and also practiced public-private contracts known as concessions with mostly European companies to build cheap infrastructure. Ali did not agree to a project linking the two seas because he did not want to cut out the duties he collected from overland trade in Egypt, but did allow Enfantin's group to work on the Delta Barrage - a type of dam - north of Cairo - with unpaid laborers - that would act to limit Nile flooding and create predictable crop yields. During his time in Egypt, Enfantine also established technical schools based on the Ecole Polytechnique model with Ali's blessing. Also in Egypt he encountered and influenced Ferdinand de Lesseps. Enfantine returned to France in 1836, lacking the patience to finish the Nile barrage project that was encountering delays.\n\nOn his return to France, he occupied minor offices. He became first a postmaster near Lyon, and in 1841 was appointed, through the influence of some of his friends who had risen to posts of power, member of a scientific commission on Algeria, which led him to engage in researches concerning North Africa and colonization in general.\n\nIn 1845 he was appointed a director of the Paris & Lyons railway. Three years later he established, in conjunction with Duveyrier, a daily journal, entitled \"Le Credit\", which was discontinued in 1850. He was afterwards attached to the administration of the railway from Lyons to the Mediterranean. \n\nFather Enfantin held fast by his ideal to the end, but he had renounced the hope of giving it a local habitation and a name in the degenerate obstinate world. His personal influence over those who associated with him was immense. \"He was a man of a noble presence, with finely formed and expressive features. He was gentle and insinuating in manner, and possessed a calm, graceful and winning delivery\" (\"Gent. M .\" Jan. 1865). His evident sincerity, his genuine enthusiasm, gave him his marvellous ascendancy. The Société d'Études du Canal de Suez was established by Enfantin in 1846 to continue study of the Suez Canal. Its members included Arlès-Dufour, Jules, Lon and Paulin Talabot, the British Robert Stephenson and Edward Starbuck, the Austrian Alois Negrelli, inspector of the Emperor Ferdinand Northern Railway, and Feronce and Sellier of Leipzig as representatives of the German interest.\n\nThe Société sent surveying teams to Egypt, developed engineering plans, determined that the elevation difference between the Red Sea and the Mediterranean Sea was negligible, but Muhammad Ali was still reticent to the idea of a canal. Upon his death in 1848, the activities of the Société were minimized until his successor was assassinated in 1854 and Ferdinand de Lesseps took up the initiative to build a canal. Lessups had corresponded at least once with the Société in the intervening years and had known the new Egyptian viceroy, Sa'id, when he was a young man. Enfantin was listed by Lessups as a founder of the Suez Canal Company.\n\nNot a few of his disciples ranked afterwards amongst the most distinguished men of France. Enfantin died suddenly at Paris on September 1, 1864.\n\nAmongst his works are: \"Doctrine de Saint-Simon\" (written in conjunction with several of his followers), published in 1830, and several times republished; \"Economie politique et politique Saint-Simonienne\" (1831); \"\" (1835–1840); \"Corresp. philos. et religieuse\" (1843–1845); and \"La Vie eternelle passee, presente, future\" (1861). A large number of articles by his hand appeared in \"Le Producteur\", \"L'Organisateur\", \"Le Globe\", and other periodicals. He also wrote in 1832 \"Le Livre nouveau\", intended as a substitute for the Christian Scriptures, but it was not published.\n\n\nAttribution:\n"}
{"id": "6919584", "url": "https://en.wikipedia.org/wiki?curid=6919584", "title": "Bube language", "text": "Bube language\n\nBube, Bohobé or Bube–Benga (Bobe, Bubi), is a Bantu or Bantoid language spoken by the Bubi, a Bantu people native to, and once the primary inhabitants of, Bioko Island in Equatorial Guinea. The language was brought to Bioko from continental Africa more than three thousand years ago when the Bubi began arriving on the island.\n\nIt has around 50,000 speakers, with three variants: North, South and Central-East. It is noted for its tonal character and the divergence of words by gender. The language is also spoken by Bubi native to Gabon and Cameroon.\n\nThe Bube language is divided into six different dialects that vary in the northern and southern regions of Bioko Island. For example, in the North, people speak \"Rebola\" and its variations: \"Basile\", \"Banapa\" and \"Basupa\". However, in the North-East, \"Bakake\" is spoken.\n\nBube is also spoken in a small area on the mainland closest to the island, where speakers are shifting to Wumboko. This has been reported as \"Bube\", \"Bubia\" or \"Wovea\" (see Wovea people).\n\nThe first Bube-to-English primer was authored in 1875 by William Barleycorn, a colonial era Primitive Methodist missionary of Igbo and Fernandino descent, while he was serving in the Bubi village of Basupu. An official language dictionary and grammar guide was published by the ethnic Bubi scholar Justo Bolekia Boleká.\n\nOther names and forms of the name include Bubé, eVoové, eBubée, Bhubhi, Bubi, Ibubi, Ibhubhi, Pove and Eviia.\n\nBube has 7 vowels that can be either short or long:\nThe nasal vowels are allophones of respective oral vowels.\n\nBube has 20 consonants. Some of them are prenasalized:\n\n\n"}
{"id": "1482700", "url": "https://en.wikipedia.org/wiki?curid=1482700", "title": "Buryat language", "text": "Buryat language\n\nBuryat or Buriat (; Buryat Cyrillic: буряад хэлэн, \"buryaad xelen\") is a variety of the Mongolic languages spoken by the Buryats that is classified either as a language or major dialect group of Mongolian.\n\nThe majority of Buryat speakers live in Russia along the northern border of Mongolia where it is an official language in the Buryat Republic, Ust-Orda Buryatia and Aga Buryatia. In the Russian census of 2002, 353,113 people out of an ethnic population of 445,175 reported speaking Buryat (72.3%). Some other 15,694 can also speak Buryat, mostly ethnic Russians. There are at least 100,000 ethnic Buryats in Mongolia and the People's Republic of China as well. Buryats in Russia have a separate literary standard, written in a Cyrillic alphabet. It is based on the Russian alphabet with three additional letters: Ү/ү, Ө/ө and Һ/һ.\n\nThe delimitation of Buryat mostly concerns its relationship to its immediate neighbors, Mongolian proper and Khamnigan. While Khamnigan is sometimes regarded as a dialect of Buryat, this is not supported by isoglosses. The same holds for Tsongol and Sartul dialects, which rather group with Khalkha Mongolian to which they historically belong. Buryat dialects are:\n\n\nBased on loan vocabulary, a division might be drawn between Russia Buriat, Mongolia Buriat and China Buriat. However, as the influence of Russian is much stronger in the dialects traditionally spoken west of Lake Baikal, a division might rather be drawn between the Khori and Bargut group on the one hand and the other three groups on the other hand.\n\nBuryat has the vowel phonemes /i, ɯ, e, a, u, ʊ, o, ɔ/ (plus a few diphthongs), short /e/ being realized as [ɯ], and the consonant phonemes /b, g, d, tʰ, m, n, x, l, r/ (each with a corresponding palatalized phoneme) and /s, ʃ, z, ʒ, h, j/. These vowels are restricted in their occurrence according to vowel harmony. The basic syllable structure is (C)V(C) in careful articulation, but word-final CC clusters may occur in more rapid speech if short vowels of non-initial syllables get dropped.\n\n[ɯ] only occurs as a sound of a short \"e\". [ə] is only an allophone of unstressed vowels.\n\n[ŋ] only occurs as an allophone of /n/.\n\nLexical stress (word accent) falls on the last heavy nonfinal syllable when one exists. Otherwise, it falls on the word-final heavy syllable when one exists. If there are no heavy syllables, then the initial syllable is stressed. Heavy syllables without primary stress receive secondary stress:\n\nSecondary stress may also occur on word-initial light syllables without primary stress, but further research is required. The stress pattern is the same as in Khalkha Mongolian.\n\nBuryat (Latin) alphabet between 1931 and 1939\nBuryat alphabet (Cyrillic)\nBuryat is an SOV language that makes exclusive use of postpositions. Buryat is equipped with eight grammatical cases:\nnominative, accusative, genitive, instrumental, ablative, comitative, dative-locative and a particular oblique form of the stem.\n\n\n\n\n"}
{"id": "2221083", "url": "https://en.wikipedia.org/wiki?curid=2221083", "title": "Castlemorton Common Festival", "text": "Castlemorton Common Festival\n\nThe Castlemorton Common Festival was a week-long free festival and rave held in the Malvern Hills near Malvern, Worcestershire, England between 22nd–29th May 1992. The media interest and controversy surrounding the festival, and concerns as to the way it was policed, inspired the legislation that would eventually became the Criminal Justice and Public Order Act 1994.\n\nIn May 1992 Avon and Somerset Police tried to end the annual Avon Free Festival, which had been held in the Bristol area around the May Bank Holiday for several years. As a result, hundreds of new age travellers en-route to the area for the expected festival were shunted into neighbouring counties by Avon and Somerset’s Operation Nomad police manoeuvres, with West Mercia Police deciding to confine them to common land at Castlemorton.\n\nThe high-profile coverage in the national media only served to swell the crowd further as ravers from far afield made their way to join the festival, thus making it an impossible task for the authorities to close the event down. An estimated 20,000-40,000 people gathered on Castlemorton Common for the party which lasted a full week, the biggest of its kind since the Stonehenge Free Festival in the mid-1980s. \n\nCastlemorton hosted many of the large sound systems of the time such as Bedlam, Circus Warp, Spiral Tribe and DiY Sound System, and bands such as Back To The Planet, Xenophobia (fronted by Spiral Tribe's MC Skallywag), AOS3 and Poisoned Electrick Head. \n\nSimon Reynolds wrote that \"during the next five days of its existence, Castlemorton will inspire questions in Parliament, make the front page of every newspaper in England and incite nationwide panic about the whereabouts of the next destination on the crusty itinerary.\" \n\nThe then local MP Michael Spicer, speaking in a House of Commons debate remarked: \"The invasion that took place at Castlemorton common in my constituency, on Friday 22 May, has prompted me to do so again. On that day, new age travellers, ravers and drugs racketeers arrived at a strength of two motorised army divisions, complete with several massed bands and, above all, a highly sophisticated command and signals system. However, they failed to bring latrines. The numbers, speed and efficiency with which they arrived--amounting at one time to as many as 30,000 people--combined to terrorise the local community to the extent that some residents had to undergo psychiatric treatment in the days that followed. Such an incident must never happen again, in my constituency or elsewhere. We need tighter laws, especially to give banning powers to the police ; a Cabinet Committee to bring responsible Departments together ; quicker and more co-ordinated police action ; and a more effective application of existing policies by national and local authorities\".\nThirteen members of Spiral Tribe were arrested after the event, and charged with public order offences. After a lengthy and costly trial, they were acquitted.\n\nConcerns about the festival and the way in which it was policed inspired the legislation which developed into the Criminal Justice and Public Order Act 1994. \nThis wide-ranging Act effectively made illegal such outdoor parties that played music, which was defined in section 63(1)(b) to include \"sounds wholly or predominantly characterised by the emission of a succession of repetitive beats.\"\n\nWhilst some have argued that Castlemorton, with its attendant publicity, led directly to the Criminal Justice Act and was the \"final nail in the coffin of the unlicensed event\", others have seen the Act as a draconian piece of legislation which was \"explicitly aimed at suppressing the activities of certain strands of alternative culture\".\n\n\n"}
{"id": "45079763", "url": "https://en.wikipedia.org/wiki?curid=45079763", "title": "Casuari", "text": "Casuari\n\nThe Casuari were an ancient Germanic people. Ptolemy mentions them as living on the southern border of Germany, east of the Abnoba mountains, that are east of the Rhine. They were therefore neighbours of the Tencteri, a tribe living between the Rhine and the Abnoba mountains. Their origins can be traced back to those of the Alemanni and Khatti, they descend from Assyrian tribes who migrated into Europe to settle. Ptolemy also mentions them as having founded the town of Suevos Casuari. The Casuari were most likely numerous during and around the time of Ptolemy, which is around 90 to 168 AD. Being a small tribe, very little remains of them, and most evidence comes from written sources.\n\nThe Casuari spoke a South Germanic dialect, related to the High, or Upper Germanic languages. The location of their tribe was in fact a crossroad between the Weser-Rhine Germanic and Elbe Germanic languages. If so, their language originated from the Rhine, Alps, Elba or the North Sea from Friesland to Jutland.\n\nBy the time of the fall of the Western Roman Empire, the Casuari had most probably been conquered by the Romans, or had either become part of the Alemanni or another major Germanic tribe on the Rhine river.\n\n"}
{"id": "48231759", "url": "https://en.wikipedia.org/wiki?curid=48231759", "title": "Census in Malaysia", "text": "Census in Malaysia\n\nThe census in Malaysia, or officially, the Population and Housing Census, is a descriptive count of everyone who is in Malaysia on the Census Day, and of their dwellings. The decennial Malaysian census has been conducted six times, As of 2010. It has been conducted every 10 years, beginning in 1960.\n\nThe decennial publication provides users with the final Malaysian Census population figures for basic demographic characteristics such as gender, age distribution, ethnic group, citizenship, religion and marital status at the state and the administrative district levels. Some of the tables also provide a breakdown by urban and rural areas.\n\nMalaysia consists of thirteen states and three federal territories. Each state is divided into several administrative districts. In Kelantan, the administrative district is known as \"Jajahan\". Each state is also stratified into urban and rural areas. Urban and rural areas are not legally defined administrative areas but are statistically defined to distinguish areas with certain socio-economic characteristics.\n\nThe data in the decennial publication represent final Malaysian Census population figures for respective decades. As in censuses in most other countries, the information obtained during enumeration is subject to coverage and content errors. In term of coverage errors, part of living quarters, households or population maybe left out, erronerously included or duplicated. Content errors in particular were based on erronerous responses on gender, age, ethnic group, citizenship, religion and marital status. As a result, the figures were \"adjusted\" based on the estimates of under-enumeration derived from the Census Coverage Evaluation Survey.\n\nAs of Census 2010, a multi-modal data collection data method was used as follows:\n\nTwo types of census questionnaires were used. The first questionnaire for persons living in private living quarters, whilst the second questionnaire was for persons living in collective living quarters such as college or university hostels, charitable or social welfare institutions, prisons and homeless persons.\n\nAs of Census 2010, it covered all persons including non-citizens who had stayed or intended to stay in Malaysia for more than six months in the year 2010. This includes:\n\nThe following categories were excluded from the census count on the basis that they were staying in the country for less than six months in the year 2010:\n\nUrban areas adopted by the previous censuses refer to gazetted local council areas with a specified population threshold. The urban areas in Census 1980 refers to a gazetted area with population of 10,000 persons or more.\n\nUrban areas in Census 1991 and 2000, were defined as gazetted areas with their adjoining built-up areas which had a combined population of 10,000 or more. Built-up areas were defined as areas contiguous to a gazetted area and had at least 60 per cent of their population (aged 10 and above) engaged in non-agricultural activities as well as having modern toilet facilities in their housing units. The definition of urban areas for Census 2000 also takes into account the special development area namely development area which is not gazetted and can be identified and separated from the gazetted area or built-up area of more than 5 km and a population of at least 10,000 with 60 per cent of their population (aged 10 and above) engaged in non-agricultural activities as well as having modern toilet facilities in their housing units.\n\nMeanwhile, urban areas in Census 2010, were defined as:\nUrbanisation is a dynamic process and keeps changing with development and growth. Thus, the urban areas for 1980, 1991, 2000 and 2010 Censuses do not necessarily refer to the same areas, as areas fulfilling the above criteria of urban continue to expand and grow.\n\nClassification of 2010 Census ethnic group is as set by Inter-Agency Technical Committee (IATC) in Appendix 1. IATC is a committee formed to co-ordinate and monitor the implementation and use of standardised codes, classifications and definitions used by the Department of Statistics, Malaysia and other government agencies. For the purpose of tabulation and analysis, as well as taking into account the diverse ethnic group in Peninsular Malaysia, Sabah, Federal Territory of Labuan and Sarawak, major ethnic groups according to region as follows:\n\nInformation collected in the census including ethnic group and citizenship was based on respondent's answer and did not refer to any official document.\n\nInformation on citizenship should be used with caution as it is subject to content and coverage errors especially for non-citizens as in censuses in most countries.\n\nThe average annual growth rate was calculated as:\n\nThe density of persons per square kilometre is the ratio of the population of a given geographic area to the number of square kilometres in the same area.\n\nThe median age is the age that divides the distribution of the population into two, such that half the population is below the age and half is above it.\n\nDependency ratio refers to the ratio of the number of persons below the age of 15 years and the number of persons aged 65 years and over to the number of persons aged 15–64 years; multiplied by 100.\n\nThe sex ratio is the number of males per 100 females.\n\n\nThe mean age at first marriage refers to the singulate mean age at marriage which is an indicate measure of the mean age at first marriage among those who would ever marry. It is derived from the proportions never married at different age groups.\n\nRefers to the identification or relationship of an individual to a system of beliefs and the practice of these beliefs. For the purpose of tabulation and analysis, the classification for religion is as follows:\n\nAfter the year 2000, there were several new areas created or boundary changes that were gazetted by the relevant authority for federal territory and administrative districts. The new federal territory created was Putrajaya. Meanwhile, the new administrative districts such as Ledang and Kulaijaya in Johor; Pokok Sena in Kedah; Kampar in Perak; Putatan in Sabah; and Pakan and Selangau in Sarawak.\n\n"}
{"id": "48459988", "url": "https://en.wikipedia.org/wiki?curid=48459988", "title": "Centre for Social Investigation", "text": "Centre for Social Investigation\n\nThe Centre for Social Investigation (CSI) is an interdisciplinary research group based at Nuffield College, Oxford University, in England.\n\nThe CSI is led by Professor Anthony Heath, CBE, FBA, Emeritus Professor of Sociology at Oxford University and Professor of Sociology at The University of Manchester. It was launched officially in March 2015. The Centre aims to address contemporary social issues of public interest, carrying out authoritative, research on central social issues which draws upon interdisciplinary expertise in economics, politics and sociology and related disciplines such as social policy. A particular focus is on examining social progress in Britain and to this aim, the Centre has undertaken research covering topics including crime, education, social capital, life expectancy, corruption, food insecurity and Beveridge’s ‘five giants’. The Centre also undertakes research on inequalities, including those relating to gender, ethnicity and class. The Centre has a strong emphasis on interdisciplinary work and rigorous methods, ranging from quantitative research to field experiments. It has undertaken collaborative work with the UK Department for Communities and Local Government.\n\nCSI's research is independent and non-partisan; as such, it has no political affiliation or leaning. The Centre is counselled by an advisory board whose members work in research, policy and the private sector. Unusually for an academic research group, the Centre has a strong emphasis on engaging and disseminating its research beyond traditional academic outputs to communicate the results of its activities in an accessible way to non-technical audiences, including policy-makers and the public more generally. The Centre maintains a blog with posts from both its researchers and from guest contributors.\n\n\n"}
{"id": "1562036", "url": "https://en.wikipedia.org/wiki?curid=1562036", "title": "Charging the mound", "text": "Charging the mound\n\nIn baseball, charging the mound is an assault by a batter against the pitcher, usually the result of being hit by a pitch or nearly being hit by a pitch, such as a brushback. The first incidence of a professional charging of the mound has not been identified, but the practice dates back to the game's early days. Charging the mound is the most common initiator of a bench-clearing brawl.\n\nBefore charging, the batter usually throws his bat and helmet aside so that he may face the pitcher unarmed (it is a very serious breach of baseball etiquette, not to mention dangerous, for the batter to charge the mound with a bat). Though serious injuries have occurred from charging in the past, usually fights are either broken up or joined by all other players so the conflict turns into posturing and name-calling; in baseball parlance, this is known as a rhubarb.\n\nCharging the mound is typically about responding to an indignity rather than an attempt to injure the pitcher. There is long-standing etiquette in baseball regarding what is an acceptable offense to warrant a beaning, and there are similar \"unwritten rules\" for charging in response to being hit. While these unwritten rules have become more vague, the response of Major League Baseball to the incidents has become far more strict. Whereas suspensions in the past were rare and usually short, Commissioner Fay Vincent and his successor Bud Selig reacted harshly to both instances of beaning and charging during their respective tenures. Recently, most incidents which have caused the benches to clear have been met with large fines and lengthy suspensions.\n\nIn Japan, pitchers tip their cap to a batter hit by a pitch if it was not their intent to hit the batter to avoid a mound charging incident.\n"}
{"id": "3800972", "url": "https://en.wikipedia.org/wiki?curid=3800972", "title": "Cricket fighting", "text": "Cricket fighting\n\nCricket fighting is a blood sport involving the fighting of male crickets. Unlike most blood sports such as bullfighting and cockfighting, cricket fighting rarely causes injuries to the animals. It is a popular pastime in China and dates back more than 1,000 years to the Tang Dynasty. However, the sport has been losing its popularity in China.\n\nCricket fighting was nurtured by Tang Dynasty emperors more than 1,000 years ago, and later popularized by commoners. In the thirteenth century, the Southern Song Dynasty prime minister Jia Sidao wrote a how-to guide for the blood sport. Jia's obsession with cricket fighting is believed to have contributed to the fall of the empire. During the Cultural Revolution (1966-1976) China's Communist government banned cricket fighting as a bourgeois predilection, but it is now undergoing a revival among a younger generation eager to embrace genuinely Chinese pastimes.\n\nMany famous hotels around Macau hold cricket fights where bets up to thousands of patacas would be waged on a single fight. Prized crickets become famous and actual funeral services would be held for them.\n\nCricket season begins in summer and championships take place after the autumn equinox in late September. In Beijing, the Association for Cricket Fighting organizes cricket fighting events and championships.\n\nWhile it is illegal in China to gamble on cricket fights, the fights themselves are legal and occur in most big cities in China. Crickets are sold openly in street markets, with more than a dozen cricket markets in Shanghai alone. In 2010 more than 400 million yuan (US$63 million) were spent in China on crickets.\n\nThe best crickets are from a few counties in northeastern Shandong Province. Crickets have pedigrees and would be carefully bred by knowledgeable keepers. Each cricket must be kept in its own clay pot and their diets include ground shrimp, red beans, goat liver, and maggots. Before fight night, female crickets are dropped in the pot to increase the male's fighting spirit.\n\nCrickets fights are arranged according to weight class. In a fighting container, handlers stimulate their cricket's antennae using a straw stick, causing the crickets to become aggressive. When both crickets are sufficiently agitated, a divider separating the pair will be lifted, and the two crickets will begin the match. The loser is the cricket that first begins avoiding contact, runs away from battle, stops chirping, or is thrown from the fighting container.\n\nStudies done indicate that the sense of \"flying\" encourages a cricket's fighting spirit. In one such study, a losing cricket put back into the ring will only go back to fight one out of ten times. If crickets are shaken and thrown in the air repeatedly, they will fight again six out of ten times.\n\nThe National Cricket Fighting Championships are a two-day event held annually in Beijing, following regional competitions at 25 locations around China. At the national event, each contestant is allowed 35 insects, each weighed and labelled before the event.\n\nThe Yu Sheng Cup, another national cricket fighting tournament, is held annually outside the Xilai Ranch in Lühua during the National Day holiday in early October, with a purse of .\n\n\n"}
{"id": "32581058", "url": "https://en.wikipedia.org/wiki?curid=32581058", "title": "Daingnet people", "text": "Daingnet people\n\nThe Daingnet people (), also known as the Thetkama people () are an ethnic group indigenous to northern Rakhine State, Myanmar. According to their own internal census in 1995 they numbered about 60,000. In 2011, the number is estimated to be around 80,000. From appearance they are indistinguishable from the Rakhine people; however, the Daingnet people have a distinct language and culture. Ethnically, they are closely related to the Chakma people of Bangladesh and Northeast India. The languages of the Daingnet and Chakma people are mutually intelligible. Daingnet people are one of 135 ethnic groups officially recognized by the Myanmar government as indigenous to Myanmar. Daingnets are one of the Tibeto-Burman tribes. Genetically they are closely related to the Tibetans, Burmans and Rakhines.\n\nDaingnets do not call themselves \"Daingnet\"; instead they call themselves \"Thaikhma\" like the nearby Chakma people of Bangladesh and Northeast India. They are believed to have originated from greater Arakan Yoma North, presently Chin State (previously Sak Buddhist homeland \"Chacomas\", occupied by Zo people from Lushai Hills). Daingnet is an exonym originally used by Rakhine people. In Rakhine language, Daingnet means \"shield warriors\" or \"armored warriors\". Centuries ago the Rakhine kings hired the Daingnets as soldiers and they showed their mastery with shield and sword. The Daingnets no longer fight with shield and sword, but the name Daingnet stuck\".\"\n\nDaingnets are indigenous to Rakhine State. They were among the first people who settled in northern Rakhine State. During British rule in Burma, the Daingnets were classified as Sak people. \"Sak\" is a generic term used by the Bamar and Rakhine peoples to denote the Chakma people.\n\nIn the spring of 1798, British explorer Francis Buchanan visited Chittagong Hill Tracts, he asked a Chakma man if they were the same as \"Sak\" people in Rakhine State. The man replied, the \"Saks\" of Rakhine State were \"Moishang Saks\". In the Rakhine language \"Moishang\" means primary or superior. What the man meant was, the Saks of Rakhine State retained the original Chakma language. Though Francis Buchanan did not visit Rakhine State he was aware of the \"Sak\" people from his earlier visit to Cox's Bazar. He might have heard about them from the East India Company officials or the Rakhine refugees who were pouring into Cox's Bazar to flee Burma-Rakhine conflict.\n\nAccording to Burmese historian Gordon Luce, the \"Saks\" (i.e. ancestors of the Daingnets) attained higher cultural level than any other minority peoples in Arakan. They were the smelters of iron, the distillers of spirits, the makers of earthen vessels, manufacturers of salt, builders of boats etc.\n\nThe Daingnet people are found in and around Maungdaw, Buthidaung, Kyauktaw, Paletwa and Mrauk-U. Many Daingnets also live in the Arakan Yoma mountains, close to the Chin state and Chittagong Hill Tracts.\n\nThe Daingnets are culturally similar to the Chakmas and Rakhines in Bangladesh and Northeast India. Rice, fish and vegetables are their favorite foods. They prefer hot food. Daingnet people consider cow as a sacred animal and as such they do not eat beef. Men sport \"lungi\" and women sport sarong (Pinuin) like dress \"thami\". Usually men do the agricultural work and women look after children and cook for the family. They usually like to settle near rivers. Some of them also live in mountains whose livelihood is slash and burn cultivation. Daingnet people usually marry early, but never before the age of 17.\n\nTheir original language was Tibeto-Burman. Due to prolonged interaction with the Assamese language, their language gradually became a mix of Indo-Aryan and Tibeto-Burman. Majority of the Daingnets are multi-lingual. Apart from their own Chakma language, they can speak Rakhine, Burmese and in Chittagonian (a dialect of Bengali).\n\nDaingnets are followers of Theravada Buddhism and for centuries the Daingnets have strictly adhered to its teachings. Almost every village has a Buddhist temple. A Daingnet male at least once in his lifetime becomes a Buddhist monk. However most of them do not remain Buddhist monks for life. Wedding and funeral ceremonies are performed by Buddhist monks. Besides wedding and funeral, the Daingnet people flock to Buddhist temples on major Buddhist festivals which usually fall on full moon days.\n\nMajority of the Daingnets are farmers. Some of them are traders. Their economy is highly dependent on agriculture and hence vulnerable to weather pattern. However, due to lack of economic development there is widespread poverty among the Daingnets. Religious conviction of Buddhism forbids them to work as fishermen or butchers. There are 2 kinds of farmers among the Daingnets, those who farm land on permanent basis and those who farm hills and mountains by slash and burn or shifting cultivation technique.\n"}
{"id": "36934223", "url": "https://en.wikipedia.org/wiki?curid=36934223", "title": "Death of Corryn Rayney", "text": "Death of Corryn Rayney\n\nCorryn Veronica Ann Rayney, née Da Silva, (born 1963) migrated to Australia with her Indian family in 1973 as refugees from Idi Amin's Uganda. Her death occurred on or about 7 August 2007, her body being discovered a week later in a clandestine grave in Kings Park, Perth, with no clearly established cause of death. Her husband Lloyd Rayney, a prominent barrister specialising in criminal prosecution, was charged with her murder but found not guilty after a trial before a judge only. The acquittal was unanimously upheld by a court of appeal in August 2013. The state's police commissioner and attorney general declined to acknowledge documented procedural mistakes, and refused to instigate a fresh search for the killers, leading to calls for a federal investigation into the matter.\n\nThe Rayneys had two daughters and lived in the Perth suburb of Como. At the time, Lloyd Rayney was involved in a Corruption and Crime Commission inquiry into the misconduct of police officers in a murder investigation. Corryn Rayney was employed as a registrar at the Supreme Court of Western Australia. She was last seen alive at 9.30 p.m. on 7 August 2007 at a bootscooting class. A week later, police discovered her abandoned car and followed a trail of oil from it to the grave in Kings Park.\n\nAlthough Lloyd Rayney was controversially described by the chief police investigator Senior Sergeant Jack Lee as the \"only suspect\" at a police press conference in September 2007, he was not charged with the murder until December 2010, more than three years after the event.\n\nAn affidavit filed by the prosecutors indicated that the case was circumstantial. The prosecutor's opening address to the trial said that the state's case was circumstantial but the evidence of motive was compelling.\n\nAt the request of the defendant in October 2011, the trial was heard by a judge only, with no jury. The reasons for the application were suppressed at the time, but later published in March 2012. Lloyd Rayney wanted a trial without a jury, because he claimed that the extensive publicity would make a fair jury trial impossible to achieve. The decision to conduct the trial without a jury was a subject of debate in Australian legal circles.\n\nFormer Northern Territory Chief Justice Brian Ross Martin was appointed as an Acting Justice of the Western Australia Supreme Court in February 2012 to preside over the trial; a judge from outside Western Australia was used to ensure impartiality, given that both the victim and accused had held senior legal positions.\n\nThe trial began on 16 July 2012, and ran until 19 October 2012 when final submissions were presented by prosecutor John Agius SC, QC and defence counsel David Edwardson QC. On 1 November, Justice Martin acquitted Lloyd Rayney when he handed down a judgment of \"not guilty\", saying that the \"case by the State is beset by improbabilities and uncertainties\". The full reasons for the verdict were published, a requirement which would not have applied to a jury verdict.\n\nThe trial judge closely examined evidence on the conduct of Lloyd Rayney and reportedly described him as a barrister who had engaged in \"disreputable conduct\" in comments which were redacted from the final official judgment (para 1594), and Justice Martin noted that \"The accused has engaged in discreditable conduct including knowingly arranging for illegal telephone interception, making a false declaration and giving deliberately false evidence to a court while on oath. The evidence raises suspicion; in some instances quite strong suspicion. But discreditable conduct does not prove guilt, and suspicion, even strong suspicion, falls well short of proof beyond reasonable doubt.\" The Judgment Summary further explained: \"Evidence concerning such conduct was not admitted to show that the accused is a person of bad character. The fact that the accused engaged in discreditable conduct and could, therefore, be viewed as a person of bad character, cannot be used to reason that the accused is the type of person who might kill his wife, or that by reason of his bad character he is likely to have killed her. Such reasoning would be unfair and is prohibited.\"\n\nAn appeal by prosecutors against the verdict was held in the Supreme Court of Western Australia in August 2013, before three judges brought in from other states. The prosecution, if granted leave to appeal, proposed to rely on the grounds that the trial judge's findings in relation to motive were not supported by the facts his Honour found in relation to the respective attitudes of the respondent and the deceased. The judges unanimously dismissed the appeal, and upheld the trial judge's verdict. The office of the NSW Director of Public Prosecutions, which ran the case, announced on 18 October 2013 that it would not be seeking a further appeal to the High Court.\nMr Rayney indicated interest in whether there would be consequences for \"police who the trial judge said gave misleading evidence, pressured a forensic pathologist to change his report, abused their position of authority and behaved reprehensibly\".\n\nAn investigation by the Western Australian Corruption and Crime Commission later cleared two police officers of \"any serious misconduct\" after their behaviour in threatening a female lawyer had been described as ranging from \"inappropriate to reprehensible\" by the trial judge. A second matter reviewed by the CCC related to \"attempts by a third officer to encourage an independent pathologist involved in the case to change a report to better fit police evidence. That officer was found to have acted unreasonably.\"\n\nIn May 2015, Rayney was acquitted of two charges of tapping his own residential telephone line. He is pursuing a multimillion-dollar defamation action against the state over his having been publicly named as \"the prime and only suspect\".\n\nIn July 2015, Rayney's licence was cancelled by the Legal Practice Board, stripping him of his right to practise law in WA. This action resulted from a finding by a magistrate that he had breached the Surveillance Devices Act by taping conversations with Corryn Rayney, then deliberately disposing of the recording devices when he knew the police had a warrant to search for them. In February 2016, Rayney was cleared by the State Administrative Tribunal of the alleged \"unlawful destruction of evidence and/or an attempt to pervert the course of justice\" and his right to practise was reinstated. He recommenced his career with the successful defence of an accused drug dealer. The trial was aborted when police evidence was shown to have been unlawfully \"destroyed, missing and contradictory\".\n\nIn his judgment summary, Justice Martin noted that police and forensic evidence was consistent with a sexual attack on Mrs Rayney, while defence lawyers identified at least two possible criminals who \"should have been investigated with as much rigour as Mr Rayney\". In a prime-time TV presentation on 21 August 2014 that was dismissed by Attorney General Michael Mischin as \"infotainment that is one-sided\", Rayney, backed by senior lawyers and a forensic scientist, spoke out for the first time and called for independent investigators to re-examine the unsolved case. State Premier Colin Barnett did not see any need for a review but would await advice from his Attorney General.\n\nIn May 2015, WA Police Commissioner Karl O'Callaghan ordered a cold case review of the matter which had not resulted in any new charges by \nMarch 2017, when a court began hearing Rayney's defamation suit against the state. Judgment was handed down on 15 December 2017 in favour of Rayney, with final damages to be assessed.\n\n\n\n"}
{"id": "27311", "url": "https://en.wikipedia.org/wiki?curid=27311", "title": "Demographics of Sierra Leone", "text": "Demographics of Sierra Leone\n\nThe demographics of Sierra Leone are made up of an indigenous population from 18 ethnic groups. The Temne in the north and the Mende in the South are the largest. About 60,000 are Krio, the descendants of freed slaves who returned to Sierra Leone from Great Britain, North America and slave ships captured on the high seas. In addition, about 5,000 Lebanese, 1,000 Indians, and 5,000 Europeans reside in the country.\n\nIn the past, some Sierra Leoneans were noted for their educational achievements, trading activity, entrepreneurial skills, and arts and crafts work, particularly woodcarving. Many are part of larger ethnic networks extending into several countries, which link West African states in the area. Their level of education and infrastructure have declined sharply over the last 30 years.\n\nAccording to the total population was in , compared to only 1 895 000 in 1950. The proportion of children below the age of 15 in 2010 was 43%, 55.1% was between 15 and 65 years of age, while 1.9% was 65 years or older\n\nRegistration of vital events is in Sierra Leone not complete. The Population Departement of the United Nations prepared the following estimates.\n\nTotal Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):\n\nFertility data as of 2013 (DHS Program):\n\nThe following demographic statistics are from the CIA World Factbook, unless otherwise indicated.\n\nRefugees currently in surrounding countries are slowly returning.\n\nSierra Leone's MMR is the worst of any country in the world, according to the 2000 WHO, UNICEF and UNFPA report.\n\n\"Definition:\"\nAge 15 and over can read and write English, Mende, Temne, or Arabic\n"}
{"id": "52964286", "url": "https://en.wikipedia.org/wiki?curid=52964286", "title": "Ethnic cleansing in Bhutan", "text": "Ethnic cleansing in Bhutan\n\nEthnic cleansing in Bhutan refers to a series of initiatives to remove Lhotshampa, or ethnic Nepalis, from Bhutan.\n\nInter-ethnic tensions escalated in Bhutan, resulting in the flight of many Lhotshampa, or ethnic Nepalis, from Bhutan to Nepal, many of whom were expelled by the Bhutanese military. By 1996, over 100,000 Bhutanese refugees were living in refugee camps in Nepal. Many have since been resettled in Western nations.\n\n"}
{"id": "2328664", "url": "https://en.wikipedia.org/wiki?curid=2328664", "title": "Freedom Day (Belarus)", "text": "Freedom Day (Belarus)\n\nFreedom Day () is an unofficial holiday in Belarus, which is celebrated on March 25 to commemorate the creation of the Belarusian Democratic Republic on that date in 1918.\n\nThe day has been celebrated as the Independence Day of Belarus by the Belarusian independence movement since the early 1920s. The day has been widely celebrated by the Belarusian diaspora.\n\nIn the United States, governors and US presidents have traditionally issued official greetings to the Belarusian American community on the occasion of March 25.\n\nIn today's Belarus, people and groups largely opposed to the current Belarusian government under president Alexander Lukashenko celebrate the holiday. The government does not recognize it for the stated reason that the Belarusian Democratic Republic was created by the Germans, who were occupying Belarus in 1918. Celebrations of the holiday are an annual occasion for anti-government protests.\n\n"}
{"id": "11632817", "url": "https://en.wikipedia.org/wiki?curid=11632817", "title": "Human impact on the nitrogen cycle", "text": "Human impact on the nitrogen cycle\n\nHuman impact on the nitrogen cycle is diverse. Agricultural and industrial nitrogen (N) inputs to the environment currently exceed inputs from natural N fixation. As a consequence of anthropogenic inputs, the global nitrogen cycle (Fig. 1) has been significantly altered over the past century. Global atmospheric nitrous oxide (NO) mole fractions have increased from a pre-industrial value of ~270 nmol/mol to ~319 nmol/mol in 2005. Human activities account for over one-third of NO emissions, most of which are due to the agricultural sector. This article is intended to give a brief review of the history of anthropogenic N inputs, and reported impacts of nitrogen inputs on selected terrestrial and aquatic ecosystems.\n\nApproximately 78% of earth's atmosphere is N gas (N), which is an inert compound and biologically unavailable to most organisms. In order to be utilized in most biological processes, N must be converted to reactive N (Nr), which includes inorganic reduced forms (NH and NH), inorganic oxidized forms (NO, NO, HNO, NO, and NO), and organic compounds (urea, amines, and proteins). N has a strong triple bond, and so a significant amount of energy (226 kcal mol) is required to convert N to Nr. Prior to industrial processes, the only sources of such energy were solar radiation and electrical discharges. Utilizing a large amount of metabolic energy and the enzyme nitrogenase, some bacteria and cyanobacteria convert atmospheric N to NH, a process known as biological nitrogen fixation (BNF). The anthropogenic analogue to BNF is the Haber-Bosch process, in which H is reacted with atmospheric N at high temperatures and pressures to produce NH. Lastly, N is converted to NO by energy from lightning, which is negligible in current temperate ecosystems, or by fossil fuel combustion.\n\nUntil 1850, natural BNF, cultivation-induced BNF (e.g., planting of leguminous crops), and incorporated organic matter were the only sources of N for agricultural production. Near the turn of the century, Nr from guano and sodium nitrate deposits was harvested and exported from the arid Pacific islands and South American deserts. By the late 1920s, early industrial processes, albeit inefficient, were commonly used to produce NH. Due to the efforts of Fritz Haber and Carl Bosch, the Haber-Bosch process became the largest source of nitrogenous fertilizer after the 1950s, and replaced BNF as the dominant source of NH production. From 1890 to 1990, anthropogenically created Nr increased almost ninefold. During this time, the human population more than tripled, partly due to increased food production.\n\nSince the industrial revolution, an additional source of anthropogenic N input has been fossil fuel combustion, which is used to generate energy (e.g., to power automobiles). During combustion of fossil fuels, high temperatures and pressures provide energy to produce NO from N oxidation. Additionally, when fossil fuel is extracted and burned, fossil N may become reactive (i.e., NO emissions). During the 1970s, scientists began to recognize that N inputs were accumulating in the environment and affecting ecosystems.\n\nBetween 1600 and 1990, global reactive nitrogen (Nr) creation had increased nearly 50%. During this period, atmospheric emissions of Nr species reportedly increased 250% and deposition to marine and terrestrial ecosystems increased over 200%. Additionally, there was a reported fourfold increase in riverine dissolved inorganic N fluxes to coasts. Nitrogen is a critical limiting nutrient in many systems, including forests, wetlands, and coastal and marine ecosystems; therefore, this change in emissions and distribution of Nr has resulted in substantial consequences for aquatic and terrestrial ecosystems.\n\nAtmospheric N inputs mainly include oxides of N (NO), ammonia (NH), and nitrous oxide (NO) from aquatic and terrestrial ecosystems, and NO from fossil fuel and biomass combustion.\n\nIn agroecosystems, fertilizer application has increased microbial nitrification (aerobic process in which microorganisms oxidize ammonium [NH] to nitrate [NO]) and denitrification (anaerobic process in which microorganisms reduce NO to atmospheric nitrogen gas [N]). Both processes naturally leak nitric oxide (NO) and nitrous oxide (NO) to the atmosphere. Of particular concern is NO, which has an average atmospheric lifetime of 114–120 years, and is 300 times more effective than CO as a greenhouse gas. NO produced by industrial processes, automobiles and agricultural fertilization and NH emitted from soils (i.e., as an additional byproduct of nitrification) and livestock operations are transported to downwind ecosystems, influencing N cycling and nutrient losses. Six major effects of NO and NH emissions have been cited: 1) decreased atmospheric visibility due to ammonium aerosols (fine particulate matter [PM]); 2) elevated ozone concentrations; 3) ozone and PM affects human health (e.g. respiratory diseases, cancer); 4) increases in radiative forcing and \nglobal climate change; 5) decreased agricultural productivity due to ozone deposition; and 6) ecosystem acidification and eutrophication.\n\nTerrestrial and aquatic ecosystems receive Nr inputs from the atmosphere through wet and dry deposition. Atmospheric Nr species can be deposited to ecosystems in precipitation (e.g., NO, NH, organic N compounds), as gases (e.g., NH and gaseous nitric acid [HNO]), or as aerosols (e.g., ammonium nitrate [NHNO]). Aquatic ecosystems receive additional nitrogen from surface runoff and riverine inputs.\nIncreased N deposition can acidify soils, streams, and lakes and alter forest and grassland productivity. In grassland ecosystems, N inputs have produced initial increases in productivity followed by declines as critical thresholds are exceeded. Nitrogen effects on biodiversity, carbon cycling, and changes in species composition have also been demonstrated. In highly developed areas of near shore coastal ocean and estuarine systems, rivers deliver direct (e.g., surface runoff) and indirect (e.g., groundwater contamination) N inputs from agroecosystems. Increased N inputs can result in freshwater acidification and eutrophication of marine waters.\n\nMuch of terrestrial growth in temperate systems is limited by N; therefore, N inputs (i.e., through deposition and fertilization) can increase N availability, which temporarily increases N uptake, plant and microbial growth, and N accumulation in plant biomass and soil organic matter. Incorporation of greater amounts of N in organic matter decreases C:N ratios, increasing mineral N release (NH) during organic matter decomposition by heterotrophic microbes (i.e., ammonification). As ammonification increases, so does nitrification of the mineralized N. Because microbial nitrification and denitrification are \"leaky\", N deposition is expected to increase trace gas emissions. Additionally, with increasing NH accumulation in the soil, nitrification processes release hydrogen ions, which acidify the soil. NO, the product of nitrification, is highly mobile and can be leached from the soil, along with positively charged alkaline minerals such as calcium and magnesium. In acid soils, mobilized aluminium ions can reach toxic concentrations, negatively affecting both terrestrial and adjacent aquatic ecosystems.\n\nAnthropogenic sources of N generally reach upland forests through deposition. A potential concern of increased N deposition due to human activities is altered nutrient cycling in forest ecosystems. Numerous studies have demonstrated both positive and negative impacts of atmospheric N deposition on forest productivity and carbon storage. Added N is often rapidly immobilized by microbes, and the effect of the remaining available N depends on the plant community's capacity for N uptake. In systems with high uptake, N is assimilated into the plant biomass, leading to enhanced net primary productivity (NPP) and possibly increased carbon sequestration through greater photosynthetic capacity. However, ecosystem responses to N additions are contingent upon many site-specific factors including climate, land-use history, and amount of N additions. For example, in the Northeastern United States, hardwood stands receiving chronic N inputs have demonstrated greater capacity to retain N and increase annual net primary productivity (ANPP) than conifer stands. Once N input exceeds system demand, N may be lost via leaching and gas fluxes. When available N exceeds the ecosystem's (i.e., vegetation, soil, and microbes, etc.) uptake capacity, N saturation occurs and excess N is lost to surface waters, groundwater, and the atmosphere. N saturation can result in nutrient imbalances (e.g., loss of calcium due to nitrate leaching) and possible forest decline.\n\nA 15-year study of chronic N additions at the Harvard Forest Long Term Ecological Research (LTER) program has elucidated many impacts of increased nitrogen deposition on nutrient cycling in temperate forests. It found that chronic N additions resulted in greater leaching losses, increased pine mortality, and cessation of biomass accumulation. Another study reported that chronic N additions resulted in accumulation of non-photosynthetic N and subsequently reduced photosynthetic capacity, supposedly leading to severe carbon stress and mortality. These findings negate previous hypotheses that increased N inputs would increase NPP and carbon sequestration.\n\nMany plant communities have evolved under low nutrient conditions; therefore, increased N inputs can alter biotic and abiotic interactions, leading to changes in community composition. Several nutrient addition studies have shown that increased N inputs lead to dominance of fast-growing plant species, with associated declines in species richness. Fast growing species have a greater affinity for nitrogen uptake, and will crowd out slower growing plant species by blocking access to sunlight with their higher above ground biomass. Other studies have found that secondary responses of the system to N enrichment, including soil acidification and changes in mycorrhizal communities have allowed stress-tolerant species to out-compete sensitive species. Trees that have arbuscular mycorrhizal associations are more likely to benefit from an increase in soil nitrogen, as these fungi are unable to break down soil organic nitrogen. Two other studies found evidence that increased N availability has resulted in declines in species-diverse heathlands. Heathlands are characterized by N-poor soils, which exclude N-demanding grasses; however, with increasing N deposition and soil acidification, invading grasslands replace lowland heath.\n\nIn a more recent experimental study of N fertilization and disturbance (i.e., tillage) in old field succession, it was found that species richness decreased with increasing N, regardless of disturbance level. Competition experiments showed that competitive dominants excluded competitively inferior species between disturbance events. With increased N inputs, competition shifted from belowground to aboveground (i.e., to competition for light), and patch colonization rates significantly decreased. These internal changes can dramatically affect the community by shifting the balance of competition-colonization tradeoffs between species. In patch-based systems, regional coexistence can occur through tradeoffs in competitive and colonizing abilities given sufficiently high disturbance rates. That is, with inverse ranking of competitive and colonizing abilities, plants can coexist in space and time as disturbance removes superior competitors from patches, allowing for establishment of superior colonizers. However, as demonstrated by Wilson and Tilman, increased nutrient inputs can negate tradeoffs, resulting in competitive exclusion of these superior colonizers/poor competitors.\n\nAquatic ecosystems also exhibit varied responses to nitrogen enrichment. NO loading from N saturated, terrestrial ecosystems can lead to acidification of downstream freshwater systems and eutrophication of downstream marine systems. Freshwater acidification can cause aluminium toxicity and mortality of pH-sensitive fish species. Because marine systems are generally nitrogen-limited, excessive N inputs can result in water quality degradation due to toxic algal blooms, oxygen deficiency, habitat loss, decreases in biodiversity, and fishery losses.\n\nAtmospheric N deposition in terrestrial landscapes can be transformed through soil microbial processes to biologically available nitrogen, which can result in surface-water acidification, and loss of biodiversity. NO and NH inputs from terrestrial systems and the atmosphere can acidify freshwater systems when there is little buffering capacity due to soil acidification. N pollution in Europe, the Northeastern United States, and Asia is a current concern for freshwater acidification. Lake acidification studies in the Experimental Lake Area (ELA) in northwestern Ontario clearly demonstrated the negative effects of increased acidity on a native fish species: lake trout (Salvelinus namaycush) recruitment and growth dramatically decreased due to extirpation of its key prey species during acidification.\n\nUrbanization, deforestation, and agricultural activities largely contribute sediment and nutrient inputs to coastal waters via rivers. Increased nutrient inputs to marine systems have shown both short-term increases in productivity and fishery yields, and long-term detrimental effects of eutrophication. Tripling of NO loads in the Mississippi River in the last half of the 20th century have been correlated with increased fishery yields in waters surrounding the Mississippi delta; however, these nutrient inputs have produced seasonal hypoxia (oxygen concentrations less than 2–3 mg L, \"dead zones\") in the Gulf of Mexico. In estuarine and coastal systems, high nutrient inputs increase primary production (e.g., phytoplankton, sea grasses, macroalgae), which increase turbidity with resulting decreases in light penetration throughout the water column. Consequently, submerged vegetation growth declines, which reduces habitat complexity and oxygen production. The increased primary (i.e., phytoplankton, macroalgae, etc.) production leads to a flux of carbon to bottom waters when decaying organic matter (i.e., senescent primary production) sinks and is consumed by aerobic bacteria lower in the water column. As a result, oxygen consumption in bottom waters is greater than diffusion of oxygen from surface waters. Additionally, certain algal blooms termed harmful algal blooms (HABs) produce toxins that can act as neuromuscular or organ damaging compounds. These algal blooms can be harmful to other marine life as well as to humans.\n\nThe above system responses to reactive nitrogen (Nr) inputs are almost all exclusively studied separately; however, research increasingly indicates that nitrogen loading problems are linked by multiple pathways transporting nutrients across system boundaries. This sequential transfer between ecosystems is termed the nitrogen cascade. (see illustration from United Nations Environment Programme). During the cascade, some systems accumulate Nr, which results in a time lag in the cascade and enhanced effects of Nr on the environment in which it accumulates. Ultimately, anthropogenic inputs of Nr are either accumulated or denitrified; however, little progress has been made in determining the relative importance of Nr accumulation and denitrification, which has been mainly due to a lack of integration among scientific disciplines.\n\nMost Nr applied to global agroecosystems cascades through the atmosphere and aquatic and terrestrial ecosystems until it is converted to N, primarily through denitrification. Although terrestrial denitrification produces gaseous intermediates (nitric oxide [NO] and nitrous oxide [NO]), the last step—microbial production of N— is critical because atmospheric N is a sink for Nr. Many studies have clearly demonstrated that managed buffer strips and wetlands can remove significant amounts of nitrate (NO) from agricultural systems through denitrification. Such management may help attenuate the undesirable cascading effects and eliminate environmental Nr accumulation.\n\nHuman activities dominate the global and most regional N cycles. N inputs have shown negative consequences for both nutrient cycling and native species diversity in terrestrial and aquatic systems. In fact, due to long-term impacts on food webs, Nr inputs are widely considered the most critical pollution problem in marine systems. In both terrestrial and aquatic ecosystems, responses to N enrichment vary; however, a general re-occurring theme is the importance of thresholds (e.g., nitrogen saturation) in system nutrient retention capacity. In order to control the N cascade, there must be integration of scientific disciplines and further work on Nr storage and denitrification rates.\n\n\n"}
{"id": "1461548", "url": "https://en.wikipedia.org/wiki?curid=1461548", "title": "Human rights in Australia", "text": "Human rights in Australia\n\nHuman rights in Australia have largely been developed under Australian Parliamentary democracy through laws in specific contexts (rather than a stand-alone, abstract bill of rights) and safeguarded by such institutions as an independent judiciary and High Court which implement the Common Law, the Australian Constitution and various other laws of Australia and its states and territories. Australia also has an independent statutory human rights body, the Australian Human Rights Commission, which investigates and conciliates complaints, and more generally promotes human rights through education, discussion and reporting.\n\nUniversal voting rights and rights to freedom of speech, freedom of association, freedom of religion and freedom from discrimination are protected in Australia. The Australian colonies were among the first political entities in the world to grant male (1850s) and female suffrage (1890s). Contemporary Australia is a liberal democracy and heir to a large post-World War II multicultural program of immigration in which forms of racial discrimination have been prohibited.\n\nAs a founding member of the United Nations, Australia assisted in the drafting of the Universal Declaration of Human Rights and it is signatory to various other international treaties on the subject of human rights. Australia is the only democratic country in the world without a national bill of rights of some kind. Racism in Australia traces both historical and contemporary racist community attitudes, as well as political non-compliance and alleged governmental negligence on United Nations human rights standard and incidents in Australia.\n\nAn ongoing human rights issue in Australia is the legacy of mistreatment of indigenous Australians, who are disproportionately of disadvantaged socioeconomic standing, have shorter life spans, and make up a disproportionately high number of imprisoned persons, thus receiving disproportionately high levels of social welfare payment as well as preferential employment and tertiary educational placement in state sectors. In 2016–17, the estimated direct expenditure per person was $44,886 for Aboriginal and Torres Strait Islander Australians, which was around twice the rate for non-Indigenous Australians ($22 356).\n\nAs Australia does not have a Bill of Rights to implement all international human rights, some states have implemented their own charters. For example, in Victoria and in the ACT. As such, other sources of rights exist to protect rights in Australia (the Constitution, through statutes, common law, and through implementation of international treaties).\n\nHuman rights are protected under the Australian Constitution in several ways: \nIn addition, as a result of certain structural implications and principles, the Constitution protects human rights indirectly through several means, including: \n\nHuman rights are protected through various statutory enactments in a broad variety of specific contexts. For example, there are statutes which prescribe and regulate police powers, use of personal information, secret recording of conversations, equal treatment when buying goods and services, consumer rights, and many other statutes.\n\nThe common law of Australia protects rights indirectly through various causes of action (such as in contract, tort, and property rights). The common law also protects human rights through principles of statutory interpretation. One example is found in Gleeson CJ's statement that it is presumed that it is not Parliament's intention to remove a fundamental human right(s) or freedom(s) unless such an intention is outlined and manifested by clear language. This is known as the principle of legality which acts as an extra layer of protection for human rights against vague or ambiguous legislation. Furthermore, Former Chief Justice of New South Wales, James Spigelman, has compiled a list of a number of rights-depriving acts which the common law presumes the legislature does not intend without clear wording, including retrospectively changing rights and obligations, infringing personal liberty, interfering with freedom of movement or speech, restricting access to the courts, interfering with vested property rights, and denying procedural fairness.\n\nIn addition, there are various common law principles which afford certain protections, such as legal professional privilege, and the privilege against self-incrimination.\n\nAustralia has signed various international treaties and conventions regarding human rights. Australia has agreed to be bound by the following treaties: \n\nAlthough Australia is a signatory to these, the rights given in the treaties are only applicable in Australia if domestic legislation is established. For example, the \"Racial Discrimination Act\" 1975 (Cth), implements the Convention on the Elimination of All Forms of Racial Discrimination, and the \"Sex Discrimination Act\" 1984 (Cth), provides some of the rights outlined in the Convention on the Elimination of All Forms of Discrimination Against Women.\n\nHowever, another way the rights provided in a treaty can be seen in Australian law is where provisions of a treaty are already a part of domestic legislation (for example, the Convention of the Rights of People of Disabilities can be seen as incorporated into domestic law through similar provisions in the \"Disability Discrimination Act\" 1992 (Cth)).\n\nThe Australian Human Rights Commission (AHRC) (previously known as the Human Rights and Equal Opportunity Commission) is a national independent statutory body of the Australian government. Established under the \"Australian Human Rights Commission Act\" 1986 (Cth), it has responsibility for the investigation of alleged infringements under Australia’s anti-discrimination legislation.\n\nMatters that can be investigated by the Commission include discrimination on the grounds of age, race, colour or ethnic origin, racial vilification, sex, sexual harassment, sexual orientation, gender identity, intersex status, marital or relationship status, actual or potential pregnancy, breastfeeding or disability.\n\nHowever, the protection of human rights has several significant limitations. For instance, human rights will often trump other public goods as it enjoys a prima facie; human rights may be violated in some circumstances or reasons like national emergency or security; human rights are protected in various States (ACT and Victoria) through legislation. However, it cannot be implemented nor enforceable at the Federal level.\n\nUnder the Australian Constitution, there is an implied freedom of political communication on government and political matters.\n\nSome restrictions on political expression exist in Australia, including laws on defamation, racial vilification, and contempt of Parliament.\n\nIn 2015, Tasmania's Anti Discrimination Commissioner found that the Catholic Church and the Archbishop of Hobart had a \"case to answer\" under Tasmanian Anti-Discrimination legislation for promoting the Catholic view of marriage. Australian Greens candidate Martine Delaney brought the matter to the Commission. The ABC reported that case has \"raised concerns about freedom of speech ahead of a national debate on same-sex marriage.\"\n\nIn 2007, Parliamentarian Lee Rhiannon of the Australian Greens referred remarks made by an Australian Catholic Cardinal opposing embryonic stem cell research to the New South Wales parliamentary privileges committee for allegedly being in \"contempt of parliament\". The Cardinal was cleared of the charge and described the move as a \"clumsy attempt to curb religious freedom and freedom of speech\".\n\nAustralians achieved voting rights decades before most other Western nations. The Australian colonies granted male suffrage from the 1850s and in 1895 the women of South Australia achieved the right to both vote and stand for Parliament, enabling Catherine Helen Spence to be the first to stand as a political candidate in 1897. After federation of the colonies in 1901, the \"Franchise Act 1902\" was passed, granting the right to vote to men and women. However, the Act also restricted votes for 'natives' unless they were already enrolled. These restrictions were unevenly applied and were relaxed after World War II, with full rights restored by the \"Commonwealth Electoral Act 1962\".\n\nIn 1856, an innovative secret ballot was introduced in Victoria, Tasmania and South Australia, in which the government supplied voting paper containing the names of candidates and voters could select in private. This system was adopted around the world, becoming known as the \"Australian Ballot\". the \"Australian Ballot\" became well known throughout Australia.\n\nThe first woman elected to any Australian Parliament was Edith Cowan, to the West Australian Legislative Assembly in 1921. Dame Enid Lyons, in the Australian House of Representatives and Senator Dorothy Tangney became the first women in the Federal Parliament in 1943. In 1971, Senator Neville Bonner became the first Aboriginal Australian to sit in the federal Parliament. Rosemary Follett was elected Chief Minister of the Australian Capital Territory in 1989, becoming the first woman elected to lead a state or territory. In 2010, Julia Gillard became the first female Prime Minister of Australia.\n\nBy 2010, the people of Australia's oldest city, Sydney, had female leaders occupying every major political office above them, with Clover Moore as Lord Mayor, Kristina Keneally as Premier of New South Wales, Marie Bashir as Governor of New South Wales, Julia Gillard as Prime Minister, Quentin Bryce as Governor General of Australia and Elizabeth II as Queen of Australia.\n\nThe last use of the death penalty in Australia was in Victoria in 1967. Ronald Joseph Ryan was hanged at Pentridge Prison on 3 February 1967 for the murder of a prison guard, George Hodson. However, Australian criminologist, Gordon Hawkins, director of Sydney University's Institute of Criminology, doubts that Ryan was guilty.\n\nCapital punishment was officially abolished for federal offences by the \"Death Penalty Abolition Act 1973\". The various states abolished capital punishment at various times, starting with Queensland in 1922 and ending with New South Wales in 1985.\n\nDiscrimination against persons with disabilities in various contexts is prohibited under the \"Disability Discrimination Act 1992\" (Cth) (DDA). The Act makes it unlawful to treat a disabled person less favorably, or to fail to make reasonable adjustments for the person, in the contexts of employment, education, publicly available premises, provision of goods and services, accommodation, clubs and associations, and other contexts. Complaints made under the DDA are made to the Australian Human Rights Commission.\n\nThe Australian Government requested the Productivity Commission to evaluate the effectiveness of the DDA, and the Commission published its findings in 2004. The Commission found that while there is still room for improvement, particularly in reducing discrimination in employment, overall the DDA has been reasonably effective. The Commission found that people with a disability were less likely to finish school, to have a TAFE or university qualification and to be employed. They are more likely to have a below average income, be on a pension, live in public housing and in prison. The average personal income for people with a disability is 44 per cent of the income of other Australians.\n\nDisabilityCare Australia, formerly known as the National Disability Insurance Scheme, is a healthcare program initiated by the Australian government. The bill was introduced into parliament in November 2012. In July 2013 the first stage of DisabilityCare Australia commenced in South Australia, Tasmania, the Hunter Region in New South Wales and the Barwon area of Victoria, while the Australian Capital Territory will commence in July 2014.\n\nThe current wellbeing of indigenous Australians is an ongoing issue in Australia.\n\nThere is significant disparity in health between indigenous and non-indigenous Australians. In 2010–2012, the estimated life expectancy at birth for Aboriginal and Torres Strait Islander males was 69.1 years, and for females 73.7 years. This was 10.6 years lower than the life expectancy of non-Indigenous males, and 9.5 years lower than that of non-Indigenous females. A 2006 study by the Australian Institute of Health and Welfare showed that 70% of the Aboriginal population die before the age of 65, compared with 20% of non-indigenous Australians. Additionally, the suicide rate among Aboriginal Australians is almost three times higher (at 4.2%) than the national average (1.5%).\n\nThe roots of the present condition can be traced to the historical treatment of Aboriginals and the dispossession of land that occurred following arrival of the British settlers in the late 1700s, where a combination of disease, loss of land (and thus food resources) and violence reduced the Aboriginal population. Later, from the 1830s, colonial governments established the now controversial offices of the Protector of Aborigines in an effort to avoid mistreatment of Indigenous peoples and conduct government policy towards them. Christian churches in Australia sought to convert Aborigines, and were often used by government to carry out welfare and assimilation policies.\n\nThe Caledon Bay crisis of 1932–4 saw one of the last incidents of frontier violence, which began when the spearing of Japanese poachers who had been molesting Yolngu women was followed by the killing of a policeman. As the crisis unfolded, national opinion swung behind the Aboriginal people involved, and the first appeal on behalf of an Indigenous Australian to the High Court of Australia was launched. Elsewhere around this time, activists like Sir Douglas Nicholls were commencing their campaigns for Aboriginal rights within the established Australian political system and the age of frontier conflict closed.\n\nIn 1962, the Menzies Government's \"Commonwealth Electoral Act\" provided that all Indigenous Australians should have the right to enroll and vote at federal elections (prior to this, indigenous people in Queensland, Western Australia and some in the Northern Territory had been excluded from voting unless they were ex-servicemen). The successor Holt Government called the 1967 Referendum which removed the discriminatory clause in the Australian Constitution which excluded Aboriginal Australians from being counted in the census – the referendum was one of the few to be overwhelmingly endorsed by the Australian electorate (over 90% voted 'yes').\n\nFrom the 1960s, Australian writers began to re-assess European assumptions about Aboriginal Australia – with works including Geoffrey Blainey's landmark history \"Triumph of the Nomads\" (1975) and the books of historian Henry Reynolds.From the late 1960s a movement for Indigenous land rights developed. In the mid-1960s, one of the earliest Aboriginal graduates from the University of Sydney, Charles Perkins, helped organise freedom rides into parts of Australia to expose discrimination and inequality. In 1966, the Gurindji people of Wave Hill station (owned by the Vestey Group) commenced strike action led by Vincent Lingiari in a quest for equal pay and recognition of land rights.\n\nThe Whitlam Labor and Fraser Liberal Governments instigated the Aboriginal Land Rights Act 1976, which, while limited to the Northern Territory, affirmed \"inalienable\" freehold title to some traditional lands. In 1985, the Hawke Government returned ownership of Uluru (formerly known as Ayers Rock) to the local Pitjantjatjara Aboriginal people.\n\nIndigenous Australians began to take up representation in Australian parliaments during the 1970s. In 1971 Neville Bonner of the Liberal Party was appointed by the Queensland Parliament to replace a retiring senator, becoming the first Aborigine in Federal Parliament. Bonner was returned as a Senator at the 1972 election and remained until 1983. Hyacinth Tungutalum of the Country Liberal Party in the Northern Territory and Eric Deeral of the National Party of Queensland, became the first Indigenous people elected to territory and state legislatures in 1974. In 1976, Sir Douglas Nicholls was appointed Governor of South Australia, becoming the first Aborigine to hold vice-regal office in Australia. Aden Ridgeway of the Australian Democrats served as a senator during the 1990s, but no indigenous person was elected to the House of Representatives, until West Australian Liberal Ken Wyatt, in August 2010.In 1992, the High Court of Australia handed down its decision in the Mabo Case, recognising native title. That same year, Prime Minister Paul Keating said in his Redfern Park Speech that European settlers were responsible for the difficulties Australian Aboriginal communities continued to face. In 1999 Parliament passed a Motion of Reconciliation drafted by Prime Minister John Howard and Aboriginal Senator Aden Ridgeway naming mistreatment of Indigenous Australians as the most \"blemished chapter in our national history\".\n\nPrior to the calling of a 2007 federal election, the then Prime Minister, John Howard, revisited the idea of bringing a referendum to seek recognition of Indigenous Australians in the Constitution (his government first sought to include recognition of Aboriginal peoples in the Preamble to the Constitution in a 1999 referendum). The Labor opposition initially supported the idea; however, Kevin Rudd withdrew this support just prior to the election.\n\nIn 2007, Prime Minister John Howard and Indigenous Affairs Minister Mal Brough launched the Northern Territory National Emergency Response. In response to the Little Children are Sacred Report into allegations of child abuse among indigenous communities in the Territory, the government banned alcohol in prescribed communities in the Northern Territory; quarantined a percentage of welfare payments for essential goods purchasing; despatched additional police and medical personnel to the region; and suspended the permit system for access to indigenous communities. The policy was largely maintained under the Rudd and Gillard Governments.\n\nNotable contemporary indigenous rights campaigners have included: federal politicians Aden Ridgeway and Ken Wyatt, lawyer Noel Pearson; academic Marcia Langton; and Australians of the Year Lowitja O'Donoghue (1984), Mandawuy Yunupingu (1992), Cathy Freeman (1998) and Mick Dodson (2009). As of 2016, there were 5 indigenous people serving in the Federal Parliament of Australia.\n\nAustralia is an immigrant nation with a large and longstanding multi-ethnic migration program.\n\nHistorically, from the 1890s to the 1950s the country adhered to the White Australia Policy, which effectively barred or impeded people of non-European descent from immigrating to Australia. The policy was dismantled by successive governments after World War II, and from the 1970s successive governments officially supported multiculturalism. Australia is a signatory to the Refugee Convention and a component of the Australian immigration program is devoted to providing protection for refugees. The majority of refugees received by Australia are identified and referred by the UNHCR. The Special Humanitarian Program further offers refuge to people subject to \"substantial discrimination amounting to gross violation of human rights in their home country\" and who are supported by a proposer within Australia. In 2009–10 a total of 13,770 visas were granted under these categories. The annual figure remained roughly stable for the years between 2004–2010 and accepted applicants from such nations as Myanmar, Iraq, Bhutan, Afghanistan and six African countries.\n\nTo varying degrees of success, recent Australian governments have sought to discourage unauthorised arrivals by people seeking refugee status in Australia by maintaining a system of mandatory detention for processing of people who arrive without a visa. In 1992, Australia adopted a policy of under which the Australian government could detain any person in the country without a valid visa. In 1994 the detention of 'unlawful non-citizens' was made mandatory. During the late 1990s and early 2000s, these unauthorised arrivals, popularly referred to as \"boat people\", were transferred to one of the Australian immigration detention facilities on the Australian mainland, or to Manus Island or Nauru as part of the Pacific Solution. These offshore processing and mandatory detention policies have attracted criticism. In 2014, the Australian Human Rights Commission published a report, which found that many basic rights outlined in the Convention on the Rights of the Child were denied to children living in immigration detention.\n\nThe content of laws relating to the equality of LGBT people is summarised in the following table:\n\nFederal\n\nAustralia was the first country to conduct a parliamentary inquiry into involuntary or coerced medical interventions on intersex people, but the report has not been implemented. A recent Family Court case authorising a gonadectomy and consequential surgery on a young child has attracted public commentary for disclosing those medical interventions, their rationales, and a prior clitorectomy and labiaplasty.\n\nIn March 2017, Australian and New Zealand community organizations issued a joint call for legal reform, including the criminalization of deferrable intersex medical interventions on children, an end to legal classification of sex, and improved access to peer support.\n\n"}
{"id": "5376312", "url": "https://en.wikipedia.org/wiki?curid=5376312", "title": "Human sacrifice in Aztec culture", "text": "Human sacrifice in Aztec culture\n\nHuman sacrifice was common to many parts of Mesoamerica. Thus the rite was nothing new to the Aztecs when they arrived to the Valley of Mexico, nor was it something unique to pre-Columbian Mexico. Other Mesoamerican cultures, such as the Purépechas and Toltecs, performed sacrifices as well and from archaeological evidence, it probably existed since the time of the Olmecs (1200–400 BC), and perhaps even throughout the early farming cultures of the region. Although the extent of human sacrifice is unknown among several Mesoamerican civilizations, such as Teotihuacán, what distinguished Maya and Aztec human sacrifice was the importance with which it was embedded in everyday life.\n\nIn 1521, Spanish explorers such as Hernán Cortés conquered the Aztec capital of Tenochtitlan and made observations of and wrote reports about the practice of human sacrifice. Bernal Díaz del Castillo, who participated in the Cortés expedition, made frequent mention of human sacrifice in his memoir \"True History of the Conquest of New Spain\". There are a number of second-hand accounts of human sacrifices written by Spanish friars, that relate to the testimonies of native eyewitnesses. The literary accounts have been supported by archeological research. Since the late 1970s, excavations of the offerings in the Great Pyramid of Tenochtitlan, and other archaeological sites, have provided physical evidence of human sacrifice among the Mesoamerican peoples.\n\nA wide variety of interpretations of the Aztec practice of human sacrifice have been proposed by modern scholars. Many scholars now believe that Aztec human sacrifice was performed in honor of the gods. Most scholars of Pre-Columbian civilization see human sacrifice among the Aztecs as a part of the long cultural tradition of human sacrifice in Mesoamerica.\n\nSacrifice was a common theme in the Aztec culture. In the Aztec \"Legend of the Five Suns\", all the gods sacrificed themselves so that mankind could live. Some years after the Spanish conquest of Mexico, a body of Franciscans confronted the remaining Aztec priesthood and demanded, under threat of death, that they desist from this traditional practice. The Aztec priests defended themselves as follows:\n\nWhat the Aztec priests were referring to was a central Mesoamerican belief: that a great, continuing sacrifice of the gods sustains the Universe. A strong sense of indebtedness was connected with this worldview. Indeed, \"nextlahualli\" (debt-payment) was a commonly used metaphor for human sacrifice, and, as Bernardino de Sahagún reported, it was said that the victim was someone who \"gave his service\".\n\nHuman sacrifice was in this sense the highest level of an entire panoply of offerings through which the Aztecs sought to repay their debt to the gods. Both Sahagún and Toribio de Benavente (also called \"Motolinía\") observed that the Aztecs gladly parted with everything. Even the \"stage\" for human sacrifice, the massive temple-pyramids, was an offering mound: crammed with the land's finest art, treasure and victims, then buried underneath for the deities.\n\nAdditionally, the sacrifice of animals was a common practice, for which the Aztecs bred dogs, eagles, jaguars and deer. The cult of Quetzalcoatl required the sacrifice of butterflies and hummingbirds.\n\nSelf-sacrifice was also quite common; people would offer maguey thorns, tainted with their own blood and would offer blood from their tongues, ear lobes, or genitals. Blood held a central place in Mesoamerican cultures. The 16th-century Florentine Codex by Franciscan friar Bernardino de Sahagún reports that in one of the creation myths, Quetzalcóatl offered blood extracted from a wound in his own genitals to give life to humanity. There are several other myths in which Nahua gods offer their blood to help humanity.\n\nIt is debated whether these rites functioned as a type of atonement for Aztec believers. Some scholars argue that the role of sacrifice was to assist the gods in maintaining the cosmos, and not as an act of propitiation. Aztec society viewed even the slightest tlatlacolli ('sin' or 'insult') as an extremely malevolent supernatural force. To avoid such calamities befalling their community, those who had erred punished themselves by extreme measures such as slitting their tongues for vices of speech or their ears for vices of listening. Other methods of atoning wrongdoings included hanging themselves, or throwing themselves down precipices.\n\nWhat has been gleaned from all of this is that the sacrificial role entailed a great deal of social expectation and a certain degree of acquiescence.\n\nAccording to Diego Durán's History of the Indies of New Spain, and a few other sources that are also based on the Crónica X, the Flower Wars were an act of ritual between the cities of Aztec Triple Alliance and Tlaxcala, Huexotzingo and Cholula. This form of ritual was motivated by the Mesoamerican cultures in 1450 after a series of droughts and famine caused many deaths within the Mexican highlands. The droughts and damage to the crops were believed to be punishment by the gods for feeling disvalued instead of being honored properly. Therefore, the Flower Wars became a way to obtain human sacrifices in a very structured and ceremonial manner which were then used as offerings.\n\nThis type of warfare differed from regular political warfare, as the Flower war was also used for combat training and as first exposure to war for new military members. In addition, regular warfare included the use of long range weapons such as atlatl darts, stones, and sling shots to damage the enemy from afar. During Flower wars, warriors were expected to fight up close and exhibit their combat abilities while aiming to injure the enemy, rather than kill them. The main objective of Aztec Flower warfare was to capture victims alive for use later as human sacrifice, and offerings to the gods. When death occurred from battling in a Flower War, it was considered much more noble than dying in a regular military battle. Additionally, death in the Flower Wars contained religious importance as those who died were thought to live in heaven with the war god, Huitzilopochtli.\n\nHuman sacrifice rituals were performed during the appropriate times each month with the appropriate number of living bodies, and other goods, to properly appease and honor the gods. These individuals were previously chosen to be sacrificed, as was the case for people embodying the gods themselves, or were members of an enemy party which had been captured and prepared to be sacrificed. Even enemies of the Aztecs understood their roles as sacrifices to the gods since many also practiced the same type of religion. For many rites, the victim had such a quantity of prescribed duties that it is difficult to imagine how the accompanying festival would have progressed without some degree of compliance on the part of the victim. For instance, victims were expected to bless children, greet and cheer passers-by, hear people's petitions to the gods, visit people in their homes, give discourses and lead sacred songs, processions and dances.\n\nA great deal of cosmological thought seems to have underlain each of the Aztec sacrificial rites. Most of the sacrificial rituals took more than two people to perform. In the usual procedure of the ritual, the sacrifice would be taken to the top of the temple. The sacrifice would then be laid on a stone slab, a \"chacmool\", by four priests, and his/her abdomen would be sliced open by a fifth priest with a ceremonial knife made of flint. The most common form of human sacrifice was heart-extraction. The Aztec believed that the heart (\"tona\") was both the seat of the individual and a fragment of the Sun's heat (\"istli\"). The \"chacmool\" was a very important religious tool used during sacrifices. The cut was made in the abdomen and went through the diaphragm. The priest would grab the heart which would be placed in a bowl held by a statue of the honored god, and the body would then be thrown down the temple's stairs. The body would land on a terrace at the base of the pyramid called an \"apetlatl\".\n\nBefore and during the killing, priests and audience, gathered in the plaza below, stabbed, pierced and bled themselves as auto-sacrifice. Hymns, whistles, spectacular costumed dances and percussive music marked different phases of the rite.\n\nThe body parts would then be disposed of, the viscera fed the animals in the zoo, and the bleeding head was placed on display in the \"tzompantli\" or the skull rack. When the consumption of individuals was involved, the warrior who captured the enemy was given the meaty limbs while the most important flesh, the stomach and chest, were offerings to the gods.\n\nOther types of human sacrifice, which paid tribute to various deities, killed the victims differently. The victim could be shot with arrows, die in gladiatorial style fighting, be sacrificed as a result of the Mesoamerican ballgame, burned, flayed after being sacrificed, or drowned.\n\nThose individuals who were unable to complete their ritual duties were disposed of in a much less honorary matter. This \"insult to the gods\" needed to be atoned, therefore the sacrifice was slain while being chastised instead of revered. The conquistadors Cortés and Alvarado found that some of the sacrificial victims they freed \"indignantly rejected [the] offer of release and demanded to be sacrificed\".\n\nSome post-conquest sources report that at the re-consecration of Great Pyramid of Tenochtitlan in 1487, the Aztecs sacrificed about 80,400 prisoners over the course of four days. This number is considered by Ross Hassig, author of \"Aztec Warfare\", to be an exaggeration. Hassig states \"between 10,000 and 80,400 persons\" were sacrificed in the ceremony. The higher estimate would average 14 sacrifices per minute during the four-day consecration. Four tables were arranged at the top so that the victims could be jettisoned down the sides of the temple. Nonetheless, according to Codex Telleriano-Remensis, old Aztecs who talked with the missionaries told about a much lower figure for the reconsecration of the temple, approximately 4,000 victims in total.\n\nMichael Harner, in his 1977 article \"The Enigma of Aztec Sacrifice\", cited an estimate by Borah of the number of persons sacrificed in central Mexico in the 15th century as high as 250,000 per year which may have been one percent of the population. Fernando de Alva Cortés Ixtlilxochitl, a Mexica descendant and the author of Codex Ixtlilxochitl, estimated that one in five children of the Mexica subjects was killed annually. Victor Davis Hanson argues that a claim by Don Carlos Zumárraga of 20,000 per annum is \"more plausible\". Other scholars believe that, since the Aztecs often tried to intimidate their enemies, it is more likely that they could have inflated the number as a propaganda tool. The same can be said for Bernal Díaz's inflated calculations when, in a state of visual shock, he grossly miscalculated the number of skulls at one of the seven Tenochtitlan tzompantlis. The counter argument is that both the Aztecs and Diaz were very precise in the recording of the many other details of Aztec life, and inflation or propaganda would be unlikely. According to the Florentine Codex, fifty years before the conquest the Aztecs burnt the skulls of the former tzompantli. Archeologist Eduardo Matos Moctezuma has unearthed and studied some tzompantlis. In 2003, archaeologist Elizabeth Graham noted that the largest number of skulls yet found at a single tzompantli was only about a dozen.\n\nEvery Aztec warrior would have to provide at least one prisoner for sacrifice. All the male population was trained to be warriors, but only the few who succeeded in providing captives could become full-time members of the warrior elite. Accounts also state that several young warriors could unite to capture a single prisoner, which suggests that capturing prisoners for sacrifice was challenging.\n\nThere is still much debate as to what social groups constituted the usual victims of these sacrifices. It is often assumed that all victims were 'disposable' commoners or foreigners. However, slaves – a major source of victims – were not a permanent class but rather persons from any level of Aztec society who had fallen into debt or committed some crime. Likewise, most of the earliest accounts talk of prisoners of war of diverse social status, and concur that virtually all child sacrifices were locals of noble lineage, offered by their own parents. That women and children were not excluded from potential victims is attested by a tzompantli found in 2015 at Templo Mayor in the Aztec capital Tenochtitlan.\n\nIt is doubtful if many victims came from far afield. In 1454, the Aztec government forbade the slaying of captives from distant lands at the capital's temples. Duran's informants told him that sacrifices were consequently 'nearly always ... friends of the [Royal] House' – meaning warriors from allied states.\n\nHuitzilopochtli was the tribal deity of the Mexica and, as such, he represented the character of the Mexican people and was often identified with the sun at the zenith, and with warfare, who burned down towns and carried a fire-breathing serpent, Xiuhcoatl. He was considered the primary god of the south and a manifestation of the sun, and a counterpart of the black Tezcatlipoca, the primary god of the north, \"a domain associated with Mictlan, the underworld of the dead\".\n\nHuitzilopochtli was worshipped at the Templo Mayor, which was the primary religious structure of the Aztec capital of Tenochtitlan. The Templo Mayor consisted of twin pyramids, one for Huitzilopochtli and one for the rain god Tlaloc (discussed below).\n\nWhen the Aztecs sacrificed people to Huitzilopochtli (the god with warlike aspects) the victim would be placed on a sacrificial stone. The priest would then cut through the abdomen with an obsidian or flint blade. The heart would be torn out still beating and held towards the sky in honor to the Sun-God. The body would then be pushed down the pyramid where the Coyolxauhqui stone could be found. The Coyolxauhqui Stone recreates the story of Coyolxauhqui, Huitzilopochtli's sister who was dismembered at the base of a mountain, just as the sacrificial victims were. The body would be carried away and either cremated or given to the warrior responsible for the capture of the victim. He would either cut the body in pieces and send them to important people as an offering, or use the pieces for ritual cannibalism. The warrior would thus ascend one step in the hierarchy of the Aztec social classes, a system that rewarded successful warriors.\n\nDuring the festival of Panquetzaliztli, of which Huitzilopochtli was the patron, sacrificial victims were adorned in the manner of Huitzilopochtli's costume and blue body paint, before their hearts would be sacrificially removed. Representations of Huitzilopochtli called teixiptla were also worshipped, the most significant being the one at the Templo Mayor which was made of dough mixed with sacrificial blood.\n\nTezcatlipoca was generally considered the most powerful god, the god of night, sorcery and destiny (the name \"tezcatlipoca\" means \"smoking mirror\", or \"obsidian\"), and the god of the north. The Aztecs believed that Tezcatlipoca created war to provide food and drink to the gods. Tezcatlipoca was known by several epithets including \"the Enemy\" and \"the Enemy of Both Sides\", which stress his affinity for discord. He was also deemed the enemy of Quetzalcoatl, but an ally of Huitzilopochtli. Tezcatlipoca had the power to forgive sins and to relieve disease, or to release a man from the fate assigned to him by his date of birth; however, nothing in Tezcatlipoca's nature compelled him to do so. He was capricious and often brought about reversals of fortune, such as bringing drought and famine. He turned himself into Mixcoatl, the god of the hunt, to make fire. To the Aztecs, he was an all-knowing, all-seeing nearly all-powerful god. One of his names can be translated as \"He Whose Slaves We Are\".\n\nSome captives were sacrificed to Tezcatlipoca in ritual gladiatorial combat. The victim was tethered in place and given a mock weapon. He died fighting against up to four fully armed jaguar knights and eagle warriors.\n\nDuring the 20-day month of Toxcatl, a young impersonator of Tezcatlipoca would be sacrificed. Throughout a year, this youth would be dressed as Tezcatlipoca and treated as a living incarnation of the god. The youth would represent Tezcatlipoca on earth; he would get four beautiful women as his companions until he was killed. In the meantime he walked through the streets of Tenochtitlan playing a flute. On the day of the sacrifice, a feast would be held in Tezcatlipoca's honor. The young man would climb the pyramid, break his flute and surrender his body to the priests. Sahagún compared it to the Christian Easter.\n\nXiuhtecuhtli is the god of fire and heat and in many cases is considered to be an aspect of Huehueteotl, the \"Old God\" and another fire deity.\n\nBoth Xiuhtecuhtli and Huehueteotl were worshipped during the festival of \"Izcalli.\" For ten days preceding the festival various animals would be captured by the Aztecs, to be thrown in the hearth on the night of celebration.\n\nTo appease Huehueteotl, the fire god and a senior deity, the Aztecs had a ceremony where they prepared a large feast, at the end of which they would burn captives; before they died they would be taken from the fire and their hearts would be cut out. Motolinía and Sahagún reported that the Aztecs believed that if they did not placate Huehueteotl, a plague of fire would strike their city. The sacrifice was considered an offering to the deity.\n\nXiuhtecuhtli was also worshipped during the New Fire Ceremony, which occurred every 52 years, and prevented the ending of the world. During the festival priests would march to the top of the volcano \"Huixachtlan\" and when the constellation \"the fire drill\" (Orion's belt) rose over the mountain, a man would be sacrificed. The victims heart would be ripped from his body and a ceremonial hearth would be lit in the hole in his chest. This flame would then be used to light all of the ceremonial fires in various temples throughout the city of Tenochtitlan.\n\nTlaloc is the god of rain, water, and earthly fertility. The Aztecs believed that if sacrifices were not supplied for Tlaloc, rain would not come, their crops would not flourish, and leprosy and rheumatism, diseases caused by Tlaloc, would infest the village.\n\nArchaeologists have found the remains of at least 42 children sacrificed to Tlaloc at the Great Pyramid of Tenochtitlan. Many of the children suffered from serious injuries before their death, they would have to have been in significant pain as Tlaloc required the tears of the young as part of the sacrifice. The priests made the children cry during their way to immolation: a good omen that Tlaloc would wet the earth in the raining season.\n\nIn the Florentine Codex, also known as \"General History of the Things of New Spain\", Sahagún wrote:\n\nXipe Totec, known as \"Our Lord the Flayed One\", is the god of rebirth, agriculture, the seasons, and craftsmen.\n\nXipe Totec was worshipped extensively during the festival of \"Tlacaxipehualiztli\", in which captured warriors and slaves were sacrificed in the ceremonial center of the city of Tenochtitlan. For forty days prior to their sacrifice one victim would be chosen from each ward of the city to act as \"ixiptla,\" dress and live as Xipe Totec. The victims were then taken to the Xipe Totec's temple where their hearts would be removed, their body dismembered and their body parts divided up to be later eaten. Prior to their death and dismemberment the victims skin would be removed, and worn by individuals who traveled throughout the city fighting battles and collecting gifts from the citizens.\n\nThe cycle of fifty-two years was central to Mesoamerican cultures. The Nahua's religious beliefs were based on a great fear that the universe would collapse after each cycle if the gods were not strong enough. Every fifty-two years a special New Fire ceremony was performed. All fires were extinguished and at midnight a human sacrifice was made. The Aztecs then waited for the dawn. If the Sun appeared it meant that the sacrifices for this cycle had been enough. A fire was ignited on the body of a victim, and this new fire was taken to every house, city and town. Rejoicing was general: a new cycle of fifty-two years was beginning, and the end of the world had been postponed, at least for another 52-year cycle.\n\nSacrifices were made on specific days. Sahagún, Juan Bautista de Pomar and Motolinía report that the Aztecs had eighteen festivities each year, one for each Aztec month. The table below shows the festivals of the 18-month year of the Aztec calendar and the deities with which the festivals were associated.\n\nVisual accounts of Aztec sacrificial practice are principally found in codices and some Aztec statuary. Many visual renderings in the codices were created for Spanish patrons, and thus may reflect European preoccupations and prejudices. Produced during the 16th century, the most prominent codices include the Ríos, Tudela, Telleriano-Remensis, Magliabechiano, and Sahagún’s Florentine. A contrast is offered in the few Aztec statues that depict sacrificial victims, which show an Aztec understanding of sacrifice. Rather than showing a preoccupation with debt repayment, they emphasize the mythological narratives that resulted in human sacrifices, and often underscore the political legitimacy of the Aztec state. For instance, the Coyolxauhqui stone found at the foot of the Templo Mayor commemorates the mythic slaying of Huitzilopochli's sister for the matricide of Coatlicue; it also, as Cecelia Kline has pointed out, \"served to warn potential enemies of their certain fate should they try to obstruct the state's military ambitions\".\n\nIn addition to the accounts provided by Sahagún and Durán, there are other important texts to be considered.\n\nJuan de Grijalva, Hernán Cortés, Juan Díaz, Bernal Díaz, Andrés de Tapia, Francisco de Aguilar, Ruy González and the Anonymous Conqueror wrote about the Conquest of Mexico. Martyr d'Anghiera, Lopez de Gomara, Oviedo y Valdes and Illescas, while not in Mesoamerica, wrote their accounts based on interviews with the participants. Bartolomé de Las Casas and Sahagún arrived later to New Spain but had access to direct testimony, especially of the indigenous people.\n\nJuan de Grijalva was one of the first Spaniards to explore Mexico and traveled on his expedition in 1518 with Juan Díaz. Diaz wrote \"Itinerario de Grijalva\" before 1520, in which he describes the aftermath of a sacrifice on an island off the coast of Veracruz. He said,When he reached said tower the Captain asked him why such deeds were committed there and the Indian answered that it was done as a kind of sacrifice and gave to understand that the victims were beheaded on the wide stone; that the blood was poured into the vase and that the heart was taken out of the breast and burnt and offered to the said idol. The fleshy parts of the arms and legs were cut off and eaten. This was done to the enemies with whom they were at war.\n\nBernal Díaz corroborates Juan Díaz's history:\n\nIn \"The Conquest of New Spain\" Díaz recounted that, after landing on the coast, they came across a temple dedicated to Tezcatlipoca. \"That day they had sacrificed two boys, cutting open their chests and offering their blood and hearts to that accursed idol\". Díaz narrates several more sacrificial descriptions on the later Cortés expedition. Arriving at Cholula, they find \"cages of stout wooden bars ... full of men and boys who were being fattened for the sacrifice at which their flesh would be eaten\". When the conquistadors reached Tenochtitlan, Díaz described the sacrifices at the Great Pyramid:\n\nAccording to Bernal Díaz, the chiefs of the surrounding towns, for example Cempoala, would complain on numerous occasions to Cortés about the perennial need to supply the Aztecs with victims for human sacrifice. It is clear from his description of their fear and resentment toward the Mexicas that, in their opinion, it was no honor to surrender their kinsmen to be sacrificed by them.\n\nAt the town of Cingapacigna Cortez told the chiefs that for them to become friends and brothers of the Spaniards they must end the practice of making sacrifices. According to Bernal Diaz: \n\nOn meeting a group of inhabitants from Cempoala who gave Cortes and his men food and invited them to their village:\nCortés was the Spanish conquistador whose expedition to Mexico in 1519 led to the fall of the Aztecs, and led to the conquering of vast sections of Mexico on behalf of the Crown of Castile.\n\nCortés wrote of Aztec sacrifice on numerous occasions, one of which in his \"Letters,\" he states:\n\nThe Anonymous Conquistador was an unknown travel companion of Cortés who wrote \"Narrative of Some Things of New Spain and of the Great City of Temestitan\" which details Aztec sacrifices.\n\nThe Anonymous Conquistador wrote, They lead him to the temple, where they dance and carry on joyously, and the man about to be sacrificed dances and carries on like the rest. At length the man who offers the sacrifice strips him naked, and leads him at once to the stairway of the tower where is the stone idol. Here they stretch him on his back, tying the hands to the sides and fastening the legs ... Soon comes the sacrificing priest—and this is no small office among them—armed with a stone knife, which cuts like steel, and is as big as one of our large knives. He plunges the knife into the breast, opens it, and tears out the heart hot and palpitating. And this as quickly as one might cross himself. At this point the chief priest of the temple takes it, and anoints the mouth of the principal idol with the blood; then filling his hand with it he flings it towards the sun, or towards some star, if it be night. Then he anoints the mouths of all the other idols of wood and stone, and sprinkles blood on the cornice of the chapel of the principal idol. Afterwards they burn the heart, preserving the ashes as a great relic, and likewise they burn the body of the sacrifice, but these ashes are kept apart from those of the heart in a different vase.\n\nModern excavations in Mexico City have found evidence of human sacrifice in the form of hundreds of skulls at the site of old temples.\n\nDifferent anthropological or other sources have attempted to explain a possible ecological explanation of the need for human sacrifices to supplement overall Aztec diet. Harner's main argument lies within his claim that cannibalism is needed to assist the diet of the Aztecs. He claimed that very high population pressure and an emphasis on maize agriculture, without domesticated herbivores, led to a deficiency of essential amino acids amongst the Aztecs. As population increased and the amount of available game decreased, the Aztecs had to compete with other carnivorous mammals, such as dogs, to find food. Harner believes that although intensified agricultural practices provided the Aztec society a surplus of carbohydrates, they did not provide sufficient nutritional balance; for this reason, the cannibalistic consumption of sacrificed humans was needed to supply an appropriate amount of protein per individual. Harris, author of \"Cannibals and Kings\", has propagated the claim, originally proposed by Harner, that the flesh of the victims was a part of an aristocratic diet as a reward, since the Aztec diet was lacking in proteins.\n\nHowever, Bernard Ortiz Montellano offers a counter argument and points out the faults of Harner's sources. First off, Ortiz challenges Harner's claim of the Aztecs needing to compete with other carnivorous mammals for protein packed food. Many other types of foods were available to the Aztecs, including meat from salamanders, fowls, armadillos, and weasels. These resources were also plenty available due to their need to subsist in Lake Texcoco, the place where the Aztecs had created their home. In addition, even if no herbivores were available to eat, the nutrients needed were found in the leaves and seeds of amaranth which also provided protein. Lastly, the Aztecs had a highly structured system in which \"chinampas\" and tribute provided a surplus of materials and therefore ensured the Aztec were able to meet their caloric needs.\n\nOrtiz's argument helps to frame and evaluate the gaps within Harner's argument. Part of the issue with Harner's reasoning for Aztec use of cannibalism was the lack of reliability of his sources. Harner recognized the numbers he used may be contradicting or conflicting with other sources, yet he continued to use these sources and claimed them as reliable. Ortiz qualifies Harner's sources as Spanish propaganda, and states the need to critique primary sources of interactions with the Aztecs. By dehumanizing and villainizing Aztec culture, the Spaniards were able to justify their own actions for conquest. Therefore, encounters with sacrificial cannibalism were grossly exaggerated and Harner used the sources to aid his argument. Overall, ecological factors alone are not sufficient to account for human sacrifice and, more recently, it is posited that religious beliefs have a significant effect on motivation.\n\nSacrifices were ritualistic and symbolic acts accompanying huge feasts and festivals, and were a way to properly honor the gods. Victims usually died in the \"center stage\" amid the splendor of dancing troupes, percussion orchestras, elaborate costumes and decorations, carpets of flowers, crowds of thousands of commoners, and all the assembled elite. Aztec texts frequently refer to human sacrifice as \"neteotoquiliztli\", \"the desire to be regarded as a god\". These members of the society became an \"ixiptla\"—that is, a god's representative, image or idol.\n\nFor each festival, at least one of the victims took on the paraphernalia, habits, and attributes of the god or goddess whom they were dying to honor or appease. Through this performance, it was said that the divinity had been given 'human form'—that the god now had an \"ixitli\" (face). Duran says such victims were 'worshipped ... as the deity' or 'as though they had been gods'. Even whilst still alive, \"ixiptla\" victims were honored, hallowed and addressed very highly. Particularly the young man who was indoctrinated for a year to submit himself to Tezcatlipoca's temple was the Aztec equivalent of a celebrity, being greatly revered and adored to the point of people \"kissing the ground\" when he passed by.\n\nPosthumously, their remains were treated as actual relics of the gods which explains why victims' skulls, bones and skin were often painted, bleached, stored and displayed, or else used as ritual masks and oracles. For example, Diego Duran's informants told him that whoever wore the skin of the victim who had portrayed god Xipe (Our Lord the Flayed One) felt he was wearing a holy relic. He considered himself 'divine'.\n\nPolitically human sacrifice was important in Aztec culture as a way to represent a social hierarchy between their own culture and the enemies surrounding their city. Additionally, it was a way to structure the society of the Aztec culture itself. The hierarchy of cities like Tenochtitlan were tiered with the \"Tlatoani\" (emperor) on the top, the remaining nobles (\"pipiltin\") next who managed the land owned by the emperor. Then the warriors, the \"pochteca\" (merchants), commoners and farmers. Then the lowest level of the hierarchy consisted of slaves and indentured servants. The only way of achieving social mobility was through successful performance as a warrior. This shows how important capturing enemies for sacrifice was as it was the singular way of achieving some type of \"nobility\".\n\nWithin the system of organization based on hierarchy, there was also a social expectation contributing to the status of an individual at the time of their sacrifice. An individual was punished if unable to confidently address their own sacrifice, i.e. the person acted cowardly beforehand instead of brave. Then, instead of being sacrificed honorably, their lowly death paralleled their new lowly status. Where one's body traveled in the afterlife also depended on the type of death awarded to the individual. Those who died while being sacrificed or while battling in war went to the second-highest heaven, while those who died of illness were the lowest in the hierarchy. Those going through the lowest hierarchy of death were required to undergo numerous torturous trials and journeys, only to culminate in a somber underworld. Additionally, death during Flower Wars was considered much more noble than death during regular military endeavors.\n\n\nIngham, John M. \"Human Sacrifice at Tenochtitln.\" Society for Comparative Studies in Society and History 26 (1984): 379-400.\n"}
{"id": "2084802", "url": "https://en.wikipedia.org/wiki?curid=2084802", "title": "John Vassall", "text": "John Vassall\n\nWilliam John Christopher Vassall (20 September 1924 – 18 November 1996) was a British civil servant who spied for the Soviet Union, allegedly under pressure of blackmail, from 1954 until his arrest in 1962. Although operating only at a junior level, he was able to provide details of naval technology which were crucial to the modernising of the Soviet Navy. He was sentenced to eighteen years' imprisonment, and was released in 1972 after ten. The Vassall scandal greatly embarrassed the Macmillan government, but was soon eclipsed by the more dramatic Profumo affair.\n\nBorn in 1924 and known throughout his life as John Vassall, he was the son of William Vassall, chaplain at St Bartholomew's Hospital, London, and Mabel Andrea Sellicks, a nurse at the same hospital. He was educated at Monmouth School. In World War II, he worked as a photographer for the Royal Air Force. After the war, in 1948, he became a clerk (clerical officer) at the Admiralty.\n\nAlthough his father was an Anglican priest, his mother converted to Roman Catholicism, a decision that led to tensions within their marriage. John himself converted in 1953.\n\nIn 1952, Vassall was appointed, still as a clerical officer, to the staff of the Naval Attaché at the British embassy in Moscow. There, he said later, he found himself socially isolated by the snobberies and class hierarchies of diplomatic life, his loneliness further exacerbated by his homosexuality, which was still illegal in both Britain and the Soviet Union at the time. He became acquainted with a Pole named Mikhailsky, who worked for the Embassy, and who introduced him to the homosexual underworld of Moscow. In 1954, he was invited to a party, where he was encouraged to become extremely drunk, and where he was photographed in compromising positions with several men.\n\nThe party, arranged by the KGB, had been a classic \"honeytrap\". The Soviets used the photographs to blackmail Vassall into working for them as a spy, initially in the Moscow embassy, and later in London, following his return there in June 1956. He returned to the Admiralty, where he worked first in the Naval Intelligence Division, and then, as the clerical officer assistant to the Private Secretary, in the Private Office of Tam Galbraith, a Conservative Party politician and Civil Lord of the Admiralty. At the time of his arrest he was working in Military Branch II. During his espionage career, Vassall provided the Soviets with several thousand classified documents, including information on British radar, torpedoes, and anti-submarine equipment. His obituary-writer in \"The Times\" commented that \"Vassall was never more than a low-level functionary, but there was nothing low-level about the damage he was able to inflict\". Similarly, Chapman Pincher regarded Vassall as \"the classic example of the spy who, while of lowly rank, can inflict enormous damage because of the excellence of his access to secret information\". Pincher continued: \"I am in no doubt that the recruitment and running of Vassall was a major triumph for the K.G.B. He provided information of the highest value to the Soviet defence chiefs in their successful drive to expand and modernise the Red Navy.\"\n\nAuthor Rebecca West, in her book \"The New Meaning of Treason\" (1964) demurred from the notion that Vassall was \"a weak and silly little man … This was unlikely to be the correct view of a man who for seven years had carried on an occupation [espionage] demanding unremitting industry in a skilled craft carried on in clandestine conditions, an endless capacity for dissimulation, and sustained contempt for personal danger.\" West termed him, rather, \"a professional spy, working within the conventions of his profession, [who] had no more been blackmailed into the exercise of his profession than any lawyer\". West suggested that the claim of blackmail was \"putting up a smoke-screen to conceal what he had done.\" Observing that Vassall had been well paid by the Soviets for his spying, West wrote: \"The drunken party may have taken place, but it was probably engineered so that Vassall might refer to it should his treachery ever be discovered … Only a very stupid and helpless man would have succumbed [to a blackmail threat], and Vassall was not stupid; he was extremely resourceful.\"\n\nVassall was identified as a potential spy after Anatoliy Golitsyn, a senior member of the KGB, defected to the United States in 1961. The KGB, worried that Vassall would be exposed, ordered him to cease operations until further notice. Another defector, Yuri Nosenko, added to the case against Vassall, but doubts about the evidence provided by both Golitsyn and Nosenko persisted. Vassall soon resumed his work. It had become obvious to his colleagues that Vassall had some other source of income, for he moved to an expensive flat in Dolphin Square, took foreign holidays, and was said to own 36 Savile Row suits. His annual expenditure was later estimated at about £3,000, when his official salary was £750; but he explained that he had an inheritance from a distant relative.\n\nOn 12 September 1962, Vassall was arrested and charged with spying. He made a full confession, and directed detectives to the cameras and films concealed in his flat. The documents which he admitted to stealing did not account for everything believed to have been taken, however, which led to speculation that there was another spy still operating in the Admiralty. Some have suggested that Vassall was deliberately sacrificed by the KGB in an attempt to protect the other (possibly more senior) spy. In October, Vassall was sentenced to 18 years in jail.\n\nThe scandal caused the Macmillan government considerable embarrassment, erupting as it did at the height of the Cold War, and only a year before the still more dramatic revelations of the Profumo affair. A tribunal was held to inquire into whether the failure to detect Vassall earlier amounted to a failure of intelligence, as many British newspapers had claimed. It also investigated suggestions that the close relations between Vassall and Galbraith had been improper. However, in its conclusions it found no evidence for impropriety, and largely exonerated the government.\n\nVassall served ten years of his sentence, in Wormwood Scrubs, Maidstone and Durham prisons. Many considered him to have been a relatively innocent victim of circumstances, and he was befriended in prison by the social reformer, Lord Longford. He was eventually released on parole in October 1972.\n\nHe then wrote a memoir, published in 1975 as \"Vassall: the autobiography of a spy\". He described it as \"a kind of self-justification, not as regards my espionage activities, but as regards my position as a human being, and, perhaps, my ability to make and keep friends in all walks of life\". Rex Winsbury called the book \"[a] cross between Jennifer's Diary [the society column of \"Queen\" magazine] and James Bond, ... bewildering both for Vassall's own transparent naivety and social snobbism, ... and for the equally transparent naivety of the British Foreign Office and security forces\". Hungarian émigré George Mikes similarly concluded that it was Vassall's \"vanity, his childish snobbery, his devouring ambition and complete lack of humour that pushed him so deep into the quagmire\".\n\nVassall subsequently changed his surname to Phillips, settled in St John's Wood, London, and worked quietly as an administrator at the British Records Association, and for a firm of solicitors in Gray's Inn. He died after suffering a heart-attack on a London bus in November 1996: it was not until nearly three weeks later that the press became aware of his death.\n\nThe suggestion of an improper relationship between Vassall and Tam Galbraith inspired a memorable sketch on the satirical BBC programme \"That Was the Week That Was\", broadcast in 1963, in which Lance Percival played a senior civil servant detecting sexual innuendo in such conventional pleasantries as the salutation \"My Dear Vassall\" at the beginning of a letter.\n\nIn 1980 the BBC broadcast a docudrama about the affair, in which Vassall was played by John Normington as \"weak, vain and keen to be thought a gentleman\". The play caused some controversy when it became known that neither Lady Hayter, the Ambassador's wife when Vassall arrived in Moscow, nor Captain Geoffrey Bennett, the naval attaché, had been consulted or advised that they were to be portrayed: they only learnt when \"Radio Times\" was published four days before the broadcast.\n\n\n"}
{"id": "8724959", "url": "https://en.wikipedia.org/wiki?curid=8724959", "title": "Joseph Rowntree (philanthropist)", "text": "Joseph Rowntree (philanthropist)\n\nJoseph Rowntree (24 May 1836 – 24 February 1925) was an English Quaker philanthropist and businessman from York. Rowntree is perhaps best known for being a champion of social reform, partner and friend of Charles Booth, and his time as a chocolatier at family business \"Rowntree's\", one of the most important in Britain. Even as a powerful businessman, he was deeply interested in improving the quality of life of his employees; this led to him becoming a philanthropist, pursuing many charitable causes. In 1904 he created three trusts, the Joseph Rowntree Village Trust (JRVT) which was originally set up to build and manage the garden village of New Earswick, the Joseph Rowntree Charitable Trust (JRCT) and the Joseph Rowntree Social Services Trust (JRSST). The latter two were both set up to effect social reform, the difference between them being that whereas the Charitable Trust was set up as a charity, the Social Services Trust was set up as a limited company so that if necessary it would be able to undertake social and political work not legally allowed by a charitable Trust. He suggested that only the JRVT would be permanent but in fact all the trusts are still in existence although the Social Services Trust has changed its name to the Joseph Rowntree Reform Trust and with the separation of the Joseph Rowntree Housing Trust from the Village Trust in 1968, there are now four trusts that exist today.\n\nRowntree was born the son of Sarah and Joseph Rowntree, on Pavement Street in York where his father owned a grocer's shop. He attended Bootham School. At fourteen he accompanied his father on a visit to Ireland, and witnessed the effects of the potato famine. This experience was to provide the grounding for his political views and business ideas later in life.\n\nHe started working in his father's grocery business as an apprentice the following year, and after his father's death in 1859 he took over the running, jointly managing the business with his brother John Stephenson Rowntree.\n\nIn 1869 he joined his brother, Henry Isaac Rowntree, who owned a chocolate factory in York. When Henry Isaac died in 1883, Joseph became the owner of the company. Joseph pursued his progressive ideas within the running of Rowntree's, in the design of the new factory opened in 1881, and in the business practices followed therein, including the founding of one of the first Occupational Pension Schemes.\n\nThe company, Rowntree's, grew from 30 to over 4,000 employees by the end of the 19th century making it Britain's eightieth largest manufacturing employer. It merged with John Mackintosh and Co. in 1969 and was taken over by Nestlé in 1988.\n\nHe had two marriages, to Julia Eliza Seebohm in 1862 who died in 1863, and then to her cousin Emma Antoinette Seebohm in 1867 with whom he had six children. The social investigator Seebohm Rowntree was one of their children.\n\nJoseph Rowntree's grave, along with many other members of his family, can be seen in the Quaker cemetery within the grounds of The Retreat on Heslington Road, York.\n\nPhilosophical and political views: Joseph Rowntree was a supporter of liberal values, and was anxious to improve the quality of life of his employees. He provided them with a library, free education, a works magazine, a social welfare officer, a doctor, a dentist and a pension fund.\n\nThe Joseph Rowntree School was built in York in 1942 by the Joseph Rowntree Village Trust. In 2010 the school relocated to new premises costing £29 million. Students refer to it as \"Jo Ro\".\n\nA campaign was started in summer 2012 to put a statue of Joseph Rowntree at a prominent site in the centre of York, with a Facebook page – \"A Joseph Rowntree statue for York City Centre\" – that stated: \"York should be proud of its greatest son! This campaign aims to place a statue of Joseph Rowntree, philanthropist, social reformer, and chocolatier, in Parliament Square, York, on the site of the repulsed and now-demolished toilet block.\"\n\n"}
{"id": "1695340", "url": "https://en.wikipedia.org/wiki?curid=1695340", "title": "Judaeo-Portuguese", "text": "Judaeo-Portuguese\n\nJudaeo-Portuguese, or Lusitanic, is the extinct Jewish language that was used by the Jews of Portugal.\n\nIt was the vernacular of Sephardi Jews in Portugal before the 16th century and also in many places of the Portuguese Jewish diaspora. Its texts were written in the Hebrew script (\"aljamiado português\") or the Latin alphabet.\n\nAs Portuguese Jews intermarried with other expelled Sephards, the language influenced the nearby Judeo-Spanish. Close similarity to Standard Portuguese made Judeo-Portuguese go extinct in Portugal, having survived in everyday usage in the diaspora until the late 18th/early 19th century. Judeo-Portuguese influenced the Papiamento and Saramaccan languages.\n\n\n"}
{"id": "11868725", "url": "https://en.wikipedia.org/wiki?curid=11868725", "title": "Københavns Lufthavne", "text": "Københavns Lufthavne\n\nKøbenhavns Lufthavne is a public limited company that operates two airports in Copenhagen, Denmark, Copenhagen Airport and Roskilde Airport. In addition, the company previously held a 49% stake in Newcastle International Airport and 10% of Aeropuertos del Sureste that operated nine airports in Mexico.\n\nThe largest owners of Københavns Lufthavne are Macquarie Infrastructure Company (52.4%) and the Government of Denmark (39.2%). The company was created by the government in 1925 to operate the airport. Until 1990 the company was a government enterprise called Københavns Lufthavnsvæsen. In 1990 it was transformed to a limited company. The government sold 25% of its stake in the company in 1994, and Københavns Lufthavne was listed on the Copenhagen Stock Exchange. In 1996 and 2000 the government sold additional 24% and 17%, respectively.\n"}
{"id": "8076950", "url": "https://en.wikipedia.org/wiki?curid=8076950", "title": "Languages of Argentina", "text": "Languages of Argentina\n\nAt least 40 spoken languages are spoken in Argentina. They include indigenous and immigrant languages, with Spanish being dominant. Some are endangered, spoken by elderly people whose descendants do not speak the languages. There is evidence of some now extinct languages.\n\nArgentina is predominantly a Spanish-speaking country — the fourth largest after Mexico, Spain, and Colombia (according to a compilation of national census figures and United Nations estimates, see List of countries with Spanish-speaking populations). Based on the 2010 national census and supporting research, there are about 40.9 million Spanish speakers in Argentina (almost the entire population).\n\nArgentina is one of several Spanish-speaking countries (along with Uruguay, Paraguay, El Salvador, Nicaragua, Honduras and Costa Rica) that almost universally use what is known as \"voseo\"—the use of the pronoun \"vos\" instead of \"tú\" (the familiar \"you\") as well as its corresponding verb forms. The most prevalent dialect is \"Rioplatense\", whose speakers are located primarily in the basin of the Río de la Plata.\n\nA phonetic study conducted by the Laboratory for Sensory Investigations of [CONICET] and the University of Toronto showed that the intonation Porteño Spanish is unlike that of other Spanish varieties, and suggested that it may be a result of convergence with Italian. Italian immigration influenced \"Lunfardo\", the slang spoken in the Río de la Plata region, permeating the vernacular vocabulary of other regions as well.\n\nAs in other large countries, the accents vary depending on geographical location. Extreme differences in pronunciation can be heard within Argentina. One notable pronunciation difference found in Argentina is the “sh” sounding y and ll. In most Spanish speaking countries the letters y and ll are pronounced somewhat like the “y” in yo-yo, however in most parts of Argentina they are pronounced like “sh” in English (such as \"shoe\") or like \"zh\" (such as the sound the <nowiki> makes in \"measure\").</nowiki>\n\nAs previously mentioned \"voseo\" is commonly used in Argentina. See the article on voseo for more details.\n\nIn many of the central and north-eastern areas of the country the trilled /r/ takes on the same sound as the <ll> and <y> ('zh' - a voiced palatal fricative sound, similar to the \"s\" in the English pronunciation of the word \"vision\".) For Example, “Río Segundo” sounds like “Zhio Segundo” and “Corrientes” sounds like “Cozhientes”.\n\nThe ISO639 code for Argentine Spanish is \"es-AR\".\n\nArgentina has more than 1,500,000 Italian speakers; this tongue is the second most spoken language in the nation as a native language. Italian immigration, which effectively began in the middle of the 19th century and reached its peak in the first two decades of the 20th century, made a lasting and significant impact on the pronunciation and vernacular of Argentina's variety of Spanish, giving it an Italian flair. In fact, Italian dialects (not Standard Italian) have contributed so much to Rioplatense that many foreigners mistake it for Italian.\n\nThere are around one million Levantine Arabic speakers in Argentina, as a result of immigration from the Middle East, mostly from Lebanon, Syria, and Palestine.\n\nSouth Bolivian Quechua is a Quechuan language spoken by some 800,000 people, mostly immigrants who have arrived in the last years. There are an estimated 70,000 speakers in Salta Province. The language is also known as Central Bolivian Quechua, which has six dialects. It is classified as a Quechua II language and is referred to as Quechua IIC by linguists.\n\nStandard German is spoken by between 400,000 and 500,000\n\nThere are around 200,000 Yiddish speakers in Argentina.\n\nGuaraní is spoken by 200,000 people, mostly in Corrientes (where it is official \"de jure\") and Misiones.\n\nThere are 174,000 speakers of the Catalan language.\n\nMapudungun is spoken by 100,000 Mapuche people in the provinces of Neuquén, Río Negro, Chubut, Buenos Aires, and La Pampa.\n\nChinese is spoken by at least half of the over 60,000 Chinese immigrants, mostly in Buenos Aires.\n\nWichí is an indigenous language spoken by 53,700 people, mainly in Chaco where, along with Kom and Moqoit, it is official \"de jure\".\n\nVlax Romani is spoken by 52,000 people.\n\nJapanese is spoken by 32,000 people.\n\nUkrainian is spoken by 27,000 people.\n\nWelsh is spoken by over 5,000 people in Chubut province. Some districts have recently incorporated it as an educational language.\n\nMocoví is spoken by 4,525 people in Santa Fe Province, while Mbyá Guaraní has 3,000 speakers in the northeast. Pilagá is spoken by about 2,000 people in the Chaco. There are 1,500 Iyo'wujwa Chorote speakers, 50% of whom are monolingual; Iyo'wujwa Chorote is spoken in the Chaco region and along the Pilcomayo river.\n\nSeveral Native American languages spoken in Argentina by the native people (1% of the population) are declining at rates that may result in only a handful of speakers within a generation. Kaiwá has 512 speakers, Nivaclé 200, Tapieté and Wichí Lhamtés Nocten only 100. These indigenous languages have suffered slow linguistic and cultural genocide. In this category in terms of number of speakers, one can also include some immigrant languages for instance Plautdietsch with only 140.\n\nSome languages are critically endangered, spoken only by a handful of isolated elderly people whose children do not speak the language; they are likely to become dead languages once the remaining speakers die. Vilela has about 20 speakers; Puelche has 5 or 6 speakers; in 2000 Tehuelche had 4 speakers, out of about 200 ethnic Tehuelche people, (2000 W. Adelaar); and in 1991 Selk'nam (also known as Ona) had 1 to 3 speakers and is nearly extinct; full blooded Ona people are already extinct. Most endangered speakers speak .\n\nAbipón, Cacán, Chané and Haush are now extinct languages that were spoken by people indigenous to Argentina before European contact. Very little is known of Cacán and Chané.\n\nThe Abipón language was a Native American language of the Mataco–Guaycuru family that was spoken by the Abipón people.\n\nCacán was spoken by Diaguita and Calchaquí aboriginals, and became extinct during the late 17th century or early 18th century; the only manuscript documenting this language was lost, and now there is not enough information to make it possible to link it to any existing language family.\n\nChané was spoken in the Salta Province, and was either a dialect of or closely related to the Terena language of the Arawakan language family.\n\nThe Haush language, belonging to the Chonan family, was an indigenous language spoken by the Haush people and was formerly spoken on the island of Tierra del Fuego.\n\nCocoliche, a Spanish-Italian creole, was spoken mainly by first and second-generation immigrants from Italy, but is no longer in daily use; it is sometimes used in comedy. Some Cocoliche terms were adopted into Lunfardo slang.\n\nOccitan, Turoyo, Ukrainian, and Vlax Romani are all reportedly spoken, but the number of speakers are not known. Many Aymará speakers have migrated to Argentina for sugar mill and other work; of more than 2.2 million speakers globally, many are in Argentina. There are Mandarin-, Cantonese-, Japanese-, Hindustani (Hindi-Urdu)-, Korean-, Slovak- and Russian-speaking immigrant communities. Chiripá is also spoken. The Argentinien-schwyzertütsch dialect (Argentine-Swiss German dialect) was introduced by Swiss immigrants.\n\n\n\n"}
{"id": "43517909", "url": "https://en.wikipedia.org/wiki?curid=43517909", "title": "Le Cercle des Gourmettes", "text": "Le Cercle des Gourmettes\n\nLe Cercle des Gourmettes was an exclusive club or \"circle\" of women in Paris devoted to gourmet food. The circle was started in 1929 by an American, Paulette Edlinger, and the name may have been a bon mot in protest of the non-feminine form of the word \"gourmet\".\n\nThe club began at a time of economic prosperity just prior to the Great Depression in France. It was started by the wives of members of Club des Cent and convened on alternate Fridays to enjoy lunches and dinners at the electric company headquarters in Paris, Electricité et Gaz. The club's most famous members were Julia Child, Simone Beck, and Louisette Bertholle. Each joined the club in the late 1940s.\n\nVersions of the club later existed in Canada and Switzerland.\n"}
{"id": "1368149", "url": "https://en.wikipedia.org/wiki?curid=1368149", "title": "Lishana Deni", "text": "Lishana Deni\n\nLishana Deni is a modern Jewish Aramaic language, often called \"Neo-Aramaic\" or \"Judeo-Aramaic\". It was originally spoken in northern Iraq and southeastern Turkey in the lands west of the Great Zab river (Athura). Following the exodus of Jews from the Muslim lands, most speakers now live in Israel, principally Jerusalem and surrounding villages.\n\nThe name \"Lishana Deni\" means 'our language', and is similar to names used by other Jewish Neo-Aramaic dialects (Lishan Didan, Lishanid Noshan). Other popular names for the language are \"Lishan Hozaye\", 'the language of the Jews', and \"Kurdit\", 'Kurdish'. Scholarly sources tend simply to refer to Lishana Deni as \"Zakho Jewish Neo-Aramaic\" although it was spoken in the entire region west of the Great Zab river.\n\nVarious Neo-Aramaic dialects were spoken across a wide area from the Zakho region, in the west, to Lake Urmia, in the northeast to Sanandaj, in the southeast (the area covers northern Iraq and northwestern Iran). The upheavals in their traditional region after the First World War and the founding of the State of Israel led most of the Jews of Kurdistan to move to Jerusalem and villages nearby.\n\nHowever, uprooted from northern Iraq, and thrown together with so many different language groups in the fledgling nation, Lishana Deni began to be replaced in the speech of younger generations by Modern Hebrew.\n\nFewer than 8,000 people are known to speak Lishana Deni, and all of them are over 50 years old. Lishana Deni is written in the Hebrew alphabet. Spelling tends to be highly phonetic, and elided letters are not written.\n\nThe language faces extinction in the next few decades. Although there is very little intelligibility between Lishana Deni and the other Jewish dialects, there is quite reasonable intelligibility between it and the Christian Neo-Aramaic dialects spoken in the region.\n\nThe Christian dialect of Chaldean Neo-Aramaic is closest to Lishana Deni, followed by the \"Ashiret\" dialects of Assyrian Neo-Aramaic. Like other Judaeo-Aramaic dialects, \"Lishana Deni\" is sometimes called \"Targumic\", due to the long tradition of translating the Hebrew Bible into Aramaic, and the production of targumim.\n\nLishana Deni was spoken in Athura (which means Assyria in NENA dialects), which is located west of the Great Zab river in northern Iraq and southeastern Turkey. Most Lishana Deni speakers are rural and were farmers and shepherds but there are urban speakers as well in cities such as Nohadra, Zakho, Amedya and more.\n\nThe regions where Lishana Deni was spoken are Bohtan, Zakho and Nineveh Plains in Upper Mesopotamia, as well as Nerwa, Sapna, Barwari and Hakkari mountains.\n\n\n\n"}
{"id": "13700748", "url": "https://en.wikipedia.org/wiki?curid=13700748", "title": "Microaggression", "text": "Microaggression\n\nA microaggression is a term used for brief and commonplace daily verbal, behavioural, or environmental indignities, whether intentional or unintentional, that communicate hostile, derogatory, or negative prejudicial slights and insults toward any group. The term was coined by psychiatrist and Harvard University professor Chester M. Pierce in 1970 to describe insults and dismissals which he regularly witnessed non-black Americans inflicting on African Americans. By the early 21st century, use of the term was applied to the casual degradation of any socially marginalized group, including LGBT, the poor and the disabled. Psychologist Derald Wing Sue defines microaggressions as \"brief, everyday exchanges that send denigrating messages to certain individuals because of their group membership\". The persons making the comments may be otherwise well-intentioned.\n\nA number of scholars and social commentators have critiqued the microaggression concept on various grounds, including that it is not well substantiated scientifically, that it is overly dependent on anecdotal evidence, and it assumes without sufficient evidence that slights perceived by the listener or recipient are always due to bias or prejudice. Critics also suggest that avoiding behaviours which may be interpreted as microaggressions restricts freedom and may itself cause emotional distress, and further that this robs both sides of the practice in skills to mediate disputes. Some suggest that while microaggressions are minor relative to serious offences (e.g. assault), and may be unintentional; they may be abused to exaggerate harm, resulting in disproportionate retribution. Some of these critics suggest that the concept is not yet developed well enough to be applied in the real world and that its current applications may be harmful to individuals and society.\n\nMicroaggressions have been defined as brief and common daily verbal, behavioral, and environmental communications, whether intentional or unintentional, that transmit hostile, derogatory, or negative messages to a target person because they belong to a stigmatized group. Although these communications typically appear harmless to observers, they are considered a form of covert racism or everyday discrimination. Microaggressions differ from what Pierce referred to as “macroaggressions”, which are more extreme forms of racism (such as lynchings or beatings) due to their ambiguity, size and commonality. Microaggressions are experienced by most stigmatized individuals and occur on a regular basis. These can be particularly stressful for people on the receiving end as they are easily denied by those committing them. They are also harder to detect by members of the dominant culture, as they are often unaware they are causing harm. Sue describes microaggressions as including statements that repeat or affirm stereotypes about the minority group or subtly demean its members. Such comments also position the dominant culture as normal and the minority one as aberrant or pathological, express disapproval of or discomfort with the minority group, assume that all minority group members are the same, minimize the existence of discrimination against the minority group, seek to deny the perpetrator's own bias, or minimize real conflict between the minority group and the dominant culture.\n\nIn conducting two focus groups with Asian-Americans, for instance, Sue proposed eight distinct themes of racial microaggression:\n\n\nIn a 2017 peer-reviewed review of the literature, Scott Lilienfeld critiqued microaggression research for hardly having advanced beyond taxonomies such as the above, which was proposed by Sue nearly ten years ago. While acknowledging the reality of \"subtle slights and insults directed toward minorities\", Lilienfeld concluded that the concept and programs for its scientific assessment are \"far too underdeveloped on the conceptual and methodological fronts to warrant real-world application\". He recommended abandonment of the term \"microaggression\" since \"the use of the root word 'aggression' in 'microaggression' is conceptually confusing and misleading\". In addition, he called for a moratorium on microaggression training programs until further research can develop the field.\n\nIn 2017 Althea Nagai, who works as a research fellow at the conservative Center for Equal Opportunity, published an article criticizing microaggression research as pseudoscience. Nagai said that the prominent critical race researchers behind microaggression theory \"reject the methodology and standards of modern science.\" She lists various technical shortcomings of microaggression research, including \"biased interview questions, reliance on narrative and small numbers of respondents, problems of reliability, issues of replicability, and ignoring alternative explanations.\"\n\nSocial scientists Sue, Bucceri, Lin, Nadal, and Torino (2007) described microaggressions as \"the new face of racism\", saying that the nature of racism has shifted over time from overt expressions of racial hatred and hate crimes, toward expressions of aversive racism, such as microaggressions, that are more subtle, ambiguous, and often unintentional. Sue says this has led some Americans to believe wrongly that non-white Americans no longer suffer from racism. One example of such subtle expressions of racism is Asian students being either pathologized or penalized as too passive or quiet. Another is a teacher correcting a student's use of \"indigenous\" in a paper by changing it from upper- to lowercase.\n\nAccording to Sue \"et al\"., microaggressions seem to appear in three forms:\n\nSome psychologists have criticized microaggression theory for assuming that all verbal, behavioral, or environmental indignities are necessarily due to bias. Thomas Schacht says that it is uncertain whether a behavior is due to racial bias or is a larger phenomenon that occurs regardless of identity conflict. However, Kanter and colleagues found that microaggressions were robustly correlated to five separate measures of bias. In reviewing the microaggression literature, Scott Lilienfeld suggested that microassaults should probably be struck from the taxonomy because the examples provided in the literature tend not to be \"micro\", but are outright assaults, intimidation, harassment and bigotry; in some cases, examples have included criminal acts. Others have pointed out that what could be perceived as subtle snubs could be due to people have conditions such as autism or social anxiety disorders and assuming ill will could be harmful to these people.\n\nExplicit sexism in society is on the decline, but still exists in a variety of subtle and non-subtle expressions. Women encounter microaggressions in which they are made to feel inferior, sexually objectified, and bound to restrictive gender roles, both in the workplace and in academia, as well as in athletics. Microaggressions based on gender are applied to female athletes when: their abilities are compared only to men, they are judged on \"attractiveness\", and individuals are restricted to or requested to wear \"feminine\" or sexually attractive attire during competition. \n\nGendered microaggressions and more overt aggression can also be found in violent rape pornography. \n\nOther examples of sexist microaggressions are \"[addressing someone by using] a sexist name, a man refusing to wash dishes because it is 'women's work,' displaying nude pin-ups of women at places of employment, someone making unwanted sexual advances toward another person\".\n\nIn focus groups, individuals identifying as bisexual report such microaggressions as others denying or dismissing their self-narratives or identity claims, being unable to understand or accept bisexuality as a possibility, pressuring them to change their bisexual identity, expecting them to be sexually promiscuous, and questioning their ability to maintain monogamous relationships. Transgender people classify being labelled as being of a gender other than the one with which they identify as an example of microaggression.\n\nSome LGBT individuals report receiving expressions of microaggression from people even within the LGBT community. They say that being excluded, or not being made welcome or understood within the gay and lesbian community is a microaggression. Roffee and Waling suggest that the issue arises, as occurs among many groups of people, because a person often makes assumptions based on individual experience, and when they communicate such assumptions, the recipient may feel that it lacks taking the second individual into account and is a form of microaggression.\n\nPeople who are members of overlapping marginal groups (e.g., a gay Asian-American man or a trans woman) experience microaggressions based in correspondingly varied forms of marginalization.\nFor example, in one study Asian-American women reported feeling they were classified as sexually exotic by majority-culture men or were viewed by them as potential trophy wives simply because of their group membership. African-American women report microaggressions related to characteristics of their hair, which may include invasion of personal space as an individual tries to touch it, or comments that a style that is different from that of a European-American woman looks \"unprofessional\".\n\nPeople with mental illness report receiving more overt forms of microaggression than subtle ones, coming from family and friends as well as from authority figures. In a study involving college students and adults who were being treated in community care, five themes were identified: invalidation, assumption of inferiority, fear of mental illness, shaming of mental illness, and being treated as a second-class citizen.\n\nMembers of marginalized groups have also described microaggressions committed by performers or artists associated with various forms of media, such as television, film, photography, music, and books. Some researchers believe that such cultural content reflects but also molds society, allowing for unintentional bias to be absorbed by individuals based on their media consumption, as if it were expressed by someone with whom they had an encounter. \n\nA study of racism in TV commercials describes microaggressions as gaining a cumulative weight, leading to inevitable clashes between races due to subtleties in the content. As an example of a racial microaggression, or microassault, this research found that black people were more likely than white counterparts to be shown eating or participating in physical activity, and more likely to be shown working for, or serving others. The research concludes by suggesting that microaggressive representations can be omitted from a body of work, without sacrificing creativity or profit.\n\nPérez Huber and Solorzano start their analysis of microaggressions with an anecdote about Mexican \"bandits\" as portrayed in a children's book read at bedtime. The article gives examples of negative stereotypes of Mexicans and Latinos in books, print, and photos, associating them with the state of racial discourse within majority culture and its dominance over minority groups in the US. The personification of these attitudes through media can also be applied to microaggressive behaviors towards other marginalized groups. \n\nA 2015 review of the portrayal of LGBT characters in film says that gay or lesbian characters are presented in \"offensive\" ways. In contrast, LGBT characters portrayed as complex characters who are more than a cipher for their sexual orientation or identity are a step in the right direction. Ideally, \"queer film audiences finally have a narrative pleasure that has been afforded to straight viewers since the dawn of film noir: a central character who is highly problematical, but fascinating.\"\n\nMicroaggression can target and marginalize any definable group, including those who share an age grouping or belief system. Microaggression is a manifestation of bullying that employs micro-linguistic power plays in order to marginalize any target with a subtle manifestation of intolerance by signifying the concept of \"other\".\n\nBecause perpetrators may be well-meaning and microaggressions are subtle, the recipients often experience attributional ambiguity, which may lead them to dismiss the event and blame themselves as overly sensitive to the encounter. If challenged by the minority person or an observer, perpetrators will often defend their microaggression as a misunderstanding, a joke, or something small that should not be blown out of proportion.\n\nA 2013 scholarly review of the literature on microaggressions concluded that \"the negative impact of racial microaggressions on psychological and physical health is beginning to be documented; however, these studies have been largely correlational and based on recall and self-report, making it difficult to determine whether racial microaggressions actually cause negative health outcomes and, if so, through what mechanisms\". A 2017 review of microaggression research pointed out that as scholars try to understand the possible harm caused by microaggressions, they have not conducted much cognitive or behavioural research, nor much experimental testing, and they have overly relied on small collections of anecdotal testimonies from samples who are not representative of any particular population.\n\nRecipients of microaggressions may feel anger, frustration, or exhaustion. African-Americans have reported feeling under pressure to \"represent\" their group or to suppress their own cultural expression and \"act white\". Over time, the cumulative effect of microaggressions is thought by some to lead to diminished self-confidence and a poor self-image for individuals, and potentially also to such mental-health problems as depression, anxiety, and trauma. Many researchers have argued that microaggressions are more damaging than overt expressions of bigotry precisely because they are small and therefore often ignored or downplayed, leading the victim to feel self-doubt for noticing or reacting to the encounter, rather than justifiable anger, and isolation rather than support from others about such incidents. Studies have found that in the U.S. when people color perceived microaggressions from mental health professionals, client satisfaction with therapy is lower.\nSome studies suggest that microaggressions represent enough of a burden that some people of color may fear, distrust, and/or avoid relationships with white people in order to evade such interaction. On the other hand, some people report that dealing with microaggressions has made them more resilient. Scholars have suggested that, although microaggressions \"might seem minor\", they are \"so numerous that trying to function in such a setting is 'like lifting a ton of feathers.\n\nKenneth R. Thomas claimed in \"American Psychologist\" that recommendations inspired by microaggression theory, if \"implemented, could have a chilling effect on free speech and on the willingness of White people, including some psychologists, to interact with people of color.\" Sociologists Bradley Campbell and Jason Manning have written in the academic journal \"Comparative Sociology\" that the microaggression concept \"fits into a larger class of conflict tactics in which the aggrieved seek to attract and mobilize the support of third parties\" that sometimes involves \"building a case for action by documenting, exaggerating, or even falsifying offenses\". The concept of microaggressions has been described as a symptom of the breakdown in civil discourse, and that microaggressions are \"yesterday's well-meaning faux pas\".\n\nOne suggested type of microaggression by an Oxford University newsletter was avoiding eye contact or speaking directly to people. This spurred a controversy when it was pointed out that such assumptions are insensitive to autistic people who may have trouble making eye contact.\n\nIn their article \"Microaggression and Moral Cultures\", sociologists Bradley Campbell and Jason Manning say that the discourse of microaggression leads to a culture of victimhood. Social psychologist Jonathan Haidt states that this culture of victimhood lessens an individual's \"ability to handle small interpersonal matters on one's own\" and \"creates a society of constant and intense moral conflict as people compete for status as victims or as defenders of victims\". Similarly, John McWhorter, linguist and social commentator suggests that \"it infantilizes black people to be taught that microaggressions, and even ones a tad more macro, hold us back, permanently damage our psychology, or render us exempt from genuine competition.\"\n\nIn \"The Atlantic\", Greg Lukianoff and Jonathan Haidt expressed concern that the focus on microaggressions can cause more emotional trauma than the experience of the microaggressions at the time of occurrence. They believe that self-policing by an individual of thoughts or actions in order to avoid committing microaggressions may cause emotional harm as a person seeks to avoid becoming a microaggressor, as such extreme self-policing may share some characteristics of pathological thinking. Referring especially to prevention programs at schools or universities, they say that the element of protectiveness, of which identifying microaggression allegations are a part, prepares students \"poorly for professional life, which often demands intellectual engagement with people and ideas one might find uncongenial or wrong\". They also said that it has become \"unacceptable to question the reasonableness (let alone the sincerity) of someone's emotional state\", resulting in adjudication of alleged microaggressions having characteristics of witch trials.\n\nWriting for \"The Federalist\", Paul Rowan Brian argued that microaggression theory pools trivial and ignorable instances of racism with real, genuine prejudice and exclusion. Amitai Etzioni, writing in \"The Atlantic\", suggested that attention to microaggressions distracts individuals and groups from dealing with much more serious acts. \n\nRalph Nader has similarly criticized the concepts of announcement of trigger warnings and political correctness on campuses as creating too much sensitivity. Viv Regan, writing for \"Spiked Online\", wondered whether the comfort provided by having a convenient label for alleged rudeness outweighs the damage caused by overreaction.\n\nAccording to Derald Wing Sue, whose works popularized the term, many critiques are based on the term being misunderstood or misused. He said that his purpose in identifying such comments or actions was to educate people and not to silence or shame them. He further notes that, for instance, identifying that someone has used racial microaggressions is not intended to imply that they are racist.\n"}
{"id": "8411029", "url": "https://en.wikipedia.org/wiki?curid=8411029", "title": "Moral police", "text": "Moral police\n\nMoral police is a blanket term used to describe vigilante groups which act to enforce a code of morality in India. Some of India's laws, and some actions of police forces in India are also considered to be instances of moral policing. The target of moral policing is any activity that vigilante groups, the government or police deem to be \"immoral\" and/or \"against Indian culture\".\n\nIndia has several vigilante groups that claim to protect the Indian culture. They resist and oppose cultural concepts that they deem to have been imported from the Western culture. They have been known to attack bars and pubs. Some of these groups have attacked or have forced to shut down art exhibitions, where they claim obscene paintings were being displayed. They have issued diktats against western attires. Some have also condemned beauty parlours. Some members of the media have also colluded with such groups. Some politicians have supported such viewpoints and occasionally such activities.\n\nIn India, the Sections 292 to 294 of the Indian Penal Code are used to deal with obscenity. Most of these laws date back to 1860. The Section 292 of the Indian Penal Code deals with sales and distribution of obscene books and other material. It criminalises materials like books and paintings if it is deemed to be \"lascivious or appeals to the prurient interest\". The Section 292 was amended in 1969 to exclude material that are for public good (like condom ads), scientific material, art and religious figures. Police also use Section 292 of the IPC to file cases against film posters and advertisement hoardings that are deemed to be \"obscene\".\n\nThe Section 293 deals with the sale of obscene material to people under 20. The Section 294 of the Indian Penal Code deals with \"obscene acts and songs\" and it states that:\n\nThere is no proper definition of an obscene act and it is open to interpretation. It is frequently used by the police to justify acts of moral policing.\n\nImmoral Traffic (Prevention) Act, 1956 (also known as Prevention of Immoral Traffic Act or PITA) was originally passed to prevent human trafficking. It allows police to raid hotels if they suspect a sex racket is being run there. Police have used this law to raid hotels and arrest consenting couples.\n\nIndia's obscenity laws have also been frequently compared to the Hicklin test.\n\nGroups like Shiva Sena, Valentine's Day is often opposed by the moral police for being a western import. Vigilante group have been known to attack gift and card shops prior to the occasion. Couples are often beaten up for holding hands or kissing in public.\n\nShiv Sena leader Uddhav Thackeray has called it an attack of the west on Indian culture and that it is attracting youth for commercial gain. Shiv Sena leader Bal Thackeray has said that people not wanting violence on the day should not celebrate it. He has also called the festival shameless and contrary to Indian culture. Occasionally, the police also try to restrict these groups but even though their activities flourish till the date.\n\n\n\nThe Central Board of Film Certification (CBFC) or the Censor Board, which is tasked with regulating the public exhibition of films under the provisions of the \"Cinematograph Act, 1952\", has been accused of moral policing by some filmmakers. Director Anurag Kashyap has argued that it is infeasible to have a single body for a large and diverse country like India. Director Prakash Jha has pointed out that even if a film is certified by the Board, it is often not allowed a release in some states due to protests from local political parties or moral police. He has also said that the Board should be scrapped and each film-maker should simply state the type of content in the film because the society is mature enough to understand it. Sudhir Mishra has noted that censor committees have been influenced to giving films lighter ratings.\n\nThe former chief of the Censor Board, Sharmila Tagore, has defended the body saying that it does not carried out moral policing. In August 2014, then chief of the Censor Board, Rakesh Kumar, was arrested for allegedly delaying certifications to films and demanding bribes to speed up the process.\n\nThroughout India, restrictions have been place by some state governments on timings for pubs, bars and other establishments that sell liquor.\n\n\nThe Adolescence Education Programme (AEP) was a sex education program designed by the Ministry of Human Resource Development (India) and National AIDS Control Organisation (NACO) to implement the policies of the National AIDS Control Programme II (NACP II). However, it faced opposition in various states, including Gujarat, Madhya Pradesh, Maharashtra, Karnataka, and Rajasthan.\n\n\n\n\n\nIn 2009, following the 2009 Mangalore pub attack, an organisation called \"Consortium of Pubgoing, Loose Forward Women\" started a movement called the \"Pink chaddi campaign\". The movement requested people to mail pink underwear to Pramod Muthalik the leader of Sri Ram Sena which was behind the attacks. About, 34,000 people participated.\n\nIn the state of Kerala, a public hugging and kissing campaign by name 'Kiss of Love' was launched in protest against moral policing on 2 November 2014. Similar events were later organized in Delhi, Kolkata, and various other cities.\n\n"}
{"id": "52438344", "url": "https://en.wikipedia.org/wiki?curid=52438344", "title": "Municipal disinvestment", "text": "Municipal disinvestment\n\nMunicipal disinvestment is a term in the United States which describes an urban planning process in which that a city or town or other municipal entity decides to abandon or neglect an unproductive zone. It can happen when a municipality is in a period of economic prosperity, and it seeks to improve its communities and infrastructure, and sees that its poorest and most blighted communities are both the cheapest targets for revitalization as well as the areas with the greatest potential for improvement. When a city is facing urban decay, and has to compromise in allocating its resources, the poorest communities are the least profitable and investing in them seems unwise from a long term perspective, and so these communities are neglected as a matter of choice. Disenfranchised neighborhoods are slated for demolition, relocation, and eventual replacement. According to one view, disinvestment in urban and suburban communities tends to fall strongly along racial and class lines, and intensifies and perpetuates the cycle of poverty exerted upon the space, since more affluent individuals with social mobility can leave the disenfranchised areas. According to one view, municipal disinvestment is a direct consequence of local, state, and federal urban planning and urban renewal projects.\n\nOut of the New Deal came the Public Works Administration, funding the construction of thousands of low-rent homes and infrastructure development. Meanwhile the Homeowners Refinancing Act, as well as the Wagner-Steagall Housing Act three years later, provided generous incentives and reimbursements aimed at Americans recovering from the Great Depression who could not yet afford to invest in equity. In the postwar era, returning veterans were seeking homes to start families. There was a period of intense residential expansion surrounding major US cities, and banks were gratuitously providing loans in order for families to afford moving there. It is in this new economic landscape where redlining, exclusionary zoning, and predatory lending flourished.\n\nThe programs enacted during this time provided more displacement than they did replacement: the beginning years of housing and infrastructure development were defined by their clearance and destruction of communities. The Housing Act of 1949 increased the mandate for public housing, but in claiming to combat \"slums\" and \"blight\" the act was worded so vaguely that those terms could have been applied to most any post-Depression urban neighborhood. What was intended to rebuild in decayed communities was used against poor but otherwise flourishing neighborhoods by labeling them ghettos.\n\nThe Federal Aid Highway Act of 1956 extended demolition of neighborhoods, with new roads cutting through the most vulnerable neighborhoods in order to create more direct arteries between the metropolitan and the downtown areas. The highway's construction expanded upon the already widening schism of urban poor and the suburban by further enabling white flight and reducing a focus on public transport.\n\nDuring the postwar era, municipalities sought to grow enriched and modernized communities from the slums they demolished. As the Civil Rights movement was in full display through highway revolts and responses to racial violence, there was a growing mindset among urban planners that a communal-focused, people-first approach should be taken, along the same lines as community development handled by the recently-enacted Peace Corps. Lyndon B. Johnson continued the policies of his predecessor John F. Kennedy, and in the first year in office he signed into law the Civil Rights Act of 1964, the Voting Rights Act of 1965, and followed soon after with a series of bills that comprised the foundation of what was termed the \"War on Poverty\" and \"Great Society\", despite the protesting of a largely anti-integration Congress. The new philosophy of the administration focused intently on Community Action Agencies, fulfilling the demand from modernist social theorists and pouring funding and resources into volunteer forces. The direction of policy was seen as some as a form of direct investment in impoverished and minority neighborhoods, in contrast to the previous focus on new construction.\n\nDaniel Patrick Moynihan served as Assistant Secretary of Labor under the Johnson Administration, and was a primary influence on policy development. Stemming from his controversial Moynihan Report, many of the programs enacted within the War on Poverty intended to educate black and poor families in order to modernize their \"culture.\" Government assistance, in the hundreds of millions of dollars, was intended for organic community growth, nurturing local governance, and a gradual transition from developing to developed urban regions. However, when shirking programs that were directly-run by municipalities, the money was diverted to smaller, unorthodox community action associations with unions or \"social protest agendas\". For example, it was an Office of Economic Opportunity-funded community center where the Black Panther Party started its development.\n\nJohnson responded to the radicalization of Black Americans Watts Riots by ceding control of local OEO's to municipal authorities such as the mayor (a reversal of the original strategy of community-lead development) while funding was reduced and the practices of the offices and local community projects were more closely supervised. Moynihan was startled by what he perceived as the consequences of the War on Poverty and changed his philosophy and its practice under Johnson. He found the social policies of the past decades naive for trying to fix the \"tangled pathology\" he described in the white paper he authored for the US Department of Labor, \"The Negro Family: The Case for National Action\". When the administration transitioned over to Nixon, Moynihan remained as Counselor to the President, where he further pushed for dismantling the OEO. Though it continued to exist into the Reagan Administration, the OEO was put under the control of Donald Rumsfeld and Dick Cheney, who more tightly controlled its function. It is during this time that Moynihan suggests to Nixon that Black communities be treated with a \"benign neglect,\" a philosophy-of-action which would later be translated into the planned shrinkage policies of the 70s and 80s.\n\nBenign neglect is a policy proposed in 1969 by Daniel Patrick Moynihan, who was at the time on President Richard Nixon's staff as an urban affairs adviser. While serving in this capacity, he sent the President a memo suggesting, \"The time may have come when the issue of race could benefit from a period of 'benign neglect.' The subject has been too much talked about. The forum has been too much taken over to hysterics, paranoids, and boodlers on all sides. We need a period in which Negro progress continues and racial rhetoric fades.\" The policy was designed to ease tensions after the Civil Rights Movement of the late 1960s. Moynihan was particularly troubled by the speeches of Vice-President Spiro Agnew. However, the policy was widely seen as an abandonment of urban neighborhoods, particularly ones with a majority black population, as Moynihan's statements and writings appeared to encourage, for instance, fire departments engaging in triage to avoid a supposedly futile war against arson.\n\nPlanned shrinkage is a controversial public policy of the deliberate withdrawal of city services to blighted neighborhoods as a means of coping with dwindling tax revenues. Planned shrinkage involves decreasing city services such as police patrols, garbage removal, street repairs, and fire protection, from selected city neighborhoods suffering from urban decay, crime, and poverty. While it has been advocated as a way to concentrate city services for maximum effectiveness given serious budgetary constraints, it has been criticized as an attempt to \"encourage the exodus of undesirable populations\" as well as to open up blighted neighborhoods for development by private interests. Planned shrinkage was mentioned as a development strategy for the South Bronx section of New York City in the 1970s, and more recently for another urban area in the United States, the city of New Orleans. The term was first used in New York City in 1976 by Housing Commissioner Roger Starr.\n\nDuring the twentieth century, a boom in suburban growth caused in part by increased automobile use led to urban decline, particularly in the poorer sections of many large cities in the United States and elsewhere. A dwindling tax base depleted many municipal resources. A common view was that it was part of a \"downward spiral\" caused first by an absence of jobs, the creation of a permanent underclass, and a declining tax base hurting many city services, including schools. It was this interplay of factors which made change difficult. New York City was described as \"so broke\" by the 1970s with neighborhoods which had become \"so desperate and depleted\" that municipal authorities wondered how to cope. Some authorities felt the process of decline was inevitable, and instead of trying to fight it, searched for alternatives. According to one view, authorities searched for ways to have the greatest population loss in the areas with the poorest non-white populations.\n\nIn the early 1970s, a RAND study examining the relation between city services and large city populations concluded that when services such as police and fire protection were withdrawn, the numbers of people in the neglected areas would decrease. There had been questions about many fires that had been happening in the South Bronx during the 1970s. One account (including the RAND report) suggested that neighborhood fires were predominantly caused by arson, while a contrasting report suggested that arson was not a major cause. If arson had been a primary cause, according to the RAND viewpoint, then it did not make sense financially for the city to try to invest further funds to improve fire protection, according to this view. The RAND report allegedly influenced then Senator Daniel Patrick Moynihan, who used the report's findings to make recommendations for urban policy. In Moynihan's view, arson was one of many social pathologies caused by large cities, and suggested that a policy of \"benign neglect\" would be appropriate as a response.\n\nPartly in response to the RAND report, and in an effort to address New York's declining population, New York's housing commissioner, Roger Starr, proposed a policy which he termed \"planned shrinkage\" to reduce the impoverished population and better preserve the tax base. According to the \"politically toxic\" proposal, the city would stop investing in troubled neighborhoods and divert funds to communities \"that could still be saved.\" He suggested that the city should \"accelerate the drainage\" in what he called the worst parts of the South Bronx through a policy of planned shrinkage by closing subway stations, firehouses and schools. Starr felt these actions were the best way to save money. Starr's arguments soon became predominant in urban planning thinking nationwide. The people who lived in the communities where his policies were applied protested vigorously; without adequate fire service and police protection, the residents faced waves of crime and fires that left much of the South Bronx and Harlem devastated. A report in 2011 in the \"New York Times\" suggested that the planned shrinkage approach was \"short-lived\". Under the planned shrinkage program, for example, an abandoned 100-unit development on one piece of land could be cleared by a real estate developer, and such an outcome would have been preferable to ten separate neighborhood-based efforts to produce 100 housing units each, according to advocates of planned shrinkage. According to this view, a planned shrinkage approach would encourage so-called \"monolithic development\", resulting in new urban growth but at much lower population densities than the neighborhoods which had existed previously. The remark by Starr caused a political firestorm: then mayor Abraham Beame disavowed the idea while City Council members called it \"inhuman,\" \"racist\" and \"genocidal.\"\n\nAccording to one report, the high inflation during the 1970s combined with the restrictive rent control policies in the city meant that buildings were worth more \"dead\" for the insurance money than \"alive\" as sources of rental income; as investments, they had limited ability to provide a solid stream of rental income. Accordingly, there was an economic incentive on the part of building owners, according to this view, to simply let the buildings burn. An alternative view was that the fires were a result of the city's municipal policies. While there are differing views about whether planned shrinkage caused fire outbreaks in the 1970s, or was a result of such fires, there is agreement that the fires in the South Bronx during these years were extensive.\n\nBy the mid-1970s, The Bronx had 120,000 fires per year, for an average of 30 fires every 2 hours. 40 percent of the housing in the area was destroyed. The response time for fires also increased, as the firefighters did not have the resources to keep responding promptly to numerous service calls. A report in \"The New York Post\" suggested that the cause of the fires was not arson but resulted from decisions by bureaucrats to abandon sections of the city. According to one report, of the 289 census tracts within the borough of the Bronx, seven census tracts lost more than 97% of their buildings, and 44 tracts lost more than 50% of their buildings, to fire and abandonment.\n\nThere have been claims that planned shrinkage impacted public health negatively. According to one source, public shrinkage programs targeted to undermine populations of African-Americans and Hispanic-Americans in the South Bronx and Harlem had an effect on the geographic pattern of the AIDS outbreak. According to this view, municipal abandonment was interrelated with health issues and helped to cause a phenomenon termed \"urban desertification\".\n\nThe populations in the South Bronx, Lower East Side, and Harlem plummeted during the two decades after 1970. Only after two decades did the city begin to invest in these areas again.\n\nNew Orleans differed from other cities in that the cause of decline was not based on economic or political shifts but rather a destructive flood caused by a hurricane. In the aftermath of Hurricane Katrina, \"planned shrinkage\" was proposed as a means to create a \"more compact, more efficient and less flood-prone city\". Areas of the city which were most damaged by the flooding - and thus, most likely to be flooded again - would not be rebuilt and would become green space. These areas were frequently less desirable, lower-income areas which had lower property values precisely because of the risk of flooding. Some residents rejected a \"top-down\" approach of \"planned shrinkage\" of municipal planners and attempted to rebuild in flood-prone areas.\n\nPlanned shrinkage in Roxbury is not unique to the RAND policies enacted in the 70s and 80s. The area succumbed to numerous fires as out-of-town landlords sought out the only way to earn back some profit on homes that no longer sold. However, the neighborhood's response to planned shrinkage through community action has made it an example to other neighborhoods of the success of people-first organizing. The neighborhood had worked with the Boston's administration, but refused to give in to bureaucratic control by the city, protesting whenever the municipality neglected their redevelopment.\n\n\"Shrink to survive\" is a contemporary form of planned shrinkage which emphasizes short-term razing of the city: Where a city enacting planned shrinkage policies takes a Laissez-faire approach to disinvesting in its communities, cities can take an active role in that reduction. This includes investing in more aggressive land buyback and enforcement of eminent domain in order to obtain ownership of a property, relocate its residents, and demolish it. \n\nSuch proposals, which began around 2009, entail razing entire districts within some cities or else bulldozing them to return the land to its pre-construction rural state. The policies have been studied not only by municipal and state authorities but also by the federal government, and may affect dozens of declining cities in the United States. One report suggested that 50 U.S. cities were potential candidates, and include Detroit, Philadelphia, Pittsburgh, Baltimore and Memphis. Proponents claim the plan will bring efficiency with less waste and fraud; detractors complain the policy has been a \"disaster\" and advocate for community-based efforts instead.\n\nShrink to survive was initiated by Dan Kildee during his term as Treasurer of Genesee County, Michigan. He proposed it as a way to handle municipal problems in Flint, which had experienced an exodus of people and business during the automobile industry downturn. Flint had been described as one of the poorest Rust Belt cities. One estimate was that its population had declined by half since 1950. In 2002, authorities established a \"municipal land bank\" to buy abandoned or foreclosed homes to prevent them from being bought up by real estate speculators. One report was that by the summer of 2009, 1,100 homes in Flint had been bulldozed and that another 3,000 had been scheduled for demolition. One estimate was that the city's size would shrink by twenty per cent, while a second estimate was that it needed to contract by 40% to once again become viable financially.\n\nShrink to survive has been enacted in other medium-sized cities in the American Rust belt such as the Michigan city Benton Harbor, as well as the Ohio city of Youngstown. One report suggested that city authorities in Youngstown had demolished 2,000 derelict homes and businesses. In addition, shrink-to-survive has been considered for inner city suburbs of Detroit.\n\nMunicipal authorities in Gary, Indiana are considering plans to shrink the city by 40%, possibly by demolition or possibly by letting Nature overgrow abandoned buildings, as a way to raise values for existing structures, reduce crime, and restore the city to fiscal health. The city has suffered a sustained decline in job losses and a consequent housing bust.\n\n\n\n"}
{"id": "10378593", "url": "https://en.wikipedia.org/wiki?curid=10378593", "title": "Murder of Harvey and Jeannette Crewe", "text": "Murder of Harvey and Jeannette Crewe\n\nDavid Harvey Crewe (20 October 1941 – 17 June 1970), known as Harvey, and Jeannette Lenore Crewe (6 February 1940 – 17 June 1970) were a New Zealand farming couple (married 18 June 1966 in Auckland) who were shot to death in their home around 17 June 1970. The murders led to the wrongful conviction and subsequent pardoning of another farmer who lived nearby, Arthur Allan Thomas. A Royal Commission set up to investigate the miscarriage of justice found that a detective had fabricated evidence and placed it at the scene of the crime. No person was ever charged with planting the evidence, and the murders remain unsolved.\n\nIn 1970, the Crewes and their 18-month-old daughter lived on their farm at Pukekawa, Lower Waikato. Jeannette was afraid to be in the house without her husband after arson attacks including one in which clothes were set on fire in a bedroom. At the time of her death, Jeannette was about to receive a half share in the neighbouring farm, on which her father, Lenard W. Demler lived alone. The bequest to Jeannette had come about after Jeannette's sister had been cut from the will of their wealthy mother, and Demler had removed Jeannette as a beneficiary of his own will in retaliation, though she had no role in the original matter. Jeannette's mother had then re-written her will to bequeath Jeannette a half share in the farm Demler lived on.\n\nHusband and wife Harvey (28) and Jeannette Crewe (30) were found to be missing from their bloodstained farmhouse at Pukekawa, Lower Waikato on 22 June 1970 by Jeannette's father, Lenard W. Demler (died 4 November 1992) who had been asked to look in on them by an alarmed neighbour because they had not answered the telephone for days. The Crewes' 18-month-old daughter Rochelle was distraught in her cot. Demler left her alone while he went on a farm errand.\n\nThe Crewes had last been seen on the 17th, and milk, bread and newspaper deliveries on the morning of the 18th had not been collected from the letterbox. No medical opinion that an infant could survive without fluids for five days is supported by any verified case of such an occurrence. Although Rochelle had tissue loss, suggesting she had not eaten anything between 17 and 22 June, the degree to which she retained water during treatment indicated that she had not ingested fluids for at most 48 hours before she was found. A witness later reported that he had seen a woman unknown to him on the property on the 19th. Len Demler was the leading suspect due to his propinquity, failure to raise the alarm until prompted, apparent guilty knowledge that Rochelle did not require immediate medical attention, blood of Jeannette's type on his car seat, and a scratch on his neck. His behaviour continued to raise suspicion; during police searches of the countryside for the Crewes, he shadowed on horseback without helping. However, the evidence against Demler was entirely circumstantial and he strongly denied any knowledge of what had happened to his daughter and her husband.\n\nJeannette's body was found on 16 August, wrapped in a duvet bound with copper wire, in the Waikato River and her husband's body was retrieved upriver on 16 September. A car axle linked to a neighbouring farmer, Arthur Allan Thomas, had apparently been used to weigh down Harvey's body and was central to police theories about the case, although it did not justify a prosecution.\n\nBoth victims had been shot to death with a .22 calibre firearm; Jeannette had broken facial bones from being struck with a blunt instrument. Demler had been considered the main suspect, but the brutality of the assault on her, and the lead investigator's belief that she had been raped, led to doubts that her father was involved. On the basis that the murderer might have used a legitimately held gun, police collected and test-fired 64 registered .22 firearms, 3% of the total recorded as held in the Pukekawa area. A forensic report on 19 August 1970 stated that, of the 64, neither Thomas' [rifle] nor one owned by the Eyre family could be eliminated as the possible murder weapon, but there was insufficient evidence pointing to one or the other. Although police suggested to Thomas during an interview that his rifle was used to kill the Crewes, the gun was returned to him on 8 September. On 27 October 1970, the garden at the Crewe house was searched for a third time and a spent cartridge case was found, apparently still lying where the murderer had left it. The case carried marks which showed it had been ejected from Thomas' rifle. In November, Thomas was arrested and charged.\n\nDespite his wife and cousin giving him a strong alibi for 17 June, Thomas was sent for trial on a charge of murdering the Crewes. The prosecution suggested Thomas's wife, Vivien, had been the woman seen at the Crewes' house, although she was not charged. The witness was certain Vivien Thomas, who he knew, was not the woman he saw. The prosecution said the motive for the murders was that Thomas had been obsessed with Jeannette, an accusation for which they provided very little evidence. A witness who did give testimony supporting the prosecution's contention that Jeannette had been pestered by Thomas was Demler; he was cross examined about why he had not mentioned such obviously relevant information before the court had begun sitting. Thomas was found guilty of the murders in a 1971 trial, but the conviction was overturned on appeal. He was tried again in 1973 and convicted. Supporters of Thomas started a campaign to bring to public attention that the key evidence against him had serious anomalies.\n\nA campaign, led in part by Pat Booth of the \"Auckland Star\", was largely responsible for getting Thomas released with a pardon. Campaigners said forensic work by Dr Jim Sprott had shown that the cartridge case had been planted at the scene and that its method of construction identified it as being from a batch that could not have contained the number 8 bullets recovered from the victims. Following David Yallop's book about the case, \"Beyond Reasonable Doubt\", Thomas was pardoned by the Governor-General in 1979, on the recommendation of Prime Minister of New Zealand Robert Muldoon, and released after serving nine years in prison. He was paid NZ$950,000 compensation for his time in jail and loss of the use of the farm.\n\nA Royal Commission of Inquiry was ordered to review the wrongful conviction of Thomas and reported to the Governor-General in November 1980. The Commissioners found that the spent cartridge case from Thomas' gun, Exhibit 350, had not been left by the murderer, but had been created weeks later by police using his impounded gun and ammunition, then planted at the Crewes'. The Royal Commission's report said Detective Inspector Bruce Hutton and Detective Sergeant Lenrick Johnston were implicated in the misconduct, and that the prosecution of Thomas for the murders had been unjustified. Despite the Royal Commission describing the conduct of Hutton and Johnston as an \"unspeakable outrage\", the New Zealand police never laid charges against any police officer involved in the investigation and prosecution of Thomas. Lenrick Johnston died in 1978. Bruce Hutton died in 2013. The case was made into the docu-drama feature film \"Beyond Reasonable Doubt\" in 1980.\n\nIn 2014 an official police review of the investigation into the homicides, at a cost of $400,000 to New Zealand taxpayers, said that evidence available in the murder of the Crewes was insufficient for any new prosecution. The review acknowledged that a key prosecution exhibit in the trials had been fabricated by detectives, but did not appear to accept that they could have been on the wrong track; the review implied that the Crewes' daughter had not ingested any fluids between 17 and 22 June, and said a witness had been mistaken in thinking he had seen a woman on the farm during that period. The review did however rule out Demler having been the killer. Rochelle Crewe expressed satisfaction that a police review of evidence had cleared her deceased grandfather, Demler, of involvement in the murders. The case remains unsolved.\n\n\n\n"}
{"id": "12158691", "url": "https://en.wikipedia.org/wiki?curid=12158691", "title": "National Integrated Ballistic Information Network", "text": "National Integrated Ballistic Information Network\n\nThe National Integrated Ballistic Information Network or NIBIN is a specialized computer network in the United States. It contains digital images of recovered pieces of ballistic evidence.\n\nRunning on the Integrated Ballistic Identification System or IBIS platform, NIBIN enables U.S. law enforcers to rapidly determine if a piece of recovered ballistic evidence came from a firearm that has been previously used in a crime.\n\nThere are certain criteria that must be met prior to entering information into the NIBIN database. For instance, cartridge cases from a .22 caliber firearm or a revolver are normally not entered. \n\nUsing NIBIN, law enforcement staff can identify firearms in new cases that were used in prior incidents. A series of seventeen different Washington state crime scenes involving seven firearms, and three different agencies in two counties, was identified using information provided by IBIS/NIBIN.\n\nIn 1999, the Bureau of Alcohol, Tobacco, Firearms and Explosives (ATF) established and began administration of the National Integrated Ballistic Information Network. In this program, ATF administers automated ballistic imaging technology for law enforcement, forensic science, and attorney agencies in the United States that have entered into a formal agreement with ATF to enter ballistic information into NIBIN. Partners use Integrated Ballistic Identification Systems to acquire digital images of the markings made on spent ammunition recovered from a crime scene or a crime gun test fire and then compare those images against earlier entries via electronic image comparison. If a high-confidence candidate for a match emerges, firearms examiners compare the original evidence with a microscope to confirm the match.\n\n"}
{"id": "39399247", "url": "https://en.wikipedia.org/wiki?curid=39399247", "title": "Nawaz Marri", "text": "Nawaz Marri\n\nMir Muhammad Nawaz Marri () was a Baloch Judge of Balochistan High Court in Quetta, Balochistan, Pakistan.\n\nJustice Mir Muhammad Nawaz Marri he was senior advocate as well as became president of Balochistan Bar Association.\n\nOn Jan 7, 2000 at 9:35 am Justice Mir Muhammad Nawaz Marri was ambushed and killed. The murder was alleged to be carried out by the orders of Khair Bakhsh Marri by his sons Balach Marri, Ghazan Marri and Mehran Marri.\n\n\n"}
{"id": "37897136", "url": "https://en.wikipedia.org/wiki?curid=37897136", "title": "Nepalese English", "text": "Nepalese English\n\nNepalese English is the register of English language characteristic of the Federal Democratic Republic of Nepal. Many Nepalis speak English as a second or foreign language, with English use being most prevalent among city dwellers residing in Kathmandu (the capital of Nepal). Although Nepali is the native language, English is the primary language used for business in Nepal. In Nepal, where modern English education began in the 1850s, there is little or no consensus among teachers and practitioners on whether to follow British, American or Indian variants of English, or allow the development of a Nepal-specific variety of English, known colloquially as Nenglish.\n\nThis dialect is classified as a non-standard dialect and has no legal status.\n\nNepalese accents vary greatly. Some Nepalese speak English with an accent close to a Standard British (Received Pronunciation) accent; others lean toward a more 'vernacular', native-tinted, accent for their English speech.\n\nThe role of English within the complex multilingual society of Nepal is far from straightforward: it is used across the country, by speakers with various degrees of proficiency; the grammar and phraseology may mimic that of the speaker's first language. While Nepalese speakers of English use idioms peculiar to their homeland, often literal translations of words and phrases from their native languages, this is far less common in proficient speakers, and the grammar itself tends to be quite close to that of Standard English.\n\nThe numeral system of Nepalese English is the Indian numbering system and is preferred for digit grouping. When written in words, or when spoken, numbers less than 100,000 are expressed just as they are in Standard English. Numbers including and beyond 100,000 are expressed in a subset of the Indian numbering system. Consequently, the following scale is used:\nLarger numbers are generally expressed as multiples of the above.\n\nThere are a few distinct expressions relating to family members in Nepalese English.\n\nIn most of the languages spoken in Nepal, both brother and cousin are referred to as brother, as there is no specific word for cousin. People therefore use the term \"own brother\" to refer to \"brother\" and \"not own brother\" to refer to \"cousin\".\n\nThe Nepalese languages have specific words to refer to aunts and uncles. For example, in Nepali, father's sister is (phupu) and mother's sister (sāno āmā). People would generally use their native language's terms to refer to those family members.\n\n"}
{"id": "11056936", "url": "https://en.wikipedia.org/wiki?curid=11056936", "title": "Operation Malaya", "text": "Operation Malaya\n\nOperation Malaya () is a Spanish anti-corruption campaign in the southern resort city of Marbella. It began in 2006 and it is being carried out by the \"Policía Nacional\" under the direction of Judge Miguel Ángel Torres. The operation, now in its third phase, has the objective of exposing a network involved in concealing numerous illicit activities, (including bribery, embezzlement, and influence peddling) carried out by members of the Marbella City Council, businessmen, and prominent lawyers, among others. More than €2.4 billion has been seized so far.\n\nOn May 3, 2007, singer Isabel Pantoja was arrested for possible tax evasion and money laundering, which she allegedly did on behalf of the former mayor of Marbella, Julián Muñoz. He has already been jailed as part of the investigation, along with more than 100 others. Pantoja was later released on €90,000 bail.\n\nIn March 2005, investigations of the White Whale Operation dismantled the largest network of money laundering in the country. The evidence collected included intercepts of telephone conversations relating to cases of urban corruption in the Costa del Sol. A year later, Marbella City Council was wound up, and was replaced by a management committee presided over by Don Diego Martín Reye, pending fresh elections.\n\nFollowing the creation of this management committee for Marbella City Hall, and after the arrival of democratic normality, the Andalusian Autonomous Government withdrew the authority to give urban planning consent [from Marbella City Council] based on Article 31.4 of the Andalusian Law on Urban Planning. This was according to the wording of the Law of November 2005 which states, \"due to its serious influence on the planning powers of the regional autonomous authority [the Junta de Andalucía], set out in the Urban Planning Law of Andalusia (LOUA)\", as reported by the Minister of the Board of Public Works and Transport.\"\n\nOperation Malaya opened the door to subsequent investigations into possible cases of urban corruption in Spain. While not all of Spain experienced such a high level of construction projects as the Costa del Sol prior to 2008, evidence of similar abuses has come to light in other parts of the country. There has already been consideration of what might be done to prevent similar webs of corruption in the future. It is widely acknowledged that the planning process in Spain could be improved to make the allocation of land for development more transparent.\n\n"}
{"id": "4250179", "url": "https://en.wikipedia.org/wiki?curid=4250179", "title": "Orang Ulu", "text": "Orang Ulu\n\nOrang Ulu (\"people of the interior\" in Malay) is an ethnic designation politically coined to group together roughly 27 very small but ethnically diverse tribal groups in northeastern Sarawak, Malaysia with populations ranging from less than 300 persons to over 25,000 persons. \"Orang Ulu\" is not a legal term, and no such racial group exists or is listed in the Malaysian Constitution. The term was popularised by the Orang Ulu National Association (OUNA), which was formed in 1969.\n\nThe Orang Ulu tribal groups are diverse, they typically live in longhouses elaborately decorated with murals and woodcarvings. They are also well known for their intricate beadwork detailed tattoos, rattan weaving, and other tribal crafts. The Orang Ulu tribes can also be identified by their unique music - distinctive sounds from their sapes, a plucked boat-shaped lute, formerly with two strings, nowadays usually with four strings. They also practice \"Kanjet\", a form of traditional dance.\n\nA vast majority of the Orang Ulu tribes are Christians, but old traditional religions are still practiced in some areas.\n\nThere are about 27 small Dayak people groups that are classified as Orang Ulu such as:-\n\n\n"}
{"id": "22860", "url": "https://en.wikipedia.org/wiki?curid=22860", "title": "Paleolithic", "text": "Paleolithic\n\nThe Paleolithic or Palaeolithic () is a period in human prehistory distinguished by the original development of stone tools that covers c. 99% of human technological prehistory. It extends from the earliest known use of stone tools by hominins c. 3.3 million years ago, to the end of the Pleistocene c. 11,650 cal BP.\n\nThe Paleolithic is followed in Europe by the Mesolithic, although the date of the transition varies geographically by several thousand years.\n\nDuring the Paleolithic, hominins grouped together in small societies such as bands, and subsisted by gathering plants and fishing, hunting or scavenging wild animals. The Paleolithic is characterized by the use of knapped stone tools, although at the time humans also used wood and bone tools. Other organic commodities were adapted for use as tools, including leather and vegetable fibers; however, due to their nature, these have not been preserved to any great degree.\n\nAbout 50,000 years ago, there was a marked increase in the diversity of artifacts. In Africa, bone artifacts and the first art appear in the archaeological record. The first evidence of human fishing is also noted, from artifacts in places such as Blombos cave in South Africa. Archaeologists classify artifacts of the last 50,000 years into many different categories, such as projectile points, engraving tools, knife blades, and drilling and piercing tools.\n\nHumankind gradually evolved from early members of the genus \"Homo—\"such as \"Homo habilis\", who used simple stone tools—into anatomically modern humans as well as behaviorally modern humans by the Upper Paleolithic. During the end of the Paleolithic, specifically the Middle or Upper Paleolithic, humans began to produce the earliest works of art and began to engage in religious and spiritual behavior such as burial and ritual. The climate during the Paleolithic consisted of a set of glacial and interglacial periods in which the climate periodically fluctuated between warm and cool temperatures. Archaeological and genetic data suggest that the source populations of Paleolithic humans survived in sparsely wooded areas and dispersed through areas of high primary productivity while avoiding dense forest cover.\n\nBy  BP, the first humans set foot in Australia. By  BP, humans lived at 61°N latitude in Europe. By  BP, Japan was reached, and by  BP humans were present in Siberia, above the Arctic Circle. At the end of the Upper Paleolithic, a group of humans crossed Beringia and quickly expanded throughout the Americas.\n\nThe term \"Palaeolithic\" was coined by archaeologist John Lubbock in 1865. It derives from Greek: παλαιός, \"palaios\", \"old\"; and λίθος, \"lithos\", \"stone\", meaning \"old age of the stone\" or \"Old Stone Age\".\n\nThe Paleolithic coincides almost exactly with the Pleistocene epoch of geologic time, which lasted from 2.6 million years ago to about 12,000 years ago. This epoch experienced important geographic and climatic changes that affected human societies.\n\nDuring the preceding Pliocene, continents had continued to drift from possibly as far as from their present locations to positions only from their current location. South America became linked to North America through the Isthmus of Panama, bringing a nearly complete end to South America's distinctive marsupial fauna. The formation of the isthmus had major consequences on global temperatures, because warm equatorial ocean currents were cut off, and the cold Arctic and Antarctic waters lowered temperatures in the now-isolated Atlantic Ocean.\n\nMost of Central America formed during the Pliocene to connect the continents of North and South America, allowing fauna from these continents to leave their native habitats and colonize new areas. Africa's collision with Asia created the Mediterranean, cutting off the remnants of the Tethys Ocean. During the Pleistocene, the modern continents were essentially at their present positions; the tectonic plates on which they sit have probably moved at most from each other since the beginning of the period.\n\nClimates during the Pliocene became cooler and drier, and seasonal, similar to modern climates. Ice sheets grew on Antarctica. The formation of an Arctic ice cap around 3 million years ago is signaled by an abrupt shift in oxygen isotope ratios and ice-rafted cobbles in the North Atlantic and North Pacific Ocean beds. Mid-latitude glaciation probably began before the end of the epoch. The global cooling that occurred during the Pliocene may have spurred on the disappearance of forests and the spread of grasslands and savannas.\nThe Pleistocene climate was characterized by repeated glacial cycles during which continental glaciers pushed to the 40th parallel in some places. Four major glacial events have been identified, as well as many minor intervening events. A major event is a general glacial excursion, termed a \"glacial\". Glacials are separated by \"interglacials\". During a glacial, the glacier experiences minor advances and retreats. The minor excursion is a \"stadial\"; times between stadials are \"interstadials\". Each glacial advance tied up huge volumes of water in continental ice sheets deep, resulting in temporary sea level drops of or more over the entire surface of the Earth. During interglacial times, such as at present, drowned coastlines were common, mitigated by isostatic or other emergent motion of some regions.\n\nThe effects of glaciation were global. Antarctica was ice-bound throughout the Pleistocene and the preceding Pliocene. The Andes were covered in the south by the Patagonian ice cap. There were glaciers in New Zealand and Tasmania. The now decaying glaciers of Mount Kenya, Mount Kilimanjaro, and the Ruwenzori Range in east and central Africa were larger. Glaciers existed in the mountains of Ethiopia and to the west in the Atlas mountains. In the northern hemisphere, many glaciers fused into one. The Cordilleran ice sheet covered the North American northwest; the Laurentide covered the east. The Fenno-Scandian ice sheet covered northern Europe, including Great Britain; the Alpine ice sheet covered the Alps. Scattered domes stretched across Siberia and the Arctic shelf. The northern seas were frozen. During the late Upper Paleolithic (Latest Pleistocene)  BP, the Beringia land bridge between Asia and North America was blocked by ice, which may have prevented early Paleo-Indians such as the Clovis culture from directly crossing Beringia to reach the Americas.\n\nAccording to Mark Lynas (through collected data), the Pleistocene's overall climate could be characterized as a continuous El Niño with trade winds in the south Pacific weakening or heading east, warm air rising near Peru, warm water spreading from the west Pacific and the Indian Ocean to the east Pacific, and other El Niño markers.\n\nThe Paleolithic is often held to finish at the end of the ice age (the end of the Pleistocene epoch), and Earth's climate became warmer. This may have caused or contributed to the extinction of the Pleistocene megafauna, although it is also possible that the late Pleistocene extinctions were (at least in part) caused by other factors such as disease and overhunting by humans. New research suggests that the extinction of the woolly mammoth may have been caused by the combined effect of climatic change and human hunting. Scientists suggest that climate change during the end of the Pleistocene caused the mammoths' habitat to shrink in size, resulting in a drop in population. The small populations were then hunted out by Paleolithic humans. The global warming that occurred during the end of the Pleistocene and the beginning of the Holocene may have made it easier for humans to reach mammoth habitats that were previously frozen and inaccessible. Small populations of woolly mammoths survived on isolated Arctic islands, Saint Paul Island and Wrangel Island, until  BCE and  BCE respectively. The Wrangel Island population became extinct around the same time the island was settled by prehistoric humans. There is no evidence of prehistoric human presence on Saint Paul island (though early human settlements dating as far back as 6500 BCE were found on the nearby Aleutian Islands).\n\nNearly all of our knowledge of Paleolithic human culture and way of life comes from archaeology and ethnographic comparisons to modern hunter-gatherer cultures such as the !Kung San who live similarly to their Paleolithic predecessors. The economy of a typical Paleolithic society was a hunter-gatherer economy. Humans hunted wild animals for meat and gathered food, firewood, and materials for their tools, clothes, or shelters.\n\nHuman population density was very low, around only one person per square mile. This was most likely due to low body fat, infanticide, women regularly engaging in intense endurance exercise, late weaning of infants, and a nomadic lifestyle. Like contemporary hunter-gatherers, Paleolithic humans enjoyed an abundance of leisure time unparalleled in both Neolithic farming societies and modern industrial societies. At the end of the Paleolithic, specifically the Middle or Upper Paleolithic, humans began to produce works of art such as cave paintings, rock art and jewellery and began to engage in religious behavior such as burial and ritual.\n\nAt the beginning of the Paleolithic, hominins were found primarily in eastern Africa, east of the Great Rift Valley. Most known hominin fossils dating earlier than one million years before present are found in this area, particularly in Kenya, Tanzania, and Ethiopia.\n\nBy  BP, groups of hominins began leaving Africa and settling southern Europe and Asia. Southern Caucasus was occupied by  BP, and northern China was reached by  BP. By the end of the Lower Paleolithic, members of the hominin family were living in what is now China, western Indonesia, and, in Europe, around the Mediterranean and as far north as England, southern Germany, and Bulgaria. Their further northward expansion may have been limited by the lack of control of fire: studies of cave settlements in Europe indicate no regular use of fire prior to  BP.\n\nEast Asian fossils from this period are typically placed in the genus \"Homo erectus\". Very little fossil evidence is available at known Lower Paleolithic sites in Europe, but it is believed that hominins who inhabited these sites were likewise \"Homo erectus\". There is no evidence of hominins in America, Australia, or almost anywhere in Oceania during this time period.\n\nFates of these early colonists, and their relationships to modern humans, are still subject to debate. According to current archaeological and genetic models, there were at least two notable expansion events subsequent to peopling of Eurasia  BP. Around 500,000 BP a group of early humans, frequently called \"Homo heidelbergensis\", came to Europe from Africa and eventually evolved into \"Homo neanderthalensis\" (Neanderthals). In the Middle Paleolithic, Neanderthals were present in the region now occupied by Poland.\n\nBoth \"Homo erectus\" and \"Homo neanderthalensis\" became extinct by the end of the Paleolithic. Descended from \"Homo Sapiens\", the anatomically modern \"Homo sapiens sapiens\" emerged in eastern Africa  BP, left Africa around 50,000 BP, and expanded throughout the planet. Multiple hominid groups coexisted for some time in certain locations. \" Homo neanderthalensis\" were still found in parts of Eurasia  BP years, and engaged in an unknown degree of interbreeding with \"Homo sapiens sapiens\". DNA studies also suggest an unknown degree of interbreeding between \"Homo sapiens sapiens\" and \"Homo sapiens denisova\".\n\nHominin fossils not belonging either to \"Homo neanderthalensis\" or to \"Homo sapiens\" species, found in the Altai Mountains and Indonesia, were radiocarbon dated to  BP and  BP respectively.\n\nFor the duration of the Paleolithic, human populations remained low, especially outside the equatorial region. The entire population of Europe between 16,000 and 11,000 BP likely averaged some 30,000 individuals, and between 40,000 and 16,000 BP, it was even lower at 4,000–6,000 individuals.\n\nPaleolithic humans made tools of stone, bone, and wood. The early paleolithic hominins, \"Australopithecus\", were the first users of stone tools. Excavations in Gona, Ethiopia have produced thousands of artifacts, and through radioisotopic dating and magnetostratigraphy, the sites can be firmly dated to 2.6 million years ago. Evidence shows these early hominins intentionally selected raw materials with good flaking qualities and chose appropriate sized stones for their needs to produce sharp-edged tools for cutting.\n\nThe earliest Paleolithic stone tool industry, the Oldowan, began around 2.6 million years ago. It contained tools such as choppers, burins, and stitching awls. It was completely replaced around 250,000 years ago by the more complex Acheulean industry, which was first conceived by \"Homo ergaster\" around 1.8–1.65 million years ago. The Acheulean implements completely vanish from the archaeological record around 100,000 years ago and were replaced by more complex Middle Paleolithic tool kits such as the Mousterian and the Aterian industries.\n\nLower Paleolithic humans used a variety of stone tools, including hand axes and choppers. Although they appear to have used hand axes often, there is disagreement about their use. Interpretations range from cutting and chopping tools, to digging implements, to flaking cores, to the use in traps, and as a purely ritual significance, perhaps in courting behavior. William H. Calvin has suggested that some hand axes could have served as \"killer Frisbees\" meant to be thrown at a herd of animals at a waterhole so as to stun one of them. There are no indications of hafting, and some artifacts are far too large for that. Thus, a thrown hand axe would not usually have penetrated deeply enough to cause very serious injuries. Nevertheless, it could have been an effective weapon for defense against predators. Choppers and scrapers were likely used for skinning and butchering scavenged animals and sharp-ended sticks were often obtained for digging up edible roots. Presumably, early humans used wooden spears as early as 5 million years ago to hunt small animals, much as their relatives, chimpanzees, have been observed to do in Senegal, Africa. Lower Paleolithic humans constructed shelters, such as the possible wood hut at Terra Amata.\n\nFire was used by the Lower Paleolithic hominins \"Homo erectus\" and \"Homo ergaster\" as early as 300,000 to 1.5 million years ago and possibly even earlier by the early Lower Paleolithic (Oldowan) hominin \"Homo habilis\" or by robust \"Australopithecines\" such as \"Paranthropus\". However, the use of fire only became common in the societies of the following Middle Stone Age and Middle Paleolithic. Use of fire reduced mortality rates and provided protection against predators. Early hominins may have begun to cook their food as early as the Lower Paleolithic ( million years ago) or at the latest in the early Middle Paleolithic ( years ago). Some scientists have hypothesized that hominins began cooking food to defrost frozen meat, which would help ensure their survival in cold regions.\n\nThe Lower Paleolithic \"Homo erectus\" possibly invented rafts ( BP) to travel over large bodies of water, which may have allowed a group of \"Homo erectus\" to reach the island of Flores and evolve into the small hominin \"Homo floresiensis\". However, this hypothesis is disputed within the anthropological community. The possible use of rafts during the Lower Paleolithic may indicate that Lower Paleolithic hominins such as \"Homo erectus\" were more advanced than previously believed, and may have even spoken an early form of modern language. Supplementary evidence from Neanderthal and modern human sites located around the Mediterranean Sea, such as Coa de sa Multa ( BP), has also indicated that both Middle and Upper Paleolithic humans used rafts to travel over large bodies of water (i.e. the Mediterranean Sea) for the purpose of colonizing other bodies of land.\n\nBy around 200,000 BP, Middle Paleolithic stone tool manufacturing spawned a tool making technique known as the prepared-core technique, that was more elaborate than previous Acheulean techniques. This technique increased efficiency by allowing the creation of more controlled and consistent flakes. It allowed Middle Paleolithic humans to create stone tipped spears, which were the earliest composite tools, by hafting sharp, pointy stone flakes onto wooden shafts. In addition to improving tool making methods, the Middle Paleolithic also saw an improvement of the tools themselves that allowed access to a wider variety and amount of food sources. For example, microliths or small stone tools or points were invented around 70,000–65,000 BP and were essential to the invention of bows and spear throwers in the following Upper Paleolithic.\n\nHarpoons were invented and used for the first time during the late Middle Paleolithic ( BP); the invention of these devices brought fish into the human diets, which provided a hedge against starvation and a more abundant food supply. Thanks to their technology and their advanced social structures, Paleolithic groups such as the Neanderthals—who had a Middle Paleolithic level of technology—appear to have hunted large game just as well as Upper Paleolithic modern humans. and the Neanderthals in particular may have likewise hunted with projectile weapons. Nonetheless, Neanderthal use of projectile weapons in hunting occurred very rarely (or perhaps never) and the Neanderthals hunted large game animals mostly by ambushing them and attacking them with mêlée weapons such as thrusting spears rather than attacking them from a distance with projectile weapons.\n\nDuring the Upper Paleolithic, further inventions were made, such as the net or  BP) bolas, the spear thrower ( BP), the bow and arrow ( or  BP) and the oldest example of ceramic art, the Venus of Dolní Věstonice ( BCE). Early dogs were domesticated, sometime between 30,000 and 14,000 BP, presumably to aid in hunting. However, the earliest instances of successful domestication of dogs may be much more ancient than this. Evidence from canine DNA collected by Robert K. Wayne suggests that dogs may have been first domesticated in the late Middle Paleolithic around 100,000 BP or perhaps even earlier.\n\nArchaeological evidence from the Dordogne region of France demonstrates that members of the European early Upper Paleolithic culture known as the Aurignacian used calendars ( BP). This was a lunar calendar that was used to document the phases of the moon. Genuine solar calendars did not appear until the Neolithic. Upper Paleolithic cultures were probably able to time the migration of game animals such as wild horses and deer. This ability allowed humans to become efficient hunters and to exploit a wide variety of game animals. Recent research indicates that the Neanderthals timed their hunts and the migrations of game animals long before the beginning of the Upper Paleolithic.\n\nThe social organization of the earliest Paleolithic (Lower Paleolithic) societies remains largely unknown to scientists, though Lower Paleolithic hominins such as \"Homo habilis\" and \"Homo erectus\" are likely to have had more complex social structures than chimpanzee societies. Late Oldowan/Early Acheulean humans such as \"Homo ergaster\"/\"Homo erectus\" may have been the first people to invent central campsites or home bases and incorporate them into their foraging and hunting strategies like contemporary hunter-gatherers, possibly as early as 1.7 million years ago; however, the earliest solid evidence for the existence of home bases or central campsites (hearths and shelters) among humans only dates back to 500,000 years ago.\n\nSimilarly, scientists disagree whether Lower Paleolithic humans were largely monogamous or polygynous. In particular, the Provisional model suggests that bipedalism arose in pre-Paleolithic australopithecine societies as an adaptation to monogamous lifestyles; however, other researchers note that sexual dimorphism is more pronounced in Lower Paleolithic humans such as \"Homo erectus\" than in modern humans, who are less polygynous than other primates, which suggests that Lower Paleolithic humans had a largely polygynous lifestyle, because species that have the most pronounced sexual dimorphism tend more likely to be polygynous.\n\nHuman societies from the Paleolithic to the early Neolithic farming tribes lived without states and organized governments. For most of the Lower Paleolithic, human societies were possibly more hierarchical than their Middle and Upper Paleolithic descendants, and probably were not grouped into bands, though during the end of the Lower Paleolithic, the latest populations of the hominin \"Homo erectus\" may have begun living in small-scale (possibly egalitarian) bands similar to both Middle and Upper Paleolithic societies and modern hunter-gatherers.\n\nMiddle Paleolithic societies, unlike Lower Paleolithic and early Neolithic ones, consisted of bands that ranged from 20–30 or 25–100 members and were usually nomadic. These bands were formed by several families. Bands sometimes joined together into larger \"macrobands\" for activities such as acquiring mates and celebrations or where resources were abundant. By the end of the Paleolithic era ( BP), people began to settle down into permanent locations, and began to rely on agriculture for sustenance in many locations. Much evidence exists that humans took part in long-distance trade between bands for rare commodities (such as ochre, which was often used for religious purposes such as ritual) and raw materials, as early as 120,000 years ago in Middle Paleolithic. Inter-band trade may have appeared during the Middle Paleolithic because trade between bands would have helped ensure their survival by allowing them to exchange resources and commodities such as raw materials during times of relative scarcity (i.e. famine, drought). Like in modern hunter-gatherer societies, individuals in Paleolithic societies may have been subordinate to the band as a whole. Both Neanderthals and modern humans took care of the elderly members of their societies during the Middle and Upper Paleolithic.\n\nSome sources claim that most Middle and Upper Paleolithic societies were possibly fundamentally egalitarian and may have rarely or never engaged in organized violence between groups (i.e. war).\nSome Upper Paleolithic societies in resource-rich environments (such as societies in Sungir, in what is now Russia) may have had more complex and hierarchical organization (such as tribes with a pronounced hierarchy and a somewhat formal division of labor) and may have engaged in endemic warfare. Some argue that there was no formal leadership during the Middle and Upper Paleolithic. Like contemporary egalitarian hunter-gatherers such as the Mbuti pygmies, societies may have made decisions by communal consensus decision making rather than by appointing permanent rulers such as chiefs and monarchs. Nor was there a formal division of labor during the Paleolithic. Each member of the group was skilled at all tasks essential to survival, regardless of individual abilities. Theories to explain the apparent egalitarianism have arisen, notably the Marxist concept of primitive communism. Christopher Boehm (1999) has hypothesized that egalitarianism may have evolved in Paleolithic societies because of a need to distribute resources such as food and meat equally to avoid famine and ensure a stable food supply. Raymond C. Kelly speculates that the relative peacefulness of Middle and Upper Paleolithic societies resulted from a low population density, cooperative relationships between groups such as reciprocal exchange of commodities and collaboration on hunting expeditions, and because the invention of projectile weapons such as throwing spears provided less incentive for war, because they increased the damage done to the attacker and decreased the relative amount of territory attackers could gain. However, other sources claim that most Paleolithic groups may have been larger, more complex, sedentary and warlike than most contemporary hunter-gatherer societies, due to occupying more resource-abundant areas than most modern hunter-gatherers who have been pushed into more marginal habitats by agricultural societies.\n\nAnthropologists have typically assumed that in Paleolithic societies, women were responsible for gathering wild plants and firewood, and men were responsible for hunting and scavenging dead animals. However, analogies to existent hunter-gatherer societies such as the Hadza people and the Aboriginal Australians suggest that the sexual division of labor in the Paleolithic was relatively flexible. Men may have participated in gathering plants, firewood and insects, and women may have procured small game animals for consumption and assisted men in driving herds of large game animals (such as woolly mammoths and deer) off cliffs. Additionally, recent research by anthropologist and archaeologist Steven Kuhn from the University of Arizona is argued to support that this division of labor did not exist prior to the Upper Paleolithic and was invented relatively recently in human pre-history.<ref name=\"NG2006/12/061207\"></ref> Sexual division of labor may have been developed to allow humans to acquire food and other resources more efficiently. Possibly there was approximate parity between men and women during the Middle and Upper Paleolithic, and that period may have been the most gender-equal time in human history. Archaeological evidence from art and funerary rituals indicates that a number of individual women enjoyed seemingly high status in their communities, and it is likely that both sexes participated in decision making. The earliest known Paleolithic shaman ( BP) was female. Jared Diamond suggests that the status of women declined with the adoption of agriculture because women in farming societies typically have more pregnancies and are expected to do more demanding work than women in hunter-gatherer societies. Like most contemporary hunter-gatherer societies, Paleolithic and the Mesolithic groups probably followed mostly matrilineal and ambilineal descent patterns; patrilineal descent patterns were probably rarer than in the Neolithic.\n\nEarly examples of artistic expression, such as the Venus of Tan-Tan and the patterns found on elephant bones from Bilzingsleben in Thuringia, may have been produced by Acheulean tool users such as \"Homo erectus\" prior to the start of the Middle Paleolithic period. However, the earliest undisputed evidence of art during the Paleolithic comes from Middle Paleolithic/Middle Stone Age sites such as Blombos Cave –South Africa– in the form of bracelets, beads, rock art, and ochre used as body paint and perhaps in ritual. Undisputed evidence of art only becomes common in the Upper Paleolithic.\n\nLower Paleolithic Acheulean tool users, according to Robert G. Bednarik, began to engage in symbolic behavior such as art around 850,000 BP. They decorated themselves with beads and collected exotic stones for aesthetic, rather than utilitarian qualities. According to him, traces of the pigment ochre from late Lower Paleolithic Acheulean archaeological sites suggests that Acheulean societies, like later Upper Paleolithic societies, collected and used ochre to create rock art. Nevertheless, it is also possible that the ochre traces found at Lower Paleolithic sites is naturally occurring.\n\nVincent W. Fallio interprets Lower and Middle Paleolithic marking on rocks at sites such as Bilzingsleben (such as zigzagging lines) as accounts or representations of altered states of consciousness though some other scholars interpret them as either simple doodling or as the result of natural processes.\n\nUpper Paleolithic humans produced works of art such as cave paintings, Venus figurines, animal carvings, and rock paintings. Upper Paleolithic art can be divided into two broad categories: figurative art such as cave paintings that clearly depicts animals (or more rarely humans); and nonfigurative, which consists of shapes and symbols. Cave paintings have been interpreted in a number of ways by modern archaeologists. The earliest explanation, by the prehistorian Abbe Breuil, interpreted the paintings as a form of magic designed to ensure a successful hunt. However, this hypothesis fails to explain the existence of animals such as saber-toothed cats and lions, which were not hunted for food, and the existence of half-human, half-animal beings in cave paintings. The anthropologist David Lewis-Williams has suggested that Paleolithic cave paintings were indications of shamanistic practices, because the paintings of half-human, half-animal paintings and the remoteness of the caves are reminiscent of modern hunter-gatherer shamanistic practices. Symbol-like images are more common in Paleolithic cave paintings than are depictions of animals or humans, and unique symbolic patterns might have been trademarks that represent different Upper Paleolithic ethnic groups. Venus figurines have evoked similar controversy. Archaeologists and anthropologists have described the figurines as representations of goddesses, pornographic imagery, apotropaic amulets used for sympathetic magic, and even as self-portraits of women themselves.\n\nR. Dale Guthrie has studied not only the most artistic and publicized paintings, but also a variety of lower-quality art and figurines, and he identifies a wide range of skill and ages among the artists. He also points out that the main themes in the paintings and other artifacts (powerful beasts, risky hunting scenes and the over-sexual representation of women) are to be expected in the fantasies of adolescent males during the Upper Paleolithic.\n\nThe \"Venus\" figurines have been theorized, not universally, as representing a mother goddess; the abundance of such female imagery has inspired the theory that Paleolithic (and later Neolithic) societies centered their religion and societies around women. Adherents of the theory include archaeologist Marija Gimbutas and feminist scholar Merlin Stone, the author of the 1976 book \"When God Was a Woman\". Other explanations for the purpose of the figurines have been proposed, such as Catherine McCoid and LeRoy McDermott's hypothesis that they were self-portraits of woman artists and R.Dale Gutrie's hypothesis that served as \"stone age pornography\".\n\nThe origins of music during the Paleolithic are unknown. The earliest forms of music probably did not use musical instruments other than the human voice or natural objects such as rocks. This early music would not have left an archaeological footprint. Music may have developed from rhythmic sounds produced by daily chores, for example, cracking open nuts with stones. Maintaining a rhythm while working may have helped people to become more efficient at daily activities. An alternative theory originally proposed by Charles Darwin explains that music may have begun as a hominin mating strategy. Bird and other animal species produce music such as calls to attract mates. This hypothesis is generally less accepted than the previous hypothesis, but nonetheless provides a possible alternative.\n\nUpper Paleolithic (and possibly Middle Paleolithic) humans used flute-like bone pipes as musical instruments, and music may have played a large role in the religious lives of Upper Paleolithic hunter-gatherers. As with modern hunter-gatherer societies, music may have been used in ritual or to help induce trances. In particular, it appears that animal skin drums may have been used in religious events by Upper Paleolithic shamans, as shown by the remains of drum-like instruments from some Upper Paleolithic graves of shamans and the ethnographic record of contemporary hunter-gatherer shamanic and ritual practices.\n\nAccording to James B. Harrod humankind first developed religious and spiritual beliefs during the Middle Paleolithic or Upper Paleolithic. Controversial scholars of prehistoric religion and anthropology, James Harrod and Vincent W. Fallio, have recently proposed that religion and spirituality (and art) may have first arisen in Pre-Paleolithic chimpanzees or Early Lower Paleolithic (Oldowan) societies. According to Fallio, the common ancestor of chimpanzees and humans experienced altered states of consciousness and partook in ritual, and ritual was used in their societies to strengthen social bonding and group cohesion.\n\nMiddle Paleolithic humans' use of burials at sites such as Krapina, Croatia ( BP) and Qafzeh, Israel ( BP) have led some anthropologists and archaeologists, such as Philip Lieberman, to believe that Middle Paleolithic humans may have possessed a belief in an afterlife and a \"concern for the dead that transcends daily life\". Cut marks on Neanderthal bones from various sites, such as Combe-Grenal and Abri Moula in France, suggest that the Neanderthals—like some contemporary human cultures—may have practiced ritual defleshing for (presumably) religious reasons. According to recent archaeological findings from \"Homo heidelbergensis\" sites in Atapuerca, humans may have begun burying their dead much earlier, during the late Lower Paleolithic; but this theory is widely questioned in the scientific community.\n\nLikewise, some scientists have proposed that Middle Paleolithic societies such as Neanderthal societies may also have practiced the earliest form of totemism or animal worship, in addition to their (presumably religious) burial of the dead. In particular, Emil Bächler suggested (based on archaeological evidence from Middle Paleolithic caves) that a bear cult was widespread among Middle Paleolithic Neanderthals. A claim that evidence was found for Middle Paleolithic animal worship  BCE originates from the Tsodilo Hills in the African Kalahari desert has been denied by the original investigators of the site. Animal cults in the Upper Paleolithic, such as the bear cult, may have had their origins in these hypothetical Middle Paleolithic animal cults. Animal worship during the Upper Paleolithic was intertwined with hunting rites. For instance, archaeological evidence from art and bear remains reveals that the bear cult apparently involved a type of sacrificial bear ceremonialism, in which a bear was sliced with arrows, finished off by a blast in the lungs, and ritualistically worshipped near a clay bear statue covered by a bear fur with the skull and the body of the bear buried separately. Barbara Ehrenreich controversially theorizes that the sacrificial hunting rites of the Upper Paleolithic (and by extension Paleolithic cooperative big-game hunting) gave rise to war or warlike raiding during the following Epipaleolithic and Mesolithic or late Upper Paleolithic.\n\nThe existence of anthropomorphic images and half-human, half-animal images in the Upper Paleolithic may further indicate that Upper Paleolithic humans were the first people to believe in a pantheon of gods or supernatural beings, though such images may instead indicate shamanistic practices similar to those of contemporary tribal societies. The earliest known undisputed burial of a shaman (and by extension the earliest undisputed evidence of shamans and shamanic practices) dates back to the early Upper Paleolithic era ( BP) in what is now the Czech Republic. However, during the early Upper Paleolithic it was probably more common for all members of the band to participate equally and fully in religious ceremonies, in contrast to the religious traditions of later periods when religious authorities and part-time ritual specialists such as shamans, priests and medicine men were relatively common and integral to religious life. Additionally, it is also possible that Upper Paleolithic religions, like contemporary and historical animistic and polytheistic religions, believed in the existence of a single creator deity in addition to other supernatural beings such as animistic spirits.\n\nVincent W. Fallio writes that ancestor cults first emerged in complex Upper Paleolithic societies. He argues that the elites of these societies (like the elites of many more contemporary complex hunter-gatherers such as the Tlingit) may have used special rituals and ancestor worship to solidify control over their societies, by convincing their subjects that they possess a link to the spirit world that also gives them control over the earthly realm. Secret societies may have served a similar function in these complex quasi-theocratic societies, by dividing the religious practices of these cultures into the separate spheres of folk religion and elite religion.\n\nReligion was possibly apotropaic; specifically, it may have involved sympathetic magic. The Venus figurines, which are abundant in the Upper Paleolithic archaeological record, provide an example of possible Paleolithic sympathetic magic, as they may have been used for ensuring success in hunting and to bring about fertility of the land and women. The Upper Paleolithic Venus figurines have sometimes been explained as depictions of an earth goddess similar to Gaia, or as representations of a goddess who is the ruler or mother of the animals. James Harrod has described them as representative of female (and male) shamanistic spiritual transformation processes.\n\nPaleolithic hunting and gathering people ate varying proportions of vegetables (including tubers and roots), fruit, seeds (including nuts and wild grass seeds) and insects, meat, fish, and shellfish. However, there is little direct evidence of the relative proportions of plant and animal foods. Although the term \"paleolithic diet\", without references to a specific timeframe or locale, is sometimes used with an implication that most humans shared a certain diet during the entire era, that is not entirely accurate. The Paleolithic was an extended period of time, during which multiple technological advances were made, many of which had impact on human dietary structure. For example, humans probably did not possess the control of fire until the Middle Paleolithic, or tools necessary to engage in extensive fishing. On the other hand, both these technologies are generally agreed to have been widely available to humans by the end of the Paleolithic (consequently, allowing humans in some regions of the planet to rely heavily on fishing and hunting). In addition, the Paleolithic involved a substantial geographical expansion of human populations. During the Lower Paleolithic, ancestors of modern humans are thought to have been constrained to Africa east of the Great Rift Valley. During the Middle and Upper Paleolithic, humans greatly expanded their area of settlement, reaching ecosystems as diverse as New Guinea and Alaska, and adapting their diets to whatever local resources were available.\n\nAnother view is that until the Upper Paleolithic, humans were frugivores (fruit eaters) who supplemented their meals with carrion, eggs, and small prey such as baby birds and mussels, and only on rare occasions managed to kill and consume big game such as antelopes. This view is supported by studies of higher apes, particularly chimpanzees. Chimpanzees are the closest to humans genetically, sharing more than 96% of their DNA code with humans, and their digestive tract is functionally very similar to that of humans. Chimpanzees are primarily frugivores, but they could and would consume and digest animal flesh, given the opportunity. In general, their actual diet in the wild is about 95% plant-based, with the remaining 5% filled with insects, eggs, and baby animals. In some ecosystems, however, chimpanzees are predatory, forming parties to hunt monkeys. Some comparative studies of human and higher primate digestive tracts do suggest that humans have evolved to obtain greater amounts of calories from sources such as animal foods, allowing them to shrink the size of the gastrointestinal tract relative to body mass and to increase the brain mass instead.\n\nAnthropologists have diverse opinions about the proportions of plant and animal foods consumed. Just as with still existing hunters and gatherers, there were many varied \"diets\"—in different groups—and also varying through this vast amount of time. Some paleolithic hunter-gatherers consumed a significant amount of meat and possibly obtained most of their food from hunting, while others are shown as a primarily plant-based diet, Most, if not all, are believed to have been opportunistic omnivores. One hypothesis is that carbohydrate tubers (plant underground storage organs) may have been eaten in high amounts by pre-agricultural humans. It is thought that the Paleolithic diet included as much as per day of fruit and vegetables. The relative proportions of plant and animal foods in the diets of Paleolithic people often varied between regions, with more meat being necessary in colder regions (which weren't populated by anatomically modern humans until  BP). It is generally agreed that many modern hunting and fishing tools, such as fish hooks, nets, bows, and poisons, weren't introduced until the Upper Paleolithic and possibly even Neolithic. The only hunting tools widely available to humans during any significant part of the Paleolithic were hand-held spears and harpoons. There's evidence of Paleolithic people killing and eating seals and elands as far as  BP. On the other hand, buffalo bones found in African caves from the same period are typically of very young or very old individuals, and there's no evidence that pigs, elephants, or rhinos were hunted by humans at the time.\n\nPaleolithic peoples suffered less famine and malnutrition than the Neolithic farming tribes that followed them. This was partly because Paleolithic hunter-gatherers accessed a wider variety natural foods, which allowed them a more nutritious diet and a decreased risk of famine. Many of the famines experienced by Neolithic (and some modern) farmers were caused or amplified by their dependence on a small number of crops. It is thought that wild foods can have a significantly different nutritional profile than cultivated foods. The greater amount of meat obtained by hunting big game animals in Paleolithic diets than Neolithic diets may have also allowed Paleolithic hunter-gatherers to enjoy a more nutritious diet than Neolithic agriculturalists. It has been argued that the shift from hunting and gathering to agriculture resulted in an increasing focus on a limited variety of foods, with meat likely taking a back seat to plants. It is also unlikely that Paleolithic hunter-gatherers were affected by modern diseases of affluence such as type 2 diabetes, coronary heart disease, and cerebrovascular disease, because they ate mostly lean meats and plants and frequently engaged in intense physical activity, and because the average lifespan was shorter than the age of common onset of these conditions.\n\nLarge-seeded legumes were part of the human diet long before the Neolithic Revolution, as evident from archaeobotanical finds from the Mousterian layers of Kebara Cave, in Israel.<ref name=\"doi10.1016/j.jas.2004.11.006\"></ref> There is evidence suggesting that Paleolithic societies were gathering wild cereals for food use at least as early as 30,000 years ago. However, seeds—such as grains and beans—were rarely eaten and never in large quantities on a daily basis.<ref name=\"doi:10.1080/11026480510032043\"></ref> Recent archaeological evidence also indicates that winemaking may have originated in the Paleolithic, when early humans drank the juice of naturally fermented wild grapes from animal-skin pouches. Paleolithic humans consumed animal organ meats, including the livers, kidneys, and brains. Upper Paleolithic cultures appear to have had significant knowledge about plants and herbs and may have, albeit very rarely, practiced rudimentary forms of horticulture. In particular, bananas and tubers may have been cultivated as early as 25,000 BP in southeast Asia. Late Upper Paleolithic societies also appear to have occasionally practiced pastoralism and animal husbandry, presumably for dietary reasons. For instance, some European late Upper Paleolithic cultures domesticated and raised reindeer, presumably for their meat or milk, as early as 14,000 BP. Humans also probably consumed hallucinogenic plants during the Paleolithic. The Aboriginal Australians have been consuming a variety of native animal and plant foods, called bushfood, for an estimated 60,000 years, since the Middle Paleolithic.\nPeople during the Middle Paleolithic, such as the Neanderthals and Middle Paleolithic Homo sapiens in Africa, began to catch shellfish for food as revealed by shellfish cooking in Neanderthal sites in Italy about 110,000 years ago and in Middle Paleolithic \"Homo sapiens\" sites at Pinnacle Point, Africa around 164,000 BP.<ref name=\"NYTIMES/10/08/07\"></ref> Although fishing only became common during the Upper Paleolithic, fish have been part of human diets long before the dawn of the Upper Paleolithic and have certainly been consumed by humans since at least the Middle Paleolithic. For example, the Middle Paleolithic \"Homo sapiens\" in the region now occupied by the Democratic Republic of the Congo hunted large -long catfish with specialized barbed fishing points as early as 90,000 years ago. The invention of fishing allowed some Upper Paleolithic and later hunter-gatherer societies to become sedentary or semi-nomadic, which altered their social structures. Example societies are the Lepenski Vir as well as some contemporary hunter-gatherers, such as the Tlingit. In some instances (at least the Tlingit), they developed social stratification, slavery, and complex social structures such as chiefdoms.\n\nAnthropologists such as Tim White suggest that cannibalism was common in human societies prior to the beginning of the Upper Paleolithic, based on the large amount of “butchered human\" bones found in Neanderthal and other Lower/Middle Paleolithic sites. Cannibalism in the Lower and Middle Paleolithic may have occurred because of food shortages. However, it may have been for religious reasons, and would coincide with the development of religious practices thought to have occurred during the Upper Paleolithic. Nonetheless, it remains possible that Paleolithic societies never practiced cannibalism, and that the damage to recovered human bones was either the result of excarnation or predation by carnivores such as saber-toothed cats, lions, and hyenas.\n\nA modern-day diet known as the Paleolithic diet exists, based on restricting consumption to the foods presumed to be available to anatomically modern humans prior to the advent of settled agriculture.\n\n"}
{"id": "15419418", "url": "https://en.wikipedia.org/wiki?curid=15419418", "title": "Patrick Critton", "text": "Patrick Critton\n\nPatrick Dolan Critton is a former teacher from Mount Vernon, New York and the first successful aircraft hijacker in Canada. \n\nOn Dec. 26, 1971, Critton boarded Air Canada DC-9 Flight 932 in Thunder Bay, Ontario bound for Toronto. Armed with a handgun and a grenade, he demanded the plane to fly to Havana, Cuba. At the time of the incident, Critton mistakenly thought he was a fugitive in New York City for a series of crimes, including bank robbery. He had fled to Canada, where he continued a life of crime.\n\nHe released all passengers in Toronto on his way to Havana and after he exited in Havana, the plane returned to Toronto safely with the board of crew members. No injuries or casualties were caused during the hijack.\n\nCanada did not have an extradition agreement with Cuba regarding hijacking by then. After arriving in Cuba, Critton served an 8-month prison sentence. Following his sentence, he first worked in the sugar cane industry in Cuba. Two years later, he moved to Tanzania, where he became a teacher. He also married and had two children before returning to the United States in 1994, feeling safe from the time passed.\n\nCritton remained a fugitive until 2001, when he was found by a detective through a Google search, which at that time produced a single hit with his name that revealed the location where he was teaching. The one hit was a March 2001 article describing his mentoring of black youth.\n\nCritton was arrested on September 10, 2001. Fingerprints on a ginger ale bottle found on the plane linked Critton to the hijacking. Prosecutors, in order to avoid prejudicing the jury, based on the views of the recent September 11 attacks, which had happened the day after Critton was arrested, portrayed Critton to jurors not as a terrorist, but as a kidnapper and robber whose motivation was to gain the flight to Cuba.\n\nCritton received a five-year prison sentence for the crimes. He was released in June 2003 after 1 year and 10 months.\n"}
{"id": "12482268", "url": "https://en.wikipedia.org/wiki?curid=12482268", "title": "Qatar Red Crescent Society", "text": "Qatar Red Crescent Society\n\nThe Qatari branch of the Red Crescent Society, the Qatar Red Crescent (QRC), was established in 1978. In 1981, it gained international recognition from the International Committee of the Red Cross in Geneva and joined the International Federation of Red Cross and Red Crescent Societies (IFRC). It is also a member of the Secretariat of Arab Red Crescent Societies in Jeddah. It became the first philanthropic organization in Qatar to establish a women's branch in 1982.\n\nQRC is headquartered in Doha, but the organization works throughout the country. Their primary goal is to reduce detrimental effects of catastrophe and relieve suffering by contributing social and humanitarian services coinciding with the mission of its parent organization, the Red Crescent.\n\nQRC was the first voluntary charity organization in Qatar. It is very active in aiding and developing relief programs for victims of disaster worldwide. Within Qatar, their programs include social development, training and qualification, awareness and education, programs and medical service, and advocacy community issues. Internationally QRC engages in relief and humanitarian activities, development and empowerment, and advocacy and humanitarian diplomacy.\n\nDuring the conflict in the Gaza strip in fall 2014, the QRC helped, with Qatari government contributions, by providing medical supplies and fuel for hospitals.\n\nQatar Red Crescent Society was the only humanitarian organization present in Northern Mali following the Islamist takeover. This presence raised questions over whether QRC was supporting the Islamists or helping local population. QRC’s stated purpose of the mission was to “distribute food aid to 1,000 households and conduct an assessment of the population’s needs in water, sanitation, health and food security.”\n\nIn the September 2014 Gaza conflict, QRC was on the ground providing medical and ambulance services. They also donated several million Qatari Riyals to provide relief for Gaza.\n\nThe QRC has also been active in Syria where it has engaged in ongoing humanitarian efforts to build roads in the Latakia Countryside and ensure secure transportation of those injured in the conflict to safer adjacent territories. They also launched a psychological support center in 2013 for refugees of the Syrian crisis.\n\nIn West Darfur, QRC has been instrumental in providing agricultural machinery to improve the livelihoods of local communities. QRC has had a representative office in West Darfur to oversee its projects there since 2009.\n\nThe International Federation of Red Cross and Red Crescent Societies partnered with Qatar-based Al Jazeera Media Network in May 2014. This agreement built on relations developed over several years between QRC and Al Jazeera. The goal of the agreement was to give a voice to the voiceless by drawing attention and support to the victims of disasters.\n\nIn coordination with QRC, the IFRC signed a partnership agreement in 2013 with Qatar’s Cultural Village Foundation (“Katara”) to help build more resilient and peaceful communities in the Middle East and North Africa.\n\nIn August 2012, QRC signed a partnership agreement with the Mali Red Cross to “ensure strong coordination in planning and implementing projects in Northern Mali”.\n\nOperating in several areas of conflict, Qatar Red Crescent Society has been accused through blogs and think tank publications of supporting Islamist and terrorist groups. However it has always been cleared from these accusations of support to terrorism by the United Nations see e.g. the UN terror list established and maintained pursuant to Security Council resolution 1267/1989/2253.\n\nIn June 2012 when Northern Mali was taken over by Islamists, Qatar Red Crescent was the only humanitarian organization that was granted access to the territory by the Islamists. In August 2014, the QRC raised more than $10 million at an event where the primary speaker was Hussam Badran, a Hamas leader who once supported bombing attacks in Tel Aviv. Qatar Red Crescent is not the only Qatari charitable organization accused of ties to extremism. Qatar Charity and the Sheikh Eid Bin Mohammad Al Thani Charity Association have also both been allegedly connected to terrorist groups, although the United Nations publicly rejected these assertions and subsequently co-organized in 2017 high-profile activities with these relief organizations. None of the Qatari humanitarian organizations has ever been listed on the UN terror list established and maintained pursuant to Security Council res. 1267/1989/2253.\n\nThe leadership of QRC includes the following individuals:\n\n"}
{"id": "21381310", "url": "https://en.wikipedia.org/wiki?curid=21381310", "title": "Raees Ahmadzai", "text": "Raees Ahmadzai\n\nRaees Ahmadzai (born 3 September 1984) is a former Afghan cricketer who represented the Afghanistan national cricket team until his retirement in May 2010. Ahmadzai is a right-handed batsman who bowls right-arm off break. He is also UNICEF National Goodwill Ambassador.\n\nAhmadzai was born in the village of Azra, Logar Province, Afghanistan. He is from the Kuchi tribe, along with former teammates Mohammad Nabi and Dawlat Ahmadzai. He is far from certain about his age. In an interview in Potchefstroom with Will Luke in April 2009, he said, \"Talking to my mother, she works out my age by seeing who the president was. Unofficially I'm nearly 25, give or take three years. Or four. I could be 21 or 28.\" Luke's own opinion, going by \"Deep-set wrinkles and a calm demeanour\", was twenty-eight. The general consensus from Cricinfo and CricketArchive is that he was born on 3 September 1984.\n\nAhmadzai spent much of his early years in refugee camps with his family, fleeing from the Soviet invasion of Afghanistan and the subsequent Civil War that followed the Soviet withdrawal. Ahmadzai, like many of his teammates learnt the game in neighbouring Pakistan, in Ahmadzai's case in a Peshawar schoolyard.\n\nAhmadzai made his debut for Afghanistan against Rahim Yar Khan in the 2002/3 Cornelius Trophy. Ahmadzai made his international debut for Afghanistan against Hong Kong in the 2004 ACC Trophy. Ahmadzai represented Afghanistan in the following tournament in 2006. During the 2006 tournament, Ahmadzai captained the side.\n\nIn 2007, Ahmadzai played two List-A matches for the Sebastianites Cricket and Athletic Club in Sri Lanka, playing matches against Lankan Cricket Club and the Sri Lanka Army Sports Club.\n\nAhmadzai was part of the rapidly rising Afghan team that from 2008 to 2009 won the World Cricket League Division Five, Division Four and Division Three, thus promoting them to Division Two and allowing them to partake in the 2009 ICC World Cup Qualifier.\n\nDuring the qualifier, Ahmadzai made his List-A debut for Afghanistan against Denmark. During the same tournament Afghanistan gained ODI status, with Ahmadzai making his One Day International debut against Scotland, where he scored 39 runs, helping Afghanistan to an 89 run victory.\n\nAhmadzai made his first class debut in the Intercontinental Cup against a Zimbabwe XI in which Afghanistan drew the match. Later, in November 2009 he was a member of Afghanistan's 2009 ACC Twenty20 Cup winning squad.\n\nAhmadzai made his full Twenty20 International debut against Ireland in the 2010 Quadrangular Twenty20 Series in Sri Lanka. Later on in February 2010, Ahmadzai was a key member of Afghanistan's victorious 2010 ICC World Twenty20 Qualifier winning squad and was later named in Afghanistan's squad for the 2010 ICC World Twenty20.\n\nIn April 2010, Ahmadzai was a key member of Afghanistan's 2010 ACC Trophy Elite winning squad which defeated Nepal in the final, with Ahmadzai scoring 52 runs in Afghanistan's innings; earning him the man of the match award. Ahmadzai played in both of Afghanistan's matches in Group C of the 2010 ICC World Twenty20. In their first match of the tournament he remained unbeaten on 5 against India and against South Africa he was caught behind by Mark Boucher off the bowling of Morné Morkel. Afghanistan lost both matches and were eliminated from the tournament.\n\nShortly before Afghanistan's match against South Africa, Ahmadzai announced he would retire following the match. He stated his reason for retiring being \"in order to focus on developing the younger generation of Afghan cricketers\". Ahmadzai has now taken up a coaching role with the national squad as well a chief selector for the Afghanistan Cricket Board. He is also a representative of Afghanaid.\n\n"}
{"id": "4892523", "url": "https://en.wikipedia.org/wiki?curid=4892523", "title": "Rodiya", "text": "Rodiya\n\nRodi or Rodiya are reported to be an untouchable social group or caste amongst the Sinhalese people of Sri Lanka. Their status was very similar to all the Untouchable castes of India with segregated communities, ritualised begging, eating off the refuse of upper castes and refusal for the women and men to cover their upper bodies.\n\nThe stories on the origin of the Rodi caste contradicts from text to text and are therefore far from certain. However, stories about their origin have been passed down orally by their generations. These folklore suggest their origin begins from the Sinhalese royalty. They believe they descended from the daughter of King Parakramabahu named Ratnavalli (also known as Navaratna Valli). Some of these stories are found in published documents as well. According to the Janawanshaya, a palm leaf record which was written in 15th century, that explains the caste hierarchy in Sri Lanka, Rodis were not considered to be purely a low caste group. According to Kandyan law, the worst punishment for high caste nobles was the exiling them to the Rodi caste. Robert Knox (sailor) and Hugh Nevill are two of the prominent writers who have mentioned about the Rodi Caste in their writings. Although these folklore do not provide much facts about the origins of the Rodi, they trace a connection between the daughter of King Parakramabahu and a Vedda with hunting, human sacrifice and cannibalism.\n\n\n\n"}
{"id": "34046933", "url": "https://en.wikipedia.org/wiki?curid=34046933", "title": "Sanga people", "text": "Sanga people\n\nThe Sanga people (also \"Luba-Garenganze\", \"Luba-Sanga\" or \"Southern Luba\") are an ethnic group that lives mostly in the Katanga Province of the Democratic Republic of the Congo.\n\nThe missionary Frederick Stanley Arnot relates that a copper trader named Kalasa became a close friend of the old chief of Sanga. At one point Kalasa's son Msidi (Msiri) visited the Sanga country instead of his father, where he found the people at war with the Baluba people, who were invading from the north. Msiri's party had four guns, unknown weapons in the area at that time, and a few shots from the guns put the Baluba to flight. The old chief was grateful, gave Msiri increasing power, end eventually made him his successor.\nMziri founded the state of Geranganze with its capital at Bunkeya and took the title of king in 1870. Shortly after this the Sanga people revolted against Msiri's rule, led by their chief Mpande.\n\nIn the 1890s the Sanga put up a strong resistance to the colonial \"Force Publique\" of King Leopold II of Belgium.\nIn one notorious incident, rebels led by a chief called Mulume Niama killed a Belgian officer.\nThe rebels were pursued by Congo Free State troops and trapped in a large chalk cave.\nWhen they refused to surrender, despite attempts to smoke them out, the cave was blocked up.\nThree months later, troops entered the cave and found 178 bodies.\nThe soldiers triggered landslides to cover the cave and destroy all evidence of what could come to be seen as a martyrdom.\n\nIn the 1950s many of the Sanga people obtained work in the mines of the Katanga copperbelt.\nMwenda Jean Bosco was the leading guitarist in Congo in the 1950s.\nHis other name is Mwenda wa Bayeke, based on a claim of descent from the Sanga noble clan of Bayeke.\nHis music draws on various sources including the traditional music of his Luba/Sanga people.\nIn 1991 the estimated population of Sanga people was about 431,000, scattered through the Lubudi, Mitwaba, and Pweto territories.\nThe Tumbwe people, a small group of about 100,000 people whose homeland is in the Kalemie Territory on the west shore of Lake Tanganyika, take their name from a hereditary chief of the Sanga people.\n\n"}
{"id": "5589562", "url": "https://en.wikipedia.org/wiki?curid=5589562", "title": "Sbitenshchik", "text": "Sbitenshchik\n\nSbitenshchik (Russian: сбитенщик) was a sbiten vendor and was spread in Old Rus' regions of Novgorod, Kiev, Moscow and other Rus' cities and regions. The vendor was used for the preparation and serving of the traditional honey-based beverage sbiten in Rus' that has been around since the 12th century. The vendor was normally used in winter as the drink was prepared in wintertime. Sbitenshchik was used in Rus' principalities often on the streets to cook the drink and to sell it to the freezing people. The vendor is documented in the Russian Lubok prints. \"Sbitenshchik\" became 1783 the main theme of the popular comic opera \"The Sbiten Vendor\" by Yakov Knyazhnin with music by Czech composer Antoine Bullant. \n"}
{"id": "762804", "url": "https://en.wikipedia.org/wiki?curid=762804", "title": "Sex assignment", "text": "Sex assignment\n\nSex assignment (sometimes known as gender assignment) is the determination of an infant's sex at birth. In the majority of births, a relative, midwife, nurse or physician inspects the genitalia when the baby is delivered, and sex and gender are assigned, without the expectation of ambiguity. Assignment may also be done prior to birth through prenatal sex discernment.\n\nSex assignment at birth usually aligns with a child's anatomical and biological sex. The number of births where the baby does not fit into strict definitions of male and female may be as high as 1.7%, of which 0.5% are due to visibly ambiguous genitals. Other reasons include atypical chromosomes, gonads, or hormones. These conditions are collectively called intersex or disorders of sex development, and may complicate sex assignment. Reinforcing sex assignments through surgical or hormonal interventions may violate the individual's human rights.\n\nThe act of assignment carries the implicit expectation that future gender identity will develop in alignment with the physical anatomy, assignment, and rearing. In the majority of cases, sex assignment matches the child's gender identity. If sex assignment and gender identity do not align, the person may be transgender or gender non-conforming (GNC). The sex assignment of an intersex individual may also contradict their future gender identity. \n\n\"Sex assignment\" is the determination of an infant's sex at birth. Terms that may be related to sex assignment are:\nAssigned male at birth (AMAB): a person of any age and irrespective of current gender whose sex assignment at birth resulted in a declaration of \"male\". For example, when an attending midwife or physician announces, \"It's a boy!\" Synonyms: male assigned at birth (MAAB) and designated male at birth (DMAB).\nAssigned female at birth (AFAB): a person of any age and irrespective of current gender whose sex assignment at birth resulted in a declaration of \"female\". For example, when an attending midwife or physician announces, \"It's a girl!\" Synonyms: female assigned at birth (FAAB) and designated female at birth (DFAB).\n\n\"Intersex\", in humans and other animals, describes variations in sex characteristics including chromosomes, gonads, sex hormones, or genitals that, according to the UN Office of the High Commissioner for Human Rights, \"do not fit typical binary notions of male or female bodies\". These may complicate the sex assignment of an infant at birth or lead to an assignment in conflict with some definitions of biological sex.\n\n\"Transgender\" people have a gender identity, or gender expression, that differs from their assigned sex. Transgender people are sometimes called \"transsexual\" if they desire medical assistance to transition from one sex to another. \n\n\"Sex reassignment\" : a treatment program consisting of a combination of psychological, medical, and surgical methods intended to physically change a person's sex to match their gender identity.\n\nThe discernment of an infant's sex was, until recently, almost universally considered an \"observation\" or \"recognition\" of an inherent aspect of a baby. The rationales for sex assignment and consequential registration appear to have been little questioned. A Dutch report on gender registration states that sex registration was introduced in 1811 as an intrinsic component in population registration, due to gender-specific rights and responsibilities, such as military conscription. Many discriminatory provisions in legislation no longer exist, but the provisions remain for rationales that include \"speed of identification procedures\".\n\nObservation or recognition of an infant's sex may be complicated in the case of intersex infants and children, and in cases of early trauma. In such cases, sex assignment is generally taken to require medical treatment to confirm that assignment, but this is disputed in part due to the human rights implications of such treatment.\n\n\"Intersex\" broadly denotes the presence of atypical sex characteristics: at least some aspect of the genitalia, internal organs, secondary sex characteristics, gonadal tissue, or chromosomes is more typical of the other sex. When the external genitalia appear to be in between, they are described as ambiguous. The intersex population is one that is not necessarily large. In the U.S., about 1 in 2000 babies born are intersex. Due to nonconsensual reassignment surgery many intersex people go about their lives in the gender that was chosen for them in a lot of cases. Not having the choice to choose their own gender can lead to depression and anxiety due to confusion and possible isolation. \n\nCases of trauma include the famous John/Joan case, where sexologist John Money claimed successful \"reassignment\" from male to female at age 17 months of a boy whose penis was destroyed during circumcision. However, this claim was later shown to be largely false. The subject, David Reimer, later identified as a man.\n\nIn approximately 1 in 2,000 infants, there is enough variation in the appearance of the external genitalia to merit hesitation about appropriate assignment by the physician involved. Typical examples would be an unusually prominent clitoris in an otherwise apparently typical girl, or complete cryptorchidism in an otherwise apparently typical boy. In most of these cases, a sex is tentatively assigned and the parents told that tests will be performed to confirm the apparent sex. Typical tests in this situation might include a pelvic ultrasound to determine the presence of a uterus, a testosterone or 17α-hydroxyprogesterone level, and/or a karyotype. In some of these cases a pediatric endocrinologist is consulted to confirm the tentative sex assignment. The expected assignment is usually confirmed within hours to a few days in these cases.\n\nIn a much smaller proportion of cases, the process of assignment is more complex, and involves both \"determining\" what the biological aspects of sex may be and \"choosing\" the best sex assignment for the purposes of rearing the child. Approximately 1 in 20,000 infants is born with enough ambiguity that assignment becomes a more drawn-out process of multiple tests and intensive education of the parents about sexual differentiation. In some of these cases, it is clear that the child will face physical difficulties or social stigma as they grow up, and deciding upon the sex of assignment involves weighing the advantages and disadvantages of either assignment.\n\nNothing currently appears to be known about sex discernment prior to the medicalization of intersex. However, in European societies, Roman law, post-classical Canon law, and later Common law, referred to a person's sex as male, female or hermaphrodite, with legal rights as male or female depending on the characteristics that appeared most dominant. Under Roman law, a hermaphrodite had to be classed as either male or female. The 12th-century \"Decretum Gratiani\" states that \"Whether an hermaphrodite may witness a testament, depends on which sex prevails\". The foundation of common law, the 16th Century \"Institutes of the Lawes of England\" described how a hermaphrodite could inherit \"either as male or female, according to that kind of sexe which doth prevaile.\" Legal cases where sex assignment was placed in doubt have been described over the centuries.\n\nWith the medicalization of intersex, criteria for assignment have evolved over the decades, as clinical understanding of biological factors and diagnostic tests have improved, as surgical techniques have changed and potential complications have become clearer, and in response to the outcomes and opinions of adults who have grown up with various intersex conditions.\n\nBefore the 1950s, assignment was based almost entirely on the appearance of the external genitalia. Although physicians recognized that there were conditions in which the apparent secondary sexual characteristics could develop contrary to the person's sex, and conditions in which the gonadal sex did not match that of the external genitalia, their ability to understand and diagnose such conditions in infancy was too poor to attempt to predict future development in most cases.\n\nIn the 1950s, endocrinologists developed a basic understanding of the major intersex conditions such as congenital adrenal hyperplasia (CAH), androgen insensitivity syndrome, and mixed gonadal dysgenesis. The discovery of cortisone allowed survival of infants with severe CAH for the first time. New hormone tests and karyotypes allowed more confident diagnosis in infancy and prediction of future development.\n\nSex assignment became more than choosing a sex of rearing, but also began to include surgical treatment. Undescended testes could be retrieved. A greatly enlarged clitoris could be amputated to the usual size, but attempts to create a penis were unsuccessful. John Money and others controversially believed that children were more likely to develop a gender identity that matched sex of rearing than might be determined by chromosomes, gonads, or hormones. The resulting medical model was termed the \"Optimal gender model.\"\n\nThe view of gender as a purely social construction, and gender identity as a result of nurture rather than nature reached near-universal acceptance, especially among liberal, progressive, and academic portions of Western society. The primary goal of assignment was to choose the sex that would lead to the least inconsistency between external anatomy and assigned psyche (gender identity). This led to the recommendation that any child without a penis or with a penis too small to penetrate a vagina could be raised as a girl, taught to be a girl, and would develop a female gender identity, and that this would be the best way to minimize future discrepancy between psyche and external anatomy in those infants determined to be biologically male but without a penis that meets medical norms (e.g., cloacal exstrophy), and also in those like in the John/Joan case who lost it to accidental trauma in early infancy.\n\nFrom the 1960s, pediatric surgeons attempted and claimed success with reconstruction of infant genitalia, especially enlargement or construction of vaginas. The recommended rules of assignment and surgery from the late 1960s until the 1990s were roughly:\n\nSince the 1990s, a number of factors have led to changes in the recommended criteria for assignment and surgery. These factors have included:\n\n\nClinical recommendations in the 2000s for assignment changed as a result:\n\n\nThese recommendations do not \"explicitly\" necessitate surgical or hormonal interventions to reinforce sex assignments, but such medical management persists worldwide, utilizing rationales such as the mitigation of parental distress and trauma, reducing the likelihood of stigma, making a child feel more \"normal\", and improving marriage prospects.\n\nControversies over surgical aspects of intersex management, have often focused on controversies regarding indications for surgery and optimal timing. However, intersex and human rights organizations have criticized medical models as they are not based on the consent of the individuals on whom such irreversible medical treatments are conducted, and outcomes may be inappropriate or poor. Anne Tamar-Mattis, for example, states that, \"The true choice is not between early and late [surgery], but early surgery versus patient autonomy. Human rights institutions now refer to such practices as \"harmful practices\".\n\nHowever, while surgical interventions remain experimental, and clinical confidence in constructing \"normal\" genital anatomies has not been borne out, medically credible pathways other than surgery do not yet exist. Changes to clinical recommendations in the current millennium do not yet address human rights concerns about consent, and the child's right to identity, privacy, freedom from torture and inhuman treatment, and physical integrity.\n\nIn 2011, Christiane Völling won the first successful case brought against a surgeon for non-consensual surgical intervention. The Regional Court of Cologne, Germany, awarded her €100,000.\n\nIn 2015, the Council of Europe recognized the right for intersex persons to \"not\" undergo sex assignment treatment, identifying issues with the pathologization of intersex bodies as inherently disordered. In April 2015, Malta became the first country to recognize a right to bodily integrity and physical autonomy, and outlaw non-consensual modifications to sex characteristics. The Act was widely welcomed by civil society organizations.\n\nSex reassignment is to a change in gender role or identity after an original and presumably incorrect sex assignment in infancy. This may occur in several types of circumstances.\n\nIn recent years, the perceived need to legally assign sex is increasingly being challenged. A report for the Dutch Ministry of Security and Justice states \"Gender increasingly seems to be perceived as a ‘sensitive’ identity feature, but so far is not regarded, nor protected as such in privacy regulations\". Australian government guidelines state that \"departments and agencies that collect sex and/or gender information must not collect information unless it is necessary for, or directly related to, one or more of the agency's functions or activities\"\n\n"}
{"id": "27493775", "url": "https://en.wikipedia.org/wiki?curid=27493775", "title": "Shower Posse", "text": "Shower Posse\n\nThe Shower Posse is a Jamaican gang which is involved with drug and arms smuggling. Its home is in Tivoli Gardens in Jamaica, but it primarily operates in the Canadian provinces of Ontario, Quebec, British Columbia, Alberta, Manitoba and the US states of New York, New Jersey, Florida, and Pennsylvania.\n\nThe gang has a strong international presence among expatriate Jamaican communities in North America. In the United States, a branch was founded by Vivian Blake and it has a prominent role in the New York City drug trade. The gang also has a large presence in Toronto.\n\n\"In 1989, former member Charles 'Little Nut' Miller was charged with drug trafficking but agreed to testify against other gang leaders in order to receive immunity. In his testimony – in which he implicated himself in nine murders - Miller revealed his connection to the JLP as a 'political enforcer,' as well as to the CIA, going as far to state that \"the United States made me what I am.\" (\"Newsweek\", July 13, 1998).\n\nThere are differing reports on the origin of the name. One theory is that it comes from the promises of its associated politicians to shower supporters with gifts. Another view is that it is a reference to the gang showering opponents with bullets. A third theory is that the gang got its name from the Jamaica Labour Party (JLP) election slogan 'Shower', which was a response to the PNP's 'Power' that was coined from Manley's 'Power for the people' slogan in the 1970s.\n\nIn 2009 the United States began to demand that Christopher Coke, then leader of the Shower Posse, with extensive and well-known links to the JLP, be extradited to New York, where he would face charges of smuggling drugs and weapons.\n\nThe prime minister of Jamaica, Bruce Golding, who was also the Minister of Parliament for that district, initially questioned the legality of the request, claiming that warrantless wiretapping had been used to collect information on Coke. However, he eventually relented, after public indignation to what many Jamaicans viewed as a cover-up to protect a politically connected drug trafficker, and on 17 May 2010 an arrest warrant was issued for Coke, leading to a state of civil unrest within Kingston, and especially Tivoli Gardens.\nCoke was eventually arrested outside of Kingston on 22 June 2010. On Friday, 15 June 2012, a New York federal district court sentenced Coke to two consecutive sentences: 20 years for racketeering and conspiracy, and an additional three years for conspiracy to commit assault.\n\nThe 2014 novel \"A Brief History of Seven Killings\", by Marlon James, features a gang called Storm Posse, who share many features with Shower Posse, based in a fictionalised version of Tivoli Gardens named \"Copenhagen City\".\n\nRapper Drake raps \"Where n***** go shower posse just to get by\" on OB O'Brien's 2014 song \"2 On/Thotful\" on which he is featured. \n"}
{"id": "48646435", "url": "https://en.wikipedia.org/wiki?curid=48646435", "title": "Shtisel", "text": "Shtisel\n\nShtisel (Hebrew: שטיסל) is an Israeli television drama series that follows the storyline of a fictional ultra-Orthodox Haredi Jewish family living in the Geula neighborhood of present-day Jerusalem. Creators and writers are Ori Elon and Yehonatan Indursky.\nThe show premiered on Yes Oh 29 June 2013 and has aired two seasons.\n\nIn October 2016, it was announced that Amazon.com had picked up Shtisel and was planning to remake it set in Brooklyn, NY under the title \"Emmis\".\n\n\n"}
{"id": "3434899", "url": "https://en.wikipedia.org/wiki?curid=3434899", "title": "Six Arts", "text": "Six Arts\n\nThe Six Arts formed the basis of education in ancient Chinese culture. \n\nDuring the Zhou Dynasty (1122–256 BCE), students were required to master the \"liù yì\" (六藝) (\"Six Arts\"):\n\n\nMen who excelled in these six arts were thought to have reached the state of perfection, a perfect gentleman.\n\nThe Six Arts were practiced by scholars and they already existed before Confucius, but became a part of Confucian philosophy. As such, Xu Gan (170–217 CE) discusses them in the \"Balanced Discourses\".\n\nThe Six Arts were practiced by the 72 disciples of Confucius.\n\nThe Six Arts concept developed during the pre-imperial period. It incorporated both military and civil components. The civil side was later associated with the Four Arts (qin playing, chess, calligraphy and painting). However, the latter was more a leisure characteristic for the late imperial time. It evidently overlaps with the Six Arts, since the qin epitomized music, the chess (Go, a board-game known by its Japanese name) related to the military strategy, while calligraphy dealt with the aesthetics of writing and the character cultivation (the rites).\n\nMath, calligraphy, literature, equestrianism, archery, music, and rites were the Six Arts.\n\nThe requirement of students to master the six arts parallels the Western concept of the Renaissance man. The emphasis on the Six Arts bred Confucian gentlemen who knew more than just canonical scholarship. The classical interest practical scholarship invigorated Chinese mathematics, astronomy, and science (e.g. Liu Hui, Zu Chongzhi, Shen Kuo, Yang Hui, Zhu Shijie). This tradition receded after the Yuan dynasty (1271–1368), when neo-Confucianism underscored the importance of the four books \"Analects\" over the other arts and technical fields.\n\nAt the Guozijian, law, math, calligraphy, equestrianism, and archery were emphasized by the Ming Hongwu Emperor in addition to Confucian classics and also required in the Imperial Examinations. Archery and equestrianism were added to the exam by Hongwu in 1370 like how archery and equestrianism were required for non-military officials at the 武舉 College of War in 1162 by the Song Emperor Xiaozong. The area around the Meridian Gate of Nanjing was used for archery by guards and generals under Hongwu.\nBy the Qing dynasty, the Chinese specialists were not able to manage the lunar calendar accurately, and the calendar was going out of phase with nature. This was a great embarrassment to the Chinese court, as the adherence to the lunar calendars by the vassal states was a recognition of the sovereignty of the Chinese court over them. Western astronomical expertise (see Jesuit China missions) was welcomed much as an aftermath of Chinese interest in astronomy and mathematics, partially formulated in the classical Six Arts agenda.\n\n"}
{"id": "29597238", "url": "https://en.wikipedia.org/wiki?curid=29597238", "title": "The Cool Stuff Collective", "text": "The Cool Stuff Collective\n\nThe Cool Stuff Collective is a British children's television programme, which was produced for three series that aired on the ITV Network and CITV respectively from 13 September 2010 to 24 December 2011. The show featured reviews of the latest and forthcoming video games, gadgets, films and music. CITV also broadcast a similar format programme called \"Play The Game\" in 2004. Sy Thomas presented the programme for the majority of its run, with Vicky Letch and The Blowfish later taking over for the final series.\n\n\nIn 2011, the programme was found to have breached Ofcom's rules on product placement. Ofcom stated that many of the items featured on the show ended with \"overt encouragements\" to consider purchase of the product.\n"}
{"id": "17571615", "url": "https://en.wikipedia.org/wiki?curid=17571615", "title": "The Freudian Coverup", "text": "The Freudian Coverup\n\nThe Freudian Cover-up is a theory first popularized by social worker Florence Rush in the 1970s, which asserts that Sigmund Freud intentionally ignored evidence that his patients were victims of sexual abuse. The theory argues that in developing his theory of infant sexuality, he misinterpreted his patients' claim of sexual abuse as symptoms of repressed incestuous desire. Therefore, Freud claimed that children who reported sexual abuse by adults had either imagined or fantasized the experience.\n\nEarly within Freud's career, he believed that little girls often experienced sexual abuse, since most of his patients were predominantly women and consistently reported childhood instances of sexual molestation. Many of Freud's patients suffered from a common Victorian diagnosis, hysteria. Since his hysterical patients repeatedly reported sexual abuse, most often naming their fathers as the abusers, Freud drew a causal connection between sexual abuse and neurosis. This became the frame for the seduction theory, in which he pointed to a direct connection between sexual abuse in childhood and adult hysteria. According to Florence Rush, author of \"The Freudian Cover-up\", this repeated and persistent incrimination of fathers by his patients made him uneasy, and led him to abandon the seduction theory. More at ease with the fantasy rather than reality of sexual abuse, Freud was even more comfortable when he could name the mother rather than the father as the seducer. Hence, the \"Oedipal complex\" came into fruition. Other feminists who supported Rush's claims are Susan Brownmiller, Louise Armstrong, and Diana Russell. \n\nBefore Freud could conclude that the seduction by fathers was a fantasy, he had to be rid of his earlier theory. Since men did not complain of maternal seduction Freud limited the imagined abuse to a specific female problem. To remove the responsibility from fathers, Freud found it necessary to undermine the perceptions of his female patients.\n\nWithin the period between the 1970s and 1980s, and 1990s arguments were made that Freud abandoned his initial beliefs in women's accounts of abuse (the seduction theory), and replaced it with the Oedipal theory; this illustrates the ways in which he withheld or altered information from his patients, which is unacceptable in a professional context. \"The Freudian Cover-up\" exposed Freud’s theory, the refusal to name the offender, but furthermore, one man's attempt to hide illegal or immoral sex practices. It was within this time that Victorian men were permitted to indulge in forbidden sex, provided they managed to keep their indiscretions hidden. Freud, who regarded the incest taboo as vital to the advance of civilization, appeared to demand only that forbidden sex be practiced with tact and discretion so that the surface of Victorian respectability was in no way disturbed. Therefore, any attempt on the part of the child or her family to expose the violator exposes her own alleged innate sexual motives and shamed her more than the offender; concealment is her only recourse.\n\nThe historian Peter Gay, author of \"\" (1988), emphasizes that Freud continued to believe that some patients were sexually abused, but realized that there was a difficulty in determining between truth and fiction. Therefore, according to Gay there was no sinister motive in changing his theory; Freud was a scientist seeking the facts and was entitled to change his views if new evidence was presented to him.\n\nA different criticism comes from Freud scholars who have examined the original documents and argue that the above account contains several misconceptions. Florence Rush based her account on Freud's later retrospective reports of the 1895-97 episode, which are seriously at variance with the original 1896 papers and other documents which show that it is not the case that Freud's female patients at that time consistently reported childhood instances of sexual molestation. Prior to the 1896 papers he had not reported a single instance of \"early childhood\" sexual abuse (and very few cases of any kind of sexual abuse). The very essence of the seduction theory entailed that only \"unconscious\" memories of early childhood sexual abuse could result in hysterical or obsessional symptoms, which is inconsistent with the notion of patients coming to him with \"reports\" of childhood sexual abuse; on Freud's theory the putative memories were deeply repressed and not accessible to consciousness in normal circumstances. (It is also the case that Freud's 1896 clinical claims were not restricted to women: in the 1896 \"Aetiology\" paper one third of the patients were men.)\n\nFreud twice stated that he would be presenting the clinical evidence for his claims, but he never did so, which critics have argued means that his clinical claims have had to be taken largely on trust. Numerous Freud scholars and academics have voiced serious doubts about the validity of his claim in 1896 to have uncovered \"unconscious memories\" (later unconscious fantasies) of infantile sexual abuse, mostly below the age of four.\n\nRush is often seen to be the founder of the recovered memory movement. This has been criticised as characterised by both a completely uncritical acceptance of accusations of abuse and an active encouragement of the invention of memories of abuse. There are some who are very critical of Freud, such as Richard Webster, who are also critical of Rush.\n\n\n"}
{"id": "3302506", "url": "https://en.wikipedia.org/wiki?curid=3302506", "title": "Tributary state", "text": "Tributary state\n\nA tributary state is a term for a pre-modern state in a particular type of subordinate relationship to a more powerful state which involved the sending of a regular token of submission, or tribute, to the superior power. This token often took the form of a substantial transfer of wealth, such as the delivery of gold, produce or slaves, so that tribute might best be seen as the payment of protection money. Or it might be more symbolic: sometimes it amounted to no more than the delivery of a mark of submission such as the bunga mas (golden flower) that rulers in the Malay peninsula used to send to the kings of Siam, or the Tribute of the Maltese Falcon that the Grand Master of the Order of St. John used to send annually to the Viceroy of Sicily in order to rule Malta. It might also involve attendance by the subordinate ruler at the court of the hegemon in order to make a public show of submission.\n\nThe modern-day heirs of tribute hegemons tend to claim that the tributary relationship should be understood as an acknowledgement of the hegemon's sovereignty in the modern world, whereas former tributary states deny that there was any transfer of sovereignty. For instance, Tributaries of Imperial China implies Chinese sovereign claim over territories not now regarded as Chinese.\n\nA formalized tribute system developed in East Asia with many neighboring East, Central, Southeast and South Asian countries and regions becoming tributary states of various Imperial Chinese dynasties. Historically, the Emperor of China saw himself as the emperor of the entire civilized world. It was not possible for such an emperor to have equal diplomatic relations with any other power, and so all diplomatic relations in the region were construed by the Chinese as tributary. The disdain of the state ideology of Confucianism for trade, and the belief that Chinese civilization had no need of products or technology from outside meant that trade, when it was permitted, was also construed as tributary. Diplomatic missions and trading parties from non-Chinese regions were interpreted in Chinese records as being tributary, regardless of the intention of those regions. Under this construction, the goods received by China constituted a tributary offering, while those that the visitors received were interpreted as gifts that the emperor in his kindness had bestowed upon his distant tributaries.\n\nIn Al Andalus, the last remaining Moorish Nasrid Dynasty in the Emirate of Granada paid tribute to the Christian Kingdom of Castile (present day Spain). Tributary states, usually on the periphery of the Ottoman Empire, were under vassalage in different forms. Some were allowed to select their own leaders, while others paid tribute for their lands. In the Western colonial system, non-Western states were sometimes incorporated into a European empire as protectorates.\n\nIn the Philippines, the Datus of the Barangays became vassals of the Spanish Empire, from the late 16th century until the Archipelago fell under the power of the United States of America in 1898. Their right to rule was recognized by King Philip II of Spain, on 11 June 1594, under the condition of paying tributes due to the Spanish Crown.\n\nFor modern forms of state subordination, see puppet state, satellite state and client state.\n\n"}
{"id": "3343584", "url": "https://en.wikipedia.org/wiki?curid=3343584", "title": "Want", "text": "Want\n\nThe idea of want can be examined from many perspectives. In secular societies want might be considered similar to the emotion desire, which can be studied scientifically through the disciplines of psychology or sociology. Want might also be examined in economics as a necessary ingredient in sustaining and perpetuating capitalist societies that are organised around principles like consumerism. Alternatively want can be studied in a non-secular, spiritual, moralistic or religious way, particularly by Buddhism but also Christianity, Islam and Judaism.\n\nIn economics, a want is something that is desired. It is said that every person has unlimited wants, but limited resources (economics is based on the assumption that only limited resources are available to us). Thus, people cannot have everything they want and must look for the most affordable alternatives.\n\nWants are often distinguished from needs. A need is something that is necessary for survival (such as food and shelter), whereas a want is simply something that a person would like to have. Some economists have rejected this distinction and maintain that all of these are simply wants, with varying levels of importance. By this viewpoint, wants and needs can be understood as examples of the overall concept of demand.\n\nWhile in modern secular societies \"want\" is considered a purely economic, social-scientific or objectively psychological reality of human existence, many religious or spiritual traditions prescribe or advise with lessons on want and wanting, which might alternatively be termed \"desire\". Buddhism is perhaps the most common example of a religious tradition that offers wisdom and advice about the concept of want and wanting or \"desire\". The second of the Four Noble Truths of Buddhism is that desire or wanting is a cause for most of the suffering experienced in life. When we want and desire, we create suffering that can never be alleviated, because as detailed in secular economics wants are \"unlimited\", and hence unfulfilled wants can cause suffering, in unlimited amount. Challenges to this dilemma might include anti-consumerism or Buddhist economics.\n\nIn Christianity, particularly Protestantism, want should be kept to a minimum, and a simple life of hard and decent work should be maintained, as described in the Protestant work ethic. From an economic-sociological point of view this might be understood as more value and energy being placed upon production instead of consumption.\n\n"}
{"id": "17114505", "url": "https://en.wikipedia.org/wiki?curid=17114505", "title": "Xokleng", "text": "Xokleng\n\nThe Xokleng or Aweikoma (sometimes called botocudos) are a Native American tribe of Brazil; their territory is located mainly in the state of Santa Catarina. They were one of the original inhabitants of the province of Misiones in Argentina. They may also sometimes be found on the Ibirama, Posto Velho, and Rio dos Pardos reservations.\n"}
