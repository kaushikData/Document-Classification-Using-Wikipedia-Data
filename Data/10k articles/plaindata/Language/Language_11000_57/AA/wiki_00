{"id": "5973269", "url": "https://en.wikipedia.org/wiki?curid=5973269", "title": "Abed Chaudhury", "text": "Abed Chaudhury\n\nAbed Chaudhury (born February 1956) is a Bangladeshi geneticist and science writer living in Australia was born in Moulvibazar.\n\nChaudhury was a student of Moulvibazar Government High School.\nChaudhury was a student of Notre Dame College of Dhaka.\n\nCompleting his bachelor's degree in chemistry from Dhaka University, Chaudhury got his PhD from University of Oregon and did post-doctoral research at National Institutes of Health and Massachusetts Institute of Technology in the United States, and was a visiting scientist at École Normale Supérieure in France.\n\nChaudhury is a well-known geneticist and writer in both Bengali and English who together with colleagues in Australia isolated the FIS (fertilization independent seed) genes; fis mutants can produce partial seed without fertilization. These are the first characterised genes involved in apomixis, a method of making seed without the father.\n\nHe is leading a group of scientists in Australia in his research.\n\nChaudhury is currently living in Australia.\n\nHis first book \"Shoibal O Ontorikko\" () was an anthology of poems published by Dibya Prokash in 1999.\n\n\n\nHe has written many articles in several science journals for scientists and in normal papers for general people in Bengali and English.\n\n"}
{"id": "38747957", "url": "https://en.wikipedia.org/wiki?curid=38747957", "title": "Amarizana language", "text": "Amarizana language\n\nAmarizana is an extinct, poorly attested, and unclassified Arawakan language. Kaufman (1994) placed it in his Piapoko branch, but this is not followed in Aikhenvald (1999).\n"}
{"id": "54863213", "url": "https://en.wikipedia.org/wiki?curid=54863213", "title": "Ameer Bux Shar", "text": "Ameer Bux Shar\n\nAmir Bux Shar was a notable Sindhi-language poet and writer of Pakistan.\n"}
{"id": "45422859", "url": "https://en.wikipedia.org/wiki?curid=45422859", "title": "Arib al-Ma'muniyya", "text": "Arib al-Ma'muniyya\n\n‘Arīb al-Ma’mūnīya (, b. 181/797-98, d. 277/890-91) was a \"qayna\" (slave trained in the arts of entertainment) of the early Abbasid period, who has been characterised as 'the most famous slave singer to have ever resided at the Baghdad court'. She lived to 96, and her career spanned the courts of five caliphs.\n\nThe main source for ‘Arīb's life is the tenth-century \"Kitāb al-Aghānī\" of Abū ’l-Faraj al-Iṣfahānī:\n\nLike her peers, he tells us, ‘Arīb was versed in poetry, composition and music performance, along with sundry other skills, backgammon, chess and calligraphy among them. Her chosen instrument was the oud, a preference she would pass on to her students, but, above all, it was her singing and composition that stood out. Citing one of his key sources, Ibn al-Mu‘tazz, Abū ’l-Faraj refers to a collection of notebooks (\"dafātir\") and loose sheets (\"ṣuḥuf\") containing her songs. These are said to have numbered around 1,000. As regards her singing, Abū ’l-Faraj declares that she knew no rival among her peers. He groups her, alone among them, with the legendary divas of the earliest Islamic period, the singers known collectively as the \"Ḥijāzīyāt\".\n\nBorn in Baghdad, Iraq, ‘Arīb was rumoured in the Middle Ages to be the daughter of vizier Ja'far al-Barmaki, a key member of the Barmakids, and one of the family's domestic servants, Fāṭima. This parentage has been questioned by modern scholars. Either way, she was clearly a slave for important portions of her early life, whether born into slavery or sold into slavery as a ten-year-old following her family's downfall. ‘Arīb's own poetry twice protests at her servile status, and she was manumitted by Abū Isḥāq al-Mu‘taṣim (r. 833-42). She allegedly rose to being the favourite singer of Caliph al-Maʾmūn (r. 813-33).\n\n‘Arīb's surviving oeuvre and associated anecdotes suggest not ony her poetic skills, but also a life in which she had a number of relationships with male lovers and patrons, indicating 'that ‘Arīb, like many of her peers, was a concubine as well as a singer when circumstances required'. It appears that she came to maintain a substantial entourage of her own and was a landowner. One of the most famous stories attached to her concerns a singing contest which she and her singing-girls won against her younger rival Shāriyah and her troupe. The evidence suggests a figure who was 'willful, deeply intelligent, impatient with those of lesser wits and, perhaps inevitably, bemused and often cynical'.\n\nAn example of ‘Arīb's verse is the following:\n\nIf the early biographical information is correct, ‘Arīb died at the age of 96.\n"}
{"id": "48995800", "url": "https://en.wikipedia.org/wiki?curid=48995800", "title": "Artikulation (Ligeti)", "text": "Artikulation (Ligeti)\n\nArtikulation is an electronic composition by György Ligeti. Composed and notated in January and February 1958, the piece was prepared and recorded on magnetic tape from February to March with the assistance of Gottfried Michael Koenig and Karlheinz Stockhausen's assistant, Cornelius Cardew, at the Studio for Electronic Music of the West German Radio (WDR) in Cologne. The piece consists of various types of sounds, \"in conditions of aggregation.\" It \"can be heard as a conversation without words.\" Ligeti explains in notes to the listening score (see below):\n\nThe (3:53-55 long) piece, in quadraphonic sound, was premiered March 25, 1958 at WDR Cologne's 'Musik der Zeit' concert series and September 4, 1958 at Darmstadt. It was heard again March 1993 at the New England Conservatory, while for recordings it has been mixed down to stereophonic sound.\n\nLigeti had just fled from Budapest to Cologne in 1956, and \"Artikulation\" is the only one of three electronic pieces written in Cologne which remain in Ligeti's catalogue.\nHe completed only two works in the electronic medium, however—\"Glissandi\" (1957) and \"Artikulation\" (1958)—before returning to instrumental music. A third work, originally entitled \"Atmosphères\" but later known as \"Pièce électronique Nr. 3\", was planned, and though the technical limitations of the time prevented Ligeti from realizing it completely, it was finally realized in 1996 by the Dutch composers Kees Tazelaar and Johan van Kreij of the Institute of Sonology.\nSee: List of compositions by György Ligeti. In composing \"Artikulation\" Ligeti, like many composers around him, was inspired by, \"the age-old question of the relationship between music and speech,\" their approach greatly inspired by phoneticist Werner Meyer-Eppler. \"AllMusic\"'s Blair Sanderson describes \"Artikulation\" as, \"difficult to judge from its brevity and similarity to other tape experiments of the time.\"\n\nLigeti used both chance, such as in selection of sound segments, and an overall plan; all related to phonetics. \"Part serial, part empirical, part aleatoric.\" Ligeti used technology including sine wave, white noise, and impulse generators, as well as filters.\n\nHaving conceived of many various possible and artificial phonemes, created recordings of them, and grouped them into various categories and bins, Ligeti created a formula (and many tables) to determine the maximum length of each tape used (the louder the shorter), and then went through a process of grabbing randomly, without-looking, similar \"phonemes\" out of their bins, combining them into \"texts\", and then cutting these in half, down to \"words\".\n\nDespite this process, the piece has been described as, \"spontaneous, even witty,\" \"humorous,\" and as, \"influential.\" Holmes argues that though Ligeti abandoned electronic music after \"Glissandi\" and \"Artikulation\", \"it seems clear he could not have conceived some of his later works [such as \"Atmosphères\"] had he not learned the techniques of composing with slowly modulating textures and timbres that came with producing tape music.\"\n\nFred Lerdahl argues that discretization is necessary not only for musical analysis but also for perception even by learned listeners, and thus that pieces such as \"Artikulation\" are inaccessible. In contrast, in 1970 graphic designer Rainer Wehinger (State University of Music and Performing Arts Stuttgart) created a \"Hörpartitur\" or \"score for listening\" for the piece, representing different sonorous effects with specific graphic symbols much like a transcription. Despite Ligeti's original notation, consisting of a large number of charts and tables, this score, approved by the composer, has been described as having, \"a unique and appealing aesthetic,\" and as being, \"easy to follow when viewed aligned with the music.\" As shown in the key to the score:\n\nThe depiction of frequency using the y-axis may be, \"very approximate.\" The score was later synced to the music and posted on YouTube. Though Holmes finds, \"the artistic value in visualizing this work...plain to see,\" Taruskin argues the score is, \"decorative or celebratory...rather than...practical.\"\n\nTom Service of \"The Guardian\" argues that even prior to Wehinger's score and its animation, \"Ligeti himself imagined the sounds of \"Artikulation\" conjuring up images and ideas of labyrinths, texts, dialogues, insects, catastrophes, transformations, disappearances.\"\n\n"}
{"id": "38521391", "url": "https://en.wikipedia.org/wiki?curid=38521391", "title": "Bentong language", "text": "Bentong language\n\nBentong is an Austronesian language of Sulawesi, Indonesia, that is closely related to Makassarese.\n"}
{"id": "2870610", "url": "https://en.wikipedia.org/wiki?curid=2870610", "title": "Breathing (lens)", "text": "Breathing (lens)\n\nBreathing refers to the shifting of angle of view of a lens when changing the focus. Some (often higher quality) lenses are designed to lessen the degree of this effect. Lens breathing does not prevent one from racking focus or following focus with this lens, but it lessens the desirability of any type of focus adjustment, since it noticeably changes the composition of the shot. This is not to be confused with the suction and expulsion of air from within the lens as its internal volume changes.\n"}
{"id": "44682", "url": "https://en.wikipedia.org/wiki?curid=44682", "title": "CMYK color model", "text": "CMYK color model\n\nThe CMYK color model (process color, four color) is a subtractive color model, used in color printing, and is also used to describe the printing process itself. CMYK refers to the four inks used in some color printing: cyan, magenta, yellow, and black.\n\nThe CMYK model works by partially or entirely masking colors on a lighter, usually white, background. The ink reduces the light that would otherwise be reflected. Such a model is called \"subtractive\" because inks \"subtract\" the colors red, green and blue from white light. White light minus red leaves cyan, white light minus green leaves magenta, and white light minus blue leaves yellow.\n\nIn additive color models, such as RGB, white is the \"additive\" combination of all primary colored lights, while black is the absence of light. In the CMYK model, it is the opposite: white is the natural color of the paper or other background, while black results from a full combination of colored inks. To save cost on ink, and to produce deeper black tones, unsaturated and dark colors are produced by using black ink instead of the combination of cyan, magenta, and yellow.\n\nWith CMYK printing, \"halftoning\" (also called \"screening\") allows for less than full saturation of the primary colors; tiny dots of each primary color are printed in a pattern small enough that humans perceive a solid color. Magenta printed with a 20% halftone, for example, produces a pink color, because the eye perceives the tiny magenta dots on the large white paper as lighter and less saturated than the color of pure magenta ink.\n\nWithout halftoning, the three primary process colors could be printed only as solid blocks of color, and therefore could produce only seven colors: the three primaries themselves, plus three secondary colors produced by layering two of the primaries: cyan and yellow produce green, cyan and magenta produce blue, yellow and magenta produce red (these subtractive secondary colors correspond roughly to the additive primary colors), plus layering all three of them resulting in black. With halftoning, a full continuous range of colors can be produced.\n\nTo improve print quality and reduce moiré patterns, the screen for each color is set at a different angle. While the angles depend on how many colors are used and the preference of the press operator, typical CMYK process printing uses any of the following screen angles:\n\nThe \"black\" generated by mixing commercially practical cyan, magenta, and yellow inks is unsatisfactory, so four-color printing uses black ink in addition to the subtractive primaries. Common reasons for using black ink include:\n\nWhen a very dark area is desirable, a colored or gray CMY \"bedding\" is applied first, then a full black layer is applied on top, making a rich, deep black; this is called \"rich black\". A black made with just CMY inks is sometimes called a \"composite black\".\n\nThe amount of black to use to replace amounts of the other ink is variable, and the choice depends on the technology, paper and ink in use. Processes called under color removal, under color addition, and gray component replacement are used to decide on the final mix; different CMYK recipes will be used depending on the printing task.\n\nCMYK or process color printing is contrasted with spot color printing, in which specific colored inks are used to generate the colors appearing on paper. Some printing presses are capable of printing with both four-color process inks and additional spot color inks at the same time. High-quality printed materials, such as marketing brochures and books, often include photographs requiring process-color printing, other graphic effects requiring spot colors (such as metallic inks), and finishes such as varnish, which enhances the glossy appearance of the printed piece.\n\nCMYK are the process printers which often have a relatively small color gamut. Processes such as Pantone's proprietary six-color (CMYKOG) Hexachrome considerably expand the gamut. Light, saturated colors often cannot be created with CMYK, and light colors in general may make visible the halftone pattern. Using a CcMmYK process, with the addition of light cyan and magenta inks to CMYK, can solve these problems, and such a process is used by many inkjet printers, including desktop models.\n\nComparisons between RGB displays and CMYK prints can be difficult, since the color reproduction technologies and properties are very different. A computer monitor mixes shades of red, green, and blue light to create color pictures. A CMYK printer instead uses light-absorbing cyan, magenta, and yellow inks, whose colors are mixed using dithering, halftoning, or some other optical technique.\n\nSimilar to monitors, the inks used in printing produce a color gamut that is \"only a subset of the visible spectrum\" although both color modes have their own specific ranges. As a result of this, items which are displayed on a computer monitor may not completely match the look of items which are printed if opposite color modes are being combined in both mediums. When designing items to be printed, designers view the colors which they are choosing on an RGB color mode (their computer screen), and it is often difficult to visualize the way in which the color will turn out post-printing because of this.\n\nTo reproduce color, the CMYK color model codes for absorbing light rather than emitting it (as is assumed by RGB). The 'K' component absorbs all wavelengths and is therefore achromatic. The Cyan, Magenta, and Yellow components are used for color reproduction and they may be viewed as the inverse of RGB. Cyan absorbs Red, Magenta absorbs Green, and Yellow absorbs Blue (-R,-G,-B).\n\nSince RGB and CMYK spaces are both device-dependent spaces, there is no simple or general conversion formula that converts between them. Conversions are generally done through color management systems, using color profiles that describe the spaces being converted. Nevertheless, the conversions cannot be exact, particularly where these spaces have different gamuts.\n\nThe problem of computing a colorimetric estimate of the color that results from printing various combinations of ink has been addressed by many scientists.\nA general method that has emerged for the case of halftone printing is to treat each tiny overlap of color dots as one of 8 (combinations of CMY) or of 16 (combinations of CMYK) colors, which in this context are known as \"Neugebauer primaries\". The resultant color would be an area-weighted colorimetric combination of these primary colors, except that the \"Yule–Nielsen\" effect of scattered light between and within the areas complicates the physics and the analysis; empirical formulas for such analysis have been developed, in terms of detailed dye combination absorption spectra and empirical parameters.\n\n\n"}
{"id": "1468228", "url": "https://en.wikipedia.org/wiki?curid=1468228", "title": "Cabin (ship)", "text": "Cabin (ship)\n\nA cabin or berthing is an enclosed space generally on a ship or an aircraft. A cabin which protrudes above the level of a ship's deck may be referred to as a deckhouse.\n\nIn sailing ships, the officers and paying passengers would have an individual or shared cabin. The captain or commanding officer would occupy the \"great cabin\" that normally spanned width of the stern with large windows. On a warship, it would be separated from the rest of the ship, and further subdivided into day and night cabins with movable panels that could be removed in time of battle to leave the deck clear the whole length of the ship.\n\nIn most modern warships, the commanding officer has a main cabin—the in-port cabin, often adjacent to the ship's central control room (operations room)—and a sea cabin adjacent to the bridge. Thus, when likely to be called from sleep or attending to administration, the CO can go to the sea cabin and thereby be able to appear at the Bridge or Ops room immediately. The sea cabin is sparsely equipped, containing just a bunk, a desk, and basic toilet facilities. The in-port cabin is more lavishly furnished, with separate bedroom and combination sitting room/office, and more elaborate toiletry facilities.\n\nFor ships intended to act as flagships, like the aircraft carrier USS \"Lexington\", the admiral also has a sea cabin (adjacent to the captain's sea cabin) and an in-port cabin, in addition to the captain's cabins. Admiral Fletcher's sea cabin in the in World War II had a bed, an easy chair, a table, and a shower.\n\nOfficers will normally have their own cabins, which double as their offices. Some senior petty officers may have cabins for similar reasons.\n\nSailors sleep in berthing areas.\n\nIn ships carrying passengers, they are normally accommodated in cabins, taking the terminology familiar to seafarers. First-class cabins were traditionally referred to as staterooms, and today many cruise lines now prefer to refer to passenger cabins as staterooms or suites.\n\nIn cruise ship terms, a cabin crawl is an event where passengers tour the cabins of fellow passengers. A cruise ship may also offer a cabin crawl of cabins or suites which did not sell for a particular sailing. The purpose of a cabin crawl is to give passengers an idea of the space and layout of various cabin options for their next cruise. Cabin crawls are normally organized prior to a cruise, through cruise-fan websites.\n\nInside spacecraft, the cabins are required to fully supply food and oxygen for their crew. On missions lasting a year or longer, the cabins have to be self-sustaining, i.e. replenish their own water and oxygen. The space cabin for any long-range manned mission is expected to be reasonably spacious, with approximately 28 cubic metres allotted to each occupant. In addition, cabins have life support systems that should have the capability to meet a variety of off-nominal conditions, including cabin fires, depressurization, and component shutdown or failure. Frequently, these conditions occur so quickly that recovery can be provided only by automatic control systems. In the late 1960s, several experimental ground facilities were developed to evaluate regenerative life support systems for crewed space flight.\n\n"}
{"id": "17565049", "url": "https://en.wikipedia.org/wiki?curid=17565049", "title": "Clean feed (television)", "text": "Clean feed (television)\n\nIn television technology, a clean feed is a video signal that does not have added graphics and text. This video signal is used in sport production to allow different television stations to add their own digital on-screen graphic image on a common signal, or in news broadcasting to produce two or more different streams, each one with the same picture but in different languages.\n\nA clean feed is a signal which has not come from the main output of the video switcher, such as the output of a vision mixer before the downstream keyer stage - the clean feed is identical to the main program output but without any captions keyed into it. Modern production equipment can actually put different keys on multiple outputs, allowing them to go to the clean feed or not. The most sophisticated vision mixers (or production switchers, according to the American nomenclature) can generate a clean feed output for any of their mix/effects (ME) buses.\n\nThe term \"clean feed\" is also used to refer to backhaul feeds of television programming sent via communication satellite or other transport (such as a national fiber-optic network) sent from another TV station or remote television production truck on-location, which does not carry any television advertisements or break bumpers, or in some cases, lower-third graphics or superimposed chyron text.\n\n"}
{"id": "2224208", "url": "https://en.wikipedia.org/wiki?curid=2224208", "title": "Cleanskin (security)", "text": "Cleanskin (security)\n\nWithin the vernacular of counter-terrorism agents and police officers, a cleanskin is an undercover operative whose identity is not known to the forces he or she is tasked to infiltrate. This is usually because such an agent has not conducted any prior undercover activity.\n\nThe phrase entered wide currency with a slightly different meaning in the United Kingdom following the London bombings of 7 July 2005. The four bombers involved in those bombings were reported in the press to be \"cleanskins\", according to police sources, meaning that their profiles did not fit the expected profile of bombers.\n\nTerrorist groups, smugglers, and others performing secretive activities prefer to subvert cleanskins as there is less chance that they will arouse suspicion. For example, a person with previous convictions for importing drugs is more likely to be detained than a person never convicted.\n\n"}
{"id": "3289570", "url": "https://en.wikipedia.org/wiki?curid=3289570", "title": "Cognitive advantages of multilingualism", "text": "Cognitive advantages of multilingualism\n\nA bilingual person can traditionally be defined as an individual who uses (understands and produces) two (or more) languages on a regular basis. A bilingual person's initial exposure to both languages may have started in early childhood, e.g. before age 3, but exposure may also begin later in life. While some people assume that bilinguals must be equally proficient in their languages, perfectly equal proficiency is rarely attested, and proficiency typically varies by domain. For example, a bilingual person may have greater proficiency for work-related terms in one language, and family-related terms in another language.\n\nBeing bilingual has been linked to a number of cognitive benefits. Research has studied how a bilingual individual's first language (L1) and second language (L2) interact, and it has been shown that both languages have an influence on the function of one another, and on cognitive function outside of language. Research on executive functions such as working memory, perception, and attentional and inhibitory control, has suggested that bilinguals can benefit from significant cognitive advantages over monolingual peers in various settings. There are also age-related benefits, which seem to help older adults on the battle against cognitive decline.\n\nThroughout the history of research into the cognitive advantages of bilingualism, views have shifted from a subtractive to an additive perspective; that is from believing that being bilingual detracts from one's abilities, to believing that being bilingual adds to an individual's abilities.\n\nThere is, however, strong disagreement over how findings on this subject should be interpreted. Systematic reviews and meta-analyses of studies in the field have found no evidence for cognitive advantages in healthy adults and that reporting is subject to publication bias, which has therefore given a distorted view of the evidence.\n\nOver the course of the past few years, the prevalence of bilinguals in the United States has increased dramatically. While the United States Census Bureau does not directly poll for bilingualism, they do poll for what languages are used in an individual's home, and if it is a language other than English, they then poll for how well that same individual speaks English. In 2012, François Grosjean, a professor of Linguistics from the University of Neuchâtel, interpreted the results from the Census Bureau as follows: 11% of the population was bilingual in 1980, 14% in 1990, and 20% in 2012. This positive increase raised a question into the depth of cognitive activity in bilinguals and whether or not there were benefits in multilingualism to be found.\n\nAccording to the Singapore Management University (SMU) School of Social Sciences, research before the 1960s on bilingual individuals was varied, but commonly supporting the idea that there were disadvantages to bilingualism. A general opinion was that bilinguals would have smaller vocabularies, stunted cognitive abilities and that children learning two languages from a young age would be spending too much of their energy differentiating and building the two languages to become competent in either one. This information alluded to the idea that being bilingual was detrimental to a child's linguistic and cognitive development. According to a journal called \"The Journal of Genetic Psychology\" various reported studies at the time that held these perceptions held a similar view on bilingualism, and it was referred to as a \"problem of bilingualism\" or the \"handicapping influence of bilingualism.\" Following studies reported that bilinguals performed worse in IQ tests and suffered in most aspects of language development. Journalist Rafael M. Diaz, from Yale University mentions that perspectives like these were in part believed to have been influenced by variables that had impacted or changed how a society functioned. \n\nIn 1977, the (American Institute for Research) published an influential study which discussed bilingualism as it relates to education and what effects there are on a child's performance maintaining with the class. This study, along with other research of its time, played a large role in our understanding of multilingualism and the effects that it has on the brain. While historically relevant and necessary, in context of meeting todays modern methodological standards, these studies in particular are argued to be missing necessary pieces of data that create controversy over their credibility. With current research and data suggesting benefits to bilingualism, the soundness of the former studies conclusions are in brought into question. According to \"The Journal of Genetic Psychology,\" the many of these studies employed unstandardized and subjective definitions of bilingualism and of a bilingual individual (\"e.g.\", labeling a person as bilingual or monolingual through assumptions based on the national origin of that person's parents or even based on that person's family name), raising the concern that there is no way of determining whether their samples were truly representative of a bilingual population. Another element that contrasted with more modern researching techniques was the lack of a control for socioeconomic status (SES) and many of them administered verbal-intelligence tests to non-proficient speakers of a second language in that second language.\n\nIn 1962, Peal and Lambert published a study highlighting the importance of controlling for such factors as age, sex, and SES, as well as of having a standardized measure for bilingualism when selecting a sample of bilinguals to be studied. In their study they carefully matched bilingual to monolingual participants, and found that the bilinguals appeared to have significant advantages to that of their monolingual peers outperforming in both verbal and non-verbal tests, more specifically in the non-verbal tests. In continuation of this study, and studies alike, the literature after this point began to undergo a shift of focus erring more particularly into areas of cognitive development and aptitude such as: perception and executive functioning.\n\nExecutive function is the domain of high-level cognitive processes that assists in goal-oriented tasks, such as problem solving, mental flexibility, attentional control, inhibitory control, and task switching. Much of the current research on cognitive effects of bilingualism investigate a correlation between bilingualism and alterations in the brain. A study titled \"Bilingualism, aging, and cognitive control\" amongst various similarly conducted studies released data suggesting that monolinguals and bilinguals were found having varying ability in the executive function part of the brain. From these tests, bilinguals showed a higher executive control than their monolingual peers. When matched in age and other background factors (e.g. socio-economic status), indications of a possible correlation between the demands on the \n\nbrain that learning multiple languages requires and higher executive functioning skills. John R. Best, an author of \"Relations between executive function and academic achievement from ages 5 to 17 in a large, representative national sample\" suggests that executive function is crucial to academic success across age groups. Executive function may also have effects for older adults. Inhibitory processes of adults who learned a second language at a young age show better controlled processing than monolingual adults. Reported links to slowing age-related cognitive decline such as dementia have been found.\n\nThe modern day approach to researching multilingualism is suggesting that there are cognitive advantages to becoming bilingual. One of the more frequently tested aspects to language development has been in relation to a possible link between bilingualism and higher executive functioning skills. In many studies bilingual groups outperformed monolingual control groups in executive function tasks. These findings suggest bilingualism is correlated with better control of attention and facilitates processing and functioning in several cognitive tasks.\n\nThere are two types of processing that aid children in language development: analysis, which involves the ability to represent and understand abstract information, and control, which involves the ability to selectively attend to specific aspects of structures whilst ignoring irrelevant information. The aspect of control is linked to the bilingual effect on cognitive abilities.\n\nIn one study, researchers administered a non-linguistic card-sorting task to participants that required flexibility in problem solving, inhibiting irrelevant information, as well as recognizing the constancy of certain variables in the face of changes in the rules. Bilingual children significantly outperformed their monolingual peers in this task, suggesting early development of inhibitory function that aids solving problems which require the ability to selectively focus attention.\n\nIn a following study, researchers aimed to determine what gave bilinguals an advantage in solving the card-sorting task (and generally an advantage in problem solving situations). Groups were equivalent in their ability to represent the stimuli (reflecting Worrall's findings, described below), and both were equally able to inhibit learned motor responses. Bilinguals performed better on the task to measure conceptual inhibition; the ability to inhibit previous associations and create new mental representations of the stimulus according to task changes.\n\nAnother study used three language groups: native bilinguals, English monolinguals, and English speakers enrolled in an immersion program. The bilingual children's scores were similar to the other groups despite lower parent socioeconomic and education level and lower verbal scores. When the two groups were adjusted for age, parent income and education, and verbal scores, the bilingual children outperformed monolinguals on conflict tasks that required resolving multiple attention demands.\n\nBogulski, Rakoczy, Goodman, and Bialystok investigated how \"lapsed bilinguals\" (participants who used to be bilingual but are now monolingual) compared to monolinguals and fluent bilinguals in executive function tasks. The lapsed bilinguals tested better than monolinguals but worse than their fluent counterparts. \n\nSome researchers demonstrate a contextual effect of sociocultural aspects of bilingualism. Others find these effects across various sociolinguistic settings such as comparison groups with bilingual children speaking a second regional and second migrant language or bilingual children of low-income immigrant families and monolingual children of low-income non-immigrant families.\n\nDavid Green offered an explanation for this phenomenon with his \"inhibitory control model.\" Proposed in 1998, this model references a bilingual's constant need to suppress one language while using another. Because this task requires suppressing a source of distraction, this kind of control is then applied to other tasks. This assertion was bolstered by a study of unimodal bilinguals (bilinguals who communicated with two spoken languages) and bimodal bilinguals (bilinguals who used one spoken language and sign language). Because bimodal bilinguals can express themselves in both languages at the same time, they may require less inhibition. This idea was supported by the results of the study; only unimodal bilinguals were found to have an advantage, as measured by the flanker task (a cognitive task that measures attentional focus and inhibition). Bimodal bilinguals also switch languages less frequently, because they are more likely to use both languages at once than to completely switch from one to the other. For this reason, the researchers of this study hypothesized that it may be the switching between languages that gives unimodal bilinguals the advantage. Prior and Gollan conducted a study investigating this idea, and found that bilinguals who switched languages often had an advantage in task shifting over bilinguals who did not frequently switch languages. However, this study did not control for similarity between the languages (languages that are more similar might require more attention to keep straight). When Verreyt, Woumans, Vandelanotte, Szmalec, and Duyck ran a similar study but with all participants having the same languages, they replicated the results of Prior and Gallan. Additionally, because their study looked at tasks measuring inhibition even though language switching should directly affect switching tasks, they argued that the effects of language-switching carry over multiple facets of executive control.\n\nBialystok and others have echoed this idea that the greater ability of bilinguals to selectively attend to important conceptual attributes of a stimulus may stem from the bilinguals' constant need to inhibit competing labels in their two languages for one object according to the currently relevant language. Bilinguals have different representations in each language for similar concepts and therefore need to constantly be aware of which language they are using and what the appropriate word is to be used in that context. This culminates in an advantage of cognitive control, since the ability to switch between languages and select the appropriate word for use is directly linked to the ability to better attend to relevant, or inhibit irrelevant, information. A further explanation refers to bilinguals' unique experience with using two languages in the same modality (spoken), differentiating them from monolingual peers, and requiring them to make the decision about how best to respond to a situation, as well as have better control over what they select.\n\nHakuta and Diaz, addressed the chicken and egg question concerning bilinguals and their reported enhanced cognitive abilities; do children with greater cognitive abilities tend to learn more than one language, or could knowing more than one language contribute to enhanced cognitive ability? They administered a set of non-verbal tests that are designed to measure cognitive ability (Raven's Progressive Matrices) to a bilingual sample of children. From these tests, the results showed a higher correlation with the degree of bilingualism (how proficient the individuals were in each of their languages) of their sample and scores on the test, as well as bilingualism did in fact predict performance (and therefore cognitive ability). However, an important point to note, is that most native bilinguals haven't learnt a second language because they are more intelligent. In most cases, they have grown up in a family where use of the two languages is necessary and therefore it is unlikely that the child's intelligence will allow them to learn the second language.\n\nThere are findings that do not support cause and effect relationship between bilingualism and enhanced executive function and those who suggest publication bias from those that do.  \n\nThe methodology has been disputed. Virginia Valian finds correlations between bilingualism and executive function to be inconsistent. Executive function is not uniformly defined and different tasks contribute to executive function. Because some of these tasks are available to monolinguals and bilinguals may similarly participate in these tasks to varying degrees, she argues bilinguals cannot be assumed superior to monolinguals in executive function. She also notes bilinguals are not consistently better at all executive function tasks.\n\nRamesh Kumar Mishra builds upon Valian's suggestions by arguing that research studies should shift to comparing bilinguals of different proficiencies instead of bilinguals to monolinguals. She argues things like exercising and video game playing can affect executive function, and since they are unrelated to language, they must be controlled for. Kaushanskaya and Prior respond to Valian that it is not only the lack of uniformity in defining executive function, but also the difficulty in defining bilingualism which make it problematic to draw strong conclusions about the effects of bilingualism on executive function.\n\nSome researchers have found results with no connection. A meta-analysis of more than 150 studies comparing the performance of monolinguals and bilinguals on different cognitive tasks found no evidence of a bilingual advantage.\n\nPaap and Greenberg assert bilinguals are not superior at executive processing. They assess their sample as similar in confounding variables and found that not only was there not evidence supporting an advantage for bilinguals, but that the evidence would argue against this.\n\nClaims of publication bias dispute validity of findings supporting bilingualism and cognitive benefits of executive control. For example, a study examining abstracts of research on bilingualism and executive control between 1999 and 2012 found the research results in support of an advantage were published more often and results that did not support the theory were published least often. Lack of difference or dispute of sample size, measure, and statistical power suggests not error but publication bias.\n\nIt has been found that a bilingual's two languages are simultaneously active, both phonologically and semantically, during language use. This activation is indicated by electrophysiological measures of performance. Not only is a person's dominant language (L1) active when using the less dominant language (L2), but their L2 is also activated when using L1. This happens once the individual is adequately proficient in the L2. They are both active when listening to speech, reading words in either language or even planning speech in either language. Also, both languages are activated even when only one language is needed by the user.\n\nBilingualism studies have mostly looked at Spanish-English or Dutch-English bilinguals. These languages share the Roman alphabet, and there are many cognates (words which have the same linguistic derivation e.g. 'piano' is the same in all 3 languages). Cross-language activation therefore seems less surprising. However, cross-language activation has also been reported in bilinguals whose two languages have different scripts (writing systems) and lexical forms (e.g. Japanese and English). A study by Hoshino & Kroll (2008) demonstrated that Japanese-English and Spanish-English bilinguals performed similarly in picture naming tasks even though the cognates for Spanish-English bilinguals shared phonological and orthographic (sound and spelling) information whereas the Japanese cognates were only phonologically similar (sound). Although the words were spelt and presented differently for Japanese-English bilinguals, this did not affect the simultaneous activation of both their languages.\n\nIn 2011, Wu and Thierry conducted a study where Chinese-English bilinguals were shown picture pairs. Participants were asked to name the second picture in the pair when it was shown and then were asked to judge whether the word pairs corresponding to the pictured objects rhymed or not. Word pairs were designed so that they either rhymed in both L1 and L2 or only in one of the two languages. Electrophysiological measures (see Event-related potential) of the effect (priming) of the sound repetition induced by the rhyming of the word pairs showed that even though the participants were performing the task in their L2, they showed a priming effect (albeit delayed) when those L2 words rhymed with words in the L1. This suggests that in regards to language use, both L1 and L2 are accessed and compete for selection during L2 production. \n\nIn 2012, Hoshino and Thierry conducted a study where Spanish-English bilingual participants were shown word pairs in English, their L2, and asked to judge whether the word pairs were related. Sometimes, things presented would be \"interlingual homographs,\" or words that sound the same in both languages but have a different meaning in each. These pairs would be primed by things relating to one of the meanings or to neither, and the effects of this priming were measured electrophysiologically. Participants judged whether the words in the pairs were related, and electrophysiological results revealed that semantic priming (facilitation of processing of the words) occurred when the words in the pairs were related to each other whether the meaning was interpreted in English or Spanish.\n\nThe two immediately preceding studies conclude that both languages of an individual are constantly unconsciously active and interfering with one another. The results, in regard to word processing, can help demonstrate how bilinguals have advantages over their monolingual peers when it comes to this area of study.\n\nThe fact that both languages are constantly activated means that they potentially compete for cognitive resources; bilinguals need to acquire a way to control or regulate the competition, so as to not use the wrong language at the wrong time. Inhibition refers to being able to ignore irrelevant information and therefore not be distracted by non-target stimuli. For example, a test that is widely used to assess this executive function is the Stroop task, where the word for a colour is printed in a different colour than the name (e.g. the word 'red' printed in blue ink). This causes interference and distraction; reaction times are measured to see how distracted the individual is by the incongruent word and colour. Bilinguals compared to monolinguals have shown an advantage at this task, suggesting that bilinguals have a more developed inhibition process, potentially due to the constant inhibition of their non-target language.\n\nInhibition has been suggested as the executive control system that allows successful linguistic selection even when both languages are co-activated in bilinguals. De Groot & Christofells (2006) proposed a distinction between two types of inhibition that may occur; global inhibition and local inhibition. Global inhibition refers to suppression of an entire language system, e.g. inhibiting Spanish when speaking English, and local inhibition refers to inhibition of a more specific competing vocabulary, e.g. the translation of the same word or phrase. Local inhibition mostly affects linguistic performance whereas global inhibition affects both linguistic and cognitive performance. Despite the apparent advantages for bilinguals in terms of non-linguistic cognitive processing, there seem to be some drawbacks for bilinguals in terms of linguistic cognitive processing: bilinguals have been shown to exhibit reduced speech fluency and speed of lexical access compared to monolinguals.\n\nBilingual individuals have also shown superiority in metalinguistic ability. This additional advantage seems closely tied to executive function. Metalinguistic awareness is the understanding of the separation between language's structure and its meaning. For example, being able to judge the grammaticality of a sentence regardless of whether it is sensical, or being able to separate the set of sounds comprising a word from the word's meaning. The ability to suppress distracting information, such as semantics, is an act of inhibition, meaning that it falls into executive function. This ability could also be exercised by being bilingual, given that a bilingual individual has to suppress their knowledge of another language system when operating in one of their languages.\n\nBialystok also studied metalinguistic abilities in bilinguals versus monolinguals by having subjects judge whether a sentence was grammatical, regardless of its logical sense. Bilinguals outperformed monolinguals in judging that a nonsensical sentence was correct. Additionally, when brainwaves of bilingual adults were observed during the task, they showed less of a reaction indicative of processing conflict as reflected on the P600 waveform.\n\nStudies by Leopold and Worrall investigated how being bilingual affected children's awareness of the fact that the connection between words and meaning is arbitrary. See language section below.\n\nThere has been a surge in interest in the benefits of bilingualism against age-related cognitive decline. Klein & Viswanathan found that the normal decrease in attention control observed in older adults was reduced in bilinguals, suggesting that bilingualism may be protective against the effects of cognitive aging. Elderly bilinguals have also been shown to be better at switching between tasks, ignoring irrelevant information and resolving conflicting cognitive alternatives. Bilingualism may be one of the environmental factors which contributes to 'cognitive reserve'. Cognitive reserve is the idea that engagement in stimulating physical or mental activity can act to maintain cognitive functioning in healthy aging and postpone the onset of symptoms in those suffering from dementia. Factors that contribute to this also include education, occupational status, higher socioeconomic class, and the continuing involvement in physical, intellectual and social activities.\n\nTo test the protection of bilingualism against Alzheimer's disease (AD), Bialystok et al. (2007) examined the hospital records of monolingual and bilingual patients who had been diagnosed with various types of dementia. After controlling for various cognitive and other factors, the researchers found that bilinguals experienced the onset of symptoms and were diagnosed approximately 3–4 years later on average than monolinguals. This was replicated with patients all diagnosed with AD. It is important to stress, however, that the studies did not show that bilingualism directly prevents one from having AD, but rather enables functional cognition for a longer period of time; it delays the onset of symptoms for those with the disease. This was confirmed by the finding that when monolingual and bilingual Alzheimer's patients' brains were scanned, bilinguals actually had more pathology (signs of disease) and damage than the monolingual patients. This suggests that active use of the two languages protects against the symptoms of the disease; areas of the brain that enable cognitive control may have benefited from the bilingual experience and so improve cognitive function in older age. \n\nThe finding that bilingualism contributes to cognitive reserve has also been replicated by several other studies For example, Abutalebi et al. (2015) tested 19 bilinguals (8 Cantonese-Mandarin and 11 Cantonese-English, age range 55-75) and 19 monolinguals (Italian speakers, age range 49-75) who had been matched for education level, performance on the Flanker Task (a cognitive response test,) and socioeconomic status. It is important to remember that this is a relatively small sample size; however, the results did confirm previous studies. According to the research, the bilinguals outperformed the monolinguals on all experimental tasks, and the researchers found that monolinguals' neural imaging showed higher signs of age-related effects on performance of tasks and decreased gray matter density. Meanwhile, the bilinguals' neural imaging showed higher levels of gray matter along the anterior cingulate cortex. Because of these results, the investigators concluded that bilingualism aids in protection against cognitive decline.\n\nThe bilingual advantage in cognitive function has been demonstrated especially in children and older adults, while the advantage in young adults has been rather variable. Suggestions for this finding may be that young adults are at their peak cognitive function, so it may be difficult to show any bilingual advantages beyond that peak level, especially in simple executive function tasks. It is thought that the benefits may be particularly beneficial to individuals at points in their lives when they are more vulnerable, for example in early development and later in life, when ordinary cognitive processes decline.\n\nA debate within the linguistic community is whether the age of acquiring one's L2 has effects on the cognitive advantages. A study on native bilingual vs late bilingual vs monolingual children in the USA revealed an overall bilingual advantage. Furthermore, native bilingual children demonstrated better performance on a selection of executive function tasks compared with their late bilingual and monolingual counterparts. Participants were controlled for age, verbal ability, and socioeconomic status (indicated by parent education level). However, there are various methodological factors which may call into question the validity of these results. Firstly, a small sample size was used, with only 12 children in the bilingual group, 21 in the late bilingual group, and 17 in the monolingual group. 'Late bilingual' in this study was classified as a monolingual child who had been in a bilingual school for 6 months (where half the lessons were in English and half in Spanish or Japanese). This may be a poor representation of 'late bilinguals', as 6 months may not be enough time for cognitive changes and adaptations to the brain to have taken place, and these children will unlikely already be 'proficient' in the L2, therefore this may not an appropriate group sample to support the claims being made. In addition, the effect sizes on all the individual executive function tests were all small to moderate effect sizes (ƞ2= 0.01 to 0.2). In combination with the lack of power due to small sample size, strong conclusions cannot be drawn from this data.\n\nAnother study, Kapa and Columbo (2013) investigated the attentional control of monolingual children, Spanish-English bilingual children who had learned both languages before the age of 3, and Spanish-English children who had learned English after age 3. Attentional control is a cognitive skill in which one can ignore unnecessary or impertinent information to the task at hand. Children were tested using an Attention Network Test. Although all groups obtained the same accuracy rates, the researchers found that early L2 learners (those who learned both languages before the age of 3) had the fastest reaction time. The late learners and monolinguals did not significantly differ in response time, illustrating that early L2 acquisition could be a decisive factor in executive control levels.\n\nAs one of the pioneers to the study of child language and bilingualism, Werner F. Leopold often used his daughter, Hildegard, to record his observations on this subject. In his studies, he observed that Hildegard had \"loose connections\" between the (phonetic) structure of words and their semantics (meaning) because of her frequent substitutions of English words with German words and vice versa. This was noted in her everyday speech and well-rehearsed songs or rhymes. He noted that she had a greater flexibility in the use of language that was unobserved in monolingual children of her age. Leopold considered that perhaps this loose connection between the meaning and form of a word could result in more abstract thinking or greater mental flexibility for bilingual children. Following this study, several others were formed to test similar things and find out more about the mental abilities of bilinguals with relation to their languages.\n\nAnita Ianco-Worrall, author of \"Bilingualism and Cognitive Development,\" designed a study to test Leopold's observations and was able to replicate them. She tested two groups of monolingual and bilingual children at ages 4–6 and 6–9. These participants were given tasks to assess whether they showed a semantic or phonetic preference when categorizing words. An example of one task given in the study was to decide which of the two words, either \"can\" or \"hat,\" was more similar to the word \"cap\". The semantic choice would be \"hat\" while the phonetic choice would be \"can\". Other tasks were designed to provide a choice between semantic and phonetic interpretation of objects. For instance, in a hypothetical situation, could you call a \"cow\" a \"dog\" and if you did, would this \"dog\" bark?\n\nThe results of Ianco-Worrall's study showed that although both monolingual and bilingual children had no differences in the way they understood the words used, 54% of the younger bilingual children consistently showed a semantic preference in contrast to their monolingual peers. In monolingual children, semantic preference increased with age, suggesting that bilingual children reach a stage of semantic development 2–3 years earlier than their monolingual peers. This finding is in stark contrast to the early research and claims about bilingualism, which warned that bilingualism stunts children's linguistic development.\n\nIn their book \"In Other Words,\" Ellen Bialystok and Kenji Hakuta, both professors studying bilingualism, examined the idea that \"the knowledge of two languages is greater than the sum of its parts.\" They argued that there are linguistic benefits to being bilingual and that they are more than simply being able to speak two languages. A child learning two languages whose structures and rules are significantly different from each other requires the child to think in cognitively demanding ways. An example of this cognitive demand in action would be in the arbitrariness of labels for objects, or distinguishing between and using two different grammatical or syntactical structures. These areas are quite arduous for a child to learn, but have been shown to increase the understanding the structure of language and introduce a greater awareness of meaning. Bilinguals who have worked through these strenuous stages of development obtain what is referred to as metalinguistic awareness (see metalinguistics abilities section above).\n\nAs an attempt to further analyze bilingualism as it relates to reading capability, there was another study done by Bialystok. This study was conducted by testing a group of children ages 4 to 5, equally familiar in secondary language as with their native tongues. The assessment was made using what Bialystok discussed as a representational principle; which refers to a symbolic representation of spoken language or the connection between the spoken and written language systems. For the testing of this principle, she gave children a \"Moving Word Task\" where a child would have to appropriately match a written word to an object on a card. If they could correctly matched the two after some rearranging of the cards, it suggested that they could understand the written words as representations of specific words whose meanings cannot change. The study was taken further in order to see when bilinguals grasped this principle in comparison to monolinguals. The results showed that bilingual children were correct on their \"Moving Word Task\" over 80% of the time, which is a percentage equal to that of monolinguals who were one year older than the bilinguals being tested. Overall, the bilinguals seemed to understand the representational principle earlier than monolinguals, meaning they were earlier prepared for literacy acquisition.\n\nIn another study done by Durgunoglu, Nagy, and Hancin-Bhatt, this same concept for bilinguals' reading abilities was also studied. For this specific study, native Spanish speaking children who were learning to read English were tested. The researchers observed these bilinguals to find that their levels of phonological awareness and word recognition in Spanish could predict how well they would be able to recognize words in English. The results showed that the phonological awareness skills established in one language could be transferred to the reading ability in another language. Again, bilinguals seem to be more advanced than monolinguals when it comes to reading ability.\n\nIt is a well-replicated finding that bilinguals have a smaller vocabulary size than their monolinguals counterparts. Given that bilinguals accumulate vocabulary from both their languages, when taking both languages into account, they have a much larger vocabulary than monolinguals. However, within each language bilinguals have a smaller vocabulary size and take longer to name pictures as seen in standardized vocabulary tests, such as the Peabody Picture Vocabulary Test and Boston Naming task. A possible explanation may be that the frequency of use of words is related to increased lexical accessibility, meaning that words that are used more frequently are accessed more quickly. Therefore, bilinguals may be 'less proficient' relative to monolinguals, purely because they use one sole language less frequently than monolinguals, who use the same language all the time. In addition, the need to select the appropriate language system makes ordinary linguistic processing more effortful. The simple act of retrieving a common word is more effortful for bilinguals than monolinguals due to the competition of the two languages.\n\nOther things to consider in this area of a bilingual's language were pointed out in Bialystok, Luk, Peets, and Yang's study from 2010. They note that certain vocabulary tests could yield artificially low scores for bilingual children according to the domain from which the test words are taken. For example, this group of researchers found that monolingual and bilingual 6-year-olds in their study had similar scores on English words that were associated with schooling. However, when the children were tested on English words that were associated with the home, the scores were significantly lower for the bilingual (English-Spanish) children. The researchers interpret this result as reflecting an asymmetry in vocabulary domains and language exposure: monolingual and bilingual children were equally exposed to the school context in the same language (English), but English was not commonly used in the home environments of the bilingual children. Therefore, one cannot conclude that the bilingual children exhibited a true deficit in vocabulary ability.\n\nIt has been suggested that prolonged naturalistic exposure to L2 affects how L2 is processed, but it may also affect how the L1 is processed. For example, in immersion contexts, the individual experiences reduced access to L1 and extensive contact with L2, which affects and facilitates processing of L2. However, this may also consequently affect processing of their L1, such as with increased difficulty in naming objects and phonology.\n\nTo test this hypothesis, Dussias & Sagarra (2007) investigated how individuals interpreted temporarily ambiguous phrases. For example, \"Alguien disparó al hijo de la actriz que estaba en el balcón' = '\"Someone shot the son of the actress who was on the balcony.\" When asked the question, \"¿Quien estaba en el balcón?\" = \"Who was on the balcony?\", monolingual Spanish speakers will typically answer \"el hijo\" = \"the son\" as they have a high attachment preference, meaning they attach the modifier to the \"higher\" verb phrase [shot the son]. This differs from monolingual English speakers who will typically answer \"the actress\" as they have a low attachment preference, meaning they attach the modifier to the \"lower\" verb phrase [the actress who was on the balcony]. The researchers found that Spanish-English bilinguals in a Spanish-speaking environment showed preference for the typical Spanish high-attachment strategy. However, Spanish-English bilinguals in an English-speaking environment showed preference for the typical English low-attachment strategy, even when reading the phrase in Spanish, their dominant language. This may be because they have more exposure to English constructions, making it more available to them. But altogether, this supports the idea that the L2, English in this case, is affecting the way the native Spanish speakers use their L1.\n\n"}
{"id": "57107342", "url": "https://en.wikipedia.org/wiki?curid=57107342", "title": "David Cole (journalist)", "text": "David Cole (journalist)\n\nDavid Christopher Cole, also known as David Stein, is an American journalist and documentary film director. As a young man, he achieved notoriety for his controversial views on the Holocaust, which have at various times been labeled Holocaust revisionism and Holocaust denial. Much of the controversy Cole attracted resulted from the fact that he is Jewish. After changing his name to David Stein following death threats, he became known for his activism on behalf of the Republican Party. He is the author of the book \"Republican Party Animal\", published by Feral House in 2014.\n\nIn his late teens, Cole embarked on a study of American political ideology. This led author Michael Shermer to coin the term \"meta-ideologue\" to describe Cole. In a 1994 article in Shermer's \"Skeptic\" magazine, Shermer wrote, \"Where the other revisionists are political and/or racial ideologues, Cole's interests run at a deeper level. He is a meta-ideologue – an existentialist on a quest to understand how ideologues invent their realities.\"\n\nIn the 1990s, Cole appeared on the \"Montel Williams Show\", the \"Phil Donahue Show\", \"60 Minutes\", \"48 Hours\", and \"The Morton Downey Jr. Show\". In an ironic twist, Ernest Hollander, the Auschwitz survivor who debated Cole on the Montel Williams Show, discovered after the show aired that the brother he thought was murdered during the Holocaust was actually alive and well. The brother's neighbor had seen the Montel episode, and recognized the resemblance between the two Hollanders. Both brothers had thought the other was dead (Williams hosted the reunion on his show). In his autobiography, Cole claimed that this supported his theory that many of the Jews deported to Auschwitz in 1944 were falsely believed to be dead.\n\nIn 1993, Cole was interviewed by Timothy Ryback for \"The New Yorker.\" Other interviews from that time include the \"Jerusalem Report\" (which referred to Cole as \"an unlikely media sensation\" and \"something of a media star\"), the \"Detroit Jewish News\", and \"Hustler\". He also penned op-eds for the \"Los Angeles Times\" under his own name, and under the name Christopher Cole.\n\nIn the early 1990s, Cole lectured on college campuses throughout the U.S. During a lecture at UCLA, he was assaulted on-stage, resulting in security rushing him off campus for his own safety.\n\nThe proper label for Cole's views is a subject of debate. According to \"The Guardian\", which chose to use the term \"revisionist,\" Cole's main thesis is that \"Auschwitz was not an extermination camp in the manner of Treblinka, Sobibor, Belzec and Chelmno – which he (Cole) acknowledged were part of a genocidal programme against Polish Jews; that the Holocaust ended in 1943, when the Nazis realised they needed Jewish slave labour for factories; and that there was no overarching, genocidal plan, but an evolving, morphing policy which claimed perhaps 4 million, rather than 6 million, Jewish lives.\" \"Gawker\" and \"The Stranger\" also used the term revisionist. Conversely, the \"Huffington Post\" and \"The Wrap\" have referred to Cole as a denier. Generally, most Jewish organizations refer to Cole as a denier, including the ADL and the Simon Wiesenthal Center (in 1994 Rabbi Abraham Cooper, associate director of the Simon Wiesenthal Center, told the Jerusalem Report \"I can't think of any other Jew who has gone so far in aiding and abetting the enemies of the Jewish people\"). Professor Deborah Lipstadt, who has made it her life's work to counter Holocaust denial, told the Detroit Jewish News in 1994 that there was a lot of \"joy and rejoicing\" in \"revisionist\" camps because of Cole. The Detroit Jewish News' editor, Phil Jacobs, went further, listing Cole as a threat to the Jewish people right alongside \"Hitler, Hussein, and Arafat\" in a January 28, 1994, editorial.\n\nIn January 1998, Irv Rubin, chairman of the Jewish Defense League, offered a monetary reward for information regarding Cole's whereabouts. The Anti-Defamation League, no fan of Cole's, agreed that the JDL's reward offer implied that \"it was prepared to take immediate, possibly violent, action\" in order to \"get rid\" of Cole. In response to the threat, Cole entered into a deal with Rubin whereby he renounced his views and the JDL removed the \"reward offer.\" The over-the-top \"recantation\" was met with widespread skepticism, with Michael Shermer writing in his 2000 book \"Denying History\" that Cole's retraction demanded \"a healthy dose of skepticism\" because it was \"so unlike his earlier positions.\" On the JDL website, Rubin bragged that Cole's recantation is \"evidence of the power of the Jewish Defense League. Cole would later admit in his autobiography that the \"recantation\" was false.\n\nFollowing the recantation, Cole changed his last name to Stein and began making mainstream Holocaust documentaries for a company he ran called the Tinbergen Archives. His 2007 film \"Nuremberg: The 60th Anniversary Director's Cut\" won the best feature-length documentary award and best score in a feature documentary at the 2008 Garden State Film Festival. His films \"Liberation Day: Dachau\", \"Auschwitz: Silent Witness\", \"The Lost Gas Chamber\", and \"War Crimes and Trials\" are preserved in the Rosenblatt Holocaust Collection at Fordham University.\n\nIn 2008, Cole (as Stein) became active in mainstream conservative politics. As a blogger, his work appeared in or was cited by the \"Daily Caller\", the \"Washington Times\", \"Commentary\", the \"Wall Street Journal\", the \"New York Daily News\", \"American Thinker\", \"The Blaze\", \"FrontPageMag\", the \"L.A. Jewish Journal\", \"WorldNetDaily\", \"History News Network\", \"PJ Media\", \"Gawker\", \"The New Republic\", and \"The Hollywood Reporter\". Cole (as Stein) appeared as a frequent guest on the Larry Elder Show on KABC Radio. He also became a member and co-organizer of the \"secret\" organization of Hollywood Republicans, Friends of Abe, founded by actor Gary Sinise. Stein also ran an organization called the Republican Party Animals, one of the largest Republican Party organizing operations on the West Coast. It held regular events in and around Los Angeles from a libertarian, non-socially conservative perspective aimed at attracting young people to the GOP. In the words of The Guardian, \"David Stein brought right-wing congressmen, celebrities, writers and entertainment industry figures together for shindigs, closed to outsiders, where they could scorn liberals and proclaim their true beliefs. Over the past five years Stein's organization, Republican Party Animals, drew hundreds to regular events in and around Los Angeles, making him a darling of conservative blogs and talkshows. That he made respected documentaries on the Holocaust added intellectual cachet and Jewish support to Stein's cocktail of politics, irreverence and rock and roll.\"\n\nIn April 2013, Stein was outed as Cole by an ex-girlfriend. The L.A. County Republican Party sent out an email warning members about his history.\n\nIn 2014, Cole's autobiography, \"Republican Party Animal\", was published by Feral House and excerpted on AlterNet. Starting in 2015, he became a regular source in The Guardian for matters relating to Hollywood conservatives (in that capacity he has also been interviewed by \"Variety\", the \"Daily Mail\", and \"Truthdig\"). He was briefly employed as a writer for The Times of Israel, until protests over his past led to his dismissal. Since January 2015, he has been a staff writer and weekly columnist for \"Taki's Magazine\".\n\nIn October 2014, Cole again made news after the \"Washington Times\" uncovered fan letters apparently written to him in the 1990s by U.S. Congresswoman Marcy Kaptur (D-OH), and Zsolt Rabai, a foreign policy adviser to the then-president of Hungary. In October 2015, a taxi driver in Ennis, Ireland, was jailed for five months after going on a vandalism spree that he claimed was inspired by Cole's videos.\n\n"}
{"id": "18938226", "url": "https://en.wikipedia.org/wiki?curid=18938226", "title": "Digital rights management", "text": "Digital rights management\n\nDigital rights management (DRM) is a set of access control technologies for restricting the use of proprietary hardware and copyrighted works. DRM technologies try to control the use, modification, and distribution of copyrighted works (such as software and multimedia content), as well as systems within devices that enforce these policies.\n\nThe use of digital rights management is not universally accepted. Proponents of DRM argue that it is necessary to prevent intellectual property from being copied freely, just as physical locks are needed to prevent personal property from being stolen, that it can help the copyright holder maintain artistic control, and that it can ensure continued revenue streams. Those opposed to DRM contend there is no evidence that DRM helps prevent copyright infringement, arguing instead that it serves only to inconvenience legitimate customers, and that DRM helps big business stifle innovation and competition. Furthermore, works can become permanently inaccessible if the DRM scheme changes or if the service is discontinued. DRM can also restrict users from exercising their legal rights under the copyright law, such as backing up copies of CDs or DVDs (instead having to buy another copy, if it can still be purchased), lending materials out through a library, accessing works in the public domain, or using copyrighted materials for research and education under the fair use doctrine. The Electronic Frontier Foundation (EFF) and the Free Software Foundation (FSF) consider the use of DRM systems to be an anti-competitive practice.\n\nWorldwide, many laws have been created which criminalize the circumvention of DRM, communication about such circumvention, and the creation and distribution of tools used for such circumvention. Such laws are part of the United States' Digital Millennium Copyright Act, and the European Union's Copyright Directive, (the French DADVSI is an example of a member state of the European Union (\"EU\") implementing the directive).\n\nThe rise of digital media and analog-to-digital conversion technologies has vastly increased the concerns of copyright-owning individuals and organizations, particularly within the music and movie industries. While analog media inevitably lost quality with each copy generation, and in some cases even during normal use, digital media files may be duplicated an unlimited number of times with no degradation in the quality.\n\nThe rise of personal computers as household appliances has made it convenient for consumers to convert media (which may or may not be copyrighted) originally in a physical, analog or broadcast form into a universal, digital form (this process is called ripping) for portability or viewing later. This, combined with the Internet and popular file-sharing tools, has made unauthorized distribution of copies of copyrighted digital media (also called digital piracy) much easier.\n\nIn 1983, a very early implementation of Digital Rights Management (DRM) was the Software Service System (SSS) devised by the Japanese engineer Ryuichi Moriya.\n\nCommon DRM techniques include restrictive licensing agreements: The access to digital materials, copyright and public domain is restricted to consumers as a condition of entering a website or when downloading software.\nEncryption, scrambling of expressive material and embedding of a tag, which is designed to control access and reproduction of information, including backup copies for personal use.\n\nDRM technologies enable content publishers to enforce their own access policies on content, such as restrictions on copying or viewing. These technologies have been criticized for restricting individuals from copying or using the content legally, such as by fair use. DRM is in common use by the entertainment industry (e.g., audio and video publishers). Many online music stores, such as Apple's iTunes Store, and e-book publishers and vendors, such as OverDrive, also use DRM, as do cable and satellite service operators, to prevent unauthorized use of content or services. However, Apple dropped DRM from all iTunes music files around 2009.\n\nIndustry has expanded the usage of DRM to more traditional hardware products, such as Keurig's coffeemakers, Philips' light bulbs, mobile device power chargers, and John Deere's tractors. For instance, tractor companies try to prevent farmers from making DIY repairs under usage of DRM-laws as DMCA.\n\nComputer games sometimes use DRM technologies to limit the number of systems the game can be installed on by requiring authentication with an online server. Most games with this restriction allow three or five installs, although some allow an installation to be 'recovered' when the game is uninstalled. This not only limits users who have more than three or five computers in their homes (seeing as the rights of the software developers allow them to limit the number of installations), but can also prove to be a problem if the user has to unexpectedly perform certain tasks like upgrading operating systems or reformatting the computer's hard drive, tasks which, depending on how the DRM is implemented, count a game's subsequent reinstall as a new installation, making the game potentially unusable after a certain period even if it is only used on a single computer.\n\nIn mid-2008, the Windows version of \"Mass Effect\" marked the start of a wave of titles primarily making use of SecuROM for DRM and requiring authentication with a server. The use of the DRM scheme in 2008's \"Spore\" backfired and there were protests, resulting in a considerable number of users seeking an unlicensed version instead. This backlash against the three-activation limit was a significant factor in \"Spore\" becoming the most pirated game in 2008, with TorrentFreak compiling a \"top 10\" list with \"Spore\" topping the list. However, Tweakguides concluded that the presence of intrusive DRM does not appear to increase the cracking of a game, noting that other games on the list such as \"Call of Duty 4\" and \"Assassin's Creed\" use SafeDisc DRM, which has no install limits and no online activation. Additionally, other video games that use intrusive DRM such as \"BioShock\", \"Crysis Warhead\", and \"Mass Effect\", do not appear on the list.\n\nMany mainstream publishers continued to rely on online DRM throughout the later half of 2008 and early 2009, including Electronic Arts, Ubisoft, Valve, and Atari, \"The Sims 3\" being a notable exception in the case of Electronic Arts. Ubisoft broke with the tendency to use online DRM in late 2008, with the release of \"Prince of Persia\" as an experiment to \"see how truthful people really are\" regarding the claim that DRM was inciting people to use illegal copies. Although Ubisoft has not commented on the results of the \"experiment\", Tweakguides noted that two torrents on Mininova had over 23,000 people downloading the game within 24 hours of its release.\n\nUbisoft formally announced a return to online authentication on 9 February 2010, through its Uplay online gaming platform, starting with \"\", \"\", and \"Assassin's Creed II\". \"Silent Hunter 5\" was first reported to have been compromised within 24 hours of release, but users of the cracked version soon found out that only early parts of the game were playable. The Uplay system works by having the installed game on the local PCs incomplete and then continuously downloading parts of the game-code from Ubisoft's servers as the game progresses. It was more than a month after the PC release in the first week of April that software was released that could bypass Ubisoft's DRM in \"Assassin's Creed II\". The software did this by emulating a Ubisoft server for the game. Later that month, a real crack was released that was able to remove the connection requirement altogether.\n\nIn early March 2010, the Uplay servers suffered a period of inaccessibility due to a large-scale DDoS attack, causing around 5% of game owners to become locked out of playing their game. The company later credited owners of the affected games with a free download, and there has been no further downtime.\n\nOther developers, such as Blizzard Entertainment are also shifting to a strategy where most of the game logic is on the \"side\" or taken care of by the servers of the game maker. Blizzard uses this strategy for its game Diablo III and Electronic Arts used this same strategy with their reboot of SimCity, the necessity of which has been questioned.\n\nBohemia Interactive have used a form of technology since \"\", wherein if the game copy is suspected of being unauthorized, annoyances like guns losing their accuracy or the players being turned into a bird are introduced.\n\nCroteam, the company that released \"\" in November 2011, implemented a different form of DRM wherein, instead of displaying error messages that stop the illicit version of the game from running, it causes a special invincible foe in the game to appear and constantly attack the player until he or she is killed.\n\nOne of the oldest and least complicated DRM protection methods for computer and Nintendo Entertainment System games was when the game would pause and prompt the player to look up a certain page in a booklet or manual that came with the game; if the player lacked access to such material, they would not be able to continue the game. A product key, a typically alphanumerical serial number used to represent a license to a particular piece of software, serve a similar function. During the installation process or launch for the software, the user is asked to input the key; if the key correctly corresponds to a valid license (typically via internal algorithms), the key is accepted, then the user who bought the game can continue. In modern practice, product keys are typically combined with other DRM practices (such as online \"activation\"), as the software could be cracked to run without a product key, or \"keygen\" programs could be developed to generate keys that would be accepted.\n\nEnterprise digital rights management (E-DRM or ERM) is the application of DRM technology to the control of access to corporate documents such as Microsoft Word, PDF, and AutoCAD files, emails, and intranet web pages rather than to the control of consumer media. E-DRM, now more commonly termed IRM (Information Rights Management), is generally intended to prevent the unauthorized use (such as industrial or corporate espionage or inadvertent release) of proprietary documents. IRM typically integrates with content management system software but corporations such as Samsung Electronics also develop their own custom DRM systems.\n\nDRM has been used by organizations such as the British Library in its secure electronic delivery service to permit worldwide access to substantial numbers of rare (and in many cases unique) documents which, for legal reasons, were previously only available to authorized individuals actually visiting the Library's document centre at Boston Spa in England.\n\nElectronic books read on a personal computer, or an e-book reader or e-reader app typically use DRM technology to limit copying, printing, and sharing of e-books. E-books are usually limited to be used on a limited number of reading devices, and some e-publishers prevent any copying or printing. Some commentors believe DRM makes e-book publishing complex.\n\n, there were five main e-book formats: EPUB, KF8, Mobipocket, PDF, and Topaz. The Amazon Kindle uses KF8, Mobipocket, and Topaz; it also supports native PDF format e-books and native PDF files. Other e-book readers mostly use EPUB format e-books, but with differing DRM schemes.\n\nThere are four main e-book DRM schemes in common use today, one each from Adobe, Amazon, Apple, and the Marlin Trust Management Organization (MTMO).\n\n\nIn one instance of DRM that caused a rift with consumers, Amazon.com in July 2009, remotely deleted purchased copies of George Orwell's \"Animal Farm\" (1945) and \"Nineteen Eighty-Four\" (1949) from customers' Amazon Kindles after providing them a refund for the purchased products. Commentors have described these actions as Orwellian and have compared Amazon to Big Brother from Orwell's \"Nineteen Eighty-Four\". After Amazon CEO Jeff Bezos issued a public apology, the Free Software Foundation wrote that this was just one more example of the excessive power Amazon has to remotely censor what people read through its software, and called upon Amazon to free its e-book reader and drop DRM. Amazon then revealed the reason behind its deletion: the e-books in question were unauthorized reproductions of Orwell's works, which were not within the public domain and to which the company that published and sold them on Amazon's service had no rights.\n\nWebsitessuch as library.nu (shut down by court order on 15 February 2012), BookFi, BookFinder, Library Genesis, and Science Hubhave emerged which allow downloading e-books by violating copyright.\n\nAn early example of a DRM system is the Content Scrambling System (CSS) employed by the DVD Forum on film DVDs circa 1996. CSS uses an encryption algorithm to encrypt content on the DVD disc. Manufacturers of DVD players must license this technology and implement it in their devices so that they can decrypt the encrypted content to play it. The CSS license agreement includes restrictions on how the DVD content is played, including what outputs are permitted and how such permitted outputs are made available. This keeps the encryption intact as the video material is played out to a TV.\n\nIn 1999, Jon Lech Johansen released an application called DeCSS, which allowed a CSS-encrypted DVD to play on a computer running the Linux operating system, at a time when no licensed DVD player application for Linux had yet been created. The legality of DeCSS is questionable: one of the authors has been the subject of a lawsuit, and reproduction of the keys themselves is subject to restrictions as illegal numbers.\n\nAlso in 1999, Microsoft released Windows Media DRM, which read instructions from media files in a rights management language that stated what the user may do with the media. The language can define how many times the media file can be played, and whether or not it can be burned to a CD, forwarded, printed, or saved to the local disk. Later versions of Windows Media DRM also allow producers to declare whether or not the user may transfer the media file to other devices, to implement music subscription services that make downloaded files unplayable after subscriptions are cancelled, and to implement regional lockout.\n\nThe Microsoft operating system, Windows Vista, contains a DRM system called the Protected Media Path, which contains the Protected Video Path (PVP). PVP tries to stop DRM-restricted content from playing while unsigned software is running, in order to prevent the unsigned software from accessing the content. Additionally, PVP can encrypt information during transmission to the monitor or the graphics card, which makes it more difficult to make unauthorized recordings.\n\nAdvanced Access Content System (AACS) is a DRM system for HD DVD and Blu-ray Discs developed by the AACS Licensing Administrator, LLC (AACS LA), a consortium that includes Disney, IBM, Intel, Microsoft, Matsushita (Panasonic), Sony, Toshiba, and Warner Brothers. In December 2006, hackers published a process key online, which enabled unrestricted access to AACS-protected HD DVD content. After the cracked keys were revoked, further cracked keys were released.\n\nMarlin (DRM) is a technology that is developed and maintained in an open industry group known as the Marlin Developer Community (MDC) and licensed by the Marlin Trust Management Organization (MTMO). Founded in 2005, by five companies: Intertrust, Panasonic, Philips, Samsung, and Sony, Marlin DRM has been deployed in multiple places around the world. In Europe, Philips NetTVs implement Marlin DRM. Also in Europe, Marlin DRM is required in such industry groups as the Open IPTV Forum and national initiatives such as HDForum in France, Tivu in Italy, and YouView in the UK, and which are starting to see broad deployments. In Japan, the acTVila IPTV service uses Marlin to encrypt video streams, which are permitted to be recorded on a DVR in the home.\n\nOMA DRM is a system invented by the Open Mobile Alliance, whose members represent information technology companies (e.g., IBM and Microsoft), mobile phone network operators (e.g., Cingular, Deutsche Telekom, Orange, O2, and Vodafone), mobile phone manufacturers (e.g., LG, Motorola, Samsung, and Sony), mobile system manufacturers (e.g., Ericsson and Openwave).\n\nDiscs with DRM schemes are not standards-compliant Compact Discs (CDs) but are rather CD-ROM media. Therefore, they all lack the CD logotype found on discs which follow the standard (known as Red Book). These CDs cannot be played on all CD players or personal computers. Personal computers running Microsoft Windows sometimes even crash when attempting to play the CDs.\n\nIn 2005, Sony BMG introduced new DRM technology which installed DRM software on users' computers without clearly notifying the user or requiring confirmation. Among other things, the installed software included a rootkit, which created a severe security vulnerability others could exploit. When the nature of the DRM involved was made public much later, Sony BMG initially minimized the significance of the vulnerabilities its software had created, but was eventually compelled to recall millions of CDs, and released several attempts to patch the surreptitiously included software to at least remove the rootkit. Several class action lawsuits were filed, which were ultimately settled by agreements to provide affected consumers with a cash payout or album downloads free of DRM.\n\nSony BMG's DRM software actually had only a limited ability to prevent copying, as it affected only playback on Windows computers, not on other equipment. Even on the Windows platform, users regularly bypassed the restrictions. And, while the Sony BMG DRM technology created fundamental vulnerabilities in customers' computers, parts of it could be trivially bypassed by holding down the \"shift\" key while inserting the CD, or by disabling the autorun feature. In addition, audio tracks could simply be played and re-recorded, thus completely bypassing all the DRM (this is known as the analog hole). Sony BMG's first two attempts at releasing a patch which would remove the DRM software from users' computers failed.\n\nIn January 2007, EMI stopped publishing audio CDs with DRM, stating that \"the costs of DRM do not measure up to the results.\" Following EMI, Sony BMG was the last publisher to abolish DRM completely, and audio CDs containing DRM are no longer released by the four largest commercial record label companies.\n\nMany internet music stores employ DRM to restrict usage of music purchased and downloaded.\n\n\nThe various services are currently not interoperable, though those that use the same DRM system (for instance the several Windows Media DRM format stores, including Napster, Kazaa and Yahoo Music) all provide songs that can be played side-by-side through the same player program. Almost all stores require client software of some sort to be downloaded, and some also need plug-ins. Several colleges and universities, such as Rensselaer Polytechnic Institute, have made arrangements with assorted Internet music suppliers to provide access (typically DRM-restricted) to music files for their students, to less than universal popularity, sometimes making payments from student activity fee funds. One of the problems is that the music becomes unplayable after leaving school unless the student continues to pay individually. Another is that few of these vendors are compatible with the most common portable music player, the Apple iPod. The Gowers Review of Intellectual Property (to HMG in the UK; 141 pages, 40+ specific recommendations) has taken note of the incompatibilities, and suggests (Recommendations 8—12) that there be explicit fair dealing exceptions to copyright allowing libraries to copy and format-shift between DRM schemes, and further allowing end users to do the same privately. If adopted, some acrimony may decrease.\n\nAlthough DRM is prevalent for Internet music, some online music stores such as eMusic, Dogmazic, Amazon, and Beatport, do not use DRM despite encouraging users to avoid sharing music. Major labels have begun releasing more music without DRM. Eric Bangeman suggests in Ars Technica that this is because the record labels are \"slowly beginning to realize that they can't have DRMed music and complete control over the online music market at the same time... One way to break the cycle is to sell music that is playable on any digital audio player. eMusic does exactly that, and their surprisingly extensive catalog of non-DRMed music has vaulted it into the number two online music store position behind the iTunes Store.\" Apple's Steve Jobs called on the music industry to eliminate DRM in an open letter titled Thoughts on Music. Apple's iTunes Store will start to sell DRM-free 256 kbit/s (up from 128 kbit/s) AAC encoded music from EMI for a premium price (this has since reverted to the standard price).\n\nIn March 2007, Musicload.de, one of Europe's largest internet music retailers, announced their position strongly against DRM. In an open letter, Musicload stated that three out of every four calls to their customer support phone service are as a result of consumer frustration with DRM.\n\nThe Open Mobile Alliance created a standard for interoperable DRM on mobile devices. The first version of OMA DRM consisted of a simple rights management language and was widely used to protect mobile phone ringtones from being copied from the phone to other devices. Later versions expanded the rights management language to similar expressiveness as Fairplay, but did not become widely used.\n\nThe CableCard standard is used by cable television providers in the United States to restrict content to services to which the customer has subscribed.\n\nThe broadcast flag concept was developed by Fox Broadcasting in 2001, and was supported by the MPAA and the U.S. Federal Communications Commission (FCC). A ruling in May 2005, by a United States courts of appeals held that the FCC lacked authority to impose it on the TV industry in the US. It required that all HDTVs obey a stream specification determining whether a stream can be recorded. This could block instances of fair use, such as time-shifting. It achieved more success elsewhere when it was adopted by the Digital Video Broadcasting Project (DVB), a consortium of about 250 broadcasters, manufacturers, network operators, software developers, and regulatory bodies from about 35 countries involved in attempting to develop new digital TV standards.\n\nAn updated variant of the broadcast flag has been developed in the Content Protection and Copy Management group under DVB (DVB-CPCM). Upon publication by DVB, the technical specification was submitted to European governments in March 2007. As with much DRM, the CPCM system is intended to control use of copyrighted material by the end-user, at the direction of the copyright holder. According to Ren Bucholz of the EFF, which paid to be a member of the consortium, \"You won't even know ahead of time whether and how you will be able to record and make use of particular programs or devices\". The normative sections have now all been approved for publication by the DVB Steering Board, and will be published by ETSI as a formal European Standard as ETSI TS 102 825-X where X refers to the Part number of specification. Nobody has yet stepped forward to provide a Compliance and Robustness regime for the standard (though several are rumoured to be in development), so it is not presently possible to fully implement a system, as there is nowhere to obtain the necessary device certificates.\n\nSometimes, metadata is included in purchased media which records information such as the purchaser's name, account information, or email address. Also included may be the file's publisher, author, creation date, download date, and various notes. This information is not embedded in the played content, like a watermark, but is kept separate, but within the file or stream.\n\nAs an example, metadata is used in media purchased from Apple's iTunes Store for DRM-free as well as DRM-restricted versions of their music or videos. This information is included as MPEG standard metadata.\n\nDigital watermarks exist since 1992. They are steganographically embedded within audio or video data during production or distribution. They can be used for recording the copyright owner, the distributor, the distribution chain or identifying the purchaser of the music.\n\nWatermarks are not complete DRM mechanisms in their own right, but are used as part of a system for copyright enforcement, such as helping provide prosecution evidence for legal purposes, rather than direct technological restriction. Some programs used to edit video and/or audio may distort, delete, or otherwise interfere with watermarks. Signal/modulator-carrier chromatography may also separate watermarks from original audio or detect them as glitches. Additionally, comparison of two separately obtained copies of audio using simple, home-grown algorithms can often reveal watermarks.\n\nSince the late-2000s the trend in media consumption has been towards renting content using online streaming services, for example Spotify for music and Netflix for video content. Copyright holders often require that these services protect the content they licence using DRM mechanisms.\n\nThe 1996 World Intellectual Property Organization Copyright Treaty (WCT) requires nations to enact laws against DRM circumvention, and has been implemented in most member states of the World Intellectual Property Organization.\n\nThe United States implementation is the Digital Millennium Copyright Act (DMCA), while in Europe the treaty has been implemented by the 2001 European directive on copyright, which requires member states of the European Union to implement legal protections for technological prevention measures. , the lower house of the French parliament adopted such legislation as part of the controversial DADVSI law, but added that protected DRM techniques should be made interoperable, a move which caused widespread controversy in the United States. The Tribunal de grande instance de Paris concluded in 2006, that the complete blocking of any possibilities of making private copies was an impermissible behaviour under French copyright law.\n\nIn 1998 \"Interim Regulations\" were founded in China, referring to the DMCA. China also has Intellectual Property Rights, which to the World Trade Organization, was \"not in compliance with the Berne Convention\". The WTO panel \"determined that China's copyright laws do not provide the same efficacy to non- Chinese nationals as they do to Chinese citizens, as required by the Berne Convention\". and that \"China's copyright laws do not provide enforcement procedures so as to permit effective action against any act of infringement of intellectual property rights\". Because China has a form of DMCA and or copyright laws, it is assumed they are using some kind of technology to enforce those laws, however there is no mention of specific DRM technology.\n\nOn 22 May 2001, the European Union passed the EU Copyright Directive, an implementation of the 1996 WIPO Copyright Treaty, that addressed many of the same issues as the DMCA.\n\nOn 25 April 2007, the European Parliament supported the first directive of EU, which aims to harmonize criminal law in the member states. It adopted a first reading report on harmonizing the national measures for fighting copyright abuse. If the European Parliament and the Council approve the legislation, the submitted directive will oblige the member states to consider a crime a violation of international copyright committed with commercial purposes. The text suggests numerous measures: from fines to imprisonment, depending on the gravity of the offense. The EP members supported the Commission motion, changing some of the texts. They excluded patent rights from the range of the directive and decided that the sanctions should apply only to offenses with commercial purposes. Copying for personal, non-commercial purposes was also excluded from the range of the directive.\n\nIn 2012, the Court of Justice of the European Union ruled in favor of reselling copyrighted games, prohibiting any preventative action that would prevent such transaction. The court said that \"The first sale in the EU of a copy of a computer program by the copyright holder or with his consent exhausts the right of distribution of that copy in the EU. A rightholder who has marketed a copy in the territory of a Member State of the EU thus loses the right to rely on his monopoly of exploitation in order to oppose the resale of that copy.\"\n\nIn 2014, the Court of Justice of the European Union ruled that circumventing DRM on game devices may be legal under some circumstances, limiting the legal protection to only cover technological measures intended to prevent or eliminate unauthorised acts of reproduction, communication, public offer or distribution.\n\nIndia is not a signatory to WIPO Copyright Treaty nor the WIPO Performances and Phonograms Treaty. However, as a part of its 2012 amendment of copyright laws, it implemented digital rights management protection. Section 65A of Copyright Act, 1957 imposed criminal sanctions on circumvention of \"effective technological protection measures\". Section 65B criminalized interference with digital rights management information. Any distribution of copies whose rights management information was modified was also criminalized by Section 65B. The terms used in the provisions were not specifically defined, with the concerned Parliamentary Standing Committee indicating the same to have been deliberate. The Standing Committee noted that similar terms in developed terms were used to considerable complexity and therefore in light of the same, it was preferable to keep it open-ended. \n\nA prison sentence is mandatory under both provisions, with a maximum term of 2 years in addition to fine, which is discretionary. While the statute doesn't include exceptions to copyright infringement, including fair use directly, Section 65A allows measures \"unless they are expressly prohibited\", which may implicitly include such exceptions. Section 65B however, lacks any exceptions. Further. Section 65B (digital rights management information) allows resort to other civil provisions, unlike Section 65A. \n\nIt is important to note that the WIPO Internet Treaties themselves do not mandate criminal sanctions, merely requiring \"effective legal remedies\". Thus, India's adoption of criminal sanctions ensures compliance with the highest standards of the WIPO internet treaties. Given the 2012 amendment, India's entry to the WIPO Internet Treaties appears facilitated, especially since ratification of the WIPO Internet Treaties is mandatory under agreements like the RCEP. \n\n Israel had not ratified the WIPO Copyright Treaty. Israeli law does not currently expressly prohibit the circumvention of technological measures used to implement digital rights management. In June 2012 The Israeli Ministry of Justice proposed a bill to prohibit such activities, but the Knesset did not pass it. In September 2013, the Supreme Court ruled that the current copyright law could not be interpreted to prohibit the circumvention of digital rights management, though the Court left open the possibility that such activities could result in liability under the law of unjust enrichment.\n\nIn May 1998, the Digital Millennium Copyright Act (DMCA) passed as an amendment to US copyright law, which criminalizes the production and dissemination of technology that lets users circumvent technical copy-restriction methods.(For a more detailed analysis of the statute, see WIPO Copyright and Performances and Phonograms Treaties Implementation Act.)\n\nReverse engineering of existing systems is expressly permitted under the Act under the specific condition of a safe harbor, where circumvention is necessary to achieve interoperability with other software . See 17 U.S.C. Sec. 1201(f). Open-source software to decrypt content scrambled with the Content Scrambling System and other encryption techniques presents an intractable problem with the application of the Act. Much depends on the intent of the actor. If the decryption is done for the purpose of achieving interoperability of open source operating systems with proprietary operating systems, it would be protected by Section 1201(f) the Act. Cf., Universal City Studios, Inc. v. Corley, 273 F.3d 429 (2d Cir. 2001) at notes 5 and 16. However, dissemination of such software for the purpose of violating or encouraging others to violate copyrights has been held illegal. See Universal City Studios, Inc. v. Reimerdes, 111 F. Supp. 2d 346 (S.D.N.Y. 2000).\n\nThe DMCA has been largely ineffective in protecting DRM systems, as software allowing users to circumvent DRM remains widely available. However, those who wish to preserve the DRM systems have attempted to use the Act to restrict the distribution and development of such software, as in the case of DeCSS.\n\nAlthough the Act contains an exception for research, the exception is subject to vague qualifiers that do little to reassure researchers. Cf., 17 U.S.C. Sec. 1201(g). The DMCA has affected cryptography, because many fear that cryptanalytic research may violate the DMCA. In 2001, the arrest of Russian programmer Dmitry Sklyarov for alleged infringement of the DMCA was a highly publicized example of the law's use to prevent or penalize development of anti-DRM measures. He was arrested in the US after a presentation at DEF CON, and spent several months in jail. The DMCA has also been cited as chilling to non-criminal inclined users, such as students of cryptanalysis including, Professor Felten and students at Princeton University; security consultants, such as Netherlands based Niels Ferguson, who declined to publish vulnerabilities he discovered in Intel's secure-computing scheme due to fear of being arrested under the DMCA when he travels to the US; and blind or visually impaired users of screen readers or other assistive technologies.\n\nIn Europe, there have been several ongoing dialog activities that are characterized by their consensus-building intention:\n\n\nMany organizations, prominent individuals, and computer scientists are opposed to DRM. Two notable DRM critics are John Walker, as expressed for instance, in his article \"The Digital Imprimatur: How Big brother and big media can put the Internet genie back in the bottle\", and Richard Stallman in his article \"The Right to Read\" and in other public statements: \"DRM is an example of a malicious feature – a feature designed to hurt the user of the software, and therefore, it's something for which there can never be toleration\". Stallman also believes that using the word \"rights\" is misleading and suggests that the word \"restrictions\", as in \"Digital Restrictions Management\", be used instead. This terminology has since been adopted by many other writers and critics unconnected with Stallman.\n\nOther prominent critics of DRM include Professor Ross Anderson of Cambridge University, who heads a British organization which opposes DRM and similar efforts in the UK and elsewhere, and Cory Doctorow, a writer and technology blogger.\n\nThere have been numerous others who see DRM at a more fundamental level. This is similar to some of the ideas in Michael H. Goldhaber's presentation about \"The Attention Economy and the Net\" at a 1997 conference on the \"Economics of Digital Information\". (sample quote from the \"Advice for the Transition\" section of that presentation: \"If you can't figure out how to afford it without charging, you may be doing something wrong.\")\n\nThe EFF and similar organizations such as FreeCulture.org also hold positions which are characterized as opposed to DRM.\n\nThe Foundation for a Free Information Infrastructure has criticized DRM's effect as a trade barrier from a free market perspective.\n\nThe final version of the GNU General Public License version 3, as released by the Free Software Foundation, has a provision that \"strips\" DRM of its legal value, so people can break the DRM on GPL software without breaking laws like the DMCA. Also, in May 2006, the FSF launched a \"Defective by Design\" campaign against DRM.\n\nCreative Commons provides licensing options encouraging the expansion of and building upon creative work without the use of DRM. In addition, Creative Commons licenses have anti-DRM clauses, therefore the use of DRM by a licensee to restrict the freedoms granted by a Creative Commons license is a breach of the Baseline Rights asserted by the licenses.\n\nBill Gates spoke about DRM at CES in 2006. According to him, DRM is not where it should be, and causes problems for legitimate consumers while trying to distinguish between legitimate and illegitimate users.\n\nAccording to Steve Jobs, Apple opposes DRM music after a public letter calling its music labels to stop requiring DRM on its iTunes Store. As of 6 January 2009, the iTunes Store is DRM-free for songs. \n\nThe Norwegian consumer rights organization \"Forbrukerrådet\" complained to Apple Inc. in 2007, about the company's use of DRM in, and in conjunction with, its iPod and iTunes products. Apple was accused of restricting users' access to their music and videos in an unlawful way, and of using EULAs which conflict with Norwegian consumer legislation. The complaint was supported by consumers' ombudsmen in Sweden and Denmark, and is currently being reviewed in the EU. Similarly, the United States Federal Trade Commission held hearings in March 2009, to review disclosure of DRM limitations to customers' use of media products.\n\nDRM opponents argue that the presence of DRM violates existing private property rights and restricts a range of heretofore normal and legal user activities. A DRM component would control a device a user owns (such as a digital audio player) by restricting how it may act with regard to certain content, overriding some of the user's wishes (for example, preventing the user from burning a copyrighted song to CD as part of a compilation or a review). Doctorow has described this possibility as \"the right to make up your own copyright laws\".\n\nAn example of this restriction to legal user activities may be seen in Microsoft's Windows Vista operating system in which content using a Protected Media Path is disabled or degraded depending on the DRM scheme's evaluation of whether the hardware and its use are 'secure'. All forms of DRM depend on the DRM-enabled device (e.g., computer, DVD player, TV) imposing restrictions that (at least by intent) cannot be disabled or modified by the user. Key issues around DRM such as the right to make personal copies, provisions for persons to lend copies to friends, provisions for service discontinuance, hardware agnosticism, software and operating system agnosticism, contracts for public libraries, and customers' protection against one-side amendments of the contract by the publisher have not been fully addressed.(see references 80–89) It has also been pointed out that it is entirely unclear whether owners of content with DRM are legally permitted to pass on their property as inheritance to another person.\n\nTools like FairUse4WM have been created to strip Windows Media of DRM restrictions.\n\nValve Corporation president Gabe Newell also stated \"most DRM strategies are just dumb\" because they only decrease the value of a game in the consumer's eyes. Newell suggests that the goal should instead be \"[creating] greater value for customers through service value\". Valve operates Steam, a service which serves as an online store for PC games, as well as a social networking service and a DRM platform.\n\nAt the 2012 Game Developers Conference, the CEO of CD Projekt Red, Marcin Iwinski, announced that the company will not use DRM in any of its future releases. Iwinski stated of DRM, \"it's just over-complicating things. We release the game. It's cracked in two hours, it was no time for . What really surprised me is that the pirates didn't use the GOG version, which was not protected. They took the SecuROM retail version, cracked it and said 'we cracked it' – meanwhile there's a non-secure version with a simultaneous release. You'd think the GOG version would be the one floating around.\" Iwinski added after the presentation, \"DRM does not protect your game. If there are examples that it does, then people maybe should consider it, but then there are complications with legit users.\"\n\nBruce Schneier argues that digital copy prevention is futile: \"What the entertainment industry is trying to do is to use technology to contradict that natural law. They want a practical way to make copying hard enough to save their existing business. But they are doomed to fail.\" He has also described trying to make digital files uncopyable as being like \"trying to make water not wet\". The creators of StarForce also take this stance, stating that \"The purpose of copy protection is not making the game uncrackable – it is impossible.\"\n\nThe Association for Computing Machinery and the Institute of Electrical and Electronics Engineers have historically opposed DRM, even going so far as to name AACS as a technology \"most likely to fail\" in an issue of IEEE Spectrum.\n\nIn reaction to opposition to DRM, many publishers and artists label their works as \"DRM-free\". Major companies that have done so include the following:\n\n\nMany DRM systems require authentication with an online server. Whenever the server goes down, or a region or country experiences an Internet outage, it effectively locks out people from registering or using the material. This is especially true for a product that requires a persistent online authentication, where, for example, a successful DDoS attack on the server would essentially make all copies of the material unusable.\n\nOne simple method to bypass DRM on audio files is to burn the content to an audio CD and then rip it into DRM-free files. Some software products simplify and automate this burn-rip process by allowing the user to burn music to a CD-RW disc or to a Virtual CD-R drive, then automatically rip and encode the music, and automatically repeat this process until all selected music has been converted, rather than forcing the user to do this one CD (72–80 minutes worth of music) at a time.\n\nMany software programs have been developed that intercept the data stream as it is decrypted out of the DRM-restricted file, and then use this data to construct a DRM-free file. These programs require a decryption key. Programs that do this for Blu-ray Discs, DVDs, and HD DVDs include universal decryption keys in the software itself. Programs that do this for iTunes audio, PlaysForSure songs, and TiVo ToGo recordings, however, rely on the user's own key – that is, they can only process content the user has legally acquired under his or her own account.\n\nAnother method is to use software to record the signals being sent through the audio or video cards or plug analog recording devices into the analog outputs of the media player. These techniques utilize the \"analog hole\".\n\nTo bypass DRM technologies embedded in video-streaming services, hackers employ a variety of methods. Besides rerecording and redistributing video streams, they place links to video-streaming services in web pages owned by the hackers, sell legitimate users' data on the black market for other people's use, and legitimate users sharing their account with family or friends who intend not to pay for the service.\n\nAll forms of DRM for audio and visual material (excluding interactive materials, e.g., videogames) are subject to the analog hole, namely that in order for a viewer to play the material, the digital signal must be turned into an analog signal containing light and/or sound for the viewer, and so available to be copied as no DRM is capable of controlling content in this form. In other words, a user could play a purchased audio file while using a separate program to record the sound back into the computer into a DRM-free file format.\n\nAll DRM to date can therefore be bypassed by recording this signal and digitally storing and distributing it in a non DRM limited form, by anyone who has the technical means of recording the analog stream. Furthermore, the analog hole cannot be overcome without the additional protection of externally imposed restrictions, such as legal regulations, because the vulnerability is inherent to all analog means of transmission. However, the conversion from digital to analog and back is likely to force a loss of quality, particularly when using lossy digital formats. HDCP is an attempt to plug the analog hole, although as of 2009, it was largely ineffective.\n\nAsus released a soundcard which features a function called \"Analog Loopback Transformation\" to bypass the restrictions of DRM. This feature allows the user to record DRM-restricted audio via the soundcard's built-in analog I/O connection.\n\nIn order to prevent this exploit, there has been some discussions between copyright holders and manufacturers of electronics capable of playing such content to no longer include analog connectivity in their devices. The movement, dubbed as \"Analog Sunset\", has seen a steady decline in analog output options on most Blu-ray devices manufactured after 2010.\n\nMany of the DRM systems in use are designed to work on general purpose computing hardware, such as desktop PCs, apparently because this equipment is felt to be a major contributor to revenue loss from disallowed copying. Large commercial copyright infringers avoid consumer equipment, so losses from such infringers will not be covered by such provisions.\n\nSuch schemes, especially software based ones, can never be wholly secure since the software must include all the information necessary to decrypt the content, such as the decryption keys. An attacker will be able to extract this information, directly decrypt and copy the content, which bypasses the restrictions imposed by a DRM system.\n\nMany DRM schemes use encrypted media which requires purpose-built hardware to hear or see the content. This appears to ensure that only licensed users (those with the hardware) can access the content. It additionally tries to protect a secret decryption key from the users of the system.\n\nWhile this in principle can work, it is extremely difficult to build the hardware to protect the secret key against a sufficiently determined adversary. Many such systems have failed in the field. Once the secret key is known, building a version of the hardware that performs no checks is often relatively straightforward. In addition user verification provisions are frequently subject to attack, pirate decryption being among the most frequented ones.\n\nA common real-world example can be found in commercial direct broadcast satellite television systems such as DirecTV and Malaysia's Astro. The company uses tamper-resistant smart cards to store decryption keys so that they are hidden from the user and the satellite receiver.\n\nWatermarks can often be removed, although degradation of video or audio can occur.\n\nMass redistribution of hard copies does not necessarily need DRM to be decrypted or removed, as it can be achieved by bit-perfect copying of a legally obtained medium without accessing the decrypted content. Additionally, still-encrypted disk images can be distributed over the Internet and played on legitimately licensed players.\n\nWhen standards and formats change, it may be difficult to transfer DRM-restricted content to new media, for instance Microsoft's new media player Zune did not support content that uses Microsoft's own PlaysForSure DRM scheme they had previously been selling. \n\nAdditionally, any system that requires contact with an authentication server is vulnerable to that server's becoming unavailable, as happened in 2007, when videos purchased from Major League Baseball (mlb.com) prior to 2006, became unplayable due to a change to the servers that validate the licenses. \n\nFurthermore, when a company undergoes business adjustment or even bankrupt, its legacy service may become unavailable. Examples include MSN Music, Yahoo! Music Store, Adobe Content Server 3 for Adobe PDF, Acetrax Video on Demand, etc.\n\nDRM can accelerate hardware obsolescence, turning it into electronic waste sooner:\n\n\nAccording to the EFF, \"in an effort to attract customers, these music services try to obscure the restrictions they impose on you with clever marketing.\"\n\nDRM laws are widely flouted: according to Australia Official Music Chart Survey, copyright infringements from all causes are practised by millions of people.\n\nJeff Raikes, ex-president of the Microsoft Business Division, stated: \"If they're going to pirate somebody, we want it to be us rather than somebody else\". An analogous argument was made in an early paper by Kathleen Conner and Richard Rummelt. A subsequent study of digital rights management for e-books by Gal Oestreicher-Singer and Arun Sundararajan showed that relaxing some forms of DRM can be beneficial to digital rights holders because the losses from piracy are outweighed by the increases in value to legal buyers.\n\nAlso, free distribution, even if unauthorized, can be beneficial to small or new content providers by spreading and popularizing content and therefore generating a larger consumer base by sharing and word of mouth. Several musicians have grown to popularity by posting their music videos on sites like YouTube where the content is free to listen to. This method of putting the product out in the world free of DRM not only generates a greater following but also fuels greater revenue through other merchandise (hats, T-shirts), concert tickets, and of course, more sales of the content to paying consumers.\n\nWhile the main intent of DRM is to prevent unauthorized copies of a product, there are mathematical models that suggest that DRM schemes can fail to do their job on multiple levels. The biggest failure that can result from DRM is that they have a potential to increase the infringement rate of a product. This goes against the held belief that DRM can always reduce unauthorized distribution. There also seems to be evidence that DRM will reduce profits.\n\nThe driving factor behind this is related to how many restrictions DRM imposes on a legal buyer. An ideal DRM would be one which imposes zero restrictions on legal buyers but makes imposing restrictions on infringers. Even if an ideal DRM can be created and used, in certain cases, it can be shown that removing the DRM will result in less unauthorized copying. For the ideal DRM, the reason why profits can increase is because of the demand is elastic. When there are more people legally buying and few people sharing the product, more profits are going to be made.\n\nThe mathematical models are strictly applied to the music industry (music CDs, downloadable music). These models could be extended to the other industries such as the gaming industry which show similarities to the music industry model. There are real instances when DRM restrain consumers in the gaming industry. Some DRM games are required to connect to the Internet in order to play them. Good Old Games' head of public relations and marketing, Trevor Longino, in agreement with this, believes that using DRM is less effective than improving a game's value in reducing video game infringement. However, TorrentFreak published a \"Top 10 pirated games of 2008\" list which shows that intrusive DRM is not the main reason why some games are copied more heavily than others. Popular games such as BioShock, Crysis Warhead, and Mass Effect which use intrusive DRM are strangely absent from the list.\n\nSeveral business models have been proposed that offer an alternative to the use of DRM by content providers and rights holders.\n\nThe first business model that dissuades illegal file sharing is to make downloading digital media easy and cheap. The use of noncommercial sites makes downloading digital media complex. For example, misspelling an artist's name in a search query will often fail to return a result, and some sites limit internet traffic, which can make downloading media a long and frustrating process. Furthermore, illegal file sharing websites are often host to viruses and malware which attach themselves to the files (see torrent poisoning). If digital media (for example, songs) are all provided on accessible, legitimate sites, and are reasonably priced, consumers will purchase media legally to overcome these frustrations.\n\nComedian Louis C.K. made headlines in 2011, with the release of his concert film \"Live at the Beacon Theater\" as an inexpensive (US$5), DRM-free download. The only attempt to deter unlicensed copies was a letter emphasizing the lack of corporate involvement and direct relationship between artist and viewer. The film was a commercial success, turning a profit within 12 hours of its release. Some, including the artist himself, have suggested that file sharing rates were lower than normal as a result, making the release an important case study for the digital marketplace.\n\nWebcomic Diesel Sweeties released a DRM-free PDF e-book on author R Stevens's 35th birthday, leading to more than 140,000 downloads in the first month, according to Stevens. He followed this with a DRM-free iBook specifically for the iPad, using Apple's new software, which generated more than 10,000 downloads in three days. That led Stevens to launch a Kickstarter project – \"ebook stravaganza 3000\" – to fund the conversion of 3,000 comics, written over 12 years, into a single \"humongous\" e-book to be released both for free and through the iBookstore; launched 8 February 2012, with the goal of raising $3,000 in 30 days, the project met its goal in 45 minutes, and went on to be funded at more than 10 times its original goal. The \"payment optional\" DRM-free model in this case was adopted on Stevens' view that \"there is a class of webcomics reader who would prefer to read in large chunks and, even better, would be willing to spend a little money on it.\"\n\nIn February 2012, Double Fine asked for an upcoming video game, \"Double Fine Adventure\", for crowdfunding on kickstarter.com and offered the game DRM-free for backers. This project exceeded its original goal of $400,000 in 45 days, raising in excess of $2 million. In this case DRM freedom was offered to backers as an incentive for supporting the project before release, with the consumer and community support and media attention from the highly successful Kickstarter drive counterbalancing Also, crowdfunding with the product itself as benefit for the supporters can be seen as pre-order or subscription business model in which one motivation for DRM, the uncertainty if a product will have enough paying customers to outweigh the development costs, is eliminated. After the success of \"Double Fine Adventure\", many games were crowd-funded and many of them offered a DRM-free game version for the backers.\n\nMany artists are using the Internet to give away music to create awareness and liking to a new upcoming album. The artists release a new song on the internet for free download, which consumers can download. The hope is to have the listeners buy the new album because of the free download. A common practice used today is releasing a song or two on the internet for consumers to indulge. In 2007, Radiohead released an album named in Rainbows, in which fans could pay any amount they want, or download it for free.\n\nThe Artistic Freedom Voucher (AFV) introduced by Dean Baker is a way for consumers to support “creative and artistic work.” In this system, each consumer would have a refundable tax credit of $100 to give to any artist of creative work. To restrict fraud, the artists must register with the government. The voucher prohibits any artist that receives the benefits from copyrighting their material for a certain length of time. Consumers can obtain music for a certain amount of time easily and the consumer decides which artists receive the $100. The money can either be given to one artist or to many, the distribution is up to the consumer.\n\n\n\n\n\n"}
{"id": "39071194", "url": "https://en.wikipedia.org/wiki?curid=39071194", "title": "Dokuo", "text": "Dokuo\n\nThe Chinese song \"Love and Honesty\" mentions \"dokuo\".\n"}
{"id": "44680097", "url": "https://en.wikipedia.org/wiki?curid=44680097", "title": "Enga Sign Language", "text": "Enga Sign Language\n\nEnga Sign Language is a briefly described village sign language of the Enga people in Papua New Guinea.\n\n"}
{"id": "54137111", "url": "https://en.wikipedia.org/wiki?curid=54137111", "title": "Glacial survival hypothesis", "text": "Glacial survival hypothesis\n\nAccording to the northern cryptic glacial refugial hypothesis (or glacial survival hypothesis), during the last ice age cold tolerant plant and animal species (e.g. Norway spruce and Norwegian lemmings) persisted in ice-free microrefugia north of the Alps in Europe. The alternative hypothesis of no persistence and postglacial immigration of plants and animals from southern refugia in Europe (southern refugia paradigm) is sometimes also called the tabula rasa hypothesis.\nOver the past plants and animals have persisted through long periods of climate change including several glacial and interglacial periods. There is a long-standing debate on what happened to the species that were inhabiting high-latitidue regions during the Pleistocene ice age. Two main scenarios are usually considered. The first scenario proposes a total extinction of species within glaciated areas with survival events in peripheral refugia in the south and successive massive postglacial migration into empty areas (tabula rasa hypothesis). The second scenario proposes long-term in situ survival within glaciated regions (glacial survival hypothesis), either in isolated northern ice-free micro-refugia at the edge of the ice sheet, or on exposed mountains not covered with ice within the ice sheet (nunatak hypothesis).\n\nFor boreal and cold-tolerant species the glacial survival hypothesis is credible, though controversial , and a growing body of molecular data is now supporting it for both plant and animal species . A number of recent studies have shown that several northern regions (above latitudes >45° N) have supported low-density boreal and temperate tree populations during the late-glacial or Early Holocene [e.g. North America, Eurasia, Alps, Scandinavia]..\n\nIn recent years several studies have combined lines of evidences coming from three major disciplines to infer the existence of past refugia: fossil records, species distribution models and molecular/phylogeographic surveys . In this way it should be possible to better describe complex migration routes followed by species and populations in and out of refugia through time and space.\n\n"}
{"id": "18126244", "url": "https://en.wikipedia.org/wiki?curid=18126244", "title": "Graphemics", "text": "Graphemics\n\nGraphemics or graphematics is the linguistic study of writing systems and their basic components, i.e. graphemes.\n\nAt the beginning of the development of this area of linguistics, Ignace Gelb coined the term \"grammatology\" for this discipline; later some scholars suggested calling it \"graphology\" to match \"phonology\", but that name is traditionally used for a pseudo-science. Others therefore suggested renaming the study of language-dependent pronunciation \"phonemics\" or \"phonematics\" instead, but this did not gain widespread acceptance either, so the terms \"graphemics\" and \"graphematics\" became more frequent.\n\nGraphemics examines the specifics of written texts in a certain language and their correspondence to the spoken language. One major task is the descriptive analysis of implicit regularities in written words and texts (\"graphotactics\") to formulate explicit rules (\"orthography\") for the writing system that can be used in prescriptive education or in computer linguistics, e.g. for speech synthesis.\n\nIn analogy to phoneme and (allo)phone in phonology, the graphic units of language are graphemes, i.e. language-specific characters, and graphs, i.e. language-specific glyphs. Different schools of thought consider different entities to be graphemes; major points of divergence are the handling of punctuation, diacritic marks, digraphs or other multigraphs and non-alphabetic scripts.\n\nanalogous to phonetics, the \"etic\" counterpart of graphemics is called graphetics and deals with the material side only (including paleography, typography and graphology).\n\nGraphotactics refers to rules which restrict the allowable sequences of letters in alphabetic languages. A common example is the partially correct \"I before E except after C\". However, there are exceptions, for example Edward Carney in his book, \"A Survey of English Spelling\", refers to the \"I before E except after C” rule instead as an example of a “phonotactic rule”.\nGraphotactical rules are useful in error detection by optical character recognition systems.\n\nIn studies of Old English, \"graphotactics\" is also used to refer to the variable-length spacing between words.\n\n"}
{"id": "558481", "url": "https://en.wikipedia.org/wiki?curid=558481", "title": "Hassaniya Arabic", "text": "Hassaniya Arabic\n\nHassānīya ( \"\"; also known as \"Hassaniyya\", \"Klem El Bithan\", \"Hasanya\", \"Hassani\", \"Hassaniya\") is a variety of Maghrebi Arabic. It was spoken by the Beni Ḥassān Bedouin tribes, who extended their authority over most of Mauritania and Morocco's southeastern and Western Sahara between the 15th and 17th centuries. Hassaniya Arabic was the language spoken in the pre-modern region around Chinguetti.\n\nThe language has now almost completely replaced the Berber languages that were originally spoken in this region. Although clearly a western dialect, Hassānīya is relatively distant from other Maghrebi variants of Arabic. Its geographical location exposed it to influence from Zenaga-Berber and Wolof. There are several dialects of Hassānīya which differ primarily phonetically. Today, Hassānīya is spoken in Algeria, Libya, Morocco, Mauritania, Mali, Niger, Senegal and the Western Sahara.\n\nThe phonological system of Hassānīya is both very innovative and very conservative. All phonemes of Classical Arabic are represented in the dialect, but there are also many new phonemes. As in other Bedouin dialects, Classical /q/ corresponds mostly to dialectal , and have merged into and the interdentals and have been preserved. In common with most Maghrebi Arabic varieties, the letter ج (j) is realised as .\n\nHowever, there is sometimes a double correspondence of a classical sound and its dialectal counterpart. Thus classical is represented by in 'to take' but by in 'scissors'. Similarly, becomes in 'laugh (noun)', but in 'to be sick'. Some consonant roots even have a double appearance: 'heavy (mentally)' vs. 'heavy (materially)'. Some of the \"classicizing\" forms are easily explained as recent loans from the literary language (such as 'law') or from sedentary dialects in case of concepts pertaining to the sedentary way of life (such as 'scissors' above). For others, there is no obvious explanation (like 'to be sick'). Etymological appears constantly as , never as .\n\nNevertheless, the phonemic status of and as well as and appears very stable, unlike in many other Arabic varieties. Somewhat similarly, classical has in most contexts disappeared or turned into or ( 'family' instead of , 'insist' instead of and 'yesterday' instead of ). In some literary terms, however, it is clearly preserved: 'suffering (participle)' (classical ).\n\nHassānīya has innovated many consonants by the spread of the distinction \"emphatic/non-emphatic\". In addition to the above-mentioned, and have a clear phonemic status and more marginally so. One additional emphatic phoneme is acquired from the neighbouring Zenaga Berber language along with a whole palatal series from Niger–Congo languages of the south. At least some speakers make the distinction /p/–/b/ through borrowings from French (and Spanish in Western Sahara). All in all, the number of consonant phonemes in Hassānīya is 33, or 39 counting the marginal cases.\n\nOn the phonetic level, the classical consonants and are usually realised as voiced (hereafter marked ) and . The latter is still, however, pronounced differently from , the distinction probably being in the amount of air blown out (Cohen 1963: 13–14). In geminated and word-final positions both phonemes are voiceless, for some speakers /θ/ apparently in all positions. The uvular fricative is likewise realised voiceless in a geminated position, although not fricative but plosive: . In other positions, etymological seems to be in free variation with (etymological , however varies only with ).\n\nVowel phonemes come in two series: long and short. The long vowels are the same as in Classical Arabic , and the short ones extend this by one: . The classical diphthongs and may be realised in many different ways, the most usual variants being and , respectively. Still, realisations like and as well as and are possible, although less common.\n\nAs in most Maghrebi Arabic dialects, etymological short vowels are generally dropped in open syllables (except for the feminine noun ending ): > 'you (f. sg.) write', > > 'he wrote'. In the remaining closed syllables dialectal /a/ generally corresponds to classical , while classical and have merged into . Remarkably, however, morphological is represented by and by in a word-initial pre-consonantal position: 'I stood up' (root \"w-g-f\"; cf. 'I wrote', root \"k-t-b\"), 'he descends' (subject prefix \"i-\"; cf. 'he writes', subject prefix \"jə-\"). In some contexts this initial vowel even gets lengthened, which clearly demonstrates its phonological status of a vowel: 'they stood up'. In addition, short vowels in open syllables are found in Berber loanwords, such as 'man', 'calves of 1 to 2 years of age', and in passive formation: 'he was met' (cf. 'he met').\n\nMany educated Hassaniya Arabic speakers also practice code-switching. In Western Sahara it is common for code-switching to occur between Hassaniya Arabic, Modern Standard Arabic, and Spanish, as Spain had previously controlled this region; in the rest of Hassaniya-speaking lands, French is the additional language spoken, except Libya, where Italian is spoken by educated speakers.\n\nAccording to \"Ethnologue\", there are approximately three million Hassaniya speakers, distributed as follows:\n\n\n\n"}
{"id": "24766072", "url": "https://en.wikipedia.org/wiki?curid=24766072", "title": "Healing the deaf mute of Decapolis", "text": "Healing the deaf mute of Decapolis\n\nHealing the deaf mute of Decapolis is one of the miracles of Jesus in the Gospels, namely Mark 7:31-37. Its narration offers many parallels with the healing of the blind man of Bethsaida in Mark 8:22-26.\n\nAccording to the Gospel of Mark, when Jesus entered the region of the Decapolis after passing through Sidon and down the Sea of Galilee, some people brought to him a man who was deaf and could hardly talk, and they begged Jesus to place his hand on him. This account follows the healing of the daughter of a Syro-Phoenician woman who speaks with Jesus about whether his mission extends to the gentiles (Mark 7:24-30). The deaf-mute man lives in the gentile Decapolis region, although the text does not specify that he is a gentile. The Gospel of Mark states:\n33 After he took him aside, away from the crowd, Jesus put his fingers into the man's ears. Then he spit and touched the man's tongue. 34 He looked up to heaven and with a deep sigh said to him, \"Ephphatha!\" (which means \"Be opened!\"). 35 At this, the man's ears were opened, his tongue was loosened and he began to speak plainly. Jesus commanded them not to tell anyone. But the more he did so, the more they kept talking about it. People were overwhelmed with amazement. \"He has done everything well,\" they said. \"He even makes the deaf hear and the mute speak.\"\nNew Testament commentator Lamar Williamson writes that this is the last unit in a series of miracles concerned with the identity of Jesus, as subsequently confirmed by the Apostle Peter's christological affirmation in Mark 8:29, where Peter exclaimed: \"You are the Messiah\".\n\n"}
{"id": "7909065", "url": "https://en.wikipedia.org/wiki?curid=7909065", "title": "Heciyê Cindî", "text": "Heciyê Cindî\n\nHeciyê Cindî (sometimes spelled Hajiye Jndi; 1908–1990) was a Yazidi linguist and researcher.\n\nCindî was born into a Yazidi family in the village of Emençayîr near Kars. During World War I and Turkish and Soviet invasions, his family fled to Armenia and settled in the village of Elegez. Later on, he lost all his family (except for one brother) to disease and massacre. In 1919, he stayed in the American orphanage in Alexandropol, and in 1926 was transferred to the orphanage in Leninakan, Armenia.\n\nDuring 1929-30, Cindî taught in the villages of Qundexsaz and Elegez, and was head of the cultural section of the Kurdish newspaper \"Riya Teze\" in 1930. He also worked as a news anchor in the Kurdish section of Radio Yerevan. In 1933, he joined the Writers Union of Armenia and attended the meeting of the Soviet Writers Congress the following year. In 1937, during Joseph Stalin's purges, he was imprisoned on March 18, 1937 on charges of spying, nationalism, being a Yezidi and helping counter-revolutionaries. After one year, several Armenian intellectuals campaigned for his release and he was pardoned, but was not allowed to work; however, with Alexander Fadeyev's help and support, he was able to resume his literary work.\n\nIn 1940, Cindî received his PhD in Kurdish folklore, and in 1941 the Armenian government, put him in charge of changing Kurdish alphabet from Latin to Cyrillic. The new alphabet was approved and published in 1946, and it was used in Kurdish education in Armenia, Georgia and several Central Asian republics. In 1959, he was employed in the Oriental department of Armenian Academy of Sciences, where he headed the Kurdology section for the next eight years.\n\nIn 1940 and 1952 his book \"Kurdish Fairy Tales\" was published in Armenian, in 1959 in Kurdish (\"Һ’ьк’йа̄тед щьмаә’тә к’ӧрдие\").\n\nFrom 1968 to 1974, Cindî taught Kurdish literature and language at the Oriental Faculty of the University of Yerevan. He wrote and translated many books, among them 15 books on folklore and literature, 33 textbooks for schools, 19 translations and 7 books in pedagogy.\n\n\n\n"}
{"id": "19013759", "url": "https://en.wikipedia.org/wiki?curid=19013759", "title": "Jesús Padilla Gálvez", "text": "Jesús Padilla Gálvez\n\nJesús Padilla Gálvez (xe'sus pa'ðiʎa 'ɣalβeθ) (born October 28, 1959) is a philosopher who worked primarily in philosophy of language, logic, and the history of sciences.\n\nJesús Padilla Gálvez studied Philosophy, History and Mathematics at the University of Cologne (Germany) and was awarded the M.A. in 1983 and a Dr. phil. in Philosophy in 1988. He was Research Assistant (1988–1991) at the University of Murcia (Spain) and later held the post of Associate Professor (1992–1994) for Logic and Philosophy of Language at the University of León (Spain). From 1994 to 1999 he was Visiting Professor at the Johannes Kepler University in Linz (Austria). Since 1999 he has been Professor at the University of Castilla-La Mancha in Toledo (Spain). He has held visiting posts at the Universities of University of Erlangen-Nuremberg (Germany), University of Graz (Austria), University of Potsdam (Germany), University of Cambridge (United Kingdom), University of Munich (Germany), University of Vienna and the University of Oxford (United Kingdom). As director of the international scientific journal Dókos. Revista Filosófica - Philosophical Review (published in Ápeiron Ediciones (Madrid) and editor of the international series \"Aporia / Aπορία\" (De Gruyter) the author is continuously engaged with highly relevant philosophical topics.\n\nIn his doctoral thesis, Padilla Gálvez examined central issues of analytic philosophy. His subsequent investigations are characterized by an application of the analytic method to the study of philosophical problems. This method involves both, the analysis of language to eliminate ambiguity as well as a profound scrutiny of the logical form of philosophical propositions. Padilla Galvez's work may be classified into five lines of investigation: philosophy of language, logic, philosophy of history of science, social changes in democratic systems and language for specific purposes (LSP). He was dealing with the works of Gottfried Leibniz, Immanuel Kant, Edmund Husserl, Alfred Tarski, Ludwig Wittgenstein, Willard Van Orman Quine, Saul Kripke, among others. Padilla Gálvez is recognized as one of the leading experts of Ludwig Wittgenstein´s philosophy in the English and Spanish-speaking world.\nAs the co-author of the \"Wittgenstein Studien\" he has published extensively in the field by focusing on logic-grammatical analysis of language. This includes such topics as history of logic and mathematics (Kurt Gödel), Action, Decision-Making and Forms of Life), Anthropology, language, aesthetics, politics (democracy and terrorism) and practical philosophy.\n\nHe is the author of\n\n\n\nand of numerous articles in Diálogos, Grazer Philosophische Studien, Journal for General Philosophy of Science, Logos, Mathesis, Modern Logic, Philosophia Naturalis, Philosophisches Jahrbuch, Wittgenstein-Studien, Zeitschrift für Philosophische Forschung and other journals and collections.\n\n"}
{"id": "1672161", "url": "https://en.wikipedia.org/wiki?curid=1672161", "title": "John Abraham (director)", "text": "John Abraham (director)\n\nJohn Abraham (11 August 1937 – 31 May 1987) was a Malayali Indian filmmaker, short story writer and screenwriter. \n\nAbraham is ranked among the greatest Indian film directors. His film \"Amma Ariyan\" (1986) was the only south Indian feature film to make the list of \"Top 10 Indian Films\" of all time by British Film Institute. \"Agraharathil Kazhuthai\" was listed among the \"100 Greatest Indian Films\" of all time by IBN Live's 2013 poll.\n\nJohn Abraham was born in Chennamkary, Kuttanadu in 1937. He is from the Vazhakkat branch, Chennamkary of the Pattamukkil Family. He completed his intermediate studies in C.M.S, College, at Kottayam staying with his grandfather, who nurtured Abraham's talent in early days. After completing his degree in [history and politics] from Marthoma College, Thiruvalla, he worked as a private college teacher and latter he joined as an office assistant with Life Insurance Corporation of India in Udupi, Karnataka. After that he joined the FTII, Pune and there he met film-makers such as Ritwik Ghatak and Mani Kaul. Abraham graduated out of the FTII with gold medals in screenwriting and film direction. He entered the film industry working as an assistant director to Mani Kaul for the film \"Uski Roti\" (1969, Hindi). He has worked for some Hindi projects that was shot in Kerala, but none were released. Abraham's first attempt in direction came in 1967 named \"Vidyarthikale Ithile Ithile\". It was the Tamil film \"Agraharathil Kazhuthai\" (1977) that gave Abraham recognition.\n\nHe completed only four films, namely \"Vidyarthikale Ithile Ithile\" (1972), \"Agraharathil Kazhuthai\" (1977, Tamil), \"Cheriachante Krurakrithyangal\" (1979, Malayalam) and \"Amma Ariyan\" (1986, Malayalam).\n\nUnder Abraham the \"Odessa Collective\" came into existence in 1984 with a street drama in Fort Kochi named \"Nayakali\" (The game of dogs). \"Odessa\" was an attempt by a group of movie enthusiasts to change the history of film production and distribution by making it a collaborative effort with the public and thus act as an empowering and liberating medium. For the financing of the first film produced by Odessa, Abraham and his friends travelled through villages and collected money from the general public. Odessa also collected funds for the film by screening Charlie Chaplin's \"The Kid\". The film, \"Amma Ariyan\" (Report to mother) (1986) was exhibited across the state of Kerala on a non-commercial basis, an initiative kept alive, after Abraham's death, by his colleague and co-founder of Odessa Collective, Odessa Sathyan.\n\nHe started shooting a documentary based on the life of E.M.S. Namboodiripad, but never completed it.\n\nThe media called him \"Ottayan\" (The Lone Tusker).\n\nHe has left behind a number of complete and incomplete scripts. A collection of his stories had been published under the title Nerchakkozhi. Another collection of his stories has been published posthumously under the title \"John Abrahaminte Kathakal\" by Pakshikkottam Books, Thiruvananthapuram in 1993.\n\nOn 30 May 1987 Abraham was admitted to the Calicut Medical College hospital following his fall from a house top after a party. He was not identified by the hospital authorities, and allegedly not given due attention and medical care, which caused his condition to deteriorate, leading to his death on 31 May. Following the allegations of medical negligence, a departmental inquiry was conducted into the incident. 26 years after Abraham's death, social activist B. Ekbal who was a surgeon at the Calicut Medical College when Abraham was admitted for treatment, revealed that the director could have been saved if his identity was known to the doctors at the time of admission. He said the doctors at the casualty did not know Abraham and mistook him for a film representative when he said that he was a filmmaker. In a Facebook post, Ekbal said the doctors failed to diagnose internal bleeding suffered by Abraham and to check his blood pressure which could have prevented him from slipping into a shock through a timely surgery.\n\n\n\nNational Film Awards:\n\nKerala State Film Awards:\n\n\n"}
{"id": "10582746", "url": "https://en.wikipedia.org/wiki?curid=10582746", "title": "John Edgar McFadyen", "text": "John Edgar McFadyen\n\nJohn Edgar McFadyen B. A. (Oxon), M. A., D. D. (17 July 1870 – 1933) a Scottish theologian, was professor of language, literature and Old Testament theology in the University of Glasgow. He was born in Glasgow and died in 1933.\n\nHe produced translations of a number of books of the Bible in what he labelled \"modern speech\". His translations of Job and Psalms strove to be metrical, to reflect their poetic originals. He learned Esperanto in 1907 during a stay in Chautauqua, New York, and was a prominent proponent of that language.\n\n\n"}
{"id": "8242738", "url": "https://en.wikipedia.org/wiki?curid=8242738", "title": "Languages of Denmark", "text": "Languages of Denmark\n\nThe Kingdom of Denmark has only one official language, Danish, the national language of the Danish people, but there are several minority languages spoken, namely Faroese, German, and Greenlandic.\n\nA large majority (86%) of Danes also speak English as a second language; it is mandatory for Danish students to learn from the first grade in the public elementary schools (), by far the most popular option in the country. In the 1st (or 3rd, depends on the school) grade of folkeskole, a third language option is given, usually German or French. The vast majority pick German (47% of Danes report being able to speak conversational German). The third most widely understood foreign language is Swedish, with 13% of Danes reporting to be able to speak it. \n\nFaroese, a North Germanic language like Danish, is the primary language of the Faroe Islands, a self-governing territory of the Kingdom. It is also spoken by some Faroese immigrants to mainland Denmark. Faroese is similar to Icelandic, and also the Old Norse language spoken.\n\nGerman is an officially recognized minority language in the former South Jutland County (part of what is now the Region of Southern Denmark), which was part of Imperial Germany prior the Treaty of Versailles. Between 15,000 and 20,000 Ethnic Germans live in South Jutland, of whom roughly 8,000 use either the standard German or the Schleswigsch variety of West Low German in daily communications. Schleswigisch is highly divergent from Standard German and can be quite difficult to understand by Standard German speakers. Outside of South Jutland, the members of St. Peter's Church in Copenhagen use German in their Church, its website, and the school that it runs.\n\nThe German minority operates its own system of primary schools with German as the primary language of instruction as well as a system of libraries throughout South Jutland. It also operates a German high school located in Aabenraa (German: Apenrade).\n\nBeside this there are also 28,584 immigrants from Germany in Denmark in 2012.\n\nGreenlandic is the main language of the 54,000 Greenlanders living in Greenland, which is, like the Faroe Islands, a self-governing territory of the Kingdom. Roughly 7,000 people speak Greenlandic in mainland Denmark. Greenlandic is similar to Inuktitut, and also the Eskimo–Aleut languages spoken in the Arctic North America, and Eastern Siberian area.\n\n"}
{"id": "3177923", "url": "https://en.wikipedia.org/wiki?curid=3177923", "title": "Literature by country", "text": "Literature by country\n\nThis is a list of literature pages categorized by country, language, or cultural group. Sometimes these literatures will be called national literatures because they help define a national identity or provide a common reference point for that country's culture. \n\n"}
{"id": "371700", "url": "https://en.wikipedia.org/wiki?curid=371700", "title": "Locale (computer software)", "text": "Locale (computer software)\n\nIn computing, a locale is a set of parameters that defines the user's language, region and any special variant preferences that the user wants to see in their user interface. Usually a locale identifier consists of at least a language code and a country/region code.\n\nOn POSIX platforms such as Unix, Linux and others, locale identifiers are defined by ISO/IEC 15897, which is similar to the BCP 47 definition of language tags, but the locale variant modifier is defined differently, and the character set is included as a part of the identifier. It is defined in this format: <nowiki>[language[_territory][.codeset][@modifier]]</nowiki>. (For example, Australian English using the UTF-8 encoding is en_AU.UTF-8.)\n\nThese settings usually include the following display (output) format settings:\n\n\nThe locale settings are about formatting output given a locale. So, the timezone information and daylight saving time are not usually part of the locale settings.\nLess usual is the input format setting, which is mostly defined on a per application basis.\n\nFurthermore, the general settings usually include the keyboard layout setting.\n\nIn these environments,\n\nand other (nowadays) Unicode-based environments, they are defined in a format similar to BCP 47. They are usually defined with just ISO 639 (language) and ISO 3166-1 alpha-2 (2-letter country) codes.\n\nOn POSIX platforms, locale identifiers are defined by ISO/IEC 15897, which is similar to the BCP 47 definition of language tags, but the locale variant modifier is defined differently, and the character set is included as a part of the identifier.\nIn the next example there is an output of command codice_1 for Czech language (cs), Czech Republic (CZ) with explicit UTF-8 encoding:\n\nWindows uses specific language and territory strings.\nThe \"locale identifier\" (LCID) for unmanaged code on Microsoft Windows is a number such as 1033 for English (United States) or 1041 for Japanese (Japan). These numbers consist of a language code (lower 10 bits) and culture code (upper bits) and are therefore often written in hexadecimal notation, such as 0x0409 or 0x0411. The list of those codesets are described in character encoding.\nMicrosoft is starting to introduce managed code application programming interfaces (APIs) for .NET that use this format. One of the first to be generally released is a function to mitigate issues with internationalized domain names, but more are in Windows Vista Beta 1.\n\nStarting with Windows Vista, new functions that use BCP 47 locale names have been introduced to replace nearly all LCID-based APIs.\n\n\n"}
{"id": "1905417", "url": "https://en.wikipedia.org/wiki?curid=1905417", "title": "Loony left", "text": "Loony left\n\nThe Loony Left is a pejorative term to describe those considered to be politically far-left. The term was widely used in the campaign for the 1987 general election and subsequently both by the Conservative Party and by British newspapers that supported the Conservatives as well as by more moderate factions within the Labour movement to refer to the activities of more militantly left-wing politicians that they believed moderate voters would perceive as extreme or unreasonable. The label was directed at the policies and actions of some Labour controlled inner-city councils and some Labour Party politicians. Although the labels \"hard left\" and \"soft left\" reflected a genuine political division within the Labour Party, \"Loony Left\" was by far the more often used label than either. While academics have depicted the era as of the \"new urban left\" (such as the rate-capping rebellion) as a throwback to earlier municipal militancy (e.g. Poplarism), wider media coverage tended to focus on the personalities of city leaders such as the Greater London Council's Ken Livingstone and Liverpool's Derek Hatton.\n\nThe term \"Loony Left\" as used to describe certain aspects of Labour politics was invented by the British popular press a few years before the 1987 general election. Throughout the run-up to the election it became a staple feature of press-coverage of the election, with many stories running detailing the \"antics\" of Labour politicians and Labour-controlled local government authorities.\n\nJolyon Jenkins recorded in 1987 that 1986 was the climax of the Loony Left campaign:\nThe ridicule of the political left by some British newspapers has a far longer history. Petley observes that the British press had long-since \"perfected a way of representing the ideas and personalities associated with socialism as so deranged and psychotic that they represented a danger to society\", thus rendering them fair game for editorial vilification. After his party's defeat in the 1983 general election, one newspaper had characterised Michael Foot's habit of swinging his walking stick around as he went for his morning walk as being \"like an escaped loony\". The election of Ken Livingstone to leader of the Greater London Council in 1981 had him regularly described in newspapers as \"barmy\" or \"loony\", with the GLC's policies labelled \"crazy\". These labels were increasingly also applied to local councils within London: The 13 March 1983 \"Sunday People\" labelled Islington local council the \"Bananas republic\"; and the 13 February 1983 \"Mail on Sunday\" labelled it \"The mad mad mad mad world of Islington\". In some ways, the \"Loony Left\" campaign was a generalisation of the Conservative campaign of demonising Livingstone and the GLC.\n\nAs recorded by Jenkins, the climax of the campaign was in 1986 and pivotal moments in its history were the London local council elections held in May 1986 and the Greenwich by-election, 1987 as well as of course the campaign for the 1987 general election.\n\nThe general theme that the \"Loony Left\" label suggested was twofold and Labour Party local government authorities were perceived to be:\n\n\"Loony Left\" was also used to describe specific individuals. Neil Kinnock, who had been subject to press vituperation since his election as party leader was associated with the \"Loony Left\" when in March 1987 he endorsed a rise of 60% in local council rates in Ealing, where he was a rate-payer. The \"Sun\" gave this the headline \"Kinnock admits — I back loonies\" and other newspapers put this forward as an example of support for extremism by the Labour Party leadership. A later story in the \"Daily Express\" about how Ken Livingstone purportedly had a left-wing takeover of the party arranged was denied by the Labour leadership only to have that reported as \"Neil Denies Truth About Left Plot\".\n\nSimilarly, Deirdrie Wood, Labour candidate in the 1987 Greenwich by-election, came to be known in the press as \"Dreadful Deirdrie\". Wood had been selected by her local constituency party against opposition from the Labour leadership. Privately, she had promised Kinnock \"I won't drop you in it\", to which he had replied \"It's not you, it's those bastards out there\", i.e. the press. Labour presented her as \"a hard-working local woman with sensible policies\", but the press portrayed her as a radical extremist both by association, as an IRA sympathiser living with a militant shop steward who was not the father of her children and directly as a \"hard left feminist, anti-racist and gay-rights supporter\" (as one \"News of the World\" report put it) who wanted to twin London schools with PLO camps.\n\nHowever, local authorities were the primary targets, in part because that is where progressives had found their platform in the 1980s. This was caused by two factors: a change in the composition of local authorities and the general election defeats for Labour from 1979 onwards. Partly because of structural changes to local authorities enacted in 1974, including the end of local aldermanic dignities and partly simply as a result of an influx of new people whose background had been in the radical youth movements of the 1960s, local government authorities became highly partisan political battlegrounds in the 1970s and 1980s that a canny politician would be able to use to construct a power base and as a stepping stone to a career in national-level politics.\n\nThis was compounded by the general election defeats for Labour, leaving the party with little ability to push its agenda at a national level in Westminster. As a result, local authorities became hotbeds of progressive and radical ideas and a conflict between local Labour local authorities and the Conservative central government on many issues ensued. Like municipal socialism before it, Labour leaderships at local level saw themselves as stronger than their Westminster party colleagues and capable of pushing socialist political agendas where they could not be pushed at national level. This resulted in an era of \"grand gesture politics\", with local authorities taking highly visible stances on national political issues such as declaring themselves nuclear-free zones and \"rainbow coalitions\" between local Labour party politicians and pressure groups for causes outside of Labour's traditional working class roots, such as anti-racism, gay rights, disabled rights and feminist groups.\n\nUnfortunately for Labour, the wide range of local-level policy initiatives that this engendered made it easy for Conservative opponents to then apply the \"Loony Left\" blanket label that the news media had handed to them, a political card that the Conservatives played at both local and national levels. The label was a particularly effective tactic against Labour-controlled local education authorities because the suggestion of innocent children being manipulated to further cynical adult political goals was a very potent image.\n\nThe label still occurs in British political discourse, even in the 21st century and has become a firmly embedded feature of British journalism. However, changes made by the Labour Party after the 1987 general election to ensure that it was no longer associated in the public mind with the images of the \"Loony Left\" from 1986–1987 have since blunted its impact and reduced its power, to the extent that it had far less impact on the United Kingdom general election, 1992, less even (according to academic studies by Butler and Kavanaugh) than Labour Party officials themselves believed at the time post-election.\n\nThese changes were in part an increased awareness of how important news media were to Labour's election campaign. One party press secretary said of Labour's attitude to the news media in the 1983 general election campaign: \"If a miracle had happened and Fleet Street had suddenly come clamouring to Walworth Road for pro-Labour material, they would have been sent away with a copy of the manifesto each\". The party leadership noted afterwards that it had been the effect of the \"Loony Left\" image that had caused it to lose the 1987 Greenwich by-election by such a large margin. This is not to say that Labour ignored the press, but it became reluctant to talk to it. Kinnock refused to talk to the press on the flight back from his visit to the U.S. President Ronald Reagan after British journalists had continually sought a story that would represent the trip in a negative light. Similarly, Patricia Hewitt, then party press secretary, considered abandoning holding daily press conferences in the run-up to the 1987 general election because \"they allow the newspaper journalists to set the agenda … and we know where they stand\".\n\nIn a widely leaked letter written to Frank Dobson after the Greenwich by-election and published by the \"Sun\" under the headline \"Gays put Kinnock in a panic — secret letter lashes loonies\", Hewitt said:\nNick Raynsford similarly ascribed the general election defeat to the \"Loony Left\" and other factors stating after the election that there were \"too many worrying skeletons in the Labour Party cupboard deterring voters\". In general, the \"soft left\" portion of the Labour Party blamed the \"Loony Left\" perception for this third general election defeat, despite the election campaign having been, in Larry Whitty's words, \"the most effective campaign the party has ever waged\". According to the \"soft left\", the Labour-controlled local government authorities had made errors in both pace and presentation, albeit that almost any initiative relating to race or sex, no matter how presented or paced, would have been seized by the press and held up for vilification.\n\nEven before the election, Labour was working hard to distance itself from the \"Loony Left\" perception. Roy Hattersley stated that at the time of the initiative of Brent Council to appoint race-relations advisers to schools \"I do not deny the existence of unacceptable behaviour in some local education authorities. I want to eliminate it\". Similarly, a question-and-answer pamphlet for voters prepared by staff at Labour headquarters had the question: \"But if I vote Labour won't I get a loony left council like those in London?\", to which the answer given was: \"Left councils are exceptions, Neil Kinnock has told them to mend their ways and he is in full charge of the Labour Party\".\n\nOn several occasions, the Labour Party leadership and others attempted to take a hard line on the \"Loony Left\" in order to gain a more favourable impression in the media. On 3 April 1987, for example, five Labour MPs with constituencies in Birmingham — Roy Hattersley, Denis Howell, Jeff Rooker, Terry Davis and Robin Corbett — wrote to Sharon Atkin, Bernie Grant and Linda Bellos in letters that they themselves leaked to the newspapers, demanding that they not attend a meeting in Birmingham, scheduled for 7 April, of activists campaigning for Black Sections within the Labour Party. Similarly, after the Greenwich by-election defeat five London Labour Party members — Brian Nicholson, Roger Godsiff, John Spellar, Roy Shaw and Dianne Hayter — formed the \"Londoners for Labour\" association, according to their press releases aimed at reclaiming the London Labour Party from \"the loonies\".\n\nThe 1980s UK press campaign against the \"Loony Left\" was echoed in the 1990s in the U.S. where sections of the press campaigned against political correctness, using much the same rhetoric. The same accusations made by the British press in the 1980s were levelled by U.S. newspapers such as \"The Chicago Tribune\", \"The New Republic\", \"Time\", \"Newsweek\" and \"New York\".\n\nAs Jenkins noted, the truth of the stories mattered less than their resonance with voter fears. Three of the most famously recorded instances of \"Loony Left\" activities - the renaming of the nursery rhyme Baa Baa Black Sheep, of \"manhole covers\" and of \"black bin-liner bags\" - were myths, outright fabrications by the press. Others stories, such as reports that London councils had insisted that homosexuals be placed at the heads of the waiting lists for council housing and that London councils had spent £500,000 on \"24 super-loos for gypsies\" were found to be highly misleading upon investigation by the Media Research Group of Goldsmiths' College, University of London.\n\nThe report of the MRG investigation estimated that some 3,000 news stories about the \"Loony Left\" ran between 1981 and 1987 in the British tabloid press alone. It determined that a large proportion of these stories were either partially or wholly fabricated and that their targets, against whom they aimed to inflame public opinion, were a small number of London local councils that were under Labour Party control.\n\nIn 1986, a parent-run nursery school banned the song \"Baa Baa Black Sheep\" over concerns the song might have racial undertones. Over time, media reports came to misreport the story, eventually incorrectly claiming the Birmingham City Council had ordered the lyrics be changed to \"Baa Baa green sheep\".\n\nVariants of this story have been reported repeatedly by the British mass media since 1986, to the state at which it has almost gained the status as an urban myth. Both \"The Age\" and \"The Herald\" reported in 2002, for example, the same \"Baa Baa White Sheep\" story, ascribing it to a parent of a child attending Paston Ridings Primary School in London.\n\nThe original story reported a ban at Beevers Nursery, a privately run nursery school in Hackney. It was originally reported by Bill Akass, then a journalist at the \"Daily Star\" in the 15 February 1986 edition under the headline \"Now it's Baa Baa Blank Sheep\". Akass had heard of a ban issued, by nursery school staff, on the singing of the nursery rhyme \"Baa, Baa, Black Sheep\" on the grounds that it was racist. In his story, he wrote:\n\nThe nursery was run by the parents, rather than by Hackney council, but Akass had telephoned Hackney council for its reaction to his story. Martin Bostock, then the press officer for Hackney council, reported that he had considered the possibility of simply responding: \"We don't know what this nursery is doing, but whatever they're doing it is up to them\". However, according to Bostock council leader Tony Millwood, Bostock rejected this advice and wanted to take a more supportive stance on the alleged ban and in conjunction with the press office drafted and issued a statement saying \"that we supported what they'd done, although making it quite clear that it was not a council nursery and not a council ban\".\n\nThree days later in the 18 February 1986 \"Hackney Gazette\", Tim Cooper took up Akass's story. He went to Beevers Nursery and asked parents there what their reactions were in turn to the Hackney council statement itself a reaction to the claim that Beevers had issued a ban. Cooper's story reported one of the nursery playleaders as saying: \"We're run by parents and if they want us to stop singing it, we would. But there have been no complaints so far, though someone once suggested it could be racist\". Cooper later stated that there had been no such ban, but that the statement issued by Millwood and Hackney council had given the story the impetus that it was then to run with:\n\nThe story was carried by the \"Sun\" in its 20 February 1986 edition under the headline \"Lefties baa black sheep\", with the ban attributed directly to \"Loony left-wing councillors\". The \"Sun\" version of events was subsequently carried by the 23 February 1986 \"Sunday World\". It was taken to the letters columns of the February 28, 1986 and 4 March 1986 \"Hackney Gazette\" and the 6 March 1986 \"Ilford Recorder\" and even reached the pages of the 4 April 1986 \"Knitting International\". Despite neither the journalists nor the letter-writers presenting any evidence for their assertions, only one paper (\"The Voice\") rejected the story in print, calling the story a deliberate attempt to discredit the council.\n\nThe 9 October 1986 \"Daily Mail\" carried the story further, with a story headlined \"Baa baa, green (yes green) sheep!\", reporting that Haringey (not Hackney) council had ordered playgroup leaders to attend a racialism awareness course, where they were instructed that the council had banned the rhyme with its original wording, mandating the alternative \"green sheep\" wording instead. This story was ascribed to an anonymous playgroup leader. From here, this new twist on the original story was carried by the \"Birmingham Evening Mail\" (\"Silly bleat\" and \"Green sheep? They've got to be joking.\"), the \"Liverpool Echo\" (\"Black sheep in the dog house\"), the \"Yorkshire Evening Press\" (\"So sheepish\"), the 10 October 1986 \"Birmingham Post\" (\"Racist sheep are a joke\"), the 12 October 1986 \"Sunday People\", \"News of the World\" (\"Green sheep take over\") and \"Sunday Mercury\", the 13 October 1986 Carlisle \"Evening News and Star\" (\"Bernie Bleat Barmy\"), the 14 October 1986 Yorkshire \"Evening Courier\" and the 15 October 1986 \"Liverpool Echo\" (\"Just Barmy\") and Ipswich \"Evening Star\" (\"A load of wollies!\").\n\nIn fact, the playgroup leaders had requested the racialism awareness course, at which attendance was not compulsory, there had been no ban imposed by Haringey council and there was no evidence that the rhyme had even been discussed on the course. As before, only newspapers for the British black community reported these facts. The attempts by the \"Daily Mail\" attempts to fact check the story that it had run, including posing as parents looking for playgroups and as supermarket managers wanting to run racialism awareness courses, had failed to elicit a single playgroup worker who would confirm the alleged council ban.\n\nHaringey council attempted to get the story straightened. It initiated legal action against the \"Daily Mail\", but was forced to drop it for lack of funds. The attempts by the council to rebut the story were ignored by the press for a week, its rebuttal only being first printed in the 16 October 1986 \"Haringey Advertiser\" (\"Black sheep still in evidence\")., but that was not enough to stem the tide as the story was now running, without independent fact checking, in newspapers all across the country, from \"Men's Wear\" to the 19 October \"Sunday Times\" letters column, the 23 October \"Hendon Times\" (\"Stop stirring up trouble\") and Auberon Waugh's column in the 19 October \"Sunday Telegraph\". The 23 October \"Mail on Sunday\" letters column carried letters noting that black pudding was henceforth to be \"green pudding\", the same day's \"Sunday Times\" letters column noted that blackheads could no longer be called blackheads.\n\nThe \"Daily Mail\" ran the story again on 20 October, comparing Haringey council to Nazi Germany. Again, the council attempted to set the record straight with a press statement that noted the irony of the \"Daily Mail\" comparing the council to Nazi Germany when the \"Mail\" itself had supported Hitler right up until the eve of World War II. Again, only the British black community newspapers (the 3 November \"Asian Herald\" and the 5 November \"West Indian News\") carried Haringey council's corrections. The story continued to be carried by many newspapers for months thereafter, including \"The Economist\" on 1 November and the \"Islington Gazette\" on 20 February 1987, this time with Islington council as the ban-issuer, a fact that was explicitly denied by a council spokesman in the piece, who said that \"it is not council policy to ban Baa Baa Black Sheep but if individual nursery workers find it offensive the council is not in the business of forcing them to teach that rhyme rather than others\". The \"Daily Express\" carried the new Islington variant on the same day (\"School bars boy's Baa-baa Black Sheep 'racist' rhyme\") as did the \"Daily Telegraph\" (\"Boy's first rhyme upsets nursery staff\"), the \"Daily Mirror\" (\"Baa Baa blacked\") and the \"Sun\" (\"Baa Baa nursery ban on sad little Dan\"), whose lead article the next day then began with \"Loony left councils have given us a good laugh over the years\".\n\nEven other political parties ran with the story. A party election broadcast for the Social Democratic Party, fronted by John Cleese, named Islington council as \"the council that accused a five year old of reciting a racially offensive poem\". Islington council sought an injunction in the High Court to have this material excluded, but this was denied by Mr Justice Drake. David Owen dropped the material anyway, stating that it was to avoid further distress to the five-year-old's family. Ironically, the press reported Owen's press conference, announcing this change as \"loony\" David Owen \"outclowning anything that Basil Fawlty could have thought up\".\n\nTwo newspapers recognised that the story had no foundation. The \"Yorkshire Evening Press\" printed on 14 November 1986 a correction to its earlier story, stating: \"We regret that the editorial, which was written in good faith, was based upon an inaccurate report\". The \"Birmingham Evening Mail\" published a letter from Bernie Grant on 22 October denying the story. Nonetheless and despite the corrections, court actions and attempts to set the record straight, the story has refused to die. The 8 February 1998 \"Sunday Times\", for example, re-hashed the story once again, eleven years after the fact, in a story about Margaret Hodge, former leader of Islington council, so too did the \"Daily Mail\" on 4 October 1999.\n\nIn 2000, the BBC reported the withdrawal of guidance to nursery schools by Birmingham City Council that the nursery rhyme \"Baa Baa Black Sheep\" should not be taught due to its \"racial offensiveness\". The advice was \"ridiculous\", according to Black British families in the area.\n\nThe London \"Evening Standard\" on 6 July 2000 reminded readers of Islington's reputation of being \"one of the country's most celebrated loony Labour councils\", again printing as fact the claim that Islington was where \"Baa Baa Black Sheep was banned for being politically incorrect\". Further stories about Hodge's ministerial promotions in the 14 June 2003 \"Sun\" and the 28 September 2003 \"Daily Mail\" also repeated this claim and attached it to Hodge.\n\nPeter Jenkins, a columnist for \"The Guardian\" and \"The Independent\", recorded policies which were dubbed \"loony left\" by the media. For instance, Haringey council allowed only Nicaraguan coffee to be sold and introduced courses on homosexuality into its nursery and primary schools.\n\nHackney London Borough Council ended its twinning arrangements with France, West Germany and Israel and made new twinnings with the Soviet Union, East Germany and Nicaragua. A spokesperson for the council explained: \"This will enable us to concentrate on our new friends\". When Sinn Féin representatives were invited to speak to Hackney council, a revolver was fired by a Liberal and there was a fight in the council chamber.\n\nLambeth London Borough Council banned the word \"family\" from council literature because this was \"discriminatory\" and police were banned from using council facilities. Lambeth council's leader, Linda Bellos, claimed: \"I think the police are bent on war\".\n\nEaling council removed all books it considered to be \"racist\" and \"sexist\" from its local libraries. An Inner London Education Authority (ILEA) teaching pack titled \"Auschwitz: Yesterday's Racism\" drew comparisons between the trade union legislation of Adolf Hitler and Margaret Thatcher. Another ILEA school in Kennington discouraged competitive games and making pupils write protest letters was made part of the school time-table.\n\n\n"}
{"id": "3534385", "url": "https://en.wikipedia.org/wiki?curid=3534385", "title": "Mararit language", "text": "Mararit language\n\nThe Mararit language is a Taman language spoken in eastern Chad. There are two dialects, Ibiri and Abou Charib, which Blench (2006) counts as distinct languages. The majority speak the Abou Charid.\n\nMararit people live in Argid Mararit, Abid Mararit, Wadah area, Donkey Kuma, Sani Kiro,in North Darfur State; in Silala area in South Darfur State and in Gienena provinvce in West Darfur State. The Talgai, Mirakawi, Wilkawi, and Tirgawi are tribes of the Mararit people .\n\n"}
{"id": "25123333", "url": "https://en.wikipedia.org/wiki?curid=25123333", "title": "Megaregions of the United States", "text": "Megaregions of the United States\n\nMegaregions of the United States are clustered networks of American cities, which are currently estimated to contain a total population exceeding 237 million. \n\n\"America 2050\", a project of the Regional Plan Association, lists 11 megaregions in the United States, Canada and Mexico. Megapolitan areas were explored in a July 2005 report by Robert E. Lang and Dawn Dhavale of the Metropolitan Institute at Virginia Tech. A later 2007 article by Lang and Nelson uses 20 megapolitan areas grouped into 10 megaregions. The concept is based on the original megalopolis model.\n\nA megaregion is a large network of metropolitan regions that share several or all of the following:\n\nA megaregion may also be known as a megalopolis or megapolitan area. More than 70 percent of the nation's population and jobs are located in 11 megaregions identified by the Regional Plan Association (RPA), which is an independent, non-profit New York-based planning organization. Megaregions are becoming the new competitive units in the global economy, characterized by the increasing movement of goods, people and capital among their metropolitan regions. \"The New Megas,\" asserted Richard Florida (2006), \"are the real economic organizing units of the world, producing the bulk of its wealth, attracting a large share of its talent and generating the lion's share of innovation.\"\n\nThe megaregion concept provides cities and metropolitan regions a context within which to cooperate across jurisdictional borders, including the coordination of policies, to address specific challenges experienced at the megaregion scale, such as planning for high-speed rail, protecting large watersheds, and coordinating regional economic development strategies.\n\nThe American-based Regional Plan Association recognizes 11 emerging megaregions:\n\nThe Regional Plan Association methodology for identifying the emerging megaregions included assigning each county a point for each of the following:\n\nThis methodology was much more successful at identifying fast-growing regions with existing metropolitan centers than more sparsely populated, slower growing regions. Nor does it include a distinct marker for connectedness between cities. The RPA method omits the eastern part of the Windsor-Quebec City urban corridor in Canada.\n\nNotes:\n\nThirteen of the top 100 American primary census statistical areas are not included in any of the 11 emerging mega-regions. However, the Lexington-based CSA in Kentucky is identified by the RPA as being part of an \"area of influence\" of the Great Lakes megalopolis, while the Albany and Syracuse-based CSAs in Upstate New York are shown as being within the influence of the Northeastern mega-region. Similarly, the Augusta, GA and Columbia, SC-based CMA are considered influenced by the Piedmont-Atlantic megalopolis, Jackson, MS CMA by the Gulf Coast megaregion, Little Rock, AR CMA by the Texas Triangle, and the Des Moines and Omaha-based CMAs by the Great Lakes megalopolis. The El Paso, TX CMA is roughly equidistant from two megaregions, being near the southeastern edge of the Arizona Sun Corridor area of influence and the southern tip of the Front Range area of influence. This leaves Honolulu, HI, Wichita, KS, Springfield, MO and Charleston, SC as the only top 100 American CMAs that have no mega-region affiliation of any kind as defined by the RPA.\n\nThough identification of the megaregions has gone through several iterations, the above identified are based on a set of criteria developed by Regional Plan Association, through its America 2050 initiative - a joint venture with the Lincoln Institute of Land Policy. Two historic publications helped lay the foundation for this new set of criteria, the book\" Megalopolis\" by Jean Gottmann (1961) and The Regions’ Growth, part of Regional Plan Association’s second regional plan.\n\nThe relationships underpinning megaregions have become more pronounced over the second half of the 20th century as a result of decentralized land development, longer daily commutes, increased business travel, and a more footloose, flexible, knowledge workforce. The identification of new geographic scales—historically based on increased population movement from the city center to lower density areas as a megaregion presents immense opportunities from a regional planning perspective, to improve the environmental, infrastructure and other issues shared among the regions within it. The most recent and only previous attempt to plan at this scale happened more than 70 years ago, with the Tennessee Valley Authority. Political issues stymied further efforts at river basin planning and development.\n\nIn 1961's Megalopolis, Gottman describes the Northeastern seaboard of the United States - or Megapologis - as \"... difficult to single out ... from surrounding areas, for its limits cut across established historical divisions, such as New England and the Middle Atlantic states, and across political entities, since it includes some states entirely and others only partially.\" On the complex nature of this regional scale, he writes:\n\nSome of the major characteristics of Megalopolis, which set it apart as a special region within the United States, are the high degree of concentration of people, things and functions crowded here, and also their variety. This kind of crowding and its significance cannot be described by simple measurements. Its various aspects will be shown on a number of maps, and if these could all be superimposed on one base map there would be demarcated an area in which so many kinds of crowding coincide in general (though not always in all the details of their geographical distribution) that the region is quite different from all neighboring regions and in fact from any other part of North America. The essential reason for its difference is the greater concentration here of a greater variety of kinds of crowding.\n\nCrowding of population, which may first be expressed in terms of densities per square mile, will, of course, be a major characteristic to survey. As this study aims at understanding the meaning of population density, we shall have to know the foundation that supports such crowding over such a very fast area. What do these people do? What is their average income and their standard of living? What is the distribution pattern of wealth and of certain more highly paid occupations? For example, the outstanding concentration of population in the City of New York and its immediate suburbs (a mass of more than ten million people by any count) cannot be separated from the enormous concentration in the same city of banking, insurance, wholesale, entertainment, and transportation activities. These various kinds of concentration have attracted a whole series of other activities, such as management of large corporations, retail business, travel agencies, advertising, legal and technical counseling offices, colleges, research organizations, and so on. Coexistence of all these facilities on an unequaled scale within the relatively small territory of New York City, and especially of its business district...has made the place even more attractive to additional banking, insurance, and mass media organizations.\n\nThe RPA report identifies megaregions that are shared between the US and Canada, and is presumably at least tangentially concerned with pan-North American issues. However, being based on largely American research, it does not clearly define the geographic extent of megaregions where they extend into Canada, a responsibility that has largely been left to Canadian geographers defining the megalopolis within their own country. The American report excludes Canadian population centres that are not deemed to be closely adjacent to US megaregions. It includes most of Southern Ontario in the Great Lakes Megaregion but excludes the St. Lawrence Valley, despite the fact that Canadian geographers usually include them as part of one larger Quebec City-Windsor Corridor.\n\nThe close relationship between large linked metropolitan regions and a nation's ability to compete in the global economy is recognized in Europe and Asia. Each has aggressively pursued strategies to manage projected population growth and strengthen economic prosperity in its large regions.\n\nThe European Spatial Development Perspective, a set of policies and strategies adopted by the European Union in 1999, is working to integrate the economies of the member regions, reduce economic disparities, and increase economic competitiveness (Faludi 2002; Deas and Lord 2006).\n\nIn East Asia, comprehensive strategic planning for large regions, centered on metropolitan areas, has become increasingly common and has progressed further than in the United States or Europe. Planning for the Hong Kong-Pearl River Delta region, for instance, aims to enhance the region's economic strength and competitiveness by overcoming local fragmentation, building on global economic cooperation, taking advantage of mutually beneficial economic factors, increasing connectivity among development nodes, and pursuing other strategic directions.\n\n"}
{"id": "37142071", "url": "https://en.wikipedia.org/wiki?curid=37142071", "title": "Melkon Giurdjian", "text": "Melkon Giurdjian\n\nMelkon \"Hrand\" Guirdjian (, 1859 in Palu, Ottoman Empire – 1915 in Ankara, Ottoman Empire) was a renowned Armenian writer, professor, and civic activist.\n\nMelkon Guirdjian was born in the Havav village in the Palu region that is now considered Elâzığ. Guirdjian attended the local Armenian school. He then moved to Constantinople at age 11 where he attended the prestigious Jemaran Armenian School of Üsküdar. After completing his studies at Jemaran and later Surp Hac Tbrevank, he became a teacher from 1878 to 1896. He taught Armenian history, language, and culture in many Armenian schools such as the prestigious Getronagan Armenian High School.\n\nIn 1893 he was imprisoned for alleged political activity. Due to the political instability, he fled to Varna, Bulgaria. In Varna, he founded the Armenian school Artzrunian, which served as a haven for Armenian refugees. During his absence from Constantinople, his house was searched by local police and many of his manuscripts and writings were burned. He felt the need to return in 1898 and was immediately arrested once again upon return. After spending 6 months in prison, Guirdjian escaped to Kastamonu, where he remains for 10 years. During his stay in Kastamonu, he gave Armenian literature and history lessons covertly. In 1906, on the basis of a secret agent's reports, his literary pieces were once again destroyed.\nWhen the Young Turk Revolution succeeded in 1908, Guirdjian returned to Constantinople and began participating in civic and literary activities once again.\n\nOn April 24, 1915, Melkon Guirdjian along with other prominent intellectuals and figures of the Constantinople's Armenian community were deported to unknown destinations within the interior provinces of Ottoman Turkey as part of the Armenian Genocide. Melkon Guirdjian found himself in the outskirts of Ankara and was ultimately killed by Turkish gendarmes.\n"}
{"id": "12910503", "url": "https://en.wikipedia.org/wiki?curid=12910503", "title": "Mike Sharwood Smith", "text": "Mike Sharwood Smith\n\nMichael Sharwood Smith (born 1942), Emeritus Professor of Languages at Heriot-Watt University & Honorary Professorial Fellow at the University of Edinburgh, is a researcher into multilingualism and the acquisition of non-native languages, a branch of developmental linguistics and cognitive science. He is a founding editor of \"Second Language Research\", successor to the \"Interlanguage Studies Bulletin\". \nTogether with John Truscott of National Tsing Hua University, Taiwan he has developed a new theoretical platform for studying language development and language use called the MOGUL Framework (Modular Growth and Use of Language) which explains language development as a by-product of processing albeit constrained by the modular architecture of the mind. His research interests have included theoretical issues concerning the way in which second languages are represented in the mind and how they interact, language attrition, the role of consciousness in language learning and also applications of linguistics such as the design of pedagogical grammars. He has introduced various concepts and associated terms into the field of second language acquisition research, notably \"grammatical consciousness-raising\" (Sharwood Smith 1981), \"crosslinguistic influence\" (Sharwood Smith 1982) and \"input enhancement\" (Sharwood Smith 1991), the last-mentioned idea focusing on the supposed learning effect of systematically making salient for learners certain specific linguistic features that are present in the language to which they are exposed. Together with James Pankhurst, he ran the annual LARS (Language Acquisition Research Symposia) meetings in Utrecht between 1983 and 1998, bringing together researchers in first and second language acquisition and also theoretical linguistics. Speakers at the LARS meetings have included leading figures such as Melissa Bowerman, Ray Jackendoff, Annette Karmiloff-Smith, Mary-Louise Kean, Brian MacWhinney, Frederick Newmeyer, Steven Pinker and Deirdre Wilson as well as many prominent researchers in second language acquisition.\n\n\n"}
{"id": "35052727", "url": "https://en.wikipedia.org/wiki?curid=35052727", "title": "Munka language", "text": "Munka language\n\nThe Bamunka language, \"Niemeng\", is a Grassfields Bantu language of Cameroon.\n\n"}
{"id": "41788896", "url": "https://en.wikipedia.org/wiki?curid=41788896", "title": "N. Ranganatha Sharma", "text": "N. Ranganatha Sharma\n\nMahamahopadya Dr. Vidvan N. Ranganatha Sharma (1916–2014) was a Sanskrit and Kannada scholar, particularly known for his erudition in grammar (vyākaraṇa) of both Sanskrit and Kannada.\n\nRanganatha Sharma was born in the village of Nadahalli on 7 January 1916. He taught in Bangalore at the Chamarajendra Sanskrit College for several decades (1948 to 1976), where he was Professor of Vyākaraṇa.\n\nHe was also a close associate of DVG (D. V. Gundappa).\n\nHis Sanskrit works include Bahubalivijayam (a Historical play, 1980) and Ekachakram (a mythological play, 1990); Guruparemitracaritam (1969) and Gommatesvarasuprabhatam (1981), and Gommatesa-panchakam.\n\nBhasantarapatha (1949), Laukika-nyayagalu (1959), Hosagannada Vyakarana, Valmikimunigala Hasyapravrtti, Varadahalli Sridhara Swamigalu, Sri Kamacandra (in the light of Bhasa, Kalidasa and Bhavabhuti, 1982), Suktivyapti (1991, Upanisattina Kathegalu (1993).\nTranslated works: Valmikiramayana, Amarakosa (1970), Viduraniti (1973), the Xth Skandha of Srimad-bhagavata (1978), Srivisnupurana (1986), Vyasatatparyanirnaya, Srtisarasamuddharana of Totakacarya, Brahma-kanda of Vakyapadiyam and Pancadasi of Vidyaranya.\nEdited works : Bhagavan¬namavali, Subhasitamanjari and Srichamarajoktivilasaramayana.\n\nHe is noted for his translation of the entire Valmiki Ramayana into Kannada, which was published with a foreword by DVG himself. He also wrote works on the Vishnu Purana, Srimad Bhagavata, Amarakosha in three \"kāṇḍa\"s, Bhartrhari and \"Sri Shankara Sookti Muktavali\". He edited DVG's \"Marulamuniyana Kagga\", a collection of poems which was published after DVG's death. Mr. Sharma has authored several books in Kannada and Sanskrit. He is a recipient of the national award for Sanskrit learning and has received the Rajyotsava Award. The Karnataka Samskrit University recently conferred an honorary doctorate on him. In his \"Laukika Nyayagalu\", he explained 219 maxims in Kannada.\n\nHis works in Sanskrit include the historical play Bāhubalivijayam (1981), and the mythological play \"Ekacakram\" (1990), based on the Adi Parva of the Mahabharata. He was the author of more than 80 works, over 45 in Kannada and 10 in Sanskrit. He wrote \"Hosagannada Vyakarana\" which was published in the year 2010.\n\nHe is a recipient of Karnatak State Award, President’s Certificate of Honour, Chunchasri Prasasti of Âdichunchanagiri Mahasamsthana and the title Mahamahopadhyaya of Rashtriya Sanskrit Vidyapeeth, Tirupati.\n\n\nVidwan N.Ranganatha Sharma died on 25 January 2014 at his residence at Kuvempunagar, Mysore at age 98.\n\n"}
{"id": "23333", "url": "https://en.wikipedia.org/wiki?curid=23333", "title": "Palimpsest", "text": "Palimpsest\n\nIn textual studies, a palimpsest () is a manuscript page, either from a scroll or a book, from which the text has been scraped or washed off so that the page can be reused for another document. Pergamene (now known as parchment) was made of lamb, calf, or goat kid skin (best made in ancient Pergamon) and was expensive and not readily available, so in the interest of economy a pergamene often was re-used by scraping the previous writing. In colloquial usage, the term \"palimpsest\" is also used in architecture, archaeology, and geomorphology to denote an object made or worked upon for one purpose and later reused for another, for example a monumental brass the reverse blank side of which has been re-engraved.\n\nThe word \"palimpsest\" derives from the Latin \"palimpsestus\", which derives from the Ancient Greek παλίμψηστος (\"palímpsēstos\", \"again scraped\"), a compound word that literally means \"scraped clean and ready to be used again\". The Ancient Greeks used wax-coated tablets, like scratch-pads, to write on with a stylus, and to erase the writing by smoothing the wax surface and writing again. This practice was adopted by Ancient Romans, who wrote (literally scratched on letters) on wax-coated tablets, which were reuseable; Cicero's use of the term \"palimpsest\" confirms such a practice.\n\nBecause parchment prepared from animal hides is far more durable than paper or papyrus, most palimpsests known to modern scholars are parchment, which rose in popularity in Western Europe after the 6th century. Where papyrus was in common use, reuse of writing media was less common because papyrus was cheaper and more expendable than costly parchment. Some papyrus palimpsests do survive, and Romans referred to this custom of washing papyrus.\n\nThe writing was washed from parchment or vellum using milk and oat bran. With the passing of time, the faint remains of the former writing would reappear enough so that scholars can discern the text (called the \"scriptio inferior\", the \"underwriting\") and decipher it. In the later Middle Ages the surface of the vellum was usually scraped away with powdered pumice, irretrievably losing the writing, hence the most valuable palimpsests are those that were overwritten in the early Middle Ages.\n\nMedieval codices are constructed in \"gathers\" which are folded (compare \"folio\", \"leaf, page\" ablative case of Latin folium), then stacked together like a newspaper and sewn together at the fold. Prepared parchment sheets retained their original central fold, so each was ordinarily cut in half, making a quarto volume of the original folio, with the overwritten text running perpendicular to the effaced text.\n\nFaint legible remains were read by eye before 20th-century techniques helped make lost texts readable. To read palimpsests, scholars of the 19th century used chemical means that were sometimes very destructive, using tincture of gall or, later, ammonium bisulfate. Modern methods of reading palimpsests using ultraviolet light and photography are less damaging.\n\nInnovative digitized images aid scholars in deciphering unreadable palimpsests. Superexposed photographs exposed in various light spectra, a technique called \"multispectral filming\", can increase the contrast of faded ink on parchment that is too indistinct to be read by eye in normal light. For example, multispectral imaging undertaken by researchers at the Rochester Institute of Technology and Johns Hopkins University recovered much of the undertext (estimated to be more than 80%) from the \"Archimedes Palimpsest\". At the Walters Art Museum where the palimpsest is now conserved, the project has focused on experimental techniques to retrieve the remaining text, some of which was obscured by overpainted icons. One of the most successful techniques for reading through the paint proved to be X-ray fluorescence imaging, through which the iron in the ink is revealed. A team of imaging scientists and scholars from the United States and Europe is currently using spectral imaging techniques developed for imaging the \"Archimedes Palimpsest\" to study more than one hundred palimpsests in the library of Saint Catherine's Monastery in the Sinai Peninsula in Egypt.\n\nA number of ancient works have survived only as palimpsests. Vellum manuscripts were over-written on purpose mostly due to the dearth or cost of the material. In the case of Greek manuscripts, the consumption of old codices for the sake of the material was so great that a synodal decree of the year 691 forbade the destruction of manuscripts of the Scriptures or the church fathers, except for imperfect or injured volumes. Such a decree put added pressure on retrieving the vellum on which secular manuscripts were written. The decline of the vellum trade with the introduction of paper exacerbated the scarcity, increasing pressure to reuse material.\n\nCultural considerations also motivated the creation of palimpsests. The demand for new texts might outstrip the availability of parchment in some centers, yet the existence of cleaned parchment that was never overwritten suggests that there was also a spiritual motivation, to sanctify pagan text by overlaying it with the word of God, somewhat as pagan sites were overlaid with Christian churches to hallow pagan ground. Or the pagan texts may have merely appeared irrelevant.\n\nTexts most susceptible to being overwritten included obsolete legal and liturgical ones, sometimes of intense interest to the historian. Early Latin translations of Scripture were rendered obsolete by Jerome's Vulgate. Texts might be in foreign languages or written in unfamiliar scripts that had become illegible over time. The codices themselves might be already damaged or incomplete. Heretical texts were dangerous to harbor—there were compelling political and religious reasons to destroy texts viewed as heresy, and to reuse the media was less wasteful than simply to burn the books.\n\nVast destruction of the broad quartos of the early centuries took place in the period which followed the fall of the Western Roman Empire, but palimpsests were also created as new texts were required during the Carolingian Renaissance. The most valuable Latin palimpsests are found in the codices which were remade from the early large folios in the 7th to the 9th centuries. It has been noticed that no entire work is generally found in any instance in the original text of a palimpsest, but that portions of many works have been taken to make up a single volume. An exception is the \"Archimedes Palimpsest\" (see below). On the whole, early medieval scribes were thus not indiscriminate in supplying themselves with material from any old volumes that happened to be at hand.\n\n\nAbout sixty palimpsest manuscripts of the Greek New Testament have survived to the present day. Uncial codices include:\n\nPorphyrianus, Vaticanus 2061 (double palimpsest), Uncial 064, 065, 066, 067, 068 (double palimpsest), 072, 078, 079, 086, 088, 093, 094, 096, 097, 098, 0103, 0104, 0116, 0120, 0130, 0132, 0133, 0135, 0208, 0209.\n\nLectionaries include:\n\n\n\n"}
{"id": "3783689", "url": "https://en.wikipedia.org/wiki?curid=3783689", "title": "Puquina language", "text": "Puquina language\n\nPuquina (or Pukina) is an extinct language once spoken by a native ethnic group in the region surrounding Lake Titicaca (Peru and Bolivia) and in the north of Chile. It is often associated with the culture that built Tiwanaku.\n\nRemnants of Puquina can be found in the Quechuan and Spanish languages spoken in the south of Peru, mainly in Arequipa, Moquegua and Tacna, as well as in Bolivia. There also seem to be remnants in the Kallawaya language, which may be a mixed language formed from Quechuan languages and Puquina. (Terrence Kaufman (1990) finds the proposal plausible.)\n\nSome theories claim that \"Qhapaq Simi\", the cryptic language of the nobility of the Inca Empire, was closely related to Puquina, and that \"Runa Simi\" (Quechuan languages) were spoken by commoners.\n\nSometimes the term \"Puquina\" is used for the Uru language, which is distinctly different.\n\n"}
{"id": "25534", "url": "https://en.wikipedia.org/wiki?curid=25534", "title": "Romanian language", "text": "Romanian language\n\nRomanian (archaically Rumanian or Roumanian; autonym: \"limba română\" , \"the Romanian language\", or \"românește\", lit. \"in Romanian\") is an Eastern Romance language spoken by approximately 24–26 million people as a native language, primarily in Romania and Moldova, and by another 4 million people as a second language. It has official status in Romania and the Republic of Moldova. In addition, it is also one of the official languages of the European Union.\n\nRomanian is a part of the Balkan-Romance group that evolved from several dialects of Vulgar Latin, which separated from the Western Romance during the 5th–8th centuries. To distinguish it within that group in comparative linguistics it is called \"Daco-Romanian\" as opposed to its closest relatives, Aromanian, Megleno-Romanian, and Istro-Romanian.\n\nRomanian is also known as \"Moldovan\" in Moldova, although the Constitutional Court ruled in 2013 that \"the official language of the republic is Romanian\".\n\nFurthermore, numerous immigrant Romanian speakers are also scattered across many other regions and countries worldwide, most notably Italy, the Iberian peninsula (both in Spain and Portugal), the German-speaking countries (Germany, Austria, Switzerland), the British Isles (both in the United Kingdom as well as in Ireland), Scandinavia (Denmark, Norway, and Sweden), North America (most notably in the United States but also in Canada), Oceania (mainly Australia and New Zealand) and South America (Mainly Brazil and Argentina).\n\nRomanian descended from the Vulgar Latin spoken in the Roman provinces of Southeastern Europe. Roman inscriptions show that Latin was primarily used to the north of the so-called Jireček Line (a hypothetical boundary between the predominantly Latin- and Greek-speaking territories of the Balkan Peninsula in the Roman Empire), but the exact territory where Proto-Romanian (or Common Romanian) developed cannot certainly be determined. Most regions where Romanian is now widely spokenBessarabia, Bukovina, Crișana, Maramureș, Moldova, and significant parts of Munteniawere not incorporated in the Roman Empire. Other regionsBanat, western Muntenia, Oltenia and Transylvaniaformed the Roman province of Dacia Traiana for about 170 years. According to the \"continuity\" theory, modern Romanian is the direct descendant of the Latin dialect of Dacia Traiana and developed primarily in the lands now forming Romania; the concurring \"immigrationist\" theory maintains that Proto-Romanian was spoken in the lands to the south of the Danube and Romanian-speakers settled in most parts of modern Romania only centuries after the fall of the Roman Empire.\n\nMost scholars agree that two major dialects developed from Common Romanian by the 10th century. Daco-Romanian (the official language of Romania and Moldova) and Istro-Romanian (a language spoken by no more than 2,000 people in Istria) descended from the northern dialect. Two other languages, Aromanian and Megleno-Romanian, developed from the southern version of Common Romanian. These two languages are now spoken in lands to the south of the Jireček Line.\n\nThe use of the denomination \"Romanian\" (\"română\") for the language and use of the demonym \"Romanians\" (\"Români\") for speakers of this language predates the foundation of the modern Romanian state. Although the followers of the former Romanian voievodships used to designate themselves as \"Ardeleni\" (or \"Ungureni\"), \"Moldoveni\" or \"Munteni\", the name of \"rumână\" or \"rumâniască\" for the Romanian language itself is attested earlier, during the 16th century, by various foreign travellers into the Carpathian Romance-speaking space, as well as in other historical documents written in Romanian at that time such as (\"The Chronicles of the land of Moldova\") by Grigore Ureche.\n\nIn 1534, Tranquillo Andronico notes: \"Valachi nunc se Romanos vocant\" (\"The Wallachians are now calling themselves Romans\"). Francesco della Valle writes in 1532 that Romanians \"are calling themselves Romans in their own language\", and he subsequently quotes the expression: \"Știi Românește?\" (\"Do you know Romanian?\").\n\nAfter travelling through Wallachia, Moldavia and Transylvania Ferrante Capecci accounts in 1575 that the indigenous population of these regions call themselves \"românești\" (\"romanesci\").\n\nPierre Lescalopier writes in 1574 that those who live in Moldavia, Wallachia and the vast part of Transylvania, \"\"se consideră adevărați urmași ai romanilor și-și numesc limba \"românește\", adică romana\"\" (\"they consider themselves as the descendants of the Romans and they name their language Romanian\").\n\nThe Transylvanian Saxon Johann Lebel writes in 1542 that \"\"Vlachi\" se numeau între ei \"Romuini\"\" and the Polish chronicler Stanislaw Orzechowski (Orichovius) notes in 1554 that \"în limba lor \"walachii\" se numesc \"romini\"\" (\"In their language the Wallachians call themselves Romini\").\n\nThe Croatian prelate and diplomat Antun Vrančić recorded in 1570 that \"\"Vlachs in Transylvania, Moldavia and Wallachia designate themselves as \"Romans\" and the Transylvanian Hungarian Martin Szentiványi in 1699 quotes the following: \"«Si noi sentem Rumeni»\" (\"Și noi suntem români\" – \"We are Romans as well\") and \"«Noi sentem di sange Rumena»\" (\"Noi suntem de sânge român\" – \"We are of Roman blood\").\n\nIn (1582) stands written \".[...] că văzum cum toate limbile au și înfluresc întru cuvintele slăvite a lui Dumnezeu numai noi românii pre limbă nu avem. Pentru aceia cu mare muncă scoasem de limba jidovească si grecească si srâbească pre limba românească 5 cărți ale lui Moisi prorocul si patru cărți și le dăruim voo frați rumâni și le-au scris în cheltuială multă... și le-au dăruit voo fraților români... și le-au scris voo fraților români\" and in Letopisețul Țării Moldovei written by the Moldavian chronicler Grigore Ureche we can read: \"«În Țara Ardialului nu lăcuiesc numai unguri, ce și sași peste seamă de mulți și români peste tot locul...»\" (\"In Transylvania there live not solely Hungarians or Saxons, but overwhelmingly many Romanians everywhere around.\").\n\nNevertheless, the oldest extant document written in Romanian remains Neacșu's letter (1521) and was written using Cyrillic letters (which remained in use up until the late 19th century). There are no records of any other documents written in Romanian from before 1521.\n\nMiron Costin, in his \"De neamul moldovenilor\" (1687), while noting that Moldavians, Wallachians, and the Romanians living in the Kingdom of Hungary have the same origin, says that although people of Moldavia call themselves \"Moldavians\", they name their language \"Romanian\" (\"românește\") instead of \"Moldavian\" (\"moldovenește\").\n\nDimitrie Cantemir, in his \"\" (Berlin, 1714), points out that the inhabitants of Moldavia, Wallachia and Transylvania spoke the same language. He notes, however, some differences in accent and vocabulary.\nCantemir's work provides one of the earliest histories of the language, in which he notes, like Ureche before him, the evolution from Latin and notices the Greek and Polish borrowings. Additionally, he introduces the idea that some words must have had Dacian roots. Cantemir also notes that while the idea of a Latin origin of the language was prevalent in his time, other scholars considered it to have derived from Italian.\n\nThe slow process of Romanian establishing itself as an official language, used in the public sphere, in literature and ecclesiastically, began in the late 15th century and ended in the early decades of the 18th century, by which time Romanian had begun to be regularly used by the Church. The oldest Romanian texts of a literary nature are religious manuscripts (\"Codicele Voroneţean\", \"Psaltirea Scheiană\"), translations of essential Christian texts. These are considered either propagandistic results of confessional rivalries, for instance between Lutheranism and Calvinism, or as initiatives by Romanian monks stationed at Peri Monastery in Maramureş to distance themselves from the influence of the Mukacheve eparchy in Ukraine.\n\nThe language remains poorly attested during the Early Modern period.\n\nThe first Romanian grammar was published in Vienna in 1780.\nFollowing the annexation of Bessarabia by Russia (after 1812), Moldavian was established as an official language in the governmental institutions of Bessarabia, used along with Russian,\nThe publishing works established by Archbishop Gavril Bănulescu-Bodoni were able to produce books and liturgical works in Moldavian between 1815–1820.\n\nThe linguistic situation in Bessarabia from 1812 to 1918 was the gradual development of bilingualism. Russian continued to develop as the official language of privilege, whereas Romanian remained the principal vernacular.\n\nThe period from 1905 to 1917 was one of increasing linguistic conflict, with the re-awakening of Romanian national consciousness. In 1905 and 1906, the Bessarabian \"zemstva\" asked for the re-introduction of Romanian in schools as a \"compulsory language\", and the \"liberty to teach in the mother language (Romanian language)\". At the same time, Romanian-language newspapers and journals began to appear, such as \"Basarabia\" (1906), \"Viața Basarabiei\" (1907), \"Moldovanul\" (1907), \"Luminătorul\" (1908), \"Cuvînt moldovenesc\" (1913), \"Glasul Basarabiei\" (1913). From 1913, the synod permitted that \"the churches in Bessarabia use the Romanian language\".\nRomanian finally became the official language with the Constitution of 1923.\n\nRomanian has preserved a part of the Latin declension, but whereas Latin had six cases, from a morphological viewpoint, Romanian has only five: the nominative, accusative, genitive, dative, and marginally the vocative. Romanian nouns also preserve the neuter gender, although instead of functioning as a separate gender with its own forms in adjectives, the Romanian neuter became a mixture of masculine and feminine. The verb morphology of Romanian has shown the same move towards a compound perfect and future tense as the other Romance languages. Compared with the other Romance languages, during its evolution, Romanian simplified the original Latin tense system in extreme ways, in particular the absence of sequence of tenses.\n\nRomanian is spoken mostly in Central and the Balkan region of Southern Europe, although speakers of the language can be found all over the world, mostly due to emigration of Romanian nationals and the return of immigrants to Romania back to their original countries. Romanian speakers account for 0.5% of the world's population, and 4% of the Romance-speaking population of the world.\n\nRomanian is the single official and national language in Romania and Moldova, although it shares the official status at regional level with other languages in the Moldovan autonomies of Gagauzia and Transnistria. Romanian is also an official language of the Autonomous Province of Vojvodina in Serbia along with five other languages. Romanian minorities are encountered in Serbia (Timok Valley), Ukraine (Chernivtsi and Odessa oblasts), and Hungary (Gyula). Large immigrant communities are found in Italy, Spain, France, and Portugal.\n\nIn 1995, the largest Romanian-speaking community in the Middle East was found in Israel, where Romanian was spoken by 5% of the population. Romanian is also spoken as a second language by people from Arabic-speaking countries who have studied in Romania. It is estimated that almost half a million Middle Eastern Arabs studied in Romania during the 1980s. Small Romanian-speaking communities are to be found in Kazakhstan and Russia. Romanian is also spoken within communities of Romanian and Moldovan immigrants in the United States, Canada and Australia, although they do not make up a large homogeneous community statewide.\n\nAccording to the Constitution of Romania of 1991, as revised in 2003, Romanian is the official language of the Republic.\n\nRomania mandates the use of Romanian in official government publications, public education and legal contracts. Advertisements as well as other public messages must bear a translation of foreign words, while trade signs and logos shall be written predominantly in Romanian.\n\nThe Romanian Language Institute (\"Institutul Limbii Române\"), established by the Ministry of Education of Romania, promotes Romanian and supports people willing to study the language, working together with the Ministry of Foreign Affairs' Department for Romanians Abroad.\n\nRomanian is the official language of the Republic of Moldova. The 1991 Declaration of Independence names the official language Romanian. The Constitution of Moldova names the state language of the country Moldovan. In December 2013, a decision of the Constitutional Court of Moldova ruled that the Declaration of Independence takes precedence over the Constitution and the state language should be called Romanian.\n\nScholars agree that Moldovan and Romanian are the same language, with the glottonym \"Moldovan\" used in certain political contexts. It has been the sole official language since the adoption of the Law on State Language of the Moldavian SSR in 1989. This law mandates the use of Moldovan in all the political, economical, cultural and social spheres, as well as asserting the existence of a \"linguistic Moldo-Romanian identity\". It is also used in schools, mass media, education and in the colloquial speech and writing. Outside the political arena the language is most often called \"Romanian\". In the breakaway territory of Transnistria, it is co-official with Ukrainian and Russian.\n\nIn the 2014 census, out of the 2,804,801 people living in Moldova, 24% (652,394) stated Romanian as their most common language, whereas 56% stated Moldovan. While in the urban centers speakers are split evenly between the two names (with the capital Chișinău showing a strong preference for the name \"Romanian\", i.e. 3:2), in the countryside hardly a quarter of Romanian/Moldovan speakers indicated Romanian as their native language. It should be noted that unofficial results of this census first showed a stronger preference for the name Romanian, however the initial reports were later dismissed by the Institute for Statistics, which led to speculations in the media regarding the forgery of the census results.\n\nThe Constitution of the Republic of Serbia determines that in the regions of the Republic of Serbia inhabited by national minorities, their own languages and scripts shall be officially used as well, in the manner established by law.\n\nThe Statute of the Autonomous Province of Vojvodina determines that, together with the Serbian language and the Cyrillic script, and the Latin script as stipulated by the law, the Croat, Hungarian, Slovak, Romanian and Rusyn languages and their scripts, as well as languages and scripts of other nationalities, shall simultaneously be officially used in the work of the bodies of the Autonomous Province of Vojvodina, in the manner established by the law. The bodies of the Autonomous Province of Vojvodina are: the Assembly, the Executive Council and the Provincial administrative bodies.\nThe Romanian language and script are officially used in eight municipalities: Alibunar, Biserica Albă (), Zitiște (Žitište), Zrenianin (Zrenjanin), Kovăcița (Kovačica), Cuvin (Kovin), Plandiște (Plandište) and Sečanj. In the municipality of Vârșeț (Vršac), Romanian is official only in the villages of Voivodinț (Vojvodinci), Marcovăț (Markovac), Straja (Straža), Jamu Mic (Mali Žam), Srediștea Mică (Malo Središte), Mesici (Mesić), Jablanka, Sălcița (Salčica), Râtișor (Ritiševo), Oreșaț (Orašac) and Coștei (Kuštilj).\n\nIn the 2002 Census, the last carried out in Serbia, 1.5% of Vojvodinians stated Romanian as their native language.\n\nIn parts of Ukraine where Romanians constitute a significant share of the local population (districts in Chernivtsi, Odessa and Zakarpattia oblasts) Romanian is taught in schools as a primary language and there are Romanian-language newspapers, TV, and radio broadcasting.\nThe University of Chernivtsi in western Ukraine trains teachers for Romanian schools in the fields of Romanian philology, mathematics and physics.\n\nIn Hertsa Raion of Ukraine as well as in other villages of Chernivtsi Oblast and Zakarpattia Oblast, Romanian has been declared a \"regional language\" alongside Ukrainian as per the 2012 legislation on languages in Ukraine.\n\nRomanian is an official or administrative language in various communities and organisations, such as the Latin Union and the European Union. Romanian is also one of the five languages in which religious services are performed in the autonomous monastic state of Mount Athos, spoken in the monk communities of Prodromos and Lacu. In the unrecognised state of Transnistria, Moldovan is one of the official languages. However, unlike all other dialects of Romanian, this variety of Moldovan is written in Cyrillic Script.\nRomanian is taught in some areas that have Romanian minority communities, such as Vojvodina in Serbia, Bulgaria, Ukraine and Hungary. The Romanian Cultural Institute (ICR) has since 1992 organised summer courses in Romanian for language teachers. There are also non-Romanians who study Romanian as a foreign language, for example the Nicolae Bălcescu High-school in Gyula, Hungary.\n\nRomanian is taught as a foreign language in tertiary institutions, mostly in European countries such as Germany, France and Italy, and the Netherlands, as well as in the United States. Overall, it is taught as a foreign language in 43 countries around the world.\n\nRomanian has become popular in other countries through movies and songs performed in the Romanian language. Examples of Romanian acts that had a great success in non-Romanophone countries are the bands O-Zone (with their No. 1 single \"Dragostea Din Tei/Numa Numa\" across the world in 2003–2004), Akcent (popular in the Netherlands, Poland and other European countries), Activ (successful in some Eastern European countries), DJ Project (popular as clubbing music) SunStroke Project (known by viral video \"Epic sax guy\") and Alexandra Stan (worldwide no.1 hit with \"Mr. Saxobeat)\" and Inna as well as high-rated movies like \"4 Months, 3 Weeks and 2 Days\", \"The Death of Mr. Lazarescu\", \"\" or \"California Dreamin'\" (all of them with awards at the Cannes Film Festival).\n\nAlso some artists wrote songs dedicated to the Romanian language. The multiplatinum pop trio O-Zone (originally from Moldova) released a song called \"Nu mă las de limba noastră\" (\"I won't forsake our language\"). The final verse of this song, \"Eu nu mă las de limba noastră, de limba noastră cea română\" is translated in English as \"I won't forsake our language, our Romanian language\". Also, the Moldovan musicians Doina and Ion Aldea Teodorovici performed a song called \"The Romanian language\".\n\nThe term \"Romanian\" is sometimes used also in a more general sense, encompassing four varieties: (Daco-)Romanian, Aromanian, Megleno-Romanian, and Istro-Romanian. When the term \"Romanian\" is used in this larger sense, the term \"Daco-Romanian\" is used for Romanian proper. The origin of the term \"Daco-Romanian\" can be traced back to the first printed book of Romanian grammar in 1780, by Samuil Micu and Gheorghe Șincai. There, the Romanian dialect spoken north of the Danube is called \"lingua Daco-Romana\" to emphasize its origin and its area of use, which includes the former Roman province of Dacia, although it is spoken also south of the Danube, in Dobrudja, Central Serbia and northern Bulgaria.\n\nThis article deals with the Romanian (i.e. Daco-Romanian) language, and thus only its dialectal variations are discussed here. The differences between the regional varieties are small, limited to regular phonetic changes, few grammar aspects, and lexical particularities. There is a single written standard (literary) Romanian language used by all speakers, regardless of region. Like most natural languages, Romanian dialects are part of a dialect continuum. The dialects of Romanian are also referred to as \"subdialects\" (see reasons for this terminology) and are distinguished primarily by phonetic differences. Romanians themselves speak of the differences as \"accents\" or \"speeches\" (in Romanian: \"accent\" or \"grai\").\n\nDepending on the criteria used for classifying these dialects, fewer or more are found, ranging from 2 to 20, although the most widespread approaches give a number of five dialects. These are grouped into two main types, southern and northern, further divided as follows:\n\nOver the last century, however, regional accents have been weakened due to mass communication and greater mobility.\n\nRomanian is a Romance language, belonging to the Italic branch of the Indo-European language family, having much in common with languages such as French, Italian, Spanish and Portuguese.\n\nHowever, the languages closest to Romanian are the other Eastern Romance languages, spoken south of the Danube: Aromanian, Megleno-Romanian and Istro-Romanian. An alternative name for Romanian used by linguists to disambiguate with the other Eastern Romance languages is \"Daco-Romanian\", referring to the area where it is spoken (which corresponds roughly to the onetime Roman province of Dacia).\n\nCompared with the other Romance languages, the closest relative of Romanian is Italian; the two languages show a limited degree of asymmetrical mutual intelligibility, especially in their cultivated forms: speakers of Romanian seem to understand Italian more easily than the other way around. Romanian has obvious grammatical and lexical similarities with French, Catalan, Spanish and Portuguese, with a high phonological similarity with Portuguese in particular; however, it is not mutually intelligible with them to any practical extent. Romanian speakers will usually need some formal study of basic grammar and vocabulary before being able to understand more than individual words and simple sentences in other Romance languages. The same is true for speakers of these languages trying to understand Romanian. Because of its separation from the other Romance languages, it has diverged from them and is an outlier in various ways, somewhat like English in regards to the other Germanic languages.\n\nRomanian has had a greater share of foreign influence than some other Romance languages such as Italian in terms of vocabulary and other aspects. A study conducted by Mario Pei in 1949 which analyzed the degree of differentiation of languages from their parental language (in the case of Romance languages to Latin comparing phonology, inflection, discourse, syntax, vocabulary, and intonation) produced the following percentages (the higher the percentage, the greater the distance from Latin):\n\n\nThe lexical similarity of Romanian with Italian has been estimated at 77%, followed by French at 75%, Sardinian 74%, Catalan 73%, Portuguese and Rhaeto-Romance 72%, Spanish 71%.\n\nThe Romanian vocabulary became predominantly influenced by French and, to a lesser extent, Italian in the nineteenth and early twentieth centuries.\n\nThe Dacian language was an Indo-European language spoken by the ancient Dacians, mostly north of the Danube river but also in Moesia and other regions south of the Danube. It may have been the first language to influence the Latin spoken in Dacia, but little is known about it. Dacian is usually considered to have been a northern branch of the Thracian language, and, like Thracian, Dacian was a satem language. \n\nAbout 300 words found only in Romanian or with a cognate in the Albanian language may be inherited from Dacian (for example: \"barză\" \"stork\", \"balaur\" \"dragon\", \"mal\" \"shore\", \"brânză\" \"cheese\"). Some of these possibly Dacian words are related to pastoral life (for example, \"brânză\" \"cheese\"). Some linguists and historians have asserted that Albanians are Dacians who were not Romanized and migrated southward.\n\nA different view is that these non-Latin words with Albanian cognates are not necessarily Dacian, but rather were brought into the territory that is modern Romania by Romance-speaking shepherds migrating north from Albania, Serbia, and northern Greece who became the Romanian people.\n\nWhile most of Romanian grammar and morphology are based on Latin, there are some features that are shared only with other languages of the Balkans and not found in other Romance languages. The shared features of Romanian and the other languages of the Balkan language area (Bulgarian, Macedonian, Albanian, Greek, and Serbian) include a suffixed definite article, the syncretism of genitive and dative case and the formation of the future and the alternation of infinitive with subjunctive constructions. According to a well-established scholarly theory, most Balkanisms could be traced back to the development of the Balkan Romance languages; these features were adopted by other languages due to language shift.\n\nThe Slavic influences on Romanian are especially noticeable at lexis, phonetics, morphology and syntax. About 20% of modern Romanian words are presently of Slavic origin. Based on 5765-8400 words, Moldavian linguist Alexandru Cihac (1825-1887) and later Tinktin (1895-1925) estimated that the Romanian vocabulary consists of the following origins: 41-45% Slavic, 20-31% Latin, 17-8% Turkish, 11-8% Modern Greek, 10-6% Hungarian, 1% Albanian, but not all etymologies were found to be correct in Cihac's overestimate and many words missing, later Ion A. Rădulescu-Pogoneanu agreed that the greater part of Romanian vocabulary was Slavic, whereas M. Gaster and R. F. Arnold claimed the Latin vocabulary outnumbers the Slavic, so in 1941 the estimated Slavic words were reduced to 16%.<ref name=\"fischer\">\n"}
{"id": "30991284", "url": "https://en.wikipedia.org/wiki?curid=30991284", "title": "Saudi Sign Language", "text": "Saudi Sign Language\n\nSaudi Sign Language is the deaf sign language of Saudi Arabia.\n\nWittmann (1991) posits that SSL is a language isolate (a 'prototype' sign language), though one developed through stimulus diffusion from an existing sign language.\n\n"}
{"id": "473856", "url": "https://en.wikipedia.org/wiki?curid=473856", "title": "Seal script", "text": "Seal script\n\nSeal script () is an ancient style of writing Chinese characters that was common throughout the latter half of the 1st millennium BCE. It evolved organically out of the Zhou dynasty script. The Qin variant of seal script eventually became the standard, and was adopted as the formal script for all of China during the Qin dynasty. It was still widely used for decorative engraving and seals (name chops, or signets) in the Han dynasty. The literal translation of the Chinese name for seal script, (\"\"), is \"decorative engraving script\", a name coined during the Han dynasty, which reflects the then-reduced role of the script for the writing of ceremonial inscriptions.\n\nThe general term seal script can be used to refer to several types of seal script, including the Large or Great Seal script ( '; Japanese '; Korean '; Vietnamese ') and the lesser or Small Seal Script ( '; Japanese '; Korean '; Vietnamese '). Most commonly, without any other clarifying terminology, it refers to the latter of these. The term \"Large Seal script\" itself can also cover a broad variety of scripts, including a variation of Qin writing earlier than the small seal characters, but also the earlier Western Zhou forms, or even oracle bone characters as well. Since the term is an imprecise one, not clearly referring to any specific historical script and not used with any consensus in meaning, modern scholars tend to avoid it, and when referring to \"seal script\", generally mean the (small) seal script of the Qin system, that is, the lineage which evolved in the state of Qin during the Spring and Autumn and Warring States period and which was standardized under the First Emperor.\n\nThere were several different variants of seal script which developed independently in each kingdom during the Spring and Autumn and Warring States periods. One of these, the \"bird-worm\" seal script (), is named for its intricate decorations on the defining strokes, and was used in the Kingdoms of Wu, Chu, and Yue. It was found on several artifacts including the Spear of Fuchai and the Sword of Goujian. This seal script variant is very difficult to read.\n\nAs a southern state, Chu was close to the Wu-Yue influences. Chu produced broad bronze swords that were similar to Wuyue swords, but not as intricate. Chu also used the bird-worm style, which was borrowed by the Wu and Yue states.\n\nThe script of the Qin system (the writing as exemplified in bronze inscriptions in the state of Qin before unification) had evolved organically from the Zhou script starting in the Spring and Autumn period. Beginning around the Warring States period, it became vertically elongated with a regular appearance. This was the period of maturation of Small Seal script. It was systematized by prime minister Li Si during the reign of the First Emperor of China Qin Shi Huang through elimination of most variant structures, and was imposed as the nationwide standard. Through Chinese commentaries, it is known that Li Si compiled the \"Cangjiepian\", a partially-extant wordbook listing some 3,300 Chinese characters in small seal script. Their form is characterized by being less rectangular and more squarish.\n\nIn the popular history of Chinese characters, the Small Seal script is traditionally considered to be the ancestor of the clerical script, which in turn gave rise to all of the other scripts in use today. However, recent archaeological discoveries and scholarship have led some scholars to conclude that the direct ancestor of clerical script was proto-clerical script, which in turn evolved out of the little-known \"vulgar\" or \"popular\" writing of the late Warring States to Qin period.\n\nThe first known character dictionary was the 3rd century BC \"Erya\", collated and bibliographed by Liu Xiang and his son Liu Xin. It is no longer extant. Not long after, the \"Shuowen Jiezi\" (AD 100–121), the lifework of Xu Shen, was written. Its 9,353 entries reproduce the standardized small-seal script variant for each entry, and for some entries other pre-Han variants from the late Zhou era. Entries are categorized under 540 section headers.\n\nIt has been anticipated that the small seal script will some day be encoded in Unicode. Codepoints U+31400 to U+33D1F (Plane 3, Tertiary Ideographic Plane) have been tentatively allocated.\n\n\n\n"}
{"id": "44856373", "url": "https://en.wikipedia.org/wiki?curid=44856373", "title": "Southern Pashto", "text": "Southern Pashto\n\nKandahārī Pashtō (), also known as Southern Pashto, Western Pashto, or Southwestern Pashto, is a Pashto dialect, spoken in southern and western Afghanistan, including the city of Kandahar. It is similar with Quetta (Kakar) and Khattak dialects (95–100% similarities in vocabulary, and 80% with Yusufzai dialect). It is one of the most archaic and most prestigious varieties of Pashto.\n\nKandahari Pashto is spoken in Kandahar, Helmand, Ghazni, most of Urozgan, Farah, and Nimruz, southeastern Ghor, the districts of Murghab, Ghormach, Muqur, and Jawand in Badghis, and parts of Zabul, Paktika, and Herat provinces of Afghanistan. It is also spoken in parts of the provinces of Razavi Khorasan and South Khorasan in Iran, where they numbered roughly 120,000 (in 1993). Kandahari Pashto is almost same as the southeastern Pashto dialect, spoken in northern Balochistan including Quetta city, except for few phonetic differences.\n\nThe Kandahari dialect retains archaic retroflex sibilants, and (in other dialects, they have shifted to ʃ/x and ʒ/g). In the Quetta and Kakar dialects they change to ʃ and ʒ. Kandahari also has the affricates and .\nPronouns are also archaic. It is differences in 'he' and 'she' pronouns. In all 3rd-person pronouns 'h' is articulated.\n"}
{"id": "9325486", "url": "https://en.wikipedia.org/wiki?curid=9325486", "title": "Stan Ageira", "text": "Stan Ageira\n\nStan Ageira (born 3 May 1961) is a writer from Mulki, a town near Mangalore, Karnataka, India.\n\nAgeira is well known in the field of Konkani literature. His first short story was published in 1975. when he was barely 14. To date he has written over 250 short stories and 18 novels in Konkani.\n\nHis father, the late Sgt. Ligoury Ageira, served the Indian Air Force for 35 years. Ageira was born and brought up in Mulki with his primary education was completed in Bethany convent, high school from Government Junior College, B.Com from Mysore University and MBA from IBAM New Delhi. He began his professional career in sales with Godrej Soaps Mumbai Branch in the early eighties before moving to Dubai in the 1990. Presently he is working and living in Dubai with his wife, Yvonne, and children, Sean and Elton.\n\nHe has been recognized and presented awards from Goa Konkani Bhasha Mandal, Konkani Kutaam Bahrain-2011, Daiji-Dubai 2014 Literary Award and Konkani Sahitya Academy, Karnataka for his novels. His novel \"Hi Moji Dhavnim\" won an AIKWO Award in 2014. He has published work to his credit in Konkani, Kannada and English. His Konkani column \"Bindaas\" in daijiworld.com is popular with Konkani readers. His collection of short stories in Konkani, \"Tambdi Mirsaang\", published in 2006, was selected for the Karnataka Konkani Sahitya Academy Award. His short stories have been published in magazines like \"Woman's Era\" and \"Alive\", published from New Delhi. He was awarded as the Konkani Story writer of 2008 by daaiz.com, a web portal which promotes Konkani literature. His short stories in English are also available in www.daijiworld.com's specially dedicated column under \"Red Chillis.\"\n\nShort stories by Stan Ageira in English:\n\n\n"}
{"id": "55703898", "url": "https://en.wikipedia.org/wiki?curid=55703898", "title": "Text nailing", "text": "Text nailing\n\nText Nailing (TN) is an information extraction method of semi-automatically extracting structured information from unstructured documents. TN was developed at Massachusetts General Hospital and was tested in multiple scenarios including the extraction of smoking status, family history of coronary artery disease, classify patients with sleep disorders, improve the accuracy of the Framingham risk score for patients with non-alcoholic fatty liver disease, and classify non-adherence to type-2 diabetes. A comprehensive review regarding extracting information from textual documents in the electronic health record is available..\n\nTN combines two concepts: 1) human-interaction with narrative text to identify highly prevalent non-negated expressions, and 2) conversion of all expressions and notes into non-negated alphabetical-only representations to create homogeneous representations. The importance of using non-negated expressions to achieve an increased accuracy of text-based classifiers was emphasized in a letter published in Communications of the ACM in October 2018.\n\nIn traditional machine learning approaches for text classification, a human expert is required to label phrases or entire notes, and then a supervised learning algorithm attempts to generalize the associations and apply them to new data. In contrast, using non-negated distinct expressions eliminates the need for an additional computational method to achieve generalizability.\n\nA sample code for extracting smoking status from narrative notes using \"nailed expressions\" is available in GitHub.\n\nIn July 2018 researchers from Virginia Tech and University of Illinois at Urbana-Champaign referred TN as an example for progressive cyber-human intelligence (PCHI).\n\nChen & Asch 2017 wrote \"With machine learning situated at the peak of inflated expectations, we can soften a subsequent crash into a “trough of disillusionment” by fostering a stronger appreciation of the technology’s capabilities and limitations.\"\n\nA letter published in Communications of the ACM, \"Beyond brute force\", emphasized that a brute force approach may perform better than traditional machine learning algorithms when applied to text. The letter stated \"... machine learning algorithms, when applied to text, rely on the assumption that any language includes an infinite number of possible expressions. In contrast, across a variety of medical conditions, we observed that clinicians tend to use the same expressions to describe patients' conditions.\"\n\nIn his viewpoint published in June 2018 concerning slow adoption of data-driven findings in medicine, Uri Kartoun, co-creator of Text Nailing states that \" ...Text Nailing raised skepticism in reviewers of medical informatics journals who claimed that it relies on simple tricks to simplify the text, and leans heavily on human annotation. TN indeed may seem just like a trick of the light at ﬁrst glance, but it is actually a fairly sophisticated method that ﬁnally caught the attention of more adventurous reviewers and editors who ultimately accepted it for publication.\"\n\nThe human in-the-loop process is a way to generate features using domain experts. Using domain experts to come up with features is not a novel concept. However, the specific interfaces and method which helps the domain experts create the features are most likely novel.\n\nIn this case the features the experts create are equivalent to regular expressions. Removing non-alphabetical characters and matching on \"smokesppd\" is equal to the regular expression /smokes[^a-zA-Z]*ppd/. Using regular expressions as features for text classification is not novel.\n\nGiven these features the classifier is a manually set threshold by the authors, decided by the performance on a set of documents. This is a classifier, it's just that the parameters of the classifier, in this case a threshold, is set manually. Given the same features and documents almost any machine learning algorithm should be able to find the same threshold or (more likely) a better one.\n\nThe authors note that using support vector machines (SVM) and hundreds of documents give inferior performance, but does not specify which features or documents the SVM was trained/tested on. A fair comparison would use the same features and document sets as those used by the manual threshold classifier.\n"}
{"id": "53107470", "url": "https://en.wikipedia.org/wiki?curid=53107470", "title": "Tirukkural translations into Latin", "text": "Tirukkural translations into Latin\n\nLatin is the first foreign language into which the Tirukkural was translated. There are three known translations of the Kural text available in Latin.\n\nThe Christian missionaries who arrived in India during the British era admired the Kural text greatly owing to the moral values found in the work. In 1730, Constantius Joseph Beschi of the Society of Jesus (1700-1742) translated it into Latin, introducing the work to the Europeans for the first time. Beschi, however, translated only the first two parts, namely, virtue and wealth, since he considered translating the section on love inappropriate for a Christian missionary. This manuscript, now found in the India Office Library, London, was edited by George Uglow Pope, who published it as 'notes' at the end of his famous English translation titled \"The Sacred Kurral\" in 1886. This saved the manuscript from having been lost in the oblivion. There are at least two more Latin translations of the Kural text available either in full or in parts. In 1856, Dr. Karl Graul published the Kural text in German, with later translated it into Latin and simple Tamil in 1865. In 1865, an unknown author made the third Latin translation of the Kural text titled \"Tiruvalluvar Kural Versione Lationa\".\n\nModern scholars have noticed several mistranslations in the Latin translation by Father Beshi. According to V. Ramasamy, \"Beschi is purposely distorting the message of the original when he renders பிறவாழி as ‘the sea of miserable life’ and the phrase பிறவிப்பெருங்கடல் as ‘sea of this birth’ which has been translated by others as ‘the sea of many births’. Beschi means thus ‘those who swim the vast sea of miseries’. The concept of rebirth or many births for the same soul is contrary to Christian principle and belief\".\n\n\n\n"}
{"id": "19202154", "url": "https://en.wikipedia.org/wiki?curid=19202154", "title": "Touo language", "text": "Touo language\n\nThe Touo language is spoken over the southern part of Rendova Island, located in the Western Province of the Solomon Islands.\n\nTouo belongs to the Central Solomons group of the Papuan languages. All the surrounding languages to Touo belong to the Oceanic subgroup of the Austronesian language family.\n\nThe Touo language is sometimes called the \"Baniata\" or \"Lokuru\" language, after the largest two villages where the language is spoken. The word \"Touo\" comes from the ethnonym that Touo speakers use to refer to themselves.\n\n\n\n"}
{"id": "31971126", "url": "https://en.wikipedia.org/wiki?curid=31971126", "title": "Uaithne Ó Cobhthaigh", "text": "Uaithne Ó Cobhthaigh\n\nUaithne Ó Cobhthaigh (murdered 1556) was an Irish poet.\n\nÓ Cobhthaigh was a member of a hereditary bardic family based in what is now County Westmeath. His father's name was William.\n\nAccording to the Annals of the Four Masters, in 1556 \"Owny, the son of William O'Coffey, the most learned in Ireland in poetry, was treacherously slain at night, at Baile-an-luig in Magh-bhachla, but it is not known by whom.\" The \"Oxford Dictionary of National Biography\" states that \"he was murdered, with his wife, at Ballinlig, Westmeath.\"\n\nAt least two of his poems are extant: \"Mó ná iarla anim Shémais\"/\"Greater than an earl is the name of James\", and the theological poem, \"Fada an cuimhne so ar choir nDe\"/\"Long be this rememberence on the justice of God\", which consists of one hundred and sixty verses.\n\n"}
{"id": "6251316", "url": "https://en.wikipedia.org/wiki?curid=6251316", "title": "Vital Speeches of the Day", "text": "Vital Speeches of the Day\n\nVital Speeches of the Day is a monthly magazine that presents speeches and other public addresses in full.\n\n\"Vital Speeches\" was established in New York City in 1934 by Thomas Daly — whose grandson Thomas Daly III moved publication to South Carolina in 1986 — and is published by Pro Rhetoric, LLC.\n\nThe magazine first appeared in February, 1935; its first issue included speeches by then-President Franklin Delano Roosevelt, Nobel Peace Prize winner Nicholas Murray Butler, David Lawrence, legal expert Ferdinand Pecora, and economist and eugenicist Irving Fisher. As of 1995, the magazine had published speeches by every president since Roosevelt, although the publication avoided campaign speeches. Its editor, Thomas Daly IV, said of such speeches, \"A lot of that is hot air.\"\n\nAccording to their policy statement:\n\nThe periodical is included in various guides to reference works. These guides typically describe \"Vital Speeches\" in politically neutral terms, as when \"Guide to Reference Materials\" offered this summary: \"Each semimonthly issue contains the full text of some 12 to 15 addresses on public issues delivered by important figures. The editors attempt to select speeches pertaining to all sides of controversial issues.\" However, descriptions of specialty collections of speeches often compare the collection favorably to the content of \"Vital Speeches\".\n\nIn January 2009, the magazine launched \"Vital Speeches International\" which compiles English-language texts from outside the United States.\n"}
{"id": "55562707", "url": "https://en.wikipedia.org/wiki?curid=55562707", "title": "Vulnerable area", "text": "Vulnerable area\n\nVulnerable area () is a term applied by police in Sweden to areas with high crime rates and social exclusion. In the December 2015 report, these areas numbered 53. In the June 2017 report, these totalled 61. The increase is reported to be due to better reporting, not a changing situation. The overall trend is that these areas are improving.\n\nAll the areas are situated south of the town of Gävle and most are areas constructed during the Million Programme (MP), although there are towns north of Gävle having MP areas they do not experience the crime rate of some southern MP areas.\n\nA vulnerable area is described as being geographically defined and having a low socioeconomic status and criminals negatively affecting society. The three categories of vulnerable area are divided along level of severity: vulnerable area, risk area and especially vulnerable area.\n\nThese areas are sometimes called no-go zones as emergency services such as fire engines and ambulances cannot drive into these areas during a tense situation without a police escort as they will be attacked by criminal gangs.\n\nThe population in these areas have higher rates of unemployment, whereas about 67% of the general population has employment, the share in vulnerable areas is about 49%.\n\nAccording to a 2017 report by Swedish Defence University , of those who have travelled from Sweden to conflict zones to participate in terrorist activities, 70% were residents in vulnerable areas.\n\nAccording to a 2018 report by Swedish Television, the overall trend is that these areas are improving. Employment rates, income and school results are generally rising.\n\nVulnerable areas have a low participation in election, where for instance in the Gårdsten district in Gothenburg only a third voted in the 2014 election. Journalists who visited Gårdsten to interview locals on why they didn't vote struggled to complete their task as many locals they encountered spoke neither Swedish nor English.\n\nAccording to Japanese economist Takaaki Mitsuhashi, foreigners primarily from Africa and the Middle East have settled in districts which they then proceeded to take over. Having visited vulnerable area Husby, he described it as \"whatever it was, it wasn't Sweden\".\n\nBy 2018 gang violence, which had long been a feature of vulnerable areas, had begun to spill out into the wider society where hospital staff reported armed confrontations in emergency rooms and school authorities reported that threats and weapons having become commonplace. \n\nAn area in the vulnerable category is characterised by a low socioeconomic status and where criminals have a negative impact on society and public institutions. Criminals may use direct threats and blackmail or indirect methods such as public displays of violence which place bystanders at risk of injury or narcotics openly traded in public spaces. The effect of their activities is that inhabitants experience lower levels of security which may make them less willing to participate or witness in judicial proceedings against criminals.\n\nAn area in this category is characterised by inhabitants having a potential threat from criminals in the area which has led to an overall disinclination to participate in judicial proceedings against criminals. In especially vulnerable areas there are systematic threats and violence against witnesses and plaintiffs. These circumstances make it very difficult or impossible for police organisations to complete their mission. In December 2015 these areas numbered 15. In June 2017 these areas numbered 23 as some areas were reclassified primarily due to more refined information, not that the situation changed.\n\nAn especially vulnerable area may to a certain extent also experience\n\nEmployment levels in February 2017 was about 47%. Many inhabitants are immigrants or children of immigrants, in this category the share of immigrants in the district is around 50-60%.\n\nIn its 2017, police stated that welfare fraud was prevalent in these areas, where benefits administered by Swedish Public Employment Service and the Swedish Social Insurance Agency were targeted. Police identified that resident registry figures were manipulated, for instance 2% of all apartments in Rinkeby have between 10-30 persons registered as residents, which leads to an inflated number people receiving welfare benefits.\n\nEducation levels are lower than the general population in these areas, where 40% of the population has not completed primary education. According to BRÅ statistics, persons with only primary education (Swedish: \"förgymnasial utbildning\") are 5.7 times more likely to be registered for crime compared to persons with post secondary education (Swedish: \"eftergymnasial utbildning\") On average in 2017, less than half of 15-year-olds in Gothenburg especially vulnerable areas qualified for secondary education. In district Bergsjön, 69.8% of 15-year-olds of \"Bergsjöskolan\" left primary education without achieving grades in numeracy and literacy to enter secondary education, the number being 67.3% for the \"Sjumilaskolan\" school of Biskopsgården district in Gothenburg compared to the national average of 17.5%. The results were lower compared to five years earlier and interpreted as a worsening trend by researcher Anders Trumberg at Örebro university.\n\nA risk area fulfills all criteria for a vulnerable area but do not qualify in all the critera for an especially vulnerable area. In December 2015 these areas numbered 6.\n\nAccording to the national operations section (sv: \"Nationella operativa avdelningen\") the districts and their classification are:\n\n"}
{"id": "11636453", "url": "https://en.wikipedia.org/wiki?curid=11636453", "title": "Wiseman hypothesis", "text": "Wiseman hypothesis\n\nThe Wiseman hypothesis, sometimes called the tablet theory, is a theory of the authorship and composition of the Book of Genesis which suggests that Moses compiled Genesis from tablets handed down through Abraham and the other patriarchs. Originally advocated by P. J. Wiseman (1888–1948) in his \"New discoveries in Babylonia about Genesis\" (1936) and republished by Wiseman's son, Donald Wiseman, as \"Ancient records and the structure of Genesis: A case for literary unity\" in 1985, the hypothesis received some support from R. K. Harrison (1969) but otherwise remained without acceptance in scholarly circles.\n\nAir Commodore P. J. Wiseman, a British officer who visited many active archaeological sites during his career in the Middle East, found that ancient narrative tablets usually ended in colophons which had a very specific format consisting of three parts; 1) \"this has been the history/book/genealogy of...\", 2) the name of the person who wrote or owned the tablet, and 3) a date (such as \"in the year of the great earthquake,\" or \"the 3rd year of king so-and-so\", etc. Wiseman noted that there are eleven phrases in Genesis which have the same colophon format, which have long been identified as the \"toledoth\" (Hebrew for \"generations\") passages; the Book is generally divided thematically along the lines of the toledot. What Wiseman brought new to the table was the idea that these apparent colophons indicated that Genesis had originally been a collection of narrative clay tablets written in cuneiform, like the ancient tablets he had seen, which Moses had edited into a single document on parchment or papyrus. This is in contrast with traditional views that Moses wrote Genesis entirely on his own without any outside sources and with the Documentary hypothesis that Genesis was compiled by much later and unknown redactors.\n\nOnce a link had been made between the tolodoth in Genesis and the ancient colophons, another point became apparent. Just as the colophons came at the end of the narratives, so too, the tolodoths may come at the end of narratives. Thus the first of these toledoth passages, Genesis 2:4, refers to the preceding Creation account beginning in Genesis 1, rather than being the introduction to the succeeding account. The traditional understanding has been that since nearly all the tolodoths are immediately followed by a list of descendants of the person named in the tolodoth, then the tolodoths were thought to be the beginning of sections in Genesis.\n\nIn his \"Creation Revealed in Six Days\", P. J. Wiseman argued that the days of creation represented the time period in which God took to reveal his work of creation, and that Genesis 1 \"is an account of what 'God said' about the things 'God made'... it is His revelation to men about His creative acts in time past.\"\n\nR. K. Harrison in his \"Introduction to the Old Testament\" wrote approvingly of [Wiseman's] approach which \"had the distinct advantage of relating the ancient Mesopotamian sources underlying Genesis to an authentic Mesopotamian life-situation, unlike the attempts of the Graf–Wellhausen school, and showed that the methods of writing and compilation employed in Genesis were in essential harmony with the processes current among the scribes of ancient Babylonia.\"\n\nHarrison noted that these examples had been discounted by scholars who follow Wellhausen and the Documentary hypothesis, since the central basis of the Documentary hypothesis is that the Pentateuch is mostly a work composed by unknown editors and authors who lived much later than the time of Moses. \n\nDonald Wiseman noted in the foreword to the revised edition of his father’s book that since it had first been written (1936) many more colophons have been discovered among Babylonian cuneiform texts which substantiated the use of this scribal device. Texts from Syria and Mesopotamia show continuity in tradition of scribal education and literary practices for more than two millennia, giving fixed and dated points. He particularly valued the implication of this theory for the early use of writing. Genesis 1-37 could be a transcript of the oldest written records.\n\nThis is a breakdown of Genesis into 'tablets' delineated by colophons according to Wiseman's theory.\nBiblical scholar Victor Hamilton states that Wiseman's hypothesis was \"the first concerted attempt to challenge the hypothesis\" of introductory colophons. Hamilton does however identify several problems with what he terms the \"Wiseman-Harrison approach\". Firstly, \"in five instances where the formula precedes a genealogy ..., it is difficult not to include the colophon with what follows.\" Secondly, the approach requires the \"unlikely\" explanation that \"Ishmael was responsible for preserving the history of Abraham\", Isaac for Ishmael's history, Esau for Jacob's and Jacob for Esau's. The third problem he identifies is that Genesis is narrative, not biographical, as that approach would suggest.\n\nHerbert M. Wolf describes the theory as \"an attractive one\", but suggests that it has \"serious shortcomings\". Firstly, he suggests that \"toledoth\" almost always fit more naturally with the verses that they precede than with the verses that precede them. Secondly he doubts if Moses would be able to read writing made before the Tower of Babel. Thirdly he also suggests that the pairings of preservers and preserved histories are \"unlikely\", given the \"rivalry and jealousy\" involved and the lack of contact between Esau and Jacob.\n\"The Bible Knowledge Commentary: Old Testament\" says that Wiseman's view is \"unconvincing\" and distinguishes between the Babylonian colophons and the \"toledoth\" of Genesis, in that the colophon is a repetition, not a description of contents, the owner named is the current owner, not the original, and the colophons do not use the Akkadian equivalent of the \"toledoth\" as part of their formula.\n\n\n\n\n"}
{"id": "5876717", "url": "https://en.wikipedia.org/wiki?curid=5876717", "title": "Yuantang (language game)", "text": "Yuantang (language game)\n\nYuantang is language game spoken by Hakka speakers at Yuantang (), a village in southern China. It is also known as the \"snake language\".\n\nExample : 食饭 → 手习花散 [sit fan] → [siu jit fa san]; eat (rice) → \"hand + learn + flower + separation\".\n\nClearly, the words 食 and 饭 are each split into two sounds, the initial and the rime, thus 食 [sit] is made up of the initial of 手 [s] and the rime of 习 [it], and similarly, 饭 [fan] is [f] from 花 and [an] from 散. This is similar to the traditional Chinese practice of representing sounds by two characters known as fanqie.\n<br>\nThis practice also resembles Jin, another Sinitic language, in its process of splitting a monosyllabic word into two syllables. A similar process is also found in Mandarin.\n\nThere is no solid evidence for the original of Yuantang. But it is believed to be an invention of a local intellectual in Qing dynasty.\n"}
