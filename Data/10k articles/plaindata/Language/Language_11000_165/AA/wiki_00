{"id": "34962527", "url": "https://en.wikipedia.org/wiki?curid=34962527", "title": "Amba language (Solomon Islands)", "text": "Amba language (Solomon Islands)\n\nAmba (also known as \"Aba\", \"Nembao\" or \"Nebao\") is the main language spoken on the island of Utupua, in the easternmost province of the Solomon Islands.\n\nThe speaker population calls their own language (with prenasalised ). This name may be rendered \"Amba\" or \"Aba\" depending on spelling conventions, which have not been fixed yet for these languages.\n\nSpeakers of neighbouring Asumboa designate the Amba language as . This form, which may be spelled \"Nembao\" or \"Nebao\", has sometimes been used by foreigners as another name for the Amba language.\n"}
{"id": "169539", "url": "https://en.wikipedia.org/wiki?curid=169539", "title": "Anilox", "text": "Anilox\n\nIn printing, anilox is a method used to provide a measured amount of ink to a flexo printing plate. An anilox roll is a hard cylinder, usually constructed of a steel or aluminum core which is coated by an industrial ceramic whose surface contains millions of very fine dimples, known as cells.\nDepending on the detail of the images to be printed, the press operator will select an anilox roll with a higher or lower line screen. Low line screen rolls are used where a heavy layer of ink is desired, such as in heavy block lettering. Higher line screens produce finer details and are used in four-color process work such as reproducing photographs. Often a job will require a different line screen for each color to be printed. Experienced press operators are skilled at determining the appropriate rolls for a given print job.\nDepending on the design of the printing press, the anilox roll is either semi-submerged in the ink fountain, or comes into contact with a so-called metering roller, which is semi-submerged in the ink fountain. In either instance, a thick layer of typically viscous ink is deposited on the roller. A doctor blade is used to shave excess ink from the surface leaving just the measured amount of ink in the cells. The roll then rotates to contact with the flexographic printing plate which receives the ink from the cells for transfer to the printed material.\n\nAnilox roll selection should be based on cell volume or capacity of the engraved cells.\nPrint results are accomplished with this volume, which transfers to the printing plate and then to the substrate. In general transfer efficiency is approximately 25% to the substrate.\nCell count remains the same for the life of the roll while volume changes over time due to wear, plugging and damage.\nCell count accommodates volume at various cell shapes available from the laser engraving process.\nThe same cell count can be engraved to or wear to different volumes.\n\nContemporary ink metering systems employ a self contained system known as a chambered doctor blade system which is basically a manifold which delivers ink to the anilox roll. Ink is pumped through a hose(s) to the chamber which fills to capacity. Ink is retained within the system by end seals and doctor blades.\nOne blade works as a retaining blade, simply holding ink within the chamber. The other reverse angle blade works as previously described, removing excess ink from the engraved surface of the anilox roll.\nChambered doctor blade systems are recognized for high quality, consistency and accuracy removing the influence of press speed and some human subjectivity.\n\nAnilox rolls were originally made using a process of mechanical engraving, utilizing hardened steel tools of various cell counts and cell shapes diamond pecking machine, but modern rolls are laser engraved.\nThe characteristics of an anilox roll determine the amount of ink that will be transferred to the plate: angle of the cells, cell volume, and line screen. A 60 degree angle ensures maximum density in a given space. Lower volume makes for less ink. Low line numbers will allow for a heavy layer of ink to be printed, whereas high line numbers will permit finer detail in printing. Both cell volume and line screen are closely correlated. \n\nThe rolls are often specified by their line screen, which is the number of cells per linear inch. These often range from around 250 to upwards of 2000, though the precise numbers vary by manufacturer. Most rolls sold are within 800 LPI, although a spike in demand for those with 800–1200 lines has been seen. \nAnilox rolls are almost always designed to be removed from the press for cleaning and for swapping out with different line screen rolls.\n\nSee above: Anilox rolls are properly specified by cell volume for the required print application.\nLower volumes = thinner ink films for the highest quality process printing.\nHigher volumes address combination, screens, line work, solid coverage and coating applications.\nIn virtually all printing applications with various print requirements, one volume will not print all to the highest quality. Limited print stations generally call for a compromise between the highest quality graphics and color.\nThere are many variables to consider when specifying an anilox roll beyond the print requirements, such as; substrate, ink system, plate material, plate mounting tape, press speed, drying capacity..., etc.\nBest course of action is to consult suppliers of all these components for proper engraving specifications from their experience.\n\nThere are four ways to specify Anilox rolls settings. Based on transfer volume, based on line count, American standard and European standard.\n\nThe European standard for transfer volume is cm³/m² or cubic centimetre per square meter which means how much volume (cm³) of ink will be transferred on how much surface (m²) of paper.\n\nSometimes, ink suppliers will give a recommended transfer weight per square meter, expressed in g/m². In which case you need to know the ink density before converting g/m² to cm³/m². \n\nThe American standard for transfer volume is BCM / in² also written BCM/sq in or BCM, which means billion cubic microns per square inch. Cubic microns (μm³) is a unit of volume and one billion cubic micron (1 000 000 000 μm³) is equal to 0.001 cm³, also we know that one square meter is equal to 1550 square inch.\n\nSo 1 BCM/in² = 1.55 cm³/m²\n\nLine count will measure the finesse of the pattern on the Anilox roll. It measures how many cells are engraved per centimetre or per inch.\n\nIn Europe the standard is LPCM or lines per centimetre.\n\nIn America, the standard is LPI or lines per inch.\n\nThe conversion from transfer volume to line count isn't straightforward. Because depending on the pattern used for the cell and depth of gravure of the anilox roll, the cell might contain higher or lower volumes of ink. So the only way to convert cm³/m² (transfer volume) to LPCM (cell finesse) or BCM to LPI is to look at the conversion chart provided by the anilox roll supplier.\n\nAlso, any given anilox roll will age after some time and its actual transfer volume will be less and less.\n\nThough large wide-web flexo rolls are only maneuverable by overhead crane, on smaller presses anilox rolls are often handled directly by operators. Extreme caution must be taken when handling these pieces of hardware as a single bump against a hard surface or sharp corner can destroy the delicate cell structure on the surface and render a roller completely useless, at a cost of around 5000 for even small narrow-web rollers. Nicks and scratches add up quickly, so fine brushes (never brass brushes) are used for cleaning the anilox roll. \n\nAnilox rollers that are used with water, solvent and oil based inks, which dry when left sitting out and unagitated, must be cleaned immediately after use or a problem known as plugging occurs, where minuscule amounts of ink dry in the cells. This leaves tiny, but unacceptable, pinholes in anything printed from the roll in the future.\n\n"}
{"id": "1107192", "url": "https://en.wikipedia.org/wiki?curid=1107192", "title": "Anthon Transcript", "text": "Anthon Transcript\n\nThe \"Anthon Transcript\" (often identified with the \"Caractors document\") is a small piece of paper on which Joseph Smith wrote several lines of characters. According to Smith, these characters were from the golden plates (the ancient record from which Smith claims to have translated the Book of Mormon) and represent the reformed Egyptian writing that was on the plates. In 1828, this paper was delivered to Charles Anthon, a well-known classical scholar of Columbia College, Columbia University, for an expert opinion on the authenticity of the characters and the translation. Some adherents to the Book of Mormon claim that Anthon attested to the characters' authenticity in writing to Martin Harris but then ripped up his certification after hearing the story of Smith and the plates. Critics of Smith claim that Anthon believed any idea of reformed Egyptian was a hoax all along and that Harris was being deceived.\n\nBelievers claim that the incident between Harris and Anthon fulfilled a biblical prophecy made by Isaiah, as Anthon is reported to have said to Harris, through Smith's telling of events, \"I cannot read a sealed book.\"\n\nIn 1980, Mark Hofmann created and sold a forgery of the Anthon Transcript to leaders of The Church of Jesus Christ of Latter-day Saints (LDS Church), which was revealed to be fraudulent when Hofmann's crimes were investigated.\n\nIn 1838, Smith related an account based on Harris's version of the meeting. Smith wrote that Anthon \"stated that the translation was correct, more so than any he had before seen translated from the Egyptian. [Harris] then showed him those not yet translated, and said they were Egyptian, Chaldaic, Assyriac, and Arabic\"; and that they were \"true characters.\" According to Harris, Anthon wrote Harris a letter of authenticity declaring the fragment to contain true Egyptian characters. Anthon was also reported to have confirmed the translation of these characters as correct. When informed that an angel of God had revealed the characters to Smith, Anthon reportedly tore up the authentication stating that there was no such thing as angels and asked Harris to bring the plates to him for translation. Harris then went to Dr. Samuel L. Mitchill, who sanctioned what Anthon said.\n\nIn 1834, Anthon stated in a letter that, \"The whole story about my having pronounced the Mormonite inscription to be 'reformed Egyptian hieroglyphics' is perfectly false ... I soon came to the conclusion that it was all a trick, perhaps a hoax ... [Harris] requested an opinion from me in writing, which of course I declined giving.\" Anthon stated in the letter that the story of his supposed authentication was false, that Anthon had identified the writings as a hoax, and that he had told Harris that the writings were part of \"a scheme to cheat the farmer [Harris] of his money\".\n\nAnthon gave a second account in 1841 that contradicted his 1834 account as to whether or not he gave Harris a written opinion about the document: \"[Harris] requested me to give him my opinion in writing about the paper which he had shown to me. I did so without hesitation, partly for the man's sake, and partly to let the individual 'behind the curtain' see that his trick was discovered. The import of what I wrote was, as far as I can now recollect, simply this, that the marks in the paper appeared to be merely an imitation of various alphabetical characters, and had, in my opinion, no meaning at all connected with them.\" In both accounts, Anthon maintained that he told Harris that Harris was the victim of a fraud. Pomeroy Tucker, a contemporary of Harris and Smith, wrote in 1867 that all the scholars whom Harris visited \"were understood to have scouted the whole pretense as too depraved for serious attention, while commiserating the applicant as the victim of fanaticism or insanity.\"\n\nThe Community of Christ purchased the handwritten slip of paper known as the Anthon Transcript, or the Caractors document from the heirs of David Whitmer. Whitmer, who once owned the document, stated that it was this slip of paper that Harris showed to Anthon. Both Mormon apologists and critics, however, claim that it is not certain that the document is the original, since Anthon had mentioned that the characters on the slip he saw were arranged in vertical columns and ended in a \"rude delineation of a circle divided into various compartments, decked with various strange marks, and evidently copied after the Aztec calendar given by Humboldt,\" (1834) or \"a rude representation of the Mexican zodiac\" (1841). Recent scholarship, including handwriting analysis, suggests the \"Caractors\" document was written by David Whitmer's brother John Whitmer in or after 1829 and therefore would not have been available to show Anthon or others in 1828. The symbols on the document were published twice in 1844, after Smith's death, as characters that had been copied from the gold plates, one of them in the December 21 issue of \"The Prophet\". In 1956 a request for review of the Caractors Document was made to three recognized Egyptologists, Sir Alan Gardiner, William C.. Hayes, and John A. Wilson. Gardiner replied that he saw no resemblance with \"any form of Egyptian writing.\" Hayes stated that it might be an inaccurate copy of something in hieratic script and that \"some groups look like hieratic numerals\", adding that \"I imagine, however, that the inscription bears a superficial resemblance to other scripts, both ancient and modern, of which I have no knowledge.\" Wilson gave the most detailed reply, saying that \"This is not Egyptian writing, as known to the Egyptologist. It obviously is not hieroglyphic, nor the \"cursive hieroglyphic\" as used in the Book of the Dead. It is not Coptic, which took over Greek characters to write Egyptian. Nor does it belong to one of the cursive stages of ancient Egyptian writing: hieratic, abnormal hieratic, or demotic.\"\n\nThe document is portrayed in the 2004 film \"The Work and the Glory\".\n\n"}
{"id": "9634607", "url": "https://en.wikipedia.org/wiki?curid=9634607", "title": "Arawá language", "text": "Arawá language\n\nArawá Aruá is an extinct language of Brazil. The people were wiped out by introduced measles, and the last speaker died in 1877. All that survives is a word list from 1869.\n"}
{"id": "2555800", "url": "https://en.wikipedia.org/wiki?curid=2555800", "title": "Aura (paranormal)", "text": "Aura (paranormal)\n\nAn aura or human energy field is, according to New Age beliefs, a colored emanation said to enclose a human body or any animal or object. In some esoteric positions, the aura is described as a subtle body. Psychics and holistic medicine practitioners often claim to have the ability to see the size, color and type of vibration of an aura.\n\nIn New Age alternative medicine, the human aura is seen as a hidden anatomy that affect the health of a client, and is often understood to comprise centers of vital force called chakra. Such claims are not supported by scientific evidence and are pseudoscience. When tested under controlled experiments, the ability to see auras has not been shown to exist.\n\nIn Latin and Ancient Greek, \"aura\" means wind, breeze or breath. It was used in Middle English to mean \"gentle breeze\". By the end of the 19th century, the word was used in some spiritualist circles to describe a speculated subtle emanation around the body.\n\nThe concept of auras was first popularized by Charles Webster Leadbeater, a former priest of the Church of England and a member of the mystic Theosophical Society. Leadbeater had studied theosophy in India, and believed he had the capacity to use his clairvoyant powers to make scientific investigations. He claimed that he had discovered that most men come from Mars but the more advanced men come from the Moon, and that hydrogen atoms are made of six bodies contained in an egg-like form. In his book \"Man Visible and Invisible\" published in 1903, Leadbeater illustrated the aura of man at various stages of his moral evolution, from the \"savage\" to the saint. In 1910, Leadbeater introduced the modern conception of auras by incorporating the Tantric notion of chakras in his book \"The Inner Life\". But Leadbeater didn’t simply present the Tantric beliefs to the West, he reconstructed and reinterpreted them by mixing them with his own ideas, without acknowledging the sources of these innovations. Some of Leadbeater’s innovations are describing chakras as energy vortexes, and associating each of them with a gland, an organ and other body parts.\n\nIn the following years, Leadbeater’s ideas on the aura and chakras where adopted and reinterpreted by other Theosophists such as Rudolf Steiner and Edgar Cayce, but his occult anatomy remained of minor interest within the esoteric counterculture until the 1980s, when it was picked up by the New Age movement.\n\nIn 1977, American esotericist Christopher Hills published the book \"Nuclear Evolution: The Rainbow Body\", which presented a modified version of Leadbeater’s occult anatomy. Whereas Leadbeater had drawn each chakras with intricately detailed shapes and multiple colors, Hills presented them as a sequence of centers, each one being associated with a color of the rainbow. Most of the subsequent New Age writers will base their representations of the aura on Hill’s interpretation of Leadbeater’s ideas. Chakras became a part of mainstream esoteric speculations in the 1980s and 1990s. Many New Age techniques that aim to clear blockages of the chakras were developed during those years, such as crystal healing and aura-soma. Chakras were, by the late 1990s, less connected with their theosophical and Hinduist root, and more infused with New Age ideas. A variety of New Age books proposed different links between each chakras and colors, personality traits, illnesses, Christian sacraments, etc. Various type of holistic healing within the New Age movement claim to use aura reading techniques, such as bioenergetic analysis, spiritual energy and energy medicine.\n\nThere have been numerous attempts to capture an energy field around the human body, going as far back as photographs by a French army officer in the 1890s. Supernatural interpretations of these images have often been the result of a lack of understanding of the simple natural phenomena behind them, such as heat emanating from a human body producing aura-like images under infrared photography.\n\nIn 1939, Semyon Davidovich Kirlian discovered that by placing an object or body part directly on photographic paper, and then passing a high voltage across the object, he would obtain the image of a glowing contour surrounding the object. This process became known as Kirlian photography. Some parapsychologists, such as Thelma Moss of UCLA, have proposed that these images show levels of psychic powers and bioenergies. However, studies have found that the Kirlian effect is caused by the presence of moisture on the object being photographed. Electricity produces an area of gas ionization around the object if it is moist, which is the case for living things. This causes an alternation of the electric charge pattern on the film. After rigorous experimentations, no mysterious process has been discovered in relation to the Kirlian photography.\n\nMore recent attempts at capturing auras include the Aura Imaging cameras and software introduced by Guy Coggins in 1992. Coggins claims that his software uses biofeedback data to color the picture of the subject. The technique has failed to yield reproducible results.\n\nTests of psychic abilities to observe alleged aura emanations have repeatedly been met with failure.\n\nOne test involved placing people in a dark room and asking the psychic to state how many auras she could observe. Only chance results were obtained.\n\nRecognition of auras has occasionally been tested on television. One test involved an \"aura reader\" standing on one side of a room with an opaque partition separating her from a number of slots which might contain either actual people or mannequins. The aura reader failed to identify the slots containing people, incorrectly stating that all contained people.\n\nIn another televised test another aura reader was placed before a partition where five people were standing. He claimed that he could see their auras from behind the partition. As each person moved out, the reader was asked to identify where that person was standing behind the slot. He identified 2 out of 5 correctly.\n\nAttempts to prove the existence of auras scientifically have repeatedly met with failure; for example people are unable to see auras in complete darkness, and auras have never been successfully used to identify people when their identifying features are otherwise obscured in controlled tests. A 1999 study concluded that conventional sensory cues such as radiated body heat might be mistaken for evidence of a metaphysical phenomenon.\n\nBridgette Perez in a review for the \"Skeptical Inquirer\" has written \"perceptual distortions, illusions, and hallucinations might promote belief in auras... Psychological factors, including absorption, fantasy proneness, vividness of visual imagery, and after-images, might also be responsible for the phenomena of the aura.\"\n\nStudies in laboratory conditions have demonstrated that the aura is best explained as a visual illusion known as an afterimage. Neurologists contend that people may perceive auras because of effects within the brain: epilepsy, migraines, or the influence of psychedelic drugs such as LSD.\n\nPsychologist Andrew Neher has written that \"there is no good evidence to support the notion that auras are, in any way, psychic in origin.\"\n\nIt has been suggested that auras may be the result of synaesthesia. However, a 2012 study discovered no link between auras and synaesthesia, concluding \"the discrepancies found suggest that both phenomena are phenomenologically and behaviourally dissimilar.\" Clinical neurologist Steven Novella has written \"Given the weight of the evidence it seems that the connection between auras and synaesthesia is speculative and based on superficial similarities that are likely coincidental.\"\n\nOther causes may include disorders within the visual system provoking optical effects.\n\n\n\n\n"}
{"id": "8767149", "url": "https://en.wikipedia.org/wiki?curid=8767149", "title": "Autocommunication", "text": "Autocommunication\n\nAutocommunication is a term used in communication studies, semiotics and other cultural studies to describe communication from and to oneself. This is distinguished from the more traditionally studied form of communication where the sender and the receiver of the message are separate. This can be called \"heterocommunication\".\n\nWhere heterocommunication gives the receiver new information, autocommunication does not. Instead it enhances and restructures the receiver's ego. Both forms of communication can be found either in individuals or within organisations. When autocommunication is done by an individual it can be called intrapersonal communication.\n\nAutocommunication is typical for religious or artistic works. Prayers, mantras and diaries are good examples. In organisations and corporations strategic plans and memos, for example, can function like mantras. But any text (or work) can become autocommunicational if it is read many times over.\n\n"}
{"id": "14159441", "url": "https://en.wikipedia.org/wiki?curid=14159441", "title": "Bank effect", "text": "Bank effect\n\nBank effect refers to the tendency of the stern of a ship to swing toward the near bank when operating in a river or constricted waterway.\n\nThe asymmetric flow around a ship induced by the vicinity of banks causes pressure differences (Bernoulli's principle) between port and starboard sides. As a result, a lateral force will act on the ship, mostly directed towards the closest bank, as well as a yawing moment pushing her bow towards the centre of the waterway. The squat effect increases due to the decreased blockage.\n\nThis can be seen on this footage video bank effects taken during model tests in a towing tank at Flanders Hydraulics Research\n\nThis phenomenon depends on many parameters, such as bank shape, water depth, ship-bank distance, ship properties, ship speed and propeller action. A reliable estimation of bank effects is important for determining the limiting conditions in which a ship can safely navigate a waterway.\n\nThis phenomenon has several different names, including bank suction, bank cushion, stern suction, and ship-bank interaction.\n"}
{"id": "2508934", "url": "https://en.wikipedia.org/wiki?curid=2508934", "title": "Bororoan languages", "text": "Bororoan languages\n\nThe Borôroan languages of Brazil are Borôro and the extinct Umotína and Otuke. They form part of the Macro-Jê proposal.\n\nThe relationship between the languages is,\n\nSee Otuke for various additional varieties which may have been dialects of it, such as Kovare and Kurumina; there are other recorded groups that may have spoken languages or dialects closer to Bororo, such as Aravirá, but nothing is directly known of their language.\n"}
{"id": "17228280", "url": "https://en.wikipedia.org/wiki?curid=17228280", "title": "Carathéodory conjecture", "text": "Carathéodory conjecture\n\nIn differential geometry, the Carathéodory conjecture is a mathematical conjecture attributed to Constantin Carathéodory by Hans Ludwig Hamburger in a session of the Berlin Mathematical Society in 1924. Carathéodory did publish a paper on a related subject, but never committed the Conjecture into writing. In, John Edensor Littlewood mentions the Conjecture and Hamburger's contribution as an example of a mathematical claim that is easy to state but difficult to prove. Dirk Struik describes in the formal analogy of the Conjecture with the Four Vertex Theorem for plane curves. Modern references to the Conjecture are the problem list of Shing-Tung Yau, the books of Marcel Berger, as well as the books.\n\nThe Conjecture claims that any convex, closed and sufficiently smooth surface in three dimensional Euclidean space needs to admit at least two umbilic points. In the sense of the Conjecture, the spheroid with only two umbilic points and the sphere, all points of which are umbilic, are examples of surfaces with minimal and maximal numbers of the umbilicus. For the conjecture to be well posed, or the umbilic points to be well-defined, the surface needs to be at least twice differentiable.\n\nThe invited address of Stefan Cohn-Vossen to the International Congress of Mathematicians of 1928 in Bologna was on the subject and in the 1929 edition of Wilhelm Blaschke's third volume on Differential Geometry he states:\nWhile this book goes into print, Mr. Cohn-Vossen has succeeded in proving that closed real-analytic surfaces do not have umbilic points of index > 2 (invited talk at the ICM in Bologna 1928). This proves the conjecture of Carathéodory for such surfaces, namely that they need to have at least two umbilics.\nHere Blaschke's index is twice the usual definition for an index of an umbilic point, and the global conjecture follows by the Poincaré–Hopf index theorem. No paper was submitted by Cohn-Vossen to the proceedings of the International Congress, while in later editions of Blaschke's book the above comments were removed. It is, therefore, reasonable to assume that this work was inconclusive.\n\nFor analytic surfaces, an affirmative answer to this conjecture was given in 1940 by Hans Hamburger in a long paper published in three parts. The approach of Hamburger was also via a local index estimate for isolated umbilics, which he had shown to imply the Conjecture in his earlier work. In 1943, a shorter proof was proposed by Gerrit Bol, see also, but, in 1959, Tilla Klotz found and corrected a gap in Bol's proof in. Her proof, in turn, was announced to be incomplete in Hanspeter Scherbel's dissertation (no results of that dissertation related to the Carathéodory conjecture were published for decades, at least nothing was published up to June 2009). Among other publications we refer to papers.\n\nAll the proofs mentioned above are based on Hamburger's reduction of the Carathéodory conjecture to the following conjecture: the index of every isolated umbilic point is never greater than one. Roughly speaking, the main difficulty lies in the resolution of singularities generated by umbilical points. All the above-mentioned authors resolve the singularities by induction on 'degree of degeneracy' of the umbilical point, but none of them was able to present the induction process clearly.\n\nIn 2002, Vladimir Ivanov revisited the work of Hamburger on analytic surfaces with the following stated intent:\n\n\"First, considering analytic surfaces, we assert with full responsibility that Carathéodory was right. Second, we know how this can be proved rigorously. Third, we intend to exhibit here a proof which, in our opinion, will convince every reader who is really ready to undertake a long and tiring journey with us.\"\n\nFirst he follows the way passed by Gerrit Bol and Tilla Klotz, but later he proposes his own way for singularity resolution where crucial role belongs to complex analysis (more precisely, to techniques involving analytic implicit functions, Weierstrass preparation theorem, Puiseux series, and circular root systems).\n\nIn 2008, Guilfoyle and Klingenberg announced a proof of the global conjecture for surfaces of smoothness formula_1. Their method uses neutral Kähler geometry of the Klein quadric, mean curvature flow, the Riemann–Roch index theorem, and the Sard–Smale theorem on regular values of Fredholm operators. This preprint has not yet been published, however.\n\nIn 2012, Ghomi and Howard showed, using a Möbius transformation, that the global conjecture for surfaces of smoothness C can be reformulated in terms of the number of umbilic points on graphs subject to certain asymptotics of the gradient.\n\n\n"}
{"id": "9089928", "url": "https://en.wikipedia.org/wiki?curid=9089928", "title": "Cardiac shunt", "text": "Cardiac shunt\n\nA cardiac shunt is a pattern of blood flow in the heart that deviates from the normal circuit of the circulatory system. It may be described as right-left, left-right or bidirectional, or as systemic-to-pulmonary or pulmonary-to-systemic. The direction may be controlled by left and/or right heart pressure, a biological or artificial heart valve or both. The presence of a shunt may also affect left and/or right heart pressure either beneficially or detrimentally.\n\nThe left and right sides of the heart are named from a dorsal view, i.e., looking at the heart from the back or from the perspective of the person whose heart it is. There are four chambers in a heart: an atrium (upper) and a ventricle (lower) on both the left and right sides. In mammals and birds, blood from the body goes to the right side of the heart first. Blood enters the upper right atrium, is pumped down to the right ventricle and from there to the lungs via the pulmonary artery. Blood going to the lungs is called the pulmonary circulation. When the blood returns to the heart from the lungs via the pulmonary vein, it goes to the left side of the heart, entering the upper left atrium. Blood is then pumped to the lower left ventricle and from there out of the heart to the body via the aorta. This is called the systemic circulation. A cardiac shunt is when blood follows a pattern that deviates from the systemic circulation, i.e., from the body to the right atrium, down to the right ventricle, to the lungs, from the lungs to the left atrium, down to the left ventricle and then out of the heart back to the systemic circulation.\n\nA left-to-right shunt is when blood from the left side of the heart goes to the right side of the heart. This can occur either through a hole in the ventricular or atrial septum that divides the left and the right heart or through a hole in the walls of the arteries leaving the heart, called great vessels. Left-to-right shunts occur when the systolic blood pressure in the left heart is higher than the right heart, which is the normal condition in birds and mammals.\n\nThe most common congenital heart defects (CHDs) which cause shunting are atrial septal defects (ASD), patent foramen ovale (PFO), ventricular septal defects (VSD), and patent ductus arteriosi (PDA). In isolation, these defects may be asymptomatic, or they may produce symptoms which can range from mild to severe, and which can either have an acute or a delayed onset. However, these shunts are often present in combination with other defects; in these cases, they may still be asymptomatic, mild or severe, acute or delayed, but they may also work to counteract the negative symptoms caused by another defect (as with d-Transposition of the great arteries).\n\nSome acquired shunts are modifications of congenital ones: a balloon septostomy can enlarge a foramen ovale (if performed on a newborn), PFO or ASD; or prostaglandin can be administered to a newborn to prevent the ductus arteriosus from closing. Biological tissues may also be used to construct artificial passages.\n\nMechanical shunts such as the Blalock-Taussig shunt are used in some cases of CHD to control blood flow or blood pressure.\n\nAll reptiles have the capacity for cardiac shunts.\n"}
{"id": "2021968", "url": "https://en.wikipedia.org/wiki?curid=2021968", "title": "Computerized physician order entry", "text": "Computerized physician order entry\n\nComputerized physician order entry (CPOE), sometimes referred to as computerized provider order entry or computerized provider order management (CPOM), is a process of electronic entry of medical practitioner instructions for the treatment of patients (particularly hospitalized patients) under his or her care.\n\nThe entered orders are communicated over a computer network to the medical staff or to the departments (pharmacy, laboratory, or radiology) responsible for fulfilling the order. CPOE reduces the time it takes to distribute and complete orders, while increasing efficiency by reducing transcription errors including preventing duplicate order entry, while simplifying inventory management and billing. \n\nCPOE is a form of patient management software.\n\nIn a graphical representation of an order sequence, specific data should be presented to CPOE system staff in cleartext, including: \n\n\nSome textual data can be reduced to simple graphics.\n\nCPOE systems use terminology familiar to medical and nursing staff, but there are different terms used to classify and concatenate orders. The following items are examples of additional terminology that a CPOE system programmer might need to know:\n\nThe application responding to, \"i.e.\", performing, a request for services (orders) or producing an observation. The filler can also originate requests for services (new orders), add additional services to existing orders, replace existing orders, put an order on hold, discontinue an order, release a held order, or cancel existing orders.\n\nA request for a service from one application to a second application. In some cases an application is allowed to place orders with itself.\n\nOne of several segments that can carry order information. Future ancillary specific segments may be defined in subsequent releases of the Standard if they become necessary.\n\nThe application or individual originating a request for services (order).\n\nA list of associated orders coming from a single location regarding a single patient.\n\nA grouping of orders used to standardize and expedite the ordering process for a common clinical scenario. (Typically, these orders are started, modified, and stopped by a licensed physician.)\n\nA grouping of orders used to standardize and automate a clinical process on behalf of a physician. (Typically, these orders are started, modified, and stopped by a nurse, pharmacist, or other licensed health professional.)\n\nFeatures of the ideal computerized physician order entry system (CPOE) include:\n\nIn the past, physicians have traditionally hand-written or verbally communicated orders for patient care, which are then transcribed by various individuals (such as unit clerks, nurses, and ancillary staff) before being carried out. Handwritten reports or notes, manual order entry, non-standard abbreviations and poor legibility lead to errors and injuries to patients, . A follow up IOM report in 2001 advised use of electronic medication ordering, with computer- and internet-based information systems to support clinical decisions. Prescribing errors are the largest identified source of preventable hospital medical error. A 2006 report by the Institute of Medicine estimated that a hospitalized patient is exposed to a medication error each day of his or her stay. While further studies have estimated that CPOE implementation at all nonrural hospitals in the United States could prevent over 500,000 serious medication errors each year. Studies of computerized physician order entry (CPOE) has yielded evidence that suggests the medication error rate can be reduced by 80%, and errors that have potential for serious harm or death for patients can be reduced by 55%, and other studies have also suggested benefits. Further, in 2005, CMS and CDC released a report that showed only 41 percent of prophylactic antibacterials were correctly stopped within 24 hours of completed surgery. The researchers conducted an analysis over an eight-month period, implementing a CPOE system designed to stop the administration of prophylactic antibacterials. Results showed CPOE significantly improved timely discontinuation of antibacterials from 38.8 percent of surgeries to 55.7 percent in the intervention hospital. CPOE/e-Prescribing systems can provide automatic dosing alerts (for example, letting the user know that the dose is too high and thus dangerous) and interaction checking (for example, telling the user that 2 medicines ordered taken together can cause health problems). In this way, specialists in pharmacy informatics work with the medical and nursing staffs at hospitals to improve the safety and effectiveness of medication use by utilizing CPOE systems.\n\nGenerally, CPOE is advantageous, as it leaves the trails of just better formatting retrospective information, similarly to traditional hospital information systems designs. The key advantage of providing information from the physician in charge of treatment for a single patient to the different roles involved in processing he treatise itself is widely innovative. This makes CPOE the primary tool for information transfer to the performing staff and lesser the tool for collecting action items for the accounting staff. However, the needs of proper accounting get served automatically upon feedback on completion of orders.\n\nCPOE is generally not suitable without reasonable training and tutoring respectively. As with other technical means, the system based communicating of information may be inaccessible or inoperable due to failures. That is not different to making use of an ordinary telephone or with conventional hospital information systems. Beyond, the information conveyed may be faulty or erratic. A concatenated validating of orders must be well organized. Errors lead to liability cases as with all professional treatment of patients.\n\nPrescriber and staff inexperience may cause slower entry of orders at first, use more staff time, and is slower than person-to-person communication in an emergency situation. Physician to nurse communication can worsen if each group works alone at their workstations.\n\nBut, in general, the options to reuse order sets anew with new patients lays the basic for substantial enhancement of the processing of services to the patients in the complex distribution of work amongst the roles involved. The basic concepts are defined with the clinical pathway approach. However, success does not occur by itself. The preparatory work has to be budgeted from the very beginning and has to be maintained all the time. Patterns of proper management from other service industry and from production industry may apply. However, the medical methodologies and nursing procedures do not get affected by the management approaches.\n\nCPOE presents several possible dangers by introducing new types of errors. Automation causes a false sense of security, a misconception that when technology suggests a course of action, errors are avoided. These factors contributed to an \"increased\" mortality rate in the Children's Hospital of Pittsburgh's Pediatric ICU when a CPOE systems was introduced. In other settings, shortcut or default selections can override non-standard medication regimens for elderly or underweight patients, resulting in toxic doses. Frequent alerts and warnings can interrupt work flow, causing these messages to be ignored or overridden due to alert fatigue. CPOE and automated drug dispensing was identified as a cause of error by 84% of over 500 health care facilities participating in a surveillance system by the United States Pharmacopoeia. Introducing CPOE to a complex medical environment requires ongoing changes in design to cope with unique patients and care settings, close supervision of overrides caused by automatic systems, and training, testing and re-training all users.\n\nCPOE systems can take years to install and configure. Despite ample evidence of the potential to reduce medication errors, adoption of this technology by doctors and hospitals in the United States has been slowed by resistance to changes in physician's practice patterns, costs and training time involved, and concern with interoperability and compliance with future national standards. According to a study by RAND Health, the US healthcare system could save more than 81 billion dollars annually, reduce adverse medical events and improve the quality of care if it were to widely adopt CPOE and other health information technology. As more hospitals become aware of the financial benefits of CPOE, and more physicians with a familiarity with computers enter practice, increased use of CPOE is predicted. Several high-profile failures of CPOE implementation have occurred, so a major effort must be focused on change management, including restructuring workflows, dealing with physicians' resistance to change, and creating a collaborative environment.\n\nAn early success with CPOE by the United States Department of Veterans Affairs (VA) is the Veterans Health Information Systems and Technology Architecture or VistA. A graphical user interface known as the Computerized Patient Record System (CPRS) allows health care providers to review and update a patient's record at any computer in the VA's over 1,000 healthcare facilities. CPRS includes the ability to place orders by CPOE, including medications, special procedures, x-rays, patient care nursing orders, diets and laboratory tests.\n\nThe world's first successful implementation of a CPOE system was at El Camino Hospital in Mountain View, California in the early 1970s. The Medical Information System (MIS) was originally developed by a software and hardware team at Lockheed in Sunnyvale, California, which became the TMIS group at Technicon Instruments Corporation. The MIS system used a light pen to allow physicians and nurses to quickly point and click items to be ordered.\n\n, one of the largest projects for a national EHR is by the National Health Service (NHS) in the United Kingdom. The goal of the NHS is to have 60,000,000 patients with a centralized electronic health record by 2010. The plan involves a gradual roll-out commencing May 2006, providing general practices in England access to the National Programme for IT (NPfIT). The NHS component, known as the \"Connecting for Health Programme\", includes office-based CPOE for medication prescribing and test ordering and retrieval, although some concerns have been raised about patient safety features.\n\nIn 2008, the Massachusetts Technology Collaborative and the New England Healthcare Institute (NEHI) published research showing that 1 in 10 patients admitted to a Massachusetts community hospital suffered a preventable medication error. The study argued that Massachusetts hospitals could prevent 55,000 adverse drug events per year and save $170 million annually if they fully implemented CPOE. The findings prompted the Commonwealth of Massachusetts to enact legislation requiring all hospitals to implement CPOE by 2012 as a condition of licensure.\n\nIn addition, the study also concludes that it would cost approximately $2.1 million to implement a CPOE system, and a cost of $435,000 to maintain it in the state of Massachusetts while it saves annually about $2.7 million per hospital. The hospitals will still see payback within 26 months through reducing hospitalizations generated by error. Despite the advantages and cost savings, the CPOE is still not well adapted by many hospitals in the US.\n\nThe Leapfrog’s 2008 Survey showed that most hospitals are still not complying with having a fully implemented, effective CPOE system. The CPOE requirement became more challenging to meet in 2008 because the Leapfrog introduced a new requirement: Hospitals must test their CPOE systems with Leapfrog’s CPOE Evaluation Tool. So the number of hospitals in the survey considered to be fully meeting the standard dropped to 7% in 2008 from 11% the previous year. Though the adoption rate seems very low in 2008, it is still an improvement from 2002 when only 2% of hospitals met this Leapfrog standard.\n\n\n"}
{"id": "7212", "url": "https://en.wikipedia.org/wiki?curid=7212", "title": "Creed", "text": "Creed\n\nA creed (also known as a \"confession\", \"symbol\", or \"statement of faith\") is a statement of the shared beliefs of a religious community in the form of a fixed formula summarizing core tenets.\n\nOne of the most widely used creeds in Christianity is the Nicene Creed, first formulated in AD 325 at the First Council of Nicaea. It was based on Christian understanding of the Canonical Gospels, the letters of the New Testament and to a lesser extent the Old Testament. Affirmation of this creed, which describes the Trinity, is generally taken as a fundamental test of orthodoxy for most Christian denominations. The Apostles' Creed is also broadly accepted. Some Christian denominations and other groups have rejected the authority of those creeds.\n\nMuslims declare the \"shahada\", or testimony: \"I bear witness that there is no god but (the One) God \"(Allah)\", and I bear witness that Muhammad is God's messenger.\"\n\nWhether Judaism is creedal has been a point of some controversy. Although some say Judaism is noncreedal in nature, others say it recognizes a single creed, the \"Shema Yisrael\", which begins: \"Hear, O Israel: the our God, the is one.\"\n\nThe word \"creed\" is particularly used for a concise statement which is recited as part of liturgy. The term is anglicized from Latin \"credo\" \"I believe\", the incipit of the Latin texts of the Apostles' Creed and the Nicene Creed. A creed is sometimes referred to as a \"symbol\" in a specialized meaning of that word (which was first introduced to Late Middle English in this sense), after Latin \"symbolum\" \"creed\" (as in \"Symbolum Apostolorum\" = \"Apostles' Creed\"), after Greek \"symbolon\" \"token, watchword\".\n\nSome longer statements of faith in the Protestant tradition are instead called \"confessions of faith\", or simply \"confession\" (as in e.g. Helvetic Confession).\nWithin Evangelicalism, the terms \"doctrinal statement\" or \"doctrinal basis\" tend to be preferred. Doctrinal statements may include positions on lectionary and translations of the Bible, particularly in fundamentalist churches of the King James Only movement.\n\nThe term \"creed\" is sometimes extended to comparable concepts in non-Christian theologies; thus the Islamic concept of \"ʿaqīdah\" (literally \"bond, tie\") is often rendered as \"creed\".\n\nSeveral creeds have originated in Christianity.\n\nProtestant denominations are usually associated with confessions of faith, which are similar to creeds but usually longer.\n\nSome Christian denominations, and particularly those descending from the Radical Reformation, do not profess a creed. This stance is often referred to as \"non-creedalism\". The Religious Society of Friends, also known as the Quakers, consider that they have no need for creedal formulations of faith. The Church of the Brethren and other Schwarzenau Brethren churches also espouses no creed, referring to the New Testament, as their \"rule of faith and practice.\" Jehovah's Witnesses contrast \"memorizing or repeating creeds\" with acting to \"do what Jesus said\". Unitarian Universalists do not share a creed.\n\nMany evangelical Protestants similarly reject creeds as definitive statements of faith, even while agreeing with some creeds' substance. The Baptists have been non-creedal \"in that they have not sought to establish binding authoritative confessions of faith on one another\". While many Baptists are not opposed to the ancient creeds, they regard them as \"not so final that they cannot be revised and re-expressed. At best, creeds have a penultimacy about them and, of themselves, could never be the basis of Christian fellowship\". Moreover, Baptist \"confessions of faith\" have often had a clause such as this from the First London (Particular) Baptist Confession (Revised edition, 1646):\nSimilar reservations about the use of creeds can be found in the Restoration Movement and its descendants, the Christian Church (Disciples of Christ), the Churches of Christ, and the Christian churches and churches of Christ. Restorationists profess \"no creed but Christ\".\n\nBishop John Shelby Spong, retired Episcopal Bishop of Newark, has written that dogmas and creeds were merely \"a stage in our development\" and \"part of our religious childhood.\" In his book, \"Sins of the Scripture\", Spong wrote that \"Jesus seemed to understand that no one can finally fit the holy God into his or her creeds or doctrines. That is idolatry.\"\n\nIn the Swiss Reformed Churches, there was a quarrel about the Apostles' Creed in the mid-19th century. As a result, most cantonal reformed churches stopped prescribing any particular creed.\n\nWithin the sects of the Latter Day Saint movement, the \"Articles of Faith\" are a list composed by Joseph Smith as part of an 1842 letter sent to \"Long\" John Wentworth, editor of the \"Chicago Democrat\". It is canonized with the \"Bible\", the \"Book of Mormon\", the \"Doctrine & Covenants\" and \"Pearl of Great Price\", as part of the standard works of The Church of Jesus Christ of Latter-day Saints.\n\nCreedal works include:\n\nWhether Judaism is creedal in character has generated some controversy. Rabbi Milton Steinberg wrote that \"By its nature Judaism is averse to formal creeds which of necessity limit and restrain thought\" and asserted in his book \"Basic Judaism\" (1947) that \"Judaism has never arrived at a creed.\" The 1976 Centenary Platform of the Central Conference of American Rabbis, an organization of Reform rabbis, agrees that \"Judaism emphasizes action rather than creed as the primary expression of a religious life.\"\n\nOthers, however, characterize the Shema Yisrael as a creedal statement in strict monotheism embodied in a single prayer: \"Hear O Israel, the Lord is our God, the Lord is One\" (; transliterated \"Shema Yisrael Adonai Eloheinu Adonai Echad\").\n\nA notable statement of Jewish principles of faith was drawn up by Maimonides as his 13 Principles of Faith.\n\nThe shahada, the two-part statement that \"There is no god but God; Muhammad is the messenger of God\" is often popularly called \"the Islamic creed\" and its utterance is one of the \"five pillars\".\n\nIn Islamic theology, the term most closely corresponding to \"creed\" is \"ʿaqīdah\" ()\nThe first such creed was written as \"a short answer to the pressing heresies of the time\" is known as \"Al-Fiqh Al-Akbar\" and ascribed to Abū Ḥanīfa. Two well known creeds were the \"Fiqh Akbar II\" \"representative\" of the al-Ash'ari, and \"Fiqh Akbar III\", \"representative\" of the Ash-Shafi'i.\n\n\"Iman\" () in Islamic theology denotes a believer's religious faith . Its most simple definition is the belief in the six articles of faith, known as \"arkān al-īmān\".\n\n\n\n"}
{"id": "42387893", "url": "https://en.wikipedia.org/wiki?curid=42387893", "title": "Crony-capitalism index", "text": "Crony-capitalism index\n\nThe crony-capitalism index aims to indicate whether the livelihood of the people from certain country or city with a capitalist economy are easily affected by crony capitalism. It is not an internationally recognized index due to its limitations.\n\nIt is a new measurement of crony capitalism designed by \"The Economist\" newspaper based on the \"work by Ruchir Sharma of Morgan Stanley Investment Management, Aditi Gandhi and Michael Walton oew Delhi's Centre for Policy Research, and others\" in 2014.\n\nThe index aims to be a measuring trend in the number of economic rent-seekers. The assumption behind is because of the favorable political policies set by the government officials, the are increasing their wealth and interest. As a result, they get a larger part of people’s fruits of labor, instead of generating more wealth for the whole society. In some extreme cases, some favored suppliers are influential on the establishment and application of the business-impacting laws and citizens pay the tax for purchasing the overpriced products supplied by the favored corporations.\n\nTen of the industries that are susceptible to monopoly or require licensing or highly depend on the government have been selected: casinos; coal, palm oil and timber; defense; deposit-taking banking and investment banking; infrastructure and pipelines; ports; airports; real estate and construction; steel and other metals; mining and commodities; utilities and telecoms services. Then, the total wealth of world’s billionaires who actively involve in rent-heavy industries from the data of Forbes will be calculated. Results can be achieved from the ratio of billionaire wealth to GDP in their own countries; higher ratio of billionaire wealth to GDP indicates higher possibility of suffering from crony capitalism.\n\nThe 2016 index was published on May 7, 2016.\n\nThe results of the crony-capitalist Index of 23 countries were published in March 15, 2014. The five largest developed countries, ten largest developing countries and eight other countries where cronyism was thought to be a big problem being included. Developing countries in general having a relatively higher Crony-Capitalism index than developed countries.\n\nThe phenomenon of concealing fortunes is common for cronies, especially in China. It has been revealed that some of the powerful politicians have disguised their properties by transferring it under the names of their friends and family members. Unreliable property records are also believed to be the obstacles of determining an individual’s actual wealth. The fact that the cronies are not willing to announce their wealth publicly has affected the accuracy of the index.\n\n\"The Economist\" roughly categorizes the industrial sectors with neglected critical instances of cronyism. \n\nThe index has only counted the wealth of billionaires which contributed to the omission of extensive data. Due to the lack of data, particular industries in a certain country cannot be exactly identified as \"crony heavy\". A group of cronies can also possibly be enriched by plenty of rent-seeking while not wealthy enough to achieve the cut-off. Therefore, this group will not be examined in the index. The index is only a crude guide to the concentration of wealth in opaque industries compared with more competitive ones.\n\nPublic opinion has been weighted heavily when determining which governments are better. Political consultant Nick Sorrentino wrote that \"The Economist\" \"was afraid to go completely down the (anti) cronyism path because if one really examines cronyism it is tied in deeply with modern government\". Certain kinds of cronyism that involved fewer of the top social classes, such as the military-industrial complex in the United States, fail to be reflected in the index, which diminishes its accuracy.\n\n\n"}
{"id": "1462301", "url": "https://en.wikipedia.org/wiki?curid=1462301", "title": "DWIM", "text": "DWIM\n\nDWIM (do what I mean) computer systems attempt to anticipate what users intend to do, correcting trivial errors automatically rather than blindly executing users' explicit but potentially incorrect inputs.\n\nThe term was coined by Warren Teitelman in his DWIM package for BBN Lisp, part of his PILOT system, some time before 1966.\n\nTeitelman's DWIM package \"correct[ed] errors automatically or with minor user intervention\", similarly to a spell checker for natural language.\n\nTeitelman and his Xerox PARC colleague Larry Masinter later described the philosophy of DWIM in the Interlisp programming environment (the successor of BBN Lisp): \n\nAlthough most users think of DWIM as a single identifiable package, it embodies a pervasive philosophy of user interface design: at the user interface level, system facilities should make reasonable interpretations when\ngiven unrecognized input. ...the style of interface used throughout Interlisp allows the user to omit various parameters and have these default to reasonable values...<br>\n\nDWIM is an embodiment of the idea that the user is interacting with an agent who attempts to interpret the\nuser's request from contextual information. Since we want the user to feel that he is conversing with the system,\nhe should not be stopped and forced to correct himself or give additional information in situations where the correction or information is obvious.\nCritics of DWIM claimed that it was \"tuned to the particular typing mistakes to which\nTeitelman was prone, and no others\" and called it \"Do What Teitelman Means,\" \"Do What Interlisp Means\" or even claimed DWIM stood for \"Damn Warren's Infernal Machine.\" \n\nThe DWIM concept has been adopted by users of the GNU Emacs text editor to describe Emacs Lisp functions or commands that \"do the right thing\" depending on context, and do not specifically correct the user's typing. The Emacs wiki gives the example of a file copy command that is able to deduce the destination path from a split window configuration that contains two dired windows, one of which displays the source path. DWIM functionality is often mentioned in the command's name; GNU Emacs has a codice_1 function that comments out a selected region if uncommented, or uncomments it when already commented out, using comment characters and indentation appropriate for the environment and current context.\n\n\n"}
{"id": "37139759", "url": "https://en.wikipedia.org/wiki?curid=37139759", "title": "Dumun language", "text": "Dumun language\n\nDumun is a nearly extinct Rai Coast language spoken in Madang Province, Papua New Guinea.\n\nDumun is reported to go by the name Bai, but evidently this is a distinct (though related) language, or at least a variety called Bai recorded by Maclay was distinct.\n"}
{"id": "34255890", "url": "https://en.wikipedia.org/wiki?curid=34255890", "title": "Dzao Min language", "text": "Dzao Min language\n\nDzao Min (, Zao Min), is a Hmong–Mien language of China. Mao (2004:306) reports a total of more than 60,000 speakers in Liannan County and Yangshan County of Guangdong, and in Yizhang County of Hunan. The speakers from Bapai, Guangdong are also called Bapai Yao (八排瑶族).\n\nThe \"Chenzhou Prefecture Gazetteer\" (1996) reports that there are 1,200 \"Bapai Yao\" (八排瑶) or \"Zao Min\" (藻敏) in Huangjiapan Village 黄家畔村, Mangshan Township 莽山乡, Yizhang County, Hunan. They are reported to have migrated from Taipingdong 太平洞, Chengjia District 称架区, Yangshan County, Guangdong in the 16th century.\n\nLong Guoyi (2011) covers the six Zao Min dialects of Daping 大坪, Junliao 军寮, Mangshan 莽山, Nan'gang 南岗, Panshi 盘石, and Youling 油岭. Long reports that other than in Liannan County, small pockets of Zao Min speakers are located in:\n\n"}
{"id": "6884197", "url": "https://en.wikipedia.org/wiki?curid=6884197", "title": "Estray", "text": "Estray\n\nEstray, in law, is any domestic animal found wandering at large or lost, particularly if the owner is unknown. In most cases this includes domesticated animals and not pets.\n\nUnder early English common law, estrays were forfeited to the king or lord of the manor; under modern statutes, provision is made for taking up stray animals and acquiring either title to them or a lien for the expenses incurred in keeping them. A person taking up an estray has a qualified ownership in it, which becomes absolute if the owner fails to claim the animal within the statutory time limit. Whether the animal escaped through the owner's negligence or through the wrongful act of a third person is immaterial. If the owner reclaims the estray, he is liable for reasonable costs of its upkeep. The use of an estray during the period of qualified ownership, other than for its own preservation or for the benefit of the owner, is not authorized. Some statutes limit the right to take up estrays to certain classes of persons, to certain seasons or places, or to animals requiring care.\n\nWhen public officials, such as a county sheriff impound stray animals, they may sell them at auction to recover the costs of upkeep, with proceeds, if any, going into the public treasury. In some places, an uncastrated male livestock animal running at large may be neutered at the owner's expense.\n\nIt is common in the US for there to be a required \"Notice of Estray\" sworn and filed in a local office. The process usually takes a prescribed time to permit the property owner to collect his property. Otherwise, the finder obtains title to the property.\n\n"}
{"id": "11736875", "url": "https://en.wikipedia.org/wiki?curid=11736875", "title": "František Lorenz", "text": "František Lorenz\n\nFrantišek Lorenz (24 December 1872 – 24 May 1957) also known as Francisco Valdomiro Lorenz or Francisco Lorenz was a Czech-born polyglot and philosopher born in Zbyslav (nowadays part of the Czech Republic). He was one of the first Esperantists in the world, and was able to communicate in over 100 different languages. In 1891, Lorenz moved to Brazil due to political reasons. In Brazil, he lived firstly in Rio de Janeiro, and then in Rio Grande do Sul (Southern Brazil). Lorenz published over 36 books in 40 languages and was one of the most prominent promoters of Esperanto movement ever in Brazil. He died in Dom Feliciano (Brazil) in 1957.\n\nBudo Esperanta de Valença - Francisco Valdomiro Lorenz - In Portuguese\n\nCentro Educacional à Distância (CED) Notícias - In Portuguese\n"}
{"id": "12823", "url": "https://en.wikipedia.org/wiki?curid=12823", "title": "Garbage in, garbage out", "text": "Garbage in, garbage out\n\nIn computer science, garbage in, garbage out (GIGO) describes the concept that flawed, or nonsense input data produces nonsense output or \"garbage\". \n\nThe principle also applies more generally to all analysis and logic, in that arguments are unsound if their premises are flawed.\n\nIt was popular in the early days of computing, but applies even more today, when powerful computers can produce large amounts of erroneous data or information in a short time. The first use of the phrase has been dated to a November 10, 1957, syndicated newspaper article about US Army mathematicians and their work with early computers, in which an Army Specialist named William D. Mellin explained that computers cannot think for themselves, and that \"sloppily programmed\" inputs inevitably lead to incorrect outputs. The underlying principle was noted by the inventor of the first programmable computing device design:\n\nMore recently, the Marine Accident Investigation Branch comes to a similar conclusion: \n\nThe term may have been derived from last-in, first-out (LIFO) or first-in, first-out (FIFO).\n\n\"Garbage in, gospel out\" is a more recent expansion of the acronym. It is a sardonic comment on the tendency to put excessive trust in \"computerised\" data, and on the propensity for individuals to blindly accept what the computer says. Since the data entered into the computer is then processed by the computer, people who do not understand the processes in question, tend to believe the data they see and take it just as seriously as if the data were Gospels:\n\nThe term can also be used as an explanation for the poor quality of a digitized audio or video file. Although digitizing can be the first step in cleaning up a signal, it does not, by itself, improve the quality. Defects in the original analog signal will be faithfully recorded, but may be identified and removed by a subsequent step by digital signal processing.\n\nGIGO is commonly used to describe failures in human decision-making due to faulty, incomplete, or imprecise data. This sort of issue predates the computer age, but the term can still be applied.\n\nGIGO was the name of a Usenet gateway program to FidoNet, MAUSnet, e.a.\n\nIn statistics, it is not possible to carry out an accurate statistical analysis of inaccurate data.\n\n"}
{"id": "16231191", "url": "https://en.wikipedia.org/wiki?curid=16231191", "title": "Glavda language", "text": "Glavda language\n\nGlavda (also known as Galavda, Gelebda, Glanda, Guelebda, Galvaxdaxa) is an Afro-Asiatic language spoken in Borno State, Nigeria and in Far North Province, Cameroon.\n"}
{"id": "66715", "url": "https://en.wikipedia.org/wiki?curid=66715", "title": "Hindustani language", "text": "Hindustani language\n\nHindustani (, ,), also known as Hindi-Urdu, and historically also known as Hindavi, Dehlavi, and Rekhta, is the \"lingua franca\" of Northern India, most parts of India, Fiji and Pakistan. It is an Indo-Aryan language, deriving its base primarily from the Khariboli dialect of Delhi. The language incorporates a large amount of vocabulary from Prakrit, Persian and Arabic, as well as Sanskrit (via Prakrit and Tatsama borrowings). It is a pluricentric language, with two official forms, Modern Standard Hindi and Modern Standard Urdu, which are its standardised registers. \nAccording to Ethnologue's 2017 estimates Hindustani is the 3rd most spoken language in the world, with approximately 329.1 million native speakers, and 697.4 million total speakers.\n\nThe colloquial registers are mostly indistinguishable, and even though the official standards are nearly identical in grammar, they differ in literary conventions and in academic and technical vocabulary, with Urdu adopting stronger Persian and Arabic influences, and Hindi relying more heavily on Sanskrit. Before the partition of India, the terms \"Hindustani,\" \"Hindi,\" and \"Urdu\" were synonymous; they all covered what would be mostly called Hindi and Urdu today. The term \"Hindustani\" is still used for the colloquial language and the \"lingua franca\" of North India and Pakistan, for example for the language of Bollywood films, as well as for several languages of the Hindi-Urdu belt spoken outside the Indian subcontinent, such as Fijian Hindi of Fiji and the Caribbean Hindustani of Trinidad and Tobago, Guyana, Suriname, and the rest of the Caribbean. Hindustani is also spoken by a small number of people in Mauritius and South Africa.\n\nEarly forms of present-day Hindustani developed from the Middle Indo-Aryan \"apabhraṃśa\" vernaculars of present-day North India in the 7th–13th centuries.The generally accepted notion is that it was a purely sanskritised Hindu language at first that took significant influence from invading Persian empires. Amir Khusrow, who lived in the thirteenth century during the Delhi Sultanate period in North India, used these forms (which was the \"lingua franca\" of the period) in his writings and referred to it as \"Hindavi\" ( literally \"of Hindus or Indians\"). The Delhi Sultanate, which comprised several Turkic and Afghan dynasties that ruled from Delhi, was succeeded by the Mughal Empire in 1526.\n\nAlthough the Mughals were of Timurid (\"Gurkānī\") Turco-Mongol descent, they were Persianised, and Persian had gradually become the state language of the Mughal empire after Babur, a continuation since the introduction of Persian by Central Asian Turkic rulers in the Indian Subcontinent, and the patronisation of it by the earlier Turko-Afghan Delhi Sultanate. The basis in general for the introduction of Persian into the subcontinent was set, from its earliest days, by various Persianised Central Asian Turkic and Afghan dynasties.\n\nIn the 18th century, towards the end of the Mughal period, with the fragmentation of the empire and the elite system, a variant of Khariboli, one of the successors of apabhraṃśa vernaculars at Delhi, and nearby cities, came to gradually replace Persian as the lingua franca among the educated elite upper class particularly in northern India, though Persian still retained much of its pre-eminence for a short period. The term \"Hindustani\" ( \"of \"Hindustan\"\") was the name given to that variant of Khariboli.\n\nFor socio-political reasons, though essentially the variant of Khariboli with Persian vocabulary, the emerging prestige dialect became also known as \"Zabān-e Urdū-e Mualla\" \"language of the court\" or \"Zabān-e Urdū\" , ज़बान-ए उर्दू, \"language of the camp\" in Persian, derived from Turkic \"Ordū\" \"camp\", cognate with English \"horde\", due to its origin as the common speech of the Mughal army). The more highly Persianised version later established as a language of the court was called \"Rekhta\", or \"mixed\".\n\nAs an emerging common dialect, Hindustani absorbed large numbers of Persian, Arabic, and Turkic words, and as Mughal conquests grew it spread as a lingua franca across much of northern India. Written in the Persian alphabet or Devanagari, it remained the primary lingua franca of northern India for the next four centuries (although it varied significantly in vocabulary depending on the local language) and achieved the status of a literary language, alongside Persian, in Muslim courts. Its development was centred on the poets of the Mughal courts of cities in Uttar Pradesh such as Delhi, Lucknow, and Agra.\n\nJohn Fletcher Hurst in his book published in 1891 mentioned that the Hindustani or camp language of the Mughal Empire's courts at Delhi was not regarded by philologists as distinct language but only as a dialect of Hindi with admixture of Persian. He continued: \"But it has all the magnitude and importance of separate language. It is linguistic result of Muslim rule of eleventh & twelfth centuries and is spoken (except in rural Bengal) by many Hindus in North India and by Musalman population in all parts of India\". Next to English it was the official language of British Raj, was commonly written in Arabic or Persian characters, and was spoken by approximately 100,000,000 people.\n\nWhen the British colonised the Indian subcontinent from the late 18th through to the late 19th century, they used the words 'Hindustani', 'Hindi' and 'Urdu' interchangeably. They developed it as the language of administration of British India, further preparing it to be the official language of modern India and Pakistan. However, with independence, use of the word 'Hindustani' declined, being largely replaced by 'Hindi' and 'Urdu', or 'Hindi-Urdu' when either of those was too specific. More recently, the word 'Hindustani' has been used for the colloquial language of Bollywood films, which are popular in both India and Pakistan and which cannot be unambiguously identified as either Hindi or Urdu.\n\nAlthough, at the spoken level, Hindi and Urdu are considered registers of a single language, they differ vastly in literary and formal vocabulary; where literary Hindi draws heavily on Sanskrit and to a lesser extent Prakrit, literary Urdu draws heavily on Persian and Arabic. The grammar and base vocabulary (most pronouns, verbs, adpositions, etc.) of both Hindi and Urdu, however, are the same and derive from a Prakritic base, and both have Persian/Arabic influence.\n\nThe standardised registers Hindi and Urdu are collectively known as \"Hindi-Urdu\". Hindustani is perhaps the \"lingua franca\" of the north and west of the Indian subcontinent, though it is understood fairly well in other regions also, especially in the urban areas. A common vernacular sharing characteristics with Sanskritised Hindi, regional Hindi and Urdu, Hindustani is more commonly used as a vernacular than highly Sanskritised Hindi or highly Arabicised/Persianised Urdu.\n\nThis can be seen in the popular culture of Bollywood or, more generally, the vernacular of North Indians and Pakistanis, which generally employs a lexicon common to both \"Hindi\" and \"Urdu\" speakers. Minor subtleties in region will also affect the 'brand' of Hindustani, sometimes pushing the Hindustani closer to Urdu or to Hindi. One might reasonably assume that the Hindustani spoken in Lucknow, Uttar Pradesh (known for its usage of Urdu) and Varanasi (a holy city for Hindus and thus using highly Sanskritised Hindi) is somewhat different.\n\nStandard Hindi, one of the official languages of India, is based on the Kharibol dialect of the Delhi region and differs from Urdu in that it is usually written in the indigenous Devanagari of India and exhibits less Persian and Arabic influence than Urdu. It has a literature of 500 years, with prose, poetry, religion and philosophy, under the Bahmani Kings and onwards. It is prevalent all over the Deccan Plateau.\nNote that the term \"Hindustani\" has generally fallen out of common usage in modern India, except to refer to \"Indian\" as a nationality and a style of Indian classical music prevalent in northern India. The term used to refer to it is \"Hindi\" or \"Urdu\", depending on the religion of the speaker, and regardless of the mix of Persian or Sanskrit words used by the speaker. One could conceive of a wide spectrum of dialects and registers, with the highly Persianised Urdu at one end of the spectrum and a heavily Sanskrit-based dialect, spoken in the region around Varanasi, at the other end. In common usage in India, the term \"Hindi\" includes all these dialects except those at the Urdu spectrum. Thus, the different meanings of the word \"Hindi\" include, among others:\n\nUrdu is the national language of Pakistan and an officially recognised regional language of India. Urdu is the official language of all Pakistani provinces and is taught in all schools as a compulsory subject up to the 12th grade. It is also an official language in the Indian states of Jammu and Kashmir, National Capital Territory of Delhi, Uttar Pradesh, Bihar, and Telangana that have significant Muslim populations.\n\nIn a specific sense, \"Hindustani\" may be used to refer to the dialects and varieties used in common speech, in contrast with the standardised Hindi and Urdu. This meaning is reflected in the use of the term \"bazaar Hindustani\", in other words, the \"language of the street or the marketplace\", as opposed to the perceived refinement of formal Hindi, Urdu, or even Sanskrit. Thus, the Webster's New World Dictionary defines the term Hindustani as \"the principal dialect of Hindi/Urdu, used as a trade language throughout north India and Pakistan.\"\n\nAmir Khusro ca. 1300 referred to this language of his writings as \"Dehlavi\" (देहलवी; 'of Delhi') or \"Hindavi\" (हिन्दवी; ). During this period, Hindustani was used by Sufis in promulgating their message across the Indian subcontinent. After the advent of the Mughals in the subcontinent, Hindustani acquired more Persian loanwords. \"Rekhta\" ('mixture') and \"Hindi\" ('Indian') became popular names for the same language until the 18th century. The name \"Urdu\" appeared around 1780. During the British Raj, the term \"Hindustani\" was used by British officials. In 1796, John Borthwick Gilchrist published a \"A Grammar of the Hindoostanee Language\". Upon partition, India and Pakistan established national standards that they called \"Hindi\" and \"Urdu,\" respectively, and attempted to make distinct, with the result that \"Hindustani\" commonly, but mistakenly, came to be seen as a \"mixture\" of Hindi and Urdu.\n\nGrierson, in his highly influential \"Linguistic Survey of India\", proposed that the names \"Hindustani, Urdu,\" and \"Hindi\" be separated in use for different varieties of the Hindustani language, rather than as the overlapping synonyms they frequently were:\n\nHindi, a major standardized register of Hindustani, is declared by the Constitution of India as the \"official language (राजभाषा, \"rājabhāśā\") of the Union\" (Art. 343(1)) (In this context, \"Union\" means the Federal Government and not the entire country – India has 23 official languages). At the same time, however, the definitive text of federal laws is officially the English text and proceedings in the higher appellate courts must be conducted in English. At the state level, Hindi is one of the official languages in 9 of the 29 Indian states and three Union Territories (respectively, Uttar Pradesh, Bihar, Jharkhand, Uttarakhand, Madhya Pradesh, Rajasthan, Chhattisgarh, Himachal Pradesh, and Haryana; Delhi, Chandigarh, and the Andaman and Nicobar Islands). In the remaining states Hindi is not an official language. In states like Tamil Nadu and Karnataka, studying Hindi is not compulsory in the state curriculum. However an option to take the same as second or third language does exist. In many other states, studying Hindi is usually compulsory in the school curriculum as a third language (the first two languages being the state's official language and English), though the intensiveness of Hindi in the curriculum varies.\n\nUrdu, also a major standardized register of Hindustani, is also one of the languages recognized in the Eighth Schedule to the Constitution of India and is an official language of the Indian states of Telangana, Bihar, Delhi, Jammu and Kashmir, and Uttar Pradesh. Although the government school system in most other states emphasises Modern Standard Hindi, at universities in cities such as Lucknow, Aligarh and Hyderabad, Urdu is spoken and learnt, and \"Saaf\" or \"Khaalis\" Urdu is treated with just as much respect as \"Shuddha\" Hindi.\n\nUrdu is also the national language of Pakistan, where it shares official language status with English. Although English is spoken by many, and Punjabi is the native language of the majority of the population, Urdu is the \"lingua franca\".\n\n\"Hindustani\" was the official language of the British Raj and was synonymous with both Hindi and Urdu. After India's independence in 1947, the Sub-Committee on Fundamental Rights recommended that the official language of India be Hindustani: \"Hindustani, written either in Devanagari or the Perso-Arabic script at the option of the citizen, shall, as the national language, be the first official language of the Union.\" However, this recommendation was not adopted by the Constituent Assembly.\n\nBesides being the \"lingua franca\" of North India and Pakistan in South Asia, Hindustani is also spoken by many in the South Asian diaspora and their descendants around the world, including North America (in Canada, for example, Hindustani is one of the fastest growing languages), Europe, and the Middle East.\n\nFiji Hindi was derived from the Hindustani linguistic group and is spoken widely by Fijians of Indian origin.\n\nHindustani was also one of the languages that was spoken widely during British rule in Burma. Many older citizens of Myanmar, particularly Anglo-Indians and the Anglo-Burmese, still know it, although it has had no official status in the country since military rule began.\n\nHindustani is also spoken in the countries of the Gulf Cooperation Council, where migrant workers from various countries live and work for several years.\n\nHindustani contains around 5,500 words of Persian and Arabic origin.\n\nHistorically, Hindustani was written in the Kaithi, Devanagari, and Urdu alphabets. Kaithi and Devanagari are two of the Brahmic scripts native to India, whereas Urdu is a derivation of the Persian Nastaʿlīq script, which is the preferred calligraphic style for Urdu.\n\nToday, Hindustani continues to be written in the Urdu alphabet in Pakistan. In India, the Hindi register is officially written in Devanagari, and Urdu in the Urdu alphabet, to the extent that these standards are partly defined by their script.\n\nHowever, in popular publications in India, Urdu is also written in Devanagari, with slight variations to establish a Devanagari Urdu alphabet alongside the Devanagari Hindi alphabet.\n\nBecause of anglicisation in South Asia and the international use of the Latin script, Hindustani is occasionally written in the Latin script. This adaptation is called Roman Urdu or Romanised Hindi, depending upon the register used. Because the Bollywood film industry is a major proponent of the Latin script, the use of Latin script to write in Hindi and Urdu is growing amongst younger Internet users. Since Urdu and Hindi are mutually intelligible when spoken, Romanised Hindi and Roman Urdu (unlike Devanagari Hindi and Urdu in the Urdu alphabet) are mutually intelligible as well.\n\nFollowing is a sample text, Article 1 of the Universal Declaration of Human Rights, in the two official registers of Hindustani, Hindi and Urdu. Because this is a formal legal text, differences in formal vocabulary are maximised.\n\nNastaliq transcription:\nTransliteration (IAST):\n\nTranscription (IPA):\n\nGloss (word-to-word):\n\nTranslation (grammatical):\n\nDevanagari transcription:\n\nTransliteration (ALA-LC):\n\nTranscription (IPA):\n\nGloss (word-to-word):\n\nTranslation (grammatical):\n\nThe predominant Indian film industry Bollywood, located in Mumbai, Maharashtra uses Hindi, Khariboli dialect, Bombay Hindi, Urdu, Awadhi, Rajasthani, Bhojpuri, and Braj Bhasha, along with the language of Punjabi and with the liberal use of English or Hinglish for the dialogue and soundtrack lyrics.\n\nMovie titles are often screened in three scripts: Latin, Devanagari and occasionally Perso-Arabic. The use of Urdu or Hindi in films depends on the film's context: historical films set in the Delhi Sultanate or Mughal Empire are almost entirely in Urdu, whereas films based on Hindu mythology or ancient India make heavy use of Hindi with Sanskrit vocabulary.\n\n\n"}
{"id": "2780919", "url": "https://en.wikipedia.org/wiki?curid=2780919", "title": "Indeterminacy of translation", "text": "Indeterminacy of translation\n\nThe indeterminacy of translation is a thesis propounded by 20th-century American analytic philosopher W. V. Quine. The classic statement of this thesis can be found in his 1960 book \"Word and Object\", which gathered together and refined much of Quine's previous work on subjects other than formal logic and set theory. The indeterminacy of translation is also discussed at length in his \"Ontological Relativity\". Crispin Wright suggests that this \"has been among the most widely discussed and controversial theses in modern analytical philosophy\". This view is endorsed by Putnam who states that it is \"the most fascinating and the most discussed philosophical argument since Kant's Transcendental Deduction of the Categories\".\n\nThree aspects of indeterminacy arise, of which two relate to indeterminacy of translation. The three indeterminacies are (i) inscrutability of reference, and (ii) holophrastic indeterminacy, and (iii) the underdetermination of scientific theory. The last of these, not discussed here, refers to Quine's assessment that evidence alone does not dictate the choice of a scientific theory. The first refers to indeterminacy in interpreting individual words or sub-sentences. The second refers to indeterminacy in entire sentences or more extensive portions of discourse.\n\nIndeterminacy of reference refers to the interpretation of words or phrases in isolation, and Quine's thesis is that no unique interpretation is possible, because a 'radical interpreter' has no way of telling which of many possible meanings the speaker has in mind. Quine uses the example of the word \"gavagai\" uttered by a native speaker of the unknown language \"Arunta\" upon seeing a rabbit. A speaker of English could do what seems natural and translate this as \"Lo, a rabbit.\" But other translations would be compatible with all the evidence he has: \"Lo, food\"; \"Let's go hunting\"; \"There will be a storm tonight\" (these natives may be superstitious); \"Lo, a momentary rabbit-stage\"; \"Lo, an undetached rabbit-part.\" Some of these might become less likely – that is, become more unwieldy hypotheses – in the light of subsequent observation. Other translations can be ruled out only by querying the natives: An affirmative answer to \"Is this the same \"gavagai\" as that earlier one?\" rules out some possible translations. But these questions can only be asked once the linguist has mastered much of the natives' grammar and abstract vocabulary; that in turn can only be done on the basis of hypotheses derived from simpler, observation-connected bits of language; and those sentences, on their own, admit of multiple interpretations.\n\nThe situation is made worse when more abstract words are used, not directly attached to public observation:\nThese observations about the need for context brings up the next topic, holophrastic indeterminacy.\n\nIt is confusing that Quine's choice of meaning for 'holophrastic', contrasting it with sub-sentential phrases, appears to run counter to its accepted meaning in linguistics, \"expressing a complex of ideas in a single word or in a fixed phrase\".\nQuine considers the methods available to a field linguist attempting to translate a hitherto unknown language he calls \"Arunta\". He suggests that there are always different ways one might break a sentence into words, and different ways to distribute functions among words. Any hypothesis of translation could be defended only by appeal to context, by determining what other sentences a native would utter. But the same indeterminacy appears there: any hypothesis can be defended if one adopts enough compensatory hypotheses about other parts of the language.\n\nIndeterminacy of translation also applies to the interpretation of speakers of one's own language, and even to one's past utterances. This does not lead to skepticism about meaning – either that meaning is hidden and unknowable, \"or\" that words are meaningless. However, when combined with a (more or less behavioristic) premise that everything that can be learned about the meaning of a speaker's utterances can be learned from his behavior, the indeterminacy of translation may be felt to suggest that there are no such entities as \"meanings\"; in this connection, it is highlighted (or claimed) that the notion of synonymy has no operational definition. But saying that there are no \"meanings\" is not to say that words are not meaningful or significant.\n\nQuine denies an absolute standard of right and wrong in translating one language into another; rather, he adopts a pragmatic stance toward translation, that a translation can be consistent with the behavioral evidence. And while Quine does admit the existence of standards for good and bad translations, such standards are peripheral to his philosophical concern with the act of translation, hinging upon such pragmatic issues as speed of translation, and the lucidity and conciseness of the results. The key point is that more than one translation meets these criteria, and hence that no unique meaning can be assigned to words and sentences.\n\nIn Quine's view, the indeterminacy of translation leads to the inability to separate \"analytic\" statements whose validity lies in the usage of language from \"synthetic\" statements, those that assert facts about the world. The argument hinges on the role of synonymy in analytic statements, \"A natural suggestion, deserving close examination, is that the synonymy of two linguistic forms consists simply in their interchangeability in all contexts without change of truth value\". However, Quine argues, because of the indeterminacy of translation, any attempt to define 'analyticity' on a substitutional basis invariably introduces assumptions of the synthetic variety, resulting in a circular argument. Thus, this kind of substitutability does not provide an adequate explanation of synonyms.\n\n\n"}
{"id": "35661692", "url": "https://en.wikipedia.org/wiki?curid=35661692", "title": "Jamaican Country Sign Language", "text": "Jamaican Country Sign Language\n\nJamaican Country Sign Language, also Country Sign, or Konchri Sain (KS) in Jamaican Patois, is an indigenous village sign language of Jamaica. It is used by a small number of Deaf and hearing Jamaicans, spread over several communities in the rural south-western parish of St. Elizabeth.\n\nThe introduction of formal education for the St. Elizabeth deaf in 1975 by American Mennonite missionaries introduced two additional signed systems which have negatively affected KS: Signed English and American Sign Language. School officials strongly discouraged the use of the language inside and outside the classroom, resulting in a significant reduction in the number of fluent KS signers and a dramatic decline in the language's prestige. Thus, currently, KS is used primarily by elderly monolingual Deaf community members, while other community members use Jamaican Sign Language, a dialect of American Sign Language.\n\nA recent sociolinguistic survey reports that there are currently forty deaf adult KS signers on the island. The language will become extinct in the next twenty to thirty years, if deliberate effort is not taken to save it by means of an effective language planning strategy. Already, the University of the West Indies in conjunction with the University of Central London has begun working on a language documentation project for the language.\n\n\n"}
{"id": "13423808", "url": "https://en.wikipedia.org/wiki?curid=13423808", "title": "José de Fontes Pereira", "text": "José de Fontes Pereira\n\nJosé de Fontes Pereira (born 1838 in Luanda - died May 1891) was a radical Angolan lawyer-journalist and writer.\n\nConsidered an early Angolan nationalist and assimilado, Pereira took advantage of a relatively free press in Angola from 1870-1890 to question Portuguese obligations and control over Angola. He addressed topics such as the export of black Angolans to São Tomé and Príncipean plantations, coerced labor within the colony, inefficiency, corruption and racial discrimination, amongst many others.\n\nWriting for the literate Portuguese settler population of Angola, Pereira wrote mostly in the Portuguese language. Pereira lost his job and was put on trial when, in 1890, he suggested that the British should take over colonial administration of the colony due to Portuguese incompetence. He died 16 months later in May 1891 of natural causes.\n\n\n"}
{"id": "1231938", "url": "https://en.wikipedia.org/wiki?curid=1231938", "title": "Ján Valašťan Dolinský", "text": "Ján Valašťan Dolinský\n\nJán Valašťan Dolinský (15 February 1892, Békéscsaba (Békéšská Čaba) – 2 March 1965, Nitra) was a Slovak composer, teacher, journalist, esperantist and collector of folk songs.\n\nHe graduated from a teacher's institute and then worked as a teacher. From 1928 he was a teacher in Martin, where he simultaneously worked in the Music department of Matica slovenská.\n\n\nHe wrote music for poems of many well-known Slovak poets and it appeared under the name \"Vám všetkým\" (1922) and under other names\n\n\n\n\nHis original songs include:\nTranslations:\n"}
{"id": "27447087", "url": "https://en.wikipedia.org/wiki?curid=27447087", "title": "Kaduo language", "text": "Kaduo language\n\nKaduo (Khatu; ) is a Southern Loloish language spoken in Mojiang, Jiangcheng, Ning'er, Zhenyuan, and Xinping counties of Yunnan by about 20,000 people. Zhu (2011) covers the Kaduo dialect of Shilong Village 石龙村, Mengnong Ethnic Yi Township 孟弄彝族乡, Mojiang County 墨江县.\n\nIn Xinping County, Yunnan, Kaduo is spoken in (Zi 2011:11):\n\n"}
{"id": "26975322", "url": "https://en.wikipedia.org/wiki?curid=26975322", "title": "Khabibullin's conjecture on integral inequalities", "text": "Khabibullin's conjecture on integral inequalities\n\nIn mathematics, Khabibullin's conjecture, named after B. N. Khabibullin, is related to Paley's problem for plurisubharmonic functions and to various extremal problems in the theory of entire functions of several variables.\n\nKhabibullin's conjecture (version 1, 1992). \"Let formula_1 be a non-negative increasing function on the half-line formula_2 such that formula_3. Assume that formula_4 is a convex function of formula_5. Let formula_6, formula_7, and formula_8. If\" \n\"then\"\n\nThis statement of the Khabibullin's conjecture completes his survey.\n\nNote that the product in the right hand side of the inequality () is related to the Euler's Beta function formula_9:\n\nFor each fixed formula_6 the function\n\nturns the inequalities () and () to\nequalities. \n\nThe Khabibullin's conjecture is valid for formula_13 without the assumption of convexity of formula_14. Meanwhile, one can show that this conjecture is not valid without some convexity conditions for formula_15. In 2010, R. A. Sharipov showed that the conjecture fails in the case formula_16 and for formula_17.\n\nKhabibullin's conjecture (version 2). \"Let formula_18 be a non-negative increasing function on the half-line formula_2 and formula_20. If\" \n\n\"then\"\n\nKhabibullin's conjecture (version 3). \"Let formula_23 be a non-negative continuous function on the half-line formula_2 and formula_20. If\" \n\n\"then\"\n"}
{"id": "34643825", "url": "https://en.wikipedia.org/wiki?curid=34643825", "title": "Killer feature", "text": "Killer feature\n\nIn marketing terminology, a killer feature is any attribute of a product that proves so useful as to become the main attraction of the product. Examples include spreadsheets as a feature of the PC, or social networking as a feature of the Internet.\n\nAnalogous to killer application, which specifically refers to \"software as a killer feature\" of its compatible hardware (meaning that the software sells the hardware), the term \"killer feature\" often more specifically refers to software features that sell the software.\n\n"}
{"id": "34869283", "url": "https://en.wikipedia.org/wiki?curid=34869283", "title": "Kimaghama language", "text": "Kimaghama language\n\nKimaama, or Kimaghama, is a language spoken on Yos Sudarso Island in Papua province, Indonesia.\n"}
{"id": "23453254", "url": "https://en.wikipedia.org/wiki?curid=23453254", "title": "List of National Treasures of Japan (writings: others)", "text": "List of National Treasures of Japan (writings: others)\n\nThe term \"National Treasure\" has been used in Japan to denote cultural properties since 1897,\nalthough the definition and the criteria have changed since the introduction of the term. The written materials in the list adhere to the current definition, and have been designated National Treasures according to the Law for the Protection of Cultural Properties that came into effect on June 9, 1951. The items are selected by the Ministry of Education, Culture, Sports, Science and Technology based on their \"especially high historical or artistic value\". The list presents 104 entries from the Western Wei dynasty to the Meiji period with most dating to the period of Classical Japan and Mid-Imperial China from the 7th to 14th century. The total number of items is higher, however, since groups of related objects have been joined as single entries.\n\nThe list contains various types of written materials such as sutra copies, Buddhist commentaries and teachings, poetry and letters. Some of the designated objects originated in China, and were imported at a time when writing was being introduced to Japan. The items in this list were predominantly made with a writing brush on manuscript scrolls, which was the preferred medium until the advent of commercial printing and publishing in the 17th century. In many cases the manuscripts are noted examples of calligraphy. They are housed in temples, museums, libraries or archives, shrines, universities and in private collections. The writings in this list represent about half of the 228 National Treasures in the category \"writings\". They are complemented by 68 Japanese and 56 Chinese book National Treasures of the and the .\n\nThe table's columns (except for \"Remarks\" and \"Image\") are sortable pressing the arrows symbols. The following gives an overview of what is included in the table and how the sorting works.\n\nThe concept of writing came to Japan from the Korean kingdom of Baekje in the form of classical Chinese books and sutras, likely written on paper and in the form of manuscript rolls (\"kansubon\"). This probably happened at the beginning of the 5th century (around 400), and certainly in conjunction with the introduction of Buddhism in the 6th century. The increasing popularity of Buddhism, strongly promoted by Prince Shōtoku (574–622), in the late-6th century and early-7th century was one of the factors leading to a rise in the importance of writing. Buddhism required the study of sutras in Chinese. To satisfy the growing demand for them, imported Sui and Tang manuscripts were copied, first by Korean and Chinese immigrants, and later in the mid-7th century by Japanese scribes. The Sangyō Gisho (\"Annotated Commentaries on the Three Sutras\"), traditionally attributed to Prince Shōtoku, is the oldest extant Japanese text of any length. By 673 the entire Buddhist canon had been systematically copied. Not a single sutra survives from before the end of the 6th century. The oldest extant complete sutra copied in Japan dates to 686 and has been designated a National Treasure. During the 7th and 8th centuries, the copying of Buddhist texts, including sutras, dominated writing. Few Chinese secular or local Japanese works (which were rare) were copied. The state founded a Sutra Copying Bureau (\"shakyōjo\") before 727 with highly specialized calligraphers, proofreaders and metal polishers to satisfy the large demand for Buddhist texts. \nSutra copying was not only for duplication but also to acquire religious merit; thus nearly all Buddhist texts were hand-copied during the 8th century despite knowledge of printing.\n\nThe peak of sutra copying occurred in the Nara period at which time the Great Perfection of Wisdom (Daihannya) sutra and the Lotus Sutra were the sutras most often copied. Most of the sutras were written in black ink on paper dyed pale yellow. However, some were made with gold or silver ink on indigo, purple or other colored paper—particularly the ones that were produced in 741 when Emperor Shōmu decreed Konkōmyō Saishōō sutras written in gold letters be distributed among provincial temples. Many sutra copies contain a colophon with the name of the sponsor—often somebody from the ruling class—and the reason of copying, usually related to the health or salvation of people or the state.\n\nAfter the \"shakyōjo\" closed at the end of the 8th century, the imperial family and leading aristocrats continued to sponsor sutra copying. Because of an enhanced belief in the powers of the Lotus Sutra, more Heian period copies of this sutra exist than of all other sutras combined. Starting in the early Heian period, styles became flowery and ornate with lavish decorations as sutras were not used only in recitation but for dedication and sacrifice. Devotional sutra copying was more often undertaken by the initiator than in the Nara period. New forms of decoration came in fashion by the early-11th century including placing each character in the outline of a stupa, on lotus pedestals or next to depictions of Boddhisattvas. Sutras were increasingly furnished with frontispieces starting in the 11th century. Calligraphy shifted from Chinese to Japanese style. Sutra copying continued into the Kamakura and subsequent periods, but only rarely to comparable artistic effect. With the import of printed Song editions in the Kamakura period, hand-copying of the complete scriptures died out and sutra copying was only practiced for its devotional aspect. Forty-seven sutras or sets of sutras from the 6th century Western Wei to 14th century Nanboku-chō period have been designated National Treasures. Some of the oldest items in this list originated in China.\nNara period Buddhism was dominated by six state-controlled sects. They were introduced from the mainland and centred around the ancient capitals in Asuka and Nara. These schools were generally academic in nature, closely connected with the court and represented a doctrine that was far removed from the daily life of the people. In 804, two Japanese monks Kūkai and Saichō travelled to China; on their return they established Tendai and Shingon Buddhism respectively. Unlike their predecessors both esoteric schools took into account the needs of the common people. Though their origins lay in China, with time they acquired local Japanese traits. Generally the 9th century was a time when Chinese learning thrived in Japan. Authors produced a wide variety of works in Chinese language, including commentaries and treatises on a variety of subjects.\n\nA number of new sects appeared in Japan in the 12th and 13th centuries as a natural reaction to the difficult teachings of older schools and partially motivated by the notion of \"mappō\". Growing out of an Amida cult, the Jōdo Shinshū Pure Land school was founded in 1224 by Shinran, and attracted a following from all classes and occupations. Three years later, Dōgen introduced the Sōtō school of Zen Buddhism emphasizing meditation and dharma practice. The first truly Japanese school of Buddhism goes back to Nichiren's proclamation of his teachings in 1253. Nichiren Buddhism was exceptional for being militant and intolerant. The central focus of Nichiren's teaching was the veneration of the Lotus Sutra.\n\nFourteen treatises and commentaries of famous Japanese monks dating from the early Heian to the Kamakura period have been designated as National Treasures. These include three commentaries by Kūkai on two of the main mantras (Dainichikyō and Kongōhannyakyō) of Shingon Buddhism, works by Shinran discussing Pure Land Buddhism, mappō and Amida, a manual on zazen \"seated meditation\" by Dōgen and two works by Nichiren related to his teachings. In addition two large scale collection of documents from the Nara to the Meiji period are listed here as National Treasures.\n\nBokuseki is a type of Japanese calligraphy practiced by Zen monks or lay practitioners of Zen meditation. Characterised by freely written bold characters, the style often ignores criteria and classical standards for calligraphy. The brush is moved continuously across the paper creating richly variated lines. Unlike other calligraphy, bokuseki is considered \"religious art\"—a manifestation of the artist's understanding of the Dharma. In this sense, the literal meaning of the word \"bokuseki\", translated as \"ink trace\", indicates the piece is considered to be a trace of the enlightened mind.\n\nThe bokuseki style developed from Song Dynasty calligraphy. It was brought from China to Japan, together with Zen Buddhism, starting with Eisai in 1191. Late-12th century works imported from China were highly regarded in Japan; subsequently Japanese priests began producing their own bokuseki in the 13th and 14th centuries. Later bokuseki became part of the zen practice and served as meditation help. They were often mounted on hanging scrolls, and displayed in temples and tea rooms. The master of the Japanese tea ceremony Sen no Rikyū considered them crucial to the tea ceremony in the sense that they put the participants in the right frame-of-mind. Bokuseki gained in importance through the chanoyu in the Muromachi and Momoyama periods. Daitō Kokushi and Musō Soseki, both from the Rinzai school of Zen Buddhism, were the most famous bokuseki masters of the time.\n\nThe bokuseki style is present in a variety of Zen genres such as Buddhist sermons or Dharma talks (hōgo), certificates of enlightenment (inkajō), death verses (yuige), gatha verses (geju), poetry (shi), letters, names and titles given to a monk by his master (jigo), exhortory sermons (shidōgo), gakuji, inscriptions on Zen paintings (san) and Zen circles. There are 23 bokuseki National Treasures of various types including inkajō, hōgo, letters and yuige. They date from the 12th to 14th centuries and have been mounted on hanging scrolls.\n\nKaishi, or futokorogami, were sheets of paper carried by high-ranking people folded in their kimonos at the breast. They were used for writing letters, or waka; similar sheets were employed during the tea ceremony. Papers came in a variety of sizes and colours, depending on the rank and sex of those using them. At court men wrote on white paper, while women wrote only on red kaishi paper. Eventually the paper format was standardized with sizes ranging from about to . The folding style, labelling, and other stylistic features, differed from school to school. Four items from the Heian and Kamakura periods have been designated as National Treasures in the kaishi category. They are single sheets or sets of sheets mounted on hanging scrolls or bound in an album and contain poetry by Japanese rulers and famous poets.\n\nCollections of exemplary calligraphy, or \"tekagami\" (lit. \"mirror of the hands\"), were created by cutting pages and sections of old books and scrolls of sutras, poems and letters, which were arranged in albums in a chronological order or according to social status. By the early-16th century, calligraphic connoisseurs of the Kohitsu house had practiced activities aimed at preserving ancient calligraphic works. \"Tekagami\" production appears to have started in the Momoyama period. These albums served as model books for calligraphy practice, the emulation of old styles, and as reference works for authentication in the growing antique market. Today, the selection of calligraphers, and the type of calligraphies in a \"tekagami\", show the changing tastes in classical Japanese-style calligraphy over the years. Four \"tekagami\" containing works from the 8th century Nara to the 15th century Muromachi period have been designated as National Treasures.\nIn Japanese calligraphy the term originally referred to works by ancient calligraphers, or poets, on scrolls or bound books, created from between the 8th to 15th centuries. In today's use, the term mainly describes copies of poetry anthologies from the Heian to mid-Kamakura period. Since they were made as artful daily items for the nobility, in addition to having a beautiful script, attention was given to the choice of paper (which was often decorated), the binding, mountings and even accompanying boxes. Stylistically, kohitsu were written in Japanese kana in cursive script (\"sōgana\"). In the Momoyama and early Edo period, surviving kohitsu were often cut (kohitsu-gire), mounted on hanging scrolls and displayed in a tea room. Five scrolls of kohitsu poetry collections from the mid-Heian period have been designated as National Treasures. They were made by two calligraphers: Fujiwara no Yukinari and Ono no Michikaze.\nThere are three National Treasures writings that do not fit in any of the above categories, all originating in China. Two are 7th century works: a copy of the Thousand Character Classic by Zhi Yong both in formal and cursive scripts, and a tracing copy of a letter by the famous Chinese calligrapher Wang Xizhi. The former work is said to have been imported to Japan by the legendary scholar Wani in ancient times. One is a 13th-century set of large-scale letters (2 or 3 each) to be displayed on walls or above doorways.\n\n\n"}
{"id": "32714036", "url": "https://en.wikipedia.org/wiki?curid=32714036", "title": "Lorenzo Hammarsköld", "text": "Lorenzo Hammarsköld\n\nLorenzo Hammarsköld (b. \"Lars Hammarskjöld\", 7 April 1785 – 15 October 1827) was a Swedish critic and literary historian. He also published poetry.\n\nHe was born at Tuna, Vimmerby. He became a student at Uppsala in 1801, but failed to take his degree in 1806. He therefore accepted a humble post at the royal library at Stockholm, with which institution he remained connected for many years.\n\nIn 1804 he published an article on Ludwig Tieck and Novalis, which attracted much attention, and was the means of founding the “Phosphoric School,” as it was called, of poetry in Sweden. Hammarsköld became the friend of Atterbom and antagonist of Wallmark, and in due time, by the bitterness of his tone, brought down on himself the scathing anger of Esaias Tegnér. In 1806 he published \"Translations and Imitations of Poets, Old and New\", in the preface of which he denounced the classic Swedish writers with much force and wit, commending Goethe and Tieck to the young poets of the day.\n\nIn 1803 appeared his \"Critique of Schiller\", and in 1810 a volume of essays of a polemical kind. In 1813 Hammarsköld published a collection of his poems, and in 1815 had to endure the ridicule of Tegnér's satire \"Hammarspik\". In 1818 appeared the first part of Hammarsköld's chief contribution to literature, his famous \"Svenska Vitterheten\", a history of polite letters in Sweden, a book that was revised and republished after his death by Sondén, in 1833.\n\nHammarsköld was among the first to recognise the talent of the poet Erik Johan Stagnelius. He collected the works of the deceased poet and published his poems in 1824-26.\n"}
{"id": "38911444", "url": "https://en.wikipedia.org/wiki?curid=38911444", "title": "Manifesto Blanco", "text": "Manifesto Blanco\n\nThe Manifiesto blanco, or White Manifesto was written in 1946 by artists and students in Buenos Aires under the direction of Lucio Fontana during his time in Argentina. The manifesto emphasized the importance of new technologies as they pertained to the arts and the pursuit of integrating art and science. \"This idea of synthesis is grounded in the concept that certain scientific and philosophical developments have transformed the human psyche to such an extent that traditional \"static\" art forms and the differentiation of artistic disciplines have become obsolete. A new, synthetic art is advocated that will involve the dynamic principle of movement through time and space.\"\n"}
{"id": "38360943", "url": "https://en.wikipedia.org/wiki?curid=38360943", "title": "Milling (machining)", "text": "Milling (machining)\n\nMilling is the process of machining using rotary cutters to remove material by advancing a cutter into a workpiece. This may be done varying direction on one or several axes, cutter head speed, and pressure. Milling covers a wide variety of different operations and machines, on scales from small individual parts to large, heavy-duty gang milling operations. It is one of the most commonly used processes for machining custom parts to precise tolerances.\n\nMilling can be done with a wide range of machine tools. The original class of machine tools for milling was the milling machine (often called a mill). After the advent of computer numerical control (CNC), milling machines evolved into machining centers: milling machines augmented by automatic tool changers, tool magazines or carousels, CNC capability, coolant systems, and enclosures. Milling centers are generally classified as vertical machining centers (VMCs) or horizontal machining centers (HMCs). \n\nThe integration of milling into turning environments, and vice versa, began with live tooling for lathes and the occasional use of mills for turning operations. This led to a new class of machine tools, multitasking machines (MTMs), which are purpose-built to facilitate milling and turning within the same work envelope.\n\nMilling is a cutting process that uses a milling cutter to remove material from the surface of a workpiece. The milling cutter is a rotary cutting tool, often with multiple cutting points. As opposed to drilling, where the tool is advanced along its rotation axis, the cutter in milling is usually moved perpendicular to its axis so that cutting occurs on the circumference of the cutter. As the milling cutter enters the workpiece, the cutting edges (flutes or teeth) of the tool repeatedly cut into and exit from the material, shaving off chips (swarf) from the workpiece with each pass. The cutting action is shear deformation; material is pushed off the workpiece in tiny clumps that hang together to a greater or lesser extent (depending on the material) to form chips. This makes metal cutting somewhat different (in its mechanics) from slicing softer materials with a blade.\n\nThe milling process removes material by performing many separate, small cuts. This is accomplished by using a cutter with many teeth, spinning the cutter at high speed, or advancing the material through the cutter slowly; most often it is some combination of these three approaches. The speeds and feeds used are varied to suit a combination of variables. The speed at which the piece advances through the cutter is called feed rate, or just feed; it is most often measured in length of material per full revolution of the cutter.\n\nThere are two major classes of milling process:\n\nMany different types of cutting tools are used in the milling process. Milling cutters such as endmills may have cutting surfaces across their entire end surface, so that they can be drilled into the workpiece (plunging). Milling cutters may also have extended cutting surfaces on their sides to allow for peripheral milling. Tools optimized for face milling tend to have only small cutters at their end corners.\n\nThe cutting surfaces of a milling cutter are generally made of a hard and temperature-resistant material, so that they wear slowly. A low cost cutter may have surfaces made of high speed steel. More expensive but slower-wearing materials include cemented carbide. Thin film coatings may be applied to decrease friction or further increase hardness.\n\nThey are cutting tools typically used in milling machines or machining centres to perform milling operations (and occasionally in other machine tools). They remove material by their movement within the machine (e.g., a ball nose mill) or directly from the cutter's shape (e.g., a form tool such as a hobbing cutter).\nAs material passes through the cutting area of a milling machine, the blades of the cutter take swarfs of material at regular intervals. Surfaces cut by the side of the cutter (as in peripheral milling) therefore always contain regular ridges. The distance between ridges and the height of the ridges depend on the feed rate, number of cutting surfaces, the cutter diameter. With a narrow cutter and rapid feed rate, these revolution ridges can be significant variations in the surface finish.\nThe face milling process can in principle produce very flat surfaces. However, in practice the result always shows visible trochoidal marks following the motion of points on the cutter's end face. These revolution marks give the characteristic finish of a face milled surface. Revolution marks can have significant roughness depending on factors such as flatness of the cutter's end face and the degree of perpendicularity between the cutter's rotation axis and feed direction. Often a final pass with a slow feed rate is used to improve the surface finish after the bulk of the material has been removed.. In a precise face milling operation, the revolution marks will only be microscopic scratches due to imperfections in the cutting edge.\nGang milling refers to the use of two or more milling cutters mounted on the same arbor (that is, ganged) in a horizontal-milling setup. All of the cutters may perform the same type of operation, or each cutter may perform a different type of operation. For example, if several workpieces need a slot, a flat surface, and an angular groove, a good method to cut these (within a non-CNC context) would be gang milling. All the completed workpieces would be the same, and milling time per piece would be minimized.\n\nGang milling was especially important before the CNC era, because for duplicate part production, it was a substantial efficiency improvement over manual-milling one feature at an operation, then changing machines (or changing setup of the same machine) to cut the next op. Today, CNC mills with automatic tool change and 4- or 5-axis control obviate gang-milling practice to a large extent.\n\nMilling is performed with a milling cutter in various forms, held in a collett or similar which, in turn, is held in the spindle of a milling machine.\n\nMill orientation is the primary classification for milling machines. The two basic configurations are vertical and horizontal. However, there are alternative classifications according to method of control, size, purpose and power source.\n\nIn the vertical mill the spindle axis is vertically oriented. Milling cutters are held in the spindle and rotate on its axis. The spindle can generally be extended (or the table can be raised/lowered, giving the same effect), allowing plunge cuts and drilling. There are two subcategories of vertical mills: the bed mill and the turret mill.\n\n\nTurret mills are generally considered by some to be more versatile of the two designs. However, turret mills are only practical as long as the machine remains relatively small. As machine size increases, moving the knee up and down requires considerable effort and it also becomes difficult to reach the quill feed handle (if equipped). Therefore, larger milling machines are usually of the bed type.\n\nA third type also exists, a lighter machine, called a mill-drill, which is a close relative of the vertical mill and quite popular with hobbyists. A mill-drill is similar in basic configuration to a small drill press, but equipped with an X-Y table. They also typically use more powerful motors than a comparably sized drill press, with potentiometer-controlled speed and generally have more heavy-duty spindle bearings than a drill press to deal with the lateral loading on the spindle that is created by a milling operation. A mill drill also typically raises and lowers the entire head, including motor, often on a dovetailed vertical, where a drill press motor remains stationary, while the arbor raises and lowers within a driving collar. Other differences that separate a mill-drill from a drill press may be a fine tuning adjustment for the Z-axis, a more precise depth stop, the capability to lock the X, Y or Z axis, and often a system of tilting the head or the entire vertical column and powerhead assembly to allow angled cutting. Aside from size and precision, the principal difference between these hobby-type machines and larger true vertical mills is that the X-Y table is at a fixed elevation; the Z-axis is controlled in basically the same fashion as drill press, where a larger vertical or knee mill has a vertically fixed milling head, and changes the X-Y table elevation. As well, a mill-drill often uses a standard drill press-type Jacob's chuck, rather than an internally tapered arbor that accepts collets. These are frequently of lower quality than other types of machines, but still fill the hobby role well because they tend to be benchtop machines with small footprints and modest price tags.\n\nA horizontal mill has the same sort but the cutters are mounted on a horizontal spindle (see Arbor milling) across the table. Many horizontal mills also feature a built-in rotary table that allows milling at various angles; this feature is called a \"universal table\". While endmills and the other types of tools available to a vertical mill may be used in a horizontal mill, their real advantage lies in arbor-mounted cutters, called side and face mills, which have a cross section rather like a circular saw, but are generally wider and smaller in diameter. Because the cutters have good support from the arbor and have a larger cross-sectional area than an end mill, quite heavy cuts can be taken enabling rapid material removal rates. These are used to mill grooves and slots. Plain mills are used to shape flat surfaces. Several cutters may be ganged together on the arbor to mill a complex shape of slots and planes. Special cutters can also cut grooves, bevels, radii, or indeed any section desired. These specialty cutters tend to be expensive. Simplex mills have one spindle, and duplex mills have two. It is also easier to cut gears on a horizontal mill. Some horizontal milling machines are equipped with a power-take-off provision on the table. This allows the table feed to be synchronized to a rotary fixture, enabling the milling of spiral features such as hypoid gears.\n\nThe choice between vertical and horizontal spindle orientation in milling machine design usually hinges on the shape and size of a workpiece and the number of sides of the workpiece that require machining. Work in which the spindle's axial movement is normal to one plane, with an endmill as the cutter, lends itself to a vertical mill, where the operator can stand before the machine and have easy access to the cutting action by looking down upon it. Thus vertical mills are most favored for diesinking work (machining a mould into a block of metal). Heavier and longer workpieces lend themselves to placement on the table of a horizontal mill.\n\nPrior to numerical control, horizontal milling machines evolved first, because they evolved by putting milling tables under lathe-like headstocks. Vertical mills appeared in subsequent decades, and accessories in the form of add-on heads to change horizontal mills to vertical mills (and later vice versa) have been commonly used. Even in the CNC era, a heavy workpiece needing machining on multiple sides lends itself to a horizontal machining center, while diesinking lends itself to a vertical one.\n\nIn addition to horizontal versus vertical, other distinctions are also important:\n\n\nA milling machine is often called a mill by machinists. The archaic term miller was commonly used in the 19th and early 20th centuries.\n\nSince the 1960s there has developed an overlap of usage between the terms milling machine and machining center. NC/CNC machining centers evolved from milling machines, which is why the terminology evolved gradually with considerable overlap that still persists. The distinction, when one is made, is that a machining center is a mill with features that pre-CNC mills never had, especially an automatic tool changer (ATC) that includes a tool magazine (carousel), and sometimes an automatic pallet changer (APC). In typical usage, all machining centers are mills, but not all mills are machining centers; only mills with ATCs are machining centers.\n\nMost CNC milling machines (also called \"machining centers\") are computer controlled vertical mills with the ability to move the spindle vertically along the Z-axis. This extra degree of freedom permits their use in diesinking, engraving applications, and 2.5D surfaces such as relief sculptures. When combined with the use of conical tools or a ball nose cutter, it also significantly improves milling precision without impacting speed, providing a cost-efficient alternative to most flat-surface hand-engraving work.\n\nCNC machines can exist in virtually any of the forms of manual machinery, like horizontal mills. The most advanced CNC milling-machines, the multiaxis machine, add two more axes in addition to the three normal axes (XYZ). Horizontal milling machines also have a C or Q axis, allowing the horizontally mounted workpiece to be rotated, essentially allowing asymmetric and eccentric turning. The fifth axis (B axis) controls the tilt of the tool itself. When all of these axes are used in conjunction with each other, extremely complicated geometries, even organic geometries such as a human head can be made with relative ease with these machines. But the skill to program such geometries is beyond that of most operators. Therefore, 5-axis milling machines are practically always programmed with CAM.\n\nThe operating system of such machines is a closed loop system and functions on feedback.\nThese machines have developed from the basic NC (NUMERIC CONTROL) machines. A computerized form of NC machines is known as CNC machines. A set of instructions (called a program) is used to guide the machine for desired operations. Some very commonly used codes, which are used in the program are:\n\nVarious other codes are also used. A CNC machine is operated by a single operator called a programmer. This machine is capable of performing various operations automatically and economically.\n\nWith the declining price of computers and open source CNC software, the entry price of CNC machines has plummeted.\n\nThe accessories and cutting tools used on machine tools (including milling machines) are referred to in aggregate by the mass noun \"tooling\". There is a high degree of standardization of the tooling used with CNC milling machines, and a lesser degree with manual milling machines. To ease up the organization of the tooling in CNC production many companies use a tool management solution.\n\nMilling cutters for specific applications are held in various tooling configurations.\n\nCNC milling machines nearly always use SK (or ISO), CAT, BT or HSK tooling. SK tooling is the most common in Europe, while CAT tooling, sometimes called V-Flange Tooling, is the oldest and probably most common type in the USA. CAT tooling was invented by Caterpillar Inc. of Peoria, Illinois, in order to standardize the tooling used on their machinery. CAT tooling comes in a range of sizes designated as CAT-30, CAT-40, CAT-50, etc. The number refers to the Association for Manufacturing Technology (formerly the National Machine Tool Builders Association (NMTB)) Taper size of the tool.\n\nAn improvement on CAT Tooling is BT Tooling, which looks similar and can easily be confused with CAT tooling. Like CAT Tooling, BT Tooling comes in a range of sizes and uses the same NMTB body taper. However, BT tooling is symmetrical about the spindle axis, which CAT tooling is not. This gives BT tooling greater stability and balance at high speeds. One other subtle difference between these two toolholders is the thread used to hold the pull stud. CAT Tooling is all Imperial thread and BT Tooling is all Metric thread. Note that this affects the pull stud only; it does not affect the tool that they can hold. Both types of tooling are sold to accept both Imperial and metric sized tools.\n\nSK and HSK tooling, sometimes called \"Hollow Shank Tooling\", is much more common in Europe where it was invented than it is in the United States. It is claimed that HSK tooling is even better than BT Tooling at high speeds. The holding mechanism for HSK tooling is placed within the (hollow) body of the tool and, as spindle speed increases, it expands, gripping the tool more tightly with increasing spindle speed. There is no pull stud with this type of tooling.\n\nFor manual milling machines, there is less standardization, because a greater plurality of formerly competing standards exist. Newer and larger manual machines usually use NMTB tooling. This tooling is somewhat similar to CAT tooling but requires a drawbar within the milling machine. Furthermore, there are a number of variations with NMTB tooling that make interchangeability troublesome. The older a machine, the greater the plurality of standards that may apply (e.g., Morse, Jarno, Brown & Sharpe, Van Norman, and other less common builder-specific tapers). However, two standards that have seen especially wide usage are the Morse #2 and the R8, whose prevalence was driven by the popularity of the mills built by Bridgeport Machines of Bridgeport, Connecticut. These mills so dominated the market for such a long time that \"Bridgeport\" is virtually synonymous with \"manual milling machine\". Most of the machines that Bridgeport made between 1938 and 1965 used a Morse taper #2, and from about 1965 onward most used an R8 taper.\n\n\nPocket milling has been regarded as one of the most widely used operations in machining. It is extensively used in aerospace and shipyard industries. In pocket milling the material inside an arbitrarily closed boundary on a flat surface of a work piece is removed to a fixed depth. Generally flat bottom end mills are used for pocket milling. Firstly roughing operation is done to remove the bulk of material and then the pocket is finished by a finish end mill.\nMost of the industrial milling operations can be taken care of by 2.5 axis CNC milling. This type of path control can machine up to 80% of all mechanical parts. Since the importance of pocket milling is very relevant, therefore effective pocketing approaches can result in reduction in machining time and cost.\nNC pocket milling can be carried out mainly by two tool paths, viz. linear and non-linear.\n\nIn this approach, the tool movement is unidirectional. Zig-zag and zig tool paths are the examples of linear tool path.\n\nIn zig-zag milling, material is removed both in forward and backward paths. In this case, cutting is done both with and against the rotation of the spindle. This reduces the machining time but increases machine chatter and tool wear.\n\nIn zig milling, the tool moves only in one direction. The tool has to be lifted and retracted after each cut, due to which machining time increases. However, in case of zig milling surface quality is better.\n\nIn this approach, tool movement is multi-directional. One example of non-linear tool path is contour-parallel tool path.\n\nIn this approach, the required pocket boundary is used to derive the tool path. In this case the cutter is always in contact with the work material. Hence the idle time spent in positioning and retracting the tool is avoided. For large-scale material removal, contour-parallel tool path is widely used because it can be consistently used with up-cut or down-cut method during the entire process. There are three different approaches that fall into the category of contour-parallel tool path generation. They are:\n\n\nIn this approach, the tool travels along a gradually evolving spiral path. The spiral starts at the center of the pocket to be machined and the tool gradually moves towards the pocket boundary. The direction of the tool path changes progressively and local acceleration and deceleration of the tool are minimized. This reduces tool wear.\n\nMilling machines evolved from the practice of rotary filing—that is, running a circular cutter with file-like teeth in the headstock of a lathe. Rotary filing and, later, true milling were developed to reduce time and effort spent hand-filing. The full story of milling machine development may never be known, because much early development took place in individual shops where few records were kept for posterity. However, the broad outlines are known, as summarized below. From a history-of-technology viewpoint, it is clear that the naming of this new type of machining with the term \"milling\" was an extension from that word's earlier senses of processing materials by abrading them in some way (cutting, grinding, crushing, etc.).\nRotary filing long predated milling. A rotary file by Jacques de Vaucanson, circa 1760, is well known.\n\nIn 1795, Eli Terry began using a milling machine at Plymouth Connecticut in the production of tall case clocks. With the use of his milling machine, Terry was the first to accomplish Interchangeable parts in the clock industry. Milling wooden parts was efficient in interchangeable parts, but inefficient in high yields. Milling wooden blanks results in a low yield of parts because the machines single blade would cause loss of gear teeth when the cutter hit parallel grains in the wood. Terry later invented a spindle cutting machine to mass produce parts in 1807. Other Connecticut clockmakers like James Harrison of Waterbury, Thomas Barnes of Litchfield, and Gideon Roberts of Bristol, also used milling machines to produce their clocks. \n\nIt is clear that milling machines as a distinct class of machine tool (separate from lathes running rotary files) first appeared between 1814 and 1818. The centers of earliest development of true milling machines were two federal armories of the U.S. (Springfield and Harpers Ferry) together with the various private armories and inside contractors that shared turnover of skilled workmen with them. \nBetween 1912 and 1916, Joseph W. Roe, a respected founding father of machine tool historians, credited Eli Whitney (one of the private arms makers mentioned above) with producing the first true milling machine. By 1918, he considered it \"Probably the first milling machine ever built—certainly the oldest now in existence […].\" However, subsequent scholars, including Robert S. Woodbury and others, have improved upon Roe's early version of the history and suggest that just as much credit—in fact, probably more—belongs to various other inventors, including Robert Johnson of Middletown, Connecticut; Captain John H. Hall of the Harpers Ferry armory; Simeon North of the Staddle Hill factory in Middletown; Roswell Lee of the Springfield armory; and Thomas Blanchard. (Several of the men mentioned above are sometimes described on the internet as \"the inventor of the first milling machine\" or \"the inventor of interchangeable parts\". Such claims are oversimplified, as these technologies evolved over time among many people.)\n\nPeter Baida, citing Edward A. Battison's article \"Eli Whitney and the Milling Machine,\" which was published in the \"Smithsonian Journal of History\" in 1966, exemplifies the dispelling of the \"Great Man\" image of Whitney by historians of technology working in the 1950s and 1960s. He quotes Battison as concluding that \"There is no evidence that Whitney developed or used a true milling machine.\" Baida says, \"The so-called Whitney machine of 1818 seems actually to have been made after Whitney's death in 1825.\" Baida cites Battison's suggestion that the first true milling machine was made not by Whitney, but by Robert Johnson of Middletown.\n\nThe late teens of the 19th century were a pivotal time in the history of machine tools, as the period of 1814 to 1818 is also the period during which several contemporary pioneers (Fox, Murray, and Roberts) were developing the planer, and as with the milling machine, the work being done in various shops was undocumented for various reasons (partially because of proprietary secrecy, and also simply because no one was taking down records for posterity).\n\nJames Nasmyth built a milling machine very advanced for its time between 1829 and 1831. It was tooled to mill the six sides of a hex nut that was mounted in a six-way indexing fixture.\n\nA milling machine built and used in the shop of Gay & Silver (aka Gay, Silver, & Co) in the 1830s was influential because it employed a better method of vertical positioning than earlier machines. For example, Whitney's machine (the one that Roe considered the very first) and others did not make provision for vertical travel of the knee. Evidently, the workflow assumption behind this was that the machine would be set up with shims, vise, etc. for a certain part design, and successive parts did not require vertical adjustment (or at most would need only shimming). This indicates that early thinking about milling machines was as production machines, not toolroom machines.\n\nIn these early years, milling was often viewed as only a roughing operation to be followed by finishing with a hand file. The idea of \"reducing\" hand filing was more important than \"replacing\" it.\n\nSome of the key men in milling machine development during this era included Frederick W. Howe, Francis A. Pratt, Elisha K. Root, and others. (These same men during the same era were also busy developing the state of the art in turret lathes. Howe's experience at Gay & Silver in the 1840s acquainted him with early versions of both machine tools. His machine tool designs were later built at Robbins & Lawrence, the Providence Tool Company, and Brown & Sharpe.) The most successful milling machine design to emerge during this era was the , which rather than being a specific make and model of machine tool is truly a family of tools built by various companies on a common configuration over several decades. It took its name from the first company to put one on the market, George S. Lincoln & Company (formerly the Phoenix Iron Works), whose first one was built in 1855 for the Colt armory.\n\nDuring this era there was a continued blind spot in milling machine design, as various designers failed to develop a truly simple and effective means of providing slide travel in all three of the archetypal milling axes (X, Y, and Z—or as they were known in the past, longitudinal, traverse, and vertical). Vertical positioning ideas were either absent or underdeveloped. The Lincoln miller's spindle could be raised and lowered, but the original idea behind its positioning was to be set up in position and then run, as opposed to being moved frequently while running. Like a turret lathe, it was a repetitive-production machine, with each skilled setup followed by extensive fairly low skill operation.\n\nIn 1861, Frederick W. Howe, while working for the Providence Tool Company, asked Joseph R. Brown of Brown & Sharpe for a solution to the problem of milling spirals, such as the flutes of twist drills. These were usually filed by hand at the time. (Helical planing existed but was by no means common.) Brown designed a \"universal milling machine\" that, starting from its first sale in March 1862, was wildly successful. It solved the problem of 3-axis travel (i.e., the axes that we now call XYZ) much more elegantly than had been done in the past, and it allowed for the milling of spirals using an indexing head fed in coordination with the table feed. The term \"universal\" was applied to it because it was ready for any kind of work, including toolroom work, and was not as limited in application as previous designs. (Howe had designed a \"universal miller\" in 1852, but Brown's of 1861 is the one considered a groundbreaking success.)\n\nBrown also developed and patented (1864) the design of formed milling cutters in which successive sharpenings of the teeth do not disturb the geometry of the form.\n\nThe advances of the 1860s opened the floodgates and ushered in modern milling practice.\n\nIn these decades, Brown & Sharpe and the Cincinnati Milling Machine Company dominated the milling machine field. However, hundreds of other firms also built milling machines at the time, and many were significant in various ways. Besides a wide variety of specialized production machines, the archetypal multipurpose milling machine of the late 19th and early 20th centuries was a heavy knee-and-column horizontal-spindle design with power table feeds, indexing head, and a stout overarm to support the arbor. The evolution of machine design was driven not only by inventive spirit but also by the constant evolution of milling cutters that saw milestone after milestone from 1860 through World War I.\n\nAround the end of World War I, machine tool control advanced in various ways that laid the groundwork for later CNC technology. The jig borer popularized the ideas of coordinate dimensioning (dimensioning of all locations on the part from a single reference point); working routinely in \"tenths\" (ten-thousandths of an inch, 0.0001\") as an everyday machine capability; and using the control to go straight from drawing to part, circumventing jig-making. In 1920 the new tracer design of J.C. Shaw was applied to Keller tracer milling machines for die-sinking via the three-dimensional copying of a template. This made diesinking faster and easier just as dies were in higher demand than ever before, and was very helpful for large steel dies such as those used to stamp sheets in automobile manufacturing. Such machines translated the tracer movements to input for servos that worked the machine leadscrews or hydraulics. They also spurred the development of antibacklash leadscrew nuts. All of the above concepts were new in the 1920s but became routine in the NC/CNC era. By the 1930s, incredibly large and advanced milling machines existed, such as the Cincinnati Hydro-Tel, that presaged today's CNC mills in every respect except for CNC control itself.\n\nIn 1936, Rudolph Bannow (1897–1962) conceived of a major improvement to the milling machine. His company commenced manufacturing a new knee-and-column vertical mill in 1938. This was the Bridgeport milling machine, often called a ram-type or turret-type mill because its head has sliding-ram and rotating-turret mounting. The machine became so popular that many other manufacturers created copies and variants. Furthermore, its name came to connote any such variant. The Bridgeport offered enduring advantages over previous models. It was small enough, light enough, and affordable enough to be a practical acquisition for even the smallest machine shop businesses, yet it was also smartly designed, versatile, well-built, and rigid. Its various directions of sliding and pivoting movement allowed the head to approach the work from any angle. The Bridgeport's design became the dominant form for manual milling machines used by several generations of small- and medium-enterprise machinists. By the 1980s an estimated quarter-million Bridgeport milling machines had been built, and they (and their clones) are still being produced today. \n\nBy 1940, automation via cams, such as in screw machines and automatic chuckers, had already been very well developed for decades. Beginning in the 1930s, ideas involving servomechanisms had been in the air, but it was especially during and immediately after World War II that they began to germinate (see also Numerical control > History). These were soon combined with the emerging technology of digital computers. This technological development milieu, spanning from the immediate pre–World War II period into the 1950s, was powered by the military capital expenditures that pursued contemporary advancements in the directing of gun and rocket artillery and in missile guidance—other applications in which humans wished to control the kinematics/dynamics of large machines quickly, precisely, and automatically. Sufficient R&D spending probably would not have happened within the machine tool industry alone; but it was for the latter applications that the will and ability to spend was available. Once the development was underway, it was eagerly applied to machine tool control in one of the many post-WWII instances of technology transfer.\n\nIn 1952, numerical control reached the developmental stage of laboratory reality. The first NC machine tool was a Cincinnati Hydrotel milling machine retrofitted with a scratch-built NC control unit. It was reported in \"Scientific American\", just as another groundbreaking milling machine, the Brown & Sharpe universal, had been in 1862.\n\nDuring the 1950s, numerical control moved slowly from the laboratory into commercial service. For its first decade, it had rather limited impact outside of aerospace work. But during the 1960s and 1970s, NC evolved into CNC, data storage and input media evolved, computer processing power and memory capacity steadily increased, and NC and CNC machine tools gradually disseminated from an environment of huge corporations and mainly aerospace work to the level of medium-sized corporations and a wide variety of products. NC and CNC's drastic advancement of machine tool control deeply transformed the culture of manufacturing. The details (which are beyond the scope of this article) have evolved immensely with every passing decade.\n\nComputers and CNC machine tools continue to develop rapidly. The personal computer revolution has a great impact on this development. By the late 1980s small machine shops had desktop computers and CNC machine tools. Soon after, hobbyists, artists, and designers began obtaining CNC mills and lathes. Manufacturers have started producing economically priced CNCs machines small enough to sit on a desktop which can cut at high resolution materials softer than stainless steel. They can be used to make anything from jewelry to printed circuit boards to gun parts, even fine art.\n\nNational and international standards are used to standardize the definitions, environmental requirements, and test methods used for milling. Selection of the standard to be used is an agreement between the supplier and the user and has some significance in the design of the mill. In the United States, ASME has developed the standards B5.45-1972 \"Milling Machines\" and B94.19-1997 \"Milling Cutters and End Mills\".\n\nGeneral tolerances include: +/-.005\" for local tolerances across most geometries, +/-.010\" for plastics with variation depending on the size of the part, 0.030\" minimum wall thickness for metals, and 0.060\" minimum wall thickness for plastics.\n\n\n"}
{"id": "28388124", "url": "https://en.wikipedia.org/wiki?curid=28388124", "title": "Mini-Estrella", "text": "Mini-Estrella\n\nThe term Mini-Estrella (Spanish for \"Mini-Star\") is used in lucha libre to describe a division of short professional wrestlers or \"luchadors\", some of whom have dwarfism. The Mexican \"Mini-Estrellas\" is comparable to Midget wrestling practiced around the world, but with the notable exception that some of the \"Mini-Estrellas\" do not have dwarfism but are simply short. Some \"Mini-Estrellas\" have later on moved on to work as regular sized competitors. The \"Mini-Estrellas\" have been featured in several promotions outside Mexico, most notably World Wrestling Entertainment (WWE) and Total Nonstop Action Wrestling (TNA).\n\nOriginally the limit for the Mini division was but in recent years wrestlers such as Pequeño Olímpico have worked the Minis division despite being tall. In the formative years of the \"Mini-Estrellas\" history they were also referred to as \"Micro Luchadors\", or \"Micro Wrestlers\".\n\nThe origins of the \"Mini-Estrella\" division lies in Midget wrestling, which in Mexico was popularized in the 1970s when promoters used the American concept and had a number of Mexican little people perform as a \"special attraction\" on lucha libre shows. In the early days saw the popularity of wrestlers such as Gran Nikolai, Pequeno Goliath and Arturito (inspired by R2-D2), especially with the children. By the 1980s midget wrestling was less popular in Mexico, especially since few new wrestlers had joined the division.\n\nIn the early 1990s Consejo Mundial de Lucha Libre (CMLL), Mexico's oldest wrestling promotion, created a new concept, the \"Mini-Estrella\" division. The division was created by Antonio Peña who worked for CMLL at the time, who came up with the idea of using both little people and short wrestlers together and to have the \"Mini-Estrellas\" work as smaller versions of popular wrestlers of the time. Peña and CMLL created the CMLL World Mini-Estrella Championship in 1992, which is considered the official birth of the division. CMLL's \"Mini-Estrellas\" division featured a number of skilled, high flying wrestlers which helped make the concept an immediate success. Original \"Mini-Estrellas\" division consisted of Mascarita Sagrada (the first CMLL Mini-Estrella champion), Aguilata Solitaria, Octagoncito, Espectrito, Mazakrito, Pierrothito, Pequeño Tritón, Mascarita Mágica, Ultimo Dragoncito, Cicloncito Ramírez, Pequeño Jaque Mate, Platita and Gargolita.\n\nIn 1993 Peña decided to leave CMLL and create his own wrestling promotion, Asistencia Asesoría y Administración (AAA), and in the process a number of the \"Mini-Estrellas\" left with Peña. Among those that left CMLL were the division's two main stars, the champion Mascarita Sagrada and his rival Espectrito. Peña later created the Mexican National Mini-Estrella Championship as AAA's \"Mini-Estrella\" championship. Due to the success of both the CMLL and AAA \"Mini-Estrella\" division other promotions such as the Universal Wrestling Association (UWA) and the World Wrestling Association (WWA) briefly promoted \"Mini-Estrella\" championships, but neither promotion gained the success of CMLL and AAA. AAA would later create two other \"Mini-Estrella\", the IWC and the LLL Mini-Estrellas championships, but both were later abandoned. The success of the \"Mini-Estrella\" division was evident as AAA put them in the main event of Triplemanía III-A, one of AAA's biggest shows of the years. The match was a 13-Minis Steel Cage Elimination match, \"Lucha de Apuesta\", \"Mask vs. Mask\" match. In the end Payasito Rojo was the last man in the cage after Bandita, Espectrito, Espectrito II, Jerrito Estrada, Fuercita Guerrera, Mascarita Sagrada, Mini Calo, Octagóncito, La Parkita, Payasito Azul, Super Muñequito, and Torerito had all left the cage. The Triplemanía match was one of the first ever \"Luchas de Apuestas\" matches in the \"Mini-Estrella\" division. AAA would later create another \"first\" in the \"Mini-Estrellas\" division as they created \"Los Mini Vipers\", a Mini version of \"Los Vipers\".\n\nIn 2002 AAA created the AAA Mascot Tag Team Championship, the first and so far only tag team championship for teams consisting of a \"Mini-Estrella\" and the regular sized wrestler he is based on. In 2007, then reigning Mexican National Mini-Estrellas Champion, Mascarita Sagrada 2000 left AAA while still holding the championship; he later appeared in CMLL, repackaged as \"Mascarita Dorada\" but the announcers still mentioned the fact that he was the Mexican National Mini-Estrella Champion. Since his initial appearance as Mascarita Dorada the Mexican National Mini-Estrellas Championship has not been mentioned and is considered inactive. Following the loss of the Mexican National title AAA decided to create a new title, the AAA World Mini-Estrellas Championship as the centerpiece of their \"Mini-Estrellas\" division, won by Mini Charly Manson. On January 11, 2009 CMLL promoted their first pay-per-view (PPV) show with the Minis division in the main event. At \"La Hora Cero\" 13 minis competed in CMLL's first ever \"Infierno en el Ring\" for \"Mini-Estrellas\". The match saw Pierrothito defeat and unmask Shockercito after Cosmico, Eléctrico, Niño de Acero, Fantasy, Mascarita Dorada, Pequeño Ninja, Pequeno Olimpico, Pequeño Warrior, Tzuki, Ultimo Dragoncito and Pequeño Universo 2000 all had escaped the cage. Later on in 2009 CMLL would hold another \"all Minis\" cage match as well as a match that saw Minis and regular sized wrestlers compete against each other. AAA's Mascota tag team title was abandoned in 2009 when the then reigning champions El Alebrije and Cuije left AAA.\n\nThe \"Mini-Estrellas\" have not been restricted to working outside Mexico, although Mexico is the only country to regularly promote the \"Mini-Estrellas\" as a specific division. Often the \"Mini-Estrellas\" are brought in for a \"special attraction\" match such the World Wrestling Council's 19th and 22nd anniversary shows that featured Mascarita Sagrada and other Minis. World Championship Wrestling (WCW) also invited the \"Mini-Estrellas\" to appear on one of their shows, a pre-PPV match at the 1996 Starrcade where Mascarita Sagrada and Octagóncito defeated Jerrito Estrada and Piratita Morgan. Northern California based Pro Wrestling Revolution is the only US based promotion to have created a specific Mini-Estrella championship, created in 2009.\n\nIn 1997 the World Wrestling Federation (WWF) and AAA began a talent sharing program, which allowed several \"Mini-Estrellas\" wrestlers to compete on WWF television. These wrestlers included Mascarita Sagrada, La Parkita and several AAA \"Mini-Estrellas\" that were given new gimmicks for their WWF appearances including Max Mini (formerly Máscarita Sagrada, Jr.), El Torito (Espectrito), Mini Goldust (Mini Karis La Momia) and Mini Vader (Piratita Morgan). Mascarita Sagrada originally wrestled under his normal name but was soon repackaged as \"Mini Nova\", a mini version of Super Nova, a \"Luchador\" that worked for the WWF at the time. Mini Nova made his in ring debut in a match at Bad Blood in 1997 where he teamed with Max Mini against Tarantula and Mosaic. The Minis appeared on \"WWF Shotgun Saturday Night\" and \"WWF Monday Night Raw\" as well as in matches at the Royal Rumble in 1997 and the Royal Rumble in 1998. In 1999 the WWF/AAA talent sharing agreement ended and all \"Mini-Estrellas\" stopped working in the United States. In October 2005 the WWE created a \"Juniors division\" exclusive to their Smackdown brand. the division featured a number of minis from Mexico that mainly appeared in backstage skits of a comedic nature. Minis included Mascarita Sagrada, Tsuki, Octagoncito and Pequeño Violencia, the division also included Super Porky, who had never worked in the \"Mini-Estrella\" division in Mexico. Super Porky only appeared in backstage skits but did not wrestle. By March 2006 the WWE gave up on the Juniors division and released all the mini wrestlers.\n\nThe US based Lucha Libre USA began promoting in 2010 and featured several \"Mini-Estrellas\", unlike in Mexican promotions, \"Mini-Estrellas\" such as Mascarita Dorada and Pequeño Halloween compete against regular sized competitors instead of in a separate division.\n\nSince the \"Mini-Estrella\" division is not restricted only to people with Dwarfism some wrestlers have moved on from the \"Mini-Estrellas\" division to the regular sized division, especially some of the competitors over the height limit the division originally had. Mike Segura originally worked as Orito (a mini version of Oro) in CMLL, but when he began working for AAA he began working in the regular sized division as Super Nova. Similarly Felinito wrestled in CMLL as a mini but when jumped to AAA he began wrestling in the regular sized division as Mach 1. Freelance became a regular sized wrestler after losing his mask as Panterita.\n\nIn late 1997 CMLL booked an eight-man \"torneo cibernetico\" elimination match where the winner would earn the right to work in the \"regular sized\" division. Damiancito El Guerrero defeated Cicloncito Ramírez, Tritoncito, Pequeño Cochisse, Platita, Guerrerito del Future, Pequeño Sayama and Fierito to earn the right to work with \"regular sized\" wrestlers. In early 1998 he made his debut as part of the regular sized division under the name \"Virus\", no mention was made that Virus used to work as Damiancito El Guerrero or the fact that he still held the CMLL World Mini-Estrella Championship. By 1999 CMLL decided that it was time to crown a new CMLL World Mini-Estrella Champion as Virus was still technically the champion despite not having worked as a mini for over a year. Instead of making Virus return to the Minis division to lose the title CMLL decided to give the championship to Ultimo Dragoncito and then subsequently announce that Ultimo Dragoncito had \"won\" the title on an undisclosed day in October 1999.\n\nCMLL is currently holding a \"Bicentennial tournament\" in August, 2010 to commemorate the 18th anniversary of the \"Mini-Estrellas\" division, where the winner will \"graduate\" to the regular sized division. The tournament consists of two \"torneo cibernetico\" elimination matches with the winner of each facing off in a singles match on August 24. Demus 3:16 won the first \"cibernetico\", defeating Eléctrico, Saturno, Fantasy, Pequeño Olímpico, Pequeño Nitro, Pequeño Violencia and Cisne to earn a place in the finals. The second \"torneo cibernetico\" took place on August 17, 2010, and was won by Pierrothito. On August 24, 2010, Demus 3:16 defeated Pierrothito in the finals of the tournament to earn his way out of the \"Mini-Estrella\" division. In March 2011, Demus 3:16 wrestled Virus in a hair vs. hair mask. Demus 3:16 lost the match and after that he returned to the minis division.\n\nSince \"Mini-Estrella\" wrestlers are smaller and possess less muscle bulk than heavyweights or even cruiserweights it lends to a high-flying wrestling style for a number of the \"Mini-Estrellas\", especially in recent years. Some wrestlers such as Mascarita Dorada are able to perform moves that his regular sized counterpart would have a hard time executing. Not all performers in the \"Mini-Estrella\" division are able to work a high flying style, especially those with more severe forms of dwarfism work a more grounded style.\n\nThe first Mini-Estrellas championship was created in 1992 when CMLL created the CMLL Mini-Estrellas World Championship. Since then rivals AAA, UWA and WWA all created a Mini-Estrellas championship, although only AAA's championship sustained any longevity. In 2008 CMLL decided to use the Mexican National Lightweight Championship as a secondary title for the \"Mini-Estrellas\" division. Up until that point the Lightweight title had not been considered a \"Mini-Estrellas\" championship.\n\n\n"}
{"id": "13680395", "url": "https://en.wikipedia.org/wiki?curid=13680395", "title": "Named entity", "text": "Named entity\n\nIn information extraction, a named entity is a real-world object, such as persons, locations, organizations, products, etc., that can be denoted with a proper name. It can be abstract or have a physical existence. Examples of named entities include Barack Obama, New York City, Volkswagen Golf, or anything else that can be named. Named entities can simply be viewed as entity instances (e.g., New York City is an instance of a city).\n\nFrom a historical perspective, the term \"Named Entity\" was coined during the MUC-6 evaluation campaign and contained ENAMEX (entity name expressions e.g. persons, locations and organizations) and NUMEX (numerical expression).\n\nA more formal definition can be derived from the rigid designator by Saul Kripke. In the expression \"Named Entity\", the word \"Named\" aims to restrict the possible set of entities to only those for which one or many rigid designators stands for the referent. A designator is rigid when it designates the same thing in every possible world. On the contrary, flaccid designators may designate different things in different possible worlds.\n\nAs an example, consider the sentence, \"Obama is the president of the United States\". Both \"Obama\" and the \"United States\" are named entities since they refer to specific objects (Barack Obama and United States). However, \"president\" is not a named entity since it can be used to refer to many different objects in different worlds (in different presidential periods referring to different persons, or even in different countries or organizations referring to different people). Rigid designators usually include proper names as well as certain natural terms like biological species and substances.\n\nThere is also a general agreement in the Named Entity Recognition community to consider as named entities temporal and numerical expressions such as amounts of money and other types of units, which may violate the rigid designator perspective.\n\nThe task of recognizing named entities in text is Named Entity Recognition while the task of determining the identity of the named entities mentioned in text is called Named Entity Disambiguation. Both tasks require dedicated algorithms and resources to be addressed.\n"}
{"id": "19629994", "url": "https://en.wikipedia.org/wiki?curid=19629994", "title": "Penis envy", "text": "Penis envy\n\nPenis envy () is a stage theorized by Sigmund Freud regarding female psychosexual development, in which young girls experience anxiety upon realization that they do not have a penis. Freud considered this realization a defining moment in a series of transitions toward a mature female sexuality and gender identity. In Freudian theory, the penis envy stage begins the transition from an attachment to the mother to competition with the mother for the attention, recognition and affection of the father. The parallel reaction of a boy's realization that women do not have a penis is castration anxiety.\n\nFreud's theories regarding psychosexual development, and in particular the \"phallic stage\", were criticized and refined by other psychoanalysts, such as Karen Horney, Otto Fenichel, Ernest Jones, Erik Erikson, Jean Piaget, Juliet Mitchell, and Clara Thompson.\n\nFreud introduced his theory of the concept of interest in—and envy of—the penis in his 1908 article \"On the Sexual Theories of Children\": it was not mentioned in the first edition of Freud's earlier \"Three Contributions to the Theory of Sex\" (1905), but a synopsis of the 1908 article was added to the third edition in 1915. In \"On Narcissism\" (1914) he described how some women develop a masculine ideal as \"a survival of the boyish nature that they themselves once possessed\". The term grew in significance as Freud gradually refined his views of sexuality, coming to describe a mental process he believed occurred as one went from the phallic stage to the latency stage (see Psychosexual development).\n\nIn Freud's psychosexual development theory, the phallic stage (approximately between the ages of 3.5 and 6) is the first period of development in which the libidinal focus is primarily on the genital area. Prior to this stage, the libido (broadly defined by Freud as the primary motivating energy force within the mind) focuses on other physiological areas. For instance, in the \"oral stage\", in the first 12 to 18 months of life, libidinal needs concentrate on the desire to eat, sleep, suck and bite. The theory suggests that the penis becomes the organ of principal interest to \"both\" sexes in the phallic stage. This becomes the catalyst for a series of pivotal events in psychosexual development. These events, known as the Oedipus complex for boys, and the Electra complex for girls, result in significantly different outcomes for each gender because of differences in anatomy.\n\nFreud thought girls:\n\nA similar process occurs in boys of the same age as they pass through the phallic stage of development; the key differences being that the focus of sexual impulses need not switch from mother to father, and that the fear of castration (castration anxiety) remains. The boy desires his mother, and identifies with his father, whom he sees as having the object of his sexual impulses. Furthermore, the boy's father, being the powerful aggressor of the family unit, is sufficiently menacing that the boy employs the defense mechanism of displacement to shift the object of his sexual desires from his mother to women in general.\n\nFreud thought this series of events occurred prior to the development of a wider sense of sexual identity, and was required for an individual to continue to enter into his or her gender role.\n\nFreud considered that in normal female development penis envy transformed into the wish for a man and/or a baby.\n\nKarl Abraham differentiated two types of adult women in whom penis envy remained intense as the wish-fulfilling and the vindictive types: The former were dominated by fantasies of having or becoming a penis—as with the singing/dancing/performing women who felt that in their acts they magically incorporated the [parental] phallus. The latter sought revenge on the male through humiliation or deprivation (whether by removing the man from the penis or the penis from the man).\n\nFreud's theories regarding psychosexual development, and in particular the \"phallic stage\", were early challenged by other psychoanalysts, such as Karen Horney, Otto Fenichel and Ernest Jones, though Freud did not accept their view of penis envy as a secondary, rather than a primary, female reaction. Later psychologists, such as Erik Erikson and Jean Piaget, challenged the Freudian model of child psychological development as a whole.\n\nJacques Lacan, however, took up and developed Freud's theory of the importance of what he called \"\"penisneid\" in the unconscious of women\" in linguistic terms, seeing what he called the phallus as the privileged signifier of humanity's subordination to language: \"the phallus (by virtue of which the unconscious is language)\". He thereby opened up a new field of debate around phallogocentrism—some figures like Juliet Mitchell endorsing a view of penis envy which \"uses, not the man, but the phallus to which the man has to lay claim, as its key term\", others strongly repudiating it.\n\nIn Freud's theory, the female sexual center shifts from the clitoris to the vagina during a heterosexual life event. Freud believed in a duality between how genders construct mature sexuality in terms of the opposite gender, whereas feminists reject the notion that female sexuality can only be defined in relation to the male. Feminists development theorists instead believe that the clitoris, not the vagina, is the mature center of female sexuality because it allows a construction of mature female sexuality independent of the penis. \n\nA significant number of feminists have been highly critical of penis envy theory as a concept and psychoanalysis as a discipline, arguing that the assumptions and approaches of the psychoanalytic project are profoundly patriarchal, anti-feminist, and misogynistic and represent women as broken or deficient men. Karen Horney—a German psychoanalyst who also placed great emphasis on childhood experiences in psychological development—was a particular advocate of this view. She asserted the concept of \"womb envy\", and saw \"masculine narcissism\" as underlying the mainstream Freudian view.\n\nSome feminists argue that Freud's developmental theory is heteronormative and denies women a mature sexuality independent of men; they also criticize it for privileging the vagina over the clitoris as the center of women's sexuality. They criticize the sociosexual theory for privileging heterosexual sexual activity and penile penetration in defining women's \"mature state of sexuality\". Others claim that the concept explains how, in a patriarchal society, women might envy the power accorded to those with a phallus.\n\nIn her influential paper \"Women and Penis Envy\" (1943), Clara Thompson reformulated the latter as \"social\" envy for the trappings of the dominant gender, a sociological response to female subordination under patriarchy.\n\nBetty Friedan referred to penis envy as a purely parasitic social bias typical of Victorianism and particularly of Freud's own biography, and showed how the concept played a key role in discrediting alternative notions of femininity in the early to mid twentieth century: \"Because Freud's followers could only see woman in the image defined by Freud – inferior, childish, helpless, with no possibility of happiness unless she adjusted to being man's passive object – they wanted to help women get rid of their suppressed envy, their neurotic desire to be equal. They wanted to help women find sexual fulfilment as women, by affirming their natural inferiority\".\n\nA small but influential number of Feminist philosophers, working in Psychoanalytic feminism, and including Luce Irigaray, Julia Kristeva, and Hélène Cixous, have taken varying post-structuralist views on the question, inspired or at least challenged by figures such as Jacques Lacan and Jacques Derrida.\n\nJuliet Mitchell in her early work attempted to reconcile Freud's thoughts on psychosexual development with Feminism and Marxism by declaring his theories to be simply observations of gender identity under capitalism.\n\n\n"}
{"id": "37689611", "url": "https://en.wikipedia.org/wiki?curid=37689611", "title": "Peter K. Palangyo", "text": "Peter K. Palangyo\n\nPeter K. Palangyo (1939 - 18 January 1993) was a Tanzanian novelist and diplomat. His reputation rests on a single novel, \"Dying in the Sun\" (1968), which is considered by many to be one of the most compelling works of modernism in African writing from this period.\n\nBorn in Arusha, Palangyo was educated locally, in Uganda and the United States. He majored in biology at St. Olaf College in Northfield, Minnesota, and went on to graduate school at the University of Minnesota. Abandoning the sciences for literature, he took a diploma of education from Makerere University College and taught in several secondary schools. In 1968 he returned to the United States to join the writers' workshop at the University of Iowa, and gained an MFA in creative writing. Returning to Tanzania in 1972, he taught at the University of Dar es Salaam before joining the diplomatic service. At one point he was Tanzania's Ambassador to France.\" In 1980 he gained a PhD from University at Buffalo, The State University of New York, with a thesis on Chinua Achebe.\n\nPalangyo died in a car accident in 1993.\"\n\n"}
{"id": "2628367", "url": "https://en.wikipedia.org/wiki?curid=2628367", "title": "Proto-Dravidian language", "text": "Proto-Dravidian language\n\nProto-Dravidian is the linguistic reconstruction of the common ancestor of the Dravidian languages. It is thought to have differentiated into Proto-North Dravidian, Proto-Central Dravidian, and Proto-South Dravidian, although the date of diversification is still debated.\n\nAs a proto-language, the Proto-Dravidian language is not itself attested in the historical record. Its modern conception is based solely on reconstruction. It is suggested that the language was spoken in the 4th millennium BCE, and started disintegrating into various branches around 3rd millennium BCE.\n\nProto-Dravidian contrasted between five short and long vowels: \"*a\", \"*ā\", \"*i\", \"*ī\", \"*u\", \"*ū\", \"*e\", \"*ē\", \"*o\", \"*ō\". The sequences \"*ai\" and \"*au\" are treated as \"*ay\" and \"*av\" (or *\"aw\")\n\nProto-Dravidian has been reconstructed as having the following consonant phonemes (Subrahmanyam 1983:p40, Zvelebil 1990, Krishnamurthi 2003):\nThe alveolar stop \"*ṯ\" in many daughter languages developed into an alveolar trill . The stop sound is retained in Kota and Toda (Subrahmanyam 1983). Malayalam still retains the original (alveolar) stop sound in gemination. (\"ibid\"). In Old Tamil it took the enunciative vowel like the other stops. In other words, \"*ṯ\" (or \"*ṟ\") did not occur word-finally without the enunciative vowel (\"ibid\").\n\nVelar nasal \"*ṅ\" occurred only before \"*k\" in Proto-Dravidian (as in many of its daughter languages). Therefore, it is not considered a separate phoneme in Proto-Dravidian. However, it attained phonemic status in languages like Malayalam, Gondi, Konda and Pengo because the original sequence \"*ṅk\" was simplified to \"*ṅ\". (Subrahmanyam 1983)\n\nThe glottal fricative \"*h\" has been proposed by Bh. Krishnamurthi to account for the Old Tamil Aytam (\"Āytam\") and other Dravidian comparative phonological phenomena (Krishnamurthi 2003).\n\nThe Northern Dravidian languages Kurukh, Malto and Brahui cannot easily be derived from the traditional Proto-Dravidian phonological system. McAlpin (2003) proposes that they branched off from an earlier stage of Proto-Dravidian than the conventional reconstruction, which would apply only to the other languages. He suggests reconstructing a richer system of dorsal stop consonants:\n\nThe origin and territory of the Proto-Dravidian speakers is uncertain, but some suggestions have been made based on the reconstructed Proto-Dravidian vocabulary. The reconstruction has been done on the basis of cognate words present in the different branches (Northern, Central and Southern) of the Dravidian language family.\n\nAccording to Dorian Fuller (2007), the botanical vocabulary of Proto-Dravidian is characteristic of the dry deciduous forests of central and peninsular India. This region extends from Saurashtra and Central India to South India. It thus represents the general area in which the Dravidians were living before separation of branches.\n\nAccording to Franklin Southworth (2005), the Proto-Dravidian vocabulary is characteristic of a rural economy based on agriculture, animal husbandry and hunting. However, there are some indications of a society more complex than a rural one:\n\n\nThese evidences are not sufficient to determine with certainty the territory of the Proto-Dravidians. These characteristics can be accommodated within multiple contemporary cultures, including:\n\n\n\n"}
{"id": "10301769", "url": "https://en.wikipedia.org/wiki?curid=10301769", "title": "Running economy", "text": "Running economy\n\nRunning economy consists of many physiological and biomechanical factors that contribute to running performance, and is measured to quantify energy utilization while running at an aerobic intensity. Oxygen consumption (VO) is the most direct method for measuring running economy, as the exchange of gases in the body, specifically oxygen and carbon dioxide, closely reflects energy metabolism. Those who are able to consume less oxygen while running at a given velocity are said to have a better running economy.\n\nIn distance running, an athlete may attempt to improve performance through training designed to improve running economy. Running economy has been found to be a good predictor of race performance; it has been found to be a stronger correlate of performance than maximal oxygen uptake (VO max) in trained runners with the same values (Saunders, 2004). The literature relating to RE is vast and the determinants of RE supported by empirical data.\n\nIn \"The Lore of Running\", Tim Noakes, a professor of exercise and sports science at the University of Cape Town, and also recreational runner, describes a number of variables that may affect running economy: vertical motion while running, the ability of the muscles to absorb energy during the shock of landing and transfer it to push-off, biomechanical factors, technique and type of activity, fitness and training, age, fatigue, gender, race, weight of clothing and shoes, and environmental conditions.\n\nVarious studies have shown marathon runners to be more economical than middle distance runners and sprinters at speeds of 6–12 miles per hour (10-19 kilometers per hour). At those speeds, film analysis has shown that sprinters and middle distance have more vertical motion than marathoners.\n\n"}
{"id": "8204445", "url": "https://en.wikipedia.org/wiki?curid=8204445", "title": "Sociocultural linguistics", "text": "Sociocultural linguistics\n\nSociocultural linguistics is a term used to encompass a broad range of theories and methods for the study of language in its sociocultural context. Its growing use is a response to the increasingly narrow association of the term sociolinguistics with specific types of research involving the quantitative analysis of linguistic features and their correlation to sociological variables. The term as it is currently used not only clarifies this distinction, but highlights an awareness of the necessity for transdisciplinary approaches to language, culture and society. \n\nThe scope of sociocultural linguistics, as described by researchers such as Kira Hall and Mary Bucholtz, is potentially vast, though often includes work drawing from disciplines such as sociolinguistics, linguistic anthropology, discourse analysis, and sociology of language, as well as certain streams of social psychology, folklore studies, media studies, social and literary theory, and the philosophy of language.\n\nSociocultural linguists, especially in the United States, take an interdisciplinary approach to the study of language and the social and cultural functions of language use. Although generative and cognitive linguistics have been dominant in the United States since the mid-twentieth century, American linguists have periodically proposed to bring their studies closer to other fields of social inquiry. For example, in 1929, Edward Sapir urged linguists to move beyond diachronic and formal analyses for their own sake and to \"become aware of what their science may mean for the interpretation of human conduct in general\" (1929:207).\n\nIt is peculiarly important that linguists, who are often accused, and accused justly, of failure to look beyond the pretty patterns of their subject matter, should become aware of what their science may mean for the interpretation of human conduct in general. Whether they like it or not, they must become increasingly concerned with the many anthropological, sociological, and psychological problems which invade the field of language. [Sapir 1929:214]\n\nForty years later, Dell Hymes (1964) lamented that the socially integrated linguistics Sapir had called for was disappearing. Hymes and others worried that new formal approaches, as well as the push for linguistics as an autonomous field, threatened to once again isolate linguists. At the same time, though, the growth of ethnolinguistics and sociolinguistics offered a venue for the socially engaged linguistics Sapir had called for four decades earlier.\n\nAfter four more decades, just as Hymes (1964) worried that linguistics had been bleached of its association with the study of human interaction in the wake of formalist studies, scholars noted that sociolinguistics in turn had narrowed to denote only specific types of study. Sociocultural linguistics is thus \"the broad interdisciplinary field concerned with the intersection of language, culture, and society\" (Bucholtz and Hall 2005: 5).\n\n\n"}
{"id": "3200594", "url": "https://en.wikipedia.org/wiki?curid=3200594", "title": "South Bolivian Quechua", "text": "South Bolivian Quechua\n\nSouth Bolivian Quechua, also known as Central Bolivian Quechua, is a dialect of Southern Quechua spoken in Bolivia and adjacent areas of Argentina, where it is also known as \"Colla\". It is not to be confused with North Bolivian Quechua, which is spoken on the northern Andean slopes of Bolivia and is phonologically distinct from the South Bolivian variety. Estimates of the number of speakers of South Bolivian Quechua range from 2.3 to 2.8 million, making it the most spoken indigenous language in Bolivia, just slightly greater than Aymara, with roughly 2 million speakers in Bolivia. In comparison, the North Bolivian dialect has roughly 116,000 speakers.\n\nSouth Bolivian Quechua is a member of the Southern branch of the Quechua language family, making it closely related to other Southern Quechua dialects including Ayacucho and particularly Cuzco Quechua, varieties which are both spoken in Peru.\n\nThe Quechua language family spans an extremely diverse set of languages, many of which are mutually unintelligible, which is why linguists have classified Quechua as a language family as opposed to one language with many dialects. Though it is believed that all Quechuan languages descended from a single ancestor, Proto-Quechua, there is still debate on how the modern Quechuan languages evolved into their current states, and what this timeline would look like. As a result of this, there have been numerous suggested classifications and theories of the relatedness of specific languages and dialects of Quechua. However, the current broad division of Quechua into four main branches is generally accepted.\n\nJoseph Greenberg, in his highly contested theory of the Amerind superfamily, places the Quechua language family in the Andean branch of Amerind, which is part of the larger Southern Amerind branch that encompasses all indigenous South American languages. Much of Greenberg's proposal has been disproved, and his claims regarding Quechua are equally suspect. Even at one of his lower subgroupings, the Andean language family, the idea that Andean languages such as Quechua and Aymara are related is still debated, and the common consensus is that similarities between Quechua and Aymara arose from language contact as opposed to a genetic relationship.\n\nThere are some dialectal differences in South Bolivian Quechua across the regions of Bolivia. These dialects include Chuquisaca, Cochabamba, Oruro, Potosi, and Sucre in Bolivia, along with Northwest Jujuy in Argentina. There are perhaps still a few speakers, out of 8,000 ethnic Quechua, in Chile. Santiagueño Quechua in Argentina, though divergent, appears to derive at least partly from South Bolivian Quechua.\n\nQuechua is recognized as an official language of Bolivia, one of the 36 indigenous languages declared official in the nation's Constitution. At approximately 2.8 million speakers out of a population of roughly 10 million, it is the most widely spoken indigenous language in Bolivia. This number is much greater than many other prominent indigenous languages of Bolivia such as North Bolivian Quechua (116,000 speakers) or Guarani (33,700 speakers). However, compared to Spanish, which is spoken by 8.7 million or roughly 87% of Bolivians, Quechua is much less widely used. There have been numerous concerns raised over the status of indigenous languages including South Bolivian Quechua due to the prestige language in the area, Spanish, potentially forcing the endangerment of other devalued languages.\n\nThe Ethnologue lists South Bolivian Quechua as \"developing\", which indicates that \"the language is in vigorous use, with literature in a standardized form being used by some though this is not yet widespread or sustainable.\" However, UNESCO's Atlas of Endangered Languages categorizes South Bolivian Quechua as \"vulnerable\", defined as the following: \"Most but not all children or families of a particular community speak the language as their first language, but it may be restricted to specific social domains (such as at home, where children interact with their parents and grandparents).\" While South Bolivian Quechua and the Quechua language family in general are much better off compared to many other indigenous languages due to the relatively large number of speakers, Quechua is still in danger due to its long history of Spanish influence and bilingualism. In addition, the linguistic, ideological, and cultural differences among its many dialects make it difficult for policymakers to approach Quechua as a whole, as each Quechua community provides different challenges in regard to language policy and planning.\n\nOver the past few decades there has been a surge in revitalization efforts for Quechua and other indigenous languages due to factors such as a growth in international tourism promoting cultural pride. Efforts have been made to increase the linguistic and cultural status of the Quechua language and peoples. In Bolivia, many policymakers are advocating the teaching of Quechua and other indigenous languages like Aymara in all public schools and government offices. However, these revitalization efforts are often met with resistance, and their effectiveness in halting Quechua's decline is still questionable.\n\nSouth Bolivian Quechua has three basic vowel sounds: unrounded front vowel /i/, rounded back vowel /u/, and low central vowel /a/. The front vowel /i/ is lowered to [e] or [ɛ] when next to a uvular stop or when separated from a uvular stop only by a non-stop consonant. The back vowel /u/ is similarly lowered in this environment, to [o] or [ɔ].\n\nThe following table displays the consonant sounds in South Bolivian Quechua using the orthographic system employed by Bills (1969). IPA equivalents are included in brackets where necessary.\n\nThere are four stops and one affricate /ch/ in the basic sound system. The five sounds contrast with both their aspirated and glottalized versions, a characteristic that occurs in many dialects of the Quechua language family and is believed to be as a result of exposure to Aymara, which makes the same distinctions. Aspiration and glottalization can be seen to be contrastive in minimal pairs such as \"puñun\" \"he sleeps\" versus \"p'uñun\" \"his jug\", and \"piña\" \"pineapple\" versus \"p\"iña\" \"wild\".\n\nAll stops, affricates, and fricatives are voiceless with the exception of /q/, which becomes a voiced uvular fricative [ʁ] syllable-initially.\n\nAdditional phonological alternations include fricativization of /k/ and /q/ syllable-finally to velar [x] and uvular [χ] respectively. The fricative /s/ has allophones [s] and [ʃ], of which the latter occurs quite infrequently. All fricatives occur only word-initially and medially, never finally.\n\nThe three nasal sounds assimilate to the point of articulation of the following consonant sound. Word-finally, /n/ is the only nasal that occuan; it becomes [ŋ].\n\nSouth Bolivian Quechua generally has a simple CV(C) syllable structure, where the coda consonant is optional. The onset consonant is also optional word-initially, as in the words \"ima\" \"what\" and \"uk\" \"one\", and Spanish borrowings can contain word-initial consonant clusters of the form CCV(C), as in \"bwenos diyas\" \"good morning\". No more than two consonants are allowed in a consonant cluster.\n\nProto-Quechua has few constraints on the combinations of consonant clusters allowed, but due to consonant lenition syllable-finally, there are greater restrictions on the types of consonant clusters that occur in South Bolivian Quechua. Some of the possible consonant clusters can be seen in the following examples:\n\n\nPrimary stress generally occurs on the penultimate syllable of the word, with secondary stresses on alternating syllables. This can be seen in the following analyses for the words \"munankičis\" and \"munankičisñaču\" (root verb \"muna\" \"want, desire\"), where stress has been numbered below:\n\nRare exceptions exist where the final syllable of the word carries the primary stress, such as in \"ari\" \"yes\". There also exist some 'emotive' suffixes in the language that are always stressed, resulting in stress on the last syllable of the word. Stress on the final syllable can also occur through the dropping of some single-syllable suffixes (for instance, the yes/no question marker \"-chu\") without a subsequent shifting of the stress.\n\nSouth Bolivian Quechua is an agglutinative, polysynthetic language with a rich derivational morphology, allowing the language to convey a large amount of information in a single word. As a result of this, words in South Bolivian Quechua can be very long.\n\nWords in the language are purely suffixal; no other types of affixes are used. These suffixes are also highly regular, with alternations generally only occurring to maintain syllable structure.\n\nMorphemes within a word are ordered as follows:\n\n\nSouth Bolivian Quechua has many clearly derivational suffixes, where a noun, verb, or adjective is derived from a different lexical category. The following are a few examples:\n\n\"Note:\" -y \"is the verb infinitive marker.\"\n\n\nOther suffixes are less clearly categorized as derivational or inflectional, including some aspectual suffixes as well as a class of suffixes termed “auxiliary”. For example, the causative suffix ‘’-chi’’ may seem straightforwardly inflectional in some instances:\n\n\nBut in other cases it can be derivational:\n\n\nThere are several categories of verbal suffixes in South Bolivian Quechua. These include modal suffixes, object markers, tense and aspect markers, and person markers.\n\nSouth Bolivian Quechua has a great amount of modal suffixes that are used to express a range of concepts. Some examples include:\n\n\nSome of these modal suffixes can be derivational if used with a non-verb—for example, \"-naya\" and \"-na\".\n\nPerson markers differentiate between first, second, and third persons and plurality, as well as an inclusive and exclusive first person plural. Object markers and subject markers are used in the language, and object markers appear before subject markers. The object marker is \"-wa\" for a first person object and \"-su\" for a second person object. The following table details possible combinations of object and subject markers. Some person categories lack a subject and/or object marker.\n\nAll non-present tenses in the indicative are marked by a suffix directly preceding the person marking. The present subjunctive is marked with a suffix following the person marking. Examples of tense markers include the simple past suffix \"-rqa\", past imperfect \"-yka\", and past perfect \"-sqa\". Tense suffixes can change form depending on person and can alter person marking in some cases: for instance, in the past imperfect tense, both the third person singular and plural subject markers (typically \"-n\", \"-nchiq\", or \"-nku\" depending on object) become \"-q\", meaning that a verb in the past imperfect with a third person subject would end in \"-yka-q\".\n\nApart from case-marking suffixes, nouns in South Bolivian Quechua can also be pluralized with the suffix \"-kuna\" (or by a numeral modifier preceding the noun). However, most speakers use the suffix -s, borrowed from Spanish, when the noun ends in a vowel. For example, wasi (\"house\") becomes wasis (\"houses\") or runa (\"person\") becomes runas (\"people/persons\"). The Quechua suffix -kuna is usually only used when a noun ends in a consonant, such as with yan (road), which becomes yankuna (roads). A collective marker, \"-ntin\", also exists to denote “togetherness”, as in \"alqu michi-ntin\" \"the dog, together with the cat\". Possessiveness is marked by a suffix attached to the noun, with the form that the morpheme takes dependent on person, plurality, and whether it is following a vowel or consonant.\n\nPronouns in the language have no person markers, but do have plural markers that vary by person. Possessive pronouns are marked by the addition of the appropriate genitive suffix.\n\nAdjectives can be made into superlatives with the suffix \"-puni\", as in \"kosa\" \"good\"; \"kosa-puni\" \"good above all others, best\".\n\nSome suffixes in South Bolivian Quechua can be used with words of any lexical category, and are generally found at the end of the word after all other suffixes. Some examples are:\n\n\nReduplication is used extensively for various purposes, and can be derivational:\n\n\nReduplicated stems can be suffixal as well:\n\n\nThe basic word order of South Bolivian Quechua is stated to be SOV. However, because nouns are marked for case, word order is in fact very flexible and is generally varied for the purposes of emphasis. For instance, the following sentences all mean \"Atahuallpa had Huascar killed\":\n\n\nOne aspect of word order that is constant in the language is the fact that noun modifiers must directly precede the noun (adjective-noun).\n\nSouth Bolivian Quechua is nominative-accusative. Nouns can have the following case markers:\n\n\nLack of a case marker indicates the nominative.\n\nPassives are marked by suffixes, including \"-sqa\" on the verb, \"-manta\" \"from, by\" on the agent, and \"-wan\" \"with\" on the instrument, as in the following examples:\n\n\nSubordination is mostly indicated by participles, and can be marked for tense only relative to the main verb. Subordination need not be explicitly marked, as certain participles can be understood as subordinative—for example, a literal gloss of \"His coming, I will leave\" can be interpreted as \"When he comes, I will leave\" or \"If he comes, I will leave\". Other suffixes such as \"-qti\" \"when\" and \"-rayku\" \"because\" can also be used to mark a subordinate clause.\n\nIn addition, subordination can also be indicated lexically by \"ukta...chaymanta...\" \"first...then...\" or \"ukta...q\"ipanta...\" \"first...afterwards...\", as in the following examples:\n\n"}
{"id": "21780446", "url": "https://en.wikipedia.org/wiki?curid=21780446", "title": "Species", "text": "Species\n\nIn biology, a species is the basic unit of classification and a taxonomic rank, as well as a unit of biodiversity, but it has proven difficult to find a satisfactory definition. Scientists and conservationists need a species definition which allows them to work, regardless of the theoretical difficulties. If as Carl Linnaeus thought, species were fixed and clearly distinct from one another, there would be no problem, but evolutionary processes cause species to change continually, and to grade into one another. A species is often defined as the largest group of organisms in which any two individuals of the appropriate sexes or mating types can produce fertile offspring, typically by sexual reproduction. While this definition is often adequate, when looked at more closely it is problematic. For example, with hybridisation, in a species complex of hundreds of similar microspecies, or in a ring species, the boundaries between closely related species become unclear. Among organisms that reproduce only asexually, the concept of a reproductive species breaks down, and each clone is potentially a microspecies. Problems also arise when dealing with fossils, since reproduction cannot be examined; the concept of the chronospecies is therefore used in palaeontology. Other ways of defining species include their karyotype, DNA sequence, morphology, behaviour or ecological niche.\n\nAll species are given a two-part name, a \"binomial\". The first part of a binomial is the genus to which the species belongs. The second part is called the specific name or the specific epithet (in botanical nomenclature, also sometimes in zoological nomenclature). For example, \"Boa constrictor\" is one of four species of the genus \"Boa\".\n\nSpecies were seen from the time of Aristotle until the 18th century as fixed kinds that could be arranged in a hierarchy, the great chain of being. In the 19th century, biologists grasped that species could evolve given sufficient time. Charles Darwin's 1859 book \"The Origin of Species\" explained how species could arise by natural selection. That understanding was greatly extended in the 20th century through genetics and population ecology. Genetic variability arises from mutations and recombination, while organisms themselves are mobile, leading to geographical isolation and genetic drift with varying selection pressures. Genes can sometimes be exchanged between species by horizontal gene transfer; new species can arise rapidly through hybridisation and polyploidy; and species may become extinct for a variety of reasons. Viruses are a special case, driven by a balance of mutation and selection, and can be treated as quasispecies.\n\nAs a practical matter, species concepts may be used to define species that are then used to measure biodiversity, though whether this is a good measure is disputed, as other measures are possible.\n\nIn his biology, Aristotle used the term γένος (génos) to mean a kind, such as a bird or fish, and εἶδος (eidos) to mean a specific form within a kind, such as (within the birds) the crane, eagle, crow, or sparrow. These terms were translated into Latin as \"genus\" and \"species\", though they do not correspond to the Linnean terms thus named; today the birds are a class, the cranes are a family, and the crows a genus. A kind was distinguished by its attributes; for instance, a bird has feathers, a beak, wings, a hard-shelled egg, and warm blood. A form was distinguished by being shared by all its members, the young inheriting any variations they might have from their parents. Aristotle believed all kinds and forms to be distinct and unchanging. His approach remained influential until the Renaissance.\n\nWhen observers in the Early Modern period began to develop systems of organization for living things, they placed each kind of animal or plant into a context. Many of these early delineation schemes would now be considered whimsical: schemes included consanguinity based on colour (all plants with yellow flowers) or behaviour (snakes, scorpions and certain biting ants). John Ray, an English naturalist, was the first to attempt a biological definition of species in 1686, as follows:\n\nIn the 18th century, the Swedish scientist Carl Linnaeus classified organisms according to shared physical characteristics, and not simply based upon differences. He established the idea of a taxonomic hierarchy of classification based upon observable characteristics and intended to reflect natural relationships. At the time, however, it was still widely believed that there was no organic connection between species, no matter how similar they appeared. This view was influenced by European scholarly and religious education, which held that the categories of life are dictated by God, forming an Aristotelian hierarchy, the \"scala naturae\" or great chain of being. However, whether or not it was supposed to be fixed, the \"scala\" (a ladder) inherently implied the possibility of climbing.\n\nFaced with evidence of hybridisation, Linnaeus came to accept that species could change, and the struggle for survival, but not that new species could freely evolve. By the 19th century, naturalists understood that species could change form over time, and that the history of the planet provided enough time for major changes. Jean-Baptiste Lamarck, in his 1809 \"Zoological Philosophy\", described the transmutation of species, proposing that a species could change over time, in a radical departure from Aristotelian thinking.\n\nIn 1859, Charles Darwin and Alfred Russel Wallace provided a compelling account of evolution and the formation of new species. Darwin argued that it was populations that evolved, not individuals, by natural selection from naturally occurring variation among individuals. This required a new definition of species. Darwin concluded that species are what they appear to be: ideas, provisionally useful for naming groups of interacting individuals, writing:\n\nThe commonly used names for kinds of organisms are often ambiguous: \"cat\" could mean the domestic cat, \"Felis catus\", or the cat family, Felidae. Another problem with common names is that they often vary from place to place, so that puma, cougar, catamount, panther, painter and mountain lion all mean \"Puma concolor\" in various parts of America, while \"panther\" may also mean the jaguar (\"Panthera onca\") of Latin America or the leopard (\"Panthera pardus\") of Africa and Asia. In contrast, the scientific names of species are chosen to be unique and universal; they are in two parts used together: the genus as in \"Puma\", and the specific epithet as in \"concolor\".\n\nA species is given a taxonomic name when a type specimen is described formally, in a publication that assigns it a unique scientific name. The description typically provides means for identifying the new species, differentiating it from other previously described and related or confusable species and provides a validly published name (in botany) or an available name (in zoology) when the paper is accepted for publication. The type material is usually held in a permanent repository, often the research collection of a major museum or university, that allows independent verification and the means to compare specimens. Describers of new species are asked to choose names that, in the words of the International Code of Zoological Nomenclature, are \"appropriate, compact, euphonious, memorable, and do not cause offence\".\n\nBooks and articles sometimes intentionally do not identify species fully and use the abbreviation \"sp.\" in the singular or \"spp.\" (standing for \"species pluralis\", the Latin for multiple species) in the plural in place of the specific name or epithet (e.g. \"Canis\" sp.) This commonly occurs when authors are confident that some individuals belong to a particular genus but are not sure to which exact species they belong, as is common in paleontology. Authors may also use \"spp.\" as a short way of saying that something applies to many species within a genus, but not to all. If scientists mean that something applies to all species within a genus, they use the genus name without the specific name or epithet. The names of genera and species are usually printed in italics. Abbreviations such as \"sp.\" should not be italicised. When a species identity is not clear a specialist may use \"cf.\" before the epithet to indicate that confirmation is required. The abbreviations \"nr.\" (near) or \"aff.\" (affine) may be used when the identity is unclear but when the species appears to be similar to the species mentioned after.\n\nWith the rise of online databases, codes have been devised to provide identifiers for species that are already defined, including:\n\nThe naming of a particular species, including which genus (and higher taxa) it is placed in, is a \"hypothesis\" about the evolutionary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be confirmed or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual organisms later identified as the same species. When two named species are discovered to be of the same species, the older species name is given priority and usually retained, and the newer name considered as a junior synonym, a process called \"synonymisation\". Dividing a taxon into multiple, often new, taxa is called \"splitting\". Taxonomists are often referred to as \"lumpers\" or \"splitters\" by their colleagues, depending on their personal approach to recognising differences or commonalities between organisms.\n\nThe nomenclatural codes that guide the naming of species, including the ICZN for animals and the ICN for plants, do not make rules for defining the boundaries of the species. Research can change the boundaries, also known as circumscription, based on new evidence. Species may then need to be distinguished by the boundary definitions used, and in such cases the names may be qualified with \"sensu stricto\" (\"in the narrow sense\") to denote usage in the exact meaning given by an author such as the person who named the species, while the antonym \"sensu lato\" (\"in the broad sense\") denotes a wider usage, for instance including other subspecies. Other abbreviations such as \"auct.\" (\"author\"), and qualifiers such as \"non\" (\"not\") may be used to further clarify the sense in which the specified authors delineated or described the species.\n\nMost modern textbooks make use of Ernst Mayr's 1942 definition, known as the Biological Species Concept as a basis for further discussion on the definition of species. It is also called a reproductive or isolation concept. This defines a species as\n\nIt has been argued that this definition is a natural consequence of the effect of sexual reproduction on the dynamics of natural selection. Mayr's use of the adjective \"potentially\" has been a point of debate; some interpretations exclude unusual or artificial matings that occur only in captivity, or that involve animals capable of mating but that do not normally do so in the wild.\n\nIt is difficult to define a species in a way that applies to all organisms. The debate about species delimitation is called the species problem. The problem was recognized even in 1859, when Darwin wrote in \"On the Origin of Species\":\n\nA simple textbook definition, following Mayr's concept, works well for most multi-celled organisms, but breaks down in several situations:\n\n\nSpecies identification is made difficult by discordance between molecular and morphological investigations; these can be categorized as two types: (i) one morphology, multiple lineages (e.g. morphological convergence, cryptic species) and (ii) one lineage, multiple morphologies (e.g. phenotypic plasticity, multiple life-cycle stages). In addition, horizontal gene transfer (HGT) makes it difficult to define a species. All species definitions assume that an organism acquires its genes from one or two parents very like the \"daughter\" organism, but that is not what happens in HGT. There is strong evidence of HGT between very dissimilar groups of prokaryotes, and at least occasionally between dissimilar groups of eukaryotes, including some crustaceans and echinoderms.\n\nThe evolutionary biologist James Mallet concludes that\n\nThe species concept is further weakened by the existence of microspecies, groups of organisms, including many plants, with very little genetic variability, usually forming species aggregates. For example, the dandelion \"Taraxacum officinale\" and the blackberry \"Rubus fruticosus\" are aggregates with many microspecies—perhaps 400 in the case of the blackberry and over 200 in the dandelion, complicated by hybridisation, apomixis and polyploidy, making gene flow between populations difficult to determine, and their taxonomy debatable. Species complexes occur in insects such as \"Heliconius\" butterflies, vertebrates such as \"Hypsiboas\" treefrogs, and fungi such as the fly agaric.\n\nNatural hybridisation presents a challenge to the concept of a reproductively isolated species, as fertile hybrids permit gene flow between two populations. For example, the carrion crow \"Corvus corone\" and the hooded crow \"Corvus cornix\" appear and are classified as separate species, yet they hybridise freely where their geographical ranges overlap.\n\nA ring species is a connected series of neighbouring populations, each of which can sexually interbreed with adjacent related populations, but for which there exist at least two \"end\" populations in the series, which are too distantly related to interbreed, though there is a potential gene flow between each \"linked\" population. Such non-breeding, though genetically connected, \"end\" populations may co-exist in the same region thus closing the ring. Ring species thus present a difficulty for any species concept that relies on reproductive isolation. However, ring species are at best rare. Proposed examples include the herring gull-lesser black-backed gull complex around the North pole, the \"Ensatina eschscholtzii\" group of 19 populations of salamanders in America, and the greenish warbler in Asia, but many so-called ring species have turned out to be the result of misclassification leading to questions on whether there really are any ring species.\n\nBiologists and taxonomists have made many attempts to define species, beginning from morphology and moving towards genetics. Early taxonomists such as Linnaeus had no option but to describe what they saw: this was later formalised as the typological or morphological species concept. Mayr emphasised reproductive isolation, but this, like other species concepts, is hard or even impossible to test. Later biologists have tried to refine Mayr's definition with the recognition and cohesion concepts, among others. Many of the concepts are quite similar or overlap, so they are not easy to count: the biologist R. L. Mayden recorded about 24 concepts, and the philosopher of science John Wilkins counted 26. Wilkins further grouped the species concepts into seven basic kinds of concepts: (1) agamospecies for asexual organisms (2) biospecies for reproductively isolated sexual organisms (3) ecospecies based on ecological niches (4) evolutionary species based on lineage (5) genetic species based on gene pool (6) morphospecies based on form or phenotype and (7) taxonomic species, a species as determined by a taxonomist.\n\nA typological species is a group of organisms in which individuals conform to certain fixed properties (a type), so that even pre-literate people often recognise the same taxon as do modern taxonomists. The clusters of variations or phenotypes within specimens (such as longer or shorter tails) would differentiate the species. This method was used as a \"classical\" method of determining species, such as with Linnaeus early in evolutionary theory. However, different phenotypes are not necessarily different species (e.g. a four-winged \"Drosophila\" born to a two-winged mother is not a different species). Species named in this manner are called \"morphospecies\".\n\nIn the 1970s, Robert R. Sokal, Theodore J. Crovello and Peter Sneath proposed a variation on this, a phenetic species, defined as a set of organisms with a similar phenotype to each other, but a different phenotype from other sets of organisms. It differs from the morphological species concept in including a numerical measure of distance or similarity to cluster entities based on multivariate comparisons of a reasonably large number of phenotypic traits.\n\nA mate-recognition species is a group of sexually reproducing organisms that recognize one another as potential mates. Expanding on this to allow for post-mating isolation, a cohesion species is the most inclusive population of individuals having the potential for phenotypic cohesion through intrinsic cohesion mechanisms; no matter whether populations can hybridize successfully, they are still distinct cohesion species if the amount of hybridization is insufficient to completely mix their respective gene pools. A further development of the recognition concept is provided by the biosemiotic concept of species.\n\nIn microbiology, genes can move freely even between distantly related bacteria, possibly extending to the whole bacterial domain. As a rule of thumb, microbiologists have assumed that kinds of Bacteria or Archaea with 16S ribosomal RNA gene sequences more similar than 97% to each other need to be checked by DNA-DNA hybridisation to decide if they belong to the same species or not. This concept was narrowed in 2006 to a similarity of 98.7%.\n\nDNA-DNA hybridisation is outdated, and results have sometimes led to misleading conclusions about species, as with the pomarine and great skua. Modern approaches compare sequence similarity using computational methods.\n\nDNA barcoding has been proposed as a way to distinguish species suitable even for non-specialists to use. The so-called barcode is a region of mitochondrial DNA within the gene for cytochrome c oxidase. A database, Barcode of Life Data Systems (BOLD) contains DNA barcode sequences from over 190,000 species. However, scientists such as Rob DeSalle have expressed concern that classical taxonomy and DNA barcoding, which they consider a misnomer, need to be reconciled, as they delimit species differently. Genetic introgression mediated by endosymbionts and other vectors can further make barcodes ineffective in the identification of species.\n\nA phylogenetic or cladistic species is an evolutionarily divergent lineage, one that has maintained its hereditary integrity through time and space. A cladistic species is the smallest group of populations that can be distinguished by a unique set of morphological or genetic traits. Molecular markers may be used to determine genetic similarities in the nuclear or mitochondrial DNA of various species. For example, in a study done on fungi, studying the nucleotide characters using cladistic species produced the most accurate results in recognising the numerous fungi species of all the concepts studied. Versions of the Phylogenetic Species Concept may emphasize monophyly or diagnosability.\n\nUnlike the Biological Species Concept, a cladistic species does not rely on reproductive isolation, so it is independent of processes that are integral in other concepts. It works for asexual lineages, and can detect recent divergences, which the Morphological Species Concept cannot. However, it does not work in every situation, and may require more than one polymorphic locus to give an accurate result. The concept may lead to splitting of existing species, for example of Bovidae, into many new ones.\n\nAn evolutionary species, suggested by George Gaylord Simpson in 1951, is \"an entity composed of organisms which maintains its identity from other such entities through time and over space, and which has its own independent evolutionary fate and historical tendencies\". This differs from the biological species concept in embodying persistence over time. Wiley and Mayden state that they see the evolutionary species concept as \"identical\" to Willi Hennig's species-as-lineages concept, and assert that the biological species concept, \"the several versions\" of the phylogenetic species concept, and the idea that species are of the same kind as higher taxa are not suitable for biodiversity studies (with the intention of estimating the number of species accurately). They further suggest that the concept works for both asexual and sexually-reproducing species.\n\nAn ecological species is a set of organisms adapted to a particular set of resources, called a niche, in the environment. According to this concept, populations form the discrete phenetic clusters that we recognise as species because the ecological and evolutionary processes controlling how resources are divided up tend to produce those clusters.\n\nA genetic species as defined by Robert Baker and Robert Bradley is a set of genetically isolated interbreeding populations. This is similar to Mayr's Biological Species Concept, but stresses genetic rather than reproductive isolation. In the 21st century, a genetic species can be established by comparing DNA sequences, but other methods were available earlier, such as comparing karyotypes (sets of chromosomes) and allozymes (enzyme variants).\n\nAn evolutionarily significant unit (ESU) or \"wildlife species\" is a population of organisms considered distinct for purposes of conservation.\n\nIn palaeontology, with only comparative anatomy (morphology) from fossils as evidence, the concept of a chronospecies can be applied. During anagenesis (evolution, not necessarily involving branching), palaeontologists seek to identify a sequence of species, each one derived from the phyletically extinct one before through continuous, slow and more or less uniform change. In such a time sequence, palaeontologists assess how much change is required for a morphologically distinct form to be considered a different species from its ancestors.\n\nViruses have enormous populations, are doubtfully living since they consist of little more than a string of DNA or RNA in a protein coat, and mutate rapidly. All of these factors make conventional species concepts largely inapplicable. A viral quasispecies is a group of genotypes related by similar mutations, competing within a highly mutagenic environment, and hence governed by a mutation–selection balance. It is predicted that a viral quasispecies at a low but evolutionarily neutral and highly connected (that is, flat) region in the fitness landscape will outcompete a quasispecies located at a higher but narrower fitness peak in which the surrounding mutants are unfit, \"the quasispecies effect\" or the \"survival of the flattest\". There is no suggestion that a viral quasispecies resembles a traditional biological species.\n\nSpecies are subject to change, whether by evolving into new species, exchanging genes with other species, merging with other species or by becoming extinct.\n\nThe evolutionary process by which biological populations evolve to become distinct or reproductively isolated as species is called speciation. Charles Darwin was the first to describe the role of natural selection in speciation in his 1859 book \"The Origin of Species\". Speciation depends on a measure of reproductive isolation, a reduced gene flow. This occurs most easily in allopatric speciation, where populations are separated geographically and can diverge gradually as mutations accumulate. Reproductive isolation is threatened by hybridisation, but this can be selected against once a pair of populations have incompatible alleles of the same gene, as described in the Bateson–Dobzhansky–Muller model. A different mechanism, phyletic speciation, involves one lineage gradually changing over time into a new and distinct form, without increasing the number of resultant species.\n\nHorizontal gene transfer between organisms of different species, either through hybridisation, antigenic shift, or reassortment, is sometimes an important source of genetic variation. Viruses can transfer genes between species. Bacteria can exchange plasmids with bacteria of other species, including some apparently distantly related ones in different phylogenetic domains, making analysis of their relationships difficult, and weakening the concept of a bacterial species.\n\nLouis-Marie Bobay and Howard Ochman suggest, based on analysis of the genomes of many types of bacteria, that they can often be grouped \"into communities that regularly swap genes\", in much the same way that plants and animals can be grouped into reproductively isolated breeding populations. Bacteria may thus form species, analogous to Mayr's biological species concept, consisting of asexually reproducing populations that exchange genes by homologous recombination.\n\nA species is extinct when the last individual of that species dies, but it may be functionally extinct well before that moment. It is estimated that over 99 percent of all species that ever lived on Earth, some five billion species, are now extinct. Some of these were in mass extinctions such as those at the ends of the Permian, Triassic and Cretaceous periods. Mass extinctions had a variety of causes including volcanic activity, climate change, and changes in oceanic and atmospheric chemistry, and they in turn had major effects on Earth's ecology, atmosphere, land surface, and waters. Another form of extinction is through the assimilation of one species by another through hybridization. The resulting single species has been termed as a \"compilospecies\".\n\nBiologists and conservationists need to categorise and identify organisms in the course of their work. Difficulty assigning organisms reliably to a species constitutes a threat to the validity of research results, for example making measurements of how abundant a species is in an ecosystem moot. Surveys using a phylogenetic species concept reported 48% more species and accordingly smaller populations and ranges than those using nonphylogenetic concepts; this was termed \"taxonomic inflation\", which could cause a false appearance of change to the number of endangered species and consequent political and practical difficulties. Some observers claim that there is an inherent conflict between the desire to understand the processes of speciation and the need to identify and to categorise. \n\nConservation laws in many countries make special provisions to prevent species from going extinct. Hybridization zones between two species, one that is protected and one that is not, have sometimes led to conflicts between lawmakers, land owners and conservationists. One of the classic cases in North America is that of the protected northern spotted owl which hybridizes with the unprotected California spotted owl and the barred owl; this has led to legal debates. It has been argued that the species problem is created by the varied uses of the concept of species, and that the solution is to abandon it and all other taxonomic ranks, and use unranked monophyletic groups instead. It has been argued, too, that since species are not comparable, counting them is not a valid measure of biodiversity; alternative measures of phylogenetic biodiversity have been proposed.\n\n\n"}
{"id": "706576", "url": "https://en.wikipedia.org/wiki?curid=706576", "title": "Suffixaufnahme", "text": "Suffixaufnahme\n\nSuffixaufnahme (, \"suffix resumption\"), also known as case stacking, is a linguistic phenomenon used in forming a genitive construction, whereby prototypically a genitive noun agrees with its head noun. It was first recognized in Old Georgian and some other Caucasian and ancient Middle Eastern languages as well as many Australian languages, and almost invariably coincides with agglutinativity.\n\nA subject, for instance, would be marked with a subjective affix as well as a genitive affix. So, for example, in Old Georgian \"perx-ni k'ac-isa-ni\" (foot- man--) 'a man's feet', the genitival noun phrase agrees in case (nominative) and number (plural) with the head noun. \n\nHowever, while such a possessive construction is most frequently found in suffixaufnahme, other nominal constructions may also show similar behavior. In Old Georgian, a postpositional phrase modifying a noun could take on that noun's case and number features: \"\"Ra turpa prinvelia!\" c'amoidzaxa ert-ma bavshv-ta-gan-ma \"[one-ERG child-GEN.PL-from-ERG] (\"'What a wonderful bird!' exclaimed one of the children\") has the ergative (also called narrative) case \"-ma\" on \"ertma\" repeated in the modifying postpositional phrase, headed by \"-gan\".\n\n\n\n\n\n\n"}
{"id": "30623411", "url": "https://en.wikipedia.org/wiki?curid=30623411", "title": "Tai tai", "text": "Tai tai\n\nTai tai (太太) is a Chinese colloquial term for an elected leader-wife or head-wife of a multi-wife (polygynous / sister-wife) family; or a wealthy married woman who does not work. It is the same as the Cantonese title for a married woman. It has the same euphemistic value as \"lady\" in English: sometimes flattery, sometimes subtle insult. One author describes it as equivalent to the English term \"ladies who lunch\".\n\nBy the time of the May Fourth Movement in 1919, the term had come to imply a wife who was \"dependent on her newly rising bourgeois husband\" for her consumerist lifestyle, and Chinese feminists and \"new women\" of that era tried to disassociate themselves from the term precisely for that reason.\n\nThe term has become well-known, and features in Western discussions in the field of Women's studies. Pearl Buck uses the term to describe Madame Liang in her novel, Three Daughters of Madame Liang.\n"}
{"id": "206586", "url": "https://en.wikipedia.org/wiki?curid=206586", "title": "Technological convergence", "text": "Technological convergence\n\nConvergence is a phenomenon in telecommunication and media industry to integrate services, content offerings, and means of communication under one core technology or ecosystem. It includes cultural, society, business, service, technology, regulatory, and content aspects that need to be considered.\n\nIt’s practical application manifests in various concrete examples, as e.g. Internet, digital convergence, media convergence, home applications, and telecommunication. This article describes a wide are of aspects of convergence, with illustrations of convergent merging technologies (NBIC, nano-, bio-, info- and cognitive technologies) and convergence of media technology.\n\n\"Convergence is a deep integration of knowledge, tools, and all relevant activities of human activity for a common goal, to allow society to answer new questions to change the respective physical or social ecosystem. Such changes in the respective ecosystem open new trends, pathways, and opportunities in the following divergent phase of the process\" (Roco 2002, Bainbridge and Roco 2016 ). \nSiddhartha Menon defines convergence, in his \"Policy initiative Dilemmas on Media Covergence: A Cross National Perspective\", as integration and digitalization. Integration, here, is defined as \"a process of transformation measure by the degree to which diverse media such as phone, data broadcast and information technology infrastructures are combined into a single seamless all purpose network architecture platform\". Digitalization is not so much defined by its physical infrastructure, but by the content or the medium. Jan van Dijk suggests that \"digitalization means breaking down signals into bytes consisting of ones and zeros\". Convergence is defined by Blackman, 1998, as a trend in the evolution of technology services and industry structures. Convergence is later defined more specifically as the coming together of telecommunications, computing and broadcasting into a single digital bit-stream. Mueller stands against the statement that convergence is really a takeover of all forms of media by one technology: digital computers.\n\nMedia technological convergence is the tendency that as technology changes, different technological system sometimes evolve toward performing similar tasks. Digital convergence refers to the convergence of four industries into one conglomerate, ITTCE (Information Technologies, Telecommunication, Consumer Electronics, and Entertainment). Previously separate technologies such as voice (and telephony features), data (and productivity applications), and video can now share resources and interact with each other synergistically. Telecommunications convergence (also called \"network convergence\") describes emerging telecommunications technologies, and network architecture used to migrate multiple communications services into a single network. Specifically this involves the converging of previously distinct media such as telephony and data communications into common interfaces on single devices, such as most smart phones can make phone calls and search the web.\n\nMedia convergence is the interlinking of computing and other information technologies, media content, media companies and communication networks that have arisen as the result of the evolution and popularization of the Internet as well as the activities, products and services that have emerged in the digital media space. Closely linked to the multilevel process of media convergence are also several developments in different areas of the media and communication sector which are also summarized under the term of media deconvergence. Many experts view this as simply being the tip of the iceberg, as all facets of institutional activity and social life such as business, government, art, journalism, health, and education are increasingly being carried out in these digital media spaces across a growing network of information and communication technology devices. Also included in this topic is the basis of computer networks, wherein many different operating systems are able to communicate via different protocols. \nConvergent services, such as VoIP, IPTV, Smart TV, and others, tend to replace the older technologies and thus can disrupt markets. IP-based convergence is inevitable and will result in new service and new demand in the market. When the old technology converges into the public-owned common, IP based services become access-independent or less dependent. The old service is access-dependent.\n\nCommunication networks were designed to carry different types of information independently. The older media, such as television and radio, are broadcasting networks with passive audiences. Convergence of telecommunication technology permits the manipulation of all forms of information, voice, data, and video. Telecommunication has changed from a world of scarcity to one of seemingly limitless capacity. Consequently, the possibility of audience interactivity morphs the passive audience into an engaged audience. The historical roots of convergence can be traced back to the emergence of mobile telephony and the Internet, although the term properly applies only from the point in marketing history when fixed and mobile telephony began to be offered by operators as joined products. Fixed and mobile operators were, for most of the 1990s, independent companies. Even when the same organization marketed both products, these were sold and serviced independently.\n\nIn the 1990s an implicit and often explicit assumption was that new media was going to replace the old media and Internet was going to replace broadcasting. In Nicholas Negroponte's \"Being Digital\", Negroponte predicts the collapse of broadcast networks in favor of an era of narrow-casting. He also suggests that no government regulation can shatter the media conglomerate. \"The monolithic empires of mass media are dissolving into an array of cottage industries... Media barons of today will be grasping to hold onto their centralized empires tomorrow... The combined forces of technology and human nature will ultimately take a stronger hand in plurality than any laws Congress can invent.\" The new media companies claimed that the old media would be absorbed fully and completely into the orbit of the emerging technologies.\nGeorge Gilder dismisses such claims saying, \"The computer industry is converging with the television industry in the same sense that the automobile converged with the horse, the TV converged with the nickelodeon, the word-processing program converged with the typewriter, the CAD program converged with the drafting board, and digital desktop publishing converged with the Linotype machine and the letterpress.\" Gilder believes that computers had come not to transform mass culture but to destroy it.\n\nMedia companies put Media Convergence back to their agenda, after the dot-com bubble burst. Erstwhile Knight Ridder promulgated concept of portable magazines, newspaper, and books in 1994.\"Within news corporations it became increasingly obvious that an editorial model based on mere replication in the internet of contents that had previously been written for print newspapers, radio, or television was no longer sufficient.\" The rise of digital communication in the late 20th century has made it possible for media organizations (or individuals) to deliver text, audio, and video material over the same wired, wireless, or fiber-optic connections. At the same time, it inspired some media organizations to explore multimedia delivery of information. This digital convergence of news media, in particular, was called \"Mediamorphosis\" by researcher Roger Fidler , in his 1997 book by that name. Today, we are surrounded by a multi-level convergent media world where all modes of communication and information are continually reforming to adapt to the enduring demands of technologies, \"changing the way we create, consume, learn and interact with each other\".\n\nNBIC, an acronym for Nanotechnology, Biotechnology, Information technology and Cognitive science, was, in 2014, the most popular term for converging technologies. It was introduced into public discourse through the publication of \"Converging Technologies for Improving Human Performance\", a report sponsored in part by the U.S. National Science Foundation. Various other acronyms have been offered for the same concept such as GNR (Genetics, Nanotechnology and Robotics) (Bill Joy, 2000, Why the future doesn't need us). Journalist Joel Garreau in \"Radical Evolution: The Promise and Peril of Enhancing Our Minds, Our Bodies — and What It Means to Be Human\" uses \"GRIN\", for Genetic, Robotic, Information, and Nano processes, while science journalist Douglas Mulhall in \"Our Molecular Future: How Nanotechnology, Robotics, Genetics and Artificial Intelligence Will Transform Our World\" uses \"GRAIN\", for Genetics, Robotics, Artificial Intelligence, and Nanotechnology. Another acronym coined by the ETC Group is \"BANG\" for \"Bits, Atoms, Neurons, Genes\".\n\nA comprehensive term used by Roco, Bainbridge, Tonn and Whitesides is Convergence of Knowledge, Technology and Society (2013). Bainbridge and Roco edited and co-authored the Springer reference Handbook of Science and Technology Convergence (2016) defining the concept of convergence in various science, technology and medical fields. Roco published Principles and Methods that Facilitate Convergence (2015) \n\nConvergent solutions include both fixed-line and mobile technologies. Recent examples of new, convergent services include:\n\n\nConvergent technologies can integrate the fixed-line with mobile to deliver convergent solutions. Convergent technologies include:\n\n\nSome media observers expect that we will eventually access all media content through one device, or \"black box\". As such, media business practice has been to identify the next \"black box\" to invest in and provide media for. This has caused a number of problems. Firstly, as \"black boxes\" are invented and abandoned, the individual is left with numerous devices that can perform the same task, rather than one dedicated for each task. For example, one may own both a computer and a video games console, subsequently owning two DVD players. This is contrary to the streamlined goal of the \"black box\" theory, and instead creates clutter. Secondly, technological convergence tends to be experimental in nature. This has led to consumers owning technologies with additional functions that are harder, if not impractical, to use rather than one specific device. Many people would only watch the TV for the duration of the meal's cooking time, or whilst in the kitchen, but would not use the microwave as the household TV. These examples show that in many cases technological convergence is unnecessary or unneeded.\n\nFurthermore, although consumers primarily use a specialized media device for their needs, other \"black box\" devices that perform the same task can be used to suit their current situation. As a 2002 Cheskin Research report explained: \"...Your email needs and expectations are different whether you're at home, work, school, commuting, the airport, etc., and these different devices are designed to suit your needs for accessing content depending on where you are- your situated context.\" Despite the creation of \"black boxes\", intended to perform all tasks, the trend is to use devices that can suit the consumer's physical position. Due to the variable utility of portable technology, convergence occurs in high end mobile devices. They incorporate multimedia services, GPS, Internet access, and mobile telephony into a single device, heralding the rise of what has been termed the \"smart phone,\" a device designed to remove the need to carry multiple devices. Convergence of media occurs when multiple products come together to form one product with the advantages of all of them, also known as the black box. This idea of one technology, concocted by Henry Jenkins, has become known more as a fallacy because of the inability to actually put all technical pieces into one. For example, while people can have e-mail and Internet on their phone, they still want full computers with Internet and e-mail in addition. Mobile phones are a good example, in that they incorporate digital cameras, mp3 players, voice recorders, and other devices. This type of convergence is popular. For the consumer, it means more features in less space; for media conglomerates it means remaining competitive.\n\nHowever, convergence has a downside. Particularly in initial forms, converged devices are frequently less functional and reliable than their component parts (e.g., a mobile phone's web browser may not render some web pages correctly, due to not supporting certain rendering methods, such as the iPhone browser not supporting Flash content). As the number of functions in a single device escalates, the ability of that device to serve its original function decreases. As Rheingold asserts, technological convergence holds immense potential for the \"improvement of life and liberty in some ways and (could) degrade it in others\". He believes the same technology has the potential to be \"used as both a weapon of social control and a means of resistance\". Since technology has evolved in the past ten years or so, companies are beginning to converge technologies to create demand for new products. This includes phone companies integrating 3G and 4G on their phones. In the mid 20th century, television converged the technologies of movies and radio, and television is now being converged with the mobile phone industry and the Internet. Phone calls are also being made with the use of personal computers. Converging technologies combine multiple technologies into one. Newer mobile phones feature cameras, and can hold images, videos, music, and other media. Manufacturers now integrate more advanced features, such as video recording, GPS receivers, data storage, and security mechanisms into the traditional cellphone.\n\nThe role of the internet has changed from its original use as a communication tool to easier and faster access to information and services, mainly through a broadband connection. The television, radio and newspapers were the world's media for accessing news and entertainment; now, all three media have converged into one, and people all over the world can read and hear news and other information on the internet. The convergence of the internet and conventional TV became popular in the 2010s, through Smart TV, also sometimes referred to as \"Connected TV\" or \"Hybrid TV\", (not to be confused with IPTV, Internet TV, or with Web TV). Smart TV is used to describe the current trend of integration of the Internet and Web 2.0 features into modern television sets and set-top boxes, as well as the technological convergence between computers and these television sets or set-top boxes. These new devices most often also have a much higher focus on online interactive media, Internet TV, over-the-top content, as well as on-demand streaming media, and less focus on traditional broadcast media like previous generations of television sets and set-top boxes always have had.\n\nDigital Convergence means inclination for various innovations, media sources, content that become similar with the time. It enables the convergence of access devices and content as well as the industry participant operations and strategy. This is how this type of technological convergence creates opportunities, particularly in the area of product development and growth strategies for digital product companies. The same can be said in the case of individual content producers such as vloggers in the YouTube video-sharing platform. The convergence in this example is demonstrated in the involvement of the Internet, home devices such as smart television, camera, the YouTube application, and the digital content. In this setup, there are the so-called \"spokes\", which are the devices that connect to a central hub, which could either be the smart TV or PC. Here, the Internet serves as the intermediary, particularly through its interactivity tools and social networking, in order to create unique mixes of products and services via horizontal integration.\n\nThe above example highlights how digital convergence encompasses three phenomena:\n\n\nAnother example is the convergence of different types of digital contents. According to Harry Strasser, former CTO of Siemens \"[digital convergence will substantially impact people's lifestyle and work style]\". The next hot trend in digital convergence is converged content, mixing personal (user-generated) content with professional (copyright protected) content. An example are personal music videos that combine user-generated photos with chart music. The German startup Trivid GmbH has developed Clipgenerator that enables users to create personal music videos with popular chart music and to share them in social communities such as Facebook, Myspace and Bebo.\n\nConvergence is a global marketplace dynamic in which different companies and sectors are being brought together, both as competitors and collaborators, across traditional boundaries of industry and technology. In a world dominated by convergence, many traditional products, services and types of companies will become less relevant, but a stunning array of new ones is possible. An array of technology developments act as accelerators of convergence, including mobility, analytics, cloud, digital and social networks. As a disruptive force, convergence is a threat to the unprepared, but a tremendous growth opportunity for companies that can out-innovate and out-execute their ever-expanding list of competitors under dramatically new marketplace rules. With convergence, lines are blurred as companies diversify outside of their original markets. For instance, mobile services are increasingly an important part of the automobile; chemicals companies work with agribusiness; device manufacturers sell music, video and books; booksellers become consumer device companies; search and advertising companies become telecommunications companies (\"telcos\"); media companies act like telcos and vice versa; retailers act like financial services companies and vice versa; cosmetics companies work with pharmaceutical companies; and more. Mobile phone usage broadens dramatically, enabling users to make payments online, watch videos, or even adjusting their home thermostat while away at work.\n\nGenerally, media convergence refers to the merging of both old and new media and can be seen as a product, a system or a process. Jenkins states that convergence is, \"the flow of content across multiple media platforms, the cooperation between multiple media industries, and the migratory behaviour of media audiences who would go almost anywhere in search of the kinds of entertainment experiences they wanted\" According to Jenkins, there are five areas of convergence: technological, economic, social or organic, cultural and global. So media convergence is not just a technological shift or a technological process, it also includes shifts within the industrial, cultural, and social paradigms that encourage the consumer to seek out new information. Convergence, simply put, is how individual consumers interact with others on a social level and use various media platforms to create new experiences, new forms of media and content that connect us socially, and not just to other consumers, but to the corporate producers of media in ways that have not been as readily accessible in the past. However, Lugmayr and Dal Zotto argued, that media convergence takes place on technology, content, consumer, business model, and management level. They argue, that media convergence is a matter of evolution and can be described through the triadic phenomenon of convergence, divergence, and coexistence. Today's digital media ecosystems coexist, as e.g. mobile app stores provide vendor lock-ins into particular eco-systems; some technology platforms are converging under one technology, due to e.g. the usage of common communication protocols as in digital TV; and other media are diverging, as e.g. media content offerings are more and more specializing and provides a space for niche media.\n\nAdvances in technology bring the ability for technological convergence that Rheingold believes can alter the \"social-side effects,\" in that \"the virtual, social and physical world are colliding, merging and coordinating.\" It was predicted in the late 1980s, around the time that CD-ROM was becoming commonplace, that a digital revolution would take place, and that old media would be pushed to one side by new media. Broadcasting is increasingly being replaced by the Internet, enabling consumers all over the world the freedom to access their preferred media content more easily and at a more available rate than ever before.\n\nHowever, when the dot-com bubble of the 1990s suddenly popped, that poured cold water over the talk of such a digital revolution. In today's society, the idea of media convergence has once again emerged as a key point of reference as newer as well as established media companies attempt to visualize the future of the entertainment industry. If this revolutionary digital paradigm shift presumed that old media would be increasingly replaced by new media, the convergence paradigm that is currently emerging suggests that new and old media would interact in more complex ways than previously predicted. The paradigm shift that followed the digital revolution assumed that new media was going to change everything. When the dot com market crashed, there was a tendency to imagine that nothing had changed. The real truth lay somewhere in between as there were so many aspects of the current media environment to take into consideration. Many industry leaders are increasingly reverting to media convergence as a way of making sense in an era of disorientating change. In that respect, media convergence in theory is essentially an old concept taking on a new meaning. Media convergence, in reality, is more than just a shift in technology. It alters relationships between industries, technologies, audiences, genres and markets. Media convergence changes the rationality media industries operate in, and the way that media consumers process news and entertainment. Media convergence is essentially a process and not an outcome, so no single black box controls the flow of media. With proliferation of different media channels and increasing portability of new telecommunications and computing technologies, we have entered into an era where media constantly surrounds us.\n\nMedia convergence requires that media companies rethink existing assumptions about media from the consumer's point of view, as these affect marketing and programming decisions. Media producers must respond to newly empowered consumers.\nConversely, it would seem that hardware is instead diverging whilst media content is converging. Media has developed into brands that can offer content in a number of forms. Two examples of this are \"Star Wars\" and \"The Matrix\". Both are films, but are also books, video games, cartoons, and action figures. Branding encourages expansion of one concept, rather than the creation of new ideas. In contrast, hardware has diversified to accommodate media convergence. Hardware must be specific to each function. While most scholars argue that the flow of cross-media is accelerating, O'Donnell suggests, especially between films and video game, the semblance of media convergence is misunderstood by people outside of the media production industry. The conglomeration of media industry continues to sell the same story line in different media. For example, Batman is in comics, films, anime, and games. However, the data to create the image of batman in each media is created individually by different teams of creators. The same character and the same visual effect repetitively appear in different media is because of the synergy of media industry to make them similar as possible. In addition, convergence does not happen when the game of two different consoles is produced. No flows between two consoles because it is faster to create game from scratch for the industry.\n\nOne of the more interesting new media journalism forms is virtual reality. Reuters, a major international news service, has created and staffed a news “island” in the popular online virtual reality environment Second Life (www.secondlife.com, accessed January 3, 2008). Open to anyone, Second Life has emerged as a compelling 3D virtual reality for millions of citizens around the world who have created avatars (virtual representations of themselves) to populate and live in an altered state where personal flight is a reality, altered egos can flourish, and real money (US$1,296,257 were spent during the 24 hours concluding at 10:19 a.m. eastern time January 7, 2008) can be made without ever setting foot into the real world. The Reuters Island in Second Life is a virtual version of the Reuters real-world news service but covering the domain of Second Life for the citizens of Second Life (numbering 11,807,742 residents as of January 5, 2008).\n\nMedia convergence in the digital era means the changes that are taking place with older forms of media and media companies. Media convergence has two roles, the first is the technological merging of different media channels – for example, magazines, radio programs, TV shows, and movies, now are available on the Internet through laptops, iPads, and smartphones. As discussed in \"Media Culture\" (by Campbell), convergence of technology is not new. It has been going on since the late 1920s. An example is RCA, the Radio Corporation of America, which purchased Victor Talking Machine Company and introduced machines that could receive radio and play recorded music. Next came the TV, and radio lost some of its appeal as people started watching television, which has both talking and music as well as visuals. As technology advances, convergence of media change to keep up. The second definition of media convergence Campbell discusses is cross-platform by media companies. This usually involves consolidating various media holdings, such as cable, phone, television (over the air, satellite, cable) and Internet access under one corporate umbrella. This is not for the consumer to have more media choices, this is for the benefit of the company to cut down on costs and maximize its profits. As stated in the article, Convergence Culture and Media Work, by Mark Deuze, “the convergence of production and consumption of media across companies, channels, genres, and technologies is an expression of the convergence of all aspects of everyday life: work and play, the local and the global, self and social identity.\"\n\nHenry Jenkins determines convergence culture to be the flow of content across multiple media platforms, the cooperation between multiple media industries, and the migratory behavior of media audiences who will go almost anywhere in search of the kinds of entertainment experiences they want. The convergence culture is an important factor in transmedia storytelling. Convergence culture introduces new stories and arguments from one form of media into many. Transmedia storytelling is defined by Jenkins as a process \"where integral elements of a fiction get dispersed systematically across multiple delivery channels for the purpose of creating a unified and coordinated entertainment experience. Ideally, each medium makes its own unique contribution to the unfolding of the story\". For instance, The Matrix starts as a film, which is followed by two other instalments, but in a convergence culture it is not constrained to that form. It becomes a story not only told in the movies but in animated shorts, video games and comic books, three different media platforms. Online, a wiki is created to keep track of the story's expanding canon. Fan films, discussion forums, and social media pages also form, expanding The Matrix to different online platforms. Convergence culture took what started as a film and expanded it across almost every type of media. Bert is Evil (images) Bert and Bin Laden appeared in CNN coverage of anti-American protest following September 11. The association of Bert and Bin Laden links back to the Ignacio's Photoshop project for fun.\n\nConvergence culture is a part of participatory culture. Because average people can now access their interests on many types of media they can also have more of a say. Fans and consumers are able to participate in the creation and circulation of new content. Some companies take advantage of this and search for feedback from their customers through social media and sharing sites such as YouTube. \nBesides marketing and entertainment, convergence culture has also affected the way we interact with news and information. We can access news on multiple levels of media from the radio, TV, newspapers, and the internet. The internet allows more people to be able to report the news through independent broadcasts and therefore allows a multitude of perspectives to be put forward and accessed by people in many different areas. Convergence allows news to be gathered on a much larger scale. For instance, photographs were taken of torture at Abu Ghraib. These photos were shared and eventually posted on the internet. This led to the breaking of a news story in newspapers, on TV, and the internet.\n\nMedia scholar Henry Jenkins has described the media convergence with participatory culture as:\n\nThe social function of the cell phone changes as the technology converges. Because of technological advancement, cell phones function more than just as a phone. They contain an internet connection, video players, MP3 players, gaming, and a camera. Another example, Rok Sako To Rok Lo (2004) was screened in Delhi, Bangalore, Hyderabad, Mumbai, and other part of India through EDGE-enabled mobile phones with live video streaming facility.\n\nThe integration of social movements in cyberspace is one of the potential strategies that social movements can use in the age of media convergence. Because of the neutrality of the internet and the end-to-end design, the power structure of the internet was designed to avoid discrimination between applications. Mexico's Zapatistas campaign for land rights was one of the most influential case in the information age; Manuel Castells defines the Zapatistas as \"the first informational guerrilla movement\". The Zapatista uprising had been marginalized by the popular press. The Zapatistas were able to construct a grassroots, decentralized social movement by using the internet. The Zapatistas Effect, observed by Cleaver, continues to organize social movements on a global scale. A sophisticated webmetric analysis, which maps the links between different websites and seeks to identify important nodal points in a network, demonstrates that the Zapatistas cause binds together hundreds of global NGOs. The majority of the social movement organized by Zapatistas targets their campaign especially against global neoliberalism. A successful social movement not only need online support but also protest on the street. Papic wrote, \"Social Media Alone Do Not Instigate Revolutions\", which discusses how the use of social media in social movements needs good organization both online and offline. A study, \"Journalism in the age of media convergence: a survey of undergraduates’ technology-related news habits\", concluded that several focus group respondents reported they generally did not actively engage in media convergence, such as viewing slide shows or listening to podcast that accompanied an online story, as part of their Web-based news consumption, a significant number of students indicated the interactive features often associated with online news and media convergence were indeed appealing to them.\n\nThe U.S. Federal Communications Commission (FCC) has not been able to decide how to regulate VoIP (Internet Telephony) because the convergent technology is still growing and changing. In addition to its growth, FCC is tentative to set regulation on VoIP in order to promote competition in the telecommunication industry. There is not a clear line between telecommunication service and the information service because of the growth of the new convergent media. Historically, telecommunication is subject to state regulation. The state of California concerned about the increasing popularity of internet telephony will eventually obliterate funding for the Universal Service Fund Some states attempt to assert their traditional role of common carrier oversight onto this new technology. Meisel and Needles (2005) suggests that the FCC, federal courts, and state regulatory bodies on access line charges will directly impact the speed in which Internet telephony market grows. On one hand, the FCC is hesitant to regulate convergent technology because VoIP with different feature from the old Telecommunication; no fixed model to build legislature yet. On the other hand, the regulations is needed because Service over the internet might be quickly replaced telecommunication service, which will affect the entire economy.\n\nConvergence has also raised several debates about classification of certain telecommunications services. As the lines between data transmission, and voice and media transmission are eroded, regulators are faced with the task of how best to classify the converging segments of the telecommunication sector. Traditionally, telecommunication regulation has focused on the operation of physical infrastructure, networks, and access to network. No content is regulated in the telecommunication because the content is considered private. In contrast, film and Television are regulated by contents. The rating system regulates its distribution to the audience. Self-regulation is promoted by the industry. Bogle senior persuaded the entire industry to pay 0.1 percent levy on all advertising and the money was used to give authority to the Advertising Standards Authority, which keeps the government away from setting legislature in the media industry.\n\nThe premises to regulate the new media, two-ways communications, concerns much about the change from old media to new media. Each medium has different features and characteristics. First, internet, the new medium, manipulates all form of information – voice, data and video. Second, the old regulation on the old media, such as radio and Television, emphasized its regulation on the scarcity of the channels. Internet, on the other hand, has the limitless capacity, due to the end-to-end design. Third, Two-ways communication encourages interactivity between the content producers and the audiences. \n\"...Fundamental basis for classification, therefore, is to consider the need for regulation in terms of either market failure or in the public interests\"(Blackman). The Electronic Frontier Foundation (EFF), founded in 1990, is a non profit organization to defend free speech, privacy, innovation and consumer rights. DMCA, Digital Millennium Copyright Act regulates and protect the digital content producers and consumers.\n\nNetwork neutrality has emerged as an issue. Wu and Lessig (2004) set out two reasons to adapt neutral network model for computer networks. First, \"a neutral network eliminates the risk of future discrimination, providing more incentive to invest in broadband application development.\" Second, \"neutral network facilitates fair competition among application, no bias between applications.\" The two reasons also coincide with FCC's interest to stimulate investment and enhance innovation in broadband technology and services.\nDespite regulatory efforts of deregulation, privatization, and liberalization, the infrastructure barrier has been a negative factor in achieving effective competition. \"Kim et al. argues that IP dissociates the telephony application from the infrastructure and Internet telephony is at the forefront of such dissociation.\" The neutrality of the network is very important for fair competition. As the former FCC Charman Michael Powell put it: \"From its inception, the Internet was designed, as those present during the course of its creating will tell you, to prevent government or a corporation or anyone else from controlling it. It was designed to defeat discrimination against users, ideas and technologies\". Because of these reasons, Shin concludes that regulator should make sure to regulate application and infrastructure separately.\n\nThe layered model was first proposed by Solum and Chug, Sicker, and Nakahata. Sicker, Warbach and Witt have supported using a layered model to regulate the telecommunications industry with the emergence of convergence services. Many researchers have different layered approach, but they all agree that the emergence of convergent technology will create challenges and ambiguities for regulations. The key point of the layered model is that it reflects the reality of network architecture, and current business model.\nThe layered Model consists of 1. Access Layer – where the physical infrastructure resides: copper wires, cable, or fiber optic. 2. transport layer – the provider of service. 3. Application layer – the interface between the data and the users. 4. content layer – the layer which users see. In \"Convergence Technologies and the Layered Policy Model: Implication for Regulating Future Communications\", Shin combines the Layered Model and Network Neutrality as the principle to regulate the future convergent Media Industry.\n\nCombination services include those that integrate SMS with voice, such as voice SMS. Providers include Bubble Motion, Jott, Kirusa, and SpinVox. Several operators have launched services that combine SMS with mobile instant messaging (MIM) and presence. Text-to-landline services also exist, where subscribers can send text messages to any landline phone and are charged at standard rates. The text messages are converted into spoken language. This service has been popular in America, where fixed and mobile numbers are similar. Inbound SMS has been converging to enable reception of different formats (SMS, voice, MMS, etc.). UK companies, including consumer goods companies and media giants, should soon be able to let consumers contact them via voice, SMS, MMS, IVR, or video using one five-digit number or long number. In April 2008, O2 UK launched voice-enabled shortcodes, adding voice functionality to the five-digit codes already used for SMS. This type of convergence is helpful for media companies, broadcasters, enterprises, call centres and help desks who need to develop a consistent contact strategy with the consumer. Because SMS is very popular today, it became relevant to include text messaging as a contact possibility for consumers. To avoid having multiple numbers (one for voice calls, another one for SMS), a simple way is to merge the reception of both formats under one number. This means that a consumer can text or call one number and be sure that the message will be received.\n\n\"Mobile service provisions\" refers not only to the ability to purchase mobile phone services, but the ability to wirelessly access everything: voice, Internet, audio, and video. Advancements in WiMAX and other leading edge technologies provide the ability to transfer information over a wireless link at a variety of speeds, distances, and non-line-of-sight conditions.\n\nMulti-play is a marketing term describing the provision of different telecommunication services, such as Internet access, television, telephone, and mobile phone service, by organisations that traditionally only offered one or two of these services. Multi-play is a catch-all phrase; usually, the terms triple play (voice, video and data) or quadruple play (voice, video, data and wireless) are used to describe a more specific meaning. A dual play service is a marketing term for the provisioning of the two services: it can be high-speed Internet (digital subscriber line) and telephone service over a single broadband connection in the case of phone companies, or high-speed Internet (cable modem) and TV service over a single broadband connection in the case of cable TV companies. The convergence can also concern the underlying communication infrastructure. An example of this is a triple play service, where communication services are packaged allowing consumers to purchase TV, Internet, and telephony in one subscription. The broadband cable market is transforming as pay-TV providers move aggressively into what was once considered the telco space. Meanwhile, customer expectations have risen as consumer and business customers alike seek rich content, multi-use devices, networked products and converged services including on-demand video, digital TV, high speed Internet, VoIP, and wireless applications. It is uncharted territory for most broadband companies.\n\nA quadruple play service combines the triple play service of broadband Internet access, television, and telephone with wireless service provisions. This service set is also sometimes humorously referred to as \"The Fantastic Four\" or \"Grand Slam\". A fundamental aspect of the quadruple play is not only the long-awaited broadband convergence but also the players involved. Many of them, from the largest global service providers to whom we connect today via wires and cables to the smallest of startup service providers are interested. Opportunities are attractive: the big three telecom services – telephony, cable television, and wireless—could combine their industries. In the UK, the merger of NTL:Telewest and Virgin Mobile resulted in a company offering a quadruple play of cable television, broadband Internet, home telephone, and mobile telephone services.\n\nEarly in the 21st century, home LAN convergence so rapidly integrated home routers, wireless access points, and DSL modems that users were hard put to identify the resulting box they used to connect their computers to their Internet service. A general term for such a combined device is a residential gateway.\n\n\n\n"}
{"id": "31546166", "url": "https://en.wikipedia.org/wiki?curid=31546166", "title": "Teushen language", "text": "Teushen language\n\nThe Teushen language is an indigenous language of Argentina, which may be extinct. It was spoken by the Teushen people, a nomadic hunter-gatherer people of Patagonia, who lived between the Puelche people to their north and the Tehuelche people to the south, who occupied the central part of the Tierra del Fuego region. The tribe is now extinct.\n\nThe language is thought to be related to the Selk'nma, Puelche, and Tehuelche languages. These collectively belong to the Chonan language family.\n\nIn the early 19th century, some Tehuelche people also spoke Teushen.\n\n\n"}
{"id": "215288", "url": "https://en.wikipedia.org/wiki?curid=215288", "title": "War dialing", "text": "War dialing\n\nWar dialing or wardialing, a.k.a. \"janning\", is a technique to automatically scan a list of telephone numbers, usually dialing every number in a local area code to search for modems, computers, bulletin board systems (computer servers) and fax machines. Hackers use the resulting lists for various purposes: hobbyists for exploration, and crackers – malicious hackers who specialize in breaching computer security – for guessing user accounts (by capturing voicemail greetings), or locating modems that might provide an entry-point into computer or other electronic systems. It may also be used by security personnel, for example, to detect unauthorized devices, such as modems or faxes, on a company's telephone network.\n\nA single wardialing call would involve calling an unknown number, and waiting for one or two rings, since answering computers usually pick up on the first ring. If the phone rings twice, the modem hangs up and tries the next number. If a modem or fax machine answers, the wardialer program makes a note of the number. If a human or answering machine answers, the wardialer program hangs up. Depending on the time of day, wardialing 10,000 numbers in a given area code might annoy dozens or hundreds of people, some who attempt and fail to answer a phone in two rings, and some who succeed, only to hear the wardialing modem's carrier tone and hang up. The repeated incoming calls are especially annoying to businesses that have many consecutively numbered lines in the exchange, such as used with a Centrex telephone system.\n\nSome newer wardialing software, such as WarVOX, does not require a modem to conduct wardialing. Rather, such programs can use VOIP connections, which can speed up the number of calls that a wardialer can make. Sandstorm Enterprises has a patent on a multi-line war dialer. (\"System and Method for Scan-Dialing Telephone Numbers and Classifying Equipment Connected to Telephone Lines Associated therewith.\") The patented technology is implemented in Sandstorm's PhoneSweep war dialer.\n\nThe popular name for this technique originated in the 1983 film \"WarGames\". In the film, the protagonist programmed his computer to dial every telephone number in Sunnyvale, California to find other computer systems. Prior to the movie's release, this technique was known as \"hammer dialing\" or \"demon dialing\", but the film introduced the method to many, such as the members of The 414s. By 1985 at least one company advertised a \"War Games Autodialer\" for Commodore computers. Such programs became common on bulletin board systems of the time, with file names often truncated to wardial.exe and the like due to length restrictions of 8 characters on such systems. Eventually, the etymology of the name fell behind as \"war dialing\" gained its own currency within computing culture.\n\nThe popularity of wardialing in 1980s and 1990s prompted some states to enact legislation prohibiting the use of a device to dial telephone numbers without the intent of communicating with a person.\n\nA more recent phenomenon is wardriving, the searching for wireless networks (Wi-Fi) from a moving vehicle. Wardriving was named after wardialing, since both techniques involve actively scanning to find computer networks. The aim of wardriving is to collect information about wireless access points (not to be confused with piggybacking).\n\nSimilar to war dialing is a port scan under TCP/IP, which \"dials\" every TCP port of every IP address to find out what services are available. Unlike wardialing, however, a port scan will generally not disturb a human being when it tries an IP address, regardless of whether there is a computer responding on that address or not. Related to wardriving is warchalking, the practice of drawing chalk symbols in public places to advertise the availability of wireless networks.\n\nThe term is also used today by analogy for various sorts of exhaustive brute force attack against an authentication mechanism, such as a password. While a dictionary attack might involve trying each word in a dictionary as the password, \"wardialing the password\" would involve trying every possible password. Password protection systems are usually designed to make this impractical, by making the process slow and/or locking out an account for minutes or hours after some low number of wrong password entries.\n\n\n"}
{"id": "7833101", "url": "https://en.wikipedia.org/wiki?curid=7833101", "title": "Zande language", "text": "Zande language\n\nZande is the largest of the Zande languages. It is spoken by the Azande, primarily in the northeast of the Democratic Republic of the Congo and western South Sudan, but also in the eastern part of the Central African Republic. It is called Pazande in the Zande language and Kizande in Lingala.\n\n"}
