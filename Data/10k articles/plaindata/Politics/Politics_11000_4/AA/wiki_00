{"id": "966606", "url": "https://en.wikipedia.org/wiki?curid=966606", "title": "Al-Awda (guerrilla organization)", "text": "Al-Awda (guerrilla organization)\n\nAl-Awda (, ) is a secular guerrilla organization in Iraq. Al-Awda's name began appearing in Iraq in June 2003 in anti-occupation graffiti and leaflets in Baghdad and to the north and west of the capital. The group is led by Mohammed Younis al-Ahmed, who is based in Syria.\n\nThe organization was believed to be a network of underground cells, mainly in the key urban areas, composed of former Ba'ath Party officials, intelligence agents, former members of the Iraqi Republican Guard, and Fedayeen Saddam militia. The group was believed to rely on the pre-war organization of the Ba'ath Party and the relationships forged between various individuals and organizations within Saddam Hussein's regime.\n\nThe group's propaganda indicated that its goal was to restore the regime of Saddam Hussein to power, as the name indicates, and expel multinational occupation forces from the country. Al-Awda is believed is to be the term coined by the insurgents for the Ba'ath Party following the fall of Saddam Hussein from power. The name was apparently chosen for propaganda reasons to raise the threat of the Ba'ath Party's return to power and to evoke the Palestinian struggle against Israel.\n\nIn contrast to the JRTN, Ahmed has focused far more on securing political rehabilitation, amnesties and the repatriation of Baathist exiles than he has the violent overthrow of the Iraqi government and a Baathist return to power.\n\nAhmed, in his attempts to reunite the party, and built a close working relationship with the Syrian government. Unlike al-Douri, who distrusts the Syrians due to their alliance with the Iranians. The Syrian government is quietly supporting Ahmed in order to gain more control over the Iraqi Ba'ath party.\n\nAhmed's attempts to recruit support in Syria from former Iraqi Ba'athists is meeting some success, particularly among the poorer Sunni Arab segment of the refugee population, due in part to Ahmed's ability to offer cash incentives and Syrian residency permits due to their closeness to the Syrian government.\n\nThe al-Awda group led by Ahmed is believed to contain most of the remaining leading party figures who were not arrested or executed, including Mezher Motni Awad, To'ma Di'aiyef Getan, Jabbar Haddoosh, Sajer Zubair, and Nihad alDulaimi.\n\nIt could be said that al-Ahmed has returned to the Ba'ath Party's original ideology of secular pan-Arab nationalism which, in many cases, has proven successful in Iraq's Shi'a dominated southern provinces. However, despite his attempts, al-Ahmed has failed in his goal to overthrow al-Douri. Al-Douri's faction is the largest and the most active on the Internet, and the large majority of Ba'athist websites are aligned to al-Douri. Another failure is that al-Ahmed's faction, which is based in Syria, does not have exclusive Syrian support and, considering that it is based in Syria, the party is susceptible to Syrian interference in its affairs. However, despite the differences between the al-Douri and al-Ahmed factions, both of them adhere to Ba'athist thought.\n\nIn contrast to al-Douri's group, al-Ahmad's faction has had success in recruiting Shi'as to the party. While al-Ahmed and the faction's senior leaders are Sunnis, there are many Shiites who are working in the organization's middle level. Upon his election as leader, an al-Ahmed's faction statement said he was \"of Shia origins and coming from Shia areas in Nineveh governorate\". In contrast to al-Ahmed, al-Douri has stuck to a more conservative policy, recruiting members from a largely Sunni-dominated areas.\n\nThe guerrillas primarily operated in the Sunni Triangle, north and west of Baghdad, where before the war, the Ba'ath Party was an omnipresent part of society and support for Saddam Hussein was strongest. During the early stages of the insurgency, at the beginning of the summer of 2003, Coalition intelligence believed al-Awda to be the greatest armed threat to the multinational coalition.\n\nSince then, other insurgent groups, especially those with Islamist leanings, have apparently eclipsed al-Awda in the insurgency and the group has not surfaced often in the media since the autumn of 2003. Nevertheless, al-Awda members are still believed to be operating in north-central Iraq, in the Sunni Arab areas between Samarra and Mosul, and playing a supporting role in the insurgency.\n\nThe most prominent member of al-Awda, Sheikh Abdul Sattar Abu Risha, has since helped to form the Anbar Salvation Council, a group allied to the Americans and the Iraqi government.\n\nThe al-Awda party has schismed from the wider party, led by al-Douri. Following al-Douri's succession as the Regional Secretary of the Ba'ath Party, Younis al-Ahmed, called for a General Conference of the Iraqi Ba'ath party in Syria to elect a new leadership. This move caused a significant amount of controversy within the party, with al-Douri issuing a statement criticizing Syria for what al-Douri claimed was an American-supported attempt to undermine the Iraqi Ba'ath party, although this statement was later downplayed. The conference elected al-Ahmed as secretary-general, and al-Ahmed issued an order expelling al-Douri from the party, resulting in al-Douri issuing a counter order expelling al-Ahmed and 150 other party members. These events led to the existence, in effect, of two Iraqi Ba'ath Parties: the main party led by al-Douri, and the splinter al-Awda party led by al-Ahmed.\n\nAccording to leaked Wikileaks cables, in March 2009 several members of the former Ba'athist government claiming to represent the Mohammed Younis al-Ahmed led faction of the Ba'ath party approached Coalition Forces and the Provincial Reconstruction Team in Saladin Province. The figures met with representatives of the Coalition, instead of representatives of the Iraqi Government, because they claimed the Iraqi government was under Iranian influence, and might seek revenge against any Ba'ath Party members.\n\nThe representatives claimed that the Younis-led faction were dissatisfied with the present government of Iraq, which they claimed was both sectarian and also failing to provide infrastructure and public services. The representatives claimed that the Younis led faction wasn't opposed to democracy, and instead wished to peacefully participate in the democratic process. They also claimed that unlike the al-Douri led faction, they recognized that the pre-2003 Ba'athist government had made many mistakes, and that Iraq could not return to that system of government.\n\nIn December 2008 some 25 security officials were arrested for membership of Awda and attempting to restore the Ba'ath party, with some claiming they were planning a coup. The actual number of those involved may have reached 35, and included both Sunnis and Shiites and high ranking generals at the Interior Ministry, some of whom Awda had allegedly recruited through bribery.\n\nAn Awda party senior official was arrested in a crackdown on the organisation in Baaquba, Diyala, on 2 July 2010.\n\nIn October 2011 Iraqi security figures announced that they had detained 350 members of the Awda party, in a large operation across several provinces. The government claimed the group had been trying to reorganize the Ba'ath party, and work to undermine stability in the country, with a mind to seizing power following the US withdrawal the following year. The group appeared to be quite active in Nasiriyah, with 36 Ba'ath party leaders arrested there.\n\n"}
{"id": "1634", "url": "https://en.wikipedia.org/wiki?curid=1634", "title": "Aquaculture", "text": "Aquaculture\n\nAquaculture (less commonly spelled aquiculture), also known as aquafarming, is the farming of fish, crustaceans, molluscs, aquatic plants, algae, and other organisms. Aquaculture involves cultivating freshwater and saltwater populations under controlled conditions, and can be contrasted with commercial fishing, which is the harvesting of wild fish. Mariculture refers to aquaculture practiced in marine environments and in underwater habitats.\n\nAccording to the Food and Agriculture Organization (FAO), aquaculture \"is understood to mean the farming of aquatic organisms including fish, molluscs, crustaceans and aquatic plants. Farming implies some form of intervention in the rearing process to enhance production, such as regular stocking, feeding, protection from predators, etc. Farming also implies individual or corporate ownership of the stock being cultivated.\" The reported output from global aquaculture operations in 2014 supplied over one half of the fish and shellfish that is directly consumed by humans; however, there are issues about the reliability of the reported figures. Further, in current aquaculture practice, products from several pounds of wild fish are used to produce one pound of a piscivorous fish like salmon.\n\nParticular kinds of aquaculture include fish farming, shrimp farming, oyster farming, mariculture, algaculture (such as seaweed farming), and the cultivation of ornamental fish. Particular methods include aquaponics and integrated multi-trophic aquaculture, both of which integrate fish farming and aquatic plant farming.\nThe indigenous Gunditjmara people in Victoria, Australia, may have raised eels as early as 6000 BC. Evidence indicates they developed about of volcanic floodplains in the vicinity of Lake Condah into a complex of channels and dams, and used woven traps to capture eels, and preserve them to eat all year round.\n\nAquaculture was operating in China \"circa\" 2000 BC. When the waters subsided after river floods, some fish, mainly carp, were trapped in lakes. Early aquaculturists fed their brood using nymphs and silkworm feces, and ate them. A fortunate genetic mutation of carp led to the emergence of goldfish during the Tang dynasty. However, ancient Egyptians might have farmed fish (especialy Gilt-head bream) from Lake Bardawil about 3,500 years ago, and they even traded them with Canaan.\n\n\"Gim\" cultivation is the oldest aquaculture in Korea. Early cultivation methods used bamboo or oak sticks, which were replaced by newer methods that utilized nets in the 19th century. Floating rafts have been used for mass production since the 1920s.\n\nJapanese cultivated seaweed by providing bamboo poles and, later, nets and oyster shells to serve as anchoring surfaces for spores.\n\nRomans bred fish in ponds and farmed oysters in coastal lagoons before 100 CE.\n\nIn central Europe, early Christian monasteries adopted Roman aquacultural practices. Aquaculture spread in Europe during the Middle Ages since away from the seacoasts and the big rivers, fish had to be salted so they did not rot. Improvements in transportation during the 19th century made fresh fish easily available and inexpensive, even in inland areas, making aquaculture less popular. The 15th-century fishponds of the Trebon Basin in the Czech Republic are maintained as a UNESCO World Heritage Site.\n\nHawaiians constructed oceanic fish ponds. A remarkable example is the \"Menehune\" fishpond dating from at least 1,000 years ago, at Alekoko. Legend says that it was constructed by the mythical Menehune dwarf people.\n\nIn the first half of the 18th century, German experimented with external fertilization of brown trouts and salmon. He wrote an article \"\"Von der künstlichen Erzeugung der Forellen und Lachse\".\" By the latter decades of the 18th century, oyster farming had begun in estuaries along the Atlantic Coast of North America.\n\nThe word aquaculture appeared in an 1855 newspaper article in reference to the harvesting of ice. It also appeared in descriptions of the terrestrial agricultural practise of subirrigation in the late 19th century before becoming associated primarily with the cultivation of aquatic plant and animal species.\n\nIn 1859, Stephen Ainsworth of West Bloomfield, New York, began experiments with brook trout. By 1864, Seth Green had established a commercial fish-hatching operation at Caledonia Springs, near Rochester, New York. By 1866, with the involvement of Dr. W. W. Fletcher of Concord, Massachusetts, artificial fish hatcheries were under way in both Canada and the United States. When the Dildo Island fish hatchery opened in Newfoundland in 1889, it was the largest and most advanced in the world. The word aquaculture was used in descriptions of the hatcheries experiments with cod and lobster in 1890.\n\nBy the 1920s, the American Fish Culture Company of Carolina, Rhode Island, founded in the 1870s was one of the leading producers of trout. During the 1940s, they had perfected the method of manipulating the day and night cycle of fish so that they could be artificially spawned year around.\n\nCalifornians harvested wild kelp and attempted to manage supply around 1900, later labeling it a wartime resource.\n\nHarvest stagnation in wild fisheries and overexploitation of popular marine species, combined with a growing demand for high-quality protein, encouraged aquaculturists to domesticate other marine species. At the outset of modern aquaculture, many were optimistic that a \"Blue Revolution\" could take place in aquaculture, just as the Green Revolution of the 20th century had revolutionized agriculture. Although land animals had long been domesticated, most seafood species were still caught from the wild. Concerned about the impact of growing demand for seafood on the world's oceans, prominent ocean explorer Jacques Cousteau wrote in 1973: \"With earth’s burgeoning human populations to feed, we must turn to the sea with new understanding and new technology.”\n\nAbout 430 (97%) of the species cultured were domesticated during the 20th and 21st centuries, of which an estimated 106 came in the decade to 2007. Given the long-term importance of agriculture, to date, only 0.08% of known land plant species and 0.0002% of known land animal species have been domesticated, compared with 0.17% of known marine plant species and 0.13% of known marine animal species. Domestication typically involves about a decade of scientific research. Domesticating aquatic species involves fewer risks to humans than do land animals, which took a large toll in human lives. Most major human diseases originated in domesticated animals, including diseases such as smallpox and diphtheria, that like most infectious diseases, move to humans from animals. No human pathogens of comparable virulence have yet emerged from marine species. \n\nBiological control methods to manage parasites are already being used, such as cleaner fish (e.g. lumpsuckers and wrasse) to control sea lice populations in salmon farming. Models are being used to help with spatial planning and siting of fish farms in order to minimize impact.\n\nThe decline in wild fish stocks has increased the demand for farmed fish. However, finding alternative sources of protein and oil for fish feed is necessary so the aquaculture industry can grow sustainably; otherwise, it represents a great risk for the over-exploitation of forage fish.\n\nAnother recent issue following the banning in 2008 of organotins by the International Maritime Organization is the need to find environmentally friendly, but still effective, compounds with antifouling effects.\n\nMany new natural compounds are discovered every year, but producing them on a large enough scale for commercial purposes is almost impossible.\n\nIt is highly probable that future developments in this field will rely on microorganisms, but greater funding and further research is needed to overcome the lack of knowledge in this field.\n\nMicroalgae, also referred to as phytoplankton, microphytes, or planktonic algae, constitute the majority of cultivated algae. Macroalgae commonly known as seaweed also have many commercial and industrial uses, but due to their size and specific requirements, they are not easily cultivated on a large scale and are most often taken in the wild.\n\nThe farming of fish is the most common form of aquaculture. It involves raising fish commercially in tanks, fish ponds, or ocean enclosures, usually for food. A facility that releases juvenile fish into the wild for recreational fishing or to supplement a species' natural numbers is generally referred to as a fish hatchery. Worldwide, the most important fish species used in fish farming are, in order, carp, salmon, tilapia, and catfish.\n\nIn the Mediterranean, young bluefin tuna are netted at sea and towed slowly towards the shore. They are then interned in offshore pens where they are further grown for the market. In 2009, researchers in Australia managed for the first time to coax southern bluefin tuna to breed in landlocked tanks. Southern bluefin tuna are also caught in the wild and fattened in grow-out sea cages in southern Spencer Gulf, South Australia.\n\nA similar process is used in the salmon-farming section of this industry; juveniles are taken from hatcheries and a variety of methods are used to aid them in their maturation. For example, as stated above, some of the most important fish species in the industry, salmon, can be grown using a cage system. This is done by having netted cages, preferably in open water that has a strong flow, and feeding the salmon a special food mixture that aids their growth. This process allows for year-round growth of the fish, thus a higher harvest during the correct seasons. An additional method, known sometimes as sea ranching, has also been used within the industry. Sea ranching involves raising fish in a hatchery for a brief time and then releasing them into marine waters for further development, whereupon the fish are recaptured when they have matured.\n\nCommercial shrimp farming began in the 1970s, and production grew steeply thereafter. Global production reached more than 1.6 million tonnes in 2003, worth about US$9 billion. About 75% of farmed shrimp is produced in Asia, in particular in China and Thailand. The other 25% is produced mainly in Latin America, where Brazil is the largest producer. Thailand is the largest exporter.\n\nShrimp farming has changed from its traditional, small-scale form in Southeast Asia into a global industry. Technological advances have led to ever higher densities per unit area, and broodstock is shipped worldwide. Virtually all farmed shrimp are penaeids (i.e., shrimp of the family Penaeidae), and just two species of shrimp, the Pacific white shrimp and the giant tiger prawn, account for about 80% of all farmed shrimp. These industrial monocultures are very susceptible to disease, which has decimated shrimp populations across entire regions. Increasing ecological problems, repeated disease outbreaks, and pressure and criticism from both nongovernmental organizations and consumer countries led to changes in the industry in the late 1990s and generally stronger regulations. In 1999, governments, industry representatives, and environmental organizations initiated a program aimed at developing and promoting more sustainable farming practices through the Seafood Watch program.\n\nFreshwater prawn farming shares many characteristics with, including many problems with, marine shrimp farming. Unique problems are introduced by the developmental lifecycle of the main species, the giant river prawn.\n\nThe global annual production of freshwater prawns (excluding crayfish and crabs) in 2007 was about 460,000 tonnes, exceeding 1.86 billion dollars. Additionally, China produced about 370,000 tonnes of Chinese river crab.\n\nAquacultured shellfish include various oyster, mussel, and clam species. These bivalves are filter and/or deposit feeders, which rely on ambient primary production rather than inputs of fish or other feed. As such, shellfish aquaculture is generally perceived as benign or even beneficial.\n\nDepending on the species and local conditions, bivalve molluscs are either grown on the beach, on longlines, or suspended from rafts and harvested by hand or by dredging. In May 2017 a Belgian consortium installed the first of two trial mussel farms on a wind farm in the North Sea.\n\nAbalone farming began in the late 1950s and early 1960s in Japan and China. Since the mid-1990s, this industry has become increasingly successful. Overfishing and poaching have reduced wild populations to the extent that farmed abalone now supplies most abalone meat. Sustainably farmed molluscs can be certified by Seafood Watch and other organizations, including the World Wildlife Fund (WWF). WWF initiated the \"Aquaculture Dialogues\" in 2004 to develop measurable and performance-based standards for responsibly farmed seafood. In 2009, WWF co-founded the Aquaculture Stewardship Council with the Dutch Sustainable Trade Initiative to manage the global standards and certification programs.\n\nAfter trials in 2012, a commercial \"sea ranch\" was set up in Flinders Bay, Western Australia, to raise abalone. The ranch is based on an artificial reef made up of 5000 () separate concrete units called \"abitats\" (abalone habitats). The 900 kg abitats can host 400 abalone each. The reef is seeded with young abalone from an onshore hatchery. The abalone feed on seaweed that has grown naturally on the abitats, with the ecosystem enrichment of the bay also resulting in growing numbers of dhufish, pink snapper, wrasse, and Samson fish, among other species.\n\nBrad Adams, from the company, has emphasised the similarity to wild abalone and the difference from shore-based aquaculture. \"We're not aquaculture, we're ranching, because once they're in the water they look after themselves.\"\n\nOther groups include aquatic reptiles, amphibians, and miscellaneous invertebrates, such as echinoderms and jellyfish. They are separately graphed at the top right of this section, since they do not contribute enough volume to show clearly on the main graph.\n\nCommercially harvested echinoderms include sea cucumbers and sea urchins. In China, sea cucumbers are farmed in artificial ponds as large as .\n\nIn 2012, the total world production of fisheries was 158 million tonnes, of which aquaculture contributed 66.6 million tonnes, about 42%. The growth rate of worldwide aquaculture has been sustained and rapid, averaging about 8% per year for over 30 years, while the take from wild fisheries] has been essentially flat for the last decade. The aquaculture market reached $86 billion in 2009.\n\nAquaculture is an especially important economic activity in China. Between 1980 and 1997, the Chinese Bureau of Fisheries reports, aquaculture harvests grew at an annual rate of 16.7%, jumping from 1.9 million tonnes to nearly 23 million tonnes. In 2005, China accounted for 70% of world production. Aquaculture is also currently one of the fastest-growing areas of food production in the U.S.\n\nAbout 90% of all U.S. shrimp consumption is farmed and imported. In recent years, salmon aquaculture has become a major export in southern Chile, especially in Puerto Montt, Chile's fastest-growing city.\n\nA United Nations report titled \"The State of the World Fisheries and Aquaculture\" released in May 2014 maintained fisheries and aquaculture support the livelihoods of some 60 million people in Asia and Africa.\n\nLaws governing aquaculture practices vary greatly by country and are often not closely regulated or easily traceable. In the United States, land-based and nearshore aquaculture is regulated at the federal and state levels; however, no national laws govern offshore aquaculture in U.S. exclusive economic zone waters. In June 2011, the Department of Commerce and National Oceanic and Atmospheric Administration released national aquaculture policies to address this issue and \"to meet the growing demand for healthy seafood, to create jobs in coastal communities, and restore vital ecosystems.\" In 2011, Congresswoman Lois Capps introduced the \"National Sustainable Offshore Aquaculture Act of 2011\" \"to establish a regulatory system and research program for sustainable offshore aquaculture in the United States exclusive economic zone\"; however, the bill was not enacted into law.\n\nChina overwhelmingly dominates the world in reported aquaculture output, reporting a total output which is double that of the rest of the world put together. However, there are some historical issues with the accuracy of China's returns.\n\nIn 2001, the fisheries scientists Reg Watson and Daniel Pauly expressed concerns in a letter to \"Nature\", that China was over reporting its catch from wild fisheries in the 1990s. They said that made it appear that the global catch since 1988 was increasing annually by 300,000 tonnes, whereas it was really shrinking annually by 350,000 tonnes. Watson and Pauly suggested this may be have been related to Chinese policies where state entities that monitored the economy were also tasked with increasing output. Also, until more recently, the promotion of Chinese officials was based on production increases from their own areas.\n\nChina disputed this claim. The official Xinhua News Agency quoted Yang Jian, director general of the Agriculture Ministry's Bureau of Fisheries, as saying that China's figures were \"basically correct\". However, the FAO accepted there were issues with the reliability of China's statistical returns, and for a period treated data from China, including the aquaculture data, apart from the rest of the world.\n\nMariculture refers to the cultivation of marine organisms in seawater, usually in sheltered coastal or offshore waters. The farming of marine fish is an example of mariculture, and so also is the farming of marine crustaceans (such as shrimp), molluscs (such as oysters), and seaweed. Atlantic salmon and mollusk farms is for example prominent in the U.S.\n\nMariculture may consist of raising the organisms on or in artificial enclosures such as in floating netted enclosures for salmon and on racks for oysters. In the case of enclosed salmon, they are fed by the operators; oysters on racks filter feed on naturally available food. Abalone have been farmed on an artificial reef consuming seaweed which grows naturally on the reef units.\n\nIntegrated multi-trophic aquaculture (IMTA) is a practice in which the byproducts (wastes) from one species are recycled to become inputs (fertilizers, food) for another. Fed aquaculture (for example, fish, shrimp) is combined with inorganic extractive and organic extractive (for example, shellfish) aquaculture to create balanced systems for environmental sustainability (biomitigation), economic stability (product diversification and risk reduction) and social acceptability (better management practices).\n\n\"Multi-trophic\" refers to the incorporation of species from different trophic or nutritional levels in the same system. This is one potential distinction from the age-old practice of aquatic polyculture, which could simply be the co-culture of different fish species from the same trophic level. In this case, these organisms may all share the same biological and chemical processes, with few synergistic benefits, which could potentially lead to significant shifts in the ecosystem. Some traditional polyculture systems may, in fact, incorporate a greater diversity of species, occupying several niches, as extensive cultures (low intensity, low management) within the same 2006\"/> A working IMTA system can result in greater total production based on mutual benefits to the co-cultured species and improved ecosystem health, even if the production of individual species is lower than in a monoculture over a short term period.\n\nSometimes the term \"integrated aquaculture\" is used to describe the integration of monocultures through water transfer. For all intents and purposes, however, the terms \"IMTA\" and \"integrated aquaculture\" differ only in their degree of descriptiveness. Aquaponics, fractionated aquaculture, integrated agriculture-aquaculture systems, integrated peri-urban-aquaculture systems, and integrated fisheries-aquaculture systems are other variations of the IMTA concept.\n\nVarious materials, including nylon, polyester, polypropylene, polyethylene, plastic-coated welded wire, rubber, patented rope products (Spectra, Thorn-D, Dyneema), galvanized steel and copper are used for netting in aquaculture fish enclosures around the world. All of these materials are selected for a variety of reasons, including design feasibility, material strength, cost, and corrosion resistance.\nRecently, copper alloys have become important netting materials in aquaculture because they are antimicrobial (i.e., they destroy bacteria, viruses, fungi, algae, and other microbes) and they therefore prevent biofouling (i.e., the undesirable accumulation, adhesion, and growth of microorganisms, plants, algae, tubeworms, barnacles, mollusks, and other organisms). By inhibiting microbial growth, copper alloy aquaculture cages avoid costly net changes that are necessary with other materials. The resistance of organism growth on copper alloy nets also provides a cleaner and healthier environment for farmed fish to grow and thrive.\n\nIf performed without consideration for potential local environmental impacts, aquaculture in inland waters can result in more environmental damaging than wild fisheries, though with less waste produced on a per kg on a global scale. Local concerns with aquaculture in inland waters may include waste handling, side-effects of antibiotics, competition between farmed and wild animals, and the potential introduction of invasive plant and animal species, or foreign pathogens, particularly if unprocessed fish are used to feed more marketable carnivorous fish. If non-local live feeds are used, aquaculture may introduce plant of animal. Improvements in methods resulting from advances in research and the availability of commercial feeds has reduced some of these concerns since their greater prevalence in the 1990s and 2000s .\n\nFish waste is organic and composed of nutrients necessary in all components of aquatic food webs. In-ocean aquaculture often produces much higher than normal fish waste concentrations. The waste collects on the ocean bottom, damaging or eliminating bottom-dwelling life. Waste can also decrease dissolved oxygen levels in the water column, putting further pressure on wild animals. An alternative model to food being added to the ecosystem, is the installation of artificial reef structures to increase the habitat niches available, without the need to add any more than ambient feed and nutrient. This has been used in the \"ranching\" of abalone in Western Australia.\n\nTilapia from aquaculture has been shown to contain more fat and a much higher ratio of omega-6 to omega-3 oils.\n\nSome carnivorous and omnivorous farmed fish species are fed wild forage fish. Although carnivorous farmed fish represented only 13 percent of aquaculture production by weight in 2000, they represented 34 percent of aquaculture production by value.\n\nFarming of carnivorous species like salmon and shrimp leads to a high demand for forage fish to match the nutrition they get in the wild. Fish do not actually produce omega-3 fatty acids, but instead accumulate them from either consuming microalgae that produce these fatty acids, as is the case with forage fish like herring and sardines, or, as is the case with fatty predatory fish, like salmon, by eating prey fish that have accumulated omega-3 fatty acids from microalgae. To satisfy this requirement, more than 50 percent of the world fish oil production is fed to farmed salmon.\n\nFarmed salmon consume more wild fish than they generate as a final product, although the efficiency of production is improving. To produce one pound of farmed salmon, products from several pounds of wild fish are fed to them - this can be described as the \"fish-in-fish-out\" (FIFO) ratio. In 1995, salmon had a FIFO ratio of 7.5 (meaning 7.5 pounds of wild fish feed were required to produce 1 pound of salmon); by 2006 the ratio had fallen to 4.9. Additionally, a growing share of fish oil and fishmeal come from residues (byproducts of fish processing), rather than dedicated whole fish. In 2012, 34 percent of fish oil and 28 percent of fishmeal came from residues. However, fishmeal and oil from residues instead of whole fish have a different composition with more ash and less protein, which may limit its potential use for aquaculture.\n\nAs the salmon farming industry expands, it requires more wild forage fish for feed, at a time when seventy five percent of the worlds monitored fisheries are already near to or have exceeded their maximum sustainable yield. The industrial scale extraction of wild forage fish for salmon farming then impacts the survivability of the wild predator fish who rely on them for food. An important step in reducing the impact of aquaculture on wild fish is shifting carnivorous species to plant-based feeds. Salmon feeds, for example, have gone from containing only fishmeal and oil to containing 40 percent plant protein. The USDA has also experimented with using grain-based feeds for farmed trout. When properly formulated (and often mixed with fishmeal or oil), plant-based feeds can provide proper nutrition and similar growth rates in carnivorous farmed fish.\n\nAnother impact aquaculture production can have on wild fish is the risk of fish escaping from coastal pens, where they can interbreed with their wild counterparts, diluting wild genetic stocks. Escaped fish can become invasive, out-competing native species.\n\nAquaculture is becoming a significant threat to coastal ecosystems. About 20 percent of mangrove forests have been destroyed since 1980, partly due to shrimp farming. An extended cost–benefit analysis of the total economic value of shrimp aquaculture built on mangrove ecosystems found that the external costs were much higher than the external benefits. Over four decades, of Indonesian mangroves have been converted to shrimp farms. Most of these farms are abandoned within a decade because of the toxin build-up and nutrient loss.\n\nSalmon farms are typically sited in pristine coastal ecosystems which they then pollute. A farm with 200,000 salmon discharges more fecal waste than a city of 60,000 people. This waste is discharged directly into the surrounding aquatic environment, untreated, often containing antibiotics and pesticides.\" There is also an accumulation of heavy metals on the benthos (seafloor) near the salmon farms, particularly copper and zinc.\n\nIn 2016, mass fish kill events impacted salmon farmers along Chile's coast and the wider ecology. Increases in aquaculture production and its associated effluent were considered to be possible contributing factors to fish and molluscan mortality.\n\nSea cage aquaculture is responsible for nutrient enrichment of the waters in which they are established. This results from fish wastes and uneaten feed inputs. Elements of most concern are nitrogen and phosphorus which can promote algal growth, including harmful algal blooms which can be toxic to fish. Flushing times, current speeds, distance from the shore and water depth are important considerations when locating sea cages in order to minimize the impacts of nutrient enrichment on coastal ecosystems.\n\nThe extent of the effects of pollution from sea-cage aquaculture varies depending on where the cages are located, which species are kept, how densely cages are stocked and what the fish are fed. Important species-specific variables include the species' food conversion ratio (FCR) and nitrogen retention. Studies prior to 2001 determined that the amount of nitrogen introduced as feed which is lost to the water column and seafloor as waste varies from 52 to 95%.\n\nA type of salmon called the AquAdvantage salmon has been genetically modified for faster growth, although it has not been approved for commercial use, due to controversy. The altered salmon incorporates a growth hormone from a Chinook salmon that allows it to reach full size in 16–28 months, instead of the normal 36 months for Atlantic salmon, and while consuming 25 percent less feed. The U.S. Food and Drug Administration reviewed the AquAdvantage salmon in a draft environmental assessment and determined that it \"would not have a significant impact (FONSI) on the U.S. environment.\"\n\nWhile some forms of aquaculture can be devastating to ecosystems, such as shrimp farming in mangroves, other forms can be very beneficial. Shellfish aquaculture adds substantial filter feeding capacity to an environment which can significantly improve water quality. A single oyster can filter 15 gallons of water a day, removing microscopic algal cells. By removing these cells, shellfish are removing nitrogen and other nutrients from the system and either retaining it or releasing it as waste which sinks to the bottom. By harvesting these shellfish the nitrogen they retained is completely removed from the system. Raising and harvesting kelp and other macroalgae directly remove nutrients such as nitrogen and phosphorus. Repackaging these nutrients can relieve eutrophic, or nutrient-rich, conditions known for their low dissolved oxygen which can decimate species diversity and abundance of marine life. Removing algal cells from the water also increase light penetration, allowing plants such as eelgrass to reestablish themselves and further increase oxygen levels.\n\nAquaculture in an area can provide for crucial ecological functions for the inhabitants. Shellfish beds or cages can provide habitat structure. This structure can be used as shelter by invertebrates, small fish or crustaceans to potentially increase their abundance and maintain biodiversity. Increased shelter raises stocks of prey fish and small crustaceans by increasing recruitment opportunities in turn providing more prey for higher trophic levels. One study estimated that 10 square meters of oyster reef could enhance an ecosystem's biomass by 2.57 kg The shellfish acting as herbivores will also be preyed on. This moves energy directly from primary producers to higher trophic levels potentially skipping out on multiple energetically-costly trophic jumps which would increase biomass in the ecosystem.\n\nAs with the farming of terrestrial animals, social attitudes influence the need for humane practices and regulations in farmed marine animals. Under the guidelines advised by the Farm Animal Welfare Council good animal welfare means both fitness and a sense of well being in the animal's physical and mental state. This can be defined by the Five Freedoms:\n\nHowever, the controversial issue in aquaculture is whether fish and farmed marine invertebrates are actually sentient, or have the perception and awareness to experience suffering. Although no evidence of this has been found in marine invertebrates, recent studies conclude that fish do have the necessary receptors (nociceptors) to sense noxious stimuli and so are likely to experience states of pain, fear and stress. Consequently, welfare in aquaculture is directed at vertebrates; finfish in particular.\n\nWelfare in aquaculture can be impacted by a number of issues such as stocking densities, behavioural interactions, disease and parasitism. A major problem in determining the cause of impaired welfare is that these issues are often all interrelated and influence each other at different times.\n\nOptimal stocking density is often defined by the carrying capacity of the stocked environment and the amount of individual space needed by the fish, which is very species specific. Although behavioural interactions such as shoaling may mean that high stocking densities are beneficial to some species, in many cultured species high stocking densities may be of concern. Crowding can constrain normal swimming behaviour, as well as increase aggressive and competitive behaviours such as cannibalism, feed competition, territoriality and dominance/subordination hierarchies. This potentially increases the risk of tissue damage due to abrasion from fish-to-fish contact or fish-to-cage contact. Fish can suffer reductions in food intake and food conversion efficiency. In addition, high stocking densities can result in water flow being insufficient, creating inadequate oxygen supply and waste product removal. Dissolved oxygen is essential for fish respiration and concentrations below critical levels can induce stress and even lead to asphyxiation. Ammonia, a nitrogen excretion product, is highly toxic to fish at accumulated levels, particularly when oxygen concentrations are low.\n\nMany of these interactions and effects cause stress in the fish, which can be a major factor in facilitating fish disease. For many parasites, infestation depends on the host's degree of mobility, the density of the host population and vulnerability of the host's defence system. Sea lice are the primary parasitic problem for finfish in aquaculture, high numbers causing widespread skin erosion and haemorrhaging, gill congestion, and increased mucus production. There are also a number of prominent viral and bacterial pathogens that can have severe effects on internal organs and nervous systems.\n\nThe key to improving welfare of marine cultured organisms is to reduce stress to a minimum, as prolonged or repeated stress can cause a range of adverse effects. Attempts to minimise stress can occur throughout the culture process. During grow out it is important to keep stocking densities at appropriate levels specific to each species, as well as separating size classes and grading to reduce aggressive behavioural interactions. Keeping nets and cages clean can assist positive water flow to reduce the risk of water degradation.\n\nNot surprisingly disease and parasitism can have a major effect on fish welfare and it is important for farmers not only to manage infected stock but also to apply disease prevention measures. However, prevention methods, such as vaccination, can also induce stress because of the extra handling and injection. Other methods include adding antibiotics to feed, adding chemicals into water for treatment baths and biological control, such as using cleaner wrasse to remove lice from farmed salmon.\n\nMany steps are involved in transport, including capture, food deprivation to reduce faecal contamination of transport water, transfer to transport vehicle via nets or pumps, plus transport and transfer to the delivery location. During transport water needs to be maintained to a high quality, with regulated temperature, sufficient oxygen and minimal waste products. In some cases anaesthetics may be used in small doses to calm fish before transport.\n\nAquaculture is sometimes part of an environmental rehabilitation program or as an aid in conserving endangered species.\n\nGlobal wild fisheries are in decline, with valuable habitat such as estuaries in critical condition. The aquaculture or farming of piscivorous fish, like salmon, does not help the problem because they need to eat products from other fish, such as fish meal and fish oil. Studies have shown that salmon farming has major negative impacts on wild salmon, as well as the forage fish that need to be caught to feed them. Fish that are higher on the food chain are less efficient sources of food energy.\n\nApart from fish and shrimp, some aquaculture undertakings, such as seaweed and filter-feeding bivalve mollusks like oysters, clams, mussels and scallops, are relatively benign and even environmentally restorative. Filter-feeders filter pollutants as well as nutrients from the water, improving water quality. Seaweeds extract nutrients such as inorganic nitrogen and phosphorus directly from the water, and filter-feeding mollusks can extract nutrients as they feed on particulates, such as phytoplankton and detritus.\n\nSome profitable aquaculture cooperatives promote sustainable practices. New methods lessen the risk of biological and chemical pollution through minimizing fish stress, fallowing netpens, and applying Integrated Pest Management. Vaccines are being used more and more to reduce antibiotic use for disease control.\n\nOnshore recirculating aquaculture systems, facilities using polyculture techniques, and properly sited facilities (for example, offshore areas with strong currents) are examples of ways to manage negative environmental effects.\n\nRecirculating aquaculture systems (RAS) recycle water by circulating it through filters to remove fish waste and food and then recirculating it back into the tanks. This saves water and the waste gathered can be used in compost or, in some cases, could even be treated and used on land. While RAS was developed with freshwater fish in mind, scientist associated with the Agricultural Research Service have found a way to rear saltwater fish using RAS in low-salinity waters. Although saltwater fish are raised in off-shore cages or caught with nets in water that typically has a salinity of 35 parts per thousand (ppt), scientists were able to produce healthy pompano, a saltwater fish, in tanks with a salinity of only 5 ppt. Commercializing low-salinity RAS are predicted to have positive environmental and economical effects. Unwanted nutrients from the fish food would not be added to the ocean and the risk of transmitting diseases between wild and farm-raised fish would greatly be reduced. The price of expensive saltwater fish, such as the pompano and combia used in the experiments, would be reduced. However, before any of this can be done researchers must study every aspect of the fish's lifecycle, including the amount of ammonia and nitrate the fish will tolerate in the water, what to feed the fish during each stage of its lifecycle, the stocking rate that will produce the healthiest fish, etc.\n\nSome 16 countries now use geothermal energy for aquaculture, including China, Israel, and the United States. In California, for example, 15 fish farms produce tilapia, bass, and catfish with warm water from underground. This warmer water enables fish to grow all year round and mature more quickly. Collectively these California farms produce 4.5 million kilograms of fish each year.\n\nAquaculture by Country:\n\n"}
{"id": "24226124", "url": "https://en.wikipedia.org/wiki?curid=24226124", "title": "Arbeiter und Soldat", "text": "Arbeiter und Soldat\n\nArbeiter und Soldat (meaning \"Worker and Soldier\" in English) was a clandestine magazine produced for German soldiers by the French Trotskyist group Parti Ouvrier Internationaliste during the World War II Nazi occupation of France.\n\nThe publication ardently opposed fascism but also refused to extend support to the western Allies, characterising them as imperialist. Twelve issues of the paper were published between 1943 and 1944, despite fierce repression of the far-left press by the Milice and Gestapo.\n\nIn August 2008 the archive of extant issues was republished in English.\n"}
{"id": "26106154", "url": "https://en.wikipedia.org/wiki?curid=26106154", "title": "Asa'ib Ahl al-Haq", "text": "Asa'ib Ahl al-Haq\n\nAsa'ib Ahl al-Haq (AAH; Aṣayib Ahl al-Haq, \"League of the Righteous\"), also known as the Khazali Network, is an Iraqi Shi'a paramilitary group active in the Iraqi insurgency and Syrian Civil War. During the Iraq War it was known as Iraq's largest \"Special Group\" (the Americans' term for Iran-backed Shia paramilitaries in Iraq), and claimed responsibility for over 6,000 attacks on American and Coalition forces. The group is currently fighting against the Islamic State of Iraq and the Levant as part of the Popular Moblization Forces. The group is funded and trained by Iran's Quds Force. In 2017, AAH created a party with the same name.\n\nQais al-Khazali split from Muqtada al-Sadr's Mahdi Army after the Shi'a uprising in 2004 to create his own Khazali network. When the Mahdi Army signed a cease-fire with the government and the Americans and the fighting stopped, Qais al-Khazali continued fighting, and during the battle Khazali was already issuing his own orders to militiamen without Muqtada al-Sadr's approval. The group's leadership (which includes Qais Khazali, Abd al-Hadi al-Darraji (a politician in Muqtada al-Sadr's Sadr Movement) and Akram al-Kaabi), however, reconciled with al-Sadr in mid-2005. In July 2006, Asa'ib Ahl al-Haq was founded and became one of the Special Groups which operated more independently from the rest of the Mahdi Army. It became a completely independent organisation after the Mahdi Army's disbanding after the 2008 Shi'a uprising. In July 2006, A part of AAH fought alongside Hezbollah in 2006 Lebanon War against Israel. In November 2008 when Sadr created the Promised Day Brigade to succeed the Mahdi Army, he asked AAH (and other Special Groups) to join, but they declined.\n\nThe group has claimed responsibility for over 6,000 attacks in Iraq including the October 10, 2006 attack on Camp Falcon, the assassination of the American military commander in Najaf, the May 6, 2006 downing of a British Lynx helicopter and the October 3, 2007 attack on the Polish ambassador. Their most known attack however, is the January 20, 2007 Karbala provincial headquarters raid where they infiltrated the U.S. Army's offices at Karbala, killed one soldier, then abducted and killed four more American soldiers. After the raid, the U.S. military launched a crackdown on the group and the raid's mastermind Azhar al-Dulaimi was killed in Baghdad, while much of the group's leadership including the brothers Qais and Laith al-Khazali and Lebanese Hezbollah member Ali Musa Daqduq who was Khazali's advisor was in charge of their relations with Hezbollah. After these arrests in 2007, Akram al-Kabi, who had been the military commander of the Mahdi Army until May 2007, led the organisation. In May 2007, the group kidnapped British IT expert Peter Moore and his four bodyguards. They demanded the release of all their fighters being imprisoned by the Iraqi authorities and US military in return for his release. His four bodyguards were killed, but Moore himself was released when the group's leader Qais al-Khazali was released in January 2010. Prior to Qazali's release, security forces had already released over 100 of the group's members including Laith al-Khazali. In 2008 many of the groups fighters and leaders fled to Iran after the Iraqi Army was allowed to re-take control of Sadr City and the Mahdi Army was disbanded. Here most fighters were re-trained in new tactics. It resulted in a major lull in the group's activity from May to July 2008.\n\nIn February 2010 the group kidnapped DoD civilian Issa T. Salomi, a naturalized American from Iraq. This was the first high-profile kidnapping of a foreigner in Iraq since the kidnapping of Peter Moore (which was also done by Asa'ib Ahl al-Haq). Salomi was released in March 2010 return for the release of 4 of their fighters being held in Iraqi custody. In total 450 members of the group have been handed over from US to Iraqi custody since the kidnapping of Peter Moore, over 250 of which have been released by the Iraqi authorities.\n\nOn July 21, 2010 General Ray Odierno said Iran was supporting three Shiite extremist groups in Iraq that had been attempting to attack US bases. One of the groups was Asa'ib Ahl al-Haq and the other two were the Promised Day Brigade and Ketaib Hezbollah.\n\nIn December 2010 it was reported that notorious Shi'a militia commanders such as Abu Deraa and Mustafa al-Sheibani were returning from Iran to work with Asa'ib Ahl al-Haq. Iranian Grand Ayatollah Kazem al-Haeri was identified as the group's spiritual leader.\n\nIn August and September 2012, Asa'ib Ahl al-Haq started a poster campaign in which they distributed over 20,000 posters of Iran's Supreme Leader Ayatollah Sayyid Ali Khamenei throughout Iraq. A senior official in Baghdad's local government said municipal workers were afraid to take the posters down in fear of retribution by Asa'ib Ahl al-Haq militiamen.\n\nIn April 2015, the group claimed responsibility for the killing of Izzat al-Douri, former Saddam Hussein deputy and leader of the anti-government insurgent Naqshbandi Army.\n\nThe group's Syrian branch is called the Haidar al-Karar Brigades. It is led by Akram al-Kabi, AAH's military leader, who is stationed in Aleppo. The group initially fought under the banner of the al-Abbas Brigade (a mixed Syrian, Iraqi, & Lebanese Shia organization), but split in 2014 due to a dispute with al-Abbas's native Syrian fighters. Like other Iraqi Shia paramilitaries in Syria, they fight in defense of the Sayyidah Zainab shrine.\n\nThe organization had candidates running in the 2014 Iraqi parliamentary election under the banner of Al-Sadiqoun Bloc. However an electoral meeting of estimated 100,000 supporters of Al-Sadiqoun was marred by violence as a series of bombs exploded at the campaign rally held at the Industrial Stadium in eastern Baghdad killing at least 37 people and wounding scores others, according to Iraqi police said. The Shia group organizers had planned to announce at the rally the names of its candidates for the parliamentary election. The Al-Sadiquun Bloc ended up winning just one seat out of 328 seats in the Iraqi Parliament.\n\nThe group's strength was estimated at some 3,000 fighters in March 2007. In July 2011, however, officials estimated there were less than 1,000 Asa'ib Ahl al-Haq militiamen left in Iraq. The group is alleged to receive some $5 million worth of cash and weapons every month from Iran. In January 2012, following the American withdrawal from Iraq in December 2011, Qais al-Khazali declared the United States was defeated and that now the group was prepared to disarm and join the political process.\n\nSince the beginning of the Iraqi war against ISIS, AAH has grown to around 10,000 members and been described as one of if not the most powerful members of the Popular Mobilization Forces. It has recruited hundreds of Sunni fighters to fight against ISIS.\n\nThe organisation is alleged to receive training and weapons from Iran's Revolutionary Guards' Quds Force as well as Iranian-backed Lebanese group Hezbollah. By March 2007, Iran was providing the network between $750,000 and $3 million in arms and financial support each month. Abu Mustafa al-Sheibani, a former Badr Brigades member who ran an important smuggling network known as the Sheibani Network played a key role in supplying the group. The group was also supplied by a smuggling network headed by Ahmad Sajad al-Gharawi a former Mahdi Army commander, mostly active in Maysan Governorate.\n\nAs of 2006 Asa'ib Ahl al-Haq had at least four major operational branches:\n\n\n\n"}
{"id": "2622430", "url": "https://en.wikipedia.org/wiki?curid=2622430", "title": "Average worker's wage", "text": "Average worker's wage\n\nAverage wage is the mean salary of a group of workers. This measure is often monitored and used by government or other organisations as a benchmark for the wage level of individual workers in an industry, area or country.\n\nThe usefulness of this measure in assessing wage levels is debatable, particularly in an economy where low pay is prevalent, due to the tendency for the wages of a minority of high earners to 'skew' the average upwards. It has been argued that the median (midpoint) worker's wage is a better indicator in these circumstances; this measure is used in the UK by both the Office for National Statistics and the Scottish Low Pay Unit in examining wage levels.\n\nCertain UK organisations, usually socialist or left-of-centre political groups, have traditionally had a policy that members should never accept wages higher than the wage of the average working class person whilst being employed by that organisation or in a representative capacity. Deputies and officials paid an average worker's wage are also a feature of the Dictatorship of the Proletariat described in Marx's account of the Paris Commune, The Civil War in France, as well as in Lenin's The State and Revolution commentary on Marx's pamphlet, although not all people who draw an average workers wage subscribe to Marxist principles.\n\nThis idea is based on the idea that politicians (or trade union officials) are there to serve the people of the country rather than earn themselves a fortune and/or raise their status (also known as careerism). Proponents claim that high wages for politicians are a waste of taxpayers' money and distance the politician from the concerns of the working class.\n\nExamples of people taking only an average worker's wage are Socialist Party politician Joe Higgins, former MP Dave Nellist in the UK, John Marek, Forward Wales' Welsh Assembly member for Wrexham, and Sinn Féin politicians in Ireland.\nIn \"The Ragged Trousered Philanthropists\", Robert Tressell notes that the Labour Representation Committee MPs of the day took only an average workers' wage.\n\n"}
{"id": "7230444", "url": "https://en.wikipedia.org/wiki?curid=7230444", "title": "Bioconservatism", "text": "Bioconservatism\n\nBioconservatism (a portmanteau of \"biology\" and \"conservatism\") is a stance of hesitancy and skepticism regarding radical technological advances, especially those that seek to modify or enhance the human condition. Bioconservatism is characterized by a belief that technological trends in today's society risk compromising human dignity, and by opposition to movements and technologies including transhumanism, human genetic modification, \"strong\" artificial intelligence, and the technological singularity. Many bioconservatives also oppose the use of technologies such as life extension and preimplantation genetic screening.\n\nBioconservatives range in political perspective from right-leaning religious and cultural conservatives to left-leaning environmentalists and technology critics. What unifies bioconservatives is skepticism about medical and other biotechnological transformations of the living world. Typically less sweeping as a critique of technological society than bioluddism, the bioconservative perspective is characterized by its defense of the natural, deployed as a moral category.\n\nThe philosophical underpinnings of bioconservative thought are diverse, ranging from religious to secular, and from left-wing to conservative. Nick Bostrom identifies three major strains of thought that might lead to concerns about radical technological change. Two of these strains of thought are secular: first, that human augmentation is innately degrading and therefore harmful, and secondly that the existence of augmented humans poses a threat to \"ordinary humans\". The third strain of thought is religious, and holds that human augmentation violates religious principles and is an insult to God..\n\nThe transhumanist Institute for Ethics and Emerging Technologies criticizes bioconservatism as a form of \"human racism\" (more commonly known as speciesism), and as being motivated by a \"yuck factor\" that ignores individual freedoms.\n\n\n\n"}
{"id": "20574808", "url": "https://en.wikipedia.org/wiki?curid=20574808", "title": "Consenting Adult Action Network", "text": "Consenting Adult Action Network\n\nThe Consenting Adult Action Network (CAAN) is a grassroots network of individuals in the United Kingdom that was formed in 2008 to protest and oppose laws restricting activities between consenting adults, most notably the criminalisation of possession of \"extreme pornography\".\n\nOn 14 June and 22 August, they attempted to seek advice on what material might be caught by the law. They took a dossier of images to three major police forces, although none of them could yet say which pictures would be deemed illegal.\n\nOn 21 October 2008, they organised a protest with Ben Westwood against the law on \"extreme pornography\".\n\nThey were awarded \"Specialists Website of the Year\" by the London Gay Sex and Fetish Awards 2008.\n\nOn 25 January 2009, the day before the law on \"extreme pornography\" came into effect, CAAN protested in London. They were supported by Backlash and The Spanner Trust. The protest was attended by Peter Tatchell and Ben Westwood.\n\nMarch 2010, CAAN announce they are publishing a book \"Beyond the Circle: Sexuality & discrimination in heteronormative Britain\" written by John Ozimek.\n\nCAAN is concerned about several issues regarding consenting adults:\n\n\n"}
{"id": "58739928", "url": "https://en.wikipedia.org/wiki?curid=58739928", "title": "Coppet group", "text": "Coppet group\n\nThe Coppet group (\"Groupe de Coppet\"), also known as the Coppet circle, was an informal intellectual and literary gathering centered around Germaine de Staël during the time period between the French Revolution and the Bourbon Restoration at Coppet Castle. The group had a considerable influence on the development of nineteenth century liberalism and romanticism. Stendhal referred to it as \"the Estates General of European opinion.\"\n\nIn addition to Stael, it included such prominent figures as Benjamin Constant, Wilhelm von Humboldt, Lord Byron, Jean de Sismondi, Jacques Necker, Charles de Villers, Charles Victor de Bonstetten, Zacharias Werner, Mathieu de Montmorency, Arnail François, marquis de Jaucourt, Adolph Ribbing, Claude Hochet, François-René de Chateaubriand, Prosper de Barante, Victor de Broglie, Adam Gottlob Oehlenschläger, Joseph Marie de Gérando, Prince Augustus of Prussia, Adelbert von Chamisso, Friederike Brun, Juliette Récamier, and August Wilhelm Schlegel.\n"}
{"id": "18503964", "url": "https://en.wikipedia.org/wiki?curid=18503964", "title": "Dayton Literary Peace Prize", "text": "Dayton Literary Peace Prize\n\nThe Dayton Literary Peace Prize is an annual United States literary award \"recognizing the power of the written word to promote peace\" that was first awarded in 2006. Awards are given for adult fiction and non-fiction books published at some point within the immediate past year that have led readers to a better understanding of other peoples, cultures, religions, and political views, with the winner in each category receiving a cash prize of $10,000. The award is an offshoot of the Dayton Peace Prize, which grew out of the 1995 peace accords ending the Bosnian War. In 2011, the former \"Lifetime Achievement Award\" was renamed the Richard C. Holbrooke Distinguished Achievement Award with a $10,000 honorarium.\n\nIn 2008, Martin Luther King, Jr. biographer Taylor Branch joined Studs Terkel and Elie Wiesel as a recipient of the Dayton Literary Peace Prize's Lifetime Achievement Award, which was presented to him by special guest Edwin C. Moses. The 2008 ceremony was held in Dayton, Ohio, on September 28, 2008. Nick Clooney, who hosted the ceremony in 2007, again served as the evening's host in 2008 and 2009.\n\nThe 2009 ceremony was held in Dayton, Ohio, on November 8, 2009, at which married authors and journalists Nicholas Kristof and Sheryl WuDunn received the Dayton Literary Peace Prize's 2009 Lifetime Achievement Award.\n\n2018\n\n2017\n\n2016 \n\n2015 \n\n2014 \n\n2013 \n\n2012 \n\n2011\n\n2010\n\n2009\n\n2008\n\n2007\n\n2006\n\n"}
{"id": "97477", "url": "https://en.wikipedia.org/wiki?curid=97477", "title": "Eastern Bloc", "text": "Eastern Bloc\n\nThe Eastern Bloc (also known as the Socialist Bloc, Communist Bloc and Soviet Bloc) was the group of Communist states stretching from Central and Eastern Europe to East and Southeast Asia largely controlled by the Soviet Union during the Cold War in opposition to the Western Bloc led by the United States. The term generally includes the USSR and its satellite states in the Comecon, including Vietnam and its satellites Laos and Kampuchea, North Korea, and China (before 1961.) Cuba is included as well after 1961, but demonstrated independence from Soviet policy following the 1962 Cuban Missile Crisis. Widespread Soviet hegemony ended with the success of the Revolutions of 1989 against the Warsaw Pact, and the 1991 collapse of the Soviet Union brought the Eastern Bloc and the Cold War to an end. \n\nDuring Joseph Stalin's lifetime, Soviet control over the Eastern Bloc was tested but never seriously challenged by the 1948 Czechoslovak coup d'état and Tito–Stalin Split over control of Yugoslavia, the 1949 Chinese Communist Revolution and Chinese and North Korean involvement in the Korean War against the United Nations. After his death in 1953, the Korean War was halted but not settled and anti-Soviet sentiment sparked the East German uprising. The Eastern Bloc started to break apart in 1956, when new leader Nikita Khrushchev's \"Secret Speech\" denouncing Stalin helped spark the anti-Soviet Hungarian Revolution of 1956, which was suppressed by a Soviet invasion, and the Sino–Soviet Split with Mao Zedong's China, which gave North Korea and North Vietnam more independence from both, and facilitated the Soviet–Albanian split. The Cuban Missile Crisis preserved the Cuban Revolution from rollback by the United States, but Fidel Castro became increasingly independent of Soviet rule afterwards, most notably in its 1975 intervention in Angola. That year, the fall of former French Indochina to communism following the end of the Vietnam War gave the Eastern Bloc renewed confidence which had been frayed by Soviet leader Leonid Brezhnev's 1968 invasion of Czechoslovakia to suppress the Prague Spring, which had led to Albania withdrawing from the Pact, briefly aligning with Mao Zedong's China until the Sino-Albanian split.\n\nUnder the Brezhnev Doctrine, the Soviet Union reserved the right to intervene in other Communist countries. In response, China moved towards the United States following a 1969 border war which almost went nuclear, and later reformed and liberalized its economy, while the Eastern Bloc stagnated economically behind the capitalist First World. Brezhnev's invasion of Afghanistan nominally expanded the Eastern Bloc, but the war proved unwinnable and too costly for the Soviets, challenged in Eastern Europe by civil resistance in Poland. In the late 1980s, Soviet leader Mikhail Gorbachev pursued policies of \"glasnost\" (\"openness\") and \"perestroika\" (\"restructuring\") to reform the Eastern Bloc and end the Cold War, which brought forth unrest throughout the bloc. Unlike previous Soviet leaders in 1953, 1956, and 1968, Gorbachev refused to use force to end the 1989 Revolutions against Marxist-Leninist rule in Eastern Europe. The fall of the Berlin Wall and end of the Warsaw Pact spread nationalist and liberal ideals throughout the Soviet Union, which would soon fall itself at the end of 1991. Conservative communist elites attempted to turn back liberal reforms and movements, which hastened the end of Marxist-Leninist rule in Eastern Europe but preserved it in China.\n\nThough the Soviet Union and its rival the United States considered Europe the most important front of the Cold War, during the Cold War, the term \"Eastern Bloc\" was often used interchangeably with the term Second World. This broadest usage of the term would include not only Maoist China and Cambodia, but short-lived Soviet satellites such as East Turkestan Republic (1944-1949), the People's Republic of Azerbaijan and Republic of Mahabad (1946), as well as the Marxist-Leninist states straddling the Second and Third Worlds before the end of the Cold War: the People's Democratic Republic of Yemen (from 1967), the People's Republic of the Congo (from 1969), the People's Republic of Benin, the People's Republic of Angola and People's Republic of Mozambique from 1975, the People's Revolutionary Government of Grenada from 1979 to 1983, the Derg/People's Democratic Republic of Ethiopia from 1974, and the Somali Democratic Republic from 1969 until the Ogaden War in 1977. Many states were also accused by the Western Bloc of being in the Eastern Bloc when they were more Non-Aligned. The most limited definition of the Eastern Bloc would only include the Warsaw Pact states and Mongolia, as former satellite states most dominated by the USSR. However, North Korea was similarly subordinate before the Korean War, and Soviet aid during the Vietnam War enabled Vietnam to dominate Laos and Cambodia until the end of the Cold War. Cuba's defiance of complete Soviet control was noteworthy enough that Cuba was sometimes excluded as a satellite state altogether, as Fidel Castro intervened in other Third World countries to spread communism without orders from Moscow, despite its alliance with the Soviets.\n\nThe only Communist states to survive the Cold War intact until the present are China, Vietnam, Cuba, North Korea, and Laos. Their state socialist experience was more in line with decolonization from the Global North and anti-imperialism towards the West instead of the Red Army occupation of the former East Bloc. The five surviving socialist states all adopted economic reforms to varying degrees; China and Vietnam are usually described as more state capitalist than the more traditionalist Cuba and Stalinist North Korea. Cambodia and Kazakhstan are still led by the same Eastern Bloc leaders as during the Cold War, though they are not officially Marxist-Leninist states. This was previously the case in Kazakhstan's fellow post-Soviet states of Uzbekistan until 2016, Turkmenistan until 2006, Kyrgyzstan until 2005, and Azerbaijan and Georgia until 2003. Azerbaijan is an authoritarian dominant-party state and North Korea is a totalitarian one-party state led by the heirs of their Eastern Bloc leaders, yet both have officially eliminated mentions of communism from their constitutions.\n\nPost-1991 usage of the term \"Eastern Bloc\" may be more limited in referring to the states forming the Warsaw Pact (1955-1991), and Mongolia (1924-1992), which are no longer communist states. Sometimes they are more generally referred to as \"the countries of Eastern Europe under communism,\" excluding Mongolia but including Yugoslavia and Albania, which had both split with the Soviet Union by the 1960s.\n\nPrior to the common use of the term, in the 1920s, \"Eastern Bloc\" was used to refer to a loose alliance of eastern and central European countries.\n\nEven though Yugoslavia was a socialist country, it was not a member of COMECON or the Warsaw Pact. Parting with the USSR in 1948, Yugoslavia did not belong to the East, but also did not belong to the West, because of its socialist system and its status as a founding member of the Non-Aligned Movement. However, many sources consider Yugoslavia to be a member of the Eastern Bloc. Others consider Yugoslavia not to be a member after it broke with Soviet policy in the 1948 Tito–Stalin split.\n\nIn 1922, the RSFSR, the Ukrainian SSR, the Byelorussian SSR, and the Transcaucasian SFSR approved the Treaty of Creation of the USSR and the Declaration of the Creation of the USSR, forming the Soviet Union. Soviet leader Joseph Stalin, who viewed the Soviet Union as a \"socialist island\", stated that the Soviet Union must see that \"the present capitalist encirclement is replaced by a socialist encirclement\".\n\nIn 1939, the USSR entered into the Molotov–Ribbentrop Pact with Nazi Germany that contained a secret protocol that divided Romania, Poland, Latvia, Lithuania, Estonia and Finland into German and Soviet spheres of influence. Eastern Poland, Latvia, Estonia, Finland and Bessarabia in northern Romania were recognized as parts of the Soviet sphere of influence. Lithuania was added in a second secret protocol in September 1939.\n\nThe Soviet Union had invaded the portions of eastern Poland assigned to it by the Molotov–Ribbentrop Pact two weeks after the German invasion of western Poland, followed by co-ordination with German forces in Poland. During the Occupation of East Poland by the Soviet Union, the Soviets liquidated the Polish state, and a German-Soviet meeting addressed the future structure of the \"Polish region.\" Soviet authorities immediately started a campaign of sovietization of the newly Soviet-annexed areas. Soviet authorities collectivized agriculture, and nationalized and redistributed private and state-owned Polish property.\n\nInitial Soviet occupations of the Baltic countries had occurred in mid-June 1940, when Soviet NKVD troops raided border posts in Lithuania, Estonia and Latvia, followed by the liquidation of state administrations and replacement by Soviet cadres. Elections for parliament and other offices were held with single candidates listed and the official results fabricated, purporting pro-Soviet candidates' approval by 92.8 percent of the voters in Estonia, 97.6 percent in Latvia, and 99.2 percent in Lithuania. The fraudulently installed peoples assemblies immediately requested admission into the USSR, which was granted by the Soviet Union, with the annexations resulting in the Estonian Soviet Socialist Republic, Latvian Soviet Socialist Republic, and Lithuanian Soviet Socialist Republic. The international community condemned this initial annexation of the Baltic states and deemed it illegal.\n\nIn 1939, the Soviet Union unsuccessfully attempted an invasion of Finland, subsequent to which the parties entered into an interim peace treaty granting the Soviet Union the eastern region of Karelia (10% of Finnish territory), and the Karelo-Finnish Soviet Socialist Republic was established by merging the ceded territories with the KASSR. After a June 1940 Soviet Ultimatum demanding Bessarabia, Bukovina, and the Hertza region from Romania, the Soviets entered these areas, Romania caved to Soviet demands and the Soviets occupied the territories.\n\nIn June 1941, Germany broke the Molotov-Ribbentrop pact by invading the Soviet Union. From the time of this invasion to 1944, the areas annexed by the Soviet Union were part of Germany's Ostland (except for the Moldavian SSR). Thereafter, the Soviet Union began to push German forces westward through a series of battles on the Eastern Front.\n\nIn the aftermath of World War II on the Soviet-Finnish border, the parties signed another peace treaty ceding to the Soviet Union in 1944, followed by a Soviet annexation of roughly the same eastern Finnish territories as those of the prior interim peace treaty as part of the Karelo-Finnish Soviet Socialist Republic.\n\nFrom 1943 to 1945, several conferences regarding Post-War Europe occurred that, in part, addressed the potential Soviet annexation and control of countries in Central Europe. British Prime Minister Winston Churchill's Soviet policy regarding Central Europe differed vastly from that of American President Franklin D. Roosevelt, with the former believing Soviet leader Joseph Stalin to be a \"devil\"-like tyrant leading a vile system.\n\nWhen warned of potential domination by a Stalin dictatorship over part of Europe, Roosevelt responded with a statement summarizing his rationale for relations with Stalin: \"I just have a hunch that Stalin is not that kind of a man. . . . I think that if I give him everything I possibly can and ask for nothing from him in return, noblesse oblige, he won't try to annex anything and will work with me for a world of democracy and peace.\" While meeting with Stalin and Roosevelt in Tehran in 1943, Churchill stated that Britain was vitally interested in restoring Poland as an independent country. Britain did not press the matter for fear that it would become a source of inter-allied friction.\n\nIn February 1945, at the conference at Yalta, Stalin demanded a Soviet sphere of political influence in Central Europe. Stalin eventually was convinced by Churchill and Roosevelt not to dismember Germany. Stalin stated that the Soviet Union would keep the territory of eastern Poland they had already taken via invasion in 1939, and wanted a pro-Soviet Polish government in power in what would remain of Poland. After resistance by Churchill and Roosevelt, Stalin promised a re-organization of the current pro-Soviet government on a broader democratic basis in Poland. He stated that the new government's primary task would be to prepare elections.\n\nThe parties at Yalta further agreed that the countries of liberated Europe and former Axis satellites would be allowed to \"create democratic institutions of their own choice\", pursuant to \"the right of all peoples to choose the form of government under which they will live.\" The parties also agreed to help those countries form interim governments \"pledged to the earliest possible establishment through free elections\" and \"facilitate where necessary the holding of such elections.\"\n\nAt the beginning of the July–August 1945 Potsdam Conference after Germany's unconditional surrender, Stalin repeated previous promises to Churchill that he would refrain from a \"sovietization\" of Central Europe. In addition to reparations, Stalin pushed for \"war booty\", which would permit the Soviet Union to directly seize property from conquered nations without quantitative or qualitative limitation. A clause was added permitting this to occur with some limitations.\n\nAt first, the Soviets concealed their role in other Eastern Bloc politics, with the transformation appearing as a modification of western \"bourgeois democracy\". As a young communist was told in East Germany: \"it's got to look democratic, but we must have everything in our control.\" Stalin felt that socioeconomic transformation was indispensable to establish Soviet control, reflecting the Marxist-Leninist view that material bases, the distribution of the means of production, shaped social and political relations.\n\nMoscow-trained cadres were put into crucial power positions to fulfill orders regarding sociopolitical transformation. Elimination of the bourgeoisie's social and financial power by expropriation of landed and industrial property was accorded absolute priority. These measures were publicly billed as \"reforms\" rather than socioeconomic transformations. Except for initially in Czechoslovakia, activities by political parties had to adhere to \"Bloc politics\", with parties eventually having to accept membership in an \"antifascist\" \"bloc\" obliging them to act only by mutual \"consensus\". The bloc system permitted the Soviet Union to exercise domestic control indirectly.\n\nCrucial departments such as those responsible for personnel, general police, secret police and youth, were strictly communist run. Moscow cadres distinguished \"progressive forces\" from \"reactionary elements\", and rendered both powerless. Such procedures were repeated until communists had gained unlimited power, and only politicians who were unconditionally supportive of Soviet policy remained.\n\nIn June 1947, after the Soviets had refused to negotiate a potential lightening of restrictions on German development, the United States announced the Marshall Plan, a comprehensive program of American assistance to all European countries wanting to participate, including the Soviet Union and those of Eastern Europe. The Soviets rejected the Plan and took a hard line position against the United States and non-communist European nations. However, Czechoslovakia was eager to accept the US aid; the Polish government had a similar attitude, and this was of great concern to the Soviets.\n\nIn one of the clearest signs of Soviet control over the region up to that point, the Czechoslovakian foreign minister, Jan Masaryk, was summoned to Moscow and berated by Stalin for considering joining the Marshall Plan. Polish Prime minister Józef Cyrankiewicz was rewarded for the Polish rejection of the Plan with a huge 5 year trade agreement, including $450 million in credit, 200,000 tons of grain, heavy machinery and factories.\n\nIn July 1947, Stalin ordered these countries to pull out of the Paris Conference on the European Recovery Programme, which has been described as \"the moment of truth\" in the post–World War II division of Europe. Thereafter, Stalin sought stronger control over other Eastern Bloc countries, abandoning the prior appearance of democratic institutions. When it appeared that, in spite of heavy pressure, non-communist parties might receive in excess of 40% of the vote in the August 1947 Hungarian elections, repressions were instituted to liquidate any independent political forces.\n\nIn that same month, annihilation of the opposition in Bulgaria began on the basis of continuing instructions by Soviet cadres. At a late September 1947 meeting of all communist parties in Szklarska Poręba, Eastern Bloc communist parties were blamed for permitting even minor influence by non-communists in their respective countries during the run up to the Marshall Plan.\n\nIn former German capital Berlin, surrounded by Soviet-occupied Germany, Stalin instituted the Berlin Blockade on June 24, 1948, preventing food, materials and supplies from arriving in West Berlin. The blockade was caused, in part, by early local elections of October 1946 in which the Socialist Unity Party of Germany (SED) was rejected in favor of the Social Democratic Party, which had gained two and a half times more votes than the SED. The United States, Britain, France, Canada, Australia, New Zealand and several other countries began a massive \"Berlin airlift\", supplying West Berlin with food and other supplies.\n\nThe Soviets mounted a public relations campaign against the western policy change and communists attempted to disrupt the elections of 1948 preceding large losses therein, while 300,000 Berliners demonstrated and urged the international airlift to continue. In May 1949, Stalin lifted the blockade, permitting the resumption of Western shipments to Berlin.\n\nAfter disagreements between Yugoslav leader Josip Broz Tito and the Soviet Union regarding Greece and Albania, a Tito–Stalin split occurred, followed by Yugoslavia being expelled from the Cominform in June 1948 and a brief failed Soviet putsch in Belgrade. The split created two separate communist forces in Europe. A vehement campaign against Titoism was immediately started in the Eastern Bloc, describing agents of both the West and Tito in all places as engaging in subversive activity.\n\nStalin ordered the conversion of the Cominform into an instrument to monitor and control the internal affairs of other Eastern Bloc parties. He also briefly considered converting the Cominform into an instrument for sentencing high-ranking deviators, but dropped the idea as impractical. Instead, a move to weaken communist party leaders through conflict was started. Soviet cadres in communist party and state positions in the Bloc were instructed to foster intra-leadership conflict and to transmit information against each other. This accompanied a continuous stream of accusations of \"nationalistic deviations\", \"insufficient appreciation of the USSR's role\", links with Tito and \"espionage for Yugoslavia.\" This resulted in the persecution of many major party cadres, including those in East Germany.\n\nThe first country to experience this approach was Albania, where leader Enver Hoxha immediately changed course from favoring Yugoslavia to opposing it. In Poland, leader Władysław Gomułka, who had previously made pro-Yugoslav statements, was deposed as party secretary-general in early September 1948 and subsequently jailed. In Bulgaria, when it appeared that Traicho Kostov, who was not a Moscow cadre, was next in line for leadership, in June 1949, Stalin ordered Kostov's arrest, followed soon thereafter by a death sentence and execution. A number of other high ranking Bulgarian officials were also jailed. Stalin and Hungarian leader Mátyás Rákosi met in Moscow to orchestrate a show trial of Rákosi opponent László Rajk, who was thereafter executed.\n\nDespite the initial institutional design of communism implemented by Joseph Stalin in the Eastern Bloc, subsequent development varied across countries. In satellite states, after peace treaties were initially concluded, opposition was essentially liquidated, fundamental steps towards socialism were enforced, and Kremlin leaders sought to strengthen control therein. Right from the beginning, Stalin directed systems that rejected Western institutional characteristics of market economies, capitalist parliamentary democracy (dubbed \"bourgeois democracy\" in Soviet parlance) and the rule of law subduing discretional intervention by the state. The resulting states aspired to total control of a political center backed by an extensive and active repressive apparatus, and a central role of Marxist-Leninist ideology.\n\nHowever, the vestiges of democratic institutions were never entirely destroyed, resulting in the façade of Western style institutions such as parliaments, which effectively just rubber-stamped decisions made by rulers, and constitutions, to which adherence by authorities was limited or non-existent. Parliaments were still elected, but their meetings occurred only a few days per year, only to legitimize politburo decisions, and so little attention was paid to them that some of those serving were actually dead, and officials would openly state that they would seat members who had lost elections.\n\nThe first or General Secretary of the central committee in each communist party was the most powerful figure in each regime. The party over which the politburo held sway was not a mass party but, conforming with Leninist tradition, a smaller selective party of between three and fourteen percent of the country's population who had accepted total obedience. Those who secured membership in this selective group received considerable rewards, such as access to special lower priced shops with a greater selection of high-quality domestic and/or foreign goods (confections, alcohol, cigars, cameras, televisions, and the like), special schools, holiday facilities, homes, high-quality domestic and/or foreign-made furniture, works of art, pensions, permission to travel abroad, and official cars with distinct license plates so that police and others could identify these members from a distance.\n\nIn addition to emigration restrictions, civil society, defined as a domain of political action outside the party's state control, was not allowed to firmly take root, with the possible exception of Poland in the 1980s. While the institutional design on the communist systems were based on the rejection of rule of law, the legal infrastructure was not immune to change reflecting decaying ideology and the substitution of autonomous law. Initially, communist parties were small in all countries except Czechoslovakia, such that there existed an acute shortage of politically \"trustworthy\" persons for administration, police, and other professions. Thus, \"politically unreliable\" non-communists initially had to fill such roles. Those not obedient to communist authorities were ousted, while Moscow cadres started a large-scale party programs to train personnel who would meet political requirements.\n\nCommunist regimes in the Eastern Bloc viewed marginal groups of opposition intellectuals as a potential threat because of the bases underlying Communist power therein. The suppression of dissidence and opposition was considered a central prerequisite to retain power, though the enormous expense at which the population in certain countries were kept under secret surveillance may not have been rational. Following a totalitarian initial phase, a post-totalitarian period followed the death of Stalin in which the primary method of Communist rule shifted from mass terror to selective repression, along with ideological and sociopolitical strategies of legitimation and the securing of loyalty. Juries were replaced by a tribunal of a professional judges and two lay assessors that were dependable party actors.\n\nThe police deterred and contained opposition to party directives. The political police served as the core of the system, with their names becoming synonymous with raw power and the threat of violent retribution should an individual become active against the State. Several state police and secret police organizations enforced communist party rule, including the following:\n\nThe press in the communist period was an organ of the state, completely reliant on and subservient to the communist party. Before the late 1980s, Eastern Bloc radio and television organizations were state-owned, while print media was usually owned by political organizations, mostly by the local communist party. Youth newspapers and magazines were owned by youth organizations affiliated with communist parties.\n\nThe control of the media was exercised directly by the communist party itself, and by state censorship, which was also controlled by the party. Media served as an important form of control over information and society. The dissemination and portrayal of knowledge were considered by authorities to be vital to communism's survival by stifling alternative concepts and critiques. Several state Communist Party newspapers were published, including:\n\nThe Telegraph Agency of the Soviet Union (TASS) served as the central agency for collection and distribution of internal and international news for all Soviet newspapers, radio and television stations. It was frequently infiltrated by Soviet intelligence and security agencies, such as the NKVD and GRU. TASS had affiliates in 14 Soviet republics, including the Lithuanian SSR, Latvian SSR, Estonian SSR, Moldavian SSR. Ukrainian SSR and Byelorussian SSR.\n\nWestern countries invested heavily in powerful transmitters which enabled services such as the BBC, VOA and Radio Free Europe (RFE) to be heard in the Eastern Bloc, despite attempts by authorities to jam the airways.\n\nUnder the state atheism of many Eastern Bloc nations, religion was actively suppressed. Since some of these states tied their ethnic heritage to their national churches, both the peoples and their churches were targeted by the Soviets.\n\nIn 1949, the Soviet Union, Bulgaria, Czechoslovakia, Hungary, Poland, and Romania founded the Comecon in accordance with Stalin's desire to enforce Soviet domination of the lesser states of Central Europe and to mollify some states that had expressed interest in the Marshall Plan, and which were now, increasingly, cut off from their traditional markets and suppliers in Western Europe. The Comecon's role became ambiguous because Stalin preferred more direct links with other party chiefs than the Comecon's indirect sophistication; it played no significant role in the 1950s in economic planning. Initially, the Comecon served as cover for the Soviet taking of materials and equipment from the rest of the Eastern Bloc, but the balance changed when the Soviets became net subsidizers of the rest of the Bloc by the 1970s via an exchange of low cost raw materials in return for shoddily manufactured finished goods.\n\nIn 1955, the Warsaw Pact was formed partly in response to NATO's inclusion of West Germany and partly because the Soviets needed an excuse to retain Red Army units in Hungary. For 35 years, the Pact perpetuated the Stalinist concept of Soviet national security based on imperial expansion and control over satellite regimes in Eastern Europe. This Soviet formalization of their security relationships in the Eastern Bloc reflected Moscow's basic security policy principle that continued presence in East Central Europe was a foundation of its defense against the West. Through its institutional structures, the Pact also compensated in part for the absence of Joseph Stalin's personal leadership since his death in 1953. The Pact consolidated the other Bloc members' armies in which Soviet officers and security agents served under a unified Soviet command structure.\n\nBeginning in 1964, Romania took a more independent course. While it did not repudiate either Comecon or the Warsaw Pact, it ceased to play a significant role in either. Nicolae Ceaușescu's assumption of leadership one year later pushed Romania even further in the direction of separateness. Albania, which had become increasingly isolated under Stalinist leader Enver Hoxha following de-Stalinization, undergoing a Soviet–Albanian split in 1961, withdrew from the Warsaw Pact in 1968 following the Warsaw Pact invasion of Czechoslovakia.\n\nIn 1917, Russia restricted emigration by instituting passport controls and forbidding the exit of belligerent nationals. In 1922, after the Treaty on the Creation of the USSR, both the Ukrainian SSR and the Russian SFSR issued general rules for travel that foreclosed virtually all departures, making legal emigration impossible. Border controls thereafter strengthened such that, by 1928, even illegal departure was effectively impossible. This later included internal passport controls, which when combined with individual city Propiska (\"place of residence\") permits, and internal freedom of movement restrictions often called the 101st kilometre, greatly restricted mobility within even small areas of the Soviet Union.\nAfter the creation of the Eastern Bloc, emigration out of the newly occupied countries, except under limited circumstances, was effectively halted in the early 1950s, with the Soviet approach to controlling national movement emulated by most of the rest of the Eastern Bloc. However, in East Germany, taking advantage of the Inner German border between occupied zones, hundreds of thousands fled to West Germany, with figures totaling 197,000 in 1950, 165,000 in 1951, 182,000 in 1952 and 331,000 in 1953. One reason for the sharp 1953 increase was fear of potential further Sovietization with the increasingly paranoid actions of Joseph Stalin in late 1952 and early 1953. 226,000 had fled in the just the first six months of 1953.\n\nWith the closing of the Inner German border officially in 1952, the Berlin city sector borders remained considerably more accessible than the rest of the border because of their administration by all four occupying powers. Accordingly, it effectively comprised a \"loophole\" through which Eastern Bloc citizens could still move west. The 3.5 million East Germans that had left by 1961, called Republikflucht, totaled approximately 20% of the entire East German population. In August 1961, East Germany erected a barbed-wire barrier that would eventually be expanded through construction into the Berlin Wall, effectively closing the loophole.\n\nWith virtually non-existent conventional emigration, more than 75% of those emigrating from Eastern Bloc countries between 1950 and 1990 did so under bilateral agreements for \"ethnic migration.\" About 10% were refugee migrants under the Geneva Convention of 1951. Most Soviets allowed to leave during this time period were ethnic Jews permitted to emigrate to Israel after a series of embarrassing defections in 1970 caused the Soviets to open very limited ethnic emigrations. The fall of the Iron Curtain was accompanied by a massive rise in European East-West migration. Famous Eastern Bloc defectors included Joseph Stalin's daughter Svetlana Alliluyeva, who denounced Stalin after her 1967 defection. \n\nEastern Bloc countries such as the Soviet Union had high rates of population growth. In 1917, the population of Russia in its present borders was 91 million. Despite the destruction in the Russian Civil War, the population grew to 92.7 million in 1926. In 1939, the population increased by 17 percent to 108. million. Despite more than 20 million deaths suffered throughout World War II, Russia's population grew to 117.2 million in 1959. The Soviet census of 1989 showed Russia's population at 147 million people.\n\nThe Soviet economical and political system produced further consequences such as, for example, in Baltic states, where the population was approximately half of what it should have been compared with similar countries such as Denmark, Finland and Norway over the years 1939–1990. Poor housing was one factor leading to severely declining birth rates throughout the Eastern Bloc. However, birth rates were still higher than in Western European countries. A reliance upon abortion, in part because periodic shortages of birth control pills and intrauterine devices made these systems unreliable, also depressed the birth rate and forced a shift to pro-natalist policies by the late 1960s, including severe checks on abortion and propagandist exhortations like the 'heroine mother' distinction bestowed on those Romanian women who bore ten or more children.\n\nIn October 1966, artificial birth control was proscribed in Romania and regular pregnancy tests were mandated for women of child-bearing age, with severe penalties for anyone who was found to have terminated a pregnancy. Despite such restrictions, birth rates continued to lag, in part, because of unskilled induced abortions. Population in Eastern Bloc countries was as follows:\n\nA housing shortage existed throughout the Eastern Bloc, especially after a severe cutback in state resources available for housing starting in 1975. Cities became filled with large system-built apartment blocks Western visitors from places like West Germany expressed surprise at the perceived shoddiness of new, box-like concrete structures across the border in East Germany, along with a relative greyness of the physical environment and the often joyless appearance of people on the street or in stores. Housing construction policy suffered from considerable organisational problems. Moreover, completed houses possessed noticeably poor quality finishes.\n\nThe near-total emphasis on large apartment blocks was a common feature of Eastern Bloc cities in the 1970s and 1980s. East German authorities viewed large cost advantages in the construction of Plattenbau apartment blocks such that the building of such architecture on the edge of large cities continued until the dissolution of the Eastern Bloc. These buildings, such as the Paneláks of Czechoslovakia and Panelház of Hungary, contained cramped concrete apartments that broadly lined Eastern Bloc streets, leaving the visitor with a \"cold and grey\" impression. Wishing to reinforce the role of the state in the 1970s and 1980s, Nicolae Ceaușescu enacted the systematization programme, which consisted of the demolition and reconstruction of existing villages, towns, and cities, in whole or in part, in order to make place to standardized apartment blocks across the country (\"blocuri\"). Under this ideology, Ceaușescu built Centrul Civic of Bucharest in the 1980s, which contains the Palace of the Parliament, in the place of the former historic center.\n\nEven by the late 1980s, sanitary conditions in most Eastern Bloc countries were generally far from adequate. For all countries for which data existed, 60% of dwellings had a density of greater than one person per room between 1966 and 1975. The average in western countries for which data was available approximated 0.5 persons per room. Problems were aggravated by poor quality finishes on new dwellings often causing occupants to undergo a certain amount of finishing work and additional repairs.\nThe worsening shortages of the 1970s and 1980s occurred during an increase in the quantity of dwelling stock relative to population from 1970 to 1986. Even for new dwellings, average dwelling size was only in the Eastern Bloc compared with in ten western countries for which comparable data was available. Space standards varied considerably, with the average new dwelling in the Soviet Union in 1986 being only 68% the size of its equivalent in Hungary. Apart from exceptional cases, such as East Germany in 1980–1986 and Bulgaria in 1970–1980, space standards in newly built dwellings rose before the dissolution of the Eastern Bloc. Housing size varied considerably across time, especially after the oil crisis in the Eastern Bloc; for instance, 1990-era West German homes had an average floor space of , compared to an average dwelling size in the GDR of in 1967.\n\nPoor housing was one of four factors, others being high female employment and education levels and abortion access, which led to severely declining birth rates throughout the Eastern Bloc. Homelessness was the most obvious effect of the housing shortage, though it was hard to define and measure in the Eastern Bloc.\n\nAs with the economy of the Soviet Union, planners in the Eastern Bloc were directed by the resulting Five Year Plans which followed paths of extensive rather than intensive development, focusing upon heavy industry as the Soviet Union had done, leading to inefficiencies and shortage economies.\n\nThe Eastern Bloc countries achieved high rates of economic and technical progress, promoted industrialisation, and ensured steady growth rates of labor productivity and rises in the standard of living. However, because of the lack of market signals, Eastern Bloc economies experienced mis-development by central planners. The Eastern Bloc also depended upon the Soviet Union for significant amounts of materials.\n\nTechnological backwardness resulted in dependency on imports from Western countries and this, in turn, in demand for Western currency. Eastern Bloc countries were heavily borrowing from Club de Paris (central banks) and London Club (private banks) and most of them by early 80's were forced to notify the creditors of their insolvency.\n\nAs a consequence of the Germans and World War II in Eastern Europe, much of the region had been subjected to enormous destruction of industry, infrastructure and loss of civilian life. In Poland alone the policy of plunder and exploitation inflicted enormous material losses to Polish industry (62% of which was destroyed), agriculture, infrastructure and cultural landmarks, the cost of which has been estimated as approximately €525 billion or $640 billion in 2004 exchange values\n\nThroughout the Eastern Bloc, both in the USSR and the rest of the Bloc, Russia was given prominence, and referred to as the \"naiboleye vydayushchayasya natsiya\" (the most prominent nation) and the \"rukovodyashchiy narod\" (the leading people). The Soviets promoted the reverence of Russian actions and characteristics, and the construction of Soviet structural hierarchies in the other countries of the Eastern Bloc.\n\nThe defining characteristic of Stalinist totalitarianism was the unique symbiosis of the state with society and the economy, resulting in politics and economics losing their distinctive features as autonomous and distinguishable spheres. Initially, Stalin directed systems that rejected Western institutional characteristics of market economies, democratic governance (dubbed \"bourgeois democracy\" in Soviet parlance) and the rule of law subduing discretional intervention by the state.\n\nThe Soviets mandated expropriation and \"etatisation\" of private property. The Soviet-style \"replica regimes\" that arose in the Bloc not only reproduced Soviet command economies, but also adopted the brutal methods employed by Joseph Stalin and Soviet-style secret police to suppress real and potential opposition.\n\nStalinist regimes in the Eastern Bloc saw even marginal groups of opposition intellectuals as a potential threat because of the bases underlying Stalinist power therein. The suppression of dissent and opposition was a central prerequisite for the security of Stalinist power within the Eastern Bloc, though the degree of opposition and dissident suppression varied by country and time throughout the Eastern Bloc.\n\nIn addition, media in the Eastern Bloc were organs of the state, completely reliant on and subservient to the government of the USSR with radio and television organisations being state-owned, while print media was usually owned by political organisations, mostly by the local party. While over 15 million Eastern Bloc residents migrated westward from 1945 to 1949, emigration was effectively halted in the early 1950s, with the Soviet approach to controlling national movement emulated by most of the rest of the Eastern Bloc.\n\nIn the USSR, because of strict Soviet secrecy under Joseph Stalin, for many years after World War II, even the best informed foreigners did not effectively know about the operations of the Soviet economy. Stalin had sealed off outside access to the Soviet Union since 1935 (and until his death), effectively permitting no foreign travel inside the Soviet Union such that outsiders did not know of the political processes that had taken place therein. During this period, and even for 25 years after Stalin's death, the few diplomats and foreign correspondents permitted inside the Soviet Union were usually restricted to within a few kilometres of Moscow, their phones were tapped, their residences were restricted to foreigner-only locations and they were constantly followed by Soviet authorities.\n\nThe Soviets also modeled economies in the rest of Eastern Bloc outside the Soviet Union along Soviet command economy lines. Before World War II, the Soviet Union used draconian procedures to ensure compliance with directives to invest all assets in state planned manners, including the collectivisation of agriculture and utilising a sizeable labor army collected in the gulag system. This system was largely imposed on other Eastern Bloc countries after World War II. While propaganda of proletarian improvements accompanied systemic changes, terror and intimidation of the consequent ruthless Stalinism obfuscated feelings of any purported benefits.\n\nStalin felt that socioeconomic transformation was indispensable to establish Soviet control, reflecting the Marxist-Leninist view that material bases, the distribution of the means of production, shaped social and political relations. Moscow trained cadres were put into crucial power positions to fulfill orders regarding sociopolitical transformation. Elimination of the bourgeoisie's social and financial power by expropriation of landed and industrial property was accorded absolute priority.\n\nThese measures were publicly billed as reforms rather than socioeconomic transformations. Throughout the Eastern Bloc, except for Czechoslovakia, \"societal organisations\" such as trade unions and associations representing various social, professional and other groups, were erected with only one organisation for each category, with competition excluded. Those organisations were managed by Stalinist cadres, though during the initial period, they allowed for some diversity.\n\nAt the same time, at the war's end, the Soviet Union adopted a \"plunder policy\" of physically transporting and relocating east European industrial assets to the Soviet Union. Eastern Bloc states were required to provide coal, industrial equipment, technology, rolling stock and other resources to reconstruct the Soviet Union. Between 1945 and 1953, the Soviets received a net transfer of resources from the rest of the Eastern Bloc under this policy of roughly $14 billion, an amount comparable to the net transfer from the United States to western Europe in the Marshall Plan. \"Reparations\" included the dismantling of railways in Poland and Romanian reparations to the Soviets between 1944 and 1948 valued at $1.8 billion concurrent with the domination of SovRoms.\n\nIn addition, the Soviets re-organised enterprises as joint-stock companies in which the Soviets possessed the controlling interest. Using that control vehicle, several enterprises were required to sell products at below world prices to the Soviets, such as uranium mines in Czechoslovakia and East Germany, coal mines in Poland, and oil wells in Romania.\n\nThe trading pattern of the Eastern Bloc countries was severely modified. Before World War II, no greater than 1%–2% of those countries' trade was with the Soviet Union. By 1953, the share of such trade had jumped to 37%. In 1947, Joseph Stalin had also denounced the Marshall Plan and forbade all Eastern Bloc countries from participating in it.\n\nSoviet dominance further tied other Eastern Bloc economies, except for Yugoslavia, to Moscow via the Council for Mutual Economic Assistance (CMEA) or Comecon, which determined countries' investment allocations and the products that would be traded within Eastern Bloc. Although Comecon was initiated in 1949, its role became ambiguous because Stalin preferred more direct links with other party chiefs than the indirect sophistication of the Council. It played no significant role in the 1950s in economic planning.\n\nInitially, Comecon served as cover for the Soviet taking of materials and equipment from the rest of the Eastern Bloc, but the balance changed when the Soviets became net subsidisers of the rest of the Bloc by the 1970s via an exchange of low cost raw materials in return for shoddily manufactured finished goods. While resources such as oil, timber and uranium initially made gaining access to other Eastern Bloc economies attractive, the Soviets soon had to export Soviet raw materials to those countries to maintain cohesion therein. Following resistance to Comecon plans to extract Romania's mineral resources and heavily utilise its agricultural production, Romania began to take a more independent stance in 1964. While it did not repudiate Comecon, it took no significant role in its operation, especially after the rise to power of Nicolae Ceauşescu.\n\nEconomic activity was governed by \"Five year plans\", divided into monthly segments, with government planners frequently attempting to meet plan targets regardless of whether a market existed for the goods being produced. Little coordination existed between departments such that cars could be produced before filling stations or roads were built, or a new hospital in Warsaw in the 1980s could stand empty for four years waiting for the production of equipment to fill it. Nevertheless, if such political objectives had been met, propagandists could boast of increased vehicle production and the completion of another new hospital.\n\nInefficient bureaucracies were frequently created, with for instance, Bulgarian farms having to meet at least six hundred different plan fulfillment figures. Socialist product requirements produced distorted black market consequences, such that broken light bulbs possessed significant market values in Eastern Bloc offices because a broken light bulb was required to be submitted before a new light bulb would be issued.\n\nFactory managers and foremen could hold their posts only if they were cleared under the nomenklatura list system of party-approved \"cadres\". All decisions were constrained by the party politics of what was considered good management. For laborers, work was assigned on the pattern of \"norms\", with sanctions for non-fulfillment. However, the system really served to increase inefficiency, because if the norms were met, management would merely increase them. The stakhanovite system was employed to highlight the achievements of successful work brigades, and \"shock brigades\" were introduced into plants to show the others how much could be accomplished.\n\n\"Lenin shifts\" or \"Lenin Saturdays\" were also introduced, requiring extra work time for no pay. However, the emphasis on the construction of heavy industry provided full employment and social mobility through the recruitment of young rural workers and women. While blue-collar workers enjoyed that they earned as much or more than many professionals, the standard of living did not match the pace of improvement in Western Europe.\n\nOnly Yugoslavia (and later Romania and Albania) engaged in their own industrial planning, though they enjoyed little more success than that of the rest of the Bloc. Albania, which had remained strongly Stalinist in ideology well after de-Stalinisation, was politically and commercially isolated from the other Eastern Bloc countries and the west. By the late 1980s, it was the poorest country in Europe, and still lacked sewerage, piped water, and piped gas.\n\nIn the Soviet Union, there was unprecedented affordability of housing, health care, and education. Apartment rent on average amounted to only 1 percent of the family budget, a figure which reached 4 percent when municipal services are factored in. Tram tickets were 20 kopecks, and a loaf of bread was 15 kopecks. The average salary of an engineer was 140–160 rubles.\n\nThe Soviet Union made major progress in developing the country's consumer goods sector. In 1970, the USSR produced 679 million pairs of leather footwear, compared to 534 million for the United States. Czechoslovakia, which had the world's highest per-capita production of shoes, exported a significant portion of its shoe production to other countries.\n\nThe rising standard of living under socialism led to a steady decrease in the workday and an increase in leisure. In 1974, the average workweek for Soviet industrial workers was 40 hours. Paid vacations in 1968 reached a minimum of 15 workdays. In the mid-1970s the number of free days per year-days off, holidays and vacations was 128-130, almost double the figure from the previous ten years.\n\nBecause of the lack of market signals in such economies, they experienced mis-development by central planners resulting in those countries following a path of extensive (large mobilisation of inefficiently used capital, labor, energy and raw material inputs) rather than intensive (efficient resource use) development to attempt to achieve quick growth. The Eastern Bloc countries were required to follow the Soviet model over-emphasising heavy industry at the expense of light industry and other sectors.\n\nSince that model involved the prodigal exploitation of natural and other resources, it has been described as a kind of \"slash and burn\" modality. While the Soviet system strove for a dictatorship of the proletariat, there was little existing proletariat in many eastern European countries, such that to create one, heavy industry needed to be built. Each system shared the distinctive themes of state-oriented economies, including poorly defined property rights, a lack of market clearing prices and overblown or distorted productive capacities in relation to analogous market economies.\n\nMajor errors and waste occurred in the resource allocation and distribution systems. Because of the party-run monolithic state organs, these systems provided no effective mechanisms or incentives to control costs, profligacy, inefficiency, and waste. Heavy industry was given priority because of its importance for the military-industrial establishment and for the engineering sector.\n\nFactories were sometimes inefficiently located, incurring high transport costs, while poor plant-organisation sometimes resulted in production hold ups and knock-on effects in other industries dependent on monopoly suppliers of intermediates. For example, each country, including Albania, built steel mills regardless of whether they lacked the requisite resource of energy and mineral ores. A massive metallurgical plant was built in Bulgaria despite the fact that its ores had to be imported from the Soviet Union and carried for 320 kilometres from the port at Burgas. A Warsaw tractor factory in 1980 had a 52-page list of unused rusting, then useless, equipment.\n\nThe emphasis on heavy industry diverted investment from the more practical production of chemicals and plastics. In addition, the plans' emphasis on quantity rather than quality made Eastern Bloc products less competitive in the world market. High costs passed through the product chain boosted the 'value' of production on which wage increases were based, but made exports less competitive. Planners rarely closed old factories even when new capacities opened elsewhere. For example, the Polish steel industry retained a plant in Upper Silesia despite the opening of modern integrated units on the periphery while the last old Siemens-Martin process furnace installed in the 19th century was not closed down immediately.\n\nThere were claims that producer goods were favoured over consumer goods, causing consumer goods to be lacking in quantity and quality in the shortage economies that resulted.\n\nBy the mid-1970s, budget deficits rose considerably and domestic prices widely diverged from the world prices, while production prices averaged 2% higher than consumer prices. Many premium goods could be bought only in special stores using foreign currency generally inaccessible to most Eastern Bloc citizens, such as Intershop in East Germany, Beryozka in the Soviet Union, Pewex in Poland, Tuzex in Czechoslovakia, and Corecom in Bulgaria. Much of what was produced for the local population never reached its intended user, while many perishable products became unfit for consumption before reaching their consumers.\n\nAs a result of the deficiencies of the official economy, black markets were created that were often supplied by goods stolen from the public sector. The second, \"parallel economy\" flourished throughout the Bloc because of rising unmet state consumer needs. Black and gray markets for foodstuffs, goods, and cash arose. Goods included household goods, medical supplies, clothes, furniture, cosmetics, and toiletries in chronically short supply through official outlets.\n\nMany farmers concealed actual output from purchasing agencies to sell it illicitly to urban consumers. Hard foreign currencies were highly sought after, while highly valued Western items functioned as a medium of exchange or bribery in Stalinist countries, such as in Romania, where Kent cigarettes served as an unofficial extensively used currency to buy goods and services. Some service workers moonlighted illegally providing services directly to customers for payment.\n\nThe extensive production industrialization that resulted was not responsive to consumer needs and caused a neglect in the service sector, unprecedented rapid urbanization, acute urban overcrowding, chronic shortages, and massive recruitment of women into mostly menial and/or low-paid occupations. The consequent strains resulted in the widespread used of coercion, repression, show trials, purges, and intimidation. By 1960, massive urbanisation occurred in Poland (48% urban) and Bulgaria (38%), which increased employment for peasants, but also caused illiteracy to skyrocket when children left school for work.\n\nCities became massive building sites, resulting in the reconstruction of some war-torn buildings but also the construction of drab dilapidated system-built apartment blocks. Urban living standards plummeted because resources were tied up in huge long-term building projects, while industrialization forced millions of former peasants to live in hut camps or grim apartment blocks close to massive polluting industrial complexes.\n\nCollectivization is a process pioneered by Joseph Stalin in the late 1920s by which Marxist-Leninist regimes in the Eastern Bloc and elsewhere attempted to establish an ordered socialist system in rural agriculture. It required the forced consolidation of small-scale peasant farms and larger holdings belonging to the landed classes for the purpose of creating larger modern \"collective farms\" owned, in theory, by the workers therein. In reality, such farms were owned by the state.\n\nIn addition to eradicating the perceived inefficiencies associated with small-scale farming on discontiguous land holdings, collectivization also purported to achieve the political goal of removing the rural basis for resistance to Stalinist regimes. A further justification given was the need to promote industrial development by facilitating the state's procurement of agricultural products and transferring \"surplus labor\" from rural to urban areas. In short, agriculture was reorganized in order to proletarianize the peasantry and control production at prices determined by the state.\n\nThe Eastern Bloc possesses substantial agricultural resources, especially in southern areas, such as Hungary's Great Plain, which offered good soils and a warm climate during the growing season. Rural collectivization proceeded differently in non-Soviet Eastern Bloc countries than it did in the Soviet Union in the 1920s and 1930s. Because of the need to conceal of the assumption of control and the realities of an initial lack of control, no Soviet dekulakisation-style liquidation of rich peasants could be carried out in the non-Soviet Eastern Bloc countries.\n\nNor could they risk mass starvation or agricultural sabotage (e.g., holodomor) with a rapid collectivization through massive state farms and agricultural producers' cooperatives (APCs). Instead, collectivization proceeded more slowly and in stages from 1948 to 1960 in Bulgaria, Romania, Hungary, Czechoslovakia, and East Germany, and from 1955 to 1964 in Albania. Collectivization in the Baltic republics of the Lithuanian SSR, Estonian SSR, and Latvian SSR took place between 1947 and 1952.\n\nUnlike Soviet collectivization, neither massive destruction of livestock nor errors causing distorted output or distribution occurred in the other Eastern Bloc countries. More widespread use of transitional forms occurred, with differential compensation payments for peasants that contributed more land to APCs. Because Czechoslovakia and East Germany were more industrialized than the Soviet Union, they were in a position to furnish most of the equipment and fertilizer inputs needed to ease the transition to collectivized agriculture. Instead of liquidating large farmers or barring them from joining APCs as Stalin had done through dekulakisation, those farmers were utilised in the non-Soviet Eastern Bloc collectivizations, sometimes even being named farm chairman or managers.\n\nCollectivisation often met with strong rural resistance, including peasants frequently destroying property rather than surrendering it to the collectives. Strong peasant links with the land through private ownership were broken and many young people left for careers in industry. In Poland and Yugoslavia, fierce resistance from peasants, many of whom had resisted the Axis, led to the abandonment of wholesale rural collectivisation in the early 1950s. In part because of the problems created by collectivisation, agriculture was largely de-collectivised in Poland in 1957.\n\nThe fact that Poland nevertheless managed to carry out large-scale centrally planned industrialisation with no more difficulty than its collectivised Eastern Bloc neighbours further called into question the need for collectivisation in such planned economies. Only Poland's \"western territories\", those eastwardly adjacent to the Oder-Neisse line that were annexed from Germany, were substantially collectivised, largely in order to settle large numbers of Poles on good farmland which had been taken from German farmers.\n\nThere was significant progress made in the economy in countries such as the Soviet Union. In 1980, the Soviet Union took first place in Europe and second worldwide in terms of industrial and agricultural production, respectively. In 1960, the USSR's industrial output was only 55% that of America, but this increased to 80% in 1980.\n\nWith the change of the Soviet leadership in 1964, there were significant changes made to economic policy. The Government on 30 September 1965 issued a decree \"On improving the management of industry\" and the 4 October 1965 resolution \"On improving and strengthening the economic incentives for industrial production\". The main initiator of these reforms was Premier A. Kosygin. Kosygin's reforms on agriculture gave considerable autonomy to the collective farms, giving them the right to the contents of private farming. During this period, there was the large-scale land reclamation program, the construction of irrigation channels, and other measures. In the period 1966–70, the gross national product grew by over 35%. Industrial output increased by 48% and agriculture by 17%. In the eighth Five-Year Plan, the national income grew at an average rate of 7.8%. In the ninth Five-Year Plan (1971–1975), the national income grew at an annual rate of 5.7%. In the 10th Five-Year Plan (1976–1981), the national income grew at an annual rate of 4.3%.\n\nThe Soviet Union made noteworthy scientific and technological progress. Unlike capitalist countries, scientific and technological potential in the USSR was used in accordance with a plan on the scale of society as a whole.\n\nIn 1980, the number of scientific personnel in the USSR was 1.4 million. The number of engineers employed in the national economy was 4.7 million. Between 1960 and 1980, the number of scientific personnel increased by a factor of 4. In 1975, the number of scientific personnel in the USSR amounted to one-fourth of the total number of scientific personnel in the world. In 1980, as compared with 1940, the number of invention proposals submitted was more than 5 million. In 1980, there were 10 all-Union research institutes, 85 specialised central agencies, and 93 regional information centres.\n\nThe world's first nuclear power plant was commissioned on June 27, 1954 in Obninsk. Soviet scientists made a major contribution to the development of computer technology. The first major achievements in the field were associated with the building of analog computers. In the USSR, principles for the construction of network analysers were developed by S. Gershgorin in 1927 and the concept of the electrodynamic analog computer was proposed by N. Minorsky in 1936. In the 1940s, the development of AC electronic antiaircraft directors and the first vacuum-tube integrators was begun by L. Gutenmakher. In the 1960s, important developments in modern computer equipment were the BESM-6 system built under the direction of S. A. Lebedev, the MIR series of small digital computers, and the Minsk series of digital computers developed by G.Lopato and V. Przhyalkovsky.\n\nThe Moscow Metro has 180 stations used by around 7 million passengers per day. It is one of the world's busiest undergrounds. In the Soviet period, the fare was 5 kopeks which permitted the rider to ride everywhere on the system.\n\nAuthor Turnock claims that transport in the Eastern Bloc was characterised by poor infrastructural maintenance. The road network suffered from inadequate load capacity, poor surfacing and deficient roadside servicing. While roads were resurfaced, few new roads were built and there were very few divided highway roads, urban ring roads or bypasses. Private car ownership remained low by Western standards.\n\nVehicle ownership increased in the 1970s and 1980s with the production of inexpensive cars in East Germany such as Trabants and the Wartburgs. However, the wait list for the distribution of Trabants was ten years in 1987 and up to fifteen years for Soviet Lada and Czechoslovakian Škoda cars. Soviet-built aircraft exhibited deficient technology, with high fuel consumption and heavy maintenance demands. Telecommunications networks were overloaded.\n\nAdding to mobility constraints from the inadequate transport systems were bureaucratic mobility restrictions. While outside of Albania, domestic travel eventually became largely regulation-free, stringent controls on the issue of passports, visas and foreign currency made foreign travel difficult inside the Eastern Bloc. Countries were inured to isolation and initial post-war autarky, with each country effectively restricting bureaucrats to viewing issues from a domestic perspective shaped by that country's specific propaganda.\n\nSevere environmental problems arose through urban traffic congestion, which was aggravated by pollution generated by poorly maintained vehicles. Large thermal power stations burning lignite and other items became notorious polluters, while some hydro-electric systems performed inefficiently because of dry seasons and silt accumulation in reservoirs. Kraków, Poland was covered by smog 135 days per year, while Wrocław was covered by a fog of chrome gas.\n\nSeveral villages were evacuated because of copper smelting at Głogów. Further rural problems arose from piped water construction being given precedence over building sewerage systems, leaving many houses with only inbound piped water delivery and not enough sewage tank trucks to carry away sewage. The resulting drinking water became so polluted in Hungary that over 700 villages had to be supplied by tanks, bottles and plastic bags. Nuclear power projects were prone to long commissioning delays.\n\nThe catastrophe at the Chernobyl nuclear plant in the Ukrainian SSR was caused by an irresponsible safety test on a reactor design that is normally safe, some operators lacking an even basic understanding of the reactor's processes and authoritarian Soviet bureaucracy, valuing party loyalty over competence, that kept promoting incompetent personnel and choosing cheapness over safety. The consequent release of fallout resulted in the evacuation and resettlement of over 336,000 people leaving a massive desolate Zone of alienation containing extensive still-standing abandoned urban development.\n\nTourism from outside the Eastern Bloc was neglected, while tourism from other Stalinist countries grew within the Eastern Bloc. Tourism drew investment, relying upon tourism and recreation opportunities existing before World War II. By 1945, most hotels were run-down, while many which escaped conversion to other uses by central planners were slated to meet domestic demands. Authorities created state companies to arrange travel and accommodation. In the 1970s, investments were made to attempt to attract western travelers, though momentum for this waned in the 1980s when no long-term plan arose to procure improvements in the tourist environment, such as an assurance of freedom of movement, free and efficient money exchange and the provision of higher quality products with which these tourists were familiar. However, western tourists were generally free to move about in Hungary, Poland and Yugoslavia and go where they wished. It was more difficult or even impossible to go as an individual tourist to East Germany, Czechoslovakia, Romania, Bulgaria and Albania. It was generally possible in all cases for relatives from the west to visit and stay with family in the Eastern Bloc countries, except for Albania. In these cases, permission had to be sought, precise times, length of stay, location and movements had to be known in advance.\n\nCatering to western visitors required creating an environment of an entirely different standard than that used for the domestic populace, which required concentration of travel spots including the building of relatively high-quality infrastructure in travel complexes, which could not easily be replicated elsewhere. In Albania, because of a desire to preserve ideological discipline and the fear of the presence of wealthier foreigners engaging in differing lifestyles, Albania segregated travelers. Because of the worry of the subversive effect of the tourist industry, travel was restricted to 6,000 visitors per year.\n\nGrowth rates in the Eastern Bloc were initially high in the 1950s and 1960s. During this first period, progress was rapid by European standards, and per capita growth within the Eastern Bloc increased by 2.4 times the European average. Eastern Europe accounted for 12.3 percent of European production in 1950 but 14.4 in 1970. However, the system was resistant to change, and did not easily adapt to new conditions. For political reasons, old factories were rarely closed, even when new technologies became available. As a result, after the 1970s, growth rates within the bloc experienced relative decline. Meanwhile, West Germany, Austria, France and other Western European nations experienced increased economic growth in the Wirtschaftswunder (\"economic miracle\"), Trente Glorieuses (\"thirty glorious years\") and the post-World War II boom. After the fall of USSR in the 1990s, growth plummeted, living standards declined, drug use, homelessness and poverty skyrocketed, and suicides increased dramatically. Growth did not begin to return to pre-reform era levels for approximately 15 years.\n\nFrom the end of the World War II to the mid-1970s, the economy of the Eastern Bloc steadily increased at the same rate as the economy in Western Europe, with the least none-reforming Stalinist nations of the Eastern Bloc having a stronger economy then the reformist-Stalinist states. While most western European economies essentially began to approach the per capita Gross Domestic Product (GDP) levels of the United States during the late 1970s and early 1980s, the Eastern Bloc countries did not, with per capita GDPs trailing significantly behind their comparable western European counterparts.\n\nThe following table displays a set of estimated growth rates of GDP from 1951 onward, for the countries of the Eastern Bloc as well as those of Western Europe, as reported by The Conference Board as part of its \"Total Economy Database\". Note that in some cases, data availability does not go all the way back to 1951.\n\nThe United Nations Statistics Division also calculates growth rates, using a different methodology, but only reports the figures starting in 1971 (note: for Slovakia and the constituent republics of the USSR, data availability begins later). Thus, according to the U.N., growth rates in Europe were as follows:\n\nWhile it can be argued the World Bank estimates of GDP used for 1990 figures underestimate Eastern Bloc GDP because of undervalued local currencies per capita incomes were undoubtedly lower than in their counterparts. East Germany was the most advanced industrial nation of the Eastern Bloc. Until the building of the Berlin Wall in 1961, East Germany was considered a weak state, hemorrhaging skilled labor to the West such that it was referred to as \"the disappearing satellite.\" Only after the wall sealed in skilled labor was East Germany able to ascend to the top economic spot in the Eastern Bloc. Thereafter, its citizens enjoyed a higher quality of life and fewer shortages in the supply of goods than those in the Soviet Union, Poland or Romania. However, many citizens in East Germany enjoyed one particular advantage over their counterparts in other Eastern Bloc countries, in that they were often supported by relatives and friends in West Germany who would bring goods from the West on visits or even send goods or money. The West German government and many organisations in West Germany supported projects in East Germany, such as rebuilding and restoration or making good some shortages in times of need (e.g. toothbrushes) from which East German citizens again benefited. The two Germanies, divided politically, remained however united by language (although with two political systems, some terms had different meanings in East and West). West German television reached East Germany, which many East Germans watched and from which they obtained information about their own state in short supply at home. Being part of a divided country, East Germany occupied a unique position therefore in the Eastern Bloc unlike, for example, Hungary in relation to Austria, which had previously been under one monarch but which were already divided by language and culture.\n\nWhile official statistics painted a relatively rosy picture, the East German economy had eroded because of increased central planning, economic autarky, the use of coal over oil, investment concentration in a few selected technology-intensive areas and labor market regulation. As a result, a large productivity gap of nearly 50% per worker existed between East and West Germany. However, that gap does not measure the quality of design of goods or service such that the actual per capita rate may be as low as 14 to 20 per cent. Average gross monthly wages in East Germany were around 30% of those in West Germany, though after accounting for taxation, the figures approached 60%.\n\nMoreover, the purchasing power of wages differed greatly, with only about half of East German households owning either a car or a color television set as late as 1990, both of which had been standard possessions in West German households. The \"Ostmark\" was only valid for transactions inside East Germany, could not be legally exported or imported and could not be used in the East German Intershops which sold premium goods. In 1989, 11% of the East German labor force remained in agriculture, 47% was in the secondary sector and only 42% in services.\n\nOnce installed, the economic system was difficult to change given the importance of politically reliable management and the prestige value placed on large enterprises. Performance declined during the 1970s and 1980s due to inefficiency when industrial input costs, such as energy prices, increased. Though growth lagged behind the west, it did occur. Consumer goods started to become more available by the 1960s.\n\nBefore the Eastern Bloc's dissolution, some major sectors of industry were operating at such a loss that they exported products to the West at prices below the real value of the raw materials. Hungarian steel costs doubled those of western Europe. In 1985, a quarter of Hungary's state budget was spent on supporting inefficient enterprises. Tight planning in Bulgaria industry meant continuing shortages in other parts of its economy.\n\nIn social terms, the 18 years (1964–1982) of Brezhnev's leadership saw real incomes grow more than 1.5 times. More than 1.6 thousand million square metres of living space were commissioned and provided to over 160 million people. At the same time, the average rent for families did not exceed 3% of the family income. There was unprecedented affordability of housing, health care, and education.\n\nIn a survey by the Sociological Research Institute of the USSR Academy of Sciences in 1986, 75% of those surveyed said that they were better off than the previous ten years. Over 95% of Soviet adults considered themselves \"fairly well off\". 55% of those surveyed felt that medical services improved, 46% believed public transportation had improved, and 48% said that the standard of services provided public service establishments had risen.\n\nDuring the years 1957–65 housing policy underwent several institutional changes with industrialisation and urbanisation had not been matched by an increase in housing after World War II. Housing shortages in the Soviet Union were worse than in the rest of the Eastern Bloc due to a larger migration to the towns and more wartime devastation, and were worsened by Stalin's pre-war refusals to invest properly in housing. Because such investment was generally not enough to sustain the existing population, apartments had to be subdivided into increasingly smaller units, resulting in several families sharing an apartment previously meant for one family.\n\nThe prewar norm became one Soviet family per room, with the toilets and kitchen shared. The amount of living space in urban areas fell from 5.7 square metres per person in 1926 to 4.5 square metres in 1940. In the rest of the Eastern Bloc during this time period, the average number of people per room was 1.8 in Bulgaria (1956), 2.0 in Czechoslovakia (1961), 1.5 in Hungary (1963), 1.7 in Poland (1960), 1.4 in Romania (1966), 2.4 in Yugoslavia (1961), and 0.9 in 1961 in East Germany.\n\nAfter Stalin's death in 1953, forms of an economic \"New Course\" brought a revival of private house construction. Private construction peaked in 1957–1960 in many Eastern Bloc countries and then declined simultaneously along with a steep increase in state and co-operative housing. By 1960, the rate of house-building per head had picked up in all countries in the Eastern Bloc. Between 1950 and 1975, worsening shortages were generally caused by a fall in the proportion of all investment made housing. However, during that period the total number of dwellings increased.\n\nDuring the last fifteen years of this period (1960 to 1975), an emphasis was made for a supply side solution, which assumed that industrialised building methods and high rise housing would be cheaper and quicker than traditional brick-built, low-rise housing. Such methods required manufacturing organisations to produce the prefabricated components and organisations to assemble them on site, both of which planners assumed would employ large numbers of unskilled workers-with powerful political contacts. The lack of participation of eventual customers, the residents, constituted one factor in escalating construction costs and poor quality work. This led to higher demolition rates and higher costs to repair poorly constructed dwellings. In addition, because of poor quality work, a black market arose for building services and materials that could not be procured from state monopolies.\n\nIn most countries, completions (new dwellings constructed) rose to a high point between 1975 and 1980 and then fell, as a result presumably of worsening international economic conditions. This occurred in Bulgaria, Hungary, East Germany, Poland, Romania (with an earlier peak in 1960 also), Czechoslovakia, and Yugoslavia, while the Soviet Union peaked in 1960 and 1970. While between 1975 and 1986, the proportion of investment devoted to housing actually rose in most of the Eastern Bloc, general economic conditions resulted in total investment amounts falling or becoming stagnant.\n\nThe employment of socialist ideology in housing policy declined in the 1980s, which accompanied a shift in authorities looking at the need of residents to an examination of potential residents' ability to pay. Yugoslavia was unique in that it continuously mixed private and state sources of housing finance, stressed self-managed building co-operatives along with central government controls.\n\nThe initial year that shortages were effectively measured and shortages in 1986 were as follows:\n\nThese are official housing figures and may be low. For example, in the Soviet Union, the figure of 26,662,400 in 1986 almost certainly underestimates shortages for the reason that it does not count shortages from large Soviet rural-urban migration; another calculation estimates shortages to be 59,917,900. By the late 1980s, Poland had an average 20-year wait time for housing, while Warsaw had between a 26- and 50-year wait time. In the Soviet Union, widespread illegal subletting occurred at exorbitant rates. Toward the end of the Eastern Bloc allegations of misallocations and illegal distribution of housing were raised in Soviet CPSU Central Committee meetings.\n\nIn Poland, housing problems were caused by slow rates of construction, poor home quality (which was even more pronounced in villages), and a large black market. In Romania, social engineering policy and concern about the use of agricultural land forced high densities and high-rise housing designs. In Bulgaria, a prior emphasis on monolithic high-rise housing lessened somewhat in the 1970s and 1980s. In the Soviet Union, housing was perhaps the primary social problem. While Soviet housing construction rates were high, quality was poor and demolition rates were high, in part because of an inefficient building industry and lack of both quality and quantity of construction materials.\n\nEast German housing suffered from a lack of quality and a lack of skilled labor, with a shortage of materials, plot and permits. In staunchly Stalinist Albania, housing blocks (\"panelka\") were spartan, with six story walk-ups being the most frequent design. Housing was allocated by workplace trade unions and built by voluntary labor organised into brigades within the workplace. Yugoslavia suffered from fast urbanisation, uncoordinated development and poor organisation resulting from a lack of hierarchical structure and clear accountability, low building productivity, the monopoly position of building enterprises, and irrational credit policies.\n\nThree months after the death of Joseph Stalin, a dramatic increase of emigration (Republikflucht, brain drain) occurred from East Germany in the first half-year of 1953. Large numbers of East Germans traveled west through the only \"loophole\" left in the Eastern Bloc emigration restrictions, the Berlin sector border. The East German government then raised \"norms\" – the amount each worker was required to produce—by 10%. Already disaffected East Germans, who could see the relative economic successes of West Germany within Berlin, became enraged. Angry building workers initiated street protests, and were soon joined by others in a march to the Berlin trade union headquarters.\n\nWhile no official spoke to them at that location, by 2:00 pm, the East German government agreed to withdraw the \"norm\" increases. However, the crisis had already escalated such that the demands were now political, including free elections, disbanding the army and resignation of the government. By 17 June, strikes were recorded in 317 locations involving approximately 400,000 workers. When strikers set ruling SED party buildings aflame and tore the flag from the Brandenburg Gate, SED General Secretary Walter Ulbricht left Berlin.\n\nA major emergency was declared and the Soviet Red Army stormed some important buildings. Within hours, Soviet tanks arrived, but they did not immediately fire upon all workers. Rather, a gradual pressure was applied. Approximately 16 Soviet divisions with 20,000 soldiers from the Group of Soviet Forces in Germany using tanks, as well as 8,000 Kasernierte Volkspolizei members, were employed. Bloodshed could not be entirely avoided, with the official death toll standing at 21, while the actual casualty toll may have been much higher. Thereafter, 20,000 arrests took place along with 40 executions.\n\nAfter Stalin's death in 1953, a period of de-Stalinization followed, with reformist Imre Nagy replacing Hungarian Stalinist dictator Mátyás Rákosi. Responding to popular demand, in October 1956, the Polish government appointed the recently rehabilitated reformist Władysław Gomułka as First Secretary of the Polish United Workers' Party, with a mandate to negotiate trade concessions and troop reductions with the Soviet government. After a few tense days of negotiations, on 19 October, the Soviets finally gave in to Gomułka's reformist requests.\n\nThe revolution began after students of the Technical University compiled a list of Demands of Hungarian Revolutionaries of 1956 and conducted protests in support of the demands on 22 October. Protests of support swelled to 200,000 by 6 pm the following day, The demands included free secret ballot elections, independent tribunals, inquiries into Stalin and Rákosi Hungarian activities and that \"the statue of Stalin, symbol of Stalinist tyranny and political oppression, be removed as quickly as possible.\" By 9:30 pm the statue was toppled (see photo to the right) and jubilant crowds celebrated by placing Hungarian flags in Stalin's boots, which was all that remained the statue. The ÁVH was called, Hungarian soldiers sided with the crowd over the ÁVH and shots were fired on the crowd.\n\nBy 2 am on 24 October, under orders of Soviet defense minister Georgy Zhukov, Soviet tanks entered Budapest. Protester attacks at the Parliament forced the dissolution of the government. A ceasefire was arranged on 28 October, and by 30 October most Soviet troops had withdrawn from Budapest to garrisons in the Hungarian countryside. Fighting had virtually ceased between 28 October and 4 November, while many Hungarians believed that Soviet military units were indeed withdrawing from Hungary.\n\nThe new government that came to power during the revolution formally disbanded ÁVH, declared its intention to withdraw from the Warsaw Pact and pledged to re-establish free elections. The Soviet Politburo thereafter moved to crush the revolution. On 4 November, a large Soviet force invaded Budapest and other regions of the country. The last pocket of resistance called for ceasefire on 10 November. Over 2,500 Hungarians and 722 Soviet troops were killed and thousands more were wounded.\n\nThousands of Hungarians were arrested, imprisoned and deported to the Soviet Union, many without evidence. Approximately 200,000 Hungarians fled Hungary, some 26,000 Hungarians were put on trial by the new Soviet-installed János Kádár government, and of those, 13,000 were imprisoned. Imre Nagy was executed, along with Pál Maléter and Miklós Gimes, after secret trials in June 1958. Their bodies were placed in unmarked graves in the Municipal Cemetery outside Budapest. By January 1957, the new Soviet-installed government had suppressed all public opposition.\n\nA period of political liberalization in Czechoslovakia called the Prague Spring took place in 1968. The event was spurred by several events, including economic reforms that addressed an early 1960s economic downturn. The event began on 5 January 1968, when reformist Slovak Alexander Dubček came to power. In April, Dubček launched an \"Action Program\" of liberalizations, which included increasing freedom of the press, freedom of speech and freedom of movement, along with an economic emphasis on consumer goods, the possibility of a multiparty government and limiting the power of the secret police.\n\nInitial reaction within the Eastern Bloc was mixed, with Hungary's János Kádár expressing support, while Soviet leader Leonid Brezhnev and others grew concerned about Dubček's reforms, which they feared might weaken the Eastern Bloc's position during the Cold War. On 3 August, representatives from the Soviet Union, East Germany, Poland, Hungary, Bulgaria, and Czechoslovakia met in Bratislava and signed the Bratislava Declaration, which affirmed unshakable fidelity to Marxism–Leninism and proletarian internationalism and declared an implacable struggle against \"bourgeois\" ideology and all \"anti-socialist\" forces.\nOn the night of 20–21 August 1968, Eastern Bloc armies from five Warsaw Pact countries – the Soviet Union, Poland, East Germany, Hungary and Bulgaria — invaded Czechoslovakia. The invasion comported with the Brezhnev Doctrine, a policy of compelling Eastern Bloc states to subordinate national interests to those of the Bloc as a whole and the exercise of a Soviet right to intervene if an Eastern Bloc country appeared to shift towards capitalism. The invasion was followed by a wave of emigration, including an estimated 70,000 Czechoslovaks initially fleeing, with the total eventually reaching 300,000.\n\nIn April 1969, Dubček was replaced as first secretary by Gustáv Husák, and a period of \"normalization\" began. Husák reversed Dubček's reforms, purged the party of liberal members, dismissed opponents from public office, reinstated the power of the police authorities, sought to re-centralize the economy and re-instated the disallowance of political commentary in mainstream media and by persons not considered to have \"full political trust\".\n\nDuring the late 1980s, the weakened Soviet Union gradually stopped interfering in the internal affairs of Eastern Bloc nations and numerous independence movements took place.\n\nFollowing the Brezhnev stagnation, the reform-minded Soviet leader Mikhail Gorbachev in 1985 signaled the trend towards greater liberalization. Gorbachev rejected the Brezhnev Doctrine, which held that Moscow would intervene if socialism were threatened in any state. He announced what was jokingly called the \"Sinatra Doctrine\" after the singer's \"My Way\", to allow the countries of Central and Eastern Europe to determine their own internal affairs during this period.\n\nGorbachev initiated a policy of \"glasnost\" (openness) in the Soviet Union, and emphasized the need for \"perestroika\" (economic restructuring). The Soviet Union was struggling economically after the long war in Afghanistan and did not have the resources to control Central and Eastern Europe.\n\nIn 1989, a wave of revolutions, sometimes called the \"Autumn of Nations\", swept across the Eastern Bloc.\n\nMajor reforms occurred in Hungary following the replacement of János Kádár as General Secretary of the Communist Party in 1988. In Poland in April 1989, the Solidarity organization was legalized and allowed to participate in parliamentary elections. It captured 99% of available parliamentary seats.\n\nOn 9 November 1989, following mass protests in East Germany and the relaxing of border restrictions in Czechoslovakia, tens of thousands of Eastern Berliners flooded checkpoints along the Berlin Wall and crossed into West Berlin. The wall was torn down and Germany was eventually reunified. In Bulgaria, the day after the mass crossings through the Berlin Wall, the leader Todor Zhivkov was ousted by his Politburo and replaced with Petar Mladenov.\n\nIn Czechoslovakia, following protests of an estimated half-million Czechs and Slovaks demanding freedoms and a general strike, the authorities, which had allowed travel to the West, abolished provisions guaranteeing the ruling Communist party its leading role. President Gustáv Husák appointed the first largely non-Communist government in Czechoslovakia since 1948, and resigned, in what was called the Velvet Revolution.\n\nRomania had rejected de-Stalinization. Following growing public protests, dictator Nicolae Ceaușescu ordered a mass rally in his support outside Communist Party headquarters in Bucharest. But mass protests against Ceauşescu proceeded. The Romanian military sided with protesters and turned on Ceauşescu. They executed him after a brief trial three days later.\n\nEven before the Bloc's last years, all of the countries in the Warsaw Pact did not always act as a unified bloc. For instance, the 1968 invasion of Czechoslovakia was condemned by Romania, which refused to take part in it. Albania withdrew from the Bloc in response to the invasion.\n\nA 2009 Pew Research Center poll showed that 72% of Hungarians and 62% of both Ukrainians and Bulgarians felt that their lives were worse off after 1989, when free markets were made dominant. A follow-up poll by Pew Research Center in 2011 showed that 45% of Lithuanians, 42% of Russians, and 34% of Ukrainians approved of the change to a market economy. Writing in 2018, the scholars Kristen R. Ghodsee and Scott Sehon assert that \"subsequent polls and qualitative research across Russia and eastern Europe confirm the persistence of these sentiments as popular discontent with the failed promises of free-market prosperity has grown, especially among older people.\"\n\nThe following countries are one-party states in which the institutions of the ruling Communist party and the state have become intertwined. They are generally adherents of Marxism–Leninism and its derivations. They are listed here together with the year of their founding and their respective ruling parties.\n\n\n\n"}
{"id": "7073591", "url": "https://en.wikipedia.org/wiki?curid=7073591", "title": "Food microbiology", "text": "Food microbiology\n\nFood microbiology is the study of the microorganisms that inhibit, create, or contaminate food, including the study of microorganisms causing food spoilage, pathogens that may cause disease especially if food is improperly cooked or stored, those used to produce fermented foods such as cheese, yogurt, bread, beer, and wine, and those with other useful roles such as producing probiotics.\n\nFood safety is a major focus of food microbiology. Numerous agents of disease, pathogens, are readily transmitted via food, including bacteria, and viruses. Microbial toxins are also possible contaminants of food. However, microorganisms and their products can also be used to combat these pathogenic microbes. Probiotic bacteria, including those that produce bacteriocins, can kill and inhibit pathogens. Alternatively, purified bacteriocins such as nisin can be added directly to food products. Finally, bacteriophages, viruses that only infect bacteria, can be used to kill bacterial pathogens. Thorough preparation of food, including proper cooking, eliminates most bacteria and viruses. However, \"toxins produced\" by contaminants may not be liable to change to non-toxic forms by heating or cooking the contaminated food due to other safety conditions.\n\nFermentation is one of the methods to preserve food and alter its quality. Yeast, especially \"Saccharomyces cerevisiae\", is used to leaven bread, brew beer and make wine. Certain bacteria, including lactic acid bacteria, are used to make yogurt, cheese, hot sauce, pickles, fermented sausages and dishes such as kimchi. A common effect of these fermentations is that the food product is less hospitable to other microorganisms, including pathogens and spoilage-causing microorganisms, thus extending the food's shelf-life. Some cheese varieties also require molds to ripen and develop their characteristic flavors.\n\nSeveral microbially produced biopolymers are used in the food industry.\n\nAlginates can be used as thickening agents.\nAlthough listed here under the category 'Microbial polysaccharides', commercial alginates are currently only produced by extraction from brown seaweeds such as \"Laminaria hyperborea\" or \"L. japonica\".\n\nPoly-γ-glutamic acid (γ-PGA) produced by various strains of \"Bacillus\" has potential applications as a thickener in the food industry.\n\nTo ensure safety of food products, microbiological tests such as testing for pathogens and spoilage organisms are required. This way the risk of contamination under normal use conditions can be examined and food poisoning outbreaks can be prevented. Testing of food products and ingredients is important along the whole supply chain as possible flaws of products can occur at every stage of production. Apart from detecting spoilage, microbiological tests can also determine germ content, identify yeasts and molds, and salmonella. For salmonella, scientists are also developing rapid and portable technologies capable of identifying unique variants of Salmonella .\n\nPolymerase Chain Reaction (PCR) is a quick and inexpensive method to generate numbers of copies of a DNA fragment at a specific band (\"PCR (Polymerase Chain Reaction),\" 2008). For that reason, scientists are using PCR to detect different kinds of viruses or bacteria, such as HIV and anthrax based on their unique DNA patterns. Various kits are commercially available to help in food pathogen nucleic acids extraction, PCR detection, and differentiation. The detection of bacterial strands in food products is very important to everyone in the world, for it helps prevent the occurrence of food borne illness. Therefore, PCR is recognized as a DNA detector in order to amplify and trace the presence of pathogenic strands in different processed food.\n\n"}
{"id": "45468719", "url": "https://en.wikipedia.org/wiki?curid=45468719", "title": "Gaburici Cabinet", "text": "Gaburici Cabinet\n\nThe Gaburici Cabinet was the Cabinet of Moldova from 18 February to 30 July 2015. It consisted of ministers from the Liberal Democratic Party (PLDM) and the Democratic Party (PDM), who together formed the Political Alliance for a European Moldova. The Cabinet was installed after a successful vote of confidence held in the Parliament of Moldova on 18 February 2015. It was a minority government.\n\nThe Cabinet consisted of the Prime Minister of Moldova Chiril Gaburici, three Deputy Prime Ministers, 14 other ministers, and two \"ex officio\" members.\n"}
{"id": "20848877", "url": "https://en.wikipedia.org/wiki?curid=20848877", "title": "Global Spin", "text": "Global Spin\n\nGlobal Spin: The Corporate Assault on Environmentalism is a book by Professor Sharon Beder. It was first published in 1997 and there have been subsequent updated editions in 2000 and 2002. The book uses many detailed case studies to build up a \"bigger picture\" of how large corporations attempt to manipulate environmental issues for their own ends. In the first edition most of the material was from the United States, where the corporate environmental impact has been greatest.\n\n"}
{"id": "1487634", "url": "https://en.wikipedia.org/wiki?curid=1487634", "title": "Hlinka Guard", "text": "Hlinka Guard\n\nHlinka Guard (; ; abbreviated as HG) was the militia maintained by the Slovak People's Party in the period from 1938 to 1945; it was named after Andrej Hlinka.\n\nThe Hlinka Guard was preceded by the Rodobrana (Home Defense/Nation's Defense) organization, which existed from 1923 to 1927, when the Czechoslovak authorities ordered its dissolution. During the crisis caused by Hitler's demand for the Sudetenland (in the summer of 1938), the Hlinka Guard emerged spontaneously, and on October 8 of that year, a week after Hitler's demand had been accepted at the Munich conference, the guard was officially set up, with Karol Sidor (1901–1953) as its first commander.\n\nUnder one of the Beneš decrees, No. 16/1945 Coll., membership of the Hlinka Guard was punishable by 5 to 20 years' imprisonment.\n\nThe guard was the Hlinka party's military arm for internal security, and it continued in that role under the autonomous government of Slovakia in federated Czecho-Slovakia. The Hlinka Guard were Slovakia's state police and most willingly helped Hitler with his plans. It operated against Jews, Czechs, Hungarians, the Left, and the opposition. By a decree issued on October 29, 1938, the Hlinka Guard was designated as the only body authorized to give its members paramilitary training, and it was this decree that established its formal status in the country. Hlinka guardsmen wore black uniforms and a cap shaped like a boat, with a woolen pompom on top, and they used the raised-arm salute. The official salute was \"Na stráž!\" (\"On guard!\").\n\nUntil March 14, 1939, when Slovakia declared its independence, the Hlinka Guard attracted recruits from all walks of life. On the following day, March 15, Alexander Mach became its commander, retaining the post up to the collapse of the pro-Nazi regime in Slovakia in 1945. Its functions were laid down in a series of government decrees: it was to be a paramilitary organization attached to the party, fostering love of country, providing paramilitary training, and safeguarding internal security. By assuming these tasks, the guard was meant to counterbalance the army and the police. In 1941 Hlinka Guard shock troops were trained in SS camps in Germany, and the SS attached an adviser to the guard. At this point many of the guardsmen who were of middle-class origin quit, and thenceforth the organization consisted of peasants and unskilled laborers, together with various doubtful elements. A social message was an integral part of the radical nationalism that it sought to impart.\n\nIn 1942, the Hlinka Guard headed deportations of Slovak Jews to Nazi concentration camp of Auschwitz. The Guard would regularly make round ups for Jews in the spring and summer months. Deportation of the Jews by Hlinka Guards lead to confiscation of Jewish property (\"\") while distributing some of that property to individual members of the Hlinka Guards. Over the course of time, the guardsmen prospered financially but their zeal for stolen wealth never abated.\n\nA small group called Náš Boj (Our Struggle), which operated under SS auspices, was the most radical element in the guard. Throughout its years of existence, the Hlinka Guard competed with the Hlinka party for primacy in ruling the country. After the anti-Nazi Slovak National Uprising in August 1944, the SS took over and shaped the Hlinka Guard to suit its own purposes. Special units of the guard (Hlinka Guard Emergency Divisions – POHG) were employed against partisans and Jews.\n\nThe Hlinka Guards are a pivotal antagonist group in the 2006 novel by Colum McCann, \"Zoli\".\n"}
{"id": "12868392", "url": "https://en.wikipedia.org/wiki?curid=12868392", "title": "Hydrography of the San Francisco Bay Area", "text": "Hydrography of the San Francisco Bay Area\n\nThe Hydrography of the San Francisco Bay Area is a complex network of watersheds, marshes, rivers, creeks, reservoirs, and bays predominantly draining into the San Francisco Bay and Pacific Ocean.\n\nThe largest bodies of water in the Bay Area are the San Francisco Bay, San Pablo Bay, and Suisun Bay. The San Francisco Bay is one of the largest bays in the world. Many inlets on the edges of the three major bays are designated as bays in their own right, such as Richardson Bay, San Rafael Bay, Grizzly Bay, and San Leandro Bay.\n\nNearby bays along the Pacific Coast include Bodega Bay, Tomales Bay, Drakes Bay, Bolinas Bay, and Half Moon Bay.\n\nThe largest rivers are the Sacramento and San Joaquin Rivers, which drain into the Sacramento-San Joaquin River Delta and thence to Suisun Bay. Other major rivers of the North Bay are the Napa River, the Petaluma River, the Gualala River, and the Russian River; the former two drain into San Pablo Bay, the latter two into the Pacific Ocean.\n\nIn the South Bay, the Guadalupe River drains into San Francisco Bay near Alviso.\n\nThe Bay Area has a network of streams that are generally called creeks, but sometimes called arroyos, due to the Spanish language heritage evident in names such as Santa Rosa Creek and San Pablo Creek. Due to low rainfall in the summer months (May–October), many Bay Area creeks are intermittent, flowing above ground only during part of the year.\n\nPolitical groups have been formed to preserve creeks or restore creeks which have been culverted for development. Baxter Creek in Contra Costa County has been daylighted in various points along its piped route by Friends of Baxter Creek. Other organizations include Friends of Five Creeks, which monitors, restores, cleans and educates in relation to creeks flowing from the Berkeley Hills to the Eastshore Estuary in the East Bay.\n\"Guide to San Francisco Bay Area Creeks\"\n\nThe Bay Area has springs which are the source of most of the minor creeks in the East Bay hills such as Garrity Creek. In the North Bay there are hot springs which serve as further tourist attractions to Wine Country tourists and spa goers.\n\nThe Bay Area has many lakes, particularly if one includes artificial ones such as Lake Berryessa. Some are very small (such as Jewel Lake in Berkeley) and others are covered (Summit Reservoir, for example). Lake Merced and Lake Merritt are salt lakes; the former is drying up while the latter is a closed off estuarine cove.\n\nDue to pollution of surface water, much of the area's potable water is located underground, for instance in the Mocho Subbasin of the Livermore Valley. As these aquifers get drawn down by pumping, there is increasing interest in ways to speed up the recharging of these resources.\n\nPrior to the introduction of European agricultural methods, the shores of San Francisco Bay consisted mostly of tidal marshes. Approximately 85% of those marshes have been lost or destroyed, but about 50 marshes and marsh fragments remain. In the Delta area, marshes were drained for farmland. In San Francisco, marshes were filled in for urban development. In the East Bay, portions were used as landfill. In the South Bay, huge tracts have served as commercial salt evaporation ponds. In the North Bay, the Napa Sonoma Marsh and Point Molate Marsh remain productive ecosystems. Some wetlands have been restored or protected from further development. Success stories include Eastshore State Park and Crissy Field. Many native and recovered wetlands are preserved in the Don Edwards San Francisco Bay National Wildlife Refuge and the San Pablo Bay National Wildlife Refuge.\n\nWetlands also exist on the Pacific Coast (the Estero Americano, for instance) and in certain inland valleys: for example, the Laguna de Santa Rosa near Santa Rosa.\n\nThe Bay Area is a large natural harbor. Around it have grown seaports and naval facilities. Active ports include the ports of Richmond, Redwood City, San Francisco, and Oakland. Ships also traverse the bay heading to and from ports in Stockton and Sacramento. During World War I and World War II the region was the United States's major shipbuiding center for the Pacific. Former naval facilities include Point Molate Naval Fuel Depot, Alameda Point Naval Facility and Mare Island Naval Shipyard.\n\nMany Bay Area cities have marinas, including Berkeley, Petaluma, and Redwood City.\n\nThere is also an extensive commuter ferry system, which is being expanded by the San Francisco Bay Water Transit Authority.\n\nOceanic harbors have been built at Bodega Bay and Half Moon Bay.\n\n"}
{"id": "2489169", "url": "https://en.wikipedia.org/wiki?curid=2489169", "title": "Institute of Zoology", "text": "Institute of Zoology\n\nThe Institute of Zoology (IoZ) is the research division of the Zoological Society of London (ZSL) in England. It is a government-funded research institute specialising in scientific issues relevant to the conservation of animal species and their habitats. The Institute is based alongside London Zoo at ZSL's Regent's Park site in the City of Westminster.\n\nThe Institute has around 25 full-time research staff, plus postdoctoral research assistants, technicians and Ph.D. students. The Institute is supported by the Higher Education Funding Council for England in partnership with the University of Cambridge, and receives additional research funding from UK research councils (NERC, BBSRC, ESRC) and research charities (the Wellcome Trust and the Leverhulme Trust). Research covers many fundamental aspects of biological sciences which have relevance to the conservation of animal species and their habitats.\n\nThe Institute offers research training through Ph.D. studentships, and hosts Undergraduate and Masters level research projects conducted as part of its own M.Sc. courses and courses at other institutions. Undergraduate projects are available at London Zoo and Whipsnade Zoo.\n\n\n"}
{"id": "29394648", "url": "https://en.wikipedia.org/wiki?curid=29394648", "title": "Instituto Nacional de Colonización", "text": "Instituto Nacional de Colonización\n\nThe Instituto Nacional de Colonización y Desarrollo Rural, , was the administrative entity that was established by the Spanish State in October 1939, shortly after the end of the Spanish Civil War, in order to repopulate certain areas of Spain. This entity depended from the Ministry of Agriculture and it sought to alleviate the effects of the devastation caused by the three years of civil war.\n\nThe Instituto acquired land which it transferred to the villagers under different conditions according to the area and the levels of poverty of the tenants. The tenants eventually were expected to pay a small sum that allowed them to become the future owners of the land they tilled.\n\nThis ambitious plan led to the establishment of new villages in different parts of Spain, some of which still survive. The Instituto reached a height of activity and influence during the first two decades of Francoist Spain, but after the Plan de Estabilización in 1959, and the subsequent Planes de Desarrollo, its autarchic goals and ideals became outdated. By 1971 the word \"Colonization\" had stopped being politically correct and the name of the entity was changed to Instituto Nacional de Reforma y Desarrollo Agrario (IRYDA).\n\nThe Instituto's main goal was to increase agricultural production in Spain by devoting more land surface to agriculture. Priority was given to the development of new irrigated areas in arid and semi-arid zones. This goal was very effective for the propaganda purposes of the new regime and triumphalistic claims were made that the colonization measures would increase self-sufficiency. But often irrigation was opposed to the traditional and sustainable methods of dryland farming that were ecologically more in tune with locally available resources in fragile environments.\n\nAlthough the plans of the IRYDA were implemented with the avowed goal of a \"better management of natural resources of the country\" (), the agricultural policies implemented were sometimes not mindful of the environment, leading to salinization of the terrain and to soil erosion in some areas. Some of the villages that were established in former wetlands or in chronic drought areas were later abandoned, along with the lands that surrounded them and that had formerly been earmarked for agriculture.\n\nMany of the new villages were given a name related to the nearest river or even a name with an explicit reference to the \"Caudillo\" in order to cast a benevolent image of Francisco Franco, like Llanos del Caudillo, \"Villafranco del Delta\", a village in the Montsià comarca nowadays rechristened as El Poblenou del Delta or Isla Mayor near Seville, the former \"Villafranco del Guadalquivir\".\n\nSome of these new settlements were built to house the families whose houses were flooded when their ancestral village was submerged by the waters of one of the many reservoirs built during the development plans of the 1950s and 1960s, like Loriguilla, Mequinensa and Faió (Fayón), among others. Others were renovations and repopulations of previously extant but abandoned towns.\n\n\n\n\n"}
{"id": "716105", "url": "https://en.wikipedia.org/wiki?curid=716105", "title": "Internal passport", "text": "Internal passport\n\nAn internal passport is an identity document. \nWhen passports first emerged, there was no clear distinction between internal and international ones. Later, some countries developed sophisticated systems of passports for various purposes and various groups of population. \n\nUses for internal passports have included restricting citizens of a subdivided state to employment in their own area (preventing their migration to richer cities or regions), clearly recording the ethnicity of citizens to enforce segregation or prevent passing, and controlling access to sensitive sites or closed cities.in form\n\nCurrently, only Russia is known to still have internal passports as a part of their bureaucratic heritage, though it is no longer used to restrict the movement of people: a Russian internal passport is essentially an identification document in the form of a booklet.\n\nCountries that currently have internal passports include:\n\nInternal passports are known to have been issued and used previously by:\n\nIn many countries, the word \"passport\" is only used in modern language to denote a document issued for the purpose of international travel, which is subject to discretionary permission. However, in post-Soviet countries, the word \"passport\" is implied to merely mean a primary identification document, especially if has the form of a booklet. Nevertheless, it is also extended by analogy to other forms of identification documents. For example, Ukrainian identity cards that are replacing old-fashioned internal passport booklets are still called passports. \n\nIn 1885 the \"pass system\" was introduced in Canada, to restrict and control the movement of First Nations people within Canada. Instituted at the time of the North-West Rebellion, it remained in force for 60 years despite having no basis in law. Any First Nation person caught outside his reservation without a pass issued by an Indian agent was returned to their reservation or incarcerated.\n\nIn France, in the past, one had to show an internal passport to change city. Former convicts who had served forced labour, even after having served their sentence, had a yellow passport, which made them outcasts. A famous holder of the yellow passport is the former ' Jean Valjean the hero of the novel by Victor Hugo.\n\nA \"décret\" issued 2 October 1795 (10 Vendémiaire year IV in the French Republican Calendar) required all persons traveling outside the limits of their canton to possess either an internal passport (for voyages within France) or external passport (for travel outside France). In 1815 an internal passport cost 2 francs and was delivered by the mayor of the commune to the residence of the passport requester. Internal passports were significantly easier to obtain than passports for foreign travel, which cost 10 francs in 1815. In the early 19th century, many emigrants obtained cheaper and easier-to-obtain internal passports to travel to the port of Havre, from which most ships to the United States departed. As control of the issuance of internal passports, which required a certificate of good behavior, was in the hands of the mayors of communes, there was some degree of favoritism in the issuance/denial of internal passports in the 18th century.\n\nIn France, the \"livret de circulation\" (booklet of circulation) and its variant the \"carnet de circulation\" (notebook of circulation) provided to those of no fixed abode were particularly constraining and discriminatory obligations imposed on itinerants.\n\nAt the end of 2012, when examining a , the Constitutional Council ended the notebook of circulation, considering that it harmed disproportionately the freedom of movement.\n\nIn South Africa, the pass laws (notably the Pass Laws Act 1952, which applied until 1986) were a component of the apartheid system. The laws regulated where, when and for how long persons could remain outside their “homeland” — which, for many people, was not their homeland, so thousands of autochthon people were forced to change region. These laws also made it compulsory for all black South Africans over the age of 15 to carry a \"pass book\" at all times. However, the legislation also required that citizens of all races have on their person an ID book, which closely approximates a passport.\n\nThe internal passport system of the Russian Empire was abandoned after the October Revolution in 1917, lifting most limitations upon internal movements of members of labouring classes in Soviet Russia. Labour booklets became the principal means of personal identification.\n\nIn 1932, the \"passport regime\" was reintroduced, its declared purpose to improve the registration of population and \"relieve\" major industrial cities and other sensitive localities of \"hiding kulaks and dangerous political elements\" and those \"not engaged in labor of social usefulness\". The \"passportization\" process developed gradually involving factories, large, medium, and small cities, settlements, and rural areas, and finally became universal by the mid-1970s.\n\nInternal passports were used in the Soviet Union for identification of persons for various purposes. In particular, passports were used to control and monitor the place of residence by means of the \"propiska\", a regulation designed to control the population's internal movement by binding a person to his or her permanent place of residence. For example, a valid \"propiska\" was necessary to receive higher education or medical treatment, although these services were not limited to the location registered. Besides marriage to a resident of another area, university education was the most popular way of circumventing one's \"propiska\" and residing elsewhere. Also, since only a minority of dwellings were privately owned, having a \"propiska\" at a certain address meant that one had the right to live there.\n\nAll residents were required by law to record their address in the document and to report any relevant changes to a local office of the Ministry of Internal Affairs. For example, citizens needed to submit photographs of themselves for their passport, taken when they were issued the document at age 16, and again at ages 25 and 45. \n\nFormally, passports were not necessary for traveling per se in late Soviet Union. Bus, train, and air tickets were sold without names, and identification documents were not necessary for boarding buses and trains (and only became necessary to board a plane in mid-1970s) except when traveling to/from border-adjacent areas and controlled cities. Nevertheless, passports were necessary for temporary propiska in a number of situations such as checking in a hotel or renting a private dwelling (no marks were placed in the document).\n\nMoreover, in the late 1980s and early 1990s, Soviet internal passports, accompanied with a special leaflet, were valid for traveling to most Comecon countries and Yugoslavia as a member of a touristic group. The leaflet functioned as an equivalent of exit visa stamped in international passports; destination countries did not require entry visas at that time.\n\nIn 1992, passports, or other photo identification documents, became necessary to board a train. Train tickets started to bear passenger names, allegedly as an effort to combat speculative reselling of the tickets.\n\nThe dissolution of the Soviet Union invoked the need to distinguish Russian citizens among the citizens of the former Soviet Union.\n\nOn 9 December 1992, special leaves were introduced which were affixed in Soviet passports, certifying that the bearer of the passport was a citizen of Russia. These leaves were optional unless travelling to the other former Soviet republics which continued to accept Soviet passports; for other occasions, other proofs of citizenship were accepted as well. Issuance of the leaves continued until the end of 2002.\n\nOn 8 July 1997, the current design of the Russian internal passport was introduced. Unlike the Soviet passports, which had three photo pages, the new passports have one. A passport is first issued at the age of 14 and then replaced upon at the ages of 20 and 45. The text in the passports is in Russian. Passports issued in autonomous entities may, on the bearer's request, contain an additional leaf duplicating all data in one of the official local languages.\n\nA passport exchange was begun; the deadline was initially set at end of 2001 but then prolonged several times and finally set at 30 June 2004. The government had first regulated that having failed to exchange one's passport would constitute a punishable violation. However, the Supreme Court ruled to the effect that citizens cannot be obliged to exchange their passports. The Soviet passports ceased to be valid as means of personal identification since mid-2004, but it is still legal (though barely practicable) to have one.\n\nThe \"propiska\" was formally abandoned soon after adoption of the current Constitution in 1993, and replaced with \"residency registration\" which, in principle, was simply notification of one's place of residence.\n\nNevertheless, under the new regulations, permanent registration records are stamped in citizens' internal passports just as were \"propiska\"s. That has led to the widespread misconception that registration was just a new name for the \"propiska\"; many continue to call it a \"propiska\". The misconception is partly reinforced by the fact that the existing rules for registration make it an onerous process, dependent on the consent of landlords, which effectively prevents tenants of flats from registering.\n\nUnlike with the \"propiska\", it is not an offense not to have registration unless one resides in a particular dwelling for more than 90 days. From a practical point of view, the long deadline makes it difficult to prove avoidance of residency registration and so to prosecute. \"De facto\" citizens have no restriction on where they reside (with the exception of closed cities or near borders). Still, many civil rights are dependent on registration, such as the right to vote.\n\nIn November 2010, the Federal Migration Service announced the possible cancellation of internal passports, which, if it were implemented, would be replaced by plastic ID cards or drivers' licenses. In 2013, a plastic ID card, Universal electronic card was introduced, and any citizen had the right to reject it and retain an old-style internal passport. This card system was abandoned on January 2017.\n\nIn Belarus, internal passports and passports for travelling abroad were merged into one kind of document 1991. Passports are the primary means of identification for citizens of Belarus both in homeland and abroad. Belarusian citizens must have a passport after they have reached the age of 14; passports can also be issued to younger children for travelling abroad. Passports are valid for 10 years regardless of age.\n\nApart from visa pages, a considerable number of pages in Belarusian passports are designated for \"internal\" records, such as place of residence and marriage. Citizens had to obtain special stamp enabling the passport bearer to cross the border of the Union State before 2005 when the Constitutional Court ruled the practice not conforming to the Constitution.\n\nCombination of primary identification document with international passport causes significant inconvenience to bearers who cannot certify their identity while their passports are processed for visas in embassies and consulates. A passport can also be easily invalidated by a careless foreign passport control official by placing a stamp in a reserved page.\n\nThe People's Republic of China (PRC) maintains a system of residency registration in mainland China known as \"hukou\", by which government permission is needed to formally change one's place of residence. It is enforced with identity cards. This system effectively controlled internal migration before the 1980s, but subsequent market reforms caused it to collapse as a means of migration control. An estimated 150 to 200 million people are part of the \"blind flow\" and have unofficially migrated, generally from poor, rural areas to wealthy, urban ones. However, unofficial residents are often denied official services such as education and medical care and are sometimes subject to both social and political discrimination.\n\nInternal passports were used in the Confederate States of America. They were also used for freed blacks in the southern slave states before the American Civil War, for example, an authenticated internal passport dated 1815 was presented to Massachusetts citizen George Barker to allow him to freely travel as a free black man to visit relatives in slave states.\n\nCivil liberties campaigners in western democracies have likened some planned counter-terrorism measures as akin to the introduction of an internal passport. Tim Lott, writing in London's \"Evening Standard\" in December 2002, said that the proposed British identity card was a possible precursor to an internal passport.\n\n\n"}
{"id": "47241955", "url": "https://en.wikipedia.org/wiki?curid=47241955", "title": "Interprofessional Guaranteed Minimum Wage", "text": "Interprofessional Guaranteed Minimum Wage\n\nThe Interprofessional guaranteed minimum wage or \"salaire minimum interprofessionnel garanti\" ('SMIG') was the first statutory minimum wage in France, adopted in 1950. A number of former French colonies also have or have previously had a law with that name or a similar name. This article, adapted from the French Wikipedia entry, concerns the SMIG laws of France and Morocco.\n\nFrench sovereignty after the occupations of World War II was restored in 1945 and the French Fourth Republic began on 13 October 1946. The years leading to 1950 were politically contentious but focused on economic reconstruction from the devastation of the war. The centrist coalition governments of the Third Force, positioned between the Communist left and the Gaullist right, adopted the country's first minimum wage law in 1950. However, the groundwork had actually been laid during the wartime Vichy government.\n\nAccording to the Paris-based Higher Institute of Labour (), in its history of minimum wage laws in France (translated from French Wikipedia):\n\nIt was the Charter of Work issued on October 4, 1941 that paved the way. It referred to a 'living minimum wage' and this notion necessarily had a universal character: a subsistence minimum is the same for everyone, whatever the profession. Still, the cost of life is not the same everywhere which was then much more sensitive than today. Therefore, they had divided the country into twenty \"pay zones\", each with a different minimum wage, but all minimum wages proceeded downward from that of Zone 0 (Paris) by a fixed percentage: -2.5% zone, -4% zone, etc., which allowed them to maintain unity while respecting [economic] diversity. This system of zones would not disappear until May 1968: By then there were only two zones.\n\nPaul Bacon (of the Christian Democratic MRP), who was the Minister of Labor from 1950 to 1956 and again in 1957-1959, is considered to be the father of the postwar Interprofessional Guaranteed Minimum Wage (SMIG) law passed in February 1950 under the second government of Georges Bidault, a Third Force Coalition government.\n\nThe value of the minimum wage was set by the High Commission for Collective Agreements, established by a decree on 3 March 1950. They were in charge of assessing the composition of the average household budget, which served to determine the value of the SMIG (i.e. minimum rate).\n\nIn August 1950, the first report of the Commission was presented to the Council of Ministers (the cabinet), which issued a decree that established the first SMIG rate at 64 francs (or 78 in Île-de-France, the Paris region). The decree did not apply at the time to certain parts of France: those overseas departments in French Algeria, Guadeloupe, Martinique, and Réunion. They would have their SMIG rate set later, depending on local conditions. The minimum rate in France also did not apply to agricultural occupations, which received a separate minimum wage law, called the SMAG, later in 1950 (see below).\n\nDefending the minimum wage as it was implemented under his first government, Prime Minister René Pleven (who also led a Third Force coalition) presented it as a means of fighting the expansion of Communism. This was one of the unifying concerns of the Third Force governments of the period. (The French minimum wage law was introduced the year after the Berlin Blockade by the Soviet Union and at the midpoint of the First Indochina War, which France's military was waging against the communist Viet Minh independence movement.)\n\nThe SMIG law was replaced in 1970 by the \"Salaire minimum interprofessionnel de croissance\" (SMIC) and by the \"minimum guarantee.\" The latter is the basis for calculating the allocation of certain social benefits beyond wages. The reason for this change was that the SMIG minimum wage, which was only indexed to prices, increased less rapidly than average wages due to the increase in productivity (which means fewer working hours are needed to produce the same quantity of goods), which President Georges Pompidou considered abnormal. The current minimum wage of France, SMIC, (as of 1 January 2015) is €1,457.52 gross monthly.\n\nToday, the French acronym \"SMIC\" is synonymous with the concept of \"minimum wage,\" and it is dated (as well as incorrect) to use the term SMIG interchangeably with SMIC to mean minimum wage. The similarity between the two acronyms has sometimes been a source of confusion (and is sometimes not distinguished clearly in computerized translations). Even Socialist Party leader François Mitterrand, who would become President of France in a later election, mistakenly used the term SMIG in a 1974 presidential debate with Giscard d'Estaing, who quipped in response that this error proved his opponent was a \"man of the past.\" At the time, the SMIC had only been the law for a few years. Mitterrand had actually been a Cabinet Minister in the Pleven government in 1950, the year of SMIG's introduction.\n\nLater in 1950, in October, a guaranteed minimum agricultural wage (SMAG or \"salaire minimum agricole garanti\") was adopted under the first government of René Pleven, a Third Force Coalition government like the Bidault government, which had passed the SMIG law for other professions earlier in the year.\n\nAt introduction, SMAG was less than the minimum wage, in consideration of factors specific to the existence of rural life of the time (lower housing costs, direct access to food, etc.). However, it was aligned with the regular minimum wage in June 1968, the rural living conditions having been reconciled with urban living conditions.\n\n\nMorocco was a protectorate of France from 1912 until 1956, just a few years after the French government had introduced the SMIG minimum wage in France. French remains a recognized national language of Morocco and French legal influences like the SMIG continue up to the present.\n\nThe \"interprofessional guaranteed minimum wage\" (SMIG) currently refers in Morocco to the country's hourly minimum wage in force now. As its name suggests, the minimum wage applies to all professional bodies with the notable exception of agricultural occupations. As France had done in 1950, Morocco made these jobs subject to a separate wage system, also called the SMAG (for guaranteed minimum agricultural wage).\n\nThe minimum wage is fixed by decree by the government. This is usually done as a result of intense tripartite negotiations between the state, the unions and employers.\n\nThe kingdom's SMIG minimum wage fixed in 2008 at 9.66 Moroccan dirham (DH) per hour, or about 1,800 DH / month (gross). In 2009 after many negotiations between the Moroccan government and the various unions minimum wage stood at 10.64 DH / h or 2,110 DH / month.\n\nOn May 26, 2011, the coalition Moroccan government under conservative Prime Minister Abbas El Fassi decided to increase the SMIG minimum wage by 15% in two tranches for the sectors of industry, of commerce, and services (and SMAG for agriculture). It was to come into effect from 1 July 2011 at the rate of 10% and from 1 July 2012 (5%), according to the Moroccan Minister of Communication and Moroccan government spokesman, Mr. Khalid Naciri, a socialist, after a meeting of the Moroccan Government Council.\n\nThus, the minimum wage in the sectors of industry, trade and services will amount to 11.70 dirhams per hour from 1 July 2011 and then to 12.24 DH / hour from 1 July 2012 . Regarding the agricultural and forestry sector, the minimum daily wage will be at 60.63 DH as of 1 July 2011 and will be of 63.39 DH from 1 July 2012.\n\nSo between 2008 and 2012 the minimum wage increased from 9.66 DH / h to 12.24 DH / h an increase of 26.7%.\n\nIn 2014, the Moroccan government announced the increase of the minimum wage by 5% in July 2014 and 5% in July 2015 to move from 12,85DH / h 13,46DH / h in summer 2015.\n\nThe legal workweek in Morocco is 44 hours per week, which means by the summer of 2015 the Moroccan minimum wage for full-time employment will equal 30,796.48 DH / year on average 2566.37 DH / month (€228.51 / month according to the exchange rate on 29.04.2014).\n"}
{"id": "6811703", "url": "https://en.wikipedia.org/wiki?curid=6811703", "title": "Jameah Islameah School", "text": "Jameah Islameah School\n\nJameah Islameah School was an independent Islamic school in East Sussex. The school was located on a 54 acre site and had residential facilities to house male students aged 11 to 16. The school was independently owned and the proprietor functioned as the principal. In December, 2005, Jameah Islameah was inspected by the Office for Standards in Education which noted that it \"does not provide a satisfactory education for its pupils.\" At the time of the inspection, the school had nine students. According to BBC News the school purported to teach students to become Islamic leaders, training them to the level high enough to teach in local Masjids and Madrassas.\n\nThere had been allegations that the school was used in the training and recruitment of terrorists. According to testimony from Al Qaeda suspects held at Guantanamo Bay, in 1997 and 1998, Abu Hamza and groups of around 30 of his followers held terrorist training camps at the school, including training with AK47 rifles and handguns, as well as a mock rocket launcher. In 2003 or 2004, the grounds of the school were used for an Islamic-themed camping trip, at which Omar Bakri Mohammed lectured. The trip, which was advertised by word-of-mouth, was attended by 50 Muslim men, most of whom were members of al-Muhajiroun. Bakri claimed the activities at the camp included lectures on Islam, football, and paintballing.\n\nOn 1 September 2006 the Jameah Islameah school was searched by up to a hundred police officers as part of their operations, although no arrests were made. The local Sussex Police held a cordon around the site for 24 days in an operation that cost them over one million pounds. Meanwhile the Metropolitan Police searched the buildings and grounds and the lake.\n\nOn the evening of 2 September over 40 police officers entered a south London halal Chinese restaurant called The Bridge to China Town and, after talking to customers for over an hour, arrested twelve on suspicion of the commission, preparation or instigation of acts of terrorism. Two further arrests were made elsewhere in London.\n\nBy 6 September two men had been released.\n\nOn February 9, 2007, the Department for Education and Skills closed down the school, because it \"continues to fail to meet the standards which all independent schools must meet under the Education Act 2002.\" The school had not been operating at the time, due to lack of students. The school's website remained open until July 2014.\n\nThe main Victorian building began life as St Michael's Orphanage. In the 1920s it later became St Joseph's College, a Roman Catholic Junior Seminary until 1970, before being converted into a ballet school. The Legat School of Ballet, formed by Nicholas Legat and his wife Nadine in London moved to the Marks Cross site in the 1970s and became residential. The ground floor of the main building holding academic lessons, the first floor housed two large dance studio and an art studio while the second floor attic was used for dormitories. The annex to the rear housed staff and senior pupils aged over 16. A third dance studio was housed in a wooden hut beside the rear driveway and several prefabricated buildings to the rear were used for academic lessons and dormitories. \n\nThe church building was converted into a theater, and other facilities such as a swimming pool and tennis courts were also provided. Many famous names from the world of ballet were associated with the school, including Eunice Bartell, Pearl Gaden, Anna Lendrum, Hans Meister and Laverne Meyer. The buildings were severely damaged during the storm of 1987 which saw a number of the pre-fabs destroyed, the main building roof lost a number of slates which then damaged other buildings and the roof of the bell tower collapsed. \n\nBy 1990, with falling admissions, the School had begun to struggle financially (despite taking on pupils from the nearby Bush Davies School of Education and Theatre Arts which had gone bankrupt the year before) and was unable to afford the repair bill. Insurance alone was insufficient to foot the bill. Closure came in July 1990 when Legat merged with Wadhurst College and moved to their site at Best Beech Hill, approximately 4 miles to the east, only for that site to close a few years later when a further merger with Bellerbys College occurred.\n\n"}
{"id": "27986874", "url": "https://en.wikipedia.org/wiki?curid=27986874", "title": "KANUKOKA", "text": "KANUKOKA\n\nKANUKOKA (, ) is the national association of Greenland's municipalities, led by Palle Jeremiassen. The name is an acronym formed from the first two letters of each of the constituent words of the organization's name in the Greenlandic language.\n\nThe aim of the organization is to facilitate cooperation between all four municipalities of Greenland: Kujalleq, Qaasuitsup, Qeqqata, and Sermersooq. Based in Nuuk, the organization runs the municipal elections every four years, with the last election taking place in 2008. All municipal authorities in Greenland are currently members of the organization. The association is overseen by the Minister for Social Affairs in the Government of Greenland (). The annual budget of the association is 12.5 million Danish kroner (DKK), with the funds coming directly from municipal budgets.\n\nKANUKOKA was founded on 24 July 1972, at which time there were 18 municipalities and 3 counties. After the administrative reform of 2008 was executed on 1 January 2009, there are four municipalities of Greenland, while the counties were discontinued. Two of the new municipalities—Qaasuitsup and Sermersooq—are the world's largest and second largest municipalities, in that order, with an area of and , respectively.\n\nThe formation of new municipalities brought new challenges to the association, with some of its tasks taken over by the new administrative entities with enlarged prerogatives. In light of this extensive reduction of scope, the association is perceived as an expensive and redundant layer of bureaucracy, hampering the direct cooperation between the municipalities and the government. There is an ongoing discussion over its future, with a possible refocus from an independent organization to one being a vehicle for coordination of growth and development within the municipalities. The proposed changes are part of the ongoing reform of the public sector in Greenland under the new government, in office following the 2009 parliamentary elections. The reform is expected to be completed by 2013.\n"}
{"id": "2294640", "url": "https://en.wikipedia.org/wiki?curid=2294640", "title": "Kontinent", "text": "Kontinent\n\nKontinent was an émigré dissident journal which focused on the politics of the Soviet Union and its satellites. Founded in 1974 by writer Vladimir Maximov, its first editor-in-chief, it was published in German and Russian and later translated into English. A Norwegian edition, \"Kontinent Skandinavia\", was published from 1979 to 1981.\n\nIts Editorial Board included Raymond Aron, George Bailey, Saul Bellow, Józef Czapski, Robert Conquest, Milovan Djilas, Alexander Galich, Jerzy Giedroyc, Gustaw Herling-Grudzinski, Eugène Ionesco, Arthur Koestler, Naum Korzhavin, Mihajlo Mihajlov, Ludek Pachman, Alexander Sakharov, Alexander Schmemann, Zïnaida Schakovskoy, Wolf Siedler, Ignazio Silone, Strannik, and Carl-Gustav Ströhm.\n\nThis initial issue featured a debate between Andrei Sakharov and Aleksandr Solzhenitsyn regarding Solzhenitsyn's \"Letter to the Soviet Leaders\".\n\n\"Kontinent\" continues to be published in English and Russian by Russia House. Currently, the editorial is located at Moscow,\nregistered in the committee on the printed materials of the Russian federation, registration license є 014255. Kontinent follows traditions of scientific journals correcting errors indicated by readers. The Russian version has been available online since 1999.\n\n\n"}
{"id": "12878241", "url": "https://en.wikipedia.org/wiki?curid=12878241", "title": "LGBT rights in Qatar", "text": "LGBT rights in Qatar\n\nLesbian, gay, bisexual, and transgender (LGBT) persons in Qatar face legal challenges not experienced by non-LGBT residents. Homosexuality is illegal in Qatar, and prevailing cultural mores view homosexuality and cross-dressing negatively. The Qatari government does not recognize same-sex marriage or civil partnerships, nor does it allow people in Qatar to campaign for LGBT rights.\n\nIn November 2008 British performer George Michael performed at a successful concert in Qatar, making him the first openly gay musician to perform in Qatar. This didn't reflect a wider change in policy.\n\nSince 2004, Article 296 of the current Penal Code (Law 11/2004) stipulates imprisonment between 1 and 3 years for sodomy between men. This is a slight revision of the original law that stipulated up to five years' imprisonment for male homosexuality.\n\nIn 1998 an American citizen visiting Qatar was sentenced to six months in shabab markeah and 420 lashes for homosexual activity. In the 1990s, Philippine Overseas Employment Administration informed Philippine workers that gay workers were prohibited in Qatar. This was in response to several mass arrests and deportations of Philippine workers in Qatar, for homosexuality.\n\nThe Qatari penal code also contains additional criminal sanctions for adultery, fornication and any speech or conduct that is judged to be disorderly, immoral or likely to offend traditional religious mores.\n\nQatari law concerning marriage, divorce and other family matters are influenced by traditional Islamic morality. Hence, cohabitation is illegal and no legal recognition exists in Qatar for same-sex marriage, civil unions or domestic partnerships. No social services exist in Qatar to help parents accept their LGBT youth.\n\nIn September 2013, it was announced that all Gulf Cooperative Countries had agreed to discuss a proposal to establish some form of, yet unknown, testing in order to ban gay foreigners from entering any of the countries.\n\nQatar's record on LGBT rights became a source of debate again, with gay rights groups criticizing FIFA for choosing to host the event in a country where homosexuality is illegal. Richard de Mos, a member of the Dutch Parliament for the Freedom Party (PVV), has proposed that the Dutch football team play in pink, instead of the country's national colour, orange, to protest the gay rights situation in Qatar.\n\n\n"}
{"id": "2597892", "url": "https://en.wikipedia.org/wiki?curid=2597892", "title": "Latin American debt crisis", "text": "Latin American debt crisis\n\nThe Latin American debt crisis (; ) was a financial crisis that originated in the early 1980s (and for some countries starting in the 1970s), often known as \"La Década Perdida \", when Latin American countries reached a point where their foreign debt exceeded their earning power, and they were not able to repay it.\n\nIn the 1960s and 1970s, many Latin American countries, notably Brazil, Argentina, and Mexico, borrowed huge sums of money from international creditors for industrialization, especially infrastructure programs. These countries had soaring economies at the time, so the creditors were happy to provide loans. Initially, developing countries typically garnered loans through public routes like the World Bank. After 1973, private banks had an influx of funds from oil-rich countries which believed that sovereign debt was a safe investment. Mexico borrowed against future oil revenues with the debt valued in US dollars, so that when the price of oil collapsed, so did the Mexican economy.\n\nBetween 1975 and 1982, Latin American debt to commercial banks increased at a cumulative annual rate of 20.4 percent. This heightened borrowing led Latin America to quadruple its external debt from US$75 billion in 1975 to more than $315 billion in 1983, or 50 percent of the region's gross domestic product (GDP). Debt service (interest payments and the repayment of principal) grew even faster as global interest rates surged, reaching $66 billion in 1982, up from $12 billion in 1975.\n\nWhen the world economy went into recession in the 1970s and 1980s, and oil prices skyrocketed, it created a breaking point for most countries in the region. Developing countries found themselves in a desperate liquidity crunch. Petroleum-exporting countries, flush with cash after the oil price increases of 1973–1980, invested their money with international banks, which \"recycled\" a major portion of the capital as loans to Latin American governments. The sharp increase in oil prices caused many countries to search out more loans to cover the high prices, and even some oil-producing countries took on substantial debt for economic development, hoping that high prices would persist and allow them to pay off their debt.\n\nAs interest rates increased in the United States of America and in Europe in 1979, debt payments also increased, making it harder for borrowing countries to pay back their debts. Deterioration in the exchange rate with the US dollar meant that Latin American governments ended up owing tremendous quantities of their national currencies, as well as losing purchasing power. The contraction of world trade in 1981 caused the prices of primary resources (Latin America's largest export) to fall.\n\nWhile the dangerous accumulation of foreign debt occurred over a number of years, the debt crisis began when the international capital markets became aware that Latin America would not be able to pay back its loans. This occurred in August 1982 when Mexico's Finance Minister, Jesús Silva-Herzog, declared that Mexico would no longer be able to service its debt. Mexico stated that it could not meet its payment due-dates, and announced unilaterally a moratorium of 90 days; it also requested a renegotiation of payment periods and new loans in order to fulfill its prior obligations.\n\nIn the wake of Mexico's sovereign default, most commercial banks reduced significantly or halted new lending to Latin America. As much of Latin America's loans were short-term, a crisis ensued when their refinancing was refused. Billions of dollars of loans that previously would have been refinanced, were now due immediately.\n\nThe banks had to somehow restructure the debts to avoid financial panic; this usually involved new loans with very strict conditions, as well as the requirement that the debtor countries accept the intervention of the International Monetary Fund (IMF). There were several stages of strategies to slow and end the crisis. The IMF moved to restructure the payments and reduce government spending in debtor countries. Later it and the World Bank encouraged opened markets. Finally, the US and the IMF pushed for debt relief, recognizing that countries would not be able to pay back in full the large sums they owed.\n\nHowever, some unorthodox economists like Stephen Kanitz attribute the debt crisis not to the high level of indebtedness nor to the disorganization of the continent's economy. They say that the cause of the crisis was leverage limits such as U.S. government banking regulations which forbid its banks from lending over ten times the amount of their capital, a regulation that, when the inflation eroded their lending limits, forced them to cut the access of underdeveloped countries to international savings.\n\nThe debt crisis of 1982 was the most serious of Latin America's history. Incomes and imports dropped; economic growth stagnated; unemployment rose to high levels; and inflation reduced the buying power of the middle classes. In fact, in the ten years after 1980, real wages in urban areas actually dropped between 20 and 40 percent. Additionally, investment that might have been used to address social issues and poverty was instead being used to pay the debt.\n\nIn response to the crisis, most nations abandoned their import substitution industrialization (ISI) models of economy and adopted an export-oriented industrialization strategy, usually the neoliberal strategy encouraged by the IMF, although there were exceptions such as Chile and Costa Rica, which adopted reformist strategies. A massive process of capital outflow, particularly to the United States, served to depreciate the exchange rates, thereby raising the real interest rate. Real GDP growth rate for the region was only 2.3 percent between 1980 and 1985, but in per capita terms Latin America experienced negative growth of almost 9 percent. Between 1982 and 1985, Latin America paid back US$108 billion.\n\nBefore the crisis, Latin American countries like Brazil and Mexico borrowed money to enhance economic stability and reduce the poverty rate. However, as their inability to pay back their foreign debts became apparent, loans ceased, stopping the flow of resources previously available for the innovations and improvements of the previous few years. This rendered several half-finished projects useless, contributing to infrastructure problems in the affected countries.\n\nDuring the international recession of the 1970s, many major nations and countries attempted to slow down and stop inflation in their countries by raising the interest rates of the money that they loaned, causing Latin America's already enormous debt to increase further. In between the years of 1970 to 1980, Latin America's debt levels increased by more than one-thousand percent.\n\nThe crisis caused the per capita income to drop and also increased poverty as the gap between the wealthy and poor increased dramatically. Due to the plummeting employment rate, children and young adults were forced into the drug trade, prostitution and terrorism. The low employment rate also worsened many problems like homicides and crime and made the affected countries undesirable places to live. Frantically trying to solve these problems, debtor countries felt pressured to constantly pay back the money that they owed, which made it hard to rebuild an economy already in ruins.\n\nLatin American countries, unable to pay their debts, turned to the IMF (International Monetary Fund), which provided money for loans and unpaid debts. In return, the IMF forced Latin America to make reforms that would favor free-market capitalism, further aggravating inequalities and poverty conditions. The IMF also forced Latin America to implement austerity plans and programs that lowered total spending in an effort to recover from the debt crisis. This reduction in government spending further deteriorated social fractures in the economy and halted industrialisation efforts. The efforts of the IMF effectively aimed to transform Latin America's economy abruptly into a capitalist free-trade type of economy, which is an economic model preferred by wealthy and fully developed countries.\n\nLatin America's growth rate fell dramatically due to government austerity plans that restricted further spending. Living standards also fell alongside the growth rate, which caused intense anger from the people towards the IMF, a symbol of \"outsider\" power over Latin America. Government leaders and officials were ridiculed and some even discharged due to involvement and defending of the IMF. In the late 1980s, Brazilian officials planned a debt negotiation meeting where they decided to \"never again sign agreements with the IMF\". The result of IMF intervention caused greater financial deepening (Financialization) and dependence on the developed world capital flows, as well as increased exposure to international volatility. The application of structural adjustment programs entailed high social costs in terms of rising unemployment and underemployment, falling real wages and incomes, and increased poverty.\n\nThe following is a list of external debt for Latin America based on a 2015 report by The World Factbook.\n\n\n\n"}
{"id": "39476776", "url": "https://en.wikipedia.org/wiki?curid=39476776", "title": "List of Canadian socialist parties", "text": "List of Canadian socialist parties\n\nThe nation of Canada has seen an array of socialist political parties over the years since 1896, including organisations which are federal and provincial in scope. These have run the gamut from reformist social democratic to anti-reformist impossibilist electorally-oriented organisations to revolutionary socialist and communist groups. A list of these parties follows, listed chronologically by their date of establishment.\n\n\n\n\n\n\n"}
{"id": "860190", "url": "https://en.wikipedia.org/wiki?curid=860190", "title": "List of animal rights groups", "text": "List of animal rights groups\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "240450", "url": "https://en.wikipedia.org/wiki?curid=240450", "title": "MUSCULAR (surveillance program)", "text": "MUSCULAR (surveillance program)\n\nMUSCULAR (DS-200B), located in the United Kingdom, is the name of a surveillance program jointly operated by Britain's Government Communications Headquarters (GCHQ) and the U.S. National Security Agency (NSA) that was revealed by documents released by Edward Snowden and interviews with knowledgeable officials. GCHQ is the primary operator of the program. GCHQ and the NSA have secretly broken into the main communications links that connect the data centers of Yahoo! and Google. Substantive information about the program was made public at the end of October 2013.\n\nThe programme is jointly run by:\n\nMUSCULAR is one of at least four other similar programs that rely on a trusted 2nd party, programs which together are known as WINDSTOP. In a 30-day period from December 2012 to January 2013, MUSCULAR was responsible for collecting 181 million records. It was however dwarfed by another WINDSTOP program known (insofar) only by its code DS-300 and codename INCENSER, which collected over 14 billion records in the same period.\n\nAccording to the leaked document the NSA's acquisitions directorate sends millions of records every day from internal Yahoo! and Google networks to data warehouses at the agency's headquarters at Fort Meade, Maryland. The program operates via an access point known as DS-200B, which is outside the United States, and it relies on an unnamed telecommunications operator to provide secret access for the NSA and the GCHQ.\n\nAccording to the \"Washington Post\", the MUSCULAR program collects more than twice as many data points (\"selectors\" in NSA jargon) compared to the better known PRISM. Unlike PRISM, the MUSCULAR program requires no (FISA or other type of) warrants.\n\nBecause of the huge amount of data involved, MUSCULAR has presented a special challenge to NSA's Special Source Operations. For example, when Yahoo! decided to migrate a large amount of mailboxes between its data centers, the NSA's PINWALE database (their primary analytical database for the Internet) was quickly overwhelmed with the data coming from MUSCULAR.\n\nClosely related programmes are called INCENSER and TURMOIL. TURMOIL, belonging to the NSA, is a system for processing the data collected from MUSCULAR.\n\nAccording to a post-it style note from the presentation, the exploitation relied on the fact that (at the time at least) data was transmitted unencrypted inside Google's private cloud, with \"Google Front End Servers\" stripping and respectively adding back SSL from/to external connections. According to the \"Washington Post\": \"Two engineers with close ties to Google exploded in profanity when they saw the drawing.\" After the information about MUSCULAR was published by the press, Google announced that it was working on deploying encrypted communication between its datacenters.\n\nIn early November 2013, Google announced that it was encrypting traffic between its data centers. In mid-November, Yahoo! announced similar plans.\n\nIn December 2013, Microsoft announced similar plans and used the expression \"advanced persistent threat\" in their press release (signed-off by their top legal representative), which the press immediately interpreted as comparison of the NSA with the Chinese government-sponsored hackers.\n\n\n"}
{"id": "467579", "url": "https://en.wikipedia.org/wiki?curid=467579", "title": "Mansion House, London", "text": "Mansion House, London\n\nMansion House is the official residence of the Lord Mayor of London. It is a Grade I listed building.\n\nIt is used for some of the City of London's official functions, including two annual white tie dinners. At the Easter banquet, the main speaker is the Foreign Secretary, who then receives a reply from the Dean of the Diplomatic Corps, i.e. the longest-serving ambassador. In early June, it is the turn of the Chancellor of the Exchequer to give his \"Mansion House Speech\" about the state of the British economy.\n\nMansion House was built between 1739 and 1752, in the then fashionable Palladian style by the surveyor and architect George Dance the Elder. The site, at the east end of Poultry, had previously been occupied by the Stocks Market, which by the time of its closure was mostly used for the sale of herbs. The construction was prompted by a wish to put an end to the inconvenient practice of lodging the Lord Mayor in one of the City Halls. Dance won a competition over designs solicited from James Gibbs and Giacomo Leoni, and uninvited submissions by Batty Langley and Isaac Ware. Construction was slowed by the discovery of springs on the site, which meant piles had to be sunk to form the foundations.\n\nThe original building had two clerestory roof extensions, nicknamed the \"Mayor's Nest\" (a pun on \"mare's nest\") and \"Noah's Ark\". In 1795 George Dance the Younger re-roofed the central courtyard, and had the \"Noah's Ark\" demolished. In the same year, the original grand staircase was removed to make way for a further two rooms. In 1835 the entrance steps were reduced to one flight, and in 1842 the \"Mayor's Nest\" was demolished after the ballroom was reconstructed. The Lord Mayor's private entrance in Walbrook was created in 1845, and in 1849 the former Swordbearer's Room was converted into the Justice Room, effectively the Magistrates' Court of the City, until 1999 when the court removed to a building on the opposite side of Walbrook.\n\nMansion House was paid for in an unusual way: the City authorities, all Church of England men, found a way to tax those of other Christian denominations, particularly the Rational Dissenters. A Unitarian named Samuel Sharpe, banker by day and amateur Egyptologist by night, wrote about it in the 1830s, striking a blow against the Test and Corporate Acts. The article was republished in 1872. Sharpe argues that Mansion House \"remains as a monument of the unjust manner in which Dissenters were treated in the last century\" (i.e. the 18th, in contrast to his own 19th, century). William Edward Hartpole Lecky in his \"History of England during the Eighteenth Century\" (1878) describes the funding of the construction of Mansion House as \"a very scandalous form of persecution\". \n\nThere are over one hundred livery companies, the senior members of which form a special electorate known as Common Hall. In 1748 the City of London Corporation devised a Catch-22 situation to raise money, passing a by-law levying a heavy fine on any man who refused to stand for election, or who, once elected to office, refused to serve. In order to serve as a Sheriff of the City of London, the individual had to have \"taken the sacrament according to the Anglican rite\" within the past year. This was exactly what English Dissenters could not, in conscience, do. \"It would appear almost incredible, if the facts were not widely attested, that under these circumstances the City of London systematically elected wealthy Dissenters to the office in order that they should be objected to and fined, and that in this manner it extorted no less than £15,000. The electors appointed these Dissenters with a clear knowledge that they would not serve, and with the sole purpose of extorting money. One of those whom they selected was blind; another was bedridden.\" Some tried to appeal, but the process was immensely risky and costly, with the City holding all the cards. Eventually a man named Evans began a challenge which lasted ten years; in 1767, the House of Lords, drawing on the Toleration Act 1688, agreed with Lord Mansfield and ruled to curtail the City's abuse of power. (See \"Harrison\" v. \"Evans\" (1767), 3 Bro. Parl. Cas. 465.) In order to avoid civil disabilities such as this financially ruinous persecution, some Dissenters were known to take Communion in their parish church once a year; in the phraseology of the time, \"occasional conformity\". Thomas Abney rose to be Lord Mayor in this fashion.\n\nThe American author Mark Twain recounts the story in his own inimitable style in \"A Connecticut Yankee in King Arthur's Court\" (1889):\n\nIt reminded me of something I had read in my youth about the ingenious way in which the aldermen of London raised the money that built the Mansion House. A person who had not taken the Sacrament according to the Anglican rite could not stand as a candidate for sheriff of London. Thus Dissenters were ineligible; they could not run if asked, they could not serve if elected. The aldermen, who without any question were Yankees in disguise, hit upon this neat device: they passed a by-law imposing a fine of £400 upon any one who should refuse to be a candidate for sheriff, and a fine of £600 upon any person who, after being elected sheriff, refused to serve. Then they went to work and elected a lot of Dissenters, one after another, and kept it up until they had collected £15,000 in fines; and there stands the stately Mansion House to this day, to keep the blushing citizen in mind of a long past and lamented day when a band of Yankees slipped into London and played games of the sort that has given their race a unique and shady reputation among all truly good and holy peoples that be in the earth.\n\nMansion House has three main storeys over a rusticated basement. The entrance facade has a portico with six Corinthian columns, supporting a pediment with a tympanum sculpture by Sir Robert Taylor, in the centre of which is a symbolic figure of the City of London trampling on her enemies. The building originally had two prominent and unusual attic structures at either end, which were removed in 1794 and 1843. The building is on a confined site. Sir John Summerson wrote that \"it leaves an impression of uneasily constricted bulk\", adding that \"on the whole, the building is a striking reminder that good taste was not a universal attribute in the eighteenth century\". The main reception room, the columned \"Egyptian Hall\", was so named because Dance used an arrangement of columns deemed to be \"Egyptian\" by Vitruvius. No Egyptian motifs were employed. It has twenty niches for sculpture. There was originally an open courtyard, later occupied by a saloon.\n\nThe residence used to have its own court of law, since the Lord Mayor is the chief magistrate of the City while in office. There were eleven holding cells (ten for men and one, nicknamed \"the birdcage\", for women). A famous prisoner here was the early 20th-century suffragette women's rights campaigner Sylvia Pankhurst.\n\nMansion House is home to The Harold Samuel Collection of Dutch and Flemish Seventeenth Century Paintings, described as \"the finest collection of such works to be formed in Britain this century\" (Sutton 1992). It consists of 84 paintings and includes some outstanding works by artists including Hendrick Avercamp, Gerard Ter Borch, Pieter Claesz, Aelbert Cuyp, Frans Hals, Pieter de Hooch, Jacob van Ruisdael, Jan Steen, David Teniers the Younger and Willem van de Velde. Mansion House also houses a plate collection, which includes among other treasures, the five ceremonial City of London swords.\n\nMansion House is not generally open to the public. However, tours can be arranged through the diary office, and there are public tours most Tuesdays.\n\nMansion House Street is the short street at the front of Mansion House, which connects Poultry, Queen Victoria Street and Bank junction above Bank Underground station.\n\nGuildhall is another venue used for important City functions.\n\n\n"}
{"id": "42466265", "url": "https://en.wikipedia.org/wiki?curid=42466265", "title": "Mimicking operations", "text": "Mimicking operations\n\nMimicking operations is a tactical concept, developed under Terrorist Tactics, Techniques, and Procedures, to explain a form of deception, commonly used by terrorists in their attacks. The concept is commonly used in military tactical modelling and scientific simulation; and is connected to the idea of shielding friendly forces (as well as foes) from detection and deception.\n\nMimicking operations is a – “cost-effective way of achieving a desired operational effect using superior deception tactics to exploit concealment and camouflage opportunities”.\n\nCarlo Kopp identified in 2002 that 'deception and Mimicry is the insertion of intentionally misleading information. It amounts to mimicking a known signal so well, that a receiver cannot distinguish the phony signal from the real signal.'\n\nThe mimicking operations concept was acknowledged as an early iteration of contemporary Fifth Dimension Operations.\n\nThe concept of mimicking operations simulates activity that resembles a bacterial attack on a large and complex organism. Attacking cells mimic the behavior of their victims, but remain dormant while awaiting an opportunity to launch an attack. The development of mimicking operations has been influenced by biology, ecology and the workings of the natural world.\n\nIn military tactics, the employment of mimicking operational strategies, is based on concepts:\n\nMimicking Operations is a subset of military deception. From a logistics perspective the capacity to develop a deception strategy, reflects the relative capacity of the combatant to collect the resources to develop suitable deception. In the case of terrorism, an individual with few resources is likely to adapt a mimicking strategy, and seek to camouflage themselves. Whereas a more resourced team of terrorists with the backing\nof an organisation may be more likely to afford the resources to develop a decoy, as part of a developed deception strategy.\n"}
{"id": "39177184", "url": "https://en.wikipedia.org/wiki?curid=39177184", "title": "Ministry of Defence (Bangladesh)", "text": "Ministry of Defence (Bangladesh)\n\nThe Ministry of Defence (MoD) () is a Bangladeshi government ministry. The MoD is headed by the Minister of Defence, a civilian and member of cabinet; the post is usually held by the Prime Minister of Bangladesh, who also serves as the President's second-in-command of the military. The MoD exercises supreme command authority over the Bangladesh Armed Forces.\n\nParliament is constitutionally responsible for working with the president and the service chiefs in ensuring the nation's defence. In practice, however, members of Parliament have never played a significant role in either national defence planning or defence budgeting, the Defence Secretary does.\n\nThe list of agencies and departments under the Ministry of Defence of Bangladesh are below:\n\n\n\n\nThe functions of the MoD are:\n"}
{"id": "1402669", "url": "https://en.wikipedia.org/wiki?curid=1402669", "title": "Movement for the Actualization of the Sovereign State of Biafra", "text": "Movement for the Actualization of the Sovereign State of Biafra\n\nThe Movement for the Actualization of the Sovereign State of Biafra (MASSOB) is a secessionist movement in Nigeria, associated with Igbo nationalism, which supports the recreation of an independent state of Biafra. It is led by an Indian-trained lawyer Ralph Uwazuruike, with headquarters in Okwe, in the Okigwe district of Imo State.\n\nMASSOB's leaders say it is a peaceful group and advertise a 25-stage plan to achieve its goal peacefully. There are two arms to the government, the Biafra Government in Exile and Biafra Shadow Government.\n\nThe Nigerian government accuses MASSOB of violence; MASSOB's leader, Ralph Uwazuruike, was arrested in 2005 and detained on treason charges; he was released in 2007. MASSOB also championed the release of oil militant Mujahid Dokubo-Asari, who faced similar charges at the time. In 2009, MASSOB launched \"the Biafran International Passport\" in response to persistent demand by Biafrans in the diaspora.\n\nMASSOB agitates for a Republic of Biafra comprising the South-East and South-South regions of Nigeria; though Uwazuruike has stated in interviews that the Niger Deltans \"can have their own republic.\" The group's philosophy is hinged on the principle of non-violence as propagated by Mahatma Gandhi.\n\nAt its inception, MASSOB concentrated on organizing rallies and peaceful protests which culminated in hoisting Biafran flags at different locations in the South East. In recent years, this practice has been reserved mainly for celebrating key dates and events or in commemoration of dead members.\n\nMASSOB members embark on protests to protest arrests and killings of its members. In one of these protests, the house of the late Nigerian leader, Nnamdi Azikiwe, was torched. The then Secretary-General of Ohanaeze Ndigbo, Col. Joe Achuzie (rtd) exonerated MASSOB from blame and accused the security agents of \"carelessness and irresponsibility.\"\n\nIn 2005, MASSOB re-introduced the old Biafran currency into circulation. This sparked a lot of excitement at the time especially as one Biafran pound was said to exchange for two hundred and seventy naira at the border communities of Togo and the Republic of Benin. In his reaction, the then President of Nigeria, Olusegun Obasanjo, likened the Biafran pounds to a collector's item and attributed its high exchange value to its relative rarity.\n\nMASSOB launched the Biafran passport in 2009 as part of the program to celebrate its 10th anniversary. MASSOB leader, Ralph Uwazuruike, said the introduction of the Biafran passport was in response to persistent demands from Biafrans in diaspora.\n\nSince its inception, MASSOB has continually alleged mass arrests and killings of its members by government forces. According to the group's sanitation grassroot information spokesperson, Kelechi A Chukwu, the government forces allegedly carries out secret executions of MASSOB members in detention centres and prisons nationwide. In May 2008, the group released a list of 2,020 members alleged to have been killed by security agents since 1999. MASSOB leader, Ralph Uwazuruike, has been arrested on several occasions and charged with treason. In 2011, Uwazuruike and 280 MASSOB members were arrested in Enugu while attending a function in honour of Ojukwu. Few days later, President Goodluck Jonathan ordered Uwazuruike's release as well as all other MASSOB members in detention.\n\nIn June 2012, the Human Rights Writers' Association of Nigeria condemned the alleged killing of 16 members of MASSOB by security agencies in Anambra.\n\nIn February 2013, MASSOB claimed that several corpses found floating in the Ezu River on the boundary of Enugu and Anambra States were those of its members previously arrested by the police. The group claimed that the police routinely executed MASSOB members without proper trial.\n\nOn September 13, 2015 police in Anambra state arrested no fewer than 25 MOSSAB members who were marking their 16th anniversary; one MASSOB member was shot. At St Charles Lwanga Catholic Church Okpoko,18 members were arrested and one shot and at Iba Pope Catholic Church, while at Awada, 11 members of MASSOB were arrested. At Awka, two MASSOB were arrested by the police according to the MASSOB former Deputy Director of Information, Mazi Chris Mocha\n\nOn May 31, 2013, President Goodluck Jonathan, a Niger Deltan Ijaw from Bayelsa State and from the South-South geopolitical zone, branded MASSOB to be one of three extremist groups threatening the security of Nigeria. Jonathan declared that “the Nigerian state faces three fundamental security challenges posed by extremist groups like Boko Haram in the North; the Movement for the Actualisation of the Sovereign State of Biafra in the South-East; and the Oodua People’s Congress in the South-West.\"\n\n\n"}
{"id": "38964604", "url": "https://en.wikipedia.org/wiki?curid=38964604", "title": "National Council for the Revolutionary Command", "text": "National Council for the Revolutionary Command\n\nThe National Council for the Revolutionary Command (NCRC) is the twenty-man council set up to rule Syria after the 1963 Syrian coup d'état.\n\nThe NCRC was composed of 12 Ba'athists and eight Nasserists and Independents. Its exact membership was initially secret for some months. Though some civilians were admitted to the council, it was dominated by military officers.\n"}
{"id": "40417489", "url": "https://en.wikipedia.org/wiki?curid=40417489", "title": "Palestinian Media Watch", "text": "Palestinian Media Watch\n\nPalestinian Media Watch (PMW; ) is an Israel-based nongovernmental organization and media watchdog group. Founded in 1996 by Itamar Marcus, Palestinian Media Watch documents cases of incitement in Palestinian media. It describes itself as \"an Israeli research institute that studies Palestinian society from a broad range of perspectives by monitoring and analyzing the Palestinian Authority through its media and schoolbooks.\"\n\nAfter a period during which Palestinian television was airing music videos promoting martyrdom and encouraging violence against U.S. troops in Iraq, in 2003 Itamar Marcus testified in front of a U.S. Senate committee hearing to discuss the phenomenon. Marcus characterized one of the videos shown at the hearing as \"Palestinians...glorifying the killing of American soldiers\" and calling on Iraqis \"to kill American soldiers.\" American-Arab Anti-Discrimination Committee communications director Hussein Ibish agreed that another video encouraged Palestinian children to commit suicide but said that only 3% of Palestinians watch Palestinian television and that a seven-minute peace song was also being aired regularly.\n\nIn 2007, Palestinian Media Watch presented the U.K. Parliament Select Committee on the European Union with evidence pertaining to Palestinian schoolbooks it said reject Israel's right to exist, glorify terrorism, and inculcate youngsters with hatred for Israel.\n\nA report Palestinian Media Watch presented to the United States House Committee on Foreign Affairs in 2008 indicated that the Palestinian Authority was engaging with enemies of the United States on a shared platform of hatred toward the U.S. The report argued that under such circumstances the creation of an independent Palestinian state would contribute to the undermining U.S. efforts toward world peace.\n\nIn 2011 Palestinian Media Watch presented U.S. congressmen with a report indicating that more than of U.S. funding for the Palestinian Authority was being used to pay salaries to Palestinian prisoners jailed in Israel on terrorism-related charges. The report also cited instances of the Palestinian Authority glorifying the perpetrators of terrorist attacks against Israelis.\n\nIn 2012, \"Haaretz\" reported that as a consequence of Israeli intelligence agencies having reduced their own real-time monitoring of mainstream Arab-language media, Palestinian Media Watch and the Washington, D.C.-based Middle East Media Research Institute provide the Israeli government with coverage of anti-Israel incitement in the Palestinian media. The Prime Minister's Bureau has stated that prior to the government citing information furnished by the two organizations, the source of the material and its credibility are verified.\n\nInternational media regularly cite and quote PMW translations as accurate representations of Palestinian Arabic-language media. News organizations that cite PMW as a source include: \"Chicago Sun-Times\" the Associated Press, \"The Telegraph\" and \"The Washington Post\".\n\nIn 2007 widespread criticism was generated around a children's television program, \"Tomorrow's Pioneers\", aired from Hamas-run studios in the Gaza Strip. The program employed a Mickey Mouse-inspired figure in order to instill anti-American and anti-Israeli sentiment and promote a message of Islamic dominion among children. Palestinian Media Watch, together with the Middle East Media Research Institute (MEMRI), were the first to raise concerns in relation to the show.\n\nIn 2008 Palestinian Media Watch generated outrage again when it circulated a recording of \"Tomorrow's Pioneers\" in which a Bugs Bunny-like figure proclaimed, \"I will eat the Jews.\"\n\nA Palestinian Media Watch report from 2012 stated that a puppet show performed in front of Palestinian children and sponsored by the United Nations-funded organization Burj al Luq Luq Community Centre and Society called on children to take up arms in lieu of cigarettes. A separate report from the same year stated that two television hosts from the Palestinian Youth Association for Leadership and Rights Activation (PYALARA), an organization that receives funding from the European Commission Humanitarian Aid Office, Save the Children U.K. and Cordaid, described suicide bombers as \"the greatest role models for us\" during a weekly TV program. The Burj al Luq Luq website was subsequently taken offline, and Save the Children U.K. launched an investigation into PYALARA.\n\nIn early 2013 a report by Palestinian Media Watch indicating that the Palestinian Authority glorified acts of terrorism against the State of Israel sparked debate in Norway over the latter's contribution to the PA of some annually. \"NRK\", the Norwegian public broadcasting network, used the PMW report in its coverage and stated that according to the Norwegian Center for Studies of Holocaust and Religious Minorities a Palestinian television channel, partly controlled by the Palestinian Authority, occasionally conveys hatred and demonization towards Israel. \"NRK\" reported that programs broadcast on Palestinian television praise prisoners involved in killing Israeli civilians. \"NRK\" also conducted a survey of its own in Jerusalem and the West Bank, finding among other things that several Palestinians believed the \"Protocols of the Elders of Zion\" forgery is authentic and that it reflects \"Jewish plans\" for world domination. The Norwegian Holocaust Center, which also contributed money to the Palestinian Authority, said PA TV demonized Israel and spread antisemitism.\n\nEmad Alsafar, the program director of the Palestinian television channel, denied the charges of demonization and antisemitism stating that Palestinians took issues with the occupation, not with Jews. He said that in live broadcasts undesirable views are occasionally expressed, but there is an effort to correct these in retrospect. Norwegian State Secretary Torgeir Larsen admitted that Palestinian Media Watch had presented examples of antisemitism but stated that these were examples and that PMW had their own agenda in presenting the material. In relation to an additional \"NRK\" report on claims made by PMW that the Palestinian Authority pays monthly salaries to Palestinians convicted of terrorist activities including murder, the Norwegian Foreign Ministry felt it important to state that PMW is on the right wing of Israeli politics and that its founder, Itamar Marcus, resides in an illegal settlement in the West Bank. Norwegian MP Peter Gitmark subsequently called on his parliament's scrutiny committee to investigate the matter of what he called Norway's \"indirect\" contribution to Palestinian terrorism.\n\nIn 2013, an Israeli family sued Palestinian officials for responsibility for \"incitement against Israel which led to “terrorist” attacks against Israel and Israelis\" in a Tel Aviv district court. Itamar Marcus was called as \"expert witness\", but Judge Dalia Gannot found Marcus “incompetent” as such. The Media Watch further argued that “the PA follows an explicit policy of incitement against Israel and the Jews,” but Judge Gannot found that to be unproven. The case against the Palestinian officials was dismissed.\n\nIn December 2010 YouTube removed Palestinian Media Watch's channel from its servers. The channel, which featured videos of Palestinian incitement against Jews and Israelis, was shut down for airing hate speech. Itamar Marcus stated that the move was triggered by a video showing a Hamas terrorist urging Palestinians to \"drink the blood of Jews.\" The channel was reactivated later in the month after supporters expressed outrage over its removal.\n\nAccording to a 2012 article in \"Haaretz\", Palestinian Media Watch and its founder are associated with Israel's right wing, and the majority of information furnished by PMW relating to Palestinian incitement is of a professional nature. \n\nIn an opinion piece for \"The Hill\", Hanan Ashrawi of the Palestine Liberation Organization accused Palestinian Media Watch of having close links with the Central Fund of Israel, which she said was involved in funding \"some of the most extreme elements in Israel's settler movement.\" Ashrawi stated also that the PMW definition of incitement included \"any action or statement critical of Israeli policy\" such as nonviolent actions against Israeli occupation or in support of Palestinian rights. In an article for \"CounterPunch\", Samah Sabawi described PMW as a right-wing propaganda site.\n\nAustralia's former ambassador to Israel Ian Wilcock, in an opinion piece for \"The Australian\", said of Palestinian Media Watch that it \"does an outstanding job of bringing attention to what the Palestinian Authority and related organisations are saying in Arabic, as opposed to the usually more temperate comments made in English for the world outside the Middle East.\" As examples Wilcock stated that \"The Protocols of the Elders of Zion\" was prominent in the ideology of the Palestinian Authority, that a Palestinian Authority youth magazine had featured a young woman dreaming of explaining to Adolf Hitler why she killed Jews, and that a Palestinian Authority newspaper had referred to Passover as \"the holiday of the apes.\"\n\nIn February 2015, PMW director was awarded the Abramowitz Israeli Prize for Media Criticism. The \"Abramowitz Israeli Prize for Media Criticism\" was funded by Mrs. Nira and Mr. Kenneth Abramowitz. Mr. Kenneth Abramowitz is a New York businessman, and a donor to Israel’s Media Watch.\n\n\n"}
{"id": "40370306", "url": "https://en.wikipedia.org/wiki?curid=40370306", "title": "Party whip (Australia)", "text": "Party whip (Australia)\n\nIn the Parliament of Australia, the political parties appoint party whips to ensure party discipline, help manage legislative business and carry out a variety of other functions on behalf of the party leadership. The most important function of a government party whip is to ensure that a sufficient number of members and senators are present to take part in votes in the chamber, to maintain a parliamentary quorum and to prevent censure motions succeeding. Their roles in the chamber include taking divisions, and arranging pairs which affects the ability of members and senators to leave parliament during sittings, as well as the entitlement to be absent during divisions.\n\nUnlike in the United Kingdom, Australian whips do not hold official office, but they are recognised for parliamentary purposes. In practice, Australian whips play a lesser role than their counterparts in the United Kingdom, as party discipline in Australia tends to be tighter. The role of the whip becomes more critical the lower the majority the government has in the lower house of Parliament.\n\nLiberal Party whips are appointed by the leader of the party, while Australian Labor Party whips are elected by the Caucus. For Labor and the Liberals, the chief whip is assisted by two deputy whips.\n\nSimilar arrangements exist in the six state and the two self-governing territory parliaments.\n\nWhips are essential to the day-to-day running of the house. They assist the party business managers (the Leader of the House, Manager of Opposition Business in the House, Manager of Government Business in the Senate, and Manager of Opposition Business in the Senate and their four deputies) arrange the order of business on the floor. They also draw up lists of speakers in debates, which (though not binding) assist the occupant of the chair in deciding whom to call on. The whips play the primary role in managing business in the parallel debating chamber (the Federation Chamber). The Government Chief Whip has the same power as ministers and parliamentary secretaries to move business motions. This right was extended with the creation of the parallel chamber (then called the Main Committee) to facilitate movement of business between it and the floor of the House of Representatives. The right can also be exercised by another whip acting on the Chief Whip's behalf.\n\nWhips also play a central role in voting. During each vote (or \"division\"), whips ensure that their fellow party members are present and seated on the correct side of the house while votes are counted. Acting as tellers, the whips also count the votes. In a typical vote in the House of Representatives, where the Government and Opposition are on opposite sides, each will provide two tellers. One Government whip and one Opposition whip will count the votes in favour of the motion, and the other two will count the votes against. In the Senate, each side provides one whip.\n\nWhips have been a part of the Federal Parliament since its beginning with much the same function as today. Early in its first session in 1901, each of the three parliamentary parties elected one whip in the House of Representatives. Labor and the Free Trade Party each appointed a whip in the Senate as well. The Protectionist Party, which formed the Government, did not appoint a whip because it did not require the confidence of the Senate to survive, though a whip was subsequently appointed for a brief period.\n\nWhile references to assistant or deputy whips in the Australian Parliament sometimes appeared in early press reporting, just when the concept of a deputy whip emerged, or how formal the position initially was, is unclear. For example, Alfred Conroy was described in a news report in 1903 as Labor's assistant whip, and when the Labor Whip Jim Page died in 1921, press reports stated that \"the Deputy Whip\", James Fenton, would act as whip until a new whip was elected. There is, however, no indication that he was elected to that position or that any Labor deputy whip was elected until Labor Whip Gil Duthie successfully lobbied for the official creation of the role in the 1960s.\n\nThe appointment of a second official whip in the House of Representatives appears to have begun with the Commonwealth Liberal Party, as its appointment of \"joint Ministerial whips\" in 1913 drew comment. Those appointments survived the party's fall from government and merger with National Labor to form the Nationalist Party. The practice continued until 1923, when the Nationalists and Country Party formed a coalition, at which point each party contributed one whip. Unlike later practice, the senior whip (also sometimes called \"Chief Government Whip\") was the one who had served longer as whip. By the 1950s, the Country Party whip was always junior.\n\nIt is unclear whether the practice continued when the parties went into opposition in 1929, but when the United Australia Party was formed from the Nationalists and some breakaway Labor MPs, a member of the latter group became deputy to James Bayley, who had been the Commonwealth Liberal whip. When the UAP and Country Party formed a new coalition following the 1931 election, they resumed the practice of each party appointing one of two Government whips.\n\nThe Liberal Party, formed in 1945, appointed Reginald Swartz as its first deputy whip in 1950, after the party had won government for the first time. This second Liberal whip was the \"Assistant Government Whip\", as opposed to the Country Party's whip, who was the \"Deputy Government Whip\". The Labor Party elected its first official deputy whip in the early 1960s.\n\nThe first deputy whip in the Senate was Annabelle Rankin in 1950. She had served as the Liberal Party's whip during the three-year period beginning 1 July 1947, when there were two Liberals senators and one from the Country Party, In a somewhat controversial move, but she was replaced as whip by Reg Wright and made deputy when senators elected as part of the expansion of the Senate took their seats in February 1950. The next year, following the double dissolution and subsequent election, Wright stood down as whip, and Rankin was elected unopposed to succeed him. The post of deputy was left unfilled.\n\nLabor elected its first deputy Senate whip in 1962. The Liberals followed suit in November 1974 Labor added a second deputy in August 1985, pointing out that its whipping operation would then match the Coalitions (which had a Liberal whip and deputy and a National Party whip). Liberal added a second whip in 2009, while in opposition.\n\nThe National Country Party, which as the Country Party had elected a whip in the House of Representatives since its early days, first elected a Senate whip in 1973 and a deputy whip for the House of Representatives in 1976. The latter, Peter Fisher MP, was subsequently elected National Country Whip in 1980, and it does not appear another deputy was elected until 1989, when the practice became standard. As of August 2013, the party does not elect a deputy whip in the Senate.\n\nBy 1994, all three major parties in the House of Representatives had a whip and a deputy whip, but only the National Party's deputy whip did not receive an additional salary. On 12 May 1994, Kim Beazley, then the Leader of the House, announced that, in conjunction with the creation of the second debating chamber, the Government Whip in the House of Representatives would become the Chief Government Whip, and his deputy would become the Government Whip. In addition, a second junior whip would be added. Beazley offered the same arrangement for the Liberal Party and to see that the Nationals' deputy received a salary. Labor elected its whip that day, and the Liberals appointed an additional whip on 2 June 1994, with the new whip responsible for business in the second chamber.\n\nThat arrangement persists today. Labor and the Liberals each have a chief whip and two whips in the House of Representatives and a chief whip and a two deputy whips in the Senate. The Nationals have a chief whip and whip in the House and one whip in the Senate. In addition, the Greens have had a whip in the Senate since May 1995. Other parties have also had whips. When the Democratic Labor Party (now the Democratic \"Labour\" Party) increased from two to four senators in July 1968, it elected a whip who served until he and the rest of the DLP senators lost their seats at the 1974 double dissolution election. The Australian Democrats had a whip from 1981 until 2008, when all four senators left office following their loss at the 2007 election.\n\nUntil 1952, whips received no salary apart from their pay as a parliamentarian. The Government whip, however, received payment from a pool each Cabinet minister paid into. The \"Parliamentary Allowances Act 1952\" provided salaries for the whips of each recognised party in both the Senate and House of Representatives. Although the parties began appointing deputy whips in the 1950s and 1960s, they were not provided a salary until 1973.\n\nWhen the Remuneration Tribunal was created, also in 1973, it was given control over parliamentary salaries and additional salaries for parliamentary officeholders, it retained salaries for the existing whips. In 1994, with the creation of the Main Committee (now the Federation Chamber), the Government effectively changed the names of the Government and Opposition Whips to \"Chief Whip\" and added one paid post of whip for each of the parties in the House of Representatives.\n\nCurrently, there are 14 paid whips in Parliament: Labor and the Liberals each have five paid whips, three in the House and two in the Senate; the Nationals have two whips in the House of Representatives and one in the Senate, and the Greens have a Senate whip. The Government and Opposition chief whips in the House of Representatives earn an additional salary equal to 26% and 23%, respectively, of their base pay as MPs. This is roughly equal to an assistant minister (until 2015 called a parliamentary secretary) (25%). Their counterparts in the Senate earn slightly less: 20% for the Government Chief Whip and 18% for the Opposition Chief Whip. Additional salaries for other whips range from 3% to 13% of base pay.\n\n"}
{"id": "4895367", "url": "https://en.wikipedia.org/wiki?curid=4895367", "title": "Philanthropinism", "text": "Philanthropinism\n\nPhilanthropinism (also philanthropism) comes from the Greek φιλος (friend) and ανθροπος (human). It was an educational reform movement in the Age of Enlightenment in the German-speaking area, established in the second half of the eighteenth century. The programme aimed at educating a new man and at the same time at reforming society. The philanthropinists’ ideas of teaching children to become philanthropic, natural and rational beings are partly derived from the theories of childhood and education proposed by John Locke (1632-1704) and Jean-Jacques Rousseau (1712-1778), among others.\n\nPhilanthropinism was a movement that founded by the German educator Johann Bernhard Basedow (1723-1790) in light of the Enlightenment in Germany in the last quarter of the eighteenth century. In 1774, Basedow published his \"\", the first in a row of efforts by philanthropinist educators \"to theorize and implement an educational plan that integrated social goals\" which were directed at improving the well-being of people and of society as a whole. Basedow’s theorisation as well as earlier treatises on education stemmed from a desire of Enlightenment thinkers to transmit their political, social and moral ideals—which could be traditional but also innovative—to the next generation. Basedow’s ideas were partly based on John Locke and Jean-Jacques Rousseau’s theories of education and childhood, of which he picked those elements which seemed most attractive, and his ideas on national education were influenced by La Chalotais. Despite differences, Basedow agreed with both Locke and Rousseau on the need of a child-appropriate education that would take into account a child’s mental capacities.\nIn order to implement his philanthropinist ideas, Basedow wrote a booklet called \"Das Noethigste aus der Vorstellung an Menschenfreunde\" (Introduction of what is Important to Friends of Humanity (Philanthropists)) wherein her lays out his plans for a new series of illustrated books (later to become his Elementarwerk) and the founding of a school to utilize his education methods. Basedow distributed this booklet to his many wealthy friends and contacts to solicit funding for his new ideas. He gathered substantial financial help from many contributors (Philanthropists) most notably the Empress of Russia, the King of Denmark, the Prince of Dessau and the Prince of Basel. In 1774, The Prince of Dessau was so impressed by Basedow's ideas that he provided a section of his Dessau Palace to hold the school. Basedow called the \"Philanthropinum, a school for Philanthropists, Learners and Teachers\". Basedow directed the school and taught there as well. The basic principles of his education system are: \n1) Everything taught must be taught according to the laws of Nature. \n2) The formation of character is more valuable than the acquisition of knowledge. \n3) One gains knowledge best through sense perceptions.\n4) The teacher must ensure that the student is happy while learning, otherwise the student's ability to learn will be reduced.\n\nRobert Sumser summarises Basedow’s emphasis as being on a \"cosmopolitan, nonconfessional, and pragmatic system of education [which] proved inspirational to a younger generation of pedagogues.\" One of Basedow's fundamental concepts was that education occurs when the students are taught as much as possible in real life and application of the knowledge was highly important. Classes were often held outside or with the use of pictures to make the subject as real as possible. The subjects taught at the Philanthropinum were Mathematics, Languages: French, German and Latin, Eloquence, All religions, Craftwork, Sciences, Sports: Fencing and Gymnastics. Among those pedagogues were German educators such as Joachim Heinrich Campe (1746-1818), (1745-1818), Christian Gotthilf Salzmann (1744- 811) but later also the Austrian Vincenz Eduard Milde (1777-1853). Their educational ideas are based on a notion that a human being is not born fully formed but is essentially formable, a notion inspired by Locke’s concept of the \"tabula rasa\". In particular the philanthropinists' goals included education to bliss, to a way of thinking as proposed by Immanuel Kant, to human dignity and morality. Another aspect dealt with the balance between individual perfection and an individual’s utility for civil society, which both needed to be attained without one hazarding the other.\n\nPhilanthropinism was a comparatively short-lived movement (late eighteenth to early nineteenth century) but influential in the later development of pedagogy, for instance by introducing nonconfessional religious education or physical education. Moreover, philanthropinist writers helped develop the modern genre of children’s literature with their writings for children and \"freed [the German children’s literature] from dependence on the great French and English models.\"\n\n"}
{"id": "456013", "url": "https://en.wikipedia.org/wiki?curid=456013", "title": "Press secretary", "text": "Press secretary\n\nA press secretary or press officer is a senior advisor who provides advice on how to deal with the news media and, using news management techniques, helps his or her employer to maintain a positive public image and avoid negative media coverage. \n\nThey often, but not always, act as the organization's senior spokesperson. Many governments also have deputy press secretaries. A deputy press secretary is typically a mid-level political staffer who assists the press secretary and communications director with aspects of public outreach. They often write the press releases and media advisories for review by the press secretary and communications director. There are usually assistant press secretaries and press officers that support the press secretary. Press secretaries give also declarations to the media when a particular event happens or an issue arises inside an organization. They are expected, therefore, to have in-depth knowledge about the institution or organization they represent, and to be able to explain and answer questions about the organization's policies, views upon a particular issue and its official standpoint on problematic questions.\n\n"}
{"id": "4695868", "url": "https://en.wikipedia.org/wiki?curid=4695868", "title": "Reformasi (Malaysia)", "text": "Reformasi (Malaysia)\n\nThe Reformasi was a protest movement that began in September 1998 throughout Malaysia initiated by Anwar Ibrahim after his sacking as Deputy Prime Minister by the country's then-Prime Minister, Mahathir Mohamad in the same month. The massive movement, which was occurred while the country hosted the Commonwealth Games, consisted of civil disobedience, demonstrations, sit-ins, rioting, occupations and online activism, involving thousands across Malaysia protesting against the Barisan Nasional government under the Mahathir Cabinet.\n\nThe movement started off as a political campaign calling for the resignation of Mahathir, Malaysia's longest serving Prime Minister and the end of corruption and cronyism allegedly associated with the government of the day.\n\nBuilding on the momentum of the Reformasi, a political movement called the Social Justice Movement (Malay: Pergerakan Keadilan Sosial) (Adil) was formed and led by Wan Azizah Wan Ismail, Anwar's wife. However, after facing difficulties in registering Adil as a political party, the reformasi movement took over a small party Ikatan Masyarakat Islam Malaysia and led to the formation of a new multiracial-based party named Parti Keadilan Nasional (National Justice Party) in 1999. On 3 August 2003, the newly merged entity (Parti Keadilan Nasional and Parti Rakyat Malaysia) was officially launched and assumed its current name, Parti Keadilan Rakyat. Although the Reformasi movement gradually ended after the 1999 general election, the Reformasi is still being used as slogan until now. Other similar slogans like \"Lawan Tetap Lawan (Fight On)\", \"Ubah\" (Change), and, \"Ini Kalilah (This Is It)\" just became popular in the 2010s.\n\nA hegemonic political compact among key ethnically based parties, partners in the ruling BN has shaped postcolonial politics in Malaysia.\n\nAgainst a backdrop of decades of percolating discontent, especially among the urban middle class, several triggers escalated calls for change.\n\nIn the midst of the 1997 Asian financial crisis, the Malaysian ringgit dropped to half of its former value. Kuala Lumpur Stock Exchange have plunged, property markets buckled, bad loans surged, and the government struggled to find the political will and expertise to set things right again.\n\nThe difficulty in doing so, particularly without recourse to the International Monetary Fund (IMF) – the then Prime Minister, Tun Dr. Mahathir forbade such an abnegation of sovereignty – which have led to debates on proper policy approaches and an increased awareness of the country's vulnerability to outside economic forces.\n\nMahathir preferred more innovative measures to stabilize the currency and cushion the economy from further speculative measures. On the other hand, Anwar Ibrahim, the deputy prime minister and minister of finance then, advocated IMF-style, free market-oriented corrective measures.\n\nEventually the economy recovered but Anwar continued his fight against Mahathir causing the creation of a political crisis.\n\nThe movement borrowed their idiom from the campaign in the neighboring country of Indonesia against President Suharto earlier that year, which protested against the thirty-some years of Suharto era in pursuit of \"Reformasi.\", which successfully ended with his resignation on 21 May. Before his arrest on 20 September, Anwar travelled across the country, giving huge crowds public lectures on justice, the prevalence of cronyism and corruption, the urgency for social safety nets and so on. These groups controlled an expansive grassroots network and were able to garner tens of thousands of mostly Malay youths to support Anwar's cause and his calls for Reformasi.\n\nOpposition parties such as the Democratic Action Party (DAP) and Pan-Malaysian Islamic Party (PAS) also extended their support. After leading a huge rally in Kuala Lumpur on 20 September 1998, amid the Commonwealth Games and UK Queen Elizabeth II's visit to Kuala Lumpur, Anwar was finally arrested and detained under the Internal Security Act (Malaysia). A number of his followers were also held under the ISA, and hundreds of demonstrators were eventually charged with illegal assembly and related offences.\n\nDemonstrations intensified by the surrounding actions against Anwar - his arrest at gunpoint, assault by the chief of police, widely publicized sexual allegations against him. and his highly controversial court cases. Thousands took to the streets in protest when Anwar was sentenced to six years jail for corruption (abuse of power) in April 1999. Police repression was again harsh and 118 people were arrested. Police dispersed protesters with the use of tear gas, chemically-laced water and bludgeons, and publicised photographs and lists in the mainstream press of people wanted for interrogation. Apart from direct confrontation in the streets, the government and the opposition maintained an acrimonious campaign against each other, in the mainstream and the alternative media respectively. The latter included the bilingual (English and Malay) PAS newspaper, Harakah, published twice a week, smaller weekly and monthly publications such as Eksklusif, Detik and Tamadun, and several sites on the Internet.\n\nIn early September 1998, Anwar Ibrahim, who was the Deputy Prime Minister, was unceremoniously removed from his positions in government and UMNO. He was fired for sexual misconduct. Even before charges were laid, Mahathir explained in graphic detail – repeated in stunningly explicit lead articles and banner headlines in the press – that his deputy was guilty of adultery, sodomy, and was trying to cover up evidences of his trysts.\n\nHowever, Anwar was not immediately detained. For 18 days he toured the country, giving extremely well attended public lectures on justice, the purported evils of Mahathirism, the prevalence of cronyism and corruption, the needs for social safety nets and the urgency to reform.\n\nAnwar averred that he had been pressing for change from within,and stressed on his role in developing low-cost housing and people-friendly policies while in government. He was largely supported by Islamic NGOs and a wide array of other groups and organizations. Islamist Groups in particular command an immense grassroots network. They were able to rouse tens of thousands of mostly Malay youths to espouse Anwar's cause and his calls for Reformasi. Opposition parties, such as DAP and PAS, have also proclaimed their support.\n\nAfter leading an enormous rally in Kuala Lumpur on 20 September 1998, Anwar was finally arrested, together with a number of his followers. In addition, hundreds of demonstrators were eventually charged with illegal assembly and related offenses. Anwar was initially held under the ISA before other charges were specified. Opposition to the ISA became a central issue to Reformasi movement.\nNine days after his arrest, Anwar appeared in court with serious head and neck injuries. As the Royal Commission of Inquiry concluded in March 1999, Anwar had been beaten in the custody by Rahim Noor, then inspector general of police, and had then been confined under ISA to conceal his injuries.\n\nIn August 8, 2000, Anwar and his adoptive brother, Sukma Dermawan Sasmitaat Madja, were both found guilty. Anwar was sentenced to a nine-year term, to run consecutively with his previous sentence, and Sukma to six years and four strokes of the cane. The unfairness and sensationalism of both trials were widely criticized by domestic and external observers.\n\nOnce Anwar had been detained, the Reformasi movement continued to develop, though \"Justice for Anwar\" remained a potent rallying call. Before his arrest,Anwar had designated his wife,Wan Azizah Wan Ismail as the successor of the movement. \nWan Azizah developed an enormous following,attracting thousands to her emotional but rather banal speeches. For a time,these followers held massive weekend street demonstrations, mostly in Kuala Lumpur but also occasionally in Penang and other cities, for \"keadilan\" (justice) and against Mahathir.\n\nReformasi protesters demanded protection for civil liberties and repeal of the ISA. They decried constraints on the media and the judiciary and lambasted what was called KNN (korupsi, kolusi dan nepotisme) (corruption, cronyism and nepotism). Others also called for Islamization as the solution to the perceived moral decay of the government and society. Shouts of Allabu Akhbar (God is great) and takbir (a call to praise God) peppered demonstrations, many of which took place around mosques and at prayer times. The demonstrations were met with increasingly harsh crackdowns by the police.\n\nThese street protests had largely tapered out by mid-November, but they resumed at key moments, such as the announcement and anniversaries of the verdicts on Anwar's cases.\n\nThe Reformasi movement linked a wide array of protesters who had divergent aims and concerns under a commodious if amorphous umbrella. Reformasi constituted a broad based popular movement for social, political and economic change. It was represented by the Barisan Alternatif (BA) coalition a precursor to Pakatan Rakyat, itself a precursor to the current ruling Pakatan Harapan government, and in civil society by conglomerations of NGOs, trade unions, and other activists cooperating across sectors and issue areas. The groundswell of opposition to Mahathir, to BN, and specific government laws and policies conveyed in opposition oriented media (especially on the internet) was also tied to the Reformasi movement.\n\nThe movement and its leaders were forced to specify precisely what they meant by \"reform\" and to articulate concrete, pragmatic objectives once the political crisis matured. Particularly since elections approached, the aims of reformasi had to be somehow encapsulated in a broadly appealing electoral platform.\n\nOver the course of events, the Reformasi movement attracted an unprecedented range of active and passive supprters. Most Malaysians seemed to agree that at least some degree of change is warranted. For instance, 85 percent of respondents to a mid 1999 survey agreed that Malaysia needed a reassessment in politics, economy and society.\n\nAmong the more comprehend documents detailing specific reforms were two that originated with Chinese activists: the \"People Are The Boss\" declaration and that of the Malaysian Chinese Organizations' Election Appeals Committee (also known as SuQiu), both of them promulgated in mid-August 1999. The online \"People Are The Boss\" campaign was started by an informal group of ethnic-Chinese journalists as a noncommunal citizens' awareness campaign.\n\nThe list of signatories included not only Chinese Malaysians but also a number of Malays and Indians. The project's \"Declaration on the People's Awareness\" explains that government is appointed by and empowered by the people; the people thus have the right and responsibility to monitor their \"employees\" and hold them accountable.\n\nSuqiu proved more controversial. At the forefront was the Suqiu Committee, a lobbying and monitoring group composed of 13 national-level Chinese guilds and associations. In addition, over 2,000 Chinese organizations nationwide endorsed a list of 17 core demands that was drafted as a wish list, submitted to all political parties, and accepted at least in principle by the BN's Malaysian Chinese Association (MCA) and Gerakan, the SUPP, and the Alternative Front.\n\nSome of the demands, such as those for promoting democracy, human rights, justice, women's rights, and national unity, are nonethnic. Others are considered communal, such as demanding for modernizing New Villages and encouraging more egalitarian multiculturalism.\n\nThe MCA presented the demands to the cabinet, but UMNO condemned them as inappropriately communal and threatening. Fiery debate on Suqiu extended well beyond the elections. UMNO swore that it would uphold the position of Malays, and UMNO Youth staged an aggressive demonstration outside the Selangor Chinese Assembly Hall in August 2000.\n\nOthers were more supportive of the initiative, particularly those from the BA and civil society, a cohort that included a multiracial array of students and youth organizations.\n\nSuqiu's defenders urged that the debate not be rendered so racially incendiary, advising that affirmative action policies be made more need-based, pointing out that the constitution guarantees Malays a special position (kedudukan istimewa) and not special rights (hak istimewa), and asserting that Islam denies any racial group specific privileges. Eventually, under intense pressure, Suqiu backed down in January 2001.\n\nAnwar's arrest prompted commentators as diverse as Amnesty International, Indonesian President B.J. Habibie, George Soros and former U.S. Vice-President Al Gore who showed signs of disapproval at Malaysia's political and economic failures. At the 1998 APEC Summit in Kuala Lumpur, Al Gore, gave a speech supporting Anwar and the Reformasi movement in front of the Prime Minister of Malaysia and other Asia-Pacific premiers.\n\n\"Democracy confers a stamp of legitimacy that reforms must have in order to be effective.\" He went on: \"And so, among nations suffering economic crises, we continue to hear calls for democracy, calls for reform, in many languages - People Power, Doi Moi, Reformasi. We hear them today - right here, right now - among the brave people of Malaysia.\"\n\nIn September 1998, Mahathir experienced his first international snub when the Islamic Society of North America (ISNA) withdrew its invitation to the Prime Minister to be a keynote speaker at one of its meetings. Mahathir's invitation by a group of pro-government students based at Cambridge University to a workshop in October also sparked a controversy when a rival group calling itself the Cambridge Coalition for a Free Malaysia reacted to the invitation by calling for a boycott of the planned event. The event was not cancelled, but Mahathir was met by protestors, mainly Malaysian students and members of the Cambridge University Amnesty International group.\n\nThe Reformasi movement attracted a wide range of previously disparate groups. About 25 Malay non-governmental organizations like the Islamic Youth Movement of Malaysia (ABIM) and the Malaysian Islamic Reform Society joined PAS in forming GERAK (Malaysian People's Movement for Justice).\n\nPredominantly non-Malay non-governmental organisations like SUARAM (Malaysian People's Voice) joined the DAP and the small but venerable Malaysian People's Party in forming GAGASAN (Coalition for People's Democracy). Further, as anticipation mounted that elections would be called, some forty non-governmental organizations involved with GERAK and GAGASAN organised Pemantau (Malaysian Citizens' Election Watch).\n\nAs a political commentator, former Deputy Prime Minister Musa Hitam remarked that \"If the reformasi movement and demonstrations could be given any significance in terms of Malaysian politics..it is more issue-based than racial. I'm fascinated.\" Musa commented that prior to Reformasi, \"any demonstration of any nature in Kuala Lumpur or Penang would always turn racial. Even if they were against the government, they would burn the Chinese shops.\"\n\nThe Reformasi movement united a disparate array of organizations from both civil society and political society. Among the groups involved were the political parties, advocacy-oriented NGOs, religious organizatiopns, trade unions, and professional associations. These groups were joined by organized and unorganized students, individual activists and alternative media.\n\nGroups motivated by different underlying philosophies and representing different constituencies have learned, through practice of advocacy in the face of an increasingly consolidated and strong regime, to trust and work with each other.\n\nTo agitate for Reformasi, two main coalitions were launched in September 1998: Gagasan Demokrasi Rakyat (Coalition for People's Democracy, Gagasan), and Majlis Gerakan Keadilan Rakyat Malaysia (Malaysian People's Movement for Justice, Gerak). The two coalitions included an overlapping range of organizations, including the DAP, PAS and PRM. Gerak, however, included more Islamist groups and maintained a more Islamist orientation than did NGO-led Gagasan. No real attempt was made to segregate political parties and NGOs in these bodies, although, both of their comparative advantages were acknowledged.\n\nNGOs in particular are often concerned not to align themselves too closely with political parties. Under the circumstances, all involved seemed to realize that broad-based cooperation on common aims was the best strategy available. Both coalitions focused on human rights and good governance, including repeal of unjust laws, expunging corruption, press freedom, judicial independence and social justice, although Anwar and the ISA dominated Gerak's Agenda.\n\nReformasi occurred due to the differing leadership styles between Mahathir and Anwar. Anwar claimed that he had significantly changed the government from within and stressed that his role in developing low-cost housing and other people-friendly policies while in government, caused him to become a focus for popular frustrations with the ruling party. Before entering UMNO, Anwar had been a student activist, then headed Angkatan Belia Islam Malaysia (ABIM) in the 1970s, making strident calls for Islamicisation and Malay-language education. Detained for two years under the ISA, Anwar was brought into Mahathir's government six years later, in 1982. Expelled from Mahathir's camp, Anwar was warmly reclaimed by ABIM and other Islamic NGOs.\n\nMonths before Anwar's arrest, Mahathir had been granting more responsibility for economic policy-making to Daim Zainuddin, a financial strongman, thus limiting Anwar's power. Mahathir accused Anwar for being a \"puppet\" of foreign powers and institutions such as the International Monetary Fund (IMF), out to re-colonize Malaysia and the arrest was partly due to Anwar's economic mismanagement. Mahathir claimed that Anwar and his supporters were guilty of corruption and cronyism and that he had led the country to the brink of economic disaster by following the wishes of the IMF. Anwar was a \"liar and an agitator, detained for this in 1974, and now returning to his old ways. Above all else, disregarding court warnings on the issue.\"\n\nDespite Mahathir's clear enmity against the IMF, Anwar stated that the government \"have an excellent rapport with the IMF officials and that they did say that Malaysia did not need IMF's rescue\". Despite Mahathir's use of state's funds to bail out several prominent conglomerates, Anwar defended his opposition to government's bailouts and lack of transparency.\n\nDuring the Asian Financial Crisis of 1997, UMNO party leaders accused Mahathir of mismanaging the economic crisis. A concert of attacks followed, including a claim made by a Time magazine article that Mahathir has funnelled a $250 million loan to his son through political party connections. Overt attempts by some in the ruling elite to protect ailing beneficiaries and \"clients\" from the full thrust of market forces in the wake of the financial crisis like the above largely contributed to the friction between Mahathir and Anwar, with the latter commonly believed to have been less willing to yield to the government's financial bail-outs of these cronies.\n\nAs more of the business conglomerates created by symbiotic relationships between government and business elites started to face financial ruin, political ramifications inevitably spread to the highest levels of the ruling United Malays National Organization (UMNO)/Barisan Nasional coalition. When Anwar continued to resist some of these attempts at rescuing politically-linked businesses, he was politically neutralized by first being arrested and then charged with corruption and sexual misconduct.\n\nThe main reasons for Reformasi do not just revolve around the episodes of the sacking and arrest of Anwar Ibrahim in 1998. For many years, UMNO have had several contentions within its party ranks and Reformasi is often viewed as a manifestation of UMNO's factionalism.\n\nIs Reformasi a typical rather than an exceptional political event in Malaysia? Reformasi took place from the overt split within the UMNO leadership in 1998. Malaysians have witnessed such splits prior to 1998. For many people the events that preceded 1998 might be just another reminder of what had happened about two decades ago: a struggle for power between elements of the governing elite over who will get to lead the nation and shape it.\n\nThe first crisis, which arose in 1975 during Tun Abdul Razak's term as Prime Minister, the communist card was used by some to persecute and intimidate rivals in the party. This early period pitted a group of \"young Turks\" including Mahathir in wanting to cause rapid social change against an \"old guard\" of disparate forces of feudal nationalists and individuals whose fortunes were tied to the institutional vestiges of the \"old system\". However, UMNO survived this crisis because the winning faction created a new role for itself as the guardians of Malay development through new social and economic affirmative action policies in the form of The New Economic Policy (NEP).\n\nIn 1987, UMNO split in half after Tengku Razaleigh Hamzah challenged Dr Mahathir for the leadership of UMNO, and failed by only 43 votes from the 1479 delegates. The result forced Tengku Razaleigh to leave UMNO and set up his own Semangat '46 party, which formed an electoral collaboration with PAS and the largely Chinese DAP to compete in the 1990 elections. After this attempt failed, Semangat members rejoined UMNO in 1996, including Tengku Razeleigh. Many expect the current opposition to meet a similar fate.\n\nTo Funston, it was no doubt an event that has impacted the Malays greatly. The majority Malay community was strongly divided by these events, particularly in the Klang Valley and the northern states. The pro-Anwar group garnered huge support from Malay youths and Muslim groups. Tensions were most visible in urban areas, but in the villages individuals also began boycotting the shops and even mosques of opposing groups. This had happened before, particularly in the 1960s but what was revolutionary about it was that it was never on such a massive scale. Many government employees, particularly teachers and military personnel, supported the opposition. Government leaders warned these officers not to challenge the government, and threatened disciplinary action against them. Non-Malays were not as involved as the Malays, but participated through non-governmental organisations (NGOs), or the DAP. Their grassroots organisations issued several substantial memoranda, including the widely publicized \"Suqiu\" or the \"Seventeen Points\".\nMeredith Weiss responds in the same light by noting that even though there are both Islamic-oriented NGOs (IONGOs) and secular issue-oriented NGOs, including human rights, women's rights and other advocacy organizations, who are active in the Reformasi movement, these collaborations between these sectors tend to remain at a rather superficial level. The underlying motivation for the IONGOs is religion and their emphasis is on moral accountability and often pro-Malay policies.\n\nThe other advocacy groups hinges on specific, non-ethnic issues, phrased usually in universal terms. In addition, the membership of IONGOs is almost exclusively Malay Muslim, with some degree of gender segregation, and most communications are in the Malay language. The advocacy groups are mostly Chinese and Indian in leadership and membership, are more gender-neutral and operate mostly in English. All support Keadilan (justice), but with varying rationales, so that when members of the different kinds of NGOs co-operate, it is often in their alternate roles as party or electoral coalition workers.\n\nOn the other hand, according to Weiss, the long-term impact of Reformasi could be significant. Current manifestations indicate a change in Malay political culture away from blind loyalty and clientelism and towards more critical engagement with political processes, the development of an opposition coalition with a chance of upsetting BN dominance and hence ushering in a more liberal form of parliamentary democracy, and a shift towards a multiracial collaboration in which communally-defined issues are less significant. Meeting these goals demands that the majority of voters accept new, issue rather than race-oriented norms of political interaction, a process which could take quite a long time.\n\nReformasi led to the formation of a new multiracial-based party named Parti Keadilan Nasional (National Justice Party). In 1999, a general election was held. The new Parti Keadilan Nasional, Parti Islam Se-Malaysia, and Democratic Action Party formed a Barisan Alternatif (Alternative Front), in a combined initiative to replace the standing Barisan Nasional (BN) coalition government. For the first time in Malaysia's history, UMNO, a Malay-based party and the dominant party in the BN coalition, received less than half of the total vote of ethnic Malays.\n\nWhat most distinguished the pro-justice agitation of Malaysia in the late 1990s from the prototypical mobilization of civil society-based pressure groups or prior coalition-building ventures of Malaysian political parties was the depth of the interaction between political parties.\n\nReformasi was never merely confined to NGOs and other nonpartisan organizations. Opposition political parties were energetically engaged from the outset,first individually and then in such coalitions as Gagasan and Gerak.\n\nAs elections drew near, the fulcrum of the movement shifted from nongovernmental to electoral sphere. \nReformasi's transition from social movement to electoral campaign began with the NGO Adil, which was superseded as of April 1999 by the political party Keadilan.\n\nThe launch of Keadilan puts to rest months of speculation about whether Wan Azizah and Anwar would merely remain in Adil,join PAS, or try to stage a coup against UMNO. Although Keadilan was multiracial, its primary target was middle-class, middle of the road Malays, particularly from UMNO.\n\nIn June 1999, PAS, Keadilan, DAP and Parti Rakyat Malaysia(PRM) announced their plans to contest as Barisan Alternatif(BA). BA was endorsed by parties \nin Sabah and Sarawak and by the unregistered Parti Sosialis Malaysia(PSM).\n\nThe principal opposition leaders formulated a list of the ten common issues in June 1999: constitutional monarchy, parliamentary democracy, human rights, rules of law, independence of the judiciary, ciizens' rights and responsibilities, Islam as official religion and freedom of religion, Malay as National language while retaining the rights for other languages, Bumiputera special position and Federalism.\nMany of the same people were involved at all stages of the movement, regardless of their affiliation, be it established political party or civil society.\n\nThese activists came both from advocacy groups, such as Aliran, Suaram, All Women's Action Society(AWAM), and from mass Islamic organizations, such as ABIM and JIM.\n\nOn 2 July 1999, before the elections, four opposition parties, PAS, DAP, Keadilan and the socialist, mainly Malay, Parti Rakyat Malaysia (PRM) declared a common programme of action emphasising on the ten principles drawn from the Malaysian constitution. These stressed constitutional principles based on democracy and the special position of Malays, the latter to reassure Malays that co-operation with the DAP would not involve prioritising DAP's interests. References to establish an Islamic state were not made because PAS agreed to drop this in the interests of opposition unity.\n\nThe coordination among the opposition parties have made the 1999 elections one of the most contested ever. For the first time, the ruling Barisan Nasional (BN) faced a coalition of the major opposition parties, campaigning on a common 'reform' platform. Even though BN won with 148 out of 193 seats, the elections still proved a major defeat for UMNO which lost 22 seats. Its parliamentary seats declined from 94 to 72. For the first time ever, UMNO seats were less than the total of its coalition partners. 4 of its ministers and 5 deputy ministers were defeated. One of the major reasons was the Malays' reaction against the government's handling of the Anwar issue. They shifted their support to the opposition.\n\nParti Islam SeMalaysia (PAS) and Keadilan (led by Anwar's wife) were the main beneficiaries. PAS, advantaging from its affiliation to reformasi, emerged as the new parliamentary opposition leader, and headed state governments in Kelantan and Terengganu.\n\nBy the time Abdullah Badawi took over in October 2003, the excitement generated by the formation of the Barisan Alternatif and its performance in the November 1999 election had started to fade. Since the DAP left the coalition in September 2001, Keadilan itself has been experiencing an acrimonious internal power struggle and looks in danger of going down the Semangat '46 (Spirit of '46) path. The root of the BA's dilemma was evidently that its component parties could not reconcile their different agendas. On matters concerning the \"Ketuanan Melayu\" and ethnic quotas, the DAP could not see eye to eye with PAS, Keadilan, and the PRM. With regard to PAS and its overriding focus on the creation of an Islamic state, it has opened a chasm that the two parties, the DAP and PAS, could not reconcile.\n\nThe opposition fall-out changed the fortunes for BN in the 2004 election. For instance, Parti Keadilan Nasional lost all of its seats in Parliament but one, which was held by its President, Wan Azizah, wife of Anwar Ibrahim. The BN coalition captured 198 out of 219 seats in Parliament on the way to its most convincing electoral performance since 1974. The Barisan Nasional performance in Northern Malaysia was particularly impressive. The Barisan Nasional's sweeping victory was also attributed to high expectations of the new Prime Minister Abdullah Badawi, who succeeded Mahathir in 2003.\n\nAlso, during the 2004 elections, the role of civil society slid quietly to the peripheries of Malaysian politics, marginalized once again by the state as well as by other political interests that intended to focus the epic struggle between UMNO and PAS as the centerpiece of the elections. Indeed, civil society movements, so proactive and politicised just five years ago with the growth of the Reformasi movement, were conspicuously absent in 2004 due to the lack of functioning space and state domination over society.\n\nHowever, Anwar Ibrahim was released from prison in September 2004 and Parti Keadilan Nasional re-emerged as Parti Keadilan Rakyat (PKR) or People's Justice Party. Even though Anwar was barred from participating in politics, he managed to become PKR's de facto leader. William Case says that both the \"Anwar factor\" and the PKR's multi-racial platform injected excitement in Malaysia's political life. In May 2007, Anwar stated that his purpose was to actively reinstate the multi-racial political coalition of PKR, DAP and PAS. His influence caused PAS to open its membership to non-Muslims in 2006 and Anwar's call to end the thirty-six-year-old New Economic Policy caught the attention and support of the non-Malays.\n\nThe excitement that Anwar caused in Malaysian politics reinvigorated the spirit of the Reformasi movement. It returned during Malaysia's 2008 general election, and contributed to the People's Justice Party's (PKR) win of 31 parliamentary seats. As a result of the electoral success of the PKR, PAS and DAP coalition, the Barisan Nasional government lost its two-thirds majority in Parliament.\n\nPKR made huge gains in the 2008 general election and became the largest opposition party in parliament. In addition, five of the eleven state governments in peninsular Malaysia fell to the PKR, Parti Islam Se-Malaysia, and Democratic Action Party coalition. The Barisan Nasional government, for the first time since 1969, lost its two-thirds majority in Parliament.\n\nAccording to O'Shannassy, the elections of 2008 represent a significant change as the opposition gains could be seen as a robust public endorsement of their multiracial aspirations.\n\nThe Reformasi movement represents a departure from past Malaysian experience in terms of its aims, scopeand institutions. The movement urged Malaysians not just to vote differently but to actually think differently about politics.\n\nIn encouraging true multiracial, Reformasi leaders advocated adoption of a unified political culture, in which all subsets of citizens in a Bangsa Malaysia Malaysian Nation will pursue a broader and nonexclusive goal.\n\nCrucial to the progress of the movement is the scope of its support. More Malays, including middle-class Malays and those associated with Islamic Organizations, got involved in Reformasi movement, as compared to similar issues in the past. As these Malays had contact with the grassroots through their organizations such as ABIM and JIM, this movement drew considerable support from lower-class Malays in the rural as well as urban areas.\n\nEven among older Malaysians, new preferences or perspectives are at least sharing political space with the older ones. \nConcrete evidence is apparent:\n\n- In the 1999 General elections, the opposition won over 40 percent of the votes and candidates who specifically ran on a social justice platform did well.\n\n- About three times the usual number of people registered to vote in the registration period that coincided with the height of Reformasi.\n\n- Opposition – supporting alternative media matched or outstripped mainstream mass media in circulation during the Reformasi period. For example, Malaysiakini, the Web-based daily, have retained a significant readership.\n\n- UMNO has made a concerted effort to change and to win back youth, women and others who have lost faith in the organization.\n\n- University students have continued their efforts on mobilization.\n\n- Terms such as transparency, accountabllity and cronyism have entered common discourse. Government leaders have been forced to respond and reform, by means ranging from admitting that corruption is a problem to setting up the National Human Rights Commission to holding interparty debates and feedback session with constituents.\n\n As a tailspin from GE14, a local grassroots NGO has started a transform- Sarawak blockchain initiative answering the state CM Abang Johari's call to digitize the local economy.\n\n\n\n"}
{"id": "3961389", "url": "https://en.wikipedia.org/wiki?curid=3961389", "title": "Reintegrationism", "text": "Reintegrationism\n\nReintegrationism (Galician and ; or , ) is the linguistic and cultural movement in Galicia which advocates for the unity of Galician and Portuguese as a single language. In other words, it postulates that Galician and Portuguese languages did not only share a common origin and literary tradition, but that they are in fact variants of the same language today. According to this, Galicia should re-integrate into the Community of Portuguese Language Countries.\n\nThere are two main views in Galicia about the Galician language:\n\n\n\nThe divergences between isolationism and reintegrationism can be traced back to the time of the Galician \"Rexurdimento\" (Revival), in the nineteenth century, when Galician began to be systematically written again in Galicia for the first time since the Middle Ages. Up to that time, written Galician was either forbidden or simply dismissed by the Spanish authorities, and certainly not used officially. Hence, \"Rexurdimento\" Galician writers realized they did not \"know\" how Galician should be spelled properly. There were three possible options: to infer it from the Medieval forms; to use a Spanish-based spelling, which was already known to all; or to use the Portuguese spelling, feeling that written Portuguese was \"what Galician would have been if it had not been forbidden\". Authors such as Castelao, among others, stated that Galician should gradually merge with Portuguese, namely in its written form. The reality was that until the 1980s Galician was often written using a mix of the three options.\n\nYet, with the end of Francoist Spain in 1975, and with the Spanish transition to democracy shortly afterwards, Galicia became an autonomous community with the Statute of Autonomy of 1981, with Galician as its official language (alongside Spanish). Establishing a fixed standard form then became urgent. Claims for Galician-Portuguese linguistic unity had already been produced, as evidenced with the \"Manifesto para a supervivência da cultura galega\" (Manifesto for the Survival of Galician Culture), first published in 1974. Still, the first draft of the language norms was produced in 1979 under the guidance of linguist Professor Ricardo Carballo Calero. These norms recommended a gradual approach to Portuguese, often allowing for a number of different solutions in the case of uncertainty.\n\nHowever, political issues forced the resignation of Carvalho Calero and, consequently, the 1979 pro-reintegrationist norms were revoked. The new official norms and reforms passed from 1982 onwards would be strongly pro-isolationist.\n\nReintegrationism accepts two possibilities for writing Galician: either adopting the standard Portuguese written norm or using a slightly modified norm following the recommendations of AGAL. In any case, reintegrationism considers that spoken Galician and all of its characteristic words, expressions, and pronunciation should not be radically substituted by standard Portuguese. The main recommendations of reintegrationism when referring to spoken Galician revolve around the avoidance of unnecessary Spanish loanwords, namely colloquialisms. In writing, the most obvious differences from the official norm (NOMIGa) are (according to AGAL):\n\n\nEven though the reintegrationist norm does not have official status, it has been recognized in courts of law. Furthermore, Galician members of the European Parliament (such as José Posada, Camilo Nogueira and Xosé Manuel Beiras) have used spoken Galician when addressing the chamber and have used standard Portuguese orthography to encode their Galician speech. In all cases, these interventions and encodings have been accepted by the Parliament as a valid form of Portuguese, that is, an official language of the European Union.\n\nFurthermore, members of Galician reintegrationist associations have been regularly present at meetings of the Community of Portuguese-Speaking Countries. In 2008, Galician delegates were invited as speakers to the Portuguese Parliament when discussing the new spelling norms for Portuguese language.\n\nThe local government of Corcubión (a Galician municipality) was the first public institution to officially use the AGAL norm, as seen on its website, offering options for \"Galician\" (NOMIGa) and \"Galician-Portuguese\" (AGAL) – plus English and Spanish.\n\nTheoretically, the positions of the standards of NOMIGa (official norm) and AGAL are not so different. Although usually accused of having pro-Spanish tendencies, it is stated at the introduction of the NOMIGa that \"standard choices must be in harmony with those of other languages, especially to those of Romance languages and especially to those of Portuguese\". Furthermore, they value \"the contribution of Peninsular and Brazilian Portuguese\" in the creation of the Galician norm.\n\nThis being the philosophy behind the \"official standard\", NOMIGa and AGAL share an initial starting point, but it is often argued that the NOMIGa are far removed from the usual speech of day-to-day and older Galician speakers, in addition to \"isolating\" Galician (hence the term \"isolationist\") from the rest of Portuguese-speaking areas by using a different writing system.\nIn any case, European and Brazilian Portuguese are usually analyzed by both isolationists and reintegrationists as a primary source from which to extract scientific and technical terminology and neologisms.\n\nAs with many other aspects of Galician society and culture, language is deeply politicized in Galicia. Traditionally, the defence and promotion of Galician language has been linked to Galician nationalism, yet this is often considered a simplification. Likewise, different political groups and parties have adopted different approaches to the \"isolationism vs reintegrationism\" polemic. Broadly speaking, pro-independence groups have traditionally expressed a greater support for the reintegrationist norm, while others have adopted the isolationist. In any case, all the linguistic organizations behind both reintegrationism and isolationism have attempted to dissociate themselves from the political debate. For example, AGAL members have often expressed that this is merely a linguistic, hence scientific, discussion, and that it should not become the arena for political fights among the community of Galician speakers.\n\n\n"}
{"id": "2424806", "url": "https://en.wikipedia.org/wiki?curid=2424806", "title": "Sanitization (classified information)", "text": "Sanitization (classified information)\n\nSanitization is the process of removing sensitive information from a document or other message (or sometimes encrypting it), so that the document may be distributed to a broader audience. When the intent is secrecy protection, such as in dealing with classified information, sanitization attempts to reduce the document's classification level, possibly yielding an unclassified document. When the intent is privacy protection, it is often called data anonymization. Originally, the term sanitization was applied to printed documents; it has since been extended to apply to computer media and the problem of data remanence as well.\n\nRedaction in its sanitization sense (as distinguished from its other editing sense) is the blacking out or deletion of text in a document, or the result of such an effort. It is intended to allow the selective disclosure of information in a document while keeping other parts of the document secret. Typically the result is a document that is suitable for publication or for dissemination to others than the intended audience of the original document. For example, when a document is subpoenaed in a court case, information not specifically relevant to the case at hand is often redacted.\n\nIn the context of government documents, redaction (also called sanitization) generally refers more specifically to the process of removing sensitive or classified information from a document prior to its publication, during declassification.\n\nThe traditional technique of redacting confidential material from a paper document before its public release involves overwriting portions of text with a wide black pen, followed by photocopying the result—the obscured text may be recoverable from the original. Alternatively opaque \"cover up tape\" or \"redaction tape\", opaque, removable adhesive tape in various widths, may be applied before photocopying.\n\nThis is a simple process with only minor security risks. For example, if the black pen or tape is not wide enough, careful examination of the resulting photocopy may still reveal partial information about the text, such as the difference between short and tall letters. The exact length of the removed text also remains recognizable, which may help in guessing plausible wordings for shorter redacted sections. Where computer-generated proportional fonts were used, even more information can leak out of the redacted section in the form of the exact position of nearby visible characters.\n\nThe UK National Archives published a document, \"Redaction Toolkit, Guidelines for the Editing of Exempt Information from Documents Prior to Release\", \"to provide guidance on the editing of exempt material from information held by public bodies.\"\n\nSecure redacting is a far more complicated problem with computer files. Word processing formats may save a revision history of the edited text that still contains the redacted text. In some file formats, unused portions of memory are saved that may still contain fragments of previous versions of the text. Where text is redacted, in Portable Document Format (PDF) or word processor formats, by overlaying graphical elements (usually black rectangles) over text, the original text remains in the file and can be uncovered by simply deleting the overlaying graphics. Effective redaction of electronic documents requires the removal of all relevant text or image data from the document file. This either requires a very detailed understanding of the internal operation of the document processing software and file formats used, which most computer users lack, or software tools designed for sanitizing electronic documents (see external links below).\n\nRedaction usually requires a marking of the redacted area with the reason that the content is being restricted. US government documents being released under the Freedom of Information Act are marked with exemption codes that denote the reason why the content has been withheld.\n\nThe US National Security Agency (NSA) published a guidance document which provides instructions for redacting PDF files.\n\nPrinted documents which contain classified or sensitive information frequently contain a great deal of information which is less sensitive. There may be a need to release the less sensitive portions to uncleared personnel. The printed document will consequently be sanitized to obscure or remove the sensitive information. Maps have also been redacted for the same reason, with highly sensitive areas covered with a slip of white paper.\n\nIn some cases, sanitizing a classified document removes enough information to reduce the classification from a higher level to a lower one. For example, raw intelligence reports may contain highly classified information such as the identities of spies, that is removed before the reports are distributed outside the intelligence agency: the initial report may be classified as Top Secret while the sanitized report may be classified as Secret.\n\nIn other cases, like the NSA report on the USS \"Liberty\" incident (right), the report may be sanitized to remove all sensitive data, so that the report may be released to the general public.\n\nAs is seen in the USS \"Liberty\" report, paper documents are generally sanitized by covering the classified and sensitive portions and then photocopying the document, resulting in a sanitized document suitable for distribution.\n\nComputer (electronic or digital) documents are more difficult to sanitize. In many cases, when information in an information system is modified or erased, some or all of the data remains in storage. This may be an accident of design, where the underlying storage mechanism (disk, RAM, etc.) still allows information to be read, despite its nominal erasure. The general term for this problem is data remanence. In some contexts (notably the US NSA, DoD, and related organizations), sanitization typically refers to countering the data remanence problem; redaction is used in the sense of this article.\n\nHowever, the retention may be a deliberate feature, in the form of an undo buffer, revision history, \"trash can\", backups, or the like. For example, word processing programs like Microsoft Word will sometimes be used to edit out the sensitive information. Unfortunately, these products do not always show the user all of the information stored in a file, so it is possible that a file may still contain sensitive information. In other cases, inexperienced users use ineffective methods which fail to sanitize the document. Metadata removal tools are designed to effectively sanitize documents by removing potentially sensitive information.\n\nIn May 2005 the US military published a report on the death of Nicola Calipari, an Italian secret agent, at a US military checkpoint in Iraq. The published version of the report was in PDF format, and had been incorrectly redacted using commercial software tools. Shortly thereafter, readers discovered that the blocked-out portions could be retrieved by copying them and pasting into a word processor.\n\nSimilarly, on May 24, 2006, lawyers for the communications service provider AT&T filed a legal brief regarding their cooperation with domestic wiretapping by the NSA. Text on pages 12 to 14 of the PDF document were incorrectly redacted, and the covered text could be retrieved using cut and paste.\n\nAt the end of 2005, the NSA released a report giving recommendations on how to safely sanitize a Microsoft Word document.\n\nIssues such as these make it difficult to reliably implement multilevel security systems, in which computer users of differing security clearances may share documents. \"The Challenge of Multilevel Security\" gives an example of a sanitization failure caused by unexpected behavior in Microsoft Word's change tracking feature.\n\nThe two most common mistakes for incorrectly redacting a document are adding an image layer over the sensitive text without removing the underlying text, and setting the background color to match the text color. In both of these cases, the redacted material still exists in the document underneath the visible appearance and is subject to searching and even simple copy and paste extraction. Proper redaction tools and procedures must be used to permanently remove the sensitive information. This is often accomplished in a multi-user workflow where one group of people mark sections of the document as proposals to be redacted, another group verifies the redaction proposals are correct, and a final group operates the redaction tool to permanently remove the proposed items.\n\n"}
{"id": "3250892", "url": "https://en.wikipedia.org/wiki?curid=3250892", "title": "Seneka Bibile", "text": "Seneka Bibile\n\nSenaka Bibile (Sinhala:සේනක බිබිලේ) (13 February 1920 – 29 September 1977) was a Sri Lankan pharmacologist. He was the founder of Sri Lanka's drug policy, which was used as a model for development of policies based on rational pharmaceutical use in other countries as well by the World Health Organization, the United Nations Conference on Trade and Development (UNCTAD) and the Non-Aligned Movement. Due to the far reaching effects of his proposals and policies, he has been called the 'greatest medical benefactor of humanity that Sri Lanka has hitherto produced'.\n\nSenaka Bibile was born at Kathaluwa Walauwa. His father was Charles William Bibile, a \"Rate Mahathmaya\" or Chief Native Feudal Official of Wellassa, his mother Sylvia Jayawardhana of Kathaluwa \"Walauwa\", the manor house of the Jayawardhana family. The Bibile family claimed descent from a 16th-century Vedda chieftain.\n\nHe received his primary and secondary education at Trinity College, Kandy. He entered the Medical College in Colombo, where he had a brilliant career, winning the gold medals for medicine and surgery, and obtained a first class honours degree in 1945. In 1949, he began post-graduate studies at the University of Edinburgh, returning to Sri Lanka in 1952 with a PhD. His doctorate thesis was a study of biological assays of cortical hormone and their application. His PhD supervisor was Dr Marthe Louise Vogt a famous British Pharmacologist of German Origin.\n\nHe joined the University of Ceylon in 1947. In 1947–49 he led a research team of doctors under the guidance of Prof. Cullumbine, Professor of Physiology. In 1958 he was selected as the first Professor of Pharmacology and became the head of a new department. He was the first dean of the faculty of medicine at the University of Peradeniya from 1967 to 1977. There he started the first medical education unit in Sri Lanka.\n\nHe is best known for his advocacy of the government-controlled pharmaceutical purchasing plans often referred to as \"rationalisation\" of pharmaceuticals and the development of a national pharmaceuticals policy. He played the leading role in developing a rational pharmaceutical policy aimed at ensuring that impoverished people would get reasonable drugs at a low price. Further, the programs are aimed towards ensuring that doctors prescribe the minimum required drugs to treat the patient's illness.\n\nA careful selection of drugs was an essential component of the policies he advocated. He was called up by the Minister of Health to address this and prepared the Ceylon Hospital Formulary of about 630 drugs under their generic names almost singlehandedly. Subsequently, the National Formulary Committee (NFC), consisting of representatives from the public and private health sectors and the University Medical Schools a formulary committee was set up with him as its first chairman. The NFC published \"Formulary Notes\" for the use of doctors, which later became \"The Prescriber\".\n\nAt the time, it was alleged and widely believed that Pharmaceutical companies in the country made considerable money by selling drugs under their trade names, giving out biased information about the branded drugs as against those named generically. The United Front Government of 1970 appointed Dr S.A. Wickremasinghe and Dr. Bibile to lead a commission of inquiry to investigate this issue and they recommended the establishment a national policy and of a state body to regularise the trade.\n\nAccordingly, in 1971 Hon. T.B. Subasinghe, the Cabinet Minister of Industries, appointed him founder chairman of the Sri Lanka State Pharmaceuticals Corporation (SPC). The SPC channelled all imports of pharmaceuticals, calling for worldwide bulk tenders which were limited to the approved drugs listed in the national formulary. The public and private health sectors obtained all their requirements from the SPC. Hence the drug trade was regulated by this body and vendors were forced to compete with each other and with generic drug producers on a cost basis only. This program is generally perceived to have resulted in a cheaper drug supply for poor countries.\n\nThis policy was supported by WHO and other UN agencies with enormous benefit to Third World countries. The UNCTAD Secretariat examined the Sri Lankan experience, concluding that an analysis of the Sri Lankan model could give other developing countries an insight into ways of formulating, developing and implementing integrated national pharmaceutical policies. With Bibile's assistance, it published \"Case Studies in the Transfer of Technology: Pharmaceutical Policies in Sri Lanka\". This document has proved to be a very valuable guideline for developing countries intending to initiate pharmaceutical reforms. Translated into other languages, it may be found with health planners of almost every Third World country.\n\nAlthough his policy was watered down by the United National Party Government of 1977, which re-opened the doors to unrestricted imports, the SPC was never dissolved and continued to supply affordable drugs. In 2005 the United People's Freedom Alliance Government promised to establish a National Medicinal Drugs Policy (NMPD) that would enable Sri Lankans significantly to cut down on drug expenses and get quality drugs at affordable prices while saving billions of rupees in foreign exchange for the country. Implementation of the NMDP could reduce the number of drugs imported, prescribed and sold in Sri Lanka to about 350 varieties.\n\nBibile's contribution to development in the area of drugs was acknowledged publicly during the 35th World Health Assembly, in Geneva in May 1982.\n\nIt is widely regarded in his home country of Sri Lanka that the threat he posed to the powerful drugs Multi-nationals may have had some bearing on his premature and mysterious death in 1977 while on a UN assignment in Guyana to introduce these policies there.\n\nSee Wikileaks information on Dr Bibile's drug policy and the United States \n\nHe was a Trotskyist and a member of the Lanka Sama Samaja Party, becoming Treasurer of its Youth Leagues. He conducted study classes on Marxism at his house in Castle Street, Colombo. At the height of the language issue crisis, when the LSSP stood for both Sinhala and Tamil being state languages, he agreed to contest a by-election for a seat on the Colombo Municipal Council as the LSSP candidate, to fight for that principle, risking his life in the process and facing certain defeat in an atmosphere of ethnic animosity.\n\nIn the mid-1950s, he, together with Herbert Keuneman, 'Bonnie' Fernando, Anil and Jeanne Moonesinghe and other members of the radical intelligentsia founded Sri Lanka's first co-operative housing scheme, the Gothatuwa Building Society. This led to the foundation of the Welikadawatte housing estate, which attained some fame as an island of intellectual creativity.\n\nDr Bibile has over 45 publications, including:\n\n\nDr Bibile died in 1977 at Guyana, under mysterious circumstances. The section regarding his death is being consistently removed from Wikipedia by an unknown entity, each time someone updates it.\n\nIt is a well known 'secret' that Dr Bibile was murdered by inducing cardiac arrest using a drug(s) that has been available, even before second world war. One of his colleagues were involved in the murder to ensure that he would not recover from the 'heart attack' by not taking prompt action as well as taking the wrong cause of action as well as a delayed cause of action to recover him. As a matter of fact, this doctor colleague of doctor Bibile from UN, asked Mrs Bibile for her forgiveness for letting him die that way, implying that he was simply a tool for corrupted multinational drug companies, in which she replied, \"If you can bring him back, then I will forgive you\". Known multinational pharmaceutical companies were involved in Dr Bibile's murder. The truth will come out one day as it happens all the time.\n\nAn oration is held every year in commemoration of Senaka Bibile, under the auspices of the Kandy Society of Medicine.\n\n\n"}
{"id": "31224832", "url": "https://en.wikipedia.org/wiki?curid=31224832", "title": "Sergey Elpatyevsky", "text": "Sergey Elpatyevsky\n\nSergey Yakovlevich Elpatyevsky (), November 3, 1854 – January 9, 1933, was a Russian writer and doctor.\n\nElpatyevsky was born in the village of Novoselki-Kudrino, Vladimir Governorate, into the family of a village priest. He studied at a religious school and, after graduating in 1868, at a seminary. In 1872 he entered the Law Faculty of Moscow State University, later transferring to the Medical Faculty.\n\nIn the 1870s Elpatyevsky participated in the narodnik movement. In 1875, he assisted in the organization of settlements under the populists. He also provided his Moscow apartment for populist meetings. In 1876/77 he helped to organize a student club along with S.V. Martynov and V.S. Lebedev. The club served to coordinate student activities and organize assistance for political exiles in Siberia.\n\nIn 1877 Elpatyevsky was investigated on charges of having links with revolutionary groups, but released for lack of evidence. In 1878 he finished his studies at the University, and began working as a doctor in Skopinsky County, Ryazan province. He also continued his revolutionary activities. In 1880 he was arrested on charges of promoting The People's Will, and deported to Ufa province under police supervision.\n\nIn 1884 Elpatyevsky was arrested for distributing illegal literature, and sentenced to exile in Eastern Siberia. He arrived in Krasnoyarsk on October 24, 1884, where he met with the writer Vladimir Korolenko. Elpatyevsky settled in the village of Verhnepashennom, in the Yeniseysky District of Yenisei province. His wife Lyudmila and their two children went with him voluntarily. His wife and children settled in the city of Yeniseysk.\n\nIn 1885 Elpatyevsky was allowed to resettle in Yeniseysk with his family. He repeatedly appealed to the governor of the Yenisei province to allow him to practice medicine, but was refused. He then began to practice medicine free of charge. Later he was granted the right to move freely throughout the countryside to fight the epidemics of diphtheria and scarlet fever in Angara and measles in Turukhansk. He received the thanks of the Governor I. K. Pedashenko for helping to fight these epidemics.\n\nIn April, 1886 a Chelyabinsk merchant named Balakshin asked Governor Pedashenko if Elpatyevsky could be allowed to accompany him to Lake Shira. The governor allowed Elpatyevsky to visit the Minusinsky District for scientific purposes. Their scientific observations about the healing properties of the water of Lake Shira were outlined in Elpatyevsky's report at a meeting of the Yenisei Province Society of Physicians.\n\nAfter his period of exile, Elpatyevsky lived in Nizhny Novgorod, and published works in the magazines \"Russian Wealth\" and \"Russian Gazette\". In 1893 he took part in the national fight against hunger and cholera . In the late 1890s he settled in Yalta, where he often met with Leo Tolstoy and Anton Chekhov, whom he treated for tuberculosis. In the early 1900s he went abroad, and met with the founders of the Socialist-Revolutionary Party. During the 1905 Russian Revolution he held meetings of members of the party in his apartment. He didn't share some of their ideas and, for this reason, became one of the creators of the Labour Popular Socialist Party. Vladimir Lenin criticized him for this. \n\nHe was arrested for publishing the booklet \"Land and Freedom\" in 1910. In 1910/11 he was imprisoned in the Peter and Paul Fortress.\n\nDuring World War 1 Elpatyevsky worked in hospitals in the All-Russian Land Union. In 1917 he left the Labour Popular Socialist Party. After the Russian Revolution of 1917 he lived in Moscow. From 1922 to 1928 he worked as a doctor in the Kremlin hospital. He died in 1933 in Moscow.\n\n"}
{"id": "30862657", "url": "https://en.wikipedia.org/wiki?curid=30862657", "title": "Sociology of race and ethnic relations", "text": "Sociology of race and ethnic relations\n\nThe sociology of race and ethnic relations is the study of social, political, and economic relations between races and ethnicities at all levels of society. This area encompasses the study of systemic racism, like residential segregation and other complex social processes between different racial and ethnic groups. The sociological analysis of race and ethnicity frequently interacts with other areas of sociology such as, but not limited to, stratification and social psychology, as well as with postcolonial theory.\n\nAt the level of political policy, ethnic relations is discussed in terms of either assimilationism or multiculturalism. Anti-racism forms another style of policy, particularly popular in the 1960s and 1970s. At the level of academic inquiry, ethnic relations is discussed either by the experiences of individual racial-ethnic groups or else by overarching theoretical issues.\n\nMarx described society as having nine \"great\" classes, the capitalist class and the working class, with the middle classes falling in behind one or the other as they see fit. He hoped for the working class to rise up against the capitalist class in an attempt to stop the exploitation of the working class. He blamed part of their failure to organize on the capitalist class, as they separated black and white laborers. This separation, specifically between Blacks and Whites in America, contributed to racism. Marx attributes capitalism's contribution to racism through segmented labor markets and a racial inequality of earnings.\n\nWeber laid the foundations for a micro-sociology of ethnic relations beginning in 1906. Weber argued that biological traits could not be the basis for group foundation unless they were conceived as shared characteristics. It was this shared perception and common customs that create and distinguish one ethnicity from another. This differs from the views of many of his contemporaries who believed that an ethnic group was formed from biological similarities alone apart from social perception of membership in a group.\n\nW.E.B. Du Bois is well known as one of the most influential black scholars and activists of the 20th century. Du Bois educated himself on his people, and sought academia as a way to enlighten others on the social injustices against his people. Du Bois research \"revealed the Negro group as a symptom, not a cause; as a striving, palpitating group, and not an inert, sick body of crime; as a long historic development and not a transient occurrence\". Du Bois believed that Black Americans should embrace higher education and use their new access to schooling to achieve a higher position within society. He referred to this idea as the Talented Tenth. With gaining popularity, he also preached the belief that for blacks to be free in some places, they must be free everywhere. After traveling to Africa and Russia, he recanted his original philosophy of integration and acknowledged it as a long term vision.\n\nBooker T. Washington was considered one of the most influential black educators of the 19th and 20th centuries. Born in 1856 as a slave in Virginia, Washington came of age as slavery was coming to an end. Just as slavery ended, however, it was replaced by a system of sharecropping in the South that resulted in black indebtedness. With growing discrimination in the South following the end of the Reconstruction era, Washington felt that the key to advancing in America rested with getting an education and improving one's economic well-being, not with political advancement. Consequently, in 1881, he founded the Tuskegee Institute, now Tuskegee University, in order to provide individuals with an education that would help them to find employment in the growing industrial sector. By focusing on education for blacks, rather than political advancement, he gained financial support from whites for his cause. Secretly, however, he pursued legal challenges against segregation and disfranchisement of blacks.\n\nPatricia Hill Collins is currently a Distinguished University Professor Emerita at the University of Maryland, College Park. She received her PhD in sociology in 1984 from Brandeis University. Collins was the president-elect for the American Sociological Association, where she was the 100th president and the first African-American woman to be president of the organization. Collins is a social theorist whose work and research primarily focuses on race, social class, sexuality, and gender. She has written a number of books and articles on said topics. Collins work focuses on Intersectionality, by looking at issues through the lens of women of color. In her work, she writes \"First, we need new visions of what oppression is, new categories of analysis that are inclusive of race, class, and gender as distinctive yet interlocking structures of oppression\".\n\nEduardo Bonilla-Silva is currently a professor of sociology at Duke University and is the 2018 president of the American Sociological Association. He received his PhD in 1993 from University of Wisconsin–Madison, which is where he met his mentor, Professor Charles Camic, of which he said \"Camic believed in me and told me, just before graduation, that I should stay in the states as I would contribute greatly to American sociology.\" Bonilla-Silva did not start off his work as a \"race scholar,\" but originally was trained in class analysis, political sociology, and sociology of development (globalization). It was not until the late 1980s when he joined a student movement calling for racial justice at the University of Wisconsin that he began his work in race. In his book, Racism without Racists, Bonilla-Silva discusses less overt racism, which he refers to as \"new racism,\" which disguises itself \"under the cloak of legality\" in order to accomplish the same things. He also discusses \"color-blind racism,\" which is essentially when people go off the basis that we have achieved equality and deny past and present discriminations.\n\nOne of the most important social psychological findings concerning race relations is that members of stereotyped groups internalize those stereotypes and thus suffer a wide range of harmful consequences. For example, in a phenomenon called stereotype threat, members of racial and ethnic groups that are stereotyped as scoring poorly on tests will perform poorer on those tests if they are reminded of this stereotype. The effect is so strong that even simply asking the test-taker to state her or his race before taking the test (such is by bubbling in \"African American\" on a multiple choice question) will significantly alter test performance. A specifically sociological contribution to this line of research has found that such negative stereotypes can be created on the spot: an experiment by Michael Lovaglia et al.(1998) demonstrated that left-handed people can be made to suffer stereotype threat if they are led to believe that they are a disadvantaged group for a particular kind of test.\n\nPsychoanalysis has much to offer the study of racism. Its central proposition is that rationality is not the natural state of the individual, and that individuals develop defence mechanisms to cope with anxiety. Humans resist change because change threatens established ways of dealing with anxiety. Individual defence mechanisms contribute to social defence mechanisms. The most regressive defence mechanism (the 'paranoid-schizoid' position) results in a complete dehumanising of the 'all-bad' group. The 'all-bad' group is admired as well as feared (often evident in the conspiracy theory). Paradoxically, the arbitrariness of the category 'race' enables the psychotic subject to invest more meaning in it.\n\nModernity's attempt at rationalisation papers over a polycentric psyche (i.e. all of us still have anxieties and desires, despite our apparent rationality). Racism is a response to the abstracting logic of modernity. The rationality of western, 'white' society is defined in opposition to the 'animality' of black, 'primitive' society.\n\nSome psychoanalytic theorists also argue that passionate anti-racism can produce psychological states analogous to racism.\n\nAnother important line of research on race takes the form of audit studies. The audit study approach creates an artificial pool of people among whom there are no average differences by race. For instance, groups of white and black auditors are matched on every category other than their race, and thoroughly trained to act in identical ways. Given nearly identical resumes, they are sent to interview for the same jobs. Simple comparisons of means can yield strong evidence regarding discrimination. The best known audit study in sociology is \"The Mark of a Criminal Record\" by Harvard University sociologist Devah Pager. This study compares job prospects of black and white men who were recently released from jail. Its key finding is that blacks are significantly discriminated against when applying for service jobs. Moreover, whites \"with\" a criminal record have about the same prospect of getting an interview as blacks \"without\" one. Another recent audit by UCLA sociologist S. Michael Gaddis examines the job prospects of black and white college graduates from elite private and high quality state higher education institutions. This research finds that blacks who graduate from an elite school such as Harvard have about the same prospect of getting an interview as whites who graduate from a state school such as UMass Amherst.\n\nIn the United States, the study of racial and ethnic relations has been widely influenced by the factors associated with each major wave of immigration as the incoming group struggles with keeping its own cultural and ethnic identity while also assimilating into the broader mainstream American culture and economy. One of the first and most prevalent topics within American study is that of the relations between white Americans and African Americans due to the heavy collective memory and culture borne out of and lingering from centuries of forced slavery in plantations. Throughout the rest of American history, each new wave of immigration to the United States has brought another set of issues as the tension between maintaining diversity and assimilating takes on new shapes. Racism and conflict often rears up during these times. However, some key currents can be gleaned from this body of knowledge: in the context of the United States, there is a tendency for minorities to be punished in times of economic, political and/or geopolitical crises. Times of social and systemic stability, however, tend to mute whatever underlying tensions exist between different groups. In times of societal crisis—whether perceived or real—patterns or retractability of American identities have erupted to the fore of America's political landscape. Notable and infamous examples can be seen in Executive Order 9066 that placed Japanese Americans in incarceration centers as well as the 19th century Chinese Exclusion Act that banned Chinese laborers from emigrating to the United States (local workers viewed Chinese laborers as a threat). Current examples include post-9/11 backlash against Muslim Americans, although these have taken place in civil society, not through public policy.\n\nIn the United Kingdom, foreign nationals were actively encouraged and sponsored to migrate in the 1950s after the dissolution of the Empire and the social devastation of the Second World War. The 1962 Commonwealth Immigrants Act changed the law so that only certain British Commonwealth members were able to migrate. This law was tightened again with the Commonwealth Immigrants Act 1968 and Immigration Act 1971. The Race Relations Act 1968 extended certain anti-discrimination policies with respect to employment, housing, commercial and other services. This was extended again with the Race Relations Act 1976.\n\nAs with the UK establishments of media and cultural studies, 'ethnic relations' is often taught as a loosely distinct discipline either within sociology departments or other schools of humanities.\n\nMajor British theorists include Paul Gilroy, Stuart Hall, Richard Jenkins, John Rex, Michael Banton and Tariq Modood.\n\n"}
{"id": "31532527", "url": "https://en.wikipedia.org/wiki?curid=31532527", "title": "Sound velocity probe", "text": "Sound velocity probe\n\n\"Note: This page refers to the device used to measure the speed of sound in water for use in hydrography\"\n\nA sound velocity probe is a device that is used for measuring the speed of sound, specifically in the water column, for oceanographic and hydrographic research purposes.\n\nEarly depth sounding was achieved using lead line sounding (or sounding line), where a lead weight attached to a length of rope marked with depth values. As this method was mechanical in nature, the only correction that was applied to the sounding was the reduction of the sounding for tidal height. In the mid 20th century, sonar systems were developed to allow the measurement of underwater distances using the two way travel time of an acoustic pulse. This allowed the surveyor to take many more soundings in a given period of time and was less labor intensive than using a lead line.\n\nFor many applications of sonar the speed of sound can be assumed to be an average speed of 1500 meters per second. However, the speed of sound in seawater can vary from 1440 to 1570 meters per second.\n\nAs the relationship of speed, time and distance are dependent, in order to accurately measure the distance one must also know the time of transmit to receive and the speed of sound in water accurately. There are two methods that this can be achieved.\n\nFirstly, the surveyor can use an air filled metal bar lowered below the transducer, attached at each end by a rope marked with depth values. If the values on the rope can be assumed to be correct, then the bar is lowered at set depth intervals and observed on the echo sounder trace. The values of echo sounder depth can be plotted against the \"true\" depth of the bar. Any fixed offset value would then be attributed to a draft value correction, and any gradient change seen is as a result of a difference in sound velocity. This method is called a \"Bar Check\", and is performed by the surveyor prior to gathering data.\n\nSecondly, the surveyor can use a sound velocity probe that can be lowered into the water in the area to be surveyed to measure the actual speed of sound. This has the advantage of being quicker than a bar check and it can be performed when there is boat motion due to swell and sea, although any draft offset of the vessel is not identified using this method.\n\nThere are two common methods to obtaining sound velocity in water using the probe method.\n\nFirstly, the main three variables that affect sound velocity may be measured using a Conductivity, Temperature & Depth Probe (CTD Probe). This instrument can determine the salinity, temperature and pressure variables, and then calculate the sound velocity of the water using one of the many formulae available.\n\nSecondly, the speed of sound may be directly measured using a small acoustic transducer and a reflecting surface, mounted at a known distance from the acoustic center of the transducer. If the distance from the transducer to the reflector is known, and the time taken from the transmit to the receive pulse is known, then the speed of sound in water can be calculated. Transducers used in sound velocity probes are typically of a high frequency (around 1 - 4 MHz) as the transmit and receive distances are close enough to mitigate any significant absorption losses.\n\n\n"}
{"id": "36078966", "url": "https://en.wikipedia.org/wiki?curid=36078966", "title": "Tej Singh (politician)", "text": "Tej Singh (politician)\n\nTej Singh is a founder president of Ambedkar Samaj Party. He is also commander of \"Bahujan Swemsewak Sangthan\".\n"}
{"id": "7183918", "url": "https://en.wikipedia.org/wiki?curid=7183918", "title": "The Secret Policeman's Ball (1979)", "text": "The Secret Policeman's Ball (1979)\n\nThe Secret Policeman's Ball was the third of the benefit shows staged by Amnesty International to raise funds for its research and campaign work in the human rights field. In later years, other Amnesty benefit shows also bore the \"Secret Policeman's\" title. They are informally referred to as \"The Secret Policeman's Balls\".\n\n\"The Secret Policeman's Ball\" took place over four consecutive nights in London on 27–30 June 1979. It was a successor to the 1976 show \"A Poke In The Eye (With A Sharp Stick)\" (the film of which was titled \"Pleasure At Her Majesty's\") and the 1977 show \"The Mermaid Frolics\".\n\nThe show was directed by Monty Python alumnus John Cleese and producers Martin Lewis and Peter Walker. It subsequently yielded a one-hour TV special, a full-length film, and two record albums (one each of comedy and music performances).\n\nOne of the sketches in the show was Peter Cook's nine-minute parody of the biased judge's instructions to the jury in the recently concluded Jeremy Thorpe trial, titled \"Entirely a Matter for You\". The sketch was, according to Freeman and Penrose, \"actually not that different from the original\". It is considered to be one of the finest works of Cook's career. Cook and show producer Martin Lewis brought out an album on Virgin Records entitled \"Here Comes the Judge: Live\" of the live performance together with three studio tracks that further lampooned the Thorpe trial. \n\nMusicians-turned-activists such as Sting, Peter Gabriel, Bob Geldof and Bono have attributed their participation in human rights issues to their exposure to Amnesty via \"The Secret Policeman's Ball\" show. Bono told \"Rolling Stone\" magazine in 1986, “I saw \"The Secret Policeman’s Ball\" and it became a part of me. It sowed a seed...\"\n"}
{"id": "11039349", "url": "https://en.wikipedia.org/wiki?curid=11039349", "title": "Timeline of abolition of slavery and serfdom", "text": "Timeline of abolition of slavery and serfdom\n\nThe abolition of slavery occurred at different times in different countries. It frequently occurred sequentially in more than one stage – for example, as abolition of the trade in slaves in a specific country, and then as abolition of slavery throughout empires. Each step was usually the result of a separate law or action. This timeline shows abolition laws or actions listed chronologically. It also covers the abolition of serfdom.\n\nAlthough slavery is still abolished \"de jure\" in all countries, some practices akin to it continue today in many places throughout the world.\n\n\n\n"}
{"id": "41593113", "url": "https://en.wikipedia.org/wiki?curid=41593113", "title": "United States military music customs", "text": "United States military music customs\n\nUnited States military music customs are the traditional, regulatory, and statutory provisions that guide performances by United States military bands during drill and ceremony and state occasions.\n\nFor hundreds of years, military forces have used music to signal their troops. The use of music retains an important place in modern diplomatic protocol and military courtesy and is part of many official military events, such as state funerals, military parades, naval christening, officer-commissioning ceremonies, and promotion ceremonies.\n\nUnlike other English-speaking nations, United States military band ceremonial music is not largely drawn from British military customs but is, rather, a mix of original styles and compositions and - to a lesser extent - French traditions. At the outset of the American Revolution, United States military units primarily relied on fife and drum corps for musical support. The U.S. was first introduced to the bugle horn (forerunner to the modern bugle) during the Battle of Harlem Heights, when British infantry used the instrument, causing Joseph Reed to later recall, \"the enemy appeared in open view, and sounded their bugles in a most insulting manner, as is usual after a fox chase. I never felt such a sensation before—it seemed to crown our disgrace.\" Some U.S. cavalry units adopted bugle horns during the war, however, a shortage of brass in the Thirteen Colonies largely limited use of the instrument to the opposing British and German forces, with U.S. troops continuing to rely heavily on fifes, drums, and even - at the Battle of Saratoga - turkey calls.\n\nThe modern bugle was first introduced to U.S. military units around the time of the War of 1812. During that conflict, only the Rifle Regiment was authorized to use the bugle. All other U.S. forces were required to continue using the traditional U.S. fife. Gradually, however, bugles became more widely adopted by the United States military. U.S. bugle calls have largely been based on early French bugle calls (the notable exception is \"Attention\", which is taken from the British bugle call \"Alarm\").\n\nThe dawn of the \"march music era\" hastened the downfall of the fife and drum corps (today, the U.S. armed forces field just a single fife and drum corps among its nearly 150 bands). Aided by the large body of work being created by prolific U.S. composers such as John Philip Sousa, Henry Filmore and Edwin Eugene Bagley, U.S. military and military-like bands became known for performing a unique style of quick-tempo marches with thundering brass and heavy percussion. One music critic, writing about the Boston Jubilee of 1872, contrasted the \"velvety smoothness\" of the invited Band of the Grenadier Guards to the follow-up performance orchestrated by U.S. Army bandmaster general Patrick Gilmore which involved \"a heterogeneous choir of nearly twenty thousand, an orchestra of about a thousand instrumentalists of decidedly mixed abilities, an organ blown by steam power ... a drum of the most preposterous magnitude, and a few batteries of artillery.\"\n\nToday, United States military bands employ music at various times as provided for in armed forces regulations, statute law, and customary practice.\n\n\n\n\nThe United States authorizes all military bands \"band regalia\" consisting of a unique unit drum major mace, baldric, tabard, and drum design. The United States Army Institute of Heraldry designs these items on behalf of military bands.\n\nIn full parade dress, drum majors of many U.S. military bands wear bearskin hats. The origin of the use of bearskins in U.S. military bands dates to 1855 when United States Marine Band director Francis Scala adopted the style for that ensemble in emulation of European trends. This transition occurred as the band was reorganizing itself from a traditional U.S. fife and drum corps into its modern incarnation. A shortage of bearskins in the late 1880s caused the price of the hats to skyrocket, with \"The New York Times\" then reporting their use might be phased out entirely. \"It can readily be seen what a price has to be paid for keeping up a custom which is rather old, it is true, but is practically a useless one save for the purpose of military display,\" the newspaper opined.\n\n"}
{"id": "25536281", "url": "https://en.wikipedia.org/wiki?curid=25536281", "title": "Vice President of Armenia", "text": "Vice President of Armenia\n\nThe Vice President of Armenia was a political position in the government of Armenia created in 1991. The position was abolished by the constitution of 1995 which took effect in February 1996. Currently the Prime Minister of Armenia is President's constitutional successor in case of vacancy.\n"}
{"id": "2143527", "url": "https://en.wikipedia.org/wiki?curid=2143527", "title": "War of Canudos", "text": "War of Canudos\n\nThe War of Canudos (, , 1895–1898) was a conflict between the state of Brazil and some 30,000 settlers who had founded a community named Canudos in the northeastern state of Bahia. After a number of unsuccessful attempts at military suppression, it came to a brutal end in October 1897, when a large Brazilian army force overran the village and killed nearly all the inhabitants. This was the deadliest civil war in Brazilian history.\n\nThe conflict had its origins in the settlement of Canudos (named by its inhabitants \"Belo Monte\" meaning \"Beautiful Hill,\" in the semi-arid backlands (\"sertão\" or \"caatinga\", in Portuguese) in the northeast tip of the state (then province) of Bahia. At this time, Bahia was a desperately poor zone with a depressed economy based on subsistence agriculture and cattle raising. It was without large cities and the disenfranchised population was composed largely of white Brazilians and mestizos. It was a fertile background for dissatisfaction with the new republic, declared November 15, 1889 after a military coup against the ruling Emperor, Dom Pedro II, who was still beloved by the common people.\n\nInto this scenario appeared one of the many mystic spiritual preachers of the time, Antônio Conselheiro (\"Antônio, the Counselor\"), who went from village to village with his followers, doing small jobs and demanding support from small farmers. He claimed to be a prophet and predicted the return of the legendary Portuguese king Sebastian of Portugal. After wandering through the provinces of Ceará, Pernambuco, Sergipe and Bahia, he decided in 1893 to settle permanently with his many followers, of which there were now a great number, in the farming community of Canudos, near Monte Santo, Bahia on the Vaza-Barris River. Soon his preaching and promises of a better world attracted almost 8,000 new residents. Fearing an invasion by the \"Conselhistas\", who had a dispute with a lumber merchant, the mayor of Juazeiro appealed to the provincial government. A visit by two Capuchin friars to Canudos was not enough to calm the population; one of them mistakenly accused Antônio Conselheiro of trying to raise a monarchist sedition.\n\nThe provincial government dispatched Captain Virgílio Pereira de Almeida with 30 men to crush the settlement, resulting in the soldiers' annihilation by a band of “jagunços”, armed hired workers sympathetic to Antônio Conselheiro. The provincial government, alarmed, asked the federal government for help. The United States of Brazil had only recently been founded, and it saw the rebel settlers as monarchists, separatists, a bad example and a threat to the new regime. \n\nPresident Prudente de Morais sent a punitive military expedition and the Brazilian Army began to prepare in November 1896. With scant information about terrain and the size and defensive resources of Canudo’s population, a small, 104-man force commanded by Lieutenant Pires Ferreira attacked the settlement on November 21, 1896. It was fiercely defended, however, by a band of 500 armed men, shouting praises to Antonio Conselheiro and the monarchy. The Brazilian soldiers retreated after incurring severe losses and killing 150 settlers, many armed only with machetes, primitive lances and axes.\nThe defeat of the Pires Ferreira campaign and the sensationalist reports about the ferocity and fanaticism of Canudos’ inhabitants provoked an outcry, and calls for reprisals against the village, which was growing by leaps and bounds and eventually reached 30,000 residents. A second expedition, under Minister of War General , consisted of 557 soldiers and officers under Major Febrônio de Brito. It attacked the well-defended village on January 6, 1897. After some initial success with the infantry and artillery against the villagers' trenches, however, the soldiers were surrounded by more than 4,000 insurrectionists. Lacking ammunition, food and water, and unable to resist the rebels, who continued to fight despite heavy losses, the soldiers retreated once again.\n\nThe Army sent a still larger expeditionary force, since the prestige of the armed forces and the new government were now at stake. An experienced colonel, Antônio Moreira César, set out with three infantry battalions, one cavalry and one artillery battalion, all newly armed and trained. Although forewarned about the numbers and resolve of the rebels, the military thought it impossible that the rebels would resist such a strong regular army force. But on March 6, 1897, they defeated Moreira César’s column after two days of fighting, resulting in further great loss of men and equipment among for the Brazilian forces, including Moreira César.\n\nPressured, the Federal government sent a new expedition under General Arthur Oscar de Andrade Guimarães, and with the direct involvement of the Minister of War, who personally visited Monte Santo, a city near Canudos which served as the gathering point for the large army force being assembled, consisting of three brigades, eight infantry battalions and two artillery battalions. Machine guns and large artillery pieces, such as mortars and howitzers, including a powerful Whitworth 32 (nicknamed \"Matadeira\" (Killer)) went with the 3,000-man force, and had to be hauled with enormous effort through the unforgiving roadless landscape.\n\nThis time, the attackers were aided by rampant hunger and malnutrition among the inhabitants of Canudos, the rebels' lack of weapons and ammunition, and the heavy losses they had suffered in the previous attacks. Further, their spiritual leader and towering figurehead, Antonio Conselheiro, had died on September 22, probably of dysentery and malnutrition provoked by fasting for penance. Canudos was encircled and unmercifully bombarded day after day until the rebels were able to resist no further, and the end came on October 2, 1897. Atrocities were carried out against the civilian population, as slicing the throat of the men, and raping many women, leading to further massacres. When peace was restored, only 150 survivors remained. The best-looking surviving women were taken captive and sent to brothels in Salvador. Antônio Conselheiro's body was disinterred, and his head was cut off and taken triumphantly to the province's capital. According to Peter Robb (\"A Death in Brazil\") it \"was taken to the Medical Faculty of Bahia to be studied for abnormalities (Robb, 2004:208).\n\nSome authors, such as Euclides da Cunha (1902) estimated the number of deaths in the War of Canudos was ca. 30,000 (25,000 residents and 5,000 attackers) , but the real number was probably lower (around 15,000, according to Levine, 1995). According to Peter Robb: \"The foreign correspondents who covered what was soon being called the War of Canudos, as if it were a conflict between nations rather than the extermination of a tiny community within a single country, were nearly all embedded with the army of the Brazilian republic.\"(Robb, 2004:215). Euclides da Cunha did not see the fighting but did bear witness afterward, Robb says, and his \"obsession with progress and modernity, the scientific racism that told him the people of the northeastern interior were doomed to backwardness by their mixed race\" led him to tell a story filled with preconceptions, which is however, the only story we have.\n\nAlthough the original town of Canudos has been covered by the reservoir of the Cocorobó Dam, built by the military regime in the 1960s, the Canudos State Park, established in 1986, preserves many of the important sites and serves as a monument to the war. The stated purpose of the park is to make it impossible to forget the martyrs led by Antônio Conselheiro.\n\n\n\n"}
{"id": "13190675", "url": "https://en.wikipedia.org/wiki?curid=13190675", "title": "Whitehouse.gov", "text": "Whitehouse.gov\n\nwhitehouse.gov is the official website of the White House and is owned by the United States government. Launched in October 1994, it contains information about the President, the Vice President, their families, press releases, proclamations, executive orders, and some speeches by White House officials. It has the official web sites of several offices in the Executive Office of the President, such as the Office of Management and Budget and the Office of Science and Technology Policy.\n\nThe website has been completely redesigned for each new president. Websites for former presidents in office are moved to archive versions. Up to late June 2018, the archived Obama White House homepage, https://obamawhitehouse.archives.gov/, contained no menu linking to the archived content (though such content did exist, e.g., https://obamawhitehouse.archives.gov/the-record/climate). In 2011, the website was considered among the best of the United States federal government.\n\nThe content of the website is in the public domain or licensed under Creative Commons Attribution license.\n\nThe current administration's website is broken into the following sections:\n\nIn July 2001, the White House started switching their web servers to an operating system based on Red Hat Linux and using the Apache HTTP Server. The installation was completed in February 2009. In October 2009, the White House servers adopted Drupal, a free and open-source content management system, which runs on Red Hat Enterprise Linux.\n\nIn December 2017, the Trump administration launched a redesigned website which it claims will save taxpayers \"as much $3 million annually\". \n\nOn September 1, 2011, David Plouffe announced in an email that the White House is releasing \"We the People\" to allow public petitions on whitehouse.gov. The launch of the petitioning platform was announced by Katelyn Sabochik September 22, 2011 in a White House blog post.\n\n\n"}
