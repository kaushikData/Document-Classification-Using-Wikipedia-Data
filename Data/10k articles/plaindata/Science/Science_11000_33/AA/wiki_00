{"id": "28855777", "url": "https://en.wikipedia.org/wiki?curid=28855777", "title": "2MASS 0036+1821", "text": "2MASS 0036+1821\n\n2MASS J00361617+1821104 (abbreviated to 2MASS 0036+1821) is a brown dwarf, located in 28.6 light-years from Earth in the constellation Pisces. It was discovered in 2000 by I. Neill Reid et al.\n\n2MASS 0036+1821 is of spectral type L3.5, the surface temperature is 1300-2000 Kelvin. As with other brown dwarfs of spectral type L, its spectrum is dominated of metal hydrides and alkali metals. Its position shifts due to its proper motion by 0.9071 arcseconds per year.\n\n\n"}
{"id": "14748610", "url": "https://en.wikipedia.org/wiki?curid=14748610", "title": "3C 305", "text": "3C 305\n\n3C 305 is a Seyfert 2 radio galaxy located in the constellation Draco.\n"}
{"id": "7213847", "url": "https://en.wikipedia.org/wiki?curid=7213847", "title": "Alpha Regio", "text": "Alpha Regio\n\nAlpha Regio is a region of the planet Venus extending for about 1500 kilometers centered at 22°S, 5°E. \n\nIt was discovered and named by Richard Goldstein in 1964. The name was approved by the International Astronomical Union's Working Group for Planetary System Nomenclature (IAU/WGPSN) between 1976 and 1979. Maxwell Montes, Alpha Regio, and Beta Regio are the three exceptions to the rule that the surface features of Venus are to be named for females: women or goddesses.\n\nThe surface of the region is what is known as \"Tessera\", meaning a terrain that has been highly deformed and where the deformation strikes in multiple directions and is closely spaced. The term comes from the Greek word for “tiled” (Russian investigators analyzing Venera 15 and Venera 16 imagery thought this terrain looked like a parquet floor). Like all \"tessera\" regions, it sits above the surrounding terrain at an elevation of 1 to 2 kilometers, and is heavily deformed by what appears to be contractional folding. Like most \"tessera\" units, the surrounding volcanic plains appear to have flowed around Alpha's margins and thus are younger than Alpha.\n\nAn infrared map prepared by the \"Venus Express\" orbiter shows that the rocks on the Alpha Regio plateau are lighter in colour and look old compared to the majority of the planet. On Earth, such light-coloured rocks are usually granite and form continents.\n\n"}
{"id": "1183728", "url": "https://en.wikipedia.org/wiki?curid=1183728", "title": "Apollo Mussin-Pushkin", "text": "Apollo Mussin-Pushkin\n\nCount Apollos Apollosovich Musin-Pushkin (; February 17, 1760 – April 18, 1805) was a Russian chemist and plant collector. He led a botanical expedition to the Caucasus in 1802 with his friend botanist Friedrich August Marschall von Bieberstein.\n\nIn 1797, he was elected a foreign member of the Royal Swedish Academy of Sciences. He was a member of the Russian mining board and developed several new methods of refining and processing of platinum. The genus of Puschkinia commemorates his name.\n"}
{"id": "54731628", "url": "https://en.wikipedia.org/wiki?curid=54731628", "title": "Arsonium", "text": "Arsonium\n\nThe arsonium cation is a positively charged polyatomic ion with the chemical formula . An arsonium salt is a salt containing either the arsonium () cation, such as arsonium bromide () and arsonium iodide (), which can be synthesized by reacting arsine with hydrogen bromide or hydrogen iodide.. Or more commonly, as organic derivative such as the quaternary arsonium salts (CAS: [123334-18-9], hydrate form) and the zwitterionic compound arsenobetaine.\n"}
{"id": "29885541", "url": "https://en.wikipedia.org/wiki?curid=29885541", "title": "Art Spivack", "text": "Art Spivack\n\nArthur J. Spivack (July 9, 1956 - Queens, New York), also known as \"Art\" or \"Arturo\", is an American geochemist. He is currently a professor at the University of Rhode Island Graduate School of Oceanography.\n\nSpivack’s research interest is the geochemistry of the oceans, atmosphere, and crust. He developed the use of boron isotopes for determining the pH of ancient oceans. This approach provides a principal basis for estimating atmospheric CO2 concentrations of the last several tens of million years. He led the investigation of the 2015 Salty Brine Beach explosion. He has also contributed to scientific understanding of geochemical fluxes in mid-ocean-ridge hydrothermal systems and subduction zones and understanding of subseafloor life.\n\nSpivack received his bachelor's degree (1980) in chemistry from the Massachusetts Institute of Technology (MIT) and his Ph.D. in Oceanography from MIT and Woods Hole Oceanographic Institution (1986).\n"}
{"id": "56800926", "url": "https://en.wikipedia.org/wiki?curid=56800926", "title": "Auditory science", "text": "Auditory science\n\nAuditory science or hearing science is a field of research and education concerning the perception of sounds by humans, animals, or machines. It is a heavily interdisciplinary field at the crossroad between acoustics, neuroscience, and psychology. It is often related to one or many of these other fields: psychophysics, psychoacoustics, audiology, physiology, otorhinolaryngology, speech science, automatic speech recognition, music psychology, linguistics, and psycholinguistics.\n\nEarly auditory research included the early 19th century work of Georg Ohm and August Seebeck and their experiments and arguments about Fourier analysis of sounds. Later in the 19th century, German physicist Hermann von Helmholtz wrote \"Sensations of Tone\" describing the founding concepts of psychoacoustics, i.e. the relationship between the physical parameters of a sound and the percept that it induces.\n\nPsychoacoutics is primarily interested in the basic workings of the ear and is, therefore, mostly studied using simple sounds like pure tones. In the 1950s, psychologists George A. Miller and J. C. R. Licklider furthered our knowledge in psychoacoustics and speech perception.\n\n\n\n\nMany members of the auditory science community follow the auditory.org mailing list, known as \"the Auditory List\".\n"}
{"id": "33695990", "url": "https://en.wikipedia.org/wiki?curid=33695990", "title": "Center for Detectors", "text": "Center for Detectors\n\nThe Center for Detectors (CfD) is a Rochester Institute of Technology College of Science academic research center. The CfD was founded in 2010 by Dr. Donald Figer. Located in the IT Collaboratory at RIT, the CfD designs, develops, and implements new advanced sensor technologies through collaboration with academic researchers, industry engineers, government scientists, and university students. The mission of the CfD is to enable scientific discovery, national security, better living, and commercial innovation through the design and development of advanced photon detectors and associated technology in a broad array of applications (e.g. astrophysics, biomedical imaging, Earth system science, and inter-planetary travel).\n\nThe CfD uses a multi-disciplinary approach, spanning the many branches of engineering, imaging science, physics and astronomy. Some projects, such as the “Cosmic Ray Damaged Image Repair” project, incorporate astronomy and imaging science. Others, like the NASA-funded Phase II: New Infrared Detectors for Astrophysics project, unite microelectronic engineers, astronomy experts, imaging scientists, and various other professionals in science fields. The Center for Detectors benefits from employees that come from a diverse range of academic programs and professional occupations. The CfD staff includes professors, engineers, and students (undergraduate, masters, and PhD). Many student researchers apply CfD research to their current academic programs at RIT. Students pursue various degrees such as Microelectronic, Computer, and Electrical Engineering. Many undergraduate student researchers choose to pursue master's degrees based on the research that they conduct at CfD.\n\nCfD is grant-funded and has been awarded more than $16M in external funding since 2006. Primary sponsors include NASA, National Science Foundation, and the Gordon and Betty Moore Foundation. Additional sources of funding include Thermo Fisher Scientific, NASA Jet Propulsion Laboratory, ITT Excelis, and Smithsonian Astrophysical Observatory.\n\nEducation supplements to two of the CfD grants have funded projects involving students from local high schools. Students mapped the “Journey of a Photon,” and presented their work at venues nationwide. A second group of high school students used an RIT-developed 3D projection system dubbed the “Planeterrainium” to explore planetary surfaces in 3D.\n\nThe Center has been featured at several conferences and in press venues. CfD researcher Kim Kolb presented her MS thesis at the SPIE Optics and Photonics Conferences. The Rochester Business Journal and RIT University News have published articles summarizing the foundations of the Center for Detectors to the public, and informing the RIT community about the purpose of the facilities.\n\nCfD has hosted a variety of distinguished speakers as well. Some presenters include Dr. Donald Hall of the University of Hawaii, Dr. Chris Packham of the University of Florida, Dr. Joss Bland-Hawthorn of the University of Sydney, Bruce Tromberg, of the University of California, Irvine, and Dr. Shouleh Nikzad, Dr. Michael Hoenk, and Bedabrata Pain of the California Institute of Technology’s Jet Propulsion Laboratory.\n\n The Center has four research labs: the Rochester Imaging Detector Laboratory, the Quantum Dot Detector Laboratory, the Clean Room Probe Testing Laboratory, and the LIDAR Laboratory. Three test systems were designed at the CfD, which are essential to the lab’s detector-testing capabilities.\n\nProjects\n"}
{"id": "39230475", "url": "https://en.wikipedia.org/wiki?curid=39230475", "title": "Configurational mechanics", "text": "Configurational mechanics\n\nConfigurational mechanics is a subdiscipline of continuum mechanics in which particular emphasis is placed on reckoning from the perspective of the material manifold. By contrast, in classical mechanics, reckoning is commonly made from the perspective of spatial coordinates.\n\n"}
{"id": "23351896", "url": "https://en.wikipedia.org/wiki?curid=23351896", "title": "Deep space exploration", "text": "Deep space exploration\n\nDeep space exploration (or deep-space exploration) is the branch of astronomy, astronautics and space technology that is involved with exploring the distant regions of outer space. Physical exploration of space is conducted both by human spaceflights (deep-space astronautics) and by robotic spacecraft.\n\nAt present the furthest space probe mankind has constructed and launched from Earth is Voyager 1, which was announced on December 5, 2011 to have reached the outer edge of the Solar system, and entered interstellar space on August 25, 2012. Deep space exploration further than this vessel's capacity is not yet possible due to limitations in the space-engine technology currently available.\n\nSome of the best candidates for future deep space engine technologies include anti-matter, nuclear power and beamed propulsion. The latter, beamed propulsion, appears to be the best candidate for deep space exploration presently available, since it uses known physics and known technology that is being developed for other purposes.\n\nIn 2012, the Defense Advanced Research Projects Agency announced the award of $500,000 to former astronaut Mae Jemison to fund a project with the goal of sending future astronauts out of the Solar System. Jemison aims to increase public interest in future deep space exploration projects. Upon awarding the money to Jemison, a \"100 Year Starship\" symposium was held in Houston, Texas to discuss interstellar travel. Topics discussed include \"time-distance solutions; life sciences in space exploration; destinations and habitats; becoming an interstellar civilization; space technologies enhancing life on earth; and commercial opportunities from interstellar efforts\".\n\nResearch in deep space is ongoing and rapidly developing. In 2011, after the retirement of the space shuttle, NASA announced its intentions to invest money into developing three technologies vital to deep space exploration. The \"must-have technologies\" include a deep space atomic clock, a large solar sail and a more advanced laser communications system to improve communication, navigation, and propulsion in future missions. In June 2013, NASA announced the selection of eight American astronauts that will begin to train for future deep space missions beyond low Earth orbit. NASA intends that these eight astronauts to train for future Mars or asteroid travel.\n\nThe Single Aperture Far-Infrared Observatory (SAFIR), a proposed cryogenic space telescope, is tentatively set to launch in 2015 with the hopes of exploring \"the formation of the first stars and galaxies\" in deep space. The telescope will be more than 1000 times more sensitive than two current telescope spacecrafts, the Spitzer Space Telescope and the Herschel Space Observatory. NASA hopes to use SAFIR to learn about black holes, galaxy formation and evolution and the formation of star systems in the far reaches of space.\n\n\n"}
{"id": "17226680", "url": "https://en.wikipedia.org/wiki?curid=17226680", "title": "Ecology: From Individuals to Ecosystems", "text": "Ecology: From Individuals to Ecosystems\n\nEcology: From Individuals to Ecosystems is a 2006 higher education textbook on general ecology written by Michael Begon, Colin R. Townsend and John L. Harper. Published by Blackwell Publishing, it is now in its fourth edition. The first three editions were published by Blackwell Science under the title \"Ecology: Individuals, Populations and Communities\". Since it first became available it has had a positive reception, and has long been one of the leading textbooks on ecology.\n\nThe book is written by Michael Begon of the University of Liverpool's School of Biosciences, Colin Townsend, from the Department of Zoology of New Zealand's University of Otago, and the University of Exeter's John L. Harper. The first edition was published in 1986. This was followed in 1990 with a second edition. The third edition became available in 1996. The most recent edition appeared in 2006 under the new subtitle \"From Individuals to Ecosystems\".\n\nOne of the book's authors, John L. Harper, is now deceased. The fourth edition cover is an image of a mural on a Wellington street created by Christopher Meech and a group of urban artists to generate thought about the topic of environmental degradation. It reads \"we did not inherit the earth from our ancestors, we borrowed it from our children.\"\n\nPart 1. ORGANISMS\n\n1. Organisms in their environments: the evolutionary backdrop\n\n2. Conditions\n\n3. Resources\n\n4. Life, death and life histories\n\n5. Intraspecific competition\n\n6. Dispersal, dormancy and metapopulations\n\n7. Ecological applications at the level of organisms and single-species populations\n\nPart 2. SPECIES INTERACTIONS\n\n8. Interspecific competition\n\n9. The nature of predation\n\n10. The population dynamics of predation\n\n11. Decomposers and detritivores\n\n12. Parasitism and disease\n\n13. Symbiosis and mutualism\n\n14. Abundance\n\n15. Ecological applications at the level of population interactions\n\nPart 3. COMMUNITIES AND ECOSYSTEMS\n\n16. The nature of the community\n\n17. The flux of energy through ecosystems\n\n18. The flux of matter through ecosystems\n\n19. The influence of population interactions on community structure\n\n20. Food webs\n\n21. Patterns in species richness\n\n22. Ecological applications at the level of communities and ecosystems\n\n"}
{"id": "27692923", "url": "https://en.wikipedia.org/wiki?curid=27692923", "title": "Economics terminology that differs from common usage", "text": "Economics terminology that differs from common usage\n\nIn any technical subject, words commonly used in everyday life acquire very specific technical meanings, and confusion can arise when someone is uncertain of the intended meaning of a word. This article explains the differences in meaning between some technical terms used in economics and the corresponding terms in everyday usage.\n\nEconomists commonly use the term \"recession\" to mean either a period of two successive calendar quarters each having negative growth of real gross domestic product—that is, of the total amount of goods and services produced within a country—or that provided by the National Bureau of Economic Research (NBER): \"...a significant decline in economic activity spread across the country, lasting more than a few months, normally visible in real GDP growth, real personal income, employment (non-farm payrolls), industrial production, and wholesale-retail sales.\" Almost all economists and policymakers defer to the NBER's determination for the precise dates of a U.S. recession's beginning and end.\n\nIn contrast, in non-expert, everyday usage, \"recession\" may refer to a period in which the unemployment rate is substantially higher than normal.\n\nLabor economists categorize people into three groups: \"employed\" - actually working at a job, even if part-time; \"unemployed\" - not working, but looking for work or awaiting a scheduled recall from a temporary layoff; and \"not in the labor force\" - neither working nor looking for work. People not in the labor force, even if they have given up looking for a job despite wanting one, are not considered unemployed. For this reason it is often thought, especially when a recession has persisted for a sustained period, that the unemployment rate understates the true amount of unemployment because some unemployment is disguised by discouraged workers having left the labor force.\n\nThe everyday usage of the word \"unemployed\" is usually broad enough to include disguised unemployment, and may include people with no intention of finding a job. For example, a dictionary definition is: \"not engaged in a gainful occupation\", which is broader than the economic definition.\n\nEconomists use the word \"money\" to mean very liquid assets which are held at any moment in time. The units of measurement are dollars or another currency, with no time dimension, so this is a stock variable. There are several technical definitions of what is included in \"money\", depending on how liquid a particular type of asset has to be in order to be included. Common measures include M1, M2, and M3.\n\nIn everyday usage, \"money\" can refer to the very liquid assets included in the technical definition, but it usually refers to something much broader. When someone says \"She has a lot of money,\" the intended meaning is almost certainly that she has a lot of what economists would call financial wealth, which includes not only the most liquid assets (which tend to pay low or zero returns), but also stocks, bonds and other financial investments not included in the technical definition. Non-financial assets, such as land and buildings, may also be included. For example, dictionary definitions of money include \"wealth reckoned in terms of money\" and \"persons or interests possessing or controlling great wealth\", neither of which correspond to the economic definition.\n\nA related but different everyday usage occurs in the sentence \"He makes a lot of money.\" This refers to a variable that economists call income. Unlike the usages mentioned above, this one has the units \"dollars, or another currency, per unit of time\", where the unit of time might be a week, month, or year, making it a flow variable.\n\nWhile financial economists use the word \"investment\" to refer to the acquisition and holding of potentially income-generating forms of wealth such as stocks and bonds, macroeconomists usually use the word for the sum of fixed investment—the purchasing of a certain amount of newly produced productive equipment, buildings or other productive physical assets per unit of time—and inventory investment—the accumulation of inventories over time. This is one of the major types of expenditure in an economy, the others being consumption expenditure, government expenditure, and expenditure on a country's export goods by people outside the country.\n\nThe everyday usage of \"investment\" coincides with the one used by financial economists—the acquisition and holding of potentially income-generating forms of wealth such as stocks and bonds.\n\nSimilarly, while financial economists use the word \"capital\" to refer to funds used by entrepreneurs and businesses to buy what they need to make their products or to provide their services, macroeconomists and microeconomists use the term capital to mean productive equipment, buildings or other productive physical assets.\n\nAs with the term \"investment\", the everyday usage of \"capital\" coincides with its use by financial economists.\n\nEconomists distinguish between government spending on newly produced goods and services, such as paying a company to build a new highway, and government spending on transfer payments, which are payments such as welfare payments intended to redistribute income. In economic models, transfer payments are normally treated as a negative component of \"taxes net of transfers\", leaving \"government spending on (newly produced) goods and services\" as a separate category, often referred to simply as \"government spending\".\n\nIn everyday usage, \"government spending\" refers to the broader concept of government spending on goods and services plus transfer payments.\n\nWelfare economics is a branch of economics that uses microeconomic techniques to evaluate economic well-being, especially relative to competitive general equilibrium, with a focus on economic efficiency and income distribution.\n\nIn general usage, including by economists outside the above context, welfare refers to a form of transfer payment.\n\nEconomists use the word efficient to mean any of several closely related things:\nAll of these definitions involve the idea that nothing more can be achieved given the resources available.\n\nIn popular usage, efficient often has the similar but less precise meaning \"functioning effectively\".\n\nThe economics term cost, also known as economic cost or opportunity cost, refers to the potential gain that is lost by foregoing one opportunity in order to take advantage of another. The lost potential gain is the cost of the opportunity that is accepted. Sometimes this cost is explicit: for example, if a firm pays $100 for a machine, its cost is $100. Other times, however, the cost is implicit: for example, if a firm diverts resources from producing output worth $200 into producing a different kind of output, then regardless of how much or how little of the latter output is produced, the opportunity cost of doing so is $200.\n\nIn accounting, there is a different technical concept of cost, which excludes implicit opportunity costs.\n\nIn common usage, as in accounting usage, \"cost\" typically does not refer to implicit costs and instead only refers to direct monetary costs.\n\nIn economics, demand refers to the strength of one or many consumers' willingness to purchase a good or goods at any in a range of prices. If, for example, a rise in income causes a consumer to be willing to purchase more of a good than before contingent on each possible price, economists say that the income rise has caused the consumer's demand for the good to rise. In contrast, if a change in market conditions leads to a decline in the price of a good resulting in a consumer's being willing to buy more of it, economists say that the consumer's quantity demanded of the good has risen. A change in \"quantity demanded\" is represented by a movement \"along\" the demand curve, while a change in \"demand\" is represented by a \"shift\" of the demand curve.\n\nIn popular usage a change in \"demand\" can refer to either what economists call a change in demand or what economists call a change in quantity demanded.\n\nWhile \"marginal\" in common usage tends to mean \"tangential\", implying limited importance, in economics \"marginal\" means \"incremental\". For example, the marginal propensity to consume refers to the incremental tendency to spend income on consumer goods: the fraction of any additional income which is spent on additional consumption (or conversely, the fraction of any decrease in income which becomes a decrease in consumption). Likewise, the marginal product of capital refers to the additional production of output that results from using an additional unit of physical capital (machinery, etc.). If very small increments are being considered, so that calculus is used, then this ratio of incremental amounts is a derivative (for example, the marginal propensity to consume becomes the derivative of consumption with respect to income).\n\nIn common usage, \"significant\" usually means \"noteworthy\" or \"of substantial importance\". In econometrics — the use of statistical techniques in economics — \"significant\" means \"unlikely to have occurred by chance\". For example, suppose one wishes to find if the minimum wage rate affects firms' decisions on how much labor to hire. If the data show, on the basis of statistical techniques, an effect of a particular non-zero magnitude, one wants to know whether that non-zero magnitude could have arisen in the data by chance when in fact the true effect is zero. If a statistical test shows that there is less than, say, a 5% chance that one would have found this particular value if the true value were zero, then it is said that the estimate is \"significant at the 5% level\". If not, then it is said that the estimate is \"insignificant at the 5% level\".\n\nNote, however, that the less precise phrase \"economically significant\" is sometimes used by economists to mean something very similar to the common usage of \"significant\". If the effect of the minimum wage on hiring decisions were found to be very small and yet the numerical result is very unlikely to have occurred only by chance, then the estimated effect is said to be statistically significant but not significant economically.\n\nIn common usage \"biased\" generally means \"prejudiced\". In econometrics, the estimate of the effect of one thing on another (say, the estimate of the effect of the minimum wage upon employment decisions) is said to be \"biased\" if the technique that was used to obtain the estimate has the effect that, \"a priori\", the expected value of the estimated effect differs from the true effect, whatever the latter may be. In this case the technique, as well as the estimate obtained with the technique, is called \"biased\". Researchers are likely to view a biased estimate with suspicion.\n\nIn general usage \"elasticity\" refers to flexibility. In economics it refers to a quantitative measurement of the degree of flexibility of something in response to something else. For example, the \"elasticity of demand with respect to income\" or the \"income elasticity of demand\" for a product refers to the percentage change in the quantity of the product demanded in response to a 1% change in consumers' income, or more generally to the ratio of the percentage change in quantity demanded to the percentage change in income. The change in the denominator always causes the change in the numerator, so the elasticity can be said to be the ratio of a percentage change that is caused to the percentage change of something that is causative.\n\nIn general usage, one is said to be rational if one is sane or lucid. In economics, rationality means that an economic agent specifies, or acts as if he implicitly specifies, a way to characterize his or someone's well-being, and then takes into account all relevant information in making choices so as to optimize that well-being. For example, an individual consumer is assumed to be rational in the sense that he maximizes a utility function, which expresses his subjective sense of well-being as a function of the amounts of various goods he consumes; firms are assumed to maximize profit or some related goal. Economists assume that in the presence of uncertainty, an agent is rational in the sense of specifying a way of evaluating sets of possible outcomes (and associated probabilities) with some function: A consumer is assumed to choose his consumption levels of various goods so as to pick the set of possible outcomes, and associated probabilities, that maximizes this function, which is often assumed to be the expected value of a von Neumann-Morgenstern utility function; a firm is often assumed to maximize the expected value of profit.\n"}
{"id": "34707510", "url": "https://en.wikipedia.org/wiki?curid=34707510", "title": "Euphenics", "text": "Euphenics\n\nEuphenics, which literally means \"good appearance\" or \"normal appearing\", is the science of making phenotypic improvements to humans after birth, generally to affect a problematic genetic condition.\n\nIn the early 1960s, Joshua Lederberg invented the term euphenics to differentiate the practice from eugenics, which was widely unpopular at the time. He emphasized that the genetic manipulation he described was intended to work on phenotype rather than genotype; he felt it was more feasible to positively change an individual's genetics rather than attempt to change the course of evolution as eugenics proposed. Theodosius Dobzhansky, an outspoken proponent of euphenics, argued that by improving genetic conditions so that people could live normal, healthy lives, people could lessen the impact of genetic conditions, thus lowering future interest in eugenics or other kinds of genetic manipulation.\n\nIn the 1970s, considerable effort was put towards the developing field of euphenics since it was seen as a positive form of genetic engineering. One of the first publicized applications of euphenics was the use of vitamins containing folic acid during pregnancy to combat neural-tube deficiencies such as spina bifida in the 1970s. However, medical science had been using euphenic strategies years before the term itself was coined. Euphenics is used today in the medical community to more generally refer to methods of affecting a genetic condition in a positive manner through diet, lifestyle or environment, such as the use of insulin to control diabetes or installation of a pacemaker to offset a heart defect.\n"}
{"id": "45587891", "url": "https://en.wikipedia.org/wiki?curid=45587891", "title": "Foros (crater)", "text": "Foros (crater)\n\nForos is an impact crater just northeast of Argyre basin in the Argyre quadrangle of Mars, located at 33.4°S and 28.87°W. It is 24.54 km in diameter and was named after Foros, a town in the Ukraine. The name was approved in 1979 by the International Astronomical Union (IAU) Working Group for Planetary System Nomenclature (WGPSN). At the time of the naming, the place was inside the Soviet Union until 1990.\n\nNearby named craters include the small Flateyri to the south and Zilar to the west-northwest. North of Foros is Pyrrhae Fossae which features a couple of troughs or trenches.\n"}
{"id": "5414614", "url": "https://en.wikipedia.org/wiki?curid=5414614", "title": "Gheorghe Munteanu-Murgoci", "text": "Gheorghe Munteanu-Murgoci\n\nGheorghe Munteanu Murgoci (July 20, 1872-March 5, 1925) was a renowned Romanian geologist, founder of the South-Eastern European Studies Institute in Bucharest. He was a member of the Romanian Academy.\n\nAs part of a group of professors, physicians, soldiers, etc., he helped bring Scouting to Romania.\n\nMunteanu Murgoci was a native of Măcin.\n"}
{"id": "34901337", "url": "https://en.wikipedia.org/wiki?curid=34901337", "title": "Granollers Museum of Natural Sciences", "text": "Granollers Museum of Natural Sciences\n\nThe Granollers Museum of Natural Sciences, in Granollers (in Vallès Oriental, Spain), covers the fields of palaeontology, geology, botany, meteorology, and, in particular, zoology. In addition, the Museum is the headquarters of the Montseny Natural Park Documentation Centre (natural sciences section) and of the Catalan Butterfly Monitoring Scheme; it also manages the Granollers Meteorological Station and the Can Cabanyes Environmental Education Centre. The Granollers Museum of Natural Sciences, part of the Barcelona Provincial Council Local Museum Network, was from October 2008 to May 2012.\n\nThe Catalan Butterfly Monitoring Scheme is a project to monitor the butterfly populations in order to detect more precisely changes in their population based on weekly repetitions of visual census using fixed transects, to later relate said changes to the different environmental factors.\n\nThe Granollers Meteorological Station has worked uninterruptedly since its creation, in 1950, by the Granollers Museum, and is part of the network of the National Institute of Meteorology of the Spanish Ministry of the Environment and Rural and Marine Affairs.\n\nThe Can Cabanyes natural area is located to the south of Granollers, between the Montmeló road and the right bank of the Congost River, and gets its name from the farmhouse once located on the property. Both environmental recovery and landscape improvement measures, such as riverbank recovery, the regeneration of existing woodland and the creation of artificial wetlands, have been carried out, which has led to an increase in the naturalisation and environmental diversification of the riverbank.\n\n\n"}
{"id": "6572686", "url": "https://en.wikipedia.org/wiki?curid=6572686", "title": "Gwyddion (software)", "text": "Gwyddion (software)\n\nGwyddion is a multiplatform modular free software for visualization and analysis of data from scanning probe microscopy (SPM) techniques (like AFM, MFM, STM, SNOM/NSOM). The project is led by its main developers David Nečas (Yeti) and Petr Klapetek who work together with several various developers across the world. The software is made available as free software under the terms of the GNU General Public License.\nThe name “Gwyddion” is that of a prominent god of Welsh Mythology, see \"Gwydion\"\n\nIt is created for the analysis of height fields and other 2D (image) data. While it is primarily intended for data originating from scanning probe microscopy techniques (like AFM, MFM, STM, SNOM/NSOM), it may also be used for the analysis of profilometry data, for instance.\nData is calculated and stored in the native file format (codice_1) in double precision.\n\nGwyddion supports many different file types and performs many image-based functions. Among others it can open and save the following graphics file formats:\n\nQuote from homepage of Gwyddion: 'Gwyddion uses a fairly general physical unit system, there are no principal limitations on the types of physical quantities data (and lateral dimensions) can represent. Units of slopes, areas, volumes, and other derived quantities are correctly calculated. SI unit system is used whenever possible.\n\nTools and other dialogs remember their parameters, not only between tool invocations during one session, but also between sessions. Gwyddion native file format (codice_1) supports saving all data specific settings: false color palette, masks, presentations, selections, associated 3D view parameters, graphs associated with that data and their settings, etc.'\n\nGwyddion is mainly developed on Linux platform using GNU set of compilers and utilities. Its graphical user interface is based on the popular interface toolkit \"GTK+\".\n\nIt is available for Linux platforms and has been ported to other unix flavors that has support the GNU Autotools or its equivalent. The Windows version is slightly incomplete due to limitations of the platform, but supports nearly all major features. The Mac OS version can be built using Xcode, and some pre-built binaries are available.\n\nApple Darwin, or OpenDarwin is the only major platform Gwyddion has not been thoroughly tested on. Gwyddion program could also be ported to other branches of the BSD operating system.\n\n"}
{"id": "14400", "url": "https://en.wikipedia.org/wiki?curid=14400", "title": "History of science", "text": "History of science\n\nThe History of science is the study of the development of science and scientific knowledge, including both the natural and social sciences. (The history of the arts and humanities is termed history of scholarship.) Science is a body of empirical, theoretical, and practical knowledge about the natural world, produced by scientists who emphasize the observation, explanation, and prediction of real world phenomena. Historiography of science, in contrast, studies the methods employed by historians of science.\n\nThe English word \"scientist\" is relatively recent—first coined by William Whewell in the 19th century. Previously, investigators of nature called themselves \"natural philosophers\". While empirical investigations of the natural world have been described since classical antiquity (for example by Thales and Aristotle), and scientific method has been employed since the Middle Ages (for example, by Ibn al-Haytham and Roger Bacon), modern science began to develop in the early modern period, and in particular in the scientific revolution of 16th- and 17th-century Europe. Traditionally, historians of science have defined science sufficiently broadly to include those earlier inquiries.\n\nFrom the 18th century through late 20th century, the history of science, especially of the physical and biological sciences, was often presented as a progressive accumulation of knowledge, in which true theories replaced false beliefs. Some more recent historical interpretations, such as those of Thomas Kuhn, tend to portray the history of science in terms of competing paradigms or conceptual systems in a wider matrix of intellectual, cultural, economic and political trends. These interpretations, however, have met with opposition for they also portray history of science as an incoherent system of incommensurable paradigms, not leading to any scientific progress, but only to the illusion of progress.\n\nIn prehistoric times, technique and knowledge were passed from generation to generation in an oral tradition. For example, the domestication of maize for agriculture has been dated to about 9,000 years ago in southern Mexico, before the development of writing systems. Similarly, archaeological evidence indicates the development of astronomical knowledge in preliterate societies. The development of writing enabled knowledge to be stored and communicated across generations with much greater fidelity.\n\nMany ancient civilizations systematically collected astronomical observations. Rather than speculate on the material nature of the planets and stars, the ancients charted the relative positions of celestial bodies, often inferring their influence on human society. This demonstrates how ancient investigators generally employed a holistic intuition, assuming the interconnectedness of all things, whereas modern science rejects such conceptual leaps.\n\nBasic facts about human physiology were known in some places, and alchemy was practiced in several civilizations. Considerable observation of macroscopic flora and fauna was also performed.\n\nThe ancient Mesopotamians had no distinction between \"rational science\" and magic. When a person became ill, doctors prescribed magical formulas to be recited as well as medicinal treatments. The earliest medical prescriptions appear in Sumerian during the Third Dynasty of Ur ( 2112 BC – 2004 BC). The most extensive Babylonian medical text, however, is the \"Diagnostic Handbook\" written by the \"ummânū\", or chief scholar, Esagil-kin-apli of Borsippa, during the reign of the Babylonian king Adad-apla-iddina (1069–1046 BC). In East Semitic cultures, the main medicinal authority was a kind of exorcist-healer known as an \"āšipu\". The profession was generally passed down from father to son and was held in extremely high regard. Of less frequent recourse was another kind of healer known as an \"asu\", who corresponds more closely to a modern physician and treated physical symptoms using primarily folk remedies composed of various herbs, animal products, and minerals, as well as potions, enemas, and ointments or poultices. These physicians, who could be either male or female, also dressed wounds, set limbs, and performed simple surgeries. The ancient Mesopotamians also practiced prophylaxis and took measures to prevent the spread of disease.\n\nThe ancient Mesopotamians had extensive knowledge about the chemical properties of clay, sand, metal ore, bitumen, stone, and other natural materials, and applied this knowledge to practical use in manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing. Metallurgy required scientific knowledge about the properties of metals. Nonetheless, the Mesopotamians seem to have had little interest in gathering information about the natural world for the mere sake of gathering information and were far more interested in studying the manner in which the gods had ordered the universe. Biology of non-human organisms was generally only written about in the context of mainstream academic disciplines. Animal physiology was studied extensively for the purpose of divination; the anatomy of the liver, which was seen as an important organ in haruspicy, was studied in particularly intensive detail. Animal behavior was also studied for divinatory purposes. Most information about the training and domestication of animals was probably transmitted orally without being written down, but one text dealing with the training of horses has survived. The Mesopotamian cuneiform tablet Plimpton 322, dating to the eighteenth century BC, records a number of Pythagorean triplets (3,4,5) (5,12,13) ..., hinting that the ancient Mesopotamians might have been aware of the Pythagorean theorem over a millennium before Pythagoras.\n\nIn Babylonian astronomy, records of the motions of the stars, planets, and the moon are left on thousands of clay tablets created by scribes. Even today, astronomical periods identified by Mesopotamian proto-scientists are still widely used in Western calendars such as the solar year and the lunar month. Using these data they developed arithmetical methods to compute the changing length of daylight in the course of the year and to predict the appearances and disappearances of the Moon and planets and eclipses of the Sun and Moon. Only a few astronomers' names are known, such as that of Kidinnu, a Chaldean astronomer and mathematician. Kiddinu's value for the solar year is in use for today's calendars. Babylonian astronomy was \"the first and highly successful attempt at giving a refined mathematical description of astronomical phenomena.\" According to the historian A. Aaboe, \"all subsequent varieties of scientific astronomy, in the Hellenistic world, in India, in Islam, and in the West—if not indeed all subsequent endeavour in the exact sciences—depend upon Babylonian astronomy in decisive and fundamental ways.\"\n\nAncient Egypt made significant advances in astronomy, mathematics and medicine. Their development of geometry was a necessary outgrowth of surveying to preserve the layout and ownership of farmland, which was flooded annually by the Nile river. The 3-4-5 right triangle and other rules of geometry were used to build rectilinear structures, and the post and lintel architecture of Egypt. Egypt was also a center of alchemy research for much of the Mediterranean. The Edwin Smith papyrus is one of the first medical documents still extant, and perhaps the earliest document that attempts to describe and analyse the brain: it might be seen as the very beginnings of modern neuroscience. However, while Egyptian medicine had some effective practices, it was often ineffective and sometimes harmful. Medical historians believe that ancient Egyptian pharmacology, for example, was largely ineffective. Nevertheless, it applied the following components to the treatment of disease: examination, diagnosis, treatment, and prognosis, which display strong parallels to the basic empirical method of science and, according to G.E.R. Lloyd, played a significant role in the development of this methodology. The Ebers papyrus (c. 1550 BC) also contains evidence of traditional empiricism.\n\nIn Classical Antiquity, the inquiry into the workings of the universe took place both in investigations aimed at such practical goals as establishing a reliable calendar or determining how to cure a variety of illnesses and in those abstract investigations known as natural philosophy. The ancient people who are considered the first \"scientists\" may have thought of themselves as \"natural philosophers\", as practitioners of a skilled profession (for example, physicians), or as followers of a religious tradition (for example, temple healers).\n\nThe earliest Greek philosophers, known as the pre-Socratics, provided competing answers to the question found in the myths of their neighbors: \"How did the ordered cosmos in which we live come to be?\" The pre-Socratic philosopher Thales (640–546 BC), dubbed the \"father of science\", was the first to postulate non-supernatural explanations for natural phenomena. For example, that land floats on water and that earthquakes are caused by the agitation of the water upon which the land floats, rather than the god Poseidon. Thales' student Pythagoras of Samos founded the Pythagorean school, which investigated mathematics for its own sake, and was the first to postulate that the Earth is spherical in shape. Leucippus (5th century BC) introduced atomism, the theory that all matter is made of indivisible, imperishable units called atoms. This was greatly expanded on by his pupil Democritus and later Epicurus.\n\nSubsequently, Plato and Aristotle produced the first systematic discussions of natural philosophy, which did much to shape later investigations of nature. Their development of deductive reasoning was of particular importance and usefulness to later scientific inquiry. Plato founded the Platonic Academy in 387 BC, whose motto was \"Let none unversed in geometry enter here\", and turned out many notable philosophers. Plato's student Aristotle introduced empiricism and the notion that universal truths can be arrived at via observation and induction, thereby laying the foundations of the scientific method. Aristotle also produced many biological writings that were empirical in nature, focusing on biological causation and the diversity of life. He made countless observations of nature, especially the habits and attributes of plants and animals on Lesbos, classified more than 540 animal species, and dissected at least 50. Aristotle's writings profoundly influenced subsequent Islamic and European scholarship, though they were eventually superseded in the Scientific Revolution.\n\nThe important legacy of this period included substantial advances in factual knowledge, especially in anatomy, zoology, botany, mineralogy, geography, mathematics and astronomy; an awareness of the importance of certain scientific problems, especially those related to the problem of change and its causes; and a recognition of the methodological importance of applying mathematics to natural phenomena and of undertaking empirical research. In the Hellenistic age scholars frequently employed the principles developed in earlier Greek thought: the application of mathematics and deliberate empirical research, in their scientific investigations. Thus, clear unbroken lines of influence lead from ancient Greek and Hellenistic philosophers, to medieval Muslim philosophers and scientists, to the European Renaissance and Enlightenment, to the secular sciences of the modern day.\nNeither reason nor inquiry began with the Ancient Greeks, but the Socratic method did, along with the idea of Forms, great advances in geometry, logic, and the natural sciences. According to Benjamin Farrington, former Professor of Classics at Swansea University:\n\nand again:\n\nThe astronomer Aristarchus of Samos was the first known person to propose a heliocentric model of the solar system, while the geographer Eratosthenes accurately calculated the circumference of the Earth. Hipparchus (c. 190 – c. 120 BC) produced the first systematic star catalog. The level of achievement in Hellenistic astronomy and engineering is impressively shown by the Antikythera mechanism (150–100 BC), an analog computer for calculating the position of planets. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe.\n\nIn medicine, Hippocrates (c. 460 BC – c. 370 BC) and his followers were the first to describe many diseases and medical conditions and developed the Hippocratic Oath for physicians, still relevant and in use today. Herophilos (335–280 BC) was the first to base his conclusions on dissection of the human body and to describe the nervous system. Galen (129 – c. 200 AD) performed many audacious operations—including brain and eye surgeries— that were not tried again for almost two millennia.\n\nIn Hellenistic Egypt, the mathematician Euclid laid down the foundations of mathematical rigor and introduced the concepts of definition, axiom, theorem and proof still in use today in his \"Elements\", considered the most influential textbook ever written. Archimedes, considered one of the greatest mathematicians of all time, is credited with using the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of Pi. He is also known in physics for laying the foundations of hydrostatics, statics, and the explanation of the principle of the lever.\n\nTheophrastus wrote some of the earliest descriptions of plants and animals, establishing the first taxonomy and looking at minerals in terms of their properties such as hardness. Pliny the Elder produced what is one of the largest encyclopedias of the natural world in 77 AD, and must be regarded as the rightful successor to Theophrastus. For example, he accurately describes the octahedral shape of the diamond, and proceeds to mention that diamond dust is used by engravers to cut and polish other gems owing to its great hardness. His recognition of the importance of crystal shape is a precursor to modern crystallography, while mention of numerous other minerals presages mineralogy. He also recognises that other minerals have characteristic crystal shapes, but in one example, confuses the crystal habit with the work of lapidaries. He was also the first to recognise that amber was a fossilized resin from pine trees because he had seen samples with trapped insects within them.\n\nMathematics: The earliest traces of mathematical knowledge in the Indian subcontinent appear with the Indus Valley Civilization (c. 4th millennium BC ~ c. 3rd millennium BC). The people of this civilization made bricks whose dimensions were in the proportion 4:2:1, considered favorable for the stability of a brick structure. They also tried to standardize measurement of length to a high degree of accuracy. They designed a ruler—the \"Mohenjo-daro ruler\"—whose unit of length (approximately 1.32 inches or 3.4 centimetres) was divided into ten equal parts. Bricks manufactured in ancient Mohenjo-daro often had dimensions that were integral multiples of this unit of length.\n\nIndian astronomer and mathematician Aryabhata (476–550), in his \"Aryabhatiya\" (499) introduced a number of trigonometric functions (including sine, versine, cosine and inverse sine), trigonometric tables, and techniques and algorithms of algebra. In 628 AD, Brahmagupta suggested that gravity was a force of attraction. He also lucidly explained the use of zero as both a placeholder and a decimal digit, along with the Hindu-Arabic numeral system now used universally throughout the world. Arabic translations of the two astronomers' texts were soon available in the Islamic world, introducing what would become Arabic numerals to the Islamic world by the 9th century. During the 14th–16th centuries, the Kerala school of astronomy and mathematics made significant advances in astronomy and especially mathematics, including fields such as trigonometry and analysis. In particular, Madhava of Sangamagrama is considered the \"founder of mathematical analysis\".\n\nAstronomy: The first textual mention of astronomical concepts comes from the Vedas, religious literature of India. According to Sarma (2008): \"One finds in the Rigveda intelligent speculations about the genesis of the universe from nonexistence, the configuration of the universe, the spherical self-supporting earth, and the year of 360 days divided into 12 equal parts of 30 days each with a periodical intercalary month.\". The first 12 chapters of the \"Siddhanta Shiromani\", written by Bhāskara in the 12th century, cover topics such as: mean longitudes of the planets; true longitudes of the planets; the three problems of diurnal rotation; syzygies; lunar eclipses; solar eclipses; latitudes of the planets; risings and settings; the moon's crescent; conjunctions of the planets with each other; conjunctions of the planets with the fixed stars; and the patas of the sun and moon. The 13 chapters of the second part cover the nature of the sphere, as well as significant astronomical and trigonometric calculations based on it.\n\nNilakantha Somayaji's astronomical treatise the Tantrasangraha similar in nature to the Tychonic system proposed by Tycho Brahe had been the most accurate astronomical model until the time of Johannes Kepler in the 17th century.\n\nLinguistics: Some of the earliest linguistic activities can be found in Iron Age India (1st millennium BC) with the analysis of Sanskrit for the purpose of the correct recitation and interpretation of Vedic texts. The most notable grammarian of Sanskrit was (c. 520–460 BC), whose grammar formulates close to 4,000 rules which together form a compact generative grammar of Sanskrit. Inherent in his analytic approach are the concepts of the phoneme, the morpheme and the root.\n\nMedicine: Findings from Neolithic graveyards in what is now Pakistan show evidence of proto-dentistry among an early farming culture. Ayurveda is a system of traditional medicine that originated in ancient India before 2500 BC, and is now practiced as a form of alternative medicine in other parts of the world. Its most famous text is the Suśrutasamhitā of Suśruta, which is notable for describing procedures on various forms of surgery, including rhinoplasty, the repair of torn ear lobes, perineal lithotomy, cataract surgery, and several other excisions and other surgical procedures.\n\nMetallurgy: The wootz, crucible and stainless steels were invented in India, and were widely exported in Classic Mediterranean world. It was known from Pliny the Elder as \"ferrum indicum\". Indian Wootz steel was held in high regard in Roman Empire, was often considered to be the best. After in Middle Age it was imported in Syria to produce with special techniques the \"Damascus steel\" by the year 1000.\nThe Hindus excel in the manufacture of iron, and in the preparations of those ingredients along with which it is fused to obtain that kind of soft iron which is usually styled Indian steel (Hindiah). They also have workshops wherein are forged the most famous sabres in the world.\n\nMathematics: From the earliest the Chinese used a positional decimal system on counting boards in order to calculate. To express 10, a single rod is placed in the second box from the right. The spoken language uses a similar system to English: e.g. four thousand two hundred seven. No symbol was used for zero. By the 1st century BC, negative numbers and decimal fractions were in use and \"The Nine Chapters on the Mathematical Art\" included methods for extracting higher order roots by Horner's method and solving linear equations and by Pythagoras' theorem. Cubic equations were solved in the Tang dynasty and solutions of equations of order higher than 3 appeared in print in 1245 AD by Ch'in Chiu-shao. Pascal's triangle for binomial coefficients was described around 1100 by Jia Xian.\n\nAlthough the first attempts at an axiomatisation of geometry appear in the Mohist canon in 330 BC, Liu Hui developed algebraic methods in geometry in the 3rd century AD and also calculated pi to 5 significant figures. In 480, Zu Chongzhi improved this by discovering the ratio formula_1 which remained the most accurate value for 1200 years.\n\nAstronomy: Astronomical observations from China constitute the longest continuous sequence from any civilisation and include records of sunspots (112 records from 364 BC), supernovas (1054), lunar and solar eclipses. By the 12th century, they could reasonably accurately make predictions of eclipses, but the knowledge of this was lost during the Ming dynasty, so that the Jesuit Matteo Ricci gained much favour in 1601 by his predictions.\nBy 635 Chinese astronomers had observed that the tails of comets always point away from the sun.\n\nFrom antiquity, the Chinese used an equatorial system for describing the skies and a star map from 940 was drawn using a cylindrical (Mercator) projection. The use of an armillary sphere is recorded from the 4th century BC and a sphere permanently mounted in equatorial axis from 52 BC. In 125 AD Zhang Heng used water power to rotate the sphere in real time. This included rings for the meridian and ecliptic. By 1270 they had incorporated the principles of the Arab torquetum.\n\nSeismology: To better prepare for calamities, Zhang Heng invented a seismometer in 132 CE which provided instant alert to authorities in the capital Luoyang that an earthquake had occurred in a location indicated by a specific cardinal or ordinal direction. Although no tremors could be felt in the capital when Zhang told the court that an earthquake had just occurred in the northwest, a message came soon afterwards that an earthquake had indeed struck 400 km (248 mi) to 500 km (310 mi) northwest of Luoyang (in what is now modern Gansu). Zhang called his device the 'instrument for measuring the seasonal winds and the movements of the Earth' (Houfeng didong yi 候风地动仪), so-named because he and others thought that earthquakes were most likely caused by the enormous compression of trapped air. See Zhang's seismometer for further details.\n\nThere are many notable contributors to the field of Chinese science throughout the ages. One of the best examples would be Shen Kuo (1031–1095), a polymath scientist and statesman who was the first to describe the magnetic-needle compass used for navigation, discovered the concept of true north, improved the design of the astronomical gnomon, armillary sphere, sight tube, and clepsydra, and described the use of drydocks to repair boats. After observing the natural process of the inundation of silt and the find of marine fossils in the Taihang Mountains (hundreds of miles from the Pacific Ocean), Shen Kuo devised a theory of land formation, or geomorphology. He also adopted a theory of gradual climate change in regions over time, after observing petrified bamboo found underground at Yan'an, Shaanxi province. If not for Shen Kuo's writing, the architectural works of Yu Hao would be little known, along with the inventor of movable type printing, Bi Sheng (990–1051). Shen's contemporary Su Song (1020–1101) was also a brilliant polymath, an astronomer who created a celestial atlas of star maps, wrote a pharmaceutical treatise with related subjects of botany, zoology, mineralogy, and metallurgy, and had erected a large astronomical clocktower in Kaifeng city in 1088. To operate the crowning armillary sphere, his clocktower featured an escapement mechanism and the world's oldest known use of an endless power-transmitting chain drive.\n\nThe Jesuit China missions of the 16th and 17th centuries \"learned to appreciate the scientific achievements of this ancient culture and made them known in Europe. Through their correspondence European scientists first learned about the Chinese science and culture.\" Western academic thought on the history of Chinese technology and science was galvanized by the work of Joseph Needham and the Needham Research Institute. Among the technological accomplishments of China were, according to the British scholar Needham, early seismological detectors (Zhang Heng in the 2nd century), the water-powered celestial globe (Zhang Heng), matches, the independent invention of the decimal system, dry docks, sliding calipers, the double-action piston pump, cast iron, the blast furnace, the iron plough, the multi-tube seed drill, the wheelbarrow, the suspension bridge, the winnowing machine, the rotary fan, the parachute, natural gas as fuel, the raised-relief map, the propeller, the crossbow, and a solid fuel rocket, the multistage rocket, the horse collar, along with contributions in logic, astronomy, medicine, and other fields.\n\nHowever, cultural factors prevented these Chinese achievements from developing into what we might call \"modern science\". According to Needham, it may have been the religious and philosophical framework of Chinese intellectuals which made them unable to accept the ideas of laws of nature:\nIn the Middle Ages the classical learning continued in three major linguistic cultures and civilizations: Greek (the Byzantine Empire), Arabic (the Islamic world), and Latin (Western Europe).\n\nBecause of the collapse of the Western Roman Empire, the intellectual level in the western part of Europe declined in the 400s. In contrast, the Eastern Roman or Byzantine Empire resisted the barbarian attacks, and preserved and improved the learning.\n\nWhile the Byzantine Empire still held learning centers such as Constantinople, Alexandria and Antioch, Western Europe's knowledge was concentrated in monasteries until the development of medieval universities in the 12th centuries. The curriculum of monastic schools included the study of the few available ancient texts and of new works on practical subjects like medicine and timekeeping.\n\nIn the sixth century in the Byzantine Empire, Isidore of Miletus compiled Archimedes' mathematical works in the Archimedes Palimpsest, where all Archimedes' mathematical contributions were collected and studied.\n\nJohn Philoponus, another Byzantine scholar, was the first to question Aristotle's teaching of physics, introducing the theory of impetus. The theory of impetus was an auxiliary or secondary theory of Aristotelian dynamics, put forth initially to explain projectile motion against gravity. It is the intellectual precursor to the concepts of inertia, momentum and acceleration in classical mechanics. The works of John Philoponus inspired Galileo Galilei ten centuries later.\n\nThe first record of separating conjoined twins took place in the Byzantine Empire in the 900s when the surgeons tried to separate a dead body of a pair of conjoined twins. The result was partly successful as the other twin managed to live for three days. The next recorded case of separating conjoined twins was several centuries later, in 1600s Germany.\n\nDuring the Fall of Constantinople in 1453, a number of Greek scholars flee to North Italy in which they fueled the era later commonly known as \"Renaissance” as they brought with them a great deal of classical learning including an understanding of botany, medicine, and zoology. Byzantium also gave the West important inputs: John Philoponus' criticism of Aristotelian physics, and the works of Dioscorides.\n\nIn the Middle East, Greek philosophy was able to find some support under the newly created Arab Empire. With the spread of Islam in the 7th and 8th centuries, a period of Muslim scholarship, known as the Islamic Golden Age, lasted until the 13th century. This scholarship was aided by several factors. The use of a single language, Arabic, allowed communication without need of a translator. Access to Greek texts from the Byzantine Empire, along with Indian sources of learning, provided Muslim scholars a knowledge base to build upon.\n\nScientific method began developing in the Muslim world, where significant progress in methodology was made, beginning with the experiments of Ibn al-Haytham (Alhazen) on optics from c. 1000, in his \"Book of Optics\". The most important development of the scientific method was the use of experiments to distinguish between competing scientific theories set within a generally empirical orientation, which began among Muslim scientists. Ibn al-Haytham is also regarded as the father of optics, especially for his empirical proof of the intromission theory of light. Some have also described Ibn al-Haytham as the \"first scientist\" for his development of the modern scientific method.\n\nIn mathematics, the mathematician Muhammad ibn Musa al-Khwarizmi (c. 780–850) gave his name to the concept of the algorithm, while the term algebra is derived from \"al-jabr\", the beginning of the title of one of his publications. What is now known as Arabic numerals originally came from India, but Muslim mathematicians made several key refinements to the number system, such as the introduction of decimal point notation.\n\nIn astronomy, Al-Battani (c. 858–929) improved the measurements of Hipparchus, preserved in the translation of Ptolemy's \"Hè Megalè Syntaxis\" (\"The great treatise\") translated as \"Almagest\". Al-Battani also improved the precision of the measurement of the precession of the Earth's axis. The corrections made to the geocentric model by al-Battani, Ibn al-Haytham, Averroes and the Maragha astronomers such as Nasir al-Din al-Tusi, Mo'ayyeduddin Urdi and Ibn al-Shatir are similar to Copernican heliocentric model. Heliocentric theories may have also been discussed by several other Muslim astronomers such as Ja'far ibn Muhammad Abu Ma'shar al-Balkhi, Abu-Rayhan Biruni, Abu Said al-Sijzi, Qutb al-Din al-Shirazi, and Najm al-Dīn al-Qazwīnī al-Kātibī.\n\nMuslim chemists and alchemists played an important role in the foundation of modern chemistry. Scholars such as Will Durant and Fielding H. Garrison considered Muslim chemists to be the founders of chemistry. In particular, Jābir ibn Hayyān (c. 721–815) is \"considered by many to be the father of chemistry\". The works of Arabic scientists influenced Roger Bacon (who introduced the empirical method to Europe, strongly influenced by his reading of Persian writers), and later Isaac Newton. The scholar Al-Razi contributed to chemistry and medicine.\n\nIbn Sina (Avicenna, c. 980–1037) is regarded as the most influential philosopher of Islam. He pioneered the science of experimental medicine and was the first physician to conduct clinical trials. His two most notable works in medicine are the \"Kitāb al-shifāʾ\" (\"Book of Healing\") and The Canon of Medicine, both of which were used as standard medicinal texts in both the Muslim world and in Europe well into the 17th century. Amongst his many contributions are the discovery of the contagious nature of infectious diseases, and the introduction of clinical pharmacology.\n\nScientists from the Islamic world include al-Farabi (polymath), Abu al-Qasim al-Zahrawi (pioneer of surgery), Abū Rayhān al-Bīrūnī (pioneer of Indology, geodesy and anthropology), Nasīr al-Dīn al-Tūsī (polymath), and Ibn Khaldun (forerunner of social sciences such as demography, cultural history, historiography, philosophy of history and sociology), among many others.\n\nIslamic science began its decline in the 12th or 13th century, before the Renaissance in Europe, and due in part to the 11th–13th century Mongol conquests, during which libraries, observatories, hospitals and universities were destroyed. The end of the Islamic Golden Age is marked by the destruction of the intellectual center of Baghdad, the capital of the Abbasid caliphate in 1258.\n\nBy the eleventh century, most of Europe had become Christian; stronger monarchies emerged; borders were restored; technological developments and agricultural innovations were made, increasing the food supply and population. Classical Greek texts were translated from Arabic and Greek into Latin, stimulating scientific discussion in Western Europe.\n\nAn intellectual revitalization of Western Europe started with the birth of medieval universities in the 12th century. Contact with the Byzantine Empire, and with the Islamic world during the Reconquista and the Crusades, allowed Latin Europe access to scientific Greek and Arabic texts, including the works of Aristotle, Ptolemy, Isidore of Miletus, John Philoponus, Jābir ibn Hayyān, al-Khwarizmi, Alhazen, Avicenna, and Averroes. European scholars had access to the translation programs of Raymond of Toledo, who sponsored the 12th century Toledo School of Translators from Arabic to Latin. Later translators like Michael Scotus would learn Arabic in order to study these texts directly. The European universities aided materially in the translation and propagation of these texts and started a new infrastructure which was needed for scientific communities. In fact, European university put many works about the natural world and the study of nature at the center of its curriculum, with the result that the \"medieval university laid far greater emphasis on science than does its modern counterpart and descendent.\"\n\nIn classical antiquity, Greek and Roman taboos had meant that dissection was usually banned, but in the Middle Ages medical teachers and students at Bologna began to open human bodies, and Mondino de Luzzi (c. 1275–1326) produced the ﬁrst known anatomy textbook based on human dissection.\n\nAs a result of the Pax Mongolica, Europeans, such as Marco Polo, began to venture further and further east. This led to the increased awareness of Indian and even Chinese culture and civilization within the European tradition. Technological advances were also made, such as the early flight of Eilmer of Malmesbury (who had studied Mathematics in 11th century England), and the metallurgical achievements of the Cistercian blast furnace at Laskill.\n\nAt the beginning of the 13th century, there were reasonably accurate Latin translations of the main works of almost all the intellectually crucial ancient authors, allowing a sound transfer of scientific ideas via both the universities and the monasteries. By then, the natural philosophy in these texts began to be extended by scholastics such as Robert Grosseteste, Roger Bacon, Albertus Magnus and Duns Scotus. Precursors of the modern scientific method, influenced by earlier contributions of the Islamic world, can be seen already in Grosseteste's emphasis on mathematics as a way to understand nature, and in the empirical approach admired by Bacon, particularly in his \"Opus Majus\". Pierre Duhem's thesis is that Stephen Tempier - the Bishop of Paris - Condemnation of 1277 led to the study of medieval science as a serious discipline, \"but no one in the field any longer endorses his view that modern science started in 1277\". However, many scholars agree with Duhem's view that the Middle Ages saw important scientific developments.\n\nThe first half of the 14th century saw much important scientific work, largely within the framework of scholastic commentaries on Aristotle's scientific writings. William of Ockham emphasised the principle of parsimony: natural philosophers should not postulate unnecessary entities, so that motion is not a distinct thing but is only the moving object and an intermediary \"sensible species\" is not needed to transmit an image of an object to the eye. Scholars such as Jean Buridan and Nicole Oresme started to reinterpret elements of Aristotle's mechanics. In particular, Buridan developed the theory that impetus was the cause of the motion of projectiles, which was a first step towards the modern concept of inertia. The Oxford Calculators began to mathematically analyze the kinematics of motion, making this analysis without considering the causes of motion.\n\nIn 1348, the Black Death and other disasters sealed a sudden end to philosophic and scientific development. Yet, the rediscovery of ancient texts was stimulated by the Fall of Constantinople in 1453, when many Byzantine scholars sought refuge in the West. Meanwhile, the introduction of printing was to have great effect on European society. The facilitated dissemination of the printed word democratized learning and allowed ideas such as algebra to propagate more rapidly. These developments paved the way for the Scientific Revolution, where scientific inquiry, halted at the start of the Black Death, resumed.\n\nThe renewal of learning in Europe began with 12th century Scholasticism. The Northern Renaissance showed a decisive shift in focus from Aristoteleian natural philosophy to chemistry and the biological sciences (botany, anatomy, and medicine). Thus modern science in Europe was resumed in a period of great upheaval: the Protestant Reformation and Catholic Counter-Reformation; the discovery of the Americas by Christopher Columbus; the Fall of Constantinople; but also the re-discovery of Aristotle during the Scholastic period presaged large social and political changes. Thus, a suitable environment was created in which it became possible to question scientific doctrine, in much the same way that Martin Luther and John Calvin questioned religious doctrine. The works of Ptolemy (astronomy) and Galen (medicine) were found not always to match everyday observations. Work by Vesalius on human cadavers found problems with the Galenic view of anatomy.\n\nThe willingness to question previously held truths and search for new answers resulted in a period of major scientific advancements, now known as the Scientific Revolution. The Scientific Revolution is traditionally held by most historians to have begun in 1543, when the books \"De humani corporis fabrica\" (\"On the Workings of the Human Body\") by Andreas Vesalius, and also \"De Revolutionibus\", by the astronomer Nicolaus Copernicus, were first printed. The thesis of Copernicus' book was that the Earth moved around the Sun. The period culminated with the publication of the \"Philosophiæ Naturalis Principia Mathematica\" in 1687 by Isaac Newton, representative of the unprecedented growth of scientific publications throughout Europe.\n\nOther significant scientific advances were made during this time by Galileo Galilei, Edmond Halley, Robert Hooke, Christiaan Huygens, Tycho Brahe, Johannes Kepler, Gottfried Leibniz, and Blaise Pascal. In philosophy, major contributions were made by Francis Bacon, Sir Thomas Browne, René Descartes, and Thomas Hobbes. The scientific method was also better developed as the modern way of thinking emphasized experimentation and reason over traditional considerations.\n\nThe Age of Enlightenment was a European affair. The 17th century brought decisive steps towards modern science, which accelerated during the 18th century. Directly based on the works of Newton, Descartes, Pascal and Leibniz, the way was now clear to the development of modern mathematics, physics and technology\nby the generation of Benjamin Franklin (1706–1790), Leonhard Euler (1707–1783), Mikhail Lomonosov (1711–1765) and Jean le Rond d'Alembert (1717–1783). Denis Diderot's \"Encyclopédie\", published between 1751 and 1772 brought this new understanding to a wider audience. The impact of this process was not limited to science and technology, but affected philosophy (Immanuel Kant, David Hume), religion (the increasingly significant impact of science upon religion), and society and politics in general (Adam Smith, Voltaire). The early modern period is seen as a flowering of the European Renaissance, in what is often known as the Scientific Revolution, viewed as a foundation of modern science.\n\nThe Romantic Movement of the early 19th century reshaped science by opening up new pursuits unexpected in the classical approaches of the Enlightenment. Major breakthroughs came in biology, especially in Darwin's theory of evolution, as well as physics (electromagnetism), mathematics (non-Euclidean geometry, group theory) and chemistry (organic chemistry). The decline of Romanticism occurred because a new movement, Positivism, began to take hold of the ideals of the intellectuals after 1840 and lasted until about 1880.\n\nThe scientific revolution established science as a source for the growth of knowledge. During the 19th century, the practice of science became professionalized and institutionalized in ways that continued through the 20th century. As the role of scientific knowledge grew in society, it became incorporated with many aspects of the functioning of nation-states.\n\nThe scientific revolution is a convenient boundary between ancient thought and classical physics. Nicolaus Copernicus revived the heliocentric model of the solar system described by Aristarchus of Samos. This was followed by the first known model of planetary motion given by Johannes Kepler in the early 17th century, which proposed that the planets follow elliptical orbits, with the Sun at one focus of the ellipse. Galileo (\"Father of Modern Physics\") also made use of experiments to validate physical theories, a key element of the scientific method. William Gilbert did some of the earliest experiments with electricity and magnetism, establishing that the Earth itself is magnetic.\n\nIn 1687, Isaac Newton published the \"Principia Mathematica\", detailing two comprehensive and successful physical theories: Newton's laws of motion, which led to classical mechanics; and Newton's Law of Gravitation, which describes the fundamental force of gravity.\n\nDuring the late 18th and early 19th century, the behavior of electricity and magnetism was studied by Luigi Galvani, Giovanni Aldini, Alessandro Volta, Michael Faraday, Georg Ohm, and others. These studies led to the unification of the two phenomena into a single theory of electromagnetism, by James Clerk Maxwell (known as Maxwell's equations).\n\nThe beginning of the 20th century brought the start of a revolution in physics. The long-held theories of Newton were shown not to be correct in all circumstances. Beginning in 1900, Max Planck, Albert Einstein, Niels Bohr and others developed quantum theories to explain various anomalous experimental results, by introducing discrete energy levels. Not only did quantum mechanics show that the laws of motion did not hold on small scales, but even more disturbingly, the theory of general relativity, proposed by Einstein in 1915, showed that the fixed background of spacetime, on which both Newtonian mechanics and special relativity depended, could not exist. In 1925, Werner Heisenberg and Erwin Schrödinger formulated quantum mechanics, which explained the preceding quantum theories. The observation by Edwin Hubble in 1929 that the speed at which galaxies recede positively correlates with their distance, led to the understanding that the universe is expanding, and the formulation of the Big Bang theory by Georges Lemaître.\n\nIn 1938 Otto Hahn and Fritz Strassmann discovered nuclear fission with radiochemical methods, and in 1939 Lise Meitner and Otto Robert Frisch wrote the first theoretical interpretation of the fission process, which was later improved by Niels Bohr and John A. Wheeler. Further developments took place during World War II, which led to the practical application of radar and the development and use of the atomic bomb. Around this time, Chien-Shiung Wu was recruited by the Manhattan Project to help develop a process for separating uranium metal into U-235 and U-238 isotopes by Gaseous diffusion. She was an expert experimentalist in beta decay and weak interaction physics. Wu designed an experiment (see Wu experiment) that enabled theoretical physicists Tsung-Dao Lee and Chen-Ning Yang to disprove the law of parity experimentally, winning them a Nobel Prize in 1957.\n\nThough the process had begun with the invention of the cyclotron by Ernest O. Lawrence in the 1930s, physics in the postwar period entered into a phase of what historians have called \"Big Science\", requiring massive machines, budgets, and laboratories in order to test their theories and move into new frontiers. The primary patron of physics became state governments, who recognized that the support of \"basic\" research could often lead to technologies useful to both military and industrial applications.\n\nCurrently, general relativity and quantum mechanics are inconsistent with each other, and efforts are underway to unify the two.\n\nModern chemistry emerged from the sixteenth through the eighteenth centuries through the material practices and theories promoted by alchemy, medicine, manufacturing and mining. A decisive moment came when 'chemistry' was distinguished from alchemy by Robert Boyle in his work \"The Sceptical Chymist\", in 1661; although the alchemical tradition continued for some time after his work. Other important steps included the gravimetric experimental practices of medical chemists like William Cullen, Joseph Black, Torbern Bergman and Pierre Macquer and through the work of Antoine Lavoisier (\"Father of Modern Chemistry\") on oxygen and the law of conservation of mass, which refuted phlogiston theory. The theory that all matter is made of atoms, which are the smallest constituents of matter that cannot be broken down without losing the basic chemical and physical properties of that matter, was provided by John Dalton in 1803, although the question took a hundred years to settle as proven. Dalton also formulated the law of mass relationships. In 1869, Dmitri Mendeleev composed his periodic table of elements on the basis of Dalton's discoveries.\n\nThe synthesis of urea by Friedrich Wöhler opened a new research field, organic chemistry, and by the end of the 19th century, scientists were able to synthesize hundreds of organic compounds. The later part of the 19th century saw the exploitation of the Earth's petrochemicals, after the exhaustion of the oil supply from whaling. By the 20th century, systematic production of refined materials provided a ready supply of products which provided not only energy, but also synthetic materials for clothing, medicine, and everyday disposable resources. Application of the techniques of organic chemistry to living organisms resulted in physiological chemistry, the precursor to biochemistry. The 20th century also saw the integration of physics and chemistry, with chemical properties explained as the result of the electronic structure of the atom. Linus Pauling's book on \"The Nature of the Chemical Bond\" used the principles of quantum mechanics to deduce bond angles in ever-more complicated molecules. Pauling's work culminated in the physical modelling of DNA, \"the secret of life\" (in the words of Francis Crick, 1953). In the same year, the Miller–Urey experiment demonstrated in a simulation of primordial processes, that basic constituents of proteins, simple amino acids, could themselves be built up from simpler molecules.\n\nGeology existed as a cloud of isolated, disconnected ideas about rocks, minerals, and landforms long before it became a coherent science. Theophrastus' work on rocks, \"Peri lithōn\", remained authoritative for millennia: its interpretation of fossils was not overturned until after the Scientific Revolution. Chinese polymath Shen Kua (1031–1095) first formulated hypotheses for the process of land formation. Based on his observation of fossils in a geological stratum in a mountain hundreds of miles from the ocean, he deduced that the land was formed by erosion of the mountains and by deposition of silt.\n\nGeology did not undergo systematic restructuring during the Scientific Revolution, but individual theorists made important contributions. Robert Hooke, for example, formulated a theory of earthquakes, and Nicholas Steno developed the theory of superposition and argued that fossils were the remains of once-living creatures. Beginning with Thomas Burnet's \"Sacred Theory of the Earth\" in 1681, natural philosophers began to explore the idea that the Earth had changed over time. Burnet and his contemporaries interpreted Earth's past in terms of events described in the Bible, but their work laid the intellectual foundations for secular interpretations of Earth history.\n\nModern geology, like modern chemistry, gradually evolved during the 18th and early 19th centuries. Benoît de Maillet and the Comte de Buffon saw the Earth as much older than the 6,000 years envisioned by biblical scholars. Jean-Étienne Guettard and Nicolas Desmarest hiked central France and recorded their observations on some of the first geological maps. Aided by chemical experimentation, naturalists such as Scotland's John Walker, Sweden's Torbern Bergman, and Germany's Abraham Werner created comprehensive classification systems for rocks and minerals—a collective achievement that transformed geology into a cutting edge field by the end of the eighteenth century. These early geologists also proposed a generalized interpretations of Earth history that led James Hutton, Georges Cuvier and Alexandre Brongniart, following in the steps of Steno, to argue that layers of rock could be dated by the fossils they contained: a principle first applied to the geology of the Paris Basin. The use of index fossils became a powerful tool for making geological maps, because it allowed geologists to correlate the rocks in one locality with those of similar age in other, distant localities. Over the first half of the 19th century, geologists such as Charles Lyell, Adam Sedgwick, and Roderick Murchison applied the new technique to rocks throughout Europe and eastern North America, setting the stage for more detailed, government-funded mapping projects in later decades.\n\nMidway through the 19th century, the focus of geology shifted from description and classification to attempts to understand \"how\" the surface of the Earth had changed. The first comprehensive theories of mountain building were proposed during this period, as were the first modern theories of earthquakes and volcanoes. Louis Agassiz and others established the reality of continent-covering ice ages, and \"fluvialists\" like Andrew Crombie Ramsay argued that river valleys were formed, over millions of years by the rivers that flow through them. After the discovery of radioactivity, radiometric dating methods were developed, starting in the 20th century. Alfred Wegener's theory of \"continental drift\" was widely dismissed when he proposed it in the 1910s, but new data gathered in the 1950s and 1960s led to the theory of plate tectonics, which provided a plausible mechanism for it. Plate tectonics also provided a unified explanation for a wide range of seemingly unrelated geological phenomena. Since 1970 it has served as the unifying principle in geology.\n\nGeologists' embrace of plate tectonics became part of a broadening of the field from a study of rocks into a study of the Earth as a planet. Other elements of this transformation include: geophysical studies of the interior of the Earth, the grouping of geology with meteorology and oceanography as one of the \"earth sciences\", and comparisons of Earth and the solar system's other rocky planets.\n\nAristarchus of Samos published work on how to determine the sizes and distances of the Sun and the Moon, and Eratosthenes used this work to figure the size of the Earth. Hipparchus later discovered the precession of the Earth.\n\nAdvances in astronomy and in optical systems in the 19th century resulted in the first observation of an asteroid (1 Ceres) in 1801, and the discovery of Neptune in 1846.\n\nIn 1925, Cecilia Payne-Gaposchkin determined that stars were composed mostly of Hydrogen and Helium. She was dissuaded by astronomer Henry Norris Russell from publishing this finding in her Ph.D.thesis because of the widely held belief that stars had the same composition as the Earth. However, four years later, in 1929, Henry Norris Russell came to the same conclusion through different reasoning and the discovery was eventually accepted.\n\nGeorge Gamow, Ralph Alpher, and Robert Herman had calculated that there should be evidence for a Big Bang in the background temperature of the universe. In 1964, Arno Penzias and Robert Wilson discovered a 3 Kelvin background hiss in their Bell Labs radiotelescope (the Holmdel Horn Antenna), which was evidence for this hypothesis, and formed the basis for a number of results that helped determine the age of the universe.\n\nSupernova SN1987A was observed by astronomers on Earth both visually, and in a triumph for neutrino astronomy, by the solar neutrino detectors at Kamiokande. But the solar neutrino flux was a fraction of its theoretically expected value. This discrepancy forced a change in some values in the standard model for particle physics.\n\nWilliam Harvey published \"De Motu Cordis\" in 1628, which revealed his conclusions based on his extensive studies of vertebrate circulatory systems. He identified the central role of the heart, arteries, and veins in producing blood movement in a circuit, and\nfailed to find any confirmation of Galen's pre-existing notions of heating and cooling functions. The History early modern biology and medicine is often told through the search for the seat of the soul. Galen in his descriptions of his foundational work in medicine presents the distinctions between arteries, veins, and nerves using the vocabulary of the soul.\n\nIn 1847, Hungarian physician Ignác Fülöp Semmelweis dramatically reduced the occurrency of puerperal fever by simply requiring physicians to wash their hands before attending to women in childbirth. This discovery predated the germ theory of disease. However, Semmelweis' findings were not appreciated by his contemporaries and handwashing came into use only with discoveries by British surgeon Joseph Lister, who in 1865 proved the principles of antisepsis. Lister's work was based on the important findings by French biologist Louis Pasteur. Pasteur was able to link microorganisms with disease, revolutionizing medicine. He also devised one of the most important methods in preventive medicine, when in 1880 he produced a vaccine against rabies. Pasteur invented the process of pasteurization, to help prevent the spread of disease through milk and other foods.\n\nPerhaps the most prominent, controversial and far-reaching theory in all of science has been the theory of evolution by natural selection put forward by the British naturalist Charles Darwin in his book On the Origin of Species in 1859. Darwin proposed that the features of all living things, including humans, were shaped by natural processes over long periods of time. The theory of evolution in its current form affects almost all areas of biology. Implications of evolution on fields outside of pure science have led to both opposition and support from different parts of society, and profoundly influenced the popular understanding of \"man's place in the universe\". In the early 20th century, the study of heredity became a major investigation after the rediscovery in 1900 of the laws of inheritance developed by the Moravian monk Gregor Mendel in 1866. Mendel's laws provided the beginnings of the study of genetics, which became a major field of research for both scientific and industrial research. By 1953, James D. Watson, Francis Crick and Maurice Wilkins clarified the basic structure of DNA, the genetic material for expressing life in all its forms. In the late 20th century, the possibilities of genetic engineering became practical for the first time, and a massive international effort began in 1990 to map out an entire human genome (the Human Genome Project).\n\nThe discipline of ecology typically traces its origin to the synthesis of Darwinian evolution and Humboldtian biogeography, in the late 19th and early 20th centuries. Equally important in the rise of ecology, however, were microbiology and soil science—particularly the cycle of life concept, prominent in the work Louis Pasteur and Ferdinand Cohn. The word \"ecology\" was coined by Ernst Haeckel, whose particularly holistic view of nature in general (and Darwin's theory in particular) was important in the spread of ecological thinking. In the 1930s, Arthur Tansley and others began developing the field of ecosystem ecology, which combined experimental soil science with physiological concepts of energy and the techniques of field biology.\n\nSuccessful use of the scientific method in the physical sciences led to the same methodology being adapted to better understand the many fields of human endeavor. From this effort the social sciences have been developed.\n\nPolitical science is a late arrival in terms of social sciences. However, the discipline has a clear set of antecedents such as moral philosophy, political philosophy, political economy, history, and other fields concerned with normative determinations of what ought to be and with deducing the characteristics and functions of the ideal form of government. The roots of politics are in prehistory. In each historic period and in almost every geographic area, we can find someone studying politics and increasing political understanding.\n\nIn Western culture, the study of politics is first found in Ancient Greece. The antecedents of European politics trace their roots back even earlier than Plato and Aristotle, particularly in the works of Homer, Hesiod, Thucydides, Xenophon, and Euripides. Later, Plato analyzed political systems, abstracted their analysis from more literary- and history- oriented studies and applied an approach we would understand as closer to philosophy. Similarly, Aristotle built upon Plato's analysis to include historical empirical evidence in his analysis.\n\nAn ancient Indian treatise on statecraft, economic policy and military strategy by Kautilya and , who are traditionally identified with (c. 350–283 BCE). In this treatise, the behaviors and relationships of the people, the King, the State, the Government Superintendents, Courtiers, Enemies, Invaders, and Corporations are analysed and documented. Roger Boesche describes the \"Arthaśāstra\" as \"a book of political realism, a book analysing how the political world does work and not very often stating how it ought to work, a book that frequently discloses to a king what calculating and sometimes brutal measures he must carry out to preserve the state and the common good.\"\n\nDuring the rule of Rome, famous historians such as Polybius, Livy and Plutarch documented the rise of the Roman Republic, and the organization and histories of other nations, while statesmen like Julius Caesar, Cicero and others provided us with examples of the politics of the republic and Rome's empire and wars. The study of politics during this age was oriented toward understanding history, understanding methods of governing, and describing the operation of governments.\n\nWith the fall of the Western Roman Empire, there arose a more diffuse arena for political studies. The rise of monotheism and, particularly for the Western tradition, Christianity, brought to light a new space for politics and political action. During the Middle Ages, the study of politics was widespread in the churches and courts. Works such as Augustine of Hippo's \"The City of God\" synthesized current philosophies and political traditions with those of Christianity, redefining the borders between what was religious and what was political. Most of the political questions surrounding the relationship between Church and State were clarified and contested in this period.\n\nIn the Middle East and later other Islamic areas, works such as the Rubaiyat of Omar Khayyam and Epic of Kings by Ferdowsi provided evidence of political analysis, while the Islamic Aristotelians such as Avicenna and later Maimonides and Averroes, continued Aristotle's tradition of analysis and empiricism, writing commentaries on Aristotle's works.\n\nDuring the Italian Renaissance, Niccolò Machiavelli established the emphasis of modern political science on direct empirical observation of political institutions and actors. Later, the expansion of the scientific paradigm during the Enlightenment further pushed the study of politics beyond normative determinations. In particular, the study of statistics, to study the subjects of the state, has been applied to polling and voting.\n\nIn the 20th century, the study of ideology, behaviouralism and international relations led to a multitude of 'pol-sci' subdisciplines including rational choice theory, voting theory, game theory (also used in economics), psephology, political geography/geopolitics, political psychology/political sociology, political economy, policy analysis, public administration, comparative political analysis and peace studies/conflict analysis.\n\nHistorical linguistics emerged as an independent field of study at the end of the 18th century. Sir William Jones proposed that Sanskrit, Persian, Greek, Latin, Gothic, and Celtic languages all shared a common base. After Jones, an effort to catalog all languages of the world was made throughout the 19th century and into the 20th century. Publication of Ferdinand de Saussure's \"Cours de linguistique générale\" created the development of descriptive linguistics. Descriptive linguistics, and the related structuralism movement caused linguistics to focus on how language changes over time, instead of just describing the differences between languages. Noam Chomsky further diversified linguistics with the development of generative linguistics in the 1950s. His effort is based upon a mathematical model of language that allows for the description and prediction of valid syntax. Additional specialties such as sociolinguistics, cognitive linguistics, and computational linguistics have emerged from collaboration between linguistics and other disciplines.\n\nThe basis for classical economics forms Adam Smith's \"An Inquiry into the Nature and Causes of the Wealth of Nations\", published in 1776. Smith criticized mercantilism, advocating a system of free trade with division of labour. He postulated an \"invisible hand\" that regulated economic systems made up of actors guided only by self-interest. Karl Marx developed an alternative economic theory, called Marxian economics. Marxian economics is based on the labor theory of value and assumes the value of good to be based on the amount of labor required to produce it. Under this assumption, capitalism was based on employers not paying the full value of workers labor to create profit. The Austrian School responded to Marxian economics by viewing entrepreneurship as driving force of economic development. This replaced the labor theory of value by a system of supply and demand.\n\nIn the 1920s, John Maynard Keynes prompted a division between microeconomics and macroeconomics. Under Keynesian economics macroeconomic trends can overwhelm economic choices made by individuals. Governments should promote aggregate demand for goods as a means to encourage economic expansion. Following World War II, Milton Friedman created the concept of monetarism. Monetarism focuses on using the supply and demand of money as a method for controlling economic activity. In the 1970s, monetarism has adapted into supply-side economics which advocates reducing taxes as a means to increase the amount of money available for economic expansion.\n\nOther modern schools of economic thought are New Classical economics and New Keynesian economics. New Classical economics was developed in the 1970s, emphasizing solid microeconomics as the basis for macroeconomic growth. New Keynesian economics was created partially in response to New Classical economics, and deals with how inefficiencies in the market create a need for control by a central bank or government.\n\nThe above \"history of economics\" reflects modern economic textbooks and this means that the last stage of a science is represented as the culmination of its history (Kuhn, 1962). The \"invisible hand\" mentioned in a lost page in the middle of a chapter in the middle of the \"Wealth of Nations\", 1776, advances as Smith's central message. It is played down that this \"invisible hand\" acts only \"frequently\" and that it is \"no part of his [the individual's] intentions\" because competition leads to lower prices by imitating \"his\" invention. That this \"invisible hand\" prefers \"the support of domestic to foreign industry\" is cleansed—often without indication that part of the citation is truncated. The opening passage of the \"Wealth\" containing Smith's message is never mentioned as it cannot be integrated into modern theory: \"Wealth\" depends on the division of labour which changes with market volume and on the proportion of productive to Unproductive labor.\n\nThe end of the 19th century marks the start of psychology as a scientific enterprise. The year 1879 is commonly seen as the start of psychology as an independent field of study. In that year Wilhelm Wundt founded the first laboratory dedicated exclusively to psychological research (in Leipzig). Other important early contributors to the field include Hermann Ebbinghaus (a pioneer in memory studies), Ivan Pavlov (who discovered classical conditioning), William James, and Sigmund Freud. Freud's influence has been enormous, though more as cultural icon than a force in scientific psychology.\n\nThe 20th century saw a rejection of Freud's theories as being too unscientific, and a reaction against Edward Titchener's atomistic approach of the mind. This led to the formulation of behaviorism by John B. Watson, which was popularized by B.F. Skinner. Behaviorism proposed epistemologically limiting psychological study to overt behavior, since that could be reliably measured. Scientific knowledge of the \"mind\" was considered too metaphysical, hence impossible to achieve.\n\nThe final decades of the 20th century have seen the rise of a new interdisciplinary approach to studying human psychology, known collectively as cognitive science. Cognitive science again considers the mind as a subject for investigation, using the tools of psychology, linguistics, computer science, philosophy, and neurobiology. New methods of visualizing the activity of the brain, such as PET scans and CAT scans, began to exert their influence as well, leading some researchers to investigate the mind by investigating the brain, rather than cognition. These new forms of investigation assume that a wide understanding of the human mind is possible, and that such an understanding may be applied to other research domains, such as artificial intelligence.\n\nIbn Khaldun can be regarded as the earliest scientific systematic sociologist. The modern sociology emerged in the early 19th century as the academic response to the modernization of the world. Among many early sociologists (e.g., Émile Durkheim), the aim of sociology was in structuralism, understanding the cohesion of social groups, and developing an \"antidote\" to social disintegration. Max Weber was concerned with the modernization of society through the concept of rationalization, which he believed would trap individuals in an \"iron cage\" of rational thought. Some sociologists, including Georg Simmel and W. E. B. Du Bois, utilized more microsociological, qualitative analyses. This microlevel approach played an important role in American sociology, with the theories of George Herbert Mead and his student Herbert Blumer resulting in the creation of the symbolic interactionism approach to sociology.\n\nIn particular, just Auguste Comte, illustrated with his work the transition from a theological to a metaphysical stage and, from this, to a positive stage. Comte took care of the classification of the sciences as well as a transit of humanity towards a situation of progress attributable to a re-examination of nature according to the\naffirmation of 'sociality' as the basis of the scientifically interpreted society (Cfr. Guglielmo Rinzivillo, \"Natura, cultura e induzione nell'età delle scienze. Fatti e idee del movimento scientifico in Francia e Inghilterra\", Roma, Nuova Cultura, 2015, pp. 79–, ).\n\nAmerican sociology in the 1940s and 1950s was dominated largely by Talcott Parsons, who argued that aspects of society that promoted structural integration were therefore \"functional\". This structural functionalism approach was questioned in the 1960s, when sociologists came to see this approach as merely a justification for inequalities present in the status quo. In reaction, conflict theory was developed, which was based in part on the philosophies of Karl Marx. Conflict theorists saw society as an arena in which different groups compete for control over resources. Symbolic interactionism also came to be regarded as central to sociological thinking. Erving Goffman saw social interactions as a stage performance, with individuals preparing \"backstage\" and attempting to control their audience through impression management. While these theories are currently prominent in sociological thought, other approaches exist, including feminist theory, post-structuralism, rational choice theory, and postmodernism.\n\nAnthropology can best be understood as an outgrowth of the Age of Enlightenment. It was during this period that Europeans attempted systematically to study human behaviour. Traditions of jurisprudence, history, philology and sociology developed during this time and informed the development of the social sciences of which anthropology was a part.\n\nAt the same time, the romantic reaction to the Enlightenment produced thinkers such as Johann Gottfried Herder and later Wilhelm Dilthey whose work formed the basis for the culture concept which is central to the discipline. Traditionally, much of the history of the subject was based on colonial encounters between Western Europe and the rest of the world, and much of 18th- and 19th-century anthropology is now classed as scientific racism.\n\nDuring the late 19th-century, battles over the \"study of man\" took place between those of an \"anthropological\" persuasion (relying on anthropometrical techniques) and those of an \"ethnological\" persuasion (looking at cultures and traditions), and these distinctions became part of the later divide between physical anthropology and cultural anthropology, the latter ushered in by the students of Franz Boas.\n\nIn the mid-20th century, much of the methodologies of earlier anthropological and ethnographical study were reevaluated with an eye towards research ethics, while at the same time the scope of investigation has broadened far beyond the traditional study of \"primitive cultures\" (scientific practice itself is often an arena of anthropological study).\n\nThe emergence of paleoanthropology, a scientific discipline which draws on the methodologies of paleontology, physical anthropology and ethology, among other disciplines, and increasing in scope and momentum from the mid-20th century, continues to yield further insights into human origins, evolution, genetic and cultural heritage, and perspectives on the contemporary human predicament as well.\n\nDuring the 20th century, a number of interdisciplinary scientific fields have emerged. Examples include:\n\nCommunication studies combines animal communication, information theory, marketing, public relations, telecommunications and other forms of communication.\n\nComputer science, built upon a foundation of theoretical linguistics, discrete mathematics, and electrical engineering, studies the nature and limits of computation. Subfields include computability, computational complexity, database design, computer networking, artificial intelligence, and the design of computer hardware. One area in which advances in computing have contributed to more general scientific development is by facilitating large-scale archiving of scientific data. Contemporary computer science typically distinguishes\nitself by emphasising mathematical 'theory' in contrast to the practical emphasis of software engineering.\n\nEnvironmental science is an interdisciplinary field. It draws upon the disciplines of biology, chemistry, earth sciences, ecology, geography, mathematics, and physics.\n\nMaterials science has its roots in metallurgy, mineralogy, and crystallography. It combines chemistry, physics, and several engineering disciplines. The field studies metals, ceramics, glass, plastics, semiconductors, and composite materials.\n\nNeuroscience is a multidisciplinary branch of science that combines physiology, neuroanatomy, molecular biology, developmental biology, cytology, mathematical modeling and psychology to understand the fundamental and emergent properties of neurons, glia, nervous systems and neural circuits.\n\nAs an academic field, history of science and technology began with the publication of William Whewell's \"History of the Inductive Sciences\" (first published in 1837). A more formal study of the history of science as an independent discipline was launched by George Sarton's publications, \"Introduction to the History of Science\" (1927) and the \"Isis\" journal (founded in 1912). Sarton exemplified the early 20th-century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress. The history of science was not a recognized subfield of American history in this period, and most of the work was carried out by interested scientists and physicians rather than professional historians. With the work of I. Bernard Cohen at Harvard, the history of science became an established subdiscipline of history after 1945.\n\nThe history of mathematics, history of technology, and history of philosophy are distinct areas of research and are covered in other articles. Mathematics is closely related to but distinct from natural science (at least in the modern conception). Technology is likewise closely related to but clearly differs from the search for empirical truth.\n\nHistory of science is an academic discipline, with an international community of specialists. Main professional organizations for this field include the History of Science Society, the British Society for the History of Science, and the European Society for the History of Science.\n\nMuch of the study of the history of science has been devoted to answering questions about what science \"is\", how it \"functions\", and whether it exhibits large-scale patterns and trends. The sociology of science in particular has focused on the ways in which scientists work, looking closely at the ways in which they \"produce\" and \"construct\" scientific knowledge. Since the 1960s, a common trend in science studies (the study of the sociology and history of science) has been to emphasize the \"human component\" of scientific knowledge, and to de-emphasize the view that scientific data are self-evident, value-free, and context-free. The field of Science and Technology Studies, an area that overlaps and often informs historical studies of science, focuses on the social context of science in both contemporary and historical periods.\n\nHumboldtian science refers to the early 19th century approach of combining scientific field work with the age of Romanticism sensitivity, ethics and aesthetic ideals. It helped to install natural history as a separate field, gave base for ecology and was based on the role model of scientist, naturalist and explorer Alexander von Humboldt. The later 19th century positivism asserted that all authentic knowledge allows verification and that all authentic knowledge assumes that the only valid knowledge is scientific.\n\nA major subject of concern and controversy in the philosophy of science has been the nature of \"theory change\" in science. Karl Popper argued that scientific knowledge is progressive and cumulative; Thomas Kuhn, that scientific knowledge moves through \"paradigm shifts\" and is not necessarily progressive; and Paul Feyerabend, that scientific knowledge is not cumulative or progressive and that there can be no demarcation in terms of method between science and any other form of investigation.\n\nThe mid 20th century saw a series of studies relying to the role of science in a social context, starting from Thomas Kuhn's \"The Structure of Scientific Revolutions\" in 1962. It opened the study of science to new disciplines by suggesting that the evolution of science was in part sociologically determined and that positivism did not explain the actual interactions and strategies of the human participants in science. As Thomas Kuhn put it, the history of science may be seen in more nuanced terms, such as that of competing paradigms or conceptual systems in a wider matrix that includes intellectual, cultural, economic and political themes outside of science. \"Partly by selection and partly by distortion, the scientists of earlier ages are implicitly presented as having worked upon the same set of fixed problems and in accordance with the same set of fixed canons that the most recent revolution in scientific theory and method made seem scientific.\"\n\nFurther studies, e.g. Jerome Ravetz 1971 \"Scientific Knowledge and its Social Problems\" referred to the role of the scientific community, as a social construct, in accepting or rejecting (objective) scientific knowledge. The Science wars of the 1990 were about the influence of especially French philosophers, which denied the objectivity of science in general or seemed to do so. They described as well differences between the idealized model of a pure science and the actual scientific practice; while scientism, a revival of the positivism approach, saw in precise measurement and rigorous calculation the basis for finally settling enduring metaphysical and moral controversies. However, more recently some of the leading critical theorists have recognized that their postmodern deconstructions have at times been counter-productive, and are providing intellectual ammunition for reactionary interests. Bruno Latour noted that \"dangerous extremists are using the very same argument of social construction to destroy hard-won evidence that could save our lives. Was I wrong to participate in the invention of this field known as science studies? Is it enough to say that we did not really mean what we meant?\"\n\nOne recurring observation in the history of science involves the struggle for recognition of first-rate scientists working on the periphery of the scientific establishment. For instance, the great physicist Lord Rayleigh looked back (cited here) on John James Waterston's seminal paper on the kinetic theory of gases. The history of the neglect of Waterston's path-breaking article, Rayleigh felt, suggests that \"a young author who believes himself capable of great things would usually do well to secure favourable recognition of the scientific world . . . before embarking upon higher flights.\"\n\nWilliam Harvey's experiences led him to an even more pessimistic view:\"But what remains to be said about the quantity and source of the blood which thus passes, is of so novel and unheard-of character that I not only fear injury to myself from the envy of a few, but I tremble lest I have mankind at large for my enemies, so much doth wont and custom, that become as another nature, and doctrine once sown and that hath struck deep root, and respect for antiquity, influence all men.\"In more general terms, Robert K. Merton remarks that \"the history of science abounds in instances of basic papers having been written by comparatively unknown scientists, only to be rejected or neglected for years.\"\n\n\n\n\n"}
{"id": "37870730", "url": "https://en.wikipedia.org/wiki?curid=37870730", "title": "International Conference of Laser Applications", "text": "International Conference of Laser Applications\n\nThe International Conference on Lasers and Applications, Lasers 'XX was an annual conference organized by \nthe former Society for Optical and Quantum Electronics. The conference, known in short by \"Lasers 'XX\" (where XX refers to the particular year), was held at various locations in The United States from 1978 to 2000.\n\nThe emphasis of these conferences was laser development and in particular the development of high-power lasers. The papers delivered at these conferences were published in a series of hard-bound volumes known as \"Proceedings of the International Conference on Lasers 'XX\" () by STS Press. In total, more than 20 book proceedings were published.\n\nA particular feature of these conferences was the organization of high-power panel discussions on timely topics of interest, such as the role of lasers in directed energy and the Strategic Defense Initiative (SDI), during the presidency of Ronald Reagan. Noted physicists, including Edward Teller and Arthur Kantrowitz, participated in these discussions. Towards the end of the Cold War this conference enjoyed the participation of numerous Soviet laser physicists, including prominent authors such as Alexander Prokhorov and Nikolay Basov. \n\nA partial list of plenary and invited speakers include (in chronological order):\n\n\nBesides the emphasis on high-power lasers and panel discussions on this subject, many scientific disclosures made at these conferences went on to contribute to, or to inspire, further research in a variety of fields including:\n\n\nFrom \"Lasers '88\" to \"Lasers '96\", the prestigious Einstein Prize for Laser Science was awarded.\n\n"}
{"id": "32149868", "url": "https://en.wikipedia.org/wiki?curid=32149868", "title": "K computer", "text": "K computer\n\nThe K computer named for the Japanese word , meaning 10 quadrillion (10) is a supercomputer manufactured by Fujitsu, currently installed at the Riken Advanced Institute for Computational Science campus in Kobe, Hyōgo Prefecture, Japan. The K computer is based on a distributed memory architecture with over 80,000 compute nodes. It is used for a variety of applications, including climate research, disaster prevention and medical research. The K computer's operating system is based on the Linux kernel, with additional drivers designed to make use of the computer's hardware.\n\nIn June 2011, TOP500 ranked K the world's fastest supercomputer, with a computation speed of over 8 petaflops, and in November 2011, K became the first computer to top 10 petaflops. It had originally been slated for completion in June 2012. In June 2012, K was superseded as the world's fastest supercomputer by the American IBM Sequoia. \n\n, K is the world's eighteenth-fastest computer, with the IBM's Summit & Sierra being the fastest supercomputers.\n\n, the K computer holds the third place for the HPCG benchmark. It held the first place till June 2018, until it was superseded by Summit & Sierra.\n\nOn 20 June 2011, the TOP500 Project Committee announced that K had set a LINPACK record with a performance of 8.162 petaflops, making it the fastest supercomputer in the world at the time; it achieved this performance with a computing efficiency ratio of 93.0%. The previous record holder was the Chinese National University of Defense Technology's Tianhe-1A, which performed at 2.507 petaflops. The TOP500 list is revised semiannually, and the rankings change frequently, indicating the speed at which computing power is increasing. In November 2011, Riken reported that K had become the first supercomputer to exceed 10 petaflops, achieving a LINPACK performance of 10.51 quadrillion computations per second with a computing efficiency ratio of 93.2%. K received top ranking in all four performance benchmarks at the 2011 HPC Challenge Awards.\n\nOn 18 June 2012, the TOP500 Project Committee announced that the California-based IBM Sequoia supercomputer replaced K as the world's fastest supercomputer, with a LINPACK performance of 16.325 petaflops. Sequoia is 55% faster than K, using 123% more CPU processors, but is also 150% more energy efficient. \n\nOn the TOP500 list, it became first on June 2011, falling down through time to lower positions, to eighteenth in November 2018.\n\nK computer holds third place in the HPCG benchmark test proposed by Jack Dongarra, with 0.6027 HPCG PFLOPS in November 2018.\n\nThe K computer comprises 88,128 2.0 GHz eight-core SPARC64 VIIIfx processors contained in 864 cabinets, for a total of 705,024 cores, manufactured by Fujitsu with 45 nm CMOS technology. Each cabinet contains 96 computing nodes, in addition to six I/O nodes. Each computing node contains a single processor and 16 GB of memory. The computer's water cooling system is designed to minimize failure rate and power consumption.\n\nThe nodes are interconnected by Fujitsu's proprietary \"Torus fusion\" (\"Tofu\") interconnect. Tofu has a six-dimensional mesh/torus topology, a scalability of over 100,000 nodes, and full-duplex links that have a peak bandwidth of 10 GB/s (5 GB/s per direction). Each node is connected to its own \"InterConnect Controller\" (\"ICC\") chip, which contains four Tofu interfaces (one for the node and three for connecting to other ICC chips) and a router. Tofu's six-dimensional mesh/torus topology is abstracted by software to appear as a three-dimensional torus; and is supported by a Tofu-optimized version of the open-source Open MPI Message Passing Interface library. Users can create application programs adapted to either a one-, two-, or three-dimensional torus network.\n\nThe system adopts a two-level local/global file system with parallel/distributed functions, and provides users with an automatic staging function for moving files between global and local file systems. Fujitsu developed an optimized parallel file system based on Lustre, called the Fujitsu Exabyte File System (FEFS), which is scalable to several hundred petabytes.\n\nAlthough the K computer reported the highest total power consumption of any 2011 TOP500 supercomputer (9.89 MW the equivalent of almost 10,000 suburban homes), it is relatively efficient, achieving 824.6 GFlop/kW. This is 29.8% more efficient than China's NUDT TH MPP (ranked #2 in 2011), and 225.8% more efficient than Oak Ridge's Jaguar-Cray XT5-HE (ranked #3 in 2011). However, K's power efficiency still falls far short of the 2097.2 GFlops/kWatt supercomputer record set by IBM's NNSA/SC Blue Gene/Q Prototype 2. For comparison, the average power consumption of a TOP 10 system in 2011 was 4.3 MW, and the average efficiency was 463.7 GFlop/kW.\n\nAccording to TOP500 compiler Jack Dongarra, professor of electrical engineering and computer science at the University of Tennessee, the K computer's performance equals \"one million linked desktop computers\". The computer's annual running costs are estimated at US$10 million.\n\nOn 1 July 2011, Kobe's Port Island Line rapid transit system renamed one of its stations \"K Computer Mae\" (meaning \"In front of K Computer\") denoting its vicinity.\n\n\n"}
{"id": "57090569", "url": "https://en.wikipedia.org/wiki?curid=57090569", "title": "L1-norm principal component analysis", "text": "L1-norm principal component analysis\n\nL1-norm principal component analysis (L1-PCA) is a general method for multivariate data analysis.\nL1-PCA is often preferred over standard L2-norm principal component analysis (PCA) when the analyzed data may contain outliers (faulty values or corruptions).\n\nBoth L1-PCA and standard PCA seek a collection of orthogonal directions (principal components) that define a subspace wherein data representation is maximized according to the selected criterion.\nStandard PCA quantifies data representation as the aggregate of the L2-norm of the data point projections into the subspace, or equivalently the aggregate Euclidean distance of the original points from their subspace-projected representations.\nL1-PCA uses instead the aggregate of the L1-norm of the data point projections into the subspace. In PCA and L1-PCA, the number of principal components (PCs) is lower than the rank of the analyzed matrix, which coincides with the dimensionality of the space defined by the original data points.\nTherefore, PCA or L1-PCA are commonly employed for dimensionality reduction for the purpose of data denoising or compression.\nAmong the advantages of standard PCA that contributed to its high popularity are low-cost computational implementation by means of singular-value decomposition (SVD) and statistical optimality when the data set is generated by a true multivariate Normal data source.\n\nHowever, in modern big data sets, data often include corrupted, faulty points, commonly referred to as outliers.\nStandard PCA is known to be sensitive to outliers, even when they appear as a small fraction of the processed data.\nThe reason is that the L2-norm formulation of L2-PCA places squared emphasis on the magnitude of each coordinate of each data point, ultimately overemphasizing peripheral points, such as outliers. \nOn the other hand, following an L1-norm formulation, L1-PCA places linear emphasis on the coordinates of each data point, effectively restraining outliers.\n\nConsider any matrix formula_1 consisting of formula_2 formula_3-dimensional data points. Define formula_4. For integer formula_5 such that formula_6, L1-PCA is formulated as:\n\n&\\text{subject to}~~ \\mathbf Q^\\top \\mathbf Q=\\mathbf I_K.\n</math>\n\nFor formula_7, () simplifies to finding the L1-norm principal component (L1-PC) of formula_8 by\n&\\text{subject to}~~ \\| \\mathbf q\\|_2 =1.\n</math>\n\nIn ()-(), L1-norm formula_9 returns the sum of the absolute entries of its argument and L2-norm formula_10 returns the sum of the squared entries of its argument. If one substitutes formula_9 in () by the Frobenius/L2-norm formula_12, then the problem becomes standard PCA and it is solved by the matrix formula_13 that contains the formula_5 dominant singular vectors of formula_8 (i.e., the singular vectors that correspond to the formula_5 highest singular values).\n\nThe maximization metric in () can be expanded as \n\nFor any matrix formula_17 with formula_18, define formula_19 as the nearest (in the L2-norm sense) matrix to formula_20 that has orthonormal columns. That is, define\n&\\text{subject to}~~ \\mathbf Q^\\top \\mathbf Q=\\mathbf I_n.\n</math>\nProcrustes Theorem states that if formula_20 has SVD formula_22, then \nformula_23.\n\nMarkopoulos, Karystinos, and Pados showed that, if formula_24 is the exact solution to the binary nuclear-norm maximization (BNM) problem \n</math>\nthen \n</math>\nis the exact solution to L1-PCA in (). The nuclear-norm formula_25 in () returns the summation of the singular values of its matrix argument and can be calculated by means of standard SVD. Moreover, it holds that, given the solution to L1-PCA, formula_26, the solution to BNM can be obtained as \n</math>\nwhere formula_27 returns the formula_28-sign matrix of its matrix argument (with no loss of generality, we can consider formula_29). In addition, it follows that formula_30. BNM in () is a combinatorial problem over antipodal binary variables. Therefore, its exact solution can be found through exhaustive evaluation of all formula_31 elements of its feasibility set, with asymptotic cost formula_32. Therefore, L1-PCA can also be solved, through BNM, with cost formula_32 (exponential in the product of the number of data points with the number of the sought-after components). It turns out that L1-PCA can be solved optimally (exactly) with polynomial complexity in formula_2 for fixed data dimension formula_3\", formula_36\".\n\nFor the special case of formula_7 (single L1-PC of formula_8), BNM takes the binary-quadratic-maximization (BQM) form \n</math>\nThe transition from () to () for formula_7 holds true, since the unique singular value of formula_40 is equal to formula_41, for every formula_42. Then, if formula_43 is the solution to BQM in (), it holds that \n</math>\nis the exact L1-PC of formula_8, as defined in (). In addition, it holds that formula_45 and formula_46.\n\nAs shown above, the exact solution to L1-PCA can be obtained by the following two-step process: \n\nBNM in () can be solved by exhaustive search over the domain of formula_50 with cost formula_51. \n\nAlso, L1-PCA can be solved optimally with cost formula_36, when formula_4 is constant with respect to formula_2 (always true for finite data dimension formula_3).\n\nIn 2008, Kwak proposed an iterative algorithm for the approximate solution of L1-PCA for formula_7. This iterative method was later generalized for formula_57 components. Another approximate efficient solver was proposed by McCoy and Tropp by means of semi-definite programming (SDP). Most recently, L1-PCA (and BNM in ()) were solved efficiently by means of bit-flipping iterations (L1-BF algorithm).\n\n 1 function L1BF(formula_8, formula_5):\n\nThe computational cost of L1-BF is formula_79.\n\nL1-PCA has also been generalized to process complex data. For complex L1-PCA, two efficient algorithms were proposed in 2018.\n\nMATLAB code for L1-PCA is available at MathWorks and other repositories.\n"}
{"id": "863741", "url": "https://en.wikipedia.org/wiki?curid=863741", "title": "Light field", "text": "Light field\n\nThe light field is a vector function that describes the amount of light flowing in every direction through every point in space. The space of all possible light rays is given by the five-dimensional plenoptic function, and the magnitude of each ray is given by the radiance. Michael Faraday was the first to propose (in an 1846 lecture entitled \"Thoughts on Ray Vibrations\") that light should be interpreted as a field, much like the magnetic fields on which he had been working for several years. The phrase \"light field\" was coined by Andrey Gershun in a classic paper on the radiometric properties of light in three-dimensional space (1936).\n\nIf the concept is restricted to geometric optics—i.e., to incoherent light and to objects larger than the wavelength of light—then the fundamental carrier of light is a ray. The measure for the amount of light traveling along a ray is radiance, denoted by \"L\" and measured in watts \"(W)\" per steradian \"(sr)\" per meter squared \"(m)\". The steradian is a measure of solid angle, and meters squared are used here as a measure of cross-sectional area, as shown at right.\n\nThe radiance along all such rays in a region of three-dimensional space illuminated by an unchanging arrangement of lights is called the plenoptic function (Adelson 1991). The plenoptic illumination function is an idealized function used in computer vision and computer graphics to express the image of a scene from any possible viewing position at any viewing angle at any point in time. It is never actually used in practice computationally, but is conceptually useful in understanding other concepts in vision and graphics. Since rays in space can be parameterized by three coordinates, \"x\", \"y\", and \"z\" and two angles \"θ\" and \"ϕ\", as shown at left, it is a five-dimensional function, that is, a function over a five-dimensional manifold equivalent to the product of 3D Euclidean space and the 2-sphere.\n\nLike Adelson, Gershun defined the light field at each point in space as a 5D function. However, he treated it as an infinite collection of vectors, one per direction impinging on the point, with lengths proportional to their radiances.\n\nIntegrating these vectors over any collection of lights, or over the entire sphere of directions, produces a single scalar value—the \"total irradiance\" at that point, and a resultant direction. The figure at right, reproduced from Gershun's paper, shows this calculation for the case of two light sources. In computer graphics, this vector-valued function of 3D space is called the \"vector irradiance field\" (Arvo, 1994). The vector direction at each point in the field can be interpreted as the orientation one would face a flat surface placed at that point to most brightly illuminate it.\n\nOne can consider time, wavelength, and polarization angle as additional variables, yielding higher-dimensional functions.\n\nIn a plenoptic function, if the region of interest contains a concave object (think of a cupped hand), then light leaving one point on the object may travel only a short distance before being blocked by another point on the object. No practical device could measure the function in such a region.\n\nHowever, if we restrict ourselves to locations outside the convex hull (think shrink-wrap) of the object, i.e. in free space, then we can measure the plenoptic function by taking many photos using a digital camera. Moreover, in this case the function contains redundant information, because the radiance along a ray remains constant from point to point along its length, as shown at left. In fact, the redundant information is exactly one dimension, leaving us with a four-dimensional function (that is, a function of points in a particular four-dimensional manifold). Parry Moon dubbed this function the \"photic field\" (1981), while researchers in computer graphics call it the \"4D light field\" (Levoy 1996) or \"Lumigraph\" (Gortler 1996). Formally, the 4D light field is defined as radiance along rays in empty space.\n\nThe set of rays in a light field can be parameterized in a variety of ways, a few of which are shown below. Of these, the most common is the two-plane parameterization shown at right (below). While this parameterization cannot represent all rays, for example rays parallel to the two planes if the planes are parallel to each other, it has the advantage of relating closely to the analytic geometry of perspective imaging. Indeed, a simple way to think about a two-plane light field is as a collection of perspective images of the \"st\" plane (and any objects that may lie astride or beyond it), each taken from an observer position on the \"uv\" plane. A light field parameterized this way is sometimes called a \"light slab\".\n\nThe analog of the 4D light field for sound is the \"sound field\" or \"wave field,\" as in wave field synthesis, and the corresponding parametrization is the Kirchhoff-Helmholtz integral, which states that, in the absence of obstacles, a sound field over time is given by the pressure on a plane. Thus this is two dimensions of information at any point in time, and over time a 3D field.\n\nThis two-dimensionality, compared with the apparent four-dimensionality of light, is because light travels in rays (0D at a point in time, 1D over time), while by Huygens–Fresnel principle, a sound wave front can be modeled as spherical waves (2D at a point in time, 3D over time): light moves in a single direction (2D of information), while sound simply expands in every direction. However, light travelling in non-vacuous media may scatter in a similar fashion, and the irreversibility or information lost in the scattering is discernible in the apparent loss of a system dimension.\n\nLight fields are a fundamental representation for light. As such, there are as many ways of creating light fields as there are computer programs capable of creating images or instruments capable of capturing them.\n\nIn computer graphics, light fields are typically produced either by rendering a 3D model or by photographing a real scene. In either case, to produce a light field views must be obtained for a large collection of viewpoints. Depending on the parameterization employed, this collection will typically span some portion of a line, circle, plane, sphere, or other shape, although unstructured collections of viewpoints are also possible (Buehler 2001).\n\nDevices for capturing light fields photographically may include a moving handheld camera or a robotically controlled camera (Levoy 2002), an arc of cameras (as in the bullet time effect used in \"The Matrix\"), a dense array of cameras (Kanade 1998; Yang 2002; Wilburn 2005), handheld cameras (Ng 2005; Georgiev 2006; Marwah 2013), microscopes (Levoy 2006), or other optical system (Bolles 1987).\n\nHow many images should be in a light field? The largest known light field (of Michelangelo's statue of Night) contains 24,000 1.3-megapixel images. At a deeper level, the answer depends on the application. For light field rendering (see the Application section below), if you want to walk completely around an opaque object, then of course you need to photograph its back side. Less obviously, if you want to walk close to the object, and the object lies astride the \"st\" plane, then you need images taken at finely spaced positions on the \"uv\" plane (in the two-plane parameterization shown above), which is now behind you, and these images need to have high spatial resolution.\n\nThe number and arrangement of images in a light field, and the resolution of each image, are together called the \"sampling\" of the 4D light field. Analyses of \"light field sampling\" have been undertaken by many researchers; a good starting point is Chai (2000). Also of interest is Durand (2005) for the effects of occlusion, Ramamoorthi (2006) for the effects of lighting and reflection, and Ng (2005) and Zwicker (2006) for applications to plenoptic cameras and 3D displays, respectively.\n\n\"Computational imaging\" refers to any image formation method that involves a digital computer. Many of these methods operate at visible wavelengths, and many of those produce light fields. As a result, listing all applications of light fields would require surveying all uses of computational imaging in art, science, engineering, and medicine. In computer graphics, some selected applications are:\n\n\nImage generation and predistortion of synthetic imagery for holographic stereograms is one of the earliest examples of computed light fields, anticipating and later motivating the geometry used in Levoy and Hanrahan's work (Halle 1991, 1994).\n\nModern approaches to light field display explore co-designs of optical elements and compressive computation to achieve higher resolutions, increased contrast, wider fields of view, and other benefits (Wetzstein 2012, 2011; Lanman 2011, 2010).\n\n\n\n\n\n\n\n\n\nPerez, C. C., Lauri, A., Symvoulidis, P., Cappetta, M., Erdmann, A., & Westmeyer, G. G. (2015). Calcium neuroimaging in behaving zebrafish larvae using a turn-key light field camera. Journal of Biomedical Optics, 20(9), 096009-096009.\n"}
{"id": "1699556", "url": "https://en.wikipedia.org/wiki?curid=1699556", "title": "List of application servers", "text": "List of application servers\n\nThis list compares the features and functionality of application servers, grouped by the hosting environment that is offered by that particular application server.\n\n\n\n\n\n\n\n\nMicrosoft positions their middle-tier applications and services infrastructure in the Windows Server operating system and the .NET Framework technologies in the role of an application server:\n\n\n\n\n\n\n\n\n\n"}
{"id": "38235873", "url": "https://en.wikipedia.org/wiki?curid=38235873", "title": "List of countries without a stock exchange", "text": "List of countries without a stock exchange\n\nThis is a list of sovereign states without a stock exchange.\n\nIn addition, the following Central America countries are served by the Eastern Caribbean Securities Exchange (ECSE), thus, they do not own an individual stock exchange on their territories:\n\n"}
{"id": "38011426", "url": "https://en.wikipedia.org/wiki?curid=38011426", "title": "List of filename extensions (A–E)", "text": "List of filename extensions (A–E)\n\nThis alphabetical list of filename extensions contains standard extensions associated with computer files.\n\n\n"}
{"id": "5971809", "url": "https://en.wikipedia.org/wiki?curid=5971809", "title": "List of mathematicians (H)", "text": "List of mathematicians (H)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "11191489", "url": "https://en.wikipedia.org/wiki?curid=11191489", "title": "Manhauling", "text": "Manhauling\n\nManhauling, sometimes expressed as man-hauling: is the pulling forward of sledges, trucks or other load-carrying vehicles by human power unaided by animals or machines. The term is used primarily in connection with travel over snow and ice, and was common during Arctic and Antarctic expeditions before the days of modern motorised traction.\n\nIn the years following the end of the Napoleonic wars the British Royal Navy took up polar/cold climate exploration as its chief peacetime activity. Due to its simplicity, manhauling was adopted by the early British naval expeditions, where it quickly became the preferred even the 'traditional' technique. In time it would be hailed as inherently more noble than the sole use of dogs as practised by the native Arctic-dwelling peoples. The technique’s chief advocate was Sir Clements Markham, President of the Royal Geographical Society during the latter part of the 19th century. A figure of considerable influence, he brought his prejudices to bear on the series of great British Antarctic ventures during the Heroic Age of Antarctic Exploration, in all of which manhauling was predominant.\n\nMany later writers would condemn manhauling, particularly with heavily loaded sledges, as inefficient and wasteful, citing it as a direct cause of the great Antarctic tragedy of 1910–12—the deaths of Captain Scott and his four companions as they man-hauled their way across the Ross Ice Shelf on their return from the South Pole.\n\nLong before the nations of Europe and America became fascinated with polar exploration the native populations of Northern Canada, Greenland, Lapland and Siberia had trained dogs to draw sledges. Attempts by the early polar explorers to adopt these techniques were rarely successful—the handling of “Eskimo” dogs was recognised as a specialized art; This led to the use of manhauling as a simpler alternative, when the Royal Navy began its long association with polar exploration. The first example of manhauling on a naval Arctic expedition was the journey by William Edward Parry across Melville Island in 1820, when he and his party dragged of equipment on a two-wheeled cart. Thereafter man-hauling began to be seen as a natural, even a 'nobler' alternative to the use of dogs. Francis Leopold McClintock earned the title of \"Father of Arctic Sledging\" for his feats of manhauling travel during one of the many expeditions despatched to search for the missing Franklin expedition. Among McClintock’s admirers on that expedition was a 21-year-old midshipman, Clements Markham\n\nBased on his experiences with McClintock and his love for naval traditions, Markham, future President of the Royal Geographical Society, became a fervent believer in the principle that manhauling was the purest form of polar travel. Markham became the driving force behind British Antarctic exploration endeavours in the early 20th century, and was the mentor of Robert Falcon Scott, to whom his thinking and drive were transferred. After his unhappy experiences with dogs in the Antarctic on the \"Discovery\" Expedition, 1901–04 Scott wrote, in his account of the expedition: \"In my mind no journey ever made with dogs can approach the height of that fine conception which is realised when a party of men go forth to face hardships, dangers, and difficulties with their own unaided efforts […] Surely in this case the conquest is more nobly and splendidly won.\"\n\nAn aversion to the use of dogs pervaded all the British expeditions during the Heroic Age of Antarctic Exploration (including those led by Ernest Shackleton), This was baffling to the great Norwegian explorers Fridtjof Nansen and Roald Amundsen. To them manhauling was \"futile toil\", to be avoided at all costs. Edward Wilson, however, on the fatal southern journey during Scott’s 1910–1913 \"Terra Nova\" Expedition expressed a profound relief, as the pole-bound party began its ascent of the Beardmore Glacier after the last of the ponies had been shot: \"Thank God the horses are now done with, and we begin the heavier work ourselves\", he wrote. His companion Lawrence Oates thought differently but kept his counsel. Later, when the Pole had been attained and Amundsen’s prior arrival discovered, Oates privately castigated “our wretched manhauling” as a cause of his party’s defeat.\n\nSome chroniclers have suggested that excessive reliance on manhauling may have cost the lives of Scott’s polar party. Each man pulling a sledge was burning around 6,000 calories a day, and consuming rations producing only 4,500 calories. Max Jones concludes that they were slowly starving to death. Much earlier, an expedition account by James Gordon Hayes had highlighted two principal causes of Scott’s disaster: dietary deficiencies and the decision to rely on men instead of dogs. In 1997, in another history of the expedition, Michael de-la-Noy concludes: “…the whole expedition had been founded upon a blind and very British belief in the moral superiority of human muscle power…Scott thought it more manly for men to haul the sledges themselves. Five of them died as a result”.\n\n\n"}
{"id": "6056217", "url": "https://en.wikipedia.org/wiki?curid=6056217", "title": "Martin Holdgate", "text": "Martin Holdgate\n\nSir Martin Wyatt Holdgate CB (born 14 January 1931 in Horsham, England) is an English biologist and environmental scientist.\n\nHoldgate was born in Horsham, England on 14 January 1931, grew up in Blackpool, and was educated at Arnold School. He then attended Cambridge University as an undergraduate at Queens' College, Cambridge from 1949, graduating in 1952 with degrees in zoology and botany and, subsequently, a doctorate in insect physiology.\n\nHe taught at Manchester University, Durham University and Cambridge, as well as undertaking expeditions to Tristan da Cunha, south-west Chile and the Antarctic. He was Chief Biologist to the British Antarctic Survey, then research director of the Nature Conservancy Council and, for eighteen years, Chief Scientist and head of research at the Department of the Environment. Subsequently, he was Director General of the International Union for Conservation of Nature.\n\nAfter his formal retirement, he was a member of the Royal Commission on Environmental Pollution and served as co-chair of the Intergovernmental Panel on Forests, and Secretary of the UN Secretary General's High-Level Board on Sustainable Development.\n\nHoldgate has received numerous awards and honours for his work.\n\nHoldgate has been President of the Zoological Society of London and of the Freshwater Biological Association. He is also a member and fellow of the Institute of Biology, making him a Chartered Biologist, and entitling him to use the designations C.Biol and F.I.Biol. In July 2014, he was appointed President of Friends of the Lake District.\n\nHoldgate edited the journal \"Antarctic Ecology\" (published for the Scientific Committee on Antarctic Research, by the Academic Press) from the first edition in 1970. His publications include the following:\n\n\n"}
{"id": "32275208", "url": "https://en.wikipedia.org/wiki?curid=32275208", "title": "Marxist Literary Group", "text": "Marxist Literary Group\n\nThe Marxist Literary Group (MLG) is an affiliate of the Modern Language Association centered on scholarly discussion of the contributions of Marxism and the Marxist tradition in the humanities and related disciplines. It holds an annual summer institute, holds sessions at the MLA convention, and publishes the journal \"Mediations\". It is also an affiliate of the Midwest Modern Language Association and occasionally sponsors sessions at other regional MLA conferences.\n\nThe MLG was formed in 1969 by Fredric Jameson and several of his graduate students at the University of California, San Diego. The group emerged from the 1968 MLA conference in New York City. Whereas groups such as the Radical Caucus focused their energies on pedagogy and social activism, the MLG was concerned with providing a firm theoretical grounding for the New Left as well as cultivating Marxist intellectuals.\n\nThe MLG quickly became the largest affiliate group in the MLA, running 14 sessions at the 1975 conference and organizing Marxist scholars nationwide. The first Institute on Culture and Society took place in St.. Cloud, Minnesota, in 1976, including speakers such as Fredric Jameson, Stanley Aronowitz, Terry Eagleton, Gayatri Spivak, Michael Ryan, Gene Holland, June Howard, and John Beverly. Subsequent Institutes have dealt with a wide range of topics, including cultural studies, postmodernism, post-structuralism, psychoanalysis, post-colonial discourse, feminism, and left politics. A newsletter was set up in the early 1970s, which evolved into the journal Mediations by 1991. These activities were instrumental in allowing Marxist theory and criticism to gain a foothold in the academy.\n\n\n\n"}
{"id": "12406330", "url": "https://en.wikipedia.org/wiki?curid=12406330", "title": "Micrixalus kottigeharensis", "text": "Micrixalus kottigeharensis\n\nMicrixalus kottigeharensis (commonly known as Kottigehar dancing frog or Kottigehar torrent frog) is a species of frog in the family Micrixalidae. It is endemic to the Western Ghats in Karnataka, India. It is one of the \"Top 100 Evolutionarily Distinct and Globally Endangered (EDGE) Amphibians\". The specific name means \"from Kottigehara\".\n\nUntil 2014, both \"Micrixalus narainensis\" and \"Micrixalus swamianus\" were considered separate species, but have since been classified as junior synonyms of \"M. kottigeharensis\" based on phylogenetic analyses.\n\nMale \"Micrixalus kottigeharensis\" grow to a snout–vent length of and females to .\n\nThe preferred habitat of \"Micrixalus kottigeharensis\" are fast-flowing streams in primary and secondary forests. It is threatened by habitat loss.\n"}
{"id": "5335049", "url": "https://en.wikipedia.org/wiki?curid=5335049", "title": "Muchea Tracking Station", "text": "Muchea Tracking Station\n\nMuchea Tracking Station was an Earth station in Australia located close to Muchea in the \nShire of Chittering, about north of Perth, Western Australia, built specifically for NASA's Project Mercury.\n\nMuchea was established in 1960, and became operational in March 1961. It was Station No. 8 of the 14 Manned Space Flight Network sites around the world used throughout the project. The only other Australian site was Station No. 9, the Island Lagoon Tracking Station at Woomera, South Australia. These \nstations were managed and operated by the Weapons Research Establishment of the Australian Department of Supply on behalf of NASA.\n\nMuchea was equipped with a \"VERy LOng Range Tracking\" (VERLORT) S band radar operating between 2700 and 2900 MHz. This was an upgraded version of the SCR-584, with its range increased from to , and the diameter of the dish increased from to . It was also equipped with acquisition aid tracking systems, telemetry reception, and air-to-ground voice communications facilities. Because of its position, close to the antipodes of Cape Canaveral, it was also selected to have a command facility. Information about the range, bearing and elevation of the spacecraft was automatically relayed to the Goddard Space Flight Center by teleprinter.\n\nDuring each mission a NASA team consisting of two flight controllers and a flight surgeon were sent to Muchea. The Senior Flight Controller, usually another astronaut, acted as capsule communicator (CAPCOM).\n\nMuchea Communications Technician Gerry O'Connor became the first Australian to speak with an astronaut on 20 February 1962, when he contacted John Glenn aboard \"Friendship 7\" on his first pass over the West Australian coast. A small plaque has been installed on the spot occupied by the Communications Technician's console which reads: \"This plaque is to mark the spot where an Australian first spoke to a space traveller\".\n\nMuchea was closed in February 1964, after the end of the Mercury Project. It was replaced by the Carnarvon Tracking Station for the Gemini and Apollo projects. Although the Muchea Tracking Station no longer exists, the Shire of Chittering has erected a small display about its history.\n\nThe following missions were supported by the Muchea Station:\n\n"}
{"id": "5609120", "url": "https://en.wikipedia.org/wiki?curid=5609120", "title": "Nikolaus Joseph Brahm", "text": "Nikolaus Joseph Brahm\n\nNikolaus Joseph Brahm, was a German zoologist (18 May 1754, Mainz – 29 June 1821, Mainz)\n"}
{"id": "5686785", "url": "https://en.wikipedia.org/wiki?curid=5686785", "title": "Northrop Grumman Switchblade", "text": "Northrop Grumman Switchblade\n\nThe Switchblade was a proposed unmanned aerial vehicle developed by Northrop Grumman for the United States. The United States Defence Advanced Research Projects Agency (DARPA) awarded Northrop Grumman a US$10.3 million contract for risk reduction and preliminary planning for an X-plane oblique flying wing demonstrator.\n\nThe program aimed at producing a technology demonstrator aircraft to explore the various challenges which the radical design entails. The proposed aircraft would be a purely flying wing (an aircraft with no other auxiliary surfaces such as tails, canards or a fuselage) where the wing is swept with one side of the aircraft forward, and one backwards in an asymmetric fashion. This aircraft configuration is believed to give it a combination of high speed, long range and long endurance. The program entailed two phases. Phase I explored the theory and result in a conceptual design, while Phase II would have resulted in the design, manufacture and flight test of an aircraft. The outcome of the program would have resulted in a dataset that could then be used when considering future military aircraft designs.\n\nFlight of the Switchblade was scheduled for 2020 with its 61-meter long oblique wing perpendicular to its engines like a typical aircraft. As the aircraft increased speed, the wing begins to pivot, so that when it breaks the sound barrier, its wing has swiveled 60 degrees, with one wingtip pointing forward and the other backward. The change in aerodynamics and the general structure would have made the aircraft very difficult to control for a human being. The plane was to be totally controlled by an on-board computer controlling flight parameters. Following Phase I the aircraft concept was cancelled in 2008.\n\nBoth Messerschmitt and Blohm + Voss aircraft companies worked on the asymmetric wing concept at the end of World War II. The Blohm & Voss P 202 and ME P.1109\nwere their initial attempts at implementing this concept.\n\nFrom 1979 to 1982, NASA successfully flew the piloted AD-1 demonstrator aircraft which validated the oblique wing concept.\n\n\n\n"}
{"id": "27327761", "url": "https://en.wikipedia.org/wiki?curid=27327761", "title": "Orion Lite", "text": "Orion Lite\n\nOrion Lite was an unofficial name used in the media for a lightweight crew capsule proposed by Bigelow Aerospace in collaboration with Lockheed Martin. It was to be based on the Orion spacecraft that Lockheed Martin was developing for NASA. It would be a lighter, less capable and cheaper version of the full Orion.\n\nThe intention of designing Orion Lite would be to provide a stripped down version of the Orion that will be available for missions to the International Space Station earlier than the more capable Orion, which is designed for longer duration missions to the Moon, Mars, Lagrange points, and Near Earth asteroids.\n\nBigelow began working with Lockheed Martin in 2004. A few years later Bigelow signed a million-dollar contract to develop \"an Orion mockup, an Orion Lite.\"\n\nOrion Lite's primary mission would be to transport crew to the International Space Station, or to private space stations such as the proposed Sundancer from Bigelow Aerospace. While Orion Lite would have the same exterior dimensions as the Orion, there would be no need for the deep space infrastructure present in the Orion configuration. As such, the Orion Lite will be able to support larger crews of around 7 people as the result of greater habitable interior volume and the reduced weight of equipment needed to support an exclusively low-Earth-orbit configuration.\n\nThe Orion Lite is also not to be confused with the Orion Crew Return Vehicle, a scaled-down version of Orion proposed by the Obama administration. In an earlier budget proposal the Orion had been slated for cancellation altogether.\n\nThe proposed collaboration between Bigelow and Lockheed Martin on the Orion Lite spacecraft has ended. Bigelow began work with Boeing on a similar capsule, the CST-100, which has no Orion heritage. The CST-100 was selected under NASA's Commercial Crew Development (CCDev) program to transport crew to the International Space Station.\n\nThe Orion Lite is proposed to be compatible with multiple launchers, including the existing Atlas V rocket. A human-rated version of the Atlas V would have to be developed in conjunction with the Orion Lite. Work on this has started, funded by stimulus funds granted under NASA's Commercial Crew Development (CCDev) program. Likewise, a human-rated version of the SpaceX Falcon 9 has also been suggested by Bigelow as a possible launch system.\n\nIn order to reduce the weight of Orion Lite, the more durable heat shield of the Orion would be replaced with a lighter weight heat shield designed to support the lower temperatures of Earth atmospheric re-entry from low Earth orbit. Additionally, the current proposal calls for a mid-air retrieval, wherein another aircraft captures the descending Orion Lite module. To date, such a retrieval method has not been employed for manned spacecraft, although it has been used with satellites.\n"}
{"id": "3209677", "url": "https://en.wikipedia.org/wiki?curid=3209677", "title": "Outline of biochemistry", "text": "Outline of biochemistry\n\nThe following outline is provided as an overview of and topical guide to biochemistry:\n\nBiochemistry – study of chemical processes in living organisms, including living matter. Biochemistry governs all living organisms and living processes.\n\n\n\nBiotechnology,\nBioluminescence,\nMolecular chemistry,\nEnzymatic chemistry,\nGenetic engineering,\nPharmaceuticals,\nEndocrinology,\nNeurochemistry\nHematology,\nNutrition,\nPhotosynthesis,\nEnvironmental,\nToxicology\n\nHistory of biochemistry\n\n\n\n\n\n\n"}
{"id": "25586758", "url": "https://en.wikipedia.org/wiki?curid=25586758", "title": "Regional Input-Output Modeling System", "text": "Regional Input-Output Modeling System\n\nRegional Input-Output Modeling System (RIMS II)\nThe Regional Input-Output Modeling System (RIMS II) is a regional economic model developed and maintained by the US Bureau of Economic Analysis (BEA).\n\nRegional input-output multipliers such as the RIMS II multipliers allow estimates of how a one-time or sustained increase in economic activity in a particular region will impact other industries located in the region - i.e., estimating local shocks on gross output, value added, earnings, and employment. RIMS II multipliers differ from macro-economic multipliers, which are used to assess the effects of fiscal stimulus on gross national product. Differences in industry-specific regional multipliers are not meaningful, nor appropriate for use in a national context.\n\nRIMS II allows for estimates at the regional level because the multipliers are based on BEA data at the national and regional level.\n\nRIMS II multipliers have been used by both the public and private sectors. There are numerous examples of their use:\n\n\nRIMS II provides six types of multipliers: final-demand multipliers for output, earnings, employment, and value added; and direct-effect multipliers for earnings and employment.\n\nHistory\n\nThe BEA introduced the RIMS tool in the early 1970s; an enhancement of the framework was completed in the 1980s, introducing RIMS II, which is the current framework.\n\nThe RIMS II framework is updated every few years with new data, depending on the resources available to the BEA. For example, after 2012, the BEA stopped releasing new updates to the multipliers. In 2015, the BEA began re-releasing the multipliers; they are generally released on an annual basis.\n\nData\n\nThe RIMS II framework currently encompasses multipliers for the \"benchmark series\". In past, the BEA also released an \"annual series;\" however, the annual series has been discontinued.\n\nThe RIMS II benchmark multiplier system is based national benchmark input-output data combined with regional data. Often, the national dataset is a few years behind the regional dataset.\n\nReleases\n\nThis list currently does not go through all releases of the BEA RIMS II multipliers, but does list the last few and the associated datasets:\n\n"}
{"id": "36520327", "url": "https://en.wikipedia.org/wiki?curid=36520327", "title": "Robert Brendel", "text": "Robert Brendel\n\nRobert Brendel (c. 1821–1898) and his son Reinhold Brendel (c. 1861–1927) were botanical modelmakers in first Breslau then Grunewald Berlin.\n\nThey produced accurate, large-scale models of plant structures. These were sold to technical universities teaching practical botany.\n\n\n"}
{"id": "17417840", "url": "https://en.wikipedia.org/wiki?curid=17417840", "title": "Russell Ormond Redman", "text": "Russell Ormond Redman\n\nRussell Ormond Redman (born 1951) is a Canadian astronomer and a specialist in radio astronomy who worked on the staff of the National Research Council of Canada at the Dominion Astrophysical Observatory until he retired in 2013. In 1966, just after 9th grade, he first volunteered for the DAO in Victoria, British Columbia. His initial publication was a list of nearest stars in the 1970 \"Observer's Handbook\", while he was still in high school. He received his Ph.D. from California Institute of Technology in 1982, and subsequently published over 75 scientific papers.\n\nThe inner main-belt asteroid 7886 Redman, discovered by Canadian astronomer David D. Balam in 1993, has been named jointly for him and for Roderick Oliver Redman, Professor of Astronomy at Cambridge University, no relation except that both worked at the DAO during significant parts of their careers. The official naming citation was published on 10 June 1998 ().\n\n"}
{"id": "50705340", "url": "https://en.wikipedia.org/wiki?curid=50705340", "title": "Samuel Sinyangwe", "text": "Samuel Sinyangwe\n\nSamuel Sinyangwe (born c. 1990) is an American policy analyst and racial justice activist. Sinyangwe is a member of the Movement for Black Lives and a co-founder of We the Protestors, a group of digital tools that include Mapping Police Violence, a database of police killings in the United States, and Campaign Zero, a policy platform to end police violence. Sinyangwe is a co-host the \"Pod Save the People\" podcast, where he discusses the week's news with a panel of other activists.\n\nSinyangwe was born circa 1990. He grew up in College Park neighborhood of Orlando, Florida and attended Winter Park High School in the International Baccalaureate program. He has discussed the influence of his upbringing in Florida, where he was a black child often surrounded by white peers, on his eventual career trajectory; he was shaken and moved to action after the 2013 acquittal of George Zimmerman in the shooting death of Trayvon Martin in Sanford, Florida, where Sinyangwe had regularly attended soccer practice: \"I was that kid. I could have been Trayvon. That’s why it hit me so personally and that’s why I realized that needed to be something that took the priority in terms of my focus.\"\n\nSinyangwe graduated from Stanford University, where he studied how race intersects with American politics, economics, and class.\n\nSinyangwe started his career at PolicyLink with the Promise Neighborhoods Institute. As protests emerged in the wake of the 2014 shooting of Michael Brown in Ferguson, Missouri, he connected with Ferguson activists online, ultimately taking a leave of absence from his job to join them in Missouri. With DeRay Mckesson and Johnetta Elzie, he began working to develop policy solutions to address police violence in America. Sinyangwe particularly noticed the absence of official government statistics on police violence and began compiling them from other sources like Fatal Encounters and KilledbythePolice.net, in order to challenge claims about police shootings being rare events or only resulting from resisting arrest.\n\nWith other activists, Sinyangwe founded We the Protestors, an organization aimed at developing a set of digital tools to support Black Lives Matter activism. We the Protestors projects include a database of police killings, Mapping Police Violence, and a platform of policy solutions to end police violence called Campaign Zero. Sinyangwe also serves as a data scientist for OurStates.org, a project focused on state legislatures and with Mckesson and Brittney Packnett founded the Resistance Manual, an open-source project aimed at connecting anti-racist activists with activists focused on intersecting issues.\n\nDuring the 2016 U.S. Presidential campaign, Sinyangwe and colleagues met with Democratic candidates Bernie Sanders and Hillary Clinton on these policy issues. He has been a vocal critic of the \"Ferguson Effect\", using data to refute the theory that policing had diminished and crime increased in face of activist scrutiny of police use of force. Melissa Harris-Perry has compared Sinyangwe to journalist and anti-lynching activist Ida B. Wells, noting that Wells began her work by \"compil[ing] the data, the social science and research about how, when and where lynchings were happening to begin to make it stop.\"\n\nSinyangwe is a co-host of Mckesson's podcast \"Pod Save the People\", which discusses the week's news with a panel of other activists including Mckesson, Packnett and Clint Smith. The podcast particularly focuses on race, grassroots activism, discrimination and other forms of inequality; recommending \"Pod Save The People\" in \"GQ\", June Diane Raphael of \"How Did This Get Made?\" wrote, \"The stories they uplift and think critically about are the ones I'm now wondering why I've never been exposed to/exposed myself to.\" Sinyangwe has also been featured on CNN, MSNBC, BBC News, FiveThirtyEight, \"The Los Angeles Times\", and other publications. He has written for the \"Huffington Post\" and \"The Guardian\".\n\nIn 2017, Sinyangwe was named to the \"Forbes\" 30 Under 30 list for law and policy. He was also a 2017 Echoing Green Black Male Achievement Fellow.\n\nSinyangwe lives in New York City.\n\n"}
{"id": "275597", "url": "https://en.wikipedia.org/wiki?curid=275597", "title": "Schön scandal", "text": "Schön scandal\n\nThe Schön scandal concerns German physicist Jan Hendrik Schön (born August 1970 in Verden an der Aller, Lower Saxony, Germany) who briefly rose to prominence after a series of apparent breakthroughs with semiconductors that were later discovered to be fraudulent. Before he was exposed, Schön had received the Otto-Klung-Weberbank Prize for Physics and the Braunschweig Prize in 2001, as well as the Outstanding Young Investigator Award of the Materials Research Society in 2002, both of which were later rescinded.\n\nThe scandal provoked discussion in the scientific community about the degree of responsibility of coauthors and reviewers of scientific articles. The debate centered on whether peer review, traditionally designed to find errors and determine relevance and originality of articles, should also be required to detect deliberate fraud.\n\nSchön's field of research was condensed matter physics and nanotechnology. He received his PhD from the University of Konstanz in 1997. In late 1997 he was hired by Bell Labs, located in New Jersey, United States. There, he worked on electronics in which conventional semiconducting elements (such as silicon) were replaced by crystalline organic materials. Specific organic materials can conduct electrical currents, and in a field-effect transistor (a refined implementation of the transistor effect, which was pioneered in 1947 in the same laboratory) the conductance can be switched on or off, a basic function in the field of electronics. Schön, however, claimed spectacular on/off behavior, far beyond anything achieved thus far with organic materials. His measurements in most cases confirmed various theoretical predictions, for example that the organic materials could be made to display superconductivity or be used in lasers. The findings were published in prominent scientific publications, including the journals \"Science\" and \"Nature\", and gained worldwide attention. However, no research group anywhere in the world succeeded in reproducing the results claimed by Schön.\n\nIn 2001 he was listed as an author on an average of one newly published research paper every eight days. In that year he announced in \"Nature\" that he had produced a transistor on the molecular scale. Schön claimed to have used a thin layer of organic dye molecules to assemble an electric circuit that, when acted on by an electric current, behaved as a transistor. The implications of his work were significant. It would have been the beginning of a move away from silicon-based electronics and towards organic electronics. It would have allowed chips to continue shrinking past the point at which silicon breaks down, and therefore continue Moore's law for much longer than is currently predicted. It also would have drastically reduced the cost of electronics.\n\nA key element in Schön's claimed successful observation of various physical phenomena in organic materials was in the transistor setup, specifically, a thin layer of aluminium oxide, which Schön incorporated in the transistors using lab-facilities of the University of Konstanz in Germany. Although the equipment and materials used were commonly used by laboratories all over the world, no one succeeded in preparing aluminium oxide layers of similar quality as claimed by Schön.\n\nAs recounted by Dan Agin in his book \"Junk Science\", soon after Schön published his work on single-molecule semiconductors, others in the physics community alleged that his data contained anomalies. Lydia Sohn, then of Princeton University, noticed that two experiments carried out at very different temperatures had identical noise. When the editors of \"Nature\" pointed this out to Schön, he claimed to have accidentally submitted the same graph twice. Paul McEuen of Cornell University then found the same noise in a paper describing a third experiment. More research by McEuen, Lynn Loo, and other physicists uncovered a number of examples of duplicate data in Schön's work. This triggered a series of reactions that quickly led Lucent Technologies (which ran Bell Labs) to start a formal investigation.\n\nIn May 2002, Bell Labs set up a committee to investigate, with Malcolm Beasley from Stanford University as chair. The committee obtained information from all of Schön's coauthors and interviewed the three principal ones (Zhenan Bao, Bertram Batlogg and Christian Kloc). It examined electronic drafts of the disputed articles, which included processed numeric data. The committee requested copies of the raw data, but found that Schön had kept no laboratory notebooks. His raw-data files had been erased from his computer. According to Schön, the files were erased because his computer had limited hard-drive space. In addition, all of his experimental samples had been discarded or damaged beyond repair.\n\nOn September 25, 2002, the committee publicly released its report. The report contained details of 24 allegations of misconduct. They found evidence of Schön's scientific misconduct in at least 16 of them. They found that whole data sets had been reused in a number of different experiments. They also found that some of his graphs, which purportedly had been plotted from experimental data, had instead been produced using mathematical functions.\n\nThe report found that all of the misdeeds had been performed by Schön alone. All of the coauthors (including Bertram Batlogg, who was the head of the team) were exonerated of scientific misconduct. This sparked widespread debate in the scientific community on how the blame for misconduct should be shared among co-authors, particularly when they share a significant part of the credit.\n\nSchön acknowledged that the data were incorrect in many of these articles. He claimed that the substitutions could have occurred by honest mistake. He omitted some data and stated that he did so to show more convincing evidence for behaviour that he observed.\n\nExperimenters at Delft University of Technology and the Thomas J. Watson Research Center have since performed experiments similar to Schön's, without achieving similar results. Even before the allegations had become public, several research groups had tried to reproduce most of his spectacular results in the field of the physics of organic molecular materials without success.\n\nSchön returned to Germany and took a job at an engineering firm.\n\nIn June 2004 the University of Konstanz issued a press release stating that Schön's doctoral degree had been revoked due to \"dishonourable conduct\". Department of Physics spokesman Wolfgang Dieterich called the affair the \"biggest fraud in physics in the last 50 years\" and said that the \"credibility of science had been brought into disrepute\". Schön appealed the ruling, but on October 28, 2009, it was upheld by the university. In response, Schön sued the university and appeared in court to testify on September 23, 2010. The court overturned the university's decision on September 27, 2010. However, in November 2010 the university moved to appeal the court's ruling. The state court ruled in September 2011 that the university was correct in revoking his doctorate. The Federal Administrative Court upheld the state court's decision in July 2013, and the Federal Constitutional Court confirmed it in September 2014.\n\nIn the meantime, in October 2004, the Deutsche Forschungsgemeinschaft (DFG, the German Research Foundation) Joint Committee announced sanctions against him. The former DFG post-doctorate fellow was deprived of his active right to vote in DFG elections or serve on DFG committees for an eight-year period. During that period, Schön was also unable to serve as a peer reviewer or apply for DFG funds.\n\nOn October 31, 2002, \"Science\" withdrew eight articles written by Schön:\n\nOn December 20, 2002, \"Physical Review\" withdrew six articles written by Schön:\n\nOn February 24, 2003, \"Applied Physics Letters\" withdrew four articles written by Schön:\n\nOn May 2, 2003, \"Science\" withdrew another article written by Schön:\n\nOn March 20, 2003, \"Advanced Materials\" withdrew two articles written by Schön:\n\nOn March 5, 2003, \"Nature\" withdrew seven articles written by Schön:\n\nThe retraction notices from February 24, 2003 in \"Applied Physics Letters\" relayed concerns about seven articles written by Schön and published in the \"Applied Physics Letters\":\n\nThe retraction notice from March 20, 2003 in \"Advanced Materials\" mentions concerns about another article written by Schön:\n\n\n\n"}
{"id": "43870570", "url": "https://en.wikipedia.org/wiki?curid=43870570", "title": "SciTech (magazine)", "text": "SciTech (magazine)\n\nSciTech is a Serbian science magazine, containing sections on physics, biology, technology, and chemistry.\n"}
{"id": "29588", "url": "https://en.wikipedia.org/wiki?curid=29588", "title": "Sextant", "text": "Sextant\n\nA sextant is a doubly reflecting navigation instrument that measures the angular distance between two visible objects. The primary use of a sextant is to measure the angle between an astronomical object and the horizon for the purposes of celestial navigation. The estimation of this angle, the altitude, is known as \"sighting\" or \"shooting\" the object, or \"taking a sight\". The angle, and the time when it was measured, can be used to calculate a position line on a nautical or aeronautical chart—for example, sighting the Sun at noon or Polaris at night (in the Northern Hemisphere) to estimate latitude. Sighting the height of a landmark can give a measure of \"distance off\" and, held horizontally, a sextant can measure angles between objects for a position on a chart. A sextant can also be used to measure the lunar distance between the moon and another celestial object (such as a star or planet) in order to determine Greenwich Mean Time and hence longitude. The principle of the instrument was first implemented around 1731 by John Hadley (1682–1744) and Thomas Godfrey (1704–1749), but it was also found later in the unpublished writings of Isaac Newton (1643–1727). Additional links can be found to Bartholomew Gosnold (1571–1607) indicating that the use of a sextant for nautical navigation predates Hadley's implementation. In 1922, it was modified for aeronautical navigation by Portuguese navigator and naval officer .\n\nThis section discusses navigators' sextants. Most of what is said about these specific sextants applies equally to other types of sextants. Navigators' sextants were primarily used for ocean navigation.\n\nLike the Davis quadrant, the sextant allows celestial objects to be measured relative to the horizon, rather than relative to the instrument. This allows excellent precision. However, unlike the backstaff, the sextant allows direct observations of stars. This permits the use of the sextant at night when a backstaff is difficult to use. For solar observations, filters allow direct observation of the sun.\n\nSince the measurement is relative to the horizon, the measuring pointer is a beam of light that reaches to the horizon. The measurement is thus limited by the angular accuracy of the instrument and not the sine error of the length of an alidade, as it is in a mariner's astrolabe or similar older instrument.\n\nA sextant does not require a completely steady aim, because it measures a relative angle. For example, when a sextant is used on a moving ship, the image of both horizon and celestial object will move around in the field of view. However, the relative position of the two images will remain steady, and as long as the user can determine when the celestial object touches the horizon, the accuracy of the measurement will remain high compared to the magnitude of the movement.\n\nThe sextant is not dependent upon electricity (unlike many forms of modern navigation) or anything human-controlled (like GPS satellites). For these reasons, it is considered an eminently practical back-up navigation tool for ships.\n\nThe frame of a sextant is in the shape of a sector which is approximately of a circle (60°), hence its name (\"sextāns, -antis\" is the Latin word for \"one sixth\"). Both smaller and larger instruments are (or were) in use: the octant, quintant (or pentant) and the (doubly reflecting) quadrant span sectors of approximately of a circle (45°), of a circle (72°) and of a circle (90°), respectively. All of these instruments may be termed \"sextants\".\n\nAttached to the frame are the \"horizon mirror\", an \"index arm\" which moves the \"index mirror\", a sighting telescope, sun shades, a graduated scale and a micrometer drum gauge for accurate measurements. The scale must be graduated so that the marked degree divisions register twice the angle through which the index arm turns. The scales of the octant, sextant, quintant and quadrant are graduated from below zero to 90°, 120°, 140° and 180° respectively. For example, the sextant shown alongside has a scale graduated from −10° to 142°, so that is basically a quintant: the frame is a sector of a circle subtending an angle of 76° (not 72°) at the pivot of the index arm.\n\nThe necessity for the doubled scale reading follows by consideration of the relations of the fixed ray (between the mirrors), the object ray (from the sighted object) and the direction of the normal perpendicular to the index mirror. When the index arm moves by an angle, say 20°, the angle between the fixed ray and the normal also increases by 20°. But the angle of incidence equals the angle of reflection so the angle between the object ray and the normal must also increase by 20°. The angle between the fixed ray and the object ray must therefore increase by 40°. This is the case shown in the graphic alongside.\n\nThere are two types of horizon mirrors on the market today. Both types give good results.\n\nTraditional sextants have a half-horizon mirror, which divides the field of view in two. On one side, there is a view of the horizon; on the other side, a view of the celestial object. The advantage of this type is that both the horizon and celestial object are bright and as clear as possible. This is superior at night and in haze, when the horizon can be difficult to see. However, one has to sweep the celestial object to ensure that the lowest limb of the celestial object touches the horizon.\n\nWhole-horizon sextants use a half-silvered horizon mirror to provide a full view of the horizon. This makes it easy to see when the bottom limb of a celestial object touches the horizon. Since most sights are of the sun or moon, and haze is rare without overcast, the low-light advantages of the half-horizon mirror are rarely important in practice.\n\nIn both types, larger mirrors give a larger field of view, and thus make it easier to find a celestial object. Modern sextants often have 5 cm or larger mirrors, while 19th-century sextants rarely had a mirror larger than 2.5 cm (one inch). In large part, this is because precision flat mirrors have grown less expensive to manufacture and to silver.\n\nAn artificial horizon is useful when the horizon is invisible, as occurs in fog, on moonless nights, in a calm, when sighting through a window or on land surrounded by trees or buildings. Professional sextants can mount an artificial horizon in place of the horizon-mirror assembly. An artificial horizon is usually a mirror that views a fluid-filled tube with a bubble.\n\nMost sextants also have filters for use when viewing the sun and reducing the effects of haze. The filters usually consist of a series of progressively darker glasses that can be used singly or in combination to reduce haze and the sun's brightness. However, sextants with adjustable polarizing filters have also been manufactured, where the degree of darkness is adjusted by twisting the frame of the filter.\n\nMost sextants mount a 1 or 3-power monocular for viewing. Many users prefer a simple sighting tube, which has a wider, brighter field of view and is easier to use at night. Some navigators mount a light-amplifying monocular to help see the horizon on moonless nights. Others prefer to use a lit artificial horizon.\n\nProfessional sextants use a click-stop degree measure and a worm adjustment that reads to a minute, 1/60 of a degree. Most sextants also include a vernier on the worm dial that reads to 0.1 minute. Since 1 minute of error is about a nautical mile, the best possible accuracy of celestial navigation is about . At sea, results within several nautical miles, well within visual range, are acceptable. A highly skilled and experienced navigator can determine position to an accuracy of about .\n\nA change in temperature can warp the arc, creating inaccuracies. Many navigators purchase weatherproof cases so that their sextant can be placed outside the cabin to come to equilibrium with outside temperatures. The standard frame designs (see illustration) are supposed to equalise differential angular error from temperature changes. The handle is separated from the arc and frame so that body heat does not warp the frame. Sextants for tropical use are often painted white to reflect sunlight and remain relatively cool. High-precision sextants have an invar (a special low-expansion steel) frame and arc. Some scientific sextants have been constructed of quartz or ceramics with even lower expansions. Many commercial sextants use low-expansion brass or aluminium. Brass is lower-expansion than aluminium, but aluminium sextants are lighter and less tiring to use. Some say they are more accurate because one's hand trembles less. Solid brass frame sextants are less susceptible to wobbling in high winds or when the vessel is working in heavy seas, but as noted are substantially heavier. Sextants with aluminum frames and brass arcs have also been manufactured. Essentially, a sextant is intensely personal to each navigator, and he or she will choose whichever model has the features which suit them best.\n\nAircraft sextants are now out of production, but had special features. Most had artificial horizons to permit taking a sight through a flush overhead window. Some also had mechanical averagers to make hundreds of measurements per sight for compensation of random accelerations in the artificial horizon's fluid. Older aircraft sextants had two visual paths, one standard and the other designed for use in open-cockpit aircraft that let one view from directly over the sextant in one's lap. More modern aircraft sextants were periscopic with only a small projection above the fuselage. With these, the navigator pre-computed his sight and then noted the difference in observed versus predicted height of the body to determine his position.\n\nA \"sight\" (or \"measure\") of the angle between the sun, a star, or a planet, and the horizon is done with the 'star telescope' fitted to the sextant using a visible horizon. On a vessel at sea even on misty days a sight may be done from a low height above the water to give a more definite, better horizon. Navigators hold the sextant by its handle in the right hand, avoiding touching the arc with the fingers.\n\nFor a sun sight, a filter is used to overcome the glare such as \"shades\" covering both index mirror and the horizon mirror designed to prevent eye damage. By setting the index bar to zero, the sun can be viewed through the telescope. Releasing the index bar (either by releasing a clamping screw, or on modern instruments, using the quick-release button), the image of the sun can be brought down to about the level of the horizon. It is necessary to flip back the horizon mirror shade to be able to see the horizon, and then the fine adjustment screw on the end of the index bar is turned until the bottom curve (the \"lower limb\") of the sun just touches the horizon. 'Swinging' the sextant about the axis of the telescope ensures that the reading is being taken with the instrument held vertically. The angle of the sight is then read from the scale on the arc, making use of the micrometer or vernier scale provided. The exact time of the sight must also be noted simultaneously, and the height of the eye above sea-level recorded.\n\nAn alternative method is to estimate the current altitude (angle) of the sun from navigation tables, then set the index bar to that angle on the arc, apply suitable shades only to the index mirror, and point the instrument directly at the horizon, sweeping it from side to side until a flash of the sun's rays are seen in the telescope. Fine adjustments are then made as above. This method is less likely to be successful for sighting stars and planets.\n\nStar and planet sights are normally taken during nautical twilight at dawn or dusk, while both the heavenly bodies and the sea horizon are visible. There is no need to use shades or to distinguish the lower limb as the body appears as a mere point in the telescope. The moon can be sighted, but it appears to move very fast, appears to have different sizes at different times, and sometimes only the lower or upper limb can be distinguished due to its phase.\n\nAfter a sight is taken, it is reduced to a position by looking at several mathematical procedures. The simplest sight reduction is to draw the equal-altitude circle of the sighted celestial object on a globe. The intersection of that circle with a dead-reckoning track, or another sighting, gives a more precise location.\n\nSextants can be used very accurately to measure other visible angles, for example between one heavenly body and another and between landmarks ashore. Used horizontally, a sextant can measure the apparent angle between two landmarks such as a lighthouse and a church spire, which can then be used to find the distance \"off\" or out to sea (provided the distance between the two landmarks is known). Used vertically, a measurement of the angle between the lantern of a lighthouse of known height and the sea level at its base can also be used for distance off.\n\nDue to the sensitivity of the instrument it is easy to knock the mirrors out of adjustment. For this reason a sextant should be checked frequently for errors and adjusted accordingly.\n\nThere are four errors that can be adjusted by the navigator and they should be removed in the following order.\n\n\n\n"}
{"id": "6027945", "url": "https://en.wikipedia.org/wiki?curid=6027945", "title": "Sexual dimorphism measures", "text": "Sexual dimorphism measures\n\nAlthough the subject of sexual dimorphism is not in itself controversial, the measures by which it is assessed differ widely. Most of the measures are used on the assumption that a random variable is considered so that probability distributions should be taken into account. In this review, a series of sexual dimorphism measures are discussed concerning both their definition and the probability law on which they are based. Most of them are sample functions, or statistics, which account for only partial characteristics, for example the mean or expected value, of the distribution involved. Further, the most widely used measure fails to incorporate an inferential support.\n\nIt is widely known that sexual dimorphism is an important component of the morphological variation in biological populations (see, e.g., Klein and Cruz-Uribe, 1983; Oxnard, 1987; Kelley, 1993). In higher Primates, sexual dimorphism is also related to some aspects of the social organization and behavior (Alexander \"et al.\", 1979; Clutton-Brock, 1985). Thus, it has been observed that the most dimorphic species tend to polygyny and a social organization based on male dominance, whereas in the less dimorphic species, monogamy and family groups are more common. Fleagle \"et al.\" (1980) and Kay (1982), on the other hand, have suggested that the behavior of extinct species can be inferred on the basis of sexual dimorphism and, e.g. Plavcan and van Schaick (1992) think that sex differences in size among primate species reflect processes of an ecological and social nature. Some references on sexual dimorphism regarding human populations can be seen in Lovejoy (1981), Borgognini Tarli and Repetto (1986) and Kappelman (1996).\n\nThese biological facts do not appear to be controversial. However, they are based on a series of different sexual dimorphism measures, or indices. Sexual dimorphism, in most works, is measured on the assumption that a random variable is being taken into account. This means that there is a law which accounts for the behavior of the whole set of values that compose the domain of the random variable, a law which is called distribution function. Because both studies of sexual dimorphism aim at establishing differences, in some random variable, between sexes and the behavior of the random variable is accounted for by its distribution function, it follows that a sexual dimorphism study should be equivalent to a study whose main purpose is to determine to what extent the two distribution functions - one per sex - overlap (see shaded area in Fig. 1, where two normal distributions are represented).\nIn Borgognini Tarli and Repetto (1986) an account of indices based on sample means can be seen. Perhaps, the most widely used is the quotient,\n\nwhere formula_2 is the sample mean of one sex (e.g., male) and formula_3 the corresponding mean of the other. Nonetheless, for instance,\n\nhave also been proposed.\n\nGoing over the works where these indices are used, the reader misses any reference to their parametric counterpart (see reference above). In other words, if we suppose that the quotient of two sample means is considered, no work can be found where, in order to make inferences, the way in which the quotient is used as a point estimate of\n\nis discussed.\n\nBy assuming that differences between populations are the objective to analyze, when quotients of sample means are used it is important to point out that the only feature of these populations that seems to be interesting is the mean parameter. However, a population has also variance, as well as a shape which is defined by its distribution function (notice that, in general, this function depends on parameters such as means or variances).\n\nMarini \"et al.\" (1999) have illustrated that it is a good idea to consider something other than sample means when sexual dimorphism is analyzed. Possibly, the main reason is that the intrasexual variability influences both the manifestation of dimorphism and its interpretation.\n\nIt is likely that, within this type of indices, the one used the most is the well-known statistic with Student's \"t\" distribution (see, for instance, Green, 1989). Marini \"et al.\" (1999) have observed that variability among females seems to be lower than among males, so that it appears advisable to use the form of the Student's \"t\" statistic with degrees of freedom given by the Welch-Satterthwaite approximation,\n\nwhere formula_10 are sample variances and sample sizes, respectively.\n\nAnyway, it is important to point out the following,\n\nHowever, in sexual dimorphism analyses, it does not appear reasonably (see Ipiña and Durand, 2000) to assume that two independent random samples have been selected. Rather on the contrary, when we sample we select some random observations - making up one sample - that sometimes correspond to one sex and sometimes to the other.\n\nChakraborty and Majumder (1982) have proposed an index of sexual dimorphism that is the overlapping area - to be precise, its complement - of two normal density functions (see Fig. 1). Therefore, it is a function of four parameters formula_12 (expected values and variances, respectively), and takes the shape of the two normals into account. Inman and Bradley (1989) have discussed this overlapping area as a measure to assess the distance between two normal densities.\n\nRegarding inferences, Chakraborty and Majumder proposed a sample function constructed by considering the Laplace-DeMoivre's theorem (an application to binomial laws of the central limit theorem). According to these authors, the variance of such a statistic is,\n\nwhere formula_14 is the statistic, and formula_15 (male, female) stand for the estimate of the probability of observing the measurement of an individual of the formula_16 sex in some interval of the real line, and the sample size of the \"i\" sex, respectively. Notice that this implies that two independent random variables with binomial distributions have to be regarded. One of such variables is \"number of individuals of the f sex in a sample of size formula_17 composed of individuals of the f sex\", which seems nonsensical.\n\nAuthors such as Josephson \"et al.\" (1996) believe that the two sexes to be analyzed form a single population with a probabilistic behavior denominated a mixture of two normal populations. Thus, if formula_18 is a random variable which is normally distributed among the females of a population and likewise this variable is normally distributed among the males of the population, then,\n\nis the density of the mixture with two normal components, where formula_20 are the normal densities and the mixing proportions of both sexes, respectively. See an example in Fig. 2 where the thicker curve represents the mixture whereas the thinner curves are the formula_21 functions.\nIt is from a population modelled like this that a random sample with individuals of both sexes can be selected. Note that on this sample tests which are based on the normal assumption cannot be applied since, in a mixture of two normal components, formula_21 is not a normal density.\n\nJosephson \"et al.\" limited themselves to considering two normal mixtures with the same component variances and mixing proportions. As a consequence, their proposal to measure sexual dimorphism is the difference between the mean parameters of the two normals involved. In estimating these central parameters, the procedure used by Josephson \"et al.\" is the one of Pearson's moments. Nowadays, the EM expectation maximization algorithm (see McLachlan and Basford, 1988) and the MCMC Markov chain Monte Carlo Bayesian procedure (see Gilks \"et al.\", 1996) are the two competitors for estimating mixture parameters.\n\nPossibly the main difference between considering two independent normal populations and a mixture model of two normal components is in the mixing proportions, which is the same as saying that in the two independent normal population model the interaction between sexes is ignored. This, in turn implies that probabilistic properties change (see Ipiña and Durand, 2000).\n\nIpiña and Durand (2000, 2004) have proposed a measure of sexual dimorphism called formula_23. This proposal computes the overlapping area between the formula_24 and formula_25 functions, which represent the contribution of each sex to the two normal components mixture (see shaded area in Fig. 2). Thus, formula_23 can be written,\n\nformula_28 being the real line.\n\nThe smaller the overlapping area the greater the gap between the two functions formula_24 and formula_25, in which case the sexual dimorphism is greater. Obviously, this index is a function of the five parameters that characterize a mixture of two normal components (formula_31. Its range is in the interval formula_32, and the interested reader can see, in the work of the authors who proposed the index, the way in which an interval estimate is constructed.\n\nMarini \"et al.\" (1999) have suggested the Kolmogorov-Smirnov distance as a measure of sexual dimorphism. The authors use the following form of the statistic,\n\nwith formula_34 being sample cumulative distributions corresponding to two independent random samples.\n\nSuch a distance has the advantage of being applicable whatever the form of the random variable distributions concerned, yet they should be continuous. The use of this distance assumes that two populations are involved. Further, the Kolmogorov-Smirnov distance is a sample function whose aim is to test that the two samples under analysis have been selected from a single distribution. If one accepts the null hypothesis, then there is not sexual dimorphism; otherwise, there is.\n\n\n"}
{"id": "53136558", "url": "https://en.wikipedia.org/wiki?curid=53136558", "title": "Synergy Moon", "text": "Synergy Moon\n\nSynergy Moon was one of five finalist teams, out of an original 33 entrants, competing for the Google Lunar X Prize —a challenge to land the first privately funded rover on the Moon.\n\nCompetition guidelines required the rover to travel 500 metres and transmit images, video, data, an sms and an email back to Earth. With working groups on in over 15 countries and on 6 continents, Team Synergy Moon promotes international cooperation in space exploration and development.\n\nTeams had until 31 March 2018 to launch their missions. On 23 January 2018, the X Prize Foundation announced that \"no team would be able to make a launch attempt to reach the Moon by the [31 March 2018] deadline... and the US$30 million Google Lunar XPrize will go unclaimed. Synergy Moon reported in February 2018 that they are negotiating with TeamIndus to possibly launch their landers together, aiming for a launch in 2019.\n\nTeam Synergy Moon was formed in 2010 when Kevin Myrick, founder and CEO of InterPlanetary Ventures, and Nebojša Stanojević, founder of The Human Synergy Moon Project, decided to merge their ventures and partner with Interorbital Systems to create a single entrant for the Google Lunar X Prize, and filed proof of its launch agreement on August 30, 2016.\n\nIn December 2016 Google Lunar X Prize entrants Team Stellar (Croatia / Australia), Team Omega Envoy (USA), Team Space META (Brasil) and Team Independence-X (Malaysia) partnered with Team Synergy Moon to become Synergy Space Explorers.\n\nThe team planned to use an Interorbital Systems Neptune N-8 LUNA launch vehicle to deploy its Tesla Surveyor rover on the surface of the Moon during the second half of 2017. The launch was planned from an open-ocean location off the California coast. but it did not happen.\n\nOn 23 January 2018, the X Prize Foundation announced that \"no team would be able to make a launch attempt to reach the Moon by the [31 March 2018] deadline... and the US$30 million Google Lunar XPRIZE will go unclaimed.\"\n\nOn 7 February 2018, Synergy Moon reported that they are negotiating with TeamIndus to launch their rovers together, aiming for a launch at the end of 2018 or early in 2019.\n\n"}
{"id": "76073", "url": "https://en.wikipedia.org/wiki?curid=76073", "title": "Thomas Fincke", "text": "Thomas Fincke\n\nThomas Fincke (6 January 1561 – 24 April 1656) was a Danish mathematician and physicist, and a professor at the University of Copenhagen for more than 60 years.\n\nFincke was born in Flensburg, Schleswig and died in Copenhagen. His lasting achievement is found in his book \"Geometria rotundi\" (1583), in which he introduced the modern names of the trigonometric functions tangent and secant.\nHis son in law was the Danish physician and natural historian, Ole Worm, who married Fincke's daughter Dorothea.\n"}
{"id": "2239197", "url": "https://en.wikipedia.org/wiki?curid=2239197", "title": "Two-factor theory of emotion", "text": "Two-factor theory of emotion\n\nThe two-factor theory of emotion, states that emotion is based on two factors: physiological arousal and cognitive label. The theory was created by researchers Stanley Schachter and Jerome E. Singer. According to the theory, when an emotion is felt, a physiological arousal occurs and the person uses the immediate environment to search for emotional cues to label the physiological arousal. This can sometimes cause misinterpretations of emotions based on the body's physiological state. When the brain does not know why it feels an emotion it relies on external stimulation for cues on how to label the emotion.\n\nStanley Schachter and Jerome E. Singer (1962) performed a study that tested how people use clues in their environment to explain physiological changes. Their hypotheses were:\n\nParticipants were told they were being injected with a new drug called \"Suproxin\" to test their eyesight. The participants were actually injected with epinephrine (which causes respiration, an increase in blood pressure and heart rate) or a placebo. There were four conditions that participants were randomly placed in: epinephrine informed, epinephrine ignorant, epinephrine misinformed and a control group. The epinephrine informed group was told they may feel side effects including that their hands would start to shake, their heart will start to pound, and their face may get warm and flushed. This condition was expected to use cues to explain their physiological change. In the epinephrine ignorant group, the experimenters did not explain to the subjects what symptoms they might feel. This group was expected to use cues to explain their physiological change. The epinephrine misinformed group was told that they would probably feel their feet go numb, and have an itching sensation over parts of their body, and a slight headache. This group was expected to use cues around them for their physiological change. The control group was injected with a placebo and was given no side effects to expect. This group was used as a control because they were not experiencing a physiological change and have no emotion of label. After the injection, a confederate interacted with the students, who was either acting euphoric or angry. The experimenters watched through a one way mirror and rated the participants' state on a three category scale. The participants were then given a questionnaire and their heart rate was checked.\n\nThe researchers found that the impact of the confederate was different for the participants in the different conditions. From high to low euphoria their ranking was as follows: epinephrine misinformed, epinephrine ignorant, placebo, epinephrine informed. In the anger condition the ranking was: epinephrine ignorant, placebo, epinephrine informed. Both results show that those participants who had no explanation of why their body felt as it did, were more susceptible to the confederate. These findings are considered to support the researchers' hypotheses.\n\nThe misattribution of arousal study tested Schachter and Singer's two-factor theory of emotion. Psychologists Donald G. Dutton and Arthur P. Aron wanted to use a natural setting that would induce physiological arousal. In this experiment, they had male participants walk across two different styles of bridges. One bridge was a very scary (arousing) suspension bridge, which was very narrow and suspended above a deep ravine. The second bridge was much safer and more stable than the first.\n\nAt the end of each bridge an attractive female experimenter met the [male] participants. She gave the participants a questionnaire which included an ambiguous picture to describe and her number to call if they had any further questions. The idea of this study was to find which group of males were more likely to call the female experimenter and to measure the sexual content of the stories the men wrote after crossing one of the bridges. They found that the men who walked across the scary bridge were more likely to call the woman to follow up on the study, and that their stories had more sexual content. \nTwo-factor theory would say that this is because they had transferred (misattributed) their arousal from fear or anxiety on the suspension bridge to higher levels of sexual feeling towards the female experimenter.\n\nIn the Schachter & Wheeler (1962) study the subjects were injected with epinephrine, chlorpromazine, or a placebo (chlorpromazine is a tranquilizer). None of the subjects had any information about the injection. After receiving the injection, the subjects watched a short comical movie. While watching the movie, the subjects were monitored for signs of humor. After the movie was watched, the subjects rated how funny the movie was and if they enjoyed. The results concluded that the epinephrine subjects demonstrated the most signs of humor. The placebo subjects demonstrated fewer reactions of humor but more than the chlorpromazine subjects.\n\nCriticism of the theory has come from attempted replications of the Schachter and Singer (1962) study. Marshall and Zimbardo (1979, and Marshall 1976) tried to replicate the Schachter and Singer’s euphoria conditions. Just as Schachter and Singer did, the subjects were injected with epinephrine or a placebo, except the administrator told the subjects that they will be experiencing non-arousal symptoms. Then the subjects were put into four different conditions: subjects injected epinephrine and were exposed to a neutral confederate, another in which they received the placebo and were told to expect arousal symptoms, and two conditions in which the dosage of epinephrine was determined by body weight rather than being fixed. The results found that euphoria confederate had little impact on the subjects. Also, that the euphoric confederate didn’t produce any more euphoria than the neutral confederate did. Concluding that the subjects who were injected with epinephrine were not more susceptible to emotional manipulations than the non-aroused placebo subjects.\n\nMaslach (1979) designed a study to try to replicate and extend on the Schachter and Singer study. Instead of being injected with epinephrine, the administrators used hypnotic suggestions for the source of arousal. Either the subjects were hypnotized or were used as a control (same as the placebo effect in the Schachter and Singer study). Subjects that were hypnotized were given a suggestion to become aroused at the presentation of a cue and were instructed not to remember the source of this arousal. Right after the subjects had been hypnotized, a confederate began acting either in a euphoric or angry condition. Later on in the study the subjects were exposed to two more euphoric confederates. One confederate was to keep aware the source of the arousal, while the other confederates told the subjects to expect different arousal symptoms.\nThe results found that all the subjects both on self-reports and on observation found that unexplained arousal causes negative conditions. Subjects still showed angry emotions regardless of the euphoric confederate. Maslach concluded that when there is a lack of explanation for an arousal it will cause a negative emotion, which will evoke either anger or fear. However, Maslach did mention a limitation that there might have been more negative emotion self-reported because there are more terms referring to negative emotions than to positive ones.\n\nThere are also criticisms of the two-factor theory that come from a theoretical standpoint. One of these criticisms is that the Schachter-Singer Theory centers primarily on the autonomic nervous system and provides no account of the emotional process within the central nervous system aside from signaling the role of cognitive factors. This is important considering the heavy implication of certain brain centers in mitigating emotional experience (e.g., fear and the amygdala).\n\nIt can also be noted that Gregorio Marañon also had early studies in the development of cognitive theories of emotion and should be recognized for making contributions to this concept.\n\n\n\n"}
{"id": "22857779", "url": "https://en.wikipedia.org/wiki?curid=22857779", "title": "Udwadia–Kalaba equation", "text": "Udwadia–Kalaba equation\n\nIn theoretical physics, the Udwadia–Kalaba equation is a method for deriving the equations of motion of a constrained mechanical system. The equation was first described by Firdaus E. Udwadia and Robert E. Kalaba in 1992. The approach is based on Gauss's principle. The Udwadia–Kalaba equation applies to a wide class of constraints, both holonomic constraints and nonholonomic ones, as long as they are linear with respect to the accelerations. The equation generalizes to constraint forces that do not obey D'Alembert's principle.\n\nThe problem of finding a simple closed form expression for mechanical systems that are subjected to equality constraints is a central issue in analytical dynamics. It remained so since the time it was first explicitly stated by Lagrange, and it has been worked on by numerous scientists and engineers for at least 200 years.\n\nThe centrality of the problem stems for the fact that the difference between the motion of a \"set\" of particles subjected to forces and that of a \"system\" of particles is that in the latter the particles are cognizant of the motion of one another, and are therefore subjected to constraints that each particle poses on some (or all) of its neighbors. The standard method of handling this problem is through the use of Lagrange multipliers, first introduced by Lagrange. This approach however, is unsuitable when the number of degrees of freedom of a system becomes large, since the multipliers are required to be found usually through the solution of nonlinear sets of equations. Other approaches such as the Gibbs-Appell approach, and its variants, suffer from similar difficulties. There has therefore been a quest for obtaining an explicit equation that describes the motion of a constrained mechanical system that is subjected to equality constraints.\n\nIn 1992, Udwadia and Kalaba published such an explicit equation, in a three-page paper. In their publications, the authors refer to the equation as the \"fundamental equation of constrained motion\". This equation is nowadays called the Udwadia-Kalaba (UK) equation.\n\nThe Udwadia–Kalaba equation has several advantages over the available (Lagrangian) approaches for describing the motion of mechanical systems, especially when dealing with nonlinear systems that have many degrees of freedom (that is, large-scale mechanical systems). The UK equation: \n\nThe Lagrangian view of analytical dynamics (motion of bodies subjected to forces) relies on the determination of the Lagrange multipliers to describe the motion of constrained mechanical systems. In contrast, Udwadia and Kalaba explain constrained motion without the notion of a Lagrange multiplier.\n\nThe physical interpretation of the equation has been of importance in areas beyond theoretical physics, such as the control of highly nonlinear general dynamical systems.\n\nIn the study of the dynamics of mechanical systems, the configuration of a given system \"S\" is, in general, completely described by \"n\" generalized coordinates so that its generalized coordinate \"n\"-vector is given by\n\nwhere T denotes matrix transpose. Using Newtonian or Lagrangian dynamics, the unconstrained equations of motion of the system \"S\" under study can be derived as a matrix equation (see matrix multiplication):\n\n(t)=\\mathbf{Q}(q,\\dot{q},t)\\,</math>\n\nwhere the dots represent derivatives with respect to time:\n\nIt is assumed that the initial conditions q(0) and formula_3 are known. We call the system \"S\" unconstrained because formula_3 may be arbitrarily assigned.\n\nThe \"n\"-vector Q denotes the total generalized force acted on the system by some external influence; it can be expressed as the sum of all the conservative forces as well as \"non\"-conservative forces.\n\nThe \"n\"-by-\"n\" matrix M is symmetric, and it can be positive definite formula_5 or semi-positive definite formula_6. Typically, it is assumed that M is positive definite; however, it is not uncommon to derive the unconstrained equations of motion of the system \"S\" such that M is only semi-positive definite; i.e., the mass matrix may be singular (it has no inverse matrix).\n\nWe now assume that the unconstrained system \"S\" is subjected to a set of \"m\" consistent equality constraints given by\nwhere A is a known \"m\"-by-\"n\" matrix of rank \"r\" and b is a known \"m\"-vector. We note that this set of constraint equations encompass a very general variety of holonomic and non-holonomic equality constraints. For example, holonomic constraints of the form\ncan be differentiated twice with respect to time while non-holonomic constraints of the form\ncan be differentiated once with respect to time to obtain the \"m\"-by-\"n\" matrix A and the \"m\"-vector b. In short, constraints may be specified that are\n\n\nAs a consequence of subjecting these constraints to the unconstrained system \"S\", an additional force is conceptualized to arise, namely, the force of constraint. Therefore, the constrained system \"S\" becomes\n\n=\\mathbf{Q}+\\mathbf{Q}_{c}(q,\\dot{q},t),</math>\n\nwhere Q—the constraint force—is the additional force needed to satisfy the imposed constraints. The central problem of constrained motion is now stated as follows:\n\n\nfind the equations of motion for the \"constrained\" system—the acceleration—at time \"t\", which is in accordance with the agreed upon principles of analytical dynamics.\n\nThe solution to this central problem is given by the Udwadia–Kalaba equation. When the matrix M is positive definite, the equation of motion of the constrained system \"S\", at each instant of time, is\n\nwhere the '+' symbol denotes the pseudoinverse of the matrix formula_13. The force of constraint is thus given explicitly as\n\nand since the matrix M is positive definite the generalized acceleration of the constrained system \"S\" is determined explicitly by\n\nIn the case that the matrix M is semi-positive definite formula_6, the above equation cannot be used directly because M may be singular. Furthermore, the generalized accelerations may not be unique unless the (\"n\" + \"m\")-by-\"n\" matrix\n\nhas full rank (rank = \"n\"). But since the observed accelerations of mechanical systems in nature are always unique, this rank condition is a necessary and sufficient condition for obtaining the uniquely defined generalized accelerations of the constrained system \"S\" at each instant of time. Thus, when formula_18 has full rank, the equations of motion of the constrained system \"S\" at each instant of time are uniquely determined by (1) creating the auxiliary unconstrained system\n\nand by (2) applying the fundamental equation of constrained motion to this auxiliary unconstrained system so that the auxiliary constrained equations of motion are explicitly given by\n\nMoreover, when the matrix formula_18 has full rank, the matrix formula_22 is always positive definite. This yields, explicitly, the generalized accelerations of the constrained system \"S\" as\n\nThis equation is valid when the matrix M is either positive definite \"or\" positive semi-definite. Additionally, the force of constraint that causes the constrained system \"S\"—a system that may have a singular mass matrix M—to satisfy the imposed constraints is explicitly given by\n\nAt any time during the motion we may consider perturbing the system by a virtual displacement δr consistent with the constraints of the system. The displacement is allowed to be either reversible or irreversible. If the displacement is irreversible, then it performs virtual work. We may write the virtual work of the displacement as\n\nThe vector formula_26 describes the non-ideality of the virtual work and may be related, for example, to friction or drag forces (such forces have velocity dependence). This is a generalized D'Alembert's principle, where the usual form of the principle has vanishing virtual work with formula_27.\n\nThe Udwadia–Kalaba equation is modified by an additional non-ideal constraint term to\n\nThe method can solve the inverse Kepler problem of determining the force law that corresponds to the orbits that are conic sections. We take there to be no external forces (not even gravity) and instead constrain the particle motion to follow orbits of the form\n\nwhere formula_30, formula_31 is the eccentricity, and \"l\" is the semi-latus rectum. Differentiating twice with respect to time and rearranging slightly gives a constraint\n\nWe assume the body has a simple, constant mass. We also assume that angular momentum about the focus is conserved as\n\nwith time derivative\n\nWe can combine these two constraints into the matrix equation\n\nThe constraint matrix has inverse\n\nThe force of constraint is therefore the expected, central inverse square law\n\nConsider a small block of constant mass on an inclined plane at an angle formula_38 above horizontal. The constraint that the block lie on the plane can be written as\n\nAfter taking two time derivatives, we can put this into a standard constraint matrix equation form\n\nThe constraint matrix has pseudoinverse\n\nWe allow there to be sliding friction between the block and the inclined plane. We parameterize this force by a standard coefficient of friction multiplied by the normal force\n\nWhereas the force of gravity is reversible, the force of friction is not. Therefore, the virtual work associated with a virtual displacement will depend on C. We may summarize the three forces (external, ideal constraint, and non-ideal constraint) as follows:\n\nCombining the above, we find that the equations of motion are\n\nThis is like a constant downward acceleration due to gravity with a slight modification. If the block is moving up the inclined plane, then the friction increases the downward acceleration. If the block is moving down the inclined plane, then the friction reduces the downward acceleration.\n\n"}
{"id": "2849397", "url": "https://en.wikipedia.org/wiki?curid=2849397", "title": "Unifying Theories of Programming", "text": "Unifying Theories of Programming\n\nUnifying Theories of Programming (UTP) in computer science deals with program semantics. It shows how denotational semantics, operational semantics and algebraic semantics can be combined in a unified framework for the formal specification, design and implementation of programs and computer systems.\n\nThe book of this title by C.A.R. Hoare and He Jifeng was published in the Prentice Hall International Series in Computer Science in 1998 and is now freely available on the web.\n\nThe semantic foundation of the UTP is the first-order predicate calculus, augmented with fixed point constructs from second-order logic. Following the tradition of Eric Hehner, programs are predicates in the UTP, and there is no distinction between programs and specifications at the semantic level. In the words of Hoare:\n\nA computer program is identified with the strongest predicate describing every relevant observation that can be made of the behaviour of a computer executing that program.\n\nIn UTP parlance, a \"theory\" is a model of a particular programming paradigm. A UTP theory is composed of three ingredients:\n\n\nProgram refinement is an important concept in the UTP. A program formula_1 is refined by formula_2 if and only if every observation that can be made of formula_2 is also an observation of formula_1.\nThe definition of refinement is common across UTP theories:\n\nformula_5\n\nwhere formula_6 denotes the universal closure of all variables in the alphabet.\n\nThe most basic UTP theory is the alphabetised predicate calculus, which has no alphabet restrictions or healthiness conditions. The theory of relations is slightly more specialised, since a relation's alphabet may consist of only:\n\n\nSome common language constructs can be defined in the theory of relations as follows:\n\n\nformula_9\n\n\nformula_15\n\n\nformula_16\n\n\nformula_17\n\n\nformula_18\n\n\nformula_21\n\n\n"}
{"id": "18076105", "url": "https://en.wikipedia.org/wiki?curid=18076105", "title": "Urania (Berlin)", "text": "Urania (Berlin)\n\nUrania is a scientific society founded in Berlin in 1888, following an idea of Alexander von Humboldt, by and Wilhelm Foerster. Its aim is to communicate the most recent scientific findings to the broad public. With its 2000 members, Urania organizes more than 1000 events per year which attract about 130 000 visitors. Since its centenary in 1988, the society has awarded the \"Urania Medaille\" annually to individuals who have supported significantly the implementation of its aims. Recipients are Nobel laureates in natural science as well as social scientists, artists, and politicians.\n\n"}
{"id": "3411121", "url": "https://en.wikipedia.org/wiki?curid=3411121", "title": "Wang Sichao", "text": "Wang Sichao\n\nWang Sichao (王思潮; 1938 or 1939 – 17 June 2016) was a Chinese astronomer and scholar.\n\nAt the time of his death, Sichao was working as a researcher at Nanjing's Zijinshan Astronomical Observatory.\n\nIn an interview with Xinhuanet, Sichao commented on the International Astronomical Union's 2006 vote over Pluto's status as a planet:\nOn August 23, 2010 Sichao stated that he believed extraterrestrial aliens exist and that their UFOs have the ability to visit our earth. He also took exception to the recent view presented by British astronomer Stephen Hawking that an encounter with between Earth's population and such aliens would be disastrous.\n\nSichao also provided some specific data from quantitative analysis of UFO observations. He informed that between the altitudes of 130 kilometers and 1,500 kilometers, UFOs have appeared many times. He stated that the observed UFOS could fly much slower than the \"first cosmic velocity\", and some as slow as 0.29 kilometers per second, and that they can fly at the altitude of 1,460 kilometers for more than 25 minutes. He concluded that these UFOs had anti-gravity abilities.\n\n"}
