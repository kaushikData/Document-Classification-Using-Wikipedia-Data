{"id": "47855030", "url": "https://en.wikipedia.org/wiki?curid=47855030", "title": "Aikhulu Chasma", "text": "Aikhulu Chasma\n\nAikhylu Chasma is a tectonic rift valley on Venus, and the landing site of the \"Venera 9\" lander. It is located in Beta Regio.\n"}
{"id": "2110074", "url": "https://en.wikipedia.org/wiki?curid=2110074", "title": "Analysis of Functional NeuroImages", "text": "Analysis of Functional NeuroImages\n\nAnalysis of Functional NeuroImages (AFNI) is an open-source environment for processing and displaying functional MRI data—a technique for mapping human brain activity.\n\nAFNI is an agglomeration of programs that can be used interactively or flexibly assembled for batch processing using shell script. The term \"AFNI\" refers both to the entire suite and to a particular interactive program often used for visualization. AFNI is actively developed by the NIMH Scientific and Statistical Computing Core and its capabilities are continually expanding.\n\nAFNI runs under many Unix-like operating systems that provide X11 and Motif libraries, including IRIX, Solaris, Linux, FreeBSD and OS X. Precompiled binaries are available for some platforms. AFNI is available for research use under the GNU General Public License. AFNI now comprises over 300,000 lines of C source code, and a skilled C programmer can add interactive and batch functions to AFNI with relative ease.\n\nAFNI was originally developed at the Medical College of Wisconsin beginning in 1994, largely by Robert W. Cox. Cox brought development to the NIH in 2001 and development continues at the NIMH Scientific and Statistical Computing Core. In a 1995 paper describing the rationale for development of the software, Cox wrote of fMRI data: \"The volume of data gathered is very large, and it is essential that easy-to-use tools for visualization and analysis of 3D activation maps be available for neuroscience investigators.\" Since then, AFNI has become one of the more commonly used analysis tools in fMRI research, alongside SPM and FSL.\n\nAlthough AFNI initially required extensive shell scripting to execute tasks, pre-made batch scripts and improvements to the graphical user interface (GUI) have since made it possible to generate analyses with less user scripting.\n\nOne of AFNI's initial offerings improved the approach to transforming scans of individual brains onto a shared standardized space. Since each person's individual brain is unique in size and shape, comparing across a number of brains requires warping (rotating, scaling, etc.) individual brains into a standard shape. Unfortunately, functional MRI data at the time of AFNI's development was too low resolution for effective transformations. Instead, researchers use the higher resolution anatomical brain scans, often acquired at the beginning of an imaging session.\n\nAFNI allows researchers to overlay a functional image to the anatomical, providing tools for aligning the two into the same space. Processes engaged to warp an individual anatomical scan to standard space are then applied also to the functional scan, improving the transformation process.\n\nAnother feature available in AFNI is the SUMA tool, developed by Ziad Saad. This tool allows users to project the 2D data onto a 3D cortical surface map. In this way researchers can view activation patterns while more easily taking into account physical cortical features like gyri.\n\n\"afni_proc.py\" is a pre-made script that will run fMRI data from a single subject through a series of pre-processing steps, starting with the raw data. The default settings will perform the following pre-processing steps and finish with a basic regression analysis:\n\n\n\n"}
{"id": "692605", "url": "https://en.wikipedia.org/wiki?curid=692605", "title": "Annie Jump Cannon Award in Astronomy", "text": "Annie Jump Cannon Award in Astronomy\n\nThe Annie Jump Cannon Award in Astronomy is awarded annually by the American Astronomical Society (AAS) to a woman resident of North America, who is within five years of receipt of a Ph.D., for distinguished contributions to astronomy or for similar contributions in related sciences which have immediate application to astronomy. The awardee shall be invited to give a talk at an AAS meeting and is given a $1,500 honorarium.\n\nFrom 1973–2004 it was awarded by the American Association of University Women on advice from the AAS. The AAS resumed distribution of the award in 2005. The award is named in honor of American astronomer Annie Jump Cannon, and is the only award in astronomy restricted to women only.\n\nAnnie Jump Cannon awardees are:\n\n"}
{"id": "4330820", "url": "https://en.wikipedia.org/wiki?curid=4330820", "title": "Atwater system", "text": "Atwater system\n\nThe Atwater system, named after Wilbur Olin Atwater, or derivatives of this system are used for the calculation of the available energy of foods. The system was developed largely from the experimental studies of Atwater and his colleagues in the later part of the 19th century and the early years of the 20th at Wesleyan University in Middletown, Connecticut. Its use has frequently been the cause of dispute, but no real alternatives have been proposed. As with the calculation of protein from total nitrogen, the Atwater system is a convention and its limitations can be seen in its derivation.\n\nAvailable energy (as used by Atwater) is equivalent to the modern usage of the term metabolisable energy (ME).\n\nformula_1\n\nIn most studies on humans, losses in secretions and gases are ignored. The gross energy (GE) of a food, as measured by bomb calorimetry is equal to the sum of the heats of combustion of the components – protein (GE), fat (GE) and carbohydrate (GE) (by difference) in the proximate system.\n\nformula_2\n\nAtwater considered the energy value of feces in the same way.\n\nformula_3 wherever faecal excretion is small, will approximate to unity and thus these \"coefficients\" have a low variance and have the appearance of constants. This is spurious since faecal excretion is variable even on a constant diet, and there is no evidence to suggest that faecal excretion is in fact related to intake in the way implied by these coefficients.\n\nThe calculation of energy values must be regarded as an alternative to direct measurement, and therefore is likely to be associated with some inaccuracy when compared with direct assessment. These inaccuracies arise for a number of reasons\n\n\nThe theoretical and physiological objections to the assumptions inherent in the Atwater system are likely to result in errors much smaller than these practical matters. Conversion factors were derived from experimental studies with young infants, but these produced values for metabolisable energy intake that were insignificantly different from those obtained by direct application of the modified Atwater factors.\n\n\n"}
{"id": "13941709", "url": "https://en.wikipedia.org/wiki?curid=13941709", "title": "BCSWomen", "text": "BCSWomen\n\nBCSWomen is a Specialist Group of the British Computer Society The Chartered Institute for IT that provides networking opportunities for all BCS professional women working in IT around the world, as well as mentoring and encouraging girls and women to enter or return to IT as a career.. BCSWomen was founded by Dr Sue Black. Dr Black was the first Chair of BCSWomen, followed by Dr Karen Petrie with Dr Hannah Dee as Deputy Chair. The current Chair of BCSWomen is Gillian Arnold of Tectre. BCSWomen has the aim of supporting women working in and considering a career in Information Technology.\n\nThe group was founded in 2001. It has more than a thousand members and an active mailing list. Activities include meetings, networking, and mentoring. They organise the Undergraduate Lovelace Colloquium for undergraduate women in computing, a one-day conference which started in Leeds in 2008 and now moves around the UK. named in honour of Ada Lovelace, often regarded as the first computer programmer. The colloquium is for U.K. university women students studying Computing and related subjects. It was started by Dr Hannah Dee, who continues to play a key role in its organisation every year. Many BCSWomen also participate in the annual London Hopper Colloquium, which showcases exciting work of women in computing research and enables new PhD researchers to meet with each other as well as with senior women computer scientists. Grace Hopper was a pioneering American computer scientist.\nBCSWomen organise other events for women in computing both technical and social, such as day trips to computer-related sites such as Bletchley Park.\n\nGillian Arnold, Chair of BCSWomen was invited to Korea on 27 October 2014 to receive the Gender Equality Main Streaming - Technology (GEM-TECH) award on behalf of the BCS and BCSWomen. This achievement award of the ITU - United Nations Women Joint Award, was for \"Promoting Women in ICT Sector\" and encouraging women to enter the computing sector and to encourage and support them during their careers.\n\n\n\n"}
{"id": "2796304", "url": "https://en.wikipedia.org/wiki?curid=2796304", "title": "Beaver Glacier (Ross Ice Shelf)", "text": "Beaver Glacier (Ross Ice Shelf)\n\nBeaver Glacier () is a glacier, long, draining the coastal mountains of the Queen Alexandra Range just northwest of Mount Fox and entering the Ross Ice Shelf at McCann Point. It was named by the New Zealand Geological Survey Antarctic Expedition (1959–60) after the Beaver aircraft \"City of Auckland\", which crashed in this area in January 1960.\n\n"}
{"id": "56286907", "url": "https://en.wikipedia.org/wiki?curid=56286907", "title": "Bindstone", "text": "Bindstone\n\nA Bindstone is a special type of carbonate rock in the Dunham classification. The term Bindstone did not appear in the original Dunham classification from 1962 and was introduced by Embry and Klovan 1971 in the modified Dunham classification.\n\nEmbry and Klovan(1971) define Bindstones as rocks that \"[...] contain in situ, tabular or lamellar fossils which encrusted and bound sediment during deposition. [...] The matrix, not the in situ fossils, forms the supporting framework of the rock, and the fossils may form as little as 15 percent of the constituents of the rock.\"\n\nWright (1991) uses Bindstone as a synonym for Boundstone, which is defined as a rock \"[...] where the\nstructure reflects the encrusting and binding action of plants or animals\"\n\nLokier and Al Nunaibi (2016) define Bindstones as \"autochthonous carbonate-dominated rock in which the original components of the supporting matrix were organically bound through stabilization of the sediment at the time of deposition.\"\n\nOne problem in the classification is that the term Bindstone is easy to confuse with the term Boundstone. Additionally the exact relation of the two terms changes depending on the classification used. For Embry and Klovan (1971), a Boundstone is used for autochthonous carbonates if there is a lack of evidence for the more precise classifications as Bafflestone, Bindstone or Framestone. In contrast to that, Wright (1991) uses Boundstone and Bindstone synonymously, which is not consistent with other authors.\n"}
{"id": "22952112", "url": "https://en.wikipedia.org/wiki?curid=22952112", "title": "Bonestell (crater)", "text": "Bonestell (crater)\n\nBonestell is a crater in the Northern hemisphere in the Mare Acidalium quadrangle of Mars, located at 42.37° North and 30.57° West. It is 42.4 km in diameter and was named after Chesley Bonestell, a famous American space artist (1888-1986), whose drawings inspired many young people to study sciences.\n\nImpact craters generally have a rim with ejecta around them. In contrast, volcanic craters usually do not have a rim or ejecta deposits. As craters get larger (greater than 10 km in diameter) they usually have a central peak. The peak is caused by a rebound of the crater floor following the impact. If one measures the diameter of a crater, the original depth can be estimated with various ratios. Because of this relationship, researchers have found that many Martian craters contain a great deal of material; much of it is believed to be ice deposited when the climate was different. Sometimes craters expose layers that were buried. Rocks from deep underground are tossed onto the surface. Hence, craters can show us what lies deep under the surface.\n\n"}
{"id": "29332991", "url": "https://en.wikipedia.org/wiki?curid=29332991", "title": "Bornmann Glacier", "text": "Bornmann Glacier\n\nBornmann Glacier () is a glacier flowing from the west side of Hallett Peninsula south of Seabee Hook and forming a short, floating ice tongue on the shore of Edisto Inlet. It was named by the New Zealand Geological Survey Antarctic Expedition, 1957–58, for Lieutenant Robert C. Bornmann, MC, U.S. Navy, surgeon and leader of the U.S. Navy Operation Deep Freeze party at Hallett Station in 1958.\n"}
{"id": "4482211", "url": "https://en.wikipedia.org/wiki?curid=4482211", "title": "Canopy interception", "text": "Canopy interception\n\nCanopy interception is the rainfall that is intercepted by the canopy of a tree and successively evaporates from the leaves. Precipitation that is not intercepted will fall as throughfall or stemflow on the forest floor.\n\nMany methods exist to measure canopy interception. The most often used method is by measuring rainfall above the canopy and subtract throughfall and stem flow (e.g., Helvey and Patric [1965]). However, the problem with this method is that the canopy is not homogeneous, which causes difficulty in obtaining representative throughfall data.\n\nAnother method that tried to avoid this problem is applied by e.g., Shuttleworth et al. [1984], Calder et al. [1986], and Calder [1990]. They covered the forest floor with plastic sheets and collected the throughfall. The disadvantage of this method is that it is not suitable for long periods, because in the end the trees will dry due to water shortage, and the method is also not applicable for snow events.\n\nThe method by Hancock and Crowther [1979] avoided these problems by making use of the cantilever effect of branches. If leaves on a branch hold water, it becomes more heavy and will bend. By measuring the displacement, it is possible to determine the amount intercepted water. Huang et al. refined this method later in 2005 by making use of strain gauges. However, the disadvantages of these methods are that only information about one single branch is obtained and it will be quite laborious to measure an entire tree or forest.\n\n"}
{"id": "8644068", "url": "https://en.wikipedia.org/wiki?curid=8644068", "title": "Carl Julius Bernhard Börner", "text": "Carl Julius Bernhard Börner\n\nCarl Julius Bernhard Börner (28 May 1880 – 14 June 1953) was a German entomologist.\n\nBörner was born in Bremen and died in Naumburg. His collections of Collembola are located in the Natural History Museum, London and the Deutsches Entomologisches Institut in Müncheberg.\n"}
{"id": "56939328", "url": "https://en.wikipedia.org/wiki?curid=56939328", "title": "Chiara Mingarelli", "text": "Chiara Mingarelli\n\nChiara Mingarelli is an Italian - Canadian Astrophysicist who researches gravitational waves at the Flatiron Institute Center for Computational Astrophysics. She is a passionate science writer and communicator.\n\nMingarelli grew up in Ottawa, Canada. She completed a bachelor's degree in mathematics and physics from Carleton University, Canada, in 2006. She moved to the University of Bologna to study for a Masters in Astrophysics and Cosmology, which she achieved in 2009. Mingarelli's PhD thesis \"Gravitational Wave Astrophysics with Pulsar Timing Arrays\", was selected by Springer Nature as an \"Outstanding PhD thesis\" in 2016. She earned her PhD at the University of Birmingham with Alberto Vecchio in 2014.\n\nMingarelli is a gravitational wave astrophysicist attempting to understand the merging of supermassive black holes. Mingarelli predicts the nanohertz gravitational wave signatures of such mergers. She will measure them using pulsar timing arrays, which can characterise the cosmic merger history of binary black hole systems. The systems emit nanohertz gravitational waves. After completing her PhD, Mingarelli was awarded a European Union Marie Curie International Fellowship, which she brought to the California Institute of Technology. There she continued to work on gravitational waves. At Caltech she taught students in the Gravitational Wave Astrophysics school about Pulsar Timing Arrays. Mingarelli spent the return phase of the Marie Curie Fellowship at the Max Planck Institute for Radio Astronomy. She is regularly an invited speaker at scientific conferences.\n\nMingarelli appeared on Stargazing Live in 2012. She was featured on the BBC Radio Cambridgeshire show The Naked Scientists. In 2013 the Royal Astronomical Society selected Mingarelli as a \"Voice of the Future\", and she attended an interview session at the House of Commons. She regularly appears on science themed podcasts and video series. She has been involved with Amy Poehler's Smart Girls, as a blogger and interviewee. After the first detection of gravitational waves, Mingarelli was featured in The New York Times. She has contributed to popular science journals, including Scientific American, Nautilus, The Wall Street Journal, Gizmodo, Wired and the New Scientist. Mingarelli maintains a social media presence on sites such as Twitter, where she is an advocate for \"science, coffee and girl power\".\n"}
{"id": "33454450", "url": "https://en.wikipedia.org/wiki?curid=33454450", "title": "Cognitive tuning", "text": "Cognitive tuning\n\nDavid Hubel and Torsten Wiesel discovered that mammalian brains contained specific neural cortical columns which were attuned to specific spatial frequencies, specific colors, specific shapes, specific motions (up and down, left and right, inward and outward, etc.). Thus the brain could be viewed as a collection of tuned filters.\n\nTheir discovery occurred accidentally. In 1962, Hubel and Wiesel were attempting to discover that triggered neurophysiological activity in a cat's brain. The cat they were testing showed no results until they displayed a \"crack\" to the cat, whereupon the cat's brain starting reacting to the stimulus. This began a long series of discoveries which continues to the present day. Hubel and Wiesel received the Nobel Prize for their discovery.\n\nPrimate brains like our own display even more arcane forms of tuning, which are occasionally discovered by fMRI or PET scans of impaired patients. These range from the recognition of faces, to the recognition of emotions, to other social, cultural, linguistic skills, etc.\n"}
{"id": "46348222", "url": "https://en.wikipedia.org/wiki?curid=46348222", "title": "Community Earth System Model", "text": "Community Earth System Model\n\nThe Community Earth System Model (CESM) is a fully coupled numerical simulation of the Earth system consisting of atmospheric, ocean, ice, land surface, carbon cycle, and other components. CESM includes a climate model providing state-of-art simulations of the Earth's past, present, and future. It is the successor of the Community Climate System Model (CCSM), specifically version 4 (CCSMv4), which provided the initial atmospheric component for CESM. Strong ensemble forecasting capabilities, CESM-LE (CESM-Large Ensemble), were developed at the onset to control for error and biases across different model runs (realizations). Simulations from the Earth's surface through the thermosphere are generated utilizing the Whole Atmosphere Community Climate Model (WACCM). CESM1 was released in 2010 with primary development by the Climate and Global Dynamics Division (CGD) of the National Center for Atmospheric Research (NCAR), and significant funding by the National Science Foundation (NSF) and the Department of Energy (DoE).\n\n\n"}
{"id": "1633910", "url": "https://en.wikipedia.org/wiki?curid=1633910", "title": "Conamara Chaos", "text": "Conamara Chaos\n\nConamara Chaos is a region of chaotic terrain on Jupiter's moon Europa. It is named after Connemara () in Ireland due to its similarly rugged landscape.\n\nConamara Chaos is a landscape produced by the disruption of the icy crust of Europa. The region consists of rafts of ice that have moved around and rotated. Surrounding these plates is a lower matrix of jumbled ice blocks which may have been formed as water, slush, or warm ice rose up from below the surface. The region is cited as evidence for a liquid ocean below Europa's icy surface.\n\n"}
{"id": "6153698", "url": "https://en.wikipedia.org/wiki?curid=6153698", "title": "Controlled lab reactor", "text": "Controlled lab reactor\n\nIn chemistry, a Controlled Lab Reactor or CLR is any reaction system where there is an element of automated control. Generally these devices refers to a jacketed glass vessel where a circulating chiller unit pumps a thermal control fluid through the jacket to accurately control the temperature of the vessel contents. Additional to this, it is common to have a series of sensors (temperature, pH, pressure) measuring and recording parameters about the reactor contents. It is additionally possible to control pumps to act on the reactor.\n\nThe first controlled lab reactors were derived from the control systems used in chemical plants. These were generally dedicated to specific tasks as reprogramming was difficult. These first systems were often home built and used hardware that was adapted rather than designed for the task\n\nModern CLR systems take a wide range of forms with the ability to work on a range of different volume reactors (and indeed reactor styles). Data is usually transmitted back to a PC to be recorded (and indeed complex recipe based control is usually performed here too) though other systems may use off-line data logging.\n\nIn the most sophisticated systems that exist, analytical instruments such as raman spectrometers and FTIR probes can also be integrated with the reactor. These more sophisticated systems also allow the closed loop control of the reactor as a result of taking readings from the sensors and analytical instruments concerned.\n\nMost reaction calorimeters can be used as controlled lab reactors (indeed some calorimeters are based on CLR's).\n\nReaction Calorimeter\n"}
{"id": "8906641", "url": "https://en.wikipedia.org/wiki?curid=8906641", "title": "Corona poling", "text": "Corona poling\n\nCorona poling is a technique in optoelectronics. \n\nCorona discharge is a partial breakdown of air, usually at atmospheric pressure, and is initiated by a discharge in an inhomogeneous electric field (see Figure 1). Corona discharge has been used to pole films of electro-optic materials to enhance their electro-optic properties. \n\nAlthough corona poling can be performed at room temperature, poling at elevated temperature has several advantages. For example, raising the temperature in a polymer guest-host system close to its glass-rubber transition temperature before poling increases the mobility of the guest molecules and allows rotation to occur during poling. If during poling the temperature is lowered well below the transition temperature, the guest molecules are fixed in their new orientation.\n"}
{"id": "31513537", "url": "https://en.wikipedia.org/wiki?curid=31513537", "title": "E-micro", "text": "E-micro\n\nAn E-micro is a futures contract traded on the Chicago Mercantile Exchange's Globex electronic trading platform, that represent an even smaller fraction of the value of the normal futures contracts, than the corresponding E-mini.\n\nCurrently, CME offers one E-micro stock market index contract, \"E-micro S&P CNX Nifty (Nifty 50) Futures\", with a notional value of $2 x the S&P CNX Nifty index of Indian stocks, and E-micro contracts for a number of currency futures pegged against the US Dollar (AUD, CAD, CHF, EUR, GBP, JPY). E-micro gold futures contracts were introduced in October 2010.\n\nThe table below lists some of the more popular E-micro contracts, with the initial and maintenance margin required by the issuing exchange. Note that individual brokers may require different margin amounts (also called performance bonds).\n\n\n"}
{"id": "38223", "url": "https://en.wikipedia.org/wiki?curid=38223", "title": "Engineer", "text": "Engineer\n\nEngineers, as practitioners of engineering, are professionals who invent, design, analyze, build, and test machines, systems, structures and materials to fulfill objectives and requirements while considering the limitations imposed by practicality, regulation, safety, and cost. The word \"engineer\" (Latin \"ingeniator\") is derived from the Latin words \"ingeniare\" (\"to create, generate, contrive, devise\") and \"ingenium\" (\"cleverness\"). The foundational qualifications of an engineer typically include a 4-year bachelor's degree in an engineering discipline, or in some jurisdictions, a master's degree in an engineering discipline plus 4–6 years of peer-reviewed professional practice (culminating in a project report or thesis) and passage of engineering board examinations.\n\nThe work of engineers forms the link between scientific discoveries and their subsequent applications to human and business needs and quality of life.\n\nIn 1961\n, the Conference of Engineering Societies of Western Europe and the United States of America defined \"professional engineer\" as follows:\n\nEngineers develop new technological solutions. During the engineering design process, the responsibilities of the engineer may include defining problems, conducting and narrowing research, analyzing criteria, finding and analyzing solutions, and making decisions. Much of an engineer's time is spent on researching, locating, applying, and transferring information. Indeed, research suggests engineers spend 56% of their time engaged in various information behaviours, including 14% actively searching for information.\n\nEngineers must weigh different design choices on their merits and choose the solution that best matches the requirements and needs. Their crucial and unique task is to identify, understand, and interpret the constraints on a design in order to produce a successful result.\n\nEngineers apply techniques of engineering analysis in testing, production, or maintenance. Analytical engineers may supervise production in factories and elsewhere, determine the causes of a process failure, and test output to maintain quality. They also estimate the time and cost required to complete projects. Supervisory engineers are responsible for major components or entire projects. Engineering analysis involves the application of scientific analytic principles and processes to reveal the properties and state of the system, device or mechanism under study. Engineering analysis proceeds by separating the engineering design into the mechanisms of operation or failure, analyzing or estimating each component of the operation or failure mechanism in isolation, and recombining the components. They may analyze risk.\n\nMany engineers use computers to produce and analyze designs, to simulate and test how a machine, structure, or system operates, to generate specifications for parts, to monitor the quality of products, and to control the efficiency of processes.\n\nMost engineers specialize in one or more engineering disciplines. Numerous specialties are recognized by professional societies, and each of the major branches of engineering has numerous subdivisions. Civil engineering, for example, includes structural and transportation engineering and materials engineering include ceramic, metallurgical, and polymer engineering. Mechanical engineering cuts across just about every discipline since its core essence is applied physics. Engineers also may specialize in one industry, such as motor vehicles, or in one type of technology, such as turbines or semiconductor materials.\n\nSeveral recent studies have investigated how engineers spend their time; that is, the work tasks they perform and how their time is distributed among these. Research suggests that there are several key themes present in engineers’ work: (1) technical work (i.e., the application of science to product development); (2) social work (i.e., interactive communication between people); (3) computer-based work; (4) information behaviours. Amongst other more detailed findings, a recent work sampling study found that engineers spend 62.92% of their time engaged in technical work, 40.37% in social work, and 49.66% in computer-based work. Furthermore, there was considerable overlap between these different types of work, with engineers spending 24.96% of their time engaged in technical and social work, 37.97% in technical and non-social, 15.42% in non-technical and social, and 21.66% in non-technical and non-social.\n\nEngineering is also an information-intensive field, with research finding that engineers spend 55.8% of their time engaged in various different information behaviours, including 14.2% actively seeking information from other people (7.8%) and information repositories such as documents and databases (6.4%).\n\nThe time engineers spend engaged in such activities is also reflected in the competencies required in engineering roles. In addition to engineers’ core technical competence, research has also demonstrated the critical nature of their personal attributes, project management skills, and cognitive abilities to success in the role.\n\nThere are many branches of engineering, each of which specializes in specific technologies and products. Typically engineers will have deep knowledge in one area and basic knowledge in related areas. For example, mechanical engineering curricula typically includes introductory courses in electrical engineering, computer science, materials science, metallurgy, mathematics, and software engineering.\n\nWhen developing a product, engineers typically work in interdisciplinary teams. For example, when building robots an engineering team will typically have at least three types of engineers. A mechanical engineer would design the body and actuators. An electrical engineer would design the power systems, sensors, electronics, embedded software in electronics, and control circuitry. Finally, a software engineer would develop the software that makes the robot behave properly. Engineers that aspire to management engage in further study in business administration, project management and organizational or business psychology. Often engineers move up the management hierarchy from managing projects, functional departments, divisions and eventually CEO's of a multi-national corporation.\n\nEngineers have obligations to the public, their clients, employers, and the profession. Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large. Each engineering discipline and professional society maintains a code of ethics, which the members pledge to uphold. Depending on their specializations, engineers may also be governed by specific statute, whistleblowing, product liability laws, and often the principles of business ethics.\n\nSome graduates of engineering programs in North America may be recognized by the iron ring or Engineer's Ring, a ring made of iron or stainless steel that is worn on the little finger of the dominant hand. This tradition began in 1925 in Canada with The Ritual of the Calling of an Engineer, where the ring serves as a symbol and reminder of the engineer's obligations to the engineering profession. In 1972, the practice was adopted by several colleges in the United States including members of the Order of the Engineer.\n\nMost engineering programs involve a concentration of study in an engineering specialty, along with courses in both mathematics and the physical and life sciences. Many programs also include courses in general engineering and applied accounting. A design course, often accompanied by a computer or laboratory class or both, is part of the curriculum of most programs. Often, general courses not directly related to engineering, such as those in the social sciences or humanities, also are required.\n\nAccreditation is the process by which engineering programs are evaluated by an external body to determine if applicable standards are met. The Washington Accord serves as an international accreditation agreement for academic engineering degrees, recognizing the substantial equivalency in the standards set by many major national engineering bodies. In the United States, post-secondary degree programs in engineering are accredited by the Accreditation Board for Engineering and Technology.\n\nIn many countries, engineering tasks such as the design of bridges, electric power plants, industrial equipment, machine design and chemical plants, must be approved by a licensed professional engineer. Most commonly titled professional engineer is a license to practice and is indicated with the use of post-nominal letters; PE or P.Eng. These are common in North America, as is European engineer (EUR ING) in Europe. The practice of engineering in the UK is not a regulated profession but the control of the titles of chartered engineer (CEng) and incorporated engineer (IEng) is regulated. These titles are protected by law and are subject to strict requirements defined by the Engineering Council UK. The title CEng is in use in much of the Commonwealth.\nMany skilled / semi-skilled trades and engineering technicians in the UK call themselves engineers. A growing movement in the UK is to legally protect the title 'Engineer' so that only professional engineers can use it; a petition was started to further this cause.\n\nIn the United States, engineering is a regulated profession whose practice and practitioners are licensed and governed by law. Licensure is generally attainable through combination of education, pre-examination (Fundamentals of Engineering exam), examination (professional engineering exam), and engineering experience (typically in the area of 5+ years). Each state tests and licenses professional engineers. Currently, most states do not license by specific engineering discipline, but rather provide generalized licensure, and trust engineers to use professional judgment regarding their individual competencies; this is the favoured approach of the professional societies. Despite this, at least one of the examinations required by most states is actually focused on a particular discipline; candidates for licensure typically choose the category of examination which comes closest to their respective expertise. In the United States, an \"industrial exemption\" allows businesses to employ employees and call them an \"engineer\", as long as such individuals are under the direct supervision and control of the business entity and function internally related to manufacturing (manufactured parts) related to the business entity, or work internally within an exempt organization. Such person does not have the final authority to approve, or the ultimate responsibility for, engineering designs, plans, or specifications that are to be: (A) incorporated into fixed works, systems, or facilities on the property of others; or (B) made available to the public. These individuals are prohibited from offering engineering services directly to the public and/or other businesses, and/or engage in practice of engineering; unless the business entity is registered with the state's board of engineering, and the practice is carried on/supervised directly only by engineers licensed to engage in the practice of engineering. In some instances, some positions, such as a “sanitation engineer”, does not have any basis in engineering sciences. \n\nIn Canada, engineering is a self regulated profession. The profession in each province is governed by its own engineering association. For instance, in the Province of British Columbia an engineering graduate with four or more years of post graduate experience in an engineering-related field and passing exams in ethics and law will need to be registered by the Association for Professional Engineers and Geoscientists (APEGBC) in order to become a Professional Engineer and be granted the professional designation of P.Eng allowing one to practice engineering.\n\nIn Continental Europe, Latin America, Turkey, and elsewhere the title is limited by law to people with an engineering degree and the use of the title by others is illegal. In Italy, the title is limited to people who both hold an engineering degree and have passed a professional qualification examination (\"Esame di Stato\"). In Portugal, professional engineer titles and accredited engineering degrees are regulated and certified by the \"Ordem dos Engenheiros\". In the Czech Republic, the title \"engineer\" (Ing.) is given to people with a (masters) degree in chemistry, technology or economics for historical and traditional reasons. In Greece, the academic title of \"Diploma Engineer\" is awarded after completion of the five-year engineering study course and the title of \"Certified Engineer\" is awarded after completion of the four-year course of engineering studies at a Technological Educational Institute (TEI).\n\nThe perception and definition of the term 'engineer' varies across countries and continents.\n\nBritish school children in the 1950s were brought up with stirring tales of \"the Victorian Engineers\", chief amongst whom were Brunel, Stephenson, Telford, and their contemporaries. In the UK, \"engineering\" has more recently been styled as an industry sector consisting of employers and employees loosely termed \"engineers\" who included semi-skilled trades. However, the 21st-century view, especially amongst the more educated members of society, is to reserve the term \"engineer\" to describe a university-educated practitioner of ingenuity represented by the Chartered (or Incorporated) Engineer qualifications. However, a large proportion of the UK public still thinks of \"engineers\" as skilled tradespeople or even semi-skilled tradespeople with a high school education. Also, UK skilled and semi-skilled tradespeople style themselves as \"engineers\". This has created confusion in the eyes of some members of the public (particularly the less well educated) to understand what professional engineers actually do, from fixing car engines, television sets and refrigerators to designing and managing the development of aircraft, spacecraft, power stations, infrastructure, and other complex technological systems.\n\nIn France, the term 'ingénieur\" (engineer) is not a protected title and can be used by anyone, even by those who do not possess an academic degree.\n\nHowever, the title \"Ingénieur Diplomé\" (Graduate Engineer) is an official academic title that is protected by the government and is associated with the \"Diplôme d'Ingénieur\", which is one of the most prestigious academic degrees in France. Anyone misusing this title in France can be fined a large sum and jailed, as it is reserved for graduates of French engineering \"grandes écoles\" that provide highly intensive training in science and engineering. Among such institutions, the most renown (and hardest to gain admission) are Ecole Centrale Paris (Centrale), Ecole des Mines de Paris (Mines Paristech), Ecole Nationale Supérieure d'Arts et Métiers, Ecole Polytechnique, and Ecole des Ponts ParisTech. Engineering schools which were created during the French revolution have a special reputation among the French people, as they helped to make the transition from a mostly agricultural country of late 18th century to the industrially developed France of the 19th century. A great part of 19th-century France's economic wealth and industrial prowess was created by engineers that have graduated from Ecole Centrale Paris, Ecole des Mines de Paris, or Ecole Polytechnique. This was also the case after the WWII when France had to be rebuilt.\n\nBefore the \"réforme René Haby\" in the 70's, it was very difficult to be admitted to such schools, and the French \"ingénieurs\" were commonly perceived as the nation's elite (hence the term \"faire les Grandes écoles\" in language of older people). However, after the Haby reform and a string of further reforms (Modernization plans of French universities), several engineering schools were created which can be accessed with relatively lower competition, and this reputation as being part of the French elite now applies to those from 'top' engineering schools for engineers, École Nationale d'Administration (ENA) for managers or politicians and École Normale Supérieure (ENS) for researchers in science and humanities. Engineers are less highlighted in current French economy as industry provides less than a quarter of the GDP.\n\nIn the US and Canada, engineering is a regulated profession whose practice and practitioners are licensed and governed by law. Licensed professional engineers in Canada and the USA are referred to as P.Eng (Canada) and PE (USA). A 2002 study by the Ontario Society of Professional Engineers revealed that engineers are the third most respected professionals behind doctors and pharmacists.\n\nIn Ontario, and all other Canadian provinces, the \"title\" Engineer is protected by law and any non-licensed individual or company using the title is committing a legal offense, and can get fined. Companies usually prefer not to use the title except for license holders because of liability reasons, for instance, if the company filed a lawsuit and the judge, investigators, or lawyers found that the company is using the word engineer for non-licensed employees this could be used by opponents to hinder the company's efforts.\n\nIn the Indian subcontinent, Russia, Middle East, Africa, and China, engineering is one of the most sought after undergraduate courses, inviting thousands of applicants to show their ability in highly competitive entrance examinations.\n\nIn Egypt, the educational system makes engineering the second-most-respected profession in the country (after medicine); engineering colleges at Egyptian universities require extremely high marks on the General Certificate of Secondary Education ( \"al-Thānawiyyah al-`Āmmah\")—on the order of 97 or 98%—and are thus considered (along with the colleges of medicine, natural science, and pharmacy) to be among the \"pinnacle colleges\" ( \"kullīyāt al-qimmah\").\n\nIn the Philippines and Filipino communities overseas, engineers who are either Filipino or not, especially those who also profess other jobs at the same time, are addressed and introduced as \"Engineer\", rather than \"Sir/Madam\" in speech or \"Mr./Mrs./Ms.\" (\"G./Gng./Bb.\" in Filipino) before surnames. That word is used either in itself or before the given name or surname.\n\nIn companies and other organizations, there is sometimes a tendency to undervalue people with advanced technological and scientific skills compared to celebrities, fashion practitioners, entertainers, and managers. In his book, \"The Mythical Man-Month\", Fred Brooks Jr says that managers think of senior people as \"too valuable\" for technical tasks and that management jobs carry higher prestige. He tells how some laboratories, such as Bell Labs, abolish all job titles to overcome this problem: a professional employee is a \"member of the technical staff.\" IBM maintain a dual ladder of advancement; the corresponding managerial and engineering or scientific rungs are equivalent. Brooks recommends that structures need to be changed; the boss must give a great deal of attention to keeping his managers and his technical people as interchangeable as their talents allow.\n\n"}
{"id": "1886355", "url": "https://en.wikipedia.org/wiki?curid=1886355", "title": "Eudoxa", "text": "Eudoxa\n\nEudoxa is a Swedish think tank, formed in 2000. Eudoxa has a transhumanist, and liberal political profile, with a focus on promoting dynamism, emerging technologies, harm reduction policy and discussing the challenges of the environment and the future. It is independent from political parties and other political and religious interest groups.\n\nEudoxa organizes seminars and conferences about these subjects, produces reports for corporations and organizations and promotes public debate.\n\nIt has a staff consisting both of scientist and humanists, in order to bridge the rift between The Two Cultures on evaluating the effects of emerging technologies, and give a better analysis. Its intellectual inspiration derives much from the book The Future and Its Enemies by Virginia Postrel. \n\nEudoxa has discussed biotechnology, harm reduction, health care, nanotechnology, RFID, and intellectual property. \n\nEudoxa is the Swedish partner in the International Property Rights Index.\n\nThe think tank currently consists of: Waldemar Ingdahl, Alexander Sanchez, and Anders Sandberg.\n\nAccording to the \"2014 Global Go To Think Tank Index Report\" (Think Tanks and Civil Societies Program, University of Pennsylvania), Eudoxa is rated number 25 (of 45) in the \"Top Science and Technology Think Tanks\" of the world.\n\n"}
{"id": "35030422", "url": "https://en.wikipedia.org/wiki?curid=35030422", "title": "Eugène Eudes-Deslongchamps", "text": "Eugène Eudes-Deslongchamps\n\nEugène Eudes-Deslongchamps (10 March 1830 – 21 December 1889) was a French paleontologist and naturalist born in Caen, the son of paleontologist Jacques Amand Eudes-Deslongchamps (1794–1867). He died at Château Matthieu, Calvados.\n\nAround 1856 he succeeded his father as professor of zoology at the faculty of sciences at the University of Caen, later becoming a professor of geology and dean (1861). After the death of his father in 1867, he devoted himself to the completion of a memoir on the teleosaurs, the joint labours being embodied in his \"Prodrome des Téléosauriens du Calvados\". He contributed several of his memoirs to the Société Linnéenne de Normandie.\n\n\n"}
{"id": "20257632", "url": "https://en.wikipedia.org/wiki?curid=20257632", "title": "Faculty of Human, Social, and Political Science, University of Cambridge", "text": "Faculty of Human, Social, and Political Science, University of Cambridge\n\nThe Faculty of Human, Social, and Political Science at the University of Cambridge was created in 2011 out of a merger of the Faculty of Archaeology and Anthropology and the Faculty of Politics, Psychology, Sociology and International Studies. Many Cambridge HSPS students go on to work in politics and government in the United Kingdom, United States, Canada, Australia, and China. Other graduate destinations include the United Nations, World Bank, World Economic Forum, and various NGO-groups.\nThe Faculty houses 3 departments: Department of Archaeology and Anthropology, Department of Politics and International Studies and Department of Sociology. Each of these departments has a worldwide reputation for teaching and research, and the undergraduate curriculum (Tripos) is designed to serve both students who have a clear disciplinary commitment at the time of application as well as those who want a broader multidisciplinary degree. Students with a passion for politics can take advantage of links with such departments as Economics and History, those with interests in Sociology can draw on Anthropology and Geography, while those dedicated to pursuing an archaeology career can specialise from the first year or combine this with Biological and Social Anthropology.\n\nUndergraduate students study several disciplines in their first year and then specialise in one or two disciplines in their second and third years. Clearly specified tracks (Archaeology, Biological Anthropology, Politics, Psychology, Social Anthropology, Sociology, or a combination of disciplines) ensure that students graduate with appropriate intellectual and professional skills. Assyriology and Egyptology are also possible specialisations, within the Archaeology track.\n\nAt the postgraduate level, there are established one-year M.Phils in Archaeology (including Assyriology and Egyptology), Biological Anthropology, International Studies, Social Anthropology and Sociology. A new M.Phil in Politics was launched in 2008. Ph.D students conduct research within a wide range of subjects within Archaeology, Assyriology, Egyptology, Biological and Social Anthropology, Politics & International Studies and Sociology.\n\nThe Faculty is currently spread across several sites. The SPS Library (now affiliated with the University Library) and the Department of Sociology are on Free School Lane at the New Museums Site. The Department of Politics & International Studies is currently located at the Alison Richard Building on the Sidgwick Site. The Department of Archaeology & Anthropology is spread across the Downing Site, New Museums Site, and Henry Wellcome Building.\n\n\n\nTripos (BA)\n\nAn Archaeology and Anthropology Tripos has been taught at Cambridge for more than a century. A Politics, Psychology and Sociology Tripos (previously known as Social and Political Sciences, \"SPS\") has been running at Cambridge University, in some form, since 1970. From 2013, the PPS and A&A Triposes will be replaced by the Human, Social, and Political Sciences Tripos (HSPS), which will offer students the opportunity to explore a wide range of multi-disciplinary options before specialising in one or two subjects, or to specialise from the first year, according to their interests. \n\nPostgraduate (MPhil/PhD)\n\nThe Faculty teaches seven masters programmes in Politics, International Studies, Sociology, Social Anthropology, Social and Developmental Psychology, Archaeology (including Assyriology and Egyptology), Biological Anthropology. The Faculty also has around 200 students studying for doctorates at any one time. Many graduate students go on to work in politics and government in the United Kingdom, United States, Canada, Australia, and China. Other graduate destinations include the United Nations, World Bank, World Economic Forum, and various NGO-groups.\n\nThe number of applicants per place for Politics, Psychology and Sociology has traditionally been one of the highest in Cambridge. On average, there are six applications per offered place, though this ratio is better at some colleges such as Murray Edwards. Colleges with particular teaching strength in Human, Social, and Political Science include Selwyn, Gonville and Caius, Queens', King's, Sidney Sussex, Corpus Christi and Trinity. Numbers of applications for the new HSPS BA course remain high across all colleges. Typical offers for the course are A*AA at A Level, or 40–42 points out of 45 with 776 or 777 at Higher Level in the International Baccalaureate.\n\nAs of 2008-2009 the MPhil in Social and Developmental Psychology received 66 applications, with 7 starting the course in October 2008. The MPhil in Modern Society and Global Transformations saw 99 applicants, with 26 starting the course in October 2008.\n\n"}
{"id": "2850083", "url": "https://en.wikipedia.org/wiki?curid=2850083", "title": "Fleming's right-hand rule", "text": "Fleming's right-hand rule\n\nFleming's Right-hand Rule (for generators) shows the direction of induced current when a conductor attached to a circuit moves in a magnetic field. It can be used to determine the direction of current in a generator's windings.\n\nWhen a conductor such as a wire attached to a circuit moves through a magnetic field, an electric current is induced in the wire due to Faraday's law of induction. The current in the wire can have two possible directions. Fleming's right-hand rule gives which direction the current flows.\n\nThe right hand is held with the thumb, index finger and middle finger mutually perpendicular to each other (at right angles), as shown in the diagram.\n\n\nThe bolded letters in the directions above give a mnemonic way to remember the order. Another mnemonic for remembering the rule is the initialism \"FBI\", standing for Force (or otherwise motion), B the symbol for the magnetic field, and I the symbol for current. The subsequent letters correspond to subsequent fingers, counting from the top. Thumb -> F; First finger -> B; Second finger -> I\n\nThere is also a Fleming's left hand rule (for electric motors). The appropriately handed rule can be recalled from the letter \"g\", which is in \"right\" and \"generator\".\n\nThese mnemonics are named after British engineer John Ambrose Fleming, who invented them.\n\n\n"}
{"id": "7070579", "url": "https://en.wikipedia.org/wiki?curid=7070579", "title": "Genetics of aggression", "text": "Genetics of aggression\n\nThe field of psychology has been greatly influenced by the study of genetics. Decades of research has demonstrated that both genetic and environmental factors play a role in a variety of behaviors in humans and animals (e.g. Grigorenko & Sternberg, 2003). The genetic basis of aggression, however, remains poorly understood. Aggression is a multi-dimensional concept, but it can be generally defined as behavior that inflicts pain or harm on another.\n\nGenetic-developmental theory states that individual differences in a continuous phenotype result from the action of a large number of genes, each exerting an effect that works with environmental factors to produce the trait. This type of trait is influenced by multiple factors making it more complex and difficult to study than a simple Mendelian trait (one gene for one phenotype).\n\nPast thought on genetic factors influencing aggression tended to seek answers from chromosomal abnormalities. Specifically, four decades ago, the XYY genotype was (erroneously) believed by many to be correlated with aggression. In 1965 and 1966, researchers at the MRC Clinical & Population Cytogenetics Research Unit led by Dr. Court Brown at Western General Hospital in Edinburgh reported finding a much higher than expected nine XYY men (2.9%) averaging almost 6 ft. tall in a survey of 314 patients at the State Hospital for Scotland; seven of the nine XYY patients were mentally retarded. In their initial reports published before examining the XYY patients, the researchers suggested they might have been hospitalized because of aggressive behavior. When the XYY patients were examined, the researchers found their assumptions of aggressive behavior were incorrect. Unfortunately, many science and medicine textbooks quickly and uncritically incorporated the initial, incorrect assumptions about XYY and aggression—including psychology textbooks on aggression.\n\nThe XYY genotype first gained wide notoriety in 1968 when it was raised as a part of a defense in two murder trials in Australia and France. In the United States, five attempts to use the XYY genotype as a defense were unsuccessful—in only one case in 1969 was it allowed to go to a jury—which rejected it.\n\nResults from several decades of long-term follow-up of scores of unselected XYY males identified in eight international newborn chromosome screening studies in the 1960s and 1970s have replaced pioneering but biased studies from the 1960s (that used only institutionalized XYY men), as the basis for current understanding of the XYY genotype and established that XYY males are characterized by increased height but are not characterized by aggressive behavior. Though the link currently between genetics and aggression has turned to an aspect of genetics different from chromosomal abnormalities, it is important to understand where the research started and the direction it is moving towards today.\n\nAggression, as well as other behavioral traits, is studied genetically based on its heritability through generations. Heritability models of aggression are mainly based on animals due to the ethical concern in using humans for genetic study. Animals are first selectively bred and then placed in a variety of environmental conditions, allowing researchers to examine the differences of selection in the aggression of animals.\n\nAs with other topics in behavioral genetics, aggression is studied in three main experimental ways to help identify what role genetics plays in the behavior:\n\nThese three main experimental types are used in animal studies, studies testing heritability and molecular genetics, and gene/environment interaction studies. Recently, important links between aggression and genetics have been studied and the results are allowing scientists to better understand the connections.\n\nThe heritability of aggression has been observed in many animal strains after noting that some strains of birds, dogs, fish, and mice seem to be more aggressive than other strains. Selective breeding has demonstrated that it is possible to select for genes that lead to more aggressive behavior in animals. Selective breeding examples also allow researchers to understand the importance of developmental timing for genetic influences on aggressive behavior. A study done in 1983 (Cairns) produced both highly aggressive male and female strains of mice dependent on certain developmental periods to have this more aggressive behavior expressed. These mice were not observed to be more aggressive during the early and later stages of their lives, but during certain periods of time (in their middle-age period) were more violent and aggressive in their attacks on other mice. Selective breeding is a quick way to select for specific traits and see those selected traits within a few generations of breeding. These characteristics make selective breeding an important tool in the study of genetics and aggressive behavior.\n\nMice are often used as a model for human genetic behavior since mice and humans have homologous genes coding for homologous proteins that are used for similar functions at some biological levels. Mice aggression studies have led to some interesting insight in human aggression. Using reverse genetics, the DNA of genes for the receptors of many neurotransmitters have been cloned and sequenced, and the role of neurotransmitters in rodent aggression has been investigated using pharmacological manipulations. Serotonin has been identified in the offensive attack by male mice against intruder male mice. Mutants were made by manipulating a receptor for serotonin by deleting a gene for the serotonin receptor. These mutant male mice with the knockout alleles exhibited normal behavior in everyday activities such as eating and exploration, but when prompted, attacked intruders with twice the intensity of normal male mice. In offense aggression in mice, males with the same or similar genotypes were more likely to fight than males that encountered males of other genotypes. Another interesting finding in mice dealt with mice reared alone. These mice showed a strong tendency to attack other male mice upon their first exposure to the other animals. The mice reared alone were not taught to be more aggressive; they simply exhibited the behavior. This implicates the natural tendency related to biological aggression in mice since the mice reared alone lacked a parent to model aggressive behavior.\n\nOxidative stress arises as a result of excess production of reactive oxygen species in relation to defense mechanisms, including the action of antioxidants such as superoxide dismutase 1 (SOD1). Knockout of the Sod1 gene was experimentally introduced in male mice leading to impaired antioxidant defense. These mice were designated (\"Sod1-/-\"). The \"Sod1-/-\" male mice proved to be more aggressive than both heterozygous knockout males (\"Sod1+/-\") that were 50% deficient in SOD1, and wild-type males (\"Sod1+/+\"). The basis for the association of oxidative stress with increased aggression has not yet been determined.\n\nExperiments designed to study biological mechanisms are utilized when exploring how aggression is influenced by genetics. Molecular genetics studies allow many different types of behavioral traits to be examined by manipulating genes and studying the effect(s) of the manipulation.\n\nA number of molecular genetics studies have focused on manipulating candidate aggression genes in mice and other animals to induce effects that can be possibly applied to humans. Most studies have focused on polymorphisms of serotonin receptors, dopamine receptors, and neurotransmitter metabolizing enzymes. Results of these studies have led to linkage analysis to map the serotonin-related genes and impulsive aggression, as well as dopamin-related genes and proactive aggression. In particular, the serotonin 5-HT seems to be an influence in inter-male aggression either directly or through other molecules that use the 5-HT pathway. 5-HT normally dampens aggression in animals and humans. Mice missing specific genes for 5-HT were observed to be more aggressive than normal mice and were more rapid and violent in their attacks. Other studies have been focused on neurotransmitters. Studies of a mutation in the neurotransmitter metabolizing enzyme monoamine oxidase A (MAO-A) have been shown to cause a syndrome that includes violence and impulsivity in humans. Studies of the molecular genetics pathways are leading to the production of pharmaceuticals to fix the pathway problems and hopefully show an observed change in aggressive behavior.\n\nIn determining if a trait is related to genetic factors or environmental factors, twin studies and adoption studies are used. These studies examine correlations based on similarity of a trait and a person's genetic or environmental factors that could influence the trait. Aggression has been examined via both twin studies and adoption studies.\n\nTwin studies manipulate the environmental factors of behavior by examining if identical twins raised apart are different from twins raised together. Before the advancement of molecular genetics, twin studies were almost the only mode of investigation of genetic influences on personality. Heritability was estimated as twice the difference between the correlation for identical, or monozygotic, twins and that for fraternal, or dizygotic, twins. Early studies indicated that personality was fifty percent genetic. Current thinking holds that each individual picks and chooses from a range of stimuli and events largely on the basis of his genotype creating a unique set of experiences; basically meaning that people create their own environments.\n\n\n"}
{"id": "42425338", "url": "https://en.wikipedia.org/wiki?curid=42425338", "title": "H-Soz-Kult", "text": "H-Soz-Kult\n\nH-Soz-u-Kult (\"Humanities – Sozial und Kulturgeschichte\") is an\nonline information and communication platform for historians which disseminates academic news and publications.\n\nThe project is committed to the principles of open access and community network. Since its founding in 1996 the central editorial office is located at the History Department of the Humboldt University of Berlin. H-Soz-u-Kult is part of H-Net and one of the most important online communication and information services for Historians in the German-speaking world. It is read by more than 20,000 email subscribers in over 70 countries. In 2012, around one million page views by up to 210,000 unique visitors were registered per month on the website.\n\nH-Soz-u-Kult publishes a wide range of book reviews, conference reports, job offers, scholarships, tables of contents of academic journals, literature reports and other news from the historical science community. Most publications are in German but the number of English publications continually increases. The book reviews are the main emphasis of H-Soz-u-Kult – more than 12,000 reviews were accessible on its website in 2013. H-Soz-u-Kult’s main editorial office at the Humboldt University of Berlin is supported by a pro bono editorial staff which consists of over 40 researchers from almost all fields of historical science.\n\nH-Soz-u-Kult is a part of Clio-online, a partner in a wide range of other academic projects, and was supported by the German Research Foundation for many years. The editorial range has been augmented with contributions from the complementary forums history.transnational and zeitgeschichte-online since 2004 and infoclio.ch since 2009.\n\nCurrent articles from the academic world can be accessed via H-Soz-u-Kult’s website, email and RSS-feeds. \n\nH-Soz-u-Kult is the official media partner of the German Union of Historians.\n\n"}
{"id": "15524101", "url": "https://en.wikipedia.org/wiki?curid=15524101", "title": "Hand-Held Maneuvering Unit", "text": "Hand-Held Maneuvering Unit\n\nThe Hand-Held Maneuvering Unit (HHMU), also known as the maneuvering gun, or informally as \"the zip gun\", was used by astronaut Ed White in the first American \"spacewalk\" (extra-vehicular activity, EVA), on Gemini 4, June 3, 1965. Different models of HHMU were present on Gemini 4, 8, 10, and 11, but were only used on Gemini 4 and 10. It was also used aboard Skylab.\n\nAstronauts described the gun as easier to use than other methods of maneuvering during space-walking. It provided an impulse to send the space-walker away from and back to the spacecraft, and was the easiest way for him to control his motions in the microgravity environment.\n\nThe Gemini 4 device received its propellant from tanks on the device and used pressurized oxygen to control and propel the astronaut via conservation of momentum. White enjoyed using the gun and found it useful, but quickly ran out of propellant, forcing him to pull on his tether to continue maneuvers. However, fellow crewman James McDivitt recalled the gun as being \"hopeless\" and \"utterly useless\" as it required precise aim through the user's center of mass in order to translate in a straight line without inducing unwanted rotation.\n\nThe device carried on Gemini 8 (March 16–17, 1966) received its Freon 14 propellant from a tank to be carried on the astronaut's back. Astronaut David Scott never got a chance to use it, because the mission had to be terminated before his EVA due to a critical thruster problem.\n\nThe Gemini 10 device used by Michael Collins received its nitrogen gas propellant from inside the spacecraft, through a hose bundled with the astronaut's umbilical connector. Collins successfully used it to move back and forth between the Gemini and the Agena Target Vehicle.\n\nRichard Gordon did not get to use his HHMU on Gemini 11, because his EVA had to be cut short when he became fatigued.\n\n"}
{"id": "2847754", "url": "https://en.wikipedia.org/wiki?curid=2847754", "title": "Hans Strøm", "text": "Hans Strøm\n\nHans Strøm (25 January 1726 – 1 February 1797) was a Norwegian clergyman. He also became a prominent zoologist and naturalist. He is best associated with his topographical description of the traditional district of Sunnmøre.\n\nHans Strøm was born at Borgund in Møre og Romsdal, Norway. His father was a clergyman and many other relatives of both his father and mother were ministers. He attended the Bergen Cathedral School. He was educated as a Lutheran clergyman and in 1745 took a theological degree at the University of Copenhagen. Then he worked from 1750 to 1764 as chaplain in Borgund. In 1764 he became parish priest, first in Volda where he served until 1779, when he went to Eiker where he served as Vicar for 18 years. He died at Hokksund in Øvre Eiker in Buskerud.\n\nHans Strøm was the first Norwegian who gave species descriptions for Norwegian animals. The results of his research was published as \"Physisk og Oeconomisk Beskrivelse over Fogderiet Søndmør I–II\" (Copenhagen, 1762–1766), a work that established his reputation as a scientific authority. He followed up this work with a number of articles, particularly where the natural sciences were strongly represented. He co-founded the Royal Norwegian Society of Sciences and Letters in 1760, with Gerhard Schoning, the historian, and Johan Ernst Gunnerus, bishop of Trondheim. In 1779, Strøm was elected a foreign member of the Royal Swedish Academy of Sciences. He was also elected as a member of a number of science academies in Norway, Denmark and Germany.\n\n"}
{"id": "3907217", "url": "https://en.wikipedia.org/wiki?curid=3907217", "title": "Hybrid neural network", "text": "Hybrid neural network\n\nThe term hybrid neural network can have two meanings: \n\nAs for the first meaning, the artificial neurons and synapses in hybrid networks can be digital or analog. For the digital variant voltage clamps are used to monitor the membrane potential of neurons, to computationally simulate artificial neurons and synapses and to stimulate biological neurons by inducing synaptic. For the analog variant, specially designed electronic circuits connect to a network of living neurons through electrodes. \n\nAs for the second meaning, incorporating elements of symbolic computation and artificial neural networks into one model was an attempt to combine the advantages of both paradigms while avoid the shortcomings. Symbolic representations have advantages with respect to explicit, direct control, fast initial coding, dynamic variable binding and knowledge abstraction. Representations of artificial neural networks, on the other hand, show advantages for biological plausibility, learning, robustness (fault-tolerant processing and graceful decay), and generalization to similar input. Since the early 1990s many attempts have been made to reconcile the two approaches. \n\n\n"}
{"id": "18561915", "url": "https://en.wikipedia.org/wiki?curid=18561915", "title": "Inuit astronomy", "text": "Inuit astronomy\n\nThe Inuit have traditional names for many constellations, asterisms and stars. A number of these constellations overlap with, or can be found within, more commonly known western constellations:\n\n\n"}
{"id": "39252984", "url": "https://en.wikipedia.org/wiki?curid=39252984", "title": "List of Austrian inventors and discoverers", "text": "List of Austrian inventors and discoverers\n\nThis is a list of Austrian inventors and discoverers. The following list comprises people from Austria, and also people of predominantly Austrian heritage, in alphabetical order of the surname.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAustrians have a history of aircraft and math\n"}
{"id": "36732637", "url": "https://en.wikipedia.org/wiki?curid=36732637", "title": "List of disorders included in newborn screening programs", "text": "List of disorders included in newborn screening programs\n\nThis is a list of disorders included in newborn screening programs around the world, along with information on testing methodologies, disease incidence and rationale for being included in screening programs.\n\nThe following conditions and disorders were recommended as a \"core panel\" by the 2005 report of the American College of Medical Genetics (ACMG). The incidences reported below are from the full report, though the rates may vary in different populations. \nBlood cell disorders \nInborn errors of amino acid metabolism \nInborn errors of organic acid metabolism \nInborn errors of fatty acid metabolism \n\nMiscellaneous multisystem diseases\nNewborn screening by other methods than blood testing\nThe following disorders are additional conditions that may be detected by screening. Many are listed as \"secondary targets\" by the 2005 ACMG report. Some states are now screening for more than 50 congenital conditions. Many of these are rare and unfamiliar to pediatricians and other primary health care professionals.\n\nBlood cell disorders\n\nInborn errors of amino acid metabolism\n\nInborn errors of organic acid metabolism\n\nInborn errors of fatty acid metabolism\n\nMiscellaneous multisystem diseases\n\nIn addition to identifying a core list of disorders that infants in the United States should be screened for, the ACMG also established a framework for nominating future conditions, and the structure under which those conditions should be considered. \n"}
{"id": "7141683", "url": "https://en.wikipedia.org/wiki?curid=7141683", "title": "List of nursing journals", "text": "List of nursing journals\n\nThis is a list of notable academic journals about nursing.\n\n\n"}
{"id": "24761285", "url": "https://en.wikipedia.org/wiki?curid=24761285", "title": "Low-voltage electron microscope", "text": "Low-voltage electron microscope\n\nLow-voltage electron microscope (LVEM) is an electron microscope which operates at accelerating voltages of a few kiloelectronvolts or less. Traditional electron microscopes use accelerating voltages in the range of 10-1000 keV.\n\nLow voltage imaging in transmitted electrons is possible in many new scanning electron detector.\n\nLow cost alternative is dedicated table top low voltage transmission electron microscope. While its architecture is very similar to a conventional transmission electron microscope, it has a few key changes that enable it to take advantage of a 5 keV electron source, but trading off many advantages of higher voltage operations, including higher resolution, possibility of X-ray microanalysis and EELS, etc... Recently a new low voltage transmission electron microscope has been introduced that operates at variable voltage ranges between 6–25 kV.\n\nA substantial decrease of electron energy allows for a significant improvement of contrast of light elements. The comparison images below show that decreasing the acceleration voltage from 80 kV to 5 kV significantly enhances the contrast of test samples. The improved contrast is a direct result of increased electron scattering associated with a reduced accelerating voltage.\n\nLVEM brings an enhancement of imaging contrast nearly twenty times higher than for 100 kV. This is very promising for biological specimens which are composed from light elements and don't exhibit sufficient contrast in classical TEMs.\n\nFurther, a relatively low mean free path (15 nm) for organic samples at 5 kV means that for samples with constant thickness, high contrast will be obtained from small variations in density. For example, for 5% contrast in the LVEM bright field image, we will only need to have a difference in density between the phases of 0.07 g/cm. This means that the usual need to stain polymers for enhanced contrast in the TEM (typically done with osmium or ruthenium tetraoxide) may not be necessary with the low voltage electron microscopy technique.\n\nThe improved contrast allows for the significant reduction, or elimination, of the heavy metal negative staining step for TEM imaging of light elements (H, C, N, O, S, P). While staining is beneficial for experiments aimed at high resolution structure determination, it is highly undesirable in certain protein sample preparations, because it could destabilize the protein sample due to its acid pH and relatively high heavy metal concentration. The addition of stain to sectioned samples such as biological materials or polymers can also introduce imaging artifacts.\n\nLVEM experiments carried out on an extracted membrane protein sample that was analyzed with and without the staining procedure show a marked improvement in the appearance of the sample when standard staining is omitted. Results show that LVEM could be even more useful than conventional EM for this particular application because it avoids the potentially disrupting staining step, thus providing an undisturbed image of the protein’s aggregation state.\n\nAdditionally, The ability to eliminate the staining step could aid to improve safety in the lab, as common heavy metal stains, such as uranyl acetate do have associated health risks.\n\nThe first low-voltage electron microscopes were capable of spatial resolutions of about 2.5 nm in TEM, 2.0 nm in STEM, and 3.0 nm in SEM modes. The SEM resolution has been improved to ~1.2 nm at 800 eV by 2010, while a 0.14 nm TEM resolution at 15 keV has been reported in 2016.\n\nCurrently available low voltage microscopes are only able to obtain resolutions of 1.0–3 nanometers. While this is well beyond resolutions possible from optical (light) microscopes, they are not yet able to compete with the atomic resolution obtainable from conventional (higher voltage) electron microscopes.\n\nLow voltage limits the maximum thickness of samples which can be studied in the TEM or STEM mode. Whereas it is about 50-90 nm in conventional TEM, it decreases to around 20–65 nanometers for LVEM @ 5 kV. However, thicknesses of the order of 20 nm or less are required to attain the maximal resolution in the TEM and STEM modes 5 kV.. These thickness are sometimes achievable with the use of an ultramicrotome.\n\nin 2015 these limitations were overcome with a 25 kV low voltage electron microscope that can produce high quality results with thin sectioned samples up to around 100 nm+.\n\n\nLVEM is especially efficient for the following applications.\n\n"}
{"id": "10021126", "url": "https://en.wikipedia.org/wiki?curid=10021126", "title": "Lumi (software)", "text": "Lumi (software)\n\nlumi is a free, open source and open development software project for the analysis and comprehension of Illumina expression and methylation microarray data. The project was started in the summer of 2006 and set out to provide algorithms and data management tools of Illumina in the framework of Bioconductor. It is based on the statistical R programming language.\n\nThe lumi package provides an analysis pipeline for probe-level Illumina expression and methylation microarray data, including probe-identifier management (nuID), updated probe-to-gene mapping and annotation using the latest release of RefSeq (nuIDblast), probe-intensity transformation (VST) and normalization (RSN), quality control (QA/QC) and preprocessing methods specific for Illumina methylation data. By extending the ExprSet object with Illumina-specific features, lumi is designed to work with other Bioconductor packages, such as Limma and GOstats to detect differential genes and conduct Gene Ontology analysis.\n\nThe lumi project was started in the summer of 2006 at the Bioinformatics Core Facility of the Robert H. Lurie Comprehensive Cancer Center, Northwestern University. Originally lumi was designed for the analysis of Illumina Expression BeadArray data. Starting from 2010 (version > 2.0), functions of analyzing Illumina methylation microarray data was added. The project team consists of Drs. Pan Du, Simon M. Lin, and Warren A. Kibbe. The project was started upon a request for collaboration from Dr. Serdar E. Bulun to analyze a set of new Illumina microarray data acquired at his lab on the study of the effect of retinoic acids on cancers. Dr. Pan Du led the software development of the project. lumi was the first software package to utilize the unique design of redundancy of beadArrays for the data transformation and normalization processes. The first release of lumi was on January 3, 2007 through the Bioconductor website. Before its formal release, it was beta-tested at Norwegian Radiumhospital, Leiden University Medical Center, Universiteit van Amsterdam, Università degli Studi di Brescia, UC Davis, Wayne State University, NIH, M.D. Anderson Cancer Center, Case Western Reserve University, Harvard University, Washington University, and Walter and Eliza Hall Institute of Medical Research.\n\n\n"}
{"id": "40014265", "url": "https://en.wikipedia.org/wiki?curid=40014265", "title": "Mad About Physics", "text": "Mad About Physics\n\nMad about Physics: Brainteasers, Paradoxes, and Curiosities by Christopher Jargodzki and Franklin Potter (2001, published by John Wiley and Sons) is a book containing 397 puzzles and their solutions. The book covers information on mechanics, electricity, magnetism, and optics, as well as the physics of sports, space exploration, and astronomy. \"Mad about Physics\" has been translated into seven languages, including German, Greek, Japanese, and Chinese. As of 2013, \"Mad about Physics\" is in its 10th reprinting.\n\nFrom the preface: \"Solutions and more than three hundred references are provided. They constitute about two-thirds of the book. Most of the puzzles contain an element of surprise. Indeed, the clash between common sense conjecture and physical reasoning is the central theme that runs through this volume. Einstein characterized common sense as the collection of prejudices acquired by age eighteen, and we agree: At least in science, common sense is to be refined and often transcended rather than venerated. The present volume tries to undermine physical preconceptions employing paradoxes (from the Greek \"para\" and \"doxes\", meaning 'beyond belief') to create cognitive dissonance. 'Though this be madness, yet there is method in't.' We believe that far from being merely amusing, paradoxes are uniquely effective in addressing specific deficiencies in understanding. (cf. Daniel W. Welch, 'Using paradoxes.' \"American Journal of Physics\" 48 [1980]: 629–632).\"\n\n\"Mad about Physics cover (1st Ed.)\" contains a multitude of questions which are claimed to be both creative and difficult.\nA few examples:\n\n\nAn extensive set of marginalia is provided, including jokes, anecdotes, offbeat scientific facts, and unusual quotations ranging from Einstein on the sensation of the mystical to Bugs Bunny on the law of gravity. For example:\n\n\nA detailed 7-page index is also included.\n\nA number of reviews discuss \"Mad about Physics\".\nPhysicist Peter Ford (University of Bath, UK) described \"Mad about Physics\" as a \"fascinating new book.\" He wrote that \"many of its problems will be useful for teachers, both at senior level in schools and at universities, for discussion with students in small groups. Such tutorials should be used to encourage students to start talking about physics and 'thinking like a physicist.' \" \n\nCarol Ryback wrote, \"Here's a quick fix for those brain-teasing inquiries that stick in your mind like an old song. While not limited to astronomy-related trivia, 'Mad about Physics' – like a top-40 countdown on the radio – has an allure that makes you want more.\"\n\nIn 2002, \"Mad about Physics\" was selected by the New York Public Library as one of the best titles of the year 2001 in the teen books and media category.\n\n\n"}
{"id": "40022512", "url": "https://en.wikipedia.org/wiki?curid=40022512", "title": "Paul Bernhard Gerhard", "text": "Paul Bernhard Gerhard\n\nPaul Bernhard Gerhard (10 March 1824 in Leipzig – 18 May 1908 in St. Louis, Missouri) was a German entomologist who specialised in Lepidoptera.\n\n\n"}
{"id": "21650481", "url": "https://en.wikipedia.org/wiki?curid=21650481", "title": "Progress of Theoretical and Experimental Physics", "text": "Progress of Theoretical and Experimental Physics\n\nProgress of Theoretical and Experimental Physics is a monthly peer-reviewed scientific journal published by Oxford University Press on behalf of the Physical Society of Japan. It was established as \"Progress of Theoretical Physics\" in July 1946 by Hideki Yukawa and obtained its current name in January 2013.\n\n\"Progress of Theoretical and Experimental Physics\" is part of the SCOAP initiative.\n"}
{"id": "58686042", "url": "https://en.wikipedia.org/wiki?curid=58686042", "title": "Rebecca Jockusch", "text": "Rebecca Jockusch\n\nRebecca Ann Jockusch, Ph.D. (UC Berkeley, 2001), is a Canadian chemist; she is an associate professor at the Department of Chemistry of the University of Toronto (UToronto) who is active in the field of mass spectrometry.\n\n"}
{"id": "31611166", "url": "https://en.wikipedia.org/wiki?curid=31611166", "title": "Relative apparent synapomorphy analysis", "text": "Relative apparent synapomorphy analysis\n\nRelative apparent synapomorphy analysis, or RASA, is a method that aims to determine whether a given character is shared between taxa due to shared ancestry or due to convergence. A synapomorphy is a shared trait found among two or more taxa and their most recent common ancestor, whose ancestor in turn does not possess the trait. RASA assigns a score to the character based on its potential to be informative.\n\nThe method performs poorly when used to select an outgroup taxon, to quantify the amount of phylogenetic signal present, or to identify taxa that may be prone to long branch attraction.\n"}
{"id": "34564906", "url": "https://en.wikipedia.org/wiki?curid=34564906", "title": "Ronald Alexander McIntosh", "text": "Ronald Alexander McIntosh\n\nRonald Alexander McIntosh (21 January 1904 – 17 May 1977) was a New Zealand journalist who was most famous for his contributions to astronomy.\n\nHe was born in Auckland, New Zealand on 21 January 1904 and left school at the age of 14. In 1926 he began a long career in journalism when he started work at The New Zealand Herald as a proofreader. He married Harriet Munro in 1930. After World War II service in New Zealand in military intelligence he re-joined \"The New Zealand Herald\", becoming a sub-editor in 1945 before leaving the following year. After spending some years working on aviation magazines and in public relations McIntosh returned to the \"Herald\" in 1957 as a senior sub-editor. Ronald McIntosh died in Auckland in 1977 and was survived by his wife, son, daughter and sister.\n\nHis interest in astronomy was kindled by the 1910 appearance of Halley's Comet, and at age 13 he made his first binocular observations of the Moon. From 1919 to 1950 he collaborated with a number of other New Zealand amateurs in naked-eye observations of meteors. The group recorded 15,627 meteors between 1927 and 1945, with McIntosh contributing about half. He drew on these observations when he published a number of research papers on meteors. One of his most important papers was \"An Index to Southern Meteor Showers\". Published in 1935, it remained the standard work for more than 40 years.\n\nIn 1927 he purchased a 14-inch Newtonian telescope which was used for lunar and planetary studies. Over a 30-year period he studied various lunar features and published papers based on his observations. He studied the crater Aristarchus for a long period, beginning in the early 1950s. His main planetary focus was Jupiter and he made many observations from 1927 to the 1960s. He also observed Saturn and Mars as well as Comets.\n\nMcIntosh was also interested in the history of science, and published papers on the history of astronomy in New Zealand. He played a role in establishing the Auckland Observatory and devoted a lot of his time to popularising astronomy through lectures, planetarium sessions and newspaper articles. He was a long-time member of the New Zealand Astronomical Society (later the Royal Astronomical Society of New Zealand), the Auckland Astronomical Society, the American Meteor Society and the Royal Astronomical Society.\n\nHis work was frequently acknowledged by Charles Pollard Olivier of the American Meteor Society and he was elected to the International Astronomical Union's Commission 22 on Meteors. He was twice awarded the Donovan Medal by the Astronomical Society of Australia.\n"}
{"id": "57865596", "url": "https://en.wikipedia.org/wiki?curid=57865596", "title": "Ronchigram", "text": "Ronchigram\n\nRonchigram is the convergent beam diffraction pattern of an amorphous material. The structure of the Ronchigram encodes information about the aberration phase field across the objective aperture. As such, Ronchigrams have become increasingly important with the invention of aberration corrected scanning transmission electron microscopy.\n\n"}
{"id": "51235085", "url": "https://en.wikipedia.org/wiki?curid=51235085", "title": "Safecast (organization)", "text": "Safecast (organization)\n\nSafecast is an international, volunteer-centered organization devoted to open citizen science for the environment. Safecast was established by Sean Bonner, Pieter Franken, and Joi Ito shortly after the Fukushima Daiichi nuclear disaster in Japan, following the Tōhoku earthquake on 11 March 2011 and manages a global open data network for ionizing radiation monitoring.\n\nThe Safecast team, with help of International Medcom, Tokyo Hackerspace and other volunteers, has designed various devices for radiation mapping. These include the bGeigie and bGeigie Nano for mobile applications (carborne and walking measurements) as well as fixed stations called Pointcast.\n\nAll data are collected via the Safecast API and are presented on the publicly available interactive Safecast Tile Map.\n\nSafecast later expanded to offer air quality sensors which also report open crowdsourced maps.\n\nSafecast bGeigie Nano is a portable radiation detector equipped with a Geiger-Mueller tube type detector, built-in GPS and logging to microSD card.\n\nThe bGeigie Nano is available as a kit, so the user needs to learn how to solder in order to build it from the supplied parts. The kit can be purchased from the Kithub product page. The device was developed in collaboration with International Medcom Inc. and shares some parts with their Inspector Alert™ detector. According to the Safecast Github page \"bGeigie Nano is a lighter version of the bGeigie Mini using an Arduino Fio, a GpsBee, an OpenLog and an Inspector Alert geiger counter. The aim is to make everything fit in a Pelican Micro Case 1040\".\nThe final version of the bGeigie is placed inside the smaller Pelican Micro Case 1010. It features the LND 7317 pancake Geiger-Mueller tube type detector, a GPS receiver and is expandable with a Bluetooth module. The mode switch offers choice between geotagged radiation logging (data saved to microSD card) and measuring without GPS - showing also Bq/m2 (Cs137) values. Inside the case the unit is calibrated for gamma radiation. The main unit taken out of its case will additionally do α- and β-detection for careful use as a surface contamination spot meter.\n\nAccording to the KitHub page Safecast devices are also used by the following institutions: \n\nSlovenian NGO IRNAS - Institute for development of advanced applied systems Rače / Inštitut za razvoj naprednih aplikativnih sistemov Rače performs monitoring in Slovenia with their one bGeigie\n\n\n"}
{"id": "3069014", "url": "https://en.wikipedia.org/wiki?curid=3069014", "title": "Section (botany)", "text": "Section (botany)\n\nIn botany, a section () is a taxonomic rank below the genus, but above the species. The subgenus, if present, is higher than the section, and the rank of series, if present, is below the section. Sections may in turn be divided into subsections.\n\nSections are typically used to help organise very large genera, which may have hundreds of species. A botanist wanting to distinguish groups of species may prefer to create a taxon at the rank of section or series to avoid making new combinations, i.e. many new binomial names for the species involved.\n\nExamples:\n"}
{"id": "39483851", "url": "https://en.wikipedia.org/wiki?curid=39483851", "title": "Self-organized criticality control", "text": "Self-organized criticality control\n\nIn applied physics, the concept of controlling self-organized criticality refers to the control of processes by which a self-organized system dissipates energy. The objective of the control is to reduce the probability of occurrence of and size of energy dissipation bursts, often called \"avalanches\", of self-organized systems. Dissipation of energy in a self-organized critical system into a lower energy state can be costly for society, since it depends on avalanches of all sizes usually following a kind of power law distribution and large avalanches can be damaging and disruptive.\n\nSeveral strategies have been proposed to deal with the issue of controlling self-organized criticality:\n\n\nThere are several events that arise in nature or society where these ideas of control may help to avoid them:\n\n\nThe failure cascades in electrical transmission and financial sectors occur because economic forces cause these systems to operate near a critical point, where avalanches of indeterminate size become possible.\n\n"}
{"id": "3615664", "url": "https://en.wikipedia.org/wiki?curid=3615664", "title": "The Curiosity Show", "text": "The Curiosity Show\n\nThe Curiosity Show is an Australian educational children's television show produced from 1972 to 1990 and hosted by zoologist Dr. Rob Morrison and Dr. Deane Hutton. The show was produced by Banksia Productions in South Australia for the Nine Network. 500 episodes were produced.\n\nBanksia Productions produced the popular children's series \"Here's Humphrey\" from 1965. The company planned to add some science segments in 1971 and sought assistance from the South Australian Institute of Technology. Rob Morrison and Deane Hutton were selected as presenters and the segments were introduced as \"Humphrey B Bear's Curiosity Show\". After positive reception from the audience, Banksia Productions and the Nine Network agreed to produce a spin-off series. Planning commenced with the working title \"The F Show\".\n\nHutton adds that until the early 1970s, children's television was aimed at younger children. The broadcasting regulations were changed to require a proportion of programmes to be aimed at school-age children, broadcast after school hours. This prompted the creation of the Curiosity Show as a separate show.\n\nFrom 1972 to 1980 the format was a 60-minute show presented by Morrison, Hutton, Ian Fairweather, Alister Smart, Belinda Davey, Gabrielle Kelly, Dr Mark Dwyer and Lynn Weston. The emphasis was on science but also included general craft and music. Producers were Neil Smith, Kate Kennedy White (1978–79), James Lingwood (1980) and Ian Smyth.\n\nFrom 1980 the show was reduced to 30 minutes, presented by Morrison and Hutton, with emphasis on science, nature and the environment.\n\n\"The Curiosity Show\" won many national and international awards, including the coveted Prix Jeunesse in 1984, voted by peers from around the world as the best factual program for children.\n\nThe program placed a strong emphasis on practical demonstrations of various science topics, and included activities such as floating a ping-pong ball on a stream of air, recreating historical devices, setting off a room full of mouse traps, the science of musical instruments and freezing objects with liquid nitrogen. Commonly, segments presented scientific concepts in the form of tricks and puzzles.\n\nMany segments described a sequence of steps to build something out of common household materials with longer builds invariably ending with the phrase \"here's one I prepared earlier\" so as to keep the segment moving. Hutton's catchphrase, after presenting a hypothesis he postulates the audience may be curious about, was to declare \"well, I'm glad you asked,\" before responding to the hypothesis. The use of household materials was deliberate, in order to demystify science and ensure that children, wherever they lived, could make what they needed rather than rely on buying it, and this proved popular with the young audience who could easily replicate the demonstrations at home. Morrison suggested that they should always show what they had made working so that children would know that their own constructions would work if made properly and also to show the limitations of the constructions to dispel any overambitious expectations. Both Morrison and Hutton always told viewers to get their parents' permission before building things or conducting experiments, especially if it involved the use of sharp objects such as knives or scissors or the use of flames or hot or dangerous liquids.\n\nFour companion books were available in 1981 produced by Jacaranda Press containing scientific explanations and instructions for experiments for children to perform at home. Each of the books was themed upon one of the four Western classical elements of earth, air, fire and water. Together, Hutton and Morrison published 11 books, including \"Supermindstretchers\", \"The Arrow book of Things to Make and Do\" and \"String for Lunch\" (Ashton Scholastic). Morrison published more than 40 additional books which included material from Curiosity Show, including \"Nature in the Making\", \"A Field Guide to the Tracks and Traces of Australian Animals\" the first such field guide in Australia and still the only one to deal with all taxa, \"Clever and Quirky Creatures\", \"It's Raining Frogs and Fishes\" and many more for the school reading programs of various publishers. Morrison's field guide to Tracks and Traces, and a segment he had prepared for Curiosity Show on dingos at Uluru, led to his involvement in the Morling enquiry into the Chamberlain Convictions. He gave evidence on dingo and dog tracks and conducted various forensic investigations on dingo gapes and behaviour. His collection of specimens and artefacts from the trial have been acquired by the National Museum of Australia.\n\nThe Children's Television Workshop wanted to make a version of the \"Curiosity Show\" using American scientists as presenters. Rob Morrison and Deane Hutton were consultants in the early planning stages. PBS didn't think that middle-aged scientists would engage a young audience (despite the popularity of the format in Australia) and insisted that any science show be hosted/presented by young people. CTW eventually reworked the concept into \"3-2-1 Contact\".\n\nIn 2014, \"The Curiosity Show\" made a brief online series which was available on YouTube. It was co-hosted by Morrison and Hutton again.\n\nIn 2013, the show's former hosts, Hutton and Morrison, announced they had purchased the remaining rights to the show for an undisclosed sum, from Banksia Productions which had gone into liquidation. On July the 12th 2013, in conjunction with producer Enabled Solutions, they launched a YouTube channel to make the episodes and segments available for a new generation of viewers. There are some 5,000 segments, and some have attracted significant audiences, especially in the USA and India; , one segment about self-starting syphons had been viewed more than a million times.\n\nIn May 2014, Hutton and Morrison released, on the YouTube channel, the \"\"Curiosity Show\"'s first new episode since 1990\". The online production was funded with the assistance of Kellogg Australia. The episode followed the original format of the programme, supported by extra internet resources, with Hutton and Morrison performing experiments related to cereal, including making homemade cornflakes and cornflour non-Newtonian fluid. The episode was praised by the channel's followers.\n\n\n"}
{"id": "16128755", "url": "https://en.wikipedia.org/wiki?curid=16128755", "title": "The Worlds of Science", "text": "The Worlds of Science\n\nThe Worlds of Science is a series of science book paperbacks by various authors published by Pyramid Books in the 1960s. The series included both reprints of works originally published independently and new works written especially for the series. Prominent contributors included Isaac Asimov and L. Sprague de Camp, among others.\n\nBooks in the series include:\n"}
{"id": "53256704", "url": "https://en.wikipedia.org/wiki?curid=53256704", "title": "Threatcasting", "text": "Threatcasting\n\nThreatcasting is a conceptual framework used to help multidisciplinary groups envision future scenarios. It is also a process that enables systematic planning against threats ten years in the future. Utilizing the threatcasting process, groups explore possible future threats and how to transform the future they desire into reality while avoiding undesired futures. Threatcasting is a continuous, multiple-step process with inputs from social science, technical research, cultural history, economics, trends, expert interviews, and science fiction storytelling. These inputs inform the exploration of potential visions of the future.\n\nOnce inputs are explored for impact and application, participants create a science fiction story (Science Fiction Prototyping) based ten years in the future to add context around human activity. Science Fiction Prototyping consists of a future story about a person in a place doing a thing. The threatcasting process results in creation of many potential futures scenarios - some futures are desirable while others are not. Identifying both types of futures (desirable and undesirable) will help the participant recognize which future to aim toward, and which to avoid. Utilizing the scenarios, participants plot actions necessary in the present and at various intervals working toward the ten year future scenario. These actions will help participants understand how to empower or disrupt the target future scenario. Flags (warning events) are also determined in order to map societal indicators onto the recommended path toward the targeted future. When identified flags appear in society, threatcasting participants map these back to the original forecast to see whether or not they are on track toward the target future scenario.\n\nThe notion of threatcasting can be traced back to Brian David Johnson, an applied futurist, who first began using threatcasting, also referred to as futurecasting, in 2011 and to George Hemingway of the Stratalis Group, who pioneered notion of futurecasting for corporate strategy and innovation industrial markets, including mining in the same year. Early adopters of threatcasting include the United States Air Force Academy, the Government of California, and the Army Cyber Institute at West Point Military Academy. Official use of the term threatcasting is attributed to Brian David Johnson in a 2014 Gazette article “Drones, smart hydrants considered by experts looking at future of firefighting.”\n\nThreatcasting is fundamentally different from traditional strategic planning and scenario building processes due to the identification of specific actions, indicators and concrete steps that can be taken today to disrupt, mitigate and recover from future threats.\n\nThe Army Cyber Institute at West Point in conjunction with Arizona State University's Global Securities Initiative and the School for the Future of Innovation in Society have established a Threatcasting Lab to host and manage a Cyber Threatcasting Project which looks to envision future cyber threats ten years in the future. The first session of this collaborative group was held at West Point, NY in August 2016.\n\n\n"}
{"id": "5685961", "url": "https://en.wikipedia.org/wiki?curid=5685961", "title": "Unicycle cart", "text": "Unicycle cart\n\nThe term unicycle is often used in robotics and control theory to mean a generalised cart or car moving in a two-dimensional world; these are also often called \"unicycle-like\" or \"unicycle-type\" vehicles. This usage is distinct from the literal sense of \"one wheeled robot bicycle\".\n\nThese theoretical vehicles are typically shown as having two parallel driven wheels, one mounted on each side of their centre, and (presumably) some sort of offset castor to maintain balance; although in general they could be any vehicle capable of simultaneous arbitrary rotation and translation. An alternative realization uses a single driven wheel with steering, and a pair of idler wheels to give balance and allow a steering torque to be applied.\n\nA physically realisable unicycle, in this sense, is a nonholonomic system. This is a system in which a return to the original internal (wheel) configuration does not guarantee return to the original system (unicycle) position. In other words, the system outcome is path-dependent.\n\n"}
{"id": "32405", "url": "https://en.wikipedia.org/wiki?curid=32405", "title": "Vegetable farming", "text": "Vegetable farming\n\nVegetable farming is the growing of vegetables for human consumption. The practice probably started in several parts of the world over ten thousand years ago, with families growing vegetables for their own consumption or to trade locally. At first manual labour was used but in time livestock were domesticated and the ground could be turned by the plough. More recently, mechanisation has revolutionised vegetable farming with nearly all processes being able to be performed by machine. Specialist producers grow the particular crops that do well in their locality. New methods such as aquaponics, Raised-bed gardening|raised beds and cultivation under glass are used. Marketing can be done locally in farmer's markets, traditional markets or pick-your-own operations, or farmers can contract their whole crops to wholesalers, canners or retailers.\n\nOriginally, vegetables were collected from the wild by hunter-gatherers and entered cultivation in several parts of the world, probably during the period 10,000 BC to 7,000 BC, when a new agricultural way of life developed. At first, plants which grew locally would have been cultivated, but as time went on, trade brought exotic crops from elsewhere to add to domestic types. Nowadays, most vegetables are grown all over the world as climate permits. \n\nTraditionally it was done in the soil in small rows or blocks, often primarily for consumption on the farm, with the excess sold in nearby towns. Later, farms on the edge of large communities could specialize in vegetable production, with the short distance allowing the farmer to get his produce to market while still fresh. The three sisters method used by Native Americans (specifically the Haudenosaunee/Iroquois) grew squash, beans and corn together so that the plants enhanced each other's growth. Planting in long rows allows machinery to cultivate the fields, increasing efficiency and output; however, the diversity of vegetable crops requires a number of techniques to be used to optimize the growth of each type of plant. Some farms, therefore, specialize in one vegetable; others grow a large variety. Due to the needs to market vegetables while fresh, vegetable gardening has high labor demands. Some farms avoid this by running u-pick operations where the customers pick their own produce. The development of ripening technologies and refrigeration has reduced the problems with getting produce to market in good condition.\n\nOver the past 100 years a new technique has emerged—raised bed gardening, which has increased yields from small plots of soil without the need for commercial, energy-intensive fertilizers. Modern hydroponic farming produces very high yields in greenhouses without using any soil.\n\nSeveral economic models exist for vegetable farms: farms may grow large quantities of few vegetables and sell them in bulk to major markets or middlemen, which requires large growing operations; farms may produce for local customers, which requires a larger distribution effort; farms may produce a variety of vegetables for sale through on-farm stalls, local farmer's markets, u-pick operations. This is quite different from commodity farm products like wheat and maize which do not have the ripeness problems and are sold off in bulk to the local granary. Large cities often have a central produce market which handles vegetables in a commodity-like manner, and manages distribution to most supermarkets and restaurants.\n\nIn America, vegetable farms are in some regions known as truck farms; \"truck\" is a noun for which its more common meaning overshadows its historically separate use as a term for \"vegetables grown for market\". Such farms are sometimes called muck farms, after the dark black soil in which vegetables grow well.\n\nVegetables which are farmed include:\n\n"}
