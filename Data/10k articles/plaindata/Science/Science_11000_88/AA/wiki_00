{"id": "23690843", "url": "https://en.wikipedia.org/wiki?curid=23690843", "title": "Absolute rotation", "text": "Absolute rotation\n\nIn physics, the concept of absolute rotation—rotation independent of any external reference—is a topic of debate about relativity, cosmology, and the nature of physical laws.\n\nFor the concept of absolute rotation to be scientifically meaningful, it must be measurable. In other words, can an observer distinguish between the rotation of an observed object and their own rotation? Newton suggested two experiments to resolve this problem. One is the effects of centrifugal force upon the shape of the surface of water rotating in a bucket, equivalent to the phenomenon of rotational gravity used in proposals for manned spaceflight.\nThe second is the effect of centrifugal force upon the tension in a string joining two spheres rotating about their center of mass.\n\nNewton suggested the shape of the surface of the water indicates the presence or absence of absolute rotation relative to the fixed stars: rotating water has a curved surface, still water has a flat surface. Because rotating water has a concave surface, if the surface you see is concave, and the water does not seem to you to be rotating, then \"you\" are rotating with the water.\n\nCentrifugal force is needed to explain the concavity of the water in a co-rotating frame of reference (one that rotates with the water) because the water appears stationary in this frame, and so should have a flat surface. Thus, observers looking at the stationary water need the centrifugal force to explain why the water surface is concave and not flat. The centrifugal force pushes the water toward the sides of the bucket, where it piles up deeper and deeper, Pile-up is arrested when any further climb costs as much work against gravity as is the energy gained from the centrifugal force, which is greater at larger radius.\n\nIf you need a centrifugal force to explain what you see, then you are rotating. Newton's conclusion was that rotation is absolute.\nOther thinkers suggest that pure logic implies only relative rotation makes sense. For example, Bishop Berkeley and Ernst Mach (among others) suggested that it is relative rotation with respect to the fixed stars that matters, and rotation of the fixed stars relative to an object has the same effect as rotation of the object with respect to the fixed stars. Newton's arguments do not settle this issue; his arguments may be viewed, however, as establishing centrifugal force as a basis for an operational definition of what we actually mean by absolute rotation.\n\nNewton also proposed another experiment to measure one's rate of rotation: using the tension in a cord joining two spheres rotating about their center of mass. Non-zero tension in the string indicates rotation of the spheres, whether or not the observer thinks they are rotating. This experiment is simpler than the bucket experiment in principle, because it need not involve gravity.\n\nBeyond a simple \"yes or no\" answer to rotation, one may actually calculate one's rotation. To do that, one takes one's measured rate of rotation of the spheres and computes the tension appropriate to this observed rate. This calculated tension then is compared to the measured tension. If the two agree, one is in a stationary (non-rotating) frame. If the two do \"not\" agree, to obtain agreement, one must include a centrifugal force in the tension calculation; for example, if the spheres appear to be stationary, but the tension is non-zero, the entire tension is due to centrifugal force. From the necessary centrifugal force, one can determine one's speed of rotation; for example, if the calculated tension is greater than measured, one is rotating in the sense opposite to the spheres, and the larger the discrepancy the faster this rotation.\n\nThe tension in the wire is the required centripetal force to sustain the rotation. What is experienced by the physically rotating observer is the centripetal force and the physical effect arising from his own inertia. The effect arising from inertia is referred to as reactive centrifugal force.\n\nWhether or not the effects from inertia are attributed to a fictitious centrifugal force is a matter of choice.\n\nIn a similar fashion, if we did not know the Earth rotates about its axis, we could infer this rotation from the centrifugal force needed to account for the bulging observed at its equator.\n\nIn his \"Principia\", Newton proposed the shape of the rotating Earth was that of a homogeneous ellipsoid formed by an equilibrium between the gravitational force holding it together and the centrifugal force pulling it apart. This effect is more easily seen with the planet Saturn which has a radius 8.5 to 9.5 times that of Earth but has a rotational period of only 10.57 hours. The ratios of Saturn's diameters is approximately 11 to 10.\n\nIsaac Newton explained this in his \"Principia Mathematica\" (1687) in which he outlined his theory and calculations on the shape of the Earth. Newton theorized correctly that the Earth was not precisely a sphere but had an oblate ellipsoidal shape, slightly flattened at the poles due to the centrifugal force of its rotation. Since the surface of the Earth is closer to its center at the poles than at the equator, gravity is stronger there. Using geometric calculations, he gave a concrete argument as to the hypothetical ellipsoid shape of the Earth.\nA modern measurement of the Earth's oblateness leads to an equatorial radius of 6378.14 km and a polar radius of 6356.77 km, about 0.1% less oblate than Newton's estimate. A theoretical determination of the precise extent of oblateness in response to a centrifugal force requires an understanding of the make-up of the planet, not only today but during its formation.\n\nIn 1672 Jean Richer found the first evidence that gravity was not constant over the Earth (as it would be if the Earth were a sphere); he took a pendulum clock to Cayenne, French Guiana and found that it lost minutes per day compared to its rate at Paris. This indicated the acceleration of gravity was less at Cayenne than at Paris. Pendulum gravimeters began to be taken on voyages to remote parts of the world, and it was slowly discovered that gravity increases smoothly with increasing latitude, gravitational acceleration being about 0.5% greater at the poles than at the equator.\n\nIt was only in 1743 that Alexis Clairaut, in \"Théorie de la figure de la terre\", was able to show that Newton's theory that the Earth was ellipsoidal was correct\nClairaut showed how Newton's equations were incorrect, and did not prove an ellipsoid shape to the Earth. \nHowever, he corrected problems with the theory, that in effect would prove Newton's theory correct. Clairaut believed that Newton had reasons for choosing the shape that he did, but he did not support it in \"Principia.\" Clairaut's article did not provide an valid equation to back up his argument as well. This created much controversy in the scientific community.\n\nFrench physicist Georges Sagnac in 1913 conducted an experiment that was similar to the Michelson–Morley experiment, but was intended to observe the effects of rotation. Sagnac set up this experiment to prove the existence of the luminiferous aether that Einstein's 1905 theory of special relativity had discarded.\n\nThe Sagnac experiment and later similar experiments showed that a stationary object on the surface of the Earth will rotate once every rotation of the Earth when using stars as a stationary reference point. Rotation was thus concluded to be absolute rather than relative.\n\nMach's principle is the name given by Einstein to a hypothesis often credited to the physicist and philosopher Ernst Mach.\n\nThe idea is that the local motion of a rotating reference frame is determined by the large-scale distribution of matter in the universe. Mach's principle says that there is a physical law that relates the motion of the distant stars to the local inertial frame. If you see all the stars whirling around you, Mach suggests that there is some physical law which would make it so you would feel a centrifugal force. The principle is often stated in vague ways, like \"mass out there influences inertia here\".\n\nThe example considered by Einstein was the rotating elastic sphere. Like a rotating planet bulging at the equator, a rotating sphere deforms into an oblate (squashed) spheroid depending on its rotation. \n\nIn classical mechanics, an explanation of this deformation requires external causes in a frame of reference in the spheroid it is not rotating, and these external causes may be taken as \"absolute \nrotation\" in classical physics and special relativity. \nIn general relativity, no external causes are invoked. The rotation is relative to the local geodesics, and since the local geodesics eventually channel information from the distant stars, there appears to be absolute rotation relative to these stars.\n\n"}
{"id": "1270090", "url": "https://en.wikipedia.org/wiki?curid=1270090", "title": "Academic administration", "text": "Academic administration\n\nAcademic administration is a branch of university or college employees responsible for the maintenance and supervision of the institution and separate from the faculty or academics, although some personnel may have joint responsibilities. Some type of separate administrative structure exists at almost all academic institutions, as fewer and fewer schools are governed by employees who are also involved in academic or scholarly work. Many senior administrators are academics who have advanced degrees and no longer teach or conduct research.\n\nKey broad administrative responsibilities (and thus administrative units) in academic institutions include:\n\nThe chief executive, the administrative and educational head of a university, depending on tradition and location, may be termed the university president, the provost, the chancellor (the United States), the vice-chancellor (many Commonwealth countries), principal (Scotland and Canada), or rector (Europe, Russia, Asia and the Middle East).\n\nAn administrative executive in charge of a university department or of some schools, may be termed a dean or some variation, such as dean emeritus. The chief executive of academic establishments other than universities, may be termed headmaster or head teacher (schools), director (used to reflect various positions ranging from the head of an institution to the head of a program), or principal, as used in primary education.\n\nAcademic administrations are structured in various ways at different institutions and in different countries.\n\nFull-time tertiary education administrators emerged as a distinct role in Australia from the mid-1970s, as institutions sought to deal with their increasing size and complexity, along with a broadening of their aspirations. As the professionalism of tertiary administrators has developed, there has been a corresponding push to recognise the uniqueness and validity of their role in the academic environment.\n\n, general staff comprised over half the employees at Australian universities. Around 65% of these are female. There has recently been a shift in the preferred nomenclature for non-academic staff at Australian universities, from \"general staff\" to \"professional staff\". It has been argued that the changing in role of the professional staff has been due to the changing work that they are performing, as professional staff assist students with technology.\n\nThe overarching body for all staff working in administration and management in Australia is the Association for Tertiary Education Management.\n\nThe structures for administration and management in higher education in the United Kingdom vary significantly between institutions. Any description of a general structure will therefore not apply to some or even many institutions, and therefore any general statement of structures may be misleading. Not all UK universities have the post of Registrar.\n\nThe Director of Finance may report to the Registrar or directly to the Vice-Chancellor, whilst other senior posts may or may not report to the Registrar. This next tier of senior positions might include Directors of Human Resources, Estates, and Corporate Affairs. The Academic Registrar is often included in this next tier. Their role is mostly to accomplish student-facing administrative processes such as admissions, student records, complaints, and graduation.\n\nThe overarching body for all staff working in administration and management in the UK is the Association of University Administrators.\n\nIn the United States, a college or university is typically supervised by a President or Chancellor who reports regularly to a Board of Trustees (made up of individuals from outside the institution) and who serves as Chief Executive Officer. Most large colleges and universities now use an administrative structure with a tier of vice presidents, among whom the Provost (or Vice President for Academic Affairs, or Academic Dean) serves as the chief academic officer.\n\nDeans may supervise various and more specific aspects of the institution, or may be CEOs of entire campuses. They may report directly to the president or chancellor. The division of responsibility among deans varies widely among institutions; some are chiefly responsible for clusters of academic fields (such as the humanities or natural sciences) or whole academic units (such as a graduate school or college), while others are responsible for non-academic but campus-wide concerns such as minority affairs. In some cases a provost supervises the institution's entire academic staff, occupying a position generally superior to any dean. In other instances the Dean of a College may be the equivalent to a Provost or Vice Chancellor or Vice President for Academic Affairs. Below deans in the administrative hierarchy are heads of individual academic departments and of individual administrative departments from groundskeeping to libraries to registrars of records. These heads (commonly styled \"chairs\" or \"directors\") then supervise the faculty and staff of their individual departments.\n\nThe Chair of a department is typically a tenured or at least tenure-track faculty member, supported by administrative staff.\n\nAll levels of the university have administrative staff, reporting to Chancellors, Presidents, Provosts, Deans, and Departmental Chairs. Each department is typically led by a Management Services Officer (MSO), the departmental head of administrative staff, who reports to the Chair of the department. Front Desk staff, Admissions staff, Graduate and Undergraduate Student Counselors, and other staff report to the MSO.\n\n\n"}
{"id": "22055207", "url": "https://en.wikipedia.org/wiki?curid=22055207", "title": "Anion exchange membrane", "text": "Anion exchange membrane\n\nAn anion exchange membrane (AEM) is a semipermeable membrane generally made from ionomers and designed to conduct anions while being impermeable to gases such as oxygen or hydrogen.\n\nAnion exchange membranes are used in electrolytic cells and fuel cells to separate reactants present around the two electrodes while transporting the anions essential for the cell operation. An important example is the hydroxide anion exchange membrane used to separate the electrodes of a direct methanol fuel cell (DMFC) or direct-ethanol fuel cell (DEFC).\n\n"}
{"id": "14529537", "url": "https://en.wikipedia.org/wiki?curid=14529537", "title": "Arecibo Catena", "text": "Arecibo Catena\n\nArecibo Catena (Arecibo Vallis until March 2013) is a catena on Mercury. It is located at latitude 27.5 S, longitude 28.4 W, in the hilly and chaotic terrain antipodal to Caloris Basin. It is named after Arecibo Observatory.\n\nArecibo Vallis is connected to Petrarch crater.\n"}
{"id": "31950201", "url": "https://en.wikipedia.org/wiki?curid=31950201", "title": "Biomedical sciences", "text": "Biomedical sciences\n\nBiomedical sciences are a set of applied sciences applying portions of natural science or formal science, or both, to knowledge, interventions, or technology that are of use in healthcare or public health. Such disciplines as medical microbiology, clinical virology, clinical epidemiology, genetic epidemiology, and biomedical engineering are medical sciences. In explaining physiological mechanisms operating in pathological processes, however, pathophysiology can be regarded as basic science.\n\nThere are at least 45 different specialisms within healthcare science, which are traditionally grouped into three main divisions:\n\nThe healthcare science workforce is an important part of the UK's National Health Service. While people working in healthcare science are only 5% of the staff of the NHS, 80% of all diagnoses can be attributed to their work.\n\nThe volume of specialist healthcare science work is a significant part of the work of the NHS. Every year, NHS healthcare scientists carry out:\n\nThe four governments of the UK have recognised the importance of healthcare science to the NHS, introducing the Modernising Scientific Careers initiative to make certain that the education and training for healthcare scientists ensures there is the flexibility to meet patient needs while keeping up to date with scientific developments.\n\n"}
{"id": "44152035", "url": "https://en.wikipedia.org/wiki?curid=44152035", "title": "Central meridian (planets)", "text": "Central meridian (planets)\n\nThe central meridian of a celestial body that presents a disc to an observer (such as planet, moon, or star) is the meridian on the body's surface that goes through the centre of the body's disc as seen from the point of view of the observer.\n\nThe term as generally used in observational astronomy refers to the central meridian of the celestial body as seen by a theoretical observer on Earth for whom the celestial body is at the zenith. An imaginary line is drawn from the centre of the Earth to the center of the other celestial body. The intersection between this line and the celestial body's surface is the sub-Earth point. The central meridian is the meridian going through the sub-Earth point.\n\nBecause of the body's rotation and orbital alignment with the observer the central meridian changes with time, as it is based on the observer's point of view. For example, consider the Earth as seen from the Moon. There will be a meridian going through the centre of the Earth's visible disc (for example 75° West). This is not always the Earth's prime meridian (0° W / 0° E), as the central meridian of the Earth as seen from the Moon changes as the Earth rotates.\n"}
{"id": "3789887", "url": "https://en.wikipedia.org/wiki?curid=3789887", "title": "Cosmos (Carl Sagan book)", "text": "Cosmos (Carl Sagan book)\n\nCosmos is a 1980 popular science book by astronomer and Pulitzer Prize-winning author Carl Sagan. Its 13 illustrated chapters, corresponding to the 13 episodes of the , which the book was co-developed with and intended to complement, explore the mutual development of science and civilization. One of Sagan's main purposes for the book and television series was to explain complex scientific ideas to anyone interested in learning. Sagan also believed the television was one of the greatest teaching tools ever invented, so he wished to capitalize on his chance to educate the world. Spurred in part by the popularity of the TV series, \"Cosmos\" spent 50 weeks on the \"Publishers Weekly\" best-sellers list and 70 weeks on the \"New York Times\" Best Seller list to become the best-selling science book ever published at the time. In 1981, it received the Hugo Award for Best Non-Fiction Book. The book's unprecedented success ushered in a dramatic increase in visibility for science-themed literature. The success of the book also jumpstarted Sagan's literary career. The sequel to \"Cosmos\" is \"Pale Blue Dot: A Vision of the Human Future in Space\" (1994).\n\nIn 2013, \"Cosmos\" was published in a new edition, with a foreword by Ann Druyan and an essay by Neil deGrasse Tyson.\n\n\"Cosmos\" has 13 chapters, corresponding to the 13 episodes of the . In the original edition, each chapter is heavily illustrated. The book covers a broad range of topics, comprising Sagan's reflections on anthropological, cosmological, biological, historical, and astronomical matters from antiquity to contemporary times. Sagan reiterates his position on extraterrestrial life—that the magnitude of the universe permits the existence of thousands of alien civilizations, but no credible evidence exists to demonstrate that such life has ever visited earth. Sagan explores 15 billion years of cosmic evolution and the development of science and civilization. He traces the origins of knowledge and the scientific method, mixing science and philosophy, and speculates about the future of science. He also discusses the underlying premises of science by providing biographical anecdotes about many prominent scientists, placing their contributions in the broader context of the development of modern science.\n\nThe book, like the television series, contains a number of Cold War undertones including subtle references to self-destruction and the futility of the arms race.\n\n\"Cosmos\" utilizes a light, conversational tone to render complex scientific topics readable for a lay audience. On many topics, the book encompasses a more concise, refined presentation of previous ideas about which Sagan had written. and was the first science book to sell more than half a million copies. Though spurred in part by the popularity of the television series, \"Cosmos\" became a best-seller by its own regard, reaching hundreds of thousands of readers. It was only surpassed in the late 1980s by Stephen Hawking's \"A Brief History of Time\" (1988). \"Cosmos\" spent 50 weeks on the \"Publishers Weekly\" best-seller's list, and 70 weeks on the \"New York Times\" Best Seller list. \"Cosmos\" sold over 900,000 copies while on these lists, and continued popularity has allowed \"Cosmos\" to sell about five million copies internationally. Shortly after \"Cosmos\" was published, Sagan received a $2 million advance for the novel \"Contact\". This was the largest release given for an unwritten fiction book at the time. The success of \"Cosmos\" made Sagan \"wealthy as well as famous.\" It also ushered in a dramatic increase in visibility for science books, opening up new options and readership for the previously fledgling genre. Science historian Bruce Lewenstein of Cornell University noted that among science books \"\"Cosmos\" marked the moment that something different was clearly going on.\"\n\nAfter the success of \"Cosmos\", Sagan turned into an early scientific celebrity. He appeared on many television programs, wrote a regular column for \"Parade\", and worked to continually advance the popularity of the science genre.\n\nLewenstein also noted the power of the book as a recruitment tool. Along with \"Microbe Hunters\" and \"The Double Helix\", he described \"Cosmos\" as one of the \"books that people cite as 'Hey, the reason I'm a scientist is because I read that book'.\" Particularly in astronomy and physics, he said, the book inspired many people to become scientists. Sagan has also been called the \"most successful popularizing scientist of our time,\" for his ability to draw such a large and varied audience.\n\nThe popularity of Sagan's \"Cosmos\" has been referenced in arguments supporting increased space exploration spending. Sagan's book was also referenced in Congress by Arthur C. Clarke in a speech promoting an end to Cold War anti-ICBM spending, instead arguing that the anti-ICBM budget would be better spent on Mars exploration.\n\nReception for Sagan's work was generally positive. In \"The New York Times Book Review\", novelist James Michener praised \"Cosmos\" as \"a cleverly written, imaginatively illustrated summary of [Sagan's]... ruminations about our universe... His style is iridescent, with lights flashing upon unexpected juxtapositions of thought.\" The American astrophysicist Neil deGrasse Tyson describes \"Cosmos\" as something \"more than Carl Sagan\". David Whitehouse of the British Broadcasting Corporation went so far as to say that \"there is not a book on astronomy – in fact not one on science – that comes close to the eloquence and intellectual sweep of Cosmos... If we send just one book to grace the libraries of distant worlds..., let it be \"Cosmos\".\" \"Kirkus Reviews\" described the book as \"Sagan at his best.\" \"Cornell News Service\" characterized it as \"an overview of how science and civilization grew up together.\" In 1981, \"Cosmos\" received the Hugo Award for Best Non-Fiction Book.\n\nThe U.S. Library of Congress designated \"Cosmos\" one of eighty-eight books \"that shaped America.\"\n\n"}
{"id": "445258", "url": "https://en.wikipedia.org/wiki?curid=445258", "title": "Draper Laboratory", "text": "Draper Laboratory\n\nDraper Laboratory is an American not-for-profit research and development organization, headquartered in Cambridge, Massachusetts; its official name is \"The Charles Stark Draper Laboratory, Inc\". The laboratory specializes in the design, development, and deployment of advanced technology solutions to problems in national security, space exploration, health care and energy.\n\nThe laboratory was founded in 1932 by Charles Stark Draper at the Massachusetts Institute of Technology (MIT) to develop aeronautical instrumentation, and came to be called the \"MIT Instrumentation Laboratory\". It was renamed for its founder in 1970 and separated from MIT in 1973 to become an independent, non-profit organization.\n\nThe expertise of the laboratory staff includes the areas of guidance, navigation, and control technologies and systems; fault-tolerant computing; advanced algorithms and software solutions; modeling and simulation; and microelectromechanical systems and multichip module technology.\n\nIn 1932 Charles Stark Draper, an MIT aeronautics professor, created a teaching laboratory to develop the instrumentation needed for tracking, controlling and navigating aircraft. During World War II, Draper’s lab was known as the “Confidential Instrument Development Laboratory”. Later, the name was changed to the MIT Instrumentation Laboratory. The laboratory was renamed for its founder in 1970 and remained a part of MIT until 1973 when it became an independent, not-for-profit research and development corporation. The transition to an independent corporation arose out of pressures for divestment of MIT laboratories doing military research at the time of the Vietnam War, despite the absence of a role of the laboratory in that war.\n\nA primary focus of the laboratory's programs throughout its history has been the development and early application of advanced guidance, navigation, and control (GN&C) technologies to meet the U.S. Department of Defense’s and NASA’s needs. The laboratory’s achievements includes the design and development of accurate and reliable guidance systems for undersea-launched ballistic missiles as well as the Apollo Guidance Computer that guided the Apollo astronauts to the Moon and back safely to Earth, every time. The laboratory contributed to the development of inertial sensors, software, and other systems for the GN&C of commercial and military aircraft, submarines, strategic and tactical missiles, spacecraft, and unmanned vehicles.\n\nInertial-based GN&C systems were central for navigating ballistic missile submarines for long periods of time undersea to avoid detection and guiding their submarine-launched ballistic missiles to their targets, starting with the UGM-27 Polaris missile program.\n\nDraper has locations in several U.S. cities:\n\n\n\nAccording to its website, the laboratory staff applies its expertise to autonomous air, land, sea and space systems; information integration; distributed sensors and networks; precision-guided munitions; biomedical engineering; chemical/biological defense; and energy system modeling and management. When appropriate, Draper works with partners to transition their technology to commercial production.\n\nThe laboratory encompasses seven areas of technical expertise:\n\nProject areas that have surfaced in the news referred to Draper Laboratory's core expertise in inertial navigation, as recently as 2003. More recently, emphasis has shifted to research in innovative space navigation topics, intelligent systems that rely on sensors and computers to make autonomous decisions, and nano-scale medical devices.\n\nThe laboratory staff has studied ways to integrate input from Global Positioning Systems (GPS) into Inertial navigation system-based navigation in order to lower costs and improve reliability. Military inertial navigation systems (INS) cannot totally rely on GPS satellite availability for course correction—required by error growth—owing to blocking or jamming of signal. A less accurate inertial system usually means a less costly system, but one that requires more frequent checking of position from another source, like GPS. Systems that integrate GPS with INS are classified as “loosely coupled” (pre-1995), “tightly coupled” (1996-2002), or \"deeply integrated\" (2002 onwards), depending on the degree of integration of the hardware. As of 2006, it was envisioned that many military and civilian uses would integrate GPS with INS, including the possibility of shells with a deeply integrated system that can withstand 20,000 g, when fired from an artillery piece.\n\nIn 2010 Draper Laboratory and MIT collaborated with two other partners as part of the Next Giant Leap team to win a grant towards achieving the Google Lunar X Prize send the first privately funded robot to the Moon. To qualify for the prize, the robot must travel 500 meters across the lunar surface and transmit video, images and other data back to Earth. A team developed a \"Terrestrial Artificial Lunar and Reduced Gravity Simulator\" to simulate operations in the space environment, using Draper Laboratory's guidance, navigation and control algorithm for reduced gravity.\n\nIn 2012, Draper laboratory engineers in Houston, Texas developed a new method for turning the International Space Station, called the “optimal propellant maneuver”, which achieved a 94 percent savings over previous practice. The algorithm takes into account everything that affects how the station moves, including \"the position of its thrusters and the effects of gravity and gyroscopic torque\".\n\nAt a personal scale Draper, as of 2013, was developing a garment for use in orbit that uses Controlled Moment Gyros (CMGs) that creates resistance to movement of an astronaut's limbs to help mitigate bone loss and maintain muscle tone during prolonged space flight. The unit is called a Variable Vector Countermeasure suit, or V2Suit, which uses CMGs also to assist in balance and movement coordination by creating resistance to movement and an artificial sense of \"down\". Each CMG module is about the size of a deck of cards. The concept is for the garment to be worn \"in the lead-up to landing back on Earth or periodically throughout a long mission.\"\n\nIn 2013, a Draper/MIT/NASA team was also developing a CMG-augmented space suit that would expand the current capabilities of NASA's \"Simplified Aid for EVA Rescue\" (SAFER)—a space suit designed for \"propulsive self-rescue\" for when an astronaut accidentally becomes untethered from a spacecraft. The CMG-augmented suit would provide better counterforce than is now available for when astronauts use tools in low-gravity environments. Counterforce is available on earth from gravity. Without it an applied force would result in an equal force in the opposite direction, either in a straight line or spinning. In space this could send an astronaut out of control. Currently, astronauts must affix themselves to the surface being worked on. The CMGs would offer an alternative to mechanical connection or gravitational force.\n\nDraper researchers develop artificial intelligence systems to allow robotic devices to learn from their mistakes, This work is in support of DARPA-funded work, pertaining to the Army Future Combat System. This capability would allow an autonomous under fire to learn that that road is dangerous and find a safer route or to recognize that its fuel status and damage status. Paul DeBitetto, reportedly led the cognitive robotics group at the laboratory in this effort, as of 2008.\n\nAs of 2009, the US Department of Homeland Security funded Draper Laboratory and other collaborators to develop a technology to detect potential terrorists with cameras and other sensors that monitor behaviors of people being screened. The project is called “Future Attribute Screening Technology’’ or FAST. The application would be for security checkpoints to assess candidates for follow-up screening. In a demonstration of the technology, the project manager Robert P. Burns explained that the system is designed to distinguish between malicious intent and benign expressions of distress by employing a substantial body research into the psychology of deception.\n\nAs of 2010 Neil Adams, a director of tactical systems programs for Draper Laboratory, led the systems integration of Defense Advanced Research Projects Agency's (DARPA) Nano Aerial Vehicle (NAV) program to miniaturize flying reconnaissance platforms. This entails managing the vehicle, communications and ground control systems allow NAVs to function autonomously to carry a sensor payload to achieve the intended mission. The NAVS must work in urban areas with little or no GPS signal availability, relying on vision-based sensors and systems.\n\nIn 2009, Draper collaborated with the Massachusetts Eye and Ear Infirmary to develop an implantable drug-delivery device, which \"merges aspects of microelectromechanical systems, or MEMS, with microfluidics, which enables the precise control of fluids on very small scales.\" The device is a \"flexible, fluid-filled machine\", which uses tubes that expand and contract to promote fluid flow through channels with a defined rhythm, driven by a micro-scale pump, which adapts to environmental input. The system, funded by the National Institutes of Health, may treat hearing loss by delivering \"tiny amounts of a liquid drug to a very delicate region of the ear, the implant will allow sensory cells to regrow, ultimately restoring the patient's hearing\".\n\nAs of 2010, Heather Clark of Draper Laboratory was developing a method to measure blood glucose concentration without finger-pricking. The method uses a nano-sensor, like a miniature tattoo, just several millimeters across, that patients apply to the skin. The sensor uses near-infrared or visible light ranges to determine glucose concentrations. Normally to regulate their blood glucose levels, diabetics must measure their blood glucose at several times a day by taking a drop of blood obtained by a pinprick and inserting the sample into a machine that can measure glucose level. The nano-sensor approach would supplant this process.\n\nLaboratory staff worked in teams to create novel navigation systems, based on inertial guidance and on digital computers to support the necessary calculations for determining spatial positioning.\n\nDraper Laboratory applies some of its resources to developing and recognizing technical talent through educational programs and accomplishments through the Draper Prize.\n\nThe research-based Draper Fellow Program sponsors about 50 graduate students each year. Students are trained to fill leadership positions in the government, military, industry, and education. The laboratory also supports on-campus funded research with faculty and principal investigators through the University R&D program. It offers undergraduate student employment and internship opportunities.\n\nDraper Laboratory conducts a STEM (Science, Technology, Engineering, and Mathematics) K-12 and community education outreach program, which it established in 1984. Each year, laboratory distributes more than $175,000 through its community relations programs. These funds include support of internships, co-ops, participation in science festivals and the provision of tours and speakers-is an extension of this mission.\n\nThe company endows the Charles Stark Draper Prize, which is administered by the National Academy of Engineering. It is awarded \"to recognize innovative engineering achievements and their reduction to practice in ways that have led to important benefits and significant improvement in the well-being and freedom of humanity.\" Achievements in any engineering discipline are eligible for the $500,000 prize.\n\n\n"}
{"id": "1754042", "url": "https://en.wikipedia.org/wiki?curid=1754042", "title": "Dresden Codex", "text": "Dresden Codex\n\nThe Dresden Codex is a Mayan book, the oldest surviving from the Americas, dating to the 13th or 14th century. The codex was rediscovered in the city of Dresden, Germany, hence the book's present name. It is located in the museum of the Saxon State Library.\n\nThe book suffered serious water damage during World War II. The pages are made of Amate, high, and can be folded accordion-style; when unfolded the codex is long. It is written in Mayan hieroglyphs and refers to an original text of some three or four hundred years earlier, describing local history and astronomical tables.\n\nThe \"Dresden Codex\" contains 78 pages with decorative board covers on the front and back. Most pages have writing on both sides. They have a border of red paint, although many have lost this framing due to age deterioration. The pages are generally divided into three sections; students of the codex have arbitrary labeled these sections \" a \", \"b\" , and \" c \". Some pages have just two horizontal sections, while one has four and another five sections. The individual sections with their own theme are generally separated by a red vertical line. Sections are generally divided into two to four columns.\n\nThe \"Dresden Codex\" is one of four hieroglyphic Maya codices that survived the Spanish Inquisition in the New World. Three, the Dresden, Madrid, and Paris codices, are named after the city where they were ultimately rediscovered. The fourth is the \"Grolier Codex\", located at the Grolier Club in New York City. The \"Dresden Codex\" is held by the Saxon State and University Library Dresden (SLUB Dresden, Saxon State Library) in Dresden, Germany. The Maya codices all have about the same size pages, with a height of about and a width of .\n\nThe pictures and glyphs were painted by skilled craftsmen using thin brushes and vegetable dyes. Black and red were the main colors used for many of the pages. Some pages have detailed backgrounds in shades of yellow, green, and the Mayan blue. The codex was written by eight different scribes, who all had their own writing style, glyph designs, and subject matter.\n\nThe \"Dresden Codex\" is described by historian J. Eric S. Thompson as writings of the indigenous people of the Yucatán Peninsula in southeastern Mexico. Maya historians Peter J. Schmidt, Mercedes de la Garza, and Enrique Nalda confirm this. Thompson further narrows the probable origin of the \"Dresden Codex\" to the area of Chichen Itza, because certain picture symbols in the codex are only found on monuments in that location. He also argues that the astronomical tables would support this as the place of origin. Thompson claims that the people of the Yucatán Peninsula were known to have done such studies around 1200 A.D. Thompson also notes the similar ceramic designs in the Chichen Itza area which are known to have ceased in the early thirteenth century. British historian Clive Ruggles suggests, based on the analyses of several scholars, that the \"Dresden Codex\" is a copy and was originally written between the twelfth and fourteenth centuries. Thompson narrows the date closer to 1200 to 1250. Maya archaeologist Linton Satterthwaite puts the date when it was made as no later than 1345. It is the oldest surviving book from the Americas.\n\nJohann Christian Götze (1692–1749), German theologian and director of the Royal Library at Dresden, purchased the codex from a private owner in Vienna in 1739 while traveling to Italy. Thompson speculates that the codex was sent as a tribute to Charles V, Holy Roman Emperor by Hernán Cortés, governor of Mexico, since examples of local writings and other Maya items were sent to the king in 1519 when he was living in Vienna.\nAlexander von Humboldt published pages 47–52 from the \"Dresden Codex\" in his 1811 atlas \"Vues des Cordillères et Monuments des Peuples Indigènes de l'Amérique\", the first reproduction of any of its pages. The first copy of the codex was published by Lord Kingsborough in his 1831 \"Antiquities of Mexico\". In 1828 Constantine Samuel Rafinesque had identified this book as being of Maya origin based on its glyphs looking like those found at Palenque. Historian Cyrus Thomas made a connection between the codex and the 260 year cycle (\"Ahau Katun\") of the Maya calendar and the 365 days in a year. Ruggles shows that in the codex the Maya related their 260-day calendar to celestial bodies, especially Venus and Mars.\n\nThe codex has played a key role in the deciphering of Mayan hieroglyphs. Dresden librarian Ernst Wilhelm Förstemann published the first complete facsimile in 1880. He deciphered the calendar section of the codex, including the Maya numerals used therein. Förstemann determined that these numbers, along with deities and day names, related to the Mayan calendar and the Mayan Long Count calendar. In the 1950s Yuri Knorozov used a phonetic approach based on the De Landa alphabet for decoding the codex, which was followed up in the 1980s by other scholars that did additional deciphering based on this concept.\n\nPaul Schellhas in 1897 and 1904 assigned letters to gods for specific glyphs since they had several possible names. For example God D could be Hunab Ku Itzam Na among several other names and God A could be Cizin (god of death) among others. The Schellhas system of assigning letters for the gods represented by certain glyphs as a noncommittal system was adopted by later researchers of Maya codices.\n\nThe \"Dresden Codex\" contains accurate astronomical tables, which are recognized by students of the codex for its detailed Venus tables and lunar tables. The lunar series has intervals correlating with eclipses, while the Venus tables correlate with the movements of the planet Venus. The codex also contains astrological tables and ritual schedules. The religious references show in a cycle of a 260-day ritual calendar the important Maya royal events. The codex also includes information on the Maya new-year ceremony tradition. The rain god Chaac is represented 134 times.\n\nItalian artist and engraver Agostino Aglio, starting in 1826, became the first to transcribe and illustrate the codex completely for Lord Kingsborough, who published it in his nine volumes of \"Antiquities of Mexico\" in 1831–1848. The codex then had some damage due to handling, sunlight, and moisture. It received direct water damage that was significantly destructive from being kept in a flooded basement during the bombing of Dresden in World War II. German historian G. Zimmerman noted that the damage was extreme on pages 2, 4, 24, 28, 34, 38, 71 and 72. Certain details of the glyph images have been lost because of this. This shows when the current codex is compared to the Kingsborough copies of 1831-1848 and the Förstemann facsimile editions of 1880 and 1892.\n\nToday’s page numbers were assigned to the codex by Agostino Aglio when he became the first to transcribe the manuscript in 1825/26. For this, he divided the original codex into two parts, labeled \"Codex A\" and \"Codex B\". He sequenced \"Codex A\" on the front side followed by its back side, with the same order on \"Codex B\". Today, historians such as Helmut Deckert and Ferdinand Anders understand that a codex reading should traverse the complete front side followed by the complete back side of the manuscript, i.e., pages 1–24 followed by 46-74 and 25-45. The librarian K. C. Falkenstein adjusted the relative position of pages for “esthetical reasons” in 1836, resulting in today’s two similar length parts. While deciphering the codex, the librarian E. W. Förstemann noticed an error in Aglio’s page assignment of the sheets 1/45 and 2/44, so he correctly reassigned Aglio’s pages 44 and 45 to become pages 1 and 2. The reversal of the sheets 6/40, 7/39 and 8/38 is due to an error when the sheets were returned to their protective glass cabinet after drying from the water damage due to the bombing of Dresden in 1945.\n\n\n\n"}
{"id": "25960701", "url": "https://en.wikipedia.org/wiki?curid=25960701", "title": "ESCAPPM", "text": "ESCAPPM\n\nESCAPPM or ESCHAPPM is a mnemonic for the organisms with inducible beta-lactamase activity that is chromosomally mediated.\n\n\n\"In vitro\" sensitivities are not applicable \"in vivo\".\n\nIn general, treatment with cephalosporins results in inducible beta-lactamase activity. Treatment with an aminoglycoside or carbapenem is usually indicated. Carbapenems are a class of beta-lactam antibiotics with a broad spectrum of antibacterial activity. They have a structure that renders them highly resistant to beta-lactamases. Examples of carbapenems include meropenem and imipenem.\n"}
{"id": "6669390", "url": "https://en.wikipedia.org/wiki?curid=6669390", "title": "Encounters with the Archdruid", "text": "Encounters with the Archdruid\n\nEncounters with the Archdruid (1971) is a narrative nonfiction book by author John McPhee. \"Encounters\" is split into three parts, each covering environmentalist David Brower's confrontations with his ideological enemies. The book chronicles his struggles against miners, developers and finally the United States Bureau of Reclamation. McPhee blurs traditional journalism—the reporting of facts and accounting of events, with thematic elements more common to fiction. The book was generally well received in the popular press and became an enduring part of the portrait of David Brower.\n\nWhile notionally a profile of Brower, \"Encounters\" is broken into three sections. The first chronicles Brower's conflict with Charles Park, a mineral engineer hoping to find and exploit mineral reserves in Glacier Peak Wilderness. Charles Park is portrayed as calculating and pragmatic, unwilling to foreclose real economic value from current generations in order to leave the environment pristine for future generations. This pragmatic view was starkly contrasted with Brower's insistence that \"I believe in wilderness for itself alone\". McPhee facilitates or observes the dialogue between these two contrasted figures as he does for the other two sections in the book.\n\nThe second section introduces Charles Fraser, a real estate developer in Hilton Head Island, South Carolina. Fraser's characterization of environmentalists as modern druids who \"worship trees and sacrifice human beings to those trees\" provides the charge against Brower that forms the title of the book. Brower came to Georgia in order to stop Fraser's plan to develop Cumberland Island. Like Park, Fraser is depicted as nuanced and pragmatic: his vision of development is controlled and regulated land use. Fraser's development of Hilton Head Island is still considered a model for planned development and McPhee notes that Fraser considers himself a true conservationist. Brower would eventually win this battle, with a groundswell of opposition forcing Fraser to sell his development on Cumberland Island to the National Park Foundation.\n\nThe third section presents David Brower's unraveling. Here Brower battles Floyd Dominy, then the commissioner of the United States Bureau of Reclamation. Displaying only some of the reserve and pragmatism of the previous two figures, Dominy relished the damming of rivers, while Brower considered damming the ultimate offense. Brower struggled to save the Glen Canyon from being flooded by the Glen Canyon Dam but failed and as the story progresses, he is increasingly marginalized in the environmental movement for his perceived militancy. Wendy Nelson Espeland, in \"The Struggle for Water\", argues that the Bureau carries much of the blame (or credit) for \"radicalizing\" Brower.\n\nMcPhee's catalog of these conflicts between the growing needs of society and the shrinking wilderness \"pre-saged\" what would become known as \"wise use\", or prescriptions for use that balance the existential value of the environment against societal needs. \"Encounters\" was positively received in both \"The New York Times\" and \"The Wall Street Journal\". \"Time\" gave a positive review as well, noting the narrative similarities to a novel. The term \"archdruid\" stuck with Brower and was used appreciatively in many of his obituaries, including one McPhee wrote for him, when he died in 2000.\n"}
{"id": "39727186", "url": "https://en.wikipedia.org/wiki?curid=39727186", "title": "Epistemology of Wikipedia", "text": "Epistemology of Wikipedia\n\nEpistemology is a major branch of philosophy and is concerned with the nature and scope of knowledge. The epistemology of Wikipedia has been a subject of interest from the earliest days of its existence.\n\nEarly analysis related the epistemology of Wikipedia to social epistemology. Other realms of epistemological research; epistemology of testimony, and epistemic value theory have been addressed with reference to Wikipedia.\n\nMore recent analysis suggests that the epistemology of Wikipedia derives from the combined epistemic values of wikis and of encyclopedias. Jankowski\ncites Ruth and Houghton\nwho define the epistemic values of wikis as:\n\nJankowski suggests that determination of the epistemic values of encyclopedias is more problematic requiring genre analysis. This analysis revealed that encyclopedias value:\n\nFallis previously identified the specific epistemic virtues of Wikipedia as\n"}
{"id": "10652407", "url": "https://en.wikipedia.org/wiki?curid=10652407", "title": "Family Tree DNA", "text": "Family Tree DNA\n\nFamily Tree DNA is a division of Gene by Gene, a commercial genetic testing company based in Houston, Texas. Family Tree DNA offers analysis of autosomal DNA, Y-DNA, and mitochondrial DNA to individuals for genealogical purpose. It is the most popular company worldwide for Y-DNA and mitochondrial DNA, and the third most popular for autosomal DNA. In Europe, it is the most common also for autosomal DNA.\n\nFamily Tree DNA was founded based on an idea conceived by Bennett Greenspan, a lifelong entrepreneur and genealogy enthusiast. In 1999, Greenspan had entered semi-retirement and was working on his family history. He began work on his mother's Nitz lineage. When faced with a roadblock in his work, he remembered two cases of genetics being used to prove ancestry that had recently been covered by the media. These were a study by University of Arizona researchers showing that many Cohen men from both Ashkenazic and Sephardic groups share the same Y-chromosome and a study that showed that male descendants of US President Thomas Jefferson and male descendants of his freed slave Sally Hemings shared the same Y-chromosome and a recent common ancestry.\n\nGreenspan had both Nitz cousins in California and had discovered someone in Argentina with the same ancestral surname and the same ancestral location in Eastern Europe. Wishing to use the same method of DNA comparison for his own genealogy, he contacted Dr. Michael Hammer at the University of Arizona. Greenspan discovered that academic labs did not offer testing directly to the public and that in general direct to consumer testing for genealogy was not commercially available either. Their conversation inspired him to start a company dedicated to using genetics to solve genealogy problems.\n\nIt was early 2000 when Greenspan with his business partners Max Blankfeld and Jim Warren officially launched Family Tree DNA. Initially, the Arizona Research Labs at the University of Arizona performed all testing for Family Tree DNA. Family Tree DNA includes among its scientific staff, Dr. Michael Hammer (PhD), one of a team of scientists that first published on the Cohen Modal Haplotype in 1997 in the journal Nature.\n\nFamily Tree DNA began with a proof in concept group of twenty-four that returned results in January. They began by offering 12 Y-chromosome STR marker tests much like those used in many scientific publications of the time in March 2000. Family Tree DNA became widely known for its Y-chromosome STR testing for the Cohen Modal Haplotype.\n\nSoon, they were offering not only DNA tests but an interface by which dedicated genealogists could run surname research studies. The first person to create such a project through the Family Tree DNA site was Doug Mumma. It was the Mumma project.\n\nThe first tests offered by Family Tree DNA were Y-chromosome STR and mitochondrial DNA (mtDNA) tests like those used by published academic studies at the time.\n\nFamily Tree DNA's initial Y-chromosome tests were described as 11 marker tests. They eventually began to call this a 12 STR marker test as one of the STRS (DYS385) almost always had two copies. This they billed as a method to affirm or disprove a genealogical connection on the direct paternal line.\n\nFamily Tree DNA's first mtDNA tests were for HVR1 (hypervariable region 1) of the mtDNA. Eventually, they added a Plus test that tested for both HVR1 and HVR2.\n\nIn the early days, they did not confirm haplogroups for either mtDNA or Y-DNA.\n\nIn 2006, Family Tree DNA bought out the assets of DNA-Fingerprint, which was a German paternity testing company that offered à la carte testing to the genetic genealogy testing community. With this buyout, Thomas and Astrid Krahn, who had owned DNA-Fingerprint, moved to Houston, Texas, and helped open the Genomics Research Center.\n\nThe Genomics Research Center initially did testing for many of the same products that had been sold by DNA-Fingerprint. They began to offer individual and panels of Y-chromosome SNP tests using Sanger testing methods. They also offered the mtDNA full genome test and upgrades to it using the Sanger testing method.\n\nSoon came the launch of the \"Walk Through the Y\" (WTY) test. The WTY test offered the most adventurous of citizen scientists the chance to seek the discovery of new Y-chromosome SNPs.\n\nMeanwhile, most testing continued to be done at the University of Arizona lab. The demand for additional test types led Greenspan and Blankfeld to move all testing to their own testing lab in Houston, Texas under the Genealogy by Genetics, Ltd. parent company.\n\nBetween 2007 and 2010, Family Tree DNA forged several new partnerships that allowed them to access additional international markets.\n\niGENEA\n\nThe first of Family Tree DNA's new partnerships was with the Switzerland-based iGENEA company. It was formed alongside the closing of DNA-Fingerprint and Thomas Krahn's helping open the Genomic Research Center in Houston. Their website is available in English, French, German, Italian, and Spanish.\n\nAfrican DNA\n\nIn late 2007, Henry Louis Gates, PhD created African DNA in partnership with Family Tree DNA to help promote genetic testing for personal ancestry among African Americans.\n\nDNA Ancestry & Family Origin\n\nDNA Ancestry & Family Origin DNA Ancestry & Family Origin is a genetic genealogy testing partnership between Family Tree DNA and Eastern Biotech & Life Sciences. Their website is available in both English and Arabic.\n\nMyHeritage\n\nIn November 2008, a dynamic partnership with MyHeritage was launched, allowing users to incorporate DNA testing and advanced family tree technologies into their family history research. MyHeritage is a website offering online, mobile and software platforms for discovering, preserving and sharing family history worldwide.\n\nIn May 2010, Family Tree DNA launched an autosomal microarray chip based DNA test. They called the new product Family Finder. The initial product used an Affymetrix microarray chip, but Family Tree DNA changed to the Illumina OmniExpress chip and retested all customers who had results from the Affymetrix chip for forward compatibility.\n\nFamily Finder allows customers to match relatives as distant as about fifth cousins. Family Finder also includes a component called myOrigins. The results of this test provide percentages of a DNA associated with general regions or specific ethnic groups (e.g. Western Europe, Asia, Jewish, Native American, etc.). Notably, unlike other testing companies, they chose to strip out markers for mendelian medical issues, mtDNA results, and Y-DNA SNP results.\n\nThe company markets a range of Y-DNA tests. the Y-chromosome is inherited from father to son, so testers can discover relatives with the same patrilineage. In many cultures these relatives will often share the same surname, since surnames are also inherited father to son. These tests cover 37-111 STR markers depending on the test, and vary in price according to the number of markers covered. Once an individual has tested at least 12 STR markers, he may take the \"Big Y\" test. Big Y tests approximately 20,000 SNPs.\n\nFamily Tree DNA also sells mtDNA testing, offering the choice of either a limited hypervariable region test, or a full sequence test of the entire mitochondrial DNA chromosome.\n\nFamily Tree DNA staff were instrumental in developing the Geno 2.0 Next Generation product for the second phase of the Genographic Project. Geno 2.0 samples for both public and scientific study were run at the Genomics Research Center in Houston, Texas (operated by Family Tree DNA's parent company, Gene by Gene, Ltd.) until 2016, when Geno 2.0 began utilizing Helix for DNA sequencing.\n\nIn September 2012, Greenspan and Blankfeld restructured Family Tree DNA's parent company, Genealogy by Genetics, Ltd. This included their renaming Genealogy by Genetics, Ltd. as Gene by Gene, Ltd. After restructuring, the business comprises four divisions one of these being Family Tree DNA for genealogical DNA tests. It is Gene by Gene, Ltd. that now operates the Genomics Research Center (GRC) lab in Houston, Texas.\n\n"}
{"id": "10902", "url": "https://en.wikipedia.org/wiki?curid=10902", "title": "Force", "text": "Force\n\nIn physics, a force is any interaction that, when unopposed, will change the motion of an object. A force can cause an object with mass to change its velocity (which includes to begin moving from a state of rest), i.e., to accelerate. Force can also be described intuitively as a push or a pull. A force has both magnitude and direction, making it a vector quantity. It is measured in the SI unit of newtons and represented by the symbol F.\n\nThe original form of Newton's second law states that the net force acting upon an object is equal to the rate at which its momentum changes with time. If the mass of the object is constant, this law implies that the acceleration of an object is directly proportional to the net force acting on the object, is in the direction of the net force, and is inversely proportional to the mass of the object.\n\nConcepts related to force include: thrust, which increases the velocity of an object; drag, which decreases the velocity of an object; and torque, which produces changes in rotational speed of an object. In an extended body, each part usually applies forces on the adjacent parts; the distribution of such forces through the body is the internal mechanical stress. Such internal mechanical stresses cause no acceleration of that body as the forces balance one another. Pressure, the distribution of many small forces applied over an area of a body, is a simple type of stress that if unbalanced can cause the body to accelerate. Stress usually causes deformation of solid materials, or flow in fluids.\n\nPhilosophers in antiquity used the concept of force in the study of stationary and moving objects and simple machines, but thinkers such as Aristotle and Archimedes retained fundamental errors in understanding force. In part this was due to an incomplete understanding of the sometimes non-obvious force of friction, and a consequently inadequate view of the nature of natural motion. A fundamental error was the belief that a force is required to maintain motion, even at a constant velocity. Most of the previous misunderstandings about motion and force were eventually corrected by Galileo Galilei and Sir Isaac Newton. With his mathematical insight, Sir Isaac Newton formulated laws of motion that were not improved for nearly three hundred years. By the early 20th century, Einstein developed a theory of relativity that correctly predicted the action of forces on objects with increasing momenta near the speed of light, and also provided insight into the forces produced by gravitation and inertia.\n\nWith modern insights into quantum mechanics and technology that can accelerate particles close to the speed of light, particle physics has devised a Standard Model to describe forces between particles smaller than atoms. The Standard Model predicts that exchanged particles called gauge bosons are the fundamental means by which forces are emitted and absorbed. Only four main interactions are known: in order of decreasing strength, they are: strong, electromagnetic, weak, and gravitational. High-energy particle physics observations made during the 1970s and 1980s confirmed that the weak and electromagnetic forces are expressions of a more fundamental electroweak interaction.\n\nSince antiquity the concept of force has been recognized as integral to the functioning of each of the simple machines. The mechanical advantage given by a simple machine allowed for less force to be used in exchange for that force acting over a greater distance for the same amount of work. Analysis of the characteristics of forces ultimately culminated in the work of Archimedes who was especially famous for formulating a treatment of buoyant forces inherent in fluids.\n\nAristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different \"natural places\" therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their \"natural place\" (e.g., for heavy bodies to fall), which led to \"natural motion\", and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general.\n\nAristotelian physics began facing criticism in medieval science, first by John Philoponus in the 6th century.\n\nThe shortcomings of Aristotelian physics would not be fully corrected until the 17th century work of Galileo Galilei, who was influenced by the late medieval idea that objects in forced motion carried an innate force of impetus. Galileo constructed an experiment in which stones and cannonballs were both rolled down an incline to disprove the Aristotelian theory of motion. He showed that the bodies were accelerated by gravity to an extent that was independent of their mass and argued that objects retain their velocity unless acted on by a force, for example friction.\n\nSir Isaac Newton described the motion of all objects using the concepts of inertia and force, and in doing so he found they obey certain conservation laws. In 1687, Newton published his thesis \"Philosophiæ Naturalis Principia Mathematica\". In this work Newton set out three laws of motion that to this day are the way forces are described in physics.\n\nNewton's First Law of Motion states that objects continue to move in a state of constant velocity unless acted upon by an external net force (resultant force). This law is an extension of Galileo's insight that constant velocity was associated with a lack of net force (see a more detailed description of this below). Newton proposed that every object with mass has an innate inertia that functions as the fundamental equilibrium \"natural state\" in place of the Aristotelian idea of the \"natural state of rest\". That is, Newton's empirical First Law contradicts the intuitive Aristotelian belief that a net force is required to keep an object moving with constant velocity. By making \"rest\" physically indistinguishable from \"non-zero constant velocity\", Newton's First Law directly connects inertia with the concept of relative velocities. Specifically, in systems where objects are moving with different velocities, it is impossible to determine which object is \"in motion\" and which object is \"at rest\". The laws of physics are the same in every inertial frame of reference, that is, in all frames related by a Galilean transformation.\n\nFor instance, while traveling in a moving vehicle at a constant velocity, the laws of physics do not change as a result of its motion. If a person riding within the vehicle throws a ball straight up, that person will observe it rise vertically and fall vertically and not have to apply a force in the direction the vehicle is moving. Another person, observing the moving vehicle pass by, would observe the ball follow a curving parabolic path in the same direction as the motion of the vehicle. It is the inertia of the ball associated with its constant velocity in the direction of the vehicle's motion that ensures the ball continues to move forward even as it is thrown up and falls back down. From the perspective of the person in the car, the vehicle and everything inside of it is at rest: It is the outside world that is moving with a constant speed in the opposite direction of the vehicle. Since there is no experiment that can distinguish whether it is the vehicle that is at rest or the outside world that is at rest, the two situations are considered to be physically indistinguishable. Inertia therefore applies equally well to constant velocity motion as it does to rest.\n\nA modern statement of Newton's Second Law is a vector equation:\nwhere formula_2 is the momentum of the system, and formula_3 is the net (vector sum) force. If a body is in equilibrium, there is zero \"net\" force by definition (balanced forces may be present nevertheless). In contrast, the second law states that if there is an \"unbalanced\" force acting on an object it will result in the object's momentum changing over time.\n\nBy the definition of momentum,\nwhere \"m\" is the mass and formula_5 is the velocity.\n\nIf Newton's second law is applied to a system of constant mass, \"m\" may be moved outside the derivative operator. The equation then becomes\nBy substituting the definition of acceleration, the algebraic version of Newton's Second Law is derived:\nNewton never explicitly stated the formula in the reduced form above.\n\nNewton's Second Law asserts the direct proportionality of acceleration to force and the inverse proportionality of acceleration to mass. Accelerations can be defined through kinematic measurements. However, while kinematics are well-described through reference frame analysis in advanced physics, there are still deep questions that remain as to what is the proper definition of mass. General relativity offers an equivalence between space-time and mass, but lacking a coherent theory of quantum gravity, it is unclear as to how or whether this connection is relevant on microscales. With some justification, Newton's second law can be taken as a quantitative definition of \"mass\" by writing the law as an equality; the relative units of force and mass then are fixed.\n\nThe use of Newton's Second Law as a \"definition\" of force has been disparaged in some of the more rigorous textbooks, because it is essentially a mathematical truism. Notable physicists, philosophers and mathematicians who have sought a more explicit definition of the concept of force include Ernst Mach and Walter Noll.\n\nNewton's Second Law can be used to measure the strength of forces. For instance, knowledge of the masses of planets along with the accelerations of their orbits allows scientists to calculate the gravitational forces on planets.\n\nWhenever one body exerts a force on another, the latter simultaneously exerts an equal and opposite force on the first. In vector form, if formula_8 is the force of body 1 on body 2 and formula_9 that of body 2 on body 1, then\nThis law is sometimes referred to as the \"action-reaction law\", with formula_11 called the \"action\" and formula_12 the \"reaction\".\n\nNewton's Third Law is a result of applying symmetry to situations where forces can be attributed to the presence of different objects. The third law means that all forces are \"interactions\" between different bodies, and thus that there is no such thing as a unidirectional force or a force that acts on only one body.\n\nIn a system composed of object 1 and object 2, the net force on the system due to their mutual interactions is zero:\nMore generally, in a closed system of particles, all internal forces are balanced. The particles may accelerate with respect to each other but the center of mass of the system will not accelerate. If an external force acts on the system, it will make the center of mass accelerate in proportion to the magnitude of the external force divided by the mass of the system.\n\nCombining Newton's Second and Third Laws, it is possible to show that the linear momentum of a system is conserved. In a system of two particles, if formula_14 is the momentum of object 1 and formula_15 the momentum of object 2, then\nUsing similar arguments, this can be generalized to a system with an arbitrary number of particles. In general, as long as all forces are due to the interaction of objects with mass, it is possible to define a system such that net momentum is never lost nor gained.\n\nIn the special theory of relativity, mass and energy are equivalent (as can be seen by calculating the work required to accelerate an object). When an object's velocity increases, so does its energy and hence its mass equivalent (inertia). It thus requires more force to accelerate it the same amount than it did at a lower velocity. Newton's Second Law\nremains valid because it is a mathematical definition. But for relativistic momentum to be conserved, it must be redefined as:\nwhere formula_19 is the rest mass and formula_20 the speed of light.\n\nThe relativistic expression relating force and acceleration for a particle with constant non-zero rest mass formula_21 moving in the formula_22 direction is:\nwhere\nis called the Lorentz factor.\n\nIn the early history of relativity, the expressions formula_25 and formula_26 were called longitudinal and transverse mass. Relativistic force does not produce a constant acceleration, but an ever-decreasing acceleration as the object approaches the speed of light. Note that formula_27 approaches asymptotically an infinite value and is undefined for an object with a non-zero rest mass as it approaches the speed of light, and the theory yields no prediction at that speed.\n\nIf formula_28 is very small compared to formula_20, then formula_30 is very close to 1 and \nis a close approximation. Even for use in relativity, however, one can restore the form of\n\nthrough the use of four-vectors. This relation is correct in relativity when formula_33 is the four-force, formula_21 is the invariant mass, and formula_35 is the four-acceleration.\n\nSince forces are perceived as pushes or pulls, this can provide an intuitive understanding for describing forces. As with other physical concepts (e.g. temperature), the intuitive understanding of forces is quantified using precise operational definitions that are consistent with direct observations and compared to a standard measurement scale. Through experimentation, it is determined that laboratory measurements of forces are fully consistent with the conceptual definition of force offered by Newtonian mechanics.\n\nForces act in a particular direction and have sizes dependent upon how strong the push or pull is. Because of these characteristics, forces are classified as \"vector quantities\". This means that forces follow a different set of mathematical rules than physical quantities that do not have direction (denoted scalar quantities). For example, when determining what happens when two forces act on the same object, it is necessary to know both the magnitude and the direction of both forces to calculate the result. If both of these pieces of information are not known for each force, the situation is ambiguous. For example, if you know that two people are pulling on the same rope with known magnitudes of force but you do not know which direction either person is pulling, it is impossible to determine what the acceleration of the rope will be. The two people could be pulling against each other as in tug of war or the two people could be pulling in the same direction. In this simple one-dimensional example, without knowing the direction of the forces it is impossible to decide whether the net force is the result of adding the two force magnitudes or subtracting one from the other. Associating forces with vectors avoids such problems.\n\nHistorically, forces were first quantitatively investigated in conditions of static equilibrium where several forces canceled each other out. Such experiments demonstrate the crucial properties that forces are additive vector quantities: they have magnitude and direction. When two forces act on a point particle, the resulting force, the \"resultant\" (also called the \"net force\"), can be determined by following the parallelogram rule of vector addition: the addition of two vectors represented by sides of a parallelogram, gives an equivalent resultant vector that is equal in magnitude and direction to the transversal of the parallelogram. The magnitude of the resultant varies from the difference of the magnitudes of the two forces to their sum, depending on the angle between their lines of action. However, if the forces are acting on an extended body, their respective lines of application must also be specified in order to account for their effects on the motion of the body.\n\nFree-body diagrams can be used as a convenient way to keep track of forces acting on a system. Ideally, these diagrams are drawn with the angles and relative magnitudes of the force vectors preserved so that graphical vector addition can be done to determine the net force.\n\nAs well as being added, forces can also be resolved into independent components at right angles to each other. A horizontal force pointing northeast can therefore be split into two forces, one pointing north, and one pointing east. Summing these component forces using vector addition yields the original force. Resolving force vectors into components of a set of basis vectors is often a more mathematically clean way to describe forces than using magnitudes and directions. This is because, for orthogonal components, the components of the vector sum are uniquely determined by the scalar addition of the components of the individual vectors. Orthogonal components are independent of each other because forces acting at ninety degrees to each other have no effect on the magnitude or direction of the other. Choosing a set of orthogonal basis vectors is often done by considering what set of basis vectors will make the mathematics most convenient. Choosing a basis vector that is in the same direction as one of the forces is desirable, since that force would then have only one non-zero component. Orthogonal force vectors can be three-dimensional with the third component being at right-angles to the other two.\n\nEquilibrium occurs when the resultant force acting on a point particle is zero (that is, the vector sum of all forces is zero). When dealing with an extended body, it is also necessary that the net torque be zero.\n\nThere are two kinds of equilibrium: static equilibrium and dynamic equilibrium.\n\nStatic equilibrium was understood well before the invention of classical mechanics. Objects that are at rest have zero net force acting on them.\n\nThe simplest case of static equilibrium occurs when two forces are equal in magnitude but opposite in direction. For example, an object on a level surface is pulled (attracted) downward toward the center of the Earth by the force of gravity. At the same time, a force is applied by the surface that resists the downward force with equal upward force (called a normal force). The situation produces zero net force and hence no acceleration.\n\nPushing against an object that rests on a frictional surface can result in a situation where the object does not move because the applied force is opposed by static friction, generated between the object and the table surface. For a situation with no movement, the static friction force \"exactly\" balances the applied force resulting in no acceleration. The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object.\n\nA static equilibrium between two forces is the most usual way of measuring forces, using simple devices such as weighing scales and spring balances. For example, an object suspended on a vertical spring scale experiences the force of gravity acting on the object balanced by a force applied by the \"spring reaction force\", which equals the object's weight. Using such tools, some quantitative force laws were discovered: that the force of gravity is proportional to volume for objects of constant density (widely exploited for millennia to define standard weights); Archimedes' principle for buoyancy; Archimedes' analysis of the lever; Boyle's law for gas pressure; and Hooke's law for springs. These were all formulated and experimentally verified before Isaac Newton expounded his Three Laws of Motion.\n\nDynamic equilibrium was first described by Galileo who noticed that certain assumptions of Aristotelian physics were contradicted by observations and logic. Galileo realized that simple velocity addition demands that the concept of an \"absolute rest frame\" did not exist. Galileo concluded that motion in a constant velocity was completely equivalent to rest. This was contrary to Aristotle's notion of a \"natural state\" of rest that objects with mass naturally approached. Simple experiments showed that Galileo's understanding of the equivalence of constant velocity and rest were correct. For example, if a mariner dropped a cannonball from the crow's nest of a ship moving at a constant velocity, Aristotelian physics would have the cannonball fall straight down while the ship moved beneath it. Thus, in an Aristotelian universe, the falling cannonball would land behind the foot of the mast of a moving ship. However, when this experiment is actually conducted, the cannonball always falls at the foot of the mast, as if the cannonball knows to travel with the ship despite being separated from it. Since there is no forward horizontal force being applied on the cannonball as it falls, the only conclusion left is that the cannonball continues to move with the same velocity as the boat as it falls. Thus, no force is required to keep the cannonball moving at the constant forward velocity.\n\nMoreover, any object traveling at a constant velocity must be subject to zero net force (resultant force). This is the definition of dynamic equilibrium: when all the forces on an object balance but it still moves at a constant velocity.\n\nA simple case of dynamic equilibrium occurs in constant velocity motion across a surface with kinetic friction. In such a situation, a force is applied in the direction of motion while the kinetic friction force exactly opposes the applied force. This results in zero net force, but since the object started with a non-zero velocity, it continues to move with a non-zero velocity. Aristotle misinterpreted this motion as being caused by the applied force. However, when kinetic friction is taken into consideration it is clear that there is no net force causing constant velocity motion.\n\nThe notion \"force\" keeps its meaning in quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schrödinger equation instead of Newtonian equations. This has the consequence that the results of a measurement are now sometimes \"quantized\", i.e. they appear in discrete portions. This is, of course, difficult to imagine in the context of \"forces\". However, the potentials \"V\"(\"x\",\"y\",\"z\") or fields, from which the forces generally can be derived, are treated similarly to classical position variables, i.e., formula_36.\n\nThis becomes different only in the framework of quantum field theory, where these fields are also quantized.\n\nHowever, already in quantum mechanics there is one \"caveat\", namely the particles acting onto each other do not only possess the spatial variable, but also a discrete intrinsic angular momentum-like variable called the \"spin\", and there is the Pauli exclusion principle relating the space and the spin variables. Depending on the value of the spin, identical particles split into two different classes, fermions and bosons. If two identical fermions (e.g. electrons) have a \"symmetric\" spin function (e.g. parallel spins) the spatial variables must be \"antisymmetric\" (i.e. they exclude each other from their places much as if there was a repulsive force), and vice versa, i.e. for antiparallel \"spins\" the \"position variables\" must be symmetric (i.e. the apparent force must be attractive). Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive.\n\nThus the notion \"force\" loses already part of its meaning.\n\nIn modern particle physics, forces and the acceleration of particles are explained as a mathematical by-product of exchange of momentum-carrying gauge bosons. With the development of quantum field theory and general relativity, it was realized that force is a redundant concept arising from conservation of momentum (4-momentum in relativity and momentum of virtual particles in quantum electrodynamics). The conservation of momentum can be directly derived from the homogeneity or symmetry of space and so is usually considered more fundamental than the concept of a force. Thus the currently known fundamental forces are considered more accurately to be \"fundamental interactions\". When particle A emits (creates) or absorbs (annihilates) virtual particle B, a momentum conservation results in recoil of particle A making impression of repulsion or attraction between particles A A' exchanging by B. This description applies to all forces arising from fundamental interactions. While sophisticated mathematical descriptions are needed to predict, in full detail, the accurate result of such interactions, there is a conceptually simple way to describe such interactions through the use of Feynman diagrams. In a Feynman diagram, each matter particle is represented as a straight line (see world line) traveling through time, which normally increases up or to the right in the diagram. Matter and anti-matter particles are identical except for their direction of propagation through the Feynman diagram. World lines of particles intersect at interaction vertices, and the Feynman diagram represents any force arising from an interaction as occurring at the vertex with an associated instantaneous change in the direction of the particle world lines. Gauge bosons are emitted away from the vertex as wavy lines and, in the case of virtual particle exchange, are absorbed at an adjacent vertex.\n\nThe utility of Feynman diagrams is that other types of physical phenomena that are part of the general picture of fundamental interactions but are conceptually separate from forces can also be described using the same rules. For example, a Feynman diagram can describe in succinct detail how a neutron decays into an electron, proton, and neutrino, an interaction mediated by the same gauge boson that is responsible for the weak nuclear force.\n\nAll of the known forces of the universe are classified into four fundamental interactions. The strong and the weak forces are nuclear forces that act only at very short distances, and are responsible for the interactions between subatomic particles, including nucleons and compound nuclei. The electromagnetic force acts between electric charges, and the gravitational force acts between masses. All other forces in nature derive from these four fundamental interactions. For example, friction is a manifestation of the electromagnetic force acting between atoms of two surfaces, and the Pauli exclusion principle, which does not permit atoms to pass through each other. Similarly, the forces in springs, modeled by Hooke's law, are the result of electromagnetic forces and the Pauli exclusion principle acting together to return an object to its equilibrium position. Centrifugal forces are acceleration forces that arise simply from the acceleration of rotating frames of reference.\n\nThe fundamental theories for forces developed from the unification of different ideas. For example, Sir. Isaac Newton unified, with his universal theory of gravitation, the force responsible for objects falling near the surface of the Earth with the force responsible for the falling of celestial bodies about the Earth (the Moon) and around the Sun (the planets). Michael Faraday and James Clerk Maxwell demonstrated that electric and magnetic forces were unified through a theory of electromagnetism. In the 20th century, the development of quantum mechanics led to a modern understanding that the first three fundamental forces (all except gravity) are manifestations of matter (fermions) interacting by exchanging virtual particles called gauge bosons. This standard model of particle physics assumes a similarity between the forces and led scientists to predict the unification of the weak and electromagnetic forces in electroweak theory, which was subsequently confirmed by observation. The complete formulation of the standard model predicts an as yet unobserved Higgs mechanism, but observations such as neutrino oscillations suggest that the standard model is incomplete. A Grand Unified Theory that allows for the combination of the electroweak interaction with the strong force is held out as a possibility with candidate theories such as supersymmetry proposed to accommodate some of the outstanding unsolved problems in physics. Physicists are still attempting to develop self-consistent unification models that would combine all four fundamental interactions into a theory of everything. Einstein tried and failed at this endeavor, but currently the most popular approach to answering this question is string theory.\n\nWhat we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as formula_37 and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of formula_21 will experience a force:\n\nFor an object in free-fall, this force is unopposed and the net force on the object is its weight. For objects not in free-fall, the force of gravity is opposed by the reaction forces applied by their supports. For example, a person standing on the ground experiences zero net force, since a normal force (a reaction force) is exerted by the ground upward on the person that counterbalances his weight that is directed downward.\n\nNewton's contribution to gravitational theory was to unify the motions of heavenly bodies, which Aristotle had assumed were in a natural state of constant motion, with falling motion observed on the Earth. He proposed a law of gravity that could account for the celestial motions that had been described earlier using Kepler's laws of planetary motion.\n\nNewton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration of a body due to gravity is proportional to the mass of the other attracting body. Combining these ideas gives a formula that relates the mass (formula_40) and the radius (formula_41) of the Earth to the gravitational acceleration:\n\nwhere formula_43 is the distance between the two objects' centers of mass and formula_44 is the unit vector pointed in the direction away from the center of the first object toward the center of the second object.\n\nThis formula was powerful enough to stand as the basis for all subsequent descriptions of motion within the solar system until the 20th century. During that time, sophisticated methods of perturbation analysis were invented to calculate the deviations of orbits due to the influence of multiple bodies on a planet, moon, comet, or asteroid. The formalism was exact enough to allow mathematicians to predict the existence of the planet Neptune before it was observed.\n\nMercury's orbit, however, did not match that predicted by Newton's Law of Gravitation. Some astrophysicists predicted the existence of another planet (Vulcan) that would explain the discrepancies; however no such planet could be found. When Albert Einstein formulated his theory of general relativity (GR) he turned his attention to the problem of Mercury's orbit and found that his theory added a correction, which could account for the discrepancy. This was the first time that Newton's Theory of Gravity had been shown to be inexact.\n\nSince then, general relativity has been acknowledged as the theory that best explains gravity. In GR, gravitation is not viewed as a force, but rather, objects moving freely in gravitational fields travel under their own inertia in straight lines through curved space-time – defined as the shortest space-time path between two space-time events. From the perspective of the object, all motion occurs as if there were no gravitation whatsoever. It is only when observing the motion in a global sense that the curvature of space-time can be observed and the force is inferred from the object's curved path. Thus, the straight line path in space-time is seen as a curved line in space, and it is called the \"ballistic trajectory\" of the object. For example, a basketball thrown from the ground moves in a parabola, as it is in a uniform gravitational field. Its space-time trajectory is almost a straight line, slightly curved (with the radius of curvature of the order of few light-years). The time derivative of the changing momentum of the object is what we label as \"gravitational force\".\n\nThe electrostatic force was first described in 1784 by Coulomb as a force that existed intrinsically between two charges. The properties of the electrostatic force were that it varied as an inverse square law directed in the radial direction, was both attractive and repulsive (there was intrinsic polarity), was independent of the mass of the charged objects, and followed the superposition principle. Coulomb's law unifies all these observations into one succinct statement.\n\nSubsequent mathematicians and physicists found the construct of the \"electric field\" to be useful for determining the electrostatic force on an electric charge at any point in space. The electric field was based on using a hypothetical \"test charge\" anywhere in space and then using Coulomb's Law to determine the electrostatic force. Thus the electric field anywhere in space is defined as\n\nwhere formula_46 is the magnitude of the hypothetical test charge.\n\nMeanwhile, the Lorentz force of magnetism was discovered to exist between two electric currents. It has the same mathematical character as Coulomb's Law with the proviso that like currents attract and unlike currents repel. Similar to the electric field, the magnetic field can be used to determine the magnetic force on an electric current at any point in space. In this case, the magnitude of the magnetic field was determined to be\n\nwhere formula_48 is the magnitude of the hypothetical test current and formula_49 is the length of hypothetical wire through which the test current flows. The magnetic field exerts a force on all magnets including, for example, those used in compasses. The fact that the Earth's magnetic field is aligned closely with the orientation of the Earth's axis causes compass magnets to become oriented because of the magnetic force pulling on the needle.\n\nThrough combining the definition of electric current as the time rate of change of electric charge, a rule of vector multiplication called Lorentz's Law describes the force on a charge moving in a magnetic field. The connection between electricity and magnetism allows for the description of a unified \"electromagnetic force\" that acts on a charge. This force can be written as a sum of the electrostatic force (due to the electric field) and the magnetic force (due to the magnetic field). Fully stated, this is the law:\n\nwhere formula_51 is the electromagnetic force, formula_46 is the magnitude of the charge of the particle, formula_53 is the electric field, formula_54 is the velocity of the particle that is crossed with the magnetic field (formula_55).\n\nThe origin of electric and magnetic fields would not be fully explained until 1864 when James Clerk Maxwell unified a number of earlier theories into a set of 20 scalar equations, which were later reformulated into 4 vector equations by Oliver Heaviside and Josiah Willard Gibbs. These \"Maxwell Equations\" fully described the sources of the fields as being stationary and moving charges, and the interactions of the fields themselves. This led Maxwell to discover that electric and magnetic fields could be \"self-generating\" through a wave that traveled at a speed that he calculated to be the speed of light. This insight united the nascent fields of electromagnetic theory with optics and led directly to a complete description of the electromagnetic spectrum.\n\nHowever, attempting to reconcile electromagnetic theory with two observations, the photoelectric effect, and the nonexistence of the ultraviolet catastrophe, proved troublesome. Through the work of leading theoretical physicists, a new theory of electromagnetism was developed using quantum mechanics. This final modification to electromagnetic theory ultimately led to quantum electrodynamics (or QED), which fully describes all electromagnetic phenomena as being mediated by wave–particles known as photons. In QED, photons are the fundamental exchange particle, which described all interactions relating to electromagnetism including the electromagnetic force.\n\nThere are two \"nuclear forces\", which today are usually described as interactions that take place in quantum theories of particle physics. The strong nuclear force is the force responsible for the structural integrity of atomic nuclei while the weak nuclear force is responsible for the decay of certain nucleons into leptons and other types of hadrons.\n\nThe strong force is today understood to represent the interactions between quarks and gluons as detailed by the theory of quantum chromodynamics (QCD). The strong force is the fundamental force mediated by gluons, acting upon quarks, antiquarks, and the gluons themselves. The (aptly named) strong interaction is the \"strongest\" of the four fundamental forces.\n\nThe strong force only acts \"directly\" upon elementary particles. However, a residual of the force is observed between hadrons (the best known example being the force that acts between nucleons in atomic nuclei) as the nuclear force. Here the strong force acts indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons, which classically transmit the nuclear force (see this topic for more). The failure of many searches for free quarks has shown that the elementary particles affected are not directly observable. This phenomenon is called color confinement.\n\nThe weak force is due to the exchange of the heavy W and Z bosons. Its most familiar effect is beta decay (of neutrons in atomic nuclei) and the associated radioactivity. The word \"weak\" derives from the fact that the field strength is some 10 times less than that of the strong force. Still, it is stronger than gravity over short distances. A consistent electroweak theory has also been developed, which shows that electromagnetic forces and the weak force are indistinguishable at a temperatures in excess of approximately 10 kelvins. Such temperatures have been probed in modern particle accelerators and show the conditions of the universe in the early moments of the Big Bang.\n\nSome forces are consequences of the fundamental ones. In such situations, idealized models can be utilized to gain physical insight.\n\nThe normal force is due to repulsive forces of interaction between atoms at close contact. When their electron clouds overlap, Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects. The normal force, for example, is responsible for the structural integrity of tables and floors as well as being the force that responds whenever an external force pushes on a solid object. An example of the normal force in action is the impact force on an object crashing into an immobile surface.\n\nFriction is a surface force that opposes relative motion. The frictional force is directly related to the normal force that acts to keep two solid objects separated at the point of contact. There are two broad classifications of frictional forces: static friction and kinetic friction.\n\nThe static friction force (formula_56) will exactly oppose forces applied to an object parallel to a surface contact up to the limit specified by the coefficient of static friction (formula_57) multiplied by the normal force (formula_58). In other words, the magnitude of the static friction force satisfies the inequality:\n\nThe kinetic friction force (formula_60) is independent of both the forces applied and the movement of the object. Thus, the magnitude of the force equals:\n\nwhere formula_62 is the coefficient of kinetic friction. For most surface interfaces, the coefficient of kinetic friction is less than the coefficient of static friction.\n\nTension forces can be modeled using ideal strings that are massless, frictionless, unbreakable, and unstretchable. They can be combined with ideal pulleys, which allow ideal strings to switch physical direction. Ideal strings transmit tension forces instantaneously in action-reaction pairs so that if two objects are connected by an ideal string, any force directed along the string by the first object is accompanied by a force directed along the string in the opposite direction by the second object. By connecting the same string multiple times to the same object through the use of a set-up that uses movable pulleys, the tension force on a load can be multiplied. For every string that acts on a load, another factor of the tension force in the string acts on the load. However, even though such machines allow for an increase in force, there is a corresponding increase in the length of string that must be displaced in order to move the load. These tandem effects result ultimately in the conservation of mechanical energy since the work done on the load is the same no matter how complicated the machine.\n\nAn elastic force acts to return a spring to its natural length. An ideal spring is taken to be massless, frictionless, unbreakable, and infinitely stretchable. Such springs exert forces that push when contracted, or pull when extended, in proportion to the displacement of the spring from its equilibrium position. This linear relationship was described by Robert Hooke in 1676, for whom Hooke's law is named. If formula_63 is the displacement, the force exerted by an ideal spring equals:\n\nwhere formula_65 is the spring constant (or force constant), which is particular to the spring. The minus sign accounts for the tendency of the force to act in opposition to the applied load.\n\nNewton's laws and Newtonian mechanics in general were first developed to describe how forces affect idealized point particles rather than three-dimensional objects. However, in real life, matter has extended structure and forces that act on one part of an object might affect other parts of an object. For situations where lattice holding together the atoms in an object is able to flow, contract, expand, or otherwise change shape, the theories of continuum mechanics describe the way forces affect the material. For example, in extended fluids, differences in pressure result in forces being directed along the pressure gradients as follows:\n\nwhere formula_67 is the volume of the object in the fluid and formula_68 is the scalar function that describes the pressure at all locations in space. Pressure gradients and differentials result in the buoyant force for fluids suspended in gravitational fields, winds in atmospheric science, and the lift associated with aerodynamics and flight.\n\nA specific instance of such a force that is associated with dynamic pressure is fluid resistance: a body force that resists the motion of an object through a fluid due to viscosity. For so-called \"Stokes' drag\" the force is approximately proportional to the velocity, but opposite in direction:\n\nwhere:\n\nMore formally, forces in continuum mechanics are fully described by a stress–tensor with terms that are roughly defined as\n\nwhere formula_73 is the relevant cross-sectional area for the volume for which the stress-tensor is being calculated. This formalism includes pressure terms associated with forces that act normal to the cross-sectional area (the matrix diagonals of the tensor) as well as shear terms associated with forces that act parallel to the cross-sectional area (the off-diagonal elements). The stress tensor accounts for forces that cause all strains (deformations) including also tensile stresses and compressions.\n\nThere are forces that are frame dependent, meaning that they appear due to the adoption of non-Newtonian (that is, non-inertial) reference frames. Such forces include the centrifugal force and the Coriolis force. These forces are considered fictitious because they do not exist in frames of reference that are not accelerating. Because these forces are not genuine they are also referred to as \"pseudo forces\".\n\nIn general relativity, gravity becomes a fictitious force that arises in situations where spacetime deviates from a flat geometry. As an extension, Kaluza–Klein theory and string theory ascribe electromagnetism and the other fundamental forces respectively to the curvature of differently scaled dimensions, which would ultimately imply that all forces are fictitious.\n\nForces that cause extended objects to rotate are associated with torques. Mathematically, the torque of a force formula_51 is defined relative to an arbitrary reference point as the cross-product:\n\nwhere\n\nTorque is the rotation equivalent of force in the same way that angle is the rotational equivalent for position, angular velocity for velocity, and angular momentum for momentum. As a consequence of Newton's First Law of Motion, there exists rotational inertia that ensures that all bodies maintain their angular momentum unless acted upon by an unbalanced torque. Likewise, Newton's Second Law of Motion can be used to derive an analogous equation for the instantaneous angular acceleration of the rigid body:\n\nwhere\n\nThis provides a definition for the moment of inertia, which is the rotational equivalent for mass. In more advanced treatments of mechanics, where the rotation over a time interval is described, the moment of inertia must be substituted by the tensor that, when properly analyzed, fully determines the characteristics of rotations including precession and nutation.\n\nEquivalently, the differential form of Newton's Second Law provides an alternative definition of torque:\n\nNewton's Third Law of Motion requires that all objects exerting torques themselves experience equal and opposite torques, and therefore also directly implies the conservation of angular momentum for closed systems that experience rotations and revolutions through the action of internal torques.\n\nFor an object accelerating in circular motion, the unbalanced force acting on the object equals:\n\nwhere formula_21 is the mass of the object, formula_28 is the velocity of the object and formula_43 is the distance to the center of the circular path and formula_44 is the unit vector pointing in the radial direction outwards from the center. This means that the unbalanced centripetal force felt by any object is always directed toward the center of the curving path. Such forces act perpendicular to the velocity vector associated with the motion of an object, and therefore do not change the speed of the object (magnitude of the velocity), but only the direction of the velocity vector. The unbalanced force that accelerates an object can be resolved into a component that is perpendicular to the path, and one that is tangential to the path. This yields both the tangential force, which accelerates the object by either slowing it down or speeding it up, and the radial (centripetal) force, which changes its direction.\n\nForces can be used to define a number of physical concepts by integrating with respect to kinematic variables. For example, integrating with respect to time gives the definition of impulse:\n\nwhich by Newton's Second Law must be equivalent to the change in momentum (yielding the Impulse momentum theorem).\n\nSimilarly, integrating with respect to position gives a definition for the work done by a force:\n\nwhich is equivalent to changes in kinetic energy (yielding the work energy theorem).\n\nPower \"P\" is the rate of change d\"W\"/d\"t\" of the work \"W\", as the trajectory is extended by a position change formula_89 in a time interval d\"t\":\n\nwith formula_91 the velocity.\n\nInstead of a force, often the mathematically related concept of a potential energy field can be used for convenience. For instance, the gravitational force acting upon an object can be seen as the action of the gravitational field that is present at the object's location. Restating mathematically the definition of energy (via the definition of work), a potential scalar field formula_92 is defined as that field whose gradient is equal and opposite to the force produced at every point:\n\nForces can be classified as conservative or nonconservative. Conservative forces are equivalent to the gradient of a potential while nonconservative forces are not.\n\nA conservative force that acts on a closed system has an associated mechanical work that allows energy to convert only between kinetic or potential forms. This means that for a closed system, the net mechanical energy is conserved whenever a conservative force acts on the system. The force, therefore, is related directly to the difference in potential energy between two different locations in space, and can be considered to be an artifact of the potential field in the same way that the direction and amount of a flow of water can be considered to be an artifact of the contour map of the elevation of an area.\n\nConservative forces include gravity, the electromagnetic force, and the spring force. Each of these forces has models that are dependent on a position often given as a radial vector formula_76 emanating from spherically symmetric potentials. Examples of this follow:\n\nFor gravity:\n\nwhere formula_96 is the gravitational constant, and formula_97 is the mass of object \"n\".\n\nFor electrostatic forces:\n\nwhere formula_99 is electric permittivity of free space, and formula_100 is the electric charge of object \"n\".\n\nFor spring forces:\n\nwhere formula_65 is the spring constant.\n\nFor certain physical scenarios, it is impossible to model forces as being due to gradient of potentials. This is often due to macrophysical considerations that yield forces as arising from a macroscopic statistical average of microstates. For example, friction is caused by the gradients of numerous electrostatic potentials between the atoms, but manifests as a force model that is independent of any macroscale position vector. Nonconservative forces other than friction include other contact forces, tension, compression, and drag. However, for any sufficiently detailed description, all these forces are the results of conservative ones since each of these macroscopic forces are the net results of the gradients of microscopic potentials.\n\nThe connection between macroscopic nonconservative forces and microscopic conservative forces is described by detailed treatment with statistical mechanics. In macroscopic closed systems, nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat. According to the Second law of thermodynamics, nonconservative forces necessarily result in energy transformations within closed systems from ordered to more random conditions as entropy increases.\n\nThe SI unit of force is the newton (symbol N), which is the force required to accelerate a one kilogram mass at a rate of one meter per second squared, or . The corresponding CGS unit is the dyne, the force required to accelerate a one gram mass by one centimeter per second squared, or . A newton is thus equal to 100,000 dynes.\n\nThe gravitational foot-pound-second English unit of force is the pound-force (lbf), defined as the force exerted by gravity on a pound-mass in the standard gravitational field of . The pound-force provides an alternative unit of mass: one slug is the mass that will accelerate by one foot per second squared when acted on by one pound-force.\n\nAn alternative unit of force in a different foot-pound-second system, the absolute fps system, is the poundal, defined as the force required to accelerate a one-pound mass at a rate of one foot per second squared. The units of slug and poundal are designed to avoid a constant of proportionality in Newton's Second Law.\n\nThe pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sthène, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf.\n\nSee also Ton-force.\n\nSee force gauge, spring scale, load cell\n\n\n\n"}
{"id": "22249817", "url": "https://en.wikipedia.org/wiki?curid=22249817", "title": "Genetic admixture", "text": "Genetic admixture\n\nGenetic admixture occurs when two or more previously isolated and genetically differentiated populations begin interbreeding. Admixture results in the introduction of new genetic lineages into a population. It has been known to slow local adaptation by introducing foreign, unadapted genotypes (known as genetic pollution). It also prevents speciation by homogenizing populations and increasing heterozygosity.\n\nClimatic cycles facilitate genetic admixture in cold periods and genetic diversification in warm periods.\nNatural flooding can cause genetic admixture within populations of migrating fish species.\nGenetic admixture may have an important role for the success of populations that colonise a new area and interbreed with individuals of native populations.\n\nAdmixture mapping is a method of gene mapping that uses a population of mixed ancestry (an admixed population) to find the genetic loci that contribute to differences in diseases or other phenotypes found between the different ancestral populations. The method is best applied to populations with recent admixture from two populations that were previously genetically isolated for tens of thousands of years, such as African Americans (admixture of African and European populations). The method attempts to correlate the degree of ancestry near a genetic locus with the phenotype or disease of interest. Genetic markers that differ in frequency between the ancestral populations are needed across the genome.\n\nAdmixture mapping is based on the assumption that differences in disease rates or phenotypes are due in part to differences in the frequencies of disease-causing or phenotype-causing genetic variants between populations. In an admixed population, these causal variants occur more frequently on chromosomal segments inherited from one or another ancestral population. The first admixture scans were published in 2005 and since then genetic contributors to a variety of disease and trait differences have been mapped. These include hypertension, multiple sclerosis, BMI, and prostate cancer in African Americans. By 2010, high-density mapping panels had been constructed for African Americans, Latino/Hispanics, and Uyghurs.\n\n"}
{"id": "31493190", "url": "https://en.wikipedia.org/wiki?curid=31493190", "title": "Group green exercise", "text": "Group green exercise\n\nGroup Green Exercise refers to physical exercise undertaken in natural environments carried out as a group. Physical exercise has positive outcomes for both physical and mental health, there is growing evidence confirming the benefits to be had from contact with nature, while the work of Prof. Jules Pretty at the University of Essex has revealed the synergistic benefits of combining the two in green exercise. New research, by Auckland University of Technology, is now investigating the additional social, physical and mental health benefits of Group Green Exercise.\n\n \n"}
{"id": "38258813", "url": "https://en.wikipedia.org/wiki?curid=38258813", "title": "H.M. Krishna Murthy", "text": "H.M. Krishna Murthy\n\nH.M. Krishna Murthy is a former researcher from the University of Alabama at Birmingham. In 2009, several of his publications were retracted from scientific journals after accusations of scientific misconduct surfaced. UAB began investigating claims of fraud in January 2007 after the validity of the proteins came into question. The University’s probe produced ten publications that were shown not to be valid. These publications have since been retracted from their respective journals. Murthy’s publications dealt with the crystal structures of several proteins that he had claimed to have determined using crystallographic methods. It was found that these protein structures were never properly determined and the primary crystallographic data for at least some of them have been fabricated by Murthy.\n\nIn 1981, Murthy began his work with protein crystallography as a post doctoral researcher at Yale University. While he was researching there, Murthy worked under renowned crystallographer Thomas Steitz. The research Murthy did while at Yale is regarded as genuine. In 1998, Murthy began working at the University of Alabama at Birmingham. There, he was a research assistant professor in the Center for Biophysical Sciences and Engineering.\n\nIn 2007, H.M. Krishna Murthy was working as a researcher at the University of Alabama at Birmingham when questions regarding the authenticity of his work surfaced. European crystallographers were looking into some of the structures that Dr. Murthy had claimed to have solved and began to see issues. Graduate students in the Netherlands working under Dr. Piet Gros also spotted problems throughout Murthy’s work. Gros pointed out the many missing layers throughout the structure of C3B. When asked to provide further information about the structures, Dr. Murthy could not provide enough information to prove their validity. Gros and several other scientists decided to check the work and discovered unrealistic and physically impossible data. After receiving this information, the university decided to launch an investigation of their own. A committee of experts (appointed by the University of Alabama at Birmingham) in the field with no conflicting interests looked into the case and determined that the structures were more than likely fabricated. The group discovered that these structures defied many physical and chemical laws, further proving that these protein structures were fictitious. The team looking into the fraud suggested that the structures be removed from the public record. \n\nIn total, twelve of H.M. Krishna Murthy’s structures and ten of his papers were affected by this scandal. The papers were retracted by their respective journals and upon retraction. However, not all of the structures have been removed from the Protein Data Bank. The protein structures in question were 1BEF, 1CMW, 1DF9, 2QID, 1G40, 1G44, 1L6L, 2OU1, 1RID, 1Y8E, 2A01, and 2HR0. Only 1BEF, 1CMW, 1DF9 and 2QID have been removed. Murthy's falsified data ended up affecting 449 papers. These papers covered a range of topics; everything from dengue to Taq DNA polymerase. \n\nThe UAB investigation turned up significant amounts of data proving Murthy falsified these protein structures. In multiple instances, they found impossible electron densities and highly unlikely geometries. Furthermore, they found inconsistencies in the data indicative of computer generated information. In some cases, the information Murthy cited as proof of the structures was not a piece of original work. This confirmed the fictitious nature of his structures by showing that he had used another protein as a model for his creation. Specifically, Murthy tried to pass some elements of a known structure of TAQ polymerase (PDB entry 1TAQ) as a new structure (PDB entry 1CMW). When the misconduct inquiry looked for Murthy's work on several of these proteins, no experimental data was made available to them. Most significantly, no one has been able to experimentally support any of Murthy's structures.\n\nMurthy’s most famous fraudulent structure was 1BEF. Published in 1999 and retracted in 2009, 1BEF was cited in over ninety publications. 1BEF was first mentioned in a now retracted article from The Journal of Biological Chemistry. Murthy claimed 1BEF could be used as a potential treatment option for patients with dengue and other flaviviral proteases. The UAB team that investigated Murthy’s publications said that 1BEF is “an improbable structure…[with] an unacceptable level of inter-atomic clashing.” \n\nMurthy still claims he did not commit any misconduct., However, most scientists agree that the evidence against him indicates otherwise. Murthy’s contract with the University of Alabama at Birmingham expired in February 2009, shortly after his fabrication of data became known to the university. The university chose not to renew the contract, ending their involvement with Murthy. Murthy’s current whereabouts are not known and no one has been able to reach him. Murthy has not published since the scandal was uncovered.\n\n"}
{"id": "38448524", "url": "https://en.wikipedia.org/wiki?curid=38448524", "title": "Index of physics articles (!$@)", "text": "Index of physics articles (!$@)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "28222511", "url": "https://en.wikipedia.org/wiki?curid=28222511", "title": "International Research Association for Talent Development and Excellence", "text": "International Research Association for Talent Development and Excellence\n\nThe International Research Association for Talent Development and Excellence (IRATDE) is a non-profit professional organization of international scientists in the fields of talent development, creativity, innovation and excellence. Founded in 2008, the IRATDE seeks to promote collaboration and cooperation between scholars, professionals, and organizations to further its goals. Chief among these goals is to initiate and support research, disseminate research findings, develop and evaluate educational programs, and to gather scholars and professionals in the fields of talent development, excellence, creativity and innovation.\n\nPresident - Prof. Abdullah Aljughaiman. Director of the National Research Center for Giftedness and Creativity. Dean of the Institute of Research and Consulting. King Faisal University, Al-Ahsa, Kingdom of Saudi Arabia.\n\nVice President - Prof. Heidrun Stoeger. Professor of Education. Chair for School Research, School Development and Evaluation. University of Regensburg, Germany.\n\nSecretary General - Prof. Albert Ziegler. Professor of Psychology. Head of Educational Psychology at the Institute of Psychology and Education. University of Ulm, Germany.\n\nTreasurer - Prof. Jiannong Shi. Director of the Division of Developmental and Educational Psychology, Institute of Psychology, Chinese Academy of Sciences. Director of the Research Center for Supernormal Children, Institute of Psychology, Chinese Academy of Sciences. People's Republic of China.\n\nExecutive Officer - Prof. Wilma Vialle. Professor in Educational Psychology and Associate Dean in the Faculty of Education, University of Wollongong. Australia.\n\nThe IRATDE hosts an international conference biennially on gifted and talented children, innovation and invention, and excellence across domains. The conference is held during odd years typically during the months of September through November. The first biennial IRATDE conference was entitled the \"International Conference on the Cultivation and Education of Creativity and Innovation\" and was held in Xi'an China (October 30 - November 2). The second IRATDE conference is scheduled in Jubail, Saudi Arabia from November 26–30, 2011. In addition, the IRATDE periodically hosts scientific symposia, such as the symposium entitled \" Nurturing Giftedness Starts Early \" hosted at King Faisal University in Saudi Arabia from May 31 through June 2, 2010.\n\nTalent Development & Excellence is the official peer-reviewed scholarly journal published by the International Research Association of Talent Development and Excellence (IRATDE). The journal is currently published twice annually and presents articles containing original research or theory in the fields of talent development, excellence, innovation or expertise.\n\nTalent Talks is the official newsletter of the International Research Association of Talent Development and Excellence. \"Talent Talks\" is an informal venue for IRATDE members to discuss issues related to their fields.\n\nThe International Research Association of Talent Development and Excellence currently includes approximately 400 members from over 60 countries. IRATDE offers Full Membership for scientists and researchers and Affiliated Memberships for organizations and practitioners. Applications for membership are available on their official web site.\n\n"}
{"id": "5235838", "url": "https://en.wikipedia.org/wiki?curid=5235838", "title": "Jean Medawar", "text": "Jean Medawar\n\nJean Shinglewood Medawar (\"née\" Taylor; 7 February 1913 – 3 May 2005) was a British author and a former chairman of the Family Planning Association, and wife of the British Nobel laureate Sir Peter Brian Medawar.\n\nMedawar was born in London, England, the daughter of Katherine Leslie (\"née\" Paton) and Charles Henry Shinglewood Taylor. Her father was a physician working in Cambridge. Her mother was an American from St Louis, Missouri.\n\nShe attended Benenden School in Kent and she won a scholarship to study zoology. She joined Somerville College, Oxford, and earned her BSc in zoology in 1935. She continued to work on the origin and development of lymphocytes under Howard Florey (who later won the Nobel Prize in Physiology or Medicine in 1945) until her marriage in 1937. In 1954, she met Margaret Pyke, Chair of the Family Planning Association, and joined the organisation. She became a member of its executive in 1960. In 1959 she became Joint Editor of the journal \"Family Planning\" (continued as \"Family Planning Today\") alongside David Pyke, Pyke's son, and remained till 1979. She also worked with the Citizens' Advice Bureau, the National Marriage Guidance Council and also with young offenders at HM Prison Holloway at Hampstead. She was appointed chairman of the FPA in 1966, owing to the death of Margaret Pyke, and held the post till 1970. She co-founded the Margaret Pyke Centre for Study and Training in Family Planning and the Margaret Pyke Memorial Trust in 1968, becoming its Director in 1976 until her death.\n\nMedawar published her memoir \"A Very Decided Preference: Life with Peter Medawar\" in 1990 in which she gives an account of her personal life. She met Peter Medawar in Oxford, with the first impression that he looked 'mildly diabolical'. She approached him for the meaning of \"heuristic\", which led to a continued tutorial and lasting friendship. Her family did not want her to marry him because Peter Medawar was of a Lebanese descent and was not financially well-to-do. Her mother asked her, \"What will you do if you have black babies?\" Her aunt described Medawar as having 'no background, no money', and eventually disinherited her. They were married on 27 February 1937. They had two sons, Charles and Alexander, and two daughters, Caroline and Louise. She devoted her time raising their children while her husband was committed to scientific research. Her husband won the Nobel Prize in Physiology or Medicine in 1960. Together they wrote \"The Life Science : Current Ideas of Biology\" in 1977, and \"Aristotle to Zoos : A Philosophical Dictionary of Biology\" in 1984.\n\n\n"}
{"id": "51184343", "url": "https://en.wikipedia.org/wiki?curid=51184343", "title": "Knowledge inertia", "text": "Knowledge inertia\n\nKnowledge inertia (KI) is a concept in knowledge management. The term initially proposed by Liao (2002) constitutes a two dimensional model of knowledge inertia which incorporates experience inertia and learning inertia. Later, another dimension—the dimension of thinking inertia has been added based on the theoretical exploration of the existing concepts of experience inertia and learning inertia. One of the central problems in knowledge management related to organizational learning is to deal with “inertia”. Besides, individuals may also exhibit a natural tendency of inertia when facing problems during utilization of knowledge. Inertia in technical jargon means inactivity or torpor. Inertia in organizational learning context may be referred to as a slowdown in organizational learning-related activities. In fact, there are many other kinds of organizational inertia; e.g., innovation inertia, workforce inertia, productivity inertia, decision inertia, emotional inertia besides others that have different meanings in their own individual contexts. Some organization theorists have adopted the definition proposed by Liao (2002) to extend its further use in organizational learning studies.\n\nKnowledge inertia (KI) may be defined as a problem solving strategy using old, redundant, stagnant knowledge and past experience without recourse to new knowledge and experience. Inertia is a concept in physics that is used to explain the state of an object either remaining in stationary or uniform motion. Organizational theorists adopted this concept of inertia and applied it to different contexts which resulted in the emergence of diverse concepts—such as, for example, organizational inertia, consumer inertia, outsourcing inertia, and cognitive inertia. Some organization theorists have adopted the definition proposed by Liao (2002) to extend its further use in organizational learning studies. Not every instances of knowledge inertia result in gloomy of negative outcome: one study suggested that knowledge inertia could positively affect a firm's product innovation.\n\nKnowledge inertia stems from the use of routine problem solving procedures that involves the utilization of redundant, stagnant knowledge and past experience without any recourse to new knowledge and thinking processes. Different methodologies exist for diverse types of knowledge that could be applied to manage knowledge efficiently. Since KI is a component of knowledge management, it is essential to consider the circulation of various knowledge types in avoiding inertia. The theory of KI supposedly studies the extent to which an organization's ability on problem solving is inhibited. Numerous factors could be attributed as enablers or inhibitors of the abilities on problem solving of an individual or an organization. Knowledge inertia applicable in the context of problem solving, therefore, may require inputs from all these diverse knowledge types, or it may require learning, new thinking, and experience. Emergence of new ideas to supplement the existing knowledge and assimilation of the same could be of help in avoiding the use of stagnant, outdated information while attempting to solve problems.\n\n"}
{"id": "5993712", "url": "https://en.wikipedia.org/wiki?curid=5993712", "title": "Koide formula", "text": "Koide formula\n\nThe Koide formula is an unexplained empirical equation discovered by Yoshio Koide in 1981. In its original form, it relates the masses of the three charged leptons; later authors have extended the relation to neutrinos, quarks, and other families of particles.\n\nThe Koide formula is:\n\nwhere the masses of the electron, muon, and tau are measured respectively as \"m\" = , \"m\" = , and \"m\" = , and the digits in parentheses are the uncertainties in the last figures. This gives \"Q\" = . \n\nIt is clear that . The superior bound follows if we assume that the square roots cannot be negative. By Cauchy-Schwarz, the value formula_2 can be interpreted as the squared cosine of the angle between the vector formula_3 and the vector\nformula_4. (See dot product).\n\nThe mystery is in the physical value. Not only is this result odd in that three apparently random numbers should give a simple fraction, but also that \"Q\" is exactly halfway between the two extremes of (should the three masses be equal) and 1 (should one mass dominate). \n\nWhile the original formula appeared in the context of preon models, other ways have been found to produce it (both by Sumino and by Koide, see references below). As a whole, however, understanding remains incomplete. Similar matches have been found for triplets of quarks depending on running masses. With alternating quarks, chaining Koide equations for consecutive triplets, it is possible to reach a result of 173.263947(6) GeV for the mass of the top quark.\n\nThere are similar empirical formulae which relate other masses. \nQuark masses depend on the energy scale used to measure them, which makes an analysis more complicated.\n\nTaking the heaviest three quarks, charm (1290 MeV), bottom (4180 MeV) and top (172440 MeV), gives the value:\n\nThe possibility that \"Q\" equals exactly 2/3 lies within the experimental uncertainties of the masses (). This was noticed by Rodejohann and Zhang in the first version of their 2011 paper but the observation was removed in the published version, so the first published mention is in 2012 from F. G. Cao.\n\nHowever, the masses of the lightest quarks, up (2.3 MeV), down (4.8 MeV), and strange (95 MeV) yield:\n\na value also cited by Cao.\n\nIn quantum field theory, quantities like coupling constant and mass \"run\" with the energy scale. That is, their value depends on the energy scale at which the observation occurs, in a way described by a renormalization group equation (RGE). One usually expects relationships between such quantities to be simple at high energies (where some symmetry is unbroken) but not at low energies, where the RG flow will have produced complicated deviations from the high energy relation. The Koide relation is exact (within experimental error) for the pole masses, which are low-energy quantities defined at different energy scales. For this reason, many physicists regard the relation as \"numerology\" (e.g.). However, the Japanese physicist Yukinari Sumino has constructed an effective field theory in which a new gauge symmetry causes the pole masses to exactly satisfy the relation. Goffinet's doctoral thesis gives a discussion on pole masses and how the Koide formula can be reformulated without taking the square roots of masses.\n\n\n"}
{"id": "1065238", "url": "https://en.wikipedia.org/wiki?curid=1065238", "title": "Kosmos 133", "text": "Kosmos 133\n\nKosmos 133 (, meaning \"Cosmos 133\"; COSPAR ID: 1966-107A), was the first unmanned test flight of the Soyuz spacecraft, and first mission of the Soyuz programme, as part of the Soviet space programme.\n\nLaunched from the Baikonur Cosmodrome aboard the maiden flight of the Soyuz carrier rocket, Kosmos 133 was planned \"all up\" test, to include an automated docking with a second Soyuz (Soyuz 7K-OK No.1), which was scheduled for launch the day after Kosmos 133. Problems found during ground testing of the second spacecraft resulted in its launch being delayed, and it was destroyed when its carrier rocket exploded on its launch pad following a scrubbed launch attempt in December.\n\nBefore this, the attitude control system of Kosmos 133 malfunctioned, resulting in rapid consumption of orientation fuel, leaving it spinning at 2 rpm. After large efforts by ground control and 5 attempts at retrofire over two days, the craft was finally coming down for a landing. Due to the inaccuracy of the reentry burn, it was determined that the capsule would land in China. The self-destruct command was given and the satellite exploded 30 November 1966 at 10:21 GMT.\n\nThe fireball passed over west Japan and was recorded by photos and a sketch. Kōichirō Tomita identified that it was the Kosmos 133 spacecraft.\n\n"}
{"id": "28768148", "url": "https://en.wikipedia.org/wiki?curid=28768148", "title": "Lanner Group Ltd", "text": "Lanner Group Ltd\n\nLanner Group Ltd is a software company specialising in simulation software such as discrete event simulation and predictive simulation, headquartered in Henley-in-Arden, Warwickshire. The business develops, markets and supports business process simulation and optimisation systems. The company has subsidiaries in the USA, China, France and Germany and a distributor network selling the company's products in 20 different countries. Lanner Group was formed following a Management Buyout of \"AT&T Istel\", a spin-off from the operational research department of British Leyland where, in 1978, the world's first \"visual interactive simulation tool\" was developed. Lanner Group services automotive, aviation, criminal justice, defence and aerospace, financial services and contact centres, food and beverage, health, logistics and supply chain, manufacturing, nuclear, oil and gas, pharmaceutical, and consumer health industries.\n\nLanner Group formed in 1996 after completing a Management Buyout from \"AT&T Istel\"; the company was initially named \"SEE WHY SOLUTIONS\" and was incorporated in 1995. Lanner Group's previous owner, AT&T Istel, formerly known as ISTEL, was initially called \"BL Systems\", and was a spin out formed in 1979 following a Merger of all the computer departments under the then British Leyland umbrella. BL System's \"SEE WHY\" tool, programmed in Fortran 77 and launched in 1980, was the world's first commercially available visual interactive simulation package and the precursor to Lanner Group's current core software product \"WITNESS\". WITNESS was the first of the industrial strength \"4GL\" simulators. The WITNESS system was launched on IBM PC in 1986 and has been revised frequently since. The latest version of Lanner's Witness was released in summer 2017.\n\nThe applications of the FORTRAN 77 / WITNESS interface have been subject to further academic Research and Development. Since 1985 the company has supported its simulation software academic program. Over 100 universities worldwide have been involved since the program began.\n\nLanner Group continued to develop the WITNESS platform further in parallel to developing its mainstay product of the same name, and since 2002 has introduced new systems providing niche simulation packages for police and healthcare organisations called \"PRISM\" and \"PX-Sim\" respectively. In 2006 the company unveiled a Java based simulation engine called \"L-SIM\" which is embedded in Business Process Management (BPM) solutions software. In May the same year, a technology partnership with BPM solution provider IDS Scheer was announced. The L-SIM product is now the simulation engine of IDS Scheer's ARIS Business Simulator. The company's WITNESS platform technology is therefore embedded into current Oracle, SAP, and IBM BPA products.\n\nFrom 1996 to 2010 the company's main investor was private equity company 3i. On 18 March 2010 Lanner Group announced that it had secured a £3 million new investment deal with \"NVM Private Equity\" replacing 3i. 3i continues to retain an interest in Lanner Group as a small minority investor.\n\n\n"}
{"id": "2862085", "url": "https://en.wikipedia.org/wiki?curid=2862085", "title": "Linklog", "text": "Linklog\n\nA linklog is a type of blog which is meant to act as a linked list. Common practice is for the post titles to link directly to an external URLs, and the content of the post includes information to complement the associated URL.\n\nLinklogs existed as a feature of computing systems before the internet as well. In distributed file systems a link log was a method of recording data in which a record is created and added to the proper log when updating a transaction. The format of a log record closely matches the specification of the transaction type it corresponds to. Link log records consisted of two parts in such a system: a set of type-independent fields, and a set of type-specific fields. The former set consists of pointers to the preceding and succeeding records of the log.\n\nIn PBX systems such as AUDIX link-logs were a collection of data collecting to assist operators in maintaining the system.\n\n\n\n"}
{"id": "3300910", "url": "https://en.wikipedia.org/wiki?curid=3300910", "title": "List of Lepidoptera that feed on Tilia", "text": "List of Lepidoptera that feed on Tilia\n\nLimes, lindens and basswoods (\"Tilia\" species) are used as food plants by the larvae of some Lepidoptera species including:\n\nSpecies which feed exclusively on \"Tilia\"\n\n\nSpecies which feed on \"Tilia\" among other plants\n\n\n"}
{"id": "9024274", "url": "https://en.wikipedia.org/wiki?curid=9024274", "title": "List of UN numbers 1001 to 1100", "text": "List of UN numbers 1001 to 1100\n\nThe UN numbers from UN1001 to UN1100 as assigned by the United Nations Committee of Experts on the Transport of Dangerous Goods.\n\n"}
{"id": "36331514", "url": "https://en.wikipedia.org/wiki?curid=36331514", "title": "List of academic journals published in Serbia", "text": "List of academic journals published in Serbia\n\nThis is a list of academic journals published in Serbia.\n\n\n\n"}
{"id": "1896674", "url": "https://en.wikipedia.org/wiki?curid=1896674", "title": "List of circle topics", "text": "List of circle topics\n\nThis list of circle topics includes things related to the geometric shape, either abstractly, as in idealizations studied by geometers, or concretely in physical space. It does not include metaphors like \"inner circle\" or \"circular reasoning\" in which the word does not refer literally to the geometric shape.\n\n\n\n"}
{"id": "33317494", "url": "https://en.wikipedia.org/wiki?curid=33317494", "title": "List of international presidential trips made by Dilma Rousseff", "text": "List of international presidential trips made by Dilma Rousseff\n\nThis is a list of international presidential trips made by Dilma Rousseff, the 36th President of Brazil. During her presidency, which began with her inauguration on 1 January 2011 and ended with her impeachment on 31 August 2016, Rousseff visited 24 countries as of July 2012.\n\nThe following international trips were made by President Dilma Rousseff in 2011:\nThe following international trips have been made by President Dilma Rousseff during her second year in office as of December 2012:\nOn 17 September 2013, President Rousseff cancelled her state visit to Washington, D.C. on 3 October 2013, because of alleged spying by the United States that targeted Brazil.\n\nThe following international trips have been made by President Dilma Rousseff in 2015:\n"}
{"id": "234855", "url": "https://en.wikipedia.org/wiki?curid=234855", "title": "Lists of integrals", "text": "Lists of integrals\n\nIntegration is the basic operation in integral calculus. While differentiation has easy rules by which the derivative of a complicated function can be found by differentiating its simpler component functions, integration does not, so tables of known integrals are often useful. This page lists some of the most common antiderivatives.\n\nA compilation of a list of integrals (Integraltafeln) and techniques of integral calculus was published by the German mathematician (aka ) in 1810. These tables were republished in the United Kingdom in 1823. More extensive tables were compiled in 1858 by the Dutch mathematician David Bierens de Haan for his \"Tables d'intégrales définies\", supplemented by \"Supplément aux tables d'intégrales définies\" in ca. 1864. A new edition was published in 1867 under the title \"Nouvelles tables d'intégrales définies\". These tables, which contain mainly integrals of elementary functions, remained in use until the middle of the 20th century. They were then replaced by the much more extensive tables of Gradshteyn and Ryzhik. In Gradshteyn and Ryzhik, integrals originating from the book by Bierens de Haan are denoted by BI.\n\nNot all closed-form expressions have closed-form antiderivatives; this study forms the subject of differential Galois theory, which was initially developed by Joseph Liouville in the 1830s and 1840s, leading to Liouville's theorem which classifies which expressions have closed form antiderivatives. A simple example of a function without a closed form antiderivative is , whose antiderivative is (up to constants) the error function.\n\nSince 1968 there is the Risch algorithm for determining indefinite integrals that can be expressed in term of elementary functions, typically using a computer algebra system. Integrals that cannot be expressed using elementary functions can be manipulated symbolically using general functions such as the Meijer G-function.\n\nMore detail may be found on the following pages for the lists of integrals:\n\n\nGradshteyn, Ryzhik, Geronimus, Tseytlin, Jeffrey, Zwillinger, Moll's (GR) \"Table of Integrals, Series, and Products\" contains a large collection of results. An even larger, multivolume table is the \"Integrals and Series\" by Prudnikov, Brychkov, and Marichev (with volumes 1–3 listing integrals and series of elementary and special functions, volume 4–5 are tables of Laplace transforms). More compact collections can be found in e.g. Brychkov, Marichev, Prudnikov's \"Tables of Indefinite Integrals\", or as chapters in Zwillinger's \"CRC Standard Mathematical Tables and Formulae\" or Bronshtein and Semendyayev's \"Guide Book to Mathematics\", \"Handbook of Mathematics\" or \"Users' Guide to Mathematics\", and other mathematical handbooks.\n\nOther useful resources include Abramowitz and Stegun and the Bateman Manuscript Project. Both works contain many identities concerning specific integrals, which are organized with the most relevant topic instead of being collected into a separate table. Two volumes of the Bateman Manuscript are specific to integral transforms.\n\nThere are several web sites which have tables of integrals and integrals on demand. Wolfram Alpha can show results, and for some simpler expressions, also the intermediate steps of the integration. Wolfram Research also operates another online service, the Wolfram Mathematica Online Integrator.\n\n\"C\" is used for an arbitrary constant of integration that can only be determined if something about the value of the integral at some point is known. Thus each function has an infinite number of antiderivatives.\n\nThese formulas only state in another form the assertions in the table of derivatives.\n\nWhen there is a singularity in the function being integrated such that the antiderivative becomes undefined or at some point (the singularity), then \"C\" does not need to be the same on both sides of the singularity. The forms below normally assume the Cauchy principal value around a singularity in the value of \"C\" but this is not in general necessary. For instance in\n\nthere is a singularity at 0 and the antiderivative becomes infinite there. If the integral above were to be used to compute a definite integral between −1 and 1, one would get the wrong answer 0. This however is the Cauchy principal value of the integral around the singularity. If the integration is done in the complex plane the result depends on the path around the origin, in this case the singularity contributes −\"i\" when using a path above the origin and \"i\" for a path below the origin. A function on the real line could use a completely different value of \"C\" on either side of the origin as in:\n\nThe following function has a non-integrable singularity at 0 for :\n\nLet be a function which has at most one root on each interval on which it is defined, and an antiderivative of that is zero at each root of (such an antiderivative exists if and only if the condition on is satisfied), then\nwhere is the sign function, which takes the values −1, 0, 1 when is respectively negative, zero or positive. This gives the following formulas (where ):\n\nwhen formula_54 for some integer .\nwhen formula_56 for some integer .\nwhen formula_54 for some integer .\nwhen formula_56 for some integer .\n\nIf the function does not have any continuous antiderivative which takes the value zero at the zeros of (this is the case for the sine and the cosine functions), then is an antiderivative of on every interval on which is not zero, but may be discontinuous at the points where . For having a continuous antiderivative, one has thus to add a well chosen step function. If we also use the fact that the absolute values of sine and cosine are periodic with period , then we get:\n\nCi, Si: Trigonometric integrals, Ei: Exponential integral, li: Logarithmic integral function, erf: Error function\n\nThere are some functions whose antiderivatives \"cannot\" be expressed in closed form. However, the values of the definite integrals of some of these functions over some common intervals can be calculated. A few useful integrals are given below.\n\nIf the function has bounded variation on the interval , then the method of exhaustion provides a formula for the integral:\n\nThe \"sophomore's dream\":\n\nattributed to Johann Bernoulli.\n\n\n\n\n\n\n"}
{"id": "55563692", "url": "https://en.wikipedia.org/wiki?curid=55563692", "title": "Louise Archer (academic)", "text": "Louise Archer (academic)\n\nLouise Archer (born 1973) is Karl Mannheim Professor of Sociology of Education at the University College London Institute of Education.\n"}
{"id": "2108291", "url": "https://en.wikipedia.org/wiki?curid=2108291", "title": "Master of Mathematics", "text": "Master of Mathematics\n\nA Master of Mathematics (or MMath) degree is a specific integrated master's degree for courses in the field of mathematics.\n\nIn the United Kingdom, the MMath is the internationally recognized standard qualification after a four-year course in mathematics at a university.\nThe MMath programme was set up by most leading universities after the \"Neumann Report\" in 1992. It is classed as a level 7 qualification in the Frameworks of Higher Education Qualifications of UK Degree-Awarding Bodies. The UCAS course codes for the MMath degrees start at \"G100\" upwards, most courses taking the codes \"G101 - G104\".\n\nUniversities which offer MMath degrees include:\n\nThe Open University offered this degree until 2007.\n\nIn Canada, the MMath is a graduate degree offered by the University of Waterloo. The length of the MMath degree program is typically between one and two years, and consists of course work along with a research component. The first Waterloo MMath degrees were awarded in 1967.\n\n"}
{"id": "1312188", "url": "https://en.wikipedia.org/wiki?curid=1312188", "title": "Multistability", "text": "Multistability\n\nIn a dynamical system, multistability is the property of having multiple stable equilibrium points in the vector space spanned by the states in the system. By mathematical necessity, there must also be unstable equilibrium points between the stable points. Points that are stable in some dimensions and unstable in others are termed unstable, as is the case with the first three Lagrangian points.\n\nBistability is the special case with two stable equilibrium points. It is the simplest form of multistability, and can occur in systems with only one state variable, as it only takes a one-dimensional space to separate two points.\n\nNear an unstable equilibrium, any system will be sensitive to noise, initial conditions and system parameters,\nwhich can cause it to develop in one of multiple divergent directions. \nIn economics and social sciences, path dependence gives rise to divergent directions of development.\nSome path dependent processes are adequately described by multistability,\nby being initially sensitive to input, before reaching a stagnant state – \nfor example market share instability, which can develop into a stable monopoly for one of multiple possible vendors.\n\nIn vision science, multistable perception characterizes the wavering percepts that can be brought about by certain visually ambiguous pattern such as the Necker cube, monocular rivalry or binocular rivalry.\nThrough lateral inhibition, a pattern in which one image, when stimulated, inhibit the activity of neighboring images.\n\n"}
{"id": "44592", "url": "https://en.wikipedia.org/wiki?curid=44592", "title": "National parks of England and Wales", "text": "National parks of England and Wales\n\nThe national parks of England and Wales are areas of relatively undeveloped and scenic landscape that are designated under the National Parks and Access to the Countryside Act (2016). Despite their similar name, national parks in England and Wales are quite different from national parks in many other countries, which are usually owned and managed by the government as a protected community resource, and which do not usually include permanent human communities. In England and Wales, designation as a national park may include substantial settlements and human land uses which are often integral parts of the landscape, and land within a national park remains largely in private ownership.\n\nThere are currently thirteen national parks () in England and Wales. Each park is operated by its own national park authority, with two \"statutory purposes\":\n\nWhen national parks carry out these purposes they also have the duty to:\n\nAn estimated 110 million people visit the national parks of England and Wales each year. Recreation and tourism bring visitors and funds into the parks, to sustain their conservation efforts and support the local population through jobs and businesses. These visitors also bring problems, such as erosion and traffic congestion, and conflicts over the use of the parks' resources. Access to cultivated land is restricted to bridleways, public footpaths, and permissive paths, with most (but not all) uncultivated areas in England and Wales having right of access for walking under the Countryside and Rights of Way Act 2000.\n\nArchaeological evidence from prehistoric Britain shows that the areas now designated as national parks have been occupied by humans since the Stone Age, at least 5,000 years ago and in some cases much earlier.\n\nBefore the 19th century, relatively wild, remote areas were often seen simply as uncivilised and dangerous. In 1725 Daniel Defoe described the High Peak as \"the most desolate, wild and abandoned country in all England\". However, by the early 19th century, romantic poets such as Byron, Coleridge and Wordsworth wrote about the inspirational beauty of the \"untamed\" countryside. Wordsworth described the English Lake District as a \"sort of national property in which every man has a right and interest who has an eye to perceive and a heart to enjoy\" in 1810. This early vision, based in the Picturesque movement, took over a century, and much controversy, to take legal form in the UK with the National Parks and Access to the Countryside Act 1949.\n\nThe idea for a form of national parks was first proposed in the United States in the 1860s, where national parks were established to protect wilderness areas such as Yosemite. This model has been used in many other countries since, but not in the United Kingdom.\n\nAfter thousands of years of human integration into the landscape, Britain lacks any substantial areas of wilderness. Furthermore, those areas of natural beauty so cherished by the romantic poets were often only maintained and managed in their existing state by human activity, usually agriculture.\n\nBy the early 1930s, increasing public interest in the countryside, coupled with the growing and newly mobile urban population, was generating increasing friction between those seeking access to the countryside and landowners. Alongside of direct action trespasses, such as the mass trespass of Kinder Scout, several voluntary bodies took up the cause of public access in the political arena.\n\nIn 1931, Christopher Addison (later Lord Addison) chaired a government committee that proposed a 'National Park Authority' to choose areas for designation as national parks. A system of national reserves and nature sanctuaries was proposed:\n\nHowever, no further action was taken after the intervention of the 1931 General Election.\n\nThe voluntary Standing Committee on National Parks first met on 26 May 1936 to put the case to the government for national parks in the UK. After World War II, the Labour Party proposed the establishment of national parks as part of the post-war reconstruction of the UK. A report by John Dower, secretary of the Standing Committee on National Parks, to the Minister of Town and Country Planning in 1945 was followed in 1947 by a Government committee, this time chaired by Sir Arthur Hobhouse, which prepared legislation for national parks, and proposed twelve national parks. Sir Arthur had this to say on the criteria for designating suitable areas:\n\nThe essential requirements of a National Park are that it should have great natural beauty, a high value for open-air recreation and substantial continuous extent. Further, the distribution of selected areas should as far as practicable be such that at least one of them is quickly accessible from each of the main centres of population in England and Wales. Lastly there is merit in variety and with the wide diversity of landscape which is available in England and Wales, it would be wrong to confine the selection of National Parks to the more rugged areas of mountain and moorland, and to exclude other districts which, though of less outstanding grandeur and wildness, have their own distinctive beauty and a high recreational value.\n\nThe National Parks and Access to the Countryside Act 1949 was passed with all party support. The first ten national parks were designated as such in the 1950s under the Act in mostly poor-quality agricultural upland. Much of the land was still owned by individual landowners, often private estates, but there was also property owned by public bodies such as the Crown, or charities which allow and encourage access such as the National Trust. Accessibility from the cities was also considered important.\n\nOther areas were also considered: for example, parts of the coast of Cornwall were considered as a possible national park in the 1950s but were thought to be too disparate to form a single coherent national park and were eventually designated as an Area of Outstanding Natural Beauty (AONB) instead. The north Pennines were also considered for designation as a national park in the 1970s but the proposal was thought to be administratively too difficult because the area was administered by five different county councils.\n\nThe Broads in East Anglia are not in the strictest sense a national park, being run by a separately constituted Broads Authority set up by a special Act of Parliament in 1988 and with a structure in which conservation is subordinate to navigational concerns (see Sandford Principle below), but it is generally regarded as being \"equivalent to\" a national park.\n\nThe New Forest, which includes the largest remaining tracts of unenclosed pasture land, heathland and old-growth forest in the heavily populated south east of the country was designated as a national park on 1 March 2005.\n\nOn 31 March 2009, Environment Secretary Hilary Benn announced that the South Downs would be designated a national park. The South Downs National Park came into effect on 31 March 2010.\n\nFollowing the Environment Act 1995, each national park has been managed by its own national park authority since April 1997. Previously, all but the Peak District and the Lake District were governed by the local county councils. The Peak District and the Lake District, the first two national parks to be designated, were under the control of planning boards that were independent of the local county councils.\n\nEach authority is required to carry out two \"statutory purposes\":\nThese purposes can conflict: in such cases, under the \"Sandford Principle\", conservation comes first. This principle was given statutory force by section 62 of the Environment Act 1995, although there are no explicit provisions as to how wildlife is to be preserved. In pursuing these purposes, national park authorities also have a duty to foster the social and economic well-being of their local communities.\n\nSlightly over half the members of each national park authority are appointees from the principal local authorities covered by the park; the remainder are appointed by the Secretary of State for Environment, Food and Rural Affairs (in England) or the Welsh Ministers (in Wales), some to represent local parish or community councils, others selected to represent the \"national interest\". The Broads Authority also has members appointed by Natural England, Great Yarmouth Port Authority and the Environment Agency. The national park authorities and the Broads Authority are covered by similar regulatory controls to those applied to local councils.\n\nFunding for national parks is complex, but the full cost of each park authority is funded from central government funds. In the past this was partly paid for by local authorities, and refunded to them from the government to varying degrees. In 2003/2004, the park authorities received around £35.5 million of central government funding.\n\nNatural England is the statutory body responsible for designating new national parks in England, subject to approval by the Secretary of State; Natural Resources Wales designates new national parks in Wales, subject to approval by the Welsh Ministers. The Association of National Park Authorities exists to provide the park authorities with a single voice when dealing with government and its agencies. The Campaign for National Parks (formerly Council for National Parks) is a charity that works to protect and enhance the national parks of England and Wales.\n\nNational park authorities are the strategic and local planning authorities for their areas, so that the local district or unitary councils do not exercise planning control in an area covered by a national park. Consequently, they have to perform all the duties of a local planning authority.\n\nThey are responsible for maintaining the local development framework — the spatial planning guide for their area. They also grant planning consent for development, within the constraints of the Framework. This gives them very strong direct control over residential and industrial development, and the design of buildings and other structures; as well as strategic matters such as mineral extraction.\n\nThe national park authorities' planning powers vary only slightly from other authorities, but the policies and their interpretation are stricter than elsewhere. This is supported and encouraged by the government who regard:\n\nTourism is an important part of the economy of the regions which contain national parks. Through attractions, shops and accommodation, visitors provide an income and a livelihood to local employers and farmers. This income provides jobs for the park. For example, within the Peak District National Park the estimate in 2004 for visitor spending is £185 million, which supports over 3,400 jobs, representing 27% of total employment in the national park.\n\nThe national park authorities have two roles: to conserve and enhance the park, and to promote its use by visitors. These two objectives cause frequent conflicts between the needs of different groups of people. It is estimated that the national parks of England and Wales receive 110 million visitors each year. Most of the time it is possible to achieve both the original two purposes by good management. Occasionally a situation arises where access for the public is in direct conflict with conservation. Following the ethos of the Sandford Principle, the Environment Act 1995 sets down how a priority may be established between conservation and recreational use. Similar provision has been made for Scottish national parks.\n\nAlthough recreation and tourism brings many benefits to an area, it also brings a number of problems. The national funding offered to national park authorities is partly in recognition of the extra difficulties created in dealing with these conflicts.\n\nAt the beginning of 2005, some 9.3% of the area of England and Wales lay within national parks; the addition of South Downs and the New Forest would increase this proportion to 10.7%. The three Welsh national parks cover around 20% of the land area of Wales.\n\n\n"}
{"id": "32944207", "url": "https://en.wikipedia.org/wiki?curid=32944207", "title": "Nonius (device)", "text": "Nonius (device)\n\nNonius is a measuring tool used in navigation and astronomy named in honour of its inventor, Pedro Nunes (Latin: Petrus Nonius), a Portuguese author, mathematician and navigator. The nonius was created in 1542 as a system for taking finer measurements on circular instruments such as the astrolabe. The system was eventually adapted into the vernier scale in 1631 by the French mathematician Pierre Vernier.\n\nThe nonius was used to improve the astrolabe's accuracy. This consisted of a number of concentric circles traced on an instrument and dividing each successive one with one fewer divisions than the adjacent outer circle. On a standard scale of 90 degrees, there are an additional 44-45 concentric circles with each divided into a specific unit size such that a scale unit on position formula_1 had an arc of formula_2 degrees. Thus, the outermost quadrant would comprise 90° in 90 equal divisions, the next inner would have 89 divisions, the next 88 and so on. When an angle was measured, the circle and the division on which the alidade fell was noted. A table was then consulted to provide the exact measure.\n\nThe astronomer Tycho Brahe applied the nonius to the astronomic quadrant.\n\nIn numerically controlled machines, the nonius is part of several absolute encoders, that measure linear or rotational displacements.\n"}
{"id": "1333017", "url": "https://en.wikipedia.org/wiki?curid=1333017", "title": "Panayotis Varotsos", "text": "Panayotis Varotsos\n\nPanayiotis Varotsos (, born November 28, 1947 in Patras) is a Greek physicist and former professor in the Department of Physics of the University of Athens, notable for his VAN method to predict earthquakes. His group claims the ability to identify electromagnetic signals that are precursors to earthquakes. They suggest the precursors are generated by electricity from piezo-stimulated effects in rocks being stressed just prior to the earthquake rupture. Onassis Foundation Laureate for the Environment (1995). Also awarded by the Academy of Athens (1978) and Empeirikion Foundation (1986). In 2016 the Union of Greek Physicists honoured him for his work with a prize awarded by the President of Greece.\n\n\n"}
{"id": "59053373", "url": "https://en.wikipedia.org/wiki?curid=59053373", "title": "Rubinisphaera", "text": "Rubinisphaera\n\nRubinisphaera is a genus of bacteria from the family of Planctomycetaceae with one known species (\"Rubinisphaera brasiliensisa\"). \"Rubinisphaera brasiliensis\" has been isolated from water from the Lagoa Vermelha from Brazil.\n"}
{"id": "11805204", "url": "https://en.wikipedia.org/wiki?curid=11805204", "title": "S.P.I.N. Technology", "text": "S.P.I.N. Technology\n\nS.P.I.N. (Super-Power-Inhibiting Nanobots) Technology or S.P.I.N. Tech is a fictional type of nanotechnology appearing in American comic books published by Marvel Comics.\n\nS.P.I.N. Tech was first mentioned in \"\" #3, when War Machine described it to cadet Komodo in a mission briefing. During the earlier Civil War, the pro-registration side had created special technology to inhibit and/or remove the super powers of those who opposed registration. This was widely known and publicized. However, what wasn't known to the public was that they had managed to miniaturize the technology into nanobots. It could be fired in special adamantium-tipped ammunition which could pierce the skin of any known individual, and thus render them unable to use their powers, causing them to no longer be a threat. Iron Man successfully used S.P.I.N. Tech to strip She-Hulk of her powers when she learned of her cousin Bruce Banner's fate at the hands of the Illuminati and vowed to punish those responsible. In the first known field mission where S.P.I.N. Tech is used, the villains Shocker, Boomerang and Hydro-Man were all successfully hit with it, though as it was not targeted to their DNA, they were not affected. Spider-Man (who was the target of the mission) managed to clog one of War Machine's guns with his webbing, and the nanomachines entered his suit's systems and shut them down, which would seem to indicate that this technology can be used against other technological devices as well. However, their effects seem to be temporary; War Machine recovered soon afterwards.\n\nIron Man later used S.P.I.N. Tech against Hulk when he returned to Earth to seek revenge on the Illuminati, but at Stark's own admission there was no telling how long the technology would keep the Hulk weakened, which would seem to further indicate that S.P.I.N. Tech is not permanent, or at least not against individuals whose power levels can increase, such as Hulk. It was, however, later revealed that the container that Stark had with him to infect the Hulk was actually empty, having previously been stolen by Initiative recruit Hardball. Amadeus Cho also managed to help She-Hulk regain her powers after they had been stripped, which implies that S.P.I.N. Tech may be able to be deactivated even after initial exposure; this is confirmed when She-Hulk reveals that Stark temporarily disabled the nanobots in her body to allow her to aid in the defense against the Hulk's return.\n\nIt is also mentioned that all the nanomachines used in S.P.I.N. Tech can be targeted to specific individuals, negating any chances of friendly fire. Though in use by S.H.I.E.L.D., the S.P.I.N. Tech has gotten into the hands of several villains and terrorist groups when it was revealed that the man who hired Hardball to steal the S.P.I.N. Tech, Congressman Woodman, is in fact a high-ranking member of HYDRA. During an encounter in Madripoor between HYDRA agents and members of the Shadow Initiative, HYDRA employs modified S.P.I.N. Tech that their agents claim is effective against any superhuman target, and uses it to negate Komodo's abilities. However, when the Shadow Initiative infiltrated their headquarters, Hardball had destroyed all the S.P.I.N. Tech, except for his own, which was stolen by Carmilla Black.\n\nWhen a group of super villains used the technology to inhibit one of Hank Pym's drug-created superheroes, Mulholland, the removal of this former mutant's drug-made powers somehow allowed her true mutant powers to resurface.\n"}
{"id": "11051153", "url": "https://en.wikipedia.org/wiki?curid=11051153", "title": "Sensu", "text": "Sensu\n\nSensu is a Latin word meaning \"in the sense of\". It is used in a number of fields including biology, geology, linguistics, semiotics, and law. Commonly it refers to how strictly or loosely an expression is used in describing any particular concept, but it also appears in expressions that indicate the convention or context of the usage.\n\n\"Sensu\" is the ablative case of the noun \"sensus\", here meaning \"sense\". It is often accompanied by an adjective (in the same case). Three such phrases are:\n\nSøren Kierkegaard uses the phrase \"sensu eminenti\" to mean \"in the pre-eminent [or most important or significant] sense\".\n\nWhen appropriate, comparative and superlative adjectives may also be used to convey the meaning of \"more\" or \"most\". Thus \"sensu stricto\" becomes \"sensu strictiore\" (\"in the stricter sense\" or \"more strictly speaking\") and \"sensu strictissimo\" (\"in the strictest sense\" or \"most strictly speaking\").\n\nCurrent definitions of the plant kingdom (Plantae) offer a biological example of when such phrases might be used. One definition of Plantae is that it consists of all green plants, all red algae and all glaucophyte algae. A stricter definition excludes the red and glaucophyte algae; the group defined in this way could be called Plantae \"sensu stricto\". An even stricter definition excludes green algae, leaving only land plants; the group defined in this way could be called Plantae \"sensu strictissimo\". Conversely, where convenient, some authors derive expressions such as \"sensu non strictissimo\", meaning \"not in the narrowest possible sense\".\n\nA similar form is in use to indicate the sense of a particular context, such as \"Nonmonophyletic groups are ... nonnatural (sensu cladistics) in that...\" or \"...computation of a cladogram (sensu phenetics)...\"\n\nAlso the expression \"sensu auctorum\" (abbreviation: \"sensu auct.\") is used to mean \"in the sense of certain authors\", who can be designated or described. It normally refers to a sense which is considered invalid and may be used in place of the author designation of a taxon in such a case (for instance, \"\"Tricholoma amethystinum\" sensu auct.\" is an erroneous name for a mushroom which should really be \"\"Lepista personata\" (Fr.) Cooke\").\n\nA related usage is in a concept-author citation (\"\"sec.\" Smith\", or \"\"sensu\" Smith\"), indicating that the intended meaning is the one defined by that author. (Here \"\"sec\".\" is an abbreviation of \"secundum\", meaning \"following\" or \"in accordance with\".) Such an author citation is different from the citation of the nomenclatural \"author citation\" or \"authority citation\". In biological taxonomy the author citation following the name of a taxon simply identifies the author who originally published the name and applied it to the type, the specimen or specimens that one refers to in case of doubt about the definition of a species. Given that an author (such as Linnaeus, for example) was the first to supply a definite type specimen and to describe it, it is to be hoped that his description would stand the tests of time and criticism, but even if it does not, then as far as practical the name that he had assigned will apply. It still will apply in preference to any subsequent names or descriptions that anyone proposes, whether his description was correct or not, and whether he had correctly identified its biological affinities or not. This does not always happen of course; all sorts of errors occur in practice. For example, a collector might scoop a netful of small fish and describe them as a new species; it then might turn out that he had failed to notice that there were several (possibly unrelated) species in the net. It then is not clear what he had named, so his name can hardly be taken seriously, either \"s.s. or s.l\".\n\nAfter a species has been established in this manner, specialist taxonomists may work on the subject and make certain types of changes in the light of new information. In modern practice it is greatly preferred that the collector of the specimens immediately passes them to specialists for naming; it is rarely possible for non-specialists to tell whether their specimens are of new species or not, and in modern times not many publications or their referees would accept an amateur description.\n\nIn any event, the person who finally classifies and describes a species has the task of taxonomic circumscription. Circumscription means in essence that anyone competent in the matter can tell \"which creatures are included in the species described, and which are excluded\". It is in this process of species description that the question of the sense arises, because that is where the worker produces and argues his view of the proper circumscription. Equally, or perhaps even more strongly, the arguments for deciding questions concerning higher taxa such as families or orders, require very difficult circumscription, where changing the \"sense\" applied could totally upset an entire scheme of classification, either constructively or disastrously.\n\nNote that the principles of circumscription apply in various ways in non-biological senses. In biological taxonomy the usual assumption is that circumscription reflects the shared ancestry perceived as most likely in the light of the currently available information; in geology or legal contexts far wider and more arbitrary ranges of logical circumscription commonly apply, not necessarily formally uniformly. However, the usage of expressions incorporating \"sensu\" remains functionally similarly intelligible among the fields. In geology for example, in which the concept of ancestry is looser and less pervasive than in biology, one finds usages such as:\n\nReaders unfamiliar with technical aspects of taxonomy might find it helpful first to think of everyday examples of the principles. When dealing with groups and parts of groups (subgroups) of different types of things, taxonomists sometimes wish to speak of the full set under consideration, and sometimes just a subset, but almost always want to refer to some particular part, to the exclusion of other elements; in issuing an instruction to poll the opinions of twenty-one members of a village community, a competent pollster would not accept the reactions of two heads of households, three infants, four dogs, five cats, six rats, and a tramcar. That would be taking \"sensu lato\" beyond good sense.\n\nInstead the instruction should specify which \"sense\" should apply, such as \"sensu stricto\" (or \"strictiore\"):\n\nor \"sensu lato\" (or \"latiore\"): \n\nThe important thing is that in each example the instruction circumscribed the appropriate subjects; that means that the interviewer could tell \"which people were wanted\" and correspondingly, \"which were to be left out\".\nThe circumscription could be in terms of very specific criteria:\n\nor the criterion could be very casual, even vague: \n\nHowever simple that may sound, it is fundamental both in formal science and in everyday affairs. Circumscription amounts to the basis for telling things apart, which in turn is the rational basis for all diagnoses, formal or informal.\n\nIn biological taxonomy, as the next section describes, the same principles apply, but they deal in various ways with circumscribing living things according to any relevant criterion. In modern biology the criterion usually has something to do with which creature descended from which kind of ancestor, in which ways it changed in the process, and by how much. However, in more general taxonomies, although the principles of circumscription are fundamentally similar, the criteria could be largely different in type as well as in detail.\n\nIn short, in every discipline the sense of circumscription in taxonomy must reflect the nature of the subject matter.\n\nSensu is used in the taxonomy of living creatures to specify which circumscription of a given taxon is meant, where more than one circumscription can be defined.\n\nExamples:\n\n\n\n\n"}
{"id": "777268", "url": "https://en.wikipedia.org/wiki?curid=777268", "title": "Shaun M. Hughes", "text": "Shaun M. Hughes\n\nShaun M. Hughes was an Australian astronomer at Siding Spring Observatory.\n\nHe co-discovered the periodic comet 130P/McNaught-Hughes.\n\nShaun Hughes left Siding Spring Observatory in 1992 for a fellowship at the California Institute of Technology, where he joined one of the Hubble Space Telescope Key Project teams, to measure the expansion rate of the universe, also known as the Hubble constant. This was achieved by observing Cepheid variable stars to measure distances to about 20 galaxies, then using these distances to tie together various other methods for measuring distances to thousands of galaxies.\n\nHe also pursued his research on Mira variables, also known as Long period variable stars. These are stars that are similar mass as the sun, only older, after they have become red giants, just prior to becoming planetary nebulae.\n\nIn 1994 he joined the Royal Greenwich Observatory in Cambridge, England, where he continued his research and supported UK astronomers observe at the La Palma Observatory.\n\nIn 1998 the UK government, on the advice of some university astronomers, decided to close the Royal Greenwich Observatory. He then had a choice of moving his family back to the United States and stay working in astronomy, or change careers and stay in Cambridge. He chose the latter, and is now a business analyst with Convergys.\n\n\n\n"}
{"id": "9775656", "url": "https://en.wikipedia.org/wiki?curid=9775656", "title": "Social Problems", "text": "Social Problems\n\nSocial Problems is the official publication of the Society for the Study of Social Problems. It is a quarterly peer-reviewed journal published by Oxford University Press since 2015 and formerly published by University of California Press. It was established in 1953. Some of the areas covered by the journal include: conflict, social action, and change; crime and juvenile delinquency; drinking and drugs; health, health policy, and health services; mental health; poverty, class, and inequality; racial and ethnic minorities; sexual behavior, politics, and communities; and youth, aging, and the life course. The journal is co-edited by Annulla Linders and Earl Wright II (University of Cincinnati).\n\n\"Social Problems\" is abstracted and indexed in the Social Sciences Citation Index. According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 2.071, ranking it 30th out of 146 journals in the category \"Sociology\".\n\n"}
{"id": "2626554", "url": "https://en.wikipedia.org/wiki?curid=2626554", "title": "Social studies of finance", "text": "Social studies of finance\n\nSocial studies of finance is an interdisciplinary research area that combines perspectives from anthropology, economic sociology, science and technology studies, international political economy, behavioral finance, cultural studies and/or economics in the study of financial markets. Work in social studies of finance emphasises the social and cultural dimensions of financial activities, but focuses also on technical and economic dimensions such as pricing and trading.\n\nFinancial markets have been an object for sociological inquiry since, at least, Max Weber’s \"Die Börse\". The raise of quantitative financial theory in financial economics from the 1950s onwards has led to an academic specialization on financial markets rather focused on economic modeling, and poorly attentive to sociological aspects. In the 1980s, a number of economic sociologists developed empirical investigation on the social structure and cultural characteristics of financial markets, especially in the US. Such pioneering researcher included contributions from Wayne E. Baker, Mitchel Y. Abolafia and Charles W. Smith, and was based on methods such as ethnographic observation or social network analysis. In the 1990s, a number of researchers from the field of science and technology studies such as Karin Knorr-Cetina and Donald A. MacKenzie started also developing empirical research in this area, with close attention to the role of expert knowledge and technology in financial activities.\n\nResearch topics in social studies of finance include the cultural world and work habits of traders and other professionals in financial markets, the globalization and regulation of financial services, the processes of innovation in the financial industry and the problems of risk and uncertainty that characterize such processes.\n\n\n"}
{"id": "5584806", "url": "https://en.wikipedia.org/wiki?curid=5584806", "title": "Space (2001 TV series)", "text": "Space (2001 TV series)\n\nSpace (Hyperspace in the United States) is a 2001 BBC documentary which ran for six episodes covering a number of topics in relation to outer space. The series is hosted and narrated by actor Sam Neill.\n\n"}
{"id": "17387465", "url": "https://en.wikipedia.org/wiki?curid=17387465", "title": "Subject Network for Sociology, Anthropology, Politics", "text": "Subject Network for Sociology, Anthropology, Politics\n\nThe Subject Network for Sociology, Anthropology, Politics of the Higher Education Academy of the United Kingdom was established in 2000 to support and enhance learning and teaching in performing arts higher education across the UK. It is based at the University of Birmingham. The director is Professor Alasdair Blair.\n\n"}
{"id": "51857719", "url": "https://en.wikipedia.org/wiki?curid=51857719", "title": "Team Plan B", "text": "Team Plan B\n\nTeam Plan B is the only Canadian entry in the Google Lunar XPRIZE, a $30 million competition to land a spacecraft on the Moon, travel 500 metres and transmit high definition video back to earth before the contest deadline of December 2017. Team Plan B aims to be the First Privately Funded Canadian Mission to the Moon.\n\nThe Google Lunar XPRIZE entry was first submitted by current team CTO Alex Dobrianski in early 2011 right before the last days of the entry deadline. Before moving to Canada with his family, Alex wrote the emulator for the mainframe IBM 360 EC-1032 (cert. 38) for the Mozhaisky Space Acedamy. Team lead Alex emigrated from Ukraine in 1995, starting fresh in Canada with his family.\n\n\"In the Ukraine, Alex developed software. He made a sale to the Russian Space Agency before immigrating to Canada in 1995 with his wife, Luda, and their three children, settling in Vancouver. “With 700 bucks to start a new life,” Sergei says, his father found work fixing parking lot lighting. Because he was tall and didn’t require a ladder, he was able to work three times faster than his shorter co-workers, he says.\"\n\nSergei Dobrianski, Alex's son has stepped in as full-time CEO in 2015, adding to the unique family team dynamic with the goal to break perceptions of what is possible in the space industry.\n\nTeam Plan B has developed a procedure for the first use of 3D printing using Lunar Soil. The Team Plan B vision is to develop lunar 3D printing rapid manufacturing of metallurgy, parts and components, fuel composition or whatever the human mind can conceive to negate prohibitively expensive launch costs restrictive to earth launch payloads.\n\nCreated by Academy Award nominated director Orlando von Einsiedel, Executive Producer J.J. Abrams, Bad Robot and Epic Digital have joined forces with Google and the XPRIZE to create a documentary web series about the teams competing for the Google Lunar XPRIZE. Team Plan B was featured as one of the 9 episodes in the Google Lunar XPRIZE Moon Shot short documentary series.\n\nAn integral part of the first privately funded Canadian mission to the moon include Bryant McGill, Jenni Young, Susaye Greene, & BCIT.\n"}
{"id": "2834437", "url": "https://en.wikipedia.org/wiki?curid=2834437", "title": "The Why Why Family", "text": "The Why Why Family\n\nThe Why Why Family ( and also known as Saban's The Why Why Family) is a French cartoon television series for children, which originally aired in 1996, written by Annabelle Perrichon and François-Emmanuel Porché and produced by Saban International Paris. Later, in 1998, the show was broadcast in the United States by Fox. Character designs and comedy elements emulate vintage cartoons.\n\n\n"}
{"id": "9820207", "url": "https://en.wikipedia.org/wiki?curid=9820207", "title": "Theories for Everything", "text": "Theories for Everything\n\nTheories for Everything: An Illustrated History of Science, From the Invention of Numbers to String Theory is a book, published by the National Geographic Society in 2006.\n\nThe book details the history of science from its earliest beginnings to the latest discoveries. Available in two versions, Deluxe and Standard, it was originally offered to National Geographic Society members and subsequently in the society's online shop. The book was co-authored by Bruce Stutz, Andrea Gianopoulos, and John Langone to whom it was dedicated. Theories for Everything is subdivided into six chapters, each focusing on a specific area of science and highlighting the achievements of scientists in that discipline.\n\n\n"}
{"id": "11513122", "url": "https://en.wikipedia.org/wiki?curid=11513122", "title": "Vacuum Tower Telescope", "text": "Vacuum Tower Telescope\n\nThe Vacuum Tower Telescope is an evacuated-optics solar telescope located at the Teide Observatory on Tenerife in the Canary Islands. It is operated by the Kiepenheuer-Institut für Sonnenphysik (KIS). \n\nIt was built between 1983 and 1986, with first light in 1988. It has a 70-centimetre (28-inch) diameter primary mirror and a focal length of . Thanks to an adaptive optics system \"KAOS\" (\"K\"iepenheuer-institute \"A\"daptive \"O\"ptic \"S\"ystem), in operation since spring 2000, it is able to resolve details down to 0.2 arc seconds (150 km) on the Sun's surface.\n\n\n"}
{"id": "16589498", "url": "https://en.wikipedia.org/wiki?curid=16589498", "title": "Vector spherical harmonics", "text": "Vector spherical harmonics\n\nIn mathematics, vector spherical harmonics (VSH) are an extension of the scalar spherical harmonics for use with vector fields. The components of the VSH are complex-valued functions expressed in the spherical coordinate basis vectors.\n\nSeveral conventions have been used to define the VSH. \nWe follow that of Barrera \"et al.\". Given a scalar spherical harmonic , we define three VSH:\n\n\nwith formula_4 being the unit vector along the radial direction in spherical coordinates and formula_5 the vector along the radial direction with the same norm as the radius, i.e., formula_6. The radial factors are included to guarantee that the dimensions of the VSH are the same as those of the ordinary spherical harmonics and that the VSH do not depend on the radial spherical coordinate.\n\nThe interest of these new vector fields is to separate the radial dependence from the angular one when using spherical coordinates, so that a vector field admits a multipole expansion\n\nThe labels on the components reflect that formula_8 is the radial component of the vector field, while formula_9 and formula_10 are transverse components (w.r.to the radius vector formula_5).\n\nLike the scalar spherical harmonics, the VSH satisfy\n\nwhich cuts the number of independent functions roughly in half. The star * indicates complex conjugate.\n\nThe VSH are orthogonal in the usual three-dimensional way at a point formula_5\n\nThey are also orthogonal in the Hilbert space\n\nAn additional result at a single point formula_5 (not reported in Barrera et al, 1985) is (for all formula_22)\n\nThe orthogonality relations allow one to compute the spherical multipole moments of a vector field as\n\nGiven the multipole expansion of a scalar field\n\nwe can express its gradient in terms of the VSH as\n\nFor any multipole field we have\n\nBy superposition we obtain the divergence of any vector field\n\nwe see that the component on is always solenoidal.\n\nFor any multipole field we have\n\nBy superposition we obtain the curl of any vector field\n\nAction of the Laplace operator formula_37 separates as follows\n\nwhere formula_39 and\n\nAlso note that this action becomes symmetric, i.e. the off-diagonal coefficients are equal to formula_43, for the properly normalized VSH.\n\n\n\n\nThe expression for negative values of m are obtained applying the symmetry relations.\n\nThe VSH are especially useful in the study of multipole radiation fields. For instance, a magnetic multipole is due to an oscillating current with angular frequency formula_65 and complex amplitude\n\nand the corresponding electric and magnetic fields can be written as\n\nSubstituting into Maxwell equations, Gauss' law is automatically satisfied\n\nwhile Faraday's law decouples in\n\nGauss' law for the magnetic field implies\n\nand Ampère-Maxwell's equation gives\n\nIn this way, the partial differential equations have been transformed into a set of ordinary differential equations.\n\nIn the calculation of the Stokes' law for the drag that a viscous fluid exerts on a small spherical particle, the velocity distribution obeys Navier-Stokes equations neglecting inertia, i.e.\n\nwith the boundary conditions\n\nbeing U the relative velocity of the particle to the fluid far from the particle. In spherical coordinates this velocity at infinity can be written as\n\nThe last expression suggest an expansion on spherical harmonics for the liquid velocity and the pressure\n\nSubstitution in the Navier-Stokes equations produces a set of ordinary differential equations for the coefficients.\n\n\n<div class=\"references\">\n\n"}
