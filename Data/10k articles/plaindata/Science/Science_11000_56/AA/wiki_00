{"id": "47777679", "url": "https://en.wikipedia.org/wiki?curid=47777679", "title": "1,2-dioleoyl-sn-glycerophosphoethanolamine", "text": "1,2-dioleoyl-sn-glycerophosphoethanolamine\n\n1,2-dioleoyl-sn-glycerophosphoethanolamine is a non-bilayer lipid that adopts non-lamellar reverse hexagonal structures.\n"}
{"id": "50208751", "url": "https://en.wikipedia.org/wiki?curid=50208751", "title": "Advocacy planning", "text": "Advocacy planning\n\nAdvocacy planning was formulated in the 1960s by Paul Davidoff. It is a pluralistic and inclusive planning theory where planners seek to represent the interests of various groups within society. Davidoff (1965) was an activist lawyer and planner who believed that advocacy planning was a necessary method for representing the low-income and minority groups who were not always on equal footing with the rich and powerful.\n\nBefore the 1960s, planning was usually performed by trained professionals within local governments who worked directly under the guidance of planning commissions. This method of planning is commonly referred to as top down and was a result of the planning practices that developed throughout history. Top down is characterised by its authoritative and undemocratic methods, where institutions and individuals plan without first consulting the various stakeholders who are involved with the use and development of the land. An example of this approach is colonialism in Africa during the early nineteenth century, where settlements were created simply for the purpose of exploiting workers and extracting the wealth produced by them.\n\nThis attitude to planning continued beyond colonialism and spread throughout the modernist movement in the 1920s. During this time, designers and planners were given opportunities to conceive a vision for utopian cities. These designs were a response to the rise of industrialisation in cities, which led to the working class living in dirty and often overcrowded slums. Although the vision and intention of these utopian cities was to create a society that sought to protect and preserve humanity through the built environment, this top down method of planning assumes that the values and beliefs held by the planner are the same as those that they are planning for. Many of these projects failed to achieve expectations and were instead discarded or set aside.\n\nDirectly following modernism and continuing this trend of a top down approach was the post world war period of reconstruction and planning. With the war over, there was a need for social and economic reconstruction. Governments were given the task to rebuild cities that had been afflicted by the damage left behind from the war. With the rise of the technocratic experts, they were consulted to design and plan the city in a scientific, logical and rigorous manner which would produce the best outcome for all stakeholders. This disconnected and elitist approach led to the constant failure of the government to meet the needs of its citizens and was met with backlash, giving rise to alternative planning practices.\n\nDavidoff understood that not all stakeholders are equally represented and involved in the planning process. Leaving the groups of lower socioeconomic status vulnerable to the interests of larger public institutions or private companies. Without sufficient protection and care, the concerns and opinions of these individuals were left unheard and unaccounted for when developing plans. Davidoff realised that it was necessary to implement a “humanistic, grassroots and pluralistic” system where planners would advocate for the interests of the oppressed and powerless.\n\nIn practice advocacy planners use their experience and knowledge within the field of planning to represent the ideas and needs of their clients. These clients are often groups of lower socioeconomic standing who are unable to access the resources, tools or skills to represent themselves. Advocate planners work with these disadvantaged groups to develop plans which incorporate and preserve their social and economic needs.\n\nThe plans are then produced in front of a planning commission where they consider the various pros and cons of each plan produced by other advocate planners. This is what Davidoff claimed would lead to a rigorous and systematic legal methodology of “fair notice and hearings, production of supporting evidence, cross-examination and reasoned decision” Allowing the planning commission to arrive at “a just decision”.\n\nDavidoff believed that upholding the political ideology of democracy through the planning process led to three major improvements within his discipline.\n\nFirst is the raising of the public’s awareness. By employing a method of participatory planning and engaging with the wider community, this helps the public to realise that planning is not simply a process engaged by well educated men of science, rather that the best planners are the people themselves. It is the realisation that the public has the freedom and choice to develop plans according to their needs.\n\nSecondly, this ideal structure for advocacy planning allows planners to compete among themselves while representing the views of their clients. Healthy amounts of competition should in fact raise the standard and quality of planning practices and outcomes. Davidoff acknowledges that “conflict keeps people honest”\n\nFinally, rather than critiquing the planners and institutions that support them, those that are critical are given the opportunity to instead provide input and feedback on the plans with which they disagree. Creating an environment that encourages positive attitudes towards constructive participation.\n\nHowever, simply providing a platform for expression is not always sufficient, as participatory and democratic planning requires a certain level of critical consciousness from the individual participating. Otherwise the participant may struggle to identify problems without being aware of the larger social and economic forces that influences their choices. The role of the advocate then, must be not only to provide assistance in developing an appropriate plan for the committee to judge, but must also be to encourage the people to be “free, informed, participating to the fullest degree, working together cooperatively, possessed of an understanding of their problems and those of their fellow men”\n"}
{"id": "680069", "url": "https://en.wikipedia.org/wiki?curid=680069", "title": "Ambilineality", "text": "Ambilineality\n\nAmbilineality is a cognatic descent system in which individuals may be affiliated either to their father's or mother's group. This type of descent results in descent groups which are non-unilineal in the sense that descent passes either through women or men, contrary to unilineal systems, whether patrilineal or matrilineal. Affiliation to a descent group will be determined either by choice or by residence. In the latter case, children will belong to the descent group with whom their parents are living.\n\nSocieties practicing ambilineal descent are especially common in Southeast Asia and the Pacific. Polynesian cultures and Micronesian cultures are often ambilineal, including, for example, Samoans, Māori, Hawaiians and people of the Gilbert Islands. The indigenous peoples of Northwestern North America are also followers of ambilineality; and it also found among the Southern Yoruba people residing in West Africa.\n\n"}
{"id": "5517979", "url": "https://en.wikipedia.org/wiki?curid=5517979", "title": "Ares I", "text": "Ares I\n\nAres I was the crew launch vehicle that was being developed by NASA as part of the Constellation program. The name \"Ares\" refers to the Greek deity Ares, who is identified with the Roman god Mars. Ares I was originally known as the \"Crew Launch Vehicle\" (CLV).\n\nNASA planned to use Ares I to launch \"Orion\", the spacecraft intended for NASA human spaceflight missions after the Space Shuttle was retired in 2011. Ares I was to complement the larger, unmanned Ares V, which was the cargo launch vehicle for Constellation. NASA selected the Ares designs for their anticipated overall safety, reliability and cost-effectiveness. However, the Constellation program, including Ares I was cancelled by U.S. president Barack Obama in October 2010 with the passage of his 2010 NASA authorization bill. In September 2011, NASA detailed the Space Launch System as its new vehicle for human exploration beyond Earth's orbit.\n\nIn 1995 Lockheed Martin produced an Advanced Transportation System Studies (ATSS) report for the Marshall Space Flight Center. A section of the ATSS report describes several possible vehicles much like the Ares I design, with liquid rocket second stages stacked above segmented solid rocket booster (SRB) first stages. The variants that were considered included both the J-2S engines and Space Shuttle Main Engines (SSMEs) for the second stage. The variants also assumed use of the Advanced Solid Rocket Motor (ASRM) as a first stage, but the ASRM was cancelled in 1993 due to significant cost overruns.\n\nPresident George W. Bush had announced the Vision for Space Exploration in January 2004, and NASA under Sean O'Keefe had solicited plans for a Crew Exploration Vehicle from multiple bidders, with the plan for having two competing teams. These plans were discarded by incoming administrator Michael Griffin, and on April 29, 2005, NASA chartered the Exploration Systems Architecture Study to accomplish specific goals:\n\nA Shuttle-derived launch architecture was selected by NASA for the Ares I. Originally, the crewed vehicle would have used a four-segment solid rocket booster (SRB) for the first stage, and a simplified Space Shuttle Main Engine (SSME) for the second stage. An unmanned version was to use a five-segment booster with the same second stage. Shortly after the initial design was approved, additional tests revealed that the Orion spacecraft would be too heavy for the four-segment booster to lift, and in January 2006 NASA announced they would slightly reduce the size of the Orion spacecraft, add a fifth segment to the solid-rocket first stage, and replace the single SSME with the Apollo-derived J-2X motor. While the change from a four-segment first stage to a five-segment version would allow NASA to construct virtually identical motors, the main reason for the change to the five-segment booster was the move to the J-2X.\n\nThe Exploration Systems Architecture Study concluded that the cost and safety of the Ares was superior to that of either of the Evolved Expendable Launch Vehicle (EELVs). The cost estimates in the study were based on the assumption that new launch pads would be needed for human-rated EELVs. The facilities for the current EELVs (LC-37 for Delta IV, LC-41 for Atlas V) are in place and could be modified, but this may not have been the most cost effective solution as LC-37 is a contractor owned and operated (COGO) facility and modifications for the Delta IV H were determined to be similar to those required for Ares I. The ESAS launch safety estimates for the Ares were based on the Space Shuttle, despite the differences, and included only launches after the post-Challenger Space Shuttle redesign. The estimate counted each Shuttle launch as two safe launches of the Ares booster. The safety of the Atlas V and Delta IV was estimated from the failure rates of all Delta II, Atlas-Centaur, and Titan launches since 1992, although they are not similar designs.\n\nIn May 2009 the previously withheld appendices to the 2006 ESAS study were leaked, revealing a number of apparent flaws in the study, which gave safety exemptions to the selected Ares I design while using a model which penalized the EELV-based designs.\n\nAres I was the crew launch component of the Constellation program. Originally named the \"Crew Launch Vehicle\" or CLV, the Ares name was chosen from the Greek deity Ares. Unlike the Space Shuttle, where both crew and cargo were launched simultaneously on the same rocket, the plans for Project Constellation outlined having two separate launch vehicles, the Ares I and the Ares V, for crew and cargo, respectively. Having two separate launch vehicles allows for more specialized designs for the crew and heavy cargo launch rockets.\n\nThe Ares I rocket was specifically being designed to launch the Orion Multi-Purpose Crew Vehicle. Orion was intended as a crew capsule, similar in design to the Apollo program capsule, to transport astronauts to the International Space Station, the Moon, and eventually Mars. Ares I might have also delivered some (limited) resources to orbit, including supplies for the International Space Station or subsequent delivery to the planned lunar base.\n\nNASA selected Alliant Techsystems, the builder of the Space Shuttle Solid Rocket Boosters, as the prime contractor for the Ares I first stage. NASA announced that Rocketdyne would be the main subcontractor for the J-2X rocket engine on July 16, 2007. NASA selected Boeing to provide and install the avionics for the Ares I rocket on December 12, 2007.\n\nOn August 28, 2007 NASA awarded the Ares I Upper Stage manufacturing contract to Boeing. Boeing built the S-IC stage of the Saturn V rocket at Michoud Aerospace Factory in the 1960s. The upper stage of Ares I was to have been built at the same rocket factory used for the Space Shuttle's External Tank and the Saturn V's S-IC first stage.\n\nAt approximately US$20–25 million per engine, the Rocketdyne-designed and produced J-2X would have cost less than half as much as the more complex Space Shuttle Main Engine (around $55 million). Unlike the Space Shuttle Main Engine, which was designed to start on the ground, the J-2X was designed from inception to be started in both mid-air and in near-vacuum. This air-start capability was critical, especially in the original J-2 engine used on the Saturn V's S-IVB stage, to propel the Apollo spacecraft to the Moon. The Space Shuttle Main Engine, on the other hand, would have required extensive modifications to add an air-start capability\n\nOn January 4, 2007, NASA announced that the Ares I had completed its system requirements review, the first such review completed for any manned spacecraft design since the Space Shuttle. This review was the first major milestone in the design process, and was intended to ensure that the Ares I launch system met all the requirements necessary for the Constellation Program. In addition to the release of the review, NASA also announced that a redesign in the tank hardware was made. Instead of separate LH and LO tanks, separated by an \"intertank\" like that of the Space Shuttle External Tank, the new LH and LOX tanks would have been separated by a common bulkhead like that employed on the Saturn V S-II and S-IVB stages. This would have provided a significant mass saving and eliminated the need to design a second stage interstage unit that would have had to carry the weight of the Orion spacecraft with it.\n\nIn January 2008, NASA Watch revealed that the first stage solid rocket of the \"Ares I\" could have created high vibrations during the first few minutes of ascent. The vibrations would have been caused by thrust oscillations inside the first stage. NASA officials had identified the potential problem at the Ares I system design review in late October 2007, stating in a press release that it wanted to solve it by March 2008. NASA admitted that this problem was very severe, rating it four out of five on a risk scale, but the agency was very confident in solving it. The mitigation approach developed by the Ares engineering team included active and passive vibration damping, adding an active tuned-mass absorber and a passive \"compliance structure\" – essentially a spring-loaded ring that would have detuned the Ares I stack. NASA also pointed out that, since this would have been a new launch system, like the Apollo or Space Shuttle systems, it was normal for such problems to arise during the development stage. According to NASA, analysis of the data and telemetry from the Ares I-X flight showed that vibrations from thrust oscillation were within the normal range for a Space Shuttle flight.\n\nA study released in July 2009 by the 45th Space Wing of the US Air Force concluded that an abort 30–60 seconds after launch would have a ~100% chance of killing all crew, due to the capsule being engulfed until ground impact by a cloud of solid propellant fragments, which would melt the capsule's nylon parachute material. NASA's study showed the crew capsule would have flown beyond the more severe danger.\n\nThe Ares I igniter was an advanced version of the flight-proven igniter used on the Space Shuttle's solid rocket boosters. It was approximately 18 inches (46 cm) in diameter and 36 inches (91 cm) long, and took advantage of upgraded insulation materials that had improved thermal properties to protect the igniter's case from the burning solid propellant. NASA successfully completed test firing of the igniter for the Ares I engines on March 10, 2009 at ATK Launch Systems test facilities near Promontory, Utah. The igniter test generated a flame 200 feet (60 meters) in length, and preliminary data showed the igniter performed as planned.\n\nDevelopment of the Ares I propulsion elements continued to make strong progress. On September 10, 2009, the first Ares I development motor (DM-1) was successfully tested in a full-scale, full-duration test firing. This test was followed by two more development motor tests, DM-2 on August 31, 2010 and DM-3 on September 8, 2011. For DM-2 the motor was cooled to a core temperature of 40 degrees Fahrenheit (4 degrees Celsius), and for DM-3 it was heated to above 90 degrees Fahrenheit (32 degrees Celsius). In addition to other objectives, these two tests validated Ares motor performance at extreme temperatures. NASA conducted a successful 500-second test firing of the J-2X rocket engine at John C. Stennis Space Center in November 2011.\n\nThe Ares I prototype, Ares I-X, successfully completed a test launch on October 28, 2009. Launch Pad 39B was damaged more than with a Space Shuttle launch. During descent, one of the three parachutes of the Ares I-X’s first stage failed to open, and another opened only partially, causing the booster to splash down harder and suffer structural damage. The launch accomplished all primary test objectives.\n\nNASA completed the Ares I system requirements review in January 2007. Project design was to have continued through the end of 2009, with development and qualification testing running concurrently through 2012. , flight articles were to have begun production towards the end of 2009 for a first launch in June 2011. Since 2006 the first launch of a human was planned for no later than 2014, which is four years after the planned retirement of the Space Shuttle.\n\nDelays in the Ares I development schedule due to budgetary pressures and unforeseen engineering and technical difficulties would have increased the gap between the end of the Space Shuttle program and the first operational flight of Ares I. Because the Constellation program was never allocated the funding originally projected, the total estimated cost to develop the Ares I through 2015 rose from $28 billion in 2006 to more than $40 billion in 2009. The Ares I-X project cost was $445 million.\n\nOriginally scheduled for first test flights in 2011, the independent analysis by the Augustine Commission found in late 2009 that due to technical and financial problems Ares I was not likely to have had its first crewed launch until 2017–2019 under the current budget, or late 2016 with an unconstrained budget. The Augustine Commission also stated that Ares I and Orion would have an estimated recurring cost of almost $1 billion per flight. However, later financial analysis in March 2010 showed that the Ares I would have cost $1 billion or more to operate per flight had the Ares I flown just once a year. If the Ares I system were flown multiple times a year the marginal costs could have fallen to as low as $138 million per launch. In December 2011, NASA administrator Charlie Bolden testified to congress that the Ares I would cost $4–4.5 billion a year, and $1.6 billion per flight. The Ares I marginal cost was predicted to have been a fraction of the Shuttle's marginal costs even had it flown multiple times per year. By comparison, the cost of launching three astronauts on a manned Russian Soyuz is $153 million.\n\nOn February 8, 2011 it was reported that Alliant Techsystems and Astrium proposed to use Ares I's first stage with the second stage from the Ariane 5 to form a new rocket named Liberty.\n\nOn February 1, 2010, President Barack Obama announced a proposal to cancel the Constellation program effective with the U.S. 2011 fiscal year budget, but later announced changes to the proposal in a major space policy speech at Kennedy Space Center on April 15, 2010. In October 2010, the NASA authorization bill for 2010 was signed into law which canceled Constellation. But previous legislation kept Constellation contracts in force until passage of a new funding bill for 2011.\n\nAres I had a payload capability in the 25-metric-ton (28-short-ton; 25-long-ton) class and was comparable to vehicles such as the Delta IV and the Atlas V. The NASA study group that selected what would become the Ares I rated the vehicle as almost twice as safe as an Atlas or Delta IV-derived design. The rocket was to have made use of an aluminum-lithium alloy which is lower in density but similar in strength compared to other aluminum alloys. The alloy is produced by Alcoa.\nThe first stage was to have been a more powerful and reusable solid fuel rocket derived from the Space Shuttle Solid Rocket Booster (SRB). Compared with the Solid Rocket Booster, which had four segments, the most notable difference was the addition of a fifth segment. This fifth segment would have enabled the Ares I to produce more thrust. Other changes made to the Solid Rocket Booster were to have been the removal of the Space Shuttle External Tank (ET) attachment points and the replacement of the Solid Rocket Booster nosecone with a new forward adapter that would have interfaced with the liquid-fueled second stage. The adapter was to have been equipped with solid-fueled separation motors to facilitate the disconnection of the stages during ascent. The grain design was also changed, and so were the insulation and liner. By the Ares I first stage ground test, the case, grain design, number of segments, insulation, liner, throat diameter, thermal protection systems and nozzle had all changed.\n\nThe upper stage, derived from the Shuttle's External Tank (ET) and based on the S-IVB stage of the Saturn V, was to be propelled by a single J-2X rocket engine fueled by liquid hydrogen (LH) and liquid oxygen (LOX). The J-2X was derived from the original J-2 engine used during the Apollo program, but with more thrust (~294,000 lbf) and fewer parts than the original engine. On July 16, 2007, NASA awarded Rocketdyne a sole-source contract for the J-2X engines to be used for ground and flight tests. Rocketdyne was the prime contractor for the original J-2 engines used in the Apollo program.\n\nAlthough its J-2X engine was derived from an established design, the upper stage itself would have been wholly new. Originally to have been based on both the internal and external structure of the ET, the original design called for separate fuel and oxidizer tanks, joined together by an \"intertank\" structure, and covered with the spray-on foam insulation to keep venting to a minimum. The only new hardware on the original ET-derived second stage would have been the thrust assembly for the J-2X engine, new fill/drain/vent disconnects for the fuel and oxidizer, and mounting interfaces for the solid-fueled first stage and the Orion spacecraft.\n\nUsing a concept going back to the Apollo program, the \"intertank\" structure was dropped to decrease mass, and in its place, a common bulkhead, similar to that used on both the S-II and S-IVB stages of the Saturn V, would have been used between the tanks. The savings from these changes were used to increase propellant capacity, which was . The spray-on foam insulation was the only part of the Shuttle's ET that would have been used on this new Saturn-derived upper stage.\n\n"}
{"id": "30965084", "url": "https://en.wikipedia.org/wiki?curid=30965084", "title": "Atílio Munari", "text": "Atílio Munari\n\nAtílio Munari (1901 – 19 October 1941) was born in Santa Maria, Rio Grande do Sul, Brazil.\n\nHe lived near the Sanga da Alemoa, and when he was 14 years old, he lived with the scientist H. Lotz, a German paleontologist, who taught him to collecting and preparing fossils. Many of the fossils collected by him, are now in Rio de Janeiro, Porto Alegre and Santa Maria. Helped a lot of paleontologists who visited the city of Santa Maria. Has contributed considerably to the Geopark of Paleorrota.\n\nIn his honor, the city of Santa Maria, received the Atilio Munari street. He was buried in \"São José Cemetery\", near where he was collecting fossils.\n\n"}
{"id": "1236458", "url": "https://en.wikipedia.org/wiki?curid=1236458", "title": "Bellman equation", "text": "Bellman equation\n\nA Bellman equation, named after Richard E. Bellman, is a necessary condition for optimality associated with the mathematical optimization method known as dynamic programming. It writes the value of a decision problem at a certain point in time in terms of the payoff from some initial choices and the value of the remaining decision problem that results from those initial choices. This breaks a dynamic optimization problem into a sequence of simpler subproblems, as Bellman's “principle of optimality” prescribes.\n\nThe Bellman equation was first applied to engineering control theory and to other topics in applied mathematics, and subsequently became an important tool in economic theory; though the basic concepts of dynamic programming are prefigured in John von Neumann and Oskar Morgenstern's \"Theory of Games and Economic Behavior\" and Abraham Wald's \"sequential analysis\".\n\nAlmost any problem that can be solved using optimal control theory can also be solved by analyzing the appropriate Bellman equation. However, the term 'Bellman equation' usually refers to the dynamic programming equation associated with discrete-time optimization problems. In continuous-time optimization problems, the analogous equation is a partial differential equation that is usually called the Hamilton–Jacobi–Bellman equation.\n\nTo understand the Bellman equation, several underlying concepts must be understood. First, any optimization problem has some objective: minimizing travel time, minimizing cost, maximizing profits, maximizing utility, etc. The mathematical function that describes this objective is called the \"objective function\".\n\nDynamic programming breaks a multi-period planning problem into simpler steps at different points in time. Therefore, it requires keeping track of how the decision situation is evolving over time. The information about the current situation that is needed to make a correct decision is called the \"state\". For example, to decide how much to consume and spend at each point in time, people would need to know (among other things) their initial wealth. Therefore, wealth formula_1 would be one of their \"state variables\", but there would probably be others.\n\nThe variables chosen at any given point in time are often called the \"control variables\". For example, given their current wealth, people might decide how much to consume now. Choosing the control variables now may be equivalent to choosing the next state; more generally, the next state is affected by other factors in addition to the current control. For example, in the simplest case, today's wealth (the state) and consumption (the control) might exactly determine tomorrow's wealth (the new state), though typically other factors will affect tomorrow's wealth too.\n\nThe dynamic programming approach describes the optimal plan by finding a rule that tells what the controls should be, given any possible value of the state. For example, if consumption (\"c\") depends \"only\" on wealth (\"W\"), we would seek a rule formula_2 that gives consumption as a function of wealth. Such a rule, determining the controls as a function of the states, is called a \"policy function\" (See Bellman, 1957, Ch. III.2).\n\nFinally, by definition, the optimal decision rule is the one that achieves the best possible value of the objective. For example, if someone chooses consumption, given wealth, in order to maximize happiness (assuming happiness \"H\" can be represented by a mathematical function, such as a utility function and is something defined by wealth), then each level of wealth will be associated with some highest possible level of happiness, formula_3. The best possible value of the objective, written as a function of the state, is called the \"value function\".\n\nRichard Bellman showed that a dynamic optimization problem in discrete time can be stated in a recursive, step-by-step form known as backward induction by writing down the relationship between the value function in one period and the value function in the next period. The relationship between these two value functions is called the \"Bellman equation\". In this approach, the optimal policy in the last time period is specified in advance as a function of the state variable's value at that time, and the resulting optimal value of the objective function is thus expressed in terms of that value of the state variable. Next, the next-to-last period's optimization involves maximizing the sum of that period's period-specific objective function and the optimal value of the future objective function, giving that period's optimal policy contingent upon the value of the state variable as of the next-to-last period decision. This logic continues recursively back in time, until the first period decision rule is derived, as a function of the initial state variable value, by optimizing the sum of the first-period-specific objective function and the value of the second period's value function, which gives the value for all the future periods. Thus, each period's decision is made by explicitly acknowledging that all future decisions will be optimally made.\n\nLet the state at time formula_4 be formula_5. For a decision that begins at time 0, we take as given the initial state formula_6. At any time, the set of possible actions depends on the current state; we can write this as formula_7, where the action formula_8 represents one or more control variables. We also assume that the state changes from formula_9 to a new state formula_10 when action formula_11 is taken, and that the current payoff from taking action formula_11 in state formula_9 is formula_14. Finally, we assume impatience, represented by a discount factor formula_15.\n\nUnder these assumptions, an infinite-horizon decision problem takes the following form:\n\nsubject to the constraints\n\nNotice that we have defined notation formula_18 to denote the optimal value that can be obtained by maximizing this objective function subject to the assumed constraints. This function is the \"value function\". It is a function of the initial state variable formula_6, since the best value obtainable depends on the initial situation.\n\nThe dynamic programming method breaks this decision problem into smaller subproblems. Richard Bellman's \"principle of optimality\" describes how to do this:Principle of Optimality: An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. (See Bellman, 1957, Chap. III.3.)\nIn computer science, a problem that can be broken apart like this is said to have optimal substructure. In the context of dynamic game theory, this principle is analogous to the concept of subgame perfect equilibrium, although what constitutes an optimal policy in this case is conditioned on the decision-maker's opponents choosing similarly optimal policies from their points of view.\n\nAs suggested by the \"principle of optimality\", we will consider the first decision separately, setting aside all future decisions (we will start afresh from time 1 with the new state formula_20). Collecting the future decisions in brackets on the right, the previous problem is equivalent to:\n\nsubject to the constraints\n\nHere we are choosing formula_23, knowing that our choice will cause the time 1 state to be formula_24. That new state will then affect the decision problem from time 1 on. The whole future decision problem appears inside the square brackets on the right.\n\nSo far it seems we have only made the problem uglier by separating today's decision from future decisions. But we can simplify by noticing that what is inside the square brackets on the right is \"the value\" of the time 1 decision problem, starting from state formula_24.\n\nTherefore, we can rewrite the problem as a recursive definition of the value function:\n\nThis is the Bellman equation. It can be simplified even further if we drop time subscripts and plug in the value of the next state:\n\nThe Bellman equation is classified as a functional equation, because solving it means finding the unknown function \"V\", which is the \"value function\". Recall that the value function describes the best possible value of the objective, as a function of the state \"x\". By calculating the value function, we will also find the function \"a\"(\"x\") that describes the optimal action as a function of the state; this is called the \"policy function\".\n\nIn the deterministic setting, other techniques besides dynamic programming can be used to tackle the above optimal control problem. However, the Bellman Equation is often the most convenient method of solving \"stochastic\" optimal control problems.\n\nFor a specific example from economics, consider an infinitely-lived consumer with initial wealth endowment formula_29 at period formula_30. He has an instantaneous utility function formula_31 where formula_32 denotes consumption and discounts the next period utility at a rate of formula_33. Assume what is not consumed in period formula_4 carries over next period with interest rate formula_35. Then the consumer's utility maximization problem is to choose a consumption plan formula_36\n\nand\n\nThe first constraint is the capital accumulation/law of motion specified by the problem, while the second constraint is a transversality condition that the consumer does not carry debt at the end of his life. The Bellman equation is\n\nAlternatively, one can treat the sequence problem directly using, for example, the Hamiltonian equations.\n\nNow, if the interest rate varies from period to period, the consumer is faced with a stochastic optimization problem. Let the interest \"r\" follow a Markov process with probability transition function formula_39 where formula_40 denotes the probability measure governing the distribution of interest rate next period if current interest rate is formula_35. The timing of the model is that the consumer decides his current period consumption after the current period interest rate is announced.\n\nRather than simply choosing a single sequence <math>\\\n"}
{"id": "37117778", "url": "https://en.wikipedia.org/wiki?curid=37117778", "title": "Biology and political science", "text": "Biology and political science\n\nThe interdisciplinary study of biology and political science is the application of theories and methods from the biology toward the scientific understanding of political behavior. The field is sometimes called biopolitics, a term that will be used in this article as a synonym although it has other, less related meanings. More generally, the field has also been called \"politics and the life sciences\".\n\nThe field can be said to originate with the 1968 manifesto of Albert Somit, \"Towards a more Biologically Oriented Political Science\", which appeared in the \"Midwest Journal of Political Science\". The term \"biopolitics\" was appropriated for this area of study by Thomas Thorton, who used it as the title of his 1970 book.\n\nThe Association for Politics and the Life Sciences was formed in 1981 and exists to study the field of biopolitics as a subfield of political science. APLS owns and publishes an academic peer-reviewed journal called \"Politics and the Life Sciences\" (PLS). The journal is edited in the United States at the University of Maryland, College Park’s School of Public Policy, in Maryland.\n\nBy the late 1990s and since, biopolitics research has expanded rapidly, especially in the areas of evolutionary theory, genetics, and neuroscience.\n\nThe historical link between biology and politics on the one hand, and sociological organicism on the other, is inescapable. The essential difference here is that the early modern application of biological ideas to politics revolved around the idea that society was a ‘social organism’, whereas the subject this article describes expressly sets out to separate the essential logic of the association of biology to human social life, from this earlier model. Hence the emphasis upon ‘politics’, denoting the primacy of the individual who engages in social life, as in political behaviour, underpinned by biological foundations. In this sense the rise of Biopolitics represents the replacement of sociological organicism that had been eradicated by the end of the Second World War, with an acceptable form of political organicism. Some discussion bearing on this point may be found in \"Biology and Politics : Recent Explorations\" by Albert Somit, 1976, which is a collection of essays, one brief essay by William Mackenzie is \"Biopolitics : A Minority Viewpoint\", in which he talks about the ‘founding father’ of Biopolitics as being Morley Roberts, because of his 1938 book of that name. But Roberts was not using the term in its modern, politically sanitized sense, but in the context of society viewed as a true living being, a social organism. And in a reply to Somit’s \"Towards a more Biologically Oriented Political Science\", published in the same journal, we find \"Some Questions about a More Biologically Oriented Political Science\" by Jerone Stephens, which sets out to warn against lurching back into the errors of previous venturers into the realms of biology and politics, as in sociological organicism.\n\nTopics addressed in political science from these perspectives include: public opinion and criminal justice attitudes, political ideology, (e.g. the correlates of biology and political orientation), origins of party systems, voting behavior, and warfare. Debates persist inside the field and out, regarding genetic and biological determinism. Important recent surveys of leading research in biopolitics have been published in the journals \"Political Psychology\" and \"Science\".\n\n\n"}
{"id": "29071162", "url": "https://en.wikipedia.org/wiki?curid=29071162", "title": "Blades Glacier", "text": "Blades Glacier\n\nBlades Glacier () is a glacier flowing east from the snow-covered saddle just north of La Gorce Peak, Alexandra Mountains. It merges with Dalton Glacier on the north side of Edward VII Peninsula. It was mapped by the United States Geological Survey from surveys and from U.S. Navy air photos, 1959–65, and named by the Advisory Committee on Antarctic Names for William Robert Blades who served as navigator during U.S. Navy Operation Highjump (1946–47) and Operation Deep Freeze (1955–59).\n\n"}
{"id": "13071937", "url": "https://en.wikipedia.org/wiki?curid=13071937", "title": "Brazilian disease", "text": "Brazilian disease\n\nBrazilian disease is a phrase in economics to describe the situation in which the Brazilian real has strengthened (trading at around R$1.95 to the US dollar) on high prices for commodities such as soybeans, making Brazilian exports of manufactured goods uncompetitive in foreign markets. The term was coined to compare the economic situation facing Brazilian exports in the late 2000s to Dutch disease, an older term referring to similar conditions faced in the Netherlands in the 1960s and 1970s due to massive natural gas exports. The term \"Brazilian disease\" has not been widely used in economic literature or the news media.\n\n"}
{"id": "33185681", "url": "https://en.wikipedia.org/wiki?curid=33185681", "title": "Centuriation", "text": "Centuriation\n\nCenturiation (in Latin \"centuriatio\" or, more usually, \"limitatio\"), also known as Roman grid, was a method of land measurement used by the Romans. In many cases land divisions based on the survey formed a field system, often referred to in modern times by the same name. According to O. A. W. Dilke, centuriation combined and developed features of land surveying present in Egypt, Etruria, Greek towns and Greek countryside.\n\nCenturiation is characterised by the regular layout of a square grid traced using surveyors' instruments. It may appear in the form of roads, canals and agricultural plots. In some cases these plots, when formed, were allocated to Roman army veterans in a new colony, but they might also be returned to the indigenous inhabitants, as at Orange (France).\n\nThe study of centuriation is very important for reconstructing landscape history in many former areas of the Roman empire.\n\nThe Romans began to use centuriation for the foundation, in the fourth century BCE, of new colonies in the \"ager Sabinus\", northeast of Rome. The development of the geometric and operational characteristics that were to become standard came with the founding of the Roman colonies in the Po valley, starting with \"Ariminum\" (Rimini) in 268 BCE.\n\nThe agrarian law introduced by Tiberius Gracchus in 133 BCE, which included the privatisation of the \"ager publicus\", gave a great impetus to land division through centuriation.\n\nCenturiation was used later for land reclamation and the foundation of new colonies as well as for the allocation of land to veterans of the many civil wars of the late Republic and early Empire, including the battle of Philippi in 42 BCE. This is mentioned by Virgil, in his Eclogues, when he complains explicitly about the allocation of his lands near Mantua to the soldiers who had participated in that battle.\n\nCenturiation was widely used throughout Italy and also in some provinces. For example, careful analysis has identified, in the area between Rome and Salerno, 80 different centuriation systems created at different times.\n\nVarious land division systems were used, but the most common was known as the \"ager centuriatus\" system.\n\nThe surveyor first identified a central viewpoint, the \"umbilicus agri\" or \"umbilicus soli\". He then took up his position there and, looking towards the west, defined the territory with the following names:\n\nHe then traced the grid using an instrument known as a groma, tracing two road axes perpendicular to each other:\n\n\nIt has been suggested that the Roman centuriation system inspired Thomas Jefferson's proposal to create a grid of townships for survey purposes, which ultimately led to the United States Public Land Survey System. The similarity of the two systems is empirically obvious in certain parts of Italy, for example, where traces of centuriation have remained.\n\nHowever, Thrower points out that, unlike the later US system, \"not all Roman centuriation displays consistent orientation\".\n\nThis is because, for practical reasons, the orientation of the axes did not always coincide with the four cardinal points and followed instead the orographic features of the area, also taking into account the slope of the land and the flow of rainwater along the drainage channels that were traced (centuriation of Florentia (Florence). In other cases, it was based on the orientation of existing lines of communication (centuriation along the Via Emilia) or other geomorphological features.\n\nCenturiation is typical of flat land, but centuriation systems have also been documented in hilly country.\n\nSometimes the \"umbilicus agri\" was located in a city or a castrum. This central point was generally referred to as \"groma\", from the name of the instrument used by the \"gromatici\" (surveyors).\n\nIn such cases, the grid was traced by extending the urban \"cardo maximus\" and the \"decumanus maximus\" through the gates of the city into the surrounding agricultural land.\n\nParallel secondary roads (\"limites quintarii\") were then traced on both sides of the initial axes at intervals of 100 \"actus\" (about 3.5 km). The territory was thus divided into square areas.\n\nThe road network density was then increased with other roads parallel to those already traced at a distance from each other of 20 \"actus\" (710.40 m). Each of the square areas – 20 × 20 \"actus\" – resulting from this further division was called a \"centuria\" or \"century\".\n\nThis dimension of the \"centuria\" became prevalent in the period when the large areas of the Po Valley were delimited, while smaller centuries of 10 × 10 \"actus\", as the name \"centuria\" suggests, had formerly been used.\n\nThe land was divided after the completion of the roads.\n\nEach century was divided into 10 strips, lying parallel to the \"cardo\" and the \"decumanus\", with a distance between them of 2 \"actus\" (71.04 m), thus forming 100 squares (\"heredia\") of about 0.5 hectares each: 100 \"heredia\" = 1 \"centuria\".\n\nEach \"heredium\" was divided in half along the north-south axis thus creating two \"jugera\": one \"jugerum\", from \"jugum\" (yoke), measured 2523 square metres, which was the amount of land that could be ploughed in one day by a pair of oxen.\n\nEven today, in some parts of Italy, the landscape of the plain is determined by the outcome of Roman centuriation, with the persistence of straight elements (roads, drainage canals, property divisions) which have survived territorial development and are often basic elements of urbanisation, at least until the twentieth century, when the human pressure of urban growth and infrastructures destroyed many of the traces scattered throughout the agricultural countryside.\n\n\n\n\n\nIn Italian:\nIn Catalan and Spanish:\nIn French:\n\nIn English:\n\n"}
{"id": "2795744", "url": "https://en.wikipedia.org/wiki?curid=2795744", "title": "Chemists Without Borders", "text": "Chemists Without Borders\n\nChemists Without Borders is a non-governmental organization involved in international development work designed to solve humanitarian problems through chemistry and related activities. As a public benefit, non-profit organization, the primary goals of Chemists Without Borders include, but are not limited to:\n\nChemists Without Borders was founded in 2004 by Bego Gerber and Steve Chambreau as a result of a letter that Gerber sent to the editor of \"Chemical and Engineering News\" in September 2004.\n\n\n"}
{"id": "445762", "url": "https://en.wikipedia.org/wiki?curid=445762", "title": "Comparison of computer viruses", "text": "Comparison of computer viruses\n\nThe compilation of a unified list of computer viruses is made difficult because of naming. To aid the fight against computer viruses and other types of malicious software, many security advisory organizations and developers of anti-virus software compile and publish lists of viruses. When a new virus appears, the rush begins to identify and understand it as well as develop appropriate counter-measures to stop its propagation. Along the way, a name is attached to the virus. As the developers of anti-virus software compete partly based on how quickly they react to the new threat, they usually study and name the viruses independently. By the time the virus is identified, many names denote the same virus.\n\nAnother source of ambiguity in names is that sometimes a virus initially identified as a completely new virus is found to be a variation of an earlier known virus, in which cases, it is often renamed. For example, the second variation of the Sobig worm was initially called \"Palyh\" but later renamed \"Sobig.b\". Again, depending on how quickly this happens, the old name may persist. \n\nIn terms of scope, there are two major variants: the list of \"in-the-wild\" viruses, which list viruses in active circulation, and lists of all known viruses, which also contain viruses believed not to be in active circulation (also called \"zoo viruses\"). The sizes are vastly different: in-the-wild lists contain a hundred viruses but full lists contain tens of thousands.\n\n\n\n\n\n\n\n"}
{"id": "25962873", "url": "https://en.wikipedia.org/wiki?curid=25962873", "title": "Credit valuation adjustment", "text": "Credit valuation adjustment\n\nCredit valuation adjustment (CVA) is the difference between the risk-free portfolio value and the true portfolio value that takes into account the possibility of a counterparty’s default. In other words, CVA is the market value of counterparty credit risk. This price depends on counterparty credit spreads as well as on the market risk factors that drive derivatives’ values and, therefore, exposure. CVA is one of a family of related valuation adjustments, collectively xVA; for further context here see Financial economics #Derivative pricing.\n\nUnilateral CVA is given by the risk-neutral expectation of the discounted loss. The risk-neutral expectation can be written as\n\nwhere formula_2  is the maturity of the longest transaction in the portfolio, formula_3 is the future value of one unit of the base currency invested today at the prevailing interest rate for maturity formula_4, formula_5 is the fraction of the portfolio value that can be recovered in case of a default, formula_6 is the time of default, formula_7 is the exposure at time formula_4, and formula_9 is the risk neutral probability of counterparty default between times formula_10 and formula_4. These probabilities can be obtained from the term structure of credit default swap (CDS) spreads.\n\nMore generally CVA can refer to a few different concepts:\n\nAccording to the Basel Committee on Banking Supervision's July 2015 consultation document regarding CVA calculations, if CVA is calculated using 100 timesteps with 10,000 scenarios per timestep, 1 million simulations are required to compute the value of CVA. Calculating CVA risk would require 250 daily market risk scenarios over the 12-month stress period. CVA has to be calculated for each market risk scenario, resulting in 250 million simulations. These calculations have to be repeated across 6 risk types and 5 liquidity horizons, resulting in potentially 8.75 billion simulations. \n\nAssuming independence between exposure and counterparty’s credit quality greatly simplifies the analysis. Under this assumption this simplifies to\n\nwhere formula_13 is the risk-neutral discounted expected exposure (EE)\n\nFull calculation of CVA is done via Monte-Carlo simulation of all risk factors which is very computationally demanding. There exists a simple approximation for CVA which consists in buying just one default protection (Credit Default Swap) for amount of NPV of netted set of derivatives for each counterparty.\n\nIn the view of leading investment banks, CVA is essentially an activity carried out by both finance and a trading desk in the Front Office. Tier 1 banks either already generate counterparty EPE and ENE (expected positive/negative exposure) under the ownership of the CVA desk (although this often has another name) or plan to do so. Whilst a CVA platform is based on an exposure measurement platform, the requirements of an active CVA desk differ from those of a Risk Control group and it is not uncommon to see institutions use different systems for risk exposure management on one hand and CVA pricing and hedging on the other. \n\nA good introduction can be found in a paper by Michael Pykhtin and Steven Zhu.\nKarlsson et al. (2016) present a numerical efficient method for calculating expected exposure, potential future exposure and CVA for interest rate derivatives, in particular Bermudan swaptions.\n\n"}
{"id": "17780895", "url": "https://en.wikipedia.org/wiki?curid=17780895", "title": "David D. Balam", "text": "David D. Balam\n\nDavid D. Balam is a Canadian astronomer and a research associate with University of Victoria's Department of Physics and Astronomy, in Victoria, British Columbia. Specializing in the search for Near-Earth objects, Balam is one of the world's most prolific contributors to this research; only two astronomers have made more such discoveries than Balam. He is credited with the discovery or co-discovery of more than 600 asteroids, over a thousand extra-galactic supernovae, and novae in the galaxy M31. Balam is also co-credited for the 1997 discovery of Comet Zhu-Balam.\n\nAmong celestial bodies discovered by Balam are the asteroid 150145 Uvic, which he named for the University of Victoria, and 197856 Tafelmusik, named for the Baroque orchestra in Toronto. Currently, Balam conducts an optical transient survey (OTS) using the 1.82-m Plaskett Telescope of the National Research Council of Canada.\n\nThe asteroid 3749 Balam is named in his honour, recognizing the fact that he developed most of the software for the university's astrometric program on minor planets and comets.\n\n"}
{"id": "595145", "url": "https://en.wikipedia.org/wiki?curid=595145", "title": "Dry distillation", "text": "Dry distillation\n\nDry distillation is the heating of solid materials to produce gaseous products (which may condense into liquids or solids). The method may involve pyrolysis or thermolysis, or it may not (for instance, a simple mixture of ice and glass could be separated without breaking any chemical bonds, but organic matter contains a greater diversity of molecules, some of which are likely to break). If there are no chemical changes, just phase changes, it resembles classical distillation, although it will generally need higher temperatures. Dry distillation in which chemical changes occur is a type of destructive distillation or cracking.\n\nThe method has been used to obtain liquid fuels from coal and wood. It can also be used to break down mineral salts such as sulfates () through thermolysis, in this case producing sulfur dioxide (SO) or sulfur trioxide (SO) gas which can be dissolved in water to obtain sulfuric acid. By this method sulfuric acid was first identified and artificially produced. When substances of vegetable origin, e.g. coal, oil shale, peat or wood, are heated in the absence of air (dry distillation), they decompose into gas, liquid products and coke/charcoal. The yield and chemical nature of the decomposition products depend on the nature of the raw material and the conditions under which the dry distillation is done. Decomposition within a temperature range of 450 to about 600°C is called carbonization or low-temperature degassing. At temperatures above 900°C, the process is called coking or high-temperature degassing. If coal is gasified to make coal gas or carbonized to make coke then Coal tar is among the by-products.\n\nWhen wood is heated above 270°C it begins to carbonize. If air is absent the final product, since there is no oxygen present to react with the wood, is charcoal. If air, which contains oxygen, is present, the wood will catch fire and burn when it reaches a temperature of about 400–500°C and the fuel product is wood ash. If wood is heated away from air, first the moisture is driven off and until this is complete, the wood temperature remains at about 100–110°C. When the wood is dry its temperature rises and at about 270°C it begins to spontaneously decompose and, at the same time, heat is evolved. This is the well known exothermic reaction which takes place in charcoal burning. At this stage evolution of the by-products of wood carbonization starts. These substances are given off gradually as the temperature rises and at about 450°C the evolution is complete. The solid residue, charcoal, is mainly carbon (about 70%) and small amounts of tarry substances which can be driven off or decomposed completely only by raising the temperature to above about 600°C.\n\nIn the common practice of charcoal burning using internal heating of the charged wood by burning a part of it, all the by-product vapors and gas escapes into the atmosphere as smoke. The by-products can be recovered by passing the off-gases through a series of water to yield so-called wood vinegar (pyroligneous acid) and the non-condensible wood gas passes on through the condenser and may be burned to provide heat. The wood gas is only usable as fuel and consists typically of 17% methane; 2% hydrogen; 23% carbon monoxide; 38% carbon dioxide; 2% oxygen and 18% nitrogen. It has a gas calorific value of about 10.8 MJoules per m (290 BTU/cu.ft.) i.e. about one third the value of natural gas. When deciduous tree woods are subjected to distillation, the products are methanol (wood alcohol) and charcoal. The distillation of pine wood causes Pine tar and pitch to drip away from the wood and leave behind charcoal. Birch tar from birch bark is a particularly fine tar, known as \"Russian oil\", suitable for leather protection. The by-products of wood tar are turpentine and charcoal.\n\nTar kilns are dry distillation ovens, historically used in Scandinavia for producing tar from wood. They were built close to the forest, from limestone or from more primitive holes in the ground. The bottom is sloped into an outlet hole to allow the tar to pour out. The wood is split into dimensions of a finger, stacked densely, and finally covered tight with dirt and moss. If oxygen can enter, the wood might catch fire, and the production would be ruined. On top of this, a fire is stacked and lit. After a few hours, the tar starts to pour out and continues to do so for a few days.\n\n"}
{"id": "2362255", "url": "https://en.wikipedia.org/wiki?curid=2362255", "title": "Eckert number", "text": "Eckert number\n\nThe Eckert number (Ec) is a dimensionless number used in continuum mechanics. It expresses the relationship between a flow's kinetic energy and the boundary layer enthalpy difference, and is used to characterize heat dissipation. It is named after Ernst R. G. Eckert.\n\nIt is defined as\n\nwhere\n"}
{"id": "16349933", "url": "https://en.wikipedia.org/wiki?curid=16349933", "title": "Food Valley", "text": "Food Valley\n\nFood Valley is a region in the Netherlands where international food companies, research institutes, and Wageningen University and Research Centre are concentrated. The Food Valley area is the home of a large number of food multinationals and within the Food Valley about 15,000 professionals are active in food related sciences and technological development. Far more are involved in the manufacturing of food products. Food Valley, with the city of Wageningen as its center, is intended to form a dynamic heart of knowledge for the international food industry.\n\nWithin this region, the Food Valley Organisation is intended to create conditions so that food manufacturers and knowledge institutes can work together in developing new and innovating food concepts. The Food Valley Organisation has joined the Food Innovation Network Europe.\n\nThe Food Valley as a region has been the subject of study by several human geographers. Even before the Food Valley was established as an organisation in 2004 and as a region in 2011 Frank Kraak and Frits Oevering made a SWOT analysis of the region using an Evolutionary economics framework and compared it with similar regions in Canada, Denmark, Italy and Sweden. A similar study was done by Floris Wieberdink. The study utilised Geomarketing concepts in the WERV, the predecessor of the Regio Food Valley. Geijer and Van der Velden studied the economic development of the Regio Food Valley using statistical data.\n\nThe research performed in the Food Valley has generated some discussion about the influence of culture on economic growth. Wieberdink argued that culture and habitat are not spatially bounded, but historically. More recently a study about the Food Valley argued that culture and habitat are in fact spatially bounded. Both studies, however, recommend the Regio Food Valley to promote its distinct culture.\n"}
{"id": "17373539", "url": "https://en.wikipedia.org/wiki?curid=17373539", "title": "Freshman's dream", "text": "Freshman's dream\n\nThe freshman's dream is a name sometimes given to the erroneous equation (\"x\" + \"y\") = \"x\" + \"y\", where \"n\" is a real number (usually a positive integer greater than 1). Beginning students commonly make this error in computing the power of a sum of real numbers, falsely assuming powers distribute over sums. When \"n\" = 2, it is easy to see why this is incorrect: (\"x\" + \"y\") can be correctly computed as \"x\" + 2\"xy\" + \"y\" using distributivity (commonly known as the FOIL method). For larger positive integer values of \"n\", the correct result is given by the binomial theorem.\n\nThe name \"freshman's dream\" also sometimes refers to the theorem that says that for a prime number \"p\", if \"x\" and \"y\" are members of a commutative ring of characteristic \"p\", then \n(\"x\" + \"y\") = \"x\" + \"y\". In this more exotic type of arithmetic, the \"mistake\" actually gives the correct result, since \"p\" divides all the binomial coefficients save the first and the last, making all intermediate terms equal to zero.\n\n\nWhen \"p\" is a prime number and \"x\" and \"y\" are members of a commutative ring of characteristic \"p\", then . This can be seen by examining the prime factors of the binomial coefficients: the \"n\"th binomial coefficient is\n\nThe numerator is \"p\" factorial, which is divisible by \"p\". However, when , neither \"n\"! nor is divisible by \"p\" since all the terms are less than \"p\" and \"p\" is prime. Since a binomial coefficient is always an integer, the \"n\"th binomial coefficient is divisible by \"p\" and hence equal to 0 in the ring. We are left with the zeroth and \"p\"th coefficients, which both equal 1, yielding the desired equation.\n\nThus in characteristic \"p\" the freshman's dream is a valid identity. This result demonstrates that exponentiation by \"p\" produces an endomorphism, known as the Frobenius endomorphism of the ring.\n\nThe demand that the characteristic \"p\" be a prime number is central to the truth of the freshman's dream. A related theorem states that if a number \"n\" is prime then in the polynomial ring formula_7. This theorem is a direct consequence of Fermat's little theorem and it is a key fact in modern primality testing.\n\nThe history of the term \"freshman's dream\" is somewhat unclear. In a 1940 article on modular fields, Saunders Mac Lane quotes Stephen Kleene's remark that a knowledge of in a field of characteristic 2 would corrupt freshman students of algebra. This may be the first connection between \"freshman\" and binomial expansion in fields of positive characteristic. Since then, authors of undergraduate algebra texts took note of the common error. The first actual attestation of the phrase \"freshman's dream\" seems to be in Hungerford's graduate algebra textbook (1974), where he quotes McBrien. Alternative terms include \"freshman exponentiation\", used in Fraleigh (1998). The term \"freshman's dream\" itself, in non-mathematical contexts, is recorded since the 19th century.\n\nSince the expansion of is correctly given by the binomial theorem, the freshman's dream is also known as the \"child's binomial theorem\" or \"schoolboy binomial theorem\".\n\n"}
{"id": "863173", "url": "https://en.wikipedia.org/wiki?curid=863173", "title": "Fyodor Minin", "text": "Fyodor Minin\n\nFyodor Alekseyevich Minin () (ca. 1709 - after 1742) was a Russian Arctic explorer.\n\nIn 1730s, Minin participated in the Second Kamchatka expedition. In 1736, he joined the unit led by Dmitry Ovtsyn. In 1738, he was in charge of a group of explorers, that would chart the Arctic Ocean coastline east of the Yenisei river. In 1738-1740, Minin made an attempt to go around the Taimyr Peninsula from the north and reached 75°15'N. Together with Dmitry Sterlegov, he mapped this part of the Arctic Ocean coastline.\n\nA cape at the Mammoth Peninsula, a peninsula, the Minina Skerries in the Kara Sea, a gulf, and a mountain on the shores of the Taimyr Peninsula bear Minin's name.\n\n"}
{"id": "13904758", "url": "https://en.wikipedia.org/wiki?curid=13904758", "title": "Giacomo Luigi Ciamician", "text": "Giacomo Luigi Ciamician\n\nGiacomo Luigi Ciamician (; 27 August 1857 – 2 January 1922) was an Italian photochemist and senator of Armenian descent.\n\nCiamician was born in Trieste, Italy (then part of Austrian Empire) from Armenian parents.\n\nCiamician was an early researcher in the area of photochemistry, where from 1900 to 1914 he published 40 notes, and nine memoirs.He received his Ph.D. from the University of Giessen. His first photochemistry experiment was published in 1886 and was titled \"On the conversion of quinone into quinol.\n\nIn 1910 he became the first man born in Trieste to be nominated Senator, in the XXIII Legislation of the Kingdom of Italy.\n\nIn 1912 he presented a paper before the 8th International Congress on Applied Chemistry later also published in \"Science\" in which he described the world's need for an energy transition to renewable energy. Ciamician saw the possibility to use photochemical devices that utilizing solar energy to produce fuels to power the human civilization and called for their development. They would not only make humanity independent from coal, but could also rebalance the economic gap between rich and poor countries. His vision makes him one early proponents of artificial photosynthesis.:\n\n\"\"On the arid lands there will spring up industrial colonies without smoke and without smokestacks; forests of glass tubes will extend over the plains and glass buildings will rise everywhere; inside of these will take place the photochemical processes that hitherto have been the guarded secret of the plants, but that will have been mastered by human industry which will know how to make them bear even more abundant fruit than nature, for nature is not in a hurry and mankind is. And if in a distant future the supply of coal becomes completely exhausted, civilization will not be checked by that, for life and civilization will continue as long as the sun shines!'\"'\n\nCiamician received the honorary Doctor of Laws (DLL) from the University of Glasgow in June 1901.\n\nCiamician died in Bologna.\n\n\n\n"}
{"id": "13572433", "url": "https://en.wikipedia.org/wiki?curid=13572433", "title": "Gough–Joule effect", "text": "Gough–Joule effect\n\nThe Gough–Joule effect (a.k.a. Gow–Joule effect) is originally the tendency of elastomers to contract when heated if they are under tension. Elastomers that are not under tension do not see this effect. The term is also used more generally to refer to the dependence of the temperature of any solid on the mechanical deformation.\n\nIf an elastic band is first stretched and then subjected to heating, it will shrink rather than expand. This effect was first observed by John Gough in 1802, and was investigated further by James Joule in the 1850s, when it then became known as the Gough–Joule effect.\n<br>\"Examples in Literature:\"\n\nThe effect is important in O-ring seal design, where the seals can be mounted in a peripherally compressed state in hot applications to prolong life.\nThe effect is also relevant to rotary seals which can bind if the seal shrinks due to overheating.\n\n"}
{"id": "41027768", "url": "https://en.wikipedia.org/wiki?curid=41027768", "title": "Hantavirus vaccine", "text": "Hantavirus vaccine\n\nHantavirus vaccine is a vaccine that protects in humans against hantavirus infections causing Hantavirus hemorrhagic fever with renal syndrome (HFRS) or hantavirus pulmonary syndrome (HPS). The vaccine is considered important as acute hantavirus infections are responsible for significant morbidity and mortality worldwide. It is estimated that about 1.5 million cases and 46,000 death happened in China from 1950 to 2007. The number of cases is estimated at 32,000 in Finland from 2005 to 2010 and 90,000 in Russia from 1996 to 2006.\n\nThe first hantavirus vaccine was developed in 1990 initially for use against Hantaan River virus which causes one of the most severe forms of HFRS. It is estimated that about two million doses of rodent brain or cell-culture derived vaccine are given in China every year. The wide use of this vaccine may be partly responsible for a significant decrease in the number of HFRS cases in China to less than 20,000 by 2007.\n\nOther hantaviruses for which the vaccine is used include Seoul (SEOV) virus. However the vaccine is thought not to be effective against European hantaviruses including Puumala (PUUV) and Dobrava-Belgrade (DOBV) viruses. The pharmaceutical trade name for the vaccine is Hantavax. As of 2013 no hantavirus vaccine have been approved for use in Europe or USA. A phase 2 study on a human HTNV/PUUV DNA hantavirus vaccine is ongoing.\n\nIn addition to Hantavax three more vaccine candidates have been studied in I–II stage clinical trials. They include a recombinant vaccine and vaccines derived from HTNV and PUUV viruses. However, their prospects are unclear.\n\n\n"}
{"id": "50740275", "url": "https://en.wikipedia.org/wiki?curid=50740275", "title": "Hostile worlds", "text": "Hostile worlds\n\nHostile worlds is sociologist Viviana Zelizer's term for the view that the market must be kept separate from intimate, sacred and otherwise important spheres if they are to retain their value and importance. As she says, it is the assumption \"that the entry of instrumental means such as monetization and cost accounting into the worlds of caring, friendship, sexuality, and parent-child relations depletes them of their richness, hence that zones of intimacy only thrive if people erect effective barriers around them.\"\n\nThis is contrasted with the \"Nothing But\" view, where we do not have separated spheres of money vs. intimate relations, but there is nothing but the market (or culture, or politics).\n\nThis theory has been productively used to study intimate relations, as in Zelizer's other work, but also to examine care relationships, art and other areas. For example, Olav Velthuis used this to understand the views of contemporary artists and their hostility to investment interests, while Coslor found this to be a prevalent view for collectors and art lovers, though not necessarily all gallerists.\n"}
{"id": "32873814", "url": "https://en.wikipedia.org/wiki?curid=32873814", "title": "Image-based flow visualization", "text": "Image-based flow visualization\n\nIn scientific visualization, image-based flow visualization (or visualisation) is a computer modelling technique developed by Jarke van Wijk to visualize two dimensional flows of liquids such as water and air, like the wind movement of a tornado. Compared with integration techniques it has the advantage of producing a whole image at every step, as the technique relies upon graphical computing methods for frame-by-frame capture of the model of advective transport of a decaying dye. It is a method from the texture advection family.\n\nThe core idea is to create a noise texture on a regular grid and then bend this grid according to the flow (the vector field). The bent grid is then sampled at the original grid locations. Thus, the output is a version of the noise, that is displaced according to the flow.\n\nThe advantage of this approach is that it can be accelerated on modern graphics hardware, thus allowing for real-time or almost real-time simulation of 2D flow data. This is particularly handy if one wants to visualise multiple scaled versions of the vector field to first gain an overview and then concentrate on the details.\n\n"}
{"id": "11270176", "url": "https://en.wikipedia.org/wiki?curid=11270176", "title": "Index of Leading Environmental Indicators", "text": "Index of Leading Environmental Indicators\n\nThe Index of Leading Environmental Indicators was a report published yearly between 1996 and 2008 by the Pacific Research Institute and American Enterprise Institute. Author Steven F. Hayward stated that the index was issued each Earth Day in an effort to track environmental trends in the U.S. and worldwide.\n\n"}
{"id": "27774511", "url": "https://en.wikipedia.org/wiki?curid=27774511", "title": "Lakes District Technocity", "text": "Lakes District Technocity\n\nThe Lakes District Technocity(established in 2004) is a science park located on the campus of Süleyman Demirel University. The technocity is a full member of International Association of Science Parks. The science park occupies 112000 squaremeters area.\n\nThe activities of the technocity include Energy and Renewable Energy, Internet Technology and Services / E-Business, Plasma Technology, and Environment Technologies. \n\nAs of 2010, 57% of firms on the tecnocity involved in the area of computer software industry.\n\nThe technocity made its first export(plasma sensors to Saudi Arabia) in 2008.\n\nThe following entities are shareholders of the park:\n\n"}
{"id": "6052711", "url": "https://en.wikipedia.org/wiki?curid=6052711", "title": "Legitimation crisis", "text": "Legitimation crisis\n\nLegitimation crisis refers to a decline in the confidence of administrative functions, institutions, or leadership. The term was first introduced in 1973 by Jürgen Habermas, a German sociologist and philosopher. Habermas expanded upon the concept, claiming that with a legitimation crisis, an institution or organization does not have the administrative capabilities to maintain or establish structures effective in achieving their end goals. The term itself has been generalized by other scholars to refer not only to the political realm, but to organizational and institutional structures as well. While there is not unanimity among social scientists when claiming that a legitimation crisis exists, a predominant way of measuring a legitimation crisis is to consider public attitudes toward the organization in question.\n\nWith respect to political theory, a state is perceived as being legitimate when its citizens treat it as properly holding and exercising political power. While the term exists beyond the political realm, as it encompasses sociology, philosophy, and psychology, legitimacy is often referred to with respect to actors, institutions, and the political orders they constitute. In other words, actors, institutions, and social orders can be seen as being either legitimate or illegitimate. When political actors engage in the process of legitimation they are pursuing legitimacy for themselves or for another institution. According to Morris Zelditch, Jr., Emeritus Professor of Sociology at Stanford, theories of legitimacy span 24 centuries, beginning with Thucydides' History of the Peloponnesian War.\n\nSome of the earliest accounts of legitimacy come from early Greek thought. Aristotle is mainly concerned with the stability of the government. While he argues that the legitimacy of the government relies upon constitutionalism and consent, he posits that political stability relies upon the legitimacy of rewards. In his book \"Politics\", Aristotle argues the ways in which rewards are distributed are found within politics, and distributive justice (the proper allocation of rewards according to merit) is what makes a government stable. When there is distributive injustice, on the other hand, the government becomes unstable. Also concerned with justness and distinguishing between right and wrong constitutions, Aristotle bases legitimacy on the rule of law, voluntary consent, and the public interest. While Aristotle's theory of distribution of rewards and legitimacy of constitutions both deal with legitimation, the prior emphasizes an actors acceptance that rewards are just, while the latter is concerned with an actors acceptance of a \"moral obligation to obey a system of power.\"\n\nDetailed at greater length in \"The Social Contract\", Rousseau insists that government legitimacy is dependent upon the \"general will\" of its members. The general will itself is the common interests of all the citizens to provide for the common good of all citizens, as opposed to individual interests. The people who express this general will, according to Rousseau, are those who have consensually entered into a civil society. However, implicit consent is not sufficient for political legitimacy; rather, it requires the active participation of citizens in the justification of state's laws, through the general will of the people. Because legitimacy rests on the general will of the people, Rousseau believes republican or popular rule is legitimate, while tyranny and despotism are illegitimate.\n\nAccording to Weber, a political regime is legitimate when the citizens have faith in that system. In his book, \"The Theory of Social and Economic Organization\", Weber expands upon this idea when he writes “the basis of every system of authority, and correspondingly of every kind of willingness to obey, is a belief, a belief by virtue of which persons exercising authority are lent prestige.\" Weber provides three main sources of legitimate rule: traditional (it has always been that way), rational-legal (trust in legality), and charismatic (faith in the ruler). However, as Weber explains in his book \"Economy and Society\", these ideal forms of legitimacy will necessarily always overlap. The example that Weber gives is with that of legal authority. Legality is partly traditional, for it is \"established and habitual.\" He argues that due to the presence of legitimate authority and the way legitimate authority structures society, citizens who do not share in the belief of this legitimacy still face incentives to act as if they did.\n\nIn his book \"Managing Legitimacy: Strategic and Institutional Approaches\", Suchman defines legitimacy as “a generalized perception or assumption that the actions of an entity are desirable, proper, appropriate within some socially constructed system of norms, values, beliefs, and definitions.\" He later adds to this definition, stating that because legitimacy is socially conferred, legitimacy is independent of individual participants, while dependent upon the collective constituency. In other words, an organization is legitimate when it enjoys public approval, even though the actions of an organization might deviate from particular individual interests. Suchman states three types of legitimacy: pragmatic legitimacy, moral legitimacy, and cognitive legitimacy.\n\nPragmatic legitimacy relies upon the self-interests of an organizations constituencies, in which the constituency scrutinizes actions and behaviors taken by the organization in order to determine their effects. This is further broken down into three sub-sections: exchange legitimacy, influence legitimacy, and dispositional legitimacy. Suchman defines exchange legitimacy as the support for organizational policies due to the policy's benefit to the constituencies. Influence legitimacy is the support for the organization not due to the benefits that constituencies believe they will receive, but rather due to their belief that the organization will be responsive to their larger interests. Dispositional legitimacy is defined as support for an organization due to the good attributes constituencies believe the organization has, such as trustworthy, decent, or wise. This is due to the fact that people typically personify organizations and characterize them as being autonomous.\n\nMoral legitimacy is dependent upon whether the actions of an organization or institution are judged to be moral. In other words, if the constituency believe the organization is breaking the rules of the political or economic system for immoral reasons, then this can threaten moral legitimacy. Suchman breaks moral legitimacy down into four sub-sections: consequential legitimacy, procedural legitimacy, structural legitimacy, and personal legitimacy. Consequential legitimacy relates to what an organization has accomplished based on criteria that is specific to that organization. Procedural legitimacy can be obtained by an organization by adhering to socially formalized and accepted procedures (e.g. regulatory oversight). In the case of structural legitimacy, people view an organization as legitimate because its structural characteristics allow it to do specific kinds of work. Suchman refers to this organization as being the \"right organization for the job.\" Lastly, personal legitimacy refers to legitimacy that is derived from the charisma of individual leaders.\n\nCognitive legitimacy is created when an organization pursues goals that society deems to be proper and desirable. Constituency support for the organization is not due to self-interest, but rather due to its taken-for-granted character. When an organization has reached this taken-for-granted status, an organization is beyond dissent. While moral and pragmatic legitimacy deal with some form of evaluation, cognitive legitimacy does not. Instead, with cognitive legitimacy society accepts these organizations as being necessary or inevitable.\n\nGerman sociologist and philosopher Jürgen Habermas was the first to use the term \"legitimation crisis,\" which he defined in \"Legitimation Crisis\", his 1973 book of the same name. A \"legitimation crisis\" is an identity crisis that results from a loss of confidence in administrative institutions, which occurs despite the fact that they still retain legal authority by which to govern. In a legitimation crisis, governing structures are unable to demonstrate that their practical functions fulfill the role for which they were instituted.\n\nA crisis is a state of jeopardy that arises because of contradicting motivations of the subsystems within a self-enclosed system. According to Habermas, the definition of crisis used in the social sciences is often based on the principles of systems theory. However, he argues that a crisis is properly understood in two dimensions, the objective and the subjective, though this connection has been difficult to grasp using conventional approaches such as systems theory or action theory.\n\nThe difference between social integration and system integration helps distinguish between the objective and subjective components of crises. Social integration refers to what Habermas calls the \"life-world,\" a term adapted from the writings of Alfred Schutz, which is composed of a consensual foundation of shared understandings, including norms and values, upon which a society is built. System integration, alternatively, refers to the determinants of a society, which break down when their structures \"allow fewer possibilities for problem solving than are necessary to the[ir] continued existence[s].\" The principles of rationalization are efficiency, calculability, predictability, and control, which are characteristic of systems as Habermas refers to them.\n\nWithin a social system exist three subsystems: the economic, the political, and the socio-cultural. The subsystem that assumes functional primacy in a society is determined by the type of social formation that exists in the society. Four types of social formations can potentially characterize a social system: primitive, traditional, capitalist (liberal and advanced/organized capitalist), and post-capitalist. Each of these, with the exception of the primitive, is a class-based society. The \"principle of organization\" of a social system determines when crises occur and what type of crisis predominates in each type of social system.\n\n\nThe political subsystem of the social world requires an input of mass loyalty in order to produce an output, which consists of legitimate administrative decisions that are executed by the state. A \"rationality crisis\" is an output crisis that occurs when the state fails to meet the demands of the economy. A \"legitimation crisis\", on the other hand, is an input crisis that occurs when \"the legitimation system does not succeed in maintaining the requisite level of mass loyalty.\" It is an identity crisis in which administrations are unable to establish normative structures to the extent required for the entire system to function properly. As a result, the state suffers a loss of support by the public when the electorate judges its administration unaccountable. This loss of public confidence is one of many characteristics of a legitimation crisis, among them issues such as policy incoherence and loss of institutional will.\n\nIn the past, there have been many examples of social upheaval and systemic power exchanges that can be classified as legitimation crises. According to Habermas, these crises have all occurred as a natural consequence of society's productive advancement, as the social system struggles to adapt to the strains on relations of production. In other words, as a society's \"technical knowledge\" advances, the equilibrium is disturbed between the technical and political aspects of production, which can result in a crisis if the imbalance isn't corrected by adequate advancement of \"moral-practical knowledge.\" A prime example of this is in the process of industrialization, where the establishment of factories and massive workforces often precedes the establishment of government regulations, workers' rights and labor unions. As sociologist Robert Merton explains, a group is most successful and stable when it is satisfied by the achievement of its institutional goals (technical/forces of production) and also with the institutional norms and regulations condoned to achieve those goals (moral-practical/relations of production). Therefore, in order to maintain legitimacy, a society, constituted by both the government and the governed, must engage in an ongoing and competitive reevaluation of its goals and norms to ensure they continue to satisfy the society's needs. The establishment of new social movements is essential to this process.\n\nHistorically, the most stable societies have been those that enjoy widespread acceptance of both the society's institutional goals and the means used to achieve them. In contrast, every crisis of legitimacy has occurred when a large and/or important portion of a society strongly disagrees with some or all aspects of the institutional norms, as established and advanced by a particular regime or government. When a government loses support, in this regard, it risks losing its legitimacy, as the public begins to question and doubt the grounds upon which the government's claim to power is built. In dealing with these crises, individuals and groups of individuals in the society resort to various modes of adjustment or adaptation. Historically, these have usually cropped up in the form of revolutions, coups and wars.\n\nAdditionally, it is important to note that the logic of legitimation strongly depends on the system of domination deployed. In fact, it's the logic of legitimation that informs the concrete ways citizens and subjects comply to authority and/or contend with authority. In other words, the basis for any claim to legitimacy is often the basis for resistance against that same claim to legitimacy. For example, in some societies the economic achievements under a particular regime or government form the basis for its legitimation claims; in those societies, counterclaims to legitimacy will often highlight economic failures in order to strategically undermine the regime or government's authority. Max Weber, who first advanced this point, summarizes it below:\n\nThe events of the French Revolution, from 1789 to 1799, and the socio-political changes that it comprised can be classified as a legitimation crisis. The revolution was characteristic of a time in Europe where the divine right of monarchical rule was being undermined and transformed as the universal rights of the common citizen were emphasized instead. Consequently, the mythological world views that underpinned the governing institutions of law and that bound popular conceptions of morality were replaced with more rational ones.\n\nThe legitimation crisis in China took place after decades of power struggles and cultural shifts that had been in effect since the 1960s. The legitimation crisis, itself, was the result of several economic and political reforms made by the Communist Party of China (CPC) as part of an effort to salvage their reputation after the socialist policies and populist leadership of Mao Zedong in the 60s and 70s had left the Chinese economy in poor condition.\n\nDuring Mao's rule, an informal social contract was established, in which the government would supply socialist benefits (e.g. egalitarianism, food and shelter, medical care, education, job security, stable prices, social stability, and elimination of social evils) in return for the public's acquiescence to one-party rule and the loss of some civil liberties and political rights. However, in the midst of the a time referred to as the Cultural Revolution from 1966 to 1976, the social contract was put in jeopardy as political and social stability faded.\nWhen Mao died in 1976, a brief crisis of legitimation followed, as the cult of personality died with him and the CPC was left without its last strong grounds for authority. Since the party's core socialist policies had also failed, in order to regain and maintain legitimacy the party was forced to shift away from its longstanding focus on Marxist ideology, economic socialism, and charismatic appeals to focusing on political and economic rationalization and legalization instead. The party's economic achievements (e.g. improved standard of living, growth and development) under its newly liberalized policies became the primary evidence of its legitimacy. In essence, the reforms were a solid move away from a control-oriented economy towards a more market-oriented, capitalist one.\n\nThe CPC faced a new legitimation crisis with the move toward capitalism, as it violated the terms of the previously established social contract (inflation rose, the income gap widened, job insecurity increased, social welfare programs deteriorated and social evils returned) and the CPC's claim to one-party rule was challenged, as the public began to wonder why they were necessary as a party if socialism had failed and capitalism was the answer; after all, the CPC's leaders were not the most qualified to exercise market-oriented economic reforms. The shift towards capitalist policies coupled with the CPC's inability to accommodate increased pressure for political liberalization and democratization eventually culminated in the Chinese democracy movement and the Tiananmen Square protests of 1989.\n\nIn the twentieth-century, as African states adjusted to postcolonial independence, legitimation crises and state collapse were constant threats. While authority was passed from colonial to independent rule successfully in most African states throughout the continent, some attempts at transition resulted in collapse. In Congo, for example, the state collapsed as its respective institutions (e.g. army, executives, local governments, populations) refused to recognize each other's authority and work together. It took international intervention and the installation of a strongman with foreign connivance to reconstitute the state there.\n\nIn other African countries, state collapse was not a strictly postcolonial issue, as most states had some success transferring between regimes. Problems arose, however, when second-generation (and later) regimes began overthrowing original nationalist ones. Chad, Uganda, and Ghana are all instances of this happening – in each, a successfully established, but dysfunctional independent regime was replaced by a military regime that managed to concentrate power, but failed to effectively wield it. Legitimation crises and state collapse soon followed.\n\nIn Eastern European countries where Stalinism was the system of domination, the legitimacy of the system was dependent on the instillment of fear among citizens and the charisma of the state leader. This was the strategy that worked for Stalin, himself, in the Soviet Union, as his brand of terror and charisma inspired a strong personality cult that placed authority and legitimacy in Stalin's hands alone. For other Eastern European states, however, Soviet communism was a foreign system that had to be imported. This proved to be a major problem, as the communist leaders in other Eastern European states lacked Stalin's charisma.\n\nFurthermore, communism was implemented in other East European states (e.g. Romania, Hungary, Poland) in a much shorter time frame and developed very differently from the way it did in the Soviet Union. In Hungary, for example, the communist party initially came to power via tacit consent to a coalition government. Over time, the party began to strategically gain more power and get rid of competition. However, the democratic means the communist parties in these states initially used to gain power lost credibility once they were seen as violent tyrannies in service of an alien power. Ultimately, populist platforms - giving farmers land, social and economic stability, and welfare benefits - gave way to brutal collectivist realities, as leaders were blamed for the very same reforms they were once praised for.\n\nThe Tunisian Revolution began with the self-immolation of Mohamed Bouazizi on December 17, 2010, although it is also possible to consider the miner strike in the west central town of Gafsa in 2008 to be the official beginning of the movement. The Tunisian people toppled Ben Ali, who had imposed a police state. The revolution, like other Arab Spring revolutions which would soon follow, was prompted by endemic poverty, rising food prices, and chronic unemployment. Tunisians demanded democracy, human rights, the end of corruption, and the end of the enforcement of the 2003 Anti-Terrorism Act, which effectively criminalized their religious ideas and practices.\n\nThe previous legitimacy of the Tunisian government had been based on a combination of the charisma of former president Bourguiba's secular legacy and an achievement legitimacy based on the modernization of the Tunisian state. After this legitimacy had failed and its accompanying regime had fallen, En-Nahdha, an Islamist party, sought to provide legitimacy through criticism of the previous regime. Tunisia initiated a top-down modernization, led by civil, urban, and secular petty bourgeoisie, contrasting with the military coups in Egypt, Syria, and Iraq, the leadership of traditional scriptural elites in Morocco and Libya, and the leadership of revolutionary armed peasantry in Algeria.\n\nTunisians asked that a National Constituent Assembly (NCA) be formed that would be charged with writing the new constitution. The party of the former regime, the Constitutional Democratic Rally (RCD) was banned from running for re-election, and En-Nahdha received 40% of the vote in an election overseen by a higher independent authority in April 2011. With its share of 89 out of 217 total seats, En-Nahdha then formed a coalition in the form of a triumvirate, or troika, with the Congress for the Republic and the Forum known as Ettakatul within the NCA.\n\nEn-Nahdha then seized considerable control by appointing 83% of public agents at all levels, and shutting down the media by physically attacking hundreds of journalists. En-Nahdha was also suspected of several assassinations, prompting the resignation of En-Nahdha prime minister Hamadi Jebali in April 2013. En-Nahdha also failed to produce a constitution by the agreed-upon time of a year, causing many political parties, including the major political party Nidaa Tounes, to declare the end of En-Nahdha's electoral legitimacy.\n\nThe Tunisian public and political parties then asked for a compromise legitimacy that consisted of a mandatory national dialogue between En-Nahdha and the other ruling members of the NCA, which began in October 2013. This effectively forced En-Nahdha to negotiate its own immediate departure from the government, while at the same time conceding the current failure of Islamism as a means of legitimacy. The national dialogue, which is still taking place, is seeking to establish a legitimate government, end the legislative process for the constitution and electoral code, and set up an independent body to organize elections and fix a definitive date.\n\nLibya's revolution is also considered part of the Arab Spring, beginning February 15, 2011, just a few months after the events in Tunisia. The revolution deposed Muammar Gaddafi, who had been the ruler of Libya for four decades and had united the country under the themes of Pan-Arabism (a form of nationalism), common geography, shared history, and Islam. The revolution was an attempt to replace these forms of legitimacy with democratic legitimacy via the National Transitional Council.\n\nGaddafi's legitimacy waned as his regime failed to benefit those of most need in the state. Although Libya has the world's ninth-largest known oil deposits and a population of only 6.5 million, in 2010, Gallup polls showed that 29% of young Libyans were unemployed, and 93% of young Libyans described their condition as “struggling” or “suffering.” As protestors took to the streets, Gaddafi dispatched tanks, jets, and mercenaries to attack them, inciting a string of defections and so further eroding his legitimacy as a ruler. The actual death count of these attacks is not known, as Gaddafi's regime shut out and shut down both world and local media and communications. However, Libya's militarily weak regime was eventually overcome, and Gaddafi was killed on October 20, 2011, leading to the disintegration of the regime.\n\nSince Gaddafi's departure, tribal elders, NGOs, youth groups, town councils, and local brigades have stepped in to fill the power vacuum. There are many different tribes in Libya, not all of which have supported the regime change, making the establishment of a new form of legitimacy difficult. However, unlike Egypt, Libya has no entrenched officer class or judiciary to prolong or obstruct the country's transition to democracy. Since the revolution, no single group has been dominant, although several brigades, or katiba, have been able to exercise considerable strength.\n\nThese katiba are “armed fighting groups ranging from 20 to 200 young men, formed along neighborhood, town or regional lines.” These brigades were central to the military strength of the revolutionary forces. After Gaddafi's overthrow, the powerful brigades from Misrata and Zintan raided Tripoli, the Libyan capital, “pillaged automobiles, took over ministries and encamped at key institutions like the airport and oilfields” in order to gain political power.\n\nIn order to establish democratic legitimacy and sovereignty, the National Transitional Council has had to deal with these brigades, a process which has so far been mostly unsuccessful due to mistrust between the two bodies and the popularly illegitimate but regardless tangible military strength of the brigades. To firmly establish democratic legitimacy, the National Transitional Council is attempting to draft a new constitution. It has also struggled in this task, for which it is looking back to Libya's first constitution in 1951.\n\nEven before the 2011 revolution, former Yemeni president Ali Abdullah Saleh’s regime’s legitimacy relied on a patronage network based on the entrenched Yemeni tribal system, effectively tying Saleh’s political legitimacy to the tribes’ much more established and trusted socio-political legitimacy. Yemen is historically tribal, with tribes being responsible for defense, keeping the peace, protecting and encouraging trade and markets, and either prohibiting or facilitating travel. For many Yemenis, tribal systems are “the main or only administrative system they know.” Tribes function effectively as local governments, introducing generators and water pumps, opening schools, and providing local services. Thus, for many “the state is not representative of the Yemeni nation to which they feel they belong.”\n\nThe Yemeni revolution, also part of the Arab Spring, was brought about by the loss of legitimacy by Saleh's regime. Yemeni youth wanted Saleh's resignation and “a more accountable and democratic system.” Though reform came slowly due to a lack of support from the international community and the poverty of the protestors – Yemen is the Arab world's poorest country – the anti-Saleh movement gained steam and high-level government officials and tribal leaders joined the opposition against Saleh. The most significant government official to join the revolutionary movement was Major General Ali Mohsin Al-Ahmar, who ordered his troops to defend antigovernment demonstrators.\n\nSaleh was deposed and his successor, Abdu Rabu Mansour Hadi, was elected in an uncontested election to serve as head of the transitional government, which includes the oppositional bloc, the Joint Meeting parties (JMP), a five-party alliance including the leading Islamist party Islah and the Yemeni Socialist Party (YSP), the Nasirist Popular Unity Party, and two small Islamist Zaydi parties. The National Dialogue Conference, launched in March 2013, brought together 565 delegates from these parties in order to write a constitution and deal with longstanding challenges to Yemeni governance, such as counterterrorism, development, and the Southern Separatist Movement.\n\nAlthough Yemen was the only country from the 2011 Arab Spring to emerge with a negotiated settlement with the current regime and a transition plan for a national dialogue, by 2013 there was “no significant redistribution of resources or hard power outside the traditional elite.” The vestiges of Saleh's regime and a lack of support from southern tribes plagued the National Dialogue Conference, which consequently finished four months later than expected, in January 2014. Further elections were indefinitely postponed, leading to speculation that Hadi and members of the parliament will keep their positions indefinitely. Due to these complications, there is currently no legitimate unifying political body in Yemen.\n\nThe implications of an international crisis of legitimacy usually reach further than domestic crises, given that the actors have power over several different countries. International crises can threaten the stability between countries, increasing the probability for conflict.\n\nThe European Union (E.U.) is a governing body over 28 European countries. The E.U. does not have complete legitimacy over the citizens of the 28 countries given that it only governs in the realm of politics and economics. Additionally, the E.U. does not operate under majority rule meaning any one country can veto laws. The E.U. suffered a legitimation crisis when it attempted to pass a constitution which failed in the French European Constitution referendum, 2005.\n\nDuring the Cold War era, most European Countries respected the authority of the United States as an international leader. Europeans accepted the United States as the primary defender against the Soviet Union. After the Soviet Union fell, however, some scholars believe the United States was no longer needed by the Europeans for that purpose and therefore lost international legitimacy.\n\nIn this time period, it was accepted by many political theorists that the United States was undergoing a crisis of legitimacy. Minorities in the United States began to question the legitimacy of the government because they felt they were being denied rights. The mindset was transferred into movements beginning in the Civil Rights Movement, which primarily involved African Americans and college students but eventually spread to a larger portion of the population. The United States government's reaction to the legitimation crisis of the late twentieth century shows that in a consolidated democracy, undergoing a legitimation crisis can strengthen legitimacy. In this case, the system adapted to the wants of the citizens and the United States re-established legitimacy. In the mid 1960s, the legitimacy of the United States government was challenged when citizens began to question the legality of the Vietnam War.\n\nIn the United States 2000 presidential election, United States presidential election, 2000, Bush lost the popular vote but still won the electoral vote. Many United States citizens did not believe this was right. The legitimacy of the United States came into question after the Supreme Court Decision, Bush v. Gore. While some believe the legitimacy of the presidency came into question, others believe the legitimacy of the court was jeopardized after the decision was released In the aftermath of the decision, six hundred and seventy five law professors argued against the decision in \"The New York Times.\"\n\nWhen an actor loses legitimacy, the public no longer trusts the actor to maintain a social contract. Without the social contract, the natural rights of the public, such as life, liberty, and property, are in jeopardy. Therefore, it is usually in the interest of both the public and the actor to end the legitimation crisis.There are several ways in which to end a legitimation crisis, but there is currently no unified theory as for the best method. Although the actor could be replaced, as seen in many of the examples above and effectively ending the legitimation crisis, this section will focus on the conflict resolution of the crisis. In this situation, the actor that was seeking legitimacy before the crisis regains legitimacy.\n\nAn actor can regain legitimacy in two ways:\n\nSource: \"The Psychology of Legitimacy Emerging: Perspectives on Ideology, Justice, and Intergroup Relations\"\n"}
{"id": "35005459", "url": "https://en.wikipedia.org/wiki?curid=35005459", "title": "List of Apple Inc. media events", "text": "List of Apple Inc. media events\n\nApple Inc. announces new products, product redesigns and upgrades through press conferences that garner a significant following in traditional and online media. Often the purpose of the event is kept as a secret to create buzz and only unveiled during the event. These events are usually streamed live on Apple's website.\n\n\nAfter a basic market update, Jobs announced that Apple would transition the Macintosh platform to Intel x86 processors. The keynote featured developers from Wolfram Research, who discussed their experience porting Mathematica to Mac OS X on the Intel platform. The conference consisted of 110 lab sessions and 95 presentation sessions, while more than 500 Apple engineers were on site alongside 3,800 attendees from 45 countries. The band The Wallflowers played at the Apple campus.\n\nIn 2006, Jobs once again delivered the keynote presentation at the WWDC, which was held from August 7 to 11 in Moscone Center West, San Francisco. The Mac Pro was announced as a replacement to the Power Mac G5, which was Apple's prior pro desktop computer and the last remaining PowerPC-based Mac. The standard Mac Pro featured two 2.66 GHz dual core Xeon (Woodcrest) processors, 1 GB RAM, 250 GB hard drive, and a 256 MB video card. An Xserve update, based on the dual core Xeons, was also announced. Redundant power and Lights Out Management were further product improvements to Apple's server lineup. While certain key Mac OS X improvements were undisclosed, there were 10 improvements in the next iteration, Mac OS X Leopard (10.5), including: full 64-bit app support, Time Machine, Boot Camp, Front Row, Photo Booth, Spaces (Virtual Desktops), Spotlight enhancements, Core Animation, Universal Access enhancements, Mail enhancements, and Dashboard enhancements (including Dashcode, and iChat enhancements). Along with the Leopard features that were announced, a major revision to the Mac OS X Server product was announced. New features to the Server included: a simplified set-up process, iCal Server (based on the CalDAV standard), Apple Teams (a set of web-based collaborative services), Spotlight Server, and Podcast Producer. The 2006 WWDC attracted 4,200 developers from 48 countries, while there were 140 sessions and 100 hands-on labs for developers. More than 1,000 Apple engineers were present at the event, and the DJ BT performed at the Apple Campus in Cupertino.\n\nAt the Macworld Expo on January 9, 2007, Jobs announced that Apple Computer, Inc. would from that point on be known as Apple Inc., because computers were no longer the main focus of the company, which had shifted its emphasis to mobile electronic devices. The event also saw the announcement of the iPhone and the Apple TV. The following day, Apple shares hit $97.80, an all-time high at that point. In May, Apple's share price passed the $100 mark.\n\nAnnouncements at the keynote included the App Store for iPhone and iPod Touch, the stable version of the iPhone SDK, a subsidized 3G version of the iPhone for Worldwide markets, version 2.0 of iPhone OS, Mac OS X Snow Leopard (10.6), and the replacement/rebranding of .Mac as MobileMe.\n\nAnnouncements at the keynote included the release of the iPhone OS 3.0 software announced to developers in March, a demonstration of Mac OS X Snow Leopard (10.6), the new 13\" MacBook Pro, updates to the 15\" and 17\" MacBook Pros, and the new iPhone 3GS.\n\nThe iPad was announced on January 27, 2010, by Steve Jobs at an Apple press conference at the Yerba Buena Center for the Arts in San Francisco.\n\nJobs later said that Apple began developing the iPad before the iPhone, but temporarily shelved the effort upon realizing that its ideas would work just as well in a mobile phone. The iPad's internal codename was K48, which was revealed in the court case surrounding leaking of iPad information before launch.\n\nApple revealed iPhone OS 4. In Apple's description: it includes \"over 100 new user features for iPhone and iPod touch owners to enjoy. And for developers, a new software development kit (SDK) offers over 1500 new APIs to create apps that are even more powerful, innovative, and amazing.\" \n\nOn June 7, 2010, Jobs announced the iPhone 4. Also, iPhone OS was renamed to iOS. The FaceTime and iMovie app for iPhone applications were also announced.\n\nApple sent invitations to journalists on February 23, 2011, for a media event on March 2. Apple CEO Steve Jobs revealed the iPad 2 device at the Yerba Buena Center for the Arts on March 2, 2011, despite being on medical leave.\n\nApple unveiled Mac OS X Lion, iOS 5, the cloud service iCloud and iTunes Match.\n\nThis keynote was not streamed live.\n\nOn October 4, 2011, Apple held a media event in which it introduced Find My Friends, refreshed the iPod Nano and iPod touch, and revealed the iPhone 4s with its all-new Siri voice assistant.\n\nOn February 28, 2012, Apple announced a media event scheduled for March 7, 2012, at the Yerba Buena Center. Apple didn't disclose in advance what would be announced at the event, but it was widely expected to be a new version of the iPad. It was also rumored that Apple might release a new television set top box. The announcement affected the tablet resale market, and Apple's stock price reached a record closing figure on the same day that the Dow Jones Industrial Average reached a closing figure of above 13,000 for the first time since the Global Financial Crisis. (Apple is not a Dow Jones component.)\n\nThe keynote began 10 AM PST (18:00 UTC) with Cook introducing , a Japanese version of Siri, and the 3rd generation Apple TV before the 3rd generation iPad. Eddy Cue gave a demo of the new Apple TV interface. At the media event, Cook talked about a 'post-PC world', a world where the personal computer is no longer the center of one's digital life, and of how the 3rd generation iPad will be one of the main contributors of the 'post-PC world'.\n\nWWDC 2012 was held in Moscone Center West from June 11 to 15. The ticket price remained the same as the 2010 WWDC, selling at US$1,599. Apple changed the purchasing process by requiring purchases to be made using an Apple ID associated with a paid Apple developer account. Tickets went on sale shortly after 8:30am Eastern Time on Wednesday April 25, 2012, and were sold out within 1 hour and 43 minutes. The keynote highlighted the launch of Apple Maps, and also announced new models of the MacBook Air, and MacBook Pro including one with Retina Display. Apple also showcased OS X Mountain Lion and iOS 6.\n\nIn prior years, attendees were required to be at least 18 years old. In 2012, Apple changed this requirement to at least 13 years after a minor who was \"accidentally\" awarded a student scholarship in 2011 successfully petitioned Tim Cook to retain the award. Despite the change, Beer Bash attendees were still required to be 18 years old, and 21 years old to consume alcohol, in accord with local and federal laws. Neon Trees performed at the WWDC Bash.\n\nThis keynote was streamed live exclusively on iOS devices and OS X, through Safari on June 11, 2012.\n\nPhil Schiller, Apple’s senior vice president of worldwide marketing, took the wraps off the new iPhone for press gathered at the company’s San Francisco event, calling the device “the most beautiful product we’ve ever made, bar none.” The iPhone 5 is made entirely of glass and aluminum, Schiller said, adding that the “exacting level of standards” exhibited by the phone is Apple’s best hardware engineering to date.\n\nIt’s the thinnest and lightest iPhone, at 7.6mm thin, and 112 grams. Schiller said those measurements make it the world’s thinnest smartphone. The iPhone 5 was also volumetrically smaller than the previous model, the iPhone 4S.\n\nOn October 23, Apple CEO Tim Cook unveiled the new iPad Mini, fourth generation iPad with Retina display, new iMac, and the 13-inch MacBook Pro with Retina display.\n\nIn 2013, WWDC 2013 was held from June 10 to 14, 2013, at Moscone West in San Francisco – the same venue as in previous years. Tickets went on sale at 10am PDT on April 25, 2013, selling out within 71 seconds (1 minute and 11 seconds). Apple also announced that it will award 150 free WWDC 2013 Student Scholarship tickets to those who want to attend in order to benefit from the conference's many workshops, with applications for a scholarship starting 9am PDT on April 29, 2013, and deadline slated for 5pm PDT on May 2, 2013. Winning applicants were notified by May 16, 2013, though Apple states that it won't reimburse winners for travel or hotel expenses. In the keynote, Apple unveiled a redesigned model of the Mac Pro, AirPort Time Capsule, and AirPort Extreme as well as updated models of the MacBook Air. Apple has also showcased OS X Mavericks, iOS 7, iWork for iCloud and a new music streaming service named iTunes Radio. Vampire Weekend performed at the Bash on June 13 at the Yerba Buena Gardens.\n\nThis keynote was streamed live on June 10, 2013.\n\nApple announced the iPhone 5C and iPhone 5S during a media event called \"This should brighten everyone's day.\" at its Cupertino headquarters on September 10, 2013. While the iPhone 5C became available for preorder on September 13, 2013, the iPhone 5S first became available on September 20, 2013. While most of the promotion focused on Touch ID, the 64-bit Apple A7 was also a highlight during the event:\n\nSchiller then showed demos of \"Infinity Blade III\" to demonstrate the A7's processing power and the iPhone 5S camera using unretouched photographs. The release of iOS 7 on September 18, 2013, was also announced during the keynote.\n\nApple held a second Fall event in 2013 under the name of \"We still have a lot to cover\". This event saw the unveiling of the iPad Air, the second-generation iPad mini with Retina display, and updates to the MacBook Pro line. Tim Cook also announced that OS X Mavericks would be available for free.\n\nAt Moscone West, Apple presented the new version of OS X named Yosemite as well as the new version of iOS. The biggest news however was the completely new programming language for Mac and iOS called Swift.\n\nThis keynote was streamed live on June 2, 2014.\n\nPresented on the \"Wish we could say more\" event was the most anticipated iPhone 6 and iPhone 6 Plus as well as a new payment system called Apple Pay. Also, the Apple Watch, the company's first smartwatch, was introduced. The event took place at Flint Center, in Cupertino.\n\nApple's “It’s been way too long” media event took place on October 16th, 2014. The company used this event to unveil the iPad Air 2, iPad mini 3, and an updated iMac with a 5K Retina Display. \n\nThis keynote was streamed live on October 16, 2014.\n\nApple Special Event 2015 ('Spring Forward') was an event by Apple that was broadcast live from Cupertino on Apple's website on March 9, 2015. It announced the release date and pricing for the anticipated Apple Watch, the MacBook's fourth redesign and iOS 8.2's same day release.. Apple also announced ResearchKit, a library designed to enable researchers to make study applications where participants can download them on their phones, electronic enroll & consent, and send survey and sensor data. Five launch studies were introduced, including My Heart Counts, which enrolled over 11,000 participants in a single day .\n\nWWDC 2015 was held from June 8 to 12, 2015 in Moscone Center West in San Francisco. The major announcements were the new features of iOS 9, the next version of OS X called OS X El Capitan, the first major software update to the Apple Watch, the June 30 debut of Apple Music, and news that the language Swift was becoming open-source software supporting iOS, OS X, and Linux. The Beer Bash was held at the Yerba Buena Gardens on June 11. Walk the Moon performed there.\n\nThis keynote was streamed live on June 8, 2015.\n\nThe \"Hey Siri, give us a hint\" event was held at the 7,000-seat Bill Graham Civic Auditorium in San Francisco. Apple announced and previewed watchOS 2 with native apps; the long-anticipated Apple TV update - with App Store, Siri Remote and tvOS; iPhone 6S and iPhone 6S Plus with the Apple A9, 3D Touch, 12MP camera; iOS 9 update coming September 16; and iPad Mini 4 together with iPad Pro with 12.9\" Retina display, optional keyboard/cover, and the Apple Pencil stylus. OneRepublic performed at the event.\n\nThis keynote was streamed live on September 9, 2015. For the first time, Windows users were able to watch it live using Microsoft Edge, the native Windows 10 browser.\n\nApple invited the press media for its event \"Let us loop you in\" on March 10, 2016 in their own theatre \"Town Hall\" (at 1 Infinite Loop). 9.7-inch iPad Pro, iPhone SE, CareKit and updates to Apple Watch, HealthKit, ResearchKit and tvOS were released.\n\nApple invited the press media and developers for its event on June 13, 2016 at Moscone West to unveil new versions of iOS, watchOS, tvOS, and macOS, a revamped Apple Music design, and the Swift Playgrounds app - a learning tool for programming.\n\nThis keynote was streamed live on June 13, 2016.\n\nApple hosted a media event on September 7, 2016 with the invitation's tagline \"See you on the 7th\". iPhone 7 and iPhone 7 Plus were announced along with Apple's new wireless AirPods. iOS 10 was also released. Sia performed at the event.\n\nApple hosted a media event on October 27, 2016 with the tagline \"hello again\". A new generation of MacBook Pro was announced.\n\nThe 2017 Apple WWDC was held from 5–9 June 2017 in San Jose, California at its Convention Center. An all-new iPad Pro model was introduced with thinner bezels and a 10.5-inch screen size. It acquired many of the specs from the iPhone 7 and an A10X chip. The iPad Pro 12.9-inch was also refreshed with updated internals. iOS 11 was announced, with a developer beta released just after. Apple previewed many new Macs such as the MacBook, MacBook Pro, iMac, and an all-new iMac Pro. The three original models got new internals, while the iMac Pro has a ground-breaking structure.\nApple's final announcement was HomePod. This smart speaker has Siri capability and is Apple's competitor to Google Home.\n\nApple hosted a media event on September 12, 2017, with the tagline \"Let's meet at our place\". The tagline was a reference to Apple holding its first-ever event at the newly completed Steve Jobs Theater in the Apple Park campus. At the event, Apple Watch Series 3, Apple TV 4K, iPhone 8 and 8 Plus, and iPhone X were announced. The release dates of iOS 11 and watchOS 4 were also announced.\n\nApple hosted a media event on March 27 at the Lane Technical College Prep High School in Chicago. The 2018 iPad was announced at the education-focused event.\n\nWWDC 2018 was held from June 4–8 2018 at the San Jose Convention Center in California. The announcements at the event included iOS 12, macOS Mojave, watchOS 5, and updates to tvOS. Panic! at the Disco performed at the Bash at Discovery Meadow Park.\n\nApple hosted a media event on September 12, 2018, with the tagline \"Gather round\". It was held at the Steve Jobs Theater in the Apple Park campus. The Apple Watch Series 4, the iPhone XS and iPhone XS Max, and the iPhone XR were announced at this event.\n\nApple hosted a media event on October 30, 2018, with the tagline \"There's more in the making.\" It was held at the Brooklyn Academy of Music in Brooklyn, New York City. The new MacBook Air 2018 model, Mac Mini 2018 model, and the 11-inch third generation iPad Pro were announced at this event.\n\n"}
{"id": "519460", "url": "https://en.wikipedia.org/wiki?curid=519460", "title": "List of archive formats", "text": "List of archive formats\n\nThis is a list of file formats used by archivers and compressors used to create archive files.\n\n"}
{"id": "57017151", "url": "https://en.wikipedia.org/wiki?curid=57017151", "title": "List of gases", "text": "List of gases\n\nThis is a list of gases at standard conditions This means the substance boils at or below 25 °C at 1 atmosphere pressure and is reasonably stable.\n\"triple\" means that is a triple point, and that the substance sublimes at one atmosphere pressure.\n\nThe following list has substances known to be gases, but with an unknown boiling point.\n\nThis list includes substances that may be gases. However reliable references are not available.\n\nThis list includes substances that boil just above standard condition temperatures. Numbers are boiling temperatures in °C.\n"}
{"id": "36565421", "url": "https://en.wikipedia.org/wiki?curid=36565421", "title": "List of medical schools in Malaysia", "text": "List of medical schools in Malaysia\n\nMedical Schools in Malaysia generally offer a five-year undergraduate program for future doctors. It is compulsory for students who have graduated from medical school to work in the Government Hospital under housemanship program for a duration of three years. Under section 40 of the Malaysian Medical Act 1971, every practitioner has to serve a minimum period of continuous total period of not less than three years within the public services upon being given full registration as a doctor. As defined under Article 132 of the Federal Constitution, this service may be completed in a government healthcare facility, the Ministry of Health, or other government agencies as determined by the Director General of Health.\n\nEntry to the public medical schools is very competitive. Courses last five or six years. The medical program is usually divided into 3 phases. Phase 1 consists of the first two years of the programme involving integrated teaching and learning of the relevant basic medical sciences. Phase II (Year 3) and Phase III (Year 4 and Year 5) involve clinical skills development and subsequently consolidation of clinical clerkship in the various clinical disciplines. Two years of pre-clinical training in an academic environment and three years clinical training at a teaching hospital. Medical schools and teaching hospitals are closely integrated. Entry requirements are basically based on Sijil Tinggi Persekolahan Malaysia (STPM), Malaysian Matriculation Programme, GCE A Level, International Baccalaureate (IB), Foundation in Science programme, Diploma in Health Sciences or Unified Examination Certificate.\n"}
{"id": "12012158", "url": "https://en.wikipedia.org/wiki?curid=12012158", "title": "List of optimization software", "text": "List of optimization software\n\nGiven a transformation between input and output values, described by a mathematical function \"f\", optimization deals with generating and selecting a best solution from some set of available alternatives, by systematically choosing input values from within an allowed set, computing the output of the function, and recording the best output values found during the process. Many real-world problems can be modeled in this way. For example, the inputs can be design parameters of a motor, the output can be the power consumption, or the inputs can be business choices and the output can be the obtained profit.\n\nAn optimization problem, in this case a minimization problem, can be represented in the following way\n\nIn continuous optimization, \"A\" is some subset of the Euclidean space R, often specified by a set of \"constraints\", equalities or inequalities that the members of \"A\" have to satisfy. In combinatorial optimization, \"A\" is some subset of a discrete space, like binary strings, permutations, or sets of integers.\n\nThe use of optimization software requires that the function \"f\" is defined in a suitable programming language and connected at compile or run time to the optimization software. The optimization software will deliver input values in \"A\", the software module realizing \"f\" will deliver the computed value \"f\"(\"x\") and, in some cases, additional information about the function like derivatives.\n\nIn this manner, a clear separation of concerns is obtained: different optimization software modules can be easily tested on the same function \"f\", or a given optimization software can be used for different functions \"f\".\n\nThe following tables provide a list of notable optimization software organized according to license and business model type.\n\n\n\n\n\n"}
{"id": "7119989", "url": "https://en.wikipedia.org/wiki?curid=7119989", "title": "List of volcanoes in the Democratic Republic of the Congo", "text": "List of volcanoes in the Democratic Republic of the Congo\n\nThis is a list of active and extinct volcanoes in the Democratic Republic of the Congo. \n"}
{"id": "2011320", "url": "https://en.wikipedia.org/wiki?curid=2011320", "title": "Lists of prehistoric fish", "text": "Lists of prehistoric fish\n\nPrehistoric fish are early fish that are known only from fossil records. They are the earliest known vertebrates, and include the first and extinct fish that lived through the Cambrian to the Quaternary. The study of prehistoric fish is called \"paleoichthyology\". A few living forms, such as the coelacanth are also referred to as prehistoric fish, or even living fossils, due to their current rarity and similarity to extinct forms. Fish which have become recently extinct are not usually referred to as prehistoric fish.\n\nGroups of various prehistoric fishes include:\n\n\n\n"}
{"id": "34297996", "url": "https://en.wikipedia.org/wiki?curid=34297996", "title": "Louis Palmer", "text": "Louis Palmer\n\nLouis Palmer (born 1971 in Budapest) is a Swiss keynote speaker, global environmental adventurer, and \"Solar Pioneer\". \n\nLouis Palmer was raised in Luzern, Switzerland. Since he was a little child, his dream was to show the benefit of renewable energy sources and solar cars to the world. After finishing school, he became a schoolteacher. After several trips to foreign countries, he discovered his vision and took his first steps into Solar Energy. In 2007 and 2008, he was the first to circumnavigate the globe on solar power.\n\nLouis Palmer is a motivational speaker at conferences worldwide. He speaks about his vision of solar energy and about his dream of a better world with renewable energy. His experience as a teacher is part of his special way of communicating to people. As a conference speaker, he has had the opportunity to speak to audiences of researchers, politicians, and students. This part of educating the audience and inspiring people to change their consumption behaviour into a reflected awareness of the environment is of great importance to him. In 2009, Palmer was awarded the European Solar Prize, and in 2011, he was awarded Champion of the Earth for his achievements.\n\nIn 2004, with the help of sponsors and technical support, Palmer began building a solar-powered car called the \"Solartaxi\". For the technical expertise, he worked together with the Hochschule für Technik und Architektur Luzern and three other Swiss universities. He drove around the world in his Solartaxi between 2007 and 2008, logging over 54,000 kilometers through over 40 counties. In 2008, the tour ended in Luzern after 18 months, making it the first tour around the world in a solar-powered car. During the tour his companion Erik Schmitt made a documentary film about the tour. In 2009 he was awarded the European Solar Prize for his Solartaxi.\n\nAfter his Solartaxi tour, Palmer drew from his previous experience when he became the initiator and tour director of an organized race. The Zero Emissions Race took place between August 2010 and February 2011. The aim was to make it around the world in 80 days and to show what emission-free vehicles can accomplish. Palmer invited four teams to enter the competition, setting vehicle criteria that were quite concrete: They had to be propelled by an electric motor, they had to be able to drive a certain distance in a certain time, and they had to be able to carry at least two passengers. Through Zero Emissions Race, Palmer was once again able to draw attention to his vision of sustainable living.\n\nPamer's third tour began on 10 September 2011 in Paris and finished on 25 September 2011 in Prague. 25 electric vehicles from eight countries joined his first WAVE across 3,000 km and eight countries. The tour stopped in around 30 cities to demonstrate to the public that electric vehicles are reliable, fun, and powerful. All vehicles had to produce their own electricity from a renewable source such as wind or solar and feed it into the grid. In December, the second WAVE took place in India. Five electric cars joined this tour from Mumbai to Bangalore and back. In 2014, 75 vehicles have already joined the tour, and Palmer set the world record for the largest electric vehicle parade with 481 vehicles, which joined the WAVE at its start in Stuttgart on May 31, 2014.\n\nTogether with his wife, Dr. Julianna Priskin, Palmer co-founded the Switzerland Explorer\" with the world's first 100% electric tour bus. The aim of Switzerland Explorer is to take groups or individual tourists on sustainable tours around Switzerland. The bus was originally in service for the German army, and then it was converted to electric drive by Design-werk. It can transport up to 16 passengers and has a range of up to 300 km.\n\n\n"}
{"id": "17375993", "url": "https://en.wikipedia.org/wiki?curid=17375993", "title": "Meyerhofferite", "text": "Meyerhofferite\n\nMeyerhofferite is a hydrated borate mineral of calcium, with the chemical formula CaBO(OH)·2HO, CaBO(OH)·HO or Ca(HBO)·4HO. It occurs principally as an alteration product of inyoite, another borate mineral.\n\nNatural meyerhofferite was discovered in 1914 in Death Valley, California It is named for German chemist Wilhelm Meyerhoffer (1864–1906), collaborator with J. H. van't Hoff on the composition and origin of saline minerals, who first synthesized the compound.\n"}
{"id": "739331", "url": "https://en.wikipedia.org/wiki?curid=739331", "title": "Mike Melvill", "text": "Mike Melvill\n\nMichael Winston \"Mike\" Melvill (born November 30, 1940 Johannesburg ) is a world-record-breaking pilot and one of the test pilots for SpaceShipOne, the experimental spaceplane developed by Scaled Composites. Melvill piloted SpaceShipOne on its first flight past the edge of space, flight 15P on June 21, 2004, thus becoming the first commercial astronaut and the 434th person to go into space. He was also the pilot on SpaceShipOne's flight 16P, the first competitive flight in the Ansari X Prize competition.\n\nIn 1978, Melvill met aerospace designer and Scaled Composites founder Burt Rutan when he flew to California to show Rutan the VariViggen he had built at his home. Rutan then hired him on the spot. In 1982, he was named Rutan's lead test pilot.\n\nIn 1997, Melvill and Dick Rutan, Burt's brother, flew two Long-Eze aircraft that they built side-by-side around the world. This \"around the world in 80 nights\" flight was called The Spirit of EAA Friendship World Tour, and some legs of it lasted for over 14 hours.\n\nLater in his career he became Vice President/General Manager at Scaled Composites.\n\nMelvill is the sole or joint holder of nine FAI aviation world records in various categories.\n\nHe was awarded the Iven C. Kincheloe Award in 1999 for high altitude, developmental flight-testing of the model 281 Proteus aircraft.\n\nThrough SpaceShipOne flight 15P in 2004, he is known as the first privately funded human spaceflight mission pilot to reach space.\n\n"}
{"id": "49174555", "url": "https://en.wikipedia.org/wiki?curid=49174555", "title": "Mithan Lal Roonwal", "text": "Mithan Lal Roonwal\n\nMithan Lal Roonwal (18 September 1908 – 22 July 1990) was an Indian zoologist and director of the Zoological Survey of India. Roonwal studied a range of taxa from termites to mammals. His landmark works were on the biology of termites and a monograph on the primates of South Asia. He was the first to note geographic patterns in the shape in which gray langurs (treated in his time as a single species) carried their tails. He described many new species during his career at the Zoological Survey of India.\n\nRoonwal was born in Jodhpur where he received his early education. He then studied at Lucknow where he obtained a Master of Science in 1930. He began work at the Locust Research Institute in 1931 at Lyallpur and went to Cambridge where he worked on the embryology of \"Schistocerca gregaria\" under the guidance of Augustus Daniel Imms and obtained a Ph.D. in 1935. He joined the Zoological Survey of India where he worked at the birds and mammals section from 1939 to 1940. Roonwal joined the army during the Second World War and served as a Major in the 5th Punjab Regiment and received a Burma Star for his services. From 1946 he continued his studies on the desert locust and published ideas on predicting the formation of swarms. He joined the Forest Research Institute in 1949 as an entomologist and worked on cataloguing the insect collections held there. He continued working there until 1956 during which time he took an interest in the systematics of termites in which he collaborated with Alfred E. Emerson. In 1956 he joined the Zoological Survey of India to succeed Sunder Lal Hora as the director. He retired from the ZSI in 1965 and joined Jodhpur University. He received a Sc.D. for his work on the morphology and systematics of termites in 1962 from Cambdrige University.\n\n"}
{"id": "547743", "url": "https://en.wikipedia.org/wiki?curid=547743", "title": "Multidisciplinary design optimization", "text": "Multidisciplinary design optimization\n\nMulti-disciplinary design optimization (MDO) is a field of engineering that uses optimization methods to solve design problems incorporating a number of disciplines. It is also known as multidisciplinary optimization and multidisciplinary system design optimization (MSDO).\n\nMDO allows designers to incorporate all relevant disciplines simultaneously. The optimum of the simultaneous problem is superior to the design found by optimizing each discipline sequentially, since it can exploit the interactions between the disciplines. However, including all disciplines simultaneously significantly increases the complexity of the problem.\n\nThese techniques have been used in a number of fields, including automobile design, naval architecture, electronics, architecture, computers, and electricity distribution. However, the largest number of applications have been in the field of aerospace engineering, such as aircraft and spacecraft design. For example, the proposed Boeing blended wing body (BWB) aircraft concept has used MDO extensively in the conceptual and preliminary design stages. The disciplines considered in the BWB design are aerodynamics, structural analysis, propulsion, control theory, and economics.\n\nTraditionally engineering has normally been performed by teams, each with expertise in a specific discipline, such as aerodynamics or structures. Each team would use its members' experience and judgement to develop a workable design, usually sequentially. For example, the aerodynamics experts would outline the shape of the body, and the structural experts would be expected to fit their design within the shape specified. The goals of the teams were generally performance-related, such as maximum speed, minimum drag, or minimum structural weight.\n\nBetween 1970 and 1990, two major developments in the aircraft industry changed the approach of aircraft design engineers to their design problems. The first was computer-aided design, which allowed designers to quickly modify and analyse their designs. The second was changes in the procurement policy of most airlines and military organizations, particularly the military of the United States, from a performance-centred approach to one that emphasized lifecycle cost issues. This led to an increased concentration on economic factors and the attributes known as the \"ilities\" including manufacturability, reliability, maintainability, etc.\n\nSince 1990, the techniques have expanded to other industries. Globalization has resulted in more distributed, decentralized design teams. The high-performance personal computer has largely replaced the centralized supercomputer and the Internet and local area networks have facilitated sharing of design information. Disciplinary design software in many disciplines (such as OptiStruct or NASTRAN, a finite element analysis program for structural design) have become very mature. In addition, many optimization algorithms, in particular the population-based algorithms, have advanced significantly.\n\nWhereas optimization methods are nearly as old as calculus, dating back to Isaac Newton, Leonhard Euler, Daniel Bernoulli, and Joseph Louis Lagrange, who used them to solve problems such as the shape of the catenary curve, numerical optimization reached prominence in the digital age. Its systematic application to structural design dates to its advocacy by Schmit in 1960. The success of structural optimization in the 1970s motivated the emergence of multidisciplinary design optimization (MDO) in the 1980s. Jaroslaw Sobieski championed decomposition methods specifically designed for MDO applications. The following synopsis focuses on optimization methods for MDO. First, the popular gradient-based methods used by the early structural optimization and MDO community are reviewed. Then those methods developed in the last dozen years are summarized.\n\nThere were two schools of structural optimization practitioners using gradient-based methods during the 1960s and 1970s: optimality criteria and mathematical programming. The optimality criteria school derived recursive formulas based on the Karush–Kuhn–Tucker (KKT) necessary conditions for an optimal design. The KKT conditions were applied to classes of structural problems such as minimum weight design with constraints on stresses, displacements, buckling, or frequencies [Rozvany, Berke, Venkayya, Khot, et al.] to derive resizing expressions particular to each class. The mathematical programming school employed classical gradient-based methods to structural optimization problems. The method of usable feasible directions, Rosen’s gradient projection (generalized reduce gradient) method, sequential unconstrained minimization techniques, sequential linear programming and eventually sequential quadratic programming methods were common choices. Schittkowski et al. reviewed the methods current by the early 1990s.\n\nThe gradient methods unique to the MDO community derive from the combination of optimality criteria with math programming, first recognized in the seminal work of Fleury and Schmit who constructed a framework of approximation concepts for structural optimization. They recognized that optimality criteria were so successful for stress and displacement constraints, because that approach amounted to solving the dual problem for Lagrange multipliers using linear Taylor series approximations in the reciprocal design space. In combination with other techniques to improve efficiency, such as constraint deletion, regionalization, and design variable linking, they succeeded in uniting the work of both schools. This approximation concepts based approach forms the basis of the optimization modules in modern structural design software such as Altair – Optistruct, ASTROS, MSC.Nastran, PHX ModelCenter, Genesis, iSight, and I-DEAS.\n\nApproximations for structural optimization were initiated by the reciprocal approximation Schmit and Miura for stress and displacement response functions. Other intermediate variables were employed for plates. Combining linear and reciprocal variables, Starnes and Haftka developed a conservative approximation to improve buckling approximations. Fadel chose an appropriate intermediate design variable for each function based on a gradient matching condition for the previous point. Vanderplaats initiated a second generation of high quality approximations when he developed the force approximation as an intermediate response approximation to improve the approximation of stress constraints. Canfield developed a Rayleigh quotient approximation to improve the accuracy of eigenvalue approximations. Barthelemy and Haftka published a comprehensive review of approximations in 1993.\n\nIn recent years, non-gradient-based evolutionary methods including genetic algorithms, simulated annealing, and ant colony algorithms came into existence. At present, many researchers are striving to arrive at a consensus regarding the best modes and methods for complex problems like impact damage, dynamic failure, and real-time analyses. For this purpose, researchers often employ multiobjective and multicriteria design methods.\n\nMDO practitioners have investigated optimization methods in several broad areas in the last dozen years. These include decomposition methods, approximation methods, evolutionary algorithms, memetic algorithms, response surface methodology, reliability-based optimization, and multi-objective optimization approaches.\n\nThe exploration of decomposition methods has continued in the last dozen years with the development and comparison of a number of approaches, classified variously as hierarchic and non hierarchic, or collaborative and non collaborative. \nApproximation methods spanned a diverse set of approaches, including the development of approximations based on surrogate models (often referred to as metamodels), variable fidelity models, and trust region management strategies. The development of multipoint approximations blurred the distinction with response surface methods. Some of the most popular methods include Kriging and the moving least squares method.\n\nResponse surface methodology, developed extensively by the statistical community, received much attention in the MDO community in the last dozen years. A driving force for their use has been the development of massively parallel systems for high performance computing, which are naturally suited to distributing the function evaluations from multiple disciplines that are required for the construction of response surfaces. Distributed processing is particularly suited to the design process of complex systems in which analysis of different disciplines may be accomplished naturally on different computing platforms and even by different teams.\n\nEvolutionary methods led the way in the exploration of non-gradient methods for MDO applications. They also have benefited from the availability of massively parallel high performance computers, since they inherently require many more function evaluations than gradient-based methods. Their primary benefit lies in their ability to handle discrete design variables and the potential to find globally optimal solutions.\n\nReliability-based optimization (RBO) is a growing area of interest in MDO. Like response surface methods and evolutionary algorithms, RBO benefits from parallel computation, because the numeric integration to calculate the probability of failure requires many function evaluations. One of the first approaches employed approximation concepts to integrate the probability of failure. The classical first-order reliability method (FORM) and second-order reliability method (SORM) are still popular. Professor Ramana Grandhi used appropriate normalized variables about the most probable point of failure, found by a two-point adaptive nonlinear approximation to improve the accuracy and efficiency. Southwest Research Institute has figured prominently in the development of RBO, implementing state-of-the-art reliability methods in commercial software. RBO has reached sufficient maturity to appear in commercial structural analysis programs like Altair's Optistruct and MSC's Nastran.\n\nUtility-based probability maximization (Bordley and Pollock, Operations Research, Sept, 2009, pg.1262) was developed in response to some logical concerns (e.g., Blau's Dilemma) with reliability-based design optimization. This approach focuses on maximizing the joint probability of both the objective function exceeding some value and of all the constraints being satisfied. When there is no objective function, utility-based probability maximization reduces to a probability-maximization problem. When there are no uncertainties in the constraints, it reduces to a constrained utility-maximization problem. (This second equivalence arises because the utility of a function can always be written as the probability of that function exceeding some random variable.) Because it changes the constrained optimization problem associated with reliability-based optimization into an unconstrained optimization problem, it often leads to computationally more tractable problem formulations.\n\nIn the marketing field there is a huge literature about optimal design for multiattribute products and services, based on experimental analysis to estimate models of consumers' utility functions. These methods are known as Conjoint Analysis. Respondents are presented with alternative products, measuring preferences about the alternatives using a variety of scales and the utility function is estimated with different methods (varying from regression and surface response methods to choice models). The best design is formulated after estimating the model. The experimental design is usually optimized to minimize the variance of the estimators. These methods are widely used in practice.\n\nProblem formulation is normally the most difficult part of the process. It is the selection of design variables, constraints, objectives, and models of the disciplines. A further consideration is the strength and breadth of the interdisciplinary coupling in the problem.\n\nA design variable is a specification that is controllable from the point of view of the designer. For instance, the thickness of a structural member can be considered a design variable. Another might be the choice of material. Design variables can be continuous (such as a wing span), discrete (such as the number of ribs in a wing), or boolean (such as whether to build a monoplane or a biplane). Design problems with continuous variables are normally solved more easily.\n\nDesign variables are often bounded, that is, they often have maximum and minimum values. Depending on the solution method, these bounds can be treated as constraints or separately.\n\nOne of the important variables that needs to be accounted is an uncertainty. Uncertainty, often referred to as epistemic uncertainty, arises due to lack of knowledge or incomplete information. Uncertainty is essentially unknown variable but it may causes the failure of system.\n\nA constraint is a condition that must be satisfied in order for the design to be feasible. An example of a constraint in aircraft design is that the lift generated by a wing must be equal to the weight of the aircraft. In addition to physical laws, constraints can reflect resource limitations, user requirements, or bounds on the validity of the analysis models. Constraints can be used explicitly by the solution algorithm or can be incorporated into the objective using Lagrange multipliers.\n\nAn objective is a numerical value that is to be maximized or minimized. For example, a designer may wish to maximize profit or minimize weight. Many solution methods work only with single objectives. When using these methods, the designer normally weights the various objectives and sums them to form a single objective. Other methods allow multiobjective optimization, such as the calculation of a Pareto front.\n\nThe designer must also choose models to relate the constraints and the objectives to the design variables. These models are dependent on the discipline involved. They may be empirical models, such as a regression analysis of aircraft prices, theoretical models, such as from computational fluid dynamics, or reduced-order models of either of these. In choosing the models the designer must trade off fidelity with analysis time.\n\nThe multidisciplinary nature of most design problems complicates model choice and implementation. Often several iterations are necessary between the disciplines in order to find the values of the objectives and constraints. As an example, the aerodynamic loads on a wing affect the structural deformation of the wing. The structural deformation in turn changes the shape of the wing and the aerodynamic loads. Therefore, in analysing a wing, the aerodynamic and structural analyses must be run a number of times in turn until the loads and deformation converge.\n\nOnce the design variables, constraints, objectives, and the relationships between them have been chosen, the problem can be expressed in the following form:\n\nwhere formula_6 is an objective, formula_1 is a vector of design variables, formula_8 is a vector of inequality constraints, formula_9 is a vector of equality constraints, and formula_10 and formula_11 are vectors of lower and upper bounds on the design variables. Maximization problems can be converted to minimization problems by multiplying the objective by -1. Constraints can be reversed in a similar manner. Equality constraints can be replaced by two inequality constraints.\n\nThe problem is normally solved using appropriate techniques from the field of optimization. These include gradient-based algorithms, population-based algorithms, or others. Very simple problems can sometimes be expressed linearly; in that case the techniques of linear programming are applicable.\n\n\n\n\n\nMost of these techniques require large numbers of evaluations of the objectives and the constraints. The disciplinary models are often very complex and can take significant amounts of time for a single evaluation. The solution can therefore be extremely time-consuming. Many of the optimization techniques are adaptable to parallel computing. Much current research is focused on methods of decreasing the required time.\n\nAlso, no existing solution method is guaranteed to find the global optimum of a general problem (see No free lunch in search and optimization). Gradient-based methods find local optima with high reliability but are normally unable to escape a local optimum. Stochastic methods, like simulated annealing and genetic algorithms, will find a good solution with high probability, but very little can be said about the mathematical properties of the solution. It is not guaranteed to even be a local optimum. These methods often find a different design each time they are run.\n\n\n"}
{"id": "50139698", "url": "https://en.wikipedia.org/wiki?curid=50139698", "title": "National Science Museum at Maynooth", "text": "National Science Museum at Maynooth\n\nThe National Science and Ecclesiology Museum at Maynooth is a science museum and museum of ecclesiology, located on the joint campus of St Patrick's College, Maynooth and Maynooth University (the southern campus of the university), Ireland. It is an institution of the college, having begun as an ecclesiological museum. \n\nThe museum holds various artefacts from the history of science in Ireland (the largest such collection open to the public in Ireland), a large collection of scientific equipment used by Nicholas Callan, and one of two death masks of Irish political leader Daniel O'Connell. The museum was founded in 1934 as the Museum of Ecclesiology but has become more focused on science, partially due to Maynooth's association with Callan.\n\nThe museum was founded in 1934 as the Museum of Ecclesiology in what was then simply St. Patrick's College, with Dr. William Moran, Professor of Dogmatic Theology, as its first curator. After Moran resigned in 1942, the Very Rev. Dr. Patrick J McLaughlin (then Professor of Experimental Physics and later Vice-President of the College) was appointed as curator. Moran oversaw the transferral of much of Callan's apparatus, which are now on display, into the museum, a process that was completed by his successor, Rev. Dr. Michael Casey.\n\nThe museum has two main collections: a collection of scientific instruments associated with Nicholas Callan and a collection of ecclesiastical artifacts.\nThe Callan collection is significant because it includes the first induction coil, invented by Callan in 1836. There are a number of his other induction coils, including his giant induction coil (pictured) which he created in 1845, which produced 600,000 volts. There are two coils in this invention, the primary coil and the secondary coil. Over twenty miles of wires in the induction coil were hand-insulated with bees wax. Callan used seminarians holding hands touching the coil to measure the strength of the current. The current passed through the seminarians and Callan judged its strength by the height the seminarians jumped. He worked with a local blacksmith to create the large electromagnet (pictured) in 1836.\n\nThe collection contains a number of documents and books, including a royal patent for galvanization.\n\nThere is also a large holding of nineteenth century batteries in the museum. Other items of note in the collection include a Norremberg polariscope, a nineteenth century polarizing microscope, and the first portable GPS device (pictured below).\n\nThe museum's ecclesiastical collection includes a set of priestly vestments including a set of robes commissioned under the royal patronage of Marie Antoinette. They include an old Ecce Homo (12C) and a leaf of an ivory diptych (14C, Northern France). In addition there are a number of altar stones. Also of historical note is a statue of Jesus that was defaced by Cromwellian soldiers during the Siege of Drogheda in 1649.\n\nNavigation instruments are also to be found in the museum, namely several octants and sextants which were the life blood of navigation on the seas. These instruments are used to fix the position of a ship on the ocean. The octant was a hand-held instrument,with mirrors, used to look at the line of the horizon, take the measurements from the marked scale, and use sea charts to calculate your position. The year 1731 saw the Hadley octant improved in design to become the forerunner of the modern sextant. The octant inscribed with the name 'Yeates' in the collection appears to refer to a George Yeates, active from 1826-1858.\nThe museum opens several days a week out of academic term, and by appointment from October to May.\n\n"}
{"id": "42838802", "url": "https://en.wikipedia.org/wiki?curid=42838802", "title": "NetGuide", "text": "NetGuide\n\nNetGuide is a live news website with weekly email newsletters dedicated to New Zealand consumers, covering technology news, product reviews and buying advice. NetGuide is the largest site in the Techday network.\n\n\"NetGuide\" was launched in September 1996 by an independent Auckland-based publisher, then owned by Australian Consolidated Press (ACP) 2003 to 2008, then shifted to Action Media. It is now owned exclusively by Techday Ltd. Originally entirely internet focused, it developed to cover wider computer, telecommunications, and technology issues. \nThe magazine competed with IDG's \"New Zealand PC World\" and ran the annual \"NetGuide Web Awards\".\n\nIn early 2014, NetGuide and the entire Techday network discontinued all print publications and moved to exclusively online content. \n\nIn April 2017, NetGuide was renamed as FutureFive NZ. \n\nThe magazine launched a version for the Australian market, \"Australian NetGuide\", which ran 1996–2009. It competed with NineMSN's \"apc\" and IDG's \"Australian PC World\". Now, Australian news is integrated into Netguide's New Zealand website, or published under one of Techday's Australian brands: IT Brief Australia, SecurityBrief Australia, or ChannelLife Australia. \n\n"}
{"id": "5473827", "url": "https://en.wikipedia.org/wiki?curid=5473827", "title": "Parachlamydiaceae", "text": "Parachlamydiaceae\n\nParachlamydiaceae is a family of bacteria in the order Chlamydiales. Species in this family have a \"Chlamydia\"–like cycle of replication and their ribosomal RNA genes are 80–90% identical to ribosomal genes in the Chlamydiaceae. The Parachlamydiaceae naturally infect amoebae and can be grown in cultured Vero cells. The Parachlamydiaceae are not recognized by monoclonal antibodies that detect Chlamydiaceae lipopolysaccharide.\n\nParachlamydiaceae species currently include:\n\n\"Isolated Endosymbionts include:\"\n\n\"Uncultured lineages include:\"\n\n\"Parachlamydia acanthamoebae\" has variable Gram staining characteristics and is mesophilic. Trophozoites of \"Acanthamoeba\" hosting these strains were isolated from asymptomatic women in Germany and also in an outbreak of humidifier fever (‘Hall’s coccus’) in Vermont USA. Four patients from Nova Scotia whose sera recognized Hall’s coccus did not show serological cross-reaction with antigens from the Chlamydiaceae.\n\nNotes:\n♠ Strain found at the National Center for Biotechnology Information (NCBI) but has no standing with the Bacteriological Code (1990 and subsequent Revision) as detailed by List of Prokaryotic names with Standing in Nomenclature (LPSN) as a result of the following reasons:\n• No pure culture isolated or available for Prokayotes.\n• Not validly published because the effective publication only documents deposit of the type strain in a single recognized culture collection.\n• Not approved and published by the International Journal of Systematic Bacteriology or the International Journal of Systematic and Evolutionary Microbiology (IJSB/IJSEM).\n"}
{"id": "32830599", "url": "https://en.wikipedia.org/wiki?curid=32830599", "title": "Repetitive control", "text": "Repetitive control\n\nRepetitive Control is a control method developed by a group of Japanese scholars in 1980s. It is based on the Internal Model Principle and used specifically in dealing with periodic signals, for example, tracking periodic reference or rejecting periodic disturbances. The repetitive control system has been proven to be a very effective and practical method dealing with periodic signals. Repetitive control has some similarities with iterative learning control. The differences between these two methods can be found in [Wang, Gao, and Doyle. 2009].\n"}
{"id": "2919118", "url": "https://en.wikipedia.org/wiki?curid=2919118", "title": "Scientistic materialism", "text": "Scientistic materialism\n\nScientistic materialism is a philosophical stance which posits a limited definition of consciousness to that which is observable and subject to the scientific method. The term is used as a pejorative by proponents of creationism and intelligent design.\n\nThe \"Wedge Document\" produced by the Discovery Institute, described \"materialism\" as denial of \"the proposition that human beings are created in the image of God,\" and that humans are instead \"animals or machines who inhabited a universe ruled by purely impersonal forces and whose behavior and very thoughts were dictated by the unbending forces of biology, chemistry and environment.\" The document states that materialism leads inevitably to \"moral relativism\" and denounce its \"stifling dominance\" in modern culture. By this definition, scientific materialism is linked to the more general version of materialism which declares that the physical world is the only thing that exists and that nothing supernatural exists.\n\n\n"}
{"id": "28888905", "url": "https://en.wikipedia.org/wiki?curid=28888905", "title": "Somerset Space Walk", "text": "Somerset Space Walk\n\nThe Somerset Space Walk is a sculpture trail model of the Solar System, located in Somerset, England. The model uses the towpath of the Bridgwater and Taunton Canal to display a model of the Sun and its planets in their proportionally correct sizes and distances apart. Unusually for a Solar System model, there are two sets of planets.\n\nAware of the inadequacies of printed pictures of the Solar System, the Space Walk was designed by inventor Pip Youngman as a way of challenging our perceptions of space and experiencing the vastness of the Solar System.\nThe model is built to a scale of 1:530,000,000, meaning that one millimetre on the model equates to 530 kilometres. The Sun is sited at Higher Maunsel Lock, and one set of planets is installed in each direction along the canal towards Taunton and Bridgwater; the distance between the Sun and each model of Pluto being . For less hardy walkers, the inner planets are within of the Sun, and near to the Maunsel Canal Centre (and tea shop) at Lower Maunsel Lock, where a more detailed leaflet about the model is available.\n\nThe Space Walk was opened on 9 August 1997 by British astronomer Heather Couper. In 2007, a project team from Somerset County Council refurbished some of the models.\n\nThe Walk is a joint venture between the Taunton Solar Model Group and British Waterways, with support from Somerset County Council, Taunton Deane Borough Council and the Somerset Waterways Development Trust. The Taunton Solar Model Group comprised Pip Youngman, Trevor Hill – a local physics teacher who had been awarded the title of \"Institute of Physics (IOP) Physics Teacher of the Year\" – and David Applegate who, during his time as Mayor of Taunton, had expressed a wish to see some kind of science initiative in the area. Pip came up with the idea for the Space Walk, and Trevor assisted by calculating the respective positions and sizes of the planets.\n\nFunding for the project came from the Committee on the Public Understanding of Science (COPUS), the initial advertising leaflet was paid for by the Particle Physics and Astronomy Research Council (PPARC) and there was also a small grant from Sustrans, who fund art installations along cyclepaths, to deal with maintenance requirements in the years before Somerset County Council took on that responsibility. In order to apply for the COPUS funding Youngman needed two 'sponsors', so he wrote to Arthur C. Clarke (a local boy himself, then living in Sri Lanka) and Patrick Moore, who both wrote warm letters in support. Arthur C Clarke's brother Fred read out his letter at the opening ceremony.\n\nReadyMix Concrete supplied the concrete for the plinths, and Avimo (now part of Thales Group), a local defence contractor, supplied the steel for the models.\n\nThe model of the Sun is a -wide 14-ton concrete sphere, with a vertical segment removed to give two vertical faces upon which explanatory plates are mounted. The solid sphere was cast by Pip Youngman and Trevor Hill in the grounds of what was the SWEB storage yard adjacent to the Obridge Viaduct in Taunton. Originally 'natural' in colour (matching the other models) it was painted yellow as part of the refurbishment, making it much more visible.\n\nEach of the smallest planet models is contained within a round-topped concrete plinth about high. The stainless steel model is held inside a circular hole through the side of the plinth; hence the model of the planet may be viewed by looking through the hole. The plinths were created by Youngman using fibreglass moulds which he had also made.\n\nThe models of the largest gas giants, Saturn and Jupiter, are moulded as part of the top face of the concrete pillars. Originally concrete-coloured, they have been painted as part of the refurbishment.\n\nEach pillar doubles-up as a milepost: the distance to Bridgwater and Taunton is cast in the concrete at ground level – below a depiction of the British Waterways 'bridge' logo – although the sculptures are sited according to the spacings needed for the model, and not at kilometre increments for the convenience of boaters.\n\nOn each pillar is a plaque containing a short inscription describing the planet. The Earth inscription reads:\n\nThe installation does not include a model of the nearest star for comparison, as this would be impossible. On the same scale as the other models, the nearest star (Proxima Centauri, which is about one-seventh the size of the Sun) would need to be a red ball in diameter sited away (roughly twice the circumference of Earth).\n\nThe Space Walk's designer, Philip Robert Vassar Youngman (born: 26 August 1924, Hunstanton, Norfolk – died: 23 May 2007, Taunton, Somerset), known as 'Pip', was a designer and inventor of mechanical apparatus. Around 1969, Youngman was approached by the Open University to adapt a mechanical calculator he had designed, originally prototyped in Lego, into a product suitable for school use. The result was the \"Ball Operated Binary Calculator And Tutor\" (BOBCAT), a mechanical model for teaching binary arithmetic and the inner workings of the computer, using ball bearings for binary data bits and plastic levers for the calculating logic.\n\nThe trail can be walked either from Taunton's Brewhouse Theatre to Maunsel Lock (Pluto to the Sun) or from Bridgwater's Morrison's Supermarket to Maunsel Lock (also Pluto to the Sun) or of course, vice versa.\n\nThe locations of the end and middle point (with postcodes and coordinates) are:\n\n\nThe models of the Solar System, in order:\n\n"}
{"id": "17010691", "url": "https://en.wikipedia.org/wiki?curid=17010691", "title": "The Red Queen: Sex and the Evolution of Human Nature", "text": "The Red Queen: Sex and the Evolution of Human Nature\n\nThe Red Queen: Sex and the Evolution of Human Nature is a popular science book by Matt Ridley exploring the evolutionary psychology of sexual selection. \"The Red Queen\" was one of seven books shortlisted for the 1994 Rhône-Poulenc Prize (now known as the Royal Society Prizes for Science Books), that was eventually won by Steve Jones' \"The Language of the Genes\".\n\nRidley argues that few, if any, aspects of human nature can be understood apart from sex, since human nature is a product of evolution, driven by sexual reproduction in the case of sexual selection in human evolution.\n\nThe book begins with an evolutionary account of sex itself, defending the theory that sex flourishes, despite its energetic costs, primarily because a sexually mixed heritage confers to offspring a defensive \"head start\" against parasites received from and originally adapted to the maternal host environment.\n\nToward the end of the book Ridley argues that human intelligence is largely a result of sexual selection. He argues that human intelligence far outstrips any survivalist demands that would have been placed on our hominid ancestors, and analogizes human intelligence to the peacock's tail, a trait widely believed to be the result of sexual selection. Human intelligence, he suggests, is used primarily to attract mates through prodigious displays of wit, charm, inventiveness, and individuality. This view of Intelligence is treated at length in Geoffrey Miller's \"The Mating Mind: How Sexual Choice Shaped the Evolution of Human Nature\" (2001).\n\n"}
{"id": "1941290", "url": "https://en.wikipedia.org/wiki?curid=1941290", "title": "The Science of Life", "text": "The Science of Life\n\nThe Science of Life is a book written by H. G. Wells, Julian Huxley and G. P. Wells, published in three volumes by The Waverley Publishing Company Ltd in 1929–30, giving a popular account of all major aspects of biology as known in the 1920s. It has been called \"the first modern textbook of biology\" and \"the best popular introduction to the biological sciences.\" Wells's most recent biographer notes that \"The Science of Life\" \"is not quite as dated as one might suppose.\"\n\nIn undertaking \"The Science of Life,\" H. G. Wells, who had published \"The Outline of History\" a decade earlier, selling over two million copies, desired the same sort of treatment for biology. He thought of his readership as \"the intelligent lower middle classes . . . [not] idiots, half-wits . . . greenhorns, religious fanatics . . . smart women or men who know all that there is to be known.\"\n\nJulian Huxley, the grandson of T. H. Huxley under whom Wells had studied biology, and his son \"Gip,\" a zoologist, divided the initial writing between them; H.G. Wells revised, dealt (with the help of his literary agent, A.P. Watt) with publishers, and acted as a strict taskmaster, often obliging his collaborators to sit down and work together and keeping them on a tight schedule. (H.G. Wells had begun the book during his wife's final illness and is said to have used work on the book as a way to keep his mind off his loss.)\n\nThe text as published is presented as the common work of a \"triplex author.\" H.G. Wells took 40% of the royalties; the remainder was split between Huxley and Wells's son. In his will, H.G. Wells left his rights in the book to G.P. Wells.\n\nIn 1927, Huxley gave up his chair of Zoology at King's College, London to concentrate on the work. Thanks to the success of the book, Huxley was able to give up teaching and devote himself to administration and experimental science.\n\nThe book was originally serialised in 31 fortnightly parts, published in 3 volumes in 1929–30 and in a single volume in 1931. The volume includes more than 300 illustrations. It was a great success, though the stock market crash and subsequent depression held back sales, in part because of declining memberships in book clubs.\n\nIt has been said of Book Four (\"The How and Why of Development and Evolution\") that it \"offers perhaps the clearest, most readable, succinct and informative popular account of the subject ever penned. It was here that [Huxley] first expounded his own version of what later developed into the evolutionary synthesis\".\n\n\"The Science of Life\" is also notable for its introduction of modern ecological concepts. It is also notable for its emphasis on the importance of behaviorism and Jung's psychology. Toward the end \"The Science of Life\" strays from the scientific to the moral realm and devotes a chapter (Book Eight, Ch. VIII: \"Modern Ideas of Conduct\") to practical moral advice to the reader, advising him (the masculine pronoun is used throughout, a universal practice circa 1930): \"After his primary duties to himself, the first duty of Mr. Everyman to others is to learn about himself, to acquire poise and make his persona as much of a cultivated gentleman as he can. He has to be considerate. He has to be trustworthy.\" In its last pages, Wells emphasises the lack of \"credibility\" of personal immortality, and advocates \"realization of [one's] participation in a greater being with which he identifies himself,\" whether this be \"the Deity\" or \"Man.\"\n\nThe full details of its publishing record are as follows:\n\nWells H.G., Huxley Julian S. and Wells G.P. 1929–30. \"The Science of Life: a summary of contemporary knowledge about life and its possibilities\". First issued in 31 fortnightly parts published by Amalgamated Press, 1929–30, bound up in three volumes as publication proceeded. A mail-order version of the book was also published, though this was dropped after the stock market crash. First issued in one volume by Cassell in 1931, reprinted 1934, 1937; popular edition, fully revised, with a new preface by H.G. Wells, 1938. Published as separate volumes by Cassell 1934–37: I \"The living body\". II \"Patterns of life\" (1934). III \"Evolution—fact and theory\". IV \"Reproduction, heredity and the development of sex\". V \"The history and adventure of life\". VI \"The drama of life\". VII \"How animals behave\" (1937). VIII \"Man's mind and behaviour\". IX \"Biology and the human race\". Published in New York by Doubleday, Doran & Co. 1931, 1934, 1939; and by The Literary Guild in 1934. Three of the Cassell spin-off books were also published by Doubleday in 1932: \"Evolution, fact and theory\"; \"The human mind and the behaviour of Man\"; \"Reproduction, genetics and the development of sex\". \"The Science of Life\" was translated into French. During World War II a one-volume edition designed for use in military classes was issued. As late as 1960 the work was still being used in college classes in the US\n\nOf historic interest is \"Book Three – The Incontrovertible Fact of Evolution\", comprising five chapters; I The fact to be proved, II The evidence in the rocks, III The evidence from plant and animal structure, IV The evidence from the variation and distribution of living things, V The evolution of Man. Considering that this was written less than five years from the Scopes Trial, it is a bold, comprehensive account of the scientific knowledge of evolution at the time. Book Four concentrates on the controversies about evolution concluding that \"the broad positions of Darwinism emerge from a scrutiny of the most exacting sort, essentially unchanged.\"\n\nThe section entitled \"The Ecological Outlook\" anticipates many of the themes of the later green movement, including stressing the importance of reducing pollution and protecting endangered species from extinction, as well as the importance of alternative power sources.\n\nThe reference given is the most complete available, but there may have been other publishers and dates, and some books may have been given alternative titles. There are editions in some other languages.\n\n\"The pagination is that of the 1934 Literary Guild edition.\"\n\n"}
{"id": "11449099", "url": "https://en.wikipedia.org/wiki?curid=11449099", "title": "Tim Naish", "text": "Tim Naish\n\nTimothy Raymond Naish is a New Zealand glaciologist. He is the director of the Antarctic Research Centre at Victoria University of Wellington. He has written about the collapse of Antarctica's Larsen B ice shelf. In 2002, between January 31 and March 7 the Larsen B ice shelf collapsed and broke up. Naish warned that the ice shelf of Weddell Sea is imperiled, and if the temperature rises by 3°C, the ice shelves of Antarctica will become thinner.\n\nIn the 2010 New Year Honours, Naish was awarded the New Zealand Antarctic Medal for services to Antarctic climate science.\n\n"}
{"id": "50138870", "url": "https://en.wikipedia.org/wiki?curid=50138870", "title": "Tomorrowland (book)", "text": "Tomorrowland (book)\n\nTomorrowland: Our Journey from Science Fiction to Science Fact is a 2015 non-fiction book by science journalist Steven Kotler and published by Amazon Publishing.\n\nThe book is composed of a series of essay articles that were published by Kotler in various online news publications, including \"The New York Times\", \"The Atlantic\", and \"Discover\". There are sixteen chapters made up of the same number of articles, each dealing with a different topic of technological innovation in a variety of fields.\n\n\"Kirkus Reviews\" praised Kotler for not just presenting the technological innovations themselves, but also focusing on the \"obsessive people behind the science\" and how his insight into their work encompasses a \"range from humane and gripping stories of redemption to indifferent research scientists unsure if their developments will even make the world a better place\". \"Library Journal\" reviewer Talea Anderson also noted the introduction of each essay and discussed technology focusing on \"presenting the array of often quirky inventors and early adopters who have engaged with it\" and ultimately recommended the book for readers of popular science. In a separate review in \"Library Journal\" of the audiobook, reviewer Lisa Youngblood recommended the book and its look at not only the technology, but also the \"social and moral questions that arise\" from the potential ramifications of the emerging technologies.\n\n\n"}
{"id": "56156830", "url": "https://en.wikipedia.org/wiki?curid=56156830", "title": "Wendy Sadler", "text": "Wendy Sadler\n\nWendy Sadler is a British science communicator and lecturer at Cardiff University. She is the founding director of Science Made Simple which focuses on engaging audiences with the physical sciences. Her areas of interest include inspiring the next generation of scientists, engineers and communicators; women in STEM; and making STEM subjects accessible to diverse audiences.\n\nSadler was born in 1972 and grew up in Wolverhampton, England. After graduating Cardiff University with a BSc in Physics and Music in 1994, Sadler considered a career as an acoustic engineer before becoming a manager at Techniquest. She has since completed an MSc in Science Communication at the Open University. Her dissertation assessed the long-term impact of science demonstration shows. She created the non-verbal theatre show called The Experimentrics, which mixed physical theatre and live science demonstrations to create \"a world of wordless mystery and fun\". Sadler is a LAMDA accredited public speaker and fellow of the Royal Society of the Arts. She regularly appears on television and radio discussing the importance of STEM education. Sadler is a Lecturer and Schools' Liaison Officer at Cardiff University. She is concerned about the state of science education in Wales.\n\nSadler is a physics communicator who has published 19 books for children. She has contributed to ITV Wales, BBC Radio and the Edinburgh Fringe Festival. In 2010 Sadler gave a TEDxCardiff talk entitled \"Music and the Machine\". \n\nSadler set up Science Made Simple (SMS) in 2002 with the mission to inspire the next generation of scientists and engineers. At the time, Sadler was the IOP Schools Lecturer. SMS develop and present interactive performances that travel to schools and festivals across the world, reaching 28 countries to date. They have produced shows, contributed to science television, radio programmes, and children's books, trained scientists and acted as consultants on UK research councils. In 2013, she received national media coverage for their tour of UK primary schools following a sell-out run at the Edinburgh Fringe Festival. SMS has reached more than 750,000 people. SMS is part of a multimillion-pound EU project investigating the use of performance as a tool to engage young people with science and society issues.\n\nSadler Chaired and co-authored the Task and Finish report on STEM engagement in Wales for the National Science Academy and was involved in the writing of the Talented Women for a Successful Wales report.\n\n"}
