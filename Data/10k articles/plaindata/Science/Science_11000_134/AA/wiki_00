{"id": "4973059", "url": "https://en.wikipedia.org/wiki?curid=4973059", "title": "AP Physics B", "text": "AP Physics B\n\nAP Physics B was an Advanced Placement science course equivalent to a year-long introductory college course in physics. High school students studied Newtonian mechanics, electricity and magnetism, fluid mechanics, and thermal physics, waves and optics, and atomic and nuclear physics in preparation for a cumulative exam given each May. The course was non-calculus-based and involved algebra and trigonometry to solve various physics problems. \n\nThis course also helped prepare students for the SAT Subject Test in Physics, which is also administered by College Board.\n\nBeginning in the 2014-2015 school year, the College Board has discontinued AP Physics B and replaced it with AP Physics 1 and AP Physics 2, making May 2014 the last administration of the AP Physics B exam.\n\nThe exam consisted of a 70 MCQ section, followed by a 6-7 FRQ section. Each section was 90 minutes and was worth 50% of the final score. The MCQ section banned calculators, while the FRQ allowed calculators and a list of common formulas. Overall, the exam was configured to approximately cover a set percentage of each of the five target categories:\n\nAccording to the College Board web site, the Physics B course provided \"a foundation in physics for students in the life sciences, premedicine, and some applied sciences, as well as other fields not directly related to science.\"\n\nStarting in the 2014–2015 school year, AP Physics B was no longer offered, and AP Physics 1 and AP Physics 2 took its place. Like AP Physics B, both are algebra-based, and both are designed to be taught as year-long courses.\n\nThe grade distributions for the Physics B scores from 2010 until its discontinuation in 2014 are as follows:\n"}
{"id": "52514644", "url": "https://en.wikipedia.org/wiki?curid=52514644", "title": "Acidicapsa borealis", "text": "Acidicapsa borealis\n\nAcidicapsa borealis is a Gram-negative, short rods and non-motile bacterium from the genus of Acidicapsa which has been isolated from sphagnum peat from the Tver Region in Russia.\n\n"}
{"id": "8608584", "url": "https://en.wikipedia.org/wiki?curid=8608584", "title": "Acoustic approximation", "text": "Acoustic approximation\n\nA fundamental principle in the field of acoustics, the acoustic approximation states that an acoustic wave is created by a small, adiabatic, pressure ripple riding on a comparatively large equilibrium (bias) pressure. Typically, the acoustic pressure is on the order of a few ppm of the equilibrium pressure.\n\nBy extension, the acoustic approximation also guarantees that an acoustic wave travels at a speed exactly equal to the local speed of sound, such that the Mach number:\n\nformula_1\n\n"}
{"id": "852721", "url": "https://en.wikipedia.org/wiki?curid=852721", "title": "Actuarial notation", "text": "Actuarial notation\n\nActuarial notation is a shorthand method to allow actuaries to record mathematical formulas that deal with interest rates and life tables.\n\nTraditional notation uses a halo system where symbols are placed as superscript or subscript before or after the main letter. Example notation using the halo system can be seen below.\n\nVarious proposals have been made to adopt a linear system where all the notation would be on a single line without the use of superscripts or subscripts. Such a method would be useful for computing where representation of the halo system can be extremely difficult. However, a standard linear system has yet to emerge.\n\nformula_1 is the annual effective interest rate, which is the \"true\" rate of interest over \"a year\". Thus if the annual interest rate is 12% then formula_2.\n\nformula_3 (pronounced \"i \"upper\" m\") is the nominal interest rate convertible formula_4 times a year, and is numerically equal to formula_4 times the effective rate of interest over one formula_4 of a year. For example, formula_7 is the nominal rate of interest convertible semiannually. If the effective annual rate of interest is 12%, then formula_8 represents the effective interest rate every six months. Since formula_9, we have formula_10 and hence formula_11. The appearing in the symbol formula_3 is not an \"exponent.\" It merely represents the number of interest conversions, or compounding times, per year. Semi-annual compounding, (or converting interest every six months), is frequently used in valuing bonds (see also fixed income securities) and similar monetary financial liability instruments, whereas home mortgages frequently convert interest monthly. Following the above example again where formula_13, we have formula_14 since formula_15.\n\nEffective and nominal rates of interest are not the same because interest paid in earlier measurement periods \"earns\" interest in later measurement periods; this is called compound interest. That is, nominal rates of interest credit interest to an investor, (alternatively charge, or debit, interest to a debtor), more frequently than do effective rates. The result is more frequent compounding of interest income to the investor, (or interest expense to the debtor), when nominal rates are used.\n\nThe symbol formula_16 represents the present value of 1 to be paid one year from now:\nThis present value factor, or discount factor, is used to determine the amount of money that must be invested now in order to have a given amount of money in the future. For example, if you need 1 in one year, then the amount of money you should invest now is: formula_18. If you need 25 in 5 years the amount of money you should invest now is: formula_19.\n\nformula_20 is the annual effective discount rate:\nThe value of formula_20 can also be calculated from the following relationships: formula_23\nThe rate of discount equals the amount of interest earned during a one-year period, divided by the balance of money at the end of that period. By contrast, an annual effective rate of interest is calculated by dividing the amount of interest earned during a one-year period by the balance of money at the beginning of the year. The present value (today) of a payment of 1 that is to be made formula_24 years in the future is formula_25. This is analogous to the formula formula_26 for the future (or accumulated) value formula_24 years in the future of an amount of 1 invested today.\n\nformula_28, the nominal rate of discount convertible formula_29 times a year, is analogous to formula_3. Discount is converted on an formula_4-ly basis.\n\nformula_32, the force of interest, is the limiting value of the nominal rate of interest when formula_4 increases without bound:\n\nIn this case, interest is convertible continuously.\n\nThe general relationship between formula_1, formula_32 and formula_20 is:\n\nTheir numerical value can be compared as follows:\n\nA life table (or a mortality table) is a mathematical construction that shows the number of people alive (based on the assumptions used to build the table) at a given age. In addition to the number of lives remaining at each age, a mortality table typically provides various probabilities associated with the development of these values.\n\nformula_40 is the number of people alive, relative to an original cohort, at age formula_41. As age increases the number of people alive decreases.\n\nformula_42 is the starting point for formula_40: the number of people alive at age 0. This is known as the radix of the table. Some mortality tables begin at an age greater than 0, in which case the radix is the number of people assumed to be alive at the youngest age in the table.\n\nformula_44 is the limiting age of the mortality tables. formula_45 is zero for all formula_46.\n\nformula_47 is the number of people who die between age formula_41 and age formula_49. formula_47 may be calculated using the formula formula_51\n\nformula_52 is the probability of death between the ages of formula_41 and age formula_49.\n\nformula_56 is the probability that a life age formula_41 will survive to age formula_49.\n\nSince the only possible alternatives from one age (formula_41) to the next (formula_61) are living and dying, the relationship between these two probabilities is:\n\nThese symbols may also be extended to multiple years, by inserting the number of years at the bottom left of the basic symbol.\n\nformula_63 shows the number of people who die between age formula_41 and age formula_65.\n\nformula_66 is the probability of death between the ages of formula_41 and age formula_65.\n\nformula_70 is the probability that a life age formula_41 will survive to age formula_65.\n\nAnother statistic that can be obtained from a life table is life expectancy.\n\nformula_74 is the curtate expectation of life for a person alive at age formula_41. This is the expected number of complete years remaining to live (you may think of it as the expected number of birthdays that the person will celebrate).\n\nA life table generally shows the number of people alive at integral ages. If we need information regarding a fraction of a year, we must make assumptions with respect to the table, if not already implied by a mathematical formula underlying the table. A common assumption is that of a Uniform Distribution of Deaths (UDD) at each year of age. Under this assumption, formula_77 is a linear interpolation between formula_40 and formula_79. i.e.\n\nThe basic symbol for the present value of an annuity is formula_81. The following notation can then be added:\n\n\nIf the payments to be made under an annuity are independent of any life event, it is known as an annuity-certain. Otherwise, in particular if payments end upon the beneficiary's death, it is called a life annuity.\n\nformula_82 (read \"a-angle-n at i\") represents the present value of an annuity-immediate, which is a series of unit payments at the end of each year for formula_83 years (in other words: the value one period before the first of \"n\" payments). This value is obtained from:\n\nformula_85 represents the present value of an annuity-due, which is a series of unit payments at the beginning of each year for formula_83 years (in other words: the value at the time of the first of \"n\" payments). This value is obtained from:\n\nformula_88 is the value at the time of the last payment, formula_89 the value one period later.\n\nIf the symbol formula_90 is added to the top-right corner, it represents the present value of an annuity whose payments occur each one formula_4th of a year for a period of formula_83 years, and each payment is one formula_4th of a unit.\n\nformula_96 is the limiting value of formula_97 when formula_4 increases without bound. The underlying annuity is known as a continuous annuity.\n\nThe present values of these annuities may be compared as follows:\n\nTo understand the relationships shown above, consider that cash flows paid at a later time have a smaller present value than cash flows of the same total amount that are paid at earlier times.\n\n\nA life annuity is an annuity whose payments are contingent on the continuing life of the annuitant. The age of the annuitant is an important consideration in calculating the actuarial present value of an annuity.\n\n\nFor example:\n\nformula_104 indicates an annuity of 1 unit per year payable at the end of each year until death to someone currently age 65\n\nformula_105 indicates an annuity of 1 unit per year payable for 10 years with payments being made at the end of each year\n\nformula_106 indicates an annuity of 1 unit per year for 10 years, or until death if earlier, to someone currently age 65\n\nformula_107 indicates an annuity of 1 unit per year until the earlier death of member or death of spouse, to someone currently age 65 and spouse age 64\n\nformula_108 indicates an annuity of 1 unit per year until the later death of member or death of spouse, to someone currently age 65 and spouse age 64. \n\nformula_109 indicates an annuity of 1 unit per year payable 12 times a year (1/12 unit per month) until death to someone currently age 65\n\nformula_110 indicates an annuity of 1 unit per year payable at the start of each year until death to someone currently age 65\n\nor in general:\n\nformula_111, where formula_41 is the age of the annuitant, formula_83 is the number of years of payments (or until death if earlier), formula_4 is the number of payments per year, and formula_101 is the interest rate.\n\nIn the interest of simplicity the notation is limited and does not, for example, show whether the annuity is payable to a man or a woman (a fact that would typically be determined from the context, including whether the life table is based on male or female mortality rates).\nThe Actuarial Present Value of life contingent payments can be treated as the mathematical expectation of a present value random variable, or calculated through the current payment form.\n\nThe basic symbol for a life insurance is formula_116. The following notation can then be added:\n\n\nFor example:\n\nformula_118 indicates a life insurance benefit of 1 payable at the end of the year of death.\n\nformula_119 indicates a life insurance benefit of 1 payable at the end of the month of death.\n\nformula_120 indicates a life insurance benefit of 1 payable at the (mathematical) instant of death.\n\nAmong actuaries, force of mortality refers to what economists and other social scientists call the hazard rate and is construed as an instantaneous rate of mortality at a certain age measured on an annualized basis.\n\nIn a life table, we consider the probability of a person dying between age (\"x\") and age \"x\" + 1; this probability is called \"q\". In the continuous case, we could also consider the conditional probability that a person who has attained age (\"x\") will die between age (\"x\") and age (\"x\" + Δ\"x\") as:\n\nwhere \"F\"(\"x\") is the cumulative distribution function of the continuous age-at-death random variable, X. As Δ\"x\" tends to zero, so does this probability in the continuous case. The approximate force of mortality is this probability divided by Δ\"x\". If we let Δ\"x\" tend to zero, we get the function for force of mortality, denoted as \"μ\"(\"x\"):\n\n\n"}
{"id": "2479571", "url": "https://en.wikipedia.org/wiki?curid=2479571", "title": "Admontite", "text": "Admontite\n\nAdmontite is a hydrated magnesium borate mineral with formula MgBO·7HO. \n\nOccurrence - In a gypsum deposit.\nAssociations: gypsum, anhydrite, hexahydrite, löweite, eugsterite, pyrite, quartz.\n\nIt is named after Admont, Austria. Its Mohs scale rating is 2 to 3.\n"}
{"id": "27138995", "url": "https://en.wikipedia.org/wiki?curid=27138995", "title": "Altimir Glacier", "text": "Altimir Glacier\n\nAltimir Glacier (, 'Lednik Altimir' \\'led-nik al-ti-'mir\\) is a long and wide glacier draining the north slopes of the Osterrieth Range on Anvers Island in the Palmer Archipelago, Antarctica. It flows northwards to enter Dalchev Cove in Fournier Bay east of Studena Point.\n\nThe glacier is named after the settlement of Altimir in northwestern Bulgaria.\n\nAltimir Glacier is located at . It was mapped by the British Directorate of Overseas Surveys in 1980.\n\n\n\n"}
{"id": "38367028", "url": "https://en.wikipedia.org/wiki?curid=38367028", "title": "Angular momentum problem", "text": "Angular momentum problem\n\nThe angular momentum problem is a problem in astrophysics identified by Leon Mestel in 1965.\n\nIt was found that the angular momentum of a protoplanetary disk is misappropriated when compared to models during stellar birth. The Sun and other stars are predicted by models to be rotating considerably faster than they actually are. The Sun, for example, only accounts for about 0.3 percent of the total angular momentum of the Solar System while about 60% is attributed to Jupiter.\n"}
{"id": "8277431", "url": "https://en.wikipedia.org/wiki?curid=8277431", "title": "Appraisal theory", "text": "Appraisal theory\n\nAppraisal theory is the theory in psychology that emotions are extracted from our evaluations (appraisals or estimates) of events that cause specific reactions in different people. Essentially, our appraisal of a situation causes an emotional, or affective, response that is going to be based on that appraisal. An example of this is going on a first date. If the date is perceived as positive, one might feel happiness, joy, giddiness, excitement, and/or anticipation, because they have appraised this event as one that could have positive long-term effects, i.e. starting a new relationship, engagement, or even marriage. On the other hand, if the date is perceived negatively, then our emotions, as a result, might include dejection, sadness, emptiness, or fear. (Scherer et al., 2001) Reasoning and understanding of one's emotional reaction becomes important for future appraisals as well. The important aspect of the appraisal theory is that it accounts for individual variability in emotional reactions to the same event.\n\nAppraisal theories of emotion are theories that state that emotions result from people's interpretations and explanations of their circumstances even in the absence of physiological arousal (Aronson, 2005). There are two basic approaches; the structural approach and process model. These models both provide an explanation for the appraisal of emotions and explain in different ways how emotions can develop. In the absence of physiological arousal we decide how to feel about a situation after we have interpreted and explained the phenomena. Thus the sequence of events is as follows: event, thinking, and simultaneous events of arousal and emotion. Social psychologists have used this theory to explain and predict coping mechanisms and people's patterns of emotionality. By contrast, for example, personality psychology studies emotions as a function of a person's personality, and thus does not take into account the person's appraisal, or cognitive response, to a situation.\n\nThe main controversy surrounding these theories argues that emotions cannot happen without physiological arousal.\n\nFor the past several decades, appraisal theory has developed and evolved as a prominent theory in the field of communication and psychology by testing affect and emotion. In history, the most basic ideology dates back to some of the most notable philosophers such as Aristotle, Plato, the Stoics, Spinoza and Hume, and even early German psychologist Stumpf (Reisenzein & Schonpflug, 1992). However, in the past fifty years, this theory has expanded exponentially with the dedication of two prominent researchers: Magda Arnold and Richard Lazarus, amongst others who have contributed appraisal theories.\n\nThe question studied under appraisal theories is why people react to things differently. Even when presented with the same, or a similar situation all people will react in slightly different ways based on their perception of the situation. These perceptions elicit various emotions that are specific to each person. About 30 years ago, psychologists and researchers began to categorize these emotions into different groups. This is where cognitive appraisal theory stems from. They decided to categorize these emotional reaction behaviors as appraisals. The two main theories of appraisal are the structural model and the process model. These models are broken down into subtypes as well (Smith & Kirby, 2009). Researchers have attempted to specify particular appraisals of events that elicit emotions (Roseman et al., 1996).\n\nDating back to the 1940s and 1950s, Magda Arnold took an avid interest in researching the appraisal of emotions accompanying general arousal. Specifically, Arnold wanted to \"introduce the idea of emotion differentiation by postulating that emotions such as fear, anger, and excitement could be distinguished by different excitatory phenomena\" (Arnold, 1950). With these new ideas, she developed her \"cognitive theory\" in the 1960s, which specified that the first step in emotion is an appraisal of the situation. According to Arnold, the initial appraisals start the emotional sequence and arouse both the appropriate actions and the emotional experience itself, so that the physiological changes, recognized as important, accompany, but do not initiate, the actions and experiences (Arnold, 1960a). A notable advancement was Arnold's idea of intuitive appraisal in which she describes emotions that are good or bad for the person lead to an action. For example, if a student studies hard all semester in a difficult class and passes the tough mid-term exam with an \"A\", the felt emotion of happiness will motivate the student to keep studying hard for that class.\n\nEmotion is a difficult concept to define as emotions are constantly changing for each individual, but Arnold's continued advancements and changing theory led her to keep researching her work within appraisal theory. Furthermore, the 1970s proved to be difficult as fellow researchers challenged her theory with questions concerning the involvement of psycho physiological factors and the psychological experiences at the Loyola Symposium on Feelings and Emotions. Despite this and re-evaluating the theory, Arnold's discoveries paved the way for other researchers to learn about variances of emotion, affect, and their relation to each other.\n\nFollowing close to Magda Arnold in terms of appraisal theory examination was Richard Lazarus who continued to research emotions through appraisal theory before his death in 2002. Since he began researching in the 1950s, this concept evolves and expands to include new research, methods, and procedures. Although Arnold had a difficult time with questions, Lazarus and other researchers discussed the biopsychological components of the theory at the Loyola Symposium (\"Towards a Cognitive Theory of Emotion\").\n\nSpecifically, he identified two essential factors in an essay in which he discusses the cognitive aspects of emotion: \"first, what is the nature of the cognitions (or appraisals) which underlie separate emotional reactions (e.g. fear, guilt, grief, joy, etc.). Second, what are the determining antecedent conditions of these cognitions.\" (Lazarus, Averill, & Opton (1970, p. 219) These two aspects are absolutely crucial in defining the reactions that stem from the initial emotions that underlie the reactions. Moreover, Lazarus specified two major types of appraisal methods which sit at the crux of the appraisal method: 1) primary appraisal, directed at the establishment of the significance or meaning of the event to the organism, and 2) secondary appraisal, directed at the assessment of the ability of the organism to cope with the consequences of the event. These two types go hand in hand as one establishes the importance of the event while the following assesses the coping mechanisms which Lazarus divided up into two parts: direct actions and cognitive reappraisal processes.\n\nTo simplify Lazarus's theory and emphasize his stress on cognition, as you are experiencing an event, your thought must precede the arousal and emotion (which happen simultaneously). For example: You are about to give a speech in front of 50 of your peers. Your mouth goes dry, your heart beat quickens, your palms sweat, and your legs begin to shake and at the same time you experience fear.\n\n\"The structural model of appraisal\" helps to explain the relation between appraisals and the emotions they elicit. This model involves examination of the appraisal process as well as examination of how different appraisals influence which emotions are experienced.\nAccording to Lazarus (1991), theories of emotion involve a relational aspect, a motivational aspect, and a cognitive aspect (Lazarus, 1991). The relational aspect involves the relationship between a person and the environment and suggests that emotions always involve an interaction between the two (Lazarus, 1991). The motivational aspect involves an assessment of the status of one's goals and is the aspect of the evaluation of a situation in which a person determines how relevant the situation is to his or her goals (Lazarus, 1991). Finally, the cognitive component involves one's appraisal of the situation, or an evaluation of how relevant and significant a situation is to one's life (Lazarus, 1991). Lazarus suggests that different emotions are elicited when situations are evaluated differently according to these three categories. In order to evaluate each emotion individually, however, a structural model of appraisal is necessary (Lazarus, 1991). This model allows for the individual components of the appraisal process to be determined for each emotion. In addition, this model allows for the evaluation of how and where the appraisal processes differ for different emotions (Lazarus, 1991).\n\nThe appraisal process is broken up into two different categories, primary appraisal and secondary appraisal (Lazarus, 1991). In a person's \"primary appraisal,\" he or she evaluates two aspects of a situation: the motivational relevance and the motivational congruence (Smith & Kirby, 2009). When evaluating motivational relevance, an individual answers the question, \"How relevant is this situation to my needs?\" Thus, the individual evaluates how important the situation is to his or her well-being. The motivational relevance aspect of the appraisal of the process has been shown to influence the intensity of the experienced emotions so that when a situation is highly relevant to one's well-being, the situation elicits a more intense emotional response (Smith & Kirby, 2009). The second aspect of an individual's primary appraisal of a situation is the evaluation of motivational congruence. When evaluating the motivational congruence of a situation, an individual answers the question, \"Is this situation congruent or incongruent (consistent or inconsistent) with my goals?\" (Smith & Kirby, 2009). Individuals experience different emotions when they view a situation as consistent with their goals than when they view it as inconsistent.\n\nPeople's emotions are also influenced by their \"secondary appraisal\" of situations. \"Secondary appraisal\" involves people's evaluation of their resources and options for coping (Lazarus, 1991). One aspect of secondary appraisal is a person's evaluation of who should be held accountable. A person can hold herself, another, or a group of other people accountable for the situation at hand. Blame may be given for a harmful event and credit may be given for a beneficial event (Lazarus, 1991). In addition, an individual might also see the situation as due to chance. The way in which people view who or what should be held accountable directs and guides their efforts to cope with the emotions they experience. Another aspect of secondary appraisal is a person's coping potential.\" Coping potential\" is potential to use either problem-focused coping or emotion-focused coping strategies to handle an emotional experience. (Smith & Kirby, 2009). \"Problem-focused coping\" refers to one's ability to take action and to change a situation to make it more congruent with one's goals (Smith & Kirby, 2009). Thus, a person's belief about their ability to perform problem-focused coping influences the emotions they experience in the situation. On the other hand, \"emotion-focused coping\" refers to one's ability to handle or adjust to the situation should the circumstances remain inconsistent with one's goals (Smith & Kirby, 2009). Again, the emotions people experience are influenced by how they perceive their ability to perform emotion-focused coping. The fourth component of secondary appraisal is one's future expectancy (Lazarus, 1991). \"Future expectancy\" refers to one's expectations of change in the motivational congruence of a situation (for any reason). Thus, an individual may believe the situation will change favorably or unfavorably (Lazarus, 1991). One's future expectancy influences the emotions elicited during a situation as well as the coping strategies used.\n\nThe structural model of appraisal suggests that the answers to the different component questions of the primary and secondary categories allow researchers to predict which emotions will be elicited from a certain set of circumstances. In other words, the theory suggests that researchers are able to examine an individual's appraisal of a situation and then predict the emotional experiences of that individual based upon his or her views of the situation. An example of a particular emotion and its underlying appraisal components can be seen when examining the emotion of anger. If a person appraises a situation as motivationally relevant, motivationally incongruent, and also holds a person other than himself accountable, the individual would most likely experience anger in response to the situation (Smith & Haynes, 1993). Another example of the appraisal components of an emotion can be given in regards to anxiety. Like anger, anxiety comes from the evaluation of a situation as motivationally relevant and motivationally incongruent (Lazarus, 1991). However, where anxiety differs from anger is in who is held accountable. For anger, another person or group of people is held accountable or blamed for a wrongdoing. However, in regards to anxiety, there is no obvious person or group to hold accountable or to blame. The structural model of appraisal allows for researchers to assess different appraisal components that lead to different emotions.\n\nAppraisal theory, however, has often been critiqued for failing to capture the dynamic nature of emotion. To better analyze the complexities of emotional appraisal, social psychologists have sought to further complement the structural model. One suggested approach was a cyclical process, which moves from appraisal to coping, and then reappraisal, attempting to capture a more long-term theory of emotional responses (Smith & Lazarus 1990). This model, however, failed to hold up under scholarly and scientific critique, largely due to the fact that it fails to account for the often rapid or automatic nature of emotional responses (Marsella & Gratch 2009). Further addressing the concerns raised with structural and cyclical models of appraisal, two different theories emerged that advocated a process model of appraisal.\n\nSmith and Kirby (2000) argue for a two-process model of appraisal, which expands on the function of the structural model of appraisal. While the structural model of appraisal focuses on what one is evaluating, the process model of appraisal focuses on how one evaluates emotional stimuli. There are three main components to the process model of appraisal: perceptual stimuli, associative processing, and reasoning. Perceptual stimuli are what the individual picks up from his or her surroundings, such as sensations of pain or pleasure, perception of facial expression (Smith & Kirby 2000). In addition to these stimuli, the process model is composed to two main appraisal processes. Associative processing is a memory-based process that makes quick connections and provides appraisal information based on activated memories that are quickly associated with the given stimulus (Marsella & Gratch 2009). Reasoning is a slower, more deliberate, and thorough process that involves logical, critical thinking about the stimulus and/or situation (Marsella & Gratch 2009). In the two-process model of appraisal theory, associative processing and reasoning work in parallel in reaction to perceptual stimuli, thus providing a more complex and cognitively based appraisal of the emotional encounter (Smith & Kirby 2000).\n\nAn alternative process model of appraisal, Scherer's multi-level sequential check model is made up of three levels of appraisal process, with sequential constraints at each level of processing that create a specifically ordered processing construct (Scherer 2001). The three levels of processing are: innate (sensory-motor), learned (schema-based), and deliberate (conceptual) (Marsella & Gratch 2009). Further, Scherer constructs a strict, ordered progression by which these appraisal processes are carried out. There are various evaluation checks throughout the processes, which allow for observation of stimuli at different points in the process sequence, thus creating a sort of step-by-step appraisal process (Scherer 2001). Such checks include: a relevance (novelty and relevance to goals) check, followed by an implication check (cause, goal conduciveness, and urgency), then coping potential check (control and power), and finally the check for normative significance (compatibility with one's standards) (Marsella & Gratch 2009). While the two-process model involves processes occurring at the same time, parallel to one another, Scherer's multi-level sequential check model is composed of processes that take place in a specific sequence.\n\n\"Roseman's theory of appraisal\" holds that there are certain appraisal components that interact to elicit different emotions (Roseman, 1996). One appraisal component that influences which emotion is expressed is \"motive consistency\". When one evaluates a situation as inconsistent with one's goals, the situation is considered motivationally inconsistent and often elicits a negative emotion, such as anger or regret (Roseman, 1996). A second component of appraisal that influences the emotional response of an individual is the \"evaluation of responsibility or accountability\" (Roseman, 1996). A person can hold oneself or another person or group accountable. An individual might also believe the situation was due to chance. An individual's evaluation of accountability influences which emotion is experienced. For example, if one feels responsible for a desirable situation, pride may be an emotion that is experienced.\n\nIn addition to the two appraisal components, the different \"intensities\" of each component also influence which emotion or emotions are elicited. Specifically, the \"certainty\" and the \"strength\" of the evaluation of accountability influences which emotions are experienced (Roseman, 1996). In addition, the \"appetitive or aversive nature of motive consistency\" also influences the emotions that are elicited (Roseman, 1996).\n\nRoseman's theory of appraisal suggests that motive consistency and accountability are the two most important components of the appraisal process (1996). In addition, the different levels of intensity of each component are important and greatly influence the emotions that are experienced due to a particular situation.\n\nMost models currently advanced are more concerned with structure or contents of appraisals than with process oriented appraisal. \"These models attempt to specify the evaluations that initiate specific emotional reactions. Examination of these models indicates that although there is significant overlap [between the two types of structural models], there are also differences: in which appraisals are included; how particular appraisals are operationalized; which emotions are encompassed by a model; and which particular combinations of appraisals are proposed to elicit a particular emotional response.\" (Scherer et al., 2001). Ultimately, structurally based appraisals rely on the idea that our appraisals cultivate the emotional responses. Process-oriented models of appraisal theory are rooted in the idea that it is important to specify the cognitive principles and operations underlying these appraisal modes. Using this orientation for evaluating appraisals, we find fewer issues with repression, a \"mental process by which distressing thoughts, memories, or impulses that may give rise to anxiety are excluded from consciousness and left to operate in the unconscious\" (Merriam-Webster, 2007).\n\nWithin the continuous versus categorical nature of appraisal and emotion, there are many standpoints of the flow of this appraisal process. To begin, Roseman's (1996) model shows that appraisal information \"can vary continuously but categorical boundaries determine which emotion will occur\". Motive consistency and inconsistency make up an example of this categorical framework. A positive or negative emotional response in conjunction with the affect has much to do with the appraisal and the amount of motivational consistency. To accurately understand this concept, an example of Roseman's model could come from a motive-consistent goal as it is caused by the self and someone else to reach one's objective in which a positive emotion is created from the specific appraisal event. In addition, Scherer's (1984) model shows that most appraisal falls in a continuous spectrum in which points along the way represent distinct emotional points made possible from the appraisal. Between appraisal space and number of emotions experienced, these two components are both positively correlated. \"According to Scherer (1984a), the major categorical labels we used to describe our emotional experiences reflect a somewhat crude attempt to highlight and describe the major or most important ways these emotional experiences vary\". With so much variation and levels within one's emotions, it can be seen as injustice to the emotional experience and the appraisal process to limit oneself to such categories. To solve the problem between categorical and continuous appraisal order, it may be a good idea to place discrete emotional categories (i.e. happiness, sadness, etc.) while continuous models represent the varieties, styles, and levels of these already defined distinct emotions.\n\nStanley Schachter's contributions should also be noted as his studies supported the relevance of emotion induced in appraisal. In 1962, Schachter and Jerome E. Singer devised an experiment to explain the physiological and psychological factors in emotional appraising behaviors. By inducing an experimental group with epinephrine while maintaining a control group, they were able to test two emotions: euphoria and anger. Using a stooge to elicit a response, the research proved three major findings relevant to appraisal:\nBy taking into account heightened emotion, reaction to the stooge, as well as prompted questions, all these elicited factors provide a negative or positive affect. Although the study took place in 1962, it is still studied in both psychology and communication fields today as an example of appraisal theory in relation to affect and emotion.\nThrough these findings, Schachter and Singer assess that an event happens which in turn elicits as physiological arousal. From the reasoning of the arousal, you are then able to have an emotion. For example: You are about to give a speech. You approach the podium and look out into the audience as your mouth goes dry, your heart beat quickens, your palms sweat, and your legs begin to shake. From this arousal, you understand you feel this way because you are about to give a speech in front of 50 of your peers. This feeling causes anxiety and you experience the emotion of fear.\n\nIn a study aimed at defining stress and the role of coping, conducted by Dewe (1991), significant relationships between primary appraisal, coping, and emotional discomfort were recorded. It was proven that primary appraisal was the main contributor of predicting how someone will cope. This finding enables psychologists to be able to begin to predict the emotion that will be elicited by a certain event and may give rise to an easier way to predict how well someone will cope with their emotion.\n\nA study by Rogers & Holmbeck (1997) explores a previous finding that \"the psychological impact of interparental conflict on children is influenced by children's cognitive appraisals.\" The researchers hypothesized that cognitive appraisal and coping would help moderate variables for the children, and therefore the emotional impact of parent conflict would vary based on the nature of the child's \"appraisals and coping strategies\" (Rogers & Holmbeck 1997). The researchers tested coping strategies and measured child adjustment based on the children's self-reported emotional and behavioral adjustment, determined from levels of self-worth and depression (Rogers & Holmbeck 1997). The results demonstrated a significant negative main effect of problematic cognitive appraisal on self-worth and a significant positive main effect of problematic cognitive appraisal on depression, thus showing the impact of cognitive appraisal on children's emotional well being and ability to deal with interparental conflict (Rogers & Holmbeck 1997). This study demonstrates the significance of cognitive appraisal in coping with emotionally difficult circumstances and their own behavioral adjustment and self-esteem. An understanding of the role of cognitive appraisal and cognitive appraisal theories can assist psychologists in understanding and facilitating coping strategies, which could contribute to work in the field that acts to facilitate healthy behavioral adjustment and coping strategies in individuals.\n\nIn another study conducted by Jacobucci (2000), findings suggested that individual differences and primary appraisals had a very strong correlation. This shows that primary appraisal is a function of personality and may be stable over time. This in fact is a very strong finding for social psychologists because it proves that if we can predict the primary appraisal strategy and thinking pattern of an individual, then coping patterns and emotional tendencies of an individual may be able to be predicted in any situation and social setting.\n\nA study by Verduyn, Mechelen, & Tuerlinckx (2011) explores the factors that affect the duration of an emotional experience. One aspect of the research focuses on the difference between rumination versus reappraisal of an emotional event, exploring how they affect the duration of an emotional experience, and in which direction (shortening or lengthening) (Verduyn et al. 2011). The researchers argue that cognition is very significant to the duration and experience of emotion, claiming that \"thoughts appear to act as fuel that stirs up the emotional fire and leads to a prolongation of the episode\" (Verduyn et al. 2011). Further, the researchers reference the significance of emotions \"lining up with\" initial appraisals of the emotion-eliciting experience, which then strengthens the emotion and may lead to prolongation of the experience (Verduyn et al. 2011). This concept alludes to the significance of congruence among emotions, appraisal, and cognitions. This particular article discusses the coping effect of appraisal and reappraisal, claiming reappraisal can act as an \"adaptive strategy,\" while rumination is not (Verduyn et al. 2011). Both reappraisal (or initial cognitive appraisal) and rumination, however, can affect the duration of an emotional experience. This study demonstrates the significance of cognitive appraisal by indicating its role in the duration of an emotional experience. Because the duration of an emotional experience can have significant effects on how an individual reacts to given stimuli, and thus have relevant real-world application in how individuals deal with emotional experiences. This study also presents reappraisal—appraising the emotional situation in a new way—can act as an adaptive strategy to deal with difficult circumstances, thus further highlighting the necessity of cognitive appraisal to coping with emotional stressors.\n\nOne study completed by Folkman et al. (1986) focuses on the relationship between appraisal and coping processes that are used across stressful events, and indicators of long-term adaptation. They define primary appraisal as \"the stakes a person has in a stressful encounter,\" and secondary appraisal as \"options for coping.\" Eighty-five California married couples with at least one child were the participants of the study, and they were interviewed in their homes once a month for 6 months. In each interview the subject was asked what their most stressful event was in the previous week, and then interviewer asked them structured questions about how they dealt with that stressor. There was a significant gender difference in primary appraisal. They also concluded that coping strategies were dependent upon psychological and somatic problems as well (Folkman, Lazarus, Gruen & DeLongis, 1986).\n\nIn another study by Folkman the goal was to look at the relationship between cognitive appraisal and coping processes and their short-term outcomes within stressful situations. Subjects were interviewed once a month for six months. Primary and secondary appraisals were assessed using different subscales. This study found that there is a functional relationship among appraisal and coping and the outcomes of stressful situations. There were significant positive correlations between primary appraisal and coping. There were also significant correlations between secondary appraisal and coping, and they were very specific about the type of stressful situation and with which each would help the most. For example, they found that appraisals of changeability and having to hold back from acting were related to the encounter outcomes (Folkman, Lazarus, Dunkel-Schetter, DeLongis & Gruen, 1986).\n\nMany current theories of emotion now place the appraisal component of emotion at the forefront in defining and studying emotional experience. However, most contemporary psychologists who study emotion accept a working definition acknowledging that emotion is not just appraisal but a complex multifaceted experience with the following components:\n\n\n"}
{"id": "16733372", "url": "https://en.wikipedia.org/wiki?curid=16733372", "title": "Astro-comb", "text": "Astro-comb\n\nAn astro-comb is a type of frequency comb which increases the resolution of previous spectrographs by nearly a hundredfold, allowing it to be used as an observational tool in astronomy and detect redshift wobbles caused by smaller exoplanets than what was detectable with traditional calibrators. Existing frequency combs were too precise to be useful for astronomy, with \"teeth\" too close together. In other words, the repetition rate or the frequency spacing between the comb lines was much smaller (<1 gigahertz) than that required for astronomical applications (~tens of gigahertz or more). A \"green astro-comb\" was installed in January 2013 in the High Accuracy Radial velocity Planet Searcher in the Northern hemisphere (HARPS-N) spectrograph at the Telescopio Nazionale Galileo on the Canary Islands. The device was developed by a team led by Chih-Hao Li of Harvard University.\n\nThe astro-comb uses a pulsed laser to filter starlight before feeding the signal into a spectrograph. It has the potential to revolutionise astrophysical spectroscopy and discover other Earth-like planets outside our solar system. Currently, it is gathering data from Venus to demonstrate its ability to discover exoplanets.\n\n\n"}
{"id": "54605709", "url": "https://en.wikipedia.org/wiki?curid=54605709", "title": "Centre pour l'Édition Électronique Ouverte", "text": "Centre pour l'Édition Électronique Ouverte\n\nThe Centre pour l'Édition Électronique Ouverte (Cléo; ), based in Marseille, France, is overseen by Aix-Marseille University, the Centre National de la Recherche Scientifique, School for Advanced Studies in the Social Sciences, and University of Avignon and the Vaucluse. It produces the open access academic publishing portal , which includes platforms Calenda, Hypotheses, , and OpenEdition Journals. OpenEdition focuses on publications in the academic fields of humanities and social sciences. The centre also issues a blog about open access.\n\nThe following list includes some examples of titles in Journals.openedition.org (prior to December 2017 known as ):\nA\n\n\nB\n\n\nC\n\n\nD\n\n\nE\n\n\nF\n\n\nG\n\n\nH\n\nI\n\n\nJ\n\n\nM\n\n\nN\n\n\nO\n\n\nP\n\n\nQ\n\n\nR\n\n\nS\n\n\nT\n\n\nV\n\n\n\n"}
{"id": "49302247", "url": "https://en.wikipedia.org/wiki?curid=49302247", "title": "Chemical conditioning", "text": "Chemical conditioning\n\nConditioning is a process in which reaction factors are stabilized or enhanced. It could be as increasing the quality of a material by using another material, improvement solids capture and physically and chemically water treatment or dewatering. There are three main conditioning systems: heat, inorganic chemicals and organic polymers. Conditioning increases always the efficiency of water removal.\n"}
{"id": "53626941", "url": "https://en.wikipedia.org/wiki?curid=53626941", "title": "Chicago principles", "text": "Chicago principles\n\nThe Chicago principles are a set of guiding principles intended to demonstrate a commitment to freedom of speech and freedom of expression on college campuses in the United States. Initially adopted by the University of Chicago following a report issued by a designated Committee on Freedom of Expression in 2014 (\"″Report of the Committee on Freedom of Expression″\"), they came to be known as the “Chicago principles” after other universities across the country committed to the principles or modelled their own based on similar goals. \n\nSince 2014, several other universities have committed to the principles, including Princeton and Purdue. As of September 2018, the Foundation for Individual Rights in Education reported that 45 American colleges and universities had \"adopted or endorsed the Chicago Statement or a substantially similar statement.\"\n\nIn July 2014, the University of Chicago formed the ‘University of Chicago's Committee on Freedom of Expression’ after a series of incidents where students at various schools sought to prevent controversial commencement speakers that year. The committee returned a report which re-emphasized the school’s commitment to principles of free expression as “an essential element of the University’s culture.” The University's commitment to free speech gained national media attention in August 2016, when Dean of Students John Ellison sent a letter to the incoming freshman class of 2020 affirming the free speech principles and stating that the University did not support the use of trigger warnings or safe spaces. \n\nIn adopting the principles, Purdue president, Mitch Daniels later said “we didn’t see how we could improve on the language.”\n\n\n"}
{"id": "21634581", "url": "https://en.wikipedia.org/wiki?curid=21634581", "title": "Children of the revolution (concept)", "text": "Children of the revolution (concept)\n\nChildren of the revolution is a concept associated with the generation growing up after revolutionary activity. It refers to the first generation of persons born after a revolution. The children of the revolution are a blank slate on which the values of the revolution are imposed. Because the generation have no shared memory of the prior world they cannot compare the new system with the old and will uncritically accept the new system as the natural order.\n\nThe phrase usually refers to political revolutions, but is also applied to revolutions in culture, science, or art.\n\nhttp://eprints.lse.ac.uk/4634/1/Children_of_the_Revolution.pdf\nhttp://www.chathamhouse.org/publications/twt/archive/view/166817\n"}
{"id": "18637184", "url": "https://en.wikipedia.org/wiki?curid=18637184", "title": "Compliance gaining", "text": "Compliance gaining\n\nCompliance gaining is a term used in the social sciences that encompasses the intentional act of altering another's behavior. Research in this area originated in the field of social psychology, but communication scholars have also provided ample research in compliance gaining. While persuasion focuses on attitudes and beliefs, compliance gaining focuses on behavior.\n\nCompliance gaining occurs whenever a person intentionally induces another person to do something that they might have not done otherwise. Compliance gaining and persuasion are related; however, they are not one and the same. Changes in attitudes and beliefs are often the goal in persuasion; compliance gaining seeks to change the behavior of a target. It is not necessary to change a person's attitude or beliefs to gain compliance. For instance, an automobile driver might have positive attitudes towards driving fast. The threat of a speeding ticket from a police officer positioned in a speed trap may gain compliance from the driver. Conversely, persuading someone to change their attitude or belief will not necessarily gain compliance. A doctor might tell a patient that tobacco use poses a serious threat to a smoker's health. The patient may accept this as a fact and view smoking negatively, but might also continue to use tobacco.\n\nCompliance gaining research has its roots in social psychology, but overlaps with many other disciplines such as communication and sociology. Compliance gaining can occur via mediated channels, but the research is most associated with interpersonal communication. In 1967, sociologists Marwell and Schmitt attempted to explain how people select compliance gaining messages. The researchers posited that people have a mental bank of strategies that they draw from when selecting a message. Marwell and Schmitt created a typology for compliance gaining techniques: promise, threat, positive expertise, negative expertise, liking, pregiving, aversive stimulation, debt, moral appeal, positive self-feeling, negative self-feeling, positive altercasting, negative altertcasting, altruism, positive esteem, and negative esteem. This study was the catalyst for more interest in compliance gaining from communication scholars.\n\nMiller, Boster, Roloff, and Seibold (1977) as well as Cody and McLaughlin (1980) studied the situational variables that influences compliance gaining strategies. The latter study identified six different typologies of situations that can influence compliance gaining behaviors: personal benefits (how much personal gain an actor can yield from the influencing behavior), dominance (the power relation between the actor and the target), rights (whether the actor has the right to expect compliance), resistance (how easy will the target be influenced), intimacy (whether the relation between actor and target is shallow), and consequences (what sort of effect this situation would have on the relationship between actor and target). Dillard and Burgoon (1985) later investigated the Cody-McLaughlin typologies. They concluded that situational variables, as described by Cody and McLaughlin, did very little to predict compliance gaining strategy selection. As early as 1982, there was already strong criticism about the strength of the relationships between situational variables and compliance gaining message selection.\n\nBy the 1990s, many research efforts attempting to link compliance gaining strategy selection and features of a situation or features of the individual \"failed to coalesce into a coherent body of knowledge\". Situational dimensions and individual differences were not effective in predicting so researchers went into other perspectives in an effort to understand compliance gaining. For instance, Schrader and Dillard (1998) linked primary and secondary goals to compliance gaining strategy. Using the theoretical framework of Goals-Plans-Actions developed by Dillard in 1980, Schrader and Dillard operate from the assumption that individuals possess and act on multiple goals. In any compliance seeking situation, the actor has primary goals that drive the attempt to influence a target. The primary goal is what the interaction is all about. For instance, if an actor wants a target to stop smoking, this is the primary goal and this is what drives the interaction. However, in the course of pursuing that goal, there are \"secondary\" goals to consider. These are goals that limit the behavior of the actor. If getting a target to stop smoking is the primary goal, then a secondary goal might be to maintain a friendly relationship with the target. Dillard specifies five types of secondary goals that temper the compliance gaining behavior: identity goals (morals and personal standards), interaction goals (impression management), relational resource goals (relationship management), personal resource goals (material concerns of the actor), and arousal management goals (efforts to manage anxiety about the compliance gaining attempt).\n\nDespite the charges of individual differences making very little progress in prediction compliance gaining strategies, some researchers in the 2000s have focused their efforts to rectify this weakness in the research to link individual differences with compliance gaining effectiveness. King (2001), acknowledging the paucity of robust situational and trait studies linked to compliance gaining, attempted to isolate one situation as a predictor for compliance gaining message selection. King's research suggested that when target of compliance gaining were perceived to be less resistant to influence attempts, the actors used more compliance gaining tactics. When targets were perceived as strongly resistant, the actors used less tactics. Elias and Loomis (2004) found that gender and race affect an instructor's ability to gain compliance in a college classroom. Punyanunt (2000) found that using humor may enhance the effectiveness of pro-social compliance gaining techniques in the classroom. Remland and Jones (1994) found that vocal intensity and touch also affect compliance gaining. Goei et al. (2003) posited that \"feelings of liking\" between target and actor as well as doing favors for the target lead to liking and obligation, which leads to increased compliance. Pre-giving (giving a target a small gift or favor such as a free sample of food) is positively associated with increased compliance in strangers. \nOne of the major criticisms of examining compliance gaining literature is that very little research studies actual compliance. Filling out a survey and reporting intent to comply with a request is certainly different than actually completing the request. For example, many people might report that they will comply with a doctor's order, but away from the doctor's office, they may ignore medical advice.\n\nCompliance gaining research has a fairly diverse background so much of the research uses alternate paradigms and perspectives. As mentioned above, the field of compliance gaining originated in social psychology, but was adopted by many communication scholars as well. Many fields from consumer psychology to primary education pedagogy have taken great interest in compliance gaining.\n\nDoctors have expressed much frustration with compliance resistance from their patients. A reported 50% of patients do not comply with medical advice and prescriptions. Researchers, as well as medical professionals, have a vested interest in learning strategies that can increase compliance in their patients. Many severe and chronic conditions can be avoided if early treatments are followed as prescribed, avoiding death, permanent injury, and costlier medical treatments later on. Researchers in communication have reported some key findings such as: clear and effective communication about a patient's condition or illness increases the likelihood of patient compliance with medical advice; doctors that use humor in their communication with patients have higher satisfaction rates; high satisfaction rates with physicians is highly correlated with patient compliance.\n\nFor teachers, gaining compliance from students is a must for effective teaching. Studies in compliance gaining have ranged from elementary education all the way to adult and higher education.\n\nAdvertising and marketing are tools of persuasion. There is literally centuries' worth of literature available about persuasion. However, changing attitudes and beliefs about a product does not necessarily change behaviors. Purchasing a product is a behavior. Researchers such as Parrish-Sprowl, Carveth, & Senk (1994) have applied compliance gaining research to effective sales.\n\nCompliance gaining was not originally conceived in the field of communication but found its roots in the late 1960s as a result of studies and research by two sociologists, Gerald Marwell and David Schmitt. In 1967, Marwell and Schmitt produced some interesting compliance-gaining tactics concerning the act of getting a teenager to study. The tactics, sixteen in all, are as follows.\n\n\nIn 1967, Marwell and Schmitt conducted experimental research, using the sixteen compliance gaining tactics and identified five basic compliance-gaining strategies: Rewarding activity, Punishing activity, Expertise, Activation of impersonal commitments, and Activation of personal commitments.\n\nAnother element of compliance-gaining was produced in the early 1960s, as French and Raven were researching the concepts of power, legitimacy, and politeness. They identified five influential aspects associated with power, which help illustrate elements of the study of compliance. The fives bases of power are as follows:\n\n\nThe study of compliance gaining has been central in the development of many commonly used or heard of techniques. The following techniques are a few of what has evolved as a product of the study of compliance gaining strategies. Note, many of these techniques have been empirically documented increasing compliance.\n\nWith research starting in 1966 by Freedman & Fraser, foot-in-the door is one of the earliest and most researched compliance gaining techniques. This technique gains compliance by making a smaller easy request then a larger more difficult request at a later time. The smaller request is usually one that would be widely accepted without scrutiny. The larger request is usually the actual the task or goal wanted to be completed.\n\nFreedman and Fraser thought that after satisfying the smaller initial request, if the person was not forced to do then they must be \"the type of person who fulfills such requests\".\n\nThe smaller task/request should relate to the larger request and not be trivial. For the foot-in-the-door technique to be successful it must generate the self-aware \"I am the kind of person who fulfills this type of request\" other wise known as the self-perception theory. Other studies found that if the initial request is easy but unusual or bizarre, it would also generate the foot-in-the-door effectiveness. This idea was developed further into the Disrupt-Then-Reframe technique.\n\nThere are other reasons besides the self-perception theory that makes the foot-in-the-door technique successful.\n\nConsistency – Cialdini and Guadagno, Asher, and Demaine believe that what makes people want to fulfill larger request is the need to be consistent.\n\nThe Norm to Help Others – Harris believed that after the first request, the norm to help others becomes clear. It only becomes evident after the person reviews his or her reason why they completed the original request.\n\nSatisfying the First Request – Crano and Sivacek thought what made the technique so effective was personal satisfaction. \"The person learns that the fulfillment of request brings the reward of a positive experience. One may assume that the likelihood that satisfaction of this type appears willi increase if the person has to react to something unusual that awakens his or her mindfulness, and will decrease in situations in which the person reacts automatically and habitually\".\n\nDoor-in-the-face was first introduced in 1975 by Cialdini and colleagues. The opposite of foot-in-the-door, in the door-in-the-face technique, the requestor asks a large objectionable request which is denied by the target instead of gaining compliance by asking a smaller easy request. The requestor seeking compliance ask a smaller more reasonable request.\n\nThere are several theories that explain why door-in-the-face is an effective gaining compliance technique.\n\nSelf-presentation theory – \"that individuals will comply with a second request due to fears one will be perceived negatively by rejecting successive prosocial request for compliance\".\n\nReciprocal concessions – this theory describes the effects of door-in-the-face as a \"process of mutual concessions.\" \"The second request represents a concession on the part of the sender (from his or her initial request), and compliance to the second request represents a concession on the part of the receiver (from his or her inclination to not comply with the first request)\".\n\nGuilt – One reason that makes door-in-the-face such an effective technique is people feel guilty for refusing to comply with a request twice.\n\nSocial Responsibility – this theory describes the social repercussions and pressures that occur if an individual declines a request.\n\nAll together the theories propose that a target who declines the first request feel a \"personal or social responsibility\" to comply with the second request. In an effort to avoid feeling guilty or reduce the sense of obligation the target would have.\n\nDTR was first introduced by Barbara Price Davis and Eric S. Knowles in 1999. This technique states that a person will be more likely to comply with a request if the initial request or pitch is confusing. The pitch is immediately followed by a reframing or a reason to comply with the request.\n\nAn example of this technique is: A waiter states that \"the steak dinner is on special for 800 pennies; it's a really good deal\". Disrupting the couple by saying \"800 pennies\" instead of \"8 dollars\", the waiter is able to increase the likelihood that they will buy the steak dinner.\n\nDTR was found to be a very effective way to gain compliance in non-profit instances such as raising money for charities or asking people to take a survey. DTR was found to be less successful as a sales technique; this may be because the message is more scrutinized, making it harder to confuse the target.\n\nPersistence used as a compliance gaining technique, gets the target to comply by repeating the message. In 1979, Cacioppo and Petty found that repeating the message more than five times lead to decrease in compliance. Success is enhanced if the repetition comes from more than one person and is enhanced further if the message has the same idea or meaning but is not exact.\n\nAn example of this technique would be: \"My wife kept reminding me to take out the trash until I finally did it.\"\n\nPersistence has a high probability of annoying the target and creating a negative interaction which could be viewed as \"nagging\". A way to avoid this would be rejecting the targets objection to your request by asking \"why not?\", then forming another message to overcome the second objection to gain compliance. This technique is called dump and chase.\n\nMechanics of this technique are urgency and guilt. When the repeated message is presented to the target it may be perceived as urgent, thus making it seem more important, and more willing to comply. By creating a sense of obligation in the request, the target may develop guilt if not willing to comply.\n\nJust-One-More was developed as a way to make a donation seem more important. The use of this technique involves using the language of \"Just-One-More\" to gain compliance. The technique is found to most useful in instances regarding volunteering and donations. It is seen as \"the last person to help will be more rewarding than being one of the first or those in the middle, due to the expectation that the requestor will appreciate the last person more than any of those who complied previously\".\nFor Example: \"Do you want to buy this car? I need just one more sale to reach my quota this month.\"\n\nIf the target finds that the requestor is lying or being deceptive about being the last one, it will create a negative outlook on the person and the organization that he or she represents. Even though losing some of the effectiveness the requestor could state that they are \"close to their goal\" or \"almost there\".\n\nIn \"Classifying Compliance Gaining Messages: Taxonomic Disorder and Strategic Confusion\", Kathy Kellermann and Tim Cole put together 64 compliance gaining strategies as an attempt to classify more than 820 previous strategies.\n\n"}
{"id": "38872295", "url": "https://en.wikipedia.org/wiki?curid=38872295", "title": "Cuvier–Geoffroy debate", "text": "Cuvier–Geoffroy debate\n\nThe Cuvier–Geoffroy debate of 1830 was a scientific debate between the two French naturalists Georges Cuvier and Étienne Geoffroy Saint-Hilaire. For around two months the debate occurred in front of the French Academy of Sciences. The debate centered primarily on animal structure; Cuvier asserted that animal structure was determined by an organism's functional needs while Geoffroy suggested an alternative theory that all animal structures were modified forms of one unified plan. In terms of scientific significance, the discussion between the two naturalists showed stark differences in scientific methods as well as general philosophy. Cuvier is generally considered the winner of the debate, as he always came better prepared to the debate with overwhelming amounts of evidence and more logical arguments, as well as having more political and academic influence. Despite this, Geoffroy's philosophy is seen as early support of evolution theory and parts of the theory of the \"unity of composition\" are generally more accepted over Cuvier's fixed species philosophy.\n\nBoth naturalists had a love for classification and description but grew up with different influences. Geoffroy came to Paris to study medicine, law and philosophy in early 1789, but shifted to the study of zoology not long after. When a priest mentor of Geoffroy's became caught up in the political turmoil of the revolution of 1789, it was through Geoffroy's testimony that the priest was released from prison. Making friends in the church led to his eventual appointment as a zoology professor in the Royal Garden (later became the Museum of Natural History), when a post was vacated due to more political problems. Although he was only twenty-one years old at the time, this position gave Geoffroy access to resources and natural collections in which he built his future theories on nature. He was even able to accompany Napoleon on an expedition to Egypt in 1798 where he studied mummies and made hypotheses on change over time in humans and other organisms. Geoffroy agreed with Buffon that any classification system was arbitrary and thus somewhat empty, but he nonetheless attempted to find the general laws that applied to all organisms in nature.\n\nBefore their well-known rivalry developed, the two scientists began as close colleagues and friends. Cuvier met a member of the Academy of Sciences, Henri-Alexandre Tessier, in Normandy in 1794 while he was a young man tutoring the children of the wealthy. Tessier was impressed by Cuvier's talent and skill, and soon after wrote glowing letters to established scientists like Geoffroy and another, Jussier. Geoffroy was charmed by Cuvier's detailed descriptions of animals and his precise sketches and decided to invite him to \"come to Paris. Come play among us the role of another legislator of natural history.\" When Cuvier joined him at the Museum, other colleagues warned Geoffroy against mentoring him, suggesting that this brilliant young scientist would eventually surpass him. Despite this premonition, both scientists worked side by side and wrote five papers together on the classification of mammals, the two-horned African rhinoceros, species of elephants, descriptions of the tarsier, and the biology of the orangutan. It was not long before Cuvier began to make a name for himself individually too, as he was highly skilled at reaching out to patrons, networking and acquiring funding for his research. By 1976, Cuvier was working on his papers on extinction, merely two years after joining the Museum, while Geoffroy had barely begun to publish. One of the most significant events that solidified the split between the two scientists was Cuvier's appointment to the Academy of Sciences on December 17, 1795 as one of the six original members of anatomy and zoology. Cuvier was only twenty-six years old (seventeen years Geoffroy's junior) and the youngest member at the time, while Geoffroy was not given admittance to the Academy for another twelve years.\n\nPrior to the debate, biological scientists were generally split into two major factions: whether animal structure was determined by functional needs of an animal or the \"conditions of existence\" (Cuvier), or a basic unified form that was modified across all animal forms (Geoffroy). By determining the answer to this question, scientists could also shed light on the mechanisms of the evolution of species, as well as potentially finding a useful system to classify species to better understand the order of nature. Cuvier was an influential scientist with significant political power and many prominent positions: permanent secretary at the French Academie of Science, a professorship at the Museum of Natural History and the College of France, as well as a Council member at the University of France. Cuvier stuck to \"positive facts\" of science and refused to flirt with hypotheses and unsupported ideas. This attitude sprang from worries that this kind of thinking would lead to unrest and disorder in France as the French Revolution had torn through only a few decades before. Cuvier encouraged young scientists to stick with facts and often won them over easily due to his strong reputation as a good ally to have in science. His political ties and academic connections gave his ideas a broader audience and generally more historical recognition. Geoffroy was heavily influenced by Buffon and Lamarck. It was in his memoir \"Philosophie anatomique\", published in 1818, that he first put his ideas on animal form and structure into print. Here he attempted to answer his own question, \"Can the organization of vertebrated animals be referred to as one uniform type?\" Cuvier on the other hand was known as Lamarck's greatest critic, as Cuvier felt Lamarck was too speculative without enough facts to back up his theories. Cuvier grew up reading the works of Linnaeus and was mostly self-taught through dissection and measurements of organisms. Geoffroy thought of facts as building blocks to science while new ideas would lead to real discovery, occasionally dipping more into philosophical hypotheses instead of testable or demonstrated research. Geoffroy kept his unpopular ideas under wraps when advantageous to his career but found it increasingly difficult to stay passive as he got older and more well known. He may even have welcomed the debate between himself and Cuvier as a chance to liven up the discussions in the Academy and generate new ideas.\n\nWhile the two biologists had disagreed on animal structure subtly through their publications, talks with other scientists at Academy meetings, and in private, a paper on mollusks written by two relatively unknown scientists drew Geoffroy and Cuvier at arms. The paper, written by Meyranx and Laurencet, was assigned to Geoffroy and Pierre André Latreille to review and report to the Academy. Meyranx and Laurencet's conclusions grabbed Geoffroy's attention by suggesting a link between vertebrate and mollusk internal anatomy. With thousands of drawings of mollusks and multiple essays, the authors argued that the organ arrangement of a mollusk resembled vertebrate organ arrangement, if the vertebrate bent backwards so that the neck was connected to the backside. Geoffroy found support for his unity of composition which would unite the vertebrates and mollusks -two of Cuvier's published embranchments of animals- on one common plan.\n\nOn February 15, 1830, Geoffroy presented his report to the Academy. In an epilogue that was eventually removed from the final report at Cuvier's insistence, Geoffroy made a clear attack at Cuvier by quoting from a work of Cuvier's from 1817, \"Memoir on the Cephalopods and on Their Anatomy\", though the author and actual manuscript name were not mentioned. Geoffroy ridiculed the old-fashioned way of describing nature by focusing on differences instead of similarities, presenting his unity of composition theory as a better alternative. Cuvier disagreed vehemently with the report and paper's findings, promising to further explain his argument in future writings.\n\nAt the next meeting of the Academy on February 22, Cuvier came fully prepared with detailed, labeled, colorful drawings, in particular one of an octopus and another of a duck bent backwards, and a new memoir called \"Considerations on the Mollusks and in Particular on the Cephalopods\". He attacked Geoffroy's argument carefully and strategically. First, he argued for clear definitions in science and no ambiguity in language. Cuvier then proposed that composition was defined as the arrangement of parts. By this definition, to imagine that all organisms consisted of the same organs arranged in the same manner was illogical and false. Cuvier called Geoffroy's unity of composition more of a vague analogy for the composition of animals than true science. He emphasized how analogies did not belong in real science. Next Cuvier gave his own report on Meyranx and Laurencet's paper, showing example after example of how vertebrate and mollusk organ arrangement differed, whether by physical location or orientation in the body. By overwhelming the audience with his plethora of knowledge on cephalopod anatomy, he undercut Geoffroy's credibility and made a convincing argument of his own. After this point the debate became less about the mollusk data and more an argument between two differing scientific philosophies.\n\nOn March 1, 1830, Geoffroy returned with his rebuttal, \"On the Theory of Analogues, to Establish Its Novelty as a Doctrine, and Its Practical Utility as an Instrument\". In response to Cuvier's comments about unity of composition being poorly defined, Geoffroy claimed that he was seeking more \"philosophical resemblances\" other than actual, observable similarities between animals. This vague explanation of unity of composition and the claim that others did not understand what unity of composition really meant did not satisfy Cuvier or his other critics. Geoffroy went on to defend his analogical theory, stressing the methodology of it all to align it to the scientific method. His theory looked at the connections between animals, and therefore was still applicable to making discoveries in science. To illustrate his points, Geoffroy used the example of the hyoid bone in vertebrates. He compared multiple animals with differing numbers of pieces making up the hyoid bone. In the cat with 9 hyoid pieces and humans with 5, Geoffroy made hypotheses for where the extra pieces had gone in humans, suggesting shifts to the ligaments and other bones of the jaw. Cuvier in response compared the hyoid bones across multiple species, where he saw different numbers of parts, different arrangements of the parts and in some, no hyoid at all. To Cuvier these differences suggested different functional needs and did not support unity of composition as Geoffroy said. To counter Cuvier's very analytical approach, Geoffroy said his rival was simply getting too bogged down in the details and forgetting the main issue of differences in philosophy. Cuvier continued to supply examples of differences amongst animal form, insisting Geoffroy explain why nature would be constrained to using the same parts similarly across all species.\n\nBy April 1830, the Academy was getting bored of the debate. At this point it had become far too personal and less about the ideas discussed but rather an continuous opportunity to bash each other publicly and academically. Geoffroy finally declined to provide further comment on April 5, 1830. His reasons for ending the debate are not entirely known, though it is generally believed that Cuvier won the debate with his commanding presence and overwhelming amounts of evidence. Geoffroy may have ended the debate to save himself from more public annihilation via Cuvier, but nonetheless promised to further publish the rest of his arguments later.\n\nAfter Geoffroy's initial inflammatory report, some other scientists in the Academy felt pressed to choose a side. In an effort to protect his reputation, the co-author on the report, Latrielle, wrote to Cuvier to deny his participation in the report presented to the Academy, as well as to disassociate himself from Geoffroy's unity of composition theory. This was likely a strategical move as Cuvier had just the year before helped Latrielle, a rather old man of sixty-seven, replace Lamarck as a professor in the Museum. Both authors of the original cephalopod paper wrote to Cuvier as well to apologize for the trouble their paper had caused. However, the damage on Meyranx and Laurencet's careers may already have been done, as no future works by the authors have been recorded, nor is their cephalopod paper to be found.\n\nAfter the debate officially ended, both naturalists continued to throw in snide mentions to each other's works in Academy discussion, though neither Geoffroy of Cuvier openly engaged each other again. The press and scientific journals followed their volatile disagreements up until Cuvier's death by cholera a mere two years after the debate. While the two naturalists were unable to reconcile their ideas in life, Geoffroy spoke at Cuvier's funeral quite fondly of his former friend, feeling pleased \"to have been the first to recognize and reveal to the learned world the scope of genius who did not yet know it himself\", as well as saying anyone who studied the science of nature/natural history owed it to Cuvier for laying the foundation with his genius and massive knowledge of the natural world. While neither Geoffroy or Cuvier's ideas were fully adopted and the other ignored in future science theory, both unity of composition and functional morphology can be seen as influences to further works on the natural world. Geoffroy's ideas of unity of composition and development of more complex organisms from a common, less complex plan often recognize Geoffroy as one of the first to accept evolution theory.\n\n"}
{"id": "561873", "url": "https://en.wikipedia.org/wiki?curid=561873", "title": "Earth pigment", "text": "Earth pigment\n\nEarth pigments are naturally occurring minerals containing metal oxides, principally iron oxides and manganese oxides, that have been used since prehistoric times as pigments. The primary types are ochre, sienna and umber.\n\nEarth pigments are known for their fast drying time in oil painting, relative inexpensiveness, and lightfastness. Cave paintings done in sienna still survive today.\n\nAfter mining, the mineral used for making a pigment is ground to a very fine powder (if not already in the form of clay), washed to remove water-soluble components, dried, and ground again to powder. For some pigments, notably sienna and umber, the color can be deepened by heating (calcination) in a process known as \"burning\", although it does not involve oxidation but instead dehydration.\n"}
{"id": "13697159", "url": "https://en.wikipedia.org/wiki?curid=13697159", "title": "Eerik Kumari", "text": "Eerik Kumari\n\nEerik Kumari (7 March 1912 in Kirbla, Lihula Parish – 8 January 1984; born as Erik Mathias Sits) was a doctor of biology, the founder of ornithology and nature conservation in Estonia, the learned director of the Institute of Zoology and Botany at the Estonian Academy of Sciences during 1952-1977. He was the president of the Estonian Naturalists' Society in 1954–1964.\n\nThe Eerik Kumari Award was established in 1989 in his name to honor those who have excelled in bioscience in Estonia.\n\n"}
{"id": "12505", "url": "https://en.wikipedia.org/wiki?curid=12505", "title": "Galilean moons", "text": "Galilean moons\n\nThe Galilean moons are the four largest moons of Jupiter—Io, Europa, Ganymede, and Callisto. They were first seen by Galileo Galilei in January 1610, and recognized by him as satellites of Jupiter in March 1610. They were the first objects found to orbit another planet. Their names derive from the lovers of Zeus. They are among the largest objects in the Solar System with the exception of the Sun and the eight planets, with a radius larger than any of the dwarf planets. Ganymede is the largest moon in the Solar System, and is even bigger than the planet Mercury, though only around half as massive. The three inner moons—Io, Europa, and Ganymede—are in a 4:2:1 orbital resonance with each other. Because of their much smaller size, and therefore weaker self-gravitation, all of Jupiter's remaining moons have irregular forms rather than a spherical shape.\n\nThe Galilean moons were observed in either 1609 or 1610 when Galileo made improvements to his telescope, which enabled him to observe celestial bodies more distinctly than ever. Galileo's observations showed the importance of the telescope as a tool for astronomers by proving that there were objects in space that cannot be seen by the naked eye. The discovery of celestial bodies orbiting something other than Earth dealt a serious blow to the then-accepted Ptolemaic world system, a geocentric theory in which everything orbits around Earth.\n\nGalileo initially named his discovery the Cosmica Sidera (\"Cosimo's stars\"), but the names that eventually prevailed were chosen by Simon Marius. Marius discovered the moons independently at the same time as Galileo, and gave them their present names, which were suggested by Johannes Kepler, in his \"Mundus Jovialis\", published in 1614.\n\nAs a result of improvements Galileo Galilei made to the telescope, with a magnifying capability of 20×, he was able to see celestial bodies more distinctly than was ever possible before. This allowed Galilei to observe in either December 1609 or January 1610 what came to be known as the Galilean moons.\n\nOn January 7, 1610, Galileo wrote a letter containing the first mention of Jupiter's moons. At the time, he saw only three of them, and he believed them to be fixed stars near Jupiter. He continued to observe these celestial orbs from January 8 to March 2, 1610. In these observations, he discovered a fourth body, and also observed that the four were not fixed stars, but rather were orbiting Jupiter.\n\nGalileo's discovery proved the importance of the telescope as a tool for astronomers by showing that there were objects in space to be discovered that until then had remained unseen by the naked eye. More importantly, the discovery of celestial bodies orbiting something other than Earth dealt a blow to the then-accepted Ptolemaic world system, which held that Earth was at the center of the universe and all other celestial bodies revolved around it. Galileo's \"Sidereus Nuncius\" (\"Starry Messenger\"), which announced celestial observations through his telescope, does not explicitly mention Copernican heliocentrism, a theory that placed the Sun at the center of the universe. Nevertheless, Galileo accepted the Copernican theory.\n\nA Chinese historian of astronomy, Xi Zezong, has claimed that a \"small reddish star\" observed near Jupiter in 362 BCE by Chinese astronomer Gan De may have been Ganymede, predating Galileo's discovery by around two millennia.\n\nIn 1605, Galileo had been employed as a mathematics tutor for Cosimo de' Medici. In 1609, Cosimo became Grand Duke Cosimo II of Tuscany. Galileo, seeking patronage from his now-wealthy former student and his powerful family, used the discovery of Jupiter's moons to gain it. On February 13, 1610, Galileo wrote to the Grand Duke's secretary:\n\n\"God graced me with being able, through such a singular sign, to reveal to my Lord my devotion and the desire I have that his glorious name live as equal among the stars, and since it is up to me, the first discoverer, to name these new planets, I wish, in imitation of the great sages who placed the most excellent heroes of that age among the stars, to inscribe these with the name of the Most Serene Grand Duke.\"\n\nGalileo asked whether he should name the moons the \"Cosmian Stars\", after Cosimo alone, or the \"Medician Stars\", which would honor all four brothers in the Medici clan. The secretary replied that the latter name would be best.\n\nOn March 12, 1610, Galileo wrote his dedicatory letter to the Duke of Tuscany, and the next day sent a copy to the Grand Duke, hoping to obtain the Grand Duke's support as quickly as possible. On March 19, he sent the telescope he had used to first view Jupiter's moons to the Grand Duke, along with an official copy of \"Sidereus Nuncius\" (\"The Starry Messenger\") that, following the secretary's advice, named the four moons the Medician Stars. In his dedicatory introduction, Galileo wrote:\nScarcely have the immortal graces of your soul begun to shine forth on earth than bright stars offer themselves in the heavens which, like tongues, will speak of and celebrate your most excellent virtues for all time. Behold, therefore, four stars reserved for your illustrious name ... which ... make their journeys and orbits with a marvelous speed around the star of Jupiter ... like children of the same family ... Indeed, it appears the Maker of the Stars himself, by clear arguments, admonished me to call these new planets by the illustrious name of Your Highness before all others.\n\nGalileo initially called his discovery the Cosmica Sidera (\"Cosimo's stars\"), in honour of Cosimo II de' Medici (1590–1621). At Cosimo's suggestion, Galileo changed the name to Medicea Sidera (\"the Medician stars\"), honouring all four Medici brothers (Cosimo, Francesco, Carlo, and Lorenzo). The discovery was announced in the \"Sidereus Nuncius\" (\"Starry Messenger\"), published in Venice in March 1610, less than two months after the first observations.\n\nOther names put forward include:\n\nThe names that eventually prevailed were chosen by Simon Marius, who discovered the moons independently at the same time as Galileo: he named them at the suggestion of Johannes Kepler after lovers of the god Zeus (the Greek equivalent of Jupiter): \"Io\", \"Europa\", \"Ganymede\" and \"Callisto\", in his \"Mundus Jovialis\", published in 1614.\n\nGalileo steadfastly refused to use Marius' names and invented as a result the numbering scheme that is still used nowadays, in parallel with proper moon names. The numbers run from Jupiter outward, thus I, II, III and IV for Io, Europa, Ganymede, and Callisto respectively. Galileo used this system in his notebooks but never actually published it. The numbered names (Jupiter \"x\") were used until the mid-20th century when other inner moons were discovered, and Marius' names became widely used.\n\nGalileo was able to develop a method of determining longitude based on the timing of the orbits of the Galilean moons. The times of the eclipses of the moons could be precisely calculated in advance, and compared with local observations on land or on ship to determine the local time and hence longitude. The main problem with the technique was that it was difficult to observe the Galilean moons through a telescope on a moving ship; a problem that Galileo tried to solve with the invention of the celatone. The method was used by Cassini and Picard to re-map France.\n\nSome models predict that there may have been several generations of Galilean satellites in Jupiter's early history. Each generation of moons to have formed would have spiraled into Jupiter and been destroyed, due to tidal interactions with Jupiter's proto-satellite disk, with new moons forming from the remaining debris. By the time the present generation formed, the gas in the proto-satellite disk had thinned out to the point that it no longer greatly interfered with the moons' orbits. \nOther models suggest that Galilean satellites formed in a proto-satellite disk, in which formation timescales were comparable to or shorter than orbital migration timescales. Io is anhydrous and likely has an interior of rock and metal. Europa is thought to contain 8% ice and water by mass with the remainder rock. These moons are, in increasing order of distance from Jupiter:\n\nIo (Jupiter I) is the innermost of the four Galilean moons of Jupiter and, with a diameter of 3642 kilometers, the fourth-largest moon in the Solar System. It was named after Io, a priestess of Hera who became one of the lovers of Zeus. Nevertheless, it was simply referred to as \"Jupiter I\", or \"The first satellite of Jupiter\", until the mid-20th century.\n\nWith over 400 active volcanos, Io is the most geologically active object in the Solar System. Its surface is dotted with more than 100 mountains, some of which are taller than Earth's Mount Everest. Unlike most satellites in the outer Solar System (which have a thick coating of ice), Io is primarily composed of silicate rock surrounding a molten iron or iron sulfide core.\n\nAlthough not proven, recent data from the Galileo orbiter indicate that Io might have its own magnetic field. Io has an extremely thin atmosphere made up mostly of sulfur dioxide (SO). If a surface data or collection vessel were to land on Io in the future, it would have to be extremely tough (similar to the tank-like bodies of the Soviet Venera landers) to survive the radiation and magnetic fields that originate from Jupiter.\n\nEuropa (Jupiter II), the second of the four Galilean moons, is the second closest to Jupiter and the smallest at 3121.6 kilometers in diameter, which is slightly smaller than the Moon. The name comes from a mythical Phoenician noblewoman, Europa, who was courted by Zeus and became the queen of Crete, though the name did not become widely used until the mid-20th century.\n\nIt has a smooth and bright surface, with a layer of water surrounding the mantle of the planet, thought to be 100 kilometers thick. The smooth surface includes a layer of ice, while the bottom of the ice is theorized to be liquid water. The apparent youth and smoothness of the surface have led to the hypothesis that a water ocean exists beneath it, which could conceivably serve as an abode for extraterrestrial life. Heat energy from tidal flexing ensures that the ocean remains liquid and drives geological activity. Life may exist in Europa's under-ice ocean. So far, there is no evidence that life exists on Europa, but the likely presence of liquid water has spurred calls to send a probe there.\n\nThe prominent markings that criss-cross the moon seem to be mainly albedo features, which emphasize low topography. There are few craters on Europa because its surface is tectonically active and young. Some theories suggest that Jupiter's gravity is causing these markings, as one side of Europa is constantly facing Jupiter. Also, volcanic water eruptions splitting the surface of Europa, and even geysers have been considered as a cause. The color of the markings, reddish-brown, is theorized to be caused by sulfur, but scientists cannot confirm that, because no data collection devices have been sent to Europa. Europa is primarily made of silicate rock and likely has an iron core. It has a tenuous atmosphere composed primarily of oxygen.\n\nGanymede (Jupiter III), the third Galilean moon is named after the mythological Ganymede, cupbearer of the Greek gods and Zeus's beloved. Ganymede is the largest natural satellite in the Solar System at 5262.4 kilometers in diameter, which makes it larger than the planet Mercury – although only at about half of its mass since Ganymede is an icy world. It is the only satellite in the Solar System known to possess a magnetosphere, likely created through convection within the liquid iron core.\n\nGanymede is composed primarily of silicate rock and water ice, and a salt-water ocean is believed to exist nearly 200 km below Ganymede's surface, sandwiched between layers of ice. The metallic core of Ganymede suggests a greater heat at some time in its past than had previously been proposed. The surface is a mix of two types of terrain—highly cratered dark regions and younger, but still ancient, regions with a large array of grooves and ridges. Ganymede has a high number of craters, but many are gone or barely visible due to its icy crust forming over them. The satellite has a thin oxygen atmosphere that includes O, O, and possibly O (ozone), and some atomic hydrogen.\n\nCallisto (Jupiter IV) is the fourth and last Galilean moon, and is the second largest of the four, and at 4820.6 kilometers in diameter, it is the third largest moon in the Solar System, and barely smaller than Mercury, though only a third of the latter's mass. It is named after the Greek mythological nymph Callisto, a lover of Zeus who was a daughter of the Arkadian King Lykaon and a hunting companion of the goddess Artemis. The moon does not form part of the orbital resonance that affects three inner Galilean satellites and thus does not experience appreciable tidal heating. Callisto is composed of approximately equal amounts of rock and ices, which makes it the least dense of the Galilean moons. It is one of the most heavily cratered satellites in the Solar System, and one major feature is a basin around 3000 km wide called Valhalla.\n\nCallisto is surrounded by an extremely thin atmosphere composed of carbon dioxide and probably molecular oxygen. Investigation revealed that Callisto may possibly have a subsurface ocean of liquid water at depths less than 300 kilometres. The likely presence of an ocean within Callisto indicates that it can or could harbour life. However, this is less likely than on nearby Europa. Callisto has long been considered the most suitable place for a human base for future exploration of the Jupiter system since it is furthest from the intense radiation of Jupiter.\n\nFluctuations in the orbits of the moons indicate that their mean density decreases with distance from Jupiter. Callisto, the outermost and least dense of the four, has a density intermediate between ice and rock whereas Io, the innermost and densest moon, has a density intermediate between rock and iron. Callisto has an ancient, heavily cratered and unaltered ice surface and the way it rotates indicates that its density is equally distributed, suggesting that it has no rocky or metallic core but consists of a homogeneous mix of rock and ice. This may well have been the original structure of all the moons. The rotation of the three inner moons, in contrast, indicates differentiation of their interiors with denser matter at the core and lighter matter above. They also reveal significant alteration of the surface. Ganymede reveals past tectonic movement of the ice surface which required partial melting of subsurface layers. Europa reveals more dynamic and recent movement of this nature, suggesting a thinner ice crust. Finally, Io, the innermost moon, has a sulfur surface, active volcanism and no sign of ice. All this evidence suggests that the nearer a moon is to Jupiter the hotter its interior. The current model is that the moons experience tidal heating as a result of the gravitational field of Jupiter in inverse proportion to the square of their distance from the giant planet. In all but Callisto this will have melted the interior ice, allowing rock and iron to sink to the interior and water to cover the surface. In Ganymede a thick and solid ice crust then formed. In warmer Europa a thinner more easily broken crust formed. In Io the heating is so extreme that all the rock has melted and water has long ago boiled out into space.\n\nJupiter's regular satellites are believed to have formed from a circumplanetary disk, a ring of accreting gas and solid debris analogous to a protoplanetary disk. They may be the remnants of a score of Galilean-mass satellites that formed early in Jupiter's history.\n\nSimulations suggest that, while the disk had a relatively high mass at any given moment, over time a substantial fraction (several tenths of a percent) of the mass of Jupiter captured from the Solar nebula was processed through it. However, the disk mass of only 2% that of Jupiter is required to explain the existing satellites. Thus there may have been several generations of Galilean-mass satellites in Jupiter's early history. Each generation of moons would have spiraled into Jupiter, due to drag from the disk, with new moons then forming from the new debris captured from the Solar nebula. By the time the present (possibly fifth) generation formed, the disk had thinned out to the point that it no longer greatly interfered with the moons' orbits. The current Galilean moons were still affected, falling into and being partially protected by an orbital resonance which still exists for Io, Europa, and Ganymede. Ganymede's larger mass means that it would have migrated inward at a faster rate than Europa or Io.\n\nAll four Galilean moons are bright enough to be viewed from Earth without a telescope, if only they could appear farther away from Jupiter. (They are, however, easily distinguished with even low-powered binoculars.) They have apparent magnitudes between 4.6 and 5.6 when Jupiter is in opposition with the Sun, and are about one unit of magnitude dimmer when Jupiter is in conjunction. The main difficulty in observing the moons from Earth is their proximity to Jupiter, since they are obscured by its brightness. The maximum angular separations of the moons are between 2 and 10 arcminutes from Jupiter, which is close to the limit of human visual acuity. Ganymede and Callisto, at their maximum separation, are the likeliest targets for potential naked-eye observation.\n\nGIF animation of the resonance of Io, Europa, and Ganymede\n\n\n<br>\n"}
{"id": "53073378", "url": "https://en.wikipedia.org/wiki?curid=53073378", "title": "Girl Develop It", "text": "Girl Develop It\n\nGirl Develop It (GDI) is a nonprofit organization devoted to getting women the materials they need to pursue careers in software development. It provides affordable programs for adult women interested in learning web and software development in a judgment-free environment. Their mission is to give women of any income level, nationality, education level, and upbringing an environment in which to learn the skills to build websites and learn code to build programs with hands-on classes in two countries in 56 cities.\n\nGirl Develop It was started in 2010 by Vanessa Hurst and Sara Chipps with their flagship location in New York City. GDI started with just one class that sold out in one day. Today more than 55,000 members have been helped. Between that, they have built up their organization to 53 cities in 33 states and districts in the United States and one in Ottawa, Ontario Canada. Recently, Girl Develop It has empowered more than 1,000 students per month with coding skills. In late 2017, GDI will start addressing international inquiries. \nThe organization and local chapters have hosted or participated in hackathons. During the Buffalo chapter's second event in 2016, developers competed to create websites for nonprofit woman- and minority-owned organizations. The organization has also hosted hackathons in Camden, in Wilmington, and in Seattle.\n\nGirl Develop It (GDI) offers materials on their website that are licensed under a Creative Commons (CC BY-NC-SA 4.0) license and that provide visitors with tools and resources to develop online. The curriculum is hosted and constructed by the GDI community on the web-based version control repository GitHub and presented in a slide format, divided by topic. On the GitHub curriculum page, materials are broken up in a color coded format that shows whether they have been reviewed by other members of the community or if the topics meet the requirements or recommendations of the curriculum. These materials are accessible by anyone on the Internet but Girl Development offers in-person courses and social communities at established chapters.\n\nThe topics of the curriculum include:\n\nHurst is a computer programmer, social entrepreneur, teacher, and lifetime girl scout, and a co-founder of Girl Develop It. In 2013 she launched the CodeMontage platform. She is also responsible for founding and running Developers For Good and also NYC-based Network of Technologists. She is a strong advocator for computing, software, and coding for everyone. She is currently based in New York City.\n\nChipps is a JavaScript developer and co-founder of Girl Develop It. Once the CTO of Flatiron School, she is currently the CEO of Jewelbots, a friendship bracelet that helps children learn to code. She is currently based in New York City.\n\nCorinne has previously held positions with the Technical.ly news network, producing large-scale technology conferences like Philly Tech Week and Baltimore Innovation Week. \n\nVanessa is a lifetime Girl Scout, a teacher, and a passionate supporter of open source software. She also founded organizes Developers for Good. \n\nPrior to joining GDI, Bindu won awards for her work developing and executing the operational strategy for the Elwyn Baring Street Center. Bindu is also founder and Board Chair of Karanso Africa.\n\nGirl Develop It (GDI) lists numerous companies and organizations on their website that have backed, partnered with, or supported them and their cause.\n\nNotable Supporters include:\n\n"}
{"id": "1104922", "url": "https://en.wikipedia.org/wiki?curid=1104922", "title": "Heron Island (Queensland)", "text": "Heron Island (Queensland)\n\nHeron Island is a coral cay located near the Tropic of Capricorn in the southern Great Barrier Reef, north-east of Gladstone, Queensland, Australia, and north-north-west of the state capital Brisbane. The island is situated on the leeward (western) side of Heron Reef, a fringing platform reef of significant biodiversity, supporting around 900 of the 1,500 fish species and 72% of the coral species found on the Great Barrier Reef.\n\nThe island is about long and at its widest, giving an area of approximately . The highest point, near the western tip, is above sea level. A dune ridge along the southern shore rises some above sea level, lower dunes on the north-eastern side are only about above the sea.\n\nHeron Island and an extrapolated version of the research station are the scene of much of the first part of Arthur C. Clarke's \"The Deep Range\".\n\nHeron Island was discovered on 12 January 1843 by a Royal Navy expedition comprising the corvette and the cutter \"Bramble\". The expedition, commanded by Captain Francis Blackwood, was engaged in surveying the eastern edge of the Great Barrier Reef to map out detailed plans for safe passages within the reef.\n\nThe island was named by Lieutenant Charles Bampfield Yule, the commander of \"Bramble\".\n\nThe island did not become inhabited until the early 20th century when a turtle cannery was established. The aim was to profit from the seasonal influx of green turtles, but the venture soon found it difficult to keep the business afloat. Other attempts at establishing fisheries were abandoned.\n\nIn 1932 Captain Christian Poulsen, engaged in bringing fishing parties to the reef, realised the potential of the island as a tourist attraction. In 1936 he bought the lease of the island for £290. On 11 September 1943, the entire island was declared a National Park.\n\nHeron Island Resort, operated by The Aldesta Group, is located in the north-west corner of the island. The resort is a popular getaway for scuba diving and snorkelling and accommodates up to 200 guests and 100 staff members. In March 2012 Heron Island Resort was featured in the BBC's nature TV series, \"Great Barrier Reef\".\n\nThe University of Queensland Heron Island Research Station is situated in the island's south-west quarter. Established in the 1950s by the Great Barrier Reef Committee with the University of Queensland becoming a partner in its operations in 1970, the facility is one of the world's principal coral reef research stations, with a wide variety of research undertaken on coral reef ecology.\nHeron Island Research Station suffered a large fire on Friday, 30 March 2007. No one was injured.\n\nIn June 2008 the new student accommodation, comprising 80 beds, was officially opened and used for the first time by Tropical Marine Network students . The teaching laboratories and new research building with 9 research labs, library, darkroom, computer room and aquaria deck were officially reopened in February 2009.\n\nIn 2010, a state of the art climate change experimental facility was opened at the Research Station.\n\nSir David Attenborough and Atlantic Productions filmed segments for the documentary, \"David Attenborough's Great Barrier Reef\", at Heron Island Research Station in late 2014.\n\nThe eastern half of the island is protected and forms part of the Capricornia Cays National Park, with a permanent ranger's station onsite.\n\nThere is a small man-made channel and wooden jetty on the western shore of the island, where the daily catamaran launch from Gladstone docks and supplies to the island are delivered. The rusted wreck of HMCS \"Protector\" lies at the entrance to the channel, and was towed to there in 1945 to form a dive and snorkelling site.\n\nThe island has no fresh water supply. A small desalination plant on the island uses reverse osmosis technology to supply water for human consumption. Similarly, two diesel generators (and some solar panels) supply electricity to the island.\n\nHeron Island has notably rich soil for a tropical coral cay, particularly in the dense southern forest. This is due to the presence of tens of thousands of wedge-tailed shearwaters (\"Ardenna pacifica\") during breeding season. These birds disturb the humus as they dig their nesting burrows, and thus prevent the formation of Jemo soil, a phosphatic hardpan topped off by raw humus. The hardpan is formed by leaching of surface- or tree-nesting seabirds' guano in the absence of burrowing animals.\n\nRich forests of \"Pisonia grandis\" dominate the centre and south of Heron Island. Towards the eastern and north-western ends, the forest is readily accessible, but its heart is a dense tangle, interrupted only by a few trails.\n\nSome trees in the heart of the forest grow to 10–11 m, but most are just 6–8 m high. The understory is largely absent here, formed only by scattered \"Celtis paniculata\", \"Ficus opposita\" and \"Pipturus argenteus\" with a height of 2–4 m; some \"Celtis\" also grow higher and emerge through the \"Pisonia\" canopy. Patches of shrubs – mainly \"Abutilon albescens\", with \"Melanthera biflora\" (probably var. \"canescens\"), and the introduced wild poinsettia (\"Euphorbia cyathophora\") – are found here and there. Herbaceous plants are scarce here, mainly consisting of the grass \"Stenotaphrum micranthum\". The more open forest is composed of much the same plants, but the \"Pisonia\" does not predominate as much. A few \"Pandanus tectorius\" screwpines are also found here, and the understory is far more prominent.\n\nNorth of the \"Pisonia\" forest, a band of open shrubland with some trees extends from the resort to the island's eastern tip. Octopus bush (\"Heliotropium foertherianum\") and sea cabbage (\"Scaevola taccada\") form the major bush cover, while \"Abutilon\" and \"Melanthera\" are the characteristic ground plants. The trees here are mainly \"Pandanus\", but also \"Celtis\", the she-oak \"Casuarina equisetifolia\" ssp. \"incana\", \"Ficus\", bay cedar (\"Suriana maritima\"). Herbs—mainly the parasitic vine \"Cassytha filiformis\" as well as \"Euphorbia tannensis\" ssp. \"eremophila\" and grasses (mainly Pacific island thintail, \"Lepturus repens\" var. \"subulatus\") are abundant.\n\nThe eastern end is marked by a similar habitat, with mainly \"Casuarina, Scaevola\" and \"Heliotropium\". This type of vegetation, with some \"Pandanus\" in between, extends along the southern and northern dune ridges. On the dune slopes, \"Boerhavia repens\", \"Commicarpus chinensis\" var. \"chinensis\" (or \"Commicarpus australis\"?), the searocket \"Cakile edentula\", yet another \"Euphorbia\" (probably \"Euphorbia sparrmanii\"), and kuroiwa grass (\"Thuarea involuta\") are common.\nEast of the resort in the north-western part of Heron Island there is another type of forest, more open than the central wood. The main tree here is the manjack \"Cordia subcordata\" of which few are found elsewhere on Heron Island; \"Pisonia\" trees are present but not dominant. The \"Abutilon\"–\"Euphorbia cyathophora\"–\"Melanthera\" scrub grows thick here. \"Scaevola\" and \"Heliotropium\" as well as patches of the dropseed grass \"Sporobolus virginicus\" occur at this forest's edge.\n\nThe sea turtle nesting area is further east, making up the central part of the northern shoreside. The animals' burrowing has prevented a proper forest from forming. Consequently, though the usual tree species are found in isolated individuals, the sand is overgrown with herbs and small shrubs, mainly \"Cakile\", \"Cassytha\", \"Euphorbia eremophila\", \"Lepturus\" and \"Melanthera\".\n\nAround the western end there is an abundance of plants introduced by the research and resort activity, some deliberately as ornamentals, others accidentally. Notable are \"Euphorbia cyathophora\" and \"Pseudognaphalium luteoalbum\", as well as papaya (\"Carica papaya\"), coconut palm (\"Cocos nucifera\"), oleander (\"Nerium oleander\") and temple tree (\"Plumeria rubra\") which have been planted.\n\nHeron Island is part of the Capricornia Cays Important Bird Area. The island's forest and surrounding dunes provide habitat for thousands of nesting seabirds, including the wedge-tailed shearwater (Ardenna pacifica) and the south-western black noddy (\"Anous minutus minutus\"), during the breeding season between October and April. Over 70,000 white-capped noddies nest on the island during this period.\n\nAll-year resident and breeding on Heron Island are:\nThough other herons may occasionally visit the island, the only member of the Ardeidae which is a breeding resident is the eastern reef egret. And even though the terms \"heron\" and \"egret\" are not scientific, the former is generally used to denote the large \"Ardea\" whereas the smaller \"Egretta\" species are usually called \"egrets\". Insofar, the only \"true\" heron that could ever be found on Heron Island is the white-necked heron (\"Ardea pacifica\"), which is only seen every now and then as a rare vagrant.\n\nAt some time in the mid-20th century, a pair of white-bellied sea eagles (\"Haliaeetus leucogaster\") nested on Heron Island. However, these birds either died or moved elsewhere, and the species today only occurs as a non-breeding visitor, albeit not too rarely.\nAt least one species of rat, probably the widespread polynesian rat (\"Rattus exulans\"), is found on the island. Though even these small rats are known to harm island birds, this is insignificant on islands so close to a continent; while the rats probably feed on eggs and nestlings, they do not threaten the breeding bird populations as a whole.\n\nHeron Island is also a major nesting site for green (\"Chelonia mydas\") and Indopacific loggerhead sea turtles (\"Caretta caretta gigas\"). Around 98% of all turtles that nest on the island are green turtles, and only 2% of them will be loggerheads. The Indopacific hawksbill sea turtle (\"Eretmochelys imbricata bissa\") has been seen on the reef but apparently does not breed on the island. Other marine life includes the inhabitants of the coral reef, and around early October, cetaceans (e.g. humpback whales, \"Megaptera novaeangliae\") pass Heron Island on their migration to their summer quarters in subantarctic waters.\n\nA notable and much-studied invertebrate of Heron Island is \"Cerithium moniliferum\", a small marine snail. These animals will form large groups as the tide recedes. Feeding on beach rock at a specific height over the average low tide level, the snails slowly move about in their clusters, preserving the precious moisture that allows them to breathe overwater.\n\nMosquitos and other biting insects are rare on the island. However, diseases such as avian malaria and avian pox, which are carried by biting mosquitoes have been found in low numbers in the island's silvereyes.\n\nHeron Reef is a lagoonal platform reef. It has developed in a high energy environment with high tidal flows promoting water turnover and unobstructed access to the ocean. The reef dates from the Holocene period but shows evidence of possible development in the Pleistocene period. Core analysis of the reef from 1937, demonstrated a thickness of at least 15m of stacked limestone, with an eastward sloping disconformity.\n\nJane Lockhart sank between 11 and the 17 of December 1868 on Lady Musgrave Island / Heron Island / Masthead Reef or One Tree Island. The vessel was a two-mast schooner. Departed from Sydney with general cargo for Broadsound; and ran aground on Lady Musgrave Is; maybe on Heron Is or One Tree Island or Masthead Reef Lost on a reef off Heron Island on the night of 17 December 1868. The crew took to the boat and safely reached the pilot station at Keppel Island.\n\nOriginally stated as on Lady Musgrave reef (most unlikely) later news reports claim wreck on Heron Island with some other reports mention the wreck on either One Tree Island or Mast Head.\nThe vessel was built in 1861 at Ulladulla, New South Wales and registered in Sydney with the Official number of 36858 and a Registered number of 9/1861.\n\nFrom the original reports\n\nOne of the boats dispatched to the wreck of the \"Jane Lockhart\", schooner, has returned with the sails and a portion of the running and standing gear. The vessel, it appears, did not strike on Bunker's Group, as reported by Captain Machen, but upon what is known as Heron Island, about ninety miles to the northward of Bunker's Group. When the boat reached the vessel she was settled in a hollow in one of the reefs, the outer formation of the hollow acting as a breakwater against the seas. One side of the vessel was quite visible, and the new copper sheathing appeared uninjured. Captain Norris, who went down in charge of the boat, unbent the sails, so that the position of the vessel might as much as possible remain unaltered; he left the yards and masts standing.\nand 6 months later it was reported as\n\nThe \"Rose\", schooner, has returned from the wreck of the \"Jane Lockhart\", on Masthead Reef, whither she went on June 15 Captain Dwyer informs us that the Jane Lockhart still lies in a very snug position, and he has no doubt but that himself, and Mr Norris, the purchaser of the wreck, will be able, ultimately, to raise the vessel and bring her safely to Rockhampton\n\nNearly the whole of the period that they were at the reef, very heavy weather prevailed, staving operations towards the recovery of the cargo, but luckily the strong SE winds lulled for about three days Captain Dwyer availed himself of the occasion, set to work, rigged up a staging between the masts of the \"Lockhart\", schooner, and by means of a rope and a South Sea Island diver, managed to bring up from eighty to ninety large iron pulley wheels, besides a quantity of machinery and sundries, comprising Ale, porter, liqueur brandy, cutlery, ironmongery, etc Unfortunately the \"Roses\" water ran out, much to the chagrin of the crew, who would have raised a great deal more, only having to run into port for supplies\nA photo of Heron Island is included on the Voyager Golden Record which was sent past the limits of our Solar System aboard the Voyager 1 and Voyager 2 spacecraft. The photo of Heron Island was selected as one of the examples that portrayed the diversity of life and culture on Earth.\n\n"}
{"id": "101336", "url": "https://en.wikipedia.org/wiki?curid=101336", "title": "High-temperature superconductivity", "text": "High-temperature superconductivity\n\nHigh-temperature superconductors (abbreviated high-\"T\" or HTS) are materials that behave as superconductors at unusually high temperatures. The first high-\"T\" superconductor was discovered in 1986 by IBM researchers Georg Bednorz and K. Alex Müller, who were awarded the 1987 Nobel Prize in Physics \"for their important break-through in the discovery of superconductivity in ceramic materials\".\n\nWhereas \"ordinary\" or metallic superconductors usually have transition temperatures (temperatures below which they are superconductive) below and must be cooled using liquid helium in order to achieve superconductivity, HTS have been observed with transition temperatures as high as , and can be cooled to superconductivity using liquid nitrogen. Until 2008, only certain compounds of copper and oxygen (so-called \"cuprates\") were known to have HTS properties, and the term high-temperature superconductor was used interchangeably with cuprate superconductor for compounds such as bismuth strontium calcium copper oxide (BSCCO) and yttrium barium copper oxide (YBCO). Several iron-based compounds (the iron pnictides) are now known to be superconducting at high temperatures.\n\nIn 2015, hydrogen sulfide (HS) under extremely high pressure (around 150 gigapascals) was found to undergo superconducting transition near 203 K (-70 °C), the highest temperature superconductor known to date.\n\nFor an explanation about \"T\" (the critical temperature for superconductivity), see and the second bullet item of .\n\nThe phenomenon of superconductivity was discovered by Kamerlingh Onnes in 1911, in metallic mercury below . Ever since, researchers have attempted to observe superconductivity at increasing temperatures with the goal of finding a room-temperature superconductor. By the late 1970s, superconductivity was observed in several metallic compounds (in particular Nb-based, such as NbTi, NbSn, and NbGe) at temperatures that were much higher than those for elemental metals and which could even exceed . In 1986, J. Georg Bednorz and K. Alex Müller, working at the IBM research lab near Zurich, Switzerland were exploring a new class of ceramics for superconductivity. Bednorz encountered a barium-doped compound of lanthanum and copper oxide whose resistance dropped to zero at a temperature around . Their results were soon confirmed by many groups, notably Paul Chu at the University of Houston and Shoji Tanaka at the University of Tokyo.\n\nShortly after, P. W. Anderson, at Princeton University came up with the first theoretical description of these materials, using the resonating valence bond theory, but a full understanding of these materials is still developing today. These superconductors are now known to possess a \"d\"-wave pair symmetry. The first proposal that high-temperature cuprate superconductivity involves \"d\"-wave pairing was made in 1987 by Bickers, Scalapino and Scalettar, followed by three subsequent theories in 1988 by Inui, Doniach, Hirschfeld and Ruckenstein, using spin-fluctuation theory, and by Gros, Poilblanc, Rice and Zhang, and by Kotliar and Liu identifying \"d\"-wave pairing as a natural consequence of the RVB theory. The confirmation of the \"d\"-wave nature of the cuprate superconductors was made by a variety of experiments, including the direct observation of the \"d\"-wave nodes in the excitation spectrum through Angle Resolved Photoemission Spectroscopy, the observation of a half-integer flux in tunneling experiments, and indirectly from the temperature dependence of the penetration depth, specific heat and thermal conductivity.\n\nUntil 2015 the superconductor with the highest transition temperature that had been confirmed by multiple independent research groups (a prerequisite to being called a discovery, verified by peer review) was mercury barium calcium copper oxide (HgBaCaCuO) at around 133 K.\n\nAfter more than twenty years of intensive research, the origin of high-temperature superconductivity is still not clear, but it seems that instead of \"electron-phonon\" attraction mechanisms, as in conventional superconductivity, one is dealing with genuine \"electronic\" mechanisms (e.g. by antiferromagnetic correlations), and instead of conventional, purely \"s\"-wave pairing, more exotic pairing symmetries are thought to be involved (\"d\"-wave in the case of the cuprates; primarily extended \"s\"-wave, but occasionally \"d\"-wave, in the case of the iron-based superconductors). In 2014, evidence showing that fractional particles can happen in quasi two-dimensional magnetic materials, was found by EPFL scientists lending support for Anderson's theory of high-temperature superconductivity.\n\nThe structure of high-\"T\" copper oxide or cuprate superconductors are often closely related to perovskite structure, and the structure of these compounds has been described as a distorted, oxygen deficient multi-layered perovskite structure. One of the properties of the crystal structure of oxide superconductors is an alternating multi-layer of CuO planes with superconductivity taking place between these layers. The more layers of CuO, the higher \"T\". This structure causes a large anisotropy in normal conducting and superconducting properties, since electrical currents are carried by holes induced in the oxygen sites of the CuO sheets. The electrical conduction is highly anisotropic, with a much higher conductivity parallel to the CuO plane than in the perpendicular direction. Generally, critical temperatures depend on the chemical compositions, cations substitutions and oxygen content. They can be classified as superstripes; i.e., particular realizations of superlattices at atomic limit made of superconducting atomic layers, wires, dots separated by spacer layers, that gives multiband and multigap superconductivity.\n\nThe first superconductor found with \"T\" > 77 K (liquid nitrogen boiling point) is yttrium barium copper oxide (YBaCuO); the proportions of the three different metals in the YBaCuO superconductor are in the mole ratio of 1 to 2 to 3 for yttrium to barium to copper, respectively. Thus, this particular superconductor is often referred to as the 123 superconductor.\n\nThe unit cell of YBaCuO consists of three pseudocubic elementary perovskite unit cells. Each perovskite unit cell contains a Y or Ba atom at the center: Ba in the bottom unit cell, Y in the middle one, and Ba in the top unit cell. Thus, Y and Ba are stacked in the sequence [Ba–Y–Ba] along the c-axis. All corner sites of the unit cell are occupied by Cu, which has two different coordinations, Cu(1) and Cu(2), with respect to oxygen. There are four possible crystallographic sites for oxygen: O(1), O(2), O(3) and O(4). The coordination polyhedra of Y and Ba with respect to oxygen are different. The tripling of the perovskite unit cell leads to nine oxygen atoms, whereas YBaCuO has seven oxygen atoms and, therefore, is referred to as an oxygen-deficient perovskite structure. The structure has a stacking of different layers: (CuO)(BaO)(CuO)(Y)(CuO)(BaO)(CuO). One of the key feature of the unit cell of YBaCuO (YBCO) is the presence of two layers of CuO. The role of the Y plane is to serve as a spacer between two CuO planes. In YBCO, the Cu–O chains are known to play an important role for superconductivity. \"T\" is maximal near 92 K when \"x\" ≈ 0.15 and the structure is orthorhombic. Superconductivity disappears at \"x\" ≈ 0.6, where the structural transformation of YBCO occurs from orthorhombic to tetragonal.\n\nThe crystal structure of Bi-, Tl- and Hg-based high-\"T\" superconductors are very similar. Like YBCO, the perovskite-type feature and the presence of CuO layers also exist in these superconductors. However, unlike YBCO, Cu–O chains are not present in these superconductors. The YBCO superconductor has an orthorhombic structure, whereas the other high-\"T\" superconductors have a tetragonal structure.\n\nThe Bi–Sr–Ca–Cu–O system has three superconducting phases forming a homologous series as BiSrCaCuO (\"n\"=1, 2 and 3). These three phases are Bi-2201, Bi-2212 and Bi-2223, having transition temperatures of 20, 85 and 110 K, respectively, where the numbering system represent number of atoms for Bi, Sr, Ca and Cu respectively. The two phases have a tetragonal structure which consists of two sheared crystallographic unit cells. The unit cell of these phases has double Bi–O planes which are stacked in a way that the Bi atom of one plane sits below the oxygen atom of the next consecutive plane. The Ca atom forms a layer within the interior of the CuO layers in both Bi-2212 and Bi-2223; there is no Ca layer in the Bi-2201 phase. The three phases differ with each other in the number of CuO planes; Bi-2201, Bi-2212 and Bi-2223 phases have one, two and three CuO planes, respectively. The \"c\" axis lattice constants of these phases increases with the number of CuO planes (see table below). The coordination of the Cu atom is different in the three phases. The Cu atom forms an octahedral coordination with respect to oxygen atoms in the 2201 phase, whereas in 2212, the Cu atom is surrounded by five oxygen atoms in a pyramidal arrangement. In the 2223 structure, Cu has two coordinations with respect to oxygen: one Cu atom is bonded with four oxygen atoms in square planar configuration and another Cu atom is coordinated with five oxygen atoms in a pyramidal arrangement.\n\nTl–Ba–Ca–Cu–O superconductor: The first series of the Tl-based superconductor containing one Tl–O layer has the general formula TlBaCaCuO, whereas the second series containing two Tl–O layers has a formula of TlBaCaCuO with \"n\" =1, 2 and 3. In the structure of TlBaCuO (Tl-2201), there is one CuO layer with the stacking sequence (Tl–O) (Tl–O) (Ba–O) (Cu–O) (Ba–O) (Tl–O) (Tl–O). In TlBaCaCuO (Tl-2212), there are two Cu–O layers with a Ca layer in between. Similar to the TlBaCuO structure, Tl–O layers are present outside the Ba–O layers. In TlBaCaCuO (Tl-2223), there are three CuO layers enclosing Ca layers between each of these. In Tl-based superconductors, \"T\" is found to increase with the increase in CuO layers. However, the value of \"T\" decreases after four CuO layers in TlBaCaCuO, and in the TlBaCaCuO compound, it decreases after three CuO layers.\n\nHg–Ba–Ca–Cu–O superconductor: The crystal structure of HgBaCuO (Hg-1201), HgBaCaCuO (Hg-1212) and HgBaCaCuO (Hg-1223) is similar to that of Tl-1201, Tl-1212 and Tl-1223, with Hg in place of Tl. It is noteworthy that the \"T\" of the Hg compound (Hg-1201) containing one CuO layer is much larger as compared to the one-CuO-layer compound of thallium (Tl-1201). In the Hg-based superconductor, \"T\" is also found to increase as the CuO layer increases. For Hg-1201, Hg-1212 and Hg-1223, the values of \"T\" are 94, 128 and the record value at ambient pressure 134 K, respectively, as shown in table below. The observation that the \"T\" of Hg-1223 increases to 153 K under high pressure indicates that the \"T\" of this compound is very sensitive to the structure of the compound.\n\nThe simplest method for preparing high-\"T\" superconductors is a solid-state thermochemical reaction involving mixing, calcination and sintering. The appropriate amounts of precursor powders, usually oxides and carbonates, are mixed thoroughly using a Ball mill. Solution chemistry processes such as coprecipitation, freeze-drying and sol-gel methods are alternative ways for preparing a homogeneous mixture. These powders are calcined in the temperature range from 800 °C to 950 °C for several hours. The powders are cooled, reground and calcined again. This process is repeated several times to get homogeneous material. The powders are subsequently compacted to pellets and sintered. The sintering environment such as temperature, annealing time, atmosphere and cooling rate play a very important role in getting good high-\"T\" superconducting materials. The YBaCuO compound is prepared by calcination and sintering of a homogeneous mixture of YO, BaCO and CuO in the appropriate atomic ratio. Calcination is done at 900–950 °C, whereas sintering is done at 950 °C in an oxygen atmosphere. The oxygen stoichiometry in this material is very crucial for obtaining a superconducting YBaCuO compound. At the time of sintering, the semiconducting tetragonal YBaCuO compound is formed, which, on slow cooling in oxygen atmosphere, turns into superconducting YBaCuO. The uptake and loss of oxygen are reversible in YBaCuO. A fully oxygenated orthorhombic YBaCuO sample can be transformed into tetragonal YBaCuO by heating in a vacuum at temperature above 700 °C.\n\nThe preparation of Bi-, Tl- and Hg-based high-\"T\" superconductors is difficult compared to YBCO. Problems in these superconductors arise because of the existence of three or more phases having a similar layered structure. Thus, syntactic intergrowth and defects such as stacking faults occur during synthesis and it becomes difficult to isolate a single superconducting phase. For Bi–Sr–Ca–Cu–O, it is relatively simple to prepare the Bi-2212 (\"T\" ≈ 85 K) phase, whereas it is very difficult to prepare a single phase of Bi-2223 (\"T\" ≈ 110 K). The Bi-2212 phase appears only after few hours of sintering at 860–870 °C, but the larger fraction of the Bi-2223 phase is formed after a long reaction time of more than a week at 870 °C. Although the substitution of Pb in the Bi–Sr–Ca–Cu–O compound has been found to promote the growth of the high-\"T\" phase, a long sintering time is still required.\n\n\"High-temperature\" has two common definitions in the context of superconductivity:\n\nThe label high-\"T\" may be reserved by some authors for materials with critical temperature greater than the boiling point of liquid nitrogen (77 K or −196 °C). However, a number of materials – including the original discovery and recently discovered pnictide superconductors – had critical temperatures below 77 K but are commonly referred to in publication as being in the high-\"T\" class.\n\nTechnological applications could benefit from both the higher critical temperature being above the boiling point of liquid nitrogen and also the higher critical magnetic field (and critical current density) at which superconductivity is destroyed. In magnet applications, the high critical magnetic field may prove more valuable than the high \"T\" itself. Some cuprates have an upper critical field of about 100 tesla. However, cuprate materials are brittle ceramics which are expensive to manufacture and not easily turned into wires or other useful shapes. Also, high-temperature superconductors do not form large, continuous superconducting domains, but only clusters of microdomains within which superconductivity occurs. They are therefore unsuitable for applications requiring actual superconducted currents, such as magnets for magnetic resonance spectrometers.\n\nAfter two decades of intense experimental and theoretical research, with over 100,000 published papers on the subject, several common features in the properties of high-temperature superconductors have been identified. , no widely accepted theory explains their properties. Relative to conventional superconductors, such as elemental mercury or lead that are adequately explained by the BCS theory, cuprate superconductors (and other unconventional superconductors) remain distinctive. There also has been much debate as to high-temperature superconductivity coexisting with magnetic ordering in YBCO, iron-based superconductors, several ruthenocuprates and other exotic superconductors, and the search continues for other families of materials. HTS are Type-II superconductors, which allow magnetic fields to penetrate their interior in quantized units of flux, meaning that much higher magnetic fields are required to suppress superconductivity. The layered structure also gives a directional dependence to the magnetic field response.\n\nCuprate superconductors are generally considered to be quasi-two-dimensional materials with their superconducting properties determined by electrons moving within weakly coupled copper-oxide (CuO) layers. Neighbouring layers containing ions such as lanthanum, barium, strontium, or other atoms act to stabilize the structure and dope electrons or holes onto the copper-oxide layers. The undoped \"parent\" or \"mother\" compounds are Mott insulators with long-range antiferromagnetic order at low enough temperature. Single band models are generally considered to be sufficient to describe the electronic properties.\n\nThe cuprate superconductors adopt a perovskite structure. The copper-oxide planes are checkerboard lattices with squares of O ions with a Cu ion at the centre of each square. The unit cell is rotated by 45° from these squares. Chemical formulae of superconducting materials generally contain fractional numbers to describe the doping required for superconductivity. There are several families of cuprate superconductors and they can be categorized by the elements they contain and the number of adjacent copper-oxide layers in each superconducting block. For example, YBCO and BSCCO can alternatively be referred to as Y123 and Bi2201/Bi2212/Bi2223 depending on the number of layers in each superconducting block (\"n\"). The superconducting transition temperature has been found to peak at an optimal doping value (\"p\"=0.16) and an optimal number of layers in each superconducting block, typically \"n\"=3.\n\nPossible mechanisms for superconductivity in the cuprates are still the subject of considerable debate and further research. Certain aspects common to all materials have been identified. Similarities between the antiferromagnetic low-temperature state of the undoped materials and the superconducting state that emerges upon doping, primarily the \"d\" orbital state of the Cu ions, suggest that electron-electron interactions are more significant than electron-phonon interactions in cuprates – making the superconductivity unconventional. Recent work on the Fermi surface has shown that nesting occurs at four points in the antiferromagnetic Brillouin zone where spin waves exist and that the superconducting energy gap is larger at these points. The weak isotope effects observed for most cuprates contrast with conventional superconductors that are well described by BCS theory.\n\nSimilarities and differences in the properties of hole-doped and electron doped cuprates:\n\nThe electronic structure of superconducting cuprates is highly anisotropic (see the crystal structure of YBCO or BSCCO). Therefore, the Fermi surface of HTSC is very close to the Fermi surface of the doped CuO plane (or multi-planes, in case of multi-layer cuprates) and can be presented on the 2D reciprocal space (or momentum space) of the CuO lattice. The typical Fermi surface within the first CuO Brillouin zone is sketched in Fig. 1 (left). It can be derived from the band structure calculations or measured by angle resolved photoemission spectroscopy (ARPES). Fig. 1 (right) shows the Fermi surface of BSCCO measured by ARPES. In a wide range of charge carrier concentration (doping level), in which the hole-doped HTSC are superconducting, the Fermi surface is hole-like (\"i.e.\" open, as shown in Fig. 1). This results in an inherent in-plane anisotropy of the electronic properties of HTSC.\n\nIron-based superconductors contain layers of iron and a pnictogen—such as arsenic or phosphorus—or a chalcogen. This is currently the family with the second highest critical temperature, behind the cuprates. Interest in their superconducting properties began in 2006 with the discovery of superconductivity in LaFePO at 4 K and gained much greater attention in 2008 after the analogous material LaFeAs(O,F) was found to superconduct at up to 43 K under pressure.\nThe highest critical temperatures in the iron-based superconductor family exist in thin films of FeSe, where a critical temperature in excess of 100 K was reported in 2014.\n\nSince the original discoveries several families of iron-based superconductors have emerged:\n\nMost undoped iron-based superconductors show a tetragonal-orthorhombic structural phase transition followed at lower temperature by magnetic ordering, similar to the cuprate superconductors. However, they are poor metals rather than Mott insulators and have five bands at the Fermi surface rather than one. \nThe phase diagram emerging as the iron-arsenide layers are doped is remarkably similar, with the superconducting phase close to or overlapping the magnetic phase. Strong evidence that the \"T\" value varies with the As-Fe-As bond angles has already emerged and shows that the optimal \"T\" value is obtained with undistorted FeAs tetrahedra. The symmetry of the pairing wavefunction is still widely debated, but an extended \"s\"-wave scenario is currently favoured.\n\nAt pressures above 90 GPa (Gigapascals), hydrogen sulfide becomes a metallic conductor of electricity. When cooled below a critical temperature this high-pressure phase exhibits superconductivity. The critical temperature increases with pressure, ranging from 23 K at 100 GPa to 150 K at 200 GPa. If hydrogen sulfide is pressurized at higher temperatures, then cooled, the critical temperature reaches , the highest accepted superconducting critical temperature as of 2015. It has been predicted that by substituting a small part of sulfur with phosphorus and using even higher pressures it may be possible to raise the critical temperature to above and achieve room-temperature superconductivity.\n\nMagnesium diboride is occasionally referred to as a high-temperature superconductor because its \"T\" value of 39 K is above that historically expected for BCS superconductors. However, it is more generally regarded as the highest-\"T\" conventional superconductor, the increased \"T\" resulting from two separate bands being present at the Fermi level.\n\nFulleride superconductors where alkali-metal atoms are intercalated into C molecules show superconductivity at temperatures of up to 38 K for CsC.\n\nSome organic superconductors and heavy fermion compounds are considered to be high-temperature superconductors because of their high \"T\" values relative to their Fermi energy, despite the \"T\" values being lower than for many conventional superconductors. This description may relate better to common aspects of the superconducting mechanism than the superconducting properties.\n\nTheoretical work by Neil Ashcroft in 1968 predicted that solid metallic hydrogen at extremely high pressure should become superconducting at approximately room-temperature because of its extremely high speed of sound and expected strong coupling between the conduction electrons and the lattice vibrations. this prediction is yet to be experimentally verified.\n\nAll known high-\"T\" superconductors are Type-II superconductors. In contrast to Type-I superconductors, which expel all magnetic fields due to the Meissner effect, Type-II superconductors allow magnetic fields to penetrate their interior in quantized units of flux, creating \"holes\" or \"tubes\" of normal metallic regions in the superconducting bulk called vortices. Consequently, high-\"T\" superconductors can sustain much higher magnetic fields.\n\nThe question of how superconductivity arises in high-temperature superconductors is one of the major unsolved problems of theoretical condensed matter physics. The mechanism that causes the electrons in these crystals to form pairs is not known. Despite intensive research and many promising leads, an explanation has so far eluded scientists. One reason for this is that the materials in question are generally very complex, multi-layered crystals (for example, BSCCO), making theoretical modelling difficult. \n\nImproving the quality and variety of samples also gives rise to considerable research, both with the aim of improved characterisation of the physical properties of existing compounds, and synthesizing new materials, often with the hope of increasing \"T\". Technological research focuses on making HTS materials in sufficient quantities to make their use economically viable and optimizing their properties in relation to applications.\n\nThere have been two representative theories for high-temperature or unconventional superconductivity. Firstly, weak coupling theory suggests superconductivity emerges from antiferromagnetic spin fluctuations in a doped system. According to this theory, the pairing wave function of the cuprate HTS should have a \"d\" symmetry. Thus, determining whether the pairing wave function has \"d\"-wave symmetry is essential to test the spin fluctuation mechanism. That is, if the HTS order parameter (pairing wave function) does not have \"d\"-wave symmetry, then a pairing mechanism related to spin fluctuations can be ruled out. (Similar arguments can be made for iron-based superconductors but the different material properties allow a different pairing symmetry.) Secondly, there was the interlayer coupling model, according to which a layered structure consisting of BCS-type (\"s\"-wave symmetry) superconductors can enhance the superconductivity by itself. By introducing an additional tunnelling interaction between each layer, this model successfully explained the anisotropic symmetry of the order parameter as well as the emergence of the HTS. Thus, in order to solve this unsettled problem, there have been numerous experiments such as photoemission spectroscopy, NMR, specific heat measurements, etc. Up to date the results were ambiguous, some reports supported the \"d\" symmetry for the HTS whereas others supported the \"s\" symmetry. This muddy situation possibly originated from the indirect nature of the experimental evidence, as well as experimental issues such as sample quality, impurity scattering, twinning, etc.\n\nThis summary makes an implicit assumption: superconductive properties can be treated by mean field theory. It also fails to mention that in addition to the superconductive gap, there is a second gap, the pseudogap. The cuprate layers are insulating, and the superconductors are doped with interlayer impurities to make them metallic. The superconductive transition temperature can be maximized by varying the dopant concentration. The simplest example is La(2)CuO(4), which consist of alternating CuO(2) and LaO layers which are insulating when pure. When 8% of the La is replaced by Sr, the latter act as dopants, contributing holes to the CuO(2) layers, and making the sample metallic. The Sr impurities also act as electronic bridges, enabling interlayer coupling. With this realistic picture of the electronic and atomic structure of high temperature superconductors, one can show from thousands of experiments that the basic pairing interaction is still interaction with phonons, just as in the old metallic superconductors with Cooper pairs. While the undoped materials are antiferromagnetic, even a few % impurity dopants introduce a smaller pseudogap in the CuO2 planes which is also caused by phonons (technically charge density waves). This gap decreases with increasing charge carriers, and as it nears the superconductive gap, the latter reaches its maximum. This picture has been extended to answer the most obvious question of high temperature superconductivity, why are the transition temperatures so high? The carriers follow zig-zag percolative paths, largely in metallic domains in the CuO(2) planes, until blocked by charge density wave domain walls, where they use dopant bridges to cross over to a metallic domain of an adjacent CuO(2) plane. This model of self-organized networks of percolative paths describes all known maximum transition temperatures with no adjustable parameters. The transition temperature maxima are reached when the host lattice has weak bond-bending forces, which produce strong electron-phonon interactions at the interlayer dopants.\n\nAn experiment based on flux quantization of a three-grain ring of YBaCuO (YBCO) was proposed to test the symmetry of the order parameter in the HTS. The symmetry of the order parameter could best be probed at the junction interface as the Cooper pairs tunnel across a Josephson junction or weak link. It was expected that a half-integer flux, that is, a spontaneous magnetization could only occur for a junction of \"d\" symmetry superconductors. But, even if the junction experiment is the strongest method to determine the symmetry of the HTS order parameter, the results have been ambiguous. J. R. Kirtley and C. C. Tsuei thought that the ambiguous results came from the defects inside the HTS, so that they designed an experiment where both clean limit (no defects) and dirty limit (maximal defects) were considered simultaneously. In the experiment, the spontaneous magnetization was clearly observed in YBCO, which supported the \"d\" symmetry of the order parameter in YBCO. But, since YBCO is orthorhombic, it might inherently have an admixture of \"s\" symmetry. So, by tuning their technique further, they found that there was an admixture of \"s\" symmetry in YBCO within about 3%. Also, they found that there was a pure \"d\" order parameter symmetry in the tetragonal TlBaCuO.\n\nDespite all these years, the mechanism of high-\"T\" superconductivity is still highly controversial, mostly due to the lack of exact theoretical computations on such strongly interacting electron systems. However, most rigorous theoretical calculations, including phenomenological and diagrammatic approaches, converge on magnetic fluctuations as the pairing mechanism for these systems. The qualitative explanation is as follows:\n\nIn a superconductor, the flow of electrons cannot be resolved into individual electrons, but instead consists of many pairs of bound electrons, called Cooper pairs. In conventional superconductors, these pairs are formed when an electron moving through the material distorts the surrounding crystal lattice, which in turn attracts another electron and forms a bound pair. This is sometimes called the \"water bed\" effect. Each Cooper pair requires a certain minimum energy to be displaced, and if the thermal fluctuations in the crystal lattice are smaller than this energy the pair can flow without dissipating energy. This ability of the electrons to flow without resistance leads to superconductivity.\n\nIn a high-\"T\" superconductor, the mechanism is extremely similar to a conventional superconductor, except, in this case, phonons virtually play no role and their role is replaced by spin-density waves. Just as all known conventional superconductors are strong phonon systems, all known high-\"T\" superconductors are strong spin-density wave systems, within close vicinity of a magnetic transition to, for example, an antiferromagnet. When an electron moves in a high-\"T\" superconductor, its spin creates a spin-density wave around it. This spin-density wave in turn causes a nearby electron to fall into the spin depression created by the first electron (water-bed effect again). Hence, again, a Cooper pair is formed. When the system temperature is lowered, more spin density waves and Cooper pairs are created, eventually leading to superconductivity. Note that in high-\"T\" systems, as these systems are magnetic systems due to the Coulomb interaction, there is a strong Coulomb repulsion between electrons. This Coulomb repulsion prevents pairing of the Cooper pairs on the same lattice site. The pairing of the electrons occur at near-neighbor lattice sites as a result. This is the so-called \"d\"-wave pairing, where the pairing state has a node (zero) at the origin.\n\nThis summary assumes superconductive properties can be treated by mean field theory. It also fails to mention that in addition to the superconductive gap, there is a second gap, the pseudogap. The cuprate layers are insulating, and the superconductors are doped with interlayer impurities to make them metallic. The superconductive transition temperature can be maximized by varying the dopant concentration. The simplest example is La(2)CuO(4), which consist of alternating CuO(2) and LaO layers which are insulating when pure. When 8% of the La is replaced by Sr, the latter act as dopants, contributing holes to the CuO(2) layers, and making the sample metallic. The Sr impurities also act as electronic bridges, enabling interlayer coupling. With this realistic picture of the electronic and atomic structure of high temperature superconductors, one can show from thousands of experiments that the basic pairing interaction is still interaction with phonons, just as in the old metallic superconductors with Cooper pairs. While the undoped materials are antiferromagnetic, even a few % impurity dopants introduce a smaller pseudogap in the CuO2 planes which is also caused by phonons (technically charge density waves). This gap decreases with increasing charge carriers, and as it nears the superconductive gap, the latter reaches its maximum. This picture has been extended to answer the most obvious question of high temperature superconductivity, why are the transition temperatures so high? The carriers follow zig-zag percolative paths, largely in metallic domains in the CuO(2) planes, until blocked by charge density wave domain walls, where they use dopant bridges to cross over to a metallic domain of an adjacent CuO(2) plane. This model describes all known maximum transition temperatures with no adjustable parameters. The maxima are reached when the host lattice has weak bond-bending forces.\n\nExamples of high-\"T\" cuprate superconductors include LaBaCuO, and YBCO (yttrium-barium-copper oxide), which is famous as the first material discovered to achieve superconductivity above the boiling point of liquid nitrogen.\n\n\n\n"}
{"id": "15983150", "url": "https://en.wikipedia.org/wiki?curid=15983150", "title": "Himalayan salt", "text": "Himalayan salt\n\nHimalayan salt is rock salt or halite from the Punjab region of Pakistan. Numerous health claims have been made concerning himalayan salt, but there is no scientific evidence that it is healthier than common table salt; such claims are considered pseudoscience.\n\nAlthough its salt is sometimes marketed as \"Jurassic Sea Salt\", this salt deposit comes from a seabed of the Permian and Cretaceous eras 100 to 200 million years ago. The first records of mining are from the Janjua people in the 1200s. Himalayan salt is mostly mined at the Khewra Salt Mine in Khewra, Jhelum District, Punjab, which is situated in the foothills of the Salt Range hill system in the Punjab province of the Pakistan Indo-Gangetic Plain.\n\nHimalayan salt is chemically similar to table salt. Some salts mined in the Himalayas are not suitable for use as food or industrial use without purification due to impurities. Some salt crystals from this region have an off-white to transparent color, while impurities in some veins of salt give it a pink, reddish, or beet-red color.\n\nHimalayan salt is used to flavor food. There is no evidence that it is healthier than common table salt. In the United States, where the salts are manufactured as dietary supplement capsules bearing false claims of health benefits, the Food and Drug Administration warned one manufacturer about inadequate manufacturing practices and illegal advertising. \n\nBlocks of salt are also used as serving dishes, baking stones, and griddles. Himalayan salt is also manufactured into trendy glowing salt lamps, which are hollowed then lit with electric lighting. Numerous health claims have been made concerning salt lamps, but no scientific evidence supports these claims.\n\n"}
{"id": "5685431", "url": "https://en.wikipedia.org/wiki?curid=5685431", "title": "Ichnotaxon", "text": "Ichnotaxon\n\nAn ichnotaxon (plural ichnotaxa) is defined by the International Code of Zoological Nomenclature as \"a taxon based on the fossilized work of an organism\", that is, the non-human equivalent of an artifact.\n\nIchnotaxa are names used to identify and distinguish morphologically distinctive ichnofossils, more commonly known as trace fossils. They are assigned genus and species ranks by ichnologists, much like organisms in Linnaean taxonomy. These are known as ichnogenera and ichnospecies, respectively. \"Ichnogenus\" and \"ichnospecies\" are commonly abbreviated as \"igen.\" and \"isp.\". The binomial names of ichnospecies and their genera are to be written in italics.\n\nMost researchers classify trace fossils only as far as the ichnogenus rank, based upon trace fossils that resemble each other in morphology but have subtle differences. Some authors have constructed detailed hierarchies up to ichnosuperclass, recognizing such fine detail as to identify ichnosuperorder and ichnoinfraclass, but such attempts are controversial.\n\n\"Ichnotaxa\" comes from the Greek ίχνος, \"ichnos\" meaning \"track\" and ταξις, \"taxis\" meaning \"ordering\".\n\nDue to the chaotic nature of trace fossil classification, several ichnogenera hold names normally affiliated with animal body fossils or plant fossils. For example, many ichnogenera are named with the suffix \"-phycus\" due to misidentification as algae.\n\nEdward Hitchcock was the first to use the now common \"-ichnus\" suffix in 1858, with \"Cochlichnus\".\n\nDue to trace fossils' history of being difficult to classify, there have been several attempts to enforce consistency in the naming of ichnotaxa.\n\nIn 1961, the International Commission on Zoological Nomenclature ruled that most trace fossil taxa named after 1930 would be no longer available.\n\n\n"}
{"id": "1330313", "url": "https://en.wikipedia.org/wiki?curid=1330313", "title": "Information broker", "text": "Information broker\n\nAn information broker or data broker collects information about individuals from public records and private sources including census and change of address records, motor vehicle and driving records, user-contributed material to social networking sites, media and court reports, voter registration lists, consumer purchase histories, most-wanted lists and terrorist watch lists, bank card transaction records, health care authorities, and web browsing histories. \n\nThe data are aggregated to create individual profiles, often made up of thousands of individual pieces of information such as a person's age, race, gender, height, weight, marital status, religious affiliation, political affiliation, occupation, household income, net worth, home ownership status, investment habits, product preferences and health-related interests. Brokers then sell the profiles to other organizations that use them mainly to target advertising and marketing towards specific groups, to verify a person's identity including for purposes of fraud detection, and to sell to individuals and organizations so they can research people for various reasons. Data brokers also often sell the profiles to government agencies, such as the FBI, thus allowing law enforcement agencies to circumvent laws that protect privacy.\n\nBeginning in the late twentieth century, technological developments such as the development of the internet, increasing computer processing power and declining costs of data storage made it much easier for companies to collect, analyze, store and transfer large amounts of data about individual people. This gave rise to the information broker or data broker industry.\n\nIndividuals generally cannot find out what data a broker holds on them, how a broker got it, or how it is used. Some data brokers retain all information indefinitely.\n\nFiles on individuals are generally sold in lists; examples cited in testimony to the U.S. Congress include lists of rape victims, seniors with dementia, financially vulnerable people, people with HIV, and police officers (by home address). Less controversial are lists of rich people, doctors, or parents.\n\nThere are probably between 3500 and 4000 data broker companies, and about a third may provide opt-outs, with some charging over a thousand dollars for them.\n\nData brokers collect information concerning myriad topics, ranging from the daily communications of an individual to more specialized data such as product registrations.\n\nData brokers in the United States include Acxiom, Experian, Epsilon, CoreLogic, Datalogix, inome, PeekYou, Exactis, and Recorded Future. Acxiom claims to have files on 10% of the world's population, with about 1500 pieces of information per consumer (quoted in Senate.gov). In 2017, Cambridge Analytica claimed that it has psychological profiles of 220 million US citizens based on 5,000 separate data sets (another 2017 claim is 230 million Americans).\n\nCredit scores were first used in the 1950s, but did not become widely known or specifically regulated until the 1990s.\n\nIn 1977 Kelly Warnken published the first fee-based information directory, which continues to be published and has expanded to cover international concerns.\n\nA U.S. Senate Committee in 2013 published \"A Review of the Data Broker Industry: Collection, Use, and Sale of Consumer Data for Marketing Purposes\". It states that \"Today, a wide range of companies known as “data brokers” collect and maintain data on hundreds of millions of consumers, which they analyze, package, and sell generally without consumer permission or input.\" Their main findings were that:\n\nThe information produced by data brokers has been criticized for enabling discrimination in pricing, services and opportunities. For example, a May 2014 White House report found that web searches that included black-seeming first names such as Jermaine were more likely to result in ads being displayed that include the word \"arrest,\" compared with web searches including white-seeming first names such as Geoffrey.\n\nAn Online Information Broker FAQ is published by Privacy Rights Clearinghouse (PRC), a nonprofit consumer organization in the United States. PRC also maintains a list of information brokers (data brokers, with links to their privacy policies, terms of service, and opt-out privisions.\n\nData brokers have also faced legal charges for security breaches due to poor data security practices.\n\nA 2007 University of California study, after requesting and analyzing information-sharing practices at 86 companies, found many operating under an opt-out model that it described as inconsistent with consumer expectations, and recommended that the California state legislature require companies to disclose their information-sharing policies using clear, unambiguous language, and consider creating a centralized, user-friendly method for consumers to opt out of information-sharing.\n\nIn 2009, the U.S. Federal Trade Commission had recommended the U.S. Congress develop legislation enabling consumers to see the information that data brokers hold about them, a recommendation it renewed in subsequent reports in 2012 and 2014. In 2013, the U.S. Government Accountability Office also called for Congress to consider legislation.\n\nThe Data Accountability and Trust Act contained a number of requirements for auditing and verification of accuracy of data held by information brokers, and additional measures in the case of a security breach. The bill also gave identified individuals the means and opportunity to review and correct the data held that related to them. It passed through the United States House of Representatives in the 111th Congress, but failed to pass the United States Senate. It was revived by the 112th Congress in 2011 as H.R. 1707., but died after being referred to committee. The bill was first introduced by Rep. Bobby Rush [D-IL1] on Apr 30, 2009, H.R. 2221\n\nIn fiction, information brokers usually find data for a story's main character(s). Fictional information brokers can be of varying importance and have varying methods. For example, a hacker can be an information broker, though they may be simply transferring whatever information they find to the main character(s). Other brokers may have memorized data and tell the main character(s) covertly. Also, a fee is not always involved. The information broker may have an alliance with the main character(s) or be one themselves.\n\nExamples of information brokers in contemporary fiction would be Edward G. Robinson's character Sol in the film \"Soylent Green\"; the Shadow Broker in the video game series \"Mass Effect\"; Nicholas Wayne, Rachel, Elean Duga, Gustav St. Germain, Carol, and the President of the Daily Days newspaper company in \"Baccano!\"; or Izaya Orihara in the light novel series \"Durarara!!\". A few of the characters in Neil Stephenson's novel Snow Crash find work selling data as \"stringers\" for the Central Intelligence Corporation. Information broker characters play a prominent role in stories published by DC Comics. The character trope is best exemplified by the superhero Oracle, but the trope is later used with the characters Calculator, Proxy, Chloe Sullivan, and Felicity Smoak as well.\n\n\n\n"}
{"id": "19325934", "url": "https://en.wikipedia.org/wiki?curid=19325934", "title": "Inyoite", "text": "Inyoite\n\nInyoite, named after Inyo County, California, where it was discovered in 1914, is a colourless monoclinic mineral. It turns white on dehydration. Its chemical formula is Ca(HBO)(OH)·4HO or CaBO(OH)·4HO.\n"}
{"id": "42303779", "url": "https://en.wikipedia.org/wiki?curid=42303779", "title": "Kuala Lumpur Bar", "text": "Kuala Lumpur Bar\n\nThe Kuala Lumpur Bar was established on 1 July 1992 at a general meeting of advocates & solicitors practising in the Federal Territory of Kuala Lumpur held under section 68(4) of the Legal Profession Act 1976.\n\nBefore that date, practitioners in Kuala Lumpur were members of the Selangor Bar, which was later called the Selangor & Federal Territory Bar on Kuala Lumpur becoming a Federal Territory in 1974.\n\nThe Kuala Lumpur Bar is led by a Committee comprising the Chairman of the KL Bar and ten other members who are elected annually at the Annual General Meeting. The Committee is also empowered to co-opt two additional members into the Committee. The co-opted members can participate in the deliberations of the Committee but have no vote. The Chairman is an ex-officio member of the Bar Council and a representative to the Bar Council is elected by members of the Kuala Lumpur Bar at their Annual General Meeting. The Honorary Secretary is appointed by the Committee from amongst the members of the Kuala Lumpur Bar but by convention will be from the elected Committee. The Kuala Lumpur Bar Committee (KLBC) currently has twelve sub-committees to undertake a host of projects and activities.\n\n\n"}
{"id": "18334140", "url": "https://en.wikipedia.org/wiki?curid=18334140", "title": "List of Alaska state symbols", "text": "List of Alaska state symbols\n\nThe following is a list of symbols of the U.S. state of Alaska.\n\n"}
{"id": "44584378", "url": "https://en.wikipedia.org/wiki?curid=44584378", "title": "List of Australian archaeologists", "text": "List of Australian archaeologists\n\nThe following is a list of notable Australian archaeologists well-known individuals with a large body of published work or notable research.\n\n\"Return to top of page\"\n\n\n"}
{"id": "7873607", "url": "https://en.wikipedia.org/wiki?curid=7873607", "title": "List of Foucault pendulums", "text": "List of Foucault pendulums\n\nThis is a list of Foucault pendulums in the world:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetic by state or province, then place.\n\n\n\n\n"}
{"id": "22515676", "url": "https://en.wikipedia.org/wiki?curid=22515676", "title": "List of fossil primates", "text": "List of fossil primates\n\nThis is a list of fossil primates—extinct primates for which a fossil record exists. Primates are generally thought to have evolved from a small, unspecialized mammal, which probably fed on insects and fruits. However, the precise source of the primates remains controversial and even their arboreal origin has recently been questioned. As it has been suggested, many other mammal orders are arboreal too, but they have not developed the same characteristics as primates. Nowadays, some well known genera, such as \"Purgatorius\" and \"Plesiadapis\", thought to be the most ancient primates for a long time, are not usually considered as such by recent authors, who tend to include them in the new order Plesiadapiformes, within superorder Euarchontoglires. Some, to avoid confusions, employ the unranked term Euprimates, which excludes Plesiadapiformes. That denomination is not used here.\n\nThere is an academic debate on the time the first primates appeared. One of the earliest probable primate fossils is the problematic \"Altiatlasius koulchii\", perhaps an Omomyid, but perhaps a non-Primate Plesiadapiform, which lived in Morocco, during the Paleocene, around 60 Ma. However, other studies, including molecular clock studies, have estimated the origin of the primate branch to have been in the mid-Cretaceous period, around 85 Ma, that is to say, in the time previous to the extinction of dinosaurs and the successful mammal radiation. Nevertheless, there seems to be a consensus about the monophyletic origin of the order, although the evidence is not clear. There are no fossils known that can be directly linked to the living African apes, nor any that could be considered representative of the last common ancestor between them and humans.\n\nThe order Primates, established by Linnaeus in 1758, includes humans and their immediate ancestors. However, contrarily to the common opinion, most primates do not have especially large brains. Brain size is a derived character, which only appeared with genus \"Homo\", and was lacking in the first hominid. In fact, hominid encephalization quotient is only 1.5 Ma more recent than that of some dolphin species. The encephalization quotient of some cetaceans is therefore higher than that of most primates, including the nearest relatives of humans, such as \"Australopithecus\".\n\nThis list follows partly from Walter Carl Hartwig's 2002 book \"The Fossil Primate Record\" and John G. Fleagle's 2013 book \"Primate Adaptation and Evolution\" (3rd edition). Parentheses around authors' names (and dates) indicates a change in generic name for the fossil, as stated in the International Code of Zoological Nomenclature (ICZN). Since the publication of the book as well as the creation of this article, new fossil taxon have been discovered that has helped improved the taxonomy among primates in general.\n\n\nSubfossil lemurs:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following is a list of books that provide useful reviews or overviews of primate fossil histories, including (e.g.) diagrams, photos and good referencing.\n\n\n"}
{"id": "20902763", "url": "https://en.wikipedia.org/wiki?curid=20902763", "title": "List of institutions using the term \"institute of technology\" or \"polytechnic\"", "text": "List of institutions using the term \"institute of technology\" or \"polytechnic\"\n\nThis is a list of institutions using the term institute of technology or polytechnic. \"Institute of technology\" is a designation employed for a wide range of learning institutions awarding different types of degrees and operating often at variable levels of the educational system. The English term \"polytechnic\" appeared in the early 19th century, from the French École Polytechnique, an engineering school founded in 1794 in Paris. The French term comes from the Greek πολύ (\"polú\" or \"polý\") meaning \"many\" and τεχνικός (\"tekhnikós\") meaning \"arts\". While the terms \"institute of technology\" and \"polytechnic\" are synonymous, the preferred term varies from country to country.\n\nThere are many university level higher learning institutions granting the highest academic degrees (including doctorate), that use the terms \"institute of technology\" or \"polytechnic\" for historic reasons:\n\nThere are many other types of higher education institutions (post-secondary education) which are not universities and use the terms \"institute of technology\" or \"polytechnic\":\n\n\nThere are also secondary education schools which use these words:\n\n\nPolytechnics in Singapore provide industry oriented education equivalent to a junior college or sixth form college.\n\n\n"}
{"id": "7119996", "url": "https://en.wikipedia.org/wiki?curid=7119996", "title": "List of volcanoes in Dominica", "text": "List of volcanoes in Dominica\n\nDominica is an island-nation in the Caribbean that is part of the Lesser Antilles chain of islands. The island has active and extinct volcanoes.\n"}
{"id": "17469697", "url": "https://en.wikipedia.org/wiki?curid=17469697", "title": "List of welding codes", "text": "List of welding codes\n\nThis page lists published welding codes, procedures, and specifications.\n\nThe American Society of Mechanical Engineers (ASME) Boiler and Pressure Vessel Code (BPVC) covers all aspects of design and manufacture of boilers and pressure vessels. All sections contain welding specifications, however most relevant information is contained in the following:\nThe American Welding Society (AWS) publishes over 241 AWS-developed codes, recommended practices and guides which are written in accordance with American National Standards Institute (ANSI) practices. The following is a partial list of the more common publications: \nThe American Petroleum Institute (API) oldest and most successful programs is in the development of API standards which started with its first standard in 1924. API maintains over 500 standards covering the oil and gas field. The following is a partial list specific to welding:\n\nStandards Australia is the body responsible for the development, maintenance and publication of Australian Standards. The following is a partial list specific to welding:\nThe [[Canadian Staor the development, maintenance and publication of CSA standards. The following is a partial list specific to welding:\n\n[[British Standards]] are developed, maintained and published by BSI Standards which is UK's National Standards Body. The following is a partial list of standards specific to welding:\n[[International Organization for Standardization]] (ISO) has developed over 18500 standards and over 1100 new standards are published every year. The following is a partial list of the standards specific to welding:\nThe [[European Committee for Standardization]] (CEN) had issued numerous standards covering welding processes, which unified and replaced former national standards. Of the former national standards, those issued by [[British Standards|BSI]] and [[DIN]] were widely used outside their countries of origin. After the Vienna Agreement with ISO, CEN has replaced most of them with equivalent ISO standards (EN ISO series). \nAdditional requirements for welding exist in CEN codes and standards for specific products, like EN 12952, EN 12953, [[EN 13445]], EN 13480, etc.\n\nNA 092 is the Standards Committee for welding and allied processes (NAS) at DIN Deutsches Institut für Normung e. V. The following is a partial list of DIN welding standards:\n\n\n\n[[Category:Lists of standards|Welding codes]]\n[[Category:Welding|Codes]]"}
{"id": "701099", "url": "https://en.wikipedia.org/wiki?curid=701099", "title": "Luttinger liquid", "text": "Luttinger liquid\n\nA Luttinger liquid, or Tomonaga–Luttinger liquid, is a theoretical model describing interacting electrons (or other fermions) in a one-dimensional conductor (e.g. quantum wires such as carbon nanotubes). Such a model is necessary as the commonly used Fermi liquid model breaks down for one dimension.\n\nThe Tomonaga–Luttinger liquid was first proposed by Tomonaga in 1950. The model showed that under certain constraints, second-order interactions between electrons could be modelled as bosonic interactions. In 1963, J.M. Luttinger reformulated the theory in terms of Bloch sound waves and showed that the constraints proposed by Tomonaga were unnecessary in order to treat the second-order perturbations as bosons. But his solution of the model was incorrect; the correct solution was given by and Elliot H. Lieb 1965.\n\nLuttinger liquid theory describes low energy excitations in a 1D electron gas as bosons. Starting with the free electron Hamiltonian:\n\nformula_1\n\nis separated into left and right moving electrons and undergoes linearization with the approximation formula_2 over the range formula_3:\n\nformula_4\n\nExpressions for bosons in terms of fermions are used to represent the Hamiltonian as a product of two boson operators in a Bogoliubov transformation.\n\nThe completed bosonization can then be used to predict spin-charge separation. Electron-electron interactions can be treated to calculate correlation functions.\n\nAmong the hallmark features of a Luttinger liquid are the following:\n\n\nThe Luttinger model is thought to describe the universal low-frequency/long-wavelength behaviour of any one-dimensional system of interacting fermions (that has not undergone a phase transition into some other state).\n\nAmong the physical systems believed to be described by the Luttinger model are:\n\n\nAttempts to demonstrate Luttinger-liquid-like behaviour in those systems are the subject of ongoing experimental research in condensed matter physics.\n\n\n\n"}
{"id": "49400615", "url": "https://en.wikipedia.org/wiki?curid=49400615", "title": "Marco Drago", "text": "Marco Drago\n\nMarco Drago is an Italian post-doctoral researcher who works on the search for gravitational waves of short duration.\n\nIn 2007-2010 Marco Drago was Ph.D. student at University of Padua. In 2010 he defended a Ph.D. thesis titled \"Search for transient gravitational wave signals with a known waveform in the LIGO Virgo network of interferometric detectors using a fully coherent algorithm\" in Padua.\n\nIn 2010-2014 Marco Drago worked as postdoc at University of Trento.\n\nFrom 2014, Marco Drago worked as postdoc at Max Planck Institute for Gravitational Physics in Hannover, Germany.\n\nHe plays classical piano and has published two fantasy novels.\n\n"}
{"id": "14847900", "url": "https://en.wikipedia.org/wiki?curid=14847900", "title": "Multiple time dimensions", "text": "Multiple time dimensions\n\nThe possibility that there might be more than one dimension of time has occasionally been discussed in physics and philosophy.\n\nSpecial relativity describes spacetime as a manifold whose metric tensor has a negative eigenvalue. This corresponds to the existence of a \"time-like\" direction. A metric with multiple negative eigenvalues would correspondingly imply \"several\" timelike directions, i.e. multiple time dimensions, but there is no consensus regarding the relationship of these extra \"times\" to time as conventionally understood.\n\nIf the special theory of relativity can be generalized for the case of \"k\"-dimensional time (\"t\", \"t\", ..., \"t\") and \"n\"-dimensional space (\"x\", \"x\", ..., \"x\"), then the (\"k\" + \"n\")-dimensional interval, being invariant, is given by the expression\n\nThe metric signature will be\n\nor\n\nThe transformations between the two inertial frames of reference \"K\" and \"K\"′, which are in a standard configuration (i.e., transformations without translations and/or rotations of the space axis in the hyperplane of space and/or rotations of the time axis in the hyperplane of time), are given as follows:\n\nwhere\nformula_6\nformula_7\nformula_8\nare the vectors of the velocities of \"K\"′ against \"K\", defined accordingly in relation to the time dimensions \"t\", \"t\", ..., \"t\";\nformula_9\nformula_10\n\"σ\" = 1, 2, ..., \"k\"; \"λ\" = \"k\"+2, \"k\"+3, ..., \"k\"+\"n\". Here \"δ\" is the Kronecker delta. These transformations are generalization of the Lorentz boost in a fixed space direction (\"x\") in the field of the multidimensional time and multidimensional space.\nLet us denote\nformula_11 and\nformula_12\nwhere \"σ\" = 1, 2, ..., \"k\"; \"η\" = \"k\"+1, \"k\"+2, ..., \"k\"+\"n\". The velocity-addition formula is then given by\n\nwhere \"σ\" = 1, 2, ..., \"k\"; \"λ\" = \"k\"+2, \"k\"+3, ..., \"k\"+\"n\".\n\nFor simplicity, let us consider only one spatial dimension \"x\" and the two time dimensions \"x\" and \"x\". (E. g., \"x\" = \"ct\", \"x\" = \"ct\", \"x\" = \"x\".) Let us assume that in point \"O\", having coordinates \"x\" = 0, \"x\" = 0, \"x\" = 0, there has been an event \"E\". Let us further assume that a given interval of time formula_15 has passed since the event \"E\". The causal region connected to the event \"E\" includes the lateral surface of the right circular cone {(\"x\") + (\"x\") − (\"x\") = 0}, the lateral surface of the right circular cylinder {(\"x\") + (\"x\") = \"c\"Δ\"T\"} and the inner region bounded by these surfaces, i.e., the causal region includes all points (\"x\", \"x\", \"x\"), for which the conditions\n\nare fulfilled.\n\nTheories with more than one dimension of time have sometimes been advanced in physics, whether as a serious description of reality or just as a curious possibility. Itzhak Bars's work on \"two-time physics\", inspired by the SO(10,2) symmetry of the extended supersymmetry structure of M-theory, is the most recent and systematic development of the concept (see also F-theory). Walter Craig and Steven Weinstein proved the existence of a well-posed initial value problem for the ultrahyperbolic equation (a wave equation in more than one time dimension). This showed that initial data on a mixed (spacelike and timelike) hypersurface obeying a particular nonlocal constraint evolves deterministically in the remaining time dimension.\n\nLet us regard the motion of a test particle with coordinate\n\nformula_16\n\nwhich is the canonical (1,3) spacetime vector formula_17 with formula_18 extended by an additional timelike coordinate formula_19. formula_20 is the second time parameter, formula_21 describes the size of the second time dimension and formula_22 is the characteristic velocity, thus the equivalent of formula_23. formula_24 describes the shape of the second time dimension and formula_25 is a normalization parameter such that formula_26 is dimensionless. Decomposing formula_27 with \n\nformula_28\n\nand using the metric formula_29, the Lagrangian becomes\n\nformula_30\n\nApplying the Euler-Lagrange Equations\n\nformula_31\n\nthe existence of the Planck length and the constancy of the speed of light can be derived.\n\nAs a consequence of this model it has been suggested that the speed of light may not have been constant in the early universe.\n\nConceptual difficulties with multiple physical time dimensions have been raised in modern analytic philosophy.\n\nAs a solution to the problem of the subjective passage of time, J. W. Dunne proposed an infinite hierarchy of time dimensions, inhabited by a similar hierarchy of levels of consciousness. Dunne suggested that, in the context of a \"block\" spacetime as modelled by General Relativity, a second dimension of time was needed in order to measure the speed of one's progress along one's own timeline. This in turn required a level of the conscious self existing at the second level of time. But the same arguments then applied to this new level, requiring a third level, and so on in an infinite regress. At the end of the regress was a \"superlative general observer\" who existed in eternity. He published his theory in relation to precognitive dreams in his 1927 book \"An Experiment with Time\" and went on to explore its relevance to contemporary physics in \"The Serial Universe\" (1934). His infinite regress was criticised as logically flawed and unnecessary, although writers such as J. B. Priestley acknowledged the possibility of his second time dimension.\n\n\n"}
{"id": "51494441", "url": "https://en.wikipedia.org/wiki?curid=51494441", "title": "NGC 167", "text": "NGC 167\n\nNGC 167 is a spiral galaxy located approximately 172 million light-years from the Solar System in the constellation Cetus. It was discovered in 1886 by Francis Preserved Leavenworth.\n\n\n"}
{"id": "2066927", "url": "https://en.wikipedia.org/wiki?curid=2066927", "title": "Nesta (charity)", "text": "Nesta (charity)\n\nNesta (formerly NESTA, National Endowment for Science, Technology and the Arts) is an innovation foundation based in the UK.\n\nThe organisation acts through a combination of practical programmes, investment, policy and research, and the formation of partnerships to promote innovation across a broad range of sectors.\n\nNesta was originally funded by a £250 million endowment from the UK National Lottery. The endowment is now kept in trust, and Nesta uses the interest from the trust to meet its charitable objects and to fund and support its projects.\n\nThe charity is registered in England and Wales with charity no. 1144091 and in Scotland with no. SC042833\n\nOld NESTA was set up in 1998 by an independent endowment in the United Kingdom established by an Act of Parliament.\n\nOn 14 October 2010 the Government announced that it would transfer old NESTA's status from an executive non-departmental public body to a new charitable body.\n\nOn 1 April 2012 the old NESTA transitioned from being an executive to a charitable body, changing its name to Nesta, and dropping the long title.\n\nNesta currently operates in the following areas:\n\nNesta's Policy and Research team publish regular research papers on how innovation can boost economic growth.\n\nIn 2012 it published Plan I, an innovation manifesto for the UK, and the latest Innovation Index figures, which showed that there had been a £24bn drop in innovation funding in the last decade.\n\nPreviously it has published \"The Vital 6 Percent\", and Mass Localism.\n\nin 2012 the charity launched a £25 million impact investment fund, \"Nesta Impact Investments\", run by its subsidiary Nesta Investment Management. Other investors are Big Society Capital and the Omidyar Network. The fund invests in social ventures with innovative products or services that address the following three challenges: an ageing population; the employability of young people; and the sustainability of UK communities.\n\nNesta runs practical programmes to find innovative ways of delivering cheaper, more efficient public services, and demonstrating how these can be scaled up across the UK.\n\nPrevious examples of work include the Innovation in Giving fund, in partnership with the Cabinet Office, which seeks to find and support new platforms for the giving of time, skills and money. They also ran an education programme, looking at how children can be taught to become digital makers, and how the education system can benefit from digital technology.\n\nIn the past they ran The Big Green Challenge, a £1 million prize fund to stimulate community action on climate change.\n\nIn 2012 Nesta published a report criticising the use of technology in schools. The report highlighted areas where new technology could be usefully employed. It argued that digital technology was often purchased, costing £450 million per year, without demonstrable evidence it was improving education methods.\n\nNesta also specialises in original research into the UK's creative industries, and runs practical programmes to help the sector.\n\nIn February 2011 Nesta produced Next Gen, in association with Ian Livingstone and Alex Hope, which helped start the debate about teaching UK schoolchildren to learn how to code. The report specifically called for ways to protect the future of the UK's video games and video effects industries by arguing for, among other things, the introduction of computer science teaching in schools.\n\nIt is runs the Digital R&D Fund, in partnership with Arts Council and the Arts and Humanites Research Council, and also runs the Creative Business Mentoring Network, which pairs mentees from creative companies with experienced business leaders.\n\nNesta runs several programs to help governments become more innovative. Its flagship initiative in this field, States of Change, aims to bring together governments to share experiences about how to implement new ideas. It has created several toolkits to try and foster the skills that will let public servants innovate.\n\nSir John Gieve chairs the organisation.\nGeoff Mulgan is CEO.\n\n\n"}
{"id": "1459075", "url": "https://en.wikipedia.org/wiki?curid=1459075", "title": "Parallel tempering", "text": "Parallel tempering\n\nParallel tempering, also known as replica exchange MCMC sampling, is a simulation method aimed at improving the dynamic properties of Monte Carlo method simulations of physical systems, and of Markov chain Monte Carlo (MCMC) sampling methods more generally. The replica exchange method was originally devised by Swendsen and Wang then extended by Geyer and later developed, among others, by Hukushima and Nemoto, Giorgio Parisi,\nSugita and Okamoto formulated a molecular dynamics version of parallel tempering: this is usually known as replica-exchange molecular dynamics or REMD.\n\nEssentially, one runs \"N\" copies of the system, randomly initialized, at different temperatures. Then, based on the Metropolis criterion one exchanges configurations at different temperatures. The idea of this method\nis to make configurations at high temperatures available to the simulations at low temperatures and vice versa.\nThis results in a very robust ensemble which is able to sample both low and high energy configurations.\nIn this way, thermodynamical properties such as the specific heat, which is in general not well computed in the canonical ensemble, can be computed with great precision.\n\nTypically a Monte Carlo simulation using a Metropolis-Hastings update consists of a single stochastic process that evaluates the energy of the system and accepts/rejects updates based on the temperature \"T\". At high temperatures updates that change the energy of the system are comparatively more probable. When the system is highly correlated, updates are rejected and the simulation is said to suffer from critical slowing down.\n\nIf we were to run two simulations at temperatures separated by a Δ\"T\", we would find that if Δ\"T\" is small enough, then the energy histograms obtained by collecting the values of the energies over a set of Monte Carlo steps N will create two distributions that will somewhat overlap. The overlap can be defined by the area of the histograms that falls over the same interval of energy values, normalized by the total number of samples. For Δ\"T\" = 0 the overlap should approach 1.\n\nAnother way to interpret this overlap is to say that system configurations sampled at temperature \"T\" are likely to appear during a simulation at \"T\". Because the Markov chain should have no memory of its past, we can create a new update for the system composed of the two systems at \"T\" and \"T\". At a given Monte Carlo step we can update the global system by swapping the configuration of the two systems, or alternatively trading the two temperatures. The update is accepted according to the Metropolis-Hastings criterion with probability\n\nand otherwise the update is rejected. The detailed balance condition has to be satisfied by ensuring that the reverse update has to be equally likely, all else being equal. This can be ensured by appropriately choosing regular Monte Carlo updates or parallel tempering updates with probabilities that are independent of the configurations of the two systems or of the Monte Carlo step.\n\nThis update can be generalized to more than two systems.\n\nBy a careful choice of temperatures and number of systems one can achieve an improvement in the mixing properties of a set of Monte Carlo simulations that exceeds the extra computational cost of running parallel simulations.\n\nOther considerations to be made: increasing the number of different temperatures can have a detrimental effect, as one can think of the 'lateral' movement of a given system across temperatures as a diffusion process.\nSet up is important as there must be a practical histogram overlap to achieve a reasonable probability of lateral moves.\n\nThe parallel tempering method can be used as a super simulated annealing that does not need restart, since a system at high temperature can feed new local optimizers to a system at low temperature, allowing tunneling between metastable states and improving convergence to a global optimum.\n\n"}
{"id": "3782133", "url": "https://en.wikipedia.org/wiki?curid=3782133", "title": "Personal Genome Project", "text": "Personal Genome Project\n\nThe Personal Genome Project (PGP) is a long term, large cohort study which aims to sequence and publicize the complete genomes and medical records of 100,000 volunteers, in order to enable research into personal genomics and personalized medicine. It was initiated by Harvard University's George M. Church in 2005. As of November 2017, more than 10,000 volunteers had joined the project. Volunteers were accepted initially if they were permanent residents of the US and were able to submit tissue and/or genetic samples. Later the project was expanded to other countries.\n\nThe Project was initially launched in the US in 2005 and later extended to Canada (2012), United Kingdom (2013), Austria (2014), Korea (2015) and China (2017).\nThe project allowed participants to publish the genotype (the full DNA sequence of all 46 chromosomes) of the volunteers, along with extensive information about their phenotype: medical records, various measurements, MRI images, etc. All data were placed within the public domain and made available over the Internet so that researchers could test various hypotheses about the relationships among genotype, environment and phenotype. Participants could decide what data they are comfortable to publish publicly and could choose to upload additional data or remove existing data at their own convenience.\n\nAn important part of the project was the exploration of the resulting risks to the participants, such as possible discrimination by insurers and employers if the genome shows a predisposition for certain diseases.\n\nThe PGP is establishing an international network of sites, including the United States (Harvard PGP), Canada (University of Toronto / Hospital for Sick Kids), and other countries that adhere to certain \"conforming implementation\" criteria such as no promise of anonymity and data return. The Harvard Medical School Institutional Review Board requested that the first set of volunteers include the principal investigator George Church and other diverse stakeholders in the scientific, medical, and social implications of personal genomes, because they were well positioned to give highly informed consent. As sequencing technology becomes cheaper, and the societal issues mentioned above are worked out, it was hoped that a large number of volunteers from all walks of life would participate. The long-term goal was that every person have access to his or her genotype to be used for personalized medical decisions.\n\nThe first ten volunteers were referred to as the \"PGP-10\". These volunteers were:\n\nIn order to enroll, each participant must pass a series of short online tests to ensure that they are providing informed consent. By 2012, 2000 participants had enrolled and by November 2017 10,000 had joined the project.\n\nIn July 2014, at the 'Genetics, Genomics and Global Health—Inequalities, Identities and Insecurities' conference, Stephan Beck, the head of the UK arm of this project indicated that they had over 1000 volunteers, and had temporarily paused collection data due to lack of funding. As of November 2016, the pause was still in effect.\n\nSince 2016, participants of the PGP could choose to obtain their whole-genome sequenced performed for $999.\n\nIn February 2018, the results were published of the first 56 Canadian participants who had their whole genome analyzed. Several DNA mutations that would have been expected by expert consensus to affect health of the participants had not done so, indicating that getting health data from the human genome was difficult.\n\nOn March 9, 2017, producers of the popular online brain-training program Lumosity announced they would collaborate with Harvard researchers to investigate the relationship between genetics and memory, attention, and reaction speed.\n\nScientists at the Wyss Institute for Biologically Inspired Engineering and the Harvard Medical School Personal Genome Project (PGP) planned to recruit 10,000 members from the PGP, to perform a set of cognitive tests from Lumos Labs’ NeuroCognitive Performance Test, a brief, repeatable, online assessment to evaluate participants’ memory functions, including object recall, object pattern memorization, and response times. The researchers would then correlate extremely high performance scores with naturally occurring variations in the participants’ genomes. To validate their findings, the team would sequence, edit, and visualize DNA, model neuronal development in 3-D brain organoids ex vivo, and finally test emerging hypotheses in experimental models of neurodegeneration.\n\n\n"}
{"id": "1664809", "url": "https://en.wikipedia.org/wiki?curid=1664809", "title": "Personnel selection", "text": "Personnel selection\n\nPersonnel selection is the methodical process used to hire (or, less commonly, promote) individuals. Although the term can apply to all aspects of the process (recruitment, selection, hiring, acculturation, etc.) the most common meaning focuses on the selection of workers. In this respect, selected prospects are separated from rejected applicants with the intention of choosing the person who will be the most successful and make the most valuable contributions to the organization. Its effect on the group is discerned when the selected accomplish their desired impact to the group, through achievement or tenure. The procedure of selection takes after strategy to gather data around a person so as to figure out whether that individual ought to be utilized. The strategies used must be in compliance with the various laws in respect to work force selection.\n\nThe professional standards of industrial-organizational psychologists (I-O psychologists) require that any selection system be based on a job analysis to ensure that the selection criteria are job-related. The requirements for a selection system are characteristics known as KSAOs – knowledge, skills, ability, and other characteristics. US law also recognizes \"bona fide occupational qualifications\" (BFOQs), which are requirements for a job which would be considered discriminatory if not necessary – such as only employing men as wardens of maximum-security male prisons, enforcing a mandatory retirement age for airline pilots, or a religious college only employing professors of its religion to teach its theology.\n\nPersonnel selection systems employ evidence-based practices to determine the most qualified candidates and involve both the newly hired and those individuals who can be promoted from within the organization.\n\nIn this respect, selection of personnel has \"validity\" if an unmistakable relationship can be shown between the system itself and the employment for which the people are ultimately being chosen for. In this way, a vital piece of selection is Job Analysis. An analysis is typically conducted before, and regularly apart of, the improvement in determination systems. Then again, a selection method may be deemed valid after it has already been executed by directing follow up job analysis and demonstrating the relationship between the selection process and the respective job.\n\nThe procedure of personnel selection includes gathering data about the potential candidates with the end goal of deciding suitability and sustainability for the employment in that particular job. This data is gathered utilizing one or more determination devices or strategies classified as such: \nDevelopment and implementation of such screening methods is sometimes done by human resources departments; larger organizations hire consultants or firms that specialize in developing personnel selection systems. I-O psychologists must evaluate evidence regarding the extent to which selection tools predict job performance, evidence that bears on the validity of selection tools. These procedures are usually validated (shown to be job relevant), using one or more of the following types of validity: content validity, construct validity, and/or criterion-related validity.\n\nChinese civil servant exams, established in AD 605, may be the first documented \"modern\" selection tests, and have influenced subsequent examination systems.\nAs a scientific and scholarly field, personnel selection owes much to psychometric theory and the art of integrating selection systems falls to human resource professionals.\n\nIn the United States of America, members of the Society for Industrial and Organizational Psychology (SIOP) conduct much of the research on selection. Primary research topics include:\n\n\nThe validity of interviews describes how useful interviews are in predicting job performance. In one of the most comprehensive meta-analytic summary to date by Weisner and Cronshaw (1988). The authors investigated interview validity as a function of interview format (individual vs board) and degree of structure( structure vs unstructured). Results of this study showed that structured interviews yielded much higher mean corrected validities than unstructured interviews (0.63 vs 0.20), and structured board interviews using consensus ratings had the highest corrected validity (0.64).\n\nIn McDaniel, Whetzel, Schmidt & Maurer's Comprehensive Review and Meta- analysis of the Validity of Interviews (1994) paper, the authors go a step further and include an examination of the validity of three different types of interview content(situational, job-related, and psychological).Their goal was to explore the possibility that validity is a function of the type of content collected.\n\nThey define the three kinds of content as follows – situational content was described as interview questions that get information on how the interviewee would behave in specific situations presented by the interviewer. For example, a question that asks whether the interviewee would choose to report a coworker for behaving in an unethical way or just let them go. Job related questions, on the other hand, assess the interviewee's past behavior and job-related information. While psychological interviews include questions intended to assess the interviewee's personality traits such as their work ethic, dependability, honesty etc.\n\nThe authors conducted a meta-analysis of all previous studies on the validity of interviews across the three types of content mentioned above. Their results show that for job-performance criteria, situational interviews yield higher mean validity(0.50) than do job-related interviews(0.39) which yield a higher mean validity than do psychology interviews(0.29). This means that when the interview is used to predict job performance, it is best to conduct situational interviews rather than job-related or psychological interviews. On the other hand, when interviews are used to predict an applicant's training performance, the mean validity of job-related interviews(0.36) is somewhat lower than the mean validity of psychological interviews(0.40).\n\nGoing beyond the content of the interview, the authors' analysis of interview validity was extended to include an assessment of how the interview was conducted. Here, two questions emerged – Are structured interviews more valid than unstructured interviews ? and are board interviews( with more than one interviewer) more valid than individual interviews.\n\nTheir answer to the first question – Are structured interviews more valid unstructured interviews was that structured interviews, regardless of content, is more valid(0.44) than unstructured interviews(0.33) in predicting job performance criteria. However, when training performance is the criteria, the validity of structured and unstructured interviews are similar (0.34 and 0.36).\n\nAs for the validity of board interviews versus individual interviews, the researchers conducted another meta-analyses comparing the validity of board interviews and individual interviews for job performance criteria. The results show that individual interviews are more valid than board interviews( 0.43 vs 0.32). This is true regardless of whether the individual interview is structured or unstructured.\n\nWhen exploring the variance in interview validity between job performance, training performance, and tenure criteria, the researchers found that the interviews are similar in predictive accuracy for job-performance and training performance( 0.37 vs 0.36). But less predictive for tenure (0.20).\n\nBased on meta-analysis results, cognitive ability tests appear to be among the most valid of all psychological tests and are valid for most occupations. However, these tests tend to do better at predicting training criteria than long term job performance. Cognitive ability tests in general provide the benefit of being generalizable. Hence they can be used across organizations and jobs and have been shown to produce large economic gains for companies that use them (Gatewood & Feild, 1998; Heneman et al., 2000).\n\nBut despite the high validity of cognitive testing, it is less frequently used as selection tools. One main reason is that cognitive ability testing has been demonstrated to produce adverse impact. In general, groups including Hispanics and African-Americans score lower than the general population while other groups including Asian – Americans score higher (Heneman et al., 2000; Lubenski, 1995). The legal issues with cognitive ability testing were amplified by the supreme court's ruling in the famous 1971 Griggs v. Duke Power case. In this case, the Supreme Court ruled that when a selection test produces adverse impact against protected group members the company must be able to defend it by showing that use of the test is a \"business necessity\" for the operation of the business. The courts have held narrow interpretations of business necessity that require companies to show that no other acceptable selection alternative exists (Sovereign, 1999). As a result, many companies abandoned cognitive ability testing ( Steven L. Thomas &Wesley A. Scroggins, 2006).\n\nWhile the utility of cognitive ability testing in selection has been broadly accepted, the utility of personality testing, until relatively recently, has not. Historically, research documenting the low predictive validity and the potential for invasion of privacy based on item content has made its application as selection instruments questionable (Hogan, Hogan, & Roberts, 1996).\n\nBut due to the legal challenges associated with cognitive ability, interest in personality instruments has recently been revived (Schmidt, Ones, & Hunter, 1992). Some have suggested that pairing personality testing with cognitive ability testing may be one means to enhance validity while reducing adverse impact (Ryan, Ployhart, & Friedel, 1998). Because it is very likely that some aspects of personality enhance individual ability to apply intellectual capacity while other personality traits limit its application (Kaufman & Lichtenberger, 1999). Hence, adding a personality to an ability test should enhance validity while reducing the adverse impact of a selection system.\n\nRecent research studies prove this assumption to be false, by showing that the addition of a predictor producing smaller group differences (i.e., personality test) to a predictor producing higher group differences (i.e., cognitive ability test) does not reduce the potential for adverse impact to the degree that is often expected (Bobko, Roth, & Potosky, 1999; Schmitt, Rogers, Chan, Sheppard, & Jennings, 1997).\n\nAlthough the use of personality tests with measures of cognitive ability may not have the desired effects on reducing adverse impact, it appears that the addition of personality measures to measures of cognitive ability as a composite predictor results in significant incremental validity (Bobko et al., 1999; Schmitt et al.,1997). These studies found that the validity of predictor composites was highest when alternative predictors were used in combination with cognitive ability. Though this combination of predictors resulted in the highest predictive validity, the inclusion of cognitive ability with these alternative predictors increased the potential for adverse impact (Steven L. Thomas & Wesley A. Scroggins, 2006).\n\nIn summary, cognitive ability testing by itself has been shown to have high levels of validity, but comes with issues relating to adverse impact. Personality tests, on the other hand, have historically been proven to have low validity due to the lack of a common understanding on what constitutes personality, and the non-standardized measures available. But a growing number of research studies show that the best way for organizations to achieve close to optimal validity and job performance prediction, is to create a predictor composite that includes a measure of cognitive ability and an additional measure such as a personality test.\n\nTwo major factors determine the quality of newly hired employees, predictor validity and selection ratio. The predictor cutoff is a test score differentiating those passing a selection measure from those who did not. People above this score are hired or are further considered while those below it are not.\n\nThe selection ratio (SR), on the other hand is the number of job openings \"n\" divided by the number of job applicants \"N\". This value will range between 0 and 1, reflecting the selectivity of the organization's hiring practices. When the SR is equal to 1 or greater, the use of any selection device has little meaning, but this is not often the case as there are usually more applicants than job openings. Finally, the base rate is defined by the percentage of employees thought to be performing their jobs satisfactorily following measurement.\n\nTests designed to determine an individual's aptitude for a particular position, company or industry may be referred to as personnel assessment tools. Such tests can aid those charged with hiring personnel in both selecting individuals for hire and in placing new hires in the appropriate positions. They vary in the measurements they use and level of standardization they employ, though all are subject to error.\n\nPredictors for selection always have less than perfect validity and scatter plots, as well as other forecasting methods such as judgmental bootstrapping, and index models can help us to refine a prediction model as well as identify any mistakes. The criterion cutoff is the point separating successful and unsuccessful performers according to a standard set by the hiring organization. True positives are applied those thought to succeed on the job as a result of having passed the selection test and who have, in fact, performed satisfactorily. True negatives describe those who were correctly rejected based on the measure because they would not be successful employees.\n\nFalse negatives occur when people are rejected as a result of selection test failure, but would have performed well on the job anyway. Finally, false positives are applied to individuals who are selected for having passed the selection measure, but do not make successful employees. These selection errors can be minimized by increasing the validity of the predictor test.\n\nStandards for determination of the cutoff score vary widely, but should be set to be consistent with the expectations of the relevant job. Adjusting the cutoff in either direction will automatically increase the error in the other. Thus, it is important to determine which type of error is more harmful on a case-by-case basis.\n\nBanding is another method for setting cutoff values. Some differences in test scores are ignored as applicants whose scores fall with in the same band (or, range) are selected not on the basis of individual scores, but of another factor spas to reduce adverse impact. The width of the band itself is a function of test reliability, the two being negatively correlated. Banding allows employers to ignore test scores altogether by using random selection, and many have criticized the technique for this reason.\n\nA meta-analysis of selection methods in personnel psychology found that general mental ability was the best overall predictor of job performance and training performance.\n\nRegarding interview procedures, there are data which put into question these tools for selecting employees. While the aim of a job interview is ostensibly to choose a candidate who will perform well in the job role, other methods of selection provide greater predictive power and often entail lower costs. Unstructured interviews are commonly used, but structured interviews tend to yield better outcomes and are considered a better practice.\n\nInterview structure is defined as \"the reduction in procedural variance across applicants, which can translate into the degree of discretion that an interviewer is allowed in conducting the interview.\" Structure in an interview can be compared to a typical paper and pencil test: we would not think it was fair if every test taker were given different questions and a different number of questions on an exam, or if their answers were each graded differently. Yet this is exactly what occurs in an unstructured interview; thus, a structured interview attempts to standardize this popular selection tool.\n\nMultiple studies and meta-analyses have also been conducted to look at the relationship between organizational citizenship behavior (OCB) and organizational performance and success. Job candidates exhibiting higher levels of helping, voice, and loyalty behaviors were generally rated as more confident, received higher salaries, and received higher salary recommendations than job candidates exhibiting these behaviors to a lesser degree. This was found to be true even candidate responses regarding task performance were taken into account. Finally, content analyses of open-ended question responses indicated selection decisions were highly sensitive to candidates with low expression of voice and helping behaviors.\n\n\n"}
{"id": "277301", "url": "https://en.wikipedia.org/wiki?curid=277301", "title": "Principles of Compiler Design", "text": "Principles of Compiler Design\n\nPrinciples of Compiler Design, by Alfred Aho and Jeffrey Ullman, is a classic textbook on compilers for computer programming languages.\n\nIt is often called the \"dragon book\" and its cover depicts a knight and a dragon in battle; the dragon is green, and labeled \"Complexity of Compiler Construction\", while the knight wields a lance and a shield labeled \"LALR parser generator\" and \"Syntax Directed Translation\" respectively, and rides a horse labeled \"Data Flow Analysis\". The book may be called the \"green dragon book\" to distinguish it from its successor, Aho, Sethi & Ullman's \"\", which is the \"red dragon book\". The second edition of \"Compilers: Principles, Techniques, and Tools\" added a fourth author, Monica S. Lam, and the dragon became purple; hence becoming the \"purple dragon book.\" The book also contains the entire code for making a compiler.\n\nThe back cover offers the original inspiration of the cover design: The dragon is replaced by windmills, and the knight is Don Quixote.\n\nThe book was published by Addison-Wesley, . The acknowledgments mention that the book was entirely typeset at Bell Labs using troff on the Unix operating system, little of which had, at that time, been seen outside the Laboratories.\n"}
{"id": "25284", "url": "https://en.wikipedia.org/wiki?curid=25284", "title": "Qubit", "text": "Qubit\n\nIn quantum computing, a qubit () or quantum bit (sometimes qbit) is the basic unit of quantum information—the quantum version of the classical binary bit physically realized with a two-state device. A qubit is a two-state (or two-level) quantum-mechanical system, one of the simplest quantum systems displaying the weirdness of quantum mechanics. Examples include: the spin of the electron in which the two levels can be taken as spin up and spin down; or the polarization of a single photon in which the two states can be taken to be the vertical polarization and the horizontal polarization. In a classical system, a bit would have to be in one state or the other. However, quantum mechanics allows the qubit to be in a coherent superposition of both states/levels at the same time, a property that is fundamental to quantum mechanics and thus quantum computing.\n\nThe coining of the term \"qubit\" is attributed to Benjamin Schumacher. In the acknowledgments of his 1995 paper, Schumacher states that the term \"qubit\" was created in jest during a conversation with William Wootters. The paper describes a way of compressing states emitted by a quantum source of information so that they require fewer physical resources to store. This procedure is now known as Schumacher compression.\n\nA binary digit, characterized as 0 and 1, is used to represent information in classical computers. A binary digit can represent up to one bit of Shannon information, where a bit is the basic unit of information. \nHowever, in this article, the word bit is synonymous with binary digit.\n\nIn classical computer technologies, a \"processed\" bit is implemented by one of two levels of low DC voltage, and whilst switching from one of these two levels to the other, a so-called forbidden zone must be passed as fast as possible, as electrical voltage cannot change from one level to another \"instantaneously\".\n\nThere are two possible outcomes for the measurement of a qubit—usually taken to have the value \"0\" and \"1\", like a bit or binary digit. However, whereas the state of a bit can only be either 0 or 1, the general state of a qubit according to quantum mechanics can be a coherent superposition of both. Moreover, whereas a measurement of a classical bit would not disturb its state, a measurement of a qubit would destroy its coherence and irrevocably disturb the superposition state. It is possible to fully encode one bit in one qubit. However, a qubit can hold more information, e.g. up to two bits using superdense coding.\n\nFor a system of \"n\" components, a complete description of its state in classical physics requires only \"n\" bits, whereas in quantum physics it requires 2−1 complex numbers.\n\nIn quantum mechanics, the general quantum state of a qubit can be represented by a linear superposition of its two orthonormal basis states (or basis vectors). These vectors are usually denoted as\nformula_1\nand \nformula_2. They are written in the conventional Dirac—or \"bra–ket\"—notation; the formula_3 and formula_4 are pronounced \"ket 0\" and \"ket 1\", respectively. These two orthonormal basis states, <math>\\\n\nIn a paper entitled: \"Solid-state quantum memory using the P nuclear spin\", published in the October 23, 2008 issue of the journal \"Nature\", a team of scientists from the U.K. and U.S. reported the first relatively long (1.75 seconds) and coherent transfer of a superposition state in an electron spin \"processing\" qubit to a nuclear spin \"memory\" qubit. This event can be considered the first relatively consistent quantum data storage, a vital step towards the development of quantum computing. Recently, a modification of similar systems (using charged rather than neutral donors) has dramatically extended this time, to 3 hours at very low temperatures and 39 minutes at room temperature. Room temperature preparation of a qubit based on electron spins instead of nuclear spin was also demonstrated by a team of scientists from Switzerland and Australia.\n\n\n"}
{"id": "348257", "url": "https://en.wikipedia.org/wiki?curid=348257", "title": "Ronald N. Bracewell", "text": "Ronald N. Bracewell\n\nRonald Newbold Bracewell AO (22 July 1921 – 12 August 2007) was the Lewis M. Terman Professor of Electrical Engineering of the Space, Telecommunications, and Radioscience Laboratory at Stanford University.\n\nBracewell was born in Sydney, Australia, in 1921, and educated at Sydney Boys High School. He graduated from the University of Sydney in 1941 with the B.Sc. degree in mathematics and physics, later receiving the degrees of B.E. (1943), and M.E. (1948) with first class honours, and while working in the Engineering Department became the President of the Oxometrical Society. During World War II he designed and developed microwave radar equipment in the Radiophysics Laboratory of the Commonwealth Scientific and Industrial Research Organisation, Sydney under the direction of Joseph L. Pawsey and Edward G. Bowen and from 1946 to 1949 was a research student at Sidney Sussex College, Cambridge, engaged in ionospheric research in the Cavendish Laboratory, where in 1949 he received his Ph.D. degree in physics under J. A. Ratcliffe.\n\nFrom October 1949 to September 1954, Dr. Bracewell was a senior research officer at the Radiophysics Laboratory of the CSIRO, Sydney, concerned with very-long-wave propagation and radio astronomy. He then lectured in radio astronomy at the Astronomy Department of the University of California, Berkeley, from September 1954 to June 1955 at the invitation of Otto Struve, and at Stanford University during the summer of 1955, and joined the Electrical Engineering faculty at Stanford in December 1955.\n\nIn 1974 he was appointed the first Lewis M. Terman Professor and Fellow in Electrical Engineering (1974–1979). Though he retired in 1979, he continued to be active until his death.\n\nProfessor Bracewell was a Fellow of the Royal Astronomical Society (1950), Fellow and life member of the Institute of Electrical and Electronic Engineers (1961), Fellow of the American Association for the Advancement of Science (1989), and was a Fellow with other significant societies and organisations.\n\nFor experimental contributions to the study of the ionosphere by means of very low frequency waves, Dr. Bracewell received the Duddell Premium of the Institution of Electrical Engineers, London in 1952. In 1992 he was elected to foreign associate membership of the Institute of Medicine of the U.S. National Academy of Sciences (1992), the first Australian to achieve that distinction, for fundamental contributions to medical imaging. He was one of Sydney University's three honourees when alumni awards were instituted in 1992, with a citation for brain scanning, and was the 1994 recipient of the Institute of Electrical and Electronic Engineers' Heinrich Hertz medal for pioneering work in antenna aperture synthesis and image reconstruction as applied to radio astronomy and to computer-assisted tomography. In 1998 Dr. Bracewell was named Officer of the Order of Australia (AO) for service to science in the fields of radio astronomy and image reconstruction.\n\nAt CSIRO Radiophysics Laboratory, work that in 1942–1945 was classified appeared in a dozen reports. Activities included design, construction, and demonstration of voice-modulation equipment for a 10 cm magnetron (July 1943), a microwave triode oscillator at 25 cm using cylindrical cavity resonators, equipment designed for microwave radar in field use (wavemeter, echo box, thermistor power meter, etc.) and microwave measurement technique. Experience with numerical computation of fields in cavities led, after the war, to a Master of Engineering degree (1948) and the definitive publication on step discontinuities in radial transmission lines (1954).\n\nWhile at the Cavendish Laboratory, Cambridge (1946–1950) Bracewell worked on observation and theory of upper atmospheric ionisation, contributing to experimental technique (1948), explaining solar effects (1949), and distinguishing two layers below the E-layer (1952), work recognised by the Duddell Premium.\n\nAt Stanford Professor Bracewell constructed a microwave spectroheliograph (1961), a large and complex radio telescope which produced daily temperature maps of the sun reliably for eleven years, the duration of a solar cycle. The first radio telescope to give output automatically in printed form, and therefore capable of worldwide dissemination by teleprinter, its daily solar weather maps received acknowledgement from NASA for support of the first manned landing on the moon.\n\nMany fundamental papers on restoration (1954–1962), interferometry (1958–1974) and reconstruction (1956–1961) appeared along with instrumental and observational papers. By 1961 the radio-interferometer calibration techniques developed for the spectroheliograph first allowed an antenna system, with 52-inch fan beam, to equal the angular resolution of the human eye in one observation. With this beam the components of Cygnus A, spaced 100-inch, were put directly in evidence without the need for repeated observations with variable spacing aperture synthesis interferometry.\n\nThe nucleus of the extragalactic source Centaurus A was resolved into two separate components whose right ascensions were accurately determined with a 2.3-minute fan beam at 9.1 cm. Knowing that Centaurus A was composite, Bracewell used the 6.7-minute beam of the Parkes Observatory 64 m radiotelescope at 10 cm to determine the separate declinations of the components and in so doing was the first to observe strong polarisation in an extragalactic source (1962), a discovery of fundamental significance for the structure and role of astrophysical magnetic fields. Subsequent observations made at Parkes by other observers with a 14-minute and wider beams at 21 cm and longer wavelengths, though not resolving the components, were compatible with the formula_1 dependence expected from Faraday rotation if magnetic fields were the polarising agent.\n\nA second major radiotelescope (1971) employing advanced concepts to achieve an angular resolution of 18 seconds of arc was designed and built at Stanford and applied to both solar and galactic studies. The calibration techniques for this leading-edge resolution passed into general use in radio interferometry via the medium of alumni.\n\nUpon the discovery of the cosmic background radiation:\n\nWith the advent of the space age, Bracewell became interested in celestial mechanics, made observations of the radio emission from Sputnik 1, and supplied the press with accurate charts predicting the path of Soviet satellites, which were perfectly visible, if you knew when and where to look. Following the puzzling performance of Explorer I in orbit, he published the first explanation (1958-9) of the observed spin instability of satellites, in terms of the Poinsot motion of a non-rigid body with internal friction. He recorded the signals from Sputniks I, II and III and discussed them in terms of the satellite spin, antenna polarisation, and propagation effects of the ionised medium, especially Faraday effect.\n\nLater (1978, 1979) he invented a spinning, nulling, two-element infrared interferometer suitable for space-shuttle launching into an orbit near Jupiter, with milliarcsecond resolution, that could lead to the discovery of planets around stars other than the sun. This concept was elaborated in 1995 by Angel and Woolf, whose space-station version with four-element double nulling became the Terrestrial Planet Finder (TPF), NASA's candidate for imaging planetary configurations of other stars.\n\nImaging in astronomy led to participation in development of computer assisted x-ray tomography, where commercial scanners reconstruct tomographic images using the algorithm developed by Bracewell for radioastronomical reconstruction from fan-beam scans. This corpus of work has been recognized by the Institute of Medicine, an award by the University of Sydney, and the Heinrich Hertz medal. Service on the founding editorial board of the \"Journal for Computer-Assisted Tomography\", to which he also contributed publications, and on the scientific advisory boards of medical instrumentation companies maintained Bracewell's interest in medical imaging, which became an important part of his regular graduate lectures on imaging, and forms an important part of his 1995 text on imaging.\n\nExperience with the optics, mechanics and control of radiotelescopes led to involvement with solar thermophotovoltaic energy at the time of the energy crisis, including the fabrication of low-cost solid and perforated paraboloidal reflectors by hydraulic inflation.\n\nBracewell is also known for being the first to propose the use of autonomous interstellar space probes for communication between alien civilisations as an alternative to radio transmission dialogs. This hypothetical concept has been dubbed the Bracewell probe after its inventor.\n\nAs a consequence of relating images to Fourier analysis, in 1983 he discovered a new factorisation of the discrete Fourier transform matrix leading to a fast algorithm for spectral analysis. This method, which has advantages over the fast Fourier algorithm, especially for images, is treated in \"The Hartley Transform\" (1986), in U.S. Patent 4,646,256 (1987, now in the public domain), and in over 200 technical papers by various authors that were stimulated by the discovery. Analogue methods of creating a Hartley transform plane first with light and later with microwaves were demonstrated in the laboratory and permitted the determination of electromagnetic phase by the use of square-law detectors. A new elementary signal representation, the Chirplet transform, was discovered (1991) that complements the Gabor elementary signal representations used in dynamic spectral analysis (with the property of meeting the bandwidth-duration minimum associated with the uncertainty principle). This advance opened a new field of adaptive dynamic spectra with wide application in information analysis.\n\nProfessor Bracewell was interested in conveying an appreciation of the role of science in society to the public, in mitigating the effects of scientific illiteracy on public decision making through contact with alumni groups, and in liberal undergraduate education within the framework of the Astronomy Course Program and the Western Culture program in Values, Technology, Science and Society, in both of which he taught for some years. He gave the 1996 Bunyan Lecture on \"The Destiny of Man\".\n\nHe was also interested in the trees of Stanford's campus and published a book about them. He also taught an undergraduate seminar titled I Dig Trees.\n\nBracewell was also a designer and builder of sundials. He and his son Mark Bracewell built one on the South side of the Terman Engineering Building; after that building was demolished, a new home for the sundial, at the same orientation, was found on the Jen-Hsun Huang Engineering Center. He built another sundial at the home of his son, and another on the deck of professor John G. Linvill's house. The Bracewell Radio Sundial at the Very Large Array was built in his honor.\n\n\nBracewell has contributed chapters to:\n\n\n\n"}
{"id": "16118019", "url": "https://en.wikipedia.org/wiki?curid=16118019", "title": "Rural cluster development", "text": "Rural cluster development\n\nA rural cluster development (RCD) is a form of residential subdivision. In an RCD, houses are clustered together in areas zoned for larger properties. The remainder of the land is often designated \"open space\". The effect is tract-home density in the middle of rural communities.\n"}
{"id": "4028894", "url": "https://en.wikipedia.org/wiki?curid=4028894", "title": "Science Made Stupid", "text": "Science Made Stupid\n\nScience Made Stupid: How to Discomprehend the World Around Us is a 1985 book written and illustrated by Tom Weller. The winner of the 1986 Hugo Award for Best Non-Fiction Book, it is a parody of a junior high or high school-level science textbook. Though now out of print, high-resolution scans are available online, as well as an abridged transcription, both of which have been endorsed by Weller . Highlights of the book include a satirical account of the creationism vs. evolution debate and Weller's drawings of fictional prehistoric animals (e.g., the duck-billed mastodon.)\n\nWeller released a companion volume, \"Culture Made Stupid\" (also spelled \"Cvltvre Made Stvpid\"), which satirizes literature and the humanities.\n\n"}
{"id": "520345", "url": "https://en.wikipedia.org/wiki?curid=520345", "title": "Shuttle Carrier Aircraft", "text": "Shuttle Carrier Aircraft\n\nThe Shuttle Carrier Aircraft (SCA) are two extensively modified Boeing 747 airliners that NASA used to transport Space Shuttle orbiters. One is a 747-100 model, while the other is a short range 747-100SR.\n\nThe SCAs were used to ferry Space Shuttles from landing sites back to the Shuttle Landing Facility at the Kennedy Space Center. The orbiters were placed on top of the SCAs by Mate-Demate Devices, large gantry-like structures that hoisted the orbiters off the ground for post-flight servicing then mated them with the SCAs for ferry flights.\n\nIn approach and landing test flights conducted in 1977, the test shuttle \"Enterprise\" was released from an SCA during flight and glided to a landing under its own control.\n\nThe Lockheed C-5 Galaxy was considered for the shuttle-carrier role by NASA, but rejected in favor of the 747. This was due to the 747's low-wing design in comparison to the C-5's high-wing design, and also because the U.S. Air Force would have retained ownership of the C-5, while NASA could own the 747s outright.\n\nThe first aircraft, a Boeing 747-123 registered N905NA, was originally manufactured for American Airlines and still carried visible American cheatlines while testing \"Enterprise\" in the 1970s. It was acquired in 1974 and initially used for trailing wake vortex research as part of a broader study by NASA Dryden, as well as Shuttle tests involving an F-104 flying in close formation and simulating a release from the 747. \n\nThe aircraft was extensively modified for NASA by Boeing in 1976. While first-class seats were kept for NASA passengers, its main cabin and insulation were stripped, mounting struts were added, and the fuselage was strengthened. Vertical stabilizers were added to the tail to aid stability when the Orbiter was being carried. The Avionics and engines were also upgraded, and an escape tunnel system similar to that used on Boeing's first 747 test flights was added. The flight crew escape tunnel system was later removed following the completion of the Approach and Landing Tests (ALT) due to concerns over possible engine ingestion of an escaping crew member.\n\nFlying with the additional drag and weight of the Orbiter imposed significant fuel and altitude penalties. The range was reduced to 1,000 nautical miles (1,850 km), compared to an unladen range of 5500 nautical miles (10,100 km), requiring an SCA to stop several times to refuel on a transcontinental flight. Without the Orbiter, the SCA needed to carry ballast to balance out its center of gravity. The SCA had an altitude ceiling of 15,000 feet and a maximum cruise speed of Mach 0.6 with the orbiter attached. A crew of 170 took a week to prepare the shuttle and SCA for flight.\n\nStudies were conducted to equip the SCA with aerial refueling equipment, a modification already made to the U.S. Air Force E-4 (modified 747-200s) and 747 tanker transports for the IIAF. However, during formation flying with a tanker aircraft to test refueling approaches, minor cracks were spotted on the tailfin of N905NA. While these were not likely to have been caused by the test flights, it was felt that there was no sense taking unnecessary risks. Since there was no urgent need to provide an aerial refueling capacity, the tests were suspended.\nBy 1983, SCA N905NA no longer carried the distinct American Airlines tricolor cheatline. NASA replaced it with its own livery, consisting of a white fuselage and a single blue cheatline. That year, this aircraft was also used to fly \"Enterprise\" on a tour in Europe, with refuelling stops in Goose Bay, Canada; Keflavik, Iceland; England; and West Germany. It then went to the Paris Air Show.\n\nIn 1988, in the wake of the \"Challenger\" accident, NASA procured a surplus 747SR-46 from Japan Airlines. Registered N911NA, it entered service with NASA in 1990 after undergoing modifications similar to N905NA. It was first used in 1991 to ferry the new shuttle \"Endeavour\" from the manufacturers in Palmdale, California to Kennedy Space Center.\n\nBased at the Dryden Flight Research Center within Edwards Air Force Base in California the two aircraft were functionally identical, although N911NA has five upper-deck windows on each side, while N905NA has only two. The rear mounting points on both aircraft were labeled with humorous instructions to \"attach orbiter here\" or \"place orbiter here\", clarified by the precautionary note \"black side down\".\n\nShuttle Carriers were capable of operating from alternative shuttle landing sites such as those in the United Kingdom, Spain, and France. Due to the reduced range of the Shuttle Carrier while mated to an orbiter, additional preparations such as removal of the payload from the orbiter may have been necessary to reduce its weight.\nBoeing transported its Phantom Ray unmanned combat aerial vehicle (UCAV) demonstrator from St. Louis, Missouri, to Edwards on a Shuttle Carrier on December 11, 2010.\n\nFerry flights generally transported the orbiters from Edwards Air Force Base, the shuttle's secondary landing site, to the Shuttle Landing Facility (SLF) at the Kennedy Space Center where the orbiter was processed. This was common in the early days of the space shuttle program when weather conditions at the SLF prevented the shuttle from landing there. Some flights started at the Dryden Flight Research Center following delivery of the orbiter from Rockwell International to NASA from the nearby facilities in Palmdale, California.\n\nAt the end of the space shuttle program the SCA was used to deliver the retired orbiters from the Kennedy Space Center to their museums. \"Discovery\" was delivered to the Udvar-Hazy Center of the Smithsonian Institution's National Air and Space Museum in Chantilly, Virginia, near Washington, D.C. on April 19, 2012. On April 17, 2012, \"Discovery\" was flown atop a Shuttle Carrier Aircraft escorted by a NASA T-38 Talon chase aircraft in a final farewell flight. The 747 and \"Discovery\" flew over Washington, D.C. and the metropolitan area around 10 am and arrived at Dulles around 11 am. The flyover and landing were widely covered on national news media.\n\nThe last ferry flight took \"Endeavour\" from Kennedy Space Center to Los Angeles between 19 and 21 September 2012 via Ellington Field and Edwards Air Force Base. After leaving Edwards the SCA with \"Endeavour\" performed low level flyovers above various landmarks across California, from Sacramento to the San Francisco Bay Area, and finally to Los Angeles. \"Endeavour\" was delivered to Los Angeles International Airport (LAX). From there the orbiter was transported through the streets of Los Angeles and Inglewood to its final destination in the California Science Center in Exposition Park.\n\nShuttle Carrier N911NA retired on February 8, 2012 after its final mission to the Dryden Flight Research Facility at Edwards Air Force Base in Palmdale, California, and was used as a source of parts for NASA's Stratospheric Observatory for Infrared Astronomy (SOFIA) aircraft, another modified Boeing 747. N911NA is now preserved and on display at the Joe Davies Heritage Airpark in Palmdale, California as part of a long-term loan to the city from NASA.\nShuttle Carrier N905NA was used to ferry the retired Space Shuttles to their respective museums. After delivering \"Endeavour\" to the Los Angeles International Airport in September 2012, the aircraft was flown to the Dryden Flight Research Facility, where NASA intended it to join N911NA as a source of spare parts for NASA's SOFIA aircraft, but when NASA engineers surveyed N905NA they determined that it had few parts usable for SOFIA. In 2013, a decision was made to preserve N905NA and display it at Space Center Houston with the mockup shuttle \"Independence\" mounted on its back. N905NA was flown to Ellington Field where it was carefully dismantled, ferried to the Johnson Space Center in seven major pieces (a process called The Big Move), reassembled, and finally mated with the replica shuttle in August 2014. The display, called Independence Plaza, opened to the public for the first time on January 23, 2016.\n\n\n"}
{"id": "14732215", "url": "https://en.wikipedia.org/wiki?curid=14732215", "title": "The Everglades: River of Grass", "text": "The Everglades: River of Grass\n\nThe Everglades: River of Grass is a non-fiction book written by Marjory Stoneman Douglas in 1947. Published the same year as the formal opening of Everglades National Park, the book was a call to attention about the degrading quality of life in the Everglades and remains an influential book on nature conservation as well as a reference for information on South Florida. It was used as recently as 2007 by \"The New York Times\".\n\nDouglas was a freelance writer who submitted stories to magazines throughout the 1920s, 1930s and 1940s. Her friend Hervey Allen was an editor at Rinehart, responsible for the \"Rivers of America Series\". Allen asked her to write a story about the Miami River, but Douglas did not find it very interesting, calling it only \"an inch long\". She began learning more about the Miami River though, and in her research, she instead suggested to her editor to write a story about the Everglades. Douglas spent five years researching the Everglades, consulting with Garald Parker of the US Geological Survey, who was studying the Everglades hydrology systems, and eventually wrote nearly 40 papers on the ecosystems in the Everglades.\n\n\"The Quarterly Review of Biology\" reviewed the book and commented on Douglas' \"convincing evidence\" in her assertion that the Everglades are a river instead of a swamp, and declared that \"it is hoped that this excellent account of the area and its history may provide the needed stimulus for the establishment of an intelligent conservation program for the entire Everglades.\"\n\nThe book has gone through numerous editions, selling over 500,000 copies since its original publication. The last three editions, published by a Florida publisher, Pineapple Press, have updated afterwords: in 1998 by Randy Lee Loftis and Marjory Stoneman Douglas; the 50th anniversary edition in 1997 by Cyril Zaneski; and the 60th anniversary edition in 2007 by Michael Grunwald, author of \"The Swamp\".\n"}
{"id": "879261", "url": "https://en.wikipedia.org/wiki?curid=879261", "title": "Three-component theory of stratification", "text": "Three-component theory of stratification\n\nThe three-component theory of stratification, more widely known as Weberian stratification or the three class system, was developed by German sociologist Max Weber with class, status and power as distinct ideal types. Weber developed a multidimensional approach to social stratification that reflects the interplay among wealth, prestige and power.\nClass, status and power have not only a great deal of effect within their individual areas but also a great deal of influence over the other areas.\n\n\nAccording to Weber, there are two basic dimensions of power: the \"possession\" of power and the \"exercising\" of power. \n\nThis essay was written shortly before World War I and was published posthumously in 1922 as part of Weber's \"Wirtschaft und Gesellschaft\". It was translated into English in the 1940s as \"Class, Status, Party\" and has been re-translated as \"The distribution of power within the community: Classes, \"Stände\", Parties\".\n\nAccording to Weber, the ability to possess power derives from the individual's ability to control various \"social resources\". \"The mode of distribution gives to the propertied a monopoly on the possibility of transferring property from the sphere of use as 'wealth' to the sphere of 'capital,' that is, it gives them the entrepreneurial function and all chances to share directly or indirectly in returns on capital\" (Lemert 2004:116). These resources can be anything and everything: they might include land, capital, social respect, physical strength, and intellectual knowledge.\n\nThe ability to exercise power takes a number of different forms, but all involve the idea that it means the ability to get your own way with others, regardless of their ability to resist you. \"For example, if we think about an individual's chances of realizing their own will against someone else, it is reasonable to believe that the person's social prestige, class position, and membership in a political group will have an effect on these chances\" (Hurst 2007:202). In terms of understanding the relationship between power and social stratification, Weber theorized the various ways in which societies are organized in hierarchical systems of domination and subordination using the several major concepts.\n\n\"Class, at its core, is an economic concept; it is the position of individuals in the market that determines their class position. And it is how one is situated in the marketplace that directly affects one's life chances\" (Hurst 2007:203). This was theorized by Weber on the basis of \"unequal access to material resources\". For example, if someone possesses something that you want or need then this makes him potentially more powerful than you. He is in a dominant position and you are in a subordinate position because he controls access to a desired social resource. A classic illustration here is the relationship between an employer and employee.\n\n\"The existence of status groups most often shows itself in the form of \n\n\nIf you respect someone or view him as your social superior, then he will potentially be able to exercise power over you (since you will respond positively to his instructions / commands). In this respect, social status is a social resource simply because he may have it while you may not. \"Not all power, however entails social honor: The Typical American Boss, as well as the typical big speculator, deliberately relinquishes social honor. Quite generally, 'mere economic' power, and especially 'naked' money power, is by no means a recognized basis or social honor\" (Lemert 2004:116).\n\nNote: The German word \"Stand\", plural \"Stände\" (English, \"status\" or \"status group\") is sometimes left untranslated in Weber, in order to keep in view the origins of this concept in medieval guilds, professions, ethnic identities, and feudal classifications.\n\nParties are associations that aim at securing \"power within an organization [or the state] for its leaders in order to attain ideal or material advantages for its active members\" (Hurst 2007:206). This form of power can be related to the way in which the State is organized in modern social systems (involving the ability to make laws, for example). If you can influence this process of law creation then you will be in a potentially powerful position. Thus, by your ability to influence a decision-making process you possess power, even though you may not directly exercise that power personally. Political parties are the organizational means to possess power through the mechanism of the State and they include not just formally organized parties, but any group that is organized to influence the way in which power is exercised legitimately through the machinery of the State. \"Since parties aim at such goals as getting their programs developed or accepted and getting positions of influence within organizations, it is clear that they operate only within a rational order within which these goals are possible to attain and only when there is a struggle for power\" (Hurst 2007:206).\n\nSocial action is in direct relation to \"political or party power\" in combination with the class situation. The influence of laws is based on the social action of members of the classes. \"The direction of interests may vary according to whether or not social action of a larger or smaller portion of those commonly affected by the class situation, or even an association among them, e.g., a trade union, has grown out of the class situation, from which the individual may expect promising results for himself\" (Lemert 2004:117). \"The degree in which \"social action\" and possibly associations emerge from the mass behavior of the members of a class is linked to general cultural conditions, especially to those of an intellectual sort. It is also likened to the extent of the contrasts that have already evolved\" (Lemert 2004:118). \"Class-conscious action is most likely if, first, [Weber says] 'the connection between the causes and consequences of the \"class situation\"' are transparent, or clear. If individuals can plainly see that there is a connection between the structure of the economic system and what happens to them in terms of life chances, class action is more likely\" (Hurst 2007:204). The greater the numbers within these class positions, will increase the chance that they will rise up in action.\n\n\"It is noncontroversial that the class situation in which each individual finds himself represents a limitation on his scope, tends to keep him within the class. It acts as an obstacle to any rise into a higher class, and as a pair of water wings with respect to the classes below…Class type, relations with class fellows, power over outward resources adapted to the class situation, and so on\" (Schumpeter 1951:163-164). In capitalist society movement between classes is a possibility. Hence the use of the term \"The American Dream\" to show the ability of people to ascend to a higher class through hard work and ingenuity. \"Class composition is forever changing, to the point where there may be a completely new set of families\" (Schumpeter 1951:165).\n\nWeber saw four classes: the propertied class, the non-propertied class, the petit bourgeoisie and the manual labourer class.\n\n"}
{"id": "52321029", "url": "https://en.wikipedia.org/wiki?curid=52321029", "title": "Web@cademie", "text": "Web@cademie\n\nWeb@cademie is a private, nonprofit and tuition-free computer programming school created and funded by French IONIS Education Group with several partners including Epitech and Zup de Co association. The school was first opened in Paris in 2010.\n\nHeadquartered in Le Kremlin-Bicêtre, the school has branches in Lyon, Strasbourg and Nancy.\n\nWeb@cademie delivers a two-year program dedicated to people with no degree and no background (without the French Baccalaureate but with a strong motivation in computer science). This is to help dropout students to have a job in a competitive industry.\n\nThe school has received the award \"\".\nThe school is a non-profit organization and is entirely free.\nMajor company such as Microsoft give financial support.\n"}
