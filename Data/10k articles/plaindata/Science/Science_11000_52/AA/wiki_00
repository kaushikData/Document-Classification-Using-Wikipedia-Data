{"id": "671262", "url": "https://en.wikipedia.org/wiki?curid=671262", "title": "Abell 1835", "text": "Abell 1835\n\nAbell 1835 is a galaxy cluster in the Abell catalogue. It is a cluster that also gravitational lenses more-distant background galaxies to make them visible to astronomers. The cluster has a red shift of around 75,900 km/s and spans 12′.\n\nIn 2004, one of the galaxies lensed by this cluster was proposed to be the most distant galaxy known, Galaxy Abell 1835 IR1916.\n\n"}
{"id": "16526084", "url": "https://en.wikipedia.org/wiki?curid=16526084", "title": "Ahrnsbrak Glacier", "text": "Ahrnsbrak Glacier\n\nAhrnsbrak Glacier is a glacier in the Enterprise Hills of the Heritage Range in Antarctica, flowing north between Sutton Peak and Shoemaker Peak to the confluent ice at the lower end of Union Glacier. It was mapped by the United States Geological Survey from surveys and U.S. Navy air photos, 1961–66, and was named by the Advisory Committee on Antarctic Names for William F. Ahrnsbrak of the United States Antarctic Research Program, a glaciologist at Palmer Station in 1965.\n\n"}
{"id": "47253892", "url": "https://en.wikipedia.org/wiki?curid=47253892", "title": "Alice Bowman", "text": "Alice Bowman\n\nAlice Bowman (born 1960) is the Mission Operations Manager for the \"New Horizons\" mission to Pluto. She is the first woman to fill that role at the Applied Physics Laboratory, taking on the position in 2002 specifically for the duration of the three billion-mile space journey.\n\nBowman grew up in Richmond, Virginia. She was influenced at an early age by the Gemini program, and in 1969 she watched the Apollo 11 moon landings. Bowman originally majored in physics and chemistry at university, gaining a BA from the University of Virginia.\n\nShe first worked in the defense industry analyzing infrared detectors and developing anti-cancer drugs. She entered the Applied Physics Laboratory as an engineer, intending to work on tracking incoming ballistic missiles.\n\nA member of the principal professional staff at the APL, she is supervisor of the university's own Space Mission Operation Group and Mission Operations Manager (MOM) of the Mission Operations Centre on the \"New Horizons\" project. This title is one, it has been suggested, that male personnel refer to traditionally as \"Ops manager\"; but Bowman, as \"a physicist, space commander and parent, embraces the broader term\" of MOM. Bowman leads a team of approximately 40 people, and personally assesses every piece of information the centre sends to the space crew before it dispatches. Ten days before the eventual Pluto-encounter day, that entailed over 20,000 commands. She has compared the levels of accuracy required to achieving a hole in one in golf.\n\nBowman is a member of the American Institute of Aeronautics and Astronautics and the International SpaceOps Committee.\n\nAsteroid 146040 Alicebowman, discovered by Marc Buie at the Kitt Peak National Observatory in 2000, was named after her. The official naming citation was published by the Minor Planet Center on 11 July 2018 ().\n\nShe is married with one son, and for leisure plays the clarinet and bass, with a particular interest in bluegrass music.\n"}
{"id": "6793569", "url": "https://en.wikipedia.org/wiki?curid=6793569", "title": "Align-m", "text": "Align-m\n\nAlign-m is a multiple sequence alignment program written by Ivo Van Walle.\n\nAlign-m has the ability to accomplish the following tasks:\n\n\n"}
{"id": "25777855", "url": "https://en.wikipedia.org/wiki?curid=25777855", "title": "Behavioral immune system", "text": "Behavioral immune system\n\nThe behavioral immune system is a phrase coined by the psychological scientist Mark Schaller to refer to a suite of psychological mechanisms that allow individual organisms to detect the potential presence of disease-causing parasites in their immediate environment, and to engage in behaviors that prevent contact with those objects and individuals.\n\nThese mechanisms include sensory processes through which cues connoting the presence of parasitic infections are perceived (e.g., the smell of a foul odor, the sight of pox or pustules), as well as stimulus–response systems through which these sensory cues trigger a cascade of aversive affective, cognitive, and behavioral reactions (e.g., arousal of disgust, automatic activation of cognitions that connote the threat of disease, behavioral avoidance).\n\nThe existence of a behavioral immune system has been documented across many animal species, including humans. It is theorized that the mechanisms that comprise the behavioral immune system evolved as a crude first line of defense against disease-causing pathogens.\n\nWithin the psychological sciences, there is extensive research linking the behavioral immune system to a variety of prejudices—including prejudices against people who aren't actually diseased but are simply characterized by some sort of visual characteristics that deviate from those of a subjectively prototypical human being. The disease–avoidant processes that characterize the behavioral immune system have been shown to contribute to prejudices against obese individuals, elderly individuals, and people with physical disfigurements or disabilities.\n\nIn addition, the behavioral immune system appears to contribute to xenophobia and ethnocentrism. One implication is that these prejudices tend to be exaggerated under conditions in which people feel especially vulnerable to the potential transmission of infectious diseases.\n\nAdditional lines of research on the behavioral immune system have shown that people engage in more reticent and conservative forms of behavior under conditions in which they feel more vulnerable to disease transmission. For instance, when the potential threat of disease is made salient, people tend to be less extraverted or sociable. Evidence suggests that the behavioral immune system also incorporates mechanisms designed to search out and process (heuristic) signs of disease at the level of basic visual attention.\n\nThe behavioral immune system also has consequences at a cultural level of analysis. Under ecological circumstances in which diseases are more prevalent, people also tend to display more reticent and socially restricted forms of behavior, and human cultures are defined by more conservative norms and value systems.\n\nSome research suggests that the behavioral immune system has implications for the functioning of the physiological immune system too. One study found that the mere visual perception of diseased-looking people stimulated white blood cells to respond more aggressively to infection (as indicated by the production of the proinflammatory cytokine Interleukin 6 in response to a bacterial stimulus).\n\nResearch also indicates that immune-relevant interventions which target pathogen transmission can interrupt behavioral responses. For example, receiving a flu vaccination or washing one's hands can reduce the extent of negative interpersonal and intergroup attitudes elicited by disease cues and concerns.\n\n"}
{"id": "839943", "url": "https://en.wikipedia.org/wiki?curid=839943", "title": "Bone fracture", "text": "Bone fracture\n\nA bone fracture (sometimes abbreviated FRX or Fx, F, or #) is a medical condition in which there is a partial or complete break in the continuity of the bone. In more severe cases, the bone may be broken into several pieces. A bone fracture may be the result of high force impact or stress, or a minimal trauma injury as a result of certain medical conditions that weaken the bones, such as osteoporosis, osteopenia, bone cancer, or osteogenesis imperfecta, where the fracture is then properly termed a pathologic fracture.\n\nAlthough bone tissue itself contains no nociceptors, bone fracture is painful for several reasons:\n\n\nDamage to adjacent structures such as nerves, muscles or blood vessels, spinal cord, and nerve roots (for spine fractures), or cranial contents (for skull fractures) may cause other specific signs and symptoms.\n\nSome fractures may lead to serious complications including a condition known as compartment syndrome. If not treated, eventually, compartment syndrome may require amputation of the affected limb. Other complications may include non-union, where the fractured bone fails to heal or mal-union, where the fractured bone heals in a deformed manner.\n\nComplications of fractures may be classified into three broad groups, depending upon their time of occurrence. These are as follows – \n\nThe natural process of healing a fracture starts when the injured bone and surrounding tissues bleed, forming a fracture hematoma. The blood coagulates to form a blood clot situated between the broken fragments. Within a few days, blood vessels grow into the jelly-like matrix of the blood clot. The new blood vessels bring phagocytes to the area, which gradually removes the non-viable material. The blood vessels also bring fibroblasts in the walls of the vessels and these multiply and produce collagen fibres. In this way, the blood clot is replaced by a matrix of collagen. Collagen's rubbery consistency allows bone fragments to move only a small amount unless severe or persistent force is applied.\n\nAt this stage, some of the fibroblasts begin to lay down bone matrix in the form of collagen monomers. These monomers spontaneously assemble to form the bone matrix, for which bone crystals (calcium hydroxyapatite) are deposited in amongst, in the form of insoluble crystals. This mineralization of the collagen matrix stiffens it and transforms it into bone. In fact, bone \"is\" a mineralized collagen matrix; if the mineral is dissolved out of bone, it becomes rubbery. Healing bone callus on average is sufficiently mineralized to show up on X-ray within 6 weeks in adults and less in children. This initial \"woven\" bone does not have the strong mechanical properties of mature bone. By a process of remodelling, the woven bone is replaced by mature \"lamellar\" bone. The whole process may take up to 18 months, but in adults, the strength of the healing bone is usually 80% of normal by 3 months after the injury.\n\nSeveral factors may help or hinder the bone healing process. For example, tobacco smoking hinders the process of bone healing, and adequate nutrition (including calcium intake) will help the bone healing process. Weight-bearing stress on bone, after the bone has healed sufficiently to bear the weight, also builds bone strength.\n\nAlthough there are theoretical concerns about NSAIDs slowing the rate of healing, there is not enough evidence to warrant withholding the use of this type analgesic in simple fractures.\n\nSmokers generally have lower bone density than non-smokers, so they have a much higher risk of fractures. There is also evidence that smoking delays bone healing.\n\nA bone fracture may be diagnosed based on the history given and the physical examination performed. Radiographic imaging often is performed to confirm the diagnosis. Under certain circumstances, radiographic examination of the nearby joints is indicated in order to exclude dislocations and fracture-dislocations. In situations where projectional radiography alone is insufficient, Computed Tomography (CT) or Magnetic Resonance Imaging (MRI) may be indicated.\n\nIn orthopedic medicine, fractures are classified in various ways. Historically they are named after the physician who first described the fracture conditions, however, there are more systematic classifications as well.\n\nThey may be divided into stable versus unstable depending on the likelihood that they may shift further.\n\n\n\n\n\n\nAn anatomical classification may begin with specifying the involved body part, such as the head or arm, followed with more specific localization. Fractures that have additional definition criteria than merely localization often may be classified as subtypes of fractures, such as a Holstein-Lewis fracture being a subtype of a humerus fracture. Most typical examples in an orthopaedic classification given in the previous section cannot be classified appropriately into any specific part of an anatomical classification, however, as they may apply to multiple anatomical fracture sites.\n\n\nThe Orthopaedic Trauma Association Committee for Coding and Classification published its classification system in 1996, adopting a similar system to the 1987 AO Foundation system. In 2007, they extended their system, unifying the two systems regarding wrist, hand, foot, and ankle fractures.\n\nA number of classifications are named after the person (eponymous) who developed it. \n\nTreatment of bone fractures are broadly classified as surgical or conservative, the latter basically referring to any non-surgical procedure, such as pain management, immobilization or other non-surgical stabilization. A similar classification is \"open\" versus \"closed treatment\", in which \"open treatment\" refers to any treatment in which the fracture site is opened surgically, regardless of whether the fracture is an open or closed fracture.\n\nIn arm fractures in children, ibuprofen has been found to be as effective as a combination of acetaminophen and codeine.\n\nSince bone healing is a natural process that will occur most often, fracture treatment aims to ensure the best possible \"function\" of the injured part after healing. Bone fractures typically are treated by restoring the fractured pieces of bone to their natural positions (if necessary), and maintaining those positions while the bone heals. Often, aligning the bone, called reduction, in a good position and verifying the improved alignment with an X-ray is all that is needed. This process is extremely painful without anaesthesia, about as painful as breaking the bone itself. To this end, a fractured limb usually is immobilized with a plaster or fibreglass cast or splint that holds the bones in position and immobilizes the joints above and below the fracture. When the initial post-fracture oedema or swelling goes down, the fracture may be placed in a removable brace or orthosis. If being treated with surgery, surgical nails, screws, plates, and wires are used to hold the fractured bone together more directly. Alternatively, fractured bones may be treated by the Ilizarov method which is a form of an external fixator.\n\nOccasionally smaller bones, such as phalanges of the toes and fingers, may be treated without the cast, by buddy wrapping them, which serves a similar function to making a cast. A device called a Suzuki frame may be used in cases of deep, complex intra-articular digit fractures. By allowing only limited movement, immobilization helps preserve anatomical alignment while enabling callus formation, toward the target of achieving union.\n\nSplinting results in the same outcome as casting in children who have a distal radius fracture with little shifting.\n\nSurgical methods of treating fractures have their own risks and benefits, but usually surgery is performed only if conservative treatment has failed, is very likely to fail, or likely to result in a poor functional outcome. With some fractures such as hip fractures (usually caused by osteoporosis), surgery is offered routinely because non-operative treatment results in prolonged immobilisation, which commonly results in complications including chest infections, pressure sores, deconditioning, deep vein thrombosis (DVT), and pulmonary embolism, which are more dangerous than surgery. When a joint surface is damaged by a fracture, surgery is also commonly recommended to make an accurate anatomical reduction and restore the smoothness of the joint.\n\nInfection is especially dangerous in bones, due to the recrudescent nature of bone infections. Bone tissue is predominantly extracellular matrix, rather than living cells, and the few blood vessels needed to support this low metabolism are only able to bring a limited number of immune cells to an injury to fight infection. For this reason, open fractures and osteotomies call for very careful antiseptic procedures and prophylactic use of antibiotics.\n\nOccasionally, bone grafting is used to treat a fracture.\n\nSometimes bones are reinforced with metal. These implants must be designed and installed with care. \"Stress shielding\" occurs when plates or screws carry too large of a portion of the bone's load, causing atrophy. This problem is reduced, but not eliminated, by the use of low-modulus materials, including titanium and its alloys. The heat generated by the friction of installing hardware can accumulate easily and damage bone tissue, reducing the strength of the connections. If dissimilar metals are installed in contact with one another (i.e., a titanium plate with cobalt-chromium alloy or stainless steel screws), galvanic corrosion will result. The metal ions produced can damage the bone locally and may cause systemic effects as well.\n\nA Cochrane review of low-intensity pulsed ultrasound to speed healing in newly broken bones found insufficient evidence to justify routine use. Other reviews have found tentative evidence of benefit. It may be an alternative to surgery for established nonunions.\n\nVitamin D supplements combined with additional calcium marginally reduces the risk of hip fractures and other types of fracture in older adults; however, vitamin D supplementation alone did not reduce the risk of fractures.\n\nIn children, whose bones are still developing, there are risks of either a growth plate injury or a greenstick fracture.\n\n\n"}
{"id": "1118061", "url": "https://en.wikipedia.org/wiki?curid=1118061", "title": "Campylite", "text": "Campylite\n\nCampylite is a variety of the lead arsenate mineral mimetite which received the name from the Greek 'kampylos'- bent, on account of the barrel-shaped bend of its crystals. It has also been used as an alternate name for pyromorphite.\n\nIt occurs in the upper lead deposits through the oxidation of galena or cerussite. The main deposits are Příbram in Bohemia and Dry Gill, Caldbeck Fells, near Wigton, Cumbria, England.\n\n\n"}
{"id": "8750824", "url": "https://en.wikipedia.org/wiki?curid=8750824", "title": "Coordination isomerism", "text": "Coordination isomerism\n\nCoordination isomerism is a form of structural isomerism in which the composition of the complex ion varies. In a coordination isomer the total ratio of ligand to metal remains the same, but the ligands attached to a specific metal ion change. Examples of a complete series of coordination isomers require at least two metal ions and sometimes more.\n\nFor example, a solution containing ([Co(NH)] and [Cr(CN)]) is a coordination isomer with a solution containing [Cr(NH)] and [Co(CN)].\n\n\n"}
{"id": "30694430", "url": "https://en.wikipedia.org/wiki?curid=30694430", "title": "Criticism of the theory of relativity", "text": "Criticism of the theory of relativity\n\nCriticism of the theory of relativity of Albert Einstein was mainly expressed in the early years after its publication in the early twentieth century, on scientific, pseudoscientific, philosophical, or ideological bases. Though some of these criticisms had the support of reputable scientists, Einstein's theory of relativity is now accepted by the scientific community.\n\nReasons for criticism of the theory of relativity have included alternative theories, rejection of the abstract-mathematical method, and alleged errors of the theory. According to some authors, antisemitic objections to Einstein's Jewish heritage also occasionally played a role in these objections. There are still some critics of relativity today, but their opinions are not shared by the majority in the scientific community.\n\nAround the end of the 19th century, the view was widespread that all forces in nature are of electromagnetic origin (the \"electromagnetic worldview\"), especially in the works of Joseph Larmor (1897) and Wilhelm Wien (1900). This was apparently confirmed by the experiments of Walter Kaufmann (1901–1903), who measured an increase of the mass of a body with velocity which was consistent with the hypothesis that the mass was generated by its electromagnetic field. Max Abraham (1902) subsequently sketched a theoretical explanation of Kaufmann's result in which the electron was considered as rigid and spherical. However, it was found that this model was incompatible with the results of many experiments (including the Michelson–Morley experiment, the Experiments of Rayleigh and Brace, and the Trouton–Noble experiment), according to which no motion of an observer with respect to the luminiferous aether (\"aether drift\") had been observed despite numerous attempts to do so. Henri Poincaré (1902) conjectured that this failure arose from a general law of nature, which he called \"the principle of relativity\". Hendrik Antoon Lorentz (1904) created a detailed theory of electrodynamics (Lorentz ether theory) that was premised on the existence of an immobile aether and employed a set of space and time coordinate transformations that Poincaré called the Lorentz transformations, including the effects of length contraction and local time. However, Lorentz's theory only partially satisfied the relativity principle, because his transformation formulas for velocity and charge density were incorrect. This was corrected by Poincaré (1905) who obtained full Lorentz covariance of the electrodynamic equations.\n\nCriticizing Lorentz's 1904 theory, Abraham (1904) held that the Lorentz contraction of electrons requires a non-electromagnetic force to ensure the electron's stability. This was unacceptable to him as a proponent of the electromagnetic worldview. He continued that as long as a consistent explanation is missing as to how those forces and potentials act together on the electron, Lorentz's system of hypotheses is incomplete and doesn't satisfy the relativity principle. Poincaré (1905) removed this objection by showing that the non-electromagnetic potential (\"Poincaré stress\") holding the electron together can be formulated in a Lorentz covariant way, and showed that in principle it is possible to create a Lorentz covariant model for gravitation which he considered non-electromagnetic in nature as well. Thus the consistency of Lorentz's theory was proven, but the electromagnetic worldview had to be given up. Eventually, Albert Einstein published in September 1905 what is now called special relativity, which was based on a radical new application of the relativity principle in connection with the constancy of the speed of light. In special relativity, the space and time coordinates depend on the inertial observer's frame of reference, and the luminiferous aether plays no role in the physics. Although this theory was founded on a very different kinematical model, it was experimentally indistinguishable from the aether theory of Lorentz and Poincaré, since both theories satisfy the relativity principle of Poincaré and Einstein, and both employ the Lorentz transformations. After Minkowski's introduction in 1908 of the geometric spacetime model for Einstein's version of relativity, most physicists eventually decided in favor of the Einstein-Minkowski version of relativity with its radical new views of space and time, in which there was no useful role for the aether.\n\nKaufmann–Bucherer–Neumann experiments: To conclusively decide between the theories of Abraham and Lorentz, Kaufmann repeated his experiments in 1905 with improved accuracy. However, in the meantime the theoretical situation had changed. Alfred Bucherer and Paul Langevin (1904) developed another model, in which the electron is contracted in the line of motion, and dilated in the transverse direction, so that the volume remains constant. While Kaufmann was still evaluating his experiments, Einstein published his theory of special relativity. Eventually, Kaufmann published his results in December 1905 and argued that they are in agreement with Abraham's theory and require rejection of the \"basic assumption of Lorentz and Einstein\" (the relativity principle). Lorentz reacted with the phrase \"I am at the end of my Latin\", while Einstein did not mention those experiments before 1908. Yet, others started to criticize the experiments. Max Planck (1906) alluded to inconsistencies in the theoretical interpretation of the data, and Adolf Bestelmeyer (1906) introduced new techniques, which (especially in the area of low velocities) gave different results and which cast doubts on Kaufmann's methods. Therefore, Bucherer (1908) conducted new experiments and arrived at the conclusion that they confirm the mass formula of relativity and thus the \"relativity principle of Lorentz and Einstein\". Yet Bucherer's experiments were criticized by Bestelmeyer leading to a sharp dispute between the two experimentalists. On the other hand, additional experiments of Hupka (1910), Neumann (1914) and others seemed to confirm Bucherer's result. The doubts lasted until 1940, when in similar experiments Abraham's theory was conclusively disproved. (It must be remarked that besides those experiments, the relativistic mass formula had already been confirmed by 1917 in the course of investigations on the theory of spectra. In modern particle accelerators, the relativistic mass formula is routinely confirmed.)\n\nIn 1902–1906, Dayton Miller repeated the Michelson–Morley experiment together with Edward W. Morley. They confirmed the null result of the initial experiment. However, in 1921–1926, Miller conducted new experiments which apparently gave positive results. Those experiments initially attracted some attention in the media and in the scientific community but have been considered refuted for the following reasons: Einstein, Max Born, and Robert S. Shankland pointed out that Miller hadn't appropriately considered the influence of temperature. A modern analysis by Roberts shows that Miller's experiment gives a null result, when the technical shortcomings of the apparatus and the error bars are properly considered. Additionally, Miller's result is in disagreement with all other experiments, which were conducted before and after. For example, Georg Joos (1930) used an apparatus of similar dimensions to Miller's, but he obtained null results. In recent experiments of Michelson–Morley type where the coherence length is increased considerably by using lasers and masers the results are still negative.\n\nIn the 2011 Faster-than-light neutrino anomaly, the OPERA collaboration published results which appeared to show that the speed of neutrinos is slightly faster than the speed of light. However, sources of errors were found and confirmed in 2012 by the OPERA collaboration, which fully explained the initial results. In their final publication, a neutrino speed consistent with the speed of light was stated. Also subsequent experiments found agreement with the speed of light, see measurements of neutrino speed.\n\nIt was also claimed that special relativity cannot handle acceleration, which would lead to contradictions in some situations. However, this assessment is not correct, since acceleration actually can be described in the framework of special relativity (see Acceleration (special relativity), Proper reference frame (flat spacetime), Hyperbolic motion, Rindler coordinates, Born coordinates). Paradoxes relying on insufficient understanding of these facts were discovered in the early years of relativity. For example, Max Born (1909) tried to combine the concept of rigid bodies with special relativity. That this model was insufficient was shown by Paul Ehrenfest (1909), who demonstrated that a rotating rigid body would, according to Born's definition, undergo a contraction of the circumference without contraction of the radius, which is impossible (Ehrenfest paradox). Max von Laue (1911) showed that rigid bodies cannot exist in special relativity, since the propagation of signals cannot exceed the speed of light, so an accelerating and rotating body will undergo deformations.\n\nPaul Langevin and von Laue showed that the twin paradox can be completely resolved by consideration of acceleration in special relativity. If two twins move away from each other, and one of them is accelerating and coming back to the other, then the accelerated twin is younger than the other one, since he was located in at least two inertial frames of reference, and therefore his assessment of which events are simultaneous changed during the acceleration. For the other twin nothing changes since he remained in a single frame.\n\nAnother example is the Sagnac effect. Two signals were sent in opposite directions around a rotating platform. After their arrival a displacement of the interference fringes occurs. Sagnac himself believed that he had proved the existence of the aether. However, special relativity can easily explain this effect. When viewed from an inertial frame of reference, it is a simple consequence of the independence of the speed of light from the speed of the source, since the receiver runs away from one beam, while it approaches the other beam. When viewed from a rotating frame, the assessment of simultaneity changes during the rotation, and consequently the speed of light is not constant in accelerated frames.\n\nAs was shown by Einstein, the only form of accelerated motion that cannot be described is the one due to gravitation, since special relativity is not compatible with the Equivalence principle. Einstein was also unsatisfied with the fact that inertial frames are preferred over accelerated frames. Thus over the course of several years (1908–1915), Einstein developed general relativity. This theory includes the replacement of Euclidean geometry by non-Euclidean geometry, and the resultant curvature of the path of light led Einstein (1912) to the conclusion that (like in accelerated frames) the speed of light is not constant in extended gravitational fields. Therefore, Abraham (1912) argued that Einstein had given special relativity a coup de grâce. Einstein responded that within its area of application (in areas where gravitational influences can be neglected) special relativity is still applicable with high precision, so one cannot speak of a coup de grâce at all.\n\nIn special relativity, the transfer of signals at superluminal speeds is impossible, since this would violate the Poincaré-Einstein synchronization, and the causality principle. Following an old argument by Pierre-Simon Laplace, Poincaré (1904) alluded to the fact that Newton's law of universal gravitation is founded on an infinitely great speed of gravity. So the clock-synchronization by light signals could in principle be replaced by a clock-synchronization by instantaneous gravitational signals. In 1905, Poincaré himself solved this problem by showing that in a relativistic theory of gravity the speed of gravity is equal to the speed of light. Although much more complicated, this is also the case in Einstein's theory of general relativity.\n\nAnother apparent contradiction lies in the fact that the group velocity in anomalously dispersive media is higher than the speed of light. This was investigated by Arnold Sommerfeld (1907, 1914) and Léon Brillouin (1914). They came to the conclusion that in such cases the signal velocity is not equal to the group velocity, but to the front velocity which is never faster than the speed of light. Similarly, it is also argued that the apparent superluminal effects discovered by Günter Nimtz can be explained by a thorough consideration of the velocities involved.\n\nAlso quantum entanglement (denoted by Einstein as \"spooky action at a distance\"), according to which the quantum state of one entangled particle cannot be fully described without describing the other particle, does not imply superluminal transmission of information (see quantum teleportation), and it is therefore in conformity with special relativity.\n\nInsufficient knowledge of the basics of special relativity, especially the application of the Lorentz transformation in connection with length contraction and time dilation, led and still leads to the construction of various apparent paradoxes. Both the twin paradox and the Ehrenfest paradox and their explanation were already mentioned above. Besides the twin paradox, also the reciprocity of time dilation (\"i.e.\" every inertially moving observer considers the clock of the other one as being dilated) was heavily criticized by Herbert Dingle and others. For example, Dingle wrote a series of letters to Nature at the end of the 1950s. However, the self-consistency of the reciprocity of time dilation had already been demonstrated long before in an illustrative way by Lorentz (in his lectures from 1910, published 1931) and many others—they alluded to the fact that it is only necessary to carefully consider the relevant measurement rules and the relativity of simultaneity. Other known paradoxes are the Ladder paradox and Bell's spaceship paradox, which also can simply be solved by consideration of the relativity of simultaneity.\n\nMany physicists (like Hendrik Lorentz, Oliver Lodge, Albert Abraham Michelson, Edmund Taylor Whittaker, Harry Bateman, Ebenezer Cunningham, Charles Émile Picard, Paul Painlevé) were uncomfortable with the rejection of the aether, and preferred to interpret the Lorentz transformation based on the existence of a preferred frame of reference, as in the aether-based theories of Lorentz, Larmor, and Poincaré. However, the idea of an aether hidden from any observation was not supported by the mainstream scientific community, therefore the aether theory of Lorentz and Poincaré was superseded by Einstein's special relativity which was subsequently formulated in the framework of four-dimensional spacetime by Minkowski.\n\nOthers such as Herbert E. Ives argued that it might be possible to experimentally determine the motion of such an aether, but it was never found despite numerous experimental tests of Lorentz invariance (see tests of special relativity).\n\nAlso attempts to introduce some sort of relativistic aether (consistent with relativity) into modern physics such as by Einstein on the basis of general relativity (1920), or by Paul Dirac in relation to quantum mechanics (1951), were not supported by the scientific community (see Luminiferous aether#End of aether?).\n\nIn his Nobel lecture, George F. Smoot (2006) described his own experiments on the Cosmic microwave background radiation anisotropy as \"New Aether drift experiments\". Smoot explained that \"one problem to overcome was the strong prejudice of good scientists who learned the lesson of the Michelson and Morley experiment and Special Relativity that there were no preferred frames of reference.\" He continued that \"there was an education job to convince them that this did not violate Special Relativity but did find a frame in which the expansion of the universe looked particularly simple.\"\n\nThe theory of complete aether drag, as proposed by George Gabriel Stokes (1844), was used by some critics as Ludwig Silberstein (1920) or Philipp Lenard (1920) as a counter-model of relativity. In this theory, the aether was completely dragged within and in the vicinity of matter, and it was believed that various phenomena, such as the absence of aether drift, could be explained in an \"illustrative\" way by this model. However, such theories are subject to great difficulties. Especially the aberration of light contradicted the theory, and all auxiliary hypotheses, which were invented to rescue it, are self-contradictory, extremely implausible, or in contradiction to other experiments like the Michelson–Gale–Pearson experiment. In summary, a sound mathematical and physical model of complete aether drag was never invented, consequently this theory was no serious alternative to relativity.\n\nAnother alternative was the so-called emission theory of light. As in special relativity the aether concept is discarded, yet the main difference from relativity lies in the fact that the velocity of the light source is added to that of light in accordance with the Galilean transformation. As the hypothesis of complete aether drag, it can explain the negative outcome of all aether drift experiments. Yet, there are various experiments that contradict this theory. For example, the Sagnac effect is based on the independence of light speed from the source velocity, and the image of Double stars should be scrambled according to this model—which was not observed. Also in modern experiments in particle accelerators no such velocity dependence could be observed. These results are further confirmed by the De Sitter double star experiment (1913), conclusively repeated in the X-ray spectrum by K. Brecher in 1977;\nand the terrestrial experiment by Alväger, \"et al\". (1963);, which all show that the speed of light is independent of the motion of the source within the limits of experimental accuracy.\n\nSome consider the \"principle of the constancy of the velocity of light\" insufficiently substantiated. However, as already shown by Robert Daniel Carmichael (1910) and others, the constancy of the speed of light can be interpreted as a natural consequence of \"two\" experimentally demonstrated facts:\n\n\nNote that measurements regarding the speed of light are actually measurements of the two-way speed of light, since the one-way speed of light depends on which convention is chosen to synchronize the clocks.\n\nEinstein emphasized the importance of general covariance for the development of general relativity, and took the position that the general covariance of his 1915 theory of gravity ensured implementation of a generalized relativity principle. This view was challenged by Erich Kretschmann (1917), who argued that every theory of space and time (even including Newtonian dynamics) can be formulated in a covariant way, if additional parameters are included, and thus general covariance of a theory would in itself be insufficient to implement a generalized relativity principle. Although Einstein (1918) agreed with that argument, he also countered that Newtonian mechanics in general covariant form would be too complicated for practical uses. Although it is now understood that Einstein's response to Kretschmann was mistaken (subsequent papers showed that such a theory would still be usable), another argument can be made in favor of general covariance: it is a natural way to express the equivalence principle, \"i.e.\", the equivalence in the description of a free-falling observer and an observer at rest, and thus it is more convenient to use general covariance together with general relativity, rather than with Newtonian mechanics. Connected with this, also the question of absolute motion was dealt with. Einstein argued that the general covariance of his theory of gravity supports Mach's principle, which would eliminate any \"absolute motion\" within general relativity. However, as pointed out by Willem de Sitter in 1916, Mach's principle is not completely fulfilled in general relativity because there exist matter-free solutions of the field equations. This means that the \"inertio-gravitational field\", which describes both gravity and inertia, can exist in the absence of gravitating matter. However, as pointed out by Einstein, there is one fundamental difference between this concept and absolute space of Newton: the inertio-gravitational field of general relativity is determined by matter, thus it is not absolute.\n\nIn the \"Bad Nauheim Debate\" (1920) between Einstein and (among others) Philipp Lenard, the latter stated the following objections: He criticized the lack of \"illustrativeness\" of Einstein's version of relativity, a condition that he suggested could only be met by an aether theory. Einstein responded that for physicists the content of \"illustrativeness\" or \"common sense\" had changed in time, so it could no longer be used as a criterion for the validity of a physical theory. Lenard also argued that with his relativistic theory of gravity Einstein had tacitly reintroduced the aether under the name \"space\". While this charge was rejected (among others) by Hermann Weyl, in an inaugural address given at the University of Leiden in 1920, shortly after the Bad Nauheim debates, Einstein himself acknowledged that according to his general theory of relativity, so-called \"empty space\" possesses physical properties that influence matter and \"vice versa\". Lenard also argued that Einstein's general theory of relativity admits the existence of superluminal velocities, in contradiction to the principles of special relativity; for example, in a rotating coordinate system in which the Earth is at rest, the distant points of the whole universe are rotating around Earth with superluminal velocities. However, as been pointed out by Weyl, it's not possible to handle a rotating extended system as a rigid body (neither in special nor in general relativity)—so the signal velocity of an object never exceeds the speed of light. Another criticism that was raised by both Lenard and Gustav Mie concerned the existence of \"fictitious\" gravitational fields in accelerating frames, which according to Einstein's Equivalence Principle are no less physically real than those produced by material sources. Lenard and Mie argued that physical forces can only be produced by real material sources, while the gravitational field that Einstein supposed to exist in an accelerating frame of reference has no concrete physical meaning. Einstein responded that, based on Mach's principle, one can think of these gravitational fields as induced by the distant masses. In this respect the criticism of Lenard and Mie has been vindicated, since according to the modern consensus, in agreement with Einstein's own mature views, Mach's principle as originally conceived by Einstein is not actually supported by general relativity, as already mentioned above.\n\nLudwik Silberstein, who initially was a supporter of the special theory, objected at different occasions against general relativity. In 1920 he argued that the deflection of light by the sun, as observed by Arthur Eddington et al. (1919), is not necessarily a confirmation of general relativity, but may also be explained by the Stokes-Planck theory of complete aether drag. However, such models are in contradiction with the aberration of light and other experiments (see \"Alternative theories\"). In 1935, Silberstein claimed to have found a contradiction in the Two-body problem in general relativity. However, also this claim was refuted by Einstein and Rosen (1935).\n\nThe consequences of relativity, such as the change of ordinary concepts of space and time, as well as the introduction of non-Euclidean geometry in general relativity, were criticized by some philosophers of different philosophical schools. It was characteristic for many philosophical critics that they had insufficient knowledge of the mathematical and formal basis of relativity, which led to the criticisms often missing the heart of the matter. For example, relativity was misinterpreted as some form of relativism. However, this is misleading as it was emphasized by Einstein or Planck. On one hand it's true that space and time became relative, and the inertial frames of reference are handled on equal footing. On the other hand, the theory makes natural laws invariant—examples are the constancy of the speed of light, or the covariance of Maxwell's equations. Consequently, Felix Klein (1910) called it the \"invariant theory of the Lorentz group\" instead of relativity theory, and Einstein (who reportedly used expressions like \"absolute theory\") sympathized with this expression as well.\n\nCritical responses to relativity were also expressed by proponents of Neo-Kantianism (Paul Natorp, Bruno Bauch, etc.), and Phenomenology (Oskar Becker, Moritz Geiger etc.). While some of them only rejected the philosophical consequences, others rejected also the physical consequences of the theory. Einstein was criticized for violating Immanuel Kant's categoric scheme, \"i.e.\", it was claimed that space-time curvature caused by matter and energy is impossible, since matter and energy already require the concepts of space and time. Also the three-dimensionality of space, Euclidean geometry, and the existence of absolute simultaneity were claimed to be necessary for the understanding of the world; none of them can possibly be altered by empirical findings. By moving all those concepts into a metaphysical area, any form of criticism of Kantianism would be prevented. Other pseudo-Kantians like Ernst Cassirer or Hans Reichenbach (1920), tried to modify Kant's philosophy. Subsequently, Reichenbach rejected Kantianism at all and became a proponent of logical positivism.\n\nBased on Henri Poincaré's conventionalism, philosophers such as Pierre Duhem (1914) or Hugo Dingler (1920) argued that the classical concepts of space, time, and geometry were, and will always be, the most convenient expressions in natural science, therefore the concepts of relativity cannot be correct. This was criticized by proponents of logical positivism such as Moritz Schlick, Rudolf Carnap, or Reichenbach. They argued that Poincaré's conventionalism could be modified, as to bring it into accord with relativity. Although it is true that the basic assumptions of Newtonian mechanics are simpler, it can only be brought into accord with modern experiments by inventing auxiliary hypotheses. On the other hand, relativity doesn't need such hypotheses, thus from a conceptual viewpoint, relativity is in fact simpler than Newtonian mechanics.\n\nSome proponents of Philosophy of Life, Vitalism, Critical realism (in German speaking countries) argued that there is a fundamental difference between physical, biological and psychological phenomena. For example, Henri Bergson (1921), who otherwise was a proponent of special relativity, argued that time dilation cannot be applied to biological organisms, therefore he denied the relativistic solution of the twin paradox. However, those claims were rejected by Paul Langevin, André Metz and others. Biological organisms consist of physical processes, so there is no reason to assume that they are not subject to relativistic effects like time dilation.\n\nBased on the philosophy of Fictionalism, the philosopher Oskar Kraus (1921) and others claimed that the foundations of relativity were only fictitious and even self-contradictory. Examples were the constancy of the speed of light, time dilation, length contraction. These effects appear to be mathematically consistent as a whole, but in reality they allegedly are not true. Yet, this view was immediately rejected. The foundations of relativity (such as the equivalence principle or the relativity principle) are not fictitious, but based on experimental results. Also, effects like constancy of the speed of light and relativity of simultaneity are not contradictory, but complementary to one another.\n\nIn the Soviet Union (mostly in the 1920s), philosophical criticism was expressed on the basis of dialectic materialism. The theory of relativity was rejected as anti-materialistic and speculative, and a mechanistic worldview based on \"common sense\" was required as an alternative. Similar criticisms also occurred in the People's Republic of China during the Cultural Revolution. (On the other hand, other philosophers considered relativity as being compatible with Marxism.)\n\nAlthough Planck already in 1909 compared the changes brought about by relativity with the Copernican Revolution, and although special relativity was accepted by most of the theoretical physicists and mathematicians by 1911, it was not before publication of the experimental results of the eclipse expeditions (1919) by a group around Arthur Stanley Eddington that relativity was noticed by the public. Following Eddington's publication of the eclipse results, Einstein was glowingly praised in the mass media, and was compared to Nikolaus Copernicus, Johannes Kepler and Isaac Newton, which caused a popular \"relativity hype\" (\"Relativitätsrummel\", as it was called by Sommerfeld, Einstein, and others). This triggered a counter-reaction of some scientists and scientific laymen who could not accept the concepts of modern physics, including relativity theory and quantum mechanics. The ensuing public controversy regarding the scientific status of Einstein's theory of gravity, which was unprecedented, was partly carried out in the press. Some of the criticism was not only directed to relativity, but personally at Einstein as well, who some of his critics accused of being behind the promotional campaign in the German press. \n\nSome academic scientists, especially experimental physicists such as the Nobel laureates Philipp Lenard and Johannes Stark, as well as Ernst Gehrcke, Stjepan Mohorovičić, Rudolf Tomaschek and others criticized the increasing abstraction and mathematization of modern physics, especially in the form of relativity theory, and later quantum mechanics. It was seen as a tendency to abstract theory building, connected with the loss of intuitive \"common sense\". In fact, relativity was the first theory, in which the inadequacy of the \"illustrative\" classical physics was thought to have been demonstrated. Some of Einstein's critics ignored these developments and tried to revitalize older theories, such as aether drag models or emission theories (see \"Alternative Theories\"). However, those qualitative models were never sufficiently advanced to compete with the success of the precise experimental predictions and explanatory powers of the modern theories. Additionally, there was also a great rivalry between experimental and theoretical physicists, as regards the professorial activities and the occupation of chairs at German universities. The opinions clashed at the \"Bad Nauheim debates\" in 1920 between Einstein and (among others) Lenard, which attracted much attention in the public.\n\nIn addition, there were many critics (with or without physical training) whose ideas were far outside the scientific mainstream. These critics were mostly people who had developed their ideas long before the publication of Einstein's version of relativity, and they tried to resolve in a straightforward manner some or all of the enigmas of the world. Therefore, Wazeck (who studied some German examples) gave to these \"free researchers\" the name \"world riddle solver\" (\"Welträtsellöser\", such as Arvid Reuterdahl, Hermann Fricke or Johann Heinrich Ziegler). Their views had their quite different roots in monism, Lebensreform, or occultism. Their views were typically characterized by the fact that they practically rejected the entire terminology and the (primarily mathematical) methods of modern science. Their works were published by private publishers, or in popular and non-specialist journals. It was significant for many \"free researchers\" (especially the monists) to explain all phenomena by intuitive and illustrative mechanical (or electrical) models, which also found its expression in their defense of the aether. For this reason they objected to the abstractness and inscrutability of the relativity theory, which was considered a pure calculation method that cannot reveal the true reasons underlying the phenomena. The \"free researchers\" often used Mechanical explanations of gravitation, in which gravity is caused by some sort of \"aether pressure\" or \"mass pressure from a distance\". Such models were regarded as an illustrative alternative to the abstract mathematical theories of gravitation of both Newton and Einstein. Additionally, also the enormous self-confidence of the \"free researchers\" is noteworthy, since they not only believed to have solved all the riddles of the world, but also had the expectation that they would rapidly convince the scientific community.\n\nSince Einstein rarely defended himself against these attacks, this task was undertaken by other relativity theoreticians, who (according to Hentschel) formed some sort of \"defensive belt\" around Einstein. Some representatives were Max von Laue, Max Born, etc. and on popular-scientific and philosophical level Hans Reichenbach, André Metz etc., who led many discussions with critics in semi-popular journals and newspapers. However, most of these discussions failed from the start. Physicists like Gehrcke, some philosophers, and the \"free researchers\" were so obsessed with their own ideas and prejudices that they were unable to grasp the basics of relativity; consequently, the participants of the discussions were talking past each other. In fact, the theory that was criticized by them was not relativity at all, but rather a caricature of it. The \"free researchers\" were mostly ignored by the scientific community, but also, in time, respected physicists such as Lenard and Gehrcke found themselves in a position outside the scientific community. However, the critics didn't believe that this was due to their incorrect theories, but rather due to a conspiracy of the relativistic physicists (and in the 1920s & 1930s of the Jews as well), which allegedly tried to put down the critics, and to preserve and improve their own positions within the academic world. For example, Gehrcke (1920/24) held that the propagation of relativity is a product of some sort of mass suggestion. Therefore, he instructed a media monitoring service to collect over 5000 newspaper clippings which were related to relativity, and published his findings in a book. However, Gehrcke's claims were rejected, because the simple existence of the \"relativity hype\" says nothing about the validity of the theory, and thus it cannot used for or against relativity.\n\nAfterward, some critics tried to improve their positions by the formation of alliances. One of them was the \"Academy of Nations\", which was founded in 1921 in the USA by Robert T. Browne and Arvid Reuterdahl. Other members were Thomas Jefferson Jackson See and as well as Gehrcke and Mohorovičić in Germany. It is unknown whether other American critics such as Charles Lane Poor, Charles Francis Brush, Dayton Miller were also members. The alliance disappeared as early as the mid-1920s in Germany and by 1930 in the USA.\n\nShortly before and during World War I, there appeared some nationalistically motivated criticisms of relativity and modern physics. For example, Pierre Duhem regarded relativity as the product of the \"too formal and abstract\" German spirit, which was in conflict with the \"common sense\". Similarly, popular criticism in the Soviet Union and China, which partly was politically organized, rejected the theory not because of factual objections, but as ideologically motivated as the product of western decadence.\n\nSo in those countries, the Germans or the Western civilization were the enemies. However, in Germany the Jewish ancestry of some leading relativity proponents such as Einstein and Minkowski made them targets of racially minded critics, although many of Einstein's German critics did not show evidence of such motives. The engineer Paul Weyland, a known nationalistic agitator, arranged the first public meeting against relativity in Berlin in 1919. While Lenard and Stark were also known for their nationalistic opinions, they declined to participate in Weyland's rallies, and Weyland's campaign eventually fizzled out due to a lack of prominent speakers. Lenard and others instead responded to Einstein's challenge to his professional critics to debate his theories at the scientific conference held annually at Bad Nauheim. While Einstein's critics, assuming without any real justification that Einstein was behind the activities of the German press in promoting the triumph of relativity, generally avoided antisemitic attacks in their earlier publications, it later became clear to many observers that antisemitism did play a significant role in some of the attacks.\n\nReacting to this underlying mood, Einstein himself openly speculated in a newspaper article that in addition to insufficient knowledge of theoretical physics, antisemitism at least partly motivated their criticisms. Some critics, including Weyland, reacted angrily and claimed that such accusations of antisemitism were only made to force the critics into silence. However, subsequently Weyland, Lenard, Stark and others clearly showed their antisemitic biases by beginning to combine their criticisms with racism. For example, Theodor Fritsch emphasized the alleged negative consequences of the \"Jewish spirit\" within relativity physics, and the far right-press continued this propaganda unhindered. After the murder of Walther Rathenau (1922) and murder threats against Einstein, he left Berlin for some time. Gehrcke's book on \"The mass suggestion of relativity theory\" (1924) was not antisemitic itself, but it was praised by the far-right press as describing an alleged typical Jewish behavior, which was also imputed to Einstein personally. Philipp Lenard in 1922 spoke about the \"foreign spirit\" as the foundation of relativity, and afterward he joined the Nazi party in 1924; Johannes Stark did the same in 1930. Both were proponents of the so-called German Physics, which only accepted scientific knowledge based on experiments, and only if accessible to the senses. According to Lenard (1936), this is the \"Aryan physics or physics by man of Nordic kind\" as opposed to the alleged formal-dogmatic \"Jewish physics\". Additional antisemitic critics can be found in the writings of Wilhelm Müller, Bruno Thüring and others. For example, Müller erroneously claimed that relativity was a purely \"Jewish affair\" and it would correspond to the \"Jewish essence\" etc., while Thüring made comparisons between the Talmud and relativity.\n\nSome of Einstein's critics, like Lenard, Gehrcke and Reuterdahl, accused him of plagiarism, and questioned his priority claims to the authorship of relativity theory. The thrust of such allegations was to promote more traditional alternatives to Einstein's abstract hypothetico-deductive approach to physics, while Einstein himself was to be personally discredited. It was argued by Einstein's supporters that such personal accusations were unwarranted, since the physical content and the applicability of former theories were quite different from Einstein's theory of relativity. However, others argued that between them Poincaré and Lorentz had earlier published several of the core elements of Einstein's 1905 relativity paper, including a generalized relativity principle that was intended by Poincaré to apply to all physics. Some examples:\n\nSome contemporary historians of science have revived the question as to whether Einstein was possibly influenced by the ideas of Poincaré, who first stated the relativity principle and applied it to electrodynamics, developing interpretations and modifications of Lorentz's electron theory that appear to have anticipated what is now called special relativity. Another discussion concerns a possible mutual influence between Einstein and David Hilbert as regards completing the field equations of general relativity (see Relativity priority dispute).\n\nA collection of various criticisms can be found in the book \"Hundert Autoren gegen Einstein\" (\"A Hundred Authors Against Einstein\"), published in 1931. It contains very short texts from 28 authors, and excerpts from the publications of another 19 authors. The rest consists of a list that also includes people who only for some time were opposed to relativity. Besides philosophic objections (mostly based on Kantianism), also some alleged elementary failures of the theory were included; however, as some commented, those failures were due to the authors' misunderstanding of relativity. For example, Hans Reichenbach described the book as an \"accumulation of naive errors\", and as \"unintentionally funny\". Albert von Brunn interpreted the book as a backward step to the 16th and 17th century, and Einstein said, in response to the book, that if he were wrong, then one author would have been enough.\n\nAccording to Goenner, the contributions to the book are a mixture of mathematical–physical incompetence, hubris, and the feelings of the critics of being suppressed by contemporary physicists advocating for the new theory. The compilation of the authors show, Goenner continues, that this was not a reaction within the physics community—only one physicist (Karl Strehl) and three mathematicians (Jean-Marie Le Roux, Emanuel Lasker and Hjalmar Mellin) were present—but a reaction of an inadequately educated academic citizenship, which didn't know what to do with relativity. As regards the average age of the authors: 57% were substantially older than Einstein, one third was around the same age, and only two persons were substantially younger. Two authors (Reuterdahl, von Mitis) were antisemitic and four others were possibly connected to the Nazi movement. On the other hand, no antisemitic expression can be found in the book, and it also included contributions of some authors of Jewish ancestry (Salomo Friedländer, Ludwig Goldschmidt, Hans Israel, Emanuel Lasker, Oskar Kraus, Menyhért Palágyi).\n\nThe theory of relativity is considered to be self-consistent, is consistent with many experimental results, and serves as the basis of many successful theories like quantum electrodynamics. Therefore, fundamental criticism (like that of Herbert Dingle, Louis Essen, Petr Beckmann, Maurice Allais and Tom van Flandern) has not been taken seriously by the scientific community, and due to the lack of quality of many critical publications (found in the process of peer review) they were rarely accepted for publication in reputable scientific journals. Just as in the 1920s, most critical works are published in small publications houses, alternative journals (like \"Apeiron\" or \"Galilean Electrodynamics\"), or private websites. Consequently, where criticism of relativity has been dealt with by the scientific community, it has mostly been in historical studies.\n\nHowever, this does not mean that there is no further development in modern physics. The progress of technology over time has led to extremely precise ways of testing the predictions of relativity, and so far it has successfully passed all tests (such as in particle accelerators to test special relativity, and by astronomical observations to test general relativity). In addition, in the theoretical field there is continuing research intended to unite general relativity and quantum theory. The most promising models are string theory and loop quantum gravity. Some variations of those models also predict violations of Lorentz invariance on a very small scale.\n\n\n\n\n\n\n"}
{"id": "30835995", "url": "https://en.wikipedia.org/wiki?curid=30835995", "title": "Cross impact analysis", "text": "Cross impact analysis\n\nCross-impact analysis is a methodology developed by Theodore Gordon and Olaf Helmer in 1966 to help determine how relationships between events would impact resulting events and reduce uncertainty in the future. The Central Intelligence Agency (CIA) became interested in the methodology in the late 1960s and early 1970s as an analytic technique for predicting how different factors and variables would impact future decisions. In the mid-1970s, futurists began to use the methodology in larger numbers as a means to predict the probability of specific events and determine how related events impacted one another. By 2006, cross-impact analysis matured into a number of related methodologies with uses for businesses and communities as well as futurists and intelligence analysts.\n\nThe basic principles of cross-impact analysis date back to the late 1960s, but the original processes were relatively simple and were based on a game design. Eventually, advanced techniques, methodologies, and programs were developed to apply the principles of cross-impact analysis, and the basic method is now applied in futures think tanks, business settings, and the intelligence community.\n\nTheodore J. Gordon writes that cross-impact analysis was the result of a question: \"can forecasting be based on perceptions about how future events may interact?\"\n\nThe first format of the method was a card game titled \"Future\", where events were determined by probabilities, a special die, and impacts from previously played events. This initial game format of cross-impact analysis was programmed for computers at UCLA in 1968. From this point on, the methodology underwent increasing development and sophistication to meet certain needs and conditions of users.\n\nAs cross-impact analysis expanded in the early 1970s, researchers and futurists modified the basic principles to improve on the methodology. In 1972, researchers at The Institute for the Future added time-series instead of \"Slice of Time\", Norman Dalkey used conditional probabilities, and Julius Kane developed \"KSIM\", a simulation technique that used interactions between time series variables rather than events. In 1974, Duperrin and Godet developed Cross Impact Systems and Matrices (or SMIC) in France for prospective forecasting studies.\n\nAdvancements in simulation models continued into the 1980s. In 1980, Selwyn Enzer at the University of California incorporated cross-impact analysis into a simulation method known as Interax, The Delphi technique was combined with Cross Impact Analysis in 1984, and researchers at Texas A&M University used Cross Impact in a process called \"EZ-IMPACT\" that was based on Kane's algorithm from KSIM.\n\nAfter simulation models and methods were developed for cross-impact analysis, analysts began to develop the range of topics that it could address. Cross-impact analysis was being used to solve real world issues as John Stover applied the methodology to simulate the economy of Uruguay. However, real world application of the methodology advanced rapidly in the 1990s. By 1993, SMIC was used for subjects as diverse as the nuclear industry, world geopolitical evolution, and corporate activities and jobs to 2000. In 1999, Robert Blanning and Bruce Reinig from the Owen Graduate School of Management at Vanderbilt University utilized a modified form of cross-impact analysis to determine futures for Hong Kong and the Hong Kong economy as the United Kingdom relinquished control to the People's Republic of China.\n\nCross-impact analysis has two schools of thought and ways of approach. The first is the futures forecasting style that originally developed the methodology. The second is a sub-school of intelligence analysts which modified the original methodology to better address their needs. Nevertheless, cross-impact analysis is based upon the premise that events and activities do not happen in a vacuum and other events and the surrounding environment can significantly influence the probability of certain events to occur.\n\nCross-impact analysis attempts to connect relationships between events and variables. These relationships are then categorized as positive or negative relative to each other, and are used to determine which events or scenarios are most probable or likely to occur within a given time frame.\n\nThe futures forecasting style is based in the systems and methods developed during the 1970s and 1980s and follows several strict steps.\n\nFirst, analysts must consider the number and type of events to be considered in the analysis and create an event set. Because each event will have an interaction with every other event, Gordon recommends that 10–40 events be used.\n\nSecond, analysts must take the initial probability of each event into account. The probabilities of events must be taken in isolation from one another.\n\nThird, analysts need to generate conditional probabilities that events have on each other. Basically, this asks the question, \"If event 'A' occurs, what is the new probability of event 'B' occurring?\" This must be done for every possible interaction between events.\n\nFourth, analysts must test their initial conditional probabilities to ensure that there are no mathematical errors. This is usually done by running simulations in a computer several times.\n\nFifth, analysts can run the analysis to determine future scenarios, or determine how significant other events are to specific events.\n\nThe futurist forecasting style of cross-impact analysis relies heavily on probabilities and mathematics in its processes. Initial probabilities and conditional probabilities are calculated using either percentages or factor numbers equivalent to percentages. Researchers must calculate the numerical values or percentages very precisely to ensure accurate results and that impacts of events on each other are realistic and not contradictory. In addition, researchers must be careful when calculating negative impacts as the negative influence can create mathematical impossibilities.\n\nThis mathematical strictness makes the futurist forecasting style of cross-impact analysis uniform and differences in actual analytic methods, simulations and programs have only minor differences to fit the needs of the specific researcher or analyst.\n\nThe accuracy of the math and specific events requires special expertise in the events or topic of discussion. In order to get the insight needed to get events and calculations, analysts typically contact a large number of experts and ask their opinions on events or probabilities in -person as groups or through surveys.\n\nThese groupings often resemble the Delphi Technique, which is an analytic technique that gathers a group of experts on a subject together and asks their opinion on a scenario or prediction. Usually, analysts consider the average prediction or scenario as the most likely to occur. The two are so closely related, that analysts often use the two techniques in combination or as part of a larger methodology.\n\nThe futurist forecasting style of cross-impact analysis carries a few key strengths. Its use of groups of experts ensures a number of opinions worth considering when calculating probabilities of events. The level of mathematics in calculating probabilities ensures that the results are as accurate as a researcher can make them. In addition, when used on consort with other analytic techniques, this type of cross-impact analysiscan give greater quantitative results to an otherwise qualitative analysis. The relative conformity of methods ensures that analysts using different methods or simulations can come to similar results, making the results testable in a broader setting.\n\nMany of the strengths of the futurist forecasting style of cross-impact analysis give rise to many of its weaknesses. The conformity of the style generates a certain level of inflexibility when dealing with variables other than events, like environmental conditions or political issues. In addition, the severe level of mathematics involved in this style leads to long delays as scenarios must be run to ensure mathematical accuracy of probabilities, or particular issues with Bayes' theorem appear. The level of math also require researchers to either be knowledgeable in math or additional computer programs to deal with the scenarios and probabilities of the method.\n\nShortly after Theodore Gordon and Olaf Helmer developed the original cross-impact method, the United States intelligence community picked up the technique and has been using it for over thirty years.\n\nWhile the basic premise of relationships and impacts between multiple variables remains the same, the intelligence community modified cross-impact analysis to meet its various needs.\n\nThe intelligence community has created a more flexible and variable system than the original methodology. Event relationships and impacts are still similar to the method incorporated by futurists. However, intelligence analysts have expanded the parameters of cross-impact analysis beyond comparing events to include variables like environment, political circumstances, and popular opinion to influence probabilities of certain events. In addition, intelligence analysts can choose to use more flexible measurements like \"enhancing,\" \"inhibiting,\" or \"unrelated\" instead of the rigid mathematics of the tradition methodology to include non-event variables.\n\nA major part of the intelligence analysis style of cross-impact analysis is the cross-impact matrix. The matrix is a visualization of the cross-impact analysis and allows for modification. It also allows an analyst to find both the most influential variables and those variables that are impacted by the most other variables, not just direct, one-to-one relationships. While several traditional cross-impact analysis methods suggest the creation of a matrix, the priority still relies in probabilities, one-to-one relationships, and the order of events.\n\nIn the intelligence analysis style cross-impact matrix, analysts use pluses and minuses instead of numerical values allowing for non-event variables and allowing the analyst to compare variables directly to all other variables without calculations.\n\nIntelligence analysis style cross-impact analysis has several key advantages. The flexibility of the model allows for analysts to measure different types of variables against each other, not just probable events. In addition, the ability to discard stringent mathematical criteria means that researchers do not need extensive mathematics training or specialized software to use cross-impact analysis. This also enables experts in a topic to use the methodology relatively quickly without having to cross-check the numerous calculations faced by the Futurist Forecasting Style.\n\nThe lack of stringent procedures of the intelligence analysis style also bring considerable drawbacks. The flexibility of the style relies heavily on the opinions and knowledge of the analysts involved, and is difficult to reproduce results with a different group. In addition, the option to remove mathematics can harm analysts by creating results that do not have numerical values to back them. This lack of mathematics may make the process easier at first, but the amount of specialized software is limited when compared to the Futurist Forecasting Style, making work more tedious as the number of variables increases.\n\nResearchers can use cross-impact analysis for a wide variety of applications. Futurists have already used the methodology for forecasting events in specific industries, politics, markets, and even entire communities.\n\nIn intelligence analysis, analysts can use the method to predict events, conditions, or decisions based on a wide variety of variables and conditions at local, national, and international levels.\n\n\n"}
{"id": "6816424", "url": "https://en.wikipedia.org/wiki?curid=6816424", "title": "Defence Nuclear Material", "text": "Defence Nuclear Material\n\nDefence Nuclear Material within the UK is defined as:\n"}
{"id": "10706365", "url": "https://en.wikipedia.org/wiki?curid=10706365", "title": "Drools", "text": "Drools\n\nDrools is a business rule management system (BRMS) with a forward and backward chaining inference based rules engine, more correctly known as a production rule system, using an enhanced implementation of the Rete algorithm.\n\nKIE (Knowledge Is Everything) is the new umbrella name to drools, optaPlanner, jBPM, Guvnor, uberFire and related technologies.\n\nDrools supports the Java Rules Engine API (Java Specification Request 94) standard for its business rule engine and enterprise framework for the construction, maintenance, and enforcement of business policies in an organization, application, or service.\n\nJBoss Enterprise BRMS is a business rule management system and reasoning engine for business policy and rules development, access, and change management. JBoss Enterprise BRMS is a productized version of Drools with enterprise-level support available. JBoss Rules is also a productized version of Drools, but JBoss Enterprise BRMS is the flagship product.\n\nComponents of the enterprise version:\n\nDrools and Guvnor are JBoss Community open source projects. As they are mature, they are brought into the enterprise-ready product JBoss Enterprise BRMS.\n\nComponents of the JBoss Community version:\n\nThis example illustrates a simple rule to print out information about a holiday in July. It checks a condition on an instance of the codice_1 class, and executes Java code if that condition is true.\n\nThe purpose of dialect \"codice_2\" is to point the Getter and Setters of the variables of your Plain Old Java Object (POJO) classes.\nConsider the above example, in which a codice_1 class is used and inside the circular brackets (parentheses) \"codice_4\" is used. So with the help dialect \"codice_2\" the getter and setters of the variable \"codice_4\" can be accessed.\n\nDialect \"codice_7\" is used to help us write our Java code in our rules. There is one restriction or characteristic on this. We cannot use Java code inside \"when\" part of the rule but we can use Java code in \"then\" part.\n\nWe can also declare a Reference variable codice_8 without the codice_9 symbol. There is no restriction on this. The main purpose of putting the codice_9 symbol before the variable is to mark the difference between variables of POJO classes and Rules.\n\n\n"}
{"id": "21960613", "url": "https://en.wikipedia.org/wiki?curid=21960613", "title": "Eddie (crater)", "text": "Eddie (crater)\n\nEddie Crater is a crater in the Elysium quadrangle of Mars at 12.3° north latitude and 217.9° west longitude. It is 89 km in diameter and was named after Lindsay Eddie, a South African astronomer (1845–1913).\n\nImpact craters generally have a rim with ejecta around them, in contrast volcanic craters usually do not have a rim or ejecta deposits. As craters get larger (greater than 10 km in diameter) they usually have a central peak, as this crater has. The peak is caused by a rebound of the crater floor following the impact. It contains material uplifted from beneath the surface.\n\n"}
{"id": "10051488", "url": "https://en.wikipedia.org/wiki?curid=10051488", "title": "Electricity (Orchestral Manoeuvres in the Dark song)", "text": "Electricity (Orchestral Manoeuvres in the Dark song)\n\n\"Electricity\" is the 1979 debut single of the English group Orchestral Manoeuvres in the Dark, featured on their eponymous debut album the following year. Inspired by Kraftwerk's \"Radioactivity\", the song addresses society's wasteful usage of energy sources. Andy McCluskey and Paul Humphreys sing the lead vocals on the track together in unison. As with single \"Messages\" from the same album, the song features a melodic synth break instead of a sung chorus.\n\nIt was on the strength of \"Electricity\" that the band were offered a recording contract with Dindisc, who twice re-issued the single. In 2012, \"Electricity\" peaked at no. 126 in the French charts.\n\nVince Clarke of Erasure (and formerly chief songwriter of Depeche Mode, Yazoo and The Assembly) has cited \"Electricity\" as his primary inspiration to pursue a career in electronic music, while BBC Radio's Steve Lamacq has named it as the track that made him want to become a DJ.\n\n\nAfter OMD's first concert, opening for Joy Division in a 1978 appearance at Eric's Club in Liverpool, McCluskey was inspired to send a demo of the song to Factory Records founder Tony Wilson. They later heard that while he was not impressed with it, his wife was, so he bought it from them and released it as a single. Its ensuing success led to them receiving a seven-album record deal worth £250,000.\n\n\"Electricity\" was a hit with veteran DJ John Peel, who gave the song regular play on his late-night radio show; as a result, the British music press quickly picked up on the song. Adrian Thrills in the \"New Musical Express\" cited it as \"the best example of Factory Records to date – excellent, melodic, synthesiser pop.\" He also lauded B-side \"Almost\", calling it \"a doleful, heartsick slab of electronic angst.\"\n\nConversely, Garry Bushell gave a negative review in \"Sounds\", in which he remarked: \"If Mike Oldfield was ten years younger and a Tubeway Army fan, this is what he'd sound like – who wants to listen to a bunch of Scousers whining about electricity anyway?\" However, David Hepworth, who re-appraised the track in the same publication, opined that OMD's sound \"commands your attention\" and lauded the single for being \"packaged with as much taste as it's played.\" \"Electricity\" featured on the \"NME\" end-of-year list for 1979.\n\nRetrospectively, AllMusic critic Ned Raggett described the song as \"pure zeitgeist, a celebration of synth pop's incipient reign\". Colleague Dave Thompson called it a \"perfect electro-pop number\".\n\nVince Clarke cited \"Electricity\" as the track that sparked his interest in electronic music. In a BBC interview he said: \"When I was 18 or 19 I heard a single called 'Electricity' by Orchestral Manoeuvres in the Dark. It sounded so different from anything I'd heard; that really made me want to make electronic music, 'cause it was so unique.\" Simple Minds frontman Jim Kerr admitted to being \"downright jealous\" of the song.\n\nMultiple versions of \"Electricity\" exist; the earliest are recordings by McCluskey and Humphreys' previous group The Id.\n\nThere are many different versions of the two songs that were present on OMD's debut single. After the band left Factory Records, DinDisc attempted twice to score a hit with \"Electricity\". Consequently, four versions of \"Electricity\" and three of \"Almost\" exist.\n\n\n\n\n\n\n\"Electricity\" has been covered by the bands NOFX and MGMT.\n\nThe following singles were released:\n\n\"Electricity\" and \"Almost\" were released on the following OMD albums:\n\nThe sleeve was designed by Factory's designer Peter Saville. The band and Saville met in a Rochdale pub and exchanged ideas. Saville told them about a book of avant-garde musical scores which he'd come across. Andy McCluskey said that he sometimes wrote down the tunes he composed in a similar shorthand. This led to the unusual graphics that feature on the sleeve. Saville suggested to use shiny black ink on black paper. Neither OMD nor Tony Wilson believed it could be done, but Saville persuaded a printer to do the job. The thermographic printing was a success, but the place set on fire three times, so eventually only 5,000 sleeves were printed. The reissue sleeves were standard white on black printed sleeves.\n\n"}
{"id": "39826536", "url": "https://en.wikipedia.org/wiki?curid=39826536", "title": "Eleftherios Goulielmakis", "text": "Eleftherios Goulielmakis\n\nEleftherios Goulielmakis (Ελευθέριος Γουλιελμάκης) is a Greek physicist specializing in lasers.\n\nHe is a professor of physics at the University of Rostock, Germany and the head of the research group \"Attoelectronics\" at the Max Planck Institute of Quantum Optics in Garching, Germany.\n\nFor his work on the attosecond control and synthesis of light waves he has been awarded the Georgios Foteinos () Prize of the Academy of Athens in 2007, the International Union of Pure and Applied Physics Young Scientist Prize in Optics of the International Commission for Optics in 2009, the Gustav Hertz Prize of the Deutsche Physikalische Gesellschaft (DPG) in 2013. and the of the Justus-Liebig-University of Giessen in 2015.\n\n\n"}
{"id": "3195196", "url": "https://en.wikipedia.org/wiki?curid=3195196", "title": "FNRS-3", "text": "FNRS-3\n\nThe FNRS-3 or FNRS III is a bathyscaphe of the French Navy. It is currently presevered at Toulon. She set world depth records, competing against a more refined version of her design, the first bathyscaphe \"Trieste\". The French Navy eventually replaced her with the bathyscaphe FNRS-4, in the 1960s.\n\nAfter damage to the FNRS-2 during its sea trials in 1948, FNRS ran out of funding, and the submersible was sold to the French Navy, in 1950. She was subsequently substantially rebuilt and improved at Toulon naval base, and renamed FNRS-3. She was relaunched in 1953, under the command of Georges Houot, a French naval officer.\n\nOn 15 February 1954, she made a dive 160 miles off Dakar, Senegal in the Atlantic Ocean, beating Piccard's 1953 record, set by the \"Trieste\", by 900 meters. (the floor of the Mediterranean off Naples, ) This record was not exceeded until a workup dive by \"Trieste\" in 1959, working up to the record shattering Challenger Deep dive.\n\n\n"}
{"id": "50559168", "url": "https://en.wikipedia.org/wiki?curid=50559168", "title": "Faustovirus", "text": "Faustovirus\n\nFaustovirus is a genus of giant virus which infects amoebae associated with humans. The virus was first isolated in 2015 and shown be around 0.2 micrometers in diameter with a double stranded DNA genome of 466 kilobases predicted to encode 451 proteins. Unusually for a virus, faustovirus genes have been shown to contain introns.\n"}
{"id": "1582785", "url": "https://en.wikipedia.org/wiki?curid=1582785", "title": "Flexiviridae", "text": "Flexiviridae\n\nThe Flexiviridae were a new family of viruses in the 2004 classification of viruses, but have since the 2009 classification been split into the three new families \"Alphaflexiviridae\", \"Betaflexiviridae\" and \"Gammaflexiviridae\". These have in turn been subsumed under the new order \"Tymovirales\" along with the old family \"Tymoviridae\" by the International Committee on Taxonomy of Viruses based on molecular phylogenetic systematic analyses of proteins (RNA polymerase and viral coat). The viruses are positive-sense ssRNA viruses, placing them in Group IV of the Baltimore classification. These viruses are filamentous and named for being highly flexible. \nMembers of these families are readily transmitted mechanically and have other vectors of transmission. Species tend to be confined to a single host plant, many species preferring woody hosts, but a diversity of angiosperm hosts are known to the \"Flexiviridae\". Viral aggregates are known to form in the cytoplasm of plant cells.\n\n\n"}
{"id": "36578256", "url": "https://en.wikipedia.org/wiki?curid=36578256", "title": "Fundamental assessment", "text": "Fundamental assessment\n\nFunctional assessment is an ongoing process collecting information to understand the reason under a problem or target behavior. The function of the assessment is to prove and aid the effectiveness of the interventions or treatments used to help eliminate the problem behavior. Through functional assessments we have learned that there are complex patterns to people's seemingly unproductive behaviors. It is important to not only pay attention to consequences that follow the behavior but also the antecedent that evokes the behavior. More work needs to be done in the future with functional assessment including balancing precision and efficiency, being more specific with variables involved and a more smooth transition from assessment to intervention.\n\nFunctional assessment is a method developed by applied behavior analysis to identify the variables that maintain a problem behavior. Behavior is lawful. Whether it is desirable or undesirable, behavior is controlled by environmental variables. Behavior is a function of the antecedent and consequences that make up the three-term contingency. Functional assessment is the process of gathering information about the antecedent stimuli and consequences functional to the problem behavior. It attempts to provide an explanation to why the problem behavior may be occurring. The information about the antecedent stimuli may include the time and place, the presence of others and the frequency. The information collected helps identify which of the antecedent and consequences are maintaining the behavior. The information collected from functional assessment can also help develop appropriate treatments for the target behavior. Stimulus that may have been found to be reinforcing for the original behavior could be transferred to reinforce a more appropriate behavior.\n\nThe purpose of conducting a functional assessment is to identify the function of the target behavior. There are four main classes of functions of problem behavior.\n\nSocial positive reinforcement is when another person delivers a positive reinforcement after the problem behavior occurs. This is include the giving of attention, fun activities or goods and services provided by the person. An example of social positive reinforcement would be Max's mother (social) dropping what she is doing and provide attention (positive reinforcement) to her son when he engages in head banging on the wall (problem behavior).\n\nSocial negative reinforcement is when another person delivers a negative reinforcement after the problem behavior occurs. The person may terminate an aversive stimuli (interaction, task or activity) and the behavior is more likely to be maintained. An example of social negative reinforcement would be Max complains (problem behavior) to his parents (social) when he is asked to do chores, as a result, his parents allows him to escape the task (negative reinforcement).\n\nAutomatic positive reinforcement is when a positive reinforcement occurs automatically and is not mediated by another person. The behavior is strengthened by an automatic reinforcing consequence. An example of automatic positive reinforcement would be an autistic child waving his hands in front of his face (problem behavior) because the sensory stimulation (automatic positive reinforcement) produced is reinforcing for the child.\n\nAutomatic negative reinforcement is when a negative reinforcement occurs automatically reducing or eliminating an aversive stimulus as a reinforcing consequence of the behavior. A popular example of automatic negative reinforcement would be binge eating. Binge eating (problem behavior) had been found to temporarily reduce any unpleasant emotions the person may be experiencing before the binge (automatic negative reinforcement).\n\nThere are various different methods used to conduct functional assessment, all of which falls into three distinct categories.\n\nIndirect functional assessment methods use behavior interviews or surveys to gather information about the person exhibiting the behavior from themselves others who know this person well. The main advantage of indirect methods is they are easy and cheap to conduct and do not take much time. The main disadvantage of indirect methods is that the people involved are relying on their memories, thus some information may be lost or inaccurate.\n\nBecause of their convenience, indirect methods are used most commonly. It is essential assessment to be clear and objective as this will produce the most accurate answers without interpretation. The goal of the indirect assessment method is to generate information on the antecedent, behavior and consequence that can help generate a hypothesis about the variables that maintains the behavior.\n\nIndirect methods can help develop a correlation hypothesis but not a functional relationship.\n\nDirect observation methods involve is present to observe and record the problem behavior as it occurs. The goal of direct observation is to record the immediate antecedent and consequences that functions with the problem behavior within a natural environment. The main advantage of direct observation is that the antecedents and consequences are recorded as it happens instead of recollection of memory. Therefore, the information recorded is generally more accurate. The main disadvantage of direct observation is it requires a considerable amount of time and effort to implement. Another thing about direct observation is, like indirect methods, it can only demonstrate a correlation but not a functional relationship.\n\nThe observer of the direct observation method should be present in the natural environment when the problem behavior is most likely to occur. The observer should also be trained to record the problem behavior and its functional antecedent and consequences immediately, correctly and objectively.\n\nDirect observation can also be can ABC observation. Together with indirect methods, direct and indirect assessments are categorized as descriptive assessment because the antecedent and consequences are described from with memory of events. The information collected aids the development of a hypothesis, but to demonstrate a functional relationship, one must use the experimental method.\n\nExperimental methods involve manipulating either the antecedent or consequent variables to determine their influence on the problem behavior. This is the only method that can demonstrate a functional relationship between the antecedent stimulus or the reinforcing consequence and the problem behavior. The main advantage of the experimental method is the demonstration of a functional relationship. The main disadvantage of the experimental method is the extensive use of time and effort to create an experiment.\n\nExperimental methods can also be called experimental analysis or functional analysis.\n\n\"For more information please visit functional analysis.\"\n\nA functional assessment should always be conducted before treating a problem behavior. To develop appropriate treatment, one must have the correct information about the antecedents and consequences controlling the behavior because treatment involves manipulating these environmental events to evoke a change in the problem behavior. Here is the proper procedure to correctly implement a functional assessment.\n\n\nAfter a function assessment has been conducted with success, the information collected is used to develop appropriate treatments and interventions. Interventions are designed to manipulate the antecedent or/and the consequence of the problem behavior to decrease its occurrence rate and increase the rate of occurrence for more desirable behaviors.\n\nFunctional interventions include extinction, differential reinforcement and antecedent manipulations. These intervention are functional because they deal with the environmental events that are functional to the problem behavior. They are also nonaversive as punishment is not involved.\n\nMore aversive interventions can be used as latter resort if previous nonaversive intervention have been tried and shown ineffective. Punishment such as time-out and response cost are considered negative punishment, which although is still controversial, is more widely accepted than positive punishment such as overcorrection, contingent exercise, guided compliance and physical restraint. As mentioned punishment should only be used as a last resort when other methods have already been considered.\n\nA lot of research being done with functional assessment deals with self injurious behaviors of mentally challenged children or adults and autistic children.\n\nCarr, Newsom and Binkoff conducted an experimental method of functional assessment on the two boys with intellectual disabilities exhibiting aggressive behaviors. They hypothesized that their aggressive behaviors were maintained by escape from academic tasks. To test their hypothesis, they set two different experimental conditions; 1. Academic demands were put on the boys, 2. Academic demands were not put on the children. If their hypothesis is true, then the problem behavior should occur much more often in the first condition than the second. Results show that their hypothesis was indeed true as the aggressive behavior occurred at a much higher frequency in the first condition. The researchers concluded that the boy's problem behavior was indeed maintained by the antecedent of academic demands and the consequence of escape from the demands.\n\nAnother functional assessment research was done by Iwata in 1982 worked with children with developmental disabilities showing self injurious behaviors. The research could not conclude what was maintaining their behavior but believed it either adult attention, escape from demands or sensory stimulation from the injuries. For each of the hypothesis, they created a condition that would fit into the category. For adult attention hypothesis, they created an environment where an adult is in the room with the child but pays no attention to him/her until after the behavior occurs. For the escape from demands hypothesis, they had an adult make a normal demand towards the child, but terminate it if the self injurious behavior occurs. For the sensory stimulation hypothesis, the child is left alone without the presence of anyone or any stimulating activities. Iwata compared the levels of self injurious behaviors across the three conditions and demonstrated that the function of the problem behavior for each child was different. Some wanted attention, others escape while some were maintained by automatic reinforcement. As shown here, it is very important to conduct a functional assessment to determine what exactly is maintaining the behavior before any function interventions are taken.\n\n"}
{"id": "26601142", "url": "https://en.wikipedia.org/wiki?curid=26601142", "title": "Geographische Zeitschrift", "text": "Geographische Zeitschrift\n\nThe Geographische Zeitschrift (English: \"The Geographical Journal\") is a German peer-reviewed academic journal specialising in human geography. It was established in 1895 and is now published by the Franz Steiner Verlag.\n"}
{"id": "39007567", "url": "https://en.wikipedia.org/wiki?curid=39007567", "title": "Gisela Mosig", "text": "Gisela Mosig\n\nGisela Mosig (November 29, 1930 – January 12, 2003) was a German-American molecular biologist best known for her work with enterobacteria phage T4. She was among the first investigators to recognize the importance of recombination intermediates in establishing new DNA replication forks, a fundamental process in DNA replication.\n\nWhile growing up on a farm in Saxony, Mosig became interested in biology and physics. \n\nAfter World War II (when she was 14 years old), the region where she lived became part of East Germany and evolutionary teaching in her high school skewed toward Lysenkoism. Finding the intellectual atmosphere intolerable, she fled to the west on her bicycle with only the belongings she could carry. After undergraduate studies at the University of Bonn, she earned her doctoral degree in plant genetics at the University of Cologne in 1959.\n\nFrom there, she was recruited to Vanderbilt University to study bacteriophage T4, a topic for which she became a leading investigator. After postdoctoral research at Vanderbilt and then the Carnegie Institute of Washington at Cold Spring Harbor (with Nobel laureate A. D. Hershey), she returned to Vanderbilt as a faculty member in 1965, and became a citizen of the United States of America in 1968. Mosig served on the Vanderbilt faculty until her death in 2003.\n\n\nMosig died at Alive Hospice in Nashville a few years after being diagnosed with metastatic ovarian cancer. She was 72 years old. In her will she endowed a fund to support scholarly travel for Vanderbilt graduate students in the biological sciences.\n\n"}
{"id": "41358224", "url": "https://en.wikipedia.org/wiki?curid=41358224", "title": "HST events", "text": "HST events\n\nHST events, things of interest related to the Hubble Space Telescope, chronologically.\n\n\n\n\n\n\n"}
{"id": "37131939", "url": "https://en.wikipedia.org/wiki?curid=37131939", "title": "Jacques Sébastien François Léonce Marie Paul Fagot", "text": "Jacques Sébastien François Léonce Marie Paul Fagot\n\nJacques Sébastien François Léonce Marie Paul Fagot (1842–1908) was a French malacologist who often published under the name Paul Fagot.\n\nFagot was part of a \"New School\" of naturalists, which included Jules-René Bourguignat, Aristide-Horace Letourneux, Jules François Mabille, and Étienne Alexandre Arnould Locard. Species of land snails that were named and described by Fagot include \"Aegopinella epipedostoma\", \"Pyrenaearia navasi\", and \"Pyrenaearia cotiellae\".\n\n"}
{"id": "46615453", "url": "https://en.wikipedia.org/wiki?curid=46615453", "title": "João Pedro de Magalhães", "text": "João Pedro de Magalhães\n\nJoão Pedro de Magalhães is a Portuguese microbiologist at the University of Liverpool. His lab at the University of Liverpool studies aging through both computational and experimental approaches. His ultimate goal is to cure human aging.\n\nIn 1999, he obtained his degree in Microbiology from Escola Superior de Biotecnologia. Under Olivier Toussaint, he obtained his PhD from the University of Namur in 2004. Then he did a postdoc in the George Church lab from 2004 to 2008.\n\nHe helps maintain several databases on aging - among them - GenAge, AnAge, DrugAge, CellAge, GenDR, the Digital Aging Atlas, and Who's Who in Gerontology. His research group helped sequence the transcriptome of the long-lived bowhead whale. He also helps advise the Lifeboat Foundation.\n\nAmong his many longevity-related scientific research projects, Magalhães has sequenced and analyzed the genome of the bowhead whale. And he has also contributed to analysis of the genome of the naked mole rat. Both of these mammals are exceptionally long-lived and exceptionally cancer-resistant.\n\n"}
{"id": "26449007", "url": "https://en.wikipedia.org/wiki?curid=26449007", "title": "Katharine Bement Davis", "text": "Katharine Bement Davis\n\nKatharine Bement Davis (January 15, 1860 – December 10, 1935) was an American progressive era social reformer and criminologist who became the first woman to head a major New York City agency when she was appointed Correction Commissioner on January 1, 1914.\n\nThe Panama-Pacific Exposition designated her one of the three most distinguished women in America.\nDavis is remembered for her pioneering science-based prison reform and groundbreaking research about female sexuality.\n\nShe was born in Buffalo, New York, on January 15, 1860, to Oscar Bill Davis and Frances Freeman. She was the oldest of five children—three girls and two boys. Katharine's mother Frances was a strong proponent of women's rights and a zealous advocate for women's suffrage.\n\nThe Davis family lived in Dunkirk, New York, during most of her childhood. Both of her parents were active in community organizations in Dunkirk. Oscar Davis worked for Bradstreet company, and when Katharine was seventeen he relocated to Rochester, New York, to manage a regional office there.\n\nIn 1879, Davis graduated from Rochester Free Academy, a public high school. Since her family was unable to afford the tuition for college, she taught at Dunkirk Academy. While at Dunkirk Academy, she established a women's equality club and led a women's literacy group.\n\nAfter teaching high school chemistry for ten years, Davis saved enough money to continue her schooling full-time. In 1890, she enrolled in Vassar College, a center of progressive education for women. Davis combined her interests in science and social reform by studying food chemistry and nutritional studies. Taking this course of study gave Davis the opportunity for career in a public health agency after graduation.\n\nAfter graduating from Vassar, Davis continued her studies at Columbia University's Barnard College, a separate women's college within the university, while teaching at Brooklyn Heights Seminary for Girls. She studied the chemistry of food.\n\nWhile studying at Barnard, Davis managed a project to develop a model home for New York State's display at the Chicago World Fair. As a result of the success of the home display, she was offered a job running a settlement house in Philadelphia.\n\nThe settlement house movement began in the late 19th century to socialize the poor and uneducated residents that occupied immigrant neighborhoods in a city to American values and customs. In the United States, \"college settlements\" were run by young women graduates of progressive colleges who as \"workers\" moved into a neighborhood house in order to Americanize the local residents.\n\nThe College Settlement on Philadelphia's St. Mary's Street served a district of indigent blacks and Russian immigrants. Davis moved into the house as \"head worker\". While in Philadelphia, Davis worked with W. E. B. Du Bois, who was then at the University of Pennsylvania conducting groundbreaking research on blacks in urban America. Davis noted in the Settlement's annual report that \"the investigation into the condition of the colored people of . . . the seventh ward which contains about 10,000 Negroes, nearly one fourth the entire number in the city\" would be carried out \"by means of house-to-house canvass.\"\n\nDavis moved to Chicago, Illinois to return to school at Chicago University where she was to be the first female Fellow in Political Science-Economics to earn a Ph.D.\n\nThe New York State Reformatory for Women at Bedford Hills, New York opened in May 1901 with Davis as the superintendent. Besides the administration building, the campus included \"a reception hall, four cottages, a laundry building, a powerhouse, a gate house, and a stable\". The reception hall had two wings, one wing modeled after a traditional prison with three tiers of 24 cells each, and the other wing contained rooms accommodating 42 inmates.\n\nIn 1909 Davis arranged for the New York Public Education Association to do a psychological testing study at Bedford. The results showed that serious mental problems existed in a significant number of the female inmates. This result indicated a disconnect between the original focus of the reformatory and the population of inmates that it was serving. In 1910 Davis advocated for judges to have access to pre-sentencing background research and evaluations so that they can made appropriate placements. Davis wrote up her ideas in a pamphlet, \"A Rational Plan for the Treatment of Women Convicted in the Courts of New York City.\" that was widely distributed to people affiliated with criminal justice system in New York City.\n\nThe Prison Association of New York recognized the reforms that Davis made at Bedford Hills saying that \"the reformatory is becoming perhaps the most scientific institution of its kind in the world.\"\n\nReform-minded Mayor John Purroy Mitchel selected Davis to head the Correction Commission making her the first woman to lead an agency in New York City on January 1, 1914. Based on her relationship with Mitchel and his associates, Davis was on the Progressive party's 1914 slate for State Constitutional Convention seat, making her the first woman to run for a New York statewide office on a major party ticket.\n\nThe Bureau of Social Hygiene in New York City was an agency incorporated by John D. Rockefeller, Jr. in 1913 for \"the study, amelioration, and prevention of those social conditions, crimes, and diseases which adversely affect the well-being of society, with special reference to prostitution and the evils associated therewith.\" Rockefeller established the agency as a result of his appointment in 1911 to a special grand jury to investigate white slavery in New York City. The Bureau's work was influenced by the view that there was a biological basis for crime. During the Bureau's early years the main focus was on prostitution, vice, and political corruption. During later years, the Bureau shifted its emphasis towards criminology and the control of crime. The Bureau stopped granting new appropriations in 1934, and by 1937 all prior commitments were completed.\n\nIn 1918 Davis became the head of the Bureau.\n\nAt the Bureau of Social Hygiene, Davis arranged for ground-breaking research on women's sexuality on \"normal\" females. These were women whose names were found from club memberships directories and college alumnae lists. They were surveyed about \"auto-erotic practices, the frequency of sexual desire, homosexual experiences, use of contraceptives, frequency of sexual intercourse, pre-marital and extra-marital sexual experiences.\"\n\nCompulsory sterilization was seen by many as a way to reduce the incidence of mental illness and mental retardation in the general population of the United States. Davis was an eugenicist and during her tenure as General Secretary, she affiliated the Bureau with leaders in the field of eugenics such as Harry Laughlin, Charles Davenport and E. S. Gosney, director of The Human Betterment Foundation in California. In 1924, Davis accepted a position on the Committee on Eugenics of the United States' Advisory Council.\n\nAt the end of 1927 her contract was not extended and she retired. On February 2, 1928, the Waldorf-Astoria ballroom was filled with Progressive Era reformers to honor Davis at a testimonial dinner. The guests including Eleanor Roosevelt, Jane Addams, Carrie Chapman Catt, Rev. Harry Emerson Fosdick, Walter Lippman, Judge William McAdoo, John D. Rockefeller Jr., Lillian Wald and Felix Warburg.\n\nDavis retired to Pacific Grove, California with her sisters. Davis died December 10, 1935 from cerebral arteriosclerosis at her home.\n\nDuring her lifetime Davis was honored numerous times by a variety of organizations. The Panama-Pacific Exposition designated her one of the three most distinguished women in America. Davis received honorary degrees from Mount Holyoke College, Western Reserve and Yale universities.\n\n\n"}
{"id": "21561303", "url": "https://en.wikipedia.org/wiki?curid=21561303", "title": "Law of increasing costs", "text": "Law of increasing costs\n\nIn economics, the law of increasing costs is a principle that states that once all factors of production (land, labor, capital) are at maximum output and efficiency, producing more will cost more than average. As production increases, the opportunity cost does as well. The best way to look at this is to review an example of an economy that only produces two things - cars and oranges. If all the resources of the economy are put into producing only oranges, there will not be any factors of production available to produce cars. So the result is an output of X number of oranges but 0 cars. The reverse is also true - if all the factors of production are used for the production of cars, 0 oranges will be produced. In between these two extremes are situations where some oranges and some cars are produced. There are three assumptions that are made in this possibility. The economy is experiencing full employment (everyone who wants to work has a job), the best technology is being used and production efficiency is being maximized. So the question becomes, what is the cost of producing more oranges or cars? If the economy is at the maximum for all inputs, then the cost of each unit will be more expensive. The economy will have to incur more variable costs, such as overtime, to produce the unit.\n\nThe law also applies to \"switching\" production in a maxed out economy. Essentially, the economy is still producing more, so the law still applies. The only difference is that resources are being taken from one area and applied to another, instead of simply producing more of the same (as assumed in the first paragraph)\n"}
{"id": "37969019", "url": "https://en.wikipedia.org/wiki?curid=37969019", "title": "List of Procellariiformes by population", "text": "List of Procellariiformes by population\n\nThis is a list of Procellariiformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is not comprehensive, as not all Procellariiformes have had their numbers quantified.\n"}
{"id": "13546580", "url": "https://en.wikipedia.org/wiki?curid=13546580", "title": "List of Schedule II drugs (US)", "text": "List of Schedule II drugs (US)\n\nThis is the list of Schedule II drugs as defined by the United States Controlled Substances Act.\nThe following findings are required for drugs to be placed in this schedule:\n\nThe complete list of Schedule II drugs follows. The Administrative Controlled Substances Code Number for each drug is included.\n"}
{"id": "401788", "url": "https://en.wikipedia.org/wiki?curid=401788", "title": "List of U.S. state and territory mottos", "text": "List of U.S. state and territory mottos\n\nAll of the United States' 50 states have a state motto, as do the District of Columbia and three US territories. A motto is a phrase intended to formally describe the general motivation or intention of an organization. State mottos can sometimes be found on state seals or state flags. Some states have officially designated a state motto by an act of the state legislature, whereas other states have the motto only as an element of their seals. The motto of the United States itself is \"In God We Trust\", proclaimed by Congress and signed into law by President Dwight D. Eisenhower on July 30, 1956. The motto \"E Pluribus Unum\" (Latin for \"One from many\") was approved for use on the Great Seal of the United States in 1782, but was never adopted as the national motto through legislative action.\n\nSouth Carolina has two official mottos, both of which are in Latin. Kentucky, North Dakota, and Vermont also have two mottos, one in Latin and the other in English. All other states and territories have only one motto, except Guam and the Northern Mariana Islands, which do not have any mottos. English and Latin are the most-used languages for state mottos, each used by 25 states and territories. Seven states and territories use another language, of which each language is only used once. Eight states and two territories have their mottos on their state quarter; thirty-eight states and four territories have their mottos on their state seals.\n\nThe dates given are, where possible, the earliest date that the motto was used in an official sense. Some state mottos are not official but are on the official state seal; in these cases the adoption date of the seal is given. The earliest use of a current motto is that of Puerto Rico, \"Johannes est nomen ejus\", granted to the island by the Spanish in 1511.\n\n\n\n"}
{"id": "2856792", "url": "https://en.wikipedia.org/wiki?curid=2856792", "title": "List of computer criminals", "text": "List of computer criminals\n\nConvicted computer criminals are people who are caught and convicted of computer crimes such as breaking into computers or computer networks. Computer crime can be broadly defined as criminal activity involving information technology infrastructure, including illegal access (unauthorized access), illegal interception (by technical means of non-public transmissions of computer data to, from or within a computer system), data interference (unauthorized damaging, deletion, deterioration, alteration or suppression of computer data), systems interference (interfering with the functioning of a computer system by inputting, transmitting, damaging, deleting, deteriorating, altering or suppressing computer data), misuse of devices, forgery (or identity theft) and electronic fraud.\n\nIn the infancy of the hacker subculture and the computer underground, criminal convictions were rare because there was an informal code of ethics that was followed by white hat hackers. Proponents of hacking claim to be motivated by artistic and political ends, but are often unconcerned about the use of criminal means to achieve them. White hat hackers break past computer security for non-malicious reasons and do no damage, akin to breaking into a house and looking around. They enjoy learning and working with computer systems, and by this experience gain a deeper understanding of electronic security. As the computer industry matured, individuals with malicious intentions (black hats) would emerge to exploit computer systems for their own personal profit.\n\nConvictions of computer crimes, or hacking, began as early as 1983 with the case of The 414s from the 414 area code in Milwaukee. In that case, six teenagers broke into a number of high-profile computer systems, including Los Alamos National Laboratory, Sloan-Kettering Cancer Center and Security Pacific Bank. On May 1, 1983, one of the 414s, Gerald Wondra, was sentenced to two years of probation.\n\nIn 2006, a prison term of nearly five years was handed down to Jeanson James Ancheta, who created hundreds of zombie computers to do his bidding via giant bot networks or botnets. He then sold the botnets to the highest bidder who in turn used them for Denial-of-service (DoS) attacks.\n\n, the longest sentence for computer crimes is that of Albert Gonzalez for 20 years.\n\nThe next longest sentences are those of 13 years for Blake Easterling, 108 months of Brian Salcedo in 2004 and upheld in 2006 by the U.S. 4th Circuit Court of Appeals, and 68 months of Kevin Mitnick in 1999.\n\n\n"}
{"id": "17797269", "url": "https://en.wikipedia.org/wiki?curid=17797269", "title": "List of countries by labour force", "text": "List of countries by labour force\n\nThis is a list of countries by size of the labour force mostly based on The World Factbook.\n"}
{"id": "8426398", "url": "https://en.wikipedia.org/wiki?curid=8426398", "title": "List of software for the TRS-80", "text": "List of software for the TRS-80\n\nThe TRS-80 series of computers were sold via Radio Shack & Tandy dealers in North America and Europe in the early 1980s. Much software was developed for these computers, particularly the relatively successful Color Computer I, II & III models, which were designed for both home office and entertainment (gaming) uses.\n\nA list of software for the TRS-80 computer series appears below. This list includes software that was sold labelled as a Radio Shack or Tandy product.\n\nTRS-80 Color Computer\n\nMany of these titles also ran on the Model I, as the Model III was designed to be backward-compatible with the Model I.\n"}
{"id": "98902", "url": "https://en.wikipedia.org/wiki?curid=98902", "title": "List of tropical cyclones", "text": "List of tropical cyclones\n\nThis is a list of tropical cyclones, subdivided by basin. See the list of tropical cyclone records for individual records set by individual tropical cyclones.\n\n\n"}
{"id": "14253403", "url": "https://en.wikipedia.org/wiki?curid=14253403", "title": "Maloca", "text": "Maloca\n\nA maloca is an ancestral long house used by the natives of the Amazon, notably in Colombia and Brazil. Each community has a maloca with its own unique characteristics. Several families with patrilineal relations live together in a maloca, distributed around the long house in different compartments. In general, the chief of the local descent group lives in the compartment nearest to the back wall of the long house. As well, each family has its own furnace.\n\nDuring festivals and in formal ceremonies, which involve dances for males, the long house space is rearranged; the centre of the long house is the most important area where the dance takes place. Each maloca has two entrances, for men and for women. Married men and women sleep together, and unmarried men sleep separately, as do unmarried women. A maloca is traditionally surrounded with two gardens: the inner called the kitchen gardens (growing plants such as bananas, papaya, mango and pineapple) and the manioc gardens growing manioc (yuca).\n\nFor many years, these long houses were Jesuit missionaries’ objects of attack. \n"}
{"id": "51494308", "url": "https://en.wikipedia.org/wiki?curid=51494308", "title": "NGC 166", "text": "NGC 166\n\nNGC 166 (also known as PGC 2143) is a spiral galaxy located around 2.6 million light-years away in the constellation Cetus, with an apparent magnitude of 15.18. It was discovered by Francis Preserved Leavenworth in 1886.\n\n\n"}
{"id": "2051812", "url": "https://en.wikipedia.org/wiki?curid=2051812", "title": "Proportional control", "text": "Proportional control\n\nProportional control, in engineering and process control, is a type of linear feedback control system in which a correction is applied to the controlled variable which is proportional to the difference between the desired value (set point, SP) and the measured value (process value, PV). Two classic mechanical examples are the toilet bowl and the fly-ball governor. \n\nThe proportnal control concept is more complex than an on–off control system like a bi-metallic domestic thermostat, but simpler than a proportional–integral–derivative (PID) control system used in something like an automobile cruise control. On–off control will work where the overall system has a relatively long response time, but can result in instability if the system being controlled has a rapid response time. Proportional control overcomes this by modulating the output to the controlling device, such as a control valve at a level which avoids instability, but applies correction as fast as practicable by applying the optimum quantity of proportional gain.\n\nA drawback of proportional control is that it cannot eliminate the residual SP − PV error in processes with compensation e.g. temperature control, as it requires an error to generate a proportional output. To overcome this the PI controller was devised, which uses a proportional term (P) to remove the gross error, and an integral term (I) to eliminate the residual offset error by integrating the error over time to produce an \"I\" component for the controller output.\n\nIn the proportional control algorithm, the controller output is proportional to the error signal, which is the difference between the setpoint and the process variable. In other words, the output of a proportional controller is the multiplication product of the error signal and the proportional gain.\n\nThis can be mathematically expressed as,\n\nwhere\nConstraints: In a real plant, actuators have physical limitations that can be expressed as constraints on formula_3. For example, formula_3 may be bounded between −1 and +1 if those are the maximum output limits.\n\nQualifications: It is preferable to express formula_4 as a unitless number. To do this, we can express formula_5 as a ratio with the span of the instrument. This span is in the same units as error (e.g. C degrees) so the ratio has no units.\n\nProportional control dictates formula_11. From the block diagram shown, assume that \"r\", the setpoint, is the flowrate into a tank and \"e\" is \"error\", which is the difference between setpoint and measured process output. formula_12 is process transfer function; the input into the block is flow rate and output is tank level. \n\nThe output as a function of the setpoint, \"r\", is known as the \"closed-loop transfer function\".\nformula_13 If the poles of formula_14 are stable, then the closed-loop system is stable.\n\nProportional control cannot eliminate the offset error, which is the difference between the desired value and the actual value, SP − PV error, as it requires an error to generate an output. When a disturbance (deviation from existing state) occurs in the process value being controlled, any corrective control action, based purely on Proportional Control, will always leave out the error between the next steady state and the desired setpoint, and result in a residual error called the offset error. This error will increase as greater process demand is put on the system, or by increasing the set point.\n\nConsider an object suspended by a spring as a simple proportional control. The spring will attempt to maintain the object in a certain location despite disturbances which may temporarily displace it. Hooke's law tells us that the spring applies a corrective force that is proportional to the object's displacement. While this will tend to hold the object in a particular location, the absolute resting location of the object will vary if its mass is changed. This difference in resting location is the offset error.\n\nImagine the same spring and object in a weightless environment. In this case, the spring will tend to hold the object in the same location regardless of its mass. There is no offset error in this case because the proportional action is not working against anything in the steady state.\n\nThe proportional band is the band of controller output over which the final control element (a control valve, for instance) will move from one extreme to another. Mathematically, it can be expressed as:\n\nformula_15 <br>\n\nSo if formula_4, the proportional gain, is very high, the proportional band is very small, which means that the band of controller output over which the final control element will go from minimum to maximum (or vice versa) is very small. This is the case with on–off controllers, where formula_4 is very high and hence, for even a small error, the controller output is driven from one extreme to another.\n\nThe car advae of proportional over on–off control can be demonstrated by car speed control. An analogy to on–off control is driving a car by applying either full power or no power and varying the duty cycle, to control speed. The power would be on until the target speed is reached, and then the power would be removed, so the car reduces speed. When the speed falls below the target, with a certain hysteresis, full power would again be applied. It can be seen that this would obviously result in poor control and large variations in speed. The more powerful the engine; the greater the instability, the heavier the car; the greater the stability. Stability may be expressed as correlating to the power-to-weight ratio of the vehicle. \n\nIn proportional control, the power output is always proportional to the (actual versus target speed) error. If the car is at target speed and the speed increases slightly due to a falling gradient, the power is reduced slightly, or in proportion to the change in error, so that the car reduces speed gradually and reaches the new target point with very little, if any, \"overshoot\", which is much smoother control than on–off control. In practice, PID controllers are used for this and the large number of control processes that require more response control than proportional alone.\n\n\n"}
{"id": "23755052", "url": "https://en.wikipedia.org/wiki?curid=23755052", "title": "Recession shapes", "text": "Recession shapes\n\nRecession shapes are used by economists to describe different types of recessions. There is no specific academic theory or classification system for recession shapes; rather the terminology is used as an informal shorthand to characterize recessions and their recoveries.\n\nThe most commonly used terms are V-shaped, U-shaped, W-shaped, and L-shaped recessions. The shapes take their names from the approximate shape economic data make in graphs during recessions. The letters can also be applied referring to the recoveries (\"ie\" \"V-shaped recovery\").\n\nIn a V-shaped recession, the economy suffers a sharp but brief period of economic decline with a clearly defined trough, followed by a strong recovery. V-shapes are the normal shape for recession: \"There is a strong historical “snap back” relationship between the strength of economic recovery and the severity of the preceding recession. Thus, recessions and their recoveries have a tendency to trace out a “V” shape.\"\n\nA clear example of a v-shaped recession is the Recession of 1953 in the United States. In the early 1950s the economy in the United States was booming, but because the Federal Reserve expected inflation it raised interest rates, tipping the economy into recession. In 1953 growth began to slow, in the third quarter, the economy shrank by 2.4 percent. In the fourth quarter the economy shrank by 6.2 percent, and in the first quarter of 1954 it shrank by 2 percent before returning to growth. By the fourth quarter of 1954, the economy was growing at an 8 percent pace, well above the trend. Thus GDP \"growth\" for this recession forms a classic v-shape. More importantly, the GDP graph itself has a V shape.\n\nA U-shaped recession is longer than a V-shaped recession, and has a less-clearly defined trough. GDP may shrink for several quarters, and only slowly return to trend growth. Simon Johnson, former chief economist for the International Monetary Fund, says a U-shaped recession is like a bathtub: \"You go in. You stay in. The sides are slippery. You know, maybe there's some bumpy stuff in the bottom, but you don't come out of the bathtub for a long time.\"\n\nThe 1973–75 recession in the United States can be considered a U-shaped recession. In early 1973 the economy began to shrink and continued to decline or have very low growth for nearly two years. After bumping along the bottom, the economy climbed back to recovery in 1975.\n\nIn a W-shaped recession, (also known as a double-dip recession), the economy falls into recession, recovers with a short period of growth, then falls back into recession before finally recovering, giving a \"down up down up\" pattern resembling the letter W.\n\nThe early 1980s recession in the United States is cited as an example of a W-shaped recession. The National Bureau of Economic Research considers two recessions to have occurred in the early 1980s. The economy fell into recession from January 1980 to July 1980, shrinking at an 8 percent annual rate from April to June 1980. The economy then entered a quick period of growth, and in the first three months of 1981 grew at an 8.4 percent annual rate. As the Federal Reserve under Paul Volcker raised interest rates to fight inflation, the economy dipped back into recession (hence, the \"double dip\") from July 1981 to November 1982. The economy then entered a period of mostly robust growth for the rest of the decade.\n\nThe European debt crisis in the early-2010s is a more recent example of a W-shaped recession. A combination of government austerity, falling business investment, rising interest rates, global economic weakness, high energy prices, and weak consumer spending after the Great Recession of 2008-2009 tipped many Eurozone countries into a second recession from 2011 to 2013. Countries affected included Italy, Spain, Portugal, France, Ireland, Germany, Cyprus, and Portugal. Greece, while part of the Eurozone, saw continuous economic contraction from 2007 to 2015, and thus does not fit the definition of a W-shaped recession, but rather an L-shaped recession. The United Kingdom is not a member of the Eurozone, but is a member of the European Union, and suffered a minor W-shaped recession when GDP contracted in Q4 2011 and Q1 2012.\n\nAn L-shaped recession or depression occurs when an economy has a severe recession and does not return to trend line growth for many years, if ever. The steep drop or degrowth, is followed by a flat line makes the shape of an L. This is the most severe of the different shapes of recession. Alternative terms for long periods of underperformance include \"depression\" and lost decade; compare also \"malaise\".\n\nThe Greek recession of 2007–2016 could be considered an example of an L-shaped recession, as Greek GDP growth in 2017 was merely 1.6%. Greece technically suffered through four separate, but compounding, periods of contractions over the 9 year period.\n\nSince the terms are informal, commentators will sprinkle in other descriptions. Financial blogger Mike Shedlock describes a WW-shaped recession with the country \"slipping in and out of recession for a prolonged period of time, perhaps 3-4 years or more.\" Financier George Soros has said the current recession may be an \"inverted square root sign\"-shaped recession. Soros explained to Reuters that this meant: \"You hit bottom and you automatically rebound some, but then you don't come out of it in a V-shape recovery or anything like that. You settle down — step down.\" During the 2001 recession, a trader named Thierry Martin writing for Jim Cramer's TheStreet.com proposed a J-shaped recovery, which he says was \"strictly for optimists\" and could be caused by \"a new high-tech product category, or even a series of improving earnings reports, [that] could send us sky-high without looking back.\" Note that the J-shaped economic recovery is unrelated to the J-curve phenomenon.\n\n"}
{"id": "40697145", "url": "https://en.wikipedia.org/wiki?curid=40697145", "title": "Ring of Bright Water", "text": "Ring of Bright Water\n\nRing of Bright Water is a book by Gavin Maxwell about his life in a remote house in coastal Scotland where he kept several wild otters as pets. First published in 1960, it became a best seller and is considered a literary masterpiece, eventually selling over two million copies. A fictionalised film of the same name was made from it and released in 1969.\n\nThe book describes how Maxwell brought a smooth-coated otter back from Iraq and raised it at \"Camusfeàrna\" (the name he used for his house at Sandaig near Glenelg), on the west coast of Scotland. He took the otter, called Mijbil, to the London Zoological Society, where it was decided that this was a previously unknown subspecies, and it was named after him: \"Lutrogale perspicillata maxwelli\" (or colloquially, \"Maxwell's otter\").\n\nThe book's title was taken from a poem by Kathleen Raine, who claimed in her autobiography that Maxwell had been the love of her life. Her relationship with Maxwell deteriorated after 1956 when she indirectly caused the death of Mijbil.\n\nA reviewer in the \"Sunday Herald\" described the book as having \"inspired a generation of naturalists\" and referred to it as a \"classic account of man and wildlife\". The review calls \"Ring of Bright Water\" \"one of the most popular wildlife books ever written\", as over two million copies had been sold worldwide by 1999.\n\nTwo sequels were published: \"The Rocks Remain\" (1963) and \"Raven Seek Thy Brother\" (1968), which were less idyllic than the first, chronicling accidents and misfortunes involving both the otters and Maxwell's life. All three books were republished as \"Ring of Bright Water: A Trilogy\" in 2011 by Nonpareil Books. The trilogy does not include the full text of the latter two volumes, but removes the tangential travel sections which take place outside Scotland.\n"}
{"id": "47119138", "url": "https://en.wikipedia.org/wiki?curid=47119138", "title": "Robert Clark (physicist)", "text": "Robert Clark (physicist)\n\nRobert (Bob) Clark is an Australian physicist. He was appointed Professor and Chair of Energy Strategy and Policy at University of New South Wales (UNSW) in 2012. Prior to this he was Chief Defence Scientist from 2008 to 2011 and Professor of Experimental Physics at University of New South Wales, where he established the National Magnet Laboratory and Semiconductor Nanofabrication Facility.\n\nClark joined the Royal Australian Navy as a Cadet Midshipman in 1969. He graduated from the RAN college with a Bachelor of Science from the University of New South Wales, then served on eight ships before leaving the navy in 1979. He holds an MA from Oxford and a PhD from the University of New South Wales. He took several positions at UNSW, culminating in Director of the Australian Research Council Centre of Excellence for Quantum Computer Technology, before taking on the role of Chief Defence Scientist.\n\nClark was appointed an Officer of the Order of Australia on Australia Day 2013 \"for distinguished service to science and technology through leadership and governance of the scientific community of the Australian Defence Force and through contributions to quantum computing and nanotechnology\". and was awarded the Centenary Medal on 1 January 2001 \"for contribution to world leading research in the field of quantum computing and physics\". He is a Fellow of the Australian Academy of Science (2001), a Distinguished Fellow of the Royal Society of New South Wales (2009), and has received numerous other acknowledgements and awards. \n"}
{"id": "2313411", "url": "https://en.wikipedia.org/wiki?curid=2313411", "title": "STS-3xx", "text": "STS-3xx\n\nSpace Shuttle missions designated STS-3xx (officially called Launch On Need (LON) missions) were rescue missions which would have been mounted to rescue the crew of a Space Shuttle if their vehicle was damaged and deemed unable to make a successful reentry. Such a mission would have been flown if Mission Control determined that the heat shielding tiles and reinforced carbon-carbon panels of a currently flying orbiter were damaged beyond the repair capabilities of the available on-orbit repair methods. These missions were also referred to as Launch on Demand (LOD) and Contingency Shuttle Crew Support. The program was initiated following loss of Space Shuttle \"Columbia\" in 2003. No mission of this type was launched during the Space Shuttle program.\n\nThe orbiter and four of the crew which were due to fly the next planned mission would be retasked to the rescue mission. The planning and training processes for a rescue flight would allow NASA to launch the mission within a period of 40 days of its being called up. During that time the damaged (or disabled) shuttle's crew would have to take refuge on the International Space Station (ISS). The ISS is able to support both crews for around 80 days, with oxygen supply being the limiting factor. Within NASA, this plan for maintaining the shuttle crew at the ISS is known as Contingency Shuttle Crew Support (CSCS) operations. Up to STS-121 all rescue missions were to be designated STS-300.\n\nIn the case of an abort to orbit, where the shuttle is unable to reach the ISS orbit and the thermal protection system inspections suggest the shuttle cannot return to Earth safely, the ISS may be capable of descent down to meet the shuttle. Such a procedure is known as a joint underspeed recovery.\n\nTo save weight, and to allow the combined crews of both shuttles to return to Earth safely, many shortcuts would have to be made, and the risks of launching another orbiter without resolving the failure which caused the previous orbiter to become disabled would have to be faced.\n\nA number of pieces of Launch on Need flight hardware were built in preparation for a rescue mission including:\n\nThe Remote Control Orbiter (RCO), also known as the Autonomous Orbiter Rapid Prototype (AORP), was a term used by NASA to describe a shuttle that could perform entry and landing without a human crew on board via remote control. NASA developed the RCO in-flight maintenance (IFM) cable to extend existing auto-land capabilities of the shuttle to allow remaining tasks to be completed from the ground. The purpose of the RCO IFM cable was to provide an electrical signal connection between the Ground Command Interface Logic (GCIL) and the flight deck panel switches. The cable is approximately long, weighs over , and has 16 connectors. With this system, signals could be sent from the Mission Control Center to the unmanned shuttle to control the following systems:\n\nThe RCO IFM cable first flew aboard STS-121 and was transferred to the ISS for storage during the mission. The cable remained aboard the ISS until the end of the Shuttle program. Prior to STS-121 the plan was for the damaged shuttle to be abandoned and allowed to burn up on reentry. The prime landing site for an RCO orbiter would be Vandenberg Air Force Base in California. Edwards Air Force Base, a site already used to support shuttle landings, was the prime RCO landing site for the first missions carrying the equipment; however Vandenberg was later selected as the prime site as it is nearer the coast, and the shuttle can be ditched in the Pacific should a problem develop that would make landing dangerous. White Sands Missile Range in New Mexico is a likely alternate site. A major consideration in determining the landing site would be the desire to perform a high-risk re-entry far away from populated areas. The flight resource book, and flight rules in force during STS-121 suggest that the damaged shuttle would reenter on a trajectory such that if it should break up, it would do so with debris landing in the South Pacific Ocean.\n\nThe Soviet Buran shuttle was also remotely controlled during its entire maiden flight without a crew aboard. Landing was carried out by an on board, automatic system.\n\nAs of March 2011 the Boeing X-37 extended duration robotic spaceplane has demonstrated autonomous orbital flight, reentry and landing. The X-37 was originally intended for launch from the Shuttle payload bay, but following the \"Columbia\" disaster, it was launched in a shrouded configuration on an Atlas V.\n\nThe STS-3xx missions were developed in the aftermath of the loss of \"Columbia\". However, NASA spent some effort researching rescue options even before the disaster. Before the ISS was launched, or in the event of a Shuttle being unable to reach the station, crews would have had to transfer directly between Shuttles. The orbiters would have been unable to dock, so while they used their RMS arms to grapple each other, the crew would have made an EVA between the Shuttles. This would have been carried out using the two EVA-designated mission specialists wearing the Shuttle/ISS Extravehicular Mobility Unit (EMU) spacesuits, while the remaining crew would have been sealed up in pressurized Personal Rescue Enclosure and carried over either by hand, or using a pulley system (akin to that of a clothesline pulley) like that employed in the Apollo program for lifting samples from the Moon's surface into the Lunar Module.\n\nHad a LON mission been required, a timeline would have been developed similar to the following:\n\n\nSTS-400 was the Space Shuttle contingency support (Launch On Need) flight that would have been launched using if a major problem occurred on during STS-125, the final Hubble Space Telescope servicing mission (HST SM-4).\n\nDue to the much lower orbital inclination of the HST compared to the ISS, the shuttle crew would have been unable to use the International Space Station as a \"safe haven,\" and NASA would not have been able to follow the usual plan of recovering the crew with another shuttle at a later date. Instead, NASA developed a plan to conduct a shuttle-to-shuttle rescue mission, similar to proposed rescue missions for pre-ISS flights. The rescue mission would have been launched only three days after call-up and as early as seven days after the launch of STS-125, since the crew of \"Atlantis\" would only have about three weeks of consumables after launch.\n\nThe mission was first rolled out in September 2008 to Launch Complex 39B two weeks after the STS-125 shuttle was rolled out to Launch Complex 39A, creating a rare scenario in which two shuttles were on launch pads at the same time. In October 2008, however, STS-125 was delayed and rolled back to the VAB.\n\nInitially, STS-125 was retargeted for no earlier than February 2009. This changed the STS-400 vehicle from \"Endeavour\" to \"Discovery\". The mission was redesignated STS-401 due to the swap from \"Endeavour\" to \"Discovery\". STS-125 was then delayed further, allowing \"Discovery\" mission STS-119 to fly beforehand. This resulted in the rescue mission reverting to \"Endeavour\", and the STS-400 designation being reinstated. In January, 2009, it was announced that NASA was evaluating conducting both launches from Complex 39A in order to avoid further delays to Ares I-X, which, at the time, was scheduled for launch from LC-39B in the September 2009 timeframe. It was planned that after the STS-125 mission in October 2008, Launch Complex 39B would undergo the conversion for use in Project Constellation for the Ares I-X rocket. Several of the members on the NASA mission management team said at the time (2009) that single-pad operations were possible, but the decision was made to use both pads.\n\nThe crew assigned to this mission was a subset of the STS-126 crew:\n\nThree different concept mission plans were evaluated: The first would be to use a shuttle-to-shuttle docking, where the rescue shuttle docks with the damaged shuttle, by flying upside down and backwards, relative to the damaged shuttle. It was unclear whether this would be practical, as the forward structure of either orbiter could collide with the payload bay of the other, resulting in damage to both orbiters. The second option that was evaluated, would be for the rescue orbiter to rendezvous with the damaged orbiter, and perform station-keeping while using its Remote Manipulator System (RMS) to transfer crew from the damaged orbiter. This mission plan would result in heavy fuel consumption. The third concept would be for the damaged orbiter to grapple the rescue orbiter using its RMS, eliminating the need for station-keeping. The rescue orbiter would then transfer crew using its RMS, as in the second option, and would be more fuel efficient than the station-keeping option.\n\nThe concept that was eventually decided upon was a modified version of the third concept. The rescue orbiter would use its RMS to grapple the end of the damaged orbiter's RMS.\n\nAfter its most recent mission (STS-123), \"Endeavour\" was taken to the Orbiter Processing Facility for routine maintenance. Following the maintenance, \"Endeavour\" was on stand-by for STS-326 which would have been flown in the case that STS-124 would not have been able to return to Earth safely. Stacking of the solid rocket boosters (SRB) began on 11 July 2008. One month later, the external tank arrived at KSC and was mated with the SRBs on 29 August 2008. \"Endeavour\" joined the stack on 12 September 2008 and was rolled out to Pad 39B one week later.\n\nSince STS-126 launched before STS-125, \"Atlantis\" was rolled back to the VAB on 20 October, and \"Endeavour\" rolled around to Launch Pad 39A on 23 October. When it was time to launch STS-125, \"Atlantis\" rolled out to pad 39A.\n\nThe Mission would not have included the extended heatshield inspection normally performed on flight day two. Instead, an inspection would have been performed after the crew was rescued. On flight day two, \"Endeavour\" would have performed the rendezvous and grapple with \"Atlantis\". On flight day three, the first EVA would have been performed. During the first EVA, Megan McArthur, Andrew Feustel and John Grunsfeld would have set up a tether between the airlocks. They would have also transferred a large size Extravehicular Mobility Unit (EMU) and, after McArthur had repressurized, transferred McArthur's EMU back to \"Atlantis\". Afterwards they would have repressurized on \"Endeavour\", ending flight day two activities.\n\nThe final two EVA were planned for flight day three. During the first, Grunsfeld would have depressurized on \"Endeavour\" in order to assist Gregory Johnson and Michael Massimino in transferring an EMU to \"Atlantis\". He and Johnson would then repressurize on \"Endeavour\", and Massimino would have gone back to \"Atlantis\". He, along with Scott Altman and Michael Good would have taken the rest of the equipment and themselves to \"Endeavour\" during the final EVA. They would have been standing by in case the RMS system should malfunction. The damaged orbiter would have been commanded by the ground to deorbit and go through landing procedures over the Pacific, with the impact area being north of Hawaii. On flight day five, \"Endeavour\" would have had a full heat shield inspection, and land on flight day eight.\n\nThis mission could have marked the end of the Space Shuttle program, as it is considered unlikely that the program would have been able to continue with just two remaining orbiters, \"Discovery\" and \"Endeavour\".\n\nOn Thursday, 21 May 2009, NASA officially released \"Endeavour\" from the rescue mission, freeing the orbiter to begin processing for STS-127. This also allowed NASA to continue processing LC-39B for the upcoming Ares I-X launch, as during the stand-down period, NASA installed a new lightning protection system, similar to those found on the Atlas V and Delta IV pads, to protect the newer, taller Ares I rocket from lightning strikes.\n\nSTS-134 was the last scheduled flight of the Shuttle program. Because no more were planned after this, a special mission was developed as STS-335 to act as the LON mission for this flight. This would have paired \"Atlantis\" with ET-122, which had been refurbished following damage by Hurricane Katrina. Since there would be no next mission, STS-335 would also carry a Multi-Purpose Logistics Module filled with supplies to replenish the station.\n\nThe Senate authorized STS-135 as a regular flight on 5 August 2010, followed by the House on 29 September 2010, and later signed by President Obama on 11 October 2010. However funding for the mission remained dependent on a subsequent appropriations bill.\n\nNonetheless NASA converted STS-335, the final Launch On Need mission, into an operational mission (STS-135) on 20 January 2011. On 13 February 2011, program managers told their workforce that STS-135 would fly \"regardless\" of the funding situation via a continuing resolution. Finally the U.S. government budget approved in mid-April 2011 called for $5.5 billion for NASA's space operations division, including the Space Shuttle and space station programs. According to NASA, the budget running through 30 September 2011 ended all concerns about funding the STS-135 mission.\n\nWith the successful completion of STS-134, STS-335 was rendered unnecessary and launch preparations for STS-135 continued as Atlantis neared LC-39A during her rollout as STS-134 landed at the nearby Shuttle Landing Facility.\n\nFor the STS-135, no shuttle was available for a rescue mission. A different rescue plan was devised, involving the four crew members remaining aboard the International Space Station, and returning aboard Soyuz spacecraft one at a time over the next year. That contingency was not required.\n"}
{"id": "17839582", "url": "https://en.wikipedia.org/wiki?curid=17839582", "title": "Science and Civilisation in China", "text": "Science and Civilisation in China\n\nScience and Civilisation in China (1954–[2016]) is a series of books initiated and edited by British biochemist, historian and sinologist Joseph Needham, Ph.D (1900–1995). Needham was a well-respected scientist before undertaking this encyclopedia and was even responsible for the \"S\" in UNESCO. They deal with the history of science and technology in China. To date there have been seven volumes in twenty-seven books. The series was on the Modern Library Board's 100 Best Nonfiction books of the 20th century. Needham's work was the first of its kind to praise Chinese scientific contributions and provide their history and connection to global knowledge in contrast to eurocentric historiography.\n\nIn 1954, Needham—along with an international team of collaborators—initiated the project to study the science, technology, and civilisation of ancient China. This project produced a series of volumes published by Cambridge University Press. The project is still continuing under the guidance of the Publications Board of the Needham Research Institute (NRI), chaired by Christopher Cullen.\n\nVolume 3 of the encyclopedia was the first body of work to describe Chinese improvements to cartography, geology, seismology and mineralogy. It also includes descriptions of nautical technology, sailing charts, and wheel-maps.\n\nNeedham's transliteration of Chinese characters uses the Wade-Giles system, though the aspirate apostrophe (e.g., \"ch'i\") was rendered 'h' (viz. \"chhi\"; traditional Chinese: 氣; Mandarin Pinyin: \"qì\"). However, it was abandoned in favor of the pinyin system by the NRI board in April 2004, with Volume 5, Part 11 becoming the first to use the new system.\n\nJoseph Needham’s interest in the history of Chinese science developed while he worked as an Embryologist at Cambridge University. At the time, Needham had already published works relating to the history of science, including his 1934 book titled \"A History of Embryology\", and was open to expanding his historical scientific knowledge. Needham’s first encounter with Chinese culture occurred in 1937 when three Chinese medical students arrived to work with him at the Cambridge Biochemical Laboratory. Needham’s interest in Chinese civilization and scientific progress grew as a result and led him to learn Chinese from his students. Two of those students,Wang Ling, and Lu Gwei-djen, would later become his collaborators on \"Science and Civilisation in China\".\n\nIn 1941, China’s eastern universities were forced to relocate to the west as a result of the Second Sino-Japanese War. Chinese academics sought the help of the British government in an effort to preserve their intellectual life. In 1942, Needham was selected and appointed as a diplomat by the British government and tasked with traveling to China and assessing the situation. During his three years there, Needham discovered that the Chinese had developed techniques and mechanisms which were centuries older than their European counterparts. Needham became concerned with the exclusion of China in the history of science and began to question why the Chinese ceased to develop new techniques after the 16th century.\n\nArmed with his new-found knowledge, Needham returned to Cambridge in 1948 and began working on a book with one of the Chinese medical students he met in Cambridge, Wang Ling, who was now a professor at a university. Initially, he planned on releasing only one volume of his findings through the Cambridge University Press, but later changed his mind and proposed up to eleven volumes. In 1954, Needham published the first volume of \"Science and Civilisation in China\", which was well received and was followed by other volumes which focused on specific scientific fields and topics. Needham, along with his collaborators, was personally involved in all of the volumes of Science and Civilization, up until Needham’s death in 1995. After Needham’s death, Cambridge University established an institution named after Needham, The Needham Research Institute. Scholars of the institution continue Needham’s work and have published 8 additional volumes of \"Science and Civilisation in China\", since his death.\n\nThere have been two summaries or condensations of the vast amount of material found in \"Science and Civilisation\". The first, a one-volume popular history book by Robert Temple entitled \"The Genius of China\", was completed in a little over 12 months to be available in 1986 for the visit of Queen Elizabeth II to China. This addressed only the contributions made by China and had a \"warm welcome\" from Joseph Needham in the introduction, though in the \"Beijing Review\" he criticized that it had \"some mistakes ... and various statements that I would like to have seen expressed rather differently\".\nA second was made by Colin Ronan, a writer on the history of science, who produced a five volume condensation \"The Shorter Science and Civilisation: An abridgement of Joseph Needham's original text\", between 1980 and his death in 1995. These volumes cover:\n\n\"Science and Civilisation in China \" is highly regarded among scholars because of its extensive comparative coverage of Chinese innovations. Needham spent a large amount of time translating, and decoding primary sources for \"Science and Civilisation in China\". All of his efforts helped to confirm that scientific advancements, and analytical ingenuity were abundant in China in early modern times. Yet, beginning with his first volume, some scholars in the scientific, history of science, and sinology fields criticized Needham’s work for being too comparative. In his work, Needham wrote that numerous Chinese inventions ended up in the west, including the magnetic compass, and the mechanical clock. Needham also wrote that once these inventions reached Europe, they had a great impact on social life, and helped to stimulate the economy, as well as usher in the Scientific Revolution. Other scholars criticized his Marxist background, his understanding of Chinese culture, and his methodology.\n\nDr. of History Roger Hart wrote \"A Post-Needham Critique\" in an East Asia journal accusing Needham of attributing scientific achievements to Chinese civilization in contrast to evidence by exaggerating claims.\n\nEditor of Volume 6, Nathan Sivin and Needham's research collaborator Lu Gwei-djen include updated research to support some of Needham's claims. However, Sivin is critical of Needham suggesting more research is required citing his assumptions of Taoisms' role in promoting scientific feats in China.\n\nNeedham's \"Science and Civilisation in China\" did not receive criticism from scholars in other fields of study.\n\nGroff Conklin of \"Galaxy Science Fiction\" in 1955 said that Vol. 1 \"presents a richly patterned tapestry of the development of civilization in the Far East\", and that \"it is for everyone who is intrigued by the unknown, whether future (science fiction) or past (scientific history)\".\n\nJonathan Spence wrote in a 1982 New York Times article \"this work is the most ambitious undertaking in Chinese studies during this century\".\n\nThe New York Times obituary for Needham stated that those educated in China hail Dr. Needham's encyclopedia and compare him to Charles Darwin in terms of importance regarding scientific knowledge.\n\nAccording to Dr. Arun Bala, the author of \"The Dialogue of Civilizations in the Birth of Modern Science,\" Needham postulates that scientific knowledge may evolve to more closely resemble Chinese philosophical views of nature; signifying his belief in Chinese inherent wisdom.\n\nAfter his extensive research of Chinese innovations, Joseph Needham became concerned with the question: Why did modern science stop developing in China after the 16th century? Needham believed this was due to China’s sociopolitical system which was not affected by Chinese inventions. China did not have a structure in which merchants could profit off of their inventions, unlike the West. Once Chinese inventions reached Europe, they revolutionized their sociopolitical system, which used the inventions to dominate political rivals. According to Needham, Chinese innovations, such as gunpowder, the compass, paper, and printing, helped transform European Feudalism into Capitalism. By the end of the 15th century, Europe was actively financing scientific discoveries, and nautical exploration. The paradox of this conclusion was that Europe surpassed China in scientific innovations, using Chinese technologies.\n\nAfter several volumes of \"Science and Civilisation in China\" had been published, Needham was questioned about his theory of the origin of science in the West. Needham, troubled by past criticism and dismissal of his work as Marxist theory, declined to publicly state his relationship to Marxism. Later, in Needham’s work \"The Grand Titration,\" he re-framed his question as: \"“why, between the first century BC and the fifteenth century AD, Chinese civilization was much more efficient than occidental in applying human natural knowledge to practical human needs”\" The reformulation of the question, changed the narrative of \"Science and Civilisation in China\". Initially, the question centered around China’s failure to develop scientifically after the 16th century. The focus shifted towards an examination of China’s accomplishments prior to development in Europe, this focus was addressed throughout \"Science and Civilisation in China\".\n\nNeedham's attempt to uncover the reasoning behind China's rise and fall as an elite scientific and technologically advanced nation has been expounded upon and debated for decades including Justin Yifu Lin's University of Chicago journal article \"The Needham Puzzle\".\n\n\n"}
{"id": "682348", "url": "https://en.wikipedia.org/wiki?curid=682348", "title": "Stigler's law of eponymy", "text": "Stigler's law of eponymy\n\nStigler's law of eponymy, proposed by University of Chicago statistics professor Stephen Stigler in his 1980 publication \"Stigler’s law of eponymy\", states that no scientific discovery is named after its original discoverer. Examples include Hubble's law which was derived by Georges Lemaître two years before Edwin Hubble, the Pythagorean theorem although it was known to Babylonian mathematicians before Pythagoras, and Halley's comet which was observed by astronomers since at least 240 BC. Stigler himself named the sociologist Robert K. Merton as the discoverer of \"Stigler's law\" to show that it follows its own decree, though the phenomenon had previously been noted by others.\n\nHistorical acclaim for discoveries is often assigned to persons of note who bring attention to an idea that is not yet widely known, whether or not that person was its original inventor – theories may be named long after their discovery. In the case of eponymy, the idea becomes named after that person, even if that person is acknowledged by historians of science not to be the one who discovered it. Often, several people will arrive at a new idea around the same time, as in the case of calculus. It can be dependent on the publicity of the new work and the fame of its publisher as to whether the scientist's name becomes historically associated.\n\nThere is a similar quote attributed to Mark Twain: \"It takes a thousand men to invent a telegraph, or a steam engine, or a phonograph, or a photograph, or a telephone or any other important thing—and the last man gets the credit and we forget the others. He added his little mite — that is all he did. These object lessons should teach us that ninety-nine parts of all things that proceed from the intellect are plagiarisms, pure and simple; and the lesson ought to make us modest. But nothing can do that.\"\n\nStephen Stigler's father, the economist George Stigler, also examined the process of discovery in economics. He said, \"If an earlier, valid statement of a theory falls on deaf ears, and a later restatement is accepted by the science, this is surely proof that the science accepts ideas only when they fit into the then-current state of the science.\" He gave several examples in which the original discoverer was not recognized as such.\n\nThe Matthew effect was coined by Robert K. Merton to describe how eminent scientists get more credit than a comparatively unknown researcher, even if their work is similar, so that credit will usually be given to researchers who are already famous. Merton notes that \"this pattern of recognition, skewed in favor of the established scientist, appears principally (i) in cases of collaboration and (ii) in cases of independent multiple discoveries made by scientists of distinctly different rank.\" The effect applies specifically to women through the Matilda effect.\n\nBoyer's law was named by Hubert Kennedy in 1972. It says, \"Mathematical formulas and theorems are usually not named after their original discoverers\" and was named after Carl Boyer, whose book \"History of Mathematics\" contains many examples of this law. Kennedy observed that \"it is perhaps interesting to note that this is probably a rare instance of a law whose statement confirms its own validity\".\n\n\"Everything of importance has been said before by somebody who did not discover it\" is an adage attributed to Alfred North Whitehead.\n\n\n\n"}
{"id": "10202396", "url": "https://en.wikipedia.org/wiki?curid=10202396", "title": "Supralittoral zone", "text": "Supralittoral zone\n\nThe supralittoral zone, also known as the splash zone, spray zone or the supratidal zone, is the area above the spring high tide line, on coastlines and estuaries, that is regularly splashed, but not submerged by ocean water. Seawater penetrates these elevated areas only during storms with high tides.\n\nOrganisms here must cope also with exposure to air, fresh water from rain, cold, heat and predation by land animals and seabirds. At the top of this area, patches of dark lichens can appear as crusts on rocks. Some types of periwinkles, Neritidae and detritus feeding Isopoda commonly inhabit the lower supralitoral.\n\n\n"}
{"id": "2285184", "url": "https://en.wikipedia.org/wiki?curid=2285184", "title": "Surface metrology", "text": "Surface metrology\n\nSurface metrology is the measurement of small-scale features on surfaces, and is a branch of metrology. Surface primary form, surface fractality and surface roughness are the parameters most commonly associated with the field. It is important to many disciplines and is mostly known for the machining of precision parts and assemblies which contain mating surfaces or which must operate with high internal pressures.\n\nSurface finish may be measured in two ways: \"contact\" and \"non-contact\" methods. Contact methods involve dragging a measurement stylus across the surface; these instruments are called profilometers. Non-contact methods include: interferometry, digital holography, confocal microscopy, focus variation, structured light, electrical capacitance, electron microscopy, and photogrammetry.\n\nThe most common method is to use a diamond stylus profilometer. The stylus is run perpendicular to the lay of the surface. The probe usually traces along a straight line on a flat surface or in a circular arc around a cylindrical surface. The length of the path that it traces is called the \"measurement length\". The wavelength of the lowest frequency filter that will be used to analyze the data is usually defined as the \"sampling length\". Most standards recommend that the measurement length should be at least seven times longer than the sampling length, and according to the Nyquist–Shannon sampling theorem it should be at least two times longer than the wavelength of interesting features. The \"assessment length\" or \"evaluation length\" is the length of data that will be used for analysis. Commonly one sampling length is discarded from each end of the measurement length. 3D measurements can be made with a profilometer by scanning over a 2D area on the surface.\n\nThe disadvantage of a profilometer is that it is not accurate when the size of the features of the surface are close to the same size as the stylus. Another disadvantage is that profilometers have difficulty detecting flaws of the same general size as the roughness of the surface. There are also limitations for non-contact instruments. For example, instruments that rely on optical interference cannot resolve features that are less than some fraction of the operating wavelength. This limitation can make it difficult to accurately measure roughness even on common objects, since the interesting features may be well below the wavelength of light. The wavelength of red light is about 650 nm, while the average roughness, (R) of a ground shaft might be 200 nm.\n\nThe first step of analysis is to filter the raw data to remove very high frequency data (called \"micro-roughness\") since it can often be attributed to vibrations or debris on the surface. Filtering out the micro-roughness at a given cut-off threshold also allows to bring closer the roughness assessment made using profilometers having different stylus ball radius e.g. 2 µm and 5 µm radii. Next, the data is separated into roughness, waviness and form. This can be accomplished using reference lines, envelope methods, digital filters, fractals or other techniques. Finally, the data is summarized using one or more roughness parameters, or a graph. In the past, surface finish was usually analyzed by hand. The roughness trace would be plotted on graph paper, and an experienced machinist decided what data to ignore and where to place the mean line. Today, the measured data is stored on a computer, and analyzed using methods from signal analysis and statistics.\n\nStylus-based contact instruments have the following advantages:\n\n\"Technologies\":\n\n\nOptical measurement instruments have some advantages over the tactile ones as follows:\n\n\"Vertical scanning\":\n\n\n\"Horizonal scanning\":\n\n\"Non-scanning\"\n\nBecause of every instrument has advantages and disadvantages the operator must choose the right instrument depending on the measurement application. In the following some advantages and disadvantages to the main technologies are listed:\n\nThe scale of the desired measurement will help decide which type of microscope will be used.\n\nFor 3D measurements, the probe is commanded to scan over a 2D area on the surface. The spacing between data points may not be the same in both directions.\n\nIn some cases, the physics of the measuring instrument may have a large effect on the data. This is especially true when measuring very smooth surfaces. For contact measurements, most obvious problem is that the stylus may scratch the measured surface. Another problem is that the stylus may be too blunt to reach the bottom of deep valleys and it may round the tips of sharp peaks. In this case the probe is a physical filter that limits the accuracy of the instrument.\n\nThe real surface geometry is so complicated that a finite number of parameters cannot provide a full description. If the number of parameters used is increased, a more accurate description can be obtained. This is one of the reasons for introducing new parameters for surface evaluation. Surface roughness parameters are normally categorised into three groups according to its functionality. These groups are defined as amplitude parameters, spacing parameters, and hybrid parameters.\n\nParameters used to describe surfaces are largely statistical indicators obtained from many samples of the surface height. Some examples include:\n\nThis is a small subset of available parameters described in standards like ASME B46.1 and ISO 4287.\nMost of these parameters originated from the capabilities of profilometers and other mechanical probe systems. \nIn addition, new measures of surface dimensions have been developed which are more directly related to the measurements made possible by high-definition optical gauging technologies.\n\nMost of these parameters can be estimated using the SurfCharJ plugin for the ImageJ.\n\nThe surface roughness can also be calculated over an area. This gives S instead of R values. The ISO 25178 series describes all these roughness values in detail. The advantage over the profile parameters are:\n\n\nSurfaces have fractal properties, multi-scale measurements can also be made such as Length-scale Fractal Analysis or Area-scale Fractal Analysis.\n\nTo obtain the surface characteristic almost all measurements are subject to filtering. It is one of the most important topics when it comes to specifying and controlling surface attributes such as roughness, waviness, and form error. These components of the surface deviations must be distinctly separable in measurement to achieve a clear understanding between the surface supplier and the surface recipient as to the expected characteristics of the surface in question. \nTypically, either digital or analog filters are used to separate form error, waviness, and roughness resulting from a measurement. Main multi-scale filtering methods are Gaussian filtering, Wavelet transform and more recentlty Discrete Modal Decomposition. There are three characteristics of these filters that should be known in order to understand the parameter values that an instrument may calculate. These are the spatial wavelength at which a filter separates roughness from waviness or waviness from form error, the sharpness of a filter or how cleanly the filter separates two components of the surface deviations and the distortion of a filter or how much the filter alters a spatial wavelength component in the separation process.\n\n\n"}
{"id": "22694540", "url": "https://en.wikipedia.org/wiki?curid=22694540", "title": "The Exploration of the Colorado River and Its Canyons", "text": "The Exploration of the Colorado River and Its Canyons\n\nThe Exploration of the Colorado River and Its Canyons by John Wesley Powell is a classic of American exploration literature. It is about the Powell Geographic Expedition of 1869 which was the first trip down the Colorado River by boat, including the first trip through the Grand Canyon. \n\nPowell's first written accounts of his exploration appeared in the January, February and March 1875 editions of \"Scribner’s Monthly\" as \"The Canons of the Colorado\". The Smithsonian published it in book form in 1875 under title \"Report of the Exploration of the Colorado River of the West and Its Tributaries. Explored in 1869, 1870, 1871, and 1872, under the direction of the Secretary of the Smithsonian Institution\". It was revised and published in 1895 as \"The Exploration of the Colorado River and Its Canyons\".\n\nIt includes hundreds of wood engravings based on photographs by E.O. Beaman, James Fennemore and John Karl Hillers, and drawings by Thomas Moran.\n\n"}
{"id": "7255094", "url": "https://en.wikipedia.org/wiki?curid=7255094", "title": "Tokamak à configuration variable", "text": "Tokamak à configuration variable\n\nThe Tokamak à configuration variable (TCV, literally \"variable configuration tokamak\") is a Swiss research fusion reactor of the École polytechnique fédérale de Lausanne. Its distinguishing feature over other tokamaks is that its torus section is three times higher than wide. This allows studying several shapes of plasmas, which is particularly relevant since the shape of the plasma has links to the performance of the reactor. The TCV was set up in November 1992.\n\n\n\nBy 2012 it had 16 poloidal plasma shaping coils and could achieve a variety of field configurations and plasma shapes.\n\n\n"}
{"id": "4145667", "url": "https://en.wikipedia.org/wiki?curid=4145667", "title": "Unit of observation", "text": "Unit of observation\n\nIn statistics, a unit of observation is the unit described by the data that one analyzes. For example, in a study of the demand for money, the unit of observation might be chosen as the individual, with different observations (data points) for a given point in time differing as to which individual they refer to; or the unit of observation might be the country, with different observations differing only in regard to the country they refer to. A study may have a differing unit of observation and unit of analysis: for example, in community research, the research design may collect data at the individual level of observation but the level of analysis might be at the neighborhood level, drawing conclusions on neighborhood characteristics from data collected from individuals. Together, the unit of observation and the level of analysis define the population of a research enterprise.\n\nA data point or observation is a set of one or more measurements on a single member of unit of observation. For example, in a study of the determinants of money demand with the unit of observation being the individual, a data point might be the values of income, wealth, age of individual, and number of dependents. Statistical inference about the population would be conducted using a statistical sample consisting of various such data points.\n\nIn addition, in statistical graphics, a \"data point\" may be an individual item with a statistical display; such points may relate to either a single member of a population or to a summary statistic calculated for a given subpopulation.\n\nThe measurements contained a unit of observation are formally \"typed\", where here \"type\" is used in a way compatible with datatype in computing; so that the type of measurement can specify whether the measurement results in a Boolean value from {yes, no}, an integer or real number, the identity of some category, or some vector or array. \n\nThe implication of \"point\" is often that the data may be plotted in a graphic display, but in many cases the data are processed numerically before that is done. In the context of statistical graphics, measured values for individuals or summary statistics for different subpopulations are displayed as separate symbols within a display; since such symbols can differ by shape, size and colour, a single \"data point\" within a display can convey multiple aspects of the set of measurements for an individual or subpopulation.\n\n"}
{"id": "27258428", "url": "https://en.wikipedia.org/wiki?curid=27258428", "title": "Veneziano amplitude", "text": "Veneziano amplitude\n\nIn theoretical physics, the Veneziano amplitude refers to the discovery made in 1968 by Italian theoretical physicist Gabriele Veneziano that the Euler beta function, when interpreted as a scattering amplitude, has many of the features needed to explain the physical properties of strongly interacting mesons, such as symmetry and duality. Conformal symmetry was soon discovered. The formula is the following:\n\n\"k\" is a vector (such as a four-vector) referring to the momentum of the n particle. Γ is the gamma function.\n\nThis discovery can be considered the birth of string theory, as the discovery and invention of string theory came about as a search for a physical model which would give rise to such a scattering amplitude.\n\n\n"}
{"id": "36555470", "url": "https://en.wikipedia.org/wiki?curid=36555470", "title": "Water filling algorithm", "text": "Water filling algorithm\n\nWater filling algorithm is a general name given to the ideas in communication systems design and practice for equalization strategies on communications channels. As the name suggests, just as water finds its level even when filled in one part of a vessel with multiple openings, as a consequence of Pascal's law, the amplifier systems in communications network repeaters, or receivers amplify each channel up to the required power level compensating for the channel impairments. See, for example, channel power allocation in MIMO systems.\n\nIn a single channel communication system the deamplification and loss present on them can be simplistically taken as attenuation by a percentage \"g\", then amplifiers restore the signal power level to the same value at transmission setup by operating at a gain of 1/ (1-g). E.g. if we experience 6dB attenuation in transmission, i.e. 75% loss, then we have to amplify the signal by a factor of \"4x\" to restore the signal to the transmitter levels.\n\nSame ideas can be carried out in presence impairments and a multiple channel system. Amplifier nonlinearity, crosstalk and power budgets prevent the use of these waterfilling algorithms to restore all channels, and only a subset can benefit from them.\n\n\n"}
