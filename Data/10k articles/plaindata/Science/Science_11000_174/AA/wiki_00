{"id": "20348643", "url": "https://en.wikipedia.org/wiki?curid=20348643", "title": "Academic studies of the political groups of the European Parliament", "text": "Academic studies of the political groups of the European Parliament\n\nAcademic studies of the political groups of the European Parliament refers to the studies of those groups by academics, the methods that they use and the conclusions that they reach.\n\nThe political groups of the European Parliament have been around in one form or another since September 1952 and the first meeting of the Parliament's predecessor, the Common Assembly. The groups are coalitions of MEPs and the Europarties and national parties that those MEPs belong to. The groups have coalesced into representations of the dominant schools of European political thought and are the primary actors in the Parliament.\n\nSome of the groups (such as PES) have become homogeneous units coterminous with their Europarty, some (such as IND/DEM) have not. But they are still coalitions, not parties in their own right, and do not issue manifestos of their own. It may therefore be difficult to discern how the groups intend to vote without first inspecting the party platforms of their constituent parties, and then with limited certainty.\n\nAdditionally, national media focus on the MEPs/national parties of their own member state, neglecting the group's activities and poorly understanding their structure or even existence. Transnational media coverage of the groups \"per se\" is limited to those organs such as the Parliament itself, or those news media (e.g. EUObserver or theParliament.com) that specialise in the Parliament. These organs cover the groups in detail but with little overarching analysis. So although such organs make it easy to find out how a group acted on a specific vote, they provide little information on the voting patterns of a specific group.\n\nAs a result, the only bodies providing analysis of the voting patterns and Weltanschauung of the groups are academics.\n\nAcademics analysing the groups include Simon Hix (London School of Economics and Political Science), Amie Kreppel University of Florida, Abdul Noury (Free University of Brussels), Gérard Roland, (University of California, Berkeley), Gail McElroy (Trinity College, Dublin, Department of Political Science), Kenneth Benoit (Trinity College, Dublin - Institute for International Integration Studies (IIIS)), Friedrich Heinemann, Philipp Mohl, and Steffen Osterloh (University of Mannheim - Centre for European Economic Research).\n\nTable 3 of the 3 January 2008 version of a working paper from the London School of Economics/Free University of Brussels by Hix and Noury considered the positions of the groups in the Sixth Parliament (2004-2009) by analysing their roll-call votes. The results for each group are shown in the adjacent diagram. The vertical scale is anti-pro Europe spectrum, (0% = extremely anti-Europe, 100% = extremely pro), and the horizontal scale is economic left-right spectrum, (0% = extremely economically left-wing, 100% = extremely economically right-wing). The results are also shown in the table below.\n\nTwo of the groups (EPP-ED and IND/DEM) were split. EPP-ED are split on Euroscepticism: the EPP subgroup ( ) were centre-right Europhiles, whereas the ED subgroup ( ) were right-wing Eurosceptics.\n\nIND/DEM was also split along its subgroups: the reformist subgroup ( , bottom-center) voted as centrist Eurosceptics, and the secessionist subgroup ( , middle-right) voted as right-wing Euroneutrals. The reformist subgroup was able to pursue a reformist agenda via the Parliament. The secessionist subgroup was unable to pursue a secessionist agenda there (it's out of the Parliament's purview) and pursued a right-wing agenda instead. This resulted in the secessionist subgroup being \"less\" eurosceptic in terms of roll-call votes than other, non-eurosceptic parties. UKIP (the major component of the secessionist subgroup) was criticised for this seeming abandonment of its Eurosceptic core principles.\n\nTable 3 of the 21 August 2008 version of the same working paper gave figures for the level of cooperation between each group (how many times they vote with a group, and how many times they vote against) for the Fifth and Sixth Parliaments. The results are given in the tables below, where 0% = never votes with, 100% = always votes with.\n\nEUL/NGL and G/EFA voted closely together, as did PES and ALDE, and EPP-ED and UEN. Surprisingly, given that PES and EPP-ED are partners in the Grand Coalition, they were not each other's closest allies, although they did vote with each other about two-thirds of the time. IND/DEM did not have close allies within the political groups, preferring instead to cooperate most closely with the Non-Inscrits.\n\nTable 2 of a 2005 discussion paper from the Institute for International Integration Studies by Gail McElroy and Kenneth Benoit analysed the group positions between April and June 2004, at the end of the Fifth Parliament and immediately before the 2004 elections. The results are given below, with 0% = extremely against, 100% = extremely for (except for the left-right spectrum, where 0% = extremely left-wing, 100% = extremely right-wing)\n\nEUL/NGL and G/EFA were the most left-wing groups, UEN and EDD the most right-wing, and that was mirrored in their attitudes towards taxation, homosexual equality, abortion, euthanasia and controlling migration into the EU. The groups fell into two distinct camps regarding further development of EU authority, with UEN and EDD definitely against and the rest broadly in favor. Opinion was wider on the CFSP, with only PES, ELDR and EPP-ED in favor and the others against. Unsurprisingly, G/EFA was far more in favor of Green issues compared to the other groups.\n\nTable 1 of an April 2008 discussion paper from the Centre for European Economic Research by Heinemann et al. analysed each Group's stance on a hypothetical generalised EU tax. The results for each Group are given in the adjacent diagram with the horizontal scale scaled so that -100% = totally against and 100% = totally for. The results are also given in the table below, rescaled so that 0% = totally against, 100% = totally for.\n\nG/EFA and PES were in favor of such a tax, IND/DEM and the Independents were definitely against, the others had no clear position.\n\n\"Cohesion\" is the term used to define whether a Group is united or divided amongst itself. Figure 1 of a 2002 paper from European Integration online Papers (EIoP) by Thorsten Faas analysed the Groups as they stood in 2002. \nThe results for each Group are given in the adjacent diagram with the horizontal scale scaled so that 0% = totally split, 100% = totally united. The results are also given in the table below.\n\nG/EFA, PES and ELDR were the most united groups, with EDD the most disunited.\n\nThe March 2006 edition of \"Social Europe: the journal of the European Left\" included a chapter called \"Women and Social Democratic Politics\" by Wendy Stokes. That chapter gave the proportion of female MEPs in each Group in the European Parliament. The results for each Group are given in the adjacent diagram. The horizontal scale denotes gender balance (0% = totally male, 100% = totally female, but no Group has a female majority, so the scale stops at 50%). The results are also given in the table below.\n\nG/EFA, PES and ALDE were the most balanced groups in terms of gender, with IND/DEM the most unbalanced.\nMartin Olof Persson's D-uppsats thesis for the Luleå University of Technology entitled \"Turkey as a member of the European Union: a discourse analysis of the views presented in the European Parliament\" considered each group's attitude towards Turkish accession. The results of that thesis are given in the table below.\n\nG/EFA, PES, EUL/NGL and ALDE supported Turkish accession either now or later, EPP-ED, IND/DEM and ITS permanently rejected it, and UEN's position was unknown.\nParty group switching in the European Parliament is the phenomenon where parliamentarians individually or collectively switch from one party group to the other. The phenomenon of EP party group switching is a well-known contributor to the volatility of the EP party system and highlights the fluidity that characterizes the composition of European parliamentary groups. On average 9% of all MEPs switch during legislative terms. Party group switching is a phenomenon that gained force especially in the legislatures during the 1990s, up to a maximum of 18% for the 1989-1994 term, with strong prevalence among representatives from France and Italy, though by no means limited to those two countries. There is a clear tendency of party group switches from the ideological extremes, both left and right, toward the center. Most switching takes place at the outset of legislative terms, with another peak around the half-term moment, when responsibilities rotate within the EP hierarchy.\n"}
{"id": "17314682", "url": "https://en.wikipedia.org/wiki?curid=17314682", "title": "Algebraic statistics", "text": "Algebraic statistics\n\nAlgebraic statistics is the use of algebra to advance statistics. Algebra has been useful for experimental design, parameter estimation, and hypothesis testing.\n\nTraditionally, algebraic statistics has been associated with the design of experiments and multivariate analysis (especially time series). In recent years, the term \"algebraic statistics\" has been sometimes restricted, sometimes being used to label the use of algebraic geometry and commutative algebra in statistics.\n\nIn the past, statisticians have used algebra to advance research in statistics. Some algebraic statistics led to the development of new topics in algebra and combinatorics, such as association schemes.\n\nFor example, Ronald A. Fisher, Henry B. Mann, and Rosemary A. Bailey applied Abelian groups to the design of experiments. Experimental designs were also studied with affine geometry over finite fields and then with the introduction of association schemes by R. C. Bose. Orthogonal arrays were introduced by C. R. Rao also for experimental designs.\n\nInvariant measures on locally compact groups have long been used in statistical theory, particularly in multivariate analysis. Beurling's factorization theorem and much of the work on (abstract) harmonic analysis sought better understanding of the Wold decomposition of stationary stochastic processes, which is important in time series statistics.\n\nEncompassing previous results on probability theory on algebraic structures, Ulf Grenander developed a theory of \"abstract inference\". Grenander's abstract inference and his theory of patterns are useful for spatial statistics and image analysis; these theories rely on lattice theory.\n\nPartially ordered vector spaces and vector lattices are used throughout statistical theory. Garrett Birkhoff metrized the positive cone using Hilbert's projective metric and proved Jentsch's theorem using the contraction mapping theorem. Birkhoff's results have been used for maximum entropy estimation (which can be viewed as linear programming in infinite dimensions) by Jonathan Borwein and colleagues.\n\nVector lattices and conical measures were introduced into statistical decision theory by Lucien Le Cam.\n\nIn recent years, the term \"algebraic statistics\" has been used more restrictively, to label the use of algebraic geometry and commutative algebra to study problems related to discrete random variables with finite state spaces. Commutative algebra and algebraic geometry have applications in statistics because many commonly used classes of discrete random variables can be viewed as algebraic varieties.\n\nConsider a random variable \"X\" which can take on the values 0, 1, 2. Such a variable is completely characterized by the three probabilities \n\nand these numbers satisfy\nConversely, any three such numbers unambiguously specify a random variable, so we can identify the random variable \"X\" with the tuple (\"p\",\"p\",\"p\")∈R.\n\nNow suppose \"X\" is a binomial random variable with parameter \"q\" and \"n = 2\", i.e. \"X\" represents the number of successes when repeating a certain experiment two times, where each experiment has an individual success probability of \"q\". Then \nand it is not hard to show that the tuples (\"p\",\"p\",\"p\") which arise in this way are precisely the ones satisfying\nThe latter is a polynomial equation defining an algebraic variety (or surface) in R, and this variety, when intersected with the simplex given by\nyields a piece of an algebraic curve which may be identified with the set of all 3-state Bernoulli variables. Determining the parameter \"q\" amounts to locating one point on this curve; testing the hypothesis that a given variable \"X\" is Bernoulli amounts to testing whether a certain point lies on that curve or not.\n\nAlgebraic geometry has also recently found applications to statistical learning theory, including a generalization of the Akaike information criterion to singular statistical models.\n\n\n"}
{"id": "27877848", "url": "https://en.wikipedia.org/wiki?curid=27877848", "title": "Antoine Sonrel", "text": "Antoine Sonrel\n\nAntoine Sonrel (died 1879) was an illustrator, engraver, and photographer in Switzerland and Boston, Massachusetts, in the 19th century. He moved from Neuchâtel to the United States around the late 1840s, and was affiliated with Louis Agassiz throughout his career. As a photographer he created numerous carte de visite portraits in the 1860s and 1870s; subjects included his friend Agassiz, Oliver Wendell Holmes, Sr., Oliver Wendell Holmes, Jr., Abbott Lawrence Rotch, and sculptor Anne Whitney.\n\nAround the 1830s in Neuchâtel, Sonrel began creating scientific illustrations for Louis Agassiz. \"Draftsmen of superior talent, trained ... to the greatest accuracy — Weber, Dinkel, and Sonrel — were constantly in [Agassiz's] employ at a regular salary. ... At the suggestion of Agassiz an extensive lithographic establishment was created in Neuchatel.\" Agassiz wrote in 1857: \"I esteem myself happy to have been able to secure the continued assistance of my old friend, Mr. A. Sonrel, in drawing the zoological figures of my work. More than twenty years ago, he began to make illustrations for my European works ; and ever since he has been engaged, with short interruptions, in executing drawings for me.\"\n\nIn the United States, Sonrel lived in Boston on Acorn Street in Beacon Hill (c. 1850), Tremont Street (c. 1873), and in Woburn, Massachusetts (c. 1852-1874). He kept a studio in Boston at 46 School Street (c. 1860s) and Washington Street (c. 1871-1874). Sonrel exhibited lithographs in the 1851 World's Fair in London and in the 1853 exhibition of the Massachusetts Charitable Mechanic Association.\n\n\n\n\n"}
{"id": "23937691", "url": "https://en.wikipedia.org/wiki?curid=23937691", "title": "Atlas of the Prehistoric World", "text": "Atlas of the Prehistoric World\n\nAtlas of the Prehistoric World is a book written by Douglas Palmer. It was published in 1999 by Random House, Inc..\n"}
{"id": "1448656", "url": "https://en.wikipedia.org/wiki?curid=1448656", "title": "Back to the Future (TV series)", "text": "Back to the Future (TV series)\n\nBack to the Future (also known as Back to the Future: The Animated Series) is an American animated science fiction comedy adventure television series for television based on the live action \"Back to the Future\" movie trilogy. The show lasted two seasons, each featuring 13 episodes, and ran on CBS from September 14, 1991, to December 26, 1992, and reran until August 14, 1993, on CBS. The network chose not to renew the show for a third season (citing low ratings). It later reran on FOX, as a part of the FoxBox block from March 22 to August 30, 2003. It was the very first production of Universal Cartoon Studios.\n\nAlthough the cartoon takes place after the movies, Bob Gale has stated that the animated series and the comic books take place in their own 'what if' and alternate timelines and are not part of the main continuity. This show marked the debut television appearance of Bill Nye on a nationally broadcast show.\n\nFollowing the conclusion of \"Back to the Future Part III\", Dr. Emmett L. Brown settled in 1991 in Hill Valley with his new wife Clara, their sons Jules and Verne, and the family dog, Einstein, moved into a farm. As with the films, time travel was achieved through the use of a modified DeLorean, which had apparently been re-built after it was destroyed at the end of the trilogy. The DeLorean now has voice-activated \"time circuits\" and can also travel instantaneously to different locations in space and time, in addition to folding into a suitcase. The characters also traveled through time using the steam engine time machine Doc invented at the end of the third film.\n\nAlthough Marty McFly is the show's main character and Jennifer Parker making occasional appearances, the show focused primarily on the Brown family, whereas the movies focused on the McFly family. The film's villain, Biff Tannen, also appeared frequently. In addition, relatives of McFly, Brown, and Tannen families were plentiful in the past or future parallel time zones visited. Unlike the films, which took place entirely in Hill Valley and the surrounding area, the series frequently took the characters to exotic locations. At the end of every episode, Doc Brown would appear to do an experiment, often related to the episode's plot. The first season also included post-credits segments with Biff Tannen telling a joke related to the episode, alluding to Thomas F. Wilson's career as a stand-up comedian.\n\nThe music for the intro is a re-created version of \"Back in Time\", originally by Huey Lewis and the News (who also recorded \"The Power of Love\" for the first film). The intro begins with Doc Brown surprised when seeing the time on his watch, before he enters the DeLorean. As he drives away, he heads to May 19, 2015 where he picks up Marty McFly, to June 10, 1885 to collect Clara, and to prehistoric times to grab Jules and Verne, before returning to 1991 (they escape from Griff Tannen, Buford \"Mad Dog\" Tannen and a Biff-like dinosaur respectively).\n\nThe following scene depicts the group sitting down to dinner, before noticing Einstein is missing. They subsequently find him driving the steam train time machine to an unknown time. For the second season, the intro replaces the collection of the main characters with clips of the first season and ends with the same sequence from season one's intro.\n\n\nMary Steenburgen (Clara Clayton Brown) and Thomas F. Wilson (Biff Tannen) voiced their characters from the films. Christopher Lloyd played Doc Brown in the live-action segments which opened and closed each episode while Dan Castellaneta provided the animated Doc Brown's voice. James Tolkan was also featured as a guest voice in one episode, however he returned only to voice a completely different character as opposed to his Principal Strickland character from the films. In addition, Bill Nye appeared as Dr. Brown's Lab Assistant during the live-action segments at the end of each episode performing scientific experiments related to the episode. Nye also serves as the show's technical advisor. These segments later led to Nye getting his own show.\n\nAlthough the show no longer airs on television, nine VHS cassettes and three laserdisc volumes of the series were released from 1993 to 1994, chronicling 18 of the 26 episodes. The entire complete show was released on DVD on October 20, 2015 for the first time, both individually and as part of the \"Back to the Future: The Complete Adventures\" collection (which also includes all three movies of the trilogy). In addition, the first episode from each season of the animated series (\"Brothers\" and \"Mac the Black\") are included as bonus materials in the \"Back to the Future: 30th Anniversary Trilogy\" set.\n\nOn June 14, 2016, Universal released an individual DVD of the show's first season in Region 1. Season 2 was released on September 13, 2016.\n\nDaytime Emmy Awards\n\nA comic book series was published by Harvey Comics detailing further adventures of the animated show. Two mini-series were published, the first being a four-issue run, the second, a three-issue run subtitled \"Forward to the Future\" and a \"Special\" issue was also released, reprinting parts of the first mini-series' first issue. The comics were written by Dwayne McDuffie with art by Nelson Dewey.\n\n"}
{"id": "26902035", "url": "https://en.wikipedia.org/wiki?curid=26902035", "title": "Cardinal number (linguistics)", "text": "Cardinal number (linguistics)\n\nIn linguistics, more precisely in traditional grammar, a cardinal number or cardinal numeral (or just cardinal) is a part of speech used to count, such as the English words \"one\", \"two\", \"three\", but also compounds, e.g. \"three hundred and forty-two\" (Commonwealth English) or \"three hundred forty-two\" (American English). Cardinal numbers are classified as definite numerals and are related to ordinal numbers, such as \"first\", \"second\", \"third\", etc.\n\n\nNotes\n"}
{"id": "57734324", "url": "https://en.wikipedia.org/wiki?curid=57734324", "title": "Cetobacterium somerae", "text": "Cetobacterium somerae\n\nCetobacterium somerae is a Gram-negative, microaerotolerant, non-spore-forming and rod-shaped bacterium from the genus of \"Cetobacterium\" which has been isolated from human feces. \"Cetobacterium somerae\" occur in intestinal tracts of freshwater fish.\nCetobacterium somerae produces cobalamin.\n"}
{"id": "53790076", "url": "https://en.wikipedia.org/wiki?curid=53790076", "title": "Conformon", "text": "Conformon\n\nFrom a biological standpoint, the goal-directed molecular motions inside living cells are carried out by biopolymers acting like molecular machines (e.g. myosin, RNA/DNA polymerase, ion pumps, etc.). These molecular machines are driven by conformons, that is sequence-specific mechanical strains generated by free energy released in chemical reactions or stress induced destabilisations in supercoiled biopolymer chains. Therefore, conformons can be defined as packets of conformational energy generated from substrate binding or chemical reactions and confined within biopolymers.\n\nOn the other hand, from a physics standpoint, the conformon is a localization of elastic and electronic energy which may propagate in space with or without dissipation. The mechanism which involves dissipationless propagation is a form of molecular superconductivity. On quantum mechanical level both elastic/vibrational and electronic energy can be quantised, therefore the conformon carries a fixed portion of energy. This has led to the definition of quantum of conformation (shape).\n"}
{"id": "28931489", "url": "https://en.wikipedia.org/wiki?curid=28931489", "title": "Consistent pricing process", "text": "Consistent pricing process\n\nA consistent pricing process (CPP) is any representation of (frictionless) \"prices\" of assets in a market. It is a stochastic process in a filtered probability space formula_1 such that at time formula_2 the formula_3 component can be thought of as a price for the formula_3 asset. \n\nMathematically, a CPP formula_5 in a market with d-assets is an adapted process in formula_6 if \"Z\" is a martingale with respect to the physical probability measure formula_7, and if formula_8 at all times formula_2 such that formula_10 is the solvency cone for the market at time formula_2.\n\nThe CPP plays the role of an equivalent martingale measure in markets with transaction costs. In particular, there exists a 1-to-1 correspondence between the CPP formula_12 and the EMM formula_13.\n"}
{"id": "6640441", "url": "https://en.wikipedia.org/wiki?curid=6640441", "title": "Crew Exploration Vehicle", "text": "Crew Exploration Vehicle\n\nThe Crew Exploration Vehicle (CEV) was the conceptual component of the U.S. NASA Vision for Space Exploration that later became known as the Orion spacecraft. The Orion CEV was part of NASA's Constellation Program to send human explorers back to the Moon, and then onward to Mars and other destinations in the solar system.\n\nOfficial planning for the vehicle began in 2004, with the final Request For Proposal issued on March 1, 2005, to begin a design competition for the vehicle. For the later design and construction phases, see Orion (spacecraft). NASA has posted project status notes at the NASA.gov website, under the name \"Orion Crew Exploration Vehicle\" (see External links, below).\n\nThe concept for the vehicle was officially announced in a speech given by George W. Bush at NASA Headquarters on January 14, 2004. The Draft Statement of Work for the CEV was issued by NASA on December 9, 2004, and slightly more than one month later, on January 21, 2005, NASA issued a Draft Request For Proposal (RFP). The Final RFP was issued on March 1, 2005, with the potential bidders being asked to answer by May 2, 2005. \n\nNASA had planned to have a suborbital or an Earth orbit fly-off called \"Flight Application of Spacecraft Technologies\" (FAST) between two teams' CEV designs before September 1, 2008. However, in order to permit an earlier date for the start of CEV operations, Administrator Michael D. Griffin had indicated that NASA would select one contractor for the CEV in 2006. From his perspective, this would both help eliminate the currently planned four-year gap between the retirement of the Shuttle in 2010 and the first manned flight of the CEV in 2014 (by allowing the CEV to fly earlier), and save over $1 billion for use in CEV development.\n\nOn June 13, 2005, NASA announced the selection of two consortia, Lockheed Martin Corp. and the team of Northrop Grumman Corp. and The Boeing Co. for further CEV development work. Each team had received a US$28 million contract to come up with a complete design for the CEV and its launch vehicle until August 2006, when NASA would award one of them the task of building the CEV. The teams would also have to develop a plan for their CEV to take part in the assembly of a lunar expedition, either with an Earth orbit rendezvous, a lunar orbit rendezvous, or with a direct ascent. The two teams were composed of:\n\n\nEach contractor-led team included subcontractors that provided the lunar expedition astronauts with equipment, life support, rocket engines, and onboard navigation systems. The planned orbital or suborbital fly-offs under FAST would have seen the competition of a CEV built by each team, or of a technology demonstrator incorporating CEV technologies. Under FAST, NASA would have chosen the winner to build the final CEV after actual demonstration of this hardware. Fly-offs are often used by the U.S. Air Force to select military aircraft; NASA has never used this approach in awarding contracts. However, as Administrator Griffin had indicated he would abandon the FAST approach, NASA pursued the more traditional approach of selecting a vehicle based on the contractors' proposals.\n\nOn August 31, 2006, NASA announced that the contract to design and develop the Orion was awarded to Lockheed Martin Corp. According to Bloomberg News, five analysts it surveyed prior to the award announcement tipped the Northrop team to win. Marco Caceres, a space industry analyst with Teal Group, had projected that Lockheed would lose, partly because of Lockheed Martin's earlier failure on the $912 million X-33 shuttle replacement program; after the contract award he suggested that Lockheed Martin's work on the X-33 gave it more recent research and development experience in propulsion and materials, which may have helped it win the contract. According to an Aerospace Daily & Defense Report summary of a NASA document explaining the rationale for the contract award, the Lockheed Martin proposal won on the basis of a superior technical approach, lower and more realistic cost estimates, and exceptional performance on Phase I of the CEV program.\n\nLockheed Martin plans to manufacture the manned spacecraft at facilities in Texas, Louisiana, and Florida.\n\nLockheed's proposed craft was a small Space Shuttle shaped lifting body design big enough for six astronauts and their equipment. Its airplane-shaped design made it easier to navigate during high-speed returns to Earth than the capsule-shaped vehicles of the past, according to Lockheed Martin. According to the French daily \"Le Figaro\" and the publication \"Aviation Week and Space Technology\", EADS SPACE Transportation would be in charge of the design and construction of the associated \"Mission Module\" (MM). The head of the Lockheed team was Cleon Lacefield. \n\nThe Lockheed Martin CEV design included several modules in the LEO (low earth orbit) and manned lunar versions of the spacecraft, plus an abort system. The abort system was an escape tower like that used in the Mercury, Apollo, Soyuz, and Shenzhou craft (Gemini, along with the Space Shuttles \"Enterprise\" and \"Columbia\" [until STS-4] used ejection seats). It would be capable of an abort during any part of the ascent phase of the mission. The crew would sit in the Rescue Module (RM) during launch. According to the publication \"Aviation Week and Space Technology\", the RM would have an outer heat shield of reinforced carbon-carbon and a redundant layer of felt reusable surface insulation underneath in case of RCC failure. The RM comprised the top half of the Crew Module (CM), which comprised the RM and the rest of the lifting-body structure. The CM included living space for four crew members. In an emergency the RM separates from the rest of the CM. The RM would seat up to six crew members, with two to a row, and the CM has living space and provisions for four astronauts for 5–7 days. Extra-Vehicular Activities (EVAs) could be conducted from the CM, which could land on land or water and could be reused 5–10 times.\n\nThe mission module would be added to the bottom of the CEV for a lunar mission, and would be able to hold extra consumables and provide extra space for a mission of lunar duration. It would also provide extra power and communications capabilities, and include a docking port for the Lunar Surface Access Module (LSAM). On the bottom of the lunar CEV stack would be the Propulsion or Trans-Earth Injection Module (TEIM) which would provide for return to the Earth from the Moon. It would probably incorporate (according to Aviation Week) 2 Pratt & Whitney RL-10 engines. Together, the RM/CM, MM, and TEIM made up the Lockheed Martin lunar stack. The original idea was to launch the CM, MM, and TEIM on three separate Evolved Expendable Launch Vehicles (EELVs), with one component in each launch. This vehicle would need additional modules to reach lunar orbit and to land on the Moon. However, this plan was to be altered according to the CFI (Call for Improvements), described below. \n\nUnlike the well-publicized Lockheed Martin CEV design, virtually no information was publicly available on the Boeing/Northrop Grumman CEV design. However, it is instructive to note that most publicly released Boeing designs for the canceled Orbital Space Plane (OSP) resembled the Apollo capsule. It was possible that the Boeing CEV is a capsule rather than a lifting body or plane design.\n\nSean O'Keefe's strategy would have seen the CEV development in two distinct phases. Phase I would have involved the design of the CEV and a demonstration by the potential contractors that they could safely and affordably develop the vehicle. Phase I would have run from bid submissions in 2005 to FAST (by Sept 2008) and downselect to one contractor. Phase II would have begun after FAST and involved final design and construction of the CEV. However, this schedule was unacceptably slow to Mike Griffin, and the plan was changed such that NASA will issue a \"Call for Improvements\" (CFI) after the release of the ESAS for Lockheed Martin and Boeing to submit Phase II proposals. NASA chose Lockheed Martin's consortium as the winning consortium on August 31, 2006. Therefore, the CEV bids submitted and described above are not necessarily representative of the final CEV design, as they will be changed in accordance with the CFI and any findings of the ESAS that are put into the CFI.\n\n"}
{"id": "54059729", "url": "https://en.wikipedia.org/wiki?curid=54059729", "title": "Cuckold Formation", "text": "Cuckold Formation\n\nThe Cuckold Formation is a stratigraphic unit of the Ediacaran Signal Hill Group, cropping out on eastern Newfoundland; it comprises red conglomerates and sandstones.\n"}
{"id": "3974014", "url": "https://en.wikipedia.org/wiki?curid=3974014", "title": "Ease of movement", "text": "Ease of movement\n\nEase of movement (EMV) is an indicator used in technical analysis to relate an asset's price change to its volume. Ease of Movement was developed by Richard W. Arms, Jr. and highlights the relationship between volume and price changes and is particularly useful for assessing the strength of a trend. High positive values indicate the price is increasing on low volume: strong negative values indicate the price is dropping on low volume. The moving average of the indicator can be added to act as a trigger line, which is similar to other indicators like the MACD.\n"}
{"id": "36974389", "url": "https://en.wikipedia.org/wiki?curid=36974389", "title": "Egide Fologne", "text": "Egide Fologne\n\nEgide Fologne (1830-1919) was a Belgian entomologist who specialised in microlepidoptera.\n\nHe was a Member of Société entomolologique de Belgique.\n\nPartial list\n\n\n"}
{"id": "15052313", "url": "https://en.wikipedia.org/wiki?curid=15052313", "title": "Entertainment district", "text": "Entertainment district\n\nAn entertainment district is a type of arts district with a high concentration of movie theaters, theatres or other entertainment venues. Such areas may be officially designated by local governments with functional zoning regulations, as well as public and private investment in distinctive urban design.\n\n\nEntertainment districts have economically helped downtowns and cities thrive through development of various businesses that attract tourism and commerce. As entertainment districts tend to create crowds, facilitation and management by police is required to promote safety and regulation. The main source that requires police interaction in the entertainment districts are the nightclubs and bars that may promote intoxication and leads towards miscreant behavior. Police and security deal with behavior consisting of bar fights, overcrowding, and public urination. The requirement of regulation and maintenance of these areas are important for the safety of the individuals utilizing and benefiting from the entertainment districts.\n\n"}
{"id": "6935378", "url": "https://en.wikipedia.org/wiki?curid=6935378", "title": "Equinox (TV series)", "text": "Equinox (TV series)\n\nEquinox was a long-running Channel 4 popular science and documentary programme. The series ran from 31 July 1986 to 21 December 2006, originally airing on a weekly basis.\n\nThe number of films per series fell over the years, from eighteen one-hour films a year originally to twelve by the late 1990s. The last regular series was shown on 21 December 2006, with twelve films. One-off films have occasionally been aired under the title \"\"Equinox\" Special\" (e.g. the 90-minute \"Secrets of the Super Psychics\" first transmitted in 1997).\n\n\n\n<br>\n"}
{"id": "5939399", "url": "https://en.wikipedia.org/wiki?curid=5939399", "title": "European League of Stuttering Associations", "text": "European League of Stuttering Associations\n\nThe European League of Stuttering Associations (ELSA) was set up in 1990 by organisations in 12 countries to promote a greater knowledge and understanding of stuttering and to bring together, as a top umbrella organisation, the national stuttering self-help organisations of Europe.\n\nELSA is a trans-national, cross-cultural organisation. It seeks resources only open to multi-national groups, extends the exchange-of-information network, and lobbies for stutterers at a prominent international level.\n\nIts main roles are:\n\n\nELSA's work has been recognised by the International Labour Office in Geneva, the World Health Organization in Geneva and the United Nations Office in Vienna. ELSA is also a founding member of the European Disability Forum, an umbrella disability organisation based in Brussels.\n\nThe current executive board consists of Edwin Farr MBE (Chair), Anita Blom (Vice-Chair) and Richard Bourgondiën (Webmaster).\n\nThe Association publishes a newsletter, \"One Voice\", which is published twice a year and is a joint project with the International Stuttering Association.\n\nTogether with the International Fluency Association and the International Stuttering Association, the International Stuttering Association celebrates, every 22 October, International Stuttering Awareness Day (ISAD) which includes an online conference on stuttering and a media campaign.\n\n\n"}
{"id": "16721711", "url": "https://en.wikipedia.org/wiki?curid=16721711", "title": "FSC15307+3253", "text": "FSC15307+3253\n\nFSC15307+3253 (or IRAS F15307+3252) is an ultraluminous infrared galaxy (ULIRG), with a luminosity between 8 and 1000 µm of approximately 2 L, possibly the highest currently known. The \"FSC\" refers to \"Faint Source Catalogue\", one of the source catalogs produced by the IRAS infrared survey mission. The emission is believed due to some combination of starburst activity and accretion onto a super-massive black hole, producing primary radiation at shorter wavelengths which is mostly blocked by obscuring dust, which is in turn heated and re-radiates in the infrared. The redshift of the source is \"z\" = 0.93, indicating a distance of the order of 7 billion light years.\n"}
{"id": "24702673", "url": "https://en.wikipedia.org/wiki?curid=24702673", "title": "Fermi motion", "text": "Fermi motion\n\nThe Fermi motion is the quantum motion of nucleons bound inside a nucleus. It was once posited as an explanation for the EMC effect..\n"}
{"id": "10761675", "url": "https://en.wikipedia.org/wiki?curid=10761675", "title": "Form classification", "text": "Form classification\n\nForm classification is the classification of organisms based on their morphology, which does not necessarily reflect their biological relationships. Form classification, generally restricted to palaeontology, reflects uncertainty; the goal of science is to move \"form taxa\" to biological taxa whose affinity is known.\n\nForm taxonomy is restricted to fossils that preserve too few characters for a conclusive taxonomic definition or assessment of their biological affinity, but whose study is made easier if a binomial name is available by which to identify them. The term \"form classification\" is preferred to \"form taxonomy\"; taxonomy suggests that the classification implies a biological affinity, whereas form classification is about giving a name to a group of morphologically-similar organisms that may not be related.\n\nA \"parataxon\" (not to be confused with parataxonomy), or \"sciotaxon\" (Gr. \"shadow taxon\"), is a classification based on incomplete data: for instance, the larval stage of an organism that cannot be matched up with an adult. It reflects a paucity of data that makes biological classification impossible. A sciotaxon is defined as a taxon thought to be equivalent to a true taxon (orthotaxon), but whose identity cannot be established because the two candidate taxa are preserved in different ways and thus cannot be compared directly.\n\nForm taxa are groupings that are based on common overall forms. Early attempts at classification of labyrinthodonts was based on skull shape (the heavily armoured skulls often being the only preserved part). The amount of convergent evolution in the many groups lead to a number of polyphyletic taxa. Such groups are united by a common mode of life, often one that is generalist, in consequence acquiring generally similar body shapes by convergent evolution. Ediacaran biota — whether they are the precursors of the Cambrian explosion of the fossil record, or are unrelated to any modern phylum — can currently only be grouped in \"form taxa\". Other examples include the seabirds and the \"Graculavidae\". The latter were initially described as the earliest family of Neornithes but are nowadays recognized to unite a number of unrelated early neornithine lineages, several of which probably later gave rise to the \"seabird\" form taxon of today.\n\nFossil eggs are classified according to the parataxonomic system called Veterovata. There are three broad categories in the scheme, on the pattern of organismal phylogenetic classification, called oofamilies, oogenera and oospecies (collectively known as ootaxa). The names of oogenera and oofamilies conventionally contain the root \"oolithus\" meaning \"stone egg\", but this rule is not always followed. They are divided up into several basic types: Testudoid, Geckoid, Crocodiloid, Dinosauroid-spherulitic, Dinosauroid-prismatic, and Ornithoid.\n\nIn paleobotany, two terms were formerly used in the codes of nomenclature, \"form genera\" and \"organ genera\", to mean groups of fossils of a particular part of a plant, such as a leaf or seed, whose parent plant is not known because the fossils were preserved unattached to the parent plant. A later term \"morphotaxa\" also allows for differences in preservational state. These three terms have been replaced as of 2011 by provisions for \"fossil-taxa\" that are more similar to the provisions for other types of plants.\n\nNames given to organ genera could only be applied to the organs in question, and could not be extended to the entire organism. Fossil-taxon names can cover several parts of an organism, or several preservational states, but do not compete for priority with any names for the same organism that are based on a non-fossil type.\n\nThe part of the plant was often, but not universally, indicated by the use of a suffix in the generic name:\n\n\"Form taxon\" can more casually be used to describe a wastebasket taxon: either a taxon that is not a natural (monophyletic) group but united by shared plesiomorphies, or a presumably artificial group of organisms whose true relationships are not known, being obscured by ecomorphological similarity. Well-known form taxa of this kind include \"ducks\", \"fish\", \"reptiles\" and \"worms\".\n\n"}
{"id": "402119", "url": "https://en.wikipedia.org/wiki?curid=402119", "title": "Game studies", "text": "Game studies\n\nGame studies, or ludology, is the study of games, the act of playing them, and the players and cultures surrounding them. It is a discipline of cultural studies that deals with all types of games throughout history. This field of research utilizes the tactics of, at least, anthropology, sociology and psychology, while examining aspects of the design of the game, the players in the game, and finally, the role the game plays in its society or culture. Game studies is oftentimes confused with the study of video games, but this is only one area of focus; in reality game studies encompasses all types of gaming, including sports, board games, etc.\n\nBefore video games, game studies often only included anthropological work, studying the games of past societies. However, once video games were introduced and became mainstream, game studies were updated to perform sociological and psychological observations; to observe the effects of gaming on an individual, his or her interactions with society, and the way it could impact the world around us.\n\nThere are three main approaches to game studies: the social science approach asks itself how games affect people and uses tools such as surveys and controlled lab experiments. The humanities approach asks itself what meanings are expressed through games, and uses tools such as ethnography and patient observation. The industrial and engineering approach applies mostly to video games and less to games in general, and examines things such as computer graphics, artificial intelligence, and networking. Like other media disciplines, such as television and film studies, game studies often involves textual analysis and audience theory.\n\nIt wasn’t until Irving Finkel organized a colloquium in 1990 that grew into the International Board Game Studies Association, Gonzalo Frasca popularized the term \"ludology\" (from the Latin word for game, ludus) in 1999, the publication of the first issues of academic journals like \"Board Game Studies\" in 1998 and \"Game Studies\" in 2001, and the creation of the Digital Games Research Association in 2003, that scholars began to get the sense that the study of games could (and should) be considered a field in its own right. As a young field, it gathers scholars from different disciplines that had been broadly studying games, such as psychology, anthropology, economy, education, and sociology. The earliest known use of the term \"ludology\" occurred in 1982, in Mihaly Csikszentmihalyi's “Does Being Human Matter – On Some Interpretive Problems of Comparative Ludology.”\n\nOne of the earliest social science theories (1971) about the role of video games in society involved violence in video games, later becoming known as the catharsis theory. The theory suggests that playing video games in which you perform violent acts might actually channel latent aggression, resulting in less aggression in the players real lives. However, a meta-study performed by Craig A. Anderson and Brad J. Bushman, in 2001, examined data starting from the 1980s up until the article was published. The purpose of this study was to examine whether or not playing violent video games led to an increase in aggressive behaviors. They concluded that exposure to violence in video games did indeed cause an increase in aggression. However, it has been pointed out, and even stressed, by psychologist Jonathan Freedman that this research was very limited and even problematic since overly strong claims were made and the authors themselves seemed extremely biased in their writings. More recent studies, such as the one performed by Christopher J. Ferguson at Texas A&M International University have come to drastically different conclusions. In this study, individuals were either randomly assigned a game, or allowed to choose a game, in both the randomized and the choice conditions exposure to violent video games caused no difference in aggression. A later study (performed by the same people) looked for correlations between trait aggression, violent crimes, and exposure to both real life violence and violence in video games, this study suggests that while family violence and trait aggression are highly correlated with violent crime, exposure to video game violence was not a good predictor of violent crime, having little to no correlation, unless also paired with the above traits that had a much higher correlation. Over the past 15 years, a large number of meta-studies have been applied to this issue, each coming to its own conclusion, resulting in little consensus in the ludology community. It is also thought that even nonviolent video games may lead to aggressive and violent behaviour. Anderson and Dill seem to believe that it may be due to the frustration of playing video games that could in turn result in violent, aggressive behaviour.\n\nGame designers Amy Jo Kim and Jane McGonigal have suggested that platforms which leverage the powerful qualities of video games in non-game contexts can maximize learning. Known as the gamification of learning, using game elements in non-game contexts extracts the properties of games from within the game context, and applies them to a learning context such as the classroom.\n\nAnother positive aspect of video games is its conducive character towards the involvement of a person in other cultural activities. The probability of game playing increases with the consumption of other cultural goods (e.g., listening to music or watching television) or active involvement in artistic activities (e.g., writing or visual arts production). Video games by being complementary towards more traditional forms of cultural consumption, inhibit thus value from a cultural perspective.\n\nMore sociologically-informed research has sought to move away from simplistic ideas of gaming as either 'negative' or 'positive', but rather seeking to understand its role and location in the complexities of everyday life.\n\nFor example, it has been suggested that the very popular MMO \"World of Warcraft\" could be used to study the dissemination of infectious diseases because of the accidental spread of a plague-like disease in the gameworld.\n\nA major focus in game studies is the debate surrounding narratology and ludology. Many ludologists believe that the two are unable to exist together, while others believe that the two fields are similar but should be studied separately. Many narratologists believe that games should be looked at for their stories, like movies or novels. The ludological perspective says that games are not like these other mediums due to the fact that a player is actively taking part in the experience and should therefore be understood on their own terms. The idea that a videogame is \"radically different to narratives as a cognitive and communicative structure\" has led the development of new approaches to criticism that are focused on videogames as well as adapting, repurposing and proposing new ways of studying and theorizing about videogames. A recent approach towards game studies starts with an analysis of interface structures and challenges the keyboard-mouse paradigm with what is called a \"ludic interface\".\n\nAcademics across both fields provide scholarly insight into the different sides of this debate. Gonzalo Frasca, a notable ludologist due to his many publications regarding game studies, argues that while games share many similar elements with narrative stories, that should not prevent games to be studied as games. He seeks not \"to replace the narratologic approach, but to complement it.\"\n\nJesper Juul, another notable ludologist, argues for a stricter separation of ludology and narratology. Juul argues that games \"for all practicality \"can not\" tell stories.\" This argument holds that narratology and ludology cannot exist together because they are inherently different. Juul claims that the most significant difference between the two is that in a narrative, events \"have to\" follow each other, whereas in a game the player has control over what happens.\n\nGarry Crawford and Victoria K. Gosling argue in favor of narratives being an essential part of games as \"it is impossible to isolate play from the social influences of everyday life, and in turn, play will have both intended and unintended consequences for the individual and society.\" \"The Last of Us\" is a video game released in 2013 that has been referred to as a narrative \"masterpiece.\" Proponents of the narratology side of game studies argue that \"The Last of Us\" and similar games that have followed it and preceded it, serve as examples that games can in fact tell stories.\n\nAs is common with most academic disciplines, there are a number of more specialized areas or sub-domains of study.\n\nAn emerging field of study looks at the \"pre-history\" of video games, suggesting that the origins of modern digital games lie in: fairground attractions and sideshows such as shooting games; early \"Coney Island\"-style pleasure parks with elements such as large roller-coasters and \"haunted house\" simulations; nineteenth century landscape simulations such as dioramas, panoramas, planetariums, and stereographs; and amusement arcades that had mechanical game machines and also peep-show film machines.\n\nIn light of population ageing, there has been an interest into the use of games to improve the overall health and social connectedness of ageing players. For example, Adam Gazzaley and his team have designed NeuroRacer (a game that improves cognitive tasks outside of the game among its 60+ year old participants), while the AARP has organized a game jam to improve older people's social connections. Researchers such as Sarah Mosberg Iversen have argued that most of the academic work on games and ageing has been informed by notions of economical productivity, while Bob De Schutter and Vero Vanden Abeele have suggested a game design approach that is not focused on age-related decline but instead is rooted in the positive aspects of older age.\n\nMassive multiplayer online games can give economists clues about the real world. Markets based on digital information can be fully tracked as they are used by players, and thus real problems in the economy, such as inflation, deflation and even recession. The solutions the game designers come up with can therefore be studied with full information, and experiments can be performed where the economy can be studied as a whole. These games allow the economists to be omniscient, they can find every piece of information they need to study the economy, while in the real world they have to work with presumptions.\n\nFormer Finance Minister of Greece and Valve's in-house economist Yanis Varoufakis studied \"EVE Online\" and argued that video game communities give economists a venue for experimenting and simulating the economies of the future. Edward Castronova has studied virtual economies within a variety of games including Everquest and World of Warcraft.\n\nThe psychological research into games has yielded theories on how playing video games may be advantageous for both children and for adults. Some theories claim that video games in fact help improve cognitive abilities rather than impede their development. These improvement theories include the improvement of visual contrast sensitivity. Other developments include the ability to locate something specific among various impediments. This is primarily done in first-person shooter games where the protagonist must look at everything in a first person view while playing. By doing this they increase their spatial attention due to having to locate something among an area of diversions. These games place the player in a high intensity environment where the player must remain observant of their surroundings in order to achieve their goal, e.g., shooting an enemy player, while impediments obstruct their gameplay in the virtual world.\n\nAnother cognitive enhancement provided by playing video games would be the improvement of brain functioning speed. This happens as the player is immersed in an unendingly changing environment where they are required to constantly think and problem solve while playing in order to do well in the game. This constant problem solving forces the brain to constantly run and so the speed of thought is sharpened greatly, because the need to think quickly is required to succeed. The attention span of the player is also benefited. High action video games, such as fighting or racing games, require the user’s constant attention and in the process the skill of concentration is sharpened.\n\nThe overcoming of the condition known as dyslexia is also considered an improvement due to the continuous utilization of controllers for the video games. This continuous process helps to train the users to overcome their condition which impedes in their abilities of interpretation. The ability of hand-eye coordination is also improved thanks in part to video games, due to the need to operate the controller and view the screen displaying the content all at the same time. The coordination of the player is enhanced due the playing and continuous observation of a video game since the game gives high mental stimulation and coordination is important and therefore enhanced due to the constant visual and physical movement that is produced from the playing of the video game.\n\nThe playing of video games can also help increase a player's social skills. This is done by playing online multiplayer games which can require constant communication, this leads to socialization between players in order to achieve the goal within the game they may be playing. In addition it can help the users to meet new friends over their online games and at the same time communicate with friends they have already made in the past; those playing together online would only strengthen their already established bond through constant cooperation. Some video games are specifically designed to aid in learning, because of this another benefit of playing video games could be the educational value provided with the entertainment. Some video games present problem solving questions that the player must think on in order to properly solve, while action orientated video games require strategy in order to successfully complete. This process of being forced to think critically helps to sharpen the mind of the player.\n\nGame Culture\n\nOne aspect of game studies is the study of gaming culture. People who play video games are a subculture of their own. Gamers will often form communities with their own languages, attend conventions where they will dress up as their favorite characters, and have gaming competitions. One of these conventions, Gamescon 2017, had a record attendance with over 350,000 attendees.\n\nDemographics of Gamers \n\nSee also\n\n"}
{"id": "1466015", "url": "https://en.wikipedia.org/wiki?curid=1466015", "title": "Gibrat's law", "text": "Gibrat's law\n\nGibrat's law (sometimes called Gibrat's rule of proportionate growth or the law of proportionate effect) is a rule defined by Robert Gibrat (1904–1980) in 1931 stating that the proportional rate of growth of a firm is independent of its absolute size. The law of proportionate growth gives rise to a distribution that is log-normal.\n\nGibrat's law is also applied to cities size and growth rate, where proportionate growth process may give rise to a distribution of city sizes that is log-normal, as predicted by Gibrat's law. While the city size distribution is often associated with Zipf's law, this holds only in the upper tail, because empirically the tail of a log-normal distribution cannot be distinguished from Zipf's law. A study using administrative boundaries (places) to define cities finds that the entire distribution of cities, not just the largest ones, is log-normal. But this last claim that the lognormal distribution cannot be rejected has been shown to be the result of a statistics with little power: the uniformly most powerful unbiased test comparing the lognormal to the power law shows unambiguously that the largest 1000 cities are distinctly in the power law regime.\n\nHowever, it has been argued that it is problematic to define cities through their fairly arbitrary legal boundaries (the places method treats Cambridge and Boston, Massachusetts, as two separate units). A clustering method to construct cities from the bottom up by clustering populated areas obtained from high-resolution data finds a power-law distribution of city size consistent with Zipf's law in almost the entire range of sizes. Note that populated areas are still aggregated rather than individual based. A new method based on individual street nodes for the clustering process leads to the concept of natural cities. It has been found that natural cities exhibit a striking Zipf's law Furthermore, the clustering method allows for a direct assessment of Gibrat's law. It is found that the growth of agglomerations is not consistent with Gibrat's law: the mean and standard deviation of the growth rates of cities follows a power-law with the city size.\n\nIn general, processes characterized by Gibrat's law converge to a limiting distribution, often proposed to be the log-normal, or a power law, depending on more specific assumptions about the stochastic growth process. However, the tail of the lognormal may fall off too quickly, and its PDF is not monotonic, but rather has a Y-intercept of zero probability at the origin. The typical power law is the Pareto I, which has a tail that cannot model fall-off in the tail at large outcomes size, and which does not extend downwards to zero, but rather must be truncated at some positive minimum value. More recently, the Weibull distribution has been derived as the limiting distribution for Gibrat processes, by recognizing that (a) the increments of the growth process are not independent, but rather correlated, in magnitude, and (b) the increment magnitudes typically have monotonic PDFs. The Weibull PDF can appear essentially log-log linear over orders of magnitude ranging from zero, while eventually falling off at unreasonably large outcome sizes.\n\nIn the study of the firms (business), the scholars do not agree that the foundation and the outcome of Gibrat's law are empirically correct.\n\n\n"}
{"id": "42221636", "url": "https://en.wikipedia.org/wiki?curid=42221636", "title": "Grant Mossop", "text": "Grant Mossop\n\nGrant Mossop (1948 - 7 October 2005) was a geologist from Canada. He earned a B.Sc. in 1970 and an M.Sc. in 1971 from the University of Calgary followed by a Ph.D. in geology from the University of London. He served as the Director of the Institute of Sedimentary and Petroleum Geology, Geological Survey of Canada Calgary. A graduate scholarship in his name was established in 2009.\n\nHe was awarded the Ambrose Medal of the Geological Association of Canada in 1995. \n"}
{"id": "19975545", "url": "https://en.wikipedia.org/wiki?curid=19975545", "title": "Grex (horticulture)", "text": "Grex (horticulture)\n\nThe term grex (pl. greges or grexes; abbreviation gx), derived from the Latin noun \"grex, gregis\" meaning 'flock', has been coined to expand botanical nomenclature to describe hybrids of orchids, based solely on their parentage. Grex names are one of the three categories of plant names governed by the International Code of Nomenclature for Cultivated Plants; within a grex the \"cultivar group\" category can be used to refer to plants by their shared characteristics (rather than by their parentage), and individual orchid plants can be selected (and propagated) and named as cultivars.\n\nWhen the name of a grex is first established, a description is required that specifies two particular parents, where each parent is specified either as a species (or nothospecies) or as a grex. The grex name then applies to all hybrids between those two parents. There is a permitted exception if the full name of one of the parents is known but the other is known only to genus level or nothogenus level.\n\nNew grex names are now established by the Royal Horticultural Society, which receives applications from orchid hybridizers.\n\nThe horticultural nomenclature of grexes exists within the framework of the botanical nomenclature of hybrid plants. Interspecific hybrids occur in nature, and are treated under the International Code of Nomenclature for algae, fungi, and plants as nothospecies, ('notho' indicating hybrid). They can optionally be given Linnean binomials with a multiplication sign \"×\" before the species epithet for example \"Crataegus\" × \"media\". An offspring of a nothospecies, either with a member of the same nothospecies or any of the parental species as the other parent, has the same nothospecific name. The nothospecific binomial is an alias for a list of the ancestral species, whether the ancestry is precisely known or not.\n\nFor example:\n\nAn earlier term was nothomorph for subordinate taxa to nothospecies. Since the 1982 meeting of the International Botanical Congress, such subordinate taxa are considered varieties (nothovars).\n\nBecause many interspecific (and even intergeneric) barriers to hybridization in the Orchidaceae are maintained in nature only by pollinator behavior, it is easy to produce complex interspecific and even intergeneric hybrid orchid seeds: all it takes is a human motivated to use a toothpick, and proper care of the mother plant as it develops a seed pod. Germinating the seeds and growing them to maturity is more difficult, however.\n\nWhen a hybrid cross is made, all of the seedlings grown from the resulting seed pod are considered to be in the same grex. Any additional plants produced from the hybridization of the same two parents (members of the same species or greges as the original parents) also belong to the grex. Reciprocal crosses are included within the same grex. If two members of the same grex produce offspring, the offspring receive the same grex name as the parents.\n\nIf a parent of a grex becomes a synonym, any grex names that were established by specifying the synonym are not necessarily discarded; the grex name that was published first is used (the Principle of Priority).\n\nAll of the members of a specific grex may be loosely thought of as \"sister plants\", and just like the brothers and sisters of any family, may share many traits or look quite different from one another. This is due to the randomization of genes passed on to progeny during sexual reproduction. The hybridizer who created a new grex normally chooses to register the grex with a registration authority, thus creating a new grex name, but there is no requirement to do this. Individual plants may be given cultivar names to distinguish them from siblings in their grex. Cultivar names are usually given to superior plants with the expectation of propagating that plant; all genetically identical copies of a plant, regardless of method of propagation (divisions or clones) share a cultivar name.\nThe grex name differs from a species name in that the gregaric part of the name is capitalized, is not italicized, and may consist of more than one word (limited to 30 characters in total, excluding spaces).\n\nFor example: an artificially produced hybrid between \"Cattleya warscewiczii\" and \"C. dowiana\" (or \"C. aurea\", which the RHS, the international orchid hybrid registration authority, considers to be a synonym of \"C. dowiana\") is called \"C.\" Hardyana gx. An artificially produced seedling that results from pollinating a \"C.\" Hardyana gx with another \"C.\" Hardyana gx is also a \"C.\" Hardyana gx. However, the hybrid produced between \"Cattleya\" Hardyana gx and \"C. dowiana\" is not \"C.\" Hardyana gx, but \"C.\" Prince John gx. In summary:\n\nA grex and a nothospecies can have the same parentage, but are not equivalent because the nothospecies includes back-crosses and the grex does not. They can even have the same epithet, distinguished by typography (see botanical name for explanation of epithets), although since January 2010 it is not permitted to publish such grex names if the nothospecies name already exists.\n\nHybrids between a grex and a species/nothospecies are named as grexes, but this is not permitted if the nothospecies parent has the same parentage as the grex parent. That situation is a back-cross, and the nothospecies name is applied to the progeny.\n\n\n"}
{"id": "15011503", "url": "https://en.wikipedia.org/wiki?curid=15011503", "title": "Ice VIII", "text": "Ice VIII\n\nIce VIII is a tetragonal crystalline form of ice formed from ice VII by cooling it below 5 °C. It is more ordered than ice VII, since the hydrogen atoms assume fixed positions.\n\nOrdinary water ice is known as ice I, (in the Bridgman nomenclature). Different types of ice, from ice II to ice XVI, have been created in the laboratory at different temperatures and pressures.\n\n"}
{"id": "31164029", "url": "https://en.wikipedia.org/wiki?curid=31164029", "title": "International Conference on Digital Audio Effects", "text": "International Conference on Digital Audio Effects\n\nThe annual International Conference on Digital Audio Effects or DAFx Conference is a meeting of enthusiasts working in research areas on audio signal processing, acoustics, and music related disciplines, who come together to present and discuss their findings. The conference evolved from an EU-COST-G6 project “Digital Audio Effects” in 1998.\n\nThe acronym DAFx stands for \"Digital Audio Effects\" and is also the name of a book\nwhich was written by people in the community around the conference\n\nA list of past and upcoming conferences together with an archive of all proceedings can be found at the website.\n\n\n"}
{"id": "10896869", "url": "https://en.wikipedia.org/wiki?curid=10896869", "title": "Jonathan M. Dorfan", "text": "Jonathan M. Dorfan\n\nJonathan Manne Dorfan (born October 10, 1947) is a particle physicist and the former president of the Okinawa Institute of Science and Technology, Graduate University. He is a former director of the Stanford Linear Accelerator Center (1999–2007; SLAC). He received his B.Sc. at the University of Cape Town in South Africa in 1969 and his Ph.D. from the University of California, Irvine in 1976. Thereafter he worked on various projects at SLAC including MARK II and BaBar before becoming director in 1999. He has served on numerous advisory boards, including the High Energy Physics Advisory Panel of the U.S. Department of Energy, the Scientific Advisory Board of the Max Planck Institute for Physics, the Board of Governors for the Weizmann Institute of Science, the Accelerator Advisory Panel of the International Linear Collider Global Design Effort, the Machine Advisory Committee for Italy's SuperB Project, the Advisory Board of the John Adams Institute for Accelerator Science, and the Board of Governors and Board of Councilors of the Okinawa Institute of Science and Technology. \n\nDorfan was named president-elect of the Okinawa Institute of Science and Technology (OIST) Graduate University in Okinawa, Japan in 2010, and he became president of the new university in November 2011. He has been credited with significantly expanding the faculty of the university, especially in the physical sciences. \n\nDorfan was awarded the American Physical Society's 2016 W.K.H. Panofsky Prize in Experimental Particle Physics. The official citation states, “For leadership in the BABAR and Belle experiments, which established the violation of CP symmetry in B meson decay, and furthered our understanding of quark mixing and quantum chromodynamics”. Drs. David Hitlin, Stephen Olsen and Fumihiko Takasaki were co-awardees.(https://www.oist.jp/news-center/news/2015/10/16/oist-president-wins-particle-physics-prize)\n\nOn 7 November 2017, the Emperor of Japan, awarded Dr. Jonathan Dorfan, President Emeritus of the Okinawa Institute of Science and Technology Graduate University (OIST), the Order of the Rising Sun, Gold and Silver Star, one of Japan's most distinguished national decorations. The official citation states that the decoration is in recognition of \"contributions to the development of our nation's research and education in science and technology\". \n\n"}
{"id": "30204976", "url": "https://en.wikipedia.org/wiki?curid=30204976", "title": "Leopoldo García-Colín", "text": "Leopoldo García-Colín\n\nLeopoldo García-Colín Scherer (born 27 November 1930 in Mexico City, died 8 October 2012 in Mexico City) was a Mexican scientist specialized in Thermodynamics and Statistical Mechanics who received the National Prize for Arts and Sciences in 1988.\n\nHe was a member of The National College, a former president of the Mexican Society of Physics (\"SMF\", 1972–1973) and has received honorary degrees from several universities, including the National Autonomous University of Mexico (UNAM) and the Metropolitan Autonomous University (UAM).\n\nHe obtained a Chemistry degree at Universidad Nacional Autónoma de México (UNAM) in 1953. The Ph.D. in Physics at the University of Maryland in 1959. He was professor at Benemérita Universidad Autónoma de Puebla from 1960 to 1963, and at Facultad de Ciencias in UNAM from 1967 to 1984.\nLater, he participated in research at the Centro Nuclear de Salazar, a Nuclear center in Mexico; assistant director of Basic Investigation of Processes at Instituto Mexicano del Petróleo from 1967 to 1974, professor at Instituto de Investigaciones de Materiales in UNAM during the period from 1984 to 1988. At the end of his career he had a tenure position at Universidad Autónoma Metropolitana at Iztapalapa. He was elected to El Colegio Nacional in September 12, 1977. He was member of the Sistema Nacional de Investigadores with the highest level, III, since 1988.\nHe received a honris causa Ph. D from the National University of Mexico in April, 2007.\n\n"}
{"id": "6205696", "url": "https://en.wikipedia.org/wiki?curid=6205696", "title": "List of anarchist movements by region", "text": "List of anarchist movements by region\n\nThis is a list of anarchist movements by region, both geographical and/or political.\n\n"}
{"id": "2979338", "url": "https://en.wikipedia.org/wiki?curid=2979338", "title": "List of computer science conferences", "text": "List of computer science conferences\n\nThis is a list of academic conferences in computer science. Only conferences with separate articles are included; within each field, the conferences are listed alphabetically by their short names.\n\n\nConferences accepting a broad range of topics from theoretical computer science, including algorithms, data structures, computability, computational complexity, automata theory and formal languages:\n\n\nConferences whose topic is algorithms and data structures considered broadly, but that do not include other areas of theoretical computer science such as computational complexity theory:\n\n\nConferences on computational geometry, graph drawing, and other application areas of geometric computing:\n\n\n\nConferences on programming languages, programming language theory and compilers:\n\nConferences on software engineering:\n\nConferences on formal methods in software engineering, including formal specification, formal verification, and static code analysis:\n\nConferences on concurrent, distributed, and parallel computing, fault-tolerant systems, and dependable systems:\n\nConferences on high-performance computing, cluster computing, and grid computing:\n\nConferences on operating systems, storage systems and middleware:\n\n\nConferences on computer architecture:\n\n\nConferences on computer hardware:\n\n\nConferences on computer-aided design and electronic design automation:\n\n\nConferences on computer networking:\n\n\nWireless networks and mobile computing, including ubiquitous and pervasive computing, wireless ad hoc networks and wireless sensor networks:\n\n\nConferences on computer security and privacy:\n\nCryptography conferences:\n\n\nConferences on databases, information systems, information retrieval, data mining and the world wide web:\n\n\nConferences on artificial intelligence and machine learning:\n\n\nConferences on Evolutionary computation.\n\nConferences on automated reasoning:\n\nConferences on computer vision (including also image analysis) and pattern recognition:\n\n\nConferences on computational linguistics and natural language processing:\n\n\nConferences on computer graphics, geometry processing, image processing, and multimedia:\n\nConferences on scientific visualization and information visualization:\n\nConferences on human–computer interaction and user interfaces:\n\n\nConferences on bioinformatics and computational biology:\n\n\nConferences on computer science education and electronic learning:\n\n\n\n"}
{"id": "1890938", "url": "https://en.wikipedia.org/wiki?curid=1890938", "title": "List of things named after Charles Darwin", "text": "List of things named after Charles Darwin\n\nSeveral places, concepts, institutions, and things are namesakes of the English biologist Charles Darwin:\n\n\n\nSome 250 species and several higher groups bear Darwin's name; most are insects.\n\n\n\n"}
{"id": "55419244", "url": "https://en.wikipedia.org/wiki?curid=55419244", "title": "Lists of mobile computers", "text": "Lists of mobile computers\n\nLists of mobile computers\n\nBy product type:\n\n\nLists that include currently available products:\n\n\n\n\nLists without any current products:\n\n"}
{"id": "7480035", "url": "https://en.wikipedia.org/wiki?curid=7480035", "title": "Machine Age", "text": "Machine Age\n\nThe Machine Age is an era that includes the early 20th century, sometimes also including the late 19th century. An approximate dating would be about 1880 to 1945. Considered to be at a peak in the time between the first and second world wars, it forms a late part of the Second Industrial Revolution. The 1940s saw the beginning of the Atomic Age, where modern physics saw new applications such as the atomic bomb, the first computers, and the transistor. The Digital Revolution ended the intellectual model of the machine age founded in the mechanical and heralding a new more complex model of high technology. The digital era has been called the Second Machine Age, with its increased focus on machines that do mental tasks.\n\nArtifacts of the Machine Age include:\n\n\n\n\nThe Machine Age is considered to have influenced:\n\n"}
{"id": "33774041", "url": "https://en.wikipedia.org/wiki?curid=33774041", "title": "Manus Trench", "text": "Manus Trench\n\nManus Trench is an oceanic trench in the Bismarck Sea north of Papua New Guinea delineating the plate tectonic boundary between the Caroline and North Bismarck plates.\n\nThere very moderate seismic activity along both these trenches, and their status as an active subduction zone has been challenged. A relative motion of or less has, nevertheless, been suggested for the Manus Trench, roughly normal to the trench.\n\nThe Kilinailau Trench east of New Ireland forms the continuation to the Manus Trench and is thought to mark the boundary between the Pacific and North Bismarck plates. It is, however, disputed whether the Caroline Plate moves independently from the Pacific Plate. If not, the Manus and Kilinailau trenches form the Pacific-North Bismarck boundary together.\n\nPerpendicular to the two trenches is another trench, the Mussau Trench separating the Caroline Plate and Pacific Plate.\n\n"}
{"id": "1787569", "url": "https://en.wikipedia.org/wiki?curid=1787569", "title": "Metropolitan planning organization", "text": "Metropolitan planning organization\n\nA metropolitan planning organization (MPO) is a federally mandated and federally funded transportation policy-making organization in the United States that is made up of representatives from local government and governmental transportation authorities. They were created to ensure regional cooperation in transportation planning. MPOs were introduced by the Federal-Aid Highway Act of 1962, which required the formation of an MPO for any urbanized area (UZA) with a population greater than 50,000. Federal funding for transportation projects and programs are channeled through this planning process. Congress created MPOs in order to ensure that existing and future expenditures of governmental funds for transportation projects and programs are based on a continuing, cooperative, and comprehensive (“3‑C”) planning process. Statewide and metropolitan transportation planning processes are governed by federal law (). Transparency through public access to participation in the planning process and electronic publication of plans now is required by federal law. As of 2015, there are 408 MPOs in the United States.\n\nPurposes of MPOs:\n\nIn other words, the federal government requires that federal transportation funds be allocated to regions in a manner that has a basis in metropolitan plans developed through intergovernmental collaboration, rational analysis, and consensus-based decision making.\n\nTypically, an MPO governance structure includes a variety of committees as well as a professional staff. The “policy committee” is the top-level decision-making body for the planning organization. In most MPOs, the policy committee comprises:\n\nWith only a few unique exceptions nationwide (such as the MPO in Portland, Oregon), MPO policy committee members are not elected directly by citizens. Rather, a policy committee member typically is an elected or appointed official of one of the MPO’s constituent local jurisdictions. The policy committee member thus has legal authority to speak and act on behalf of that jurisdiction in the MPO setting. Federal law, however, does not require members of an MPO policy committee to be representatives of the metropolitan areas' populations. Systematic studies have found that MPO policy committees' representations of urban municipalities and disadvantaged minority populations in their areas are less than proportional to population. The policy committee’s responsibilities include debating and making decisions on key MPO actions and issues, including adoption of the metropolitan long-range transportation plans, transportation improvement programs, annual planning work programs, budgets, and other policy documents. The policy committee also may play an active role in key decision points or milestones associated with MPO plans and studies, as well as conducting public hearings and meetings. An appointed advisory committee (CAC) develops the recommendations for consideration by the policy committee and establishes a ranked proposal for work plans.\n\nMost MPOs also establish a technical committee to act as an advisory body to the policy committee for transportation issues that primarily are technical in nature. The technical committee interacts with the MPO’s professional staff on technical matters related to planning, analysis tasks, and projects. Through this work, the technical committee develops recommendations on projects and programs for policy committee consideration. Metropolitan travel forecasting is one of the key roles that the technical committee supports. The technical committee typically comprises staff-level officials of local, state, and federal agencies. In addition, a technical committee may include representatives of interest groups, various transportation modes, and local citizens. A 2005 survey of MPOs nationally commissioned in preparation of \"Special Report 288\" of the Transportation Research Board of the National Academies found that \"forecast by negotiation\" was a common method of projecting future population and employment growth for use in travel forecasting, suggesting rent-seeking behavior on the part of MPO committees influencing the technical staff.\n\nUsually MPOs retain a core professional staff in order to ensure the ability to carry out the required metropolitan planning process in an effective and expeditious manner. The size and qualifications of this staff may vary by MPO, since no two metropolitan areas have identical planning needs Most MPOs, however, require at least some staff dedicated solely to MPO process oversight and management because of the complexity of the process and need to ensure that requirements are properly addressed.\n\nThere are five core functions of an MPO:\n\nIf the metropolitan area is designated as an air quality non-attainment or maintenance area, then\n\n\nPresently, most MPOs have no authority to raise revenues such as to levy taxes on their own, rather, they are designed to allow local officials to decide collaboratively how to spend available federal and other governmental transportation funds in their urbanized areas. The funding for the operations of an MPO comes from a combination of federal transportation funds and required matching funds from state and local governments.\n\nIn some regions, MPOs have been given authority to handle expanded functions:\n\n\nMPOs differ greatly in various parts of the country and even within states. Some have large staffs, while others may include only a director and a transportation planner. Sometimes the professional staff of an MPO is provided by a county or a council of governments. In many urban areas, existing organizations such as county governments or councils of government also function as MPOs. The MPO role also may be played by an independent governmental organization or a regional government. In the Portland, Oregon, metropolitan area, for example, Metro is the MPO. In the Minneapolis-St. Paul, Minnesota, metropolitan area, the Metropolitan Council is the MPO.\n\nAn example of a medium-sized MPO is the Lexington Area MPO in Kentucky. An example of a small MPO is the Kittery Area MPO in Maine.\n\nAnother MPO planning organization has developed in the area of western central Florida. Several MPOs there, with governance over eight counties, have developed a greater regional planning committee, the Chairs Coordinating Committee (CCC), composed of the chairs of seven MPOs and the chairs of their appointed advisory committee (or their representatives) in order to coordinate transportation planning for the region, that is compatible with all, as well as addressing the challenges of long range planning for a large and growing region that has overlapping issues among the MPOs or transportation plans that extend throughout the entire area. Often the members of the executive committee of an MPO act interchangeably as the representative to this seven-MPO regional committee. This committee meets less frequently than the participating MPOs.\n\nThe enactment of the 1991 Intermodal Surface Transportation Efficiency Act (ISTEA) ushered in a “renaissance” for MPOs. After a decade or more of being consigned to a minimal role in transportation planning, ISTEA directed additional federal funding to MPOs, expanded their authority to select projects, and mandated new metropolitan planning initiatives. For the first time, state transportation officials were required to consult seriously with local representatives on MPO governing boards regarding matters of project prioritization and decision-making. These changes had their roots in the need to address increasingly difficult transportation problems—in particular, the more complicated patterns of traffic congestion that arose with the suburban development boom in the previous decades. Many recognized that the problems could only be addressed effectively through a stronger federal commitment to regional planning.\n\nThe legislation that emerged, the Intermodal Surface Transportation Efficiency Act (ISTEA), was signed into federal law by President George H. W. Bush in December 1991. It focused on improving transportation, not as an end in itself, but as the means to achieve important national goals including economic progress, cleaner air, energy conservation, and social equity. ISTEA promoted a transportation system in which different modes and facilities—highway, transit, pedestrian, bicycle, aviation, and marine—were integrated to allow a \"seamless\" movement of both goods and people. New funding programs provided greater flexibility in the use of funds, particularly regarding using previously restricted highway funds for transit development, improved \"intermodal\" connections, and emphasized upgrades to existing facilities over building new capacity—particularly roadway capacity.\n\nTo accomplish more serious metropolitan planning, ISTEA doubled federal funding for MPO operations and required the agencies to evaluate a variety of multimodal solutions to roadway congestion and other transportation problems. MPOs also were required to broaden public participation in the planning process and to see that investment decisions contributed to meeting the air quality standards of the Clean Air Act Amendments.\n\nIn addition, ISTEA placed a new requirement on MPOs to conduct “fiscally constrained planning”, and ensure that long-range transportation plans and short-term transportation improvement programs were fiscally constrained; in other words, adopted plans and programs can not include more projects than reasonably can be expected to be funded through existing or projected sources of revenues. This new requirement represented a major conceptual shift for many MPOs (and others in the planning community), since the imposition of fiscal discipline on plans now required, not only understanding how much money might be available, but how to prioritize investment needs and make difficult choices among competing needs. Adding to this complexity is the need to plan across transportation modes and develop approaches for multimodal investment prioritization and decision making. It is in this context of greater prominence, funding, and requirements that MPOs function today.\n\nAn annual element is composed of transportation improvement projects contained in an area's transportation improvement program (TIP), which is proposed for implementation during the current year. The annual element is submitted to the U.S. Department of Transportation as part of the required planning process.\n\nThe passage of Safe, Accountable, Flexible, Efficient Transportation Equity Act: A Legacy for Users SAFETEA‑LU in 2005 created new and revised requirements for transportation planning and programs. Although SAFETEA-LU increased standards, most MPOs already were in compliance with the regulations. Some of the planning topic areas include transportation systems security, emergency preparedness, public participation plans for metropolitan planning, and requiring the electronic publication of plans and TIP/STIP by the MPOs.\n\nSAFETEA-LU requires that the statewide transportation planning process and the metropolitan planning process provide for consideration of projects and strategies that will protect and enhance the environment, promote energy conservation, improve the quality of life, and promote consistency between transportation improvements and state and local planned growth and economic development patterns.\n\nThere are a large number of metropolitan planning organizations in the United States. \n\n\n"}
{"id": "4044625", "url": "https://en.wikipedia.org/wiki?curid=4044625", "title": "New Frontiers program", "text": "New Frontiers program\n\nThe New Frontiers program is a series of space exploration missions being conducted by NASA with the purpose of researching several of the Solar System bodies, including the dwarf planet Pluto.\n\nNASA is encouraging both domestic and international scientists to submit mission proposals for the program. New Frontiers was built on the innovative approach used by the Discovery and Explorer Programs of principal investigator-led missions. It is designed for medium-class missions that cannot be accomplished within the cost and time constraints of Discovery, but are not as large as Large Strategic Science Missions (Flagship misions). There are currently three New Frontiers missions in progress: \"New Horizons\", which was launched in 2006 and reached Pluto in 2015, \"Juno\", which was launched in 2011 and entered Jupiter orbit in 2016, and \"OSIRIS-REx\", launched in September 2016 towards asteroid Bennu for detailed studies from 2018 to 2021 and a sample return to Earth in 2023.\n\nThe New Frontiers program was developed and advocated by NASA and granted by Congress in CY 2002 and 2003. This effort was led by two long-time NASA executives at Headquarters at that time: Edward Weiler, Associate Administrator of Science, and Colleen Hartman, Solar System Exploration Division Director. The mission to Pluto had already been selected before this program was successfully endorsed and funded, so the mission to Pluto, called \"New Horizons\", was \"grandfathered\" into the New Frontiers program. The 2003 Planetary Science Decadal Survey from the National Academy of Sciences identified destinations that then served as the source of the first competition for the New Frontiers program. The program name was selected by Hartman based on President John F. Kennedy's speech in 1960, in which he said \"We stand, today, on the edge of a New Frontier.\"\n\nExamples of proposed mission concepts include two broad groups based on Planetary Science Decadal Survey goals.\n\n\"New Horizons\", a mission to Pluto, was launched on January 19, 2006. After a Jupiter gravity assist in February 2007 the spacecraft continued towards Pluto. The primary mission flyby occurred in July 2015 and the spacecraft was then targeted toward one Kuiper Belt object called for a January 1, 2019 flyby. Another mission that was considered with this mission was \"New Horizons 2\".\n\n\"Juno\" is a Jupiter exploration mission which launched on August 5, 2011 and arrived in July 2016. It is the first solar-powered spacecraft to explore an outer planet. The craft was placed into a polar orbit in order to study the planet's magnetic field and internal structure.\nNASA's \"Galileo\" mission to Jupiter provided extensive knowledge about its upper atmosphere, however, further study of Jupiter is crucial not only to the understanding of its origin and nature of the Solar System, but also of giant extrasolar planets in general. The \"Juno\" spacecraft investigation is intended to address the following objectives for Jupiter:\n\n\"OSIRIS-REx\" stands for \"Origins, Spectral Interpretation, Resource Identification, Security, Regolith Explorer\", and was launched on 8 September 2016. This mission plan is to orbit an asteroid, at the time named (now 101955 Bennu), by 2020. After extensive measurements, the spacecraft will collect a sample from the asteroid's surface for return to Earth in 2023. The mission, excluding the launch vehicle, is expected to cost approximately $800 million. The returned sample will help scientists answer long-held questions about the formation of the Solar System and the origin of complex organic molecules necessary for the origin of life.\n\nAsteroid Bennu is a potential future Earth impactor and is listed on the Sentry Risk Table with the third highest rating on the Palermo Technical Impact Hazard Scale (circa 2015). In the late 2100s there is a cumulative chance of about 0.07% it could strike Earth, therefore there is a need to measure the composition and Yarkovsky effect of the asteroid.\n\nCompetition for the fourth mission began in January 2017. NASA selected two proposals for additional concept studies on December 20, 2017, will select a winner in the competition in 2019, and launch it by 2024. Investigators may propose the use of Multi-Mission Radioisotope Thermoelectric Generators (MMRTG), and the NASA Evolutionary Xenon Thruster (NEXT) ion propulsion system. The development cost cap is approximately $1 billion.\n\nPer recommendation by the Decadal Survey, NASA's announcement of opportunity was limited to six mission themes:\nThe Decadal Survey recommended the Io Observer and Lunar Geophysical Network proposals for New Frontiers 5, in addition to the previous recommendations. NASA's Planetary Science Division responded to the Decadal Survey with support, stating that the recommendations appear well-aligned with the agency's goals.\n\nNASA received and reviewed 12 proposals:\n\nThe two finalists, announced on 20 December 2017, are Dragonfly to Titan, and CAESAR (Comet Astrobiology Exploration Sample Return) which is a sample-return mission from comet 67P/Churyumov–Gerasimenko; CAESAR was proposed by the Cornell University. Comet 67P was previously explored by the European Space Agency's probe \"Rosetta\" and its lander \"Philae\" during 2014-2015.\n\nThe two proposals will each receive $4 million funding through the end of 2018 to further develop and mature their concepts. NASA will decide in mid-2019 which one of the two to build.\n\n\n"}
{"id": "25920713", "url": "https://en.wikipedia.org/wiki?curid=25920713", "title": "Official community plan", "text": "Official community plan\n\nIn Canada, an official community plan is a comprehensive plan created by an incorporated municipality which dictates public policy in terms of transportation, utilities, land use, recreation, and housing. OCPs typically encompass large geographical areas, a broad range of topics, and cover a long-term time horizon. The process of creating an OCP is today often referred to as a Community Vision.\n\nIn the United States such a plan is known as a comprehensive plan.\n\nIn some large jurisdictions and metropolitan areas experiencing significant growth, regional transportation plans are made that work in conjunction with municipal OCPS.\n\nOfficial community plans is the formal term for documents created by an incorporated municipality and filed with the provincial government, usually the Ministry of Municipal Affairs.\n\nOCPs have to be periodically updated to remain relevant. For example, the City of North Vancouver created an Official Community Plan in 1980, 1992, and again in 2002. When objectives of the plan have been achieved new objectives are set. For example, the City of North Vancouver in Metro Vancouver states as its achievements the construction of 5,000 units of housing in the city center, commercial and institutional development, a balanced mix of transportation modes, modern telecommunications infrastructure, a high percentage of multifamily housing, an accessible waterfront, and a balance between jobs and labour force.\n\nCommunity planning may also be done on a smaller scale. The resulting plan is not an official community plan but is known as a neighborhood plan. In Vancouver such Neighborhood Plans are also known as \"community visions\". The primary motive for these neighborhood plans in Vancouver was to find ways to accommodate more housing (or new housing choices) in existing neighborhoods in a way sensitive and responsive to the concerns of existing residents. The official statement being more vague:\n\"THAT Council and Departments use the ... Community vision directions to\nhelp guide policy decisions, corporate work, priorities, budgets and capital plans in\nthis community.\"\n\nList of Official Community Plans in Metro Vancouver:\n"}
{"id": "5766910", "url": "https://en.wikipedia.org/wiki?curid=5766910", "title": "RV Marcus Langseth", "text": "RV Marcus Langseth\n\nR/V \"Marcus Langseth\" is a research vessel owned by the National Science Foundation and operated by the Lamont-Doherty Earth Observatory (LDEO) of Columbia University as a part of the University-National Oceanographic Laboratory System (UNOLS) fleet.\nThe \"Marcus G. Langseth\" was dedicated on December 4 2007, came into service in early 2008, replacing the R/V \"Maurice Ewing\". \n\"Langseth\" is intended primarily to collect multichannel seismic data, including 3-D surveys. The ship was purchased from the geophysical survey company WesternGeco in 2004, having previously been named \"M/V Western Legend\".\n\nThe \"Marcus Langseth\" was named for Marcus G. Langseth, a Lamont scientist.\n\nIn March 2009 Chinese authorities denied the vessel permission to pass between Taiwan and China.\n\nIn August 2009, \"Marcus Langseth\" was named in a Canadian lawsuit seeking to halt its seismic tomography experiment. \nThe lawsuit was dismissed, diplomatic clearance was issued and the ship sailed after a delay of a day.\n\n"}
{"id": "55204428", "url": "https://en.wikipedia.org/wiki?curid=55204428", "title": "Ramesside star clocks", "text": "Ramesside star clocks\n\nThe Ramesside star clocks are ancient Egyptian star clocks appearing on the ceilings of several royal tombs of the Ramesside period. Two sets of star clocks appear on the tomb Ramesses VII and Ramesses IX. Although all of the sets are corrupt to some degree, a prototype clock can be reconstructed from them.\n"}
{"id": "52690690", "url": "https://en.wikipedia.org/wiki?curid=52690690", "title": "Ron Giovanelli", "text": "Ron Giovanelli\n\nRonald Gordon Giovanelli DSc FAA (1915–1984) was an Australian physicist and solar researcher. \n"}
{"id": "37060172", "url": "https://en.wikipedia.org/wiki?curid=37060172", "title": "SeaDataNet", "text": "SeaDataNet\n\nSeaDataNet is an international project of oceanography. Its main goal is to enable the scientific community to access historical datasets owned by national data centers.\n\nThis project aims to provide a web service permitting to retrieve validated datasets (temperature, oxygen, salinity, nutrients, etc.) from 45 different National Data Centers of 35 countries having coasts along European seas. Therefore SeaDataNet is a standardized system for managing the large and diverse data sets collected by the oceanographic fleets and the automatic observation systems. Additional objectives consist in creating product with aggregated data such as climatological descriptions. This European funded project has started in 2004, the project is currently in its second phase with fundings for 2012 to 2016. Most of the datasets are free of access, but some are restricted to institutes. \nIn term of harmonization SeaDataNet has chosen standards, vocabularies, tools that are used in the different NODC(National Oceanographic Data Center). For example they use Ocean Data View to validate or visualize datasets, they also use DIVA software to perform objective analysis. Datasets are covering the years 1800 up to 2012. In 2012 400 data originators are registered into Seadatanet project.\nUsers of SeaDataNet who want to retrieve datasets coming from multiple Data Centers log to the Common Data Index web-service to define their request. They can provide many details such as the type of platform wanted, the parameter wanted, the rate of sampling, the position, the originator country, etc. Then users send their request, the request is analysed and split into as much request as there are data centers concerned. At the end the user receive an email giving a FTP address where to retrieve all the data ordered in the file format wanted (ASCII, NetCDF or Ocean Data View format).\n"}
{"id": "9207062", "url": "https://en.wikipedia.org/wiki?curid=9207062", "title": "Space warfare in fiction", "text": "Space warfare in fiction\n\nSpace warfare has served as a central theme within the science-fiction genre. One can trace its roots back to classical times, and to the \"future war\" novels of the nineteenth century. An interplanetary, or more often an interstellar or intergalactic war, has become a staple plot device in space operas. Space warfare has a predominant role in military science fiction but is not believed to be a realistic possibility because of the distances involved and the logistical impracticalities.\n\nIn his second-century satire \"True History\", Lucian of Samosata depicts an imperial war between the king of the Sun and the king of the Moon over the right to colonise the Morning Star. It is the earliest known work of fiction to address the concept.\n\nThe first \"future war\" story was George T. Chesney's \"The Battle of Dorking,\" a story about a British defeat after a German invasion of Britain, published in 1871 in \"Blackwood's Magazine\". Many such stories were written prior to the outbreak of World War I. George Griffith's \"The Angel of the Revolution\" (1892) featured self-styled \"Terrorists\" armed with then-nonexistent arms and armour such as airships, submarines, and high explosives. The inclusion of yet-nonexistent technology became a standard part of the genre. Griffith's last \"future war\" story was \"The Lord of Labour\", written in 1906 and published in 1911, which included such technology as disintegrator rays and missiles.\n\nH. G. Wells' novel \"The War of the Worlds\" inspired many other writers to write stories of alien incursions and wars between Earth and other planets, and encouraged writers of \"future war\" fiction to employ wider settings than had been available for \"naturalistic\" fiction. Wells' several other \"future war\" stories included the atomic war novel \"The World Set Free\" (1914) and \"The Land Ironclads,\" which featured a prophetic description of the tank, albeit of an unfeasibly large scale.\n\nThe modern form of space warfare in science fiction, in which mobile spaceships battle both planets and one another with destructive superweapons, appeared with the advent of space opera. Garrett P. Serviss' 1898 newspaper serial \"Edison's Conquest of Mars\" was inspired by Wells and intended as a sequel to \"Fighters from Mars,\" an un-authorized and heavily altered Edisonade version of \"The War of the Worlds\" in which the human race, led by Thomas Edison, pursues the invading Martians back to their home planet. David Pringle considers Serviss' story to be the very first space opera, although the work most widely regarded as the first space opera is E. E. \"Doc\" Smith's \"The Skylark of Space\". It and its three successor novels exemplify the present form of space warfare in science fiction, as giant spaceships employ great ray guns that send bolts of energy across space to shatter planets in a war between humans and alien species.\n\nDavid Weber's \"Honorverse\" novels present a view of space warfare that simply transplants the naval warfare of Horatio Nelson and Horatio Hornblower into space. The space navy battle tactics in the Honorverse are much like those of Nelson, with the simple addition of a third dimension.\n\nMore recent depictions of space warfare departed from the jingoism of the pulp science fiction of the 1930s and 1940s. Joe Haldeman's \"The Forever War\", was partly a response to or a rebuttal of Robert A. Heinlein's \"Starship Troopers\", wherein space warfare involved the effects of time dilation and resulted in the alienation of the protagonists from the human civilization on whose behalf they were fighting. Both novels have in the past been required reading at the United States Military Academy.\n\nScience fiction writers from the end of World War II onwards have examined the morality and consequences of space warfare. With Heinlein's \"Starship Troopers\" are A. E. van Vogt's \"War against the Rull\" (1959) and Fredric Brown's \"Arena\" (1944). Opposing them are Murray Leinster's \"First Contact\" (1945), Barry Longyear's \"Enemy Mine,\" Kim Stanley Robinson's \"The Lucky Strike,\" Connie Willis' \"Schwarzchild Radius,\" and John Kessel's \"Invaders.\" In Orson Scott Card's \"Ender's Game\", the protagonist wages war remotely, with no realization that he is doing so.\n\nSeveral writers in the 1980s were accused of writing fiction as part of a propaganda campaign in favour of the Strategic Defense Initiative. Ben Bova's 1985 novel \"Privateers\" has been given as an example.\n\nEarly television productions such as \"Captain Video and His Video Rangers\" (1949) were severely constrained by the available special effects technology, and effect sequences were typically difficult to set up. This, combined with the fact that early shows were often live productions, meant that space action sequences were usually short and simple.\n\nProduction techniques improved throughout the 1950s and 1960s, and most programming moved to pre-recorded productions. This allowed more complex effects to be used, and increased the ability of producers to show action sequences such as space warfare. \"\" is from this period. While the future presented in the original \"Star Trek\" series was not one of open warfare, the machinery of war was ever present, and was used in many episodes. Ships carried missiles armed with antimatter warheads, known as \"photon torpedoes\", and deflector shields for defense. Battles were shown on screen, but the expense and difficulty of advanced special effects meant that most battles were short and involved few craft. The costs of special effects dropped dramatically over the years, but remained high enough that larger battles showed relatively few ships firing and/or being hit. \"\" (1993) used computer graphics developed in part by Industrial Light & Magic, and could show battles between numerous classes of ships using tactics developed by military strategists.\n\nGeorge Lucas' 1977 film \"Star Wars\" broke new ground in its depiction of space warfare. Advances in technology, combined with the film's comparatively high budget, allowed Lucas to create long, complex space action sequences. The battle sequences were modeled after World War II-era dogfighting from films such as \"The Dam Busters\", and were a major milestone in fictional space combat. \n\nA number of more ambitious films and television series soon followed, including ABC's \"Battlestar Galactica\" (1978). \"Battlestar Galactica\" used expensive effects influenced by those of Lucas' film and followed his lead in concentrating on battles between starfighters. It, and contemporary shows such as \"Buck Rogers in the 25th Century\", set new standards in television space battles. The series primarily used laser-type energy weapons in defense and offence on battleships, although analogues to ballistic weaponry are present in several episodes. The 2003 \"re-imagining\" of \"Battlestar Galactica\" uses more conventional weaponry, such as guns and missiles mounted on the primary capital ships and starfighters, and use pure Newtonian physics to achieve a more realistic representation of how space warfare would actually appear.\nJames Cameron's \"Aliens\", the 1986 sequel to the 1979 film \"Alien\", used \"Starship Troopers\" as the basis for its futuristic military. The movie involves a small unit of the United States Colonial Marine Corps who provide emergency response to a planetary colony in 2179. The film showed futuristic twists on many modern types of military vehicle and gear, including a dropship, 10x25mm caseless \"Pulse\" M41A1 rifles, flamethrowers and machine guns, and realistic body armor and tactical equipment.\n\nThe 1993 television series \"Babylon 5\" chronicled a turbulent time in galactic politics, which involved several inter-species wars. Political and humanitarian aspects were explored, such as atrocities against civilian populations, and telepathy was used as a weapon. The series made an attempt to faithfully depict the physics of combat in a vacuum, instead of using motion modelled on aeroplanes within our atmosphere.\n\nThe 1995 American TV series \"\" centered around the \"Wildcards\", a group of marines in the 2060s who serve as both infantry and fighter pilots. The show attempted to depict technology that was near-future, but based on research. It also explored the alienation of deep space warfare, the horrors of loss and survival on the battefield, the bonds that form in combat, and a fight against an enemy of which they knew little. \"Space: Above and Beyond\" differs from many other military science fiction works in that its soldiers use weapons that fire bullets, and fight in space suits in alien environments.\n\nThe British TV series \"Doctor Who\", has numerous instances of this. \"Frontier in Space\", set in 2540, mentions a war between Earth and Draconia, fought 20 years earlier. This story involves renegade Time Lord the Master trying to start another war on behalf of the Daleks, who plan to conquer the Galaxy. The Time War is a major plot point in the revived series, during which the Time Lords, the species of the Doctor, fought the Daleks. This war seems to have ended with the destruction of the Time Lord planet Gallifrey, which the Doctor did hoping the Daleks would also be destroyed.\n\nUsually, lasers are used rather than bullets. An August 1939 \"Astounding Science-Fiction\" issue consisted of an article written by Willy Ley claimed that bullets would be a more effective weapon in a real space battle.\n\nDestruction of planets and stars has been a frequently used aspect of interstellar warfare since the \"Lensman\" series. This is not a realistic capability, as it has been calculated that a force on the order of 10 joules of energy, or roughly the total output of the sun in a week, would be required to overcome the gravity that holds together an Earth-sized planet. The destruction of Alderaan in \"Star Wars Episode IV: A New Hope\" is estimated to require 1.0 × 10 joules of energy, millions of times more than would be necessary to break the planet apart at a slower rate.\n\nFictional space warfare tends to borrow elements from naval warfare. David Weber's Honorverse series of novels portrays several \"space navies\" such as the Royal Manticoran Navy, which imitate themes from Napoleonic-era naval warfare. \nThe Federation Starfleet (\"Star Trek\"), Imperial Navy (\"Star Wars\") Alliance Navy (\"Mass Effect\") and Earthforce (\"Babylon 5\") also use a naval-style rank-structure and hierarchy. The former is based on the United States Navy and the Royal Navy. The United Nations Space Command in \"Halo\" fully echoes all ranks of the United States armed forces, even the pay-grade system. Naval ship-classes such as frigate or destroyer sometimes serve as marker to show how the craft are assembled and their designed purpose.\n\nSome fictional universes have different implementations. The Colonial Fleet in \"Battlestar Galactica\" uses a mixture of army and navy ranks, and the \"Stargate\" universe has military spacecraft under the control of modern air forces, and uses air-force ranks. In the \"Andromeda\" universe, officers of Systems Commonwealth ships follow naval ranking, but Lancers (soldiers analogous to Marines) use army ranks.\n\n\n"}
{"id": "2099179", "url": "https://en.wikipedia.org/wiki?curid=2099179", "title": "T-norm", "text": "T-norm\n\nIn mathematics, a t-norm (also T-norm or, unabbreviated, triangular norm) is a kind of binary operation used in the framework of probabilistic metric spaces and in multi-valued logic, specifically in fuzzy logic. A t-norm generalizes intersection in a lattice and conjunction in logic. The name \"triangular norm\" refers to the fact that in the framework of probabilistic metric spaces t-norms are used to generalize triangle inequality of ordinary metric spaces.\n\nA t-norm is a function T: [0, 1] × [0, 1] → [0, 1] which satisfies the following properties:\n\nSince a t-norm is a binary algebraic operation on the interval [0, 1], infix algebraic notation is also common, with the t-norm usually denoted by formula_1.\n\nThe defining conditions of the t-norm are exactly those of the partially ordered Abelian monoid on the real unit interval [0, 1]. \" (Cf. ordered group.)\" The monoidal operation of any partially ordered Abelian monoid \"L\" is therefore by some authors called a \"triangular norm on L\".\n\nT-norms are a generalization of the usual two-valued logical conjunction, studied by classical logic, for fuzzy logics. Indeed, the classical Boolean conjunction is both commutative and associative. The monotonicity property ensures that the degree of truth of conjunction does not decrease if the truth values of conjuncts increase. The requirement that 1 be an identity element corresponds to the interpretation of 1 as \"true\" (and consequently 0 as \"false\"). Continuity, which is often required from fuzzy conjunction as well, expresses the idea that, roughly speaking, very small changes in truth values of conjuncts should not macroscopically affect the truth value of their conjunction.\n\nT-norms are also used to construct the intersection of fuzzy sets or as a basis for aggregation operators (see fuzzy set operations). In probabilistic metric spaces, t-norms are used to generalize triangle inequality of ordinary metric spaces. Individual t-norms may of course frequently occur in further disciplines of mathematics, since the class contains many familiar functions.\n\nA t-norm is called \"continuous\" if it is continuous as a function, in the usual interval topology on [0, 1]. (Similarly for \"left-\" and \"right-continuity\".)\n\nA t-norm is called \"strict\" if it is continuous and strictly monotone.\n\nA t-norm is called \"nilpotent\" if it is continuous and each \"x\" in the open interval (0, 1) is its nilpotent element, i.e., there is a natural number \"n\" such that \"x\" formula_1 ... formula_1 \"x\" (\"n\" times) equals 0.\n\nA t-norm formula_1 is called \"Archimedean\" if it has the Archimedean property, i.e., if for each \"x\", \"y\" in the open interval (0, 1) there is a natural number \"n\" such that \"x\" formula_1 ... formula_1 \"x\" (\"n\" times) is less than or equal to \"y\".\n\nThe usual partial ordering of t-norms is pointwise, i.e.,\nAs functions, pointwise larger t-norms are sometimes called \"stronger\" than those pointwise smaller. In the semantics of fuzzy logic, however, the larger a t-norm, the \"weaker\" (in terms of logical strength) conjunction it represents.\n\n\nThe drastic t-norm is the pointwise smallest t-norm and the minimum is the pointwise largest t-norm:\n\nFor every t-norm T, the number 0 acts as null element: T(\"a\", 0) = 0 for all \"a\" in [0, 1].\n\nA t-norm T has zero divisors if and only if it has nilpotent elements; each nilpotent element of T is also a zero divisor of T. The set of all nilpotent elements is an interval [0, \"a\"] or [0, \"a\"), for some \"a\" in [0, 1].\n\nAlthough real functions of two variables can be continuous in each variable without being continuous on [0, 1], this is not the case with t-norms: a t-norm T is continuous if and only if it is continuous in one variable, i.e., if and only if the functions \"f\"(\"x\") = T(\"x\", \"y\") are continuous for each \"y\" in [0, 1]. Analogous theorems hold for left- and right-continuity of a t-norm.\n\nA continuous t-norm is Archimedean if and only if 0 and 1 are its only idempotents.\n\nA continuous Archimedean t-norm is strict if 0 is its only nilpotent element; otherwise it is nilpotent. By definition, moreover, a continuous Archimedean t-norm T is nilpotent if and only if \"each\" \"x\" < 1 is a nilpotent element of T. Thus with a continuous Archimedean t-norm T, either all or none of the elements of (0, 1) are nilpotent. If it is the case that all elements in (0, 1) are nilpotent, then the t-norm is isomorphic to the Łukasiewicz t-norm; i.e., there is a strictly increasing function \"f\" such that\nIf on the other hand it is the case that there are no nilpotent elements of T, the t-norm is isomorphic to the product t-norm. In other words, all nilpotent t-norms are isomorphic, the Łukasiewicz t-norm being their prototypical representative; and all strict t-norms are isomorphic, with the product t-norm as their prototypical example. The Łukasiewicz t-norm is itself isomorphic to the product t-norm undercut at 0.25, i.e., to the function \"p\"(\"x\", \"y\") = max(0.25, \"x\" · \"y\") on [0.25, 1].\n\nFor each continuous t-norm, the set of its idempotents is a closed subset of [0, 1]. Its complement — the set of all elements which are not idempotent — is therefore a union of countably many non-overlapping open intervals. The restriction of the t-norm to any of these intervals (including its endpoints) is Archimedean, and thus isomorphic either to the Łukasiewicz t-norm or the product t-norm. For such \"x\", \"y\" that do not fall into the same open interval of non-idempotents, the t-norm evaluates to the minimum of \"x\" and \"y\". These conditions actually give a characterization of continuous t-norms, called the Mostert–Shields theorem, since every continuous t-norm can in this way be decomposed, and the described construction always yields a continuous t-norm. The theorem can also be formulated as follows:\n\nA similar characterization theorem for non-continuous t-norms is not known (not even for left-continuous ones), only some non-exhaustive methods for the construction of t-norms have been found.\n\nFor any left-continuous t-norm formula_14, there is a unique binary operation formula_17 on [0, 1] such that\nfor all \"x\", \"y\", \"z\" in [0, 1]. This operation is called the \"residuum\" of the t-norm. In prefix notation, the residuum to a t-norm formula_14 is often denoted by formula_21 or by the letter R.\n\nThe interval [0, 1] equipped with a t-norm and its residuum forms a residuated lattice. The relation between a t-norm T and its residuum R is an instance of adjunction (specifically, a Galois connection): the residuum forms a right adjoint R(\"x\", –) to the functor T(–, \"x\") for each \"x\" in the lattice [0, 1] taken as a poset category.\n\nIn the standard semantics of t-norm based fuzzy logics, where conjunction is interpreted by a t-norm, the residuum plays the role of implication (often called \"R-implication\").\n\nIf formula_17 is the residuum of a left-continuous t-norm formula_14, then\nConsequently, for all \"x\", \"y\" in the unit interval,\nand\n\nIf formula_1 is a left-continuous t-norm and formula_17 its residuum, then\nIf formula_1 is continuous, then equality holds in the former.\n\nIf \"x\" ≤ \"y\", then R(\"x\", \"y\") = 1 for any residuum R. The following table therefore gives the values of prominent residua only for \"x\" > \"y\".\n\nT-conorms (also called S-norms) are dual to t-norms under the order-reversing operation which assigns 1 – \"x\" to \"x\" on [0, 1]. Given a t-norm formula_14, the complementary conorm is defined by\nThis generalizes De Morgan's laws.\n\nIt follows that a t-conorm satisfies the following conditions, which can be used for an equivalent axiomatic definition of t-conorms independently of t-norms:\n\nT-conorms are used to represent logical disjunction in fuzzy logic and union in fuzzy set theory.\n\nImportant t-conorms are those dual to prominent t-norms:\n\nMany properties of t-conorms can be obtained by dualizing the properties of t-norms, for example:\n\nFurther properties result from the relationships between t-norms and t-conorms or their interplay with other operators, e.g.:\n\nA negator formula_42 is a monotonous falling, i. e. order-reversion mapping with formula_43 and formula_44 (in other notation: formula_45 and formula_46). \nA negator n is called\nThe standard (canonical) negator is \nformula_49, \nwhich appears to be strong. As the standard negator is used in the above definition of a t-norm/t-conorm pair, this can be generalized as follows:\n\nA De Morgan Triplet is a triple (T,⊥,n) iff (if and only if)\n\n\n"}
{"id": "404891", "url": "https://en.wikipedia.org/wiki?curid=404891", "title": "The Case for Mars", "text": "The Case for Mars\n\nThe Case for Mars: The Plan to Settle the Red Planet and Why We Must is a nonfiction science book by Robert Zubrin, first published in 1996, and revised and updated in 2011.\n\nThe book details Zubrin's Mars Direct plan to make the first human landing on Mars. The plan focuses on keeping costs down by making use of automated systems and available materials on Mars to manufacture the return journey's fuel \"in situ\". The book also reveals possible Mars colony designs and weighs the prospects for a colony's material self-sufficiency and for the terraforming of Mars.\n\nThe Mars Direct plan was originally detailed by Zubrin and David Baker in 1990. \"The Case for Mars\" is, according to Zubrin, a comprehensive condensation for laymen of many years' work and research. Chapters one and four deal with Mars Direct most completely.\n\nFor Robert Zubrin, the attractiveness of Mars Direct does not rest on a single cost-effective mission. He envisions a series of regular Martian missions with the ultimate goal of colonization, which he details in the seventh through ninth chapters. As initial explorers leave hab-structures on the planet, subsequent missions become easier to undertake.\n\nLarge subsurface, pressurized habitats would be the first step toward human settlement; the book suggests they can be built as Roman-style atria underground with easily produced Martian brick. During and after this initial phase of habitat construction, hard-plastic radiation- and abrasion-resistant geodesic domes could be deployed on the surface for eventual habitation and crop growth. Nascent industry would begin using indigenous resources: the manufacture of plastics, ceramics and glass.\n\nThe larger work of terraforming requires an initial phase of global warming to release atmosphere from the regolith and to create a water-cycle. Three methods of global warming are described in the work and, Zubrin suggests, are probably best deployed in tandem: orbital mirrors to heat the surface; factories on the surface to pump halocarbons into the atmosphere; and the seeding of bacteria which can metabolize water, nitrogen and carbon to produce ammonia and methane (these would aid in global warming). While the work of warming Mars is on-going, true colonization can begin.\n\n\"The Case for Mars\" acknowledges that any Martian colony will be partially Earth-dependent for centuries. However, it suggests that Mars may be a profitable place for two reasons. First, it may contain concentrated supplies of metals of equal or greater value to silver which have not been subjected to millennia of human scavenging and may be sold on Earth for profit. Secondly, the concentration of deuterium – a possible fuel for commercial nuclear fusion – is five times greater on Mars. Humans emigrating to Mars thus have an assured industry and the planet will be a magnet for settlers as wage costs will be high. The book asserts that “the labor shortage that will prevail on Mars will drive Martian civilization toward both technological and social advances.”\n\nWhile detailing the exploration and colonization, \"The Case for Mars\" also addresses a number of attendant scientific and political factors.\n\nThe fifth chapter analyzes various risks that putatively rule out a long-term human presence on Mars. Zubrin dismisses the idea that radiation and zero-gravity are unduly hazardous. He claims that cancer rates \"do\" increase for astronauts who have spent extensive time in space, but only marginally. Similarly, while zero-gravity presents challenges, “near total recovery of musculature and immune system occurs after reentry and reconditioning to a one-gravity environment.” Furthermore, since his plan has the spacecraft spinning at the end of a long tether to create artificial gravity, worries about zero gravity do not apply to this mission in any case. Back-contamination – humans acquiring and spreading Martian viruses – is described as \"just plain nuts\", because there are no host organisms on Mars for disease organisms to have evolved.\n\nIn the same chapter, Zubrin decisively denounces and rejects suggestions that the Moon should be used as waypoint to Mars or as a training area. It is ultimately much easier to journey to Mars from low Earth orbit than from the moon and using the latter as a staging point is a pointless diversion of resources. While the Moon may superficially appear a good place to perfect Mars exploration and habitation techniques, the two bodies are radically different. The moon has no atmosphere, no analogous geology and a much greater temperature range and rotational period. Antarctica or desert areas of Earth provide much better training grounds at lesser cost.\n\nIn the third and tenth chapters, \"The Case for Mars\" addresses the politics and costs of the ideas described. The authors argue that the colonization of Mars is a logical extension of the settlement of North America. They envision a frontier society, providing opportunities for innovation and social experimentation.\n\nZubrin suggests three models to provide the will and capital to drive Mars exploration forward: the J.F.K. model, in which a far-sighted U.S. leader provides the funding and mobilizes national public opinion around the idea; the Sagan model, in which international co-operation is the driving force; and the Gingrich approach, which emphasizes incentives and even prizes for private sector actors who take on research and development tasks. In keeping with the third idea, Zubrin describes twelve challenges that address various aspects of the exploration program. A monetary prize – from five hundred million to twenty billion dollars – is offered to companies who successfully complete the challenges.\n\nThe prize-based approach to hardware development has emerged within the private aeronautics community, though not yet on the scale envisioned by Zubrin. Ventures such as the Ansari X-Prize and Robert Bigelow's America's Space Prize seek low-cost spaceflight development through private enterprise, and crucially, for the attainment of very specific predetermined goals in order to win the prizes.\n\nThe underlying political and economic problems of raising sufficient capital for terraforming using halocarbon emissions is critiqued by John Hickman.\n\nIn 2017, a Russian translation of the book was published under the title of \"Курс на Марс\" (\"On Course for Mars\") (). \n\n\n\n"}
{"id": "11818397", "url": "https://en.wikipedia.org/wiki?curid=11818397", "title": "The Facts of Life (book)", "text": "The Facts of Life (book)\n\nThe Facts of Life is a book published in 1953 by C. D. Darlington of the subject of race, heredity and evolution. Darlington was a major contributor to the field of genetics around the time of the Modern synthesis.\n"}
{"id": "13892965", "url": "https://en.wikipedia.org/wiki?curid=13892965", "title": "The Internet Galaxy", "text": "The Internet Galaxy\n\nThe Internet Galaxy: Reflections on the Internet, Business, and Society is a book by Manuel Castells, Professor of Sociology and Professor of City and Regional Planning at the University of California. It was published by Oxford University Press in 2001. The title is a reference to \"The Gutenberg Galaxy\", a 1962 book by Marshall McLuhan. It is regarded as a good introduction to Social informatics.\n\nThe book contains 9 chapters. Castells starts with the history of Internet, focuses on the process of Internet evolution influence our society. He emphasizes the development of Internet from 1962 to 1995, the extension from ARPANET to WWW.\n\nCastells believes that \"The openness of the Internet's architecture was the source of its main strength\". Then he states that the 'Internet Culture' is structured by four kinds of culture including: 'the techno-meritocratic culture', 'the hacker culture', 'the virtual communication culture', and 'the entrepreneurial culture'.\n\nNext, Castells analyses the vital status of Internet in the business and economy fields, and he refers to the impact of virtual communication which is based on the Internet communication to the reality in the following chapter. In terms of the Politics of the Internet, Castells points that 'social movement' and 'the political process' use Internet as a new communication medium to 'acting' and 'informing'. And there is an issue between 'Privacy and Liberty in Cyberspace' relates to 'the politics of the Internet' is mentioned in this book.\n\nIn the last three chapters, Castells analyses the Internet from multimedia, geography and 'the digital divide in a global perspective'. Finally, he talks about the challenges of the network society such as freedom of the Internet.\n\nThe title used for the preface or introductory text is called \"Opening\" and the name given to this Opening is \"The Network is the Message\". It is a mimicry of Marshall McLuhan's famous slogan \"The medium is the message\". By substituting \"network\" for \"medium\", Castells reinforces McLuhan's message that, in this case, it is the network which is important not the content. The opening may then be seen to be an invitation to explore the meaning of network via the content of the book. The word network itself is of ambiguous interpretation: infrastructure or society? Both interpretations are at play in the book. Since Castells is by profession a sociologist, then one expects a focus on network as society.\n\nCastells introduces the label ″Libertarian″ to characterize all those who participated with \"big science\" and \"military research\" in bringing the Internet into being.\nThe history of the Internet is diverse and well documented. Castells makes considerable use of John Naughton's text, \"A Brief History of the Future\", who noted for example that the Request for Comment Feature (RFC), introduced by Steve Crocker in 1969-04-07, not only gave rise to a de facto documenting of the research ideas at the time of their fermenting but also to the Open Source movement. Castells gives his own take on the subject. Ultimately, for him, the Internet is a cultural creation.\n\n\"The culture of the Internet is a culture made up\nof a technocratic belief in the progress of humans\nthrough technology, enacted by communities of hackers\nthriving on free and open technological creativity,\nembedded in virtual networks aimed at reinventing\nsociety, and materialized by money-driven entrepreneurs into\nthe workings of the new economy.\"\nIt is important to take note of how Castells understands and uses the word Network.\nFor him, the network is a word that often has connotations of community. So, when he speaks of virtual networks he is not (necessarily) speaking of virtual networks in the technological sense but in the community sense of people networking.\n\n\"But markets also react to macro-economic conditions, and to policy decisions—or to their anticipation. Or to the disparity between the anticipation and the actual event. Markets react as well on the basis of non-economic criteria. These are influenced by what I call \"informationTurbulences\" from various sources, such as political uncertainty... technological anticipations... or even personal moods or statements from key decision-makers...\"\n\n\"In contrast with the notorious cartoon published by\n\"The New Yorker\" in the pre-history of on-line communication,\non the Internet you better make sure\nthat everyone knows that you are a dog, and not a cat,\nor you will find yourself immersed in the intimate world of cats.\nBecause on the Internet, you are what you say you are,\nas it is on the basis of this expectation that a network\nof social interaction is constructed over time.\"\n\n\"In this context [of a world dominated by homogeneous, global information flows], communication of values, mobilization around meaning, become fundamental. Cultural movements... are built around communication—essentially the Internet and the media... to affect the consciousness of society as a whole.\"\nCastells shows how the Internet has been used for mobilizing people to support certain kinds of political, religious, or other social causes:\n\n\n\"Unless governments stop fearing their people, and therefore the Internet,\nsociety will resort once again to the barricades to defend freedom and this will mark a stunning historical continuity.\"\nThis is that chapter of the book which one must read in the context of the pre-9/11 world.\n\nCastells mentions a few official programs of governments:\n\n\"Human culture only exists in and by human minds,\nusually connected to human bodies.\nTherefore, if our minds have the material capability\nto access the whole realm of cultural expressions—select them,\nrecombine them—we do have a hypertext: the hypertext is inside us.\"\n\"Cities are faced with a challenge... It follows that public space and monumentality (museums, cultural centers, public art, architectural icons) will play a key role in marking space, and facilitating meaningful interaction.\"\nThere are different ways in which to picture the geography of the Internet.\nThe picture of the graph on the cover of the book resembles that of Matt Britt shown on the right.\n\n\"Education, information, science, and technology\nbecome the critical sources of value creation\nin the Internet-based economy.\nEducational, informational, and technological resources are\ncharacterized by extremely uneven distribution throughout the world\n(UNESCO, 1999).\"\n\n\"I imagine one could say: ″Why don't you leave me alone?!\nI want no part of your Internet, of your technological civilization, of your network society! I just want to live my life!″...\"\nOne of the significant features of the book (published in 2001 before\nthe September 11 attacks and around the time of the\ndot-com bubble) is the\ninclusion of the e-Links section at the end of every chapter. Each e-Link is given as a URL, followed by a short text of one or two lines to describe the content. For example, at the end of Chapter 6 \"The Politics of the Internet II: Privacy and Liberty in Cyberspace\", a collection of 4 e-Links is given:\nand the short explanatory text following is \"Websites providing technological resources to protect privacy.\"\n\nThere is one major flaw associated with the e-Links. None of the e-Links in \"The Internet Galaxy\" provide \"the date of last access\".\n\n"}
{"id": "7403861", "url": "https://en.wikipedia.org/wiki?curid=7403861", "title": "The Making of the Atomic Bomb", "text": "The Making of the Atomic Bomb\n\nThe Making of the Atomic Bomb is a contemporary history book written by the American journalist and historian Richard Rhodes, first published by Simon & Schuster in 1987. It won the Pulitzer Prize for General Non-Fiction, the National Book Award for Nonfiction, and a National Book Critics Circle Award. The narrative covers people and events from early 20th century discoveries leading to the science of nuclear fission, through the Manhattan Project and the atomic bombings of Hiroshima and Nagasaki.\n\nPraised both by historians and former Los Alamos weapon scientists, the book is considered a general authority on early nuclear weapons history, as well as the development of modern physics in general, during the first half of the 20th century. Nobel Laureate I. I. Rabi, one of the prime participants in the dawn of the atomic age, called it \"an epic worthy of Milton. No where else have I seen the whole story put down with such elegance and gusto and in such revealing detail and simple language which carries the reader through wonderful and profound scientific discoveries and their application.\"\n\n"}
{"id": "31747442", "url": "https://en.wikipedia.org/wiki?curid=31747442", "title": "Von Babo's law", "text": "Von Babo's law\n\nVon Babo's law (sometimes styled Babo's law) is a scientific law formulated by German chemist Lambert Heinrich von Babo. It states that the vapor pressure of solution decreases according to the concentration of solute.\n"}
{"id": "49933065", "url": "https://en.wikipedia.org/wiki?curid=49933065", "title": "We Are Afghan Women: Voices of Hope", "text": "We Are Afghan Women: Voices of Hope\n\nWe Are Afghan Women: Voices of Hope is a 2016 non-fiction book about women's rights in Afghanistan. It was published by the George W. Bush Institute at the George W. Bush Presidential Center, with an introduction by former First Lady Laura Bush.\n\nThe introduction of the book was written by Former First lady Laura Bush.The book is a collection of life stories of Afghan women. The book is about those determined women of Afghan who are defying the odds to lead Afghanistan to a better future.\n"}
{"id": "10084984", "url": "https://en.wikipedia.org/wiki?curid=10084984", "title": "What Is This Thing Called Science?", "text": "What Is This Thing Called Science?\n\nWhat Is This Thing Called Science? is a best-selling textbook by Alan Chalmers. \n\nThe book is a guide to the philosophy of science which outlines the shortcomings of naive empiricist accounts of science, and describes and assesses modern attempts to replace them. The book is written with minimal use of technical terms. \"What Is This Thing Called Science?\" was first published in 1976, and has been translated into many languages.\n\n\n\n"}
{"id": "713497", "url": "https://en.wikipedia.org/wiki?curid=713497", "title": "Wireless mesh network", "text": "Wireless mesh network\n\nA wireless mesh network (WMN) is a communications network made up of radio nodes organized in a mesh topology. It is also a form of wireless ad hoc network.\n\nA mesh refers to rich interconnection among devices or nodes. Wireless mesh networks often consist of mesh clients, mesh routers and gateways. Mobility of nodes is less frequent. If nodes constantly or frequently move, the mesh spends more time updating routes than delivering data. In a wireless mesh network, topology tends to be more static, so that routes\ncomputation can converge and delivery of data to their destinations can occur. Hence, this is \na low-mobility centralized form of wireless ad hoc network. Also, because it sometimes \nrelies on static nodes to act as gateways, it is not a truly all-wireless ad hoc network.\n\nMesh clients are often laptops, cell phones, and other wireless devices. Mesh routers forward traffic to and from the gateways, which may, but need not, be connected to the Internet. The coverage area of all radio nodes working as a single network is sometimes called a mesh cloud. Access to this mesh cloud depends on the radio nodes working together to create a radio network. A mesh network is reliable and offers redundancy. When one node can no longer operate, the rest of the nodes can still communicate with each other, directly or through one or more intermediate nodes. Wireless mesh networks can self form and self heal. Wireless mesh networks work with different wireless technologies including 802.11, 802.15, 802.16, cellular technologies and need not be restricted to any one technology or protocol. See also mesh networking.\n\nWireless mesh architecture is a first step towards providing cost effective and low mobility over a specific coverage area. Wireless mesh infrastructure is, in effect, a network of routers minus the cabling between nodes. It is built of peer radio devices that do not have to be cabled to a wired port like traditional WLAN access points (AP) do. Mesh infrastructure carries data over large distances by splitting the distance into a series of short hops. Intermediate nodes not only boost the signal, but cooperatively pass data from point A to point B by making forwarding decisions based on their knowledge of the network, i.e. perform routing by first deriving the topology of the network.\n\nWireless mesh networks is a relatively \"stable-topology\" network except for the occasional failure of nodes or addition of new nodes. The path of traffic, being aggregated from a large number of end users, changes infrequently. Practically all the traffic in an infrastructure mesh network is either forwarded to or from a gateway, while in wireless ad hoc networks or client mesh networks the traffic flows between arbitrary pairs of nodes.\n\nIf rate of mobility among nodes are high, i.e., link breaks happen frequently, wireless mesh networks start to break down and have low communication performance.\n\nThis type of infrastructure can be decentralized (with no central server) or centrally managed (with a central server). Both are relatively inexpensive, and can be very reliable and resilient, as each node needs only transmit as far as the next node. Nodes act as routers to transmit data from nearby nodes to peers that are too far away to reach in a single hop, resulting in a network that can span larger distances. The topology of a mesh network must be relatively stable, i.e., not too much mobility. If one node drops out of the network, due to hardware failure or any other reason, its neighbors can quickly find another route using a routing protocol.\n\nMesh networks may involve either fixed or mobile devices. The solutions are as diverse as communication needs, for example in difficult environments such as emergency situations, tunnels, oil rigs, battlefield surveillance, high-speed mobile-video applications on board public transport, real-time racing-car telemetry, or self-organizing Internet access for communities. An important possible application for wireless mesh networks is VoIP. By using a quality of service scheme, the wireless mesh may support routing local telephone calls through the mesh. Most applications in wireless mesh networks are similar to those in wireless ad hoc networks. \n\nSome current applications:\n\nThe principle is similar to the way packets travel around the wired Internet—data hops from one device to another until it eventually reaches its destination. Dynamic routing algorithms implemented in each device allow this to happen. To implement such dynamic routing protocols, each device needs to communicate routing information to other devices in the network. Each device then determines what to do with the data it receives – either pass it on to the next device or keep it, depending on the protocol. The routing algorithm used should attempt to always ensure that the data takes the most appropriate (fastest) route to its destination.\n\nMulti-radio mesh refers to having different radios operating at different frequencies to interconnect nodes in a mesh. This means there is a unique frequency used for each wireless \nhop and thus a dedicated CSMA collision domain. With\nmore radio bands, communication throughput is likely to increase as a result of more available\ncommunication channels. This is similar to providing dual or multiple radio paths to transmit\nand receive data.\n\nOne of the more often cited papers on Wireless Mesh Networks identified the following areas as open research problems in 2005\n\n\nThere are more than 70 competing schemes for routing packets across mesh networks. Some of these include:\n\nThe IEEE has developed a set of standards under the title 802.11s.\n\nA less thorough list can be found at Ad hoc routing protocol list.\n\nStandard autoconfiguration protocols, such as DHCP or IPv6 stateless autoconfiguration may be used over mesh networks.\n\nMesh network specific autoconfiguration protocols include:\n\n\n\n"}
