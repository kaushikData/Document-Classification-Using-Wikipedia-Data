{"id": "52518750", "url": "https://en.wikipedia.org/wiki?curid=52518750", "title": "Acidipila dinghuensis", "text": "Acidipila dinghuensis\n\nAcidipila dinghuensis is a chemoorganotrophic, aerobic and non-motile bacterium from the genus of Acidipila which has been isolated from the forest of Dinghushan Biosphere Reserve in the Guangdong Province in China.\n\n"}
{"id": "14435941", "url": "https://en.wikipedia.org/wiki?curid=14435941", "title": "Amiga productivity software", "text": "Amiga productivity software\n\nThis article deals with productivity software created for the Amiga line of computers and covers the AmigaOS operating system and its derivates AROS and MorphOS and is a split of main article Amiga software.\nSee also related articles Amiga Internet and communications software, Amiga music software, Amiga programming languages, and Amiga support and maintenance software for other information regarding software that run on Amiga.\n\nThe Amiga originally supported such prestigious software titles as \"WordPerfect\", Electronic Arts' \"Deluxe Paint\", and \"Lattice C\". Newtek's \"Video Toaster\", one of the first all-in-one graphics and video editing packages, began on the Amiga. The \"Video Toaster\" was one of the few accessories for the \"big box\" Amigas (2000, 3000 and 4000) that used the video slot and enabled users to turn their Amiga into the heart of an entire TV production suite. The later addition of the \"Video Flyer\" by Newtek made possible the first non-linear video editing program for the Amiga. The Amiga made 3D raytracing graphics available for the masses with \"Sculpt 3D\". Before the Amiga, raytracing was only available for dedicated graphic workstations such as the SGI. Other raytracing software also included TurboSilver. The Amiga was well known for its 3D rendering capability, with many titles being added to the mix as the years went by. Some titles were later ported to Microsoft Windows and continue to thrive there, such as the rendering software \"Cinema 4D\" from Maxon, and \"LightWave\" from Newtek, which was originally part of the \"Video Toaster\". The \"Video Toaster\" itself has even been ported to the Windows platform. \"LightWave\" was used for low-cost computer generated special effects during the early 1990s, with \"Babylon 5\" being a notable example of a TV-series utilizing \"LightWave\".\nEven Microsoft produced software for use on the Amiga. \"AmigaBASIC\", an advanced BASIC software development environment, complete with an integrated development environment (IDE), was written by Microsoft under contract.\n\nAmiga had its beginnings in 1985 with a strong attitude for graphics, more so than other PCs of its age due to its peculiar hardware, and its multimedia chipset. The graphical chip Agnus could access directly RAM and pilot it with DMA (Direct Memory Access) privileges, and featured Bit Blitter and Copper circuits capable to move ranges of pixels on the screen and deal directly with the electronic beam of the TV set. It could render graphic screens of various number of colors (2, 4, 8, 16, 32, 64 and 4096 color HAM modes) starting from 320x200 up to 720x576 pixel graphic pages. Amiga released a vast number of graphics software programs, such as Graphicraft, Deluxe Paint, TVPaint, Photon Paint, Brilliance!, (a program entirely realized upon the suggestions and wishes of well known computer artist Jim Sachs), Aegis Images, ArtEffect, fxPAINT by IOSpirit, Personal Paint from Cloanto, Photogenics, Express Paint, Digi Paint, XiPaint, PerfectPaint, SketchBlock 24 bit painting program by Andy Broad for AmigaOS 4.x users\n\nUnlike Commodore Amiga systems, AmigaOne systems have no integrated multimedia chipsets. AmigaOne systems, similar to Mac or PC, sport AGP/PCIe graphic cards, embedded audio AC'97 sound system, and can use PCI/PCIe audio cards, even some professional models. The expanded capability of faster CPU performance, and the availability of standard expansion graphic cards, lead to a new generation of graphic software being born for the AmigaOne machines such as Hollywood \"Visual Programming Suite\". This made it also easy to port modern Open Source software like Blender3D.\n\nHollywood suite of programs by German software house Airsoft SoftWair is a multimedia and presentation program available for all Amigas (AmigaOS, MorphOS, AROS) and recently, , a version of Hollywood became available for Microsoft Windows as well. It is able to load Scala projects and Microsoft Powerpoint \".PPT\" files. Its module Hollywood Designer is not only a modern multimedia authoring software but also a true complete cross platform multimedia application layer capable of creating whole Amiga programs through a Visual design approach. It also can save executables in various formats: 68k Amiga, WarpUP, AmigaOS 4 and MorphOS executables and Intel X86 code for AROS. Recent versions of Hollywood allow for creating executable programs for Intel Windows machines, Mac OS X for PPC processors and Mac OS X for Intel processors.\n\nThere are fairly modern, recent graphic software that is available for AmigaOne machines, and some are still usable on Amiga platforms. TV Paint was born in 1991, and it was one of the first commercial 32bit graphic software on the market. Latest Amiga version (3.59) was released in 1994 and actually is distributed publicly, but the source code is still proprietary. It is still a valid graphic program, and continues to be used despite its age, due to its ease of use and its vast number of features. Programs like Candy Factory for AOS 4.0 are designed to create special effects for images, brushes and fonts to create gorgeous internet objects and buttons used in designing web pages. Pixel image editor, formerly Pixel32 is available for MorphOS. Blender 3D is one of the best Open Source cross platform software. Also a first pre-release of GIMP is available on AmigaOS 4.0 through the AmiCygnix X11 graphic engine.\nBeginning with release 2.1 in 2008, MorphOS has included its own standard Paint utility called Sketch, simple but powerful, and AROS has bundled with the last free version of Luna Paint which become actually a commercial paint program for various operating systems.\n\nAs in any Operating Systems there can't exists only bitmap and vector paint software. All around these main software aimed at creating drawings directly by an author, or aimed at manipulating existing image files it exists a vast market of graphic utilities with peculiar features created in order to support main graphic programs. For example, in Amiga it existed very professional software and it is noteworthy to mention at least some the most important and widely used into Amiga market like Cinematte, CineMorph, Morph Plus, Impact!, Essence, Magic Lantern and Pixel 3D Pro, that were only some of the most notorious ones in the vast range of graphic utilities that could be purchased by skilled and professional users of Amiga platform in its golden age. Cinematte utility allow the user to easily make complex photo-realistic composites of subjects that are photographed against a \"bluescreen\", or green screen background. It uses the same sophisticated techniques used worldwide in motion picture technology for precise bluescreen compositing. CineMorph it is a program to automatically create morphing effects between two given original images, and create a compound third image, or even all the animation movie associated with the morphing effect. Morph Plus performed same effects as Cinemorph. Impact! created physics simulation in 3D scenes. Essence was a texture maker to apply textures on 3D surfaces of objects created by 3D tracing programs. Amiga Magic Lantern was a true color animation compressor and player for the Amiga, Pixel 3D Pro utility it was used to create models for 3D objects and save it in various 3D formats, or to transform any model object from a 3D file format to another.\n\nCommon widely used format for vector graphics in Amiga are EPS and IFF DR2D. It originated from the fact that Amiga was the first platform on which ran Ghostscript natively, and also IFF DR2D was the original standard for vector graphics generated by Amiga ProVector and later adopted by other applications such as Art Expression and Professional Draw. Foremost used Amiga drawing and vector graphics utilities are Aegis Draw, ProDraw (Professional Draw) from Gold Disk Inc., DrawStudio, Art Expression, ProVector, and for some basic vector graphics, also the tools of Professional Page and PageStream are useful. The most modern vector graphics programs on Amiga are actually MindSpace 1.1, which is aimed mainly at design flowcharts, mindtables, UML and diagrams, and Steam Draw a 2D simple vector paint program available for MorphOS.\n\nSWFTools is a collection of command line programs to convert and save various raster(bitmap) image formats from and to Flash SWF vector animation format.\n\nOn AmigaOS are available the widely used free distributable vector to graphics conversion facilities Autotrace, Potrace, XTrace which can run also in AROS Amiga Open Source clone OS and MorphOS Amiga-Like system. The Desktop Publishing software PageStream has a tracing utility as bundled software.\n\nVarious programs can read DXF (almost all Amiga CAD programs), EMF, SVG, CGM, GEM, WMF, an example of converting tool that reads many formats and output DR2D is the Amiga program MetaView. It exists also a SVG Datatype to support directly in the OS, on any program, the feature of loading and saving files in SVG (Scalable Vector Graphics) format.\n\nAt its beginning Amiga was considered to offer the most powerful graphic platform at a reasonable price. It had various CAD programs available for it, such as X-CAD, IntelliCAD, DynaCaDD, MaxonCAD, IntroCAD, and even programs to design and test electronic circuits, such as ElektroCAD.\n\nDue to the peculiar capabilities of Amiga in multimedia, and the features of the blit blitter circuit, Amiga was capable of performing advanced animation and video authoring at professional level in the eighties, and thus it was created a vast amount of software which filled also this segment of the professional video editing market. For Amiga there were available animation programs like: Aegis Animator, Lights!Camera!Action!, DeLuxe Video, Disney Animation Studio, versions later than 3 of Deluxe Paint, The Director (a BASIC-like language oriented to animation), Scala, Vision from Commodore itself, VisualFX from ClassX, Adorage Multi Effect program from proDAD, Millennium from Nova Design. ImageFX, and Art Department Pro.\n\nComic Setter was an interesting tools to create printed comics by arranging brushes representing comic characters, joining it with background images and superimposing it the right frames and \"ballons\" with their own text speech and captions. It could then print in color the comics that were created.\n\nDisney Animation Studio was one of the most powerful 2D programs for realizing cartoons. Born on Amiga, this program, equipped also with a complete cell-frame preview feature was used by many cartoon studios worldwide at its age and still used by some several studios in Europe as a useful preview tool.\n\nin its golden age Amiga can count on a vast range of animation and video authoring software; Aegis Animator, Lights!Camera!Action!, DeLuxe Video, Disney Animation Studio, versions later than 3 of Deluxe Paint, The Director (a BASIC-like language oriented to animation), Scala, Amiga Vision from Commodore itself, VisualFX from ClassX, Adorage Multi Effect program from proDAD, Wildfire by Andreas Maschke (ported by author to Java later), Millennium from Nova Design, ImageFX from Nova Design, and Art Department Pro.\n\n3D rendering and animation software includes Sculpt 3D, TurboSilver, Aladdin4D, Videoscape 3D, Caligari, Maxon Cinema4D, Imagine, LightWave from Newtek, Real3D from Realsoft, Vista Pro, World Construction Set 3D terrain rendering programs, and Tornado3D by the Italian company Eyelight.\n\nAmiga was one of the first commercial computer platform to allow amateur and professional video editing, due to its capability in connecting to TV sets, and deal with Chroma-Key, Genlock signal, at full screen with overscan features, and a good noise-gain ratio.\nAmiga and its video peripherals (mainly Genlock boxes and digitizing boxes) in the nineties were available at reasonable prices and then this made the Amiga to become one of the professional video market leader platforms. It was also capable of dealing with broadcast video production (Newtek VideoToaster), and in the age around the 1992-1994, despite of the Commodore demise, Amiga knew its golden age as a professional video platform and there were available for Amiga a vast amount of any kind of video software, graphic facilities and reselling of any of gfx and image gallery data files that could be applied to video productions. Amongst these software it is worth mentioning the main Amiga video-editing programs for desktop video with both linear and non linear editing with 4.2.2 capabilities as the ones from Newtek available with VideoToaster Flyer external module for Video Toaster and just called NLE! (Non Linear Editing), Amiga MainActor, Broadcaster Elite, Wildfire by Andreas Maschke for vfx (now in Java), MovieShop for the expansion Amiga cards PAR, VLab Motion, and VLab Pro.\n\nWhile desktop video proved to be a major market for the Amiga, a surge of word processing, page layout and graphic software filled out the professional needs starting from the first Amiga text program Textcraft which was a mix between a real word processor and an advanced text editor, but capable of changing page layouts, fonts, enlarging or reducing their width, changing their colors and adding color images to the text.\n\nNotable word processing programs for Amiga included the then-industry standard WordPerfect up to version 4.1, Shakespeare, Excellence, Maxon Word, Final Writer, Amiga Writer, Scribble!, ProWrite, Wordworth and the little Personal Write by Cloanto.\n\nThe page layout software included Page Setter and Professional Page from Gold Disk, and PageStream by Soft-Logik, known today as Grasshopper LLC). Only PageStream was ported to other platforms and continues to be developed and supported by the developers. Graphic software included vector drawing applications like Art Expression from Soft-Logik, ProVector by Taliesin, Draw Studio, and Professional Draw from Gold Disk.\n\nAmiga lacked an office suite as the term is meant now, but integrated software was available. Pen Pal was a word processor integrated with a database and a form editor. Scribble!, Analyze! and Organize! were bundled together as the Works! suite combining a word processor, spreadsheet and database. Despite the similarity in name, it had no connection to Microsoft Works.\n\nThe page layout language LaTeX was available in two ports: AmigaTeX, which is no longer available (the first LaTeX can be edited with a front end program), and PasTEX, available on Aminet repository.\n\nModern software AbiWord is available today on AmigaOS 4.0 through the AmiCygnix X11 graphical engine, Scriba and Papyrus Office pre-release is available for MorphOS.\n\nText editors available on Amiga include Vim, Emacs and MicroEMACS, Cygnus Editor also known as CED, and GoldED, which then evolved in 2006 into Cubic IDE. The UNIX ne editor and the vi-clone Vim were initially developed on the Amiga.\n\nDevelopment of Text editors never stopped in Amiga. Since 2001, in MorphOS, a limited edition version of GoldEd called MorphEd is available, and since 2008 Cinnamon Writer and NoWin ED, a universal editor which runs on any Amiga-like platform, are available. Cinnamon Writer is increasing new features to all new releases and aspires to become a full-featured WordProcessor.\n\nIn the first age of Amiga (1986–1989) there were cross-platform spreadsheets available, such as MaxiPlan, which was available also for MS-DOS and Macintosh. Logistix (real name LoGisTiX), one of the first spreadsheets for Amiga, Microfiche Filer Plus was a database which gave the user the experience of exploring data as using microfilms. SuperBase was one of the finest programs available for C64. It was then ported on Atari, Amiga, and later on PC. But on Amiga, it would become a standard reference, available in two versions Superbase Personal and SuperBase Professional It could handle SQL databases and had a query internal language such as BASIC. It was capable of creating forms and masks on records and handling multimedia files into its records years before Microsoft Access. Superbase also featured VCR control style buttons to browse records of any database. Softwood File II was another simple multimedia database which then evolved into Final Data, a good database available for Amiga from Softwood Inc. From the same firm there was Final Calc, a very powerful spreadsheet, similar to TurboCalc from the German company Schatztruhe. ProChart was a tool to draw flow charts and diagrams. Analyze! was a fairly full featured (for the time) spreadsheet developed for the Amiga. Organize! was a flat file database package. Gnumeric spreadsheet has also been ported on Amiga through an X11 engine called AmiCygnix.\n\nIn recent times MUIbase was born and mainly cross-platform MySQL database language became a reference on Amiga also. SQLite, a self-contained, embeddable, zero-configuration SQL database engine, can also be found available on AmigaOS 4 and MorphOS.\n\nIn February 2010, Italian programmer Andrea Palmatè ported IODBC standard to AmigaOS 4.\n\nMaple V is one of the best general purpose mathematics software (a.k.a. Mathematic-CAD) ever made. It was available for Amiga also, and appreciated by many scientists using Amiga in its time. Distant Suns, Galileo, Digital Almanac and Amiga Digital Universe (from Bill Eaves for the OS4) were stellar sky exploring programs and astronomic calculators. During the age of CDTV many historic, science, and art CDs like Timetable of Science, Innovation, Timetable of Business, Politics, Grolier's Encyclopedia, Guinness Disk of Records, Video Creator, American Heritage Dictionary, Illustrated Holy Bible, Illustrated Works of Shakespeare, etc. were available.\n\nFor Amiga there were literally hundreds of entertainment software. Some notable programs for kids and learning were: Adventures in Math series of floppy disks, from Free Spirit Software, Animal Kingdom series of disks from Unicorn Software, Art School all the series of Barney Bear software, the Discovery series, including Discovery trivia, Donald's Alphabet Chase, Mickey's 123's and Mickey's ABC's by Disney Software, the Electric Crayon and Ferngully series of educational coloring book software (Ferngully was taken from the animation movie), Fun School series of disks, Kid Pix set of disks from the well known Broderbund Software house, which was famous in the nineties, Miracle Piano Teaching System to teach music to kids, various tales of Mother Goose, and World Atlas by Centaur Software.\n\nZoneXplorer from Elena Novaretti is considered amongst Amiga users one of the best fractal experience programs ever made on Amiga, if not on any platform. In 1989 the X-Specs 3D Glasses from Haitex Resources, one of the first interactive 3D solutions for home computers were created. Also created on Amiga, were the multimedia interactive TV non immersive Virtual reality exploring software Mandala from Vivid Group Inc., and the Virtuality System Virtuality 1000 CS 3D VRML all-immersive simulator from W-Industries (then Virtuality Inc.), for game entertainment in big arcade installations and theme parks, based on A3000.\n\nMagellan v.1.1 (Artificial Intelligence Software), not to be confused with Directory Opus Magellan, was a program to emulate Artificial intelligence responses on Amiga, by creating heuristic programmed rules based on machine learning in its form of supervised learning. The user would choose into decision trees and decision tables system of AI featured by the Magellan program, in which to input objects, and desired outputs and describe all associate conditions and rules which the machine should follow in order to output pseudo-intelligent solutions to given problems.\n\nAmiATLAS v.6, was a complete Route planner tool for Amiga computers. It provided worldwide interactive maps and found optimal routes for traveling from one place to another. It also featured multiple map loading, an integrated CityGuide-System with information to interesting towns, places or regions, some even with pictures, and information about many parks and points of interest.\n\nDigita Organizer v.1.1 from Digita International was the best Amiga program to let the user to note about dates, meetings, remember expiry dates, etcetera. PolyOrga for MorphOS by Frédéric Rignault.\n\nEasy Banker, Home Accounts, Small Business Accounts, Small Business Manager, Account Master, Accountant, AmigaMoney, Banca Base III, HomeBank, CashMaster, Counting House, etc.\n\nAVT (Amiga Video Transceiver), was a software and hardware Slow-scan television system originally developed by \"Black Belt Systems\" (USA) around 1990 for the Amiga home computer popular all over the world before the IBM PC family gained sufficient audio quality with the help of special sound cards.\n\nRichmond Sound Design (RSD) created both show control (a.k.a. MSC or \"MIDI Show Control\") and theatre sound design software which was used extensively in the theatre, theme park, display, exhibit, stage managing, show and themed entertainment industries in the 1980s and 1990s and at one point in the mid 90s, there were many high-profile shows at major theme parks around the world being controlled by Amigas through software simply called Stage Manager which then evolved into its Microsoft Windows version called ShowMan. There were dozens at Walt Disney World alone and more at all other Disney, Universal Studios, Six Flags and Madame Tussauds properties as well as in many venues in Las Vegas including The Mirage hotel Volcano and Siegfried and Roy show, the MGM Grand EFX show, Broadway theatre, London's West End, the Royal Shakespeare Company's many venues, most of Branson, Missouri's theatres, and scores of theatres on cruise ships, amongst hundreds of others. RSD purchased used Amigas on the web and reconditioned them to provide enough systems for all the shows that specified them and only stopped providing new Amiga installations in 2000. There are still an unknown number of shows on cruise ships and in themed venues being run by Amigas.\n\n"}
{"id": "44455145", "url": "https://en.wikipedia.org/wiki?curid=44455145", "title": "Automation engineering", "text": "Automation engineering\n\nAutomation engineering has two different meanings:\n\nAutomation engineers are experts who have the knowledge and ability to design, create, develop and manage systems, for example, factory automation, process automation and warehouse automation.\n\nAutomation engineering is the integration of standard engineering fields.\nAutomatic control of various control system for operating various systems or machines to reduce human efforts & time to increase accuracy.\n\nGraduates can work for both government and private sector entities such as industrial production, \ncompanies that create and use automation systems, for example paper industry, automotive industry or \nfood and agricultural industry and water treatment.\n\nAutomation engineers can design, program, simulate and test automated machinery and processes. Automation engineers usually are employed in industries such as the energy sector in plants, car manufacturing facilities or food processing plants and robots. Automation engineers are responsible for detailed design specifications and other documents in their creations.\n"}
{"id": "6090423", "url": "https://en.wikipedia.org/wiki?curid=6090423", "title": "BDORT", "text": "BDORT\n\nThe Bi-Digital O-Ring Test (BDORT), characterized as a form of applied kinesiology, is a patented alternative medicine diagnostic procedure in which a patient forms an 'O' with his or her fingers, and the diagnostician subjectively evaluates the patient's health according to the patient's finger strength as the diagnostician tries to pry them apart.\n\nBDORT has been cited and characterized at length by the American Institute for Technology and Science Education as a specific and noteworthy example of pseudoscientific quackery.\n\nBDORT was invented by Yoshiaki Omura, along with several other related alternative medicine techniques. They are featured in Omura's self-published \"Acupuncture & Electro-Therapeutics Research, The International Journal,\" of which Omura is founder and editor-in-chief, as well as in seminars presented by Omura and his colleagues.\n\nOmura is registered to practice acupuncture in New York State.\n\nIn the only known full, formal independent evaluation of BDORT or of any other BDORT-related treatment and technique by a mainstream scientific or medical body, the Medical Practitioners Disciplinary Tribunal of New Zealand ruled, in two separate cases brought before it in 2003, that Richard Warwick Gorringe, MB, ChB of Hamilton, New Zealand, who used BDORT (which he also called \"Peak Muscle Resistance Testing\", or \"PMRT\") to the exclusion of conventional diagnoses on his patients, was guilty of malpractice. In the first case, the Tribunal found it \"is not a plausible, reliable, or scientific technique for making medical decisions\" and \"there is no plausible evidence that PMRT has any scientific validity\".\nIn the second case the Tribunal ruled Gorringe again relied on BDORT to the exclusion of traditional diagnoses, which ultimately led to the death of a patient. As a result of these findings and conclusions, Gorringe was fined and stripped of his license to practice medicine.\n\n is president and founder of the \"International College of Acupuncture & Electro-Therapeutics,\" president and founder of the \"International Bi-Digital O-Ring Test Medical Association,\" and medical research director of the \"Heart Disease Research Foundation.\"\n\nThe test is a subjective evaluation of a patient's opposing muscle strength in which a diagnostician employs the thumb and forefinger of each hand, formed in the shape of an O, to attempt to force apart an O shape formed by the patient who places the fingertips of their thumb and one of their remaining fingers together. At the same time, the patient holds a slide of organ tissue, a sample of medication, potential allergen, etc., in their free hand, or is otherwise 'probed' at an appropriate acupuncture point by the use of a metal rod or laser pointer. The diagnostician then uses their perception of the strength required to force apart the patient's 'O-Ring' of thumb and one of the remaining fingers to assess the patient's health.\n\nThe United States Patent and Trademark Office (USPTO) rejected the initial BDORT patent application as 'too unbelievable to be true'. The application was then resubmitted in 1987, and the USPTO again rejected it. After receiving expert testimony from Omura's \"associates in clinical fields and basic sciences, both in Japan and the United States\" regarding BDORT, the USPTO issued in 1993.\n\nThe fact that a patent was granted to the BDORT has been cited as an example of 'high weirdness' by one firm of patent attorneys.\n\nThe BDORT is capable, according to its proponents, of a wide range of applications in the diagnosis, prescription of treatment, and evaluation of efficacy of treatment of, amongst others: heart conditions, cancers, 'pre-cancers', allergic reactions, viral and bacterial infections, a range of organic and/or environmental stresses, as well as the precise location of acupuncture points and meridians previously unknown or inappropriately identified.\n\nOther than the New Zealand Medical Practitioners Disciplinary Tribunal's reports, there is no known independent mainstream scientific or medical evaluation or validation of any of the BDORT or BDORT-related claims, including the following BDORT variants:\n\n\n\n\n\n\nThe New Zealand Medical Practitioners Disciplinary Tribunal, ruled on two separate malpractice cases against Richard Warwick Gorringe, MB, ChB, of Hamilton, New Zealand.\nIn the first, held in Wellington in 2003, where BDORT was also referred to as 'PMRT' ('Peak Muscle Resistance Testing') by Gorringe, the tribunal examined and dismissed any claims of scientific validity of BDORT, offering the following summary statement of findings:\n\nAs a result of these findings and conclusions, Gorringe was fined and stripped of his license to practice medicine.\n\nIn separate hearings the Medical Practitioners Disciplinary Tribunal held in December 2003 and ruled upon in May 2004 in Auckland, found Gorringe guilty of malpractice in the death of an earlier patient, and concluded that Gorringe's reliance on BDORT to the exclusion of conventional diagnoses led to the patient's death.\n\nSeveral expert witnesses provided testimony about BDORT at the MPDT Wellington hearings, with which the tribunal concurred:\n\nIn the first New Zealand MPDT report from Wellington in 2003, the tribunal defines the terms PMRT and BDORT as equivalent:\n\nLater in the same report, the tribunal again equates PMRT and BDORT, but states that the technique used by Gorringe is different from Dr. Omura's:\n\nThe tribunal uses the terms BDORT and PMRT interchangeably throughout the Wellington report from 2003.\n\nIn the second MPDT report from Auckland in 2004, the tribunal does not mention PMRT at all, and refers to Gorringe's technique exclusively as 'BDORT'.\n\nThe Quackwatch article reviewing these two New Zealand MPDT reports also equates PMRT and BDORT, stating:\n\nBDORT-related seminars, given by Omura, are conducted monthly in New York. The University of the State of New York Education Department allows these seminars to count towards course credit for physicians and dentists seeking certification for the application of acupuncture in the course of their practice.\n\nIn a Decision of 15 May 2007 the Victorian Civil and Administrative Tribunal, in Victoria, Australia, in an appeal against a decision by the Chinese Medical Registration Board of Victoria refusing registration to practice as an acupuncturist, found that attendance and participation in Yoshiaki Omura's \"Annual International Symposium on Acupuncture & Electro-Therapeutics\" as accredited by the University of the State of New York Education Department, in addition to \"clinical experience ... with these subjects in respect of real patients\" did not meet the Chinese Medicine Board's requirement of \"competencies substantially equivalent to\" those taught in a Board certified acupuncture class. Given this, the Tribunal ruled that the Board was not required to certify the applicant as a practitioner of Chinese medicine.\n\n"}
{"id": "35625218", "url": "https://en.wikipedia.org/wiki?curid=35625218", "title": "Bilafond Glacier", "text": "Bilafond Glacier\n\nBilafond Glacier () is located in Siachen region across Karakoram Range in Pakistan. It is a main source for Saltoro River. It is under Pakistani control.\n\n"}
{"id": "1018018", "url": "https://en.wikipedia.org/wiki?curid=1018018", "title": "Burzynski Clinic", "text": "Burzynski Clinic\n\nThe Burzynski Clinic is a controversial clinic offering unproven cancer treatment. It was founded in 1976 and is in Texas, United States. It is best known for the controversial \"antineoplaston therapy\", using compounds it calls antineoplastons, devised by the clinic's founder Stanislaw Burzynski in the 1970s. There is no accepted scientific evidence of benefit from antineoplaston combinations for various diseases.\n\nThe clinic has been the focus of criticism primarily due to the way its antineoplaston therapy is promoted, the costs for people with cancer participating in \"trials\" of antineoplastons, problems with the way these trials are run, and legal cases brought as a result of the sale of the therapy without state board approval.\n\nBurzynski was born on January 23, 1943 in Lublin, Poland. He graduated from the Medical Academy in Lublin. The following year he earned a Ph.D. in biochemistry.\n\nBurzynski moved to the United States in 1970, working at Baylor College until 1977, when he established the Burzynski Research Laboratory where he administered antineoplaston therapy, initially to 21 patients but then more widely as \"experimental\" treatment. This opened him up to \"charges of unethical conduct and to the suspicion he had become a merchant of false hope\", which led to several instances of media controversy.\n\nBurzynski founded the Burzynski Research Institute in 1984. His scientific papers have caused academic controversy, with reviewers disputing the design of the trials and scientific validity of the published results.\n\nIn February 2017 following lengthy hearings the Texas Medical Board recommended Burzynski's medical license be revoked, with the revocation suspended, and a fine of $360,000 for billing irregularities and other violations.\n\n\"Antineoplaston\" is a name coined by Burzynski for a group of peptides, peptide derivatives, and mixtures that he uses as an alternative cancer treatment. The word is derived from \"neoplasm\".\n\nAntineoplaston therapy has been offered in the U.S. since 1984 but is not approved for general use. The compounds are not licensed as drugs but are instead sold and administered as part of clinical trials at the Burzynski Clinic and the Burzynski Research Institute.\n\nBurzynski stated that he began investigating the use of antineoplastons after detecting what he considered significant differences in the presence of peptides between the blood of cancer patients and a control group. He first identified antineoplastons from human blood. Since similar peptides had been isolated from urine, early batches of Burzynski's treatment were isolated from urine. Burzynski has since produced the compounds synthetically.\n\nThe first active peptide fraction identified was called antineoplaston A-10 (3-phenylacetylamino-2,6-piperidinedione). From A-10, antineoplaston AS2-1 was derived – a 4:1 mixture of phenylacetic acid and phenylacetylglutamine. The Burzynski Clinic website states that the active ingredient of antineoplaston A10-I is phenylacetylglutamine.\n\nSince 2011, the clinic has marketed itself as offering \"personalized gene-targeted cancer therapy\", which has stirred further controversy, as the treatment bears no relationship to gene-targeted therapy and only superficially incorporates elements of personalized medicine. The clinic's version of personalized medicine bears little resemblance to targeted cancer therapy, as the clinic includes chemotherapy drugs and antineoplastons are part of this treatment.\n\nAccording to the National Cancer Institute, as of April 2013, \"no phase III randomized, controlled trials of antineoplastons as a treatment for cancer have been conducted. Publications have taken the form of case reports, phase I clinical trials, toxicity studies, and phase II clinical trials\", and \"for the most part, these publications have been authored by the developer of the therapy, Dr. Burzynski, in conjunction with his associates at the Burzynski Clinic. Although these studies often report remissions, other investigators have not been successful in duplicating these results.\"\n\nFrom 1991 to 1995, the NCI initiated multiple phase II trials of antineoplastons. In 1995, after over $1 million had been spent on these trials, they were stopped before the effectiveness of antineoplastons could be determined.\n\nSince the mid-1990s, Burzynski registered some sixty clinical trials of antineoplastons and, in December 2010, a Phase III trial which did not open for patient recruitment. Burzynski has not published full results for any of these. According to his lawyer, Richard Jaffe:\nAll trials were paused (no new patients allowed) following a 2013 FDA inspections which found (for the third consecutive time) significant issues with his Institutional Review Board, and, according to papers published in November 2013, substantial issues with the conduct of both the clinic and Burzynski as principal investigator.\n\nAlthough Burzynski and his associates claim success in the use of antineoplaston combinations for the treatment of various diseases, and some of the clinic's patients say they have been helped, there is no evidence of the clinical efficacy of these methods. The consensus among the professional community, as represented by the American Cancer Society and Cancer Research UK among others, is that antineoplaston therapy is unproven and the overall probability of the treatment turning out to be as claimed is low due to lack of credible mechanisms and the poor state of research after more than 35 years of investigation. While the antineoplaston therapy is marketed as a non-toxic alternative to chemotherapy, it is a form of chemotherapy with significant known side effects including severe neurotoxicity.\n\nIndependent scientists have been unable to reproduce the positive results reported in Burzynski's studies: NCI observed that researchers other than Burzynski and his associates have not been successful in duplicating his results, and Cancer Research UK states that \"available scientific evidence does not support claims that antineoplaston therapy is effective in treating or preventing cancer.\"\n\nThere is no convincing evidence from randomized controlled trials in the scientific literature that antineoplastons are useful treatments of cancer, and the U.S. Food and Drug Administration (FDA) has not approved these products for the treatment of any disease. The American Cancer Society has stated since 1983 that there is no evidence that antineoplastons have any beneficial effects in cancer and recommended that people do not buy these products since there could be serious health consequences. A 2004 medical review described antineoplaston treatment as a \"disproven therapy\".\n\nIn 1998, three oncologists were enlisted by the weekly Washington newsletter \"The Cancer Letter\" to conduct independent reviews of Burzynski's clinical trial research on antineoplastons. They concluded that the studies were poorly designed, not interpretable, and \"so flawed that it cannot be determined whether it really works\". One of them characterized the research as \"scientific nonsense\". In addition to questioning Burzynski's research methods, the oncologists found significant and possibly life-threatening toxicity in some patients treated with antineoplastons.\n\nThe Memorial Sloan-Kettering Cancer Center has stated: \"Bottom Line: There is no clear evidence to support the anticancer effects of antineoplastons in humans.\"\n\nAccording to the American Cancer Society, \"Treatment can cost from $7,000 to $9,500 per month or more, depending on the type of treatment, number of consultations, and the need for surgery to implant a catheter for drug delivery. Available information suggests that health insurance plans often do not reimburse costs linked to this treatment.\" As of March 2015, the Burzynski Clinic required patients to provide a deposit before treatment starts and their website informed patients that \"Since we are classified as \"out of network\" we are unable to accept Medicare, Medicaid and any HMO insurance\".\n\nBurzynski's use and advertising of antineoplastons as an unapproved cancer therapy were deemed to be unlawful by the U.S. FDA and the Texas Attorney General, and limits on the sale and advertising of the treatment were imposed as a result.\n\nIn 2009, the FDA issued a warning letter to the Burzynski Research Institute, stating that an investigation had determined the Burzynski Institutional Review Board (IRB) \"did not adhere to the applicable statutory requirements and FDA regulations governing the protection of human subjects.\" It identified a number of specific findings, among them that the IRB had approved research without ensuring risk to patients was minimized, had failed to prepare required written procedures or retain required documentation, and had failed to conduct required continuing reviews for studies, among others. The Institute was given fifteen days to identify the steps it would take to prevent future violations.\n\nAnother warning issued in October 2012 notes that the Burzynski Clinic is advertising investigational drugs as being \"safe and effective\", noting: \n\nThe letter requires cessation of noncompliant promotional activities, including use of testimonials and promotional interviews with Burzynski himself.\n\nIn June 2012, antineoplaston trials were paused following the death of a child patient. In January and February 2013, the FDA inspected Burzynski and his IRB in Houston. In December 2013, the FDA issued its findings in warning letters to Burzynski, expressing \"concerns about subject safety and data integrity, as well as concerns about the adequacy of safeguards in place at your site to protect patients...\"\n\nIn November 2013 the FDA released the observational notes from an inspection of Burzynski as a principal investigator that took place between January and March 2013. Among the findings were “[failure] to comply with protocol requirements related to the primary outcome, therapeutic response [...], for 67% of study subjects reviewed during the inspection,” admitting patients who failed to meet inclusion criteria, failing to stop treatment when patients had severe toxic reactions to antineoplastons, and failure to report all adverse events. Further, the FDA told Burzynski, \"You failed to protect the rights, safety, and welfare of subjects under your care. Forty-eight (48) subjects experienced 102 investigational overdoses between January 1, 2005 and February 22, 2013 [...] There is no documentation to show that you have implemented corrective actions during this time period to ensure the safety and welfare of subjects.” The FDA also observed that Burzynski had denied patients informed consent by not informing them of extra costs that they might incur during treatment and that he could not account for his stock of the investigational drug. Lastly, the FDA observed: \"Your [...] tumor measurements initially recorded on worksheets at baseline and on-study treatment [...] studies for all study subjects were destroyed and are not available for FDA inspectional review\", meaning that there was no way for the FDA to verify either initial tumor sizes or effects that the antineoplastons may have had.\n\nIn Burzynski's written response to the 2013 FDA investigation he states that the investigators '\"complied with all criteria for evaluation of response and made accurate assessments for tumor response.\"'\n\nIn December 2013, the FDA issued two warning letters, one to the Burzynski Institutional Review Board and one to Burzynski, the subjects of the investigations in February. The FDA found that Burzynski and the IRB had largely failed to address the concerns identified in the initial observation reports. The letter to Burzynski noted serious problems with patient medical files with respect to a pediatric patient who died while treated by Burzynski and whose death apparently initiated the investigation.\n\nOn March 23, 2014, \"USA Today\" reported that the FDA had decided to permit \"a handful\" of cancer patients to receive Burzynski's treatment provided that the patients did not receive the treatment directly from him.\n\nDavid Gorski wrote in 2014 that over four decades the FDA and state medical boards have been unable to shut down Burzynkski's business selling unproven treatments – \"these organizations are supposed to protect the public from practitioners like Burzynski, but all too often they fail at their charges, in this case spectacularly.\"\n\nThe Burzynski Clinic has also made use of compassionate use exemptions. According to an investigative report by STAT News published in August 2016, the clinic has benefited by political lobbying of Burzynski's supporters, including the families of patients with terminal diagnoses. According to FDA documents obtained by STAT, \"From 2011 to 2016, 37 members of Congress wrote to the FDA about Burzynski. […] Most of the lawmakers asked the agency to grant constituents 'compassionate use exemptions' so that they could try his unapproved drugs, or to allow his clinical trials to proceed.\" According to Burzynski, \"interventions by lawmakers were helpful.\"\n\nHowever, appeals to the FDA compassionate use exemptions are not always successful. In one case, \"Burzynski said he used a Texas state law to circumvent the agency and start treatment.\" Critics state that \"the congressional advocacy risks giving the terminally ill and their families a false sense of hope, while also conferring a measure of legitimacy on him that many believe he does not deserve.\".\n\nIn December 2010, the use of chemotherapeutic agents by the clinic has been characterized as \"random\" and their use of unapproved combinations \"with no known benefits but clear harms\" by the Texas Medical Board, which regulates and licenses physicians in the state of Texas, led to a case against Burzynski by that board. Burzynski was acquitted because he had not personally written the prescriptions.\n\nIn July 2014, the board filed a 202-page complaint against Burzynski to the Texas State Office of Administrative Hearings. The complaint addressed allegations by the Board including misleading patients into paying exorbitant charges, misrepresenting unlicensed persons to patients as licensed medical doctors, and misleading patients into accepting care from providers without significant education or training related to cancer treatment. Citing examples of problems with 29 patients, which were listed in the document, the board said that \"unapproved combinations of highly toxic chemotherapy\" were prescribed \"in ways that caused harm to several patients.\" In July 2015, Burzynski's counsel, Richard Jaffe, withdrew from the case citing a potential conflict of interest, a result of Jaffe pursuing actions against Burzynski in bankruptcy court. With the addition of replacement counsel, the hearing was set to begin in November 2015.\n\nIn November 2015, the Texas Medical Board took Burzynski to court in Houston, Texas. Burzynski was accused of bait-and-switch tactics, improperly charging patients, not informing patients that he owns the pharmacy they were required to use to fill their medications, and of off-label prescribing of drugs. Burzynski's former attorney Richard Jaffe has filed suit in Federal Court claiming unpaid legal fees of over $250,000. Burzynski through his current attorney denied all charges.\n\nOn March 3, 2017, the Texas Medical Board sanctioned Burzynski, placing him on probation and fining him $40,000. After being sanctioned for over 130 violations, he was allowed to keep his medical license and to continue to practice. Staff recommendations had been more punitive. Probation terms included additional medical training, disclosure of the Board's ruling to patients and medical facilities, and monitoring of his patient records.\n\nIn 1983, a federal court issued an injunction against Burzynski, barring him from shipping antineoplastons in interstate commerce without FDA approval. Burzynski continued to use antineoplastons and was charged with 75 federal counts of mail fraud and violations of federal drug laws. In 1994, a 20-day trial resulted in the dismissal of the 34 counts of mail fraud. On the other 41 counts, the jury deadlocked, failing to come to a verdict. In a separate administrative proceeding, the Texas State Board of Medical Examiners charged Burzynski with violations of Texas state law relating to his use of antineoplastons. An administrative law judge ruled that Burzynski violated a section of the Texas Health and Safety Code dealing with prescriptions for unapproved drugs. The Texas Court of Appeals ultimately upheld this determination in a 1996 decision.\n\nIn December 2010, the Texas State Board of Medical Examiners filed a multi-count complaint in the Texas State Office of Administrative Hearings against Burzynski for failure to meet state medical standards. In November 2012, a Texas State Office of Administrative Hearings administrative law judge ruled that Burzynski was not vicariously liable under Texas administrative law for the actions of staff at the clinic.\n\nIn January 2012, Lola Quinlan, an elderly, stage IV cancer patient, sued Burzynski for using false and misleading tactics to \"swindle her out of $100,000\". She also sued his companies, The Burzynski Clinic, the Burzynski Research Institute and Southern Family Pharmacy, in Harris County Court. She sued for negligence, negligent misrepresentation, fraud, deceptive trade and conspiracy.\n\nIn November 2011, a music writer and editor for the British newspaper \"The Observer\" sought help raising £200,000 to have his 4-year-old niece, who was diagnosed with glioma, treated at the Burzynski Clinic. Several bloggers reported other cases of patients who had spent similar amounts of money on the treatment, and had died, and challenged the validity of Burzynski's treatments. Marc Stephens, identifying himself as a representative of the Burzynski Clinic, sent emails accusing them of libel and demanding that coverage of Burzynski be removed from their sites. One of the bloggers who received threatening e-mails from Stephens was Rhys Morgan, a 17-year-old sixth-form student from Cardiff, Wales, at the time, previously noted for exposing the Miracle Mineral Supplement. Another was Andy Lewis, a skeptic and publisher of the Quackometer blog.\n\nFollowing the publicity fallout resulting from the legal threats made by Stephens against the bloggers, the Burzynski Clinic issued a press release on November 29, 2011 confirming that the Clinic had hired Stephens \"to provide web optimization services and to attempt to stop the dissemination of false and inaccurate information concerning Dr. Burzynski and the Clinic\", apologizing for comments made by Stephens to bloggers and for the posting of personal information, and announcing that Stephens \"no longer has a professional relationship with the Burzynski Clinic.\"\n\nThe story, including the threats against the bloggers, was covered by the \"BMJ\" (formerly the \"British Medical Journal\"). The chief clinician at Cancer Research UK expressed his concern at the treatment offered, and Andy Lewis of Quackometer and science writer Simon Singh, who had previously been sued by the British Chiropractic Association, said that English libel law harms public discussion of science and medicine, and thus public health.\n\nIn an article in Skeptical Inquirer published in March 2014, skeptic Robert Blaskiewicz chronicled the activities by skeptics to investigate and challenge Burzynski's claim of cancer treatments. He claimed aggressive actions by Burzynski's supporters toward the critics, including contacting their employers, lodging complaints to state licensing boards and defamation. Blaskiewicz pointedly indicated that, although Burzynski had dismissed Marc Stephens, his clinic has not retracted the warnings of the possibility of lawsuits against critics, that it is \"a threat that hangs over all of these activists every day\".\n\n\n\n\n"}
{"id": "2131266", "url": "https://en.wikipedia.org/wiki?curid=2131266", "title": "Compressibility factor", "text": "Compressibility factor\n\nThe compressibility factor (Z), also known as the compression factor or the gas deviation factor, is a correction factor which describes the deviation of a real gas from ideal gas behavior. It is simply defined as the ratio of the molar volume of a gas to the molar volume of an ideal gas at the same temperature and pressure. It is a useful thermodynamic property for modifying the ideal gas law to account for the real gas behavior. In general, deviation from ideal behavior becomes more significant the closer a gas is to a phase change, the lower the temperature or the larger the pressure. Compressibility factor values are usually obtained by calculation from equations of state (EOS), such as the virial equation which take compound-specific empirical constants as input. For a gas that is a mixture of two or more pure gases (air or natural gas, for example), the gas composition must be known before compressibility can be calculated. \nAlternatively, the compressibility factor for specific gases can be read from generalized compressibility charts that plot formula_1 as a function of pressure at constant temperature.\n\nThe compressibility factor should not be confused with the compressibility (also known as coefficient of compressibility or isothermal compressibility) of a material which is the measure of the relative volume change of a fluid or solid in response to a pressure change.\n\nThe compressibility factor is defined in thermodynamics and engineering frequently as:\nwhere p is the pressure, formula_3 is the density of the gas and formula_4 is the specific gas constant, formula_5 being the molar mass, and the formula_6 is the absolute temperature (Kelvin or Rankine scale).\n\nIn statistical mechanics the description is:\nwhere p is the pressure, n is the number of moles of gas, formula_6 is the absolute temperature, and formula_9 is the gas constant.\n\nFor an ideal gas the compressibility factor is formula_10 per definition. In many real world applications requirements for accuracy demand that deviations from ideal gas behaviour, i.e., real gas behaviour, is taken into account. The value of formula_1 generally increases with pressure and decreases with temperature. At high pressures molecules are colliding more often. This allows repulsive forces between molecules to have a noticeable effect, making the molar volume of the real gas (formula_12) greater than the molar volume of the corresponding ideal gas (formula_13), which causes formula_1 to exceed one. When pressures are lower, the molecules are free to move. In this case attractive forces dominate, making formula_15. The closer the gas is to its critical point or its boiling point, the more formula_1 deviates from the ideal case.\n\nThe compressibility factor is linked to the fugacity coefficient by the relation\n\nThe unique relationship between the compressibility factor and the reduced temperature, formula_18, and the reduced pressure, formula_19, was first recognized by Johannes Diderik van der Waals in 1873 and is known as the two-parameter principle of corresponding states. The principle of corresponding states expresses the generalization that the properties of a gas which are dependent on intermolecular forces are related to the critical properties of the gas in a universal way. That provides a most important basis for developing correlations of molecular properties.\n\nAs for the compressibility of gases, the principle of corresponding states indicates that any pure gas at the same reduced temperature, formula_18, and reduced pressure, formula_19, should have the same compressibility factor.\n\nThe reduced temperature and pressure are defined by\nHere formula_24 and formula_25 are known as the critical temperature and critical pressure of a gas. They are characteristics of each specific gas with formula_24 being the temperature above which it is not possible to liquify a given gas and formula_25 is the minimum pressure required to liquify a given gas at its critical temperature. Together they define the critical point of a fluid above which distinct liquid and gas phases of a given fluid do not exist.\n\nThe pressure-volume-temperature (PVT) data for real gases varies from one pure gas to another. However, when the compressibility factors of various single-component gases are graphed versus pressure along with temperature isotherms many of the graphs exhibit similar isotherm shapes.\n\nIn order to obtain a generalized graph that can be used for many different gases, the reduced pressure and temperature, formula_19 and formula_18, are used to normalize the compressibility factor data. Figure 2 is an example of a generalized compressibility factor graph derived from hundreds of experimental PVT data points of 10 pure gases, namely methane, ethane, ethylene, propane, n-butane, i-pentane, n-hexane, nitrogen, carbon dioxide and steam.\n\nThere are more detailed generalized compressibility factor graphs based on as many as 25 or more different pure gases, such as the Nelson-Obert graphs. Such graphs are said to have an accuracy within 1–2 percent for formula_1 values greater than 0.6 and within 4–6 percent for formula_1 values of 0.3–0.6.\n\nThe generalized compressibility factor graphs may be considerably in error for strongly polar gases which are gases for which the centers of positive and negative charge do not coincide. In such cases the estimate for formula_1 may be in error by as much as 15–20 percent.\n\nThe quantum gases hydrogen, helium, and neon do not conform to the corresponding-states behavior and the reduced pressure and temperature for those three gases should be redefined in the following manner to improve the accuracy of predicting their compressibility factors when using the generalized graphs:\n\nwhere the temperatures are in kelvins and the pressures are in atmospheres.\n\nIn order to read a compressibility chart, the reduced pressure and temperature must be known. If either the reduced pressure or temperature is unknown, the reduced specific volume must be found. Unlike the reduced pressure and temperature, the reduced specific volume is not found by using the critical volume. The reduced specific volume is defined by,\n\nwhere formula_36 is the specific volume.\n\nOnce two of the three reduced properties are found, the compressibility chart can be used. In a compressibility chart, reduced pressure is on the x-axis and Z is on the y-axis. When given the reduced pressure and temperature, find the given pressure on the x-axis. From there, move up on the chart until the given reduced temperature is found. Z is found by looking where those two points intersect. the same process can be followed if reduced specific volume is given with either reduced pressure or temperature.\n\nThere are three observations that can be made when looking at a generalized compressibility chart. These observations are:\n\n\nThe virial equation is especially useful to describe the causes of non-ideality at a molecular level (very few gases are mono-atomic) as it is derived directly from statistical mechanics:\n\nWhere the coefficients in the numerator are known as virial coefficients and are functions of temperature.\n\nThe virial coefficients account for interactions between successively larger groups of molecules. For example, formula_38 accounts for interactions between pairs, formula_39 for interactions between three gas molecules, and so on. Because interactions between large numbers of molecules are rare, the virial equation is usually truncated after the third term.\n\nThe compressibility factor is linked to the intermolecular-force potential φ by:\n\nThe Real gas article features more theoretical methods to compute compressibility factors.\n\nDeviations of the compressibility factor, \"Z\", from unity are due to attractive and repulsive Intermolecular forces. At a given temperature and pressure, repulsive forces tend to make the volume larger than for an ideal gas; when these forces dominate \"Z\" is greater than unity. When attractive forces dominate, \"Z\" is less than unity. The relative importance of attractive forces decreases as temperature increases (see effect on gases).\n\nAs seen above, the behavior of \"Z\" is qualitatively similar for all gases. Molecular nitrogen, N, is used here to further describe and understand that behavior. All data used in this section were obtained from the NIST Chemistry WebBook. It is useful to note that for N the normal boiling point of the liquid is 77.4 K and the critical point is at 126.2 K and 34.0 bar.\nThe figure on the right shows an overview covering a wide temperature range. At low temperature (100 K), the curve has a characteristic check-mark shape, the rising portion of the curve is very nearly directly proportional to pressure. At intermediate temperature (160 K), there is a smooth curve with a broad minimum; although the high pressure portion is again nearly linear, it is no longer directly proportional to pressure. Finally, at high temperature (400 K), \"Z\" is above unity at all pressures. For all curves, \"Z\" approaches the ideal gas value of unity at low pressure and exceeds that value at very high pressure.\nTo better understand these curves, a closer look at the behavior for low temperature and pressure is given in the second figure. All of the curves start out with \"Z\" equal to unity at zero pressure and \"Z\" initially decreases as pressure increases. N is a gas under these conditions, so the distance between molecules is large, but becomes smaller as pressure increases. This increases the attractive interactions between molecules, pulling the molecules closer together and causing the volume to be less than for an ideal gas at the same temperature and pressure. Higher temperature reduces the effect of the attractive interactions and the gas behaves in a more nearly ideal manner.\n\nAs the pressure increases, the gas eventually reaches the gas-liquid coexistence curve, shown by the dashed line in the figure. When that happens, the attractive interactions have become strong enough to overcome the tendency of thermal motion to cause the molecules to spread out; so the gas condenses to form a liquid. Points on the vertical portions of the curves correspond to N being partly gas and partly liquid. On the coexistence curve, there are then two possible values for \"Z\", a larger one corresponding to the gas and a smaller value corresponding to the liquid. Once all the gas has been converted to liquid, the volume decreases only slightly with further increases in pressure; then \"Z\" is very nearly proportional to pressure.\n\nAs temperature and pressure increase along the coexistence curve, the gas becomes more like a liquid and the liquid becomes more like a gas. At the critical point, the two are the same. So for temperatures above the critical temperature (126.2 K), there is no phase transition; as pressure increases the gas gradually transforms into something more like a liquid. Just above the critical point there is a range of pressure for which \"Z\" drops quite rapidly (see the 130 K curve), but at higher temperatures the process is entirely gradual.\n\nThe final figures shows the behavior at temperatures well above the critical temperatures. The repulsive interactions are essentially unaffected by temperature, but the attractive interaction have less and less influence. Thus, at sufficiently high temperature, the repulsive interactions dominate at all pressures.\n\nThis can be seen in the graph showing the high temperature behavior. As temperature increases, the initial slope becomes less negative, the pressure at which \"Z\" is a minimum gets smaller, and the pressure at which repulsive interactions start to dominate, i.e. where \"Z\" goes from less than unity to greater than unity, gets smaller. At the Boyle temperature (327 K for N), the attractive and repulsive effects cancel each other at low pressure. Then \"Z\" remains at the ideal gas value of unity up to pressures of several tens of bar. Above the Boyle temperature, the compressibility factor is always greater than unity and increases slowly but steadily as pressure increases.\n\nIt is extremely difficult to generalize at what pressures or temperatures the deviation from the ideal gas becomes important. As a rule of thumb, the ideal gas law is reasonably accurate up to a pressure of about 2 atm, and even higher for small non-associating molecules. For example, methyl chloride, a highly polar molecule and therefore with significant intermolecular forces, the experimental value for the compressibility factor is formula_41 at a pressure of 10 atm and temperature of 100 °C. For air (small non-polar molecules) at approximately the same conditions, the compressibility factor is only formula_42 (see table below for 10 bars, 400 K).\n\nNormal air comprises in crude numbers 80 percent nitrogen and 20 percent oxygen . Both molecules are small and non-polar (and therefore non-associating). We can therefore expect that the behaviour of air within broad temperature and pressure ranges can be approximated as an ideal gas with reasonable accuracy. Experimental values for the compressibility factor confirm this.\n\nformula_1 values are calculated from values of pressure, volume (or density), and temperature in Vassernan, Kazavchinskii, and Rabinovich, \"Thermophysical Properties of Air and Air Components;' Moscow, Nauka, 1966, and NBS-NSF Trans. TT 70-50095, 1971: and Vassernan and Rabinovich, \"Thermophysical Properties of Liquid Air and Its Component, \"Moscow, 1968, and NBS-NSF Trans. 69-55092, 1970.\n\n\n\n<br>\n"}
{"id": "5118041", "url": "https://en.wikipedia.org/wiki?curid=5118041", "title": "Constraint algebra", "text": "Constraint algebra\n\nIn theoretical physics, a constraint algebra is a linear space of all constraints and all of their polynomial functions or functionals whose action on the physical vectors of the Hilbert space should be equal to zero.\n\nFor example, in electromagnetism, the equation for the Gauss' law\nis an equation of motion that does not include any time derivatives. This is why it is counted as a constraint, not a dynamical equation of motion. In quantum electrodynamics, one first constructs a Hilbert space in which Gauss' law does not hold automatically. The true Hilbert space of physical states is constructed as a subspace of the original Hilbert space of vectors that satisfy\nIn more general theories, the constraint algebra may be a noncommutative algebra.\n\n"}
{"id": "30303286", "url": "https://en.wikipedia.org/wiki?curid=30303286", "title": "Decoding Reality", "text": "Decoding Reality\n\nDecoding Reality: The Universe as Quantum Information is a popular science book by Vlatko Vedral published by Oxford University Press in 2010. Vedral examines information theory and proposes information as the most fundamental building block of reality. He argues what a useful framework this is for viewing all natural and physical phenomena. In building out this framework the books touches upon the origin of information, the idea of entropy, the roots of this thinking in thermodynamics, the replication of DNA, development of social networks, quantum behaviour at the micro and macro level, and the very role of indeterminism in the universe. The book finishes by considering the answer to the ultimate question: where did all of the information in the Universe come from? The ideas address concepts related to the nature of particles, time, determinism, and of reality itself.\n\nVedral believes in the principle that information is physical. \"Creation ex nihilo\" comes from Catholic dogma, the idea being that God created the universe out of nothing. Vedral says that invoking a supernatural being as an explanation for creation does not explain reality because the supernatural being would have to come into existence itself too somehow presumably from nothing (or else from an infinite regression of supernatural beings), thus of course the reality can come from nothing without a supernatural being. Occam's razor principle favours the simplest explanation. Vedral believes information is the fundamental building block of reality as it occurs at the macro level (economics, human behaviour etc.) as well as the subatomic level. Vedral argues that information is the only candidate for such a building block that can explain its own existence as information generates additional information that needs to be compressed thus generating more information. 'Annihilation of everything' is a more fitting term than \"creation ex nihilo\" Vedral states, as compression of possibilities is the process of how new information is created.\n\nVedral uses an Italo Calvino philosophical story about a tarot-like card game as the kernel for his metaphor of conscious life arriving \"in medias res\" to a pre-existing contextual reality. In this game the individual observers/players (Vedral suggests: quantum physics, thermodynamics, biology, sociology, economics, philosophy) lay down cards with ambiguous meanings as an attempt to communicate messages to deduce meaning out of the other players' interactions. The results (information) of previous rounds establish contextual rules for observers/players in subsequent rounds. The point of this game is not established until the last card has been played as later cards can change the meaning of previous events, as in the case of the quantum explanation for the photoelectric effect instantly disproving classical physics. Vedral points out that in our reality there is no last card.\n\nShannon entropy or information content measured as the surprise value of a particular event, is essentially inversely proportional to the logarithm of the event's probability, i = log(1/p). Claude Shannon's information theory arose from research at Bell labs, building upon George Boole's digital logic. As information theory predicts common and easily predicted words tend to become shorter for optimal communication channel efficiency while less common words tend to be longer for redundancy and error correction. Vedral compares the process of life to John von Neumann's self replicating automata. These are enduring information carriers that will survive wear and tear of the individual by producing copies that can in turn go on to produce more copies.\n\nGenetic code as an efficient digital information store, containing built in codon redundancy for error correction in transcription.\n\nExamines the Second law of thermodynamics and the process of information increasing entropy. Maxwell's Demon was thought to be a way around this inevitability, however such a demon would run out of information storage space and have to delete unwanted data thus having to do work to do so, increasing entropy.\n\nBlackjack as controlled risk taking using Shannon's information theory probability formulas. Casino as a ′cool′ financial entropy source and the gambler as a ′hot′ financial source, once again the Second law of thermodynamics means the flow is almost always from hot to cold in the long run. For managed risk spread bets widely and in high risk high reward investments (assuming a known probability), this is the Log optimal portfolio approach.\n\nSix degrees of separation means well connected people tend to be more successful as their social networks expose them to more chances to make choices they want. Schelling precommitment as strategy in social and self-control, for example burning your bridges by buying gym membership to help motivated self win over lazy self. Mutual information resulting in phase transitions in social and political demography as well as physical systems, like water freezing into ice at a particular critical temperature or magnetic fields spontaneously aligning in certain atoms when cooling from a molten state.\n\nVedral examines the basis of quantum information, the qubit, and examines one-time pad quantum cryptography as the most secure form of encryption because of its uncomputability. Quantum entanglement demonstrates the importance of mutual information in defining outcomes in a reality.\n\nQuantum computers offer a search advantage over classical computers by searching many database elements at once as a result of quantum superpositions. A sufficiently advanced quantum computer would break current encryption methods by factorizing large numbers several orders of magnitude faster than any existing classical computer. Any computable problem may be expressed as a general quantum search algorithm although classical computers may have an advantage over quantum search when using more efficient tailored classical algorithms. The issue with quantum computers is that a measurement must be made to determine if the problem is solved which collapses the superposition. Vedral points out that unintentional interaction with the environment can be mitigated with redundancy, and this would be necessary if we were to scale up current quantum computers to achieve greater utility, i.e. to utilize 10 qubits have a 100 atom quantum system so that if one atom decoheres a consensus will still be held by the other 9 for the state of the same qubit.\n\nRandomness is key to generating new sources of surprise in a reality. Compression of these new sources to discard unimportant information is the deterministic element and organising principle.\n\nThe information content of the universe as measured in bits or qubits. Vedral uses the initial effort of Archimedes of Syracuse in calculating the amount of sand that could theoretically fit inside the universe and compares it to a modern-day attempt to calculate the bit content of the universe. Vedral however sees this content as ultimately limitless as possibly maximum entropy is never reached as compression of complexity is an open ended process and random events will continue to occur. As Vedral sees information as the ultimate building block of physical reality, he speculates that information originating at any scale can force outcomes in all other scales to abide where mutual information is shared. For example, a human performed macro-level scientific test in search of a behaviour in a quantum particle could set parameters for that type of particle in the future when subjected to a similar test.\n\nThe information basis for \"creation ex nihilo\". According to John von Neumann, starting trivially from an empty set of numbers an infinite sequence of numbers can bootstrap their way out. An empty set creates the number 1 by observing an empty set within itself which is enough of a basis for distinguishability. It creates the number 2 by observing an empty set within the second empty set and the number 1, and so on. Vedral sees this not as creation but as data compression, as every event of a reality breaks the symmetry of the pre-existing formlessness. Science is the process of describing a large amount of observed phenomena in a compressed programmatic way to predict future outcomes, and in this process of data compression science creates new information by eliminating all contrary possibilities to explain those phenomena.\n\nThe book explains the world as being made up of information. The Universe and its workings are the ebb and flow of information. We are all transient patterns of information, passing on the recipe for our basic forms to future generations using a four-letter digital code called DNA. In this engaging and mind-stretching account, Vlatko Vedral considers some of the deepest questions about the Universe and considers the implications of interpreting it in terms of information. He explains the nature of information, the idea of entropy, and the roots of this thinking in thermodynamics. He describes the bizarre effects of quantum behaviour - effects such as 'entanglement', which Einstein called 'spooky action at a distance' and explores cutting edge work on the harnessing quantum effects in hyperfast quantum computers, and how recent evidence suggests that the weirdness of the quantum world, once thought limited to the tiniest scales, may reach into the macro world. Vedral finishes by considering the answer to the ultimate question: where did all of the information in the Universe come from? The answers he considers are exhilarating, drawing upon the work of distinguished physicist John Wheeler and his concept of “it from bit”. The ideas challenge our concept of the nature of particles, of time, of determinism, and of reality itself.\n\n"}
{"id": "3776242", "url": "https://en.wikipedia.org/wiki?curid=3776242", "title": "Detlef Quadfasel", "text": "Detlef Quadfasel\n\nDetlef Rudolf Quadfasel is a professor of Geophysics at Niels Bohr Institute for Astronomy, Physics and Geophysics at Copenhagen University and Oceanography at the Institut für Meereskunde, Hamburg. He is joint editor of \"Progress in Oceanography\". He is involved in a number of projects, including Climate monitoring - Greenland Sea Convection.\n\n\n"}
{"id": "5112137", "url": "https://en.wikipedia.org/wiki?curid=5112137", "title": "Distancing language", "text": "Distancing language\n\nDistancing language is phrasing used by a person to psychologically \"distance\" themselves from a statement. It is used in an effort to separate a particular topic, idea, discussion, or group from their own personal identity for the purpose of self-deception, deceiving others, or disunifying oneself from a team, among others. The use of distancing language is primarily subconscious as a means to disengage oneself from acts or ideas that conflict with their personal values, beliefs, and ideals, and is often used to identify if a person is lying. Conscious uses of distancing language are often euphemistic in nature in order to downplay or desensitize a loaded topic in an effort to separate the speaker from the subject at hand. \n\nThe use of first-person pronouns as a singular (\"I\", \"me\", \"my\", \"myself\"), and as a plural (\"we\", \"us\", \"our\", \"ourselves\") indicates a psychological closeness between the speaker and the topic of discussion. Omitting first-person pronouns suggests the absence of responsibility and identification of the ideas conveyed in the statement.\n\nIn the English language, the pronoun \"you\" can be used as an appropriate use of distancing language in a universal context, where the statements are intended to be applied to anyone in the general public (e.g. the statement \"You should never drink and drive\" pertains to anyone who drives). Consequently, “you” is a common replacement for a first-person pronoun, often to hint at one’s lack of commitment to or interest in a group or organization.\n\n\nThe use of a passive voice allows for the omission of identity or ownership, since the nature of a passive voice is that an action is done to an object, and the action's agent is not necessary.\n\n\nDemonstrative determiners such as “that” and “those” illustrate a physical or psychological distance between the object and the speaker. The demonstratives “this” and “these” refer to an object that is close to the speaker.\n\n\nAvoiding straight-forward statements is common in deception, either to another person or in self-deception, in order to avoid details that might signify ownership or personal knowledge of the topic. This is accomplished through statements that deflect the topic or that minimize the importance and impact of the topic; alternatively, the speaker can keep statements vague or use hedge words.\n\n\nEuphemisms are used as a means of minimizing negative emotions that the statement may elicit (either from the speaker or others), in order to make the speaker appear unaffected and the situation impersonal.\n\n\nFormer U.S. President Bill Clinton's statement denying a sexual relationship with Monica Lewinsky is a famous example of distancing language used in deception, referring to Lewinsky as \"that woman\" despite the fact it was public knowledge that Clinton knew her. In addition, Clinton avoided first-person pronouns, alternatively using the \"you\" pronoun in reference to the White House (as a whole) being inundated with phone calls. Current U.S. President Donald Trump has shown similar distancing language when dismissing sexual misconduct allegations and shady business deals, and reverts to using \"you\" when speaking about plans he does not wish to disclose.\n\n"}
{"id": "55053362", "url": "https://en.wikipedia.org/wiki?curid=55053362", "title": "ESO 269-57", "text": "ESO 269-57\n\nESO 269-57 is a large spiral galaxy located about 150 million light-years away in the constellation Centaurus. ESO 269-57 has a diameter of about 200,000 light-years. It is part of group of galaxies known as GG 342.\n\nESO 269-57 has an inner ring surrounding its bright center. The ring is made up of several tightly wound spiral arms. Surrounding the inner ring, there are two outer arms which are made up of star-forming regions that appear to split into several branches of arms.\n\nOn March 3, 1992 a type Ia supernova was discovered in ESO 269-57.\n\n"}
{"id": "1543423", "url": "https://en.wikipedia.org/wiki?curid=1543423", "title": "Eye tracking", "text": "Eye tracking\n\nEye tracking is the process of measuring either the point of gaze (where one is looking) or the motion of an eye relative to the head. An eye tracker is a device for measuring eye positions and eye movement. Eye trackers are used in research on the visual system, in psychology, in psycholinguistics, marketing, as an input device for human-computer interaction, and in product design. There are a number of methods for measuring eye movement. The most popular variant uses video images from which the eye position is extracted. Other methods use search coils or are based on the electrooculogram.\n\nIn the 1800s, studies of eye movement were made using direct observations.\n\nIn 1879 in Paris, Louis Émile Javal observed that reading does not involve a smooth sweeping of the eyes along the text, as previously assumed, but a series of short stops (called fixations) and quick saccades. This observation raised important questions about reading, questions which were explored during the 1900s: On which words do the eyes stop? For how long? When do they regress to already seen words?\nEdmund Huey built an early eye tracker, using a sort of contact lens with a hole for the pupil. The lens was connected to an aluminum pointer that moved in response to the movement of the eye. Huey studied and quantified regressions (only a small proportion of saccades are regressions), and he showed that some words in a sentence are not fixated.\n\nThe first non-intrusive eye-trackers were built by Guy Thomas Buswell in Chicago, using beams of light that were reflected on the eye and then recording them on film. Buswell made systematic studies into reading and picture viewing.\n\nIn the 1950s, Alfred L. Yarbus did important eye tracking research and his 1967 book is often quoted. He showed that the task given to a subject has a very large influence on the subject's eye movement. He also wrote about the relation between fixations and interest:\n\nIn the 1970s, eye-tracking research expanded rapidly, particularly reading research. A good overview of the research in this period is given by Rayner.\n\nIn 1980, Just and Carpenter formulated the influential \"Strong eye-mind hypothesis\", that \"there is no appreciable lag between what is fixated and what is processed\". If this hypothesis is correct, then when a subject looks at a word or object, he or she also thinks about it (process cognitively), and for exactly as long as the recorded fixation. The hypothesis is often taken for granted by researchers using eye-tracking. However, gaze-contingent techniques offer an interesting option in order to disentangle overt and covert attentions, to differentiate what is fixated and what is processed.\n\nDuring the 1980s, the eye-mind hypothesis was often questioned in light of covert attention, the attention to something that one is not looking at, which people often do. If covert attention is common during eye-tracking recordings, the resulting scan-path and fixation patterns would often show not where our attention has been, but only where the eye has been looking, failing to indicate cognitive processing.\n\nThe 1980s also saw the birth of using eye-tracking to answer questions related to human-computer interaction. Specifically, researchers investigated how users search for commands in computer menus. Additionally, computers allowed researchers to use eye-tracking results in real time, primarily to help disabled users.\n\nMore recently, there has been growth in using eye tracking to study how users interact with different computer interfaces. Specific questions researchers ask are related to how easy different interfaces are for users. The results of the eye tracking research can lead to changes in design of the interface. Yet another recent area of research focuses on Web development. This can include how users react to drop-down menus or where they focus their attention on a website so the developer knows where to place an advertisement.\n\nAccording to Hoffman, current consensus is that visual attention is always slightly (100 to 250 ms) ahead of the eye. But as soon as attention moves to a new position, the eyes will want to follow.\n\nWe still cannot infer specific cognitive processes directly from a fixation on a particular object in a scene. For instance, a fixation on a face in a picture may indicate recognition, liking, dislike, puzzlement etc. Therefore, eye tracking is often coupled with other methodologies, such as introspective verbal protocols.\n\nEye-trackers measure rotations of the eye in one of several ways, but principally they fall into three categories: (i) measurement of the movement of an object (normally, a special contact lens) attached to the eye; (ii) optical tracking without direct contact to the eye; and (iii) measurement of electric potentials using electrodes placed around the eyes.\n\nThe first type uses an attachment to the eye, such as a special contact lens with an embedded mirror or magnetic field sensor, and the movement of the attachment is measured with the assumption that it does not slip significantly as the eye rotates. Measurements with tight-fitting contact lenses have provided extremely sensitive recordings of eye movement, and magnetic search coils are the method of choice for researchers studying the dynamics and underlying physiology of eye movement. This method allows the measurement of eye movement in horizontal, vertical and torsion directions.\n\nThe second broad category uses some non-contact, optical method for measuring eye motion. Light, typically infrared, is reflected from the eye and sensed by a video camera or some other specially designed optical sensor. The information is then analyzed to extract eye rotation from changes in reflections. Video-based eye trackers typically use the corneal reflection (the first Purkinje image) and the center of the pupil as features to track over time. A more sensitive type of eye-tracker, the dual-Purkinje eye tracker, uses reflections from the front of the cornea (first Purkinje image) and the back of the lens (fourth Purkinje image) as features to track. A still more sensitive method of tracking is to image features from inside the eye, such as the retinal blood vessels, and follow these features as the eye rotates. Optical methods, particularly those based on video recording, are widely used for gaze-tracking and are favored for being non-invasive and inexpensive.\nThe third category uses electric potentials measured with electrodes placed around the eyes. The eyes are the origin of a steady electric potential field which can also be detected in total darkness and if the eyes are closed. It can be modelled to be generated by a dipole with its positive pole at the cornea and its negative pole at the retina. The electric signal that can be derived using two pairs of contact electrodes placed on the skin around one eye is called Electrooculogram (EOG). If the eyes move from the centre position towards the periphery, the retina approaches one electrode while the cornea approaches the opposing one. This change in the orientation of the dipole and consequently the electric potential field results in a change in the measured EOG signal. Inversely, by analysing these changes in eye movement can be tracked. Due to the discretisation given by the common electrode setup, two separate movement components – a horizontal and a vertical – can be identified. A third EOG component is the radial EOG channel, which is the average of the EOG channels referenced to some posterior scalp electrode. This radial EOG channel is sensitive to the saccadic spike potentials stemming from the extra-ocular muscles at the onset of saccades, and allows reliable detection of even miniature saccades.\n\nDue to potential drifts and variable relations between the EOG signal amplitudes and the saccade sizes, it is challenging to use EOG for measuring slow eye movement and detecting gaze direction. EOG is, however, a very robust technique for measuring saccadic eye movement associated with gaze shifts and detecting blinks. \nContrary to video-based eye-trackers, EOG allows recording of eye movements even with eyes closed, and can thus be used in sleep research. It is a very light-weight approach that, in contrast to current video-based eye-trackers, requires only very low computational power; works under different lighting conditions; and can be implemented as an embedded, self-contained wearable system. It is thus the method of choice for measuring eye movement in mobile daily-life situations and REM phases during sleep. The major disadvantage of EOG is its relatively poor gaze-direction accuracy compared to a video tracker. That is, it is difficult to determine with good accuracy exactly where a subject is looking, though the time of eye movements can be determined.\n\nThe most widely used current designs are video-based eye-trackers. A camera focuses on one or both eyes and records eye movement as the viewer looks at some kind of stimulus. Most modern eye-trackers use the center of the pupil and infrared / near-infrared non-collimated light to create corneal reflections (CR). The vector between the pupil center and the corneal reflections can be used to compute the point of regard on surface or the gaze direction. A simple calibration procedure of the individual is usually needed before using the eye tracker.\n\nTwo general types of infrared / near-infrared (also known as active light) eye-tracking techniques are used: bright-pupil and dark-pupil. Their difference is based on the location of the illumination source with respect to the optics. If the illumination is coaxial with the optical path, then the eye acts as a retroreflector as the light reflects off the retina creating a bright pupil effect similar to red eye. If the illumination source is offset from the optical path, then the pupil appears dark because the retroreflection from the retina is directed away from the camera.\n\nBright-pupil tracking creates greater iris/pupil contrast, allowing more robust eye-tracking with all iris pigmentation, and greatly reduces interference caused by eyelashes and other obscuring features. It also allows tracking in lighting conditions ranging from total darkness to very bright.\n\nAnother, less used, method is known as passive light. It uses visible light to illuminate, something which may cause some distractions to users. Another challenge with this method is that the contrast of the pupil is less than in the active light methods, therefore, the center of iris is used for calculating the vector instead. This calculation needs to detect the boundary of the iris and the white sclera (limbus tracking). It presents another challenge for vertical eye movements due to obstruction of eyelids.\n\nEye-tracking setups vary greatly: some are head-mounted, some require the head to be stable (for example, with a chin rest), and some function remotely and automatically track the head during motion. Most use a sampling rate of at least 30 Hz. Although 50/60 Hz is more common, today many video-based eye trackers run at 240, 350 or even 1000/1250 Hz, speeds needed in order to capture fixational eye movements or correctly measure saccade dynamics.\n\nEye movements are typically divided into fixations and saccades – when the eye gaze pauses in a certain position, and when it moves to another position, respectively. The resulting series of fixations and saccades is called a scanpath. Smooth pursuit describes the eye following a moving object. Fixational eye movements include microsaccades: small, involuntary saccades that occur during attempted fixation. Most information from the eye is made available during a fixation or smooth pursuit, but not during a saccade.\n\nScanpaths are useful for analyzing cognitive intent, interest, and salience. Other biological factors (some as simple as gender) may affect the scanpath as well. Eye tracking in human–computer interaction (HCI) typically investigates the scanpath for usability purposes, or as a method of input in gaze-contingent displays, also known as gaze-based interfaces.\n\nInterpretation of the data that is recorded by the various types of eye-trackers employs a variety of software that animates or visually represents it, so that the visual behavior of one or more users can be graphically resumed. The video is generally manually coded to identify the AOIs(Area Of Interests) or recently using artificial intelligence. Graphical presentation is rarely the basis of research results, since they are limited in terms of what can be analysed - research relying on eye-tracking, for example, usually requires quantitative measures of the eye movement events and their parameters, The following visualisations are the most commonly used:\n\nAnimated representations of a point on the interface\nThis method is used when the visual behavior is examined individually indicating where the user focused their gaze in each moment, complemented with a small path that indicates the previous saccade movements, as seen in the image.\n\nStatic representations of the saccade path\nThis is fairly similar to the one described above, with the difference that this is static method. A higher level of expertise than with the animated ones is required to interpret this.\n\nHeat maps\nAn alternative static representation, used mainly for the agglomerated analysis of the visual exploration patterns in a group of users. In these representations, the ‘hot’ zones or zones with higher density designate where the users focused their gaze (not their attention) with a higher frequency. Heat maps are the best known visualization technique for eyetracking studies.\n\nBlind zones maps, or focus maps\nThis method is a simplified version of the Heat maps where the visually less attended zones by the users are displayed clearly, thus allowing for an easier understanding of the most relevant information, that is to say, we are informed about which zones were not seen by the users.\n\nEye-trackers necessarily measure the rotation of the eye with respect to some frame of reference. This is usually tied to the measuring system. Thus, if the measuring system is head-mounted, as with EOG or a video-based system mounted to a helmet, then eye-in-head angles are measured. To deduce the line of sight in world coordinates, the head must be kept in a constant position or its movements must be tracked as well. In these cases, head direction is added to eye-in-head direction to determine gaze direction.\n\nIf the measuring system is table-mounted, as with scleral search coils or table-mounted camera (“remote”) systems, then gaze angles are measured directly in world coordinates. Typically, in these situations head movements are prohibited. For example, the head position is fixed using a bite bar or a forehead support. Then a head-centered reference frame is identical to a world-centered reference frame. Or colloquially, the eye-in-head position directly determines the gaze direction.\n\nSome results are available on human eye movements under natural conditions where head movements are allowed as well. The relative position of eye and head, even with constant gaze direction, influences neuronal activity in higher visual areas.\n\nA great deal of research has gone into studies of the mechanisms and dynamics of eye rotation, but the goal of eye- tracking is most often to estimate gaze direction. Users may be interested in what features of an image draw the eye, for example. It is important to realize that the eye-tracker does not provide absolute gaze direction, but rather can measure only changes in gaze direction. In order to know precisely what a subject is looking at, some calibration procedure is required in which the subject looks at a point or series of points, while the eye tracker records the value that corresponds to each gaze position. (Even those techniques that track features of the retina cannot provide exact gaze direction because there is no specific anatomical feature that marks the exact point where the visual axis meets the retina, if indeed there is such a single, stable point.) An accurate and reliable calibration is essential for obtaining valid and repeatable eye movement data, and this can be a significant challenge for non-verbal subjects or those who have unstable gaze.\n\nEach method of eye-tracking has advantages and disadvantages, and the choice of an eye-tracking system depends on considerations of cost and application. There are offline methods and online procedures like AttentionTracking. There is a trade-off between cost and sensitivity, with the most sensitive systems costing many tens of thousands of dollars and requiring considerable expertise to operate properly. Advances in computer and video technology have led to the development of relatively low-cost systems that are useful for many applications and fairly easy to use. Interpretation of the results still requires some level of expertise, however, because a misaligned or poorly calibrated system can produce wildly erroneous data.\n\nThe eye movement of two groups of drivers have been filmed with a special head camera by a team of the Swiss Federal Institute of Technology: Novice and experienced drivers had their eye-movement recorded while approaching a bend of a narrow road.\nThe series of images has been condensed from the original film frames to show 2 eye fixations per image for better comprehension.\n\nEach of these stills corresponds to approximately 0.5 seconds in realtime.\n\nThe series of images shows an example of eye fixations #9 to #14 of a typical novice and an experienced driver.\n\nComparison of the top images shows that the experienced driver checks the curve and even has Fixation No. 9 left to look aside while the novice driver needs to check the road and estimate his distance to the parked car.\n\nIn the middle images, the experienced driver is now fully concentrating on the location where an oncoming car could be seen. The novice driver concentrates his view on the parked car.\n\nIn the bottom image the novice is busy estimating the distance between the left wall and the parked car, while the experienced driver can use his peripheral vision for that and still concentrate his view on the dangerous point of the curve: If a car appears there, he has to give way, i. e. stop to the right instead of passing the parked car.\n\nWhile walking, elderly subjects depend more on foveal vision than do younger subjects. Their walking speed is decreased by a limited visual field, probably caused by a deteriorated peripheral vision.\n\nYounger subjects make use of both their central and peripheral vision while walking. Their peripheral vision allows faster control over the process of walking.\n\nA wide variety of disciplines use eye-tracking techniques, including cognitive science; psychology (notably psycholinguistics; the visual world paradigm); human-computer interaction (HCI); human factors and ergonomics; marketing research and medical research (neurological diagnosis). Specific applications include the tracking eye movement in language reading, music reading, human activity recognition, the perception of advertising, and the playing of sports.\n\nIn recent years, the increased sophistication and accessibility of eye-tracking technologies have generated a great deal of interest in the commercial sector. Applications include web usability, advertising, sponsorship, package design and automotive engineering. In general, commercial eye-tracking studies function by presenting a target stimulus to a sample of consumers while an eye tracker is used to record the activity of the eye. Examples of target stimuli may include websites; television programs; sporting events; films and commercials; magazines and newspapers; packages; shelf displays; consumer systems (ATMs, checkout systems, kiosks); and software. The resulting data can be statistically analyzed and graphically rendered to provide evidence of specific visual patterns. By examining fixations, saccades, pupil dilation, blinks and a variety of other behaviors, researchers can determine a great deal about the effectiveness of a given medium or product. While some companies complete this type of research internally, there are many private companies that offer eye-tracking services and analysis.\n\nOne of the most prominent fields of commercial eye-tracking research is web usability. While traditional usability techniques are often quite powerful in providing information on clicking and scrolling patterns, eye-tracking offers the ability to analyze user interaction between the clicks and how much time a user spends between clicks, thereby providing valuable insight into which features are the most eye-catching, which features cause confusion and which are ignored altogether. Specifically, eye-tracking can be used to assess search efficiency, branding, online advertisements, navigation usability, overall design and many other site components. Analyses may target a prototype or competitor site in addition to the main client site.\n\nEye-tracking is commonly used in a variety of different advertising media. Commercials, print ads, online ads and sponsored programs are all conducive to analysis with current eye-tracking technology. For instance in newspapers, eye-tracking studies can be used to find out in what way advertisements should be mixed with the news in order to catch the reader’s eyes. Analyses focus on visibility of a target product or logo in the context of a magazine, newspaper, website, or televised event. One example is an analysis of eye movements over advertisements in the Yellow Pages. The study focused on what particular features caused people to notice an ad, whether they viewed ads in a particular order and how viewing times varied. The study revealed that ad size, graphics, color, and copy all influence attention to advertisements. Knowing this allows researchers to assess in great detail how often a sample of consumers fixates on the target logo, product or ad. As a result, an advertiser can quantify the success of a given campaign in terms of actual visual attention. Another example of this is a study that found that in a search engine results page, authorship snippets received more attention than the paid ads or even the first organic result.\n\n\n"}
{"id": "5569055", "url": "https://en.wikipedia.org/wiki?curid=5569055", "title": "First-order hold", "text": "First-order hold\n\nFrst-order hold (FOH) is a mathematical model of the practical reconstruction of sampled signals that could be done by a conventional digital-to-analog converter (DAC) and an analog circuit called an integrator. For FOH, the signal is reconstructed as a piecewise linear approximation to the original signal that was sampled. A mathematical model such as FOH (or, more commonly, the zero-order hold) is necessary because, in the sampling and reconstruction theorem, a sequence of Dirac impulses, \"x\"(\"t\"), representing the discrete samples, \"x\"(\"nT\"), is low-pass filtered to recover the original signal that was sampled, \"x\"(\"t\"). However, outputting a sequence of Dirac impulses is impractical. Devices can be implemented, using a conventional DAC and some linear analog circuitry, to reconstruct the piecewise linear output for either predictive or delayed FOH.\n\nEven though this is \"not\" what is physically done, an identical output can be generated by applying the hypothetical sequence of Dirac impulses, \"x\"(\"t\"), to a linear time-invariant system, otherwise known as a linear filter with such characteristics (which, for an LTI system, are fully described by the impulse response) so that each input impulse results in the correct piecewise linear function in the output.\n\nFirst-order hold is the hypothetical filter or LTI system that converts the ideally sampled signal\n\nto the piecewise linear signal\n\nresulting in an effective impulse response of\n\nThe effective frequency response is the continuous Fourier transform of the impulse response.\n\nThe Laplace transform transfer function of FOH is found by substituting \"s\" = \"i\" 2 π \"f\":\n\nThis is an acausal system in that the linear interpolation function moves toward the value of the next sample before such sample is applied to the hypothetical FOH filter.\n\nDelayed first-order hold, sometimes called causal first-order hold, is identical to FOH above except that its output is delayed by one sample period resulting in a delayed piecewise linear output signal\n\nresulting in an effective impulse response of\n\nThe effective frequency response is the continuous Fourier transform of the impulse response.\n\nThe Laplace transform transfer function of the delayed FOH is found by substituting \"s\" = \"i\" 2 π \"f\":\n\nThe delayed output makes this a causal system. The impulse response of the delayed FOH does not respond before the input impulse.\n\nThis kind of delayed piecewise linear reconstruction is physically realizable by implementing a digital filter of gain \"H\"(\"z\") = 1 − \"z\", applying the output of that digital filter (which is simply \"x\"[\"n\"]−\"x\"[\"n\"−1]) to an ideal conventional digital-to-analog converter (that has an inherent zero-order hold as its model) and integrating (in continuous-time, \"H\"(\"s\") = 1/(\"sT\")) the DAC output.\n\nLastly, the predictive first-order hold is quite different. This is a \"causal\" hypothetical LTI system or filter that converts the ideally sampled signal\n\ninto a piecewise linear output such that the current sample and immediately previous sample are used to linearly extrapolate up to the next sampling instance. The output of such a filter would be \n\nresulting in an effective impulse response of\n\nThe effective frequency response is the continuous Fourier transform of the impulse response.\n\nThe Laplace transform transfer function of the predictive FOH is found by substituting \"s\" = \"i\" 2 π \"f\":\n\nThis a causal system. The impulse response of the predictive FOH does not respond before the input impulse.\n\nThis kind of piecewise linear reconstruction is physically realizable by implementing a digital filter of gain \"H\"(\"z\") = 1 − \"z\", applying the output of that digital filter (which is simply \"x\"[\"n\"]−\"x\"[\"n\"−1]) to an ideal conventional digital-to-analog converter (that has an inherent zero-order hold as its model) and applying that DAC output to an analog filter with transfer function \"H\"(\"s\") = (1+\"sT\")/(\"sT\").\n\n\n"}
{"id": "20110387", "url": "https://en.wikipedia.org/wiki?curid=20110387", "title": "Flexion teardrop fracture", "text": "Flexion teardrop fracture\n\nA flexion teardrop fracture is a fracture of the anteroinferior aspect of a cervical vertebral body due to flexion of the spine along with vertical axial compression. The fracture continues sagittally through the vertebral body, and is associated with deformity of the body and subluxation or dislocation of the facet joints at the injured level. A flexion teardrop fracture is usually associated with a spinal cord injury, often a result of displacement of the posterior portion of the vertebral body into the spinal canal.\n\nThe flexion teardrop fracture should not be confused with a similar-looking vertebral fracture called \"extension teardrop fracture\". Both usually occur in the cervical spine, but as their names suggest, they result from different mechanisms (flexion-compression vs. hyperextension). Both are associated with a small fragment being broken apart from the anteroinferior corner of the affected vertebra. Flexion teardrop fractures usually involve instability in all elements of the spine at the injured level, commonly occur at the C4-C7 vertebra, and have a high association with spinal cord injury (in particular anterior cord syndrome). In comparison, the extension-type fracture occurs more commonly at C2 or C3, causes less if any disruption to the middle and posterior elements, and does not usually result in spinal cord injury (however it may co-occur with more dangerous spine injuries). \n"}
{"id": "54491165", "url": "https://en.wikipedia.org/wiki?curid=54491165", "title": "Goop (company)", "text": "Goop (company)\n\nGoop (often stylized as goop) is a company owned by actress Gwyneth Paltrow. It was launched as a \"lifestyle brand\" by Paltrow in September 2008, beginning as a weekly e-mail newsletter providing new age advice, such as \"police your thoughts\" and \"eliminate white foods\", and the slogan \"Nourish the Inner Aspect\". A lifestyle website was later added, and then Goop expanded into e-commerce, collaborating with fashion brands, launching pop-up shops, holding a \"wellness summit\", and launching a print magazine.\n\nGoop has faced criticism for promoting and selling products and treatments that have no scientific basis, lack efficacy, and are recognized by the medical establishment as harmful or as misleading. In 2017, consumer advocacy group Truth in Advertising filed a complaint with the government regulatory agency regarding over 50 health claims made by Goop as dangerous and false.\n\nGoop began as a newsletter in 2008, with an editor's note in each email from Paltrow offering insight into her daily life. Goop was incorporated in 2011. According to Paltrow, the company's name \"is a nickname, like my name is G.P., so that is really where it came from. And I wanted it to be a word that means nothing and could mean anything.\"\n\nSeb Bishop served as CEO of Goop from 2011 to 2014. Several other celebrities were inspired by Goop to launch their own lifestyle websites. In 2014, Goop hired Oxygen Media founder and former CEO of Martha Stewart Living Omnimedia Lisa Gersh as CEO. From 2014 onwards, Goop's wellness content became increasingly radical: to the point where doctors would begin to call it pseudoscience and media outlets would describe Goop's content as \"no longer ludicrous — no, now it was dangerous\".\n\nIn 2015, Paltrow stated that she wanted goop to \"be its own stand-alone brand\". As of 2015, around two dozen people worked for Goop.\n\nThe company had approximately 60 employees in 2016. In the same year, Goop's most searched topic was \"detox\". Later in 2016, Paltrow moved the company's operations to Los Angeles from the UK following her separation from Chris Martin and Gersh left the position of CEO. The position remained vacant until early 2017, when the board named Paltrow - who had previously announced in 2016 that she would be stepping away from Goop - to fill the post.\n\nBy 2017, Goop had 90 employees, and was attracting criticism, including a lawsuit filed by Truth In Advertising. In an April 2017 Jezebel article, Stassa Edwards criticized Goop's marketing and retail strategy, claiming that the company profits \"from endless illness.\" Jill Avery, a brand analyst, has noted how Goop's response to criticism seems designed to \"strengthen their brand and draw their customers closer\", noting Goop's use of feminism, traditional Asian medicines and Eastern philosophies, and anti-establishment politics to do so. Paltrow characterizes criticisms as \"cultural firestorms\" which cause an influx of website traffic, stating that she can \"monetize those eyeballs\".\n\nIn 2018, Goop began assembling an internal science and regulatory team. Goop in partnership with Google also began selling Google Home smart speakers and accessories in the Goop Lab stores and holiday pop-up shops, and opened its first permanent East Coast Goop store called Goop Lab located at 25 Bond Street in New York. This follows the permanent location they opened in Brentwood, California in September 2017.\n\nIn 2011, Goop generated £81,000 in sales, and the year after it generated £1.1 million in sales, with a loss of £23,000. As of 2013, Goop had net liabilities of £540,086. Goop doubled its revenues from 2014 to 2015, and again from 2015 to 2016.\n\nIn 2016, Goop received $15 million USD in Series B funding from venture capital firms NEA, Felix Capital and 14W Venture Partner. This corresponded with Goop centralizing their operations in the Los Angeles area, where Paltrow is based, and away from New York, where Gersh lives.\n\nIn 2018, Goop raised $50 million in Series C funding from firms including NEA, Lightspeed and Felix Capital, bringing total investment in the company to $82 million. The company's valuation rose to $250 million.\n\nGoop launched an online shop in 2012, and it earned $1.5 million for Goop during its first year of operation. By 2014, the Goop newsletter had an estimated 700,000 subscribers.\n\nIn 2015, Goop launched a publishing imprint, Goop Press, with Grand Central Publishing, and planned to release one title per year.\n\nIn 2016, additional funds also led to an increase in staffing for Goop, as well as the launch of new products, including a fashion label with a focus on practical, tailored clothing.\n\nIn 2017, Goop entered the vitamin and supplements market. Paltrow first had the idea to market supplements after receiving a Myers' cocktail from Alejandro Junger in 2007. The supplements range sold over $100,000 worth of product on the launch day. In April, Goop announced that they had entered into an agreement with Condé Nast to launch a new print magazine under the name Goop. The quarterly magazine was launched in September, but only ran for two issues, with Nast replacing pieces that failed their fact-checking process by travel articles, and a disagreement over the use of the magazine to promote Goop products. In December 2017, Goop announced the launch of a digital shop in Canada, following its first physical store in the country in partnership with Nordstrom.\n\nGoop generates revenue from advertising and also sells a Goop-branded clothing line, a perfume, and books. As of 2017 Goop readers have an average household income of $100k+ per year.\n\nGoop launched a podcast in March 2018 using Cadence13 as its digital platform. Its first guest was Oprah Winfrey. The podcast reached number one in the Apple Podcast charts on March 8. It is largely hosted by Goop CCO Elise Loehnen and has a weekly audience of 100,000 to 650,000. The company also partnered with Westin Hotels & Resorts to offer a range of fitness programs called \"G. Sport Sessions\" later that summer.\n\nGoop expanded its fashion offerings in 2018, hiring Danielle Pergament of Allure to be Goop's editor in chief, directly under CCO Elise Loehnen. Ali Pew of InStyle was hired as Goop's fashion director and Anne Keane, formerly of Lucky magazine is now Goop's fashion strategy director, and they will be attending New York Fashion Week as representatives of the company. In June 2018, Goop launched a capsule collection with fashion designer Lilly Pulitzer.\n\nIn June 2018, Goop opened its first international pop-up shop in London. The company also hired its first chief marketing officer, Andres Sosa, to work on further expansion in the UK. In August 2018, Goop launched a furniture and home decor line with Crate and Barrel's CB2 brand.\n\nIn October 2018, Goop started selling Suzy Batiz's new brand, Supernatural, in the Highland Park Village pop-up shop and on the Goop website. Batiz explained that she was interested in the partnership after being a panelist at In Goop Health and seeing the brands at the event.\n\nSince Goop's inception in 2008, it has launched a number of brands and product lines. The \"Beauty\" section of the website works in tandem with the products sold in the shop. Goop brands and product lines are sold online, at the Goop Lab in Brentwood Country Mart in Los Angeles, and at pop-up shops in Los Angeles, New York, Chicago, the Hamptons, Dallas and Aspen, and in collaboration with Nordstrom stores. Goop-branded products quadrupled in value in 2017. These brands include:\n\n\nGoop held its first wellness summit in June 2017. The event had over 600 attendees. The company's second In goop Health summit in New York City in January 2018. Celebrity guests and speakers included Drew Barrymore, Chelsea Handler and Laura Linney. Panelists included Kelly Brogan, a \"holistic health psychiatrist\", who has disputed the effectiveness of both vaccinations and HIV medications. The invitation of Brogan has been criticized by \"Page Six\" and \"Jezebel\". The third summit was held in Culver City, California in June 2018. Guests included Meg Ryan and Janet Mock. Paltrow is considering ways to take the wellness summit \"on the road\", so that it can access a wider audience. In October 2018, Goop held its fourth summit at the Stanley Park pavilion in Vancouver.\n\nIn December 2016 a book parodying Goop called \"Glop: Nontoxic Ideas That Will Make You Look Ridiculous and Feel Pretentious\" was published and included advice like, \"Our modern lifestyles are absolutely full of toxins — nearly everything we come into daily contact with, from Egyptian cotton sheets to 8-carat diamonds to yacht paint, is dangerously noxious.\"\n\nBy June 2017 the \"New York Times\" reported that parodying Goop had \"become a national pastime\".\n\nPaltrow appeared on The Late Show in 2018 in a spoof segment announcing a collaboration between Goop and Stephen Colbert's lifestyle brand, Covetton House, featuring a sponge which cost $900. Later that year, Colbert and Paltrow collaborated on a line of goods to raise money for education in the United States.\n\nIn March 2018 Botnik Studios created a newsletter called \"goob\" parodying Goop, which generated text using predictive text; headlines included \"Listen to Your Body: Your Migraines are Podcasts Trying to be Produced\".\n\nA performance artist sold \"Hot Dog Water\" outside the fourth In Goop Health summit, a bottle of water with a hotdog inside, which parodied a product sold on Goop with a crystal inside a bottle of water.\n\nGoop has been criticized for showcasing expensive products and making \"out of touch\" recommendations that many readers cannot afford, which Paltrow has responded to by stating that such products and recommendations are \"aspirational\", furthermore arguing that the items available cannot be made for a lower price point, and that the content of Goop is free. Dana Logan argues that Goop is an example of asceticism as part of consumer culture. Goop has also drawn criticism for selling cosmetics containing the same ostensibly harmful chemicals which the site tells people to avoid. In April 2015, Paltrow and the Goop staff participated in a food stamp challenge in an attempt to demonstrate that her readers could abide by her dietary recommendations despite living on food stamps. Critics suggested that people on food stamps could not afford Paltrow's recommendations, and Paltrow gave up on the challenge after four days. \nA number of products sold by Goop, as well as ideas promoted in its blog posts, have been criticized for having no scientific basis, not producing the desired results, having no medical benefit, or potentially even being harmful:\n\nDespite routinely posting articles which purport to give health and nutrition advice on a wide range of topics, these articles typically end with this disclaimer:\nIn early 2018 Goop started labeling articles with disclaimers indicating whether their content is \"For Your Enjoyment\" or \"Supported by Science\".\n\nIn response, on June 29, 2018 the watchdog group Truth in Advertising sent a letter to the California Food Drug and Medical Device Task Force, saying that\n\nLabels and website descriptions of dietary supplement products required to include the standard wording for the U.S. Food and Drug Administration disclaimer: \"These statements have not been evaluated by the Food and Drug Administration (FDA). This product is not intended to diagnose, treat, cure, or prevent any disease.\" Printing this disclaimer on labels and website does not permit health claims beyond the very basic Structure:Function vocabulary regardless of how strong any clinical trail evidence is.\n\nIn 2016 Goop said it would voluntarily, permanently discontinue claims for Moon Juice ‘Brain Dust’ and ‘Action Dust’ following an inquiry from the National Advertising Division of the Council of Better Business Bureaus.\n\nOn September 4, 2018, Bloomberg News reported that: \"Goop Inc., the lifestyle company founded by Oscar-winning actress Gwyneth Paltrow, agreed to pay $145,000 to settle allegations it made unscientific claims about the benefits of three products.\" Two of the products were 'vaginal eggs' and the third was Inner Judge Flower Essence Blend. To settle this California case, Goop also agreed to refund money to customers and stop making health claims for those products. At the time of the settlement, Goop had sold around 3,000 vaginal eggs.\n"}
{"id": "17325025", "url": "https://en.wikipedia.org/wiki?curid=17325025", "title": "Great Observatories Origins Deep Survey", "text": "Great Observatories Origins Deep Survey\n\nThe Great Observatories Origins Deep Survey, or GOODS, is an astronomical survey combining deep observations from three of NASA's Great Observatories: the Hubble Space Telescope, the Spitzer Space Telescope, and the Chandra X-ray Observatory, along with data from other space-based telescopes, such as XMM Newton, and some of the world's most powerful ground-based telescopes.\n\nGOODS is intended to enable astronomers to study the formation and evolution of galaxies in the distant, early universe.\n\nThe Great Observatories Origins Deep Survey consists of optical and near-infrared imaging taken with the Advanced Camera for Surveys on the Hubble Space Telescope, the Very Large Telescope and the 4-m telescope at Kitt Peak National Observatory; infrared data from the Spitzer Space Telescope. These are added to pre-existing x-ray data from the Chandra X-ray Observatory and ESAs XMM-Newton, two fields of 10' by 16'; one centered on the Hubble Deep Field North (12h 36m 55s, +62° 14m 15s) and the other on the Chandra Deep Field South (3h 32m 30s, -27° 48m 20s). \n\nThe two GOODS fields are the most data-rich areas of the sky in terms of depth and wavelength coverage.\n\nGOODS consists of data from the following space-based observatories:\n\nGOODs used the Hubble Space Telescope's Advanced Camera for Surveys with four filters, centered at 435, 606, 775 and 850 nm. The resulting map covers 30 times the area of the Hubble Deep Field to a photometric magnitude less sensitivity, and has enough resolution to allow the study of 1 kpc-scale objects at redshifts up to 6. It also provides photometric redshifts for over 60,000 galaxies within the field, providing an excellent sample for studying bright galaxies at high redshifts.\n\nIn May 2010, scientists announced that the infrared data from the Herschel Space Observatory was joining the GOODS dataset, after initial analysis of data using Herschel's PACS and SPIRE instruments. In October 2009, Herschel observed the GOODS-North field, and in January 2010 the GOODS-South field. In so doing, Herschel identified sources for the Cosmic Infrared Background.\n\n"}
{"id": "18571333", "url": "https://en.wikipedia.org/wiki?curid=18571333", "title": "Histolysis", "text": "Histolysis\n\nHistolysis is the decay and dissolution of organic tissues or of blood. It is sometimes referred to as histodialysis. In cells, histolysis may be caused by uracil-DNA degradation.\n\nOrigin: New Latin, from Greek (histos) tissue + (lusis) dissolution from to loosen, dissolve.\n"}
{"id": "30876902", "url": "https://en.wikipedia.org/wiki?curid=30876902", "title": "Independent and identically distributed random variables", "text": "Independent and identically distributed random variables\n\nIn probability theory and statistics, a sequence or collection of random variables is independent and identically distributed (\"i.i.d.\" or \"iid\" or IID) if each random variable has the same probability distribution as the others and all are mutually independent. Identically distributed, on its own, is often abbreviated ID. For uniformity, as both are discussed—and in widespread use—this article uses the visually cleaner IID in preference to the more prevalent convention \"i.i.d.\"\n\nThe annotation IID is particularly common in statistics, where observations in a sample are often assumed to be effectively IID for the purposes of statistical inference. The assumption (or requirement) that observations be IID tends to simplify the underlying mathematics of many statistical methods (see mathematical statistics and statistical theory). However, in practical applications of statistical modeling the assumption may or may not be realistic. To test how realistic the assumption is on a given data set, the autocorrelation can be computed, lag plots drawn or turning point test performed.\nThe generalization of exchangeable random variables is often sufficient and more easily met.\n\nThe assumption is important in the classical form of the central limit theorem, which states that the probability distribution of the sum (or average) of IID variables with finite variance approaches a normal distribution.\n\nOften the IID assumption arises in the context of sequences of random variables. Then \"independent and identically distributed\" in part implies that an element in the sequence is independent of the random variables that came before it. In this way, an IID sequence is different from a Markov sequence, where the probability distribution for the \"n\"th random variable is a function of the previous random variable in the sequence (for a first order Markov sequence). An IID sequence does not imply the probabilities for all elements of the sample space or event space must be the same. For example, repeated throws of loaded dice will produce a sequence that is IID, despite the outcomes being biased.\n\nLet the random variables be defined to assume values in formula_1.\n\nWe say two random variables formula_2 and formula_3 are identically distributed \"iff\" formula_4.\n\nWe say two random variables formula_2 and formula_3 are independent \"iff\" formula_7. See .\n\nThe following are examples or applications of IID random variables:\n\n\nMany results that were first proven under the assumption that the random variables are IID have been shown to be true even under a weaker distributional assumption.\n\nThe most general notion which shares the main properties of IID variables are exchangeable random variables, introduced by Bruno de Finetti. Exchangeability means that while variables may not be independent, future ones behave like past ones – formally, any value of a finite sequence is as likely as any permutation of those values – the joint probability distribution is invariant under the symmetric group.\n\nThis provides a useful generalization – for example, sampling without replacement is not independent, but is exchangeable – and is widely used in Bayesian statistics.\n\nIn stochastic calculus, IID variables are thought of as a discrete time Lévy process: each variable gives how much one changes from one time to another.\nFor example, a sequence of Bernoulli trials is interpreted as the Bernoulli process.\nOne may generalize this to include continuous time Lévy processes, and many Lévy processes can be seen as limits of IID variables—for instance, the Wiener process is the limit of the Bernoulli process.\n\nWhite noise is a simple example of IID.\n\n"}
{"id": "2702229", "url": "https://en.wikipedia.org/wiki?curid=2702229", "title": "Index of software engineering articles", "text": "Index of software engineering articles\n\nThis is an alphabetical list of articles pertaining specifically to software engineering.\n\n2D computer graphics —\n3D computer graphics\n\nAbstract syntax tree —\nAbstraction —\nAccounting software —\nAda —\nAddressing mode —\nAgile software development —\nAlgorithm —\nAntipattern —\nApplication framework —\nApplication software —\nArtificial intelligence —\nArtificial neural network —\nASCII —\nAspect-oriented programming —\nAssembler —\nAssembly language —\nAssertion —\nAutomata theory —\nAutomotive software —\nAvionics software\n\nBackward compatibility —\nBASIC —\nBCPL —\nBerkeley Software Distribution —\nBeta test —\nBoolean logic —\nBusiness software\n\nC —\nC++ —\nC# —\nCAD —\nCanonical Model —\nCapability Maturity Model —\nCapability Maturity Model Integration —\nCOBOL —\nCode coverage —\nCohesion —\nCompilers —\nComplexity —\nComputation —\nComputational complexity theory —\nComputer —\nComputer-aided design —\nComputer-aided manufacturing —\nComputer architecture —\nComputer bug —\nComputer file —\nComputer graphics —\nComputer model —\nComputer multitasking —\nComputer programming —\nComputer science —\nComputer software —\nComputer term etymologies —\nConcurrent programming —\nConfiguration management —\nCoupling —\nCyclomatic complexity\n\nData structure —\nData-structured language —\nDatabase —\nDead code —\nDecision table —\nDeclarative programming —\nDesign pattern —\nDevelopment stage —\nDevice driver —\nDisassembler —\nDisk image —\nDomain-specific language\n\nEEPROM —\nElectronic design automation —\nEmbedded system —\nEngineering —\nEngineering model —\nEPROM —\nEven-odd rule —\nExpert system —\nExtreme programming\n\nFIFO (computing and electronics) —\nFile system —\nFilename extension —\nFinite state machine —\nFirmware —\nFormal methods —\nForth —\nFortran —\nForward compatibility —\nFunctional decomposition —\nFunctional design —\nFunctional programming\n\nGame development —\nGame programming —\nGame tester —\nGIMP Toolkit —\nGraphical user interface\n\nHierarchical database —\nHigh-level language —\nHoare logic —\nHuman–computer interaction —\nHyperlink —\nHyper-threading\n\n\"IEEE Software\" —\nImperative programming —\nInformation technology engineering —\nInformation systems —\nInformation technology —\nInstruction set —\nInteractive programming —\nInterface description language —\nIntermediate language —\nInterpreter —\nInvariant —\nISO —\nISO 9000 —\nISO 9001 —\nISO 9660 —\nISO/IEC 12207 —\nISO image —\nIterative development\n\nJava —\nJava Modeling Language —\nJava virtual machine\n\nKernel —\nKnowledge management\n\nLevel design —\nLevel designer —\nLIFO —\nLinux —\nList of programming languages —\nLiterate programming\n\nMachine code —\nMachine language —\nMainframe —\nMedical informatics —\nMedical software —\nMesh networking —\nMetadata (computing) —\nMicrocode —\nMicroprogram —\nMicrosoft Windows —\nMinicomputer —\nMIPS architecture —\nMulti-paradigm programming language\n\nNesC —\nNeural network software —\nNumerical analysis\n\nObject code —\nObject database —\nObject-oriented programming —\nOntology —\nOpcode —\nOpen implementation —\nOpen-source software —\nOperating system\n\nPacket writing —\nPair programming —\nParallax scrolling —\nPascal —\np-code machine —\nPerl —\nPHP —\nPost-object programming —\nPrivacy Engineering -\nProcedural programming —\nProcessor register —\nProgram specification —\nProgramming language —\nProgramming paradigm —\nProgramming tool —\nProject lifecycle —\nProprietary software —\nPython\n\nQt (toolkit) —\nQuery optimizer —\nQueueing theory\n\nRapid application development —\nRational Unified Process —\nReal-time operating system —\nRefactoring —\nReflection —\nRegression testing —\nRelational database —\nRelease to manufacturing —\nReliability (engineering) —\nRequirement —\nRequirements analysis —\nRevision control —\nRobotics\n\nScripting language —\nSecond-system effect —\nSignal analysis —\nSimulation —\nSoftware —\nSoftware architecture —\nSoftware bloat —\nSoftware brittleness —\nSoftware componentry —\nSoftware configuration management —\nSoftware development cycle —\nSoftware development process —\nSoftware engineering —\nSoftware framework —\nSoftware maintenance —\nSoftware metric —\nSource code —\nSource lines of code —\nSpecification language —\nSprite —\nSQL —\nStandard data model —\nSCAMPI —\nStack (data structure) —\nStatic code analysis —\nStatic single assignment form —\nStatistical package —\nString —\nStructured programming —\nStructured Query Language —\nSubroutine —\nSupercomputer —\nSystem development life cycle —\nSystems architect —\nSystems design —\nSPICE (ISO15504)\n\nTcl —\nTexture mapping —\nTheory of computation —\nThink aloud protocol —\nThread —\nThreaded code —\nThree-address code —\nTimeboxing —\nTinyOS\n\nUCSD p-System —\nUnix —\nUsability —\nUsability testing —\nUser interface\n\nVideo games —\nVirtual finite state machine —\nVisual Basic\n\nWaterfall model —\nWiki —\nWindows —\nWindows Vista\n\nXerox PARC\n\nYouTube-\n\nZ notation\n"}
{"id": "57980582", "url": "https://en.wikipedia.org/wiki?curid=57980582", "title": "John Jonides", "text": "John Jonides\n\nJohn Jonides (born December 8, 1947) is an American cognitive neuroscientist and psychologist. He is the Edward E. Smith Professor of Psychology and Neuroscience at the University of Michigan. He has been a fellow of the American Association for the Advancement of Science since 1995 and of the Society of Experimental Psychologists since 1996. He is known for his research on the malleability of human intelligence, and on the effects of Facebook use on happiness and life satisfaction. In 2011, he received the Association for Psychological Science's William James Fellow Award.\n\n"}
{"id": "31494112", "url": "https://en.wikipedia.org/wiki?curid=31494112", "title": "Kelly Jemison", "text": "Kelly Jemison\n\nKelly Jemison is an American academic geologist specializing in Antarctic diatoms. She studied at Florida State University. She has participated in the ANDRILL(ANtarctic geological DRILLing) project. In 2011, she was awarded the Antarctica Service Medal.\n\n"}
{"id": "46642517", "url": "https://en.wikipedia.org/wiki?curid=46642517", "title": "Leo Supercluster", "text": "Leo Supercluster\n\nThe Leo Supercluster is a supercluster in the Northern Celestial Hemisphere that stretches across the constellations Ursa Major and Leo. It covers an area approximately 130 megaparsecs long by 60 megaparsecs wide. The redshifts of member galaxy clusters range from 0.032 to 0.043. The brightest cluster in the system is Abell 1185.\n\n"}
{"id": "37895228", "url": "https://en.wikipedia.org/wiki?curid=37895228", "title": "List of Apodiformes by population", "text": "List of Apodiformes by population\n\nThis is a list of Apodiformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is not comprehensive, as not all Apodiformes have had their numbers quantified.\n"}
{"id": "30945264", "url": "https://en.wikipedia.org/wiki?curid=30945264", "title": "List of Copper Country mines", "text": "List of Copper Country mines\n\nMany copper mines have existed in the Copper Country of the U.S. state of Michigan. These include both large-scale commercial ventures and small operations. There are hundreds of ancient mining pits in and around the Copper Country area, especially on Isle Royale (several of these were developed). Numerous small modern diggings exist around the area as well, including some around Fort Wilkins and the Copper Harbor Light.\n\n\n\n"}
{"id": "15323406", "url": "https://en.wikipedia.org/wiki?curid=15323406", "title": "List of Sri Lankan flags", "text": "List of Sri Lankan flags\n\nThis is a list of flags used in Sri Lanka.\n\nThis flag is personal to every President of Sri Lanka as such the design changes when a new president assumes office. \n"}
{"id": "54739286", "url": "https://en.wikipedia.org/wiki?curid=54739286", "title": "List of androgens/anabolic steroids (alternate)", "text": "List of androgens/anabolic steroids (alternate)\n\nThis is a list of androgens/anabolic steroids (AAS) or testosterone derivatives. Esters are mostly not included in this list; for esters, see here instead. The major classes of testosterone derivatives include the following (as well as combinations thereof):\n\n\nThe last group consists of progestins with mostly only very weak androgenic/anabolic activity.\n\nAAS that are listed as marketed may be marketed as one or more esters rather than as the listed AAS itself.\n\nThis list specifically pertains to \"steroidal\" androgens; \"nonsteroidal\" androgens like the selective androgen receptor modulators (SARMs) andarine and enobosarm (ostarine) are not included here.\n\n\n<nowiki>*</nowiki> Only present endogenously in trace/very small amounts or present in other species.\n\n\n\n\n\n\n"}
{"id": "9894301", "url": "https://en.wikipedia.org/wiki?curid=9894301", "title": "List of computer simulation software", "text": "List of computer simulation software\n\nThe following is a list of notable computer simulation software.\n\n\n\n"}
{"id": "27640408", "url": "https://en.wikipedia.org/wiki?curid=27640408", "title": "List of sewing occupations", "text": "List of sewing occupations\n\nList of occupations requiring sewing skills.\n\n"}
{"id": "21059213", "url": "https://en.wikipedia.org/wiki?curid=21059213", "title": "List of summer schools of nature sciences", "text": "List of summer schools of nature sciences\n\nThis is a list of summer schools of nature science.\n\n"}
{"id": "1334167", "url": "https://en.wikipedia.org/wiki?curid=1334167", "title": "List of transitional fossils", "text": "List of transitional fossils\n\nThis is a tentative partial list of transitional fossils (fossil remains of groups that exhibits both \"primitive\" and derived traits). The fossils are listed in series, showing the transition from one group to another, representing significant steps in the evolution of major features in various lineages. These changes often represent major changes in morphology and anatomy, related to mode of life, like the acquisition of feathered wings for an aerial lifestyle in birds, or limbs in the fish/tetrapod transition onto land.\n\nAlmost all of the transitional forms in this list do not actually represent ancestors of any living group or other transitional forms. Darwin noted that transitional forms could be considered common ancestors, direct ancestors or collateral ancestors of living or extinct groups, but believed that finding actual common or direct ancestors linking different groups was unlikely. Collateral ancestors are relatives like cousins in genealogies in which they are not in your direct line of descent but do share a common ancestor (in this case it is a grandparent). This kind of thinking can be extended to groups of life. For instance, the well-known \"Archaeopteryx\" is a transitional form between non-avian dinosaurs and birds, but it is not the most recent common ancestor of all birds nor is it a direct ancestor of any species of bird alive today. Rather, it is considered an extinct close evolutionary \"cousin\" to the direct ancestors. This may not always be the case, though, as some fossil species are proposed to be directly ancestral to others, like how \"Australopithecus anamensis\" is most likely to be ancestral to \"Australopithecus afarensis\".\n\n\n"}
{"id": "50900397", "url": "https://en.wikipedia.org/wiki?curid=50900397", "title": "Matteo Botti", "text": "Matteo Botti\n\nMatteo Botti (ca. 1570–1621) was an Italian scientific instrument maker.\n\nBotti came from a Cremonese family on which Cosimo I de' Medici (1519–1574) conferred Florentine citizenship. He was a member of the Accademia degli Alterati and the Accademia Fiorentina. He undertook various missions abroad as an ambassador for Ferdinand I (1549–1609) and Cosimo II (1590–1621). In 1591, he was made a Knight of the Order of Saint Stephen.\n"}
{"id": "26863567", "url": "https://en.wikipedia.org/wiki?curid=26863567", "title": "Meltwater Entrepreneurial School of Technology", "text": "Meltwater Entrepreneurial School of Technology\n\nThe Meltwater Entrepreneurial School of Technology (MEST) is a Pan-African training program, seed fund and incubator in Accra, Ghana. The three-phase school and incubator program were founded in 2008 by Jørn Lyseggen to provide training, investment, and mentoring for aspiring technology entrepreneurs with the goal of creating globally successful companies that create wealth and jobs locally in Africa.\n\nSince launching in 2008, MEST backed startups have been acquired by investors, or recognized internationally by organisations such as Techstars, 500 Startups, Y Combinator and at events like the LAUNCH Conference in San Francisco.\n\nMEST was announced in 2007, when Meltwater announced plans to establish an entrepreneurial training program in Africa. The school opened in February 2008 with the first class of Entrepreneurs in Training (EITs). In February 2010, the first class of EITs graduated and the MEST Incubator was launched with seed funding for incubator companies started by the first group of EITs.\n\nIn January 2015, MEST partnered with Vodafone Ghana to sponsor a 48-hour hackathon to help develop access to information, simplify transactions and shorten business processes. In February 2015, the school was recognized as one of the top 10 most innovative companies of Africa by Fast Company. MEST announced a partnership with Samsung in April 2015. As part of the agreement, Samsung provided subsidized devices and internship positions, as well as marketing services and mobile application distribution, in exchange for the opportunity to invest in MEST companies through its Samsung Enterprise Business Partners. MEST hosted the first Africa Technology Summit in November 2015 in Accra, Ghana, the event featured panels and keynote presentations from a variety of global and African businesses.\n\nIn 2016, MEST began recruiting EITs from the French-speaking Ivory Coast and opened an additional business incubator in Lagos while planning to open an additional location in Cape Town. That same year, MEST entered a partnership with Kosmos Energy to form the Kosmos Innovation Centre to focus on creating agriculture solutions. In December 2016, MEST hosted the second Africa Tech Summit in Lagos, Nigeria, including speakers from Google, Uber, Interswitch, Vodacom and Samsung.\n\nIn September 2017, MEST named Aaron Fu as its Managing Director. The organization also announced that it would expand its presence across the continent with several new incubator spaces in additional African markets.\n\nMEST sponsors African entrepreneurs in completing a 12-month program focused on software development, business fundamentals and entrepreneurship. Top graduates from schools in Ghana, Nigeria, Kenya and South Africa Entrepreneurs-In-Training (EITs) are selected each year to receive comprehensive training in computer programming, product management, finance, marketing, sales, business leadership, and other skills required to build a successful technology business. Upon completion of the training program, the best business ideas are provided seed funding and incubated as startup companies.\n\n\n\n\n"}
{"id": "65776", "url": "https://en.wikipedia.org/wiki?curid=65776", "title": "Metcalfe's law", "text": "Metcalfe's law\n\nMetcalfe's law states the effect of a telecommunications network is proportional to the square of the number of connected users\nof the system (\"n\"). First formulated in this form by George Gilder in 1993, and attributed to Robert Metcalfe in regard to Ethernet, Metcalfe's law was originally presented, c. 1980, not in terms of users, but rather of \"compatible communicating devices\" (for example, fax machines, telephones, etc.). Only later with the globalization of the Internet did this law carry over to users and networks as its original intent was to describe Ethernet purchases and connections. The law is also very much related to economics and business management, especially with competitive companies looking to merge with one another.\n\nMetcalfe's law characterizes many of the network effects of communication technologies and networks such as the Internet, social networking and the World Wide Web. Former Chairman of the U.S. Federal Communications Commission Reed Hundt said that this law gives the most understanding to the workings of the Internet. Metcalfe's Law is related to the fact that the number of unique possible connections in a network of formula_1 nodes can be expressed mathematically as the triangular number formula_2, which is asymptotically proportional to formula_3.\n\nThe law has often been illustrated using the example of fax machines: a single fax machine is useless, but the value of every fax machine increases with the total number of fax machines in the network, because the total number of people with whom each user may send and receive documents increases. Likewise, in social networks, the greater number of users with the service, the more valuable the service becomes to the community.\n\nIn addition to the difficulty of quantifying the \"value\" of a network, the mathematical justification for Metcalfe's law measures only the \"potential\" number of contacts, i.e., the technological side of a network. However the social utility of a network depends upon the number of nodes \"in contact\". If there are language barriers or other reasons why large parts of a network are not in contact with other parts then the effect may be smaller.\n\nMetcalfe’s law assumes that the value of each node n is of equal benefit. If this is not the case, for example because the one fax machines serves 50 workers in a company, the second fax machine serves half of that, the third one third, and so on, then the relative value of an additional connection decreases. Likewise, in social networks, if users that join later use the network less than early adopters, then the benefit of each additional user may lessen, making the overall network less efficient if costs per users are fixed.\n\nWithin the context of social networks, many, including Metcalfe himself, have proposed modified models in which the value of the network grows as \"n\" × log \"n\" rather than \"n\". Reed and Odlyzko have sought out possible relationships to Metcalfe's Law in terms of describing the relationship of a network and one can read about how those are related. Tongia and Wilson also examine the related question of the costs to those excluded.\n\nDespite many arguments about Metcalfe' law, no real data based evidence for or against was available for more than 30 years. Only in July 2013, Dutch researchers managed to analyze European Internet usage patterns over a long enough time and found \"n\" proportionality for small values of \"n\" and (\"n\" × log \"n\") proportionality for large values of \"n\". A few months later, Metcalfe himself provided further proof, as he used Facebook's data over the past 10 years to show a good fit for Metcalfe's law (the model is \"n\" ).\n\nIn 2015, Zhang, Liu and Xu extend Metcalfe's results utilizing data from Tencent, China's largest social network company, and Facebook. Their work showed that Metcalfe's law held for both, despite the difference in audience between the two sites; Facebook serving a worldwide audience and Tencent serving only Chinese users. The Metcalfe's functions of the two sites given in the paper were formula_4 and formula_5 respectively. \n\n\n\n"}
{"id": "49786", "url": "https://en.wikipedia.org/wiki?curid=49786", "title": "Microclimate", "text": "Microclimate\n\nA microclimate is a local set of atmospheric conditions that differ from those in the surrounding areas, often with a slight difference but sometimes with a substantial one. The term may refer to areas as small as a few square meters or square feet (for example a garden bed or a cave) or as large as many square kilometers or square miles. Because climate is statistical, which implies spatial and temporal variation of the mean values of the describing parameters, within a region there can occur and persist over time sets of statistically distinct conditions, that is, microclimates. Microclimates can be found in most places.\n\nMicroclimates exist, for example, near bodies of water which may cool the local atmosphere, or in heavy urban areas where brick, concrete, and asphalt absorb the sun's energy, heat up, and re-radiate that heat to the ambient air: the resulting urban heat island is a kind of microclimate.\n\nAnother contributing factor of microclimate is the slope or aspect of an area. South-facing slopes in the Northern Hemisphere and north-facing slopes in the Southern Hemisphere are exposed to more direct sunlight than opposite slopes and are therefore warmer for longer periods of time, giving the slope a warmer microclimate than the areas around the slope. The lowest area of a glen may sometimes frost sooner or harder than a nearby spot uphill, because cold air sinks, a drying breeze may not reach the lowest bottom, and humidity lingers and precipitates, then freezes.\n\nThe terminology \"micro-climate\" first appeared in the 1950s in publications such as \"Climates in Miniature: A Study of Micro-Climate Environment\" (Thomas Bedford Franklin, 1955).\n\nThe area in a developed industrial park may vary greatly from a wooded park nearby, as natural flora in parks absorb light and heat in leaves that a building roof or parking lot just radiates back into the air. Advocates of solar energy argue that widespread use of solar collection can mitigate overheating of urban environments by absorbing sunlight and putting it to work instead of heating the foreign surface objects.\n\nA microclimate can offer an opportunity as a small growing region for crops that cannot thrive in the broader area; this concept is often used in permaculture practiced in northern temperate climates. Microclimates can be used to the advantage of gardeners who carefully choose and position their plants. Cities often raise the average temperature by zoning, and a sheltered position can reduce the severity of winter. Roof gardening, however, exposes plants to more extreme temperatures in both summer and winter.\n\nIn an urban area, tall buildings create their own microclimate, both by overshadowing large areas and by channeling strong winds to ground level. Wind effects around tall buildings are assessed as part of a microclimate study. (Urban Microclimate)\n\nMicroclimates can also refer to purpose-made environments, such as those in a room or other enclosure. Microclimates are commonly created and carefully maintained in museum display and storage environments. This can be done using passive methods, such as silica gel, or with active microclimate control devices.\n\nUsually, if the inland areas have a humid continental climate, the coastal areas stay much milder during winter months, in contrast to the hotter summers. This is the case further north on the American west coast, such as in British Columbia, Canada, where Vancouver has an oceanic wet winter with rare frosts, but inland areas that average several degrees warmer in summer have cold and snowy winters.\n\nThe type of soil found in an area can also affect microclimates. For example, soils heavy in clay can act like pavement, moderating the near ground temperature. On the other hand, if soil has many air pockets, then the heat could be trapped underneath the topsoil, resulting in the increased possibility of frost at ground level.\n\nTwo main parameters to define a microclimate within a certain area are temperature and humidity. A source of a drop in temperature and/or humidity can be attributed to different sources or influences.\nOften microclimate is shaped by a conglomerate of different influences and is a subject of microscale meteorology.\n\nThe well known examples of cold air pool (CAP) effect are\nGstettneralm Sinkhole in Austria (lowest recorded temperature )\nThe main criterion on the wind speed formula_1 in order to create a warm air flow penetration into a CAP is the following:\n\nwhere formula_3 is the Froude number, formula_4 --- the Brunt–Väisälä frequency, formula_5 --- depth of the valley, and formula_6 --- Froude number at the threshold wind speed.\n\nThe presence of permafrost close to the surface in a crater creates a unique microclimate environment.\n\nAs similar as lava tubes can be to caves which are not formed due to volcanic activity the microclimate within the former is different due to dominant presence of basalt.\nLava tubes and basaltic caves are important astrobiological targets on Earth and Mars (see also Martian lava tube).\n\nAs pointed out by Rudolf Geiger in his book not only climate influences the living plant but the opposite effect of the interaction of plants on their environment can also take place, and is known as \"plant climate\".\n\nArtificial reservoirs as well as natural ones create microclimates and often influence the macroscopic climate as well.\n\n\n\n\n\n"}
{"id": "51624378", "url": "https://en.wikipedia.org/wiki?curid=51624378", "title": "Mixed conductor", "text": "Mixed conductor\n\nMixed conductor which is known as mixed ion-electron conductor (MIEC) refers to a single phase material which has a significant conduction ionically and electronically. Due to the mixed conduction, a formally neutral species can transport in a solid and therefore mass storage and redistribution are enabled. Mixed conductors are well known in conjugation with High-temperature superconductivity and are able to capacitate rapid solid-state reactions. They are used as catalysts (for oxidation), permeation membranes, sensors and electrode in batteries since they allow for rapidly transducing chemical signals and permeating chemical components. Strontium titanate (SrTiO), Titanium oxide (TiO), (La,Ba, Sr)(Mn,Fe,Co)O, La2CuO, Cerium(IV) oxide (CeO), Lithium iron phosphate (LiFePO) and LiMnPO are the examples for the mixed conductors.\n\n"}
{"id": "8365274", "url": "https://en.wikipedia.org/wiki?curid=8365274", "title": "O-ring theory of economic development", "text": "O-ring theory of economic development\n\nThe O-ring theory of economic development is a model of economic development put forward by Michael Kremer in 1993, which proposes that tasks of production must be executed proficiently together in order for any of them to be of high value. The key feature of this model is positive assortative matching, whereby people with similar skill levels work together.\n\nThe name comes from the 1986 Challenger shuttle disaster, a catastrophe caused by the failure of a single O-ring.\n\nKremer thinks that the O-ring development theory explains why rich countries produce more complicated products, have larger firms and much higher worker productivity than poor countries.\n\nThere are five major assumptions of this model: firms are risk-neutral, labor markets are competitive, workers supply labor inelastically, workers are imperfect substitutes for one another, and there is a sufficient complementarity of tasks.\n\nProduction is broken down into \"n\" tasks. Laborers can use a multitude of techniques of varying efficiency to carry out these tasks depending on their skill. Skill is denoted by \"q\", where 0≤\"q\"≤1. The concept of \"q\" differs depending on interpretation. It could mean: the probability of a laborer successfully completing a task, the quality of task completion expressed as a percentage, or the quality of task completion with the condition of a margin of error that could reduce quality. Output is determined by multiplying the \"q\" values of each of the \"n\" tasks together and then multiplying this result by another term (lets say, \"B\") denoting the individual characteristics of the firm. \"B\" is positively correlated with the number of tasks. The production function here is simple:\n\nThe important implication of this production function is positive assortative matching. We can observe this through a hypothetical four-person economy with two low skill workers () and two high skill workers (). This equation dictates the productive efficiency of skill matching:\n\nBy this equation total product is maximized by pairing those with similar skill levels.\n\nThere are several implications one can derive from this model:\n\n\nThis model helps explain brain drain and international economic disparity. As Kremer puts it, \"If strategic complementarity is sufficiently strong, microeconomically identical nations or groups within nations could settle into equilibria with different levels of human capital\".\n\nGarett Jones (2013) builds upon Kremer's O-ring theory to explain why differences in worker skills are associated with \"massive\" differences in international productivity levels despite causing only modest differences in wages within a country. For this purpose, he distinguishes between O-ring jobs - jobs featuring high strategic complementarities in terms of skill - and foolproof jobs - jobs characterized by diminishing returns to labor - and assumes both production technologies to be available to all countries. He then goes on to show that small international variations in average worker skill per country result in both large international and small intra-national income inequality.\n"}
{"id": "8858097", "url": "https://en.wikipedia.org/wiki?curid=8858097", "title": "Pitsford Hall weather station", "text": "Pitsford Hall weather station\n\nPitsford Weather Centre, formerly Pitsford Hall weather station, is an independent climatological station maintained by Pitsford School in the village of Pitsford, Northamptonshire. The centre was established in 1998 and issues local forecasts for the county as well as maintaining a detailed and continuous weather record. The weather centre is a regular contributor to weather-related articles in the local press and has featured on local and national TV and radio. An analysis of each month's weather as well special articles are published in the centre's Monthly Weather report received by the British Library and available online and in hard copy to subscribers. The weather station was ran by Sixth Form students using traditional meteorological instruments from its inception in 1998 until 2016 when the site became fully automatic and renamed Pitsford Weather Centre. Weather information is now available via Twitter and Facebook as well as smartphone apps for iPhone and Android. \n\nAn extensive archive of county weather records is held by the station which date back to 1880 and the station continues to receive weather records from a number of sites from across the county.\n\nThe weather centre's most popular feature remains its Daily Weather Report, a comprehensive 3 day forecast for Northamptonshire, distributed by free subscription email. \n\nThe station enjoys the patronage of broadcast meteorologists Michael Fish MBE (now retired) and Alex Deakin. It is a corporate member of the Royal Meteorological Society.\n"}
{"id": "3134466", "url": "https://en.wikipedia.org/wiki?curid=3134466", "title": "Polarizable vacuum", "text": "Polarizable vacuum\n\nGravitation can be described via a \"scalar theory of gravitation\", using a \"stratified conformally flat metric\", in which the field equation arises from the notion that the vacuum behaves like an optical polarizable medium.\nIt was proposed by R. H. Dicke (1957) and then H. E. Puthoff (1998).\nHarold Puthoff (see also Bernard Haisch and SED)\nIn theoretical physics, polarizable vacuum (PV) and its associated theory refers to proposals by Harold Puthoff, Robert H. Dicke, and others to develop an analogue of general relativity to describe gravity and its relationship to electromagnetism.\n\nIn essence, Dicke and Puthoff proposed that the presence of mass alters the electric permittivity and the magnetic permeability of flat spacetime, ε and μ respectively by multiplying them by a scalar function, K:\nε→ε = Kε, μ→μ = Kμ\narguing that this will affect the lengths of rulers made of ordinary matter, so that in the presence of a gravitational field the spacetime metric of Minkowski spacetime is replaced by\nwhere formula_2 is the so-called \"dielectric constant of the vacuum\". This is a \"diagonal\" metric given in terms of a Cartesian chart and having the same \"stratified conformally flat\" form in the Watt-Misner theory of gravitation. However, according to Dicke and Puthoff, κ must satisfy a field equation which differs from the field equation of the Watt-Misner theory. In the case of a static spherically symmetric vacuum, this yields the asymptotically flat solution\nThe resulting Lorentzian spacetime happens to agree with the analogous solution in the Watt-Misner theory, and it has the same weak-field limit, and the same far-field, as the Schwarzschild vacuum solution in general relativity, and it satisfies three of the four classical tests of relativistic gravitation (redshift, deflection of light, precession of the perihelion of Mercury) to within the limit of observational accuracy. However, as shown by Ibison (2003), it yields a different prediction for the inspiral of test particles due to gravitational radiation.\n\nHowever, requiring stratified-conformally flat metrics rules out the possibility of recovering the weak-field Kerr metric, and is certainly inconsistent with the claim that PV can give a \"general\" \"approximation\" of the general theory of relativity. In particular, this theory exhibits no frame-dragging effects. Also, the effect of gravitational radiation on test particles differs profoundly between scalar theories and tensor theories of gravitation such as general relativity. LIGO is not intended primarily as a test ruling out scalar theories, but is widely expected to do so as a side benefit once it detects unambiguous gravitational wave signals exhibiting the characteristics expected in general relativity.\n\nIbison has considered a \"cosmological solution\" of PV, analogous to the Friedmann dust solution with flat orthogonal hyperslices in general relativity, and argues that this model is inconsistent with various observational and theoretical constraints. He also finds a rate of inspiral disagreeing with observation. The latter result disagrees with that of Watt and Misner, whose Lorentzian manifold differs from PV in the case of cosmology.\n\nIt is widely accepted that no scalar theory of gravitation can reproduce all of general relativity's successes, contrary to Puthoff's claims. It might be noted that De Felice uses constitutive relations to obtain a \"susceptibility tensor\" which lives in spatial hyperslices; this provides extra degrees of freedom which help make up for the degree of freedom lacking in PV and other scalar theories.\n\nIn 2005 Depp proposed a modification to the original PV work of Dicke that addresses many of the objections above. The revised model is not a replacement for general relativity but is meant to provide insight into the possible underlying physics of general relativity.\n\nDepp presents a PV Lagrangian field from which can be derived the exact Schwarzschild solution and the exact Reissner-Nordstrom solution. In addition it is also shown that electron charge renormalization can be obtained from the model that is in good agreement with QED.\nDesiato argues that Depp's modification was actually a correction that consistently uses the refractive index c/K throughout. Where, the original formulations by Dicke and Puthoff were inconsistent, using c^4/K or c^4/K^2 in the leading term of the Lagrangian, rather than Depp's (c/K)^4. PV and GR are now on the same experimental footing because they produce identical metric predictions. Illustrating that PV is just an alternative interpretation of the same phenomenon described by GR, and is well suited for engineering purposes.\n\nDesiato recently published an article in JBIS, using the Modified PV Model to derive an alternative to Alcubierre's warp drive equation, and offers a different interpretation of the Negative Energy Density required to achieve this goal.\n\nPuthoff himself has apparently offered various characterizations of his proposal, which has been variously characterized as\n\nPV has origins in more mainstream work by such physicists as Robert Dicke, but in current parlance the term does appear to be most closely associated with the speculations of Puthoff. The claims have not been accepted in mainstream physics.\n\nMainstream physicists agree that PV is \n\nAntecedents of PV and more recent related proposals include the following:\n\n\n\n"}
{"id": "52017312", "url": "https://en.wikipedia.org/wiki?curid=52017312", "title": "Poper Scientific Stand up", "text": "Poper Scientific Stand up\n\nPoper Scientific Stand Up is the first Latin American stand-up comedy group that is engaged in the popularization of science. It is an independent continuation of an early 2015 initiative by Diego Golombek, which was made in conjunction with the Ministry of Science, Technology and Innovation, Tecnopolis and TECtv. The proposal was to invite university students and graduates of different science fields to participate in a Scientific Stand Up comedy course given by renowned humorist Diego Wainstein, for them to create monologues with scientific content. Using the Stand Up format as a resource, they speak about scientific topics to all age groups, whether they have previous knowledge or not, and they also talk about scientists as people and about the perception of science in society. The name \"Poper\" is a pun on the initials of \"Popularización Entre Risas\" (Popularizing Among Laughter), and a tribute to Epistemologist Karl Popper.\n\nIn early 2015, the Argentine Ministry of Science, Technology and Innovation, alongside with Tecnopolis and TECtv, organized the first Argentine course of Scientific Stand Up, in order to provide innovative teaching tools for the popularization of science. About 200 people enrolled in the course, and 30 were selected to take part in it, chosen for their scientific, artistic and general backgrounds. Those selected were undergraduates, graduates and researchers who had developed different activities in several scientific institutions in Argentina. The course content included several stand-up techniques such as character building, creating stand-up routines, jokes building, working with props and microphones, vocal work, body language, and communication with the public. During the course, each participant produced monologues that were later exhibited in Tecnopolis, under the name \"Humor Científico,\" in 18 shows. They were well received by a diverse audience, which was composed mainly by people outside scientific fields, reaching around 900 spectators a show. After the completion of this course, some of its graduates formed Poper Scientific Stand Up.\n\nCurrently Poper consists of 18 members (9 women and 9 men). They range from undergraduates, graduates and researchers, and they cover various areas of knowledge.\n\nAramburu, Roxana\n\nShe graduated in Biology and PhD in Natural Sciences from the National University of La Plata. She is head of practical work and adscripta researcher at the Faculty of Natural Sciences (UNLP). She has published her research, mainly in bird ecology in numerous local and foreign scientific journals. She trained in acting and playwriting in La Plata and Buenos Aires. As a playwright, it has more than thirty plays of her own. She received 16 distinctions and awards from national and foreign institutions for their theatrical texts. In 2014 she was elected to Authors Argentinas cycle, the Teatro Nacional Cervantes.\n\nChiaramoni, Nadia\n\nHe graduated in Biotechnology National University of Quilmes and a PhD in Basic and Applied Sciences. He is currently a research assistant CONICET. Study drug transport systems. He is a university professor and directs fellows, postgraduate students and seminarians. He has numerous publications in international journals. He made different courses Stand-Up and script workshop. He participated as a comedian Stand up-Eros (Complejo La Plaza) and as a producer and actor in \"A Touch of Evil\" in Absinth, Comedy Club. He currently presents a weekly in-person \"Positive Mind\" in Absinth, La Casa de la Comedia.\n\nCorapi, Enrique\n\nIt is Lic. in Science. Biological, Master in Biomedical Sciences and is currently doing his PhD in functional glycomics applied to Cancer, in the Faculty of Natural Sciences of the University of Buenos Aires a scholarship of CONICET. He worked on various television programs Production and recreational events performance in Science \"Luigi and brain; technical advisor, production in the Doctors program Telefe and technical advisor, production and performance in the program \"La Nave de Marley\" TELEFE. It also participated in numerous activities of university extension in FCEN, UBA as the Night of the Museums and Chemistry Week.\n\nCorapi, Luis\n\nHe graduated in Chemistry. He studied Food Engineering and works in water purification in aysa. He worked on various television programs such as production and performance in recreational events Science \"Luigi and brain; technical advisor, production in the Doctors program TELEFE and technical advisor, production and performance in the program \"La Nave de Marley\" Telefe.\nDe Almeida, Julian\n\nHe graduated in Biology from the University of Buenos Aires and a PhD in Neurosciences received at the University of Barcelona. He has several publications in international journals. He is the author of the book Part of Existence, a biologist for Latin America. He was interviewed on various radio programs as an author and also has many publications on-line \"Part of existence.\"\n\nFarina, Martin Ezequiel\n\nHe is a student of paleontology in University of Buenos Aires. He participated in many outreach projects such as in Tecnopolis and Argentine Museum of Natural Sciences. It gives talks in schools on the Integral Nature Reserve and Laguna de Rocha Mixed. He studied theater since he was 10 years and participated in several productions as an actor, producer and assistant director. Also he participated in the development and production of various TV programs as \"El Rastro Fossil\" and \"Museodinámica\" and was co-director and scriptwriter along with Andrés Pujol and Daniel Sanchez of \"MonteGrandeando\"; short historical disclosure about the city of Monte Grande. Radio columnist was \"Cancels Siesta\" program, among others.\n\nFernandez Piana, Lucas\n\nHe holds a degree in Mathematics from the University of Buenos Aires. He is currently a Fellow of CONICET pursuing a PhD in UBA. His area of interest is the statistical models specifically robust regression on functional data. He is a university professor and studied theater, improvisation, acrobatics and aerial floor, circus and clown. Currently a member of the unpronounceable group dedicated to improvisational theater.\n\nGarcia de Souza, Javier\n\nHe graduated in Biology and Doctor of Natural Sciences National University of La Plata. Currently is a Research Assistant CONICET works in aquatic ecology and organic aquaculture. He published scientific articles in national and international journals, technical reports and transfer jobs linked to organic aquaculture. He teaches, extension and scientist. He made several theater courses, physical training for the actor and contemporary dance. I was cast 4 15 plays and dance theater. Currently it integrates two dance theater companies (Espiardanza and La pecueca) and directs two works registered in his name.\n\nGilles, Facundo\n\nHe graduated in Physics at the University of Buenos Aires. Currently is finishing his PhD on the study of solid-state nanochannels modified with polyelectrolyte monolayers, with a PhD scholarship given by CONICET. He has published his work in international journals. He is interested in computational modelling at the interface between physics and chemistry, spread of science through non-conventional ways and teaching science. He is working as teacher assistant at the National University of La Plata. and performed various tasks of popularization of science through his participation in the famous Argentinian park of science: Tecnópolis. Other known abilities: folklore and salsa dancer.\n\nHoijemberg, Mauro\nHe is a student of Biology and Technical Garden of the University of Buenos Aires. He held a workshop in improvisational theater. He participated in the production and performance of Parripollo TV and PEUHEC-UBA (university extension program in school and community gardens). He teaches chemistry at the secondary level and was professor of educational workshop for children garden and urban agriculture workshop.\n\nKristoff, Gisela\n\nShe is Biochemistry and a PhD in chemistry from the University of Buenos Aires. He is Research Associate at CONICET. Directs his research group: Ecotoxicology Acuática- native invertebrates. He is a university professor and director of research, postgraduate students graduate, doctoral and master. He has numerous publications in international journals and popular articles. He is a member of C. Board of the Society of Environmental Toxicology and Chemistry. He worked in university extension leading a project: Science Re-creative, at book fairs, gardens, Chemical Week and Night of Museums. He dramaturgy and camera courses and studied theater since 2009. He performed in 8 plays and 3 short.\n\nMarcias, Maria Laura\n\nShe graduated in Cs. Biological, Aquatic Biology orientation and is pursuing the Specialization in Public Communication of Science and Technology at the University of Buenos Aires. It is diver and dance Hip Hop. He participated in various tasks extension and popularization of science: in Tecnopolis, Museum Night, Book Fair and the Week of Biology (UBA). It is volunteer Whale Conservation Institute and participated for 7 years in writing articles for boys disclosure on the conservation of whales and the environment (List Franca Junior).\n\nOlivieri, Vanesa\n\nShe graduated in Biology, University of Buenos Aires, environmentalist. He made various stand-up courses and participated as a comedian and producer in fixed shows as \"sharp\" and \"From to 2\". He was a finalist in National contest the first Stand Up and participated in the Festival Emergent City. He also courses improvisation and comedy.\nOtero, Laura\n\nWith a degree in Biotechnology and Molecular Genetics orientation technique graduated Laboratory National University of Quilmes. He has teaching experience, he participated in various research projects and has experience in university teaching. He made various courses Stand Up and humorous writing workshop. He also appeared in different places, having a weekly show.\n\nRodriguez, Victoria\n\nHe is a student of biology at the University of Buenos Aires and Performing Arts. He participated in various tasks of university extension and outreach such as being guide Tecnopolis.\n\nSaint Esteven, Alejandro\n\nHe graduated in Biology with a concentration in Animal Morphology and Systematics graduated from the University of Buenos Aires. He is currently doing a PhD with a scholarship UBA. He participated in various university extension tasks such as being educational guide at the Argentine Museum, instructor Argentine sign language, Biology Week (FCEN), Night of the Museums. As an actor I had regular presentations at the fair of science and technology Tecnopolis and the Tunnel of Sciences.\n\nSaponara, Juliana\n\nShe graduated in Astronomy graduate of the National University of La Plata. He is currently doing a doctorate in astronomy with a scholarship of CONICET. He participated in various extension work and the Expo-University astronomy workshop for kids, talk outreach Planetarium and worked at the Extension Department of Astronomy and Geophysics. He made various courses of theater, clown, mime and improvisation and had different artistic presentations.\n\nSganga, Daniela\n\nShe graduated in Biology, graduated from the University of Buenos Aires, focusing on systematic animal and morphology. He is currently a fellow of CONICET and doctoral student at the UBA. His research topic is the reproduction of crustaceans. He made several courses of theater, circus and gymnastics. He served as a teacher in secondary and university level in various subjects. He participated in various activities to popularize science, as the week of Biology, organized by the Faculty of Natural Sciences (UBA) and Night of the Museums and formed part of the area biology in multidisciplinary projects with the Speleological Group Argentino .\n\nIn May 2016 begins in Tandil (Buenos Aires) national tour that took them to travel 10 cities in 7 provinces throughout Argentina Republic along with a presentation in the city of Montevideo:\n\nPoper is characterized by a wide range of audience reception in both scientific fields and which are totally unrelated to the subject. So much so that it has regularly presented in such different spaces between them as comedy clubs or scientific congresses, through secondary schools, Weddings and other private events. The group not only offers shows, but also provides workshops Scientific Communication. Among its outstanding presentations can mention the 2015 season Tecnopolis, the weekly show Absinth, the House of Comedy and regular show at the Cultural Center of Science:\n\nIn October 2016 the signal TECtv announced it is working on a TV show with the comedian Poper group and Dalia Gutmann. It was filmed in the Crash Hall of the City of Buenos Aires, the Polo Scientific Technology and Avenida Corrientes.\n\n\nIn December 2015 they received the \"I love what you do\" that delivers the renowned broadcaster Carlos Ulanovsky Award of LRA Radio Nacional\n\n"}
{"id": "6413640", "url": "https://en.wikipedia.org/wiki?curid=6413640", "title": "Qualitative Social Work", "text": "Qualitative Social Work\n\nQualitative Social Work is a peer-reviewed academic journal that publishes papers four times a year in the field of Social Work. The journal's founding editors were Roy Ruckdeschel (Saint Louis University) and Ian Shaw (University of York). The current co-editors are Karen Staller (University of Michigan) and Lisa Morriss (University of Birmingham). The journal has been in publication since 2002 and is currently published by SAGE Publications.\n\n\"Qualitative Social Work\" is primarily aimed at those interested in qualitative research and evaluation and in qualitative approaches to practice. The journal provides a forum for debate on the nature of reflective inquiry and practice, emerging applications of critical realism in social work, the potential of social constructionist and narrative approaches to research and practice.\n\n\"Qualitative Social Work\" is abstracted and indexed in the following databases:\n\n"}
{"id": "13921852", "url": "https://en.wikipedia.org/wiki?curid=13921852", "title": "Redstone Test Stand", "text": "Redstone Test Stand\n\nThe Redstone Test Stand or Interim Test Stand was used to develop and test fire the \nRedstone missile, Jupiter-C sounding rocket, Juno I launch vehicle and Mercury-Redstone launch vehicle. It was declared an Alabama Historic Civil Engineering Landmark in 1979 and a National Historic Landmark in 1985. It is located at NASA's George C. Marshall Space Flight Center (MSFC) in Huntsville, Alabama on the Redstone Arsenal, designated Building 4665. The Redstone missile was the first missile to detonate a nuclear weapon. Jupiter-C launched to test components for the Jupiter missile. Juno I put the first American satellite Explorer 1 into orbit. Mercury Redstone carried the first American astronaut Alan Shepard into space. The Redstone earned the name \"Old Reliable\" because of this facility and the improvements it made possible.\n\nThe Interim Test Stand was built in 1953 by Dr. Wernher von Braun's team for a mere $25,000 out of materials salvaged from the Redstone Arsenal. In 1957 the permanent test facility called the Static Test Tower was finally finished, but the Army decided to continue operations at the Interim Test Stand rather than move. From 1953 to 1961, 362 static rocket tests were conducted there, including 200 that led directly to improvements in the Redstone rocket for the Mercury manned flight program. Adapted over the years, it never experienced the growth in size and cost that typified test stands in general, remaining a testament to the engineering ingenuity of the rocket pioneers.\n\nLiquid-propellant rocket development has always proceeded in three steps: \nFirst, prototype engines are tested in a Rocket engine test facility, where the most promising designs are refined during a period of extensive testing. After an engine has been proven, the complete rocket is assembled. In this second step, the rocket is anchored to a static test stand. With the rocket held down, engineers run the engine at full power and refine the system. The test launch is third, when the missile is fired into the sky. Wernher von Braun and his team used this process to develop the V-2 or A4 missile in Germany during WWII.\n\nVon Braun and members of his team decided to surrender to the United States military to ensure they were not captured by the advancing Soviets or shot by the Nazis to prevent their capture. They came to the United States via Operation Paperclip. The Army first assigned the Germans to teach German missile technology, assist with the launching of captured V-2's, and continue rocket research as part of the Hermes project at Fort Bliss, Texas and White Sands Proving Grounds\n\nOn April 15, 1950, the Army consolidated their far-flung guided missile and rocket research and development efforts into the Ordnance Guided Missile Center (OGMC) at Redstone Arsenal. The Army bought the former WWII munitions facility from the Army Chemical Corps. That summer and fall, members of the German rocket team moved from Fort Bliss to Huntsville. They conducted a preliminary study for proposed range missiles and began developing one, called Hermes C-1. The study envisioned warhead payloads of , with the first test launch in 20 months. Cold War tensions escalated by the Korean War drove the payload up to a atomic bomb with a reduced range. The system with its new specifications took the name Redstone, and had to be highly reliable, accurate, and quickly produced, priority 1A. The development program for the Redstone began in earnest on May 1, 1951. Separate from the missile development program, another budget line item was to bear the cost of constructing facilities for research and development at Redstone Arsenal because those facilities could also be used for other projects. However, the construction of facilities was not funded.\n\nThe first twelve missiles were built at Redstone Arsenal. Assembly of the first Redstone began in the fall of 1952. Engineers needed a propulsion test stand to improve the missile, but they were not allowed to spend research and development funds on constructing facilities even for a cause vital to national security. Rather than wait for funding to go through the two-year Congressional appropriation process, then wait further for construction, Fritz A. Vandersee designed an interim test stand for $25,000, the maximum amount allowed. The large concrete foundation cost nearly all of the money. On this base, welders built a small stand with metal salvaged from around the arsenal. Three railroad tank cars that had been used to transport chemicals at the arsenal during the war were cleaned, modified, and buried away to serve as control and observation bunkers. To view the firings, the tanks also contain two periscopes believed to have been from two surplus Army tanks.\n\nWhen workers assembled the first Redstone missile at Redstone Arsenal in spring of 1953, the Redstone Interim Test Stand stood ready. A crane hoisted the missile (without the warhead) onto the stand and placed a frame atop the missile. Cables were attached to the frame to steady the missile. After extensive tests, workers fueled the missile and fired the engine for tests lasting no more than 15 seconds. After several successful test runs, the missile went to Cape Canaveral Air Force Station for the test flight. Launches provided valuable information on the guidance system, but most improvements on the propulsion system came from lessons learned at the Interim Test Stand, where engineers could evaluate the internal workings of the propulsion system while it was firmly anchored to the ground. A total of fourteen tests were performed with the first four missiles.\nBefore congressional appropriation and construction of permanent facilities was completed, the engineers used the information gained during static testing at the Interim Stand to steadily improve the Redstone system. The next eight missiles stood for twenty-two tests. The tower to the left of the missile (shown ) is the Cold Calibration Unit, built in 1954. It held only the Redstone's alcohol and liquid oxygen tanks, pumps, valves and flow meters in various configurations. The liquids flowed into another set of tanks and were used to test and calibrate the valves and flow meters to assure that accurate measurements were made during the static fire testing and to assure a proper alcohol to oxygen mixture ratio. Oxygen-rich propellant mixtures had caused most engine explosions in the early years of liquid rocket development.\n\nIn the original version of the facility, flames were directed in a trench beneath the rocket in two opposite directions. In December 1955, workers installed a new more durable elbow-shaped flame deflector designed by Rocketdyne engineer Carl Kassner. Water injected through small holes in the elbow quickly turned to steam, keeping the flame away from the metal elbow.\n\nThe Army Ballistic Missile Agency (ABMA) was established on February 1, 1956 to turn the experimental Redstone rocket into an operational weapon and to develop a new Jupiter Intermediate Range Ballistic Missile (IRBM).\nThe Redstone missile development continued with routine missile qualification tests and several improvements were made to the Interim Test Stand. A load cell was added to directly measure the thrust of the missile. A cutoff system was added to detect rough combustion in the engine and automatically stop tests. This system prevented engine damage while engineers solved the problem. The first Redstone built by Chrysler was tested at the Interim Stand. Chrysler built thirty-eight developmental Redstone missiles and all sixty-three tactical Redstones in Detroit. In addition, several of the Redstone missiles were modified to aid the Jupiter missile development program. These longer missiles were called Jupiter-C and test fired on the Interim Stand after it was enlarged and strengthened. A series of tests using propellants chilled to established that the Redstone could be deployed in the Arctic.\n\nIn 1957 the permanent Propulsion and Structural Test Facility was finally completed using the funds appropriated by Congress for the Redstone, but the ABMA decided to continue using the Interim Test Stand for the Redstone. After four years of development, the interim facilities had proven adequate for testing the Redstone and Jupiter-C, and the Army felt that a move to the new facilities would be disruptive to its busy schedule.\nDr. von Braun had proposed to Project Orbiter using a Redstone as the main booster for launching artificial satellites on June 25, 1954. The day Sputnik 1 launched, October 4, 1957, von Braun had been showing incoming Defense Secretary Neil McElroy around the Redstone Arsenal. They received the news about Sputnik as they relaxed that afternoon. Von Braun turned to McElroy. \"We could have been in orbit a year ago,\" he said. \"We knew they [the Soviets] were going to do it! Vanguard will never make it. We have the hardware on the shelf… We can put up a satellite in 60 days.\" McElroy was not confirmed until the next week and did not have the power to back their proposal. On November 8, McElroy directed the Army to modify two Jupiter-C missiles and to place a satellite in orbit by March 1958.\nThe first stage was soon test fired.\nEighty-four days later, on January 31, 1958, the ABMA launched the first US satellite, Explorer I, into orbit. Following this successful launch, five more of these modified Jupiter-C missiles (subsequently re-designated Juno I) were launched in attempts to place additional Explorer satellites in orbit. During this satellite program, the Department of the Army gathered a great deal of knowledge about space. Explorer I gathered and transmitted data on atmospheric densities and the earth's oblateness. It is primarily remembered, though, as the discoverer of the Van Allen cosmic radiation belt.\n\nBecause of its proven reliability and accuracy, the Department of Defense decided to use the Redstone missile in tests to study the effects of nuclear detonations in the upper atmosphere, Operation Hardtack I. After being static-fired at the Interim Stand in January 1958, two missiles were shipped to the Pacific Test Range. In July and August, the missiles became the first missiles ever to detonate atomic warheads.\n\nIn 1958, Redstone development ended and Chrysler began mass production for deployment. Only a few of these missiles were tested at the Interim Test Stand because the propulsion system had become so reliable.\n\nAs the space race continued, the civilian space agency, NASA started on October 1, 1958 but the Army kept von Braun and the ABMA for another year and a half. NASA's Project Mercury chose the \"Old Reliable\" Redstone, with its unmatched launch record, as America's first manned launch vehicle. Nevertheless, the Army had to make improvements for manned missions. The crew at the Interim Test Stand ran over 200 static firings to improve the Redstone propulsion system. In addition, all eight Mercury-Redstone launch vehicles endured a full duration acceptance test at the interim stand. On July 1, 1960, 4,670 people transferred from the ABMA to NASA forming the Marshall Space Flight Center (MSFC).\n\nThe first test flight, Mercury-Redstone 1, occurred on November 21, 1960. After rising a few inches off the launch pad, electrical cables disconnecting in the wrong order caused the engine to shut down. The launch vehicle sustained minor damage and was returned to MSFC. Static fire testing on the Redstone Test Stand in February 1961 verified that repairs were successful. The second test launch, Mercury-Redstone 1A, was successful on December 19, 1960. On January 31, 1961, a chimpanzee named Ham flew into space on Mercury-Redstone 2. Another test flight, Mercury-Redstone BD, added to evaluate changes, confirmed the system was ready.\n\nAlan B. Shepard, Jr. became the first American in space on May 5, 1961. Mercury-Redstone 3 was a suborbital flight to an altitude of 115 miles and a range of 302 miles. This flight demonstrated that man was capable of controlling a space vehicle during periods of weightlessness and high accelerations. The last Mercury-Redstone flight, Mercury-Redstone 4,\nalso a manned suborbital flight, carried Virgil I. Grissom to a peak altitude of 118 miles and safely landed him 303 miles downrange. The Redstone Test Stand contributed to the success of the first two Americans to fly in space.\n\nThe Redstone Test Stand phased out of use in October, 1961. \nAfter becoming rundown and littered, the site was restored for the US Bicentennial. A Redstone missile, which US Army Missile Command (MICOM) loaned to NASA, was installed on February 27, 1976.\nIt was listed on the National Register of Historic Places as being nationally significant on May 13, 1976.\nAlabama Section, American Society of Civil Engineers declared it an Alabama Historic Civil Engineering Landmark in 1979.\nIt was declared a National Historic Landmark on October 3, 1985.\nThe Interim Test Stand is in good condition.\n\n\n\n"}
{"id": "34382650", "url": "https://en.wikipedia.org/wiki?curid=34382650", "title": "Sarah Salmond", "text": "Sarah Salmond\n\nSarah Salmond (7 August 1864 in Abbey St Bathans, Berwickshire, Scotland – 18 October 1956 in Dunedin, New Zealand) was a notable New Zealand governess and astronomer.\n"}
{"id": "14589447", "url": "https://en.wikipedia.org/wiki?curid=14589447", "title": "Soku hi", "text": "Soku hi\n\nSoku-hi () means \"is and is not\". The term is primarily used by the representatives of the Kyoto School of Eastern philosophy.\n\nThe logic of soku-hi or \"is and is not\" represents a balanced logic of symbolization reflecting sensitivity to the mutual determination of universality and particularity in nature, and a corresponding emphasis on nonattachment to linguistic predicates and subjects as representations of the real.\n\n\n"}
{"id": "39793951", "url": "https://en.wikipedia.org/wiki?curid=39793951", "title": "Sport pedagogy", "text": "Sport pedagogy\n\nSport Pedagogy is the academic field of study, which is located at the intersection between sport and education. As a discipline, sport pedagogy is concerned with learning, teaching and instruction in sport, physical education and related areas of physical activity. Whilst sport pedagogy is mostly regarded as a sub-discipline of sport science (in North America frequently referred to as kinesiology), its theoretical grounding is also underpinned by the general education sciences. As a scientific subdiscipline sport pedagogy is therefore allied to both fields, sport science and education.\n\nIn its original meaning the word pedagogy, and therefore also sport pedagogy, relates to the purposeful development of children and young people. The word pedagogy derives from the Greek (pais=’child’; agogein=’to lead’; ‘to instruct’ and implies the purposeful art of leading, educating or teaching young people. In recognition of the fact that humans are lifelong learners and also potentially engaged in lifelong physical activity, current definitions of sport pedagogy favour a broader view of the context in which sport pedagogy is located. To reflect this, the term sport pedagogy is used more holistically to include pedagogies that relate to adult learning and participation in sport and physical activity across all age ranges.\n\nHistorically, the roots of sport pedagogy as an academic subdiscipline of sport science can be traced back to the systematic study of physical education as a subject. As an academic discipline sport pedagogy was first explicitly recognised during the late 1960s in continental Europe, where the discipline was seen to provide a theoretical framework for the planning and teaching of physical education in schools. In Germany for instance, the publication of Ommo Grupe’s influential book ‘Grundlagen der Sportpädagogik’ (Foundations of Sport Pedagogy) provided a key moment in defining the concepts and content of sport pedagogy as an academic subject. It also provided the impetus for further research in this field and by the end of the 1970s, professorships in sport pedagogy were well established in sport and exercise science departments throughout Germany Universities.\n\nIn the English speaking world, the recognition of sport pedagogy as a discipline is more recent. In 1989, the notable German researcher Herbert Haag observed that the meaning of the term 'sport pedagogy' was not as yet fully established in the English speaking sport science literature. Nevertheless, Haag confirmed the ascendence and usefulness of the term 'sport pedagogy' to communicate about research in learning and teaching in physical education and sport to international academic audiences in this field. The relatively late adoption of the term 'sport pedagogy' in the English speaking academic literature is also observed by the distinguished Australian researcher Richard Tinning (2008, p. 405) who notes that ‘notwithstanding the fact that our European colleagues had been using the terms pedagogy and sport pedagogy for many years, the English-speaking world of kinesiology has only relatively recently embraced the terms.’ Nonetheless, Tinning observes that sport pedagogy is now ‘firmly established as a credible academic subdiscipline’.\n\nWhile research in sport pedagogy and research in the field of physical education continue to overlap, sport pedagogy is now seen to be the overarching academic discipline, informing learning, teaching and instruction in a wide range of sport, physical activity and exercise contexts. At the centre of the inquiry is the pedagogical encounter between the teacher/coach/instructor and the learner/participant. In this, it is the purpose of sport pedagogy 'to support the needs of learners in sport, and other forms of physical activity, wherever and whenever they seek to learn through the life-course'. To achieve this end, sport pedagogy researchers should be encouraged to engage in inter-disciplinary work, in order to transcend the respective academic silos that sometimes exist between the distinct sub-disciplines in sport science.\n"}
{"id": "3225840", "url": "https://en.wikipedia.org/wiki?curid=3225840", "title": "Sublunary sphere", "text": "Sublunary sphere\n\nIn Aristotelian physics and Greek astronomy, the sublunary sphere is the region of the geocentric cosmos below the Moon, consisting of the four classical elements: earth, water, air, and fire.\n\nThe sublunary sphere was the realm of changing nature. Beginning with the Moon, up to the limits of the universe, everything (to classical astronomy) was permanent, regular and unchanging – the region of aether where the planets and stars are located. Only in the sublunary sphere did the powers of physics hold sway.\n\nPlato and Aristotle helped to formulate the original theory of a sublunary sphere in antiquity - the idea usually going hand in hand with geocentrism and the concept of a spherical Earth.\n\nAvicenna carried forward into the Middle Ages the Aristotelian idea of generation and corruption being limited to the sublunary sphere. Medieval scholastics like Thomas Aquinas - who charted the division between celestial and sublunary spheres in his work \"Summa Theologica\" - also drew on Cicero and Lucan for an awareness of the great frontier between Nature and Sky, sublunary and aetheric spheres. The result for medieval/Renaissance mentalities was a pervasive awareness of the existence, at the Moon, of what C.S. Lewis called 'this \"great divide\"...from aether to air, from 'heaven' to 'nature', from the realm of gods (or angels) to that of daemons, from the realm of necessity to that of contingence, from the incorruptible to the corruptible\"\n\nHowever, the theories of Copernicus began to challenge the sublunary/aether distinction. In their wake Tycho Brahe's observations of a new star (nova) and of comets in the supposedly unchanging heavens further undermined the Aristotelian view. Thomas Kuhn saw scientists' new ability to see change in the 'incorruptible' heavens as a classic example of the new possibilities opened up by a paradigm shift.\n\nDante envisaged Mt Purgatory as being so high that it reached above the sublunary sphere, so that “These slopes are free from every natural change”.\n\nSamuel Johnson praised Shakespeare's plays as “exhibiting the real state of sublunary nature, which partakes of good and evil, joy and sorrow, intermingled”.\n\n\n"}
{"id": "35845845", "url": "https://en.wikipedia.org/wiki?curid=35845845", "title": "UAW Local 5810", "text": "UAW Local 5810\n\nUAW Local 5810 is the labor union representing postdoctoral researchers at the University of California. It is an affiliate of the International Union, United Automobile, Aerospace and Agricultural Implement Workers of America, AFL-CIO or, UAW. UAW Local 5810 was chartered in 2008. The stand-alone postdoctoral scholar contract negotiated between UC and UAW is notable for being the first of its kind in the United States.\n\nLocal 5810 represents over 6,000 postdoctoral researchers at the University of California, or approximately one tenth of all postdoctoral researchers in the United States.\n\nUC postdocs work in all fields of the academy, but are overwhelmingly concentrated in the fields of science, technology, engineering and math (STEM). Postdocs perform highly technical work, often under exacting conditions. Prior to ratification of their first contract, the majority of postdocs at UC were paid less than $41,000 per year.\n\nIn 2005, a group of postdoctoral researchers employed by the UC, many of whom were previously members of UAW Local 2865 and other local unions representing academic student employees, approached the UAW and asked for help forming a union. The group began an organizing drive as Postdoctoral Researchers Organize/UAW or PRO/UAW.\n\nOn August 19, 2008, the California Public Employment Relations Board (PERB) certified that a majority of UC postdocs had chosen PRO/UAW as their union. Contract negotiations began in February 2009.\n\nNegotiations of the first contract, which spanned a year and a half, were protracted and contentious. During that time, union supporters picketed at all ten University of California campuses. On April 30, 2010, the House of Representatives Committee on Education and Labor held a field hearing at UC Berkeley to evaluate the unusually lengthy amount of time it was taking to reach agreement.\n\nDuring the hearing, Vice President of Human Resources for the University of California Dwaine Duckett testified that the delay was due to difficulty tracking complex funding formulas used to allocate postdoc salaries. A postdoctoral researcher also testified about her difficult experience as an expectant and new mother within the UC system. \n\nAfter the hearing, Committee Chairman George Miller wrote UC President Mark Yudof expressing \"deep concern\" about UC's slow-paced approach, saying he \"left the hearing thoroughly disappointed\" in UC's efforts to reach agreement. Congressman Miller was then joined by Congresswoman Barbara Lee and Congresswoman Lynn Woolsey in sending a letter to the Government Accountability Office asking the agency to look into \"how universities, including the University of California, track how funds provided for laboratory research grants are spent.\"\n\nThe contract was ratified on August 11, 2010 with 96 percent of the vote, and made significant advances in pay, health benefits and safety on the job for postdocs. Postdoc negotiators were successful in linking minimum salaries to an experience-based scale set by the NIH for NRSA Postdoctoral Fellowships. UC postdocs are guaranteed minimum salary increases with every year of experience.\n\nAs of September 2015, the union was in negotiations for a second contract.\n\n"}
{"id": "33971", "url": "https://en.wikipedia.org/wiki?curid=33971", "title": "William Herschel", "text": "William Herschel\n\nFrederick William Herschel, (; ; 15 November 1738 – 25 August 1822) was a German-British astronomer, composer and brother of fellow astronomer Caroline Herschel, with whom he worked. Born in the Electorate of Hanover, Herschel followed his father into the Military Band of Hanover, before migrating to Great Britain in 1757 at the age of nineteen. His works were praised by Mozart, Haydn (who met Herschel in London) and Beethoven. \n\nHerschel constructed his first large telescope in 1774, after which he spent nine years carrying out sky surveys to investigate double stars. The resolving power of the Herschel telescopes revealed that the nebulae in the Messier catalogue were clusters of stars. Herschel published catalogues of nebulae in 1802 (2,500 objects) and in 1820 (5,000 objects). In the course of an observation on 13 March 1781, he realized that one celestial body he had observed was not a star, but a planet, Uranus. This was the first planet to be discovered since antiquity and Herschel became famous overnight. As a result of this discovery, George III appointed him Court Astronomer. He was elected as a Fellow of the Royal Society and grants were provided for the construction of new telescopes.\n\nHerschel pioneered the use of astronomical spectrophotometry, using prisms and temperature measuring equipment to measure the wavelength distribution of stellar spectra. In addition, Herschel discovered infrared radiation. \nOther work included an improved determination of the rotation period of Mars, the discovery that the Martian polar caps vary seasonally, the discovery of Titania and Oberon (moons of Uranus) and Enceladus and Mimas (moons of Saturn). Herschel was made a Knight of the Royal Guelphic Order in 1816. He was the first President of the Royal Astronomical Society when it was founded in 1820. He died in August 1822, and his work was continued by his only son, John Herschel.\n\nHerschel was born in the Electorate of Hanover in Germany, then part of the Holy Roman Empire, one of ten children of Isaac Herschel by his marriage to Anna Ilse Moritzen, of German Lutheran ancestry. It has been proposed by Hershel's biographer Holden that his father's family traced its roots back to Jews from Moravia who converted to Christianity in the seventeenth century, and they themselves were Lutheran Christians.\n\nHerschel's father was an oboist in the Hanover Military Band. In 1755 the Hanoverian Guards regiment, in whose band Wilhelm and his brother Jakob were engaged as oboists, was ordered to England. At the time the crowns of Great Britain and Hanover were united under King George II. As the threat of war with France loomed, the Hanoverian Guards were recalled from England to defend Hanover. After they were defeated at the Battle of Hastenbeck, Herschel's father Isaak sent his two sons to seek refuge in England in late 1757. Although his older brother Jakob had received his dismissal from the Hanoverian Guards, Wilhelm was accused of desertion (for which he was pardoned by George III in 1782).\n\nWilhelm, nineteen years old at this time, was a quick student of the English language. In England he went by the English rendition of his name, Frederick William Herschel.\nIn addition to the oboe, he played the violin and harpsichord and later the organ. He composed numerous musical works, including 24 symphonies and many concertos, as well as some church music. Six of his symphonies were recorded in April 2002 by the London Mozart Players, conducted by Matthias Bamert (Chandos 10048).\n\nHerschel moved to Sunderland in 1761 when Charles Avison immediately engaged him as first violin and soloist for his Newcastle orchestra, where he played for one season. In \"Sunderland in the County of Durh: apprill [sic] 20th 1761\" he wrote his \"Symphony No. 8 in C Minor.\" He was head of the Durham Militia band from 1760 to 1761. He visited the home of Sir Ralph Milbanke at Halnaby Hall near Darlington in 1760, where he wrote two symphonies, as well as giving performances himself.\nAfter Newcastle, he moved to Leeds and Halifax where he was the first organist at St John the Baptist church (now Halifax Minster).\n\nIn 1766, Herschel became organist of the Octagon Chapel, Bath, a fashionable chapel in a well-known spa, in which city he was also Director of Public Concerts. He was appointed as the organist in 1766 and gave his introductory concert on 1 January 1767. As the organ was still incomplete, he showed off his versatility by performing his own compositions including a violin concerto, an oboe concerto and a harpsichord sonata. On 4 October 1767, he performed on the organ for the official opening of the Octagon Chapel.\n\nHis sister Caroline arrived in England on 24 August 1772 to live with William in New King Street, Bath. The house they shared is now the location of the Herschel Museum of Astronomy. Herschel's brothers Dietrich, Alexander and Jakob (1734–1792) also appeared as musicians of Bath. In 1780, Herschel was appointed director of the Bath orchestra, with his sister often appearing as soprano soloist.\n\nHerschel's reading in natural philosophy during the 1770s indicates his personal interests but also suggests an intention to be upwardly mobile socially and professionally. He was well-positioned to engage with eighteenth-century \"philosophical Gentleman\" or philomaths, of wide-ranging logical and practical tastes.\nHerschel's intellectual curiosity and interest in music eventually led him to astronomy. After reading Robert Smiths \"Harmonics, or the Philosophy of Musical Sounds\" (1749), he took up Smith’s \"A Compleat System of Opticks\" (1738), which described techniques of telescope construction. He also read James Fergusons \"Astronomy explained upon Sir Isaac Newton's principles and made easy to those who have not studied mathematics\" (1756) and William Emersons \"The elements of trigonometry\" (1749), \"The elements of optics\" (1768) and \"The principles of mechanics\" (1754).\n\nHerschel took lessons from a local mirror-builder and having obtained both tools and a level of expertise, started building his own reflecting telescopes. He would spend up to 16 hours a day grinding and polishing the speculum metal primary mirrors. He relied on the assistance of other family members, particularly his sister Caroline and his brother Alexander, a skilled mechanical craftsperson.\n\nHe \"began to look at the planets and the stars\" in May 1773 and on 1 March 1774 began an astronomical journal by noting his observations of Saturn's rings and the Great Orion Nebula (M 42).\nThe English Astronomer Royal Nevil Maskelyne visited the Herschels while they were at Walcot (which they left on 29 September 1777).\nBy 1779, Herschel had also made the acquaintance of Sir William Watson, who invited him to join the Bath Philosophical Society. Herschel became an active member, and through Watson would greatly enlarge his circle of contacts.\n\nHerschel's early observational work soon focused on the search for pairs of stars that were very close together visually. Astronomers of the era expected that changes over time in the apparent separation and relative location of these stars would provide evidence for both the proper motion of stars and, by means of parallax shifts in their separation, for the distance of stars from the Earth. The latter was a method first suggested by Galileo Galilei.\nFrom the back garden of his house in New King Street, Bath, and using a , (f/13) Newtonian telescope \"with a most capital speculum\" of his own manufacture, in October 1779, Herschel began a systematic search for such stars among \"every star in the Heavens\", with new discoveries listed through 1792. He soon discovered many more binary and multiple stars than expected, and compiled them with careful measurements of their relative positions in two catalogues presented to the Royal Society in London in 1782 (269 double or multiple systems) and 1784 (434 systems). A third catalogue of discoveries made after 1783 was published in 1821 (145 systems).\n\nThe Rev. John Michell of Thornhill published work in 1767 on the distribution of double stars, and in 1783 on \"dark stars\" (black holes), that may have influenced Herschel. After Michell's death in 1793, Herschel bought a ten foot long, 30-inch reflecting telescope from Michell's estate.\n\nIn 1797, Herschel measured many of the systems again, and discovered changes in their relative positions that could not be attributed to the parallax caused by the Earth's orbit. He waited until 1802 (in \"Catalogue of 500 new Nebulae, nebulous Stars, planetary Nebulae, and Clusters of Stars; with Remarks on the Construction of the Heavens\") to announce the hypothesis that the two stars might be \"binary sidereal systems\" orbiting under mutual gravitational attraction, a hypothesis he confirmed in 1803 in his \"Account of the Changes that have happened, during the last Twenty-five Years, in the relative Situation of Double-stars; with an Investigation of the Cause to which they are owing\". In all, Herschel discovered over 800 confirmed double or multiple star systems, almost all of them physical rather than optical pairs. His theoretical and observational work provided the foundation for modern binary star astronomy; new catalogues adding to his work were not published until after 1820 by Friedrich Wilhelm Struve, James South and John Herschel.\n\nIn March 1781, during his search for double stars, Herschel noticed an object appearing as a disk. Herschel originally thought it was a comet or a stellar disc, which he believed he might actually resolve. He reported the sighting to Nevil Maskelyne the Astronomer Royal. He made many more observations of it, and afterwards Russian Academician Anders Lexell computed the orbit and found it to be probably planetary.\n\nHerschel agreed, determining that it must be a planet beyond the orbit of Saturn. He called the new planet the \"Georgian star\" (Georgium sidus) after King George III, which also brought him favour; the name did not stick. In France, where reference to the British king was to be avoided if possible, the planet was known as \"Herschel\" until the name \"Uranus\" was universally adopted. The same year, Herschel was awarded the Copley Medal and elected a Fellow of the Royal Society. In 1782, he was appointed \"The King's Astronomer\" (not to be confused with the Astronomer Royal).\n\nOn 1 August 1782 Herschel and his sister Caroline moved to Datchet (then in Buckinghamshire but now in Berkshire). There he continued his work as an astronomer and telescope maker. He achieved an international reputation for their manufacture, profitably selling over 60 completed reflectors to British and Continental astronomers.\n\nFrom 1782 to 1802, and most intensively from 1783 to 1790, Herschel conducted systematic surveys in search of \"deep sky\" or nonstellar objects with two , and telescopes (in combination with his favoured 6-inch aperture instrument). Excluding duplicated and \"lost\" entries, Herschel ultimately discovered over 2400 objects defined by him as nebulae. (At that time, nebula was the generic term for any visually diffuse astronomical object, including galaxies beyond the Milky Way, until galaxies were confirmed as extragalactic systems by Edwin Hubble in 1924.)\n\nHerschel published his discoveries as three catalogues: \"Catalogue of One Thousand New Nebulae and Clusters of Stars\" (1786), \"Catalogue of a Second Thousand New Nebulae and Clusters of Stars\" (1789) and the previously cited \"Catalogue of 500 New Nebulae ...\" (1802). He arranged his discoveries under eight \"classes\": (I) bright nebulae, (II) faint nebulae, (III) very faint nebulae, (IV) planetary nebulae, (V) very large nebulae, (VI) very compressed and rich clusters of stars, (VII) compressed clusters of small and large [faint and bright] stars, and (VIII) coarsely scattered clusters of stars. Herschel's discoveries were supplemented by those of Caroline Herschel (11 objects) and his son John Herschel (1754 objects) and published by him as \"General Catalogue of Nebulae and Clusters\" in 1864. This catalogue was later edited by John Dreyer, supplemented with discoveries by many other 19th century astronomers, and published in 1888 as the \"New General Catalogue\" (abbreviated NGC) of 7840 deep sky objects. The NGC numbering is still the most commonly used identifying label for these celestial landmarks.\n\nHe Discovered; NGC 12, NGC 13, NGC 14, NGC 16, NGC 23, NGC 24 (work in progress)\n\nFollowing the death of their father, William suggested that Caroline join him in Bath, England. In 1772, Caroline was first introduced to astronomy by her brother.\nCaroline spent many hours polishing the mirrors of high performance telescopes so that the amount of light captured was maximized. She also copied astronomical catalogues and other publications for William. After William accepted the office of King's Astronomer to George III, Caroline became his constant assistant.\n\nIn October 1783, a new 20-foot telescope came into service for William. During this time, William was attempting to observe and then record all of the observations. He had to run inside and let his eyes readjust to the artificial light before he could record anything, and then he would have to wait until his eyes were adjusted to the dark before he could observe again. Caroline became his recorder by sitting at a desk near an open window. William would shout out his observations and she would write them down along with any information he needed from a reference book.\n\nCaroline began to make astronomical discoveries in her own right, particularly comets. In 1783, William built her a small Newtonian reflector telescope, with a handle to make a vertical sweep of the sky. Between 1783 and 1787, she made an independent discovery of M110 (NGC 205), which is the second companion of the Andromeda Galaxy. During the years 1786–1797, she discovered or observed eight comets. \nShe found fourteen new nebulae \nand, at her brother's suggestion, updated and corrected Flamsteed's work detailing the position of stars. She also rediscovered Comet Encke in 1795.\n\nCaroline Herschel's eight comets were published between 28 August 1782 to 5 February 1787. Five of her comets were published in \"Philosophical Transactions of the Royal Society\". William was even summoned to Windsor Castle to demonstrate Caroline's comet to the royal family. William recorded this phenomenon himself, terming it \"My Sister's Comet.\" She wrote letters to the Astronomer Royal to announce the discovery of her second comet, and wrote to Joseph Banks upon the discovery of her third and fourth comets.\n\nThe \"Catalogue of stars taken from Mr Flamsteed's observations\" contained an index of more than 560 stars that had not been previously included. Caroline Herschel was honoured by the Royal Astronomical Society for this work in 1828.\n\nCaroline also continued to serve as William Herschel's assistant, often taking notes while he observed at the telescope.\nFor her work as William's assistant, she was granted an annual salary of £50 by George III. Her appointment made her the first female in England to be honored with a government position. It also made her the first woman to be given a salary as an astronomer.\n\nIn June 1785, owing to damp conditions, William and Caroline moved to Clay Hall in Old Windsor. On 3 April 1786, the Herschels moved to a new residence on Windsor Road in Slough. Herschel lived the rest of his life in this residence, which came to be known as Observatory House. It was demolished in 1963.\n\nOn 8 May 1788, William Herschel married the widow Mary Pitt (née Baldwin) at St Laurence's Church, Upton in Slough. The marriage caused a lot of tension in the brother-sister relationship. Caroline has been referred to as a bitter, jealous woman who worshipped her brother and resented her sister-in-law for invading her domestic life. With the arrival of Mary, Caroline lost her managerial and social responsibilities in the household, and with them much of her status. Caroline destroyed her journals between the years 1788 to 1798, so her feelings during this period are not entirely known. According to her memoir, Caroline then moved to separate lodgings, but continued to work as her brother's assistant. When her brother and his family were away from their home, she would often return to take care of it for them. In later life, Caroline and Lady Herschel exchanged affectionate letters.\n\nCaroline continued her astronomical work after William's death in 1822. She worked to verify and confirm his findings as well as putting together catalogues of nebulae. Towards the end of her life, she arranged two-and-a-half thousand nebulae and star clusters into zones of similar polar distances. She did this so that her nephew, John, could re-examine them systematically. Eventually, this list was enlarged and renamed the \"New General Catalogue\". In 1828, she was awarded the Gold Medal of the Royal Astronomical Society for her work.\n\nThe most common type of telescope at that time was the refracting telescope, which involved the refraction of light through a tube using a convex glass lens. \nThis design was subject to chromatic aberration, a distortion of an image due to the failure of light of different component wavelengths to converge. Optician John Dollond (1706-1761) tried to correct for this distortion by combining two separate lenses, but it was still difficult to achieve good resolution for far distant light sources.\n\nReflector telescopes, invented by Isaac Newton in 1668, used a single concave mirror rather than a convex lens. The concave mirror gathered more light than a lens, reflecting it onto a flat mirror at the end of the telescope for viewing. A smaller mirror could provide greater magnification and a larger field of view than a convex lens. Newton's first mirror was 1.3 inches in diameter; such mirrors were rarely more than 3 inches in diameter.\n\nBecause of the poor reflectivity of mirrors made of speculum metal, Herschel eliminated the small diagonal mirror of a standard newtonian reflector from his design and tilted his primary mirror so he could view the formed image directly. This \"front view\" design has come to be called the Herschelian telescope.\n\nThe creation of larger, symmetrical mirrors was extremely difficult. Any flaw would result in a blurred image. Because no one else was making mirrors of the size and magnification desired by Herschel, he determined to make his own. This was no small undertaking. He was assisted by his sister Caroline and other family members. Caroline Herschel described the pouring of a 30-ft focal length mirror:\n\nHerschel is reported to have cast, ground, and polished more than four hundred mirrors for telescopes, varying in size from 6 to 48 inches in diameter. Herschel and his assistants built and sold at least sixty complete telescopes of various sizes.\nCommissions for the making and selling of mirrors and telescopes provided Herschel with an additional source of income. The King of Spain reportedly paid £3150 for a telescope.\n\nAn essential part of constructing and maintaining telescopes was the grinding and polishing of their mirrors. This had to be done repeatedly, whenever the mirrors deformed or tarnished during use.\nThe only way to test the accuracy of a mirror was to use it.\n\nThe largest and most famous of Herschel's telescopes was a reflecting telescope with a primary mirror and a 40-foot (12 m) focal length. The 40-foot telescope was, at that time, the largest scientific instrument that had been built. It was hailed as a triumph of \"human perseverance and zeal for the sublimest science\".\n\nIn 1785 Herschel approached King George for money to cover the cost of building the 40-foot telescope. He received £4,000. Without royal patronage, the telescope could not have been created. As it was, it took five years, and went over budget.\n\nThe Herschel home in Slough became a scramble of \"labourers and workmen, smiths and carpenters\". A 40-foot telescope tube had to be cast of iron. The tube was large enough to walk through. Mirror blanks were poured from Speculum metal, a mix of copper and tin. They were almost 4 feet in diameter and weighed 1,000 pounds. When the first disk deformed due to its weight, a second thicker one was made with a higher content of copper. The mirrors had to be hand polished, a painstaking process. A mirror was repeatedly put into the telescope and removed again to ensure that it was properly formed. When a mirror deformed or tarnished, it had to be removed, repolished and replaced in the apparatus. A huge rotating platform was built to support the telescope, enabling it to be repositioned by assistants as a sweep progressed. A platform near the top of the tube enabled the viewer to look down into the tube and view the resulting image.\n\nIn 1789, shortly after this instrument was operational, Herschel discovered a new moon of Saturn: Mimas, only 250 miles in diameter. Discovery of a second moon (Enceladus) followed, within the first month of observation.\n\nThe 40-foot telescope proved very cumbersome, and in spite of its size, not very effective at showing clearer images. Herschel's technological innovations had taken him to the limits of what was possible with the technology of his day. The 40-foot would not be improved upon until the Victorians developed techniques for the precision engineering of large, high-quality mirrors. \nWilliam Herschel was disappointed with it. \nMost of Herschel's observations were done with a smaller reflector. Nonetheless, the 40-foot caught the public imagination. It inspired scientists and writers including Erasmus Darwin and William Blake, and impressed foreign tourists and French dignitaries. King George was pleased.\n\nHerschel discovered that unfilled telescope apertures can be used to obtain high angular resolution, something which became the essential basis for interferometric imaging in astronomy (in particular aperture masking interferometry and hypertelescopes).\n\nIn 2012, the BBC \"Stargazing Live\" television programme built a replica of the 20-foot telescope using Herschel's original plans but modern materials. It is to be considered a close modern approximation rather than an exact replica. A modern glass mirror was used, the frame uses metal scaffolding and the tube is a sewer pipe. The telescope was shown on the programme in January 2013 and stands on the Art, Design and Technology campus of the University of Derby where it will be used for educational purposes.\n\nHerschel was sure that he had found ample evidence of life on the Moon and compared it to the English countryside. He did not refrain himself from theorising that the other planets were populated, with a special interest in Mars, which was in line with most of his contemporary scientists. At Herschel's time, scientists tended to believe in a plurality of civilised worlds; in contrast, most religious thinkers referred to unique properties of the earth.\nHerschel went so far to speculate that the interior of the sun was populated.\n\nHerschel started to examine the correlation of solar variation and solar cycle and climate. Over a period of 40 years (1779–1818), Herschel had regularly observed sunspots and their variations in number, form and size. Most of his observations took place in a period of low solar activity, the Dalton minimum, when sunspots were relatively few in number. This was one of the reasons why Herschel was not able to identify the standard 11-year period in solar activity.\nHerschel compared his observations with the series of wheat prices published by Adam Smith in \"The Wealth of Nations\".\n\nIn 1801, Herschel reported his findings to the Royal Society and indicated five prolonged periods of few sunspots correlated with the price of wheat. Herschel's study was ridiculed by some of his contemporaries but did initiate further attempts to find a correlation. Later in the 19th century, William Stanley Jevons proposed the 11-year cycle with Herschel's basic idea of a correlation between the low number of sunspots and lower yields explaining recurring booms and slumps in the economy.\n\nHerschel's speculation on a connection between sunspots and regional climate, using the market price of wheat as a proxy, continues to be cited.\nAccording to one study, the influence of solar activity can actually be seen on the historical wheat market in England over ten solar cycles between 1600 and 1700. The evaluation is controversial and the significance of the correlation is doubted by some scientists.\n\nIn his later career, Herschel discovered two moons of Saturn, Mimas and Enceladus; as well as two moons of Uranus, Titania and Oberon. He did not give these moons their names; they were named by his son John in 1847 and 1852, respectively, after his death. Herschel measured the axial tilt of Mars and discovered that the martian ice caps, first observed by Giovanni Domenico Cassini (1666) and Christiaan Huygens (1672), changed size with that planet's seasons. It has been suggested that Herschel might have discovered rings around Uranus.\n\nHerschel introduced but did not create the word \"asteroid\", meaning \"star-like\" (from the Greek \"asteroeides\", \"aster\" \"star\" + \"-eidos\" \"form, shape\"), in 1802 (shortly after Olbers discovered the second minor planet, 2 Pallas, in late March), to describe the star-like appearance of the small moons of the giant planets and of the minor planets; the planets all show discs, by comparison. By the 1850s 'asteroid' became a standard term for describing certain minor planets.\n\nFrom studying the proper motion of stars, the nature and extent of the solar motion was first demonstrated by Herschel in 1783, along with first determining the direction for the solar apex to Lambda Herculis, only 10° away from today's accepted position.\nHerschel also studied the structure of the Milky Way and was the first to propose a model of the galaxy based on observation and measurement. He concluded that it was in the shape of a disk, but incorrectly assumed that the sun was in the centre of the disk. This Heliocentric view was eventually replaced by Galactocentrism due to the work of Harlow Shapley, Heber Doust Curtis and Edwin Hubble in the 1900s. All three men used significantly more far-reaching and accurate telescopes than Herschel's.\n\nOn 11 February 1800, Herschel was testing filters for the sun so he could observe sun spots. When using a red filter he found there was a lot of heat produced. Herschel discovered infrared radiation in sunlight by passing it through a prism and holding a thermometer just beyond the red end of the visible spectrum. This thermometer was meant to be a control to measure the ambient air temperature in the room. He was shocked when it showed a higher temperature than the visible spectrum. Further experimentation led to Herschel's conclusion that there must be an invisible form of light beyond the visible spectrum.\n\nHerschel used a microscope to establish that coral was not a plant – as many at the time believed – because it lacked the cell walls characteristic of plants.\n\nWilliam Herschel and Mary had one child, John, born at Observatory House on 7 March 1792. William's personal background and rise as man of science had a profound impact on the upbringing of his son and grandchildren. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1788. In 1816, William was made a Knight of the Royal Guelphic Order by the Prince Regent and was accorded the honorary title 'Sir' although this was not the equivalent of an official British knighthood. He helped to found the Astronomical Society of London in 1820, which in 1831 received a royal charter and became the Royal Astronomical Society. In 1813, he was elected a foreign member of the Royal Swedish Academy of Sciences.\n\nOn 25 August 1822, Herschel died at Observatory House, Windsor Road, Slough, after a long illness. He was buried at nearby St Laurence's Church, Upton, Slough. Herschel's epitaph is\nCaroline was deeply distressed by his death, and soon after his burial she returned to Hanover, a decision she later regretted. She had lived in England for fifty years. Her interests were much more in line with her nephew John Herschel, also an astronomer, than with her surviving family in Hanover. She continued to work on the organization and cataloguing of nebulae, creating what would later become the basis of the New General Catalogue. She died on 9 January 1848.\n\nWilliam Herschel lived most of his life in Slough, a town then in Buckinghamshire. He died in the town and was buried under the tower of the Church of St Laurence, Upton-cum-Chalvey, near Slough.\n\nHerschel is quite respected in Slough and there are several memorials to him and his discoveries. In 2011 a new bus station, the design of which was inspired by the infrared experiment of William Herschel, was built in the centre of Slough.\n\nHis house at 19 New King Street in Bath, Somerset, where he made many telescopes and first observed Uranus, is now home to the Herschel Museum of Astronomy.\n\nHerschel's complete musical works were as follows:\n\nVarious vocal works including a \"Te Deum\", psalms, motets and sacred chants along with some catches.\nKeyboard works for organ and harpsichord: \n\n\n\n\n\n"}
{"id": "41151248", "url": "https://en.wikipedia.org/wiki?curid=41151248", "title": "Woman in Science", "text": "Woman in Science\n\nWoman in Science is a book written by H. J. Mozans (a pseudonym for John Augustine Zahm) in 1913. It is an account of women who have contributed to the sciences, up to the time when it was published.\n\nThe comprehensive theme that is depicted throughout \"Woman in Science\" is that of women's biological capacity. It is asserted that women being less prominent than men in science is due to the lack of educational and career opportunities available rather than, the biological aspects of brain size or structure. In addition, the book encompasses the many developments of science throughout history. The main objective of the author/book was for women to become more involved and gain a respected position in the scientific field, in addition to increasing educational and career opportunities for women interested in science. It was one of the first collaborations of women's contributions to the scientific community, and it \"explored the barriers to women's participation in science.\"\n\nPREFACE\n\n\nThe biographies include, but are not limited to, the following women, by chapter:\n\n"}
{"id": "1563736", "url": "https://en.wikipedia.org/wiki?curid=1563736", "title": "World clock", "text": "World clock\n\nA world clock is a clock which displays the time for various cities around the world. \nThe display can take various forms:\nThere are also worldtime watches, both wrist watches and pocket watches. Sometime manufacturers of timekeepers erroneously apply the worldtime label to instruments that merely indicate time for two or a few time zones, but the term should be used only for timepieces that indicate time for all major time zones of the globe.\n\n"}
