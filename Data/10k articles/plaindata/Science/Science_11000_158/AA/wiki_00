{"id": "1884381", "url": "https://en.wikipedia.org/wiki?curid=1884381", "title": "A-not-B error", "text": "A-not-B error\n\nA-not-B error (also known as \"stage 4 error\" or \"perseverative error\") is a phenomenon uncovered by the work of Jean Piaget in his theory of cognitive development of children. The A-not-B error is a particular error made by infants during substage 4 of their sensorimotor stage. \n\nA typical A-not-B task goes like this: An experimenter hides an attractive toy under box \"A\" within the baby's reach. The baby searches for the toy, looks under box \"A\", and finds the toy. This activity is usually repeated several times (always with the researcher hiding the toy under box \"A\"). Then, in the critical trial, the experimenter moves the toy under box \"B\", also within easy reach of the baby. Babies of 10 months or younger typically make the perseveration error, meaning they look under box \"A\" even though they saw the researcher move the toy under box \"B\", and box \"B\" is just as easy to reach. This demonstrates a lack of, or incomplete, schema of object permanence. Children of 12 months or older typically do not make this error.\n\nTraditionally, this phenomenon has been explained as the child seeing an image and remembering where it was, rather than where it is. Other accounts deal with the development of planning, reaching, and deciding things. There are also behaviorist accounts that explain the behavior in terms of reinforcement. This account argues that the repeated trials with hiding the toy in box \"A\" is reinforcing that specific behavior, so that the child still reaches for box \"A\" because the action has been reinforced before. However, this account does not explain the shift in behavior that occurs around 12 months.\n\nSmith and Thelen used a dynamic systems approach to the A-not-B task. They found that various components of the activity (strength of memory trace, salience of targets, waiting time, stance) combine in the \"B\"-trial (where the object is hidden in the \"B\" location rather than \"A\") so the child either correctly or incorrectly searches for the toy. They experimentally manipulated the task to see if they could make 10-month-old babies (who typically make the perseverative error of searching at \"A\") perform like 12-month-old babies (who typically search correctly). Changing the stance of the baby (sitting or standing) was one manipulation they found could make the 10-month-old search correctly. Just standing instead of sitting for the \"B\"-trial made the prior experience of searching in location \"A\" less salient to the child, who then searched correctly. The researchers concluded that the length of wait time was one of the crucial elements of the task that is influenced by age.\n\n"}
{"id": "16788828", "url": "https://en.wikipedia.org/wiki?curid=16788828", "title": "Aceramic", "text": "Aceramic\n\nAceramic is defined as \"not producing pottery\". In archaeology, the term means \"without pottery\".\n\nAceramic societies usually used bark, basketry, gourds and leather for containers. It is sometimes used to refer to a specific early Neolithic period before a culture develops ceramics, such as the Middle Eastern Pre-Pottery Neolithic A, in which case it is a synonym of preceramic (or pre-pottery).\n\nIt should be distinguished from the specific term Pre-Ceramic, which is a period in many chronologies of the archaeology of the Americas, typically showing some agriculture and developed textiles but no fired pottery. For example, in the Norte Chico civilization and other cultures of Peru, the cultivation of cotton seems to have been very important in economic and power relations, from around 3200 BC. Here, Cotton Pre-Ceramic may be used as a period. The \"Pre-Ceramic\" may be followed by \"Ceramic\" periods or a formative stage.\n\n\"Aceramic\" is also used to describe a culture at any time prior to its development of pottery as well as cultures that lack pottery altogether. A preceramic period is traditionally regarded as occurring in the early stage of the Neolithic period of a culture, but recent findings in Japan and China have pushed the origin of ceramic technology there well back into the Paleolithic era.\n\nThe Aceramic Neolithic period began roughly around 8500 BC and can be identified with over a half a dozen sites. The period was most prominent in Western Asia in an economy based on the cultivation of crops or the rearing of animals or both. Aceramic Neolithic groups are more rare outside Western Asia. Aceramic Neolithic villages had many attributes of agricultural communities: large settlement size, substantial architecture, long settlement duration, intensive harvesting of seeds with sickles, equipment and facilities for storing and grinding seeds, and containers. Morphological evidence for domestication of plants comes only from Middle PPNB (Pre-Pottery Neolithic B), and by Late PPNB some animals, notably goats, were domesticated or at least managed in most of the sites.\n\nSome of the most famous Aceramic sites are located in the Republic of Cyprus. There was an Early Aceramic Neolithic phase beginning around 8200 BC. The phase can be best thought of as a \"colony\", or initial settlement of the island. Until the relatively recent discoveries of the Akrotiri and the Early Aceramic Neolithic phases, the Aceramic Neolithic culture known as the Khirokitia culture was thought to be the earliest human settlement on Cyprus, from 7000 to 5000 BC. There are a number of Late Aceramic Neolithic sites throughout the island. The two most important are called Khirokitia and Kalavasos-Tenta. Late Aceramic Cyprus did not have much external contact because of a lack of settlement in the west or northwest during the period. However, Late Aceramic Cyprus was a well-structured society.\n\n"}
{"id": "47696373", "url": "https://en.wikipedia.org/wiki?curid=47696373", "title": "Ariadne (crater)", "text": "Ariadne (crater)\n\nAriadne Crater is a crater on Venus. Its central peak serves as the prime meridian of the planet, a status formerly held by Eve Crater until relocated.\n\n"}
{"id": "315197", "url": "https://en.wikipedia.org/wiki?curid=315197", "title": "Arnold Orville Beckman", "text": "Arnold Orville Beckman\n\nArnold Orville Beckman (April 10, 1900 – May 18, 2004) was an American chemist, inventor, investor, and philanthropist. While a professor at California Institute of Technology, he founded Beckman Instruments based on his 1934 invention of the pH meter, a device for measuring acidity, later considered to have \"revolutionized the study of chemistry and biology\". He also developed the DU spectrophotometer, \"probably the most important instrument ever developed towards the advancement of bioscience\". Beckman funded the first transistor company, thus giving rise to Silicon Valley. After retirement, he and his wife Mabel (1900-1989) were numbered among the top philanthropists in the United States.\n\nArnold Orville Beckman was born in Cullom, Illinois, a village of about 500 people in a farming community. He was the youngest son of George Beckman, a blacksmith, and his second wife Elizabeth Ellen Jewkes. He was curious about the world from an early age. When he was nine, Beckman found an old chemistry textbook, Joel Dorman Steele's \"Fourteen Weeks in Chemistry\", and began trying out the experiments. His father encouraged his scientific interests by letting him convert a toolshed into a laboratory.\n\nBeckman's mother, Elizabeth, died of diabetes in 1912. Beckman's father sold his blacksmith shop, and became a travelling salesman for blacksmithing tools and materials. A housekeeper, Hattie Lange, was engaged to look after the Beckman children. Arnold Beckman earned money as a \"practice pianist\" with a local band, and as an \"official cream tester\" running a centrifuge for a local store.\n\nIn 1914, the Beckman family moved to Normal, located just north of Bloomington, Illinois, so that the young Beckmans could attend University High School in Normal, a \"laboratory school\" associated with Illinois State University. In 1915 they moved to Bloomington itself, but continued to attend University High, where Arnold Beckman obtained permission to take university level classes from professor of chemistry Howard W. Adams. While still in high school, Arnold started his own business, \"Bloomington Research Laboratories\", doing analytic chemistry for the local gas company. He also performed at night as a movie-house pianist, and played with local dance bands. He graduated valedictorian of his class, with an average of 89.41 over four years, the highest attained.\n\nBeckman was allowed to leave school a few months early to contribute to the First World War effort in early 1918 by working as a chemist. At Keystone Steel and Iron he took samples of molten iron and tested them to see if the chemical composition of carbon, sulfur, manganese and phosphorus was suitable for pouring steel.\n\nWhen Beckman turned 18 in August 1918, he enlisted in the United States Marines. After three months at marine boot camp on Parris Island, South Carolina, he was sent to the Brooklyn Navy Yard, for transit to the war in Europe. Because of a train delay, another unit embarked in place of Beckman's unit. Then, counted into groups in the barracks, Beckman missed being sent to Russia by one space in line. Instead, Arnold spent Thanksgiving at the local YMCA, where he met 17-year-old Mabel Stone Meinzer, who was helping to serve the meal. Mabel would become his wife. A few days later, the armistice was signed, ending the war.\n\nArnold Beckman attended the University of Illinois at Urbana–Champaign beginning in the fall of 1918. During his freshman year, he worked with Carl Shipp Marvel on the synthesis of organic mercury compounds, but both men became ill from exposure to toxic mercury. As a result, Beckman changed his major from organic chemistry to physical chemistry, where he worked with Worth Rodebush, T. A. White, and Gerhard Dietrichson. He earned his bachelor's degree in chemical engineering in 1922 and his master's degree in physical chemistry in 1923. For his master's degree he studied the thermodynamics of aqueous ammonia solutions, a subject introduced to him by T. A. White.\n\nSoon after arriving at the University of Illinois, Beckman joined the Delta Upsilon Fraternity. He was initiated into Zeta Chapter of Alpha Chi Sigma, the chemistry fraternity, in 1921 and the Gamma Alpha Graduate Scientific Fraternity in December 1922.\n\nBeckman decided to go to California Institute of Technology (Caltech) for his doctorate. He stayed there for a year, before returning to New York to be near his fiancée, Mabel, who was working as a secretary for the Equitable Life Assurance Society. He found a job with Western Electric's engineering department, the precursor to the Bell Telephone Laboratories. Working with Walter A. Shewhart, Beckman developed quality control programs for the manufacture of vacuum tubes and learned about circuit design. It was here that Beckman discovered his interest in electronics.\n\nBeckman married Mabel on June 10, 1925. In 1926 the couple moved back to California and Beckman resumed his studies at Caltech. He became interested in ultraviolet photolysis and worked with his doctoral advisor, Roscoe G. Dickinson, on an instrument to find the energy of ultraviolet light. It worked by shining the ultraviolet light onto a thermocouple, converting the incident heat into electricity, which drove a galvanometer. After receiving a Ph.D. in photochemistry in 1928 for this application of quantum theory to chemical reactions, Beckman was asked to stay on at Caltech as an instructor and then as a professor. Linus Pauling, another of Roscoe G. Dickinson's graduate students, was also asked to stay on at Caltech.\n\nIn 1933, Beckman and his family built a home in Altadena, California, in the foothills and adjacent to Pasadena. They lived in Altadena for over 27 years, raising their family.\n\nDuring his time at Caltech, Beckman was active in teaching at both the introductory and advanced graduate levels. Beckman shared his expertise in glass-blowing by teaching classes in the machine shop. He also taught classes in the design and use of research instruments. Beckman dealt first-hand with the chemists' need for good instrumentation as manager of the chemistry department's instrument shop. Beckman's interest in electronics made him very popular within the chemistry department at Caltech, as he was very skilled in building measuring instruments.\n\nOver the time that he was at Caltech, the focus of the department increasingly moved towards pure science and away from chemical engineering and applied chemistry. Arthur Amos Noyes, head of the chemistry division, encouraged both Beckman and chemical engineer William Lacey to be in contact with real-world engineers and chemists, and Robert Andrews Millikan, Caltech's president, referred technical questions to Beckman from government and businesses. With their blessing, Beckman began accepting outside work as a scientific and technical consultant. He also acted as a scientific expert in legal trials.\n\nIn 1934, Millikan referred I. H. Lyons from the National Postal Meter Company to Arnold Beckman. Lyons wanted a non-clogging ink so that postage could be printed by machines, instead of having clerks lick stamps. Beckman's solution was to make ink with butyric acid, a malodorous substance. Because of this ingredient, no manufacturer wanted to manufacture it. Beckman decided to make it himself. He started the National Inking Appliance Company, obtaining space in a garage owned by instrument maker Fred Henson and hiring two Caltech students, Robert Barton and Henry Fracker. Beckman developed and took out a couple of patents for re-inking typewriter ribbons, but marketing them was not successful. This was Beckman's first experience at running a company and marketing a product, and while this first product failed, Beckman repurposed the company for another product.\n\nSunkist Growers was having problems with its own manufacturing process. Lemons that were not saleable as produce were made into pectin or citric acid, with sulfur dioxide used as a preservative. Sunkist needed to know the acidity of the product at any given time, and the colorimetric methods then in use, such as readings from litmus paper, did not work well because sulfur dioxide interfered with them. Chemist Glen Joseph at Sunkist was attempting to measure the hydrogen-ion concentration in lemon juice electrochemically, but sulfur dioxide damaged hydrogen electrodes, and non-reactive glass electrodes produced weak signals and were fragile.\n\nJoseph approached Beckman, who proposed that instead of trying to increase the sensitivity of his measurements, he amplify his results. Beckman, familiar with glassblowing, electricity, and chemistry, suggested a design for a vacuum-tube amplifier and ended up building a working apparatus for Joseph. The glass electrode used to measure pH was placed in a grid circuit in the vacuum tube, producing an amplified signal which could then be read by an electronic meter. The prototype was so useful that Joseph requested a second unit.\n\nBeckman saw an opportunity, and rethinking the project, decided to create a complete chemical instrument which could be easily transported and used by nonspecialists. By October 1934, he had registered patent application US Patent No. 2,058,761 for his \"acidimeter\", later renamed the pH meter. The Arthur H. Thomas Company, a nationally known scientific instrument dealer based in Philadelphia, was willing to try selling it. Although it was priced expensively at $195, roughly the starting monthly wage for a chemistry professor at that time, it was significantly cheaper than the estimated cost of building a comparable instrument from individual components, about $500. The original pH meter weighed in at nearly 7 kg, but was a substantial improvement over a benchful of delicate equipment. The earliest meter had a design glitch, in that the pH readings changed with the depth of immersion of the electrodes, but Beckman fixed the problem by sealing the glass bulb of the electrode.\n\nOn April 8, 1935, Beckman renamed his company National Technical Laboratories, formally acknowledging his new focus on the making of scientific instruments. The company rented larger quarters at 3330 Colorado Street, and began manufacturing pH meters. The pH meter is an important device for measuring the pH of a solution, and by 11 May 1939, sales were successful enough that Beckman left Caltech to become the full-time president of National Technical Laboratories. By 1940, Beckman was able to take out a loan to build his own 12,000 square foot factory in South Pasadena.\n\nIn 1940, the equipment needed to measure light energy in the visible spectrum could cost a laboratory as much as $3,000, a huge amount at that time. There was also growing interest in examining ultraviolet spectra beyond that range. Just as Beckman had created a single easy-to-use instrument for measuring pH, he made it a goal to create an easy-to-use instrument for spectrophotometry. Beckman's research team, led by Howard Cary, developed several models.\n\nThe new spectrophotometers used a prism to separate light into its absorption spectrum and a phototube to electrically measure the light energy across the spectrum. They allowed the user to plot the light absorption spectrum of a substance, giving a standardized \"fingerprint\", characteristic of a compound. With Beckman's model D, later known as the DU spectrophotometer, National Technical Laboratories successfully provided the first easy-to-use single instrument containing both the optical and electronic components needed for ultraviolet-absorption spectrophotometry. The user could insert a sample, dial up the desired wavelength of light, and read the amount of absorption of that frequency from a simple meter. It produced accurate absorption spectra in both the ultraviolet and the visible regions of the spectrum with relative ease and repeatable accuracy. The National Bureau of Standards ran tests to certify that the DU's results were accurate and repeatable and recommended its use.\nBeckman's DU spectrophotometer has been referred to as the \"Model T\" of scientific instruments: \"This device forever simplified and streamlined chemical analysis, by allowing researchers to perform a 99.9% accurate quantitative measurement of a substance within minutes, as opposed to the weeks required previously for results of only 25% accuracy.\" Theodore L. Brown notes that it \"revolutionized the measurement of light signals from samples\". Nobel laureate Bruce Merrifield is quoted as calling the DU spectrophotometer \"probably the most important instrument ever developed towards the advancement of bioscience.\"\n\nDevelopment of the spectrophotometer also had direct relevance to the war effort. For example, the role of vitamins in health was being studied, and scientists wanted to identify Vitamin A-rich foods to keep soldiers healthy. Previous methods involved feeding rats for several weeks, then performing a biopsy to estimate Vitamin A levels. The DU spectrophotometer yielded better results in a matter of minutes. The DU spectrophotometer was also an important tool for scientists studying and producing the new wonder drug penicillin. By the end of the war, American pharmaceutical companies were producing 650 billion units of penicillin each month. Much of the work done in this area during World War II was kept secret until after the war.\n\nBeckman and his company were involved in a number of secret projects. There was a critical shortage of rubber, which was used in jeep and airplane tires and in tanks. Natural sources from the Far East were unavailable because of the war, and scientists sought a reliable synthetic substitute. Beckman was approached by the Office of Rubber Reserve about developing an infrared spectrophotometer to aid in the study of chemicals such as toluene and butadiene. The Office of Rubber Reserve met secretly in Detroit with Robert Brattain of the Shell Development Company, Arnold O. Beckman, and R. Bowling Barnes of American Cyanamid. Beckman was asked to secretly produce a hundred infrared spectrophotometers to be used by authorized government scientists, based on a design for a single-beam spectrophotometer which had already been developed by Robert Brattain for Shell. The result was the Beckman IR-1 Spectrophotometer.\n\nBy September 1942, the first of the instruments was being shipped. Approximately 75 IR-1s were made between 1942 and 1945 for use by the US synthetic-rubber effort. The researchers were not allowed to publish or discuss anything related to the new machines until after the war. Other researchers who were independently pursuing the development of infrared spectrometry, were able to publish and to develop instruments during this time without being affected by secrecy restrictions.\n\nBeckman had continued to develop the infrared spectrophotometer after the release of the IR-1. Facing stiff competition, he decided in 1953 to go forward with a radical redesign of the instrument. The result was the IR-4, which could be operated using either a single or double beam of infrared light. This allowed a user to take both the reference measurement and the sample measurement at the same time.\n\nAt the same time that Beckman was approached about infrared spectrometry, he was contacted by Paul Rosenberg. Rosenberg worked at MIT's Radiation Laboratory. The lab was part of a secret network of research institutions in both the United States and Britain that were working to develop radar, \"radio detecting and ranging\". The project was interested in Beckman because of the high quality of the tuning knobs or \"potentiometers\" which were used on his pH meters. Beckman had trademarked the design of the pH meter knobs, under the name \"helipot\" for \"helical potentiometer\". Rosenberg had found that the helipot was more precise, by a factor of ten, than other knobs. Nonetheless, for use in continuously moving airplanes, ships, or submarines, which might be under attack, a redesign would be needed to ensure that the knobs could withstand shocks and vibrations.\n\nBeckman was not allowed to tell his staff the reason behind the redesign, and they were not particularly interested in the problem; he eventually came up with a solution himself. Instead of using a wire wrapped around a coil, with pressure from a small spring to create a single contact point, he redesigned the knob to have a continuous groove, in which the contact point was contained. The contact point could then move smoothly and continuously, and could not be jarred out of contact. Beckman's Model A Helipot was in tremendous demand by the military. Within the first year of its production, its sales became 40% of the company's income. Beckman spun off a separate company, the Helipot Corporation, to take on the electronics component manufactory.\n\nLinus Pauling at Caltech was also doing secret work for the military. The National Defense Research Committee called a meeting on October 3, 1940, wanting an instrument that could reliably measure oxygen content in a mixture of gases, so that they could measure oxygen conditions in submarines and airplanes. Pauling designed the Pauling oxygen meter for them. Originally approached to supply housing boxes for the meter by Holmes Sturdivant, Pauling's assistant, Beckman was soon asked to produce the entire instrument.\n\nWhile the board of the National Technical Laboratory was unwilling to support the secret project, whose details they could not be told, they agreed that Beckman was free to follow up on it independently. Beckman set up a second spinoff company, Arnold O. Beckman, Inc., for their manufacture. Creating the oxygen meter was a technical challenge, involving the creation of tiny, highly precise glass dumbbells. Beckman created a tiny glass-blowing machine which would generate a precisely measured puff of air to create the glass balls.\n\nAfter the war, Beckman developed oxygen analyzers for another market. They were used to monitor conditions in incubators for premature babies. Doctors at Johns Hopkins University used them to determine recommendations for healthy oxygen levels for incubators.\n\nBeckman instruments were also used by the Manhattan Project. Scientists in the project were attempting to develop instruments to measure radiation in gas-filled, electrically charged ionization chambers in nuclear reactors. It was difficult to get reliable readings because the signals were weak. Beckman realized that with a relatively minor adjustment - substituting an input-load resistor for the glass electrode - the pH meter could be adapted to do the job. As a result, Beckman Instruments developed a new product, the micro-ammeter.\n\nIn addition, Beckman developed a dosimeter for measuring exposure to radiation, to protect personnel of the Manhattan project. The dosimeter was a miniature ionization chamber, charged with 170 volts. It had a small calibrated scale on top, whose needle was a platinum-covered quartz fiber. The dosimeters were also manufactured by Beckman's spinoff company, Arnold O. Beckman, Inc.\n\nIn postwar Southern California, including the area of Pasadena where the Beckmans lived, smog was becoming an increasing topic of conversation, as well as an unpleasant experience. First characterized as \"gas attacks\" in 1943, suspicion fell on a variety of possible causes including the smudge pots used by orange growers, the smoke produced by local industrial plants, and car exhausts. The Los Angeles Chamber of Commerce was one of the organizations concerned about the possible causes and effects of smog, as it related both to industry (and jobs) and to quality of life in the area. Beckman was involved with the Chamber of Commerce.\n\nIn 1947, California governor Earl Warren signed a statewide air pollution control act, authorizing the creation of Air Pollution Control Districts (APCDs) in every county of the state. The Los Angeles Chamber of Commerce asked Beckman to represent them in dealing with creation of a local APCD. The new APCD, when formed, asked Beckman to become the scientific consultant to the Air Pollution Control Officer. He held the position from 1948 to 1952.\n\nThe Air Pollution Control Officer in question was Louis McCabe, a geologist with a background in chemical engineering. McCabe initially suspected that smog was a result of sulfur dioxide pollution, and proposed that the county convert the suspected pollutant into fertilizer through a costly process. Beckman was not convinced that sulfur dioxide was the real culprit behind Los Angeles smog. He visited Gary, Indiana, where steps were being taken to address sulfur dioxide pollution, and was struck by the characteristic smell of sulfur in the air. Returning, Beckman convinced McCabe that they needed to search for a different cause.\n\nBeckman got in touch with a Caltech professor who was working on smog, Arie Jan Haagen-Smit. They developed an apparatus to collect particulate matter from Los Angeles air, using a system of tubing intermittently cooled by liquid nitrogen. Haagen-Smit identified the substance they collected as a peroxy organic material. He agreed to spend a year studying the chemistry of smog. His results, presented in 1952, identified ozone and hydrocarbons from smokestacks, refineries and car exhausts as key ingredients in the formation of smog.\n\nWhile Haagen-Smit worked out the genesis of smog, Beckman developed an instrument to measure it. On October 7, 1952, he was granted a patent for an \"oxygen recorder\" that used colorimetric methods to measure the levels of compounds present in the atmosphere. Beckman Instruments eventually developed a range of instruments for various uses in monitoring and treating automobile exhaust and air pollution. They even produced \"air quality monitoring vans\", customized laboratories on wheels for use by government and industry.\n\nBeckman himself was approached by California governor Goodwin Knight to head a Special Committee on Air Pollution, to propose ways to combat smog. At the end of 1953, the committee made its findings public. The \"Beckman Bible\" advised key steps to be taken immediately:\n\nBeckman Instruments also acquired the Liston-Becker Instrument Company in June 1955. Founded by Max D. Liston, Liston-Becker had a successful record in the development of infrared gas analyzers. Liston developed instruments to measure smog and car exhaust emissions, essential to attempts to improve Los Angeles air quality in the 1950s.\n\nBeckman helped to create the Air Pollution Foundation, a non-profit organization to support research on finding solutions to smog, and educating the public about scientific issues related to smog.\n\nIn 1954, he became a member of the board of directors of the Los Angeles Chamber of Commerce, and chairman of its Air Pollution Committee. He advocated for stronger powers for the APCD, and encouraged industry, business, and citizens to support for their work. He helped the Chamber of Commerce to develop a unified approach to monitoring smog, broadcasting smog alerts, and addressing the smog problem. On January 25, 1956, he became president of the Los Angeles Chamber of Commerce. He identified the two key issues of his term as battling smog, and supporting the collaboration of local science, technology, industry, and education.\n\nBeckman recognized that air quality would not improve overnight. His work with air quality continued for years, and brought him national attention. In 1967, Beckman was appointed to the Federal Air Quality Board for a four-year term, by President Richard Nixon.\n\nJohn J. Murdock held substantial stock in National Technical Laboratories. He and Arnold Beckman signed a stock option agreement by which Beckman could purchase Murdock's NTL stock from his estate after his death. When Murdock died in 1948, Beckman was able to gain a controlling interest in the company. On April 27, 1950, National Technical Laboratories was renamed Beckman Instruments, Incorporated. In 1952, Beckman Instruments became a publicly traded company on the New York Curb Exchange, generating new capital for expansion, including overseas expansion.\n\nHelipot Corporation, the spinoff company that Beckman had created when NTL's board were dubious about electronics, was reincorporated into Beckman Instruments and became the Helipot Division in 1958. Helipot researchers were experimenting with cermets, composite materials made by mixing ceramics and metals. Potentiometers made with cermet instead of metal were more heat-resistant, suitable for use at extreme temperatures.\n\nIn 1954, Beckman Instruments acquired ultracentrifuge maker Spinco (Specialized Instruments Corp.), founded by Edward Greydon Pickels in 1946. This acquisition was the basis of Beckman's Spinco centrifuge division. The division went on to design and manufacture a range of preparative and analytical ultracentrifuges.\n\nIn 1955, Beckman was contacted by William Shockley. Shockley, who had been one of Beckman's students at Caltech, led Bell Labs research program into semiconductor technology. Semiconductors were, in some ways, similar to cermets. Shockley wanted to create a new company, and asked Beckman to serve on the board. After considerable discussion, Beckman became more closely involved: he and Shockley signed a letter of intent to create the Shockley Semiconductor Laboratory as a subsidiary of Beckman Instruments, under William Shockley's direction. The new group would specialize in semiconductors, beginning with the automated production of diffused-base transistors.\n\nBecause Shockley's aging mother lived in Palo Alto, Shockley wanted to establish the laboratory in nearby Mountain View, California. Frederick Terman, provost at Stanford University, offered the firm space in Stanford's new industrial park. The firm launched in February 1956, the same year that Shockley received the Nobel Prize in Physics along with John Bardeen and Walter Houser Brattain \"for their researches on semiconductors and their discovery of the transistor effect\". Shockley Semiconductor Laboratory was the first establishment working on silicon semiconductor devices in what came to be known as Silicon Valley.\n\nShockley, however, lacked experience in business and industrial management. Moreover, he decided that the lab would research an invention of his own, the four-layer diode, rather than developing the diffused silicon transistor that he and Beckman had agreed upon. Beckman was reassured by his engineers that the scientific ideas behind Shockley's project were still sound. When appealed to by members of Shockley's lab, Beckman chose not to interfere with its management. In 1957, eight leading scientists including Gordon Moore and Robert Noyce left Shockley's group to form a competing startup, Fairchild Semiconductor, which would successfully develop silicon transistors. In 1960, Beckman sold the Shockley subsidiary to the Clevite Transistor Company, ending his formal association with semiconductors. Nonetheless, Beckman had been an essential backer of the new industry in its initial stages.\n\nBeckman also saw that computers and automation offered a myriad of opportunities for integration into instruments, and the development of new instruments. Beckman Instruments purchased Berkeley Scientific Company in the 1950s, and later developed a Systems Division within Beckman Instruments \"to develop and build industrial data systems for automation\". Berkeley developed the EASE analog computer, and by 1959 Beckman had contracts with major companies in the aerospace, space, and defense industries, including Boeing Aerospace, Lockheed Aircraft, North American Aviation, and Lear Siegler. The Beckman Systems Division also developed specialized computer systems to handle large volumes of telemetric radio data from satellites and unmanned spacecraft. These included systems to process photographs of the moon, taken by NASA's Ranger spacecraft.\n\nThe 1960s were a time of change for the Beckmans. Mabel fell in love with a house by the sea in Corona del Mar near Newport Beach, California. They bought the house in 1960, renovated it, and lived there together until Mabel's death in 1989.\n\nBeckman also chose to retire. He and his wife Mabel became increasingly active as philanthropists, with the stated intention of giving away their personal wealth before their deaths. In 1964, Beckman was asked to become chairman of the Caltech Board of Trustees, and accepted the position. He had been a member of the board since 1953. In 1965, he retired as president of Beckman Instruments, and became instead the chairman of its board of directors. On November 23, 1981, he agreed to sell the company, which was then merged with SmithKline to form SmithKline Beckman.\n\nThe Beckmans' first major philanthropic gift went to Caltech.\nIn supporting Caltech, they expanded on the long-term relationship that Beckman had begun as a student at Caltech, and continued as a teacher and trustee. In 1962, they funded the construction of a concert hall, the Beckman Auditorium, designed by architect Edward Durrell Stone. Over a period of years, they also supported the Beckman Institute, Beckman Auditorium, Beckman Laboratory of Behavioral Sciences, and Beckman Laboratory of Chemical Synthesis at the California Institute of Technology. In the words of Caltech's president emeritus David Baltimore, Beckman \"has shaped the destiny of Caltech.\" The Beckmans are also named in the Beckman Institute for Advanced Science and Technology and the Beckman Quadrangle at the University of Illinois at Urbana-Champaign.\n\nThe Arnold and Mabel Beckman Foundation was incorporated in September 1977. At the time of Beckman's death, the Foundation had given more than 400 million dollars to a variety of charities and organizations. In 1990, it was considered one of the top ten foundations in California, based on annual gifts. Donations chiefly went to scientists and scientific causes as well as Beckman's \"alma maters\". He is quoted as saying, \"I accumulated my wealth by selling instruments to scientists... so I thought it would be appropriate to make contributions to science, and that's been my number one guideline for charity.\"\n\nIn the 1980s, they funded five major centers:\n\nThe Beckmans also gave to:\n\nAfter Mabel's death in 1989, Arnold Beckman reorganized the foundation to continue in perpetuity, and developed new initiatives for the foundation's giving.\n\nA major focus became the improvement of science education. Beginning in 1998 the Foundation has provided over $23 million to support K-6 hands-on, research-based science education to school districts in Orange County, California, stimulating schools to integrate science into the K-6 curriculum as a core subject.\n\nArnold Beckman envisioned the Beckman Scholars and Beckman Young Investigators programs to support young scientists at the university level. Each year, the Beckman Foundation selects a list of universities and colleges, each of which selects student from its institution for the Beckman Scholars Program. The Beckman Young Investigators Program provides research support to promising faculty members in the early stages of academic careers in the chemical and life sciences, particularly those whose work involves methods, instruments and materials that may open up new avenues of research in science.\n\nThe Arnold and Mabel Beckman Foundation also supports vision research through its Beckman Initiative in Macular Research and the Beckman-Argyros Award in Vision Research. Supported activities include research into laser surgery and macular degeneration.\n\nArnold Beckman died May 18, 2004, at the age of 104, in hospital in La Jolla, Calif. Mabel and Arnold Beckman are buried beneath a simple headstone in West Lawn Cemetery in Cullom, Illinois, the small town where he was born.\n\nArnold Beckman was elected a Fellow of the American Academy of Arts and Sciences in 1976. Beckman was inducted into the Junior Achievement US Business Hall of Fame in 1985. In 1987, he was inducted into the National Inventors Hall of Fame in Akron, Ohio. In 2004 he received its Lifetime Achievement Award.\n\nIn 1989, Beckman received the Charles Lathrop Parsons Award for public service from the American Chemical Society.\nHe was inducted into the Alpha Chi Sigma Hall of Fame in 1996. In 2000, he received a Special Millennium Edition of the Othmer Gold Medal from the Chemical Heritage Foundation in recognition of his multifaceted contributions to chemical and scientific heritage.\n\nBeckman was awarded the National Medal of Technology in 1988. It is the highest honor the United States can confer to a US citizen for achievements related to technological progress President George H. W. Bush presented Beckman with the National Medal of Science Award in 1989, \"for his leadership in the development of analytical instrumentation and for his deep and abiding concern for the vitality of the nation's scientific enterprise.\". He had previously been recognized by the Reagan administration as one of about 30 citizens receiving the 1989 Presidential Citizens Medal for exemplary deeds of service.\n\nBeckman was awarded the Order of Lincoln, the state of Illinois' highest honor, by The Lincoln Academy of Illinois, in 1991.\n\nBeckman was awarded the Public Welfare Medal from the National Academy of Sciences in 1999.\n\nAsteroid 3737 Beckman was named after Arnold O. Beckman in 1983.\n\nThe Arnold O. Beckman High School in Irvine, California which has a focus in science education, was named in honor of Arnold O. Beckman. It was not, however, funded by Beckman.\n\nThe Beckman Coulter Heritage exhibit, which discusses the work of scientists Arnold Beckman and Wallace Coulter, is located at the Beckman Coulter headquarters in Brea, California.\n\n\n"}
{"id": "1517834", "url": "https://en.wikipedia.org/wiki?curid=1517834", "title": "Axial pen force", "text": "Axial pen force\n\nIn graphonomics, Axial pen force is the component of the normal pen force that is parallel to the pen. It is dependent upon pen tilt. In the special case of a perfectly vertical orientation of the writing instrument the axial pen force\nequals the normal pen force.\n\n\n"}
{"id": "459591", "url": "https://en.wikipedia.org/wiki?curid=459591", "title": "Beer (Martian crater)", "text": "Beer (Martian crater)\n\nBeer is a crater lying situated within the Margaritifer Sinus quadrangle (MC-19) region of the planet Mars, named in honor of the German astronomer, Wilhelm Beer. It is located at 14.4°S 351.8°E .\n\nBeer and collaborator Johann Heinrich Mädler produced the first reasonably good maps of Mars in the early 1830s. When doing so, they selected a particular feature for the prime meridian of their charts. Their choice was strengthened when Giovanni Schiaparelli used the same location in 1877 for his more famous maps of Mars. The feature was later called Sinus Meridiani (\"Middle Bay\"), but following the landing of the NASA probe MER-B Opportunity in 2004 it is perhaps better known as Meridiani Planum. Currently the Martian prime meridian is the crater Airy-0.\n\nBeer lies in the southwest of Meridiani Planum, about 8° from the prime meridian and about 10° west from the crater Mädler. Schiaparelli is also in the region.\n\n"}
{"id": "34267457", "url": "https://en.wikipedia.org/wiki?curid=34267457", "title": "Billietite", "text": "Billietite\n\nBillietite is an uncommon mineral of Uranium that contains Barium. It has the chemical formula: Ba(UO)O(OH)•8HO. It usually occurs as clear yellow orthorhombic crystals. \nBillietite is named after Valere Louis Billiet (1903–1944), Belgian crystallographer at the University of Ghent, Ghent, Belgium.\n\n"}
{"id": "1533679", "url": "https://en.wikipedia.org/wiki?curid=1533679", "title": "Cage effect", "text": "Cage effect\n\nThe cage effect in chemistry describes how the properties of a molecule are affected by its surroundings. First introduced by Franck and Rabinowitch in 1934, the cage effect suggests that instead of acting as an individual particle, molecules in solvent are more accurately described as an encapsulated particle. In order to interact with other molecules, the caged particle must diffuse from its solvent cage. The typical lifetime of a solvent cage is 10s.\n\nIn free radical polymerization, radicals formed from the decomposition of an initiator molecule are surrounded by a cage consisting of solvent and/or monomer molecules. Within the cage, the free radicals undergo many collisions leading to their recombination or mutual deactivation. This can be described in the follow reaction:\n\nformula_1\n\nAfter recombination, free radicals can either react with monomer molecules within the cage walls or diffuse out of the cage. In polymers, the probability of a free radical pair to escape recombination in the cage is 0.1 - 0.01 and 0.3-0.8 in liquids.\n\nThe cage effect can be quantitatively described as the cage recombination efficiency F where:\n\nformula_2\n\nHere F is defined as the ratio of the rate constant for cage recombination (k) to the sum of the rate constants for all cage processes. According to mathematical models, F is dependent on changes on several parameters including radical size, shape, and solvent viscosity. It is reported that the cage effect will increase with an increase in radical size and a decrease in radical mass.\n\nIn free radical polymerization, the rate of initiation is dependent on how effective the initiator is. Low initiator efficiency, ƒ, is largely attributed to the cage effect. The rate of initiation is described as:\n\nformula_3 \n\nwhere R is the rate of initiation, k is the rate constant for initiator dissociation, [I] is the initial concentration of initiator. Initiator efficiency represents the fraction of primary radicals R·, that actually contribute to chain initiation. Due to the cage effect, free radicals can undergo mutual deactivation which produces stable products instead of initiating propagation - reducing the value of ƒ.\n"}
{"id": "50900671", "url": "https://en.wikipedia.org/wiki?curid=50900671", "title": "Charles Cabrier II", "text": "Charles Cabrier II\n\nCharles Cabrier II (18th century) was a notable London clock and scientific instrument maker.\n\nCharles Cabrier II, son of Charles I and father of Charles III, was the most prominent clockmaker of the three namesakes. The Cabriers were a celebrated dynasty of Huguenot clockmakers who settled in London after the Revocation of the Edict of Nantes (1685). A relatively large number of their clocks - built over a period of half a century - have survived.\n\nApprenticed in 1719, Charles Cabrier II joined the Clockmakers' Company in 1726; he was Master from 1757 to 1772. Many examples of his work, including table clocks, carriage clocks, and pocket watches, are preserved in collections and museums in Britain and America, Germany, Sweden, and Russia.\n"}
{"id": "52194820", "url": "https://en.wikipedia.org/wiki?curid=52194820", "title": "Cinco (crater)", "text": "Cinco (crater)\n\nCinco is a small crater in the Descartes Highlands of the moon visited by the astronauts of Apollo 16. The crater is one of a group of five (hence the name, Spanish for \"five\") craters that were collectively called the Cinco craters during the Apollo 16 mission. The craters were designated \"a\", \"b\", \"c\", \"d\", and \"e\", and the largest (\"a\") was officially named \"Cinco\" after the mission in 1973 by the IAU.\n\nOn April 21, 1972, the Apollo 16 lunar module (LM) \"Orion\" landed about 4 km north of Cinco, northeast of the prominent South Ray crater. The astronauts John Young and Charles Duke explored the area over the course of three EVAs using a Lunar Roving Vehicle, or rover. They drove up Stone Mountain to Station 4, about 80 m west of Cinco crater, on EVA 2. The primary goal of sampling on Stone Mountain was to attempt to sample the Descartes Formation, although it remains unclear to this day if the Descartes was sampled.\n\n"}
{"id": "2306210", "url": "https://en.wikipedia.org/wiki?curid=2306210", "title": "Coma Supercluster", "text": "Coma Supercluster\n\nThe Coma Supercluster (SCl 117) is a nearby supercluster of galaxies comprising the Coma Cluster (Abell 1656) and the Leo Cluster (Abell 1367).\n\nLocated 300 million light-years from Earth, it is in the center of the Great Wall and a part of Coma Filament. The Coma Supercluster is the nearest massive cluster of galaxies to our own Virgo Supercluster.\n\nIt is roughly spherical, about 20 million light-years in diameter and contains more than 3,000 galaxies. It is located in the constellation Coma Berenices. Being one of the first superclusters to be discovered, the Coma Supercluster helped astronomers understand the large scale structure of the universe.\n\n"}
{"id": "10609024", "url": "https://en.wikipedia.org/wiki?curid=10609024", "title": "Deterministic context-free grammar", "text": "Deterministic context-free grammar\n\nIn formal grammar theory, the deterministic context-free grammars (DCFGs) are a proper subset of the context-free grammars. They are the subset of context-free grammars that can be derived from deterministic pushdown automata, and they generate the deterministic context-free languages. DCFGs are always unambiguous, and are an important subclass of unambiguous CFGs; there are non-deterministic unambiguous CFGs, however.\n\nDCFGs are of great practical interest, as they can be parsed in linear time and in fact a parser can be automatically generated from the grammar by a parser generator. They are thus widely used throughout computer science. Various restricted forms of DCFGs can be parsed by simpler, less resource-intensive parsers, and thus are often used. These grammar classes are referred to by the type of parser that parses them, and important examples are LALR, SLR, and LL.\n\nIn the 1960s, theoretical research in computer science on regular expressions and finite automata led to the discovery that context-free grammars are equivalent to nondeterministic pushdown automata. These grammars were thought to capture the syntax of computer programming languages. The first computer programming languages were under development at the time (see History of programming languages) and writing compilers was difficult. But using context-free grammars to help automate the parsing part of the compiler simplified the task. Deterministic context-free grammars were particularly useful because they could be parsed sequentially by a deterministic pushdown automaton, which was a requirement due to computer memory constraints. In 1965, Donald Knuth invented the LR(k) parser and proved that there exists an LR(k) grammar for every deterministic context-free language. This parser still required a lot of memory. In 1969 Frank DeRemer invented the LALR and Simple LR parsers, both based on the LR parser and having greatly reduced memory requirements at the cost of less language recognition power. The LALR parser was the stronger alternative. These two parsers have since been widely used in compilers of many computer languages. Recent research has identified methods by which canonical LR parsers may be implemented with dramatically reduced table requirements over Knuth's table-building algorithm.\n\n"}
{"id": "1393819", "url": "https://en.wikipedia.org/wiki?curid=1393819", "title": "Diamond turning", "text": "Diamond turning\n\nDiamond turning is turning with diamond as the cutting tool. It is a process of mechanical machining of precision elements using lathes or derivative machine tools (e.g., turn-mills, rotary transfers) equipped with natural or synthetic diamond-tipped tool bits. The term single-point diamond turning (SPDT) is sometimes applied, although as with other lathe work, the \"single-point\" label is sometimes only nominal (radiused tool noses and contoured form tools being options). The process of diamond turning is widely used to manufacture high-quality aspheric optical elements from crystals, metals, acrylic, and other materials. Plastic optics are frequently molded using diamond turned mold inserts. Optical elements produced by the means of diamond turning are used in optical assemblies in telescopes, video projectors, missile guidance systems, lasers, scientific research instruments, and numerous other systems and devices. Most SPDT today is done with computer numerical control (CNC) machine tools. Diamonds also serve in other machining processes, such as milling, grinding, and honing. Diamond turned surfaces have a high specular brightness and require no additional polishing or buffing, unlike other conventionally machined surfaces\n\nDiamond turning is a multi-stage process. Initial stages of machining are carried out using a series of CNC lathes of increasing accuracy. A diamond-tipped lathe tool is used in the final stages of the manufacturing process to achieve sub-nanometer level surface finishes and sub-micrometer form accuracies. The surface finish quality is measured as the peak-to-valley distance of the grooves left by the lathe. The form accuracy is measured as a mean deviation from the ideal target form. Quality of surface finish and form accuracy is monitored throughout the manufacturing process using such equipment as contact and laser profilometers, laser interferometers, optical and electron microscopes. Diamond turning is most often used for making infrared optics, because at longer wavelengths optical performance is less sensitive to surface finish quality, and because many of the materials used are difficult to polish with traditional methods.\n\nTemperature control is crucial, because the surface must be accurate on distance scales shorter than the wavelength of light. Temperature changes of a few degrees during machining can alter the form of the surface enough to have an effect. The main spindle may be cooled with a liquid coolant to prevent temperature deviations.\n\nThe diamonds that are used in the process are incredibly strong in the vertical downward direction but very weak in the upward and sideways directions.\n\nFor best possible quality natural diamonds are used as single-point cutting elements during the final stages of the machining process. A CNC SPDT lathe rests atop a high-quality granite base with micrometer surface finish quality. The granite base is placed on air suspension on a solid foundation, keeping its working surface strictly horizontal. The machine tool components are placed on top of the granite base and can be moved with high degree of accuracy using a high-pressure air cushion or hydraulic suspension. The machined element is attached to an air chuck using negative air pressure and is usually centered manually using a micrometer. The chuck itself is separated from the electric motor that spins it by another air suspension.\n\nThe cutting tool is moved with sub-micron precision by a combination of electric motors and piezoelectric actuators. As with other CNC machines, the motion of the tool is controlled by a list of coordinates generated by a computer. Typically, the part to be created is first described using a CAD model, then converted to G-code using a CAM program, and the G-code is then executed by the machine control computer to move the cutting tool. The final surface is achieved with a series of cutting passes of decreasing depth.\n\nAlternative methods of diamond machining in practice also include diamond fly cutting and diamond milling. Diamond fly cutting can be used to generate diffraction gratings and other linear patterns with appropriately contoured diamond shapes. Diamond milling can be used to generate aspheric lens arrays by annulus cutting methods with a spherical diamond tool.\n\nDiamond turning is specifically useful when cutting materials that are viable as infrared optical components and certain non-linear optical components such as KDP. KDP is a perfect material in application for diamond turning, because the material is very desirable for its optical modulating properties, yet it is impossible to make optics from this material using conventional methods. KDP is water-soluble, so conventional grinding and polishing techniques are not effective in producing optics. Diamond turning works well to produce optics from KDP.\n\nGenerally, diamond turning is restricted to certain materials. Materials that are readily machinable include:\n\n\nThe most often requested materials that are not readily machinable are:\n\nFerrous materials are not readily machinable because the carbon in the diamond tool chemically reacts with the substrate, leading to tool damage and dulling after short cut lengths. Several techniques have been investigated to prevent this reaction, but few have been successful for long diamond machining processes at mass production scales. \n\nTool life improvement has been under consideration in diamond turning as the tool is expensive. Hybrid processes such as laser-assisted machining have emerged in this industry recently.\nThe laser softens hard and difficult-to-machine materials such as ceramics and semiconductors, making them easier to cut.\n\nDespite all the automation involved in the diamond turning process, the human operator still plays the main role in achieving the final result. Quality control is a major part of the diamond turning process and is required after each stage of machining, sometimes after each pass of the cutting tool. If it is not detected immediately, even a minute error during any of the cutting stages results in a defective part. The extremely high requirements for quality of diamond-turned optics leave virtually no room for error.\n\nThe SPDT manufacturing process produces a relatively high percentage of defective parts, which must be discarded. As a result, the manufacturing costs are high compared to conventional polishing methods. Even with the relatively high volume of optical components manufactured using the SPDT process, this process cannot be classified as mass production, especially when compared with production of polished optics. Each diamond-turned optical element is manufactured on an individual basis with extensive manual labor.\n\n"}
{"id": "1013999", "url": "https://en.wikipedia.org/wiki?curid=1013999", "title": "Engel's law", "text": "Engel's law\n\nEngel's law is an observation in economics stating that as income rises, the proportion of income spent on food falls, even if absolute expenditure on food rises. In other words, the income elasticity of demand of food is between 0 and 1.\n\nThe law was named after the statistician Ernst Engel (1821–1896).\n\nEngel's law does not imply that food spending remains unchanged as income increases: It suggests that consumers increase their expenditures for food products in percentage terms less than their increases in income.\n\nOne application of this statistic is treating it as a reflection of the living standard of a country. As this proportion — or \"Engel coefficient\" — increases, the country is by nature poorer; conversely a low Engel coefficient indicates a higher standard of living.\n\nThe interaction between Engel's law, technological progress, and the process of structural change is crucial for explaining long term economic growth as suggested by Leon, and Pasinetti.\n\n"}
{"id": "837770", "url": "https://en.wikipedia.org/wiki?curid=837770", "title": "Entropy of fusion", "text": "Entropy of fusion\n\nThe entropy of fusion is the increase in entropy when melting a substance. This is almost always positive since the degree of disorder increases in the transition from an organized crystalline solid to the disorganized structure of a liquid; the only known exception is helium. It is denoted as formula_1 and normally expressed in J mol K\n\nA natural process such as a phase transition will occur when the associated change in the Gibbs free energy is negative. \nSince this is a thermodynamic equation, the symbol T refers to the absolute thermodynamic temperature, measured in kelvins (K).\nEquilibrium occurs when the temperature is equal to the melting point formula_4 so that \n\nand the entropy of fusion is the heat of fusion divided by the melting point.\n\nHelium-3 has a negative entropy of fusion at temperatures below 0.3 K. Helium-4 also has a very slightly negative entropy of fusion below 0.8 K. This means that, at appropriate constant pressures, these substances freeze with the addition of heat.\n\n\n"}
{"id": "418736", "url": "https://en.wikipedia.org/wiki?curid=418736", "title": "Family Re-Union", "text": "Family Re-Union\n\nFamily Re-Union is an annual conference, hosted by former Vice President of the United States Al Gore and Tipper Gore, in Nashville, Tennessee whose goal is to bring together families and those who work with them to talk and design better ways to strengthen family life in America. At the center of Family Re-Union is the belief that programs and guidelines should respond to the needs of families and communities, and should build on their strengths.\n\n"}
{"id": "361082", "url": "https://en.wikipedia.org/wiki?curid=361082", "title": "Gallery of sovereign state flags", "text": "Gallery of sovereign state flags\n\nThis gallery of sovereign state flags shows the flags of sovereign states that appear on the list of sovereign states. For other flags, please see , , and gallery of flags of dependent territories. Each flag is depicted as if the flagpole is positioned on the left of the flag, except for those of Iran, Iraq, and Saudi Arabia which are depicted with the hoist to the right.\n\n"}
{"id": "43769798", "url": "https://en.wikipedia.org/wiki?curid=43769798", "title": "Gene co-expression network", "text": "Gene co-expression network\n\nA gene co-expression network (GCN) is an undirected graph, where each node corresponds to a gene, and a pair of nodes is connected with an edge if there is a significant co-expression relationship between them. Having gene expression profiles of a number of genes for several samples or experimental conditions, a gene co-expression network can be constructed by looking for pairs of genes which show a similar expression pattern across samples, since the transcript levels of two co-expressed genes rise and fall together across samples. Gene co-expression networks are of biological interest since co-expressed genes are controlled by the same transcriptional regulatory program, functionally related, or members of the same pathway or protein complex.\n\nThe direction and type of co-expression relationships are not determined in gene co-expression networks; whereas in a gene regulatory network (GRN) a directed edge connects two genes, representing a biochemical process such as a reaction, transformation, interaction, activation or inhibition. Compared to a GRN, a GCN does not attempt to infer the causality relationships between genes and in a GCN the edges represent only a correlation or dependency relationship among genes. Modules or the highly connected subgraphs in gene co-expression networks correspond to clusters of genes that have a similar function or involve in a common biological process which causes many interactions among themselves.\n\nGene co-expression networks are usually constructed using datasets generated by high-throughput gene expression profiling technologies such as Microarray or RNA-Seq.\n\nThe concept of gene co-expression networks was first introduced by Butte and Kohane in 1999 as \"relevance networks\". They gathered the measurement data of medical laboratory tests (e.g. hemoglobin level ) for a number of patients and they calculated the Pearson correlation between the results for each pair of tests and the pairs of tests which showed a correlation higher than a certain level were connected in the network (e.g. insulin level with blood sugar). Bute and Kohane used this approach later with mutual information as the co-expression measure and using gene expression data for constructing the first gene co-expression network.\n\nA good number of methods have been developed for constructing gene co-expression networks. In principle, they all follow a two step approach: calculating co-expression measure, and selecting significance threshold. In the first step, a co-expression measure is selected and a similarity score is calculated for each pair of genes using this measure. Then, a threshold is determined and gene pairs which have a similarity score higher than the selected threshold are considered to have a significant co-expression relationship and are connected by an edge in the network.\n\nThe input data for constructing a gene co-expression network is often represented as a matrix. If we have the gene expression values of \"m\" genes for \"n\" samples (conditions), the input data would be an \"m×n\" matrix, called expression matrix. For instance, in a microarray experiment the expression values of thousands of genes are measured for several samples. In first step, a similarity score (co-expression measure) is calculated between each pair of rows in expression matrix. The resulting matrix is an \"m×m\" matrix called the similarity matrix. Each element in this matrix shows how similarly the expression levels of two genes change together. In the second step, the elements in the similarity matrix which are above a certain threshold (i.e. indicate significant co-expression) are replaced by 1 and the remaining elements are replaced by 0. The resulting matrix, called the adjacency matrix, represents the graph of the constructed gene co-expression network. In this matrix, each element shows whether two genes are connected in the network (the 1 elements) or not (the 0 elements).\n\nThe expression values of a gene for different samples can be represented as a vector, thus calculating the co-expression measure between a pair of genes is the same as calculating the selected measure for two vectors of numbers.\n\nPearson's correlation coefficient, Mutual Information, Spearman's rank correlation coefficient and Euclidean distance are the four mostly used co-expression measures for constructing gene co-expression networks. Euclidean distance measures the geometric distance between two vectors, and so considers both the direction and the magnitude of the vectors of gene expression values. Mutual information measures how much knowing the expression levels of one gene reduces the uncertainty about the expression levels of another. Pearson’s correlation coefficient measures the tendency of two vectors to increase or decrease together, giving a measure of their overall correspondence. Spearman's rank correlation is the Pearson’s correlation calculated for the ranks of gene expression values in a gene expression vector. Several other measures such as partial correlation, regression, and combination of partial correlation and mutual information have also been used.\n\nEach of these measures have their own advantages and disadvantages. The Euclidean distance is not appropriate when the absolute levels of functionally related genes are highly different. Furthermore, if two genes have consistently low expression levels but are otherwise randomly correlated, they might still appear close in Euclidean space. One advantage to mutual information is that it can detect non-linear relationships; however this can turn into a disadvantage because of detecting sophisticated non-linear relationships which does not look biologically meaningful. In addition, for calculating mutual information one should estimate the distribution of the data which needs a large number of samples for a good estimate. Spearman’s rank correlation coefficient is more robust to outliers, but on the other hand it is less sensitive to expression values and in datasets with small number of samples may detect many false positives.\n\nPearson’s correlation coefficient is the most popular co-expression measure used in constructing gene co-expression networks. The Pearson's correlation coefficient takes a value between -1 and 1 where absolute values close to 1 show strong correlation. The positive values correspond to an activation mechanism where the expression of one gene increases with the increase in the expression of its co-expressed gene and vice versa. When the expression value of one gene decreases with the increase in the expression of its co-expressed gene, it corresponds to an underlying suppression mechanism and would have a negative correlation.\n\nThere are two disadvantages for Pearson correlation measure: it can only detect linear relationships and it is sensitive to outliers. Moreover, Pearson correlation assumes that the gene expression data follow a normal distribution. Song et al. have suggested \"biweight midcorrelation (bicor)\" as a good alternative for Pearson’s correlation. \"Bicor is a median based correlation measure, and is more robust than the Pearson correlation but often more powerful than the Spearman's correlation\". Furthermore, it has been shown that \"most gene pairs satisfy linear or monotonic relationships\" which indicates that \"mutual information networks can safely be replaced by correlation networks when it comes to measuring co-expression relationships in stationary data\".\n\nSeveral methods have been used for selecting a threshold in constructing gene co-expression networks. A simple thresholding method is to choose a co-expression cutoff and select relationships which their co-expression exceeds this cutoff. Another approach is to use Fisher’s Z-transformation which calculates a z-score for each correlation based on the number of samples. This z-score is then converted into a p-value for each correlation and a cutoff is set on the p-value. Some methods permute the data and calculate a z-score using the distribution of correlations found between genes in permuted dataset. Some other approaches have also been used such as threshold selection based on clustering coefficient or random matrix theory.\n\nThe problem with p-value based methods is that the final cutoff on the p-value is chosen based on statistical routines(e.g. a p-value of 0.01 or 0.05 is considered significant), not based on a biological insight.\n\nWGCNA is a framework for constructing and analyzing weighted gene co-expression networks. The WGCNA method selects the threshold for constructing the network based on the scale-free topology of gene co-expression networks. This method constructs the network for several thresholds and selects the threshold which leads to a network with scale-free topology. Moreover, the WGCNA method constructs a weighted network which means all possible edges appear in the network, but each edge has a weight which shows how significant is the co-expression relationship corresponding to that edge.\n\n"}
{"id": "34608703", "url": "https://en.wikipedia.org/wiki?curid=34608703", "title": "George Hogben", "text": "George Hogben\n\nGeorge Hogben CMG (14 July 1853 – 26 April 1920) was a New Zealand educationalist and seismologist. He was born in Islington, Middlesex, England on 14 July 1853, and died after a short illness at home in Khandallah, Wellington . He was Inspector-General of Schools in New Zealand and was appointed CMG in the 1915 New Year Honours.\n\n"}
{"id": "16794316", "url": "https://en.wikipedia.org/wiki?curid=16794316", "title": "Geroch energy", "text": "Geroch energy\n\nThe Geroch energy or Geroch mass is one of the possible definitions of mass in general relativity. It can be derived from the Hawking energy, itself a measure of the bending of ingoing and outgoing rays of light that are orthogonal to a 2-sphere surrounding the region of space whose mass is to be defined, by leaving out certain (positive) terms related to the sphere's external and internal curvature.\n\n\n"}
{"id": "54986003", "url": "https://en.wikipedia.org/wiki?curid=54986003", "title": "Gillian Einstein", "text": "Gillian Einstein\n\nGillian Einstein is a faculty member at the Dalla Lana School of Public Health and the Department of Psychology at the University of Toronto, and holder of the inaugural Wilfred and Joyce Posluns Chair in Women's Brain Health and Aging.\n\nEinstein was born in New York City, US. As her father was a member of the U.S. Air Force, her family moved between New York City, Texas and Massachusetts. She completed a Bachelor of Arts degree in the History of Art at Harvard University. Einstein then earned her PhD in 1984, working on neuroanatomy at the University of Pennsylvania.\n\nEinstein joined Duke University in 1989 as an assistant professor and moved to the Centre for Research on Women’s Health at Women's College Hospital in 2004.\n\nIn 2006, Einstein established the University of Toronto’s Collaborative Graduate Program in Women's Health (now known as the Collaborative Specialization in Women's Health). This program is based at the Dalla Lana School of Public Health, and is affiliated with the Women's College Research Institute (based at the Women's College Hospital). Einstein led the program as a Director from 2006 to 2016.\n\nIn 2007, Einstein edited \"Sex and the Brain\" for MIT Press.\n\nEinstein's work is focussed on women's health, specifically, the anatomy of the female brain. With 72% of Canadian Alzheimer's sufferers being represented by women, Einstein explores why brain disorders like Alzheimer's disproportionately affect women. Einstein's research looks at the relationship between early menopause and decreased estrogen levels, and how this may negatively affect cognition. Further to the differences that gender may play on the human brain, Einstein further posits that the social and cultural context that accompanies being female or male can also have a significant effect on our biology.\n\nEinstein has been outspoken on the subject of gender disparity in clinical health research. She participated in the scientific discussion that led to the US National Institutes of Health to form policies requiring even gender distribution in cell and animal studies.\n\nIn 2016, Einstein was awarded the inaugural Wilfred and Joyce Posluns Chair in Women's Brain Health and Aging. This chair is supported through the Wilfred Posluns' Family Foundation, the Canadian Institutes of Health Research, Alzheimer Society of Canada, and the Ontario Brain Institute.\n\nGillian Einstein is a distant cousin of Albert Einstein.\n"}
{"id": "14260233", "url": "https://en.wikipedia.org/wiki?curid=14260233", "title": "Graduate Women in Science", "text": "Graduate Women in Science\n\nGraduate Women in Science (GWIS), formerly known as Sigma Delta Epsilon Graduate Women in Science (SDE-GWIS), is an international organization for women in science, first established in 1921 at Cornell University in Ithaca, New York, United States. The organization currently has over 1,000 members and dozens of chapters spread across the United States as well as an international chapter that was established in 2013.\n\nThe organization is a non-profit 501(c)(3) organization that works to connect, lead, and empower women in science. It does so through offering grants, awards, and fellowships; cultivating a powerful international network of women scientists; holding annual conferences and sponsoring additional meetings and symposia; publishing a free monthly newsletter; and promoting the participation and representation of women in science-related events. Membership is open to anyone, regardless of sex, who has at least a bachelor's degree in a scientific discipline.\n\nThe GWIS National Meeting is held annually in June and is hosted by a local chapter.\n\nBuilding a global community to inspire, support, recognize, and empower women in science.\n\nGWIS was established in 1921 at Cornell University by a group of women pursuing graduate degrees in the sciences, interested in the status of women in scientific fields, and interested in building a social network for women in science. Initially, the organization was formally named the Sigma Delta Epsilon Graduate Women's Scientific Fraternity—SDE for short. In 1922, SDE joined forces with women at the University of Wisconsin-Madison, voting and accepting its National Constitution, thus establishing the organization as a national organization. The first National Convention was held in 1922. In 1931, GWIS established its Formal Fellowships Fund, and the First Research Fellowship was awarded in 1941 to Frances Dorris-Humm, PhD at Yale University studying experimental embryology. In 1970, the Eloise Gerry Fellowship Fund, the first of the GWIS fellowships funded by a single individual rather than by fundraising and small membership contributions, was established.\n\n\n"}
{"id": "20016887", "url": "https://en.wikipedia.org/wiki?curid=20016887", "title": "History of the diesel car", "text": "History of the diesel car\n\nDiesel engines began to be used in automobiles in the 1930s. Mainly used for commercial applications early on, they did not gain popularity for passenger travel until their development in Europe in the 1960s. Diesel cars continue to develop into highly desired, high performance while retaining power and performance. In terms of pollution, diesel engines generally produce less carbon dioxide than gasoline-based engines but produce more nitrous oxides \nCharles Wallace Chapman at Perkins Engines at Peterborough, England, developed an engine, the high speed diesel engine, for automobiles; previously diesel engines were too large and heavy.\n\nProduction diesel car history started in 1933 with Citroën's Rosalie, which featured a diesel engine option (the 1,766 cc 11UD engine) in the Familiale (estate or station wagon) model. The Mercedes-Benz 260D and the Hanomag Rekord were introduced in 1936.\n\nImmediately after World War II, and throughout the 1950s and 1960s, diesel-powered cars began to gain limited popularity, particularly for commercial applications, such as ambulances, taxis, and station wagons used for delivery work. Most were conventional in design. Mercedes-Benz offered a continuous stream of diesel-powered taxis, beginning in 1949 with their 170D powered by the OM-636 engine. Later, in 1959 their OM-621 engine was introduced in the 180D. This 2.0 L engine produced at 4,350 rpm. Beginning in 1959, Peugeot offered the 403D with their TMD-85 four-cylinder engine of 1.8 L and , followed in 1962 by the 404D with the same engine. In 1964, the 404D became available with the improved XD88 four-cylinder engine of 2.0 L and . Other cars available with diesel power during this era included the Austin A60 Cambridge, Isuzu Bellel, Fiat 1400-A, Standard Vanguard, Borgward Hansa, and a few others.\n\nIn 1967, Peugeot introduced the world's first compact, high-speed diesel car, the Peugeot 204BD. Its 1.3 L XL4D engine produces at 5,000 rpm. Following the 1970s oil crisis (1973 and 1979), Volkswagen introduced their first diesel, the VW Golf, with a 1.5 L naturally aspirated indirect-injection engine which was a redesigned (dieselised) version of a gasoline engine. Mercedes-Benz tested turbodiesels in cars (e.g. by the Mercedes-Benz C111 experimental and record-setting vehicles) and the first production turbo diesel cars were, in 1978, the 3.0 5-cylinder 115 hp (86 kW) Mercedes 300 SD, available only in North America, and the Peugeot 604.\n\nThe biggest single step forward for mass-market diesel cars came in 1982 when PSA Peugeot Citroën introduced the XUD engine in the Peugeot 305, Peugeot 205 and Talbot Horizon. This was the class leading automotive diesel engine until the mid-1990s. The first mass market turbo diesel was the XUD powered, 1988 Citroën BX and then the 1989 Peugeot 405, they gave power and refinement approaching petrol engine standards, with the best chassis in their class. Diesel Car magazine said of the Citroën BX \"We can think of no other car currently on sale in the UK that comes anywhere near approaching the BX Turbo's combination of performance, accommodation and economy\". These were the cars that started the diesel boom in Europe that has now hit 50% of the market in new car sales.\n\nDiesels carried a 2.5% share of the European Community market in 1973. Following the fuel crisis, this share increased to 4.1% in 1975. This more than doubled (to 8.6%) by 1980, and by 1983 diesels represented 11% of new car sales in the EU.\n\nDiesel Powered Passenger Cars in North America have been steadily increasing in popularity, especially in the early-mid 2000s. Diesels have typically only been used in trucks and commercial vehicles and buses. Jeep had offered a Perkins Diesel option for its models in the early 1960s and Chrysler offered them as well although mainly for the European market though some have probably been retrofitted to older AMC and Chrysler models in the US, particularly as taxis. Oldsmobile released a 350 cubic Inch (5.7 Litre) V8 diesel Engine starting in the late 1970s, most Buick, Oldsmobile, Pontiac, Chevrolet and even Cadillac divisions of General Motors, had received this engine by the 1980 model year, and continued to be sold until the engine was discontinued in 1985. Whilst a very good idea at the time, owing to the recent 1979 energy crisis these engines had gained a terrible reputation for reliability. Part of this had to do with the fact that, whilst the engine block itself was indeed a diesel specific design (contrary to most people's belief that it was simply a dieselised version of the famous rocket V8), GM management made significant cost saving measures by using many parts from the gas V8 counterpart which were not up to the task of diesel engine operation, specifically the number and type of head bolts used, and the lack of a water and fuel separator in the fuel line, a big issue as a lot of diesel fuel contained water and also high levels of sulfur at the time. The engine was later revised into the DX block and most of the issues were fixed, a V6 version displacing 4.3 litres and a smaller 4.3 V8 were also offered around this time, mainly for the downsized front-wheel drive models of the mid-1980s and although they were much better in the reliability department, the damage had already been done and the North American diesel car market was severely crippled as a result, despite the best efforts by Mercedes-Benz, Audi, BMW, Peugeot, VW and others in offering their more refined and powerful diesel engines. All of these olds designed units were non-turbocharged (or natural-aspirated) diesels. This undoubtedly hampered them in the performance and efficiency departments, although there have been retrofits of turbochargers to Olds diesel blocks which are carried out by enthusiasts and collectors of these cars. It isn't well proven if turbos are reliable long-term with these engines, but so far they seem to be suitable, however, what performance benefit they bring could be considered as dubious given how low their power output was to begin with.\n\nDiesels steadily gained in acceptance with private buyers in the 1970s and into the 1990s. Having originally been mainly marketed to commercial users such as taxi drivers, European diesel sales increased steadily and reached 17.3 per cent of the overall European market by 1992.\n\nMany Audi enthusiasts claim that the Audi 100 TDI was the first turbo charged direct injection diesel sold in 1989, but actually it isn't true, as the Fiat Croma TD-i.d. was sold with turbo direct injection in 1987 and one year later Austin Rover Montego. What was pioneering about the Audi 100, however, was the use of electronic control of the engine, as the Fiat and Austin had Bosch mechanically controlled injection. In the writers experience the Audi 100 TDI was very troublesome and the engine code \"AAT\" is often known as the difficult Audi . The electronic control of direct injection really made a difference in terms of emissions, refinement and power. All earlier generation car direct injection diesel engines benefit greatly from the use of biodiesel fuel, which reduces emissions and greatly improves refinement without engine modifications, provided they use compatible 'Viton' type rubber in their fuel systems.\n\nThe diesel car markets are the same ones who pioneered various developments (Mercedes-Benz, BMW, Peugeot/Citroën, Fiat, Alfa Romeo, Volkswagen Group) There were also small diesel engines produced in England by British Leyland and Perkins. For reasons of economy the petrol BMC \"B\" series engine was dieselised and produced in capacities of 1.5 and 1.8 litres. Perkins produced the 4.99, 4.107 and 4.108 engines all of which were extremely reliable. Later BL produced the five main bearing \"O\" series engine which was extremely strong. Petrol turbo variants could make 200HP and the engine was ideal for converting to a diesel. In fact, the 1988 Austin-Rover MDi unit (also known as the 'Perkins Prima') was developed by Perkins Engines of Peterborough, who have designed and built high-speed diesels since the 1930s. It is still in production as a marine engine however in the writers opinion timing belts at sea are not a good idea. Engines that rely on timing belts are more suited to inland waterways. It is not easy to make a lightweight and powerful top class diesel engine owing to the immense pressures and heat produced within the engine. These problems were solved by VM Motori of Cento and the engines were apparently so good that Rover, Ford and Jeep bought them. The interesting features of the engines were the tunnel-bore block and separate cylinder heads to allow for expansion. VM engines were marinised by BMW and sold as BMW stern-drive packages. Mercury Marine also used the VM engines. As they have aged in car applications, they have developed a reputation for blowing head gaskets, due to the separate cylinder head design.\n\nIn 1997, the first common rail diesel passenger car was introduced, the Alfa Romeo 156.\n\nIn 1998, for the very first time in the history of racing, in the legendary 24 Hours Nürburgring race, a diesel-powered car was the overall winner: the BMW works team 320d, a BMW E36 fitted with modern high-pressure diesel injection technology from Robert Bosch GmbH. The low fuel consumption and long range, allowing 4 hours of racing at once, made it a winner, as comparable petrol-powered cars spent more time refueling.\n\nChrysler put a VM Motori SpA engine in the Jeep Liberty sport utility vehicle in 2005 and 2006 to assess the American market's interest in modern high performance diesel engines.\n\nIn 2004 Honda released their first diesel engine, the N22A branded as the i-CTDI, it first featured in the Honda Accord. The engine featured an aluminium block, DOHC chain driven valvetrain, common rail direct injection and variable geometry turbocharger.\nIn Spring 2005, Mercedes-Benz unveiled their first application of a mass-produced aluminum block diesel engine for passenger vehicles and commercial use. While aluminum is traditionally considered of inferior strength and temperature resistance to withstand diesel applications, Mercedes engineers made extensive use of CAD/CAM design to arrive at an aluminum block that would meet with Mercedes' testing and reliability standards. First use was in 2006 model-year vehicles in the E-Class sedan and ML-class and GL-class SUVs. Similar in weight () to the five-cylinder it replaced, and considerably lighter than the in-line six-cylinder it also replaced, this 3.0L V-6 produces 165 kW (224 hp) at 3,800 rpm and max torque of 510 Nm (376 ft·lbf) at 1,600-2,800 rpm and makes use of a four-valve head. Additionally, fitment of Mercedes-Benz BlueTec system, a concert of emissions control strategies, renders this new diesel 50-state legal in the U.S. beginning in 2008 (stringent NOx limits have made U.S. passenger-car diesels unpopular or impossible in parts of the U.S. in recent years).\n\nIn 2006, the new Audi R10 TDI LMP1 entered by Joest Racing became the first diesel-engined car to win the 24 Hours of Le Mans. The winning car also bettered the post-1990 course configuration lap record by 1, at 380 laps. However, this fell short of the all-time distance record set in 1971 by over .\n\nThe Subaru car company of Japan is preparing to sell its station wagon version of their Legacy mid-size car (called the Subaru Outback in North America) with a 2.0-litre, boxer engine format opposed-four-cylinder diesel engine of power, and of torque, in the United Kingdom. Sales in continental Europe started in 2008.\n\nSince the numerous Diesel emissions scandals of recent years the most high profile of which was the Volkswagen emissions scandal of 2015 it has been revealed that the levels of toxic emissions coming from diesel cars are higher and pose a greater risk to human health than those of vehicles powered by other means.\nAlthough the weight and lower output of a diesel engine tend to keep them away from automotive racing applications, there are many diesels being raced in classes that call for them, mainly in truck racing and tractor pulling, as well in types of racing where these drawbacks are less severe, such as land speed record racing or endurance racing. Even diesel-engined dragsters exist, despite the diesel's drawbacks of weight and low peak rpm, specifications central to performance in this sport. However, in 2006, the new Audi R10 TDI LMP1 entered by Joest Racing became the first diesel-engined car to win the 24 Hours of Le Mans.\n\nAs early as 1931, Clessie Cummins installed his diesel in the Cummins \"Diesel Special\" race car, hitting at Daytona and at the Indianapolis 500 race, where Dave Evans became the first driver to complete the Indianapolis 500 without making a single pit stop, completing the full distance on the lead lap and finishing 13th, relying on torque and fuel efficiency to overcome weight and low peak power.\n\nIn 1933, a 1925 Bentley with a Gardner 4LW engine was the first diesel-engine car to take part in the Monte Carlo Rally when it was driven by Lord Howard de Clifford. It was the leading British car and finished fifth overall.\n\nIn 1952, Fred Agabashian in a Cummins diesel won the pole at the Indianapolis 500 race with a turbocharged 6.6-liter diesel car, setting a record for pole position lap speed, . Don Cummins and his chief engineer Neve Reiners recognized that the low center of gravity of the flat engine configuration (designed to lie beneath the floor of a bus) plus the power advantage gained by the novel use of Elliott turbocharging would be a winning combination.\n\nAt the start, a slow pace lap (reportedly less than ) apparently induced what is now referred to as \"turbo lag\" and badly hampered the throttle response of the Cummins Diesel. Although Agabashian found himself in eighth place before reaching the first turn, he moved up to fifth in a few laps and was running competitively (albeit well back in the field after a tire change) until the badly situated air intake of the car swallowed enough debris from the track to disable the turbocharger at lap 71; he finished 27th.\n\nIn the 1990s and rule makers supported the concept, BMW and Volkswagen raced diesel touring cars, with BMW winning the 1998 24 Hours Nürburgring with a 320d against other factory-entered diesel competition of VW and about 200 normally powered cars, mainly by being able to drive very long stints. Alfa Romeo even organized a racing series with their Alfa Romeo 147 1.9 JTD models.\n\nIn 2006, a BMW 120d repeated a similar result, scoring 5th in a field of 220 cars, many of them much more powerful, a significantly stronger competition than in 1998. The VW Dakar Rally race Touareg for 2005 and 2006 are powered by their own line of TDI engines in order to challenge for the first overall diesel win there.\n\nMeanwhile, the five time 24 Hours of Le Mans winner Audi R8 race car was replaced by the Audi R10 TDI in 2006, which is powered by a and V12 TDI common rail diesel engine, mated to a 5-speed gearbox, instead of the 6 used in the R8, to handle the extra torque produced. The gearbox is considered the main problem, as earlier attempts by others failed due to the lack of suitable transmissions that could stand the torque long enough.\n\nAfter winning the 12 Hours of Sebring in 2006 with their diesel-powered R10 TDI, Audi obtained the overall win at the 2006 24 Hours of Le Mans, too. This is the first time a sports car could compete for overall victories with diesel fuel against cars powered with regular fuel or methanol and bio-ethanol. However, the significance of this is slightly lessened by the fact that the ACO/ALMS race rules encourage the use of alternative fuels such as diesel.\n\nAudi again triumphed at Sebring in 2007. It had both a speed and fuel economy advantage over the entire field including the Porsche RS Spyders, gasoline powered purpose-built race cars. Audi's diesels won again the 2007 24 Hours of Le Mans, against competition coming from the Peugeot 908 HDi FAP diesel powered racer.\n\nIn 2006, the JCB Dieselmax broke the diesel land speed record posting an average speed of over . The vehicle used \"two diesel engines that have a combined total of 1,500 horsepower (1120 kilowatts). Each is a 4-cylinder, 4.4-liter engine used commercially in a backhoe loader.\" \n\nIn the 2008 BTCC, Jason Plato and Darren Turner are racing factory sponsored SEAT Leon TDI with some success against a variety of gasoline powered competitors.\n\n\n"}
{"id": "1925184", "url": "https://en.wikipedia.org/wiki?curid=1925184", "title": "Hot mirror", "text": "Hot mirror\n\nA hot mirror is a specialized dielectric mirror, a dichroic filter, often employed to protect optical systems by reflecting infrared light back into a light source, while allowing visible light to pass. Hot mirrors can be designed to be inserted into the optical system at an incidence angle varying between zero and 45 degrees, and are useful in a variety of applications where the buildup of waste heat can damage components or adversely affect spectral characteristics of the illumination source. Wavelengths reflected by an infrared hot mirror range from about 750 to 1250 nanometers. By transmitting visible light wavelengths while reflecting infrared, hot mirrors can also serve as dichromatic beam splitters for specialized applications in fluorescence microscopy or optical eye tracking.\n\nSome early digital cameras designed for visible light capture, such as the Associated Press NC2000 and Nikon Coolpix 950, were unusually sensitive to infrared radiation, and tended to produce colours that were contaminated with infrared. This was particularly problematic with scenes that contained strong sources of infrared, such as fires, although the effect could be moderated by inserting a photographic hot mirror filter into the imaging pathway. Conversely, these cameras could be used for infrared photography by inserting a cold mirror filter into the imaging pathway, most commonly by mounting the filter on the front of the lens.\n\nNew incandescent bulbs incorporate hot mirrors, increasing efficiency by redirecting unwanted infrared frequencies back to the filament.\n"}
{"id": "90075", "url": "https://en.wikipedia.org/wiki?curid=90075", "title": "Index of economics articles", "text": "Index of economics articles\n\nThis aims to be a complete article list of economics topics:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[Category:Economics|*Index]]\n[[Category:Economics lists|*]]\n[[Category:Indexes of business topics|Economics]]\n[[Category:Wikipedia indexes|Economics]]"}
{"id": "43422362", "url": "https://en.wikipedia.org/wiki?curid=43422362", "title": "Joel Dudley", "text": "Joel Dudley\n\nJoel Dudley is currently Associate Professor of Genetics and Genomic Sciences and founding Director of the Institute for Next Generation Healthcare at the Icahn School of Medicine at Mount Sinai. In March, 2018 Dr. Dudley was named Executive Vice President for Precision Health for the Mount Sinai Health System (MSHS). In 2017 he was awarded an Endowed Professorship by Mount Sinai in Biomedical Data Science. Prior to Mount Sinai, he held positions as Co-founder and Director of Informatics at NuMedii, Inc. and Consulting Professor of Systems Medicine in the Department of Pediatrics at Stanford University School of Medicine. His work is focused at the nexus of -omics, digital health, artificial intelligence (AI), scientific wellness, and healthcare delivery. His work has been featured in the Wall Street Journal, Scientific American, MIT Technology Review, CNBC, and other popular media outlets. He was named in 2014 as one of the 100 most creative people in business by Fast Company magazine. He is co-author of the book Exploring Personal Genomics from Oxford University Press. Dr. Dudley received a BS in Microbiology from Arizona State University and an MS and PhD in Biomedical Informatics from Stanford University School of Medicine.\n\n\n\n\nGoogle Scholar Citations\n\nhttps://scholar.google.com/citations?user=206DEM0AAAAJ\n\n"}
{"id": "876948", "url": "https://en.wikipedia.org/wiki?curid=876948", "title": "Korabl-Sputnik 3", "text": "Korabl-Sputnik 3\n\nKorabl-Sputnik 3 ( meaning \"Ship-Satellite 3\") or Vostok-1K No.3, also known as Sputnik 6 in the West, was a Soviet spacecraft which was launched in 1960. It was a test flight of the Vostok spacecraft, carrying two dogs; Pcholka and Mushka (\"little bee\" and \"little fly\"; affectionate diminutives of \"pchela\" and \"mukha\", respectively), as well as a television camera and scientific instruments.\n\nKorabl-Sputnik 3 was launched at 07:30:04 UTC on 1 December 1960, atop a Vostok-L carrier rocket flying from Site 1/5 at the Baikonur Cosmodrome. It was successfully placed into low Earth orbit. The flight lasted one day, after which the spacecraft was deorbited ahead of its planned recovery. The deorbit burn began at 07:15 UTC on 2 December, however the engine did not cut off as planned at the end of the burn, and instead the spacecraft's fuel burned to depletion. This resulted in it reentering the atmosphere on a trajectory which might have permitted foreign powers to inspect the capsule. To prevent this, an explosive charge was detonated during reentry. Both Pchyolka and Mushka were killed in the resulting disintegration. They were the last dogs to die in a Soviet space mission, after Laika, who was never intended to survive her Sputnik 2 flight, and Chaika and Lisichka, perishing after the rocket carrying their \"Korabl Sputnik\" spacecraft disintegrated 20 seconds into the flight.\n\nThree weeks later, a follow-up launch to Korabl-Sputnik 3 also failed. On 22 December, the dogs Damka and Krasavka lifted off from LC-1 using a booster with an enhanced Blok E stage which produced more thrust than the previous model. This variant (the 8K72K) would be used for manned Vostok launches. The strap-ons and core stage of the booster performed normally, but the untested new Blok E failed a few seconds after ignition when the gas generator malfunctioned. Since the engine could not produce sufficient thrust to achieve orbital velocity, the mission had to be aborted. The Vostok was ejected with the Blok E still firing and the dogs subjected to a rough ballistic reentry in freezing winter weather. Recovery crews frantically searched for the descent module before its automatic self-destruct mechanism activated, and after some hours, succeeded in disarming the mechanism and retrieving the dogs, which were returned to Baikonour Cosmodrome alive.\n"}
{"id": "291472", "url": "https://en.wikipedia.org/wiki?curid=291472", "title": "Lattice model (physics)", "text": "Lattice model (physics)\n\nIn physics, a lattice model is a physical model that is defined on a lattice, as opposed to the continuum of space or spacetime. Lattice models originally occurred in the context of condensed matter physics, where the atoms of a crystal automatically form a lattice. Currently, lattice models are quite popular in theoretical physics, for many reasons. Some models are exactly solvable, and thus offer insight into physics beyond what can be learned from perturbation theory. Lattice models are also ideal for study by the methods of computational physics, as the discretization of any continuum model automatically turns it into a lattice model. Examples of lattice models in condensed matter physics include the Ising model, the Potts model, the XY model, the Toda lattice. The exact solution to many of these models (when they are solvable) includes the presence of solitons. Techniques for solving these include the inverse scattering transform and the method of Lax pairs, the Yang-Baxter equation and quantum groups. The solution of these models has given insights into the nature of phase transitions, magnetization and scaling behaviour, as well as insights into the nature of quantum field theory. Physical lattice models frequently occur as an approximation to a continuum theory, either to give an ultraviolet cutoff to the theory to prevent divergences or to perform numerical computations. An example of a continuum theory that is widely studied by lattice models is the QCD lattice model, a discretization of quantum chromodynamics. However, digital physics considers nature fundamentally discrete at the Planck scale, which imposes , aka Holographic principle. More generally, lattice gauge theory and lattice field theory are areas of study. Lattice models are also used to simulate the structure and dynamics of polymers. Examples include the bond fluctuation model and the 2nd model.\n\n"}
{"id": "38000528", "url": "https://en.wikipedia.org/wiki?curid=38000528", "title": "List of Trogoniformes by population", "text": "List of Trogoniformes by population\n\nThis is a list of Trogoniformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is not comprehensive, as not all Trogoniformes have had their numbers quantified.\n"}
{"id": "24716084", "url": "https://en.wikipedia.org/wiki?curid=24716084", "title": "List of currently erupting volcanoes", "text": "List of currently erupting volcanoes\n\nA list of erupting volcanoes follows below.\n\n\n\n\n\n\n\n\n"}
{"id": "75977", "url": "https://en.wikipedia.org/wiki?curid=75977", "title": "List of inventors", "text": "List of inventors\n\nThis is a list of notable inventors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "16191887", "url": "https://en.wikipedia.org/wiki?curid=16191887", "title": "List of lakes in Vermont", "text": "List of lakes in Vermont\n\nThis is a list of lakes in Vermont.\n\nThe Vermont Department of Health and Department of Environmental Conservation establish the limits of E.coli allowed before swimming is permitted. The E.coli is not checked for type, some of which are not harmful to humans. They allow up to 235 colonies per of water sampled.\n"}
{"id": "6611493", "url": "https://en.wikipedia.org/wiki?curid=6611493", "title": "List of stars in Aquarius", "text": "List of stars in Aquarius\n\nThis is the list of notable stars in the constellation Aquarius, sorted by decreasing brightness.\n\n"}
{"id": "53941711", "url": "https://en.wikipedia.org/wiki?curid=53941711", "title": "Little Science, Big Science", "text": "Little Science, Big Science\n\nLittle Science, Big Science is a book of collected lectures given by Derek J. De Solla Price, first published in 1963. The book presents the 1962 Brookhaven National Laboratory Pegram Lectures, a series of lectures dedicated to discussing science and its place in society. Price's goal in the lectures is to outline what it may look like for science to be analysed scientifically, by applying methods of measuring, hypothesizing, and deriving to science itself. With this goal in mind, he sets out to define quasi-mathematically how the shape and size of science has shifted from \"small science\" to \"big science\" in a historical and sociological way. Price presents a quantification of science as a measurable entity via an analogy to thermodynamics, conceptualizing science like a gas with individual molecules possessing individual velocities and interactions, a total volume, and general properties or laws.\n\nPrice begins the lectures by setting forth a demarcation in science centered around the modern period. He describes the phenomenon that, at the time of the lectures, 80 to 90 percent of important scientific work had occurred in one normal human life span. With this facet in mind, he sets out to describe the development of the term \"Big Science,\" as coined by Alvin M. Weinberg in 1961. As a general directive, he seeks to show that the transition from \"Little Science\" to \"Big Science,\" specifically the socio-economic and methodological changes to science in the 20th century, have been mostly gradual. To illustrate this point, he presents empirical statistical evidence from various aspects and fields of science, all of which show that the mode of growth of science is exponential, growing at compound interest. This assertion Price claims is the \"fundamental law of any analysis of science,\" stating that it even holds accurately over long time periods. With this fundamental law in mind, he states that for general measures the size of science in manpower or number of publications doubles in size every 10 to 15 years. If this rate of expansion is considered broadly, then from the 1600s until now such size measures of science have increased by a factor of 10. From this observation, Price moves to describe the \"coefficient of immediacy:\" the number of scientists alive compared to the number of scientists who have ever been, a ratio or percentage he states as 7:8 and 87.5% respectively. This measure serves to show numerically how the majority of important science has taken place within the average human life span at the time of the lecture presentation. As a result of the consistent exponential growth rate and immediacy of science, the statement that the majority of scientists throughout history are alive at any given moment must be consistent throughout history as well, meaning that in 1700 the majority of all scientists ever were alive, true also for 1800 and 1900 and so on. As a result of this facet, Price states that science has been constantly exploding into the population, increasing its size at a rate faster than the increase of total humans able to conduct it.\n\nHowever, Price asserts that this exponential growth rate cannot simply explain the transition from \"Little Science\" to \"Big Science,\" as the constant growth would not make the modern period under question any more likely to produce \"Big Science\" than any other. He conjectures that two statistical phenomena hold true for science generally, that individual metrics of science may grow at rates different from that of the exponential growth, and that the exponential growth rate may be starting to diminish. In response to his second point, he claims that the normal exponential growth may give way to a logistic growth rate, growing exponentially until it reaches a maximum size and then ceasing to grow. The possibility that science follows a rate of growth modeled by a logistic curve is suggested further by the fact that if science had continued to grow at an exponential rate in 1962, then by now there would be more scientists than people. With his claim that the growth rate actually observes a logistic curve, he provides a second basic law of the analysis of science, namely that the exponential growth rates previously mentioned must be in fact logistic. If this claim is correct, then the exponential growth rate previously observed must break down at a point in the future, and Price implies as a conclusion to this section that the onset of this breakdown may be associated with an upper bound to the size of science brought on by \"Big Science.\"\n\nIn this chapter, Price suggests various ideas and methods about conducting a science of science, or scientometrics, by first narrating some peculiar contributions to statistics made by Francis Galton. His overall goal is to further the possibility of applying scientific methods to science itself by suggesting various metrics and measures of the size, growth rate, and distribution of science. He focuses on Galton's work concerning the distribution of high achieving scientists and statesmen in the upper echelons of British society, specifically \"Hereditary Genius\" and \"English Men of Science\". These works are reviewed with the goal of understanding a basic metric for the number of people or papers in science that reach different levels of quality, an idea basic in Price's formulation of scientometrics. Further, he suggests that understanding such a metric would allow predictions to be made of science and scientists when changes associated with Big Science arrive. Galton's original approach was to estimate the distribution of high achieving practitioners of science among the eminent parts of British society, and Price takes this as a starting step in grasping a scientific metric of the productivity of science. In analyzing Galton's work and the work of another statistics researcher, Alfred J. Lotka, Price suggests that there may be a rough inverse-square law of productivity. Price moves next to define a quantity he calls someone's \"solidness\" \"s\", as the logarithm of the total papers published in one scientist's life. Keeping in mind the previous productivity law, for each unit increase in a scientist's solidness, the total number of scientists of that solidness decreases at a constant rate. With these two observations, among others, Price asserts that the foundations for an econometric-like study of science have been suggested, with the analysis of time series suggesting exponential or logistic growth and the distribution law of scientific productivity comprising them. He concludes by suggesting that these distributions and analyses contain errors relating to the non-uniform distribution of scientists across populations, noting that they tend to congregate in certain fields, institutions, countries, and journals. In keeping with his gas analogy, he maintains that just as one cannot measure the exact positions and velocities of gas molecules, one cannot pinpoint the exact productivity or contribution levels of individual scientists within science.\n\nThis chapter serves multiple purposes but overall achieves the same goal as the previous, providing a further conception of the productivity measure in science. This conclusion is reached through defining historically, sociologically, and from a communications perspective what a scientific paper is for, specifically what the purpose of this form of scientific communication is. To begin this analysis, he begins by looking at the history of the scientific paper, tracing its original purpose to discovering what was of interest within scientific practice. With the emergence of this scientific social practice, seen not as a means of publishing new knowledge but of communication between practitioners, the process of situating papers within the general body of literature came in to play. Specifically, each scientific paper is built from the foundation created by all previous papers, and with this facet exists a possibility of quantifying this foundation, the citation of references. With the idea that scientific papers were a social device of scientific communication, Price suggests that the driving force behind their emergent usage was the ability to assert and claim intellectual property within science. The possibility of communicating priority in disputes over scientific discoveries promoted the scientific paper as the best means of communication, leaving the information dissemination quality of papers as incidental in their overall purpose. With the quantification of scientific productivity by citation number and rate, there arrives a metric in science that gives the scientific importance of an individual's work or journal as its total usage within scientific practice, its total citations or references in other papers or journals. With this in mind, Price observes the fact that the total number of scientific references at a specific date across science is proportional to the total literature available within science at that date.\n\nMoving from the ability of scientific papers to facilitate communication and interactions between scientists, Price outlines an idea that allows further maximization of interactions between scientists. His term for this organizational method is the \"invisible college,\" specifically the circuit of institutions, research centers, journals, and conferences that allow intermingling and interactions within specific fields of science. Groups of scientists naturally form as a result of collaborations between individuals focusing on similar problems, but the ability for researchers to move around the globe in order to achieve interpersonal relationships with their fellow researchers is what Price suggests maximizes the group size able to keep up regular productive interactions. Thus Price defines the sociological structure of scientific practice communicating through published papers.\n\nThe final section of the lectures focuses on a larger-picture analysis of science and the monetary trends within it. As a general first statement, Price proposes that the cost of science has been increasing proportional to the square of the number of scientists. He points out that the cost of research in terms of the GDP did not increase in the years preceding World War II, yet afterward began increasing at the rate previously mentioned. As research amounts increase, the current and necessary number of researchers increases, promoting the inducement of scientists with higher salaries and better facilities in turn increasing the overall costs of science. Price suggests that it is this feedback loop that is a potential decelerator for the growth of science, and the main difference between Little Science and Big Science. What follows is his analysis of the \"explosion of science\" within non-developed countries, specifically Japan. He shows through this analysis that the United States' lack of experience of this explosion of science within the 20th century up to this point is due to the saturation of society with the activities of science, nearing costs not maintainable by the country. In countries where science has yet reached an exponential growth curve, this saturation is not present which allows the growth rate to set out at an exponential pace.\n\nThe final conceptual measure that Price offers is the idea of the \"mavericity\" of a scientist, or the likelihood that an individual will test new and unique combinations of theories and experiments unexpected in the current literature. The reactions and interactions within science to this mavericity also characterizes Big Science over Little Science, where the former serves to limit and restrain the most maverick investigators due to collaborative work and specific directed goals for scientific research. Thus the emergence of Big Science not only influences the growth rate, connectedness, and significance of science, but also the individual facets of the scientific pursuit.\n"}
{"id": "2224150", "url": "https://en.wikipedia.org/wiki?curid=2224150", "title": "MARAUDER", "text": "MARAUDER\n\nMARAUDER (Magnetically accelerated ring to achieve ultrahigh directed energy and radiation) is, or was, a United States Air Force Research Laboratory project concerning the development of a coaxial plasma railgun. It is one of several United States Government efforts to develop plasma-based projectiles. The first computer simulations occurred in 1990, and its first published experiment appeared on August 1, 1993.\n\nThe first MARAUDER experiment was motivated by RACE, or the Ring Accelerator Experiment, which took place in 1991 at Lawrence Livermore National Laboratory. The ultimate goal of the MARAUDER program was to accelerate highly dense toroids containing plasma to high speeds. Such a system could be used for “hypervelocity projectiles,” x-ray production, and electrical power amplification. The stated goals of the program included studying the “formation, compression, and acceleration of magnetized plasma rings.”\n\nSpecifically, the objective of the program was the acceleration of a toroid of 0.5-2.0 mg plasma to a kinetic energy level on the order of megajoules using a 5-10 MJ coaxial gun design.\n\nIn 1993, the Phillips Laboratory developed a coaxial plasma gun capable of accelerating 1–2 mg of plasma by up to 10g in a toroid of 1 meter in diameter. The toroids are similar to spheromaks, but differ in that an inner conductor is used to accelerate the plasma and that confinement behavior results from interactions of the toroid with its surrounding atmosphere. It utilized the Shiva Star capacitor bank to satisfy the large energy requirements that were needed.\n\nThe plasma projectiles would be shot at a speed expected to be 3000 km/s in 1995 and 10,000 km/s (3% of the speed of light) by 2000. A shot has the energy of 5 pounds of TNT exploding. Doughnut-shaped rings of plasma and balls of lightning exploded with devastating thermal and mechanical effects when hitting their target and produced pulse of electromagnetic radiation that could scramble electronics, the energy would shower the interior of the target with high-energy x-rays that would potentially destroy the electronics inside.\n\nAs of 1993 the project appeared to be in the early experimental stages. The weapon was able to produce doughnut-shaped rings of plasma and balls of lightning that exploded with devastating thermal and mechanical effects when hitting their target and produced pulse of electromagnetic radiation that could scramble electronics. The project's initial success led to it becoming classified, and only a few references to MARAUDER appeared after 1993. No information about the fate of the project has been published after 1995.\n\n\n"}
{"id": "15430033", "url": "https://en.wikipedia.org/wiki?curid=15430033", "title": "Manual for Streets", "text": "Manual for Streets\n\nIn England and Wales, the Manual for Streets, published in March 2007, provides guidance for practitioners involved in the planning, design, provision and approval of new streets, and modifications to existing ones. It aims to increase the quality of life through good design which creates more people-oriented streets. Although the detailed guidance in the document applies mainly to residential streets, the overall design principles apply to all streets within urban areas.\n\nA \"street\" is defined as \"a highway with important public realm functions beyond the movement of motor traffic\" – i.e. by its function rather than some arbitrary traffic flow limit.\n\nThe UK Department for Transport (DfT) and the Department for Communities and Local Government (DCLG), with support from the Commission for Architecture and the Built Environment (CABE), commissioned WSP Group, Transport Research Laboratory (TRL), Llewelyn Davies Yeang and Phil Jones Associates to develop \"Manual for Streets\" to give guidance to a range of practitioners on effective street design.\n\n\"Manual for Streets\" was published on 29 March 2007. It superseded \"Design Bulletin 32 – Residential Roads and Footpaths – Layout Considerations\" (DB32) and the companion guide \"Places, Streets and Movement\", which have now been withdrawn. A copy of the manual as well a summary and supporting research can be downloaded from the Department for Transport.\n\n\"Manual for Streets\" has updated geometric guidelines for low trafficked residential streets, examines the effect of the environment on road user behaviour, and draws on practice in other countries. Research undertaken by TRL provides the evidence base upon which the revised geometric guidelines in the \"Manual for Streets\" are based, including link widths, forward visibility, visibility splays and junction spacing.\n\n\"Manual for Streets\" applies in England and Wales and is national guidance, not a policy document.\n\nThe Scottish Government commissioned WSP Group, Phil Jones Associates and Edaw to produce \"Designing Streets\", a version of \"Manual for Streets\" for application in Scotland and was published in 2010. Unlike \"Manual for Streets\", it is published as a \"policy statement\".\n\nManual for Streets 2: Wider Application of the Principles was launched on 29 September 2010 in London. It is designed to be read alongside the original \"Manual\" rather than to supersede it. It is available to buy for £40 in paper form from its publisher, the Chartered Institution of Highways & Transportation (CIHT), as well as the usual retail outlets. CIHT staff reported at the launch that it will not be available to download for up to a year.\n\n\"Manual for Streets\" has been criticised for its approach to permeability of street networks. Critics argue that, by encouraging permeability of street networks for motor vehicles, \"MfS\" undermines its declared intention to reduce the domination of streets by motor traffic. Sustrans, the sustainable transport charity, while giving a cautious welcome to the \"Manual\", argues that the guidance should limit permeability for motor vehicles and provide full permeability for walking and cycling. Melia (2008) went further, arguing:\n\n\n"}
{"id": "24570445", "url": "https://en.wikipedia.org/wiki?curid=24570445", "title": "Matter collineation", "text": "Matter collineation\n\nA matter collineation (sometimes matter symmetry and abbreviated to MC) is a vector field that satisfies the condition,\n\nwhere formula_2 are the energy-momentum tensor components. The intimate relation between geometry and physics may be highlighted here, as the vector field formula_3 is regarded as preserving certain physical quantities along the flow lines of formula_3, this being true for any two observers. In connection with this, it may be shown that \"every Killing vector field is a matter collineation\" (by the Einstein field equations (EFE), with or without cosmological constant). Thus, given a solution of the EFE, \"a vector field that preserves the metric necessarily preserves the corresponding energy-momentum tensor\". When the energy-momentum tensor represents a perfect fluid, every Killing vector field preserves the energy density, pressure and the fluid flow vector field. When the energy-momentum tensor represents an electromagnetic field, a Killing vector field does \"not necessarily\" preserve the electric and magnetic fields.\n\n"}
{"id": "1988996", "url": "https://en.wikipedia.org/wiki?curid=1988996", "title": "Narrow-gap semiconductor", "text": "Narrow-gap semiconductor\n\nNarrow-gap semiconductors are semiconducting materials with a band gap that is comparatively small compared to that of silicon, i.e. smaller than 1.11 eV at room temperature. They are used as infrared detectors or thermoelectrics.\n\n(incomplete)\n\nDornhaus, R., Nimtz, G., Schlicht, B. (1983). \"Narrow-Gap Semiconductors\". Springer Tracts in Modern Physics 98, (print) (online) \n\nNimtz, G. (1980), \"Recombination in Narrow-Gap Semiconductors\", Physics Reports, 63, 265-300\n"}
{"id": "6139776", "url": "https://en.wikipedia.org/wiki?curid=6139776", "title": "National Watch and Clock Library", "text": "National Watch and Clock Library\n\nThe National Watch and Clock Library is one of the world's pre-eminent libraries devoted to horology and is located in Columbia, Pennsylvania, United States.\n\nIt is operated by the National Association of Watch and Clock Collectors for the benefit of both the public and the members of the association. It is a sister institution to the National Watch and Clock Museum.\n\nThe library is open to the public, members of whom may use the collection on the premises, and are invited to bring their own watches and clocks if they would like to ask the staff a question for a fee. Books are only lent to members of the NAWCC. The collection is open to researchers, members or the public alike.\n\nThe collection covers material on timekeeping, time and timekeepers in around dozen different languages. In addition to its book collection of about 10,000 titles (some of them dating back to the 16th century), the library has over 1,000 different videos (both VCR and DVD formats), a comprehensive collection of horological periodicals (both current and historic) from around the world, many thousands of historic catalogs of watches and clocks, as well as large quantities of archival material chronicling the history of timekeepers, including the research papers of many prominent American horological scholars of the past.\n\nSister public institutions serving similar purposes include: \n\n"}
{"id": "3387783", "url": "https://en.wikipedia.org/wiki?curid=3387783", "title": "Nicolae Leon", "text": "Nicolae Leon\n\nNicolae Leon (1862–1931) was a Romanian biologist. He was the elder half brother of the naturalist Grigore Antipa.\n\n"}
{"id": "53545555", "url": "https://en.wikipedia.org/wiki?curid=53545555", "title": "Outage probability", "text": "Outage probability\n\nIn Information theory, outage probability of a communication channel is the probability that a given information rate is not supported, because of variable channel capacity. Outage probability is defined as the probability that information rate is less than the required threshold information rate. It is the probability that an outage will occur within a specified time period.\n\nFor example, the channel capacity for slow-fading channel is \"C\" = log(1 + \"h\" SNR), where \"h\" is the fading coefficient and SNR is a signal to noise ratio without fading. As \"C\" is random, no constant rate is available. There may be a chance that information rate may go below to required threshold level. For slow fading channel, outage probability = P(\"C\" < \"r\") = P(log(1 + \"h\" SNR) < \"r\"), where \"r\" is the required threshold information rate.\n\n"}
{"id": "53839775", "url": "https://en.wikipedia.org/wiki?curid=53839775", "title": "Patrick Huerre", "text": "Patrick Huerre\n\nPatrick Huerre from the Ecole Polytechnique, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Fluid Dynamics in 1993, for \"his creative contributions to shear flow instability problems including critical layers, the distinction between closed and open systems in spatially developing flows, the concept of global modes, and the principles underlying frequency and pattern selection.\"\n"}
{"id": "23985799", "url": "https://en.wikipedia.org/wiki?curid=23985799", "title": "Physician supply", "text": "Physician supply\n\nPhysician supply refers to the number of trained physicians working in a health care system or active in the labour market. The supply depends primarily on the number of graduates of medical schools in a country or jurisdiction, but also on the number who continue to practice medicine as a career path and who remain in their country of origin. The number of physicians needed in a given context depends on several different factors, including the demographics and epidemiology of the local population, the numbers and types of other health care practitioners working in the system, as well as the policies and goals in place of the health care system. If more physicians are trained than needed, then supply exceeds demand; if too few physicians are trained and retained, then some people may have difficulty accessing health care services. A physician shortage is a situation in which there are not enough physicians to treat all patients in need of medical care. This can be observed at the level of a given health care facility, a province/state, a country, or worldwide.\n\nGlobally, the World Health Organization (WHO) estimates a shortage of 4.3 million physicians, nurses and other health workers worldwide, especially in many developing countries. Developing nations often have physician shortages due to limited numbers and capacity of medical schools and because of international migration: physicians can usually earn much more money and enjoy better working conditions in other countries. Many developed countries also report doctor shortages, especially in rural and other underserved areas. For example, shortages are being discussed in the U.S., Canada, the U.K., Australia, New Zealand, and Germany.\n\nSeveral causes of the current and anticipated shortages have been suggested; however, not everyone agrees that there is a true physician shortage, at least not in the United States. On the KevinMD medical news blog, for example, it has been argued that inefficiencies introduced into the healthcare system, often driven by government initiatives, have reduced the number of patients physicians can see; by forcing physicians to spend much of their time on data entry and public health issues, these initiatives have limited the physicians' time available for direct patient care.\n\nAnything that changes the number of available physicians or the demand for their services affects the supply and demand balance. If the number of physicians is decreased, or the demand for their services increases, then an \"under\"-supply or shortage can result. If the number of physicians increases, or demand for their services decreases, then an \"over\"-supply can result.\n\nSubstitution factors can significantly affect the production of physician services and the availability of physicians to see more patients. For example, an accountant can replace some of the financial responsibilities for a physician who owns his or her own practice, allowing for more time to treat patients. Disposable supplies can substitute for labor and capital (the time and equipment needed to sterilize instruments). Sound record keeping by physicians can substitute for legal services by avoiding malpractice suits. However, the extent of substitution of physician production is limited by technical and legal factors. Technology cannot replace all skills possessed by physicians, such as surgical skill sets. Legal factors can include only allowing licensed physicians to perform surgeries, but nurses or doctors administering other surgical care.\n\nDemand of physicians is also dependent on a country's economic status. Especially in developing nations, health care spending is closely related to growth of their Gross domestic product (GDP). Theoretically, as GDP increases, the health care labor force expands and in turn, physician supply also increases. However, developing countries face additional challenges in retaining competent physicians to higher-income countries such as the United States, Australia, and Canada. Emigration of physicians from lower-income and developing countries contribute to Brain drain, creating issues on maintaining sufficient physician supply. However, higher-income countries can also experience an outflow of physicians who decide to return to their naturalized countries after receiving extensive education and training, without ever benefiting from their gained medical knowledge and skill set.\n\nIncreasing the number of students enrolled in existing medical schools is one way to address physician shortage, or increasing the number of schools, but other factors may also play a role.\n\nBecoming a physician requires several years of training beyond undergraduate education. Consequently, physician supply is affected by the number of students eligible for medical training. Students that do not finish earlier levels of education, including high school dropouts and those that leave university without an undergraduate degree or associates degree, do not qualify for entrance to medical school. The more people that fail to complete the prerequisites, the fewer people that are eligible for training as physicians.\n\nIn most countries, the number of placements for students in medical schools and clinical internships is limited, typically according to the number of teachers and other resources, including the amount of funding provided by governments. In many countries that do not charge tuition payments to prospective physicians, public funding is the only significant limitation on the number of physicians trained. In the United States, the American Medical Association says that federal funding is the most important limitation in the supply of physicians. The high cost of tuition combined with the cost of supporting oneself during medical school discourages some people from enrolling to become a physician. Limited scholarships and financial aid to medical students may exacerbate this problem, while low expected pay for practicing physicians in some countries may convince some that the cost is not appropriate.\n\nIt has been speculated that politics and social conditions can sometimes motivate medical student placements. For example, racial quotas have been cited in some places as preventing some people from enrolling in medical school. Racial discrimination and gender discrimination, either overt or disguised, have also been cited as resulting in people being denied the opportunity to train as a physician on the basis of their race or gender.\n\nOnce trained, the current supply of physicians can be affected by the number of those who continue to practice this profession. The number of working physicians can be affected by:\n\n\nThe demand for physician services is influenced by the local job market (e.g. the number of job openings in local health care facilities), the demographics and epidemiology of the population being served, the nature of the health policies in place for health care delivery and financing in a jurisdiction, and also the international job market (e.g. increasing demand in other countries puts pressure on local competition). As of 2010, the WHO proposes a ratio of at least one primary care physician per 1000 people to sufficiently attend the basic needs of the population in a developed country.\n\nFor example, population ageing has been attributed with increased demand for physician services in many countries, as more previously young and healthy people become older with increased likelihood of a variety of chronic medical conditions associated with ageing, such as type 2 diabetes mellitus, hypertension, osteoporosis, and some types of cancers and neurodegenerative diseases.\n\nIn the United States, the Patient Protection and Affordable Care Act has expanded health insurance coverage and access to an estimated 32 million United States citizens, increasing the demand of physicians, especially primary care physicians, across the country. Expanded coverage is predicted to increase the number of annual primary care visits between 15.07 million and 24.26 million by 2019. Assuming stable levels of physicians’ productivity, between 4,307 and 6,940 additional primary care physicians would be needed to accommodate this increase.\n\nThe PPACA may have also affected the supply of Medicaid physicians. Incentives and higher reimbursement rates may have increased the number of physicians accepting Medicaid patients leading up to 2014. With the expansion of Medicaid and a decrease in incentives and reimbursement rates in 2014, the supply of physicians in Medicaid may drop substantially, fluctuating the supply of Medicaid physicians. A study examining variation between states in 2005 showed that average time for Medicaid reimbursements was directly correlated with Medicaid participation, and physicians in states with faster reimbursement times had a higher probability of accepting new Medicaid patients.\n\nPhysician shortages have been linked to a number of effects, including:\n\nA number of solutions, including short-term fixes and long-term solutions, have been proposed to address physician shortages. Some have been tested and applied in national health workforce policies and plans, while others remain subject to ongoing debate.\n\n\nIn the US alone, the Association of American Medical Colleges (AAMC) estimates a shortage of 91,500 physicians by 2020 and up to 130,600 by the year 2025. However, a bias would clearly exist in their estimates as expanding medical education serves the direct financial needs of the AAMC. As previously mentioned, the World Health Organization (WHO) estimates a shortage of 4.3 million physicians, nurses and other health workers worldwide. The WHO produced a list of countries with a “Human Resources for Health crisis”. In these countries, there are only 1.13 doctors for every 1,000 people. In the United States, there are approximately 2.5 doctors for every 1,000 people. One quarter of physicians practicing in the United States are from foreign countries. Thousands of foreign doctors come to practice in the United States each year while only a few hundred doctors from the United States leave to practice in foreign countries even short-term.\n\nThere are various organizations that assist United States physicians and others in serving internationally. These organizations may be filling temporary or permanent positions. Two temporary agencies are Global Medical Staffing and VISTA staffing. A locum doctor will serve in the temporary absence of another physician. These positions are typically 1-year placements but can vary by location, specialty, and other factors. Agencies that attempt to provide international aid in various ways often have a strong medical component. Some of these organizations helping to provide medical care internationally include Reach Out Worldwide (ROWW), Doctors Without Borders (Médecins Sans Frontières), Mercy Ships, the US Peace Corps, and International Medical Corps.\n\nAdditionally, smaller non-profits that work regionally around the world have also implemented task-shifting strategies in order to increase impact. Non profits, such as the MINDS Foundation educated community health workers or teachers to perform simple medicinal tasks, thereby freeing up health professionals to focus on more pressing concerns.\n\n"}
{"id": "4393843", "url": "https://en.wikipedia.org/wiki?curid=4393843", "title": "Reactivity–selectivity principle", "text": "Reactivity–selectivity principle\n\nIn chemistry the reactivity–selectivity principle or RSP states that a more reactive chemical compound or reactive intermediate is less selective in chemical reactions. In this context selectivity represents the ratio of reaction rates.\n\nThis principle was generally accepted until the 1970s when too many exceptions started to appear. The principle is now considered obsolete.\n\nA classic example of perceived RSP found in older organic chemistry textbooks concerns the free radical halogenation of simple alkanes. Whereas the relatively unreactive bromine reacts with 2-methylbutane predominantly to 2-bromo-2-methylbutane, the reaction with much more reactive chlorine results in a mixture of all four regioisomers.\n\nAnother example of RSP can be found in the selectivity of the reaction of certain carbocations with azides and water. The very stable triphenylmethyl carbocation derived from solvolysis of the corresponding triphenylmethyl chloride reacts 100 times faster with the azide anion than with water. When the carbocation is the very reactive tertiary adamantane carbocation (as judged from diminished rate of solvolysis) this difference is only a factor of 10.\n\nConstant or inverse relationships are just as frequent. For example, a group of 3- and 4-substituted pyridines in their reactivity quantified by their pKa show the same selectivity in their reactions with a group of alkylating reagents.\n\nThe reason for the early success of RSP was that the experiments involved very reactive intermediates with reactivities close to kinetic diffusion control and as a result the more reactive intermediate appeared to react slower with the faster substrate.\n\nGeneral relationships between reactivity and selectivity in chemical reactions can successfully explained by Hammond's postulate.\n\nWhen reactivity-selectivity relationships do exist they signify different reaction modes. In one study the reactivity of two different free radical species (A, sulfur, B carbon) towards addition to simple alkenes such as acrylonitrile, vinyl acetate and acrylamide was examined.\n\nThe sulfur radical was found to be more reactive (6*10 vs. 1*10 M.s) and less selective (selectivity ratio 76 vs 1200) than the carbon radical. In this case the effect can be explained by extending the Bell–Evans–Polanyi principle with a factor formula_1 accounting for transfer of charge from the reactants to the transition state of the reaction which can be calculated in silico:\n\nformula_2\n\nwith formula_3 the activation energy and formula_4 the reaction enthalpy change. With the electrophilic sulfur radical the charge transfer is largest with electron-rich alkenes such as acrylonitrile but the resulting reduction in activation energy (β is negative) is offset by a reduced enthalpy. With the nucleophilic carbon radical on the other hand both enthalpy and polar effects have the same direction thus extending the activation energy range.\n\n"}
{"id": "15081678", "url": "https://en.wikipedia.org/wiki?curid=15081678", "title": "Science.tv", "text": "Science.tv\n\nScience.tv is a virtual community for people interested in science. It enables users to upload videos and categorize them according to subject matter and intended audience.\n\nscience.tv was founded by Matt Thurling, a digital media pioneer and film-maker based in Bristol, England. Research and development work began in 2005 and the site was launched in December 2007. Thurling writes the science.tv blog, where he sets out the vision for the site and invites feedback and suggestions for improvements.\n\nThe .tv (\"dot-tv\") extension is a Top-level domain name, originally associated with the Polynesian island nation of Tuvalu. The entire domain was purchased from the islands' residents in 2000 and the suffix is now increasingly being used for sites featuring, or relating to, video content.\n\nA look at the source code of science.tv reveals a front-end built with Cascading Style Sheets and JavaScript. Video is delivered using the Adobe Flash codec. The back-end relies on a PHP interface to a mySQL content management system.\n\nThe focus of science.tv is user-generated content, and much of the content on the site is actually embedded from other video sites, including YouTube. The intended audience is broad, ranging from school students to academics and professional programme-makers.\nUsers are able to navigate for content via site-wide search, tags and by preset categories, which include: physics, chemistry and biology.\n"}
{"id": "24742187", "url": "https://en.wikipedia.org/wiki?curid=24742187", "title": "Simulated growth of plants", "text": "Simulated growth of plants\n\nThe simulated growth of plants is a significant task in of systems biology and mathematical biology, which seeks to reproduce plant morphology with computer software.\nElectronic trees (e-trees) usually use L-systems to simulate growth. L-systems are very important in the field of complexity science and A-life.\nA universally accepted system for describing changes in plant morphology at the cellular or modular level has yet to be devised. \nThe most widely implemented tree-generating algorithms are described in the papers \"Creation and Rendering of Realistic Trees\", and Real-Time Tree Rendering\n\nThe realistic modeling of plant growth is of high value to biology, but also for computer games.\n\nA biologist, Aristid Lindenmayer (1925–1989) worked with yeast and filamentous fungi and studied the growth patterns of various types of algae, such as the blue/green bacteria \"Anabaena catenula\". Originally the L-systems were devised to provide a formal description of the development of such simple multicellular organisms, and to illustrate the neighbourhood relationships between plant cells. Later on, this system was extended to describe higher plants and complex branching structures.\nCentral to L-systems, is the notion of rewriting, where the basic idea is to define complex objects by successively replacing parts of a simple object using a set of rewriting rules or productions. The rewriting can be carried out recursively. L-Systems are also closely related to Koch curves.\nA challenge for plant simulations is to consistently integrate environmental factors, such as surrounding plants, obstructions, water and mineral availability, and lighting conditions. \nis to build virtual/environments with as many parameters as computationally feasible, thereby, not only simulating the growth of the plant, but also the environment it is growing within, and, in fact, whole ecosystems.\nChanges in resource availability influence plant growth, which in turn results in a change of resource availability. Powerful models and powerful hardware will be necessary to effectively simulate these recursive interactions of recursive structures. \n\n\"see Comparison of tree generators and A Survey of Modeling and Rendering Trees \n\n\n"}
{"id": "11139657", "url": "https://en.wikipedia.org/wiki?curid=11139657", "title": "Social Research Association", "text": "Social Research Association\n\nThe Social Research Association (usually abbreviated SRA), founded in 1978 by Janet Lewis and Malcolm Cross, and supported by an active Board that included Roger Jowell, is a British and Irish organisation open to practitioners and researchers interested in all branches of social research. It is a Learned society member of the UK Academy of Social Sciences. In addition to the umbrella organisation, it has branches that cater specifically to researchers in Scotland, Wales and Ireland (including both the Republic of Ireland and Northern Ireland). Among other activities, it publishes a code of conduct for social researchers which is widely adopted as a standard of research ethics by funding agencies in the social sciences.\n\n"}
{"id": "32874332", "url": "https://en.wikipedia.org/wiki?curid=32874332", "title": "Tensor glyph", "text": "Tensor glyph\n\nIn scientific visualization a tensor glyph is an object that can visualize all or most of the nine degrees of freedom, such as acceleration, twist, or shear – of a formula_1 matrix. It is used for tensor field visualization, where a data-matrix is available at every point in the grid. \"Glyphs, or icons, depict multiple data values by mapping them onto the shape, size, orientation, and surface appearance of a base geometric primitive.\" Tensor glyphs are a particular case of multivariate data glyphs.\n\nThere are certain types of glyphs that are commonly used:\n\nAccording to Thomas Schultz and Gordon Kindlmann, specific types of tensor fields \"play a central role in scientific and biomedical studies as well as in image analysis\nand feature-extraction methods.\"\n\n"}
{"id": "35520902", "url": "https://en.wikipedia.org/wiki?curid=35520902", "title": "Timeline of computational physics", "text": "Timeline of computational physics\n\nThe following timeline starts with the invention of the modern computer in the late interwar period.\n\n\n\n\n\n\n\n\n"}
{"id": "39947682", "url": "https://en.wikipedia.org/wiki?curid=39947682", "title": "Wiley Bad Science Series", "text": "Wiley Bad Science Series\n\nThe Wiley Bad Science Series is a series of books by John Wiley & Sons Publishing about scientific misconceptions.\n\nThe \"Publishers Weekly\" review of the first book in the series, \"Bad Astronomy\", mentioned that the subsequent books will be about scientific misconceptions in biology, weather and the earth.\n\n"}
