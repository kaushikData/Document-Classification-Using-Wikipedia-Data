{"id": "1865258", "url": "https://en.wikipedia.org/wiki?curid=1865258", "title": "AP Chemistry", "text": "AP Chemistry\n\nAdvanced Placement Chemistry (AP Chemistry or AP Chem) is a course and examination offered by the College Board as a part of the Advanced Placement Program to give American and Canadian high school students the opportunity to demonstrate their abilities and earn college-level credit. AP Chemistry has the distinction of having the lowest known test participation rate, with 49.5% of AP Chemistry students taking the exam in one study. Another, smaller study found that 52.7% of students enrolled in AP Chemistry took their course's AP test.\n\nAP Chemistry is a course geared toward students with interests in chemical and physical sciences, as well as any of the biological sciences. The course aims to prepare students to take the AP Chemistry exam toward the end of the academic year. AP Chemistry topics include atomic theory, chemical bonding, phases of matter, solutions, types of reactions, chemical equilibrium, reaction kinetics, electrochemistry, and thermodynamics.\n\nThe College Board recommends successful completion of High School Chemistry and Algebra II; however, requirement of this may differ from school to school. AP Chemistry usually requires knowledge of Algebra II; however, some schools allow students to take Algebra II concurrently with this class. The requirement of regular or honors level High School Chemistry may also be waived, but usually requires completion of a special assignment or exam.\n\nThe exam covers most of the introductory chemistry topics (except organic chemistry), including:\n\n\n\nThe annual AP Chemistry examination, which is typically administered in May, is divided into two major sections (multiple-choice questions and free response essays).\n\nThe old test was composed of two sections: a multiple-choice section consisting of 75 questions with five answer choices each, and a free-response section consisting of six essay prompts that required the authoring of chemical equations, solution of problems, and development of thoughtful essays in response to hypothetical scenarios.\n\n\nWhile the use of calculators was prohibited during Section I and Section II Part B, a periodic table, a list of selected standard reduction potentials, and two pages of equations and conventions are available for use during the entirety of Section II.\n\nThe 2014 AP Chemistry exam was the first administration of a redesigned test as a result of a redesigning of the AP Chemistry course. The exam format is now different from the previous years, with 60 multiple choice questions (now with only four answer choices per question), 3 long free response questions, and 4 short free response questions. The new exam has a focus on longer, more in depth, lab-based questions. The penalty for incorrect answers on the multiple choice section was also removed. More detailed information can be found at the related link.\n\nThe score distributions since 2007 were: \n\n\n"}
{"id": "23224820", "url": "https://en.wikipedia.org/wiki?curid=23224820", "title": "A Glorious Accident", "text": "A Glorious Accident\n\nEen schitterend ongeluk (translated \"A Glorious Accident\" in English) was a 1993 documentary featuring several prominent scientists and philosophers. Hosted by Wim Kayzer, guests included Daniel Dennett, Freeman Dyson, Stephen Jay Gould, Oliver Sacks, Rupert Sheldrake, and Stephen Toulmin. It was filmed in the Netherlands by the VPRO and produced by Nellie Kamer.\n\n"}
{"id": "4631496", "url": "https://en.wikipedia.org/wiki?curid=4631496", "title": "Agricultural soil science", "text": "Agricultural soil science\n\nAgricultural soil science is a branch of soil science that deals with the study of edaphic conditions as they relate to the production of food and fiber. In this context, it is also a constituent of the field of agronomy and is thus also described as soil agronomy.\n\nPrior to the development of pedology in the 19th century, agricultural soil science (or edaphology) was the only branch of soil science. The bias of early soil science toward viewing soils only in terms of their agricultural potential continues to define the soil science profession in both academic and popular settings . (Baveye, 2006)\n\nAgricultural soil science follows the holistic method. Soil is investigated in relation to and as integral part of terrestrial ecosystems but is also recognized as a manageable natural resource.\n\nAgricultural soil science studies the chemical, physical, biological, and mineralogical composition of soils as they relate to agriculture. Agricultural soil scientists develop methods that will improve the use of soil and increase the production of food and fiber crops. Emphasis continues to grow on the importance of soil sustainability. Soil degradation such as erosion, compaction, lowered fertility, and contamination continue to be serious concerns. They conduct research in irrigation and drainage, tillage, soil classification, plant nutrition, soil fertility, and other areas.\n\nAlthough maximizing plant (and thus animal) production is a valid goal, sometimes it may come at high cost which can be readily evident (e.g. massive crop disease stemming from monoculture) or long-term (e.g. impact of chemical fertilizers and pesticides on human health). An agricultural soil scientist may come up with a plan that can maximize production using sustainable methods and solutions, and in order to do that he must look into a number of science fields including agricultural science, physics, chemistry, biology, meteorology and geology.\n\nSome soil variables of special interest to agricultural soil science are:\n\n\nAgricultural soil scientists study ways to make soils more productive. They classify soils and test them to determine whether they contain nutrients vital to plant growth. Such nutritional substances include compounds of nitrogen, phosphorus, and potassium. If a certain soil is deficient in these substances, fertilizers may provide them. Agricultural soil scientists investigate the movement of nutrients through the soil, and the amount of nutrients absorbed by a plant's roots. Agricultural soil scientists also examine the development of roots and their relation to the soil. Some agricultural soil scientists try to understand the structure and function of soils in relation to soil fertility. They grasp the structure of soil as porous solid. The solid frames of soil consist of mineral derived from the rocks and organic matter originated from the dead bodies of various organisms. The pore space of the soil is essential for the soil to become productive. Small pores serve as water reservoir supplying water to plants and other organisms in the soil during the rain-less period. The water in the small pores of soils is not pure water; they call it soil solution. In soil solution, various plant nutrients derived from minerals and organic matters in the soil are there. This is measured through the cation exchange capacity. Large pores serve as water drainage pipe to allow the excessive water pass through the soil, during the heavy rains. They also serve as air tank to supply oxygen to plant roots and other living beings in the soil. In short, agricultural soil scientists see the soil as a vessel, the most precious one for us, containing all of the substances needed by the plants and other living beings on earth.\n\nIn addition, agricultural soil scientists develop methods to preserve the agricultural productivity of soil and to decrease the effects on productivity of erosion by wind and water. For example, a technique called contour plowing may be used to prevent soil erosion and conserve rainfall. Researchers in agricultural soil science also seek ways to use the soil more effectively in addressing associated challenges. Such challenges include the beneficial reuse of human and animal wastes using agricultural crops; agricultural soil management aspects of preventing water pollution and the build-up in agricultural soil of chemical pesticides.\n\nMost agricultural soil scientists are consultants, researchers, or teachers. Many work in the developed world as farm advisors, agricultural experiment stations, federal, state or local government agencies, industrial firms, or universities. Within the USA they may be trained through the USDA's Cooperative Extension Service offices, although other countries may use universities, or research agencies. Elsewhere, agricultural soil scientists may serve in international organizations such as the Agency for International Development and the Food and Agriculture Organization of the United Nations.\n\n\n"}
{"id": "2651748", "url": "https://en.wikipedia.org/wiki?curid=2651748", "title": "Ash (analytical chemistry)", "text": "Ash (analytical chemistry)\n\nIn analytical chemistry, ashing or ash content determination is the process of mineralization for preconcentration of trace substances prior to a chemical analysis, such as chromatography, or optical analysis, such as spectroscopy. \n\nThe residues after a sample is completely burnt - in contrast to the ash remaining after incomplete combustion - consist mostly of metal oxides. Ash is one of the components in the proximate analysis of biological materials, consisting mainly of salty, inorganic constituents. It includes metal salts which are important for processes requiring ions such as Na (Sodium), K (Potassium), and Ca (Calcium). It also includes trace minerals which are required for unique molecules, such as chlorophyll and hemoglobin.\n\nA crucible can be used to determine the percentage of ash contained in an otherwise burnable sample of material such as coal, wood, oil, rubber or plastics. The ISO mandates ash content determination for most foodstuffs. Examples include \n\nSome necessary apparatus include:\nA crucible and its lid are pre-weighed after thorough drying. The sample is added to the completely dry crucible and lid and together they are weighed to determine the mass of the sample by difference. The sample is placed in the hot furnace long enough so that complete combustion of the sample occurs. The crucible, lid and ash then are re-weighed.\nThe analysis of honey shows:\n\n\nIn this example the ash would include all the minerals in honey.\n\n"}
{"id": "47957102", "url": "https://en.wikipedia.org/wiki?curid=47957102", "title": "Atheronals", "text": "Atheronals\n\nAtheronals are biologically relevant oxysterols formed in the reaction of cholesterol with ozone. Atheronal A is the major product of ozonolysis which is 3β-hydroxy-5-oxo-5,6-secocholestan-6-al. Atheronal B is formed by the intramolecular aldol reaction of Atheronal A, which is 3β-hydroxy-5β-hydroxy-B-norcholestane-6β-carboxaldehyde.\n"}
{"id": "9551741", "url": "https://en.wikipedia.org/wiki?curid=9551741", "title": "Auto insurance risk selection", "text": "Auto insurance risk selection\n\nAuto insurance risk selection is the process by which vehicle insurers determine whether or not to insure an individual and what insurance premium to charge. Depending on the jurisdiction, the insurance premium can be either mandated by the government or determined by the insurance company in accordance to a framework of regulations set by the government. Often, the insurer will have more freedom to set the price on physical damage coverages than on mandatory liability coverages.\n\nWhen the premium is not mandated by the government, it is usually derived from the calculations of an actuary based on statistical data. The premium can vary depending on many factors that are believed to affect the expected cost of future claims. Those factors can include the car characteristics, the coverage selected (deductible, limit, covered perils), the profile of the driver (age, gender, driving history) and the usage of the car (commute to work or not, predicted annual distance driven).\n\nConventional methods for determining costs of motor vehicle insurance involve gathering relevant historical data from a personal interview with, or a written application completed by, the applicant for the insurance and by referencing the applicant's public motor vehicle driving record that is maintained by a governmental agency, such as a Bureau of Motor Vehicles. Such data results in a classification of the applicant to a broad actuarial class for which insurance rates are assigned based upon the empirical experience of the insurer. Many factors are deemed relevant to such classification in a particular actuarial class or risk level, such as age, sex, marital status, location of residence and driving record.\n\nThe current system of insurance creates groupings of vehicles and drivers (actuarial classes) based on the following types of classifications.\n\n\nThe classifications, such as age, are further broken into actuarial classes, such as 21- to 24-year-olds, to develop a unique vehicle insurance cost based on the specific combination of attributes for a particular risk. For example, the following information would produce a unique vehicle insurance cost:\n\n\nA change to any of this information might result in a different premium being charged if the change resulted in a different actuarial class or risk level for that variable. For instance, a change in the drivers' age from 38 to 39 may not result in a different actuarial class because 38- and 39-year-old people may be in the same actuarial class. However, a change in driver age from 38 to 45 may result in a different premium because the records of the insurer indicate a difference in risk associated with those ages and, therefore, the age difference results in a change in actuarial class or assigned risk level.\n\nCurrent insurance rating systems also provide discounts and surcharges for some types of use of the vehicle, equipment on the vehicle and type of driver. Common surcharges and discounts include:\n\n\nConventional rating systems are primarily based on past realized losses and the past record of other drivers with similar characteristics. More recently, electronic systems have been introduced whereby the actual driving performance of a given driver is monitored and communicated directly to the insurance company. The insurance company then assigns the driver to a risk class based on the monitored driving behavior. An individual, therefore, can be put into different risk classes from month to month depending upon how they drive. For example, a driver who drives long distance at high speed in one month might be placed into a high risk class for that month and pay a large premium. If the same driver drives for short distances at low speed the next month, however, then he or she might be placed into a lower risk class and charged a lower premium.\n"}
{"id": "33881639", "url": "https://en.wikipedia.org/wiki?curid=33881639", "title": "Bacilloviridae", "text": "Bacilloviridae\n\nThe \"Bacilloviridae\" are a family of double stranded DNA viruses that infect species belonging to the archaeal genera \"Sulfolobus\" and \"Thermoproteus\".\n\nThe virons are rod-shaped and inflexible. At the ends of the viron are structures involved in adherence to the host cell.\n\nThe genome is linear.\n\nSIRV may be related to the Haloarcula hispanica SH1 virus.\n"}
{"id": "22696802", "url": "https://en.wikipedia.org/wiki?curid=22696802", "title": "Base period", "text": "Base period\n\nIn economics, a base period or reference period is a point in time used as a reference point for comparison with other periods. It is generally used as a benchmark for measuring financial or economic data. Base periods typically provide a point of reference for economic studies, consumer demand, and unemployment benefit claims.\n\nIn public transport scheduling, the base period is the period of reduced service on weekdays that generally exists between the morning and afternoon rush hours.\n\n\n"}
{"id": "19105306", "url": "https://en.wikipedia.org/wiki?curid=19105306", "title": "Bernard Lauth", "text": "Bernard Lauth\n\nBernard Lauth (August 23, 1820 in Alsace, France – June 25, 1894) he founded the American Iron Works in 1850, and formed a partnership with B.F. Jones in 1851. In 1854, Lauth retired from the steel firm, selling his partnership to James H. Laughlin, who led the company to be renamed Jones and Laughlin Steel Company. He invented and patented the process for cold rolling of iron in 1859. In 1871, he purchased the iron furnace at Howard, Pennsylvania, where he built a rolling mill in 1882.\n"}
{"id": "48629072", "url": "https://en.wikipedia.org/wiki?curid=48629072", "title": "Botanical Garden and Zoo of Asunción", "text": "Botanical Garden and Zoo of Asunción\n\nThe Botanical Garden and Zoo of Asunción () is a botanical garden and zoo located in Asunción, capital of the Republic of Paraguay.\n\nThe Botanical Garden and Zoo is one of the principal open spaces of the city of Asunción, set in natural forest covering to the north of the city. The zoo is home to nearly seventy species of wildlife including birds, mammals and reptiles, mostly representing the fauna of South America. The botanical garden is home to native species, exhibiting in particular the variety and beauty of its lush trees.\n\nThis sprawling property was the former country house and estate of Carlos Antonio Lopez, president of Paraguay between 1842 and 1862. Lopez ordered the construction of \"Casa Lopez\" as his home in the 1840s. Besides its historical value, the main building is very representative of the era in which it was built, in terms of technology, architecture and decoration, and is registered in the \"Catalogue of Buildings and Sites of Urban Planning, architectural , Historical and Artistic Heritage of the city of Asunción\",and specially protected by Law 946/82 \"Protection of Cultural Property\".\n\nIn 1896, Lopez's descendants sold the estate to the Agricultural Bank, by then in state ownership.\n\nThe garden was created as such in 1914 by German scientists Karl Freibig and his wife, Anna Gertz. Fiebrig was professor of botany and zoology at the University of Asunción, having settled in Paraguay in 1910 following plant and insect collecting trips to Paraguay for European museums between 1904 and 1909. Fiebrig founded a school of Agriculture in 1916. Fiebrig also founded a \"Cotton Institute\" which helped fund the garden complex. The zoo was subsequently established by the same scientists, with a very advanced approach for the time, housing the animals in a setting as close as possible to their natural habitat. Gertz is credited with much of the landscape design of the gardens, though some projects were truncated following her death in May 1920. She was buried in the gardens.\n\nFiebrig continued as director of the garden and zoo, and remarried in 1925. In addition to his professorship, in 1934, was also made director of the Paraguayan Department of Agriculture. In 1936, in the aftermath of the Chaco War, a wave of xenophobic sentiment forced Fiebrig to leave Paraguay with his second wife and family. Responsibility for the estate passed from the state to its present owner, the Municipality of Asunción.\n\nHistorically, the estate covered more than , and had over 1 km frontage along the bank of the Paraguay River, a port, a railway station and 60 km of road network. In the last 50 years, there have been several incursions, such as the riverside (ESSAP) \"Viñas Cué\" water treatment plant, the Copaco transmission station (built at the time of dictator Stroessner), the Asunción Golf Club, laid out by Fiebrig, and several other divisions due to illegal occupations.\n\nSince 2013 the garden and zoo's director has been Maris Llorens.\n\nThe facilities include:\n\nThe nursery is located behind the Upper House and contains over 500 species specialising in medicinal plants. It is open to the public and works in cooperation with the Botanic Garden and Conservatory of the City of Geneva, Switzerland.\n\nEstablished for over 10 years, it has undertaken investigations into the cultivation, distribution and introduction of plants, specifically native, but also medicinal plants introduced by Paraguayan settlers.\n\nThe work of the Conservatory is to preserve the culture of the knowledge of medicinal plants in Paraguay. \"The Paraguayan people consume herbs and know the use of at least 50 species. The work of the nursery is to investigate cultivation, harvesting and propagation and to use that knowledge for education\".\n\nAmong its collections are:\n\n\nOn 4 May 2006 the gardens launched the exhibition \"Ethnobotany 2006\" \"Our plants, our people\" (\"\"), under the Paraguayan Ethnobotany Project (EPY) (which lasted for about ten years), with the support of the Conservatory and Botanical Garden of the City of Geneva and under the auspices of the organization \"Tesãi Reka Paraguay\" (TRP).\n\nThe project helped improve the botanical garden and created a large Paraguayan medicinal plant collection and the Center for Conservation and Environmental Education (CCEAM) located in the Botanical Garden, which develops numerous educational activities.\n\nThe zoo is located on the same site. The zoo's collections focus on the fauna of Paraguay but include animals from elsewhere.\n\n\nBirds include:\n\n\n"}
{"id": "30174623", "url": "https://en.wikipedia.org/wiki?curid=30174623", "title": "Brook Glacier", "text": "Brook Glacier\n\nBrook Glacier () is a glacier that flows westward between Mount Strybing, Mount Allen and Krusha Peak on the west side of Owen Ridge in southern Sentinel Range, Ellsworth Mountains in Antarctica, and joins Bender Glacier east of Chaplin Peak. It was named by the Advisory Committee on Antarctic Names (2006) after Edward J. Brook, Professor of Geosciences, Oregon State University; U.S. Antarctic Project investigator of Antarctic paleoclimate in numerous field seasons from 1988; Chair, U.S. National Ice Core Working Group for use of Antarctic ice cores for research purposes, 2004–05.\n\n\n\n"}
{"id": "6106797", "url": "https://en.wikipedia.org/wiki?curid=6106797", "title": "Charge carrier density", "text": "Charge carrier density\n\nCharge carrier density, also known as carrier concentration, denotes the number of charge carriers per volume. In SI units, it is measured in m. As with any density, it can depend on position.\n\nThe carrier density is obtained by integrating the charge density over the energy that the charges are allowed to have.\n\nCharge carrier density is a particle density, so integrating it over a volume formula_1 gives the number of charge carriers formula_2 in that volume\n\nformula_3.\n\nwhere\n\nIf the density does not depend on position and is instead equal to a constant formula_5 this equation simplifies to\n\nformula_6.\n\nCharge carrier densities involve equations concerning the electrical conductivity and related phenomena like the thermal conductivity.\n\nThe carrier density is important for semiconductors, where it is an important quantity for the process of chemical doping. Using band theory, the electron density,formula_5 is number of electrons per unit volume in the conduction band. For holes, formula_8 is the number of holes per unit volume in the valance band. To calculate this number for electrons, we start with the idea that the total density of conduction-band electrons, formula_5, is just adding up the conduction electron density across the different energies in the band, from the bottom of the band formula_10 to the top of the band formula_11.\n\nBecause electrons are fermions, the density of conduction electrons at any particular energy,formula_13 is the product of the density of states,formula_14 or how many conducting states are possible, with the Fermi-Dirac distribution,formula_15 which tells us the portion of those states which will actually have electrons in them.\n\nIn order to simplify the calculation, instead of treating the electrons as Fermions, according to the Fermi-dirac distribution, we instead treat them as a classical non-interacting gas, which is given by the Maxwell–Boltzmann distribution. This approximation has negligible effects when the magnitude formula_17, which is true for semiconductors near room temperature. This approximation is invalid at very low temperatures or an extremely small band-gap.\n\nThe three-dimensional density of states is:\n\nAfter combination and simplification, these expressions lead to:\n\nA similar expression can be derived for holes. The carrier concentration can be calculated by treating electrons moving back and forth across the bandgap just like the equilibrium of a reversible reaction from chemistry, leading to an electronic mass action law. The mass aciton law defines a quantity formula_22 called the intrinsic carrier concentraion, which for undoped materials:\n\nThe following table lists a few values of the intrinsic carrier concentration for intrinsic semiconductors. The carrier concentrations will change if these materials are doped.\nThe carrier density is also applicable to metals, where it can be calculated from the simple Drude model. In this case, the carrier density (in this context, also called the free electron density) can be calculated by:\n\nWhere formula_25 is the Avogadro constant, Z is the number of valence electrons, formula_26 is the density of the material, and formula_27 is the atomic mass.\n\nThe density of charge carriers can be determined in many cases using the Hall effect , the voltage of which depends inversely on the density.\n"}
{"id": "2247707", "url": "https://en.wikipedia.org/wiki?curid=2247707", "title": "Charles F. Hoffmann", "text": "Charles F. Hoffmann\n\nCharles Frederick Hoffmann (1838–1913) was a German-American topographer working in California U.S. from 1860 to 1880.\n\nHoffmann was born in Frankfurt, Germany, 1838. After receiving an education in engineering, he emigrated to America. In 1857 he was topographer for Frederick Lander's survey to the Rocky Mountains. He came to California in 1858. He was recruited by Josiah Whitney to join the California Geological Survey because of his valuable skill as a topographer. Hoffmann is largely responsible for introducing topography to the United States. He helped explore the Sierra Nevada of California, from 1860 through 1870, and 1873 through 1874. As a member of the Survey, Hoffmann created the official maps from the expeditions made by the survey team. Hoffmann achieved a number of first ascents in the Sierra Nevada:\n\nIn 1870 he married Lucy Mayotta Browne. In 1871 and 1872 he was Professor of Topographical Engineering at Harvard University. Later, he was a mining engineer at Virginia City, Nevada, San Francisco, California, and Mexico.\nCharles Hoffmann died 1913 in Oakland, California.\n\nMount Hoffmann, a high peak in central Yosemite National Park, is named after him.\n\n"}
{"id": "30404380", "url": "https://en.wikipedia.org/wiki?curid=30404380", "title": "Charles Joseph Devillers", "text": "Charles Joseph Devillers\n\nCharles Joseph Devillers or de Villers (1724 in Rennes – 1810) was a French naturalist.\n\nCharles Devillers was a member of l’Académie des sciences belles-lettres et arts de Lyon from 1764 to 1810. He had a cabinet of curiosities and was interested in physics and mathematics. He published \"Caroli Linnaei entomologia\", in 1789, a collection of the insect descriptions of Carl von Linné. He was a friend of Philibert Commerson (1727–1773), Jean-Emmanuel Gilibert (1741–1814) and Marc Antoine Louis Claret de La Tourrette (1729–1793).\n\n"}
{"id": "19363963", "url": "https://en.wikipedia.org/wiki?curid=19363963", "title": "Collisional family", "text": "Collisional family\n\nIn astronomy, a collisional family is a group of objects that are thought to have a common origin in an impact (collision). They have similar compositions, and most share similar orbital elements.\n\nKnown or suspected collisional families include numerous asteroid families, most of the irregular moons of the outer planets, the Earth and the Moon, and the dwarf planets Pluto, Eris, and Haumea and their moons.\n"}
{"id": "34728827", "url": "https://en.wikipedia.org/wiki?curid=34728827", "title": "Communication, Culture &amp; Critique", "text": "Communication, Culture &amp; Critique\n\nCommunication, Culture & Critique is a quarterly peer-reviewed academic journal covering research on the role of communication and cultural criticism, spanning the fields of communication, media, and cultural studies. It was established in 2008 with Karen Ross (University of Liverpool) as the founding editor-in-chief. Since 2014, the editor-in-chief has been Radhika Parameswaran (Indiana University Bloomington). It is published by Wiley-Blackwell on behalf of the International Communication Association.\n\nThe journal is abstracted and indexed in Communication & Mass Media Index and the MLA International Bibliography.\n"}
{"id": "54773435", "url": "https://en.wikipedia.org/wiki?curid=54773435", "title": "Curt Fredén", "text": "Curt Fredén\n\nCurt Fredén (born 1937) is a Swedish Quaternary geologist. Most of his work has centered on the Holocene geology of the Baltic Sea. He was a member of the landslide commission () that existed from 1988 to 1996. In 2002 he was awarded the prize Geologist of the Year () by Naturvetarna. He has been editor for \"Berg och jord\", the geology volume of the Swedish National Atlas and worked on various geological maps of Quaternary deposits. Fredén was one of geologists who helped make the High Coast a World Heritage Site.\n\nFredén has notably contributed to advance the understanding of the \"enigmatic\" Ancylus Lake and to discard the controversial Sveafallen at Degerfors as the lake's outlet.\n"}
{"id": "31117841", "url": "https://en.wikipedia.org/wiki?curid=31117841", "title": "EOn", "text": "EOn\n\neOn was a distributed computing project for the BOINC client, which uses theoretical chemistry techniques to solve problems in condensed matter physics and materials science. It was a project of the Institute for Computational Engineering and Sciences at the University of Texas.\n\nTraditional molecular dynamics can accurately model events that occur within a fraction of a millisecond. In order to model events that take place on much longer timescales, Eon combines transition state theory with kinetic Monte Carlo. The result is a combination of classical mechanics and quantum methods like density functional theory.\n\nSince the generation of new work units depended on the results of previous units, the project could only give each host a few units at a time.\n\nOn May 26, 2014, it was announced that eOn would be retiring from BOINC.\n\n"}
{"id": "57444135", "url": "https://en.wikipedia.org/wiki?curid=57444135", "title": "Electron–ion collider", "text": "Electron–ion collider\n\nAn electron–ion collider (EIC) is a proposed type of particle accelerator designed to collide spin-polarized beams of electrons and ions, in order to study the properties of nuclear matter in detail via deep inelastic scattering. In 2015, the Department of Energy Nuclear Science Advisory Committee (NSAC) named the construction of an electron–ion collider one of the top priorities for the near future in nuclear physics in the United States.\n\nBoth Brookhaven National Laboratory and Thomas Jefferson National Accelerator Facility have proposed designs for an EIC.\n\nBrookhaven National Laboratory's conceptual design, eRHIC, proposes upgrading the existing Relativistic Heavy Ion Collider, which collides beams light to heavy ions including polarized protons, with a polarized electron facility.\n\nJefferson Lab's conceptual design, JLEIC (formerly known as MEIC), proposes upgrading the existing CEBAF 12 GeV electron accelerator with an electron ring built from magnets previously used in the PEP-II accelerator, and an ion complex built from new magnets.\n"}
{"id": "174945", "url": "https://en.wikipedia.org/wiki?curid=174945", "title": "Elementary charge", "text": "Elementary charge\n\nThe elementary charge, usually denoted by or sometimes , is the electric charge carried by a single proton, or equivalently, the magnitude of the electric charge carried by a single electron, which has charge . This elementary charge is a fundamental physical constant. To avoid confusion over its sign, \"e\" is sometimes called the elementary positive charge. This charge has a measured value of approximately (coulombs) and after the 20 May 2019, its value will be \"exactly\" by definition of the coulomb. In cgs units, it is .\n\nThe magnitude of the elementary charge was first measured in Robert A. Millikan's oil drop experiment in 1909.\n\nIn some natural unit systems, such as the system of atomic units, \"e\" functions as the unit of electric charge, that is \"e\" is equal to 1 e in those unit systems. The use of elementary charge as a unit was promoted by George Johnstone Stoney in 1874 for the first system of natural units, called Stoney units. Later, he proposed the name \"electron\" for this unit. At the time, the particle we now call the electron was not yet discovered and the difference between the particle \"electron\" and the unit of charge \"electron\" was still blurred. Later, the name \"electron\" was assigned to the particle and the unit of charge \"e\" lost its name. However, the unit of energy electronvolt reminds us that the elementary charge was once called \"electron\".\n\nThe maximum capacity of each pixel in a charge-coupled device image sensor, known as the \"well depth\", is typically given in units of electrons, commonly around 10 per pixel.\n\nIn high-energy physics (HEP) Lorentz–Heaviside units are used, and the charge unit is a dependent one, formula_1, so that = 0.30282212088 .\n\n\"Charge quantization\" is the principle that the charge of any object is an integer multiple of the elementary charge. Thus, an object's charge can be exactly 0 \"e\", or exactly 1 \"e\", −1 \"e\", 2 \"e\", etc., but not, say,  \"e\", or −3.8 \"e\", etc. (There may be exceptions to this statement, depending on how \"object\" is defined; see below.)\n\nThis is the reason for the terminology \"elementary charge\": it is meant to imply that it is an indivisible unit of charge.\n\nThere are two known sorts of exceptions to the indivisibility of the elementary charge: quarks and quasiparticles.\n\n\nAll known elementary particles, including quarks, have charges that are integer multiples of  \"e\". Therefore, one can say that the \"quantum of charge\" is  \"e\". In this case, one says that the \"elementary charge\" is three times as large as the \"quantum of charge\".\n\nOn the other hand, all \"isolatable\" particles have charges that are integer multiples of \"e\". (Quarks cannot be isolated: they only exist in collective states like protons that have total charges that are integer multiples of \"e\".) Therefore, one can say that the \"quantum of charge\" is \"e\", with the proviso that quarks are not to be included. In this case, \"elementary charge\" would be synonymous with the \"quantum of charge\".\n\nIn fact, both terminologies are used. For this reason, phrases like \"the quantum of charge\" or \"the indivisible unit of charge\" can be ambiguous, unless further specification is given. On the other hand, the term \"elementary charge\" is unambiguous: it refers to a quantity of charge equal to that of a proton.\n\nIf the Avogadro constant \"N\" and the Faraday constant \"F\" are independently known, the value of the elementary charge can be deduced, using the formula\n\nThis method is \"not\" how the \"most accurate\" values are measured today: Nevertheless, it is a legitimate and still quite accurate method, and experimental methodologies are described below:\n\nThe value of the Avogadro constant \"N\" was first approximated by Johann Josef Loschmidt who, in 1865, estimated the average diameter of the molecules in air by a method that is equivalent to calculating the number of particles in a given volume of gas. Today the value of \"N\" can be measured at very high accuracy by taking an extremely pure crystal (often silicon), measuring how far apart the atoms are spaced using X-ray diffraction or another method, and accurately measuring the density of the crystal. From this information, one can deduce the mass (\"m\") of a single atom; and since the molar mass (\"M\") is known, the number of atoms in a mole can be calculated: \"N\" = \"M\"/\"m\".\n\nThe value of \"F\" can be measured directly using Faraday's laws of electrolysis. Faraday's laws of electrolysis are quantitative relationships based on the electrochemical researches published by Michael Faraday in 1834. In an electrolysis experiment, there is a one-to-one correspondence between the electrons passing through the anode-to-cathode wire and the ions that plate onto or off of the anode or cathode. Measuring the mass change of the anode or cathode, and the total charge passing through the wire (which can be measured as the time-integral of electric current), and also taking into account the molar mass of the ions, one can deduce \"F\".\n\nThe limit to the precision of the method is the measurement of \"F\": the best experimental value has a relative uncertainty of 1.6 ppm, about thirty times higher than other modern methods of measuring or calculating the elementary charge.\n\nA famous method for measuring \"e\" is Millikan's oil-drop experiment. A small drop of oil in an electric field would move at a rate that balanced the forces of gravity, viscosity (of traveling through the air), and electric force. The forces due to gravity and viscosity could be calculated based on the size and velocity of the oil drop, so electric force could be deduced. Since electric force, in turn, is the product of the electric charge and the known electric field, the electric charge of the oil drop could be accurately computed. By measuring the charges of many different oil drops, it can be seen that the charges are all integer multiples of a single small charge, namely \"e\".\n\nThe necessity of measuring the size of the oil droplets can be eliminated by using tiny plastic spheres of a uniform size. The force due to viscosity can be eliminated by adjusting the strength of the electric field so that the sphere hovers motionless.\n\nAny electric current will be associated with noise from a variety of sources, one of which is shot noise. Shot noise exists because a current is not a smooth continual flow; instead, a current is made up of discrete electrons that pass by one at a time. By carefully analyzing the noise of a current, the charge of an electron can be calculated. This method, first proposed by Walter H. Schottky, can determine a value of \"e\" of which the accuracy is limited to a few percent. However, it was used in the first direct observation of Laughlin quasiparticles, implicated in the fractional quantum Hall effect.\n\nAnother accurate method for measuring the elementary charge is by inferring it from measurements of two effects in quantum mechanics: The Josephson effect, voltage oscillations that arise in certain superconducting structures; and the quantum Hall effect, a quantum effect of electrons at low temperatures, strong magnetic fields, and confinement into two dimensions. The Josephson constant is\n(where \"h\" is the Planck constant). It can be measured directly using the Josephson effect.\n\nThe von Klitzing constant is\nIt can be measured directly using the quantum Hall effect.\n\nFrom these two constants, the elementary charge can be deduced:\n\nIn the most recent CODATA adjustments, the elementary charge is not an independently defined quantity. Instead, a value is derived from the relation\nwhere \"h\" is the Planck constant, \"α\" is the fine-structure constant, \"μ\" is the magnetic constant, \"ε\" is the electric constant, and \"c\" is the speed of light. The uncertainty in the value of \"e\" is currently determined almost entirely by the uncertainty in the Planck constant.\n\nThe most precise values of the Planck constant come from Kibble balance experiments, which are used to measure the product \"K\"\"R\". The most precise values of the fine structure constant come from comparisons of the measured and calculated value of the gyromagnetic ratio of the electron.\n\n\n"}
{"id": "15964561", "url": "https://en.wikipedia.org/wiki?curid=15964561", "title": "Forensic materials engineering", "text": "Forensic materials engineering\n\nForensic materials engineering, a branch of forensic engineering, focuses on the material evidence from crime or accident scenes, seeking defects in those materials which might explain why an accident occurred, or the source of a specific material to identify a criminal. Many analytical methods used for material identification may be used in investigations, the exact set being determined by the nature of the material in question, be it metal, glass, ceramic, polymer or composite. An important aspect is the analysis of trace evidence such as skid marks on exposed surfaces, where contact between dissimilar materials leaves material traces of one left on the other. Provided the traces can be analysed successfully, then an accident or crime can often be reconstructed. Another aim will be to determine the cause of a broken component using the technique of fractography.\n\nMetal and alloy surfaces can be analyzed in a number of ways, including by spectroscopy and EDX used during scanning electron microscopy. The nature and composition of the metal can normally be established by sectioning and polishing the bulk, and examining the flat section using optical microscopy after etching solutions have been used to provide contrast in the section between alloy constituents. Such solutions (often an acid) attack the surface preferentially, so isolating features or inclusions of one composition, enabling them to be seen much more clearly than in the polished but untreated surface. Metallography is a routine technique for examining the microstructure of metals, but can also be applied to ceramics, glasses and polymers. SEM can often be critical in determining failures modes by examining fracture surfaces. The origin of a crack can be found and the way it grew assessed, to distinguish, for example, overload failure from fatigue. Often however, fatigue fractures are easy to distinguish from overload failures by the lack of ductility, and the existence of a fast crack growth region and the slow crack growth area on the fracture surface. Crankshaft fatigue for example is a common failure mode for engine parts. The example shows just two such zones, the slow crack at the base, the fast at the top.\n\nHard products like ceramic pottery and glass windscreens can be studied using the same SEM methods used for metals, especially ESEM conducted at low vacuum. Fracture surfaces are especially valuable sources of information because surface features like hachures can enable the origin or origins of the cracks to be found. Analysis of the surface features is carried out using fractography.\n\nThe position of the origin can then be matched with likely loads on the product to show how an accident occurred, for example. Inspection of bullet holes can often show the direction of travel and energy of the impact, and the way common glass products like bottles can be analysed to show whether deliberately or accidentally broken in a crime or accident. Defects such as foreign particles will often occur near or at the origin of the critical crack, and can be readily identified by ESEM.\n\nThermoplastics, thermosets, and composites can be analyzed using FTIR and UV spectroscopy as well as NMR and ESEM. Failed samples can either be dissolved in a suitable solvent and examined directly (UV, IR and NMR sepctroscopy) or as a thin film cast from solvent or cut using microtomy from the solid product. The slicing method is preferable since there are no complications from solvent absorption, and the integrity of the sample is partly preserved. Fractured products can be examined using fractography, an especially useful method for all fractured components using macrophotography and optical microscopy. Although polymers usually possess quite different properties to metals and ceramics, they are just as susceptible to failure from mechanical overload, fatigue and stress corrosion cracking if products are poorly designed or manufactured. Many plastics are susceptible to attack by active chemicals like chlorine, present at low levels in potable water supplies, especially if the injection mouldings are faulty.\n\nESEM is especially useful for providing elemental analysis from viewed parts of the sample being investigated. It is effectively a technique of microanalysis and valuable for examination of trace evidence. On the other hand, colour rendition is absent, and there is no information provided about the way in which those elements are bonded to one another. Specimens will be exposed to a vacuum, so any volatiles may be removed, and surfaces may be contaminated by substances used to attach the sample to the mount.\n\nRubber products are often safety-critical parts of machines, so that failure can often cause accidents or loss of function. Failed products can be examined with many of the generic polymer methods, although it is more difficult if the sample is vulcanized or cross-linked. Attenuated total reflectance infra-red spectroscopy is useful because the product is usually flexible so can be pressed against the selenium crystal used for analysis. Simple swelling tests can also help to identify the specific elastomer used in a product. Often the best technique is ESEM using the X-ray elemental analysis facility on the microscope. Although the method only provides elemental analysis, it can provide clues as to the identity of the elastomer being examined. Thus the presence of substantial amounts of chlorine indicates polychloroprene while the presence of nitrogen indicates nitrile rubber. The method is also useful in confirming ozone cracking by the large amounts of oxygen present on cracked surfaces. Ozone attacks susceptible elastomers such as natural rubber, nitrile rubber and polybutadiene and associated copolymers. Such elastomers possess double bonds in their main chains, the group which is attacked during ozonolysis.\n\nThe problem occurs when small concentrations of ozone gas are present near to exposed elastomer surfaces, such as O-rings and diaphragm seals. The product must be in tension, but only very low strains are sufficient to cause degradation.\n\n\n"}
{"id": "36174770", "url": "https://en.wikipedia.org/wiki?curid=36174770", "title": "HARPS-N", "text": "HARPS-N\n\nHARPS-N, the High Accuracy Radial velocity Planet Searcher for the Northern hemisphere is a high-precision radial-velocity spectrograph, installed at the Italian Telescopio Nazionale Galileo, a 3.58-metre telescope located at the Roque de los Muchachos Observatory on the island of La Palma, Canary Islands, Spain.\n\nHARPS-N is the counterpart for the Northern Hemisphere of the similar HARPS instrument installed at on the ESO 3.6 m Telescope at La Silla Observatory in Chile. It allows for planetary research in the northern sky which hosts the Cygnus and Lyra constellations. In particular it allows for detailed follow up research to Kepler mission planet candidates, which are located in the Cygnus constellation region.\n\nThe instrument's main scientific goals are the discovery and characterization of terrestrial super-Earths by combining the measurements using transit photometry and doppler spectroscopy which provide both, the size and mass of the exoplanet. Based on the resulting density, rocky (terrestrial) Super-Earths can be distinguished from gaseous exoplanets.\n\nThe HARPS-N Project is a collaboration between the Geneva Observatory (lead), the Center for Astrophysics in Cambridge (Massachusetts), the Universities of St. Andrews and Edinburgh, the Queen's University Belfast, the UK Astronomy Technology Centre and the Italian Istituto Nazionale di Astrofisica.\n\nFirst light on sky was obtained by HARPS-N on March 27, 2012 and official operations started on August 1, 2012.\n\n\n"}
{"id": "15010858", "url": "https://en.wikipedia.org/wiki?curid=15010858", "title": "Ice V", "text": "Ice V\n\nIce V (pronounced \"ice five\") is a monoclinic crystalline phase of water, formed by cooling water to 253 K at 500 MPa. It has a complicated structure, including 4-membered, 5-membered, 6-membered, and 8-membered rings and a total of 28 molecules in the unit cell. Ganymede's interior probably includes a liquid water ocean with tens to hundreds of kilometers of Ice V at its base.\n"}
{"id": "44706819", "url": "https://en.wikipedia.org/wiki?curid=44706819", "title": "Ice XVI", "text": "Ice XVI\n\nIce XVI is the least dense (0.81 g/cm) experimentally obtained crystalline form of ice. It is topologically equivalent to the empty structure of sII clathrate hydrates. It was first obtained in 2014 by removing gas molecules from a neon clathrate under vacuum at temperatures below 147 K. The resulting empty water frame, ice XVI, is thermodynamically unstable at the experimental conditions, yet it can be preserved at cryogenic temperatures. Above 145–147 K at positive pressures ice XVI transforms into the stacking-faulty Ice I and further into ordinary Ice I . Theoretical studies predict Ice XVI to be thermodynamically stable at negative pressures (that is under tension). \n"}
{"id": "39155654", "url": "https://en.wikipedia.org/wiki?curid=39155654", "title": "International Society on Toxinology", "text": "International Society on Toxinology\n\nInternational Society on Toxinology (IST) is a global society of scientists and clinicians working for the advancement of venoms, poisons and toxins. It was founded in 1962 with an aim to advance knowledge on venoms, poisons and toxins of animal, plant and microbial origin.\n\nIn 1664 an Italian polymath Francesco Redi wrote his first monumental work \"Osservazioni intorno alle vipere\" (Observations about the Viper) in which he first elucidated the scientific basis of snakebite and venom of the viper. He showed that the venom is produced from the fang, it is not poisonous when swallowed, and dangerous only when it enters the bloodstream. He even showed how to stop or slow down the venom action in the blood by tight ligature before the wound. This work is heralded as the foundation of toxinology and the beginning of experimental toxicology.\n\nIn 1962 a group of scientists and clinicians who have made significant contributions to toxinology joined forces to establish a working society. The Society had its first international meeting in 1966 in Atlantic City, USA. Findlay E. Russell was the first president of the Society.\n\nThe Society aims to advance knowledge on the properties of toxins and antitoxins and to bring together scholars interested in the field. Full membership is open to those who have published meritorious original investigations in toxinology, while persons who do not qualify for membership but are interested in the field of toxinology are eligible for associate membership. There is also student membership. World Congresses of the Society and Symposia of the IST-Sections (European, Pan-American, Asia-Pacific) are regularly organised to promote communication and exchange of results in toxinology research. The officers of the Society, the President and Secretary-Treasurer, are assisted by a Council and they all are elected by the members.\n\nIST sponsors the \"Redi Award\" to honour scientists who have contributed distinguished work in the field of toxinology. This is the most prestigious award in the world in toxinology. The award is given at each World Congress of IST (generally held every three years) since 1967.\n\nAt the Asia-Pacific Section Congress in Vietnam in December 2008, a proposal by Prof David Warrell (recipient of the Redi Award 2012) that \"The Global Snakebite Initiative be formally endorsed as an official initiative of the IST\" was passed, and confirmed unanimously at the IST World Congress in Recife, Brazil, in March 2009. The Global Snakebite Initiative is the follow-up of the World Health Organization programme on snakebite as one of the Neglected Tropical Diseases.\n\n\"Toxicon\" is the official journal of the Society, started in 1963, and is now published monthly by Elsevier.\n\n"}
{"id": "1695906", "url": "https://en.wikipedia.org/wiki?curid=1695906", "title": "International Standard Classification of Education", "text": "International Standard Classification of Education\n\nThe International Standard Classification of Education (ISCED) is a statistical framework for organizing information on education maintained by the United Nations Educational, Scientific and Cultural Organization (UNESCO). It is a member of the international family of economic and social classifications of the United Nations.\nThe ISCED was designed in the early 2009 to serve ‘as an instrument suitable for assembling, compiling and presenting statistics of education both within individual countries and internationally’. The first version, known as ISCED 2001, was approved by the International Conference on Education (Geneva, 1975), and was subsequently endorsed by UNESCO’s 19th General Conference in 1976.\n\nThe second version, known as ISCED 1997, was approved by the UNESCO General Conference at its 29th session in November 1997 as part of efforts to increase the international comparability of education statistics. It covered primarily two cross-classification variables: \"levels\" (7) and \"fields\" of education (25). The UNESCO Institute for Statistics led the development of a third version, which was adopted by UNESCO’s 36th General Conference in November 2011 and which will replace ISCED 1997 in international data collections in the coming years. ISCED 2011 has 9 rather than 7 levels, created by dividing the tertiary pre-doctorate level into three levels. It also extended the lowest level (ISCED 0) to cover a new sub-category of early childhood educational development programmes, which target children below the age of 3 years.\n\nDuring the review and revision, which led to the adoption of ISCED 2011, UNESCO Member States agreed that the fields of education should be examined in a separate process. This review is now underway with the view to establishing an independent but related classification called the ISCED Fields of Education and Training.\n\nRelated materials from the European Centre for the Development of Vocational Training and also Eurostat provide further information and statistical guidance for the classification of sub-fields of education as a companion to ISCED.\n\n\nSource:International Standard Classification of Education (ISCED). \n\n\n\n"}
{"id": "1465085", "url": "https://en.wikipedia.org/wiki?curid=1465085", "title": "Lindelöf hypothesis", "text": "Lindelöf hypothesis\n\nIn mathematics, the Lindelöf hypothesis is a conjecture by Finnish mathematician Ernst Leonard Lindelöf (see ) about the rate of growth of the Riemann zeta function on the critical line. This hypothesis is implied by the Riemann hypothesis. It says that, for any \"ε\" > 0,\n\nas \"t\" tends to infinity (see O notation). Since \"ε\" can be replaced by a smaller value, we can also write the conjecture as, for any positive \"ε\",\n\nIf σ is real, then μ(σ) is defined to be the infimum of all real numbers \"a\" such that \"ζ\"(\"σ\" + \"iT\") = O(\"T\"). It is trivial to check that \"μ\"(\"σ\") = 0 for \"σ\" > 1, and the functional equation of the zeta function implies that μ(\"σ\") = \"μ\"(1 − \"σ\") − \"σ\" + 1/2. The Phragmen–Lindelöf theorem implies that μ is a convex function. The Lindelöf hypothesis states μ(1/2) = 0, which together with the above properties of \"μ\" implies that \"μ\"(\"σ\") is 0 for \"σ\" ≥ 1/2 and 1/2 − σ for \"σ\" ≤ 1/2.\n\nLindelöf's convexity result together with \"μ\"(1) = 0 and \"μ\"(0) = 1/2 implies that 0 ≤ \"μ\"(1/2) ≤ 1/4. The upper bound of 1/4 was lowered by Hardy and Littlewood to 1/6 by applying Weyl's method of estimating exponential sums to the approximate functional equation. It has since been lowered to slightly less than 1/6 by several authors using long and technical proofs, as in the following table:\n (1918–1919) showed that the Lindelöf hypothesis is equivalent to the following statement about the zeros of the zeta function: for every \"ε\" > 0, the number of zeros with real part at least 1/2 + \"ε\" and imaginary part between \"T\" and \"T\" + 1 is o(log(\"T\")) as \"T\" tends to infinity. The Riemann hypothesis implies that there are no zeros at all in this region and so implies the Lindelöf hypothesis. The number of zeros with imaginary part between \"T\" and \"T\" + 1 is known to be O(log(\"T\")), so the Lindelöf hypothesis seems only slightly stronger than what has already been proved, but in spite of this it has resisted all attempts to prove it.\n\nThe Lindelöf hypothesis is equivalent to the statement that\n\nfor all positive integers \"k\" and all positive real numbers ε. This has been proved for \"k\" = 1 or 2, but the case \"k\" = 3 seems much harder and is still an open problem.\n\nThere is a much more precise conjecture about the asymptotic behavior of the integral: it is believed that\n\nfor some constants \"c\". This has been proved by Littlewood for \"k\" = 1 and by for \"k\" = 2 \n(extending a result of who found the leading term).\n\nfor the leading coefficient when \"k\" is 6, and used random matrix theory to suggest some conjectures for the values of the coefficients for higher \"k\". The leading coefficients are conjectured to be the product of an elementary factor, a certain product over primes, and the number of \"n\" by \"n\" Young tableaux given by the sequence\n\nDenoting by \"p\" the \"n\"-th prime number, a result by Albert Ingham, shows that the Lindelöf hypothesis implies that, for any \"ε\" > 0,\n\nif \"n\" is sufficiently large. However, this result is much worse than that of the large prime gap conjecture.\n\n"}
{"id": "52751822", "url": "https://en.wikipedia.org/wiki?curid=52751822", "title": "List of British computers", "text": "List of British computers\n\nThis is a list of computers designed in Britain.\n\n"}
{"id": "45050138", "url": "https://en.wikipedia.org/wiki?curid=45050138", "title": "List of Royal Society of Chemistry journals", "text": "List of Royal Society of Chemistry journals\n\nThis is a list of scientific journals published by the Royal Society of Chemistry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "231877", "url": "https://en.wikipedia.org/wiki?curid=231877", "title": "List of emulators", "text": "List of emulators\n\nThis article lists software emulators.\n\n\n\n\n\n\n\n\n\nCisco Packet Tracer\n\nRouter Sim\n\n\n\n\n"}
{"id": "21938890", "url": "https://en.wikipedia.org/wiki?curid=21938890", "title": "List of interstellar radio messages", "text": "List of interstellar radio messages\n\nThis is a list of interstellar radio messages.\n\nThere are eleven realized IRM projects:\n\n\nThe Across the Universe message, A Simple Response to an Elemental Message and Hello From Earth are not always considered serious. The first two of them were sent to Polaris, which is 431 light years distant from us and whose planetary system, even if it exists, may not be suited for life, because it is a supergiant star, spectral type F7Ib which is only 70 million years old. In addition, both transmission rates were very high, about 128 kbit/s, for such moderate transmitter power (about 18 kW). The main defect of the \"Hello From Earth\" is an insufficient scientific and technical justification, since no famous SETI scientist made statements with validation of HFE's design. As it follows from : \"After the final message was collected on Monday 24 August 2009, messages were exported as a text file and sent to NASA's Jet Propulsion Laboratory in California, where they were encoded into binary, packaged and tested before transmission\", but nobody explained why he hopes that such encoded and packaged text will be understood and conceived by possible extraterrestrials.\n\nSome use the term \"Active SETI Project\", but Alexander Zaitsev, who was a scientific head of composing and transmissions of Cosmic Call 1999 & 2003, and Teen Age Message 2001, and a scientific consultant of A Message From Earth, emphasized that he considers above IRMs as the METI (\"Messaging to Extra-Terrestrial Intelligence Projects\").\n\nThese seven messages have targeted stars between 20 and 69 light-years from the Earth. The exception is the Arecibo message, which targeted globular cluster M13, approximately 24,000 light-years away. The first message to reach its destination will be A Message From Earth, which should reach the Gliese 581 planetary system in Libra in 2029.\n\nOn 13 February 2015, scientists (including Geoffrey Marcy, Seth Shostak, Frank Drake, Elon Musk and David Brin) at a convention of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea; one result was a statement, (which was not signed by Seth Shostak or Frank Drake), that a \"worldwide scientific, political and humanitarian discussion must occur before any message is sent\".\n\nStars to which messages were sent include:\n\nAlong with serious IRM projects, a bulk of pseudo-METI projects also exist:\n\n"}
{"id": "14485692", "url": "https://en.wikipedia.org/wiki?curid=14485692", "title": "List of members of the National Academy of Sciences (Engineering sciences)", "text": "List of members of the National Academy of Sciences (Engineering sciences)\n\nThe designation (d) after the name means the member is deceased.\n"}
{"id": "468843", "url": "https://en.wikipedia.org/wiki?curid=468843", "title": "List of moments of inertia", "text": "List of moments of inertia\n\nIn physics and applied mathematics, the mass moment of inertia, usually denoted by , measures the extent to which an object resists rotational acceleration about a particular axis, and is the rotational analogue to mass. Mass moments of inertia have units of dimension ML([mass] × [length]). It should not be confused with the second moment of area, which is used in beam calculations. The mass moment of inertia is often also known as the rotational inertia, and sometimes as the angular mass.\n\nFor simple objects with geometric symmetry, one can often determine the moment of inertia in an exact closed-form expression. Typically this occurs when the mass density is constant, but in some cases the density can vary throughout the object as well. In general, it may not be straightforward to symbolically express the moment of inertia of shapes with more complicated mass distributions and lacking symmetry. When calculating moments of inertia, it is useful to remember that it is an additive function and exploit the parallel axis and perpendicular axis theorems.\n\nThis article mainly considers symmetric mass distributions, with constant density throughout the object, and the axis of rotation is taken to be through the center of mass unless otherwise specified.\n\nFollowing are scalar moments of inertia. In general, the moment of inertia is a tensor, see below.\n\n\\right)</math>\n\nThis list of moment of inertia tensors is given for principal axes of each object.\n\nTo obtain the scalar moments of inertia I above, the tensor moment of inertia I is projected along some axis defined by a unit vector n according to the formula:\n\nwhere the dots indicate tensor contraction and the Einstein summation convention is used. In the above table, n would be the unit Cartesian basis e, e, e to obtain \"I\", \"I\", \"I\" respectively.\n\n\n"}
{"id": "57723846", "url": "https://en.wikipedia.org/wiki?curid=57723846", "title": "List of volcanoes in Algeria", "text": "List of volcanoes in Algeria\n\nThis is a list of volcanoes in Algeria.\n"}
{"id": "3012803", "url": "https://en.wikipedia.org/wiki?curid=3012803", "title": "Lucien Cayeux", "text": "Lucien Cayeux\n\nLucien Cayeux (26 March 1864–1 November 1944) was a French sedimentary petrographer.\n\nIn 1902, he joined the \"l'Ecole des Mines\" and become a professor of geology. In 1912, he was named as professor of geology at the Collège de France. He was admitted to the Académie des Sciences in 1928.\n\nHe is noted for his study of sediments with the polarizing microscope, and was one of the pioneers in this field.\n\nThe wrinkle ridge Dorsum Cayeux on the moon is named after him.\n\nHe also published a number of scientific papers.\n\n"}
{"id": "30193699", "url": "https://en.wikipedia.org/wiki?curid=30193699", "title": "Metamaterials Handbook", "text": "Metamaterials Handbook\n\nMetamaterials Handbook is a two-volume handbook on metamaterials edited by Filippo Capolino (University of California).\n\nThe series is designed to cover all theory and application topics related to electromagnetic metamaterials. Disciplines have combined to study, and develop electromagnetic metamaterials. Some of these disciplines are optics, physics, electromagnetic theory (including computational methods) microfabrication, microwaves, nanofabrication, nanotechnology, and nanochemistry.\n\n\"Theory and Phenomena of Metamaterials\" is the first volume of the \"Metamaterials Handbook\". It contains contributions from researchers (scientists) who have produced accepted results in the field of metamaterials. Most of the contributors are associated with Metamorphose VI AISBL, a non-profit, European organization that focuses on artificial electromagnetic materials and metamaterials. Metamorphose provided access to the network of contributors (researchers) who work in a variety of scientific disciplines, involved with metamaterials\n\nThis book is in an article review format, covering prior work in metamaterials. It focuses on theories underpinning metamaterial research along with the properties of metamaterials. The text covers all areas of metamaterial research.\n\n\"Applications of Metamaterials\" is the second volume of the \"Metamaterials Handbook\". This book derives its organization for discussion of its topics from the previous volume. Theory, modeling, and basic properties of metamaterials that were explored in the first volume, are now shown how they work when applied. Devices based on electromagnetic metamaterials continue to expand understanding of principles and modeling begun in the first volume. The applications for metamaterials are shown to be wide-ranging, encompassing electronics, telecommunications, sensing, medical instrumentation, and data storage. This book also discusses the key domains of where metamaterials have already been developed.\n\nThe material in this book is obtained from highly regarded sources, such as many scientific, peer reviewed, journal articles.\n\n"}
{"id": "26235145", "url": "https://en.wikipedia.org/wiki?curid=26235145", "title": "Mini-international neuropsychiatric interview", "text": "Mini-international neuropsychiatric interview\n\nThe Mini-international neuropsychiatric interview is a short structured clinical interview which enables researchers to make diagnoses of psychiatric disorders according to DSM-IV or ICD-10. The administration time of the interview is approximately 15 minutes and was designed for epidemiological studies and multicenter clinical trials.\n\n"}
{"id": "488361", "url": "https://en.wikipedia.org/wiki?curid=488361", "title": "Mining engineering", "text": "Mining engineering\n\nMining engineering is an engineering discipline that applies science and technology to the extraction of minerals from the earth. Mining engineering is associated with many other disciplines, such as geology, mineral processing and metallurgy, geotechnical engineering and surveying. A mining engineer may manage any phase of mining operations – from exploration and discovery of the mineral resource, through feasibility study, mine design, development of plans, production and operations to mine closure.\n\nWith the process of Mineral extraction, some amount of waste and uneconomic material are generated which are the primary source of pollution in the vicinity of mines. Mining activities by their nature cause a disturbance of the natural environment in and around which the minerals are located. Mining engineers must therefore be concerned not only with the production and processing of mineral commodities, but also with the mitigation of damage to the environment both during and after mining as a result of the change in the mining area.\n\nFrom prehistoric times to the present, mining has played a significant role in the existence of the human race. Since the beginning of civilization people have used stone and ceramics and, later, metals found on or close to the Earth's surface. These were used to manufacture early tools and weapons. For example, high quality flint found in northern France and southern England were used to set fire and break rock. Flint mines have been found in chalk areas where seams of the stone were followed underground by shafts and galleries. The oldest known mine on archaeological record is the \"Lion Cave\" in Swaziland. At this site, which radiocarbon dating indicates to be about 43,000 years old, paleolithic humans mined mineral hematite, which contained iron and was ground to produce the red pigment ochre.\n\nThe ancient Romans were innovators of mining engineering. They developed large scale mining methods, such as the use of large volumes of water brought to the minehead by numerous aqueducts for hydraulic mining. The exposed rock was then attacked by fire-setting where fires were used to heat the rock, which would be quenched with a stream of water. The thermal shock cracked the rock, enabling it to be removed. In some mines the Romans utilized water-powered machinery such as reverse overshot water-wheels. These were used extensively in the copper mines at Rio Tinto in Spain, where one sequence comprised 16 such wheels arranged in pairs, lifting water about .\n\nBlack powder was first used in mining in Banská Štiavnica, Kingdom of Hungary (present-day Slovakia) in 1627. This allowed blasting of rock and earth to loosen and reveal ore veins, which was much faster than fire-setting. The Industrial Revolution saw further advances in mining technologies, including improved explosives and steam-powered pumps, lifts, and drills as long as they remained safe.\n\nThere are many ways to become a Mining Engineer but all include a university degree in Mining Engineering. Primarily, training includes a Bachelor of Engineering (B.Eng. or B.E.), Bachelor of Science (B.Sc. or B.S.), Bachelor of Technology (B.Tech.) or Bachelor of Applied Science (B.A.Sc.) in Mining Engineering. Depending on the country and jurisdiction, to be licensed as a mining engineer a Master's degree; Master of Engineering (M.Eng.), Master of Science (M.Sc or M.S.) or Master of Applied Science (M.A.Sc.) maybe required.\nThere are also mining engineers who have come from other disciplines e.g. from engineering fields like Mechanical Engineering, Civil Engineering, Electrical Engineering, Geomatics Engineering, Environmental Engineering or from science fields like Geology, Geophysics, Physics, Geomatics, Earth Science, Mathematics, However, this path requires taking a graduate degree such as M.Eng, M.S., M.Sc. or M.A.Sc. in Mining Engineering after graduating from a different quantitative undergraduate program in order to be qualified as a mining engineer.\n\nThe fundamental subjects of mining engineering study usually include:\n\nIn the United States, the University of Arizona offers a B.S. in Mining Engineering with tracks in mine operations, geomechanics, sustainable resource development and mineral processing. South Dakota School of Mines and Technology offers a B.S. in Mining Engineering and also an M.S. in Mining Engineering and Management and Colorado School of Mines offers a M.S. in Mining and Earth-Systems Engineering, also Doctorate (Ph.D.) degrees in Mining and Earth-Systems Engineering and Underground Construction and Tunnel Engineering respectively.\n\nIn Canada, McGill University offers both undergraduate (B.Sc. or B.Eng.) and graduate (M.Sc. or M.S.) degrees in Mining Engineering. and the University of British Columbia in Vancouver offers a Bachelor of Applied Science (B.A.Sc.) in Mining Engineering and also graduate degrees (M.A.Sc. or M.Eng and Ph.D.) in Mining Engineering.\n\nIn Europe most programs are integrated (B.S. plus M.S. into one) after the Bologna Process and take 5 years to complete. In Portugal, the University of Porto offers a M.Eng. in Mining and Geo-Environmental Engineering and in Spain the Technical University of Madrid offers degrees in Mining Engineering with tracks in Mining Technology, Mining Operations, Fuels and Explosives, Metallurgy.\n\nIn South Africa, the University of the Witwatersrand offers a 4-year Bachelor of Science in Engineering (B.Sc.(Eng.)) in Mining Engineering as well as graduate programs (M.Sc.(Eng.) and Ph.D.) in Mining Engineering.\n\nSome Mining Engineers go on to pursue Doctorate degree programs such as Doctor of Philosophy (Ph.D., DPhil), Doctor of Engineering (D.Eng., Eng.D.) these programs involve a very significant original research component and are usually seen as entry points into Academia.\n\nMining salaries are usually determined by the level of skill required, where the position is, and what kind of organization the engineer is working for. When comparing salaries from one region to another, cost of living and other factors need to be taken into consideration.\n\nMining engineers in India earn relatively high salaries in comparison to many other professions, with an average salary of $15,250. However, in comparison to mining engineer salaries in other regions, such as Canada, the United States, Australia and the United Kingdom, Indian salaries are low. In the United States, there are an estimated 6,150 employed mining engineers, with a mean yearly salary of U.S. $103,710.\n\nMineral exploration is the process of finding ores (commercially viable concentrations of minerals) to mine. Mineral exploration is a much more intensive, organized and professional form of mineral prospecting and, though it frequently uses the services of prospecting, the process of mineral exploration on the whole is much more involved.\n\nThe foremost stage of mining starts with the process of finding and exploration of the mineral deposit. In the initial process of mineral exploration, however, the role of geologists and surveyors is prominent in the pre-feasibility study of the future mining operation. Mineral exploration and estimation of reserve through various prospecting methods are done to determine the method and type of mining in addition to profitability condition.\n\nOnce a mineral discovery has been made, and has been determined to be of sufficient economic quality to mine, mining engineers will then work on developing a plan to mine this effectively and efficiently.\n\nThe discovery can be made from research of mineral maps, academic geological reports or local, state, and national geological reports. Other sources of information include property assays, well drilling logs, and local word of mouth. Mineral research may also include satellite and airborne photographs. Unless the mineral exploration is done on public property, the owners of the property may play a significant role in the exploration process, and may be the original discoverer of the mineral deposit.\n\nAfter a prospective mineral is located, the mining geologist and/or mining engineer then determines the ore properties. This may involve chemical analysis of the ore to determine the composition of the sample. Once the mineral properties are identified, the next step is determining the quantity of the ore. This involves determining the extent of the deposit as well as the purity of the ore. The geologist drills additional core samples to find the limits of the deposit or seam and calculates the quantity of valuable material present in the deposit.\n\nOnce the mineral identification and reserve amount is reasonably determined, the next step is to determine the feasibility of recovering the mineral deposit. A preliminary study shortly after the discovery of the deposit examines the market conditions such as the supply and demand of the mineral, the amount of ore needed to be moved to recover a certain quantity of that mineral as well as analysis of the cost associated with the operation. This pre-feasibility study determines whether the mining project is likely to be profitable; if it is then a more in-depth analysis of the deposit is undertaken. After the full extent of the ore body is known and has been examined by engineers, the feasibility study examines the cost of initial capital investment, methods of extraction, the cost of operation, an estimated length of time to payback, the gross revenue and net profit margin, any possible resale price of the land, the total life of the reserve, the total value of the reserve, investment in future projects, and the property owner or owners' contract. In addition, environmental impact, reclamation, possible legal ramifications and all government permitting are considered. These steps of analysis determine whether the mine company should proceed with the extraction of the minerals or whether the project should be abandoned. The mining company may decide to sell the rights to the reserve to a third party rather than develop it themselves, or the decision to proceed with extraction may be postponed indefinitely until market conditions become favorable.\n\nMining engineers working in an established mine may work as an engineer for operations improvement, further mineral exploration, and operation capitalization by determining where in the mine to add equipment and personnel. The engineer may also work in supervision and management, or as an equipment and mineral salesperson. In addition to engineering and operations, the <nowiki>mining engineer</nowiki> may work as an environmental, health and safety manager or design engineer.\n\nThe act of mining required different methods of extraction depending on the mineralogy, geology, and location of the resources. Characteristics such as mineral hardness, the mineral stratification, and access to that mineral will determine the method of extraction.\n\nGenerally, mining is either done from the surface or underground. Mining can also occur with both surface and underground operations taking place on the same reserve. Mining activity varies as to what method is employed to remove the mineral.\n\nSurface mining comprises 90% of the world's mineral tonnage output. Also called open pit mining, surface mining is removing minerals in formations that are at or near the surface. Ore retrieval is done by material removal from the land in its natural state. Surface mining often alters the land characteristics, shape, topography, and geological make-up.\n\nSurface mining involves quarrying which is excavating minerals by means of machinery such as cutting, cleaving, and breaking. Explosives are usually used to facilitate breakage. Hard rocks such as limestone, sand, gravel, and slate are generally quarried into a series of benches.\n\nStrip mining is done on softer minerals such as clays and phosphate are removed through use of mechanical shovels, track dozers, and front end loaders. Softer Coal seams can also be extracted this way.\n\nWith placer mining, minerals can also be removed from the bottoms of lakes, rivers, streams, and even the ocean by dredge mining. In addition, in-situ mining can be done from the surface using dissolving agents on the ore body and retrieving the ore via pumping. The pumped material is then set to leach for further processing. Hydraulic mining is utilized in forms of water jets to wash away either overburden or the ore itself.\n\nBlasting:<br>\nExplosives are used to break up a rock formation and aid in the collection of ore in a process called blasting. Blasting utilizes the heat and immense pressure of the detonated explosives to shatter and fracture a rock mass. The type of explosives used in mining are high explosives which vary in composition and performance properties. The mining engineer is responsible for the selection and proper placement of these explosives, in order to maximize efficiency and safety. Blasting occurs in many phases of the mining process, such as development of infrastructure as well as production of the ore.\n\nLeaching:<br>\nLeaching is the loss or extraction of certain materials from a carrier into a liquid (usually, but not always a solvent). Mostly used in rare-earth metals extraction.\n\nFlotation:<br>\nFlotation (also spelled floatation) involves phenomena related to the relative buoyancy of minerals.It is the most widely used metal separate method.\n\nElectrostatic separation:<br>\nSeparating minerals by electro-characteristic differences.\n\nGravity separation:<br>\nGravity separation is an industrial method of separating two components, either a suspension, or dry granular mixture where separating the components with gravity is sufficiently practical.\n\nMagnetic separation:<br>\nMagnetic separation is a process in which magnetically susceptible material is extracted from a mixture using a magnetic force. \n\nHydraulic separation:<br>\nHydraulic separation is a process that using the density difference to separate minerals. Before hydraulic separation, minerals were crushed into uniform size; because minerals have uniform size and different density will have different settling velocities in water, and that can be used to separate target minerals.\n\nLegal attention to Mining Health and Safety began in the late 19th century and in the subsequent 20th century progressed to a comprehensive and stringent codification of enforcement and mandatory health and safety regulation. A mining engineer in whatever role they occupy must follow all federal, state, and local mine safety laws.\n\nThe United States Congress, through the passage of the Federal Mine Safety and Health Act of 1977, known as the Miner's Act, created the Mine Safety and Health Administration (MSHA) under the US Department of Labor.\n\nThis comprehensive Act provides miners with rights against retaliation for reporting violations, consolidated regulation of coal mines with metallic and nonmetallic mines, and created the independent Federal Mine Safety and Health Review Commission to review MSHA's reported violations.\n\nThe Act as codified in Code of Federal Regulations § 30 (CFR § 30) covers all miners at an active mine. When a mining engineer works at an active mine he or she is subject to the same rights, violations, mandatory health and safety regulations, and mandatory training as any other worker at the mine. The mining engineer can be legally identified as a \"miner.\"\n\nThe Act establishes the rights of miners. The miner may report at any time a hazardous condition and request an inspection. The miners may elect a miners' representative to participate during an inspection, pre-inspection meeting, and post-inspection conference. The miners and miners' representative shall be paid for their time during all inspections and investigations.\n\nLand reclamation is regulated for surface and underground mines according to the Surface Mining Control and Reclamation Act of 1977. The law creates as a part of the Department of Interior, the Bureau of Surface Mining (OSM). OSM states on their website, “OSM is charged with balancing the nation’s need for continued domestic coal production with protection of the environment.”\n\nThe law requires that states set up their own Reclamation Departments and legislate laws related to reclamation for coal mining operations. The states may impose additional regulations and regulate other minerals in addition to coal for land reclamation.\n\n\n\n"}
{"id": "357616", "url": "https://en.wikipedia.org/wiki?curid=357616", "title": "Outline of software engineering", "text": "Outline of software engineering\n\nThe following outline is provided as an overview of and topical guide to software engineering:\n\nSoftware engineering – application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; that is the application of engineering to software.\n\nSkilled software engineers use technologies and practices from a variety of fields to improve their productivity in creating software and to improve the quality of the delivered product.\n\nSoftware engineers build software (applications, operating systems, system software) that people use.\n\nApplications influence software engineering by pressuring developers to solve problems in new ways. For example, consumer software emphasizes low cost, medical software emphasizes high quality, and Internet commerce software emphasizes rapid development.\n\nMany technologies and practices are (mostly) confined to software engineering,\nthough many of these are shared with computer science.\n\n\n\n\n\n\n\n\n\nA platform combines computer hardware and an operating system. As platforms grow more powerful and less costly, applications and tools grow more widely available.\n\n\n\nSkilled software engineers know a lot of computer science including what is possible and impossible, and what is easy and hard for software.\n\nDiscrete mathematics is a key foundation of software engineering.\n\nOther\n\n\nDeliverables must be developed for many SE projects. Software engineers rarely make all of these deliverables themselves. They usually cooperate with the writers, trainers, installers, marketers, technical support people, and others who make many of these deliverables.\n\n\n\n\n\n\nHistory of software engineering\n\nMany people made important contributions to SE technologies, practices, or applications.\n\nSee also\n\n\nSee also:\n\n\n\n\n"}
{"id": "27960249", "url": "https://en.wikipedia.org/wiki?curid=27960249", "title": "Parasitic oscillation", "text": "Parasitic oscillation\n\nParasitic oscillation is an undesirable electronic oscillation (cyclic variation in output voltage or current) in an electronic or digital device. It is often caused by feedback in an amplifying device. The problem occurs notably in RF, audio, and other electronic amplifiers as well as in digital signal processing. It is one of the fundamental issues addressed by control theory.\n\nParasitic oscillation is undesirable for several reasons. The oscillations may be coupled into other circuits or radiate as radio waves, causing electromagnetic interference (EMI) to other devices. In audio systems, parasitic oscillations can sometimes be heard as annoying sounds in the speakers or earphones. The oscillations waste power and may cause undesirable heating. For example, an audio power amplifier that goes into parasitic oscillation may generate enough power to damage connected speakers. A circuit that is oscillating will not amplify linearly, so desired signals passing through the stage will be distorted. In digital circuits, parasitic oscillations may only occur on particular logic transitions and may result in erratic operation of subsequent stages; for example, a counter stage may see many spurious pulses and count erratically.\n\nParasitic oscillation in an amplifier stage occurs when part of the output energy is coupled into the input, with the correct phase and amplitude to provide positive feedback at some frequency. The coupling can occur directly between input and output wiring with stray capacitance or mutual inductance between input and output. In some solid-state or vacuum electron devices there is sufficient internal capacitance to provide a feedback path. Since the ground is common to both input and output, output current flowing through the impedance of the ground connection can also couple signals back to the input.\n\nSimilarly, impedance in the power supply can couple input to output and cause oscillation. \nWhen a common power supply is used for several stages of amplification, the supply voltage may vary with the changing current in the output stage. The power supply voltage changes will appear in the input stage as positive feedback. An example is a transistor radio which plays well with a fresh battery, but squeals or \"motorboats\" when the battery is old. \n\nIn audio systems, if a microphone is placed close to a loudspeaker, parasitic oscillations may occur. This is caused by positive feedback, from amplifier's output to loudspeaker to sound waves, and back via the microphone to the amplifier input. See Audio feedback.\n\nFeedback control theory developed to address the problem of parasitic oscillation in servo control systems – the system oscillated rather than performing their intended function, for example velocity control in engines. The Barkhausen stability criterion gives the necessary condition for oscillation; the loop gain around the feedback loop, which is equal to the amplifier gain multiplied by the transfer function of the inadvertent feedback path, must be equal to one, and the phase shift around the loop must be zero or a multiple of 360° (2π radians). \n\nIn practice, feedback may occur over a range of frequencies (for example the operating range of an amplifier); at various frequencies, the phase of the amplifier may be different. If there is one frequency where the feedback is positive and the amplitude condition is also fulfilled – the system will oscillate at that frequency.\n\nThese conditions can be expressed in mathematical terms using the Nyquist plot. Another method used in control loop theory uses Bode plots of gain and phase vs. frequency. Using Bode plots, a design engineer checks whether there is a frequency where both conditions for oscillations are met: the phase is zero (positive feedback) and the loop gain is 1 or greater.\n\nWhen parasitic oscillations occur, the designer can use the various tools of control loop engineering to correct the situation – to reduce the gain or to change the phase at problematic frequencies.\n\nSeveral measures are used to prevent parasitic oscillation. Amplifier circuits are laid out so that input and output wiring are not adjacent, preventing capacitive or inductive coupling. A metal shield may be placed over sensitive portions of the circuit. Bypass capacitors may be put at power supply connections, to provide a low-impedance path for AC signals and prevent interstage coupling through the power supply. Where printed circuit boards are used, high- and low-power stages are separated and ground return traces are arranged so that heavy currents don't flow in mutually shared portions of the ground trace. In some cases the problem may only be solved by introduction of another feedback \"neutralization\" network, calculated and adjusted to eliminate the negative feedback within the passband of the amplifying device.\n"}
{"id": "12743856", "url": "https://en.wikipedia.org/wiki?curid=12743856", "title": "Polymer engineering", "text": "Polymer engineering\n\nPolymer engineering is generally an engineering field that designs, analyses, or modifies polymer materials. Polymer engineering covers aspects of the petrochemical industry, polymerization, structure and characterization of polymers, properties of polymers, compounding and processing of polymers and description of major polymers, structure property relations and applications.\n\nThe basic division of polymers into thermoplastics, elastomers and thermosets helps define their areas of application. The latter group of materials includes phenolic resins, polyesters and epoxy resins, all of which are used widely in composite materials when reinforced with stiff fibres such as fibreglass and aramids. Since crosslinking stabilises the thermoset polymer matrix of these materials, they have physical properties more similar to traditional engineering materials like steel. However, their very much lower densities compared with metals makes them ideal for lightweight structures. In addition, they suffer less from fatigue, so are ideal for safety-critical parts which are stressed regularly in service.\n\nThermoplastics have relatively low tensile moduli, but also have lower densities and properties such as transparency which make them ideal for consumer products and medical products. They include polyethylene, polypropylene, nylon, acetal resin, polycarbonate and PET, all of which are widely used materials.\n\nElastomers are polymers which have very low moduli and show reversible extension when strained, a valuable property for vibration absorption and damping. They may either be thermoplastic (in which case they are known as Thermoplastic elastomers) or crosslinked, as in most conventional rubber products such as tyres. Typical rubbers used conventionally include natural rubber, nitrile rubber, polychloroprene, polybutadiene, styrene-butadiene and fluorinated rubbers.\n\nTypical uses of composites are monocoque structures for aerospace and automobiles, as well as more mundane products like fishing rods and bicycles. The stealth bomber was the first all-composite aircraft, but many passenger aircraft like the Airbus and the Boeing 787 use an increasing proportion of composites in their fuselages, such as hydrophobic melamine foam. The quite different physical properties of composites gives designers a much greater freedom in shaping parts, which is why composite products often look different to conventional products. On the other hand, some products such as drive shafts, helicopter rotor blades, and propellers look identical to metal precursors owing to the basic functional needs of such components.\n\n\n\n"}
{"id": "25013400", "url": "https://en.wikipedia.org/wiki?curid=25013400", "title": "Predicted outcome value theory", "text": "Predicted outcome value theory\n\nPredicted outcome value theory introduced in 1986 by Michael Sunnafrank, posits that people seek information in initial interactions and relationships to determine the benefits of interpersonal relationships by predicting the value of future outcomes whether negative or positive. If a person predicts a positive outcome in the relationship this can lead to increased attraction, however if a person predicts a negative outcome then he or she would pursue limited interaction or possibly relationship termination. The processes of predicted outcome value directly link to continued relationship development and communication as well as stronger attraction and intimacy within the relationship.\n\nPredicted outcome value theory proposes that initial interaction behaviors serve two related functions in individuals' attempts to maximize future relational outcomes. First, communication is directed at reducing uncertainty (Berger & Calabrese, 1975) about new acquaintances to determine likely outcome-values for the relational future. Second, communication proceeds in a manner predicted to result in the most positive outcomes. In a broad sense, these outcome value predictions would lead to communicative attempts to terminate or curtail the conversation, to continue the entry-level conversation, or to escalate the conversation and relationship beyond this level. Attempts to continue or escalate would result from positive predicted outcome values, while attempts to terminate or curtail would result from negative predictions. Given this, predicted outcome value theory proposes that several specific behaviors associated with conversational termination-escalation should be related to predicted outcome value levels.\nAmong these, Sunnafrank (1986) posits that predicted outcome value is positively related to amount of verbal communication, intimacy level of communication content, nonverbal affiliative expressiveness, and liking.\n\n1. Attraction increases as the predicted outcome value increases\n2. Prediction of positive future outcomes leads to future interactions\n3. Prediction of negative future outcomes ends future interactions\n4. People focus and discuss topics in conversations that facilitate positive predicted outcomes\n\nPredicted outcome value theory is an alternative to uncertainty reduction theory, which Charles R. Berger and Richard J. Calabrese introduced in 1975. Uncertainty reduction theory states that the driving force in initial interactions is to collect information to predict attitudes and behaviors for future relationship development.\n\nSunnafrank challenges uncertainty reduction theory with his predicted outcome value theory in that initial interactions are not solely about reducing uncertainty within the relationship but motivated by the predicted outcome value. Sunnafrank ascertains that initial interactions only lead to continued relationship development after the person has determined future rewards and costs of that relationship.\n\nMichael Sunnafrank is a Professor in the Department of Communication, College of Liberal Arts at the University of Minnesota, Duluth. He joined the faculty at UMD in 1990 after fourteen years of teaching and research at Michigan State University, University of California Davis, Arizona State University and Texas A&M University. He was awarded his PhD and master's degrees by Michigan State University.\nSunnafrank continues to work on research testing predicted outcome value theory.\n\nSunnafrank's (1986) study examined a potential initial acquaintance association between perceived similarity and attraction may be present, though undetected, in previous interpersonal goals research and finds support for interpersonal goals claims regarding the perceived similarity/attraction relationship.\nThe findings extended Sunnafrank (1983, 1984, 1985) and Sunnafrank and Miller's (1981) claim that attitude similarity and attraction are unassociated in beginning communicative relationships. While their research supports this claim for attitude similarity revealed in pre-acquaintance, it also shows that this claim generalizes to more normally occurring perceptions of attitude similarity produced during initial relational stages. The results strongly suggested that traditional beliefs about, and theoretical interpretations of, the attitude similarity/attraction association are incorrect, at least regarding the initial acquaintance period.\n\nAnother study, \"Predicted Outcome Value in Initial Conversations\", provided the initial test of several predicted outcome value theory (Sunnafrank, 1986) propositions. Findings supported all hypotheses tested, and demonstrated that during initial conversations predicted outcome value is strongly and positively related to the amount of verbal communication, intimacy of communication content, nonverbal affiliation, liking and perceived similarity.\n\nMore recent research was designed to investigate positive and negative predictors of possible relationships by focusing on decisions to engage in future dates. Interpersonal attraction, homophily, and nonverbal immediacy have been linked to the predicted outcome value of relationships during initial encounters. This study investigates how these variables influence date decisions in a six-minute speed-dating experience. Results indicated interpersonal attraction and nonverbal immediacy significantly predict predicted outcome value, but not future date decisions.\n\nMichael Sunnafrank and Artemio Ramirez Jr., assistant professor of communication at Ohio State University, conducted research which was published in the \"Journal of Social and Personal Relationships\" in 2004 and also featured on ABC news. Sunnafrank and Ramirez studied 164 college freshmen over a nine-week period to determine the predicted outcome value of relationship development between strangers.\n\nThe study involved pairing two students of the same sex together the first day of class and introducing one another and then engaging in conversation for either three, six, or 10 minutes. Afterwards, researchers asked each student to predict the outcome of the relationship by describing it as a nodding acquaintance, casual acquaintance, acquaintance, close acquaintance, friend, and close friend. The students also completed extensive questionnaires about the other person to determine how much they liked one another (Sunnafrank & Ramirez, 2004).\n\nOver the nine-week period, Sunnafrank and Ramirez (2004) facilitated a classroom environment that allowed the students to continue to get to know one another. At the end of the nine weeks, those students who predicted positive outcomes developed strong relationships and their behavior indicated such as those students sat together in class and communicated on a regular basis. The strongest effect of the study existed in the relationships where there was a predicted negative outcome. If students decided after the initial interaction that they did not want the relationship to progress then the students acted accordingly by restricting conversation, avoiding eye contact, and avoiding each other period.\nRamirez points out that there is power in first impressions. “People want to quickly determine if a person they just met is someone they are going to want to hang out with, or date, or spend more time with in the future. We don’t want to waste our time.”\n\nOne important point to consider about this study is that Sunnafrank says that college freshmen can be “aggressive” in pursuing new friendships and relationships within social circles and that behavior tends to change over time. “In most situations in life, our time is pretty much claimed by work and family matters, so even when you meet people you really like, chances are not much is going to happen. As life goes on and social networks become solidified, acting on that first impression becomes less likely.”\n\nSunnafrank concluded that other events throughout the course of relationships can change the predicted outcome value, which he calls “surprising events,” such as betraying one another’s trust by “back stabbing” and that not all relationships turn out as originally predicted. However, in this study, it is apparent that initial impressions and interactions can determine the outcome of the relationship, which is the predicted outcome value theory. While the college freshmen had numerous opportunities to develop their relationships over the nine-week period, the first impressions made lasting impacts.\n\n\n"}
{"id": "22860503", "url": "https://en.wikipedia.org/wiki?curid=22860503", "title": "Prior Scientific", "text": "Prior Scientific\n\n<mapframe latitude=\"52.1904\" longitude=\"0.1751\" zoom=\"11\" width=\"263\" height=\"257\" align=\"right\" />Prior Scientific Instruments Ltd was established in London in 1919 as a manufacturer of optical microscopes, today it is a manufacturer of automated optical equipment. It is the last traditional microscope manufacturer and has survived from the one thriving British Microscope Industry serviced by well known manufacturers such as Vickers, W.Watson and Son, Baker, Charles Perry, Cooke, Troughton & Simms and many others who have ceased to produce microscopes.\n\nW.R.Prior & Co.Ltd. was founded by Walter Robert Prior in 1919. Many records and a collection of microscopes from the early days of the Company were lost in a fire in 1988 but some illustrated catalogues have survived which give an indication of the range of quality instruments produced by W.R.Prior over the years for the highly competitive market of the time.\n\nW.R.Prior & Co.Ltd offices were originally located at 9,10,11 Eagle Street, Holborn, London and later at 28a Devonshire Street, London  W.1. Little is known about the Company during this time and no records or catalogues exist as to when and why they began to produce microscopes under the Prior name.\n\nA W.R.Prior catalogue of microscopes and accessories dated February 1950 gives the location of the office as Devonshire Street and the factory as Bishop’s Stortford, Herts. England.(13 Northgate End) where manufacturing was carried out from 1942-1957).\n\nThe Eagle Street factory was destroyed during the London Blitz and the Company relocated to 13 Northgate End,Bishop Stortford.Herts. The building occupied was a garage and work commenced producing  optical gun sights, tank periscopes, and range finders for the Ministry of Defence. No microscopes were produced during the war.\n\nThe first post war abridged catalogue includes a wood block illustration of a student “C” limb microscope with the limb engraved with the Eagle Street address.\n\nOn 10 January 1947 Walter Robert Prior died at the early age of 55.\n\n1953 brought the start of manufacturing micromanipulators for Prior and later the micropositioner was developed. Both of these are still made to this day.\n\nThe Company relocated to a new factory at London Road, Bishop Stortford in 1956 and continued to produce and develop new microscopes. Because of the need for new universities and colleges during the post war period there was a demand for microscopes and accessories and Prior was innovative in developing instruments for this market while maintaining the pre-war quality and finish. The Science Master Microscope, introduced in 1957, was in great demand for use in schools. The established Prior Monocular Dissecting Microscope was widely supplied for educational use. The robustness and simplicity of these instruments gave many years of trouble free use.\n\nFor the Imperial Cancer Research Fund in London 1962, Prior supplied a Modular Inverted Microscope housed in a temperature controlled cabine for cell culture examination. The provision of a cine camera enabled continuous or time lapse cine-photography to be carried out.The Cinemicroscope was also adaptable for still photography or closed circuit television. This microscope was certainly advanced of its time and one of the fore-runners of the modern inverted microscope-now such an essential tool in cell biology and tissue culture work. A unique Prior Tissue Culture Chamber was also supplied and enabled work to be carried out when using an inverted or standard erect microscope.\n\nIn 1978 W.R.Prior & Co.Ltd. was acquired by The Gwyndann Group of Companies and the name of the company was changed in 1979 to that of Prior Scientific Instruments Ltd.\n\nIn 1981 Prior Scientific Instruments Ltd. merged with James Swift and Son Ltd. James Swift & Son was founded in 1854 and had an international reputation for producing advanced student and research polarising microscopes.The unique Swift Point Counter complimented the polarising microscope range and was sold worldwide. The James Swift factory was initially at Basingstoke, Hants. this was closed after a year and the company joined Prior Scientifc Instruments at London Road, Bishop’s Stortford.\n\nIn 1981 Prior launched a new modular series of stereomicroscopes-the S2000 series. The intention was that this instrument would replace and upgrade the Stereomaster microscope. The design had been based on that of an operating microscope featuring a long working distance it also had a tumbler rotating magnification system and a common objective. The design was a complete deviation from that of the previous Prior Greenough type microscope.\n\nFire at the Bishop Stortford factory in 1988 meant the company had to relocate to its present site in Fulbourn, Cambridge, England, offering precision mechanical engineering, optics, electronics and precision assembly.\n\nExpanding its operation further the company opened a new office in 1991, Prior Scientific Inc, based in Rockland near Boston USA, in 2008 Prior GMBH in Jena, Germany, in 2010 Prior KK in Tokyo, Japan and in 2018 Prior China in Suzhou China.\n\nThe company continues to manufacture in the UK and has renewed emphasis on OEM projects, the company looks forward to celebrating its 100 year anniversary in 2019.\n\n"}
{"id": "8564033", "url": "https://en.wikipedia.org/wiki?curid=8564033", "title": "Ronchi test", "text": "Ronchi test\n\nIn optical testing a Ronchi test is a method of determining the surface shape (figure) of a mirror used in telescopes and other optical devices.\n\nIn 1923 Italian physicist Vasco Ronchi published a description of the eponymous Ronchi test, which is a variation of the Foucault knife-edge test and which uses simple equipment to test the quality of optics, especially concave mirrors. . A \"Ronchi tester\" consists of:\n\nA Ronchi grating consists of alternate dark and clear stripes. One design is a small frame with several evenly spaced fine wires attached.\n\nLight is emitted through the Ronchi grating (or a single slit), reflected by the mirror being tested, then passes through the Ronchi grating again and is observed by the person doing the test. The observer's eye is placed close to the centre of curvature of the mirror under test looking at the mirror through the grating. The Ronchi grating is a short distance (less than 2 cm) closer to the mirror.\n\nThe observer sees the mirror covered in a pattern of stripes that reveal the shape of the mirror. The pattern is compared to a mathematically generated diagram (usually done on a computer today) of what it should look like for a given figure. Inputs to the program are line frequency of the Ronchi grating, focal length and diameter of the mirror, and the figure required. If the mirror is spherical, the pattern consists of straight lines.\n\nThe Ronchi test is used in the testing of mirrors for reflecting telescopes especially in the field of amateur telescope making. It is much faster to set up than the standard Foucault knife-edge test.\n\nThe Ronchi test differs from the knife-edge test, requiring a specialized target (the Ronchi grating, which amounts to a periodic series of knife edges) and being more difficult to interpret. This procedure offers a quick evaluation of the mirror's shape and condition. It readily identifies a 'turned edge' (rolled down outer diameter of the mirror), a common fault that can develop in objective mirror making.\n\nThe figure quality of a convex lens may be visually tested using a similar principle. The grating is moved around the focal point of the lens while viewing the virtual image through the opposite side. Distortions in the lens surface figure then appear as asymmetries in the periodic grating image.\n\n"}
{"id": "20647505", "url": "https://en.wikipedia.org/wiki?curid=20647505", "title": "Strangelet", "text": "Strangelet\n\nA strangelet is a hypothetical particle consisting of a bound state of roughly equal numbers of up, down, and strange quarks. An equivalent description is that a strangelet is a small fragment of strange matter, small enough to be considered a particle. The size of an object composed of strange matter could, theoretically, range from a few femtometers across (with the mass of a light nucleus) to arbitrarily large. Once the size becomes macroscopic (on the order of metres across), such an object is usually called a strange star. The term \"strangelet\" originates with Edward Farhi and R. L. Jaffe. Strangelets have been suggested as a dark matter candidate.\n\nThe known particles with strange quarks are unstable because the strange quark is heavier than the up and down quarks, so strange particles, such as the Lambda particle, which contains an up, down, and strange quark, always lose their strangeness, by decaying via the weak interaction to lighter particles containing only up and down quarks. But states with a larger number of quarks might not suffer from this instability. This is the \"strange matter hypothesis\" of Bodmer and Witten. According to this hypothesis, when a large enough number of quarks are collected together, the lowest energy state is one which has roughly equal numbers of up, down, and strange quarks, namely a strangelet. This stability would occur because of the Pauli exclusion principle; having three types of quarks, rather than two as in normal nuclear matter, allows more quarks to be placed in lower energy levels.\n\nA nucleus is a collection of a large number of up and down quarks, confined into triplets (neutrons and protons). According to the strange matter hypothesis, strangelets are more stable than nuclei, so nuclei are expected to decay into strangelets. But this process may be extremely slow because there is a large energy barrier to overcome: as the weak interaction starts making a nucleus into a strangelet, the first few strange quarks form strange baryons, such as the Lambda, which are heavy. Only if many conversions occur almost simultaneously will the number of strange quarks reach the critical proportion required to achieve a lower energy state. This is very unlikely to happen, so even if the strange matter hypothesis were correct, nuclei would never be seen to decay to strangelets because their lifetime would be longer than the age of the universe. \n\nThe stability of strangelets depends on their size. This is because of (a) surface tension at the interface between quark matter and vacuum (which affects small strangelets more than big ones), and (b) screening of charges, which allows small strangelets to be charged, with a neutralizing cloud of electrons/positrons around them, but requires large strangelets, like any large piece of matter, to be electrically neutral in their interior. The charge screening distance tends to be of the order of a few femtometers, so only the outer few femtometers of a strangelet can carry charge.\n\nThe surface tension of strange matter is unknown. If it is smaller than a critical value (a few MeV per square femtometer) then large strangelets are unstable and will tend to fission into smaller strangelets (strange stars would still be stabilized by gravity). If it is larger than the critical value, then strangelets become more stable as they get bigger.\n\nAlthough nuclei do not decay to strangelets, there are other ways to create strangelets, so if the strange matter hypothesis is correct there should be strangelets in the universe. There are at least three ways they might be created in nature:\n\n\nThese scenarios offer possibilities for observing strangelets. If there are strangelets flying around the universe, then occasionally a strangelet should hit Earth, where it would appear as an exotic type of cosmic ray. If strangelets can be produced in high-energy collisions, then we might make them at heavy-ion colliders.\n\nAt heavy ion accelerators like the Relativistic Heavy Ion Collider (RHIC), nuclei are collided at relativistic speeds, creating strange and antistrange quarks that could conceivably lead to strangelet production. The experimental signature of a strangelet would be its very high ratio of mass to charge, which would cause its trajectory in a magnetic field to be very nearly, but not quite, straight. The STAR collaboration has searched for strangelets produced at the RHIC, but none were found. The Large Hadron Collider (LHC) is even less likely to produce strangelets, but searches are planned for the LHC detector.\n\nThe Alpha Magnetic Spectrometer (AMS), an instrument that is mounted on the International Space Station, could detect strangelets.\n\nIn May 2002, a group of researchers at Southern Methodist University reported the possibility that strangelets may have been responsible for seismic events recorded on October 22 and November 24 in 1993. The authors later retracted their claim, after finding that the clock of one of the seismic stations had a large error during the relevant period.\n\nIt has been suggested that the International Monitoring System being set up to verify the Comprehensive Nuclear Test Ban Treaty (CTBT) after entry into force may be useful as a sort of \"strangelet observatory\" using the entire Earth as its detector. The IMS will be designed to detect anomalous seismic disturbances down to energy release or less, and could be able to track strangelets passing through Earth in real time if properly exploited.\n\nIt has been suggested that strangelets of subplanetary i.e. heavy meteorite mass, would puncture planets and other solar system objects, leading to impact (exit) craters which show characteristic features.\n\nIf the strange matter hypothesis is correct \"and\" a stable negatively-charged strangelet with a surface tension larger than the aforementioned critical value exists, then a larger strangelet would be more stable than a smaller one. One speculation that has resulted from the idea is that a strangelet coming into contact with a lump of ordinary matter could convert the ordinary matter to strange matter. This \"ice-nine\"-like disaster scenario is as follows: one strangelet hits a nucleus, catalyzing its immediate conversion to strange matter. This liberates energy, producing a larger, more stable strangelet, which in turn hits another nucleus, catalyzing its conversion to strange matter. In the end, all the nuclei of all the atoms of Earth are converted, and Earth is reduced to a hot, large lump of strange matter. \n\nThis is not a concern for strangelets in cosmic rays because they are produced far from Earth and have had time to decay to their ground state, which is predicted by most models to be positively charged, so they are electrostatically repelled by nuclei, and would rarely merge with them. But high-energy collisions could produce negatively charged strangelet states which live long enough to interact with the nuclei of ordinary matter.\n\nThe danger of catalyzed conversion by strangelets produced in heavy-ion colliders has received some media attention, and concerns of this type were raised at the commencement of the RHIC experiment at Brookhaven, which could potentially have created strangelets. A detailed analysis concluded that the RHIC collisions were comparable to ones which naturally occur as cosmic rays traverse the solar system, so we would already have seen such a disaster if it were possible. RHIC has been operating since 2000 without incident. Similar concerns have been raised about the operation of the LHC at CERN but such fears are dismissed as far-fetched by scientists.\n\nIn the case of a neutron star, the conversion scenario seems much more plausible. A neutron star is in a sense a giant nucleus (20 km across), held together by gravity, but it is electrically neutral and so does not electrostatically repel strangelets. If a strangelet hit a neutron star, it could convert a small region of it, and that region would grow to consume the entire star, creating a quark star.\n\nThe strange matter hypothesis remains unproven. No direct search for strangelets in cosmic rays or particle accelerators has seen a strangelet (see references in earlier sections). If any of the objects such as neutron stars could be shown to have a surface made of strange matter, this would indicate that strange matter is stable at zero pressure, which would vindicate the strange matter hypothesis. However there is no strong evidence for strange matter surfaces on neutron stars (see below).\n\nAnother argument against the hypothesis is that if it were true, all neutron stars should be made of strange matter, and otherwise none should be. Even if there were only a few strange stars initially, violent events such as collisions would soon create many strangelets flying around the universe. Because a single strangelet will convert a neutron star to strange matter, by now all neutron stars would have been converted. This argument is still debated, but if it is correct then showing that one neutron star has a conventional nuclear matter crust would disprove the strange matter hypothesis.\n\nBecause of its importance for the strange matter hypothesis, there is an ongoing effort to determine whether the surfaces of neutron stars are made of strange matter or nuclear matter. The evidence currently favors nuclear matter. This comes from the phenomenology of X-ray bursts, which is well explained in terms of a nuclear matter crust, and from measurement of seismic vibrations in magnetars.\n\n\n\n"}
{"id": "53961341", "url": "https://en.wikipedia.org/wiki?curid=53961341", "title": "Supersymmetric theory of stochastic dynamics", "text": "Supersymmetric theory of stochastic dynamics\n\nSupersymmetric theory of stochastic dynamics or stochastics (STS) is an exact theory of stochastic (partial) differential equations (SDEs), the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise. The main utility of the theory from the physical point of view is a rigorous theoretical explanation of the ubiquitous spontaneous long-range dynamical behavior that manifests itself across disciplines via such phenomena as 1/f, flicker, and crackling noises and the power-law statistics, or Zipf's law, of instantonic processes like earthquakes and neuroavalanches. From the mathematical point of view, STS is interesting because it bridges the two major parts of mathematical physics – the dynamical systems theory and topological field theories. Besides these and related disciplines such as algebraic topology and supersymmetric field theories, STS is also connected with the traditional theory of stochastic differential equations and the theory of pseudo-Hermitian operators.\n\nThe theory begun with the application of BRST gauge fixing procedure to Langevin SDEs, that was later adapted to classical mechanics and its stochastic generalization, higher-order Langevin SDEs, and, more recently, to SDEs of arbitrary form, which allowed to link BRST formalism to the concept of transfer operators and recognize spontaneous breakdown of BRST supersymmetry as a stochastic generalization of dynamical chaos.\n\nThe main idea of the theory is to study, instead of trajectories, the SDE-defined temporal evolution of differential forms. This evolution has an intrinsic BRST or topological supersymmetry representing the preservation of topology and/or the concept of proximity in the phase space by continuous time dynamics. The theory identifies a model as chaotic, in the generalized, stochastic sense, if its ground state is not supersymmetric, i.e., if the supersymmetry is broken spontaneously. Accordingly, the emergent long-range behavior that always accompanies dynamical chaos and its derivatives such as turbulence and self-organized criticality can be understood as a consequence of the Goldstone theorem.\n\nThe first relation between supersymmetry and stochastic dynamics was established by Giorgio Parisi and Nicolas Sourlas who demonstrated that the application of the BRST gauge fixing procedure to Langevin SDEs, i.e., to SDEs with linear phase spaces, gradient flow vector fields, and additive noises, results in N=2 supersymmetric models. Since then, the so-emerged supersymmetry of Langevin SDEs has been studied rather extensively. Relations between this supersymmetry and a few physical concepts have been established including the fluctuation dissipation theorems, Jarzynski equality, Onsager principle of microscopic reversibility, solutions of Fokker-Planck equations, self-organization, etc.\n\nSimilar approach was used to establish that classical mechanics, its stochastic generalization, and higher-order Langevin SDEs also have supersymmetric representations. Real dynamical systems, however, are never purely Langevin or classical mechanical. In addition, physically meaningful Langevin SDEs never break supersymmetry spontaneously. Therefore, for the purpose of the identification of the spontaneous supersymmetry breaking as dynamical chaos, the generalization of the Parisi-Sourlas approach to SDEs of general form is needed. This generalization could come only after a rigorous formulation of the theory of pseudo-Hermitian operators because the stochastic evolution operator is pseudo-Hermitian in the general case. Such generalization showed that all SDEs possess N=1 BRST or topological supersymmetry (TS) and this finding completes the story of relation between supersymmetry and SDEs.\n\nIn parallel to the BRST procedure approach to SDEs, mathematicians working in the dynamical systems theory introduced and studied the concept of generalized transfer operator defined for random dynamical systems. This concept underlies the most important object of the STS, the stochastic evolution operator, and provides it with a solid mathematical meaning.\n\nSTS has a close relation with algebraic topology and its topological sector belongs to the class of models known as Witten-type topological or cohomological field theory. As a supersymmetric theory, BRST procedure approach to SDEs can be viewed as one of the realizations of the concept of Nicolai map.\n\nIn the context of supersymmetric approach to stochastic dynamics, the term Langevin SDEs denotes SDEs with Euclidean phase space, formula_1, gradient flow vector field, and additive Gaussian white noise,\n\nformula_2where formula_3 , formula_4is the noise variable, formula_5 is the noise intensity, and formula_6, which in coordinates formula_7 and formula_8, is the gradient flow vector field with formula_9 being the Langevin function often interpreted as the energy of the purely dissipative stochastic dynamical system.\n\nThe Parisi-Sourlas method is a way of construction of the path integral representation of the Langevin SDE. It can be thought of as a BRST gauge fixing procedure that uses the Langevin SDE as a gauge condition. Namely, one considers the following functional integral,\n\nformula_10\n\nwhere formula_11 denotes the r.h.s. of the Langevin SDE, formula_12 is the operation of stochastic averaging with formula_13 being the normalized distribution of noise configurations,\n\nformula_14\n\nis the Jacobian of the corresponding functional derivative, and the path integration is over all closed paths, formula_15, where formula_16 and formula_17are the initial and final moments of temporal evolution.\n\nTopological aspects of the Parisi-Sourlas construction can be briefly outlined in the following manner. The delta-functional, i.e., the collection of the infinite number of delta-functions, ensures that only solutions of the Langevin SDE contribute to formula_18. In the context of BRST procedure, these solutions can be viewed as Gribov copies. Each solution contributes either positive or negative unity: formula_19with formula_20 being the index of the so-called Nicolai map, formula_21, which in this case is the map from the space of closed paths in formula_22 to the space of noise configurations, a map that provides a noise configuration at which a given closed path is a solution of the Langevin SDE. formula_23 can be viewed as a realization of Poincaré–Hopf theorem on the infinite-dimensional space of close paths with the Langevin SDE playing the role of the vector field and with the solutions of Langevin SDE playing the role of the critical points with index formula_24. formula_23 is independent of the noise configuration because it is of topological character. The same it true for its stochastic average, formula_18, which is not the partition function of the model but, instead, its Witten index.\n\nWith the help of a standard field theoretic technique that involves introduction of additional field called Lagrange multiplier, formula_27, and a pair of fermionic fields called Faddeev–Popov ghosts, formula_28, the Witten index can be given the following form,\n\nformula_29\n\nwhere formula_30 denotes collection of all the fields, p.b.c. stands for periodic boundary conditions, the so-called gauge fermion, formula_31, with formula_32 and formula_33, and the BRST symmetry defined via its action on arbitrary functional formula_34 as formula_35. In the BRST formalism, the Q-exact pieces like, formula_36, serve as gauge fixing tools. Therefore, the path integral expression for formula_18 can be interpreted as a model whose action contains nothing else but the gauge fixing term. This is a definitive feature of Witten-type topological field theories and in this particular case of BRST procedure approach to SDEs, the BRST symmetry can be also recognized as the topological supersymmetry.\n\nA common way to explain the BRST procedure is to say that the BRST symmetry generates the fermionic version of the gauge transformations, whereas its overall effect on the path integral is to limit the integration only to configurations that satisfy a specified gauge condition. This interpretation also applies to Parisi-Sourlas approach with the deformations of the trajectory and the Langevin SDE playing the roles of the gauge transformations and the gauge condition respectively.\n\nPhysical fermions in the high-energy physics and condensed matter models have antiperiodic boundary conditions in time. The unconventional periodic boundary conditions for fermions in the path integral expression for the Witten index is the origin of the topological character of this object. These boundary conditions reveal themselves in the operator representation of the Witten index as the alternating sign operator,formula_38where formula_39 is the operator of the number of ghosts/fermions and the finite-time stochastic evolution operator (SEO), formula_40, where, formula_41is the infinitesimal SEO with formula_42 being the Lie derivative along the subscript vector field, formula_43 being the Laplacian, formula_44 being the exterior derivative, which is the operator representative of the TS, and formula_45, where formula_46 and formula_47 are bosonic and fermionic momenta, and with square brackets denoting bi-graded commutator, i.e., it is an anticommutator if both operators are fermionic (contain odd total number of formula_48's and formula_49's) and a commutator otherwise. The exterior derivative and formula_50 are supercharges. They are nilpotent, e.g., formula_51, and commutative with the SEO. In other words, Langevin SDEs possess N=2 supersymmetry. The fact that formula_50 is a supercharge is accidental. For SDEs of arbitrary form, this is not true.\n\nThe wavefunctions are functions not only of the bosonic variables, formula_53, but also of the Grassmann numbers or fermions, formula_54, from the tangent space of formula_55. The wavefunctions can be viewed as differential forms on formula_55 with the fermions playing the role of the differentials formula_57. The concept of infinitesimal SEO generalizes the Fokker–Planck operator, which is essentially the SEO acting on top differential forms that have the meaning of the total probability distributions. Differential forms of lesser degree can be interpreted, at least locally on formula_55, as conditional probability distributions. Viewing the spaces of differential forms of all degrees as wavefunctions of the model is a mathematical necessity. Without it, the Witten index representing the most fundamental object of the model—the partition function of the noise—would not exist and the dynamical partition function would not represent the number of fixed points of the SDE (see below). The most general understanding of the wavefunctions is the coordinate-free objects that contain information not only on trajectories but also on the evolution of the differentials and/or Lyapunov exponents.\n\nIn Ref., a model has been introduced that can be viewed as a 1D prototype of the topological nonlinear sigma models (TNSM), a subclass of the Witten-type topological field theories. The 1D TNSM is defined for Riemannian phase spaces while for Euclidean phase spaces it reduces to the Parisi-Sourlas model. Its key difference from STS is the diffusion operator which is the Hodge Laplacian for 1D TNSM and formula_59for STS . This difference in unimportant in the context of relation between STS and algebraic topology, the relation established by the theory of 1D TNSM (see, e.g., Refs.).\n\nThe model is defined by the following evolution operator formula_60, where formula_61 with formula_62 being the metric, formula_63 is the Hodge Laplacian, and the differential forms from the exterior algebra of the phase space, formula_64, are viewed as wavefunctions. There exists a similarity transformation, formula_65, that brings the evolution operator to the explicitly Hermitian form formula_66 with formula_67. In the Euclidean case, formula_68 is the Hamiltonian of a N=2 supersymmetric quantum mechanics. One can introduce two Hermitian operators, formula_69 and formula_70, such that formula_71 . This demonstrates that the spectrum of formula_68 and/or formula_73 is real and nonnegative. This is also true for SEOs of Langevin SDEs. For the SDEs of arbitrary form, however, this is no longer true as the eigenvalues of the SEO can be negative and even complex, which actually allows for the TS to be broken spontanenously.\n\nThe following properties of the evolution operator of 1D TNSM hold even for the SEO of the SDEs of arbitrary form. The evolution operator commutes with the operator of the degree of differential forms. As a result, formula_74, where formula_75 and formula_76 is the space of differential forms of degree formula_77. Furthermore, due to the presence of TS, formula_78, where formula_79 are the supersymmetric eigenstates, formula_80, non-trivial in de Rham cohomology whereas the rest are the pairs of non-supersymmetric eigenstates of the form formula_81 and formula_82. All supersymmetric eigenstates have exactly zero eigenvalue and, barring accidental situations, all non-supersymmetric states have non-zero eigenvalues. Non-supersymmetric pairs of eigenstates do not contribute to the Witten index, which equals the difference in the numbers of the supersymmetric states of even and odd degrees, formula_83For compact formula_84, each de Rham cohomology class provides one supersymmetric eigenstate and the Witten index equals the Euler characteristic of the phase space.\n\nThe Parisi-Sourlas method of BRST procedure approach to Langevin SDEs have also been adapted to classical mechanics, stochastic generalization of classical mechanics, higher order Langevin SDEs, and, more recently, to SDEs of arbitrary form. While there exist standard techniques that allow to consider models with colored noises, higher-dimensional \"base spaces\" described by partial SDEs etc., the key elements of STS can be discussed using the following basic class of SDEs, formula_85where formula_86 is a point in the phase space assumed for simplicity a closed topological manifold, formula_87 is a sufficiently smooth vector field, called flow vector field, from the tangent space of formula_88, and formula_89 is a set of sufficiently smooth vector fields that specify how the system is coupled to the noise, which is called additive/multiplicative depending on whether formula_90's are independent/dependent on the position on formula_55.\n\nBRST gauge fixing procedure goes along the same lines as in case of Langevin SDEs. The topological interpretation of the BRST procedure is just the same and the path integral representation of the Witten index is defined by the gauge fermion, formula_92, given by the same expression but with the generalized version offormula_93. There is one important subtlety, however, that appears on the way to the operator representation of the model. Unlike for Langevin SDEs, classical mechanics, and other SDEs with additive noises, the path integral representation of the finite-time SEO is an ambiguous object. This ambiguity originates from non-commutativity of momenta and position operators, e.g., formula_94. As a result, formula_95 in the path integral representation has a whole one-parameter family of possible interpretations in the operator representation, formula_96, where formula_97 denotes an arbitrary wavefunction. Accordingly, there is a whole formula_98-family of infinitesimal SEOs, formula_99with formula_100, formula_101 being the interior multiplication by the subscript vector field, and the \"shifted\" flow vector field being formula_102. Noteworthy, unlike in Langevin SDEs, formula_103 is not a supercharge and STS cannot be identified as a N=2 supersymmetric theory in the general case.\n\nThe path integral representation of stochastic dynamics is equivalent to the traditional understanding of SDEs as of a continuous time limit of stochastic difference equations where different choices of parameter formula_104 are called \"interpretations\" of SDEs. The choice formula_105, for which formula_106 and which is known in quantum theory as Weyl symmetrization rule, is known as the Stratonovich interpretation, whereas formula_107 as the Ito interpretation. While in quantum theory the Weyl symmetrization is preferred because it guaranties hermiticity of Hamiltonians, in STS the Weyl-Stratonovich approach is preferred because it corresponds to the most natural mathematical meaning of the finite-time SEO discussed below—the stochastically averaged pullback induced by the SDE-defined diffeomorphisms.\n\nAs compared to the SEO of Langevin SDEs, the SEO of a general form SDE is pseudo-Hermitian. As a result, the eigenvalues of non-supersymmetric eigenstates are not restricted to be real positive, whereas the eigenvalues of supersymmetric eigenstates are still exactly zero. Just like for Langevin SDEs and nonlinear sigma model, the structure of the eigensystem of the SEO reestablishes the topological character of the Witten index: the contributions from the non-supersymmetric pairs of eigenstates vanish and only supersymmetric states contribute the Euler characteristic of (closed) formula_88. Among other properties of the SEO spectra is that formula_109 and formula_110 never break TS, i.e., formula_111. As a result, there are three major types of the SEO spectra presented in the figure on the right. The two types that have negative (real parts of) eigenvalues correspond to the spontaneously broken TS. All types of the SEO spectra are realizable as can be established, e.g., from the exact relation between the theory of kinematic dynamo and STS.\n\nThe finite-time SEO can be obtained in another, more mathematical way based on the idea to study the SDE-induced actions on differential forms directly, without going through the BRST gauge fixing procedure. The so-obtained finite-time SEO is known in dynamical systems theory as the generalized transfer operator and it has also been used in the classical theory of SDEs (see, e.g., Refs. ). The contribution to this construction from STS is the exposition of the supersymmetric structure underlying it and establishing its relation to the BRST procedure for SDEs.\n\nNamely, for any configuration of the noise, formula_112, and an initial condition, formula_113, SDE defines a unique solution/trajectory, formula_114. Even for noise configurations that are non-differentiable with respect to time, formula_115, the solution is differentiable with respect to the initial condition, formula_116. In other words, SDE defines the family of the noise-configuration-dependent diffeomorphisms of the phase space to itself, formula_117. This object can be understood as a collection and/or definition of all the noise-configuration-dependent trajectories, formula_118. The diffeomorphisms induce actions or pullbacks, formula_119. Unlike, say, trajectories in formula_55, pullbacks are linear objects even for nonlinear formula_55. Linear objects can be averaged and averaging formula_122 over the noise configurations, formula_112, results in the finite-time SEO which is unique and corresponds to the Weyl-Stratonovich interpretation of the BRST procedure approach to SDEs, formula_124.\n\nWithin this definition of the finite-time SEO, the Witten index can be recognized as the sharp trace of the generalized transfer operator. It also links the Witten index to the Lefschetz index,formula_125, a topological constant that equals the Euler characteristic of the (closed) phase space. Namely, formula_126.\n\nThe N=2 supersymmetry of Langevin SDEs has been linked to the Onsager principle of microscopic reversibility and Jarzynski equality. In classical mechanics, a relation between the corresponding N=2 supersymmetry and ergodicity has been proposed. In general form SDEs, where physical arguments may not be applicable, a lower level explanation of the TS is available. This explanation is based on understanding of the finite-time SEO as a stochastically averaged pullback of the SDE-defined diffeomorphisms (see subsection above). In this picture, the question of why any SDE has TS is the same as the question of why exterior derivative commutes with the pullback of any diffeomorphism. The answer to this question is differentiability of the corresponding map. In other words, the presence of TS is the algebraic version of the statement that continuous-time flow preserves continuity of formula_55. Two initially close points will remain close during evolution, which is just yet another way of saying that formula_128 is a diffeomorphism.\n\nIn deterministic chaotic models, initially close points can part in the limit of infinitely long temporal evolution. This is the famous butterfly effect, which is equivalent to the statement that formula_128 losses differentiability in this limit. In algebraic representation of dynamics, the evolution in the infinitely long time limit is described by the ground state of the SEO and the butterfly effect is equivalent to the spontaneous breakdown of TS, i.e., to the situation when the ground state is not supersymmetric. Noteworthy, unlike traditional understanding of deterministic chaotic dynamics, the spontaneous breakdown of TS works also for stochastic cases. This is the most important generalization because deterministic dynamics is, in fact, a mathematical idealization. Real dynamical systems cannot be isolated from their environments and thus always experience stochastic influence.\n\nBRST gauge fixing procedure applied to SDEs leads directly to the Witten index. The Witten index is of topological character and it does not respond to any perturbation. In particular, all response correlators calculated using the Witten index vanish. This fact has a physical interpretation within the STS: the physical meaning of the Witten index is the partition function of the noise and since there is no backaction from the dynamical system to the noise, the Witten index has no information on the details of the SDE. In contrast, the information on the details of the model is contained in the other trace-like object of the theory, the dynamical partition function, formula_130where a.p.b.c. denotes antiperiodic boundary conditions for the fermionic fields and periodic boundary conditions for bosonic fields. In the standard manner, the dynamical partition function can be promoted to the generating functional by coupling the model to external probing fields.\n\nFor a wide class of models, dynamical partition function provides lower bound for the stochastically averaged number of fixed points of the SDE-defined diffeomorphisms,formula_131Here, index formula_132 runs over \"physical states\", i.e., the eigenstates that grow fastest with the rate of the exponential growth given as,formula_133, and parameter formula_134 can be viewed as stochastic version of dynamical entropy such as topological entropy. Positive entropy is one of the key signatures of deterministic chaos. Therefore, the situation with positive formula_135 must be identified as chaotic in the generalized, stochastic sense as it implies positive entropy: formula_136. At the same time, positive formula_135 implies that TS is broken spontaneously, that is, the ground state in not supersymmetric because its eigenvalue is not zero. In other words, positive dynamical entropy is a reason to identify spontaneous TS breaking as the stochastic generalization of the concept of dynamical chaos. Noteworthy, Langevin SDEs are never chaotic because the spectrum of their SEO is real non-negative.\n\nThe complete list of reasons why spontaneous TS breaking must be viewed as the stochastic generalization of the concept of dynamical chaos is as follows. \nAll the above features of TS breaking work for both deterministic and stochastic models. This is in contrast with the traditional deterministic chaos whose trajectory-based properties such as the topological mixing cannot in principle be generalized to stochastic case because, just like in quantum dynamics, all trajectories are possible in the presence of noise and, say, the topological mixing property is satisfied trivially by all models with non-zero noise intensity.\n\nThe topological sector of STS can be recognized as a member of the Witten-type topological field theories. In other words, some objects in STS are of topological character with the Witten index being the most famous example. There are other classes of topological objects. One class of objects is related to instantons, i.e., transient dynamics. Crumpling paper, protein folding, and many other nonlinear dynamical processes in response to quenches, i.e., to external (sudden) changes of parameters, can be recognized as instantonic dynamics. From the mathematical point of view, instantons are families of solutions of deterministic equations of motion, formula_140, that lead from, say, less stable fixed point of formula_139 to a more stable fixed point. Certain matrix elements calculated on instantons are of topological nature. An example of such matrix elements can be defined for a pair of critical points, formula_142 and formula_143, with formula_144 being more stable than formula_145,formula_146Here formula_147 and formula_148 are the bra and ket of the corresponding perturbative supersymmetric ground states, or vacua, which are the Poincare duals of the local stable and unstable manifolds of the corresponding critical point; formula_149 denotes chronological ordering; formula_150's are observables that are the Poincare duals of some closed submanifolds in formula_84; formula_152 are the observables in the Heisenberg representation with formula_153 being an unimportant reference time moment. The critical points have different indexes of stability so that the states formula_154 and formula_155 are topologically inequivalent as they represent unstable manifolds of different dimensionalities. The above matrix elements are independent of formula_156 as they actually represent the intersection number of formula_157-manifolds on the instanton as exemplified in the figure.\n\nThe above instantonic matrix elements are exact only in the deterministic limit. In the general stochastic case, one can consider global supersymmetric states, formula_158's, from the De Rham cohomology classes of formula_55 and observables, formula_160, that are Poincare duals of closed manifolds non-trivial in homology of formula_55. The following matrix elements, formula_162 are topological invariants representative of the structure of De Rham cohomology ring of formula_84.\n\nSupersymmetric theory of stochastic dynamics can be interesting in different ways. For example, STS offers a promising realization of the concept of supersymmetry. In general, there are two major problems in the context of supersymmetry. The first is establishing connections between this mathematical entity and the real world. Within STS, supersymmetry is the most common symmetry in nature because it is pertinent to all continuous time dynamical systems. The second is the spontaneous breakdown of supersymmetry. This problem is particularly important for particle physics because supersymmetry of elementary particles, if exists at extremely short scale, must be broken spontaneously at large scale. This problem is nontrivial because supersymmetries are hard to break spontaneously, the very reason behind the introduction of soft or explicit supersymmetry breaking. Within STS, spontaneous breakdown of supersymmetry is indeed a nontrivial dynamical phenomenon that has been variously known across disciplines as chaos, turbulence, self-organized criticality etc.\n\nA few more specific applications of STS are as follows.\n\nSTS provides classification for stochastic models depending on whether TS is broken and integrability of flow vector field. In can be exemplified as a part of the general phase diagram at the border of chaos (see figure on the right). The phase diagram has the following properties: \n\nMany sudden (or instantonic) processes in nature, such as, e.g., crackling noise, exhibit scale-free statistics often called the Zipf's law. As an explanation for this peculiar spontaneous dynamical behavior, it was proposed to believe that some stochastic dynamical systems have a tendency to self-tune themselves into a critical point, the phenomenological approach known as self-organized criticality (SOC). STS offers an alternative perspective on this phenomenon. Within STS, SOC is nothing more than dynamics in the N-phase. Specifically, the definitive feature of the N-phase is the peculiar mechanism of the TS breaking. Unlike in the C-phase, where the TS is broken by the non-integrability of the flow, in the \"N\"-phase, the TS is spontaneously broken due to the condensation of the configurations of instantons and noise-induced antiinstantons, i.e., time-reversed instantons. These processes can be roughly interpreted as the noise-induced tunneling events between, e.g., different attractors. Qualitatively, the dynamics in the \"N\"-phase appears to an external observer as a sequence of sudden jumps or \"avalanches\" that must exhibit a scale-free behavior/statistics as a result of the Goldstone theorem. This picture of dynamics in the N-phase is exactly the dynamical behavior that the concept of SOC was designed to explain. In contrast with the original understanding of SOC, its STS interpretation has little to do with the traditional critical phenomena theory where scale-free behavior is associated with unstable fixed points of the renormalization group flow.\n\nMagnetohydrodynamical phenomenon of kinematic dynamo can also be identified as the spontaneous breakdown of TS. This result follows from equivalence between the evolution operator of the magnetic field and the SEO of the corresponding SDE describing the flow of the background matter. The so emerged \"STS-kinematic dynamo\" correspondence proves, in particular, that both types of TS breaking spectra are possible, with the real and complex ground state eigenvalues, because kinematic dynamo with both types of the fastest growing eigenmodes are known.\n\nIt is well known that various types of transient dynamics, such as quenches, exhibit spontaneous long-range behavior. In case of quenches across phase transitions, this behavior is often attributed to the proximity of criticality. At the same time, other quenches, not across a phase transition, are also known to exhibit long-range characteristics with the best known examples being the Barkhausen effect and other realizations of the concept of crackling noise. It is intuitively appealing that theoretical explanation for the scale-free behavior in quenches must be the same disregard whether the quench is across a phase transition or not. STS offers such an explanation. Namely, transient dynamics is essentially a composite instanton and TS is intrinsically broken within instantons. Even though TS breaking within instantons is not exactly the phenomenon of the spontaneous breakdown of a symmetry by a global ground state, this effective TS breaking must also result in a scale-free behavior. This understanding is supported by the fact that condensed instantons lead to appearance of logarithms in the correlation functions. This picture of transient dynamics explains computational efficiency of the digital memcomputing machines.\n\nIn physics, spontaneous symmetry breaking is known as \"ordering\". For example, the spontaneous breakdown of translational symmetry in a liquid is the mathematical essence of crystallization or spatial \"ordering\" of molecules into a lattice. Therefore, spontaneous TS breaking picture of chaotic dynamics is in a certain sense opposite to the semantics of word \"chaos\". Due to its temporal character, it is actually Chronos, not Chaos, that appears to be the primordial Greek deity closest in its spirit to the TS breaking order. Perhaps, a more accurate identifier than \"chaos\" should be coined for TS breaking in the future. As of this moment, this qualitatively new understanding of dynamical chaos already points into a research direction that may lead to resolutions of some important problems such as turbulence and neurodynamics. Namely, as in case of any other \"ordering\", a simplified yet accurate description of chaotic dynamics can be achieved in terms of the low-energy effective theory for an order parameter. While the low-energy effective description of chaotic dynamics may be very case specific, its order parameter must always be a representative of the gapless fermions or goldstinos of the spontaneously broken TS.\n"}
{"id": "18482886", "url": "https://en.wikipedia.org/wiki?curid=18482886", "title": "Sussex Manifesto", "text": "Sussex Manifesto\n\nThe Sussex Manifesto was a report on science and technology for development written at the request of the United Nations and published in 1970.\n\nIn the late 1960s the United Nations asked for recommendations on science and technology for development from a team of academics at the Institute of Development Studies (IDS) and SPRU Science and Technology Policy Research (formerly called Science Policy Research Unit) at the University of Sussex, UK. This team became known as the Sussex Group and their report, \"Science and Technology to Developing Countries during the Second Development Decade\", became known as the \"Sussex Manifesto\".\n\nThe \"Sussex Manifesto\" was intended as the introductory chapter to the \"UN World Plan of Action on Science and Technology for Development\". But the solutions presented in the Manifesto were deemed too radical to be used for that purpose. It was instead published in 1970 as an annex in \"Science and Technology for Development: Proposals for the Second United Nations Development Decade\", a UN report by the Advisory Committee on the Application of Science and Technology to Development (ACAST).\n\nThe \"Sussex Manifesto\" helped raise awareness of science and technology for development in UN circles influenced the design of development institutions and was used for teaching courses in both North and South universities.\n\nThe Sussex Group were Hans Singer (Chairman), Charles Cooper (Secretary), R.C. Desai, Christopher Freeman, Oscar Gish, Stephen Hill and Geoffrey Oldham.\n\nIn 2008 one of the authors of the original report Professor Geoff Oldham gave a seminar at the STEPS Centre – a research centre and policy engagement based at IDS and SPRU. Following this event the STEPS Centre decided to create a new manifesto in association its partners around the world and Professor Oldham. The new publication, \"Innovation, Sustainability, Development: A New Manifesto\", was launched in 2010, forty years after the original.\n\nThe New Manifesto has also been translated into Chinese, French, Portuguese and Spanish. \n\nThe STEPS Centre is funded by the Economic and Social Research Council (ESRC).\n\n"}
{"id": "28513417", "url": "https://en.wikipedia.org/wiki?curid=28513417", "title": "The Twisted Sisterhood", "text": "The Twisted Sisterhood\n\nThe Twisted Sisterhood: Unraveling the Dark Legacy of Female Friendships is a non-fiction book by essayist and attorney Kelly Valen published by Random House/Ballantine Books on October 26, 2010.\n\nThe book is based on the author's December 2007 article and includes the results of a women's relationships survey the author conducted with 3000 women, studies and insights from experts in the fields of sociology, psychology, and neuroscience, and elements of humor, memoir and popular culture.\n\n"}
{"id": "3062306", "url": "https://en.wikipedia.org/wiki?curid=3062306", "title": "The World of Chemistry", "text": "The World of Chemistry\n\nThe World of Chemistry is a television series on introductory chemistry hosted by Nobel prize-winning chemist Roald Hoffmann. The series consists of 26 half-hour video programs, along with coordinated books, which explore various topics in chemistry through experiments conducted by Stevens Point emeritus professor Don Showalter the \"series demonstrator\" and interviews with working chemists, it also includes physics and earth science related components. The series was produced by the University of Maryland, College Park and the Educational Film Center and was funded by the Annenberg/CPB Project (now the Annenberg Foundation), it was filmed in 1988 and first aired on PBS in 1990. This series supports science standards recognized nationally by the United States (NSTA and NCSESA) and is still widely used in high school and college chemistry courses. The entire series is currently available on learner.org for free in an online video streaming format.\n\nThe awards won by The World of Chemistry are given below\n\n"}
{"id": "3144720", "url": "https://en.wikipedia.org/wiki?curid=3144720", "title": "Weaponeering", "text": "Weaponeering\n\nWeaponeering is the field of designing an attack with weapons. It is a portmanteau of \"weapon\" and \"engineering\". The term should not be confused with weapons engineering, which is the actual engineering design and development of weapon systems.\n\nThe United States Department of Defense defines the term as the process of determining the quantity of a specific type of lethal or nonlethal weapons required to achieve a specific level of damage to a given target, considering target vulnerability, weapon effect, munitions delivery accuracy, damage criteria, probability of kill and weapon reliability.\n\n"}
