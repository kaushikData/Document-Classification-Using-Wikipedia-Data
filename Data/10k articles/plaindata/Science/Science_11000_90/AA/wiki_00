{"id": "24891124", "url": "https://en.wikipedia.org/wiki?curid=24891124", "title": "Across the Universe (message)", "text": "Across the Universe (message)\n\nAcross the Universe is an interstellar radio message (IRM) consisting of the song \"Across the Universe\" by The Beatles that was transmitted on 4 February 2008, at 00:00 UTC by NASA in the direction of the star Polaris. This transmission was made using a 70-meter dish in the Deep Space Network's (DSN) Madrid Deep Space Communication Complex, located in Robledo, near Madrid, Spain.\n\nThis action was done in order to celebrate the 40th anniversary of the song's recording, the 45th anniversary of the DSN, and the 50th anniversary of NASA. The idea was hatched by Beatles historian Martin Lewis, who encouraged all Beatles fans to play the track as it was beamed to the distant star. The event marked the second time a song had ever been intentionally transmitted into deep space (the first being Russia's Teen Age Message in 2001), and was approved by Paul McCartney, Yoko Ono, and Apple Records.\n\nThis IRM project has some significant defects in that the message was aimed at Polaris, which is 431 light years distant from us and whose planetary system, even if it exists, may not be suited for life, because it is a supergiant star, spectral type F7Ib. In addition, the transmission rate was very high, about 128 kbit/s, for such moderate transmitter power (about 18 kW).\n\n\n"}
{"id": "30427673", "url": "https://en.wikipedia.org/wiki?curid=30427673", "title": "Arriens Glacier", "text": "Arriens Glacier\n\nArriens Glacier () is a small Antarctic glacier, south of Casey Point in the Mawson Escarpment, flowing west to reach Lambert Glacier. It was plotted from ANARE aerial photographs taken in 1956, 1960 and 1973, and named by the Australian Antarctic Names and Medals Committee after P. Arriens, geochronologist with the ANARE Prince Charles Mountains survey party in 1973.\n\n"}
{"id": "57969010", "url": "https://en.wikipedia.org/wiki?curid=57969010", "title": "Asmeret Asefaw Berhe", "text": "Asmeret Asefaw Berhe\n\nAsmeret Asefaw Berhe is a soil biogeochemist, political ecologist and associate professor at University of California, Merced. Her research group works to understand how soil helps regulate the earth's climate.\n\nAsmeret was born and raised in Asmara, Eritrea in northeast Africa bordering the Red Sea. She received her Bachelors of Science in Soil and Water Conservation at the University of Asmara in Eritrea. There, she was one of three women in a 55-person class in the soil science department. She later attended Michigan State University for her Masters in Political Ecology with an emphasis on the effects of land degradation, working to understand how landmines cause land degradation. She then performed her doctoral work at University of California, Berkeley, where she received her PhD in Biogeochemistry in ecologist John Harte's laboratory, where she was also co-advised by Margaret Torn (Lawrence Berkeley National Laboratory) and Jennifer Harden (US Geological Survey, Menlo Park). Her graduate work sought to understand how erosion affected the exchange of carbon between the land and the air. She found that erosion can actually cause soil to store more carbon. She continued her postdoctoral research at UC Berkeley with the support of the President's Postdoctoral Fellowship Program under the mentorship of Johan Six and Jillian Banfield, and then moved to University of California, Davis to continue her postdoctoral work.\n\nAsmeret's research interests center on the effect of changing environmental conditions—specifically fire, erosion, and climate change—on important soil processes. Her group is working to understand how perturbations in the environment affect how essential elements like carbon and nitrogen cycle through the soil system. One of her group's projects is to understand how drought and wildfire affect soil's ability to store carbon, taking her out to Yosemite National Park and the Sierra Nevada for fieldwork. Given the prevalence of drought in California, this work is of particular public importance, and as a result, has been highlighted by public figures like California Congressman Jerry McNerney (D-CA 9th District).\n\nHer research extends to political ecology, working to understand the contribution of armed conflict to land degradation and how people interact with their environment. She has co-authored a review taking stock of the relationship between global change, soil, and human security (including food security and water quality) in the 21st century, citing possible interventions and solutions for sustainable soil management.\n\nAsmeret's work has garnered support from a number of funding sources, including the National Science Foundation CAREER Award, the University of California President's Research Catalyst Awards, the United States Department of Energy, and more.\n\nAsmeret's work at the intersection of soil, climate change, and political ecology lends itself well to a number of global issues. During her graduate career, she was a member of the working group that produced the Millennium Ecosystem Assessment, which was called for by the United Nations Secretary Kofi Annan to assess the impact of humans on the environment. She was one of the lead authors on the 2005 report's chapter on \"Drivers of Change in Ecosystem Condition and Services.\" The Assessment received the Zayed International Prize for the Environment in 2005.\n\nIn 2018, Asmeret was selected as part of the inaugural National Academies of Sciences, Engineering, and Medicine New Voices in Sciences, Engineering, and Medicine cohort, as an early career leader working to advance the conversation around key emerging global issues issues and communicate the evidence base around those challenges.\n\nAn advocate for women in science, Berhe is currently a co-Principal Investigator of ADVANCEGeo, which is working to transform the workplace climate of the geosciences to increase retention of women in the field and develop a sustainable model that can be transferred to other scientific domains. Currently, the Earth Science Women's Network, the Association for Women Geoscientists, and the American Geophysical Union have partnered to address the issue of sexual harassment in the earth, space and environmental sciences. The program led by Erika Marín-Spiotta and is run with support from a four-year $1.1 million grant from the National Science Foundation.\n\nShe currently serves as an advisory board member of 500 Women Scientists, a grassroots organization working to make science open, inclusive, and accessible, and is on the leadership board of the Earth Science Women's Network.\n\n"}
{"id": "41861637", "url": "https://en.wikipedia.org/wiki?curid=41861637", "title": "Average earnings index", "text": "Average earnings index\n\nIn the United Kingdom, the average earnings index (AEI) was an indicator of inflationary pressures emanating from the labour market.\n\nThe AEI was replaced by the average weekly earnings (AWE) as the lead measure of short-term earnings growth in January 2010.\n\n\nhttp://www.ons.gov.uk/ons/rel/awe/average-weekly-earnings/the-development-of-the-average-weekly-earnings-indicator/development-of-the-average-weekly-earnings-indicator.pdf\n"}
{"id": "18949111", "url": "https://en.wikipedia.org/wiki?curid=18949111", "title": "Bad Science (Taubes book)", "text": "Bad Science (Taubes book)\n\nBad Science: The Short Life and Weird Times of Cold Fusion is book of science history by Gary Taubes about the early years (1989–1991) of the cold fusion controversy.\n\nThis text is not a scholarly work, but a popular retelling of the events, based on interviews with over 260 people. The book presents a timeline of the events, making the case that the cold fusion field has many examples of poorly performed science. The actions of Martin Fleischmann, Stanley Pons, and Steven E. Jones, the scientists who made the dramatic first claims of fusion, are described in rich detail. The book then shows the worldwide reaction and later disrepute of the cold fusion field, with Taubes placing himself in the side of \"good science\". Taubes says at the end that cold fusion had only demonstrated that research can continue even if the phenomenon doesn't actually exist, as long as there is funding available. Taubes had previously written an article for \"Science\" in which he insinuates that the cold fusion work of A&M University was fraudulent.\n\nThe book received a positive review in \"American Journal of Physics\". While observing that the book was \"readable, suspenseful, and insightful\", the reviewer criticized it for including too many footnotes (over 300), some of which were deemed unimportant.\n\n"}
{"id": "20258930", "url": "https://en.wikipedia.org/wiki?curid=20258930", "title": "Bankole Johnson", "text": "Bankole Johnson\n\nBankole A. Johnson, DSc, MD, MPhil, FRCPsych (born 5 November 1959) is a licensed physician and board-certified psychiatrist throughout Europe and the United States who served as Alumni Professor and Chairman of the Department of Psychiatry and Neurobehavioral Sciences at the University of Virginia. Johnson's primary area of research expertise is the psychopharmacology of medications for treating addictions, and he is well known in the field for his discovery that topiramate, a gamma-aminobutyric acid (GABA) facilitator and glutamate antagonist, is an effective treatment for alcoholism. Professor Johnson also received national media attention for his appearance in the Home Box Office (HBO) original documentary feature, \"Addiction\", which won the prestigious Governors Award, a special Emmy Award, from the Academy of Television Arts and Sciences. Professor Johnson recently accepted an appointment to join the University of Maryland as the Chairman of Psychiatry and to lead a Brain Science Research Consortium in the neurosciences.\n\nJohnson was born on 5 November 1959 in Nigeria. Johnson attended King's College in Lagos, Nigeria and received his diploma in 1975. He then went on to Davies' College in Sussex, England followed by the Institute Catholique de Paris in Paris, France. Johnson graduated from the University of Glasgow in Scotland in 1982 with a Medicinae Baccalaureum et Chirurgie Baccalaureum degree. He went on to train in psychiatry at the Royal London and Maudsley and Bethlem Royal Hospitals, and to train in research at the Institute of Psychiatry (University of London). In 1991, Johnson graduated from the University of London with a Master of Philosophy degree in neuropsychiatry. Johnson conducted his doctoral research at Oxford University and obtained a doctorate degree in medicine, Medicinae Doctorem, from the University of Glasgow in 1993. Most recently, in 2004, Johnson earned his Doctor of Science degree in medicine from the University of Glasgow – the highest degree that can be granted in science by a British university.\n\nJohnson joined the faculty at the University of Texas Health Science Center in Houston in 1993 and later became the Deputy chairman for Research and Chief of the Division of Alcohol and Drug Addiction in the Department of Psychiatry at the University of Texas Health Science Center in San Antonio in 1998. In 2001, Johnson received the Dan Anderson Research Award from the Hazelden Foundation for his \"distinguished contribution as a researcher who has advanced the scientific knowledge of addiction recovery.\" In 2002, Johnson received the Distinguished Senior Scholar of Distinction Award from the National Medical Association. Johnson was inducted into the Texas Hall of Fame in 2003 for his contributions to science, mathematics, and technology. On 1 September 2004, Johnson accepted an appointment to serve as Alumni Professor and Chairman of the Department of Psychiatric Medicine at the University of Virginia. Johnson became a fellow of the Royal College of Psychiatrists in 2007. In 2009 Johnson was named Associate Editor of the Editorial Board of \"The American Journal of Psychiatry\", and from 2010 to 2011 he served as Field Editor-in-Chief of \"Frontiers in Psychiatry\".\n\nJohnson's research focus is on the neuropsychopharmacology of addiction. His work integrates the neuroscience and behavioural aspects of addiction medicine with the goal of formulating a more thorough understanding of the basis of drug-seeking behaviour and developing effective treatments. Central to his research is the role of and interaction between midbrain monoamine systems with a focus on serotonin, gamma-aminobutyric acid (GABA)/glutamate and dopamine.\n\nJohnson's Journal of the American Medical Association (JAMA) paper, titled \"Topiramate for treating alcohol dependence: a randomized controlled trial\" and published in 2007, gained national and international media attention. The 14-week US multi-site clinical trial involved 371 male and female alcoholics. Those patients taking topiramate had reduced heavy drinking and showed better results with lowering cholesterol, body mass index, liver enzymes, and blood pressure than those taking the placebo. The study results were featured on Reuters, MSNBC, CBS, ABC, CNN, Fox News, USA Today, the Associated Press, and many other media outlets.\n\nJohnson’s current research involves clinical trials and human laboratory studies, and includes neuroimaging and molecular genetics. He now incorporates neuroimaging evaluations into his drug interaction studies to identify the site-specific effects of abused drugs and to evaluate the effectiveness of potential medications for the treatment of addiction. Current studies include a clinical trial aimed at determining the effectiveness of ondansetron, a serotonin-3 antagonist, for the treatment of subtypes of alcoholics, as well as a human laboratory project trying to elucidate the effects of naltrexone and acamprosate on hepatic and renal function in alcohol-dependent individuals.\n\n\n"}
{"id": "52772200", "url": "https://en.wikipedia.org/wiki?curid=52772200", "title": "Bavarian School of Public Policy", "text": "Bavarian School of Public Policy\n\nThe Bavarian School of Public Policy (German: \"Hochschule für Politik München\") - is an independent institution within the Technical University of Munich. The Bavarian School of Public Policy is a unique institution, focusing on future-oriented, transdisciplinary, and applied research and teaching. In cooperation with the Technical University of Munich (TUM), the Bavarian School of Public Policy is dedicated to educating the next generation of political scientists. Technological progress in areas like energy, the environment, climate change, big data, data security, and mobility is having an increasingly important impact on the society. To keep up with tomorrow’s key technologies, the course program at HfP provides an opportunity for students to combine the traditional content of a political science degree with technology modules run by diverse TUM departments.\n\nThe Bavarian School of Public Policy was founded during the period of upheaval that followed the end of World War II. Prompted by the US military government, and with the participation of constitutional law lecturer Prof. Hans Nawiasky, a decision was made in fall 1948 to set up an institution with the primary goal of teaching the principles of democracy. The discipline of “political science”, already well-established in the US, would serve as the school’s model. On July 14, 1950 the “Hochschule für Politische Wissenschaften” was founded – as a corporation under private law.\n\nOn October 27, 1970, the Bavarian Parliament enacted the “Law governing the Bavarian School of Public Policy”. This gave the school the status of an “institutionally autonomous institution at the University of Munich”. This made HfP the only institution of higher education in the Federal Republic of Germany whose legal basis is a higher education act unique to that institution. Moreover, the law was also the first higher education act in the Federal Republic of Germany.\n\nIn a law dated February 16, 1981, the Bavarian School of Public Policy was given the legal status of a public body. The main advantage for students was the opportunity to earn a full academic qualification and potentially carry on to complete a PhD.\n\nThe Bavarian Parliament instigated the reform process that is currently underway in October 2013. In July 2014, it decided to make the Technical University of Munich (TUM) the new host university for the Bavarian School of Public Policy. This landmark decision opens up new opportunities to develop the discipline of political science by incorporating insights from TUM’s excellent work in the social sciences, the natural sciences and engineering.\n\nThe law governing the Bavarian School of Public Policy was enacted in early December 2014, and the Constitution in January 2015. On December 18, 2014 the appointment procedure for the new HfP professorships began. Out of 350 applications from Germany and abroad, seven professors were appointed, who took up their duties at the Bavarian School of Public Policy on 1 March / 1 July 2016. In July 2016, the Bavarian School of Public Policy moved to its new premises in the Briennerforum at Königsplatz.\n\nBringing Politics and Technology together\n\nThe Bachelor’s degree in Political Science (B.Sc.) started in the 2016/17 Winter Semester. In the 2017/18 Winter Semester the Bavarian School of Public Policy established the new Master's program in Politics & Technology (M.Sc.). Both study programs based on a unique concept worldwide. This provides a \"full-scale political science education with possible emphasis in policy areas at the interface between politics and technology\". \n\nAppointed professors at the Bavarian School of Public Policy:\n\n\n\n"}
{"id": "14662101", "url": "https://en.wikipedia.org/wiki?curid=14662101", "title": "Biositemap", "text": "Biositemap\n\nA Biositemap is a way for a biomedical research institution of organisation to show how biological information is distributed throughout their Information Technology systems and networks. This information may be shared with other organisations and researchers.\n\nThe Biositemap enables web browsers, crawlers and robots to easily access and process the information to use in other systems, media and computational formats. Biositemaps protocols provide clues for the Biositemap web harvesters, allowing them to find resources and content across the whole interlink of the Biositemap system. This means that human or machine users can access any relevant information on any topic across all organisations throughout the Biositemap system and bring it to their own systems for assimilation or analysis.\n\nThe information is normally stored in a biositemap.rdf or biositemap.xml file which contains lists of information about the data, software, tools material and services provided or held by that organisation. Information is presented in metafields and can be created online through sites such as the biositemaps online editor.\n\nThe information is a blend of sitemaps and RSS feeds and is created using the Information Model (IM) and Biomedical Resource Ontology (BRO). The IM is responsible for defining the data held in the metafields and the BRO controls the terminology of the data held in the resource_type field. The BRO is critical in aiding the interactivity of both the other organisations and third parties to search and refine those searches.\n\nThe Biositemaps Protocol allows scientists, engineers, centers and institutions engaged in modeling, software tool development and analysis of biomedical and informatics data to broadcast and disseminate to the world the information about their latest computational biology resources (data, software tools and web services). The biositemap concept is based on ideas from \"Efficient, Automated Web Resource Harvesting\" and \n\"Crawler-friendly Web Servers\", and it integrates the features of sitemaps and RSS feeds into a decentralized mechanism for computational biologists and bio-informaticians to openly broadcast and retrieve meta-data about biomedical resources.\n\nThese site, institution, or investigator specific \"biositemap\" descriptions are published in RDF format online and are searched, parsed, monitored and interpreted by web search engines, web applications specific to biositemaps and ontologies, and other applications interested in discovering updated or novel resources for bioinformatics and biomedical research investigations. The biositemap mechanism separates the providers of biomedical resources (investigators or institutions) from the consumers of resource content (researchers, clinicians, news media, funding agencies, educational and research initiatives).\n\nA Biositemap is an RDF file that lists the biomedical and bioinformatics resources for a specific research group or consortium. It allows developers of biomedical resources to describe the functionality and usability of each of their software tools, databases or web-services.\n\nBiositemaps supplement and do not replace the existing frameworks for dissemination of data, tools and services. Using a biositemap does not guarantee that resources will be included in search indexes nor does it influence the way that tools are ranked or perceived by the community. What the Biositemaps protocol will do is provide clues, information and directives to all Biositemap web harvesters that point to the existence and content of biomedical resources at different sites.\n\nThe Biositemap protocol relies on an extensible information model that includes specific properties that are commonly used and necessary for characterizing biomedical resources:\n\nUp-to-date documentation on the information model is available at the Biositemaps website.\n\n\n"}
{"id": "37342733", "url": "https://en.wikipedia.org/wiki?curid=37342733", "title": "Burkholderia phage phiE202", "text": "Burkholderia phage phiE202\n\nBurkholderia phage phiE202 is a bacteriophage (a virus that infects bacteria) of the family Myoviridae, genus \"P2likevirus\".\n"}
{"id": "54773669", "url": "https://en.wikipedia.org/wiki?curid=54773669", "title": "Carl-Olof Morfeldt", "text": "Carl-Olof Morfeldt\n\nCarl-Olof Morfeldt (1919–2003) was a Swedish geologist and businessman. He was a founder and CEO of Hagconsult AB, an enterprise dedicated to geotechnical consultancy. In 1964, he became also its majority shareholder. Morfeldt was the foremost authority on geotechnical studies for building in Sweden. An expert on building tunnels and rooms in bedrock, Morfeldt was awarded a honorary doctorate from Chalmers University of Technology in 1979. In 1999, he was awarded the prize Geologist of the Year () by Naturvetarna.\n"}
{"id": "1279334", "url": "https://en.wikipedia.org/wiki?curid=1279334", "title": "Chiefdom", "text": "Chiefdom\n\nA chiefdom is a form of hierarchical political organization in non-industrial societies usually based on kinship, and in which formal leadership is monopolized by the legitimate senior members of select families or 'houses'. These elites form a political-ideological aristocracy relative to the general group.\n\nIn anthropological theory, one model of human social development rooted in ideas of cultural evolution describes a chiefdom as a form of social organization more complex than a tribe or a band society, and less complex than a state or a civilization.\n\nWithin general theories of cultural evolution, chiefdoms are characterized by permanent and institutionalized forms of political leadership (the chief), centralized decision-making, economic interdependence, and social hierarchy.\n\nChiefdoms are described as intermediate between tribes and states in the progressive scheme of sociopolitical development formulated by Elman Service: \"band - tribe - chiefdom - state\". A chief’s status is based on kinship, so it is inherited or ascribed, in contrast to the achieved status of Big Man leaders of tribes. Another feature of chiefdoms is therefore pervasive social inequality. They are ranked societies, according to the scheme of progressive sociopolitical development formulated by Morton Fried: \"egalitarian - ranked - stratified - state\".\n\nThe most succinct definition of a chiefdom in anthropology is by Robert L. Carneiro: \"An autonomous political unit comprising a number of villages or communities under the permanent control of a paramount chief\" (Carneiro 1981: 45).\n\nIn archaeological theory, Service's definition of chiefdoms as “redistributional societies with a permanent central agency of coordination” (Service 1962: 144) has been most influential. Many archaeologists, however, dispute Service's reliance upon redistribution as central to chiefdom societies, and point to differences in the basis of finance (staple finance v. wealth finance). Service argued that chief rose to assume a managerial status to redistribute agricultural surplus to ecologically specialised communities within this territory (staple finance). Yet in re-studying the Hawaiian chiefdoms used as his case study, Timothy Earle observed that communities were rather self-sufficient. What the chief redistributed was not staple goods, but prestige goods to his followers that helped him to maintain his authority (wealth finance).\n\nSome scholars contest the utility of the chiefdom model for archaeological inquiry. The most forceful critique comes from Timothy Pauketat, whose \"Chiefdom and Other Archaeological Delusions\" outlines how chiefdoms fail to account for the high variability of the archaeological evidence for middle-range societies. Pauketat argues that the evolutionary underpinnings of the chiefdom model are weighed down by racist and outdated theoretical baggage that can be traced back to Lewis Morgan's 19th century cultural evolution. From this perspective, pre-state societies are treated as underdeveloped, the savage and barbaric phases that preceded civilization. Pauketat argues that the chiefdom type is a limiting category that should be abandoned, and takes as his main case study Cahokia, a central place for the Mississippian culture of North America.\n\nPauketat's provocation, however, fails to offer a sound alternative to the chiefdom type. For while he claims that chiefdoms are a delusion, he describes Cahokia as a civilization. This upholds rather than challenges the evolutionary scheme he contests.\n\nChiefdoms are characterized by centralization of authority and pervasive inequality. At least two inherited social classes (elite and commoner) are present. (The ancient Hawaiian chiefdoms had as many as four social classes.) An individual might change social class during a lifetime by extraordinary behavior. A single lineage/family of the elite class becomes the ruling elite of the chiefdom, with the greatest influence, power, and prestige. Kinship is typically an organizing principle, while marriage, age, and sex can affect one's social status and role.\n\nA single simple chiefdom is generally composed of a central community surrounded by or near a number of smaller subsidiary communities. All of the communities recognize the authority of a single kin group or individual with hereditary centralized power, dwelling in the primary community. Each community will have its own leaders, which are usually in a tributary and/or subservient relationship to the ruling elite of the primary community.\n\nA complex chiefdom is a group of simple chiefdoms controlled by a single paramount center, and ruled by a paramount chief. Complex chiefdoms have two or even three tiers of political hierarchy. Nobles are clearly distinct from commoners and do not usually engage in any form of agricultural production. The higher members of society consume most of the goods that are passed up the hierarchy as a tribute.\n\nReciprocal obligations are fulfilled by the nobles carrying out ritual that only they can perform. They may also make token, symbolic redistributions of food and other goods. In two or three tiered chiefdoms, higher ranking chiefs have control over a number of lesser ranking individuals, each of whom controls specific territory or social units. Political control rests on the chief's ability to maintain access to a sufficiently large body of tribute, passed up the line by lesser chiefs. These lesser chiefs in turn collect from those below them, from communities close to their own center. At the apex of the status hierarchy sits the paramount.\n\nAnthropologists and archaeologists have demonstrated through research that chiefdoms are a relatively unstable form of social organization. They are prone to cycles of collapse and renewal, in which tribal units band together, expand in power, fragment through some form of social stress, and band together again. An example of this kind of social organization were the Germanic Peoples who conquered the western Roman Empire in the 5th century CE. Although commonly referred to as tribes, anthropologists classified their society as chiefdoms. They had a complex social hierarchy consisting of kings, a warrior aristocracy, common freemen, serfs and slaves.\n\nThe American Indian tribes sometimes had ruling kings or satraps (governors) in some areas and regions. The Cherokee, for example, had an imperial-family ruling system over a long period of history. The early Spanish explorers in the Americas reported on the Indian kings and kept extensive notes during what is now called the conquest. Some of the native tribes in the Americas had princes, nobles, and various classes and castes. The \"Great Sun\" was somewhat like the Great Khans of Asia and eastern Europe. Much like an emperor, the Great Sun of North America is the best example of chiefdoms and imperial kings in North American Indian history. The Aztecs of Mexico had a similar culture.\n\nThe \"Arthashastra\", a work on politics written some time between the 4th century BC and 2nd century AD by Indian author Kautilya, similarly describes the Rajamandala (or \"Raja-mandala,\") as circles of friendly and enemy states surrounding the state of a king (\"raja\"). Also see Suhas Chatterjee, \"Mizo Chiefs and the Chiefdom\" (1995).\n\nTusi (), also known as Headmen or Chieftains, were tribal leaders recognized as imperial officials by the Yuan, Ming, and Qing-era Chinese governments, principally in Yunnan. The arrangement is generally known as the Native Chieftain System (, p \"Tǔsī Zhìdù\").\n\nIn prehistoric South-West Asia, alternatives to chiefdoms were the non-hierarchical systems of complex acephalous communities, with a pronounced autonomy of single-family households. These communities have been analyzed recently by Berezkin, who suggests the \"Apa Tanis\" as their ethnographic parallel (Berezkin 1995). Frantsouzoff (2000) finds a more developed example of such type of polities in ancient South Arabia in the Wadi Hadhramawt of the 1st millennium BCE.\n\nIn Southeast Asian history up to the early 19th century, the metaphysical view of the cosmos called the mandala (i.e., circle) is used to describe a Southeast Asian political model, which in turn describes the diffuse patterns of political power distributed among Mueang (principalities) where circles of influence were more important than central power. The concept counteracts modern tendencies to look for unified political power like that of the large European kingdoms and nation states, which were an inadvertent byproduct of 15th-century advances in map-making technologies.\n\nNikolay Kradin has demonstrated that an alternative to the state seems to be represented by the supercomplex chiefdoms created by some nomads of Eurasia. The number of structural levels within such chiefdoms appears to be equal, or even to exceed those within the average state, but they have a different type of political organization and political leadership. Such types of political entities do not appear to have been created by the agriculturists (e.g., Kradin 2000, 2002, 2003, 2004).\n\n\n\n"}
{"id": "50900796", "url": "https://en.wikipedia.org/wiki?curid=50900796", "title": "Chérubin d'Orléans", "text": "Chérubin d'Orléans\n\nChérubin d'Orléans (1613-1697) was a French scientific instrument maker.\n\nA Capucin father and distinguished physicist, Chérubin d'Orléans (François Lasséré) devoted himself to the study of optics and to vision-related problems, which he discussed in \"La dioptrique oculaire and La vision parfaite\" (Paris, 1671 and 1677 respectively). He invented the first binocular telescope. He devised and may also have built a special type of eyepiece that replaced the lens with a short tube. Chérubin is also credited with producing models of the eyeball for studying the lens function of the eye.\n\n"}
{"id": "34896724", "url": "https://en.wikipedia.org/wiki?curid=34896724", "title": "Digital transcriptome subtraction", "text": "Digital transcriptome subtraction\n\nDigital transcriptome subtraction (DTS) is a bioinformatics method to detect the presence of novel pathogen transcripts through computational removal of the host sequences. DTS is the direct \"in silico\" analogue of the wet-lab approach Representational Difference Analysis (RDA), and is made possible by unbiased high-throughput sequencing and the availability of a high-quality, annotated reference genome of the host. The method specifically examines the etiological agent of infectious diseases and is best known for discovering Merkel cell polymavirus, the suspect causative agent in Merkel cell carcinoma.\n\nUsing computational subtraction to discover novel pathogens was first proposed in 2002 by Meyerson et al. using human expressed sequence tag (EST) datasets. In a proof of principle experiment, Meyerson et al. demonstrated that it was a feasible approach using Epstein-Barr virus-infected lymphocytes in post-transplant lymphoproliferative disorder (PTLD).\n\nIn 2007, the term \"Digital Transcriptome Subtraction\" was coined by the Chang-Moore group, and was used to discover Merkel cell polymavirus in Merkel cell carcinoma.\n\nSimultaneously to the MCV discovery, this approach was used to implicate a novel arenavirus as cause of fatality in a case where three patients died of similar illnesses shortly following organ transplantations from a single donor.\n\nAfter treatment with DNase I to eliminate human genomic DNA, total RNA is extracted from primary infected tissue. Messenger RNA is then purified using an oligo-dT column that binds to the poly-A tail, a signal specifically found on transcribed genes. Using random hexamers priming, reverse transcriptase (RT) convert all mRNA into cDNA and cloned into bacterial vectors. Bacteria, usually \"E. coli\", are then transformed using the cDNA vectors and selected using a marker, the collection of transformed clones is the cDNA library. This generates a snap-shot of tissue mRNA that is stable and can be sequenced at a later stage.\n\nThe cDNA library must be sequenced to great depth (i.e. number of clones sequenced) in order to detect a theoretical rare pathogen sequence (Table 1), especially if the foreign sequence is novel. Chang-Moore recommend a sequencing depth of 200,000 transcripts or greater using multiple sequencing platforms.\n\nStringent quality control are then applied to the raw sequences to minimize false-positive results. The initial quality screen uses several general parameters to exclude ambiguous sequences, leaving behind a dataset of high-fidelity (Hi-Fi) reads.\n\n\nUsing MEGABLAST, Hi-Fi reads are then matched to sequences in annotated databases and any positive matches are then subtracted from the dataset. Minimum hit length for a positive match of human sequence is typically 30 consecutive identical bases, which equates to a BLAST score of 60; generally, the remaining sequence is BLAST again with less stringent parameters to allow for slight mismatches (1 in 20 nucleotide). The vast majority of sequences (>99%) should be removed from the dataset at this stage.\n\nSubtracted sequences typically include:\n\nAfter stringent rounds of subtraction, the remaining sequences are clustered into non-redundant contigs and aligned to known pathogen sequences using low-stringency parameters. As pathogen genomes mutates quickly, nucleotide-nucleotide alignments, or blastn, is usually uninformative as it is possible to have mutations at certain bases without changing the amino acid residue due to codon degeneracy. Matching the \"in silico\" translated protein sequences of all 6 open reading frames to the amino acid sequence to annotated proteins, or blastx, is the preferred alignment method as it increases the likelihood of identifying a novel pathogen by matching to a related strain/species. Experimental extension of candidate sequences might also be used at this stage to maximize chances of a positive match.\n\nIn cases where alignment to known pathogens is uninformative or ambiguous, contigs of candidate sequence can be used as templates for primer walking in primary infected tissue to generate the complete pathogen genome sequence. As viral transcripts are exceedingly rare ratio tissue mRNA (10 transcripts in 1 million), it is unlikely to generate a transcriptome based on the original candidate sequences alone due to low coverage.\n\nOnce a putative pathogen has been identified in the high-throughput sequencing data, it is imperative to validate the presence of pathogen in infected patients using more sensitive techniques, such as:\n\n\nThe primary application for DTS lies in identification of pathogenic viruses in cancer. It can also be used to identify viral pathogens in non-cancer related disease. Future clinical applications could include the use of DTS on a routine basis in individuals. \nDTS could also apply to agriculture, identifying pathogens that have an effect on output. Computation subtraction was already used in a metagenomics study that associated viral infection by IAPV with colony collapse disorder in honey bees.\n\n\n"}
{"id": "57734899", "url": "https://en.wikipedia.org/wiki?curid=57734899", "title": "Dual Air Density Explorer", "text": "Dual Air Density Explorer\n\nDual Air Density Explorer was a set of 2 satellites, DADE-A and DADE-B, released as part of NASA's Explorers program. DADE-A and DADE-B was launched on 6 December 1975 at 15:35 UTC, by a Scout F-1 launch vehicle from Launch Complex 5, Vandenberg AFB, California. The launch of the DADE satellites failed.\n\n\n"}
{"id": "9798051", "url": "https://en.wikipedia.org/wiki?curid=9798051", "title": "ERA-40", "text": "ERA-40\n\nERA-40 is an ECMWF re-analysis of the global atmosphere and surface conditions for 45-years, over the period from September 1957 through August 2002 by ECMWF. Many sources of the meteorological observations were used, including radiosondes, balloons, aircraft, buoyes, satellites, scatterometers. This data was run through the ECMWF computer model at a 125 km resolution. As the ECMWF's computer model is one of the more highly regarded in the field of forecasting, many scientists take its reanalysis to have similar merit. The data is stored in GRIB format. The reanalysis was done in an effort to improve the accuracy of historical weather maps and aid in a more detailed analysis of various weather systems through a period that was severely lacking in computerized data. With the data from reanalyses such as this, many of the more modern computerized tools for analyzing storm systems can be utilized, at least in part, because of this access to a computerized simulation of the atmospheric state.\n\n"}
{"id": "1441023", "url": "https://en.wikipedia.org/wiki?curid=1441023", "title": "Fictive kinship", "text": "Fictive kinship\n\nFictive kinship is a term used by anthropologists and ethnographers to describe forms of kinship or social ties that are based on neither consanguineal (blood ties) nor affinal (\"by marriage\") ties, in contrast to \"true kinship\" ties.\n\nTo the extent that consanguineal and affinal kinship ties might be considered \"real\" or \"true\" kinship, the term \"fictive kinship\" has in the past been used to refer to those kinship ties that are \"fictive\", in the sense of \"not-real\". Invoking the concept as a cross-culturally valid anthropological category therefore rests on the presumption that the inverse category of \"(true) kinship\" built around consanguinity and affinity is similarly cross-culturally valid. Use of the term was common until the mid-to-late twentieth century, when anthropology effectively deconstructed and revised many of the concepts and categories around the study of kinship and social ties. In particular, anthropologists established that a consanguinity basis for kinship ties is not universal across cultures, and that—on the contrary—it may be a culturally specific symbol of kinship only in particular cultures (see the articles on kinship and David M. Schneider for more information on the history of kinship studies).\n\nStemming from anthropology's early connections to legal studies, the term \"fictive kinship\" may also be used in a legal sense, and this use continues in societies where these categories and definitions regarding kinship and social ties have legal currency; e.g. in matters of inheritance.\n\nAs part of the deconstruction of kinship mentioned above, anthropologists now recognize that—cross-culturally—the kinds of social ties and relationships formerly treated under the category of \"kinship\" are very often not necessarily predicated on blood ties or marriage ties, and may rather be based on shared residence, shared economic ties, nurture kinship, or familiarity via other forms of interaction.\n\nIn sociology of the family, this idea is referred to as chosen kin, fictive kin or voluntary kin. Sociologists define the concept as a form of extended family members who are not related by either blood or marriage. The bonds allowing for chosen kinship may include religious rituals, close friendship ties, or other essential reciprocal social or economic relationships. Examples of chosen kin include godparents, informally adopted children, and close family friends. The idea of fictive kin has been used to analyze aging, foreign fighters, immigrant communities, and minorities in modern societies. Some researchers state that peers have the potential to create fictive kin networks.\n\nTypes of relations often described by anthropologists as \"fictive kinship\" include compadrazgo relations, foster care, common membership in a unilineal descent group, and legal adoption. A noted Gurung tradition is the institution of \"Rodi\", where teenagers form fictive kinship bonds and become Rodi members to socialize, perform communal tasks, and find marriage partners. In Western culture, a person may refer to close friends of one's parents as \"aunt\" or \"uncle\" (and their children as \"cousin\"), or may refer to close friends as \"brother\" or \"sister\", although this is just a mere courtesy treatment and does not represent an actual valuation as such. In particular, college fraternities and sororities in some North American cultures usually use \"brother\" and \"sister\" to refer to members of the organization. Monastic, Masonic, and Lodge organisations also use the term \"Brother\" for members. \"Nursing Sister\" is used to denote a rank of nurse, and the term \"Sisterhood\" may be used for feminists. Fictive kinship was discussed by Jenny White in her work on female migrant workers in Istanbul. In her work, she draws on ideas of production and the women she works with being drawn together through \"webs of indebtedness\" through which the women refer to each other as kin. These relationships are, however, less frequent than kin relationships, and serve purposes that are not comparable to, nor exclude, a natural family and, much less, natural and universal parents-having-children reproduction system.\n\n\nRecently, many anthropologists have abandoned a distinction between \"real\" and \"fictive\" kin, because many cultures do not base their notion of kinship on genealogical relations. This was argued most forcefully by David M. Schneider, in his 1984 book \"A critique of the study of kinship\". In response to this insight, Janet Carsten developed the idea of \"relatedness\". She developed her initial ideas from studies with the Malays in looking at what was socialized and biological. Here she uses the idea of relatedness to move away from a pre-constructed analytics opposition which exists in anthropological thought between the biological and the social. Carsten argued that relatedness should be described in terms of indigenous statements and practices, some of which fall outside what anthropologists have conventionally understood as kinship.\n\nThis does not imply, however, that human non-kin relationships, such as in tit-for-tat situations, even within a friendship relation, are more important than kin relationships, since their motivation is also related to one's survival and perpetuation, or that people are necessarily bound to the culture they are inserted in, nor can it be generalized to the point of claiming all individuals always undervalue kinship in the absence of nurturing. Herbert Gintis, in his review of the book \"Sex at Dawn\", critiques the idea that human males were unconcerned with parentage, \"which would make us unlike any other species I can think of\". Such individuals can be considered out of the natural tendency of living beings for survival through offspring.\n\nIn response to a similar model advanced by E. O. Wilson, Rice University’s David Queller said that such new model \"involves, and I suspect requires, close kinship\". The theory also overlooks phenomena of survivalist non-kin or not close kin such as the one that can be seen on tribalism or ethnic nationalism.\n\nIn the biological and animal behavioural sciences, the term \"kinship\" has a different meaning from the current anthropological usage of the term, and more in common with the former anthropological usage that assumed that blood ties are ontologically prior to social ties. In these sciences, \"kinship\" is commonly used as a shorthand for \"the regression coefficient of (genetic) relatedness\", which is a metric denoting the proportion of shared genetic material between any two individuals relative to average degrees of genetic variance in the population under study. This coefficient of relationship is an important component of the theory of inclusive fitness, a treatment of the evolutionary selective pressures on the emergence of certain forms of social behavior. Confusingly, inclusive fitness theory is more popularly known through its narrower form, kin selection theory, whose name clearly resonates with former conceptions of \"kinship\" in anthropology.\n\nWhilst inclusive fitness theory thus describes one of the necessary conditions for the evolutionary emergence of social behaviors, the details of the proximate conditions mediating the expression of social bonding and cooperation have been less investigated in sociobiology. In particular, the question of whether genetic relatedness (or \"blood ties\") must necessarily be present for social bonding and cooperation to be expressed has been the source of much confusion, partly due to thought experiments in W. D. Hamilton's early theoretical treatments. In addition to setting out the details of the evolutionary selection pressure, Hamilton roughly outlined two possible mechanisms by which the expression of social behaviors might be mediated:\n\nTraditional sociobiology did not consider the divergent consequences between these basic possibilities for the expression of social behavior, and instead assumed that the expression operates in the \"recognition\" manner, whereby individuals are behaviorally primed to discriminate which others are their true genetic relatives, and engage in cooperative behavior with them. But when expression has evolved to be primarily location-based or context-based—depending on a society's particular demographics and history—social ties and cooperation may or may not coincide with blood ties. Reviews of the mammal, primate, and human evidence demonstrate that expression of social behaviors in these species are primarily location-based and context-based (see nurture kinship), and examples of what used to be labeled as \"fictive kinship\" are readily understood in this perspective. Social cooperation, however, does not mean people see each other as family or family-like, nor that people will value those known not to be related with them more than the ones who are or simply neglect relatedness.\n\n"}
{"id": "50901001", "url": "https://en.wikipedia.org/wiki?curid=50901001", "title": "Filippo De Palma", "text": "Filippo De Palma\n\nFilippo De Palma (1813 – after 1873) was an Italian scientific instrument maker.\n\nScientific-instrument maker active in Naples in the third quarter of the nineteenth century. Exhibited in Paris in 1867 and in Vienna in 1873. Known to have produced electrical machines and physics instruments.\n\n"}
{"id": "5643965", "url": "https://en.wikipedia.org/wiki?curid=5643965", "title": "George Lindor Brown", "text": "George Lindor Brown\n\nSir George Lindor Brown (9 February 1903, Liverpool – 22 February 1971) was an English physiologist and secretary of the Royal Society, of which he was elected a Fellow in 1946.\n\nHe was commonly referred to as Sir Lindor Brown; by his own preference.\n\nHe was Waynflete Professor of Physiology at the University of Oxford from 1960 to 1967. He resigned from this post to become Principal of Hertford College.\n\n"}
{"id": "1079412", "url": "https://en.wikipedia.org/wiki?curid=1079412", "title": "Glossary of leaf morphology", "text": "Glossary of leaf morphology\n\nThe following is a defined list of terms which are used to describe leaf morphology in the description and taxonomy of plants. Leaves may be simple (a single leaf blade or lamina) or compound (with several leaflets). The edge of the leaf may be regular or irregular, may be smooth or bearing hair, bristles or spines. For more terms describing other aspects of leaves besides their overall morphology see the leaf article.\n\nLeaves of most plants include a flat structure called the blade or lamina, but not all leaves are flat, some are cylindrical. Leaves may be simple, with a single leaf blade, or compound, with several leaflets. In flowering plants, as well as the blade of the leaf, there may be a petiole and stipules; compound leaves may have a rachis supporting the leaflets. Leaf structure is described by several terms that include:\nBeing one of the more visible features, leaf shape is commonly used for plant identification. Similar terms are used for other plant parts, such as petals, tepals, and bracts.\n\nLeaf margins (edges) are frequently used in visual plant identification because they are usually consistent within a species or group of species, and are an easy characteristic to observe. Edge and margin are interchangeable in the sense that they both refer to the outside perimeter of a leaf.\nLeaves may also be folded or rolled in various ways. The folding of leaves within a bud is vernation, ptyxis is the folding of an individual leaf in a bud.\nThe Latin word for 'leaf', , is neuter. In descriptions of a single leaf, the neuter singular ending of the adjective is used, e.g. 'lanceolate leaf', 'linear leaf'. In descriptions of multiple leaves, the neuter plural is used, e.g. 'linear leaves'. Descriptions commonly refer to the plant using the ablative singular or plural, e.g. 'with ovate leaves'.\n\n\n\n"}
{"id": "15796200", "url": "https://en.wikipedia.org/wiki?curid=15796200", "title": "HCM-6A", "text": "HCM-6A\n\nHCM-6A is an LAE galaxy that was found in 2002 by a team led by Esther Hu from the University of Hawaii, using the Keck II Telescope in Hawaii. HCM-6A is located behind the Abell 370 galactic cluster, near M77 in the constellation Cetus, which enabled the astronomers to use Abell 370 as a gravitational lens to get a clearer image of the object.\n\nHCM-6A was the farthest object known at the time of its discovery. It exceeded SSA22−HCM1 (\"z\" = 5.74) as the most distant normal galaxy known, and quasar SDSSp J103027.10+052455.0 (\"z\" = 6.28) as the most distant object known. In 2003, SDF J132418.3+271455 (\"z\" = 6.578) was discovered, and took over the title of most remote object known, most remote galaxy known, and most remote normal galaxy known.\n"}
{"id": "45374332", "url": "https://en.wikipedia.org/wiki?curid=45374332", "title": "Holmes (crater)", "text": "Holmes (crater)\n\nHolmes is an impact crater in the Mare Australe quadrangle of Mars, located at 75.0°S latitude and 293.2°W longitude. It is 122.0  km in diameter and was named after Arthur Holmes, and the name was approved in 1973 by the International Astronomical Union (IAU) Working Group for Planetary System Nomenclature (WGPSN).\n\n"}
{"id": "18344001", "url": "https://en.wikipedia.org/wiki?curid=18344001", "title": "Institute for the Encouragement of Scientific Research and Innovation of Brussels", "text": "Institute for the Encouragement of Scientific Research and Innovation of Brussels\n\nThe Institute for the Encouragement of Scientific Research and Innovation of Brussels or ISRIB (French: \"Institut d'Encouragement de la Recherche Scientifique et de l'Innovation de Bruxelles\" - IRSIB, Dutch: \"Instituut ter bevordering van het Wetenschappelijk Onderzoek en de Innovatie van Brussel\" - IWOIB) promotes scientific research and technological innovation in the Brussels-Capital Region of Belgium within companies, universities and higher education institutes within the region. It provides support to both profit-oriented research and non-profit-oriented-research.\n\nThe ISRIB was founded by the Brussels decree of 26 June 2003, and it started its activities on 1 July 2004.\n\n\n\n"}
{"id": "23662086", "url": "https://en.wikipedia.org/wiki?curid=23662086", "title": "J. C. McConnell", "text": "J. C. McConnell\n\nDr. James Culbertson McConnell, usually abbreviated as J. C. McConnell (born 1844 - died July 25, 1904, Liberty, New York) was one of the world's most acclaimed scientific illustrators.\n\nMcConnell was an anatomist with the Army Medical Museum in Washington, D.C. For thirty-five years, he drew \"many thousand exquisite drawings\" of fossils, shells and bones for scientific publications. In an obituary, it was stated that, \"as a draughtsman, in black and white line for scientific purposes, he had no equal in this country, if in the world.\"\n\nHe is most well known for his \"incomparable pictures of shells\" and illustrated a number of publications by the famous malacologist William Healey Dall. McConnell also illustrated fossils described by Charles Doolittle Walcott.\n\nMcConnell's illustrations continued to be used long after his death. For example, most of the black and white illustrations in R. Tucker Abbott's \"American Seashells\" (1954) were by McConnell.\n\nMcConnell has been described as \"one of those shadowy-figured artisans about whom little is known.\" Although he held a medical degree and used the title \"doctor\", \"officially he was a clerk.\"\n\nMcConnell's illustrations appear in (partial list):\n\n"}
{"id": "50900895", "url": "https://en.wikipedia.org/wiki?curid=50900895", "title": "John Cuff (optician)", "text": "John Cuff (optician)\n\nJohn Cuff (c. 1708 – c. 1772) was an important English scientific instrument maker, particularly of microscopes.\n\nHe was apprenticed to the optical instrument maker James Mann. Cuff eventually set up his own establishment as a \"SPECTACLE \"and\" MICROSCOPE \"Maker\", At the sign of the Reflecting MICROSCOPE and SPECTACLES opposite Serjeant's Inn\" \"(1737-57) & Double Microscope, three Pairs of Golden Spectacles & Hadley's Quadrant opposite Salisbury Court (1757-8) both in Fleet St & Strand, all in London, England.\" In 1743, he advertised that he made and sold \"Wholesale and Retale, all Manner of curious Optical INSTRUMENTS\".\n\nCuff failed to gain membership in the Royal Society, but at a Society meeting in the winter of 1738-1739, he encountered Johann Nathanael Lieberkühn, a German physician who was promoting two microscopes of his own invention. Cuff soon made improvements to the designs. In 1745, the Swiss naturalist Abraham Trembley visited London and asked him to design a microscope that would make it easier to observe aquatic creatures as they were moving about. Two years later, Cuff produced the \"AQUATIC MICROSCOPE\", \"invented by him for the Examination of Water Animals.\"\n\nThe naturalist Henry Baker complained to him about the shortcomings of Baker's Culpeper-type microscope: \"Pulling the body of the Instrument up and down was likewise subject to Jerks, which caused a Difficulty in fixing it exactly at the Focus: there was also no good Contrivance for viewing opake Objects\". Under Baker's direction, Cuff designed and produced an improved \"Double Microscope\" that quickly supplanted the Culpeper type and became much sought-after not only in England, but all over Europe. While superior to other microscopes of the time, optically it was no improvement, and like them it still \"suffered from severe chromatic and spherical aberration.\"\n\nUnfortunately, Cuff was apparently not much of a businessman: despite Baker's support, he had to declare bankruptcy in 1750. In 1757, a Benjamin Martin opened a competing shop next door to Cuff's establishment on Fleet Street and drove him out of business the following year. \n\nAccording to the Royal Collection Trust, the German painter Johan Zoffany was commissioned by King George III, a purchaser of Cuff's microscopes, to depict him. However, doubts have been expressed whether the painting titled \"John Cuff\" actually is a portrayal of him, and it has also been known as \"The Lapidaries\" or \"Two Old Men\".\n\nCuff's instruments are found in several major collections of scientific instruments, including:\n"}
{"id": "13468572", "url": "https://en.wikipedia.org/wiki?curid=13468572", "title": "List of University of California Press journals", "text": "List of University of California Press journals\n\nThis is a list of journals published by the University of California Press.\n\n"}
{"id": "44074002", "url": "https://en.wikipedia.org/wiki?curid=44074002", "title": "List of common names of lichen genera", "text": "List of common names of lichen genera\n\nThis is a list of common names of lichen genera. When a common name for a lichen genus is the same as the scientific name for that genus, it is not included in the following list. This list only includes genera common names that are widely used, as indicated by the common name either appearing in a peer reviewed scientific publication or in a scientifically reliable reference source.\n\nA common name for a lichen genus will often uniquely refer to that genus, but not always. Sometimes the same common name may be used to refer to several different genera, which may not be related by sharing common ancestry. An example is that \"wart lichen\" refers to at least five different genera in four different families. Sometimes the same genus may have more than one widely used common name. For example, members of the genus \"Staurothele\" are commonly called \"wart lichens\", and also \"rock pimples\". Lichen genus common names my come from the shape, color, or other feature of some members of a genus. Other members may not share that trait, but are still referred to by the common name for the genus. For example, \"Caloplaca albovariegata\" is an orange lichen, but it is not orange in color.\n\nLichen species common names are often the same as the common name of the genus they are in, or are a modification of that common name by adding an adjective. But sometimes the parts of a lichen species common name are common names of \"other\" lichen genera. For example, \"Psilolechia lucida\", in the genus \"Psilolechia\", is commonly called \"sulphur dust lichen\". But \"sulphur lichen\" refers to the genus \"Fulgensia\", and \"dust lichen\" refers either to the genus \"Chrysothrix\" or the genus \"Lepraria\".\n"}
{"id": "236342", "url": "https://en.wikipedia.org/wiki?curid=236342", "title": "List of diseases (H)", "text": "List of diseases (H)\n\nThis is a list of diseases starting with the letter \"H\".\n\n\n\n\n\n\n\n\n\n\n\nHereditary a – Hereditary m\nHereditary n – Hereditary t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3687926", "url": "https://en.wikipedia.org/wiki?curid=3687926", "title": "List of mathematics journals", "text": "List of mathematics journals\n\nThis is a list of mathematics journals.\n\nThis is a list of mathematical journals with existing Wikipedia articles on them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following journals had the highest impact factor among mathematics journals over the period 1981–2006.\n\n\nThese journals received a rating of A*, the highest rating, in the 2009 report of the Australian Mathematical Society. These rankings have now been officially discontinued by the Australian Research Council (ARC),\nwith evidence they were being misused and no longer available from the ARC. Any journal that started publication after 2007 does not have a ranking, and the rankings are not updated, so this list may omit journals that are comparable to those listed. The journals are listed alphabetically.\nThe following fit ranking criteria by an organization that intends to fill a gap where rankings are desired. These are top ranked journals according to Red Jasper.\n\n\n\n"}
{"id": "7120259", "url": "https://en.wikipedia.org/wiki?curid=7120259", "title": "List of volcanoes in Sudan", "text": "List of volcanoes in Sudan\n\nThis is a list of active and extinct volcanoes in Sudan. \n"}
{"id": "7120278", "url": "https://en.wikipedia.org/wiki?curid=7120278", "title": "List of volcanoes in Uganda", "text": "List of volcanoes in Uganda\n\nThis is a list of active and extinct volcanoes in Uganda. \n"}
{"id": "3346545", "url": "https://en.wikipedia.org/wiki?curid=3346545", "title": "Lithic technology", "text": "Lithic technology\n\nLithic technology includes a broad array of techniques and styles in archaeology, which are used to produce usable tools from various types of stone. The earliest stone tools were recovered from modern Ethiopia and were dated to between two-million and three-million years old. The archaeological record of lithic technology is divided into three major time periods: the Paleolithic (Old Stone Age), Mesolithic (Middle Stone Age), and Neolithic (New Stone Age). Not all cultures in all parts of the world exhibit the same pattern of lithic technological development, and stone tool technology continues to be used to this day, but these three time periods represent the span of the archaeological record when lithic technology was paramount. By analysing modern stone tool usage within an ethnoarchaeological context insight into the breadth of factors influencing lithic technologies in general may be studied. See: Stone tool. For example, for the Gamo of Southern Ethiopia, political, environmental, and social factors influence the patterns of technology variation in different subgroups of the Gamo culture; through understanding the relationship between these different factors in a modern context, archaeologists can better understand the ways that these factors could have shaped the technological variation that is present in the archaeological record.\n\nSome types of raw materials are:\n\nThese raw materials all have common characteristics which make them ideal for stone tool production. To make a stone material ideal for tool production, it must be non-crystalline or glassy, which allows for conchoidal fracturing. These characteristics allow the person forming the stone (\"flintknapper\") to control the reduction precisely in order to make a wide variety of tools.\n\nThere are numerous factors as to why some raw materials would be chosen over others and can result in the use of low quality materials. A few examples of such factors include the availability of materials, the proximity to materials, and the quality of materials. To help understand this, archaeologists have applied models of risk management to stone artifacts. Theories have suggested that in times of high risk, more effort will be put into acquiring high quality material that is more reliable and can be maintained over longer periods of time. In times of low risk, lower quality materials may be acquired from closer sources. However, Mackay and Marwick (2011) found that this pattern does not always hold true in their application of this theory to the South African Pleistocene record. They then used computer simulations to understand why the relationship between the time put into producing technology and subsistence acquisition would produce the patterns they saw. Mackay and Marwick found that when less time was put into acquiring material for and producing technology, that extra time increased the chances of encounters and thus increases the chances of acquiring more resources in a shorter period of time. This demonstrates that raw material choice is not always straightforward, nor are high quality materials always sought out.\n\nStone tools are manufactured using a process known as lithic reduction. The technique used is dependent upon the level of detail required for the desired tool. The technique with the least detail is conducted using a hammerstone, in which a hard rock (often sandstone) is struck against the raw material in order to chip off large flakes, and begin to shape the stone. Using a hammerstone produces what is known as a preform, which is the core of the tool in need of more detailed refinements. The next technique allows for an increased level of detail; using a soft hammer (often made of wood or bone), one can chip away flakes of material with more precision. The most precise technique is known as pressure flaking. This technique involves pressing small flakes off rather than by means of percussion. Bone and antlers are often used as punches in order to create a very precisely detailed tool. Another technique, known as indirect percussion, combines the use of a punch and a hammer in order to apply pressure to a precise area of the stone. For the most part, stone cores can only be used to a certain extent before they become exhausted cores. As such, it is typically the flakes, or debitage, that are the basis for stone tools. The flakes are shaped using the lithic reduction techniques, allowing for creation of various tools such as arrowheads and handaxes.\n\nTwo stone characteristics will determine whether one is able to chip away big enough flakes to make tools out of: whether the stone is of a cryptocrystalline structure, and how conchoidally the stone fractures. A cryptocrystalline stone is one that is made up of minute crystals that can only be seen with a microscope. Conchoidal fractures are described as smooth, curved breaks from the base stone. Stones that have both of these characteristics allow for flakes that are big and sharp enough for a variety of tools to be made. Obsidian is a great example of a material that is perfect for making tools with, as it is both cryptocrystalline and it fractures conchoidally. Many early Middle Eastern and American civilizations used obsidian as a basis for tools as its internal structure made it easier to chip away than most of the other stones in the area.\n\nDuring an experiment conducted by Dibble and Whittaker, they found that even the angle hit at the exterior platform would produce different flake types. The exterior platform angle is an angle formed by hitting the intersection of the platform surface and the exterior of the core. When hitting the core at a low exterior platform angle, a feather termination is produced. When the exterior platform angle is hit near a mid-range to low angle, a hinge termination is produced. The highest exterior platforms produce the overshoots. The desired termination, however, is generally the feather termination due to its sharp edge.\n\n"}
{"id": "22814744", "url": "https://en.wikipedia.org/wiki?curid=22814744", "title": "Lithium depletion boundary", "text": "Lithium depletion boundary\n\nThe lithium depletion boundary (LDB) technique is a method proposed for dating open clusters based on a determination of the lithium abundances of a cluster's stars whose masses are at about the hydrogen burning mass limit.\n"}
{"id": "2841557", "url": "https://en.wikipedia.org/wiki?curid=2841557", "title": "Long delayed echo", "text": "Long delayed echo\n\nLong delayed echoes (LDEs) are radio echoes which return to the sender several seconds after a radio transmission has occurred. Delays of longer than 2.7 seconds are considered LDEs. LDEs have a number of proposed scientific origins.\n\nThese echoes were first observed in 1927 by civil engineer and amateur radio operator Jørgen Hals from his home near Oslo, Norway. Hals had repeatedly observed an unexpected second radio echo with a significant time delay after the primary radio echo ended. Unable to account for this strange phenomenon, he wrote a letter to Norwegian physicist Carl Størmer, explaining the event:\n\nAt the end of the summer of 1927 I repeatedly heard signals from the Dutch short-wave transmitting station PCJJ at Eindhoven. At the same time as I heard these I also heard echoes. I heard the usual echo which goes round the Earth with an interval of about 1/7 of a second as well as a weaker echo about three seconds after the principal echo had gone. When the principal signal was especially strong, I suppose the amplitude for the last echo three seconds later, lay between 1/10 and 1/20 of the principal signal in strength. From where this echo comes I cannot say for the present, I can only confirm that I really heard it.\n\nPhysicist Balthasar van der Pol helped Hals and Stormer investigate the echoes, but due to the sporadic nature of the echo events and variations in time-delay, did not find a suitable explanation.\n\nLong delayed echoes have been heard sporadically from the first observations in 1927 and up to our time.\n\nShlionskiy lists 15 possible \"natural\" explanations in two groups: reflections in outer space, and reflections within the Earth's magnetosphere. Vidmar and Crawford suggest five of them are the most likely. Sverre Holm, professor of signal processing at the University of Oslo details those five; in summary,\n\n\nRadio waves of frequency less than about 7 MHz can become trapped in magnetic field-aligned ionization ducts with L values (distance from the center of the earth to the field line at the magnetic equator) less than about 4. These waves after being trapped can propagate to the opposite hemisphere where they become reflected in the topside ionosphere. They can return along the duct, leave it, and propagate to the receiver.\n\n\n\nThe signals from two separated transmitters T1 and T2, T2 transmitting CW or quasi-CW signals, interact nonlinearly in the ionosphere or magnetosphere. If the wave vector and frequency of the forced oscillation at the difference frequency of the two signals satisfies the dispersion relation for electrostatic waves, such waves would exist and begin to propagate. This wave could grow in amplitude due to wave-particle interaction. At a later time it could interact with the CW signal and propagate to T1.\n\n\n\nNone of these hypotheses can explain everything. Only the first mechanism is well established, and none of the other four are well-established enough to deserve the term \"theory\". The phenomena are often fleeting and non-repeatable. Our understanding of how the magnetosphere interacts with the solar wind is still evolving.\n\nSome believe that the aurora activity that follows a solar storm is the source of LDEs.\n\nStill others believe that LDEs are double EME (EMEME) reflections, i.e. the signal is reflected by the moon and that reflected signal is reflected by the Earth back to the moon and reflected again by the moon back to the earth.\n\nDuncan Lunan proposed the radio echoes observed by Størmer and Van der Pol in 1928 might have been transmissions from a Bracewell probe, an artifact of aliens trying to communicate with us by bouncing back our own signals. This concept is also addressed by Holm.\n\nVolker Grassmann writing in \"VHF Communications\" noted the possibility of individuals hoaxing LDEs, saying, \"Attempts at deception can in no case be ruled out, and it is to be feared that less serious radio amateurs contribute to deliberate falsification... Short transmissions using different frequencies are a relatively simple procedure for excluding potential troublemakers.\" To reduce the possibilities of errors or hoaxes a worldwide logging system has been developed.\n\n\n\n\n"}
{"id": "18912788", "url": "https://en.wikipedia.org/wiki?curid=18912788", "title": "Malawi Liverpool Wellcome Trust", "text": "Malawi Liverpool Wellcome Trust\n\nMLW was established in 1995 to conduct internationally excellent science to benefit human health with a focus on sub-Saharan Africa. MLW is built around excellent laboratories, strategically located in the largest hospital in Malawi, Queen Elizabeth Central Hospital. It is closely linked with the community and is an integral part of the College of Medicine. These relationships provide a unique opportunity replicated in few centres in Africa to study major health issues spanning both community and hospital.\n\nSince its inception, MLW has maintained partnership with the Liverpool School of Tropical Medicine, the University of Liverpool and the Wellcome Trust. MLW is based in Blantyre with field research sites around urban Blantyre, Thyolo, Chikhwawa, Zomba, Mangochi and Machinga.\n\n"}
{"id": "17780458", "url": "https://en.wikipedia.org/wiki?curid=17780458", "title": "Matter power spectrum", "text": "Matter power spectrum\n\nThe matter power spectrum describes the density contrast of the universe (the difference between the local density and the mean density) as a function of scale. It is the Fourier transform of the matter correlation function. On large scales, gravity competes with cosmic expansion, and structures grow according to linear theory. In this regime, the density contrast field is Gaussian, Fourier modes evolve independently, and the power spectrum is sufficient to completely describe the density field. On small scales, gravitational collapse is non-linear, and can only be computed accurately using N-body simulations. Higher-order statistics are necessary to describe the full field at small scales.\n"}
{"id": "58979992", "url": "https://en.wikipedia.org/wiki?curid=58979992", "title": "Morris Kiruga", "text": "Morris Kiruga\n\nMorris Kiruga (born sometime between 9th-20th June1990), popularly know simply as M., is a Kenyan thinker, non-fiction writer, researcher, forensic scientist, and blogger. He is most renown for his work on Owaahh.com, a content site that runs longform, investigative pieces on various topics, and various sites such as Quora. He writes mostly on little-known, quirky and bizarre stories, and uses history as an anchor to most of his analysis and writing.\n\nHis blog has twice won the Best Topical Blog at the annual BAKE (Bloggers Association of Kenya) Awards for 2017 and 2018. He also writes for numerous publications on a wide variety of topics including history, business, crime, travel, and culture. His work on Owaahh.com was the inspiration for the theatre show Too Early for Birds, whose name is also inspired by the original title of his blog \"\"Too Late for Worms\".\"\n\nAlthough Morris Kiruga writes regularly on politics and religion, he is an avowed atheist and abstainer from electoral politics. \"As a writer/historian/journalist, I am a chronicler of the political process, as well as a rather captivated student of it. I don’t vote because I belong to a small, quiet group of non-voters who are best described as abstainers.\"\n\nMorris Kiruga has a BSc (Forensic Science) from Kenyatta University (2009-2015). Although he never practised forensics beyond the classroom, he uses the forensic process to research and investigate for his stories. \n\nMorris Kiruga's work as an investigative writer has covered banks, geopolitics, Kenyan politics, business, and other subjects. Among his most prominent works are \"The Sack of Imperial Bank\"\",\" a 4-part series dissecting the fall of Imperial Bank Ltd. He also wrote a 2-series longform piece on 4 Kenyans jailed in South Sudan titled \"Blood, Money & Prison\" and an indepth analysis of the civil war within President Uhuru Kenyatta's communications team from 2010 to 2018 titled \"The Rise and Fall of the State House Boys\"\".\" He also wrote about the attempted murder of Australian-Kenyan businessman Andrew White by former Kenyan legislator Betty Tett, over a business squabble.\n\nIn April 2016, he questioned the logic of burning ivory stockpiles in a piece titled \"The Politics of Ivory and Fire\"\",\" which was syndicated by Daily Nation. \"To most black Kenyans, conservation remains a mzungu affair. And every one is in it for the money, not the common good.\"His September 27th 2018 article \"The Man Who Sold a Country\" was a hard-hitting analysis of the perspective of a modern Kenyan millennial. \n\nMorris' work has been published in several publications, including Heinrich Boll Foundation where he wrote about the politics of satire in Africa. \"The audience has become the satirist in the digital age, as the internet has greatly expanded the space for political satire. Where early Kenyan satirists had to conceptualise the rules of their art, audiences now. lead in content creation. A crowd favourite are memes that mock specific situations, especially potentially explosive issues and scandals. By caricaturing the punchlines that politicians and political spaces themselves create, audiences become more critical of their political environments. \"\"\n\nMorris has been involved in several book projects. As Special Projects Editor at Storymoja Africa, he curated and edited Careerpedia, a two-volume career encyclopedia for children and young adults. His work has also featured in coffee table books by Safaricom for the 2014 and 2016 calendars. He was also involved in some of the early stages of Boniface Mwangi's memoirs \"Unbounded.\"\n\nMorris runs a regular column, \"A Kenyan Traveller,\" on Nomad Magazine. He is also a regular contributor on different topics to \"The Africa Report, Msafiri,\" and The Elephant and an infrequent contributor to \"Daily Nation, Mail & Guardian\"\", Mail & Guardian Africa\" [defunct], \"The Star, EA Destination Magazine\" [Defunct]\", Sage Magazine\" [defunct], \"Africapedia\" and others.\n\nMorris has been a contributor, research, curator and writer in projects such as \"#147NotJustANumber\", \"Team Courage\", \"Storymoja Festival's \"Future of Men\" series and masterclasses\".\n\nMorris Kiruga revealed his identity as the curator behind Owaahh.com numerous times since the blog's inception, but audiences tend to see M. and Owaahh as alter egos of the same person. \"\"Owaahh is proper and collected while Morris Kiruga is the happy-go-lucky split end. Owaahh does the heavy lifting by investigating and researching on mysteries, while Morris Kiruga beats the resulting stories into shape. Whether they lead a double life or coexist dependently, we are yet to establish\".\"He is notorious for refusing to have photographs of himself taken, and publicly prefers to remain anonymous. The anonymity of being faceless, he says, let's him be the fly-on-the-wall, even when his subjects know his identity. \"Being a researcher, his incognito identity helps him observe easily without drawing attention to himself. That’s the only way one can tell a good story according to him. Asked why there’s not a single photo of him online, he mumbles something about maintaining the mystery. Owaahh agrees that his head is quite big and will compete with the story or distract the reader.\"\"One of the weirdest stories he has written is of a lady who works at a morgue and bites off the thread after sewing up the corpses. You definitely need a faceless big head to blend in while doing such stories.\"\n"}
{"id": "101900", "url": "https://en.wikipedia.org/wiki?curid=101900", "title": "Musical instrument classification", "text": "Musical instrument classification\n\nThroughout history, various methods of musical instrument classification have been used. The most commonly used system divides instruments into string instruments, woodwind instruments, brass instruments and percussion instruments; however, other schemes have been devised.\n\nThe oldest known scheme of classifying instruments is Chinese and dates from the 3rd millennium BC. It grouped instruments according to the materials they are made of. Instruments made of stone were in one group, those of wood in another, those of silk are in a third, and those of bamboo in a fourth, as recorded in the \"Yo Chi\" (record of ritual music and dance), compiled from sources of the Chou period (9th-5th centuries BC) and corresponding to the four seasons and four winds.\n\nThe eight-fold system of pa yin (八音 \"eight sounds\", \"octave\"), from the same source, occurred gradually, and in the legendary Emperor Zhun's time (3rd millennium BC) it is believed to have been presented in the following order: metal (\"chin\"), stone (\"shih\"), silk (\"ssu\"), bamboo (\"che\"), gourd (\"piao\"), clay (\"t'u\"), leather (\"hoes\"), and wood (\"mutt\") classes, and it correlated to the eight seasons and eight winds of Chinese culture, autumn and west, autumn-winter and NW, summer and south, spring and east, winter-spring and NE, summer-autumn and SW, winter and north, and spring-summer and SE, respectively.\n\nHowever, the Chou-Li (Rites of Chou), an anonymous treatise compiled from earlier sources in about the 2nd century BC, had the following order: metal, stone, clay, leather, silk, wood, gourd, and bamboo. The same order was presented in the Tso Chuan (Commentary of Tso), attributed to Tso Chiu-Ming, probably compiled in the 4th century BC.\n\nMuch later, Ming dynasty (14th-17th century) scholar Chu Tsai Yu recognized three groups: those instruments using muscle power or used for musical accompaniment, those that are blown, and those that are rhythmic, a scheme which was probably the first scholarly attempt, while the earlier ones were traditional, folk taxonomies.\n\nMore usually, instruments are classified according to how the sound is initially produced (regardless of post-processing, i.e., an electric guitar is still a string-instrument regardless of what analog or digital/computational post-processing effects pedals may be used with it).\n\nThe modern system divides instruments into wind, strings and percussion. It is of Greek origin (in the Hellenistic period, prominent proponents being Nicomachus and Porphyry). The scheme was later expanded by Martin Agricola, who distinguished plucked string instruments, such as guitars, from bowed string instruments, such as violins. Classical musicians today do not always maintain this division (although plucked strings are grouped separately from bowed strings in sheet music), but distinguish between wind instruments with a reed (woodwinds) and those where the air is set in motion directly by the lips (brass instruments).\n\nMany instruments do not fit very neatly into this scheme. The serpent, for example, ought to be classified as a brass instrument, as a column of air is set in motion by the lips. However, it looks more like a woodwind instrument, and is closer to one in many ways, having finger-holes to control pitch, rather than valves.\n\nKeyboard instruments do not fit easily into this scheme. For example, the piano has strings, but they are struck by hammers, so it is not clear whether it should be classified as a string instrument or a percussion instrument. For this reason, keyboard instruments are often regarded as inhabiting a category of their own, including all instruments played by a keyboard, whether they have struck strings (like the piano), plucked strings (like the harpsichord) or no strings at all (like the celesta).\n\nIt might be said that with these extra categories, the classical system of instrument classification focuses less on the fundamental way in which instruments produce sound, and more on the technique required to play them.\n\nVarious names have been assigned to these three traditional Western groupings:\n\nAn ancient system of Indian origin, dating from the 4th or 3rd century BC, in the Natya Shastra, a theoretical treatise on music and dramaturgy, by Bharata Muni, divides instruments into four main classification groups: instruments where the sound is produced by vibrating strings (\"tata vadya\", \"stretched instruments\"); instruments where the sound is produced by vibrating columns of air (\"susira vadya\", \"hollow instruments\"); percussion instruments made of wood or metal (\"Ghana vadya\", \"solid instruments\"); and percussion instruments with skin heads, or drums (\"avanaddha vadya\", \"covered instruments\").\n\nVictor-Charles Mahillon later adopted a system very similar to this. He was the curator of the musical instrument collection of the conservatoire in Brussels, and for the 1888 catalogue of the collection divided instruments into four groups: strings, winds, drums, and other percussion. This scheme was later taken up by Erich von Hornbostel and Curt Sachs who published an extensive new scheme for classication in \"Zeitschrift für Ethnologie\" in 1914. Their scheme is widely used today, and is most often known as the Hornbostel–Sachs system (or the Sachs–Hornbostel system).\n\nThe original Sachs–Hornbostel system classified instruments into four main groups; the fifth group, electrophones, was added later by Sachs:\n\nIn the Hornbostel–Sachs classification of musical instruments, lamellophones are considered plucked idiophones, a category that includes various forms of jaw harp and the European mechanical music box, as well as the huge variety of African and Afro-Latin thumb pianos such as the mbira and marimbula.\n\nLater Sachs added a fifth category, electrophones, such as theremins, which produce sound by electronic means. Modern synthesizers and electronic instruments fall in this category. Within each category are many subgroups. The system has been criticised and revised over the years, but remains widely used by ethnomusicologists and organologists.\n\nOne notable example of this criticism is that there is care to be taken with electrophones, as some electronic instruments like the electric guitar (chordophone) and some electronic keyboards (sometimes idiophones or chordophones) can produce music without electricity or the use of an amplifier.\n\nIn 1932, comparative musicologist (ethnomusicologist) André Schaeffner developed a new classification scheme that was \"exhaustive, potentially covering all real and conceivable instruments\".\n\nSchaeffner's system has only two top-level categories which he denoted by Roman numerals:\n\nThe system agrees with Mahillon and Hornbostel–Sachs for chordophones, but groups percussion instruments differently.\n\n2nd-century Greek grammarian, sophist, and rhetoritician Julius Pollux, in the chapter called De Musica of his ten-volume \"Onomastikon\", presented the two-class system, percussion (including strings) and winds, which persisted in medieval and postmedieval Europe. It was used by St. Augustine (4th and 5th centuries), in his De Ordine, applying the terms rhythmic (percussion and strings), organic (winds), and adding harmonic (the human voice); Isidore of Seville (6th to 7th centuries); Hugh of Saint Victor (12th century), also adding the voice; Magister Lambertus (13th century), adding the human voice as well; and Michael Praetorius (17th century).\n\nThe Kpelle of West Africa also use this system. They distinguish the struck (\"yàle\"), including both beaten and plucked, and the blown (\"fêe\"). The \"yàle\" group is subdivided into five categories: instruments possessing lamellas (the sanzas); those possessing strings; those possessing a membrane (various drums); hollow wooden, iron, or bottle containers; and various rattles and bells. The Hausa, also of West Africa, classify drummers into those who beat drums and those who beat (pluck) strings (the other four player classes are blowers, singers, acclaimers, and talkers), Kartomi does not specify if these two classifications pre-date Schaeffner or Pollux. The concept, the way the person produces the sound, is human-centered, which is part of their traditional culture so presumably they at least pre-date Schaeffner.\n\nThe MSA (Multi-Dimensional Scalogram Analysis) of René Lysloff and Jim Matson, using 37 variables, including characteristics of the sounding body, resonator, substructure, sympathetic vibrator, performance context, social context, and instrument tuning and construction, corroborated Schaeffner, producing two categories, aerophones and the chordophone-membranophone-idiophone combination.\n\nAnother similar system is the five-class, physics-based organology that was presented by Steve Mann in 2007, comprises Gaiaphones (Chordophones, Membranophones, and Idiophones), Hydraulophones, Aerophones, Plasmaphones, and Quintephones (electrically and optically produced music), the names referring to the five essences, earth, water, wind, fire and the quintessence, thus adding three new categories to the Schaeffner taxonomy.\nElementary organology, also known as physical organology, is a classification scheme based on the elements (i.e. states of matter) in which sound production takes place. \"Elementary\" refers both to \"element\" (state of matter) and to something that is fundamental or innate (physical). The elementary organology map can be traced to Kartomi, Schaeffner, Yamaguchi, and others, as well as to the Greek and Roman concepts of elementary classification of all objects, not just musical instruments.\n\nElementary organology categorizes musical instruments by their classical element, i.e.\n\nInstruments can be classified by their musical range in comparison with other instruments in the same family. These terms are named after singing voice classifications:\n\nSome instruments fall into more than one category: for example, the cello may be considered either tenor or bass, depending on how its music fits into the ensemble, and the trombone may be alto, tenor, or bass and the French horn, bass, baritone, tenor, or alto, depending on which range it is played. In a typical concert band setting, the first alto saxophone covers soprano parts, while the second alto saxophone covers alto parts.\n\nMany instruments include their range as part of their name: soprano saxophone, alto saxophone, tenor saxophone, baritone saxophone, baritone horn, alto flute, bass flute, alto recorder, bass guitar, etc. Additional adjectives describe instruments above the soprano range or below the bass, for example: sopranino saxophone, contrabass clarinet.\n\nWhen used in the name of an instrument, these terms are relative, describing the instrument's range in comparison to other instruments of its family and not in comparison to the human voice range or instruments of other families. For example, a bass flute's range is from C to F, while a bass clarinet plays about one octave lower.\n\nInstruments can be categorized according to a common use, such as signal instruments, a category that may include instruments in different Hornbostel–Sachs categories such as trumpets, drums, and gongs. An example based on this criterion is Bonanni (e.g., festive, military, and religious). He separately classified them according to geography and era.\n\nJean-Benjamin de la Borde (1780) classified instruments according to ethnicity, his categories being black, Abyssinian, Chinese, Arabic, Turkisk, and Greek.\n\nInstruments can be classified according to the ensemble in which they play, or the role they play in the ensemble. For example, the horn section in popular music typically includes both brass instruments and woodwind instruments. The symphony orchestra typically has the strings in the front, the woodwinds in the middle, and the basses, brass, and percussion in the back.\n\nClassifications done for the Indonesian ensemble, the gamelan, were done by Jaap Kunst (1949), Martopangrawit, Poerbapangrawit, and Sumarsam (all in 1984). Kunst described five categories: nuclear theme (\"cantus firmus\" in Latin and \"balungan\" (\"skeletal ramework\") in Indonesian); colotomic ( a word invented by Kunst) (interpunctuating), the gongs; countermelodic; paraphrasing (\"panerusan\"), subdivided as close to the nuclear theme and ornamental filling; agogic (tempo-regulating), drums.\n\nR. Ng. Martopangrawit has two categories, irama (the rhythm instruments) and lagu (the melodic instruments), the former corresponds to Kunst's classes 2 and 5, and the latter to Kunst's 1, 3, and 4.\n\nKodrat Poerbapangrawit, similar to Kunst, derives six categories: \"balungan\", the \"saron\", \"demung\", and \"slenthem\"; \"rerenggan\" (ornamental), the \"gendèr\", \"gambang\", and \"bonang\"); \"wiletan\" (variable formulaic melodic), \"rebab\" and male chorus (\"gerong\"); \"singgetan\" (interpunctuating); \"kembang\" (floral), flute and female voice; jejeging wirama (tempo regulating), drums.\n\nSumarsam's scheme comprises:\n\nThe gamelan is also divided into front, middle, and back, much like the symphony orchestra.\n\nAn orally-transmitted Javanese taxonomy has 8 groupings:\n\nA Javanese classification transmitted in literary form is as follows:\n\nThis is much like the pa yin. It is suspected of being old but its age is unknown.\n\nMinangkabau musicians (of West Sumatra) use the following taxonomy for \"bunyi-bunyian\" (\"objects that sound\"): \"dipukua\" (\"beaten\"), \"dipupuik\" (\"blown), \"dipatiek\" (\"plucked\"), \"ditariek\" (\"pulled\"), \"digesek\" (\"bowed\"), \"dipusiang\" (\"swung\"). The last one is for the bull-roarer. They also distinguish instruments on the basis of origin because of sociohistorical contacts, and recognize three categories: Mindangkabau (\"Minangkabau asli\"), Arabic (\"asal Arab\"), and Western (\"asal Barat\"), each of these divided up according to the five categories. Classifying musical instruments on the basis sociohistorical factors as well as mode of sound production is common in Indonesia.\n\nThe Batak of North Sumatra recognize the following classes: beaten (\"alat pukul\" or \"alat palu\"), blown (\"alat tiup\"), bowed (\"alat gesek\"), and plucked (\"alat petik\") instruments, but their primary classification is of ensembles.\n\nIn West Africa, tribes such as the Dan, Gio, Kpelle, Hausa, Akan, and Dogon, use a human-centered system. It derives from 4 myth-based parameters: the musical instrument's nonhuman owner (spirit, mask, sorcerer, or animal), the mode of transmission to the human realm (by gift, exchange, contract, or removal), the making of the instrument by a human (according to instructions from a nonhuman, for instance), and the first human owner. Most instruments are said to have a nonhuman origin, but some are believed invented by humans, e.g., the xylophone and the lamellophone.\n\nIn 1960, German musicologist Kurt Reinhard presented a stylistic taxonomy, as opposed to a morphological one, with two divisions determined by either single or multiple voiced playing. Each of these two divisions was subdivided according to pitch changeability (not changeable, freely changeable, and changeable by fixed intervals), and also by tonal continuity (discontinuous (as the marimba and drums) and continuous (the friction instruments (including bowed) and the winds), making 12 categories. He also proposed classification according to whether or not they had dynamic tonal variability, a characteristic that separates whole eras (e.g., the baroque from the classical) as in the transition from the terraced dynamics of the harpsichord to the crescendo of the piano, grading by degree of absolute loudness, timbral spectra, tunability, and degree of resonance.\n\nAl-Farabi, Persian scholar of the 10th century, also distinguished tonal duration. In one of his four schemes, in his two-volume \"Kitab al-Musiki al-Kabir\" (\"Great Book of Music\") he identified five classes, in order of ranking, as follows: the human voice, the bowed strings (the \"rebab\") and winds, plucked strings, percussion, and dance, the first three pointed out as having continuous tone.\n\nIbn Sina, Persian scholar of the 11th century, presented a scheme in his \"Kitab al-Najat\" (Book of the Delivery), made the same distinction. He used two classes. In his \"Kitab al-Shifa\" (Book of Soul Healing), he proposed another taxonomy, of five classes: fretted instruments, unfretted (open) stringed, lyres and harps, bowed stringed, wind (reeds and some other woodwinds, such as the flute and bagpipe), other wind instruments such as the organ, and the stick-struck santur (a board zither). The distinction between fretted and open was in classic Persian fashion.\n\n"}
{"id": "48567867", "url": "https://en.wikipedia.org/wiki?curid=48567867", "title": "Nano aquarium", "text": "Nano aquarium\n\nNano aquariums are aquariums with the emphasis on small scale inhabitants inside smaller sized aquariums. They can be either freshwater nano aquariums or saltwater nano aquariums. Nano fish species are chosen from any smaller sized species of tropical fish. Other possible inhabitants are freshwater and seawater shrimps. Any plants for a nano aquarium also have to be of the smaller species.\nThe marine nano aquarium is more difficult to maintain than the nano freshwater aquarium. \n\nNano reefs\n"}
{"id": "2227366", "url": "https://en.wikipedia.org/wiki?curid=2227366", "title": "Photoheterotroph", "text": "Photoheterotroph\n\nPhotoheterotrophs (\"Gk\": photo = light, hetero = (an)other, troph = nourishment) are heterotrophic phototrophs—that is, they are organisms that use light for energy, but cannot use carbon dioxide as their sole carbon source. Consequently, they use organic compounds from the environment to satisfy their carbon requirements; these compounds include carbohydrates, fatty acids, and alcohols. Examples of photoheterotrophic organisms include purple non-sulfur bacteria, green non-sulfur bacteria, and heliobacteria. Recent research has indicated that the oriental hornet and some aphids may be able to use light to supplement their energy supply.\n\nPhotoheterotrophs generate ATP using light, in one of two ways: they use a bacteriochlorophyll-based reaction center, or they use a bacteriorhodopsin. The chlorophyll-based mechanism is similar to that used in photosynthesis, where light excites the molecules in a reaction center and causes a flow of electrons through an electron transport chain (ETS). This flow of electrons through the proteins causes hydrogen ions to be pumped across a membrane. The energy stored in this proton gradient is used to drive ATP synthesis. Unlike in photoautotrophs, the electrons flow only in a cyclic pathway: electrons released from the reaction center flow through the ETS and return to the reaction center. They are not utilized to reduce any organic compounds. Purple non-sulfur bacteria, green non-sulfur bacteria and heliobacteria are examples of bacteria that carry out this scheme of photoheterotrophy.\n\nOther organisms, including halobacteria and flavobacteria and vibrios have purple-rhodopsin-based proton pumps that supplement their energy supply. The archaeal version is called bacteriorhodopsin, while the eubacterial version is called proteorhodopsin. The pump consists of a single protein bound to a Vitamin A derivative, retinal. The pump may have accessory pigments (e.g., carotenoids) associated with the protein. When light is absorbed by the retinal molecule, the molecule isomerises. This drives the protein to change shape and pump a proton across the membrane. The hydrogen ion gradient can then be used to generate ATP, transport solutes across the membrane, or drive a flagellar motor. One particular flavobacterium cannot reduce carbon dioxide using light, but uses the energy from its rhodopsin system to fix carbon dioxide through anaplerotic fixation. The flavobacterium is still a heterotroph as it needs reduced carbon compounds to live and cannot subsist on only light and CO. It cannot carry out reactions in the form of\nwhere HD may be water, HS or another compound/compounds providing the reducing electrons and protons; the 2D + HO pair represents an oxidized form.\n\nHowever, it can fix carbon in reactions like:\nwhere malate or other useful molecules are otherwise obtained by breaking down other compounds by\n\nThis method of carbon fixation is useful when reduced carbon compounds are scarce and cannot be wasted as CO during interconversions, but energy is plentiful in the form of sunlight.\n\n\n\nUniversity of Wisconsin, Madison Microbiology Online Textbook\n"}
{"id": "15364809", "url": "https://en.wikipedia.org/wiki?curid=15364809", "title": "Process development execution system", "text": "Process development execution system\n\nProcess development execution systems (PDES) are software systems used to guide the development of high-tech manufacturing technologies like semiconductor manufacturing, MEMS manufacturing, photovoltaics manufacturing, biomedical devices or nanoparticle manufacturing. Software systems of this kind have similarities to product lifecycle management (PLM) systems. They guide the development of new or improved technologies from its conception, through development and into manufacturing. Furthermore they borrow on concepts of manufacturing execution systems (MES) systems but tailor them for R&D rather than for production. PDES integrate people (with different backgrounds from potentially different legal entities), data (from diverse sources), information, knowledge and business processes.\n\nDocumented benefits of process development execution systems include:\n\nA process development execution system (PDES) is a system used by companies to perform development activities for high-tech manufacturing processes.\nSoftware systems of this kind leverage diverse concepts from other software categories like PLM, manufacturing execution system (MES), ECM but focus on tools to speed up the technology development rather than the production.\n\nA PDES is similar to a manufacturing execution systems (MES) in several ways. The key distinguishing factor of a PDES is that it is tailored for steering the development of a manufacturing process, while MES is tailored for executing the volume production using the developed process. Therefore, the toolset and focus of a PDES is on lower volume but higher flexibility and experimentation freedom. The tools of an MES are more focused on less variance, higher volumes, tighter control and logistics. Both types of application software increase traceability, productivity, and quality of the delivered result. For PDESs quality refers to the capability of the process to perform without failure under a wide range of conditions, i.e. the robustness of the developed manufacturing process. For MESs quality refers to the quality of the manufactured good/commodity. Additionally both software types share functions including equipment tracking, product genealogy, labour and item tracking, costing, electronic signature capture, defect and resolution monitoring, executive dashboards, and other various reporting solutions.\n\nIn contrast to PLM systems, PDES typically address the collaboration and innovation challenges with a bottom-up approach. They start-out with the details of manufacturing technologies (like PPLM), a single manufacturing step with all its physical aware parameterization and integrating steps into sequences, into devices, into systems, etc.\n\nOther rather similar software categories are laboratory information management systems (LIMS) and laboratory information system (LIS). PDESs offer a wider set of functionalities e.g. virtual manufacturing techniques, while they are typically not integrated with the equipment in the laboratory.\n\nPDESs have many parts and can be deployed on various scales – from simple Work in Progress tracking, to a complex solution integrated throughout an enterprise development infrastructure. The latter connects with other enterprise systems like enterprise resource and planning systems (ERPs), manufacturing execution systems (MESs), product lifecycle management (PLM), supervisory, control and data acquisition (SCADA) solutions and scheduling and planning systems (both long-term and short-term tactical).\n\nNew ideas for manufacturing processes (for new goods/commodities or improved manufacturing) are often based on, or can at least benefit from, previous developments and recipes already in use. The same is true when developing new devices, for example, a MEMS sensor or actuator. A PDES offers an easy way to access these previous developments in a structured manner. Information can be retrieved faster, and previous results can be taken into account more efficiently. A PDES typically offers means to display and search for result data from different viewpoints, and to categorise the data according to the different aspects. These functionalities are applied to all result data, such as materials, process steps, machines, experiments, documents and pictures. The PDES also provides a way to relate entities belonging to the same or similar context and to explore the resulting information.\n\nIn the assembly phase from process steps to process flows, a PDES helps to easily build, store, print, and transfer new process flows. By providing access to previously assembled process flows the designer is able to use those as building blocks or modules in the newly developed flow. The usage of standard building blocks can dramatically reduce the design time and the probability of errors.\n\nA PDES demonstrates its real benefits in the verification phase. Knowledge (for example in the semiconductor device fabrication – clean before deposition; After polymer spin-on no temperature higher than 100 °C until resist is removed) is provided in a format that can be interpreted by a computer as rules. If a domain expert enters the rules for his/her process steps, all engineers can later use these rules to check newly developed process flows, even if the domain expert is not available. For a PDES, this means it has to be able to \n\nThe processing rule check gives no indication about the functionality or even the structure of the produced good or device. In the area of semiconductor device fabrication, the techniques of semiconductor process simulation / TCAD can provide an idea about the produced structures. To support this ’virtual fabrication’, a PDES is able to manage simulation models for process steps. Usually the simulation results are seen as standalone data. To rectify this situation PDESs are able to manage the resulting files in combination with the process flow. This enables the engineer to easily compare the expected results with the simulated outcome. The knowledge gained from the comparison can again be used to improve the simulation model.\n\nAfter virtual verification the device is produced in an experimental fabrication environment. A PDES allows a transfer of the process flow to the fabrication environment (for example in semiconductor: FAB). This can be done by simply printing out a runcard for the operator or by interfacing to the Manufacturing Execution Systems (MES) of the facility. On the other hand a PDES is able to manage and document last minute changes to the flow like parameter adjustments during the fabrication.\nDuring and after processing a lot of measurements are taken. The results of these measurements are often produced in the form of files such as images or simple text files containing rows and columns of data. The PDES is able to manage these files, to link related results together, and to manage different versions of certain files, for example reports. Paired with flexible text, and graphical retrieval and search methods, a PDES provides the mechanism to view and assess the accumulated data, information and knowledge from different perspectives. It provides insight into both the information aspects as well as the time aspects of previous developments.\n\nDevelopment activities within high tech industries are an increasingly collaborative effort. This leads to the need to exchange information between the partners or to transfer process intellectual property from a vendor to a customer. PDESs' support this transfer while being selective to protect the IPR of the company.\n\n\n\n"}
{"id": "21119753", "url": "https://en.wikipedia.org/wiki?curid=21119753", "title": "Quarter-life crisis", "text": "Quarter-life crisis\n\nIn popular psychology, a quarter-life crisis is a crisis \"involving anxiety over the direction and quality of one's life\" which is most commonly experienced in a period ranging from a person's twenties up to their mid-thirties (although it can begin as early as 18). It is defined by clinical psychologist Alex Fowke as “a period of insecurity, doubt and disappointment surrounding your career, relationships and financial situation\".\n\nAccording to Merideth Goldstein of \"The Boston Globe\", the \"quarter-life crisis\" occurs in one's twenties, after entering the \"real world\" (i.e., after graduating college, and/or after moving out of the family home). German psychologist Erik H. Erikson, who proposed eight crises that humans face during their development, proposed the existence of a life crisis occurring at this age. The conflict he associated with young adulthood is the \"Intimacy vs. Isolation\" crisis. According to Erikson after establishing a personal identity in adolescence, young adults seek to form intense, usually romantic relationships with other people.\n\nCommon symptoms of a quarter-life crisis are often feelings of being \"lost, scared, lonely or confused\" about what steps to take in early adulthood. Studies have shown that unemployment and choosing a career path is a major cause for young adults to undergo stress or anxiety. Early stages of one living on their own for the first time and learning to cope without parental help can also induce feelings of isolation and loneliness. Re-evaluation of one's close personal relationships can also be a factor, with sufferers feeling they have outgrown their partner or believing others may be more suitable for them.\n\nRecently, millennials are sometimes referred to as the \"Boomerang Generation\" or \"Peter Pan Generation\", because of the members' perceived penchant for delaying some rites of passage into adulthood for longer periods than previous generations. These labels were also a reference to a trend toward members returning home after college and/or living with their parents for longer periods than previous generations. These tendencies might also be partly explained by changes in external social factors rather than characteristics intrinsic to milennials (e.g., higher levels of student loan debt in the US among milennials when compared to earlier generations can make it more difficult for young adults to achieve traditional markers of independence such as marriage, home ownership or investing).\n\nThe notion of the quarter-life crisis is explored by the 1967 film \"The Graduate\", one of the first film depictions of this issue. Other notable films that also do so are \"Bright Lights, Big City\"; \"The Paper Chase\"; \"St. Elmo's Fire\"; \"How to Be\"; \"Reality Bites\"; \"Garden State\"; \"Accepted\"; \"Ghost World\"; \"High Fidelity\"; \"(500) Days of Summer\"; \"Lost in Translation\"; \"Silver Linings Playbook\"; \"Vicky Cristina Barcelona\"; and \"Shaun of the Dead\"; as well as the musical \"Avenue Q\", in the television show \"The Office\", and the HBO television series \"Girls\". The 2008 web series \"Quarterlife\" was so named for the phenomenon. Other movies exploring the quarter-life crisis include: \"Tiny Furniture\", \"The Puffy Chair\", \"Fight Club\", \"Stranger than Fiction\", \"Greenberg\", \"Frances Ha\" and \"Eternal Sunshine of the Spotless Mind\". A 2014 comedy directed by Lynn Shelton titled \"Laggies\" lightheartedly delves into the complexities of a quarter-life crisis.\n\nThe 2003 John Mayer single \"Why Georgia\" explores the concept of a quarter-life crisis. The song was based upon John Mayer's experiences during this age period, when he moved to Georgia.\n\nThe 1975 Fleetwood Mac song \"Landslide\", written by Stevie Nicks in her late twenties, explores many of the self-doubts and fears of the quarter-life crisis, at a time when Nicks professed to be uncertain about her musical career and her romantic life.\n\nEnglish indie rock band Spector's song \"True Love (For Now)\", the opening track to their 2012 album \"Enjoy It While It Lasts\", references a quarter-life crisis.\n\n\"20 Something\", the final track on \"SZA\"s 2017 album Ctrl, delves into the many insecurities she experienced in her twenties, both personal and professional, and the urgency she felt to make the most of her life before entering into mature adulthood.\n\n"}
{"id": "33127110", "url": "https://en.wikipedia.org/wiki?curid=33127110", "title": "Rapid Eye Mount telescope", "text": "Rapid Eye Mount telescope\n\nThe Rapid Eye Mount telescope (REM) is a fully automatic, 60 cm aperture telescope located at ESO's La Silla Observatory at 2,400 metres altitude on the edge of the Atacama Desert in Chile. The telescope's aim is to catch the afterglows of gamma-ray bursts (GRBs). REM is triggered by a signal from a high-energy satellite such as Swift and rapidly points to the detected location in the sky. It is operated for the Italian National Institute for Astrophysics since 2002.\n\nThe telescope has been designed to be a fast pointing instrument, and its relatively small size is in fact balanced by a 10°/s accurate fast pointing. This velocity makes REM suitable for immediate response to random alerts.\n\nThe telescope hosts two instruments: REMIR, an infrared imaging camera, and ROSS, a visible imager and slitless spectrograph. The two cameras can observe simultaneously thanks to a dichroic placed before telescope focus the same field of view of 10×10 arc minutes. In the infrared range from 1 to 2.3 µm REMIR can use a (z′, J, H, K′) filter set. ROSS is equipped with a standard filter set (V, R, I) and an slitless Amici prism.\n\nThe observing procedure is completely robotic and the nightly schedule is optimized for the observation of scheduled targets but it is immediately overdriven by GRB (or other) alerts. Typically REM can observe the new target after 30 seconds from notification. \n\nREM has been installed in its place during June 2003 and has been gathering data on GRB and other sources since then. Also it is a bench for experimental instrumentation and equipment. \n\nIn 2006 a wide-field camera parallel to the REM telescope, the TORTORA camera (Telescopio Ottimizzato per la Ricerca dei Transienti Ottici RApidi) was installed. TORTORA has a field of view of 24°x32° through an objective of 120 mm diameter. The instrument was optimized for photometry of fast transients with a best time resolution of about 0.1 s.\n\nThe observatory is operated for the Instituto Nazionale di Astrofisica by the REM Team.\n\nSince its installation and commissioning at ESO, REM rapid and multi-band observations allowed to contribute to several important discoveries in some cases widely reported by ESO press releases; for instance, the observations a few seconds after its discovery of GRB060418, GRB06067A and GRB080319B.\n\nIn addition, a larger set of observations of targets different from GRBs has been performed, securing long time series of data for variable stars, AGNs, etc.\n\n"}
{"id": "37170", "url": "https://en.wikipedia.org/wiki?curid=37170", "title": "Red slender loris", "text": "Red slender loris\n\nThe red slender loris (\"Loris tardigradus\") is a small, nocturnal strepsirrhine primate native to the rainforests of Sri Lanka. This is #6 of the 10 focal species and #22 of the 100 EDGE mammal species worldwide considered the most evolutionarily distinct and globally endangered. Two subspecies have been identified, \"L. t. tardigradus\" and \"L. t. nycticeboides\".\n\n\nThe ears are less prominent in \"L. tardigradus tardigradus\" compared to \"Loris lydekkerianus\". The ears of \"L. tardigradus nycticeboides\" are almost invisible.\n\nThis small, slender primate is distinguished by large forward-facing eyes used for precise depth perception, long slender limbs, a well-developed index finger, the absence of tail, and large prominent ears, which are thin, rounded and hairless at the edges. The soft dense fur is reddish-brown color on the back, and the underside is whitish-grey with a sprinkling of silver hair. Its body length on average is , with an average weight of a mere . This loris has a four-way grip on each foot. The big toe opposes the other 4 toes for a pincer-like grip on branches and food. It has a dark face mask with central pale stripe, much like the slow lorises.\n\n\"L. tardigradus tardigradus\" is reddish brown in the back and creamy yellow below, while \"L. tardigradus nycticeboides\" is dark brown dorsally and very light brown in upperparts.\n\nThe red slender loris favors lowland rainforests (up to 700 m in altitude), tropical rainforests and inter-monsoon forests of the south western wet-zone of Sri Lanka. Masmullah Proposed Forest Reserve harbors one of few remaining red slender loris populations, and is considered a biodiversity hotspot. The most common plant species eaten was \"Humboldtia laurifolia\", occurring at 676 trees/ha, with overall density at 1077 trees/ha. \"Humboldtia laurifolia\" is vulnerable and has a mutualistic relationship with ants, providing abundant food for lorises. Reports from the 1960s suggest that it once also occurred in the coastal zone, however it is now thought to be extinct there.\n\nThe red slender loris differ from its close relative the gray slender loris in its frequent use of rapid arboreal locomotion. It forms small social groups, containing adults of both sexes as well as young animals. This species is among the most social of the nocturnal primates. During daylight hours the animals sleep in groups in branch tangles, or curled up on a branch with their heads between their legs. The groups also undertake mutual grooming and play at wrestling. The adults typically hunt separately during the night. They are primarily insectivorous but also eat bird eggs, berries, leaves, buds and occasionally invertebrates as well as geckos and lizards. To maximize protein and nutrient uptake they consume every part of their prey, including the scales and bones. They make nests out of leaves or find hollows of trees or a similar secure place to live in.\n\nFemales are dominant. The female reaches her sexual maturity at 10 months and is receptive to the male twice a year. This species mates while hanging upside down from branches; individuals in captivity will not breed if no suitable branch is available. The gestation period is 166–169 days after which the female will bear 1–2 young which feed from her for 6–7 months. The lifespan of this species is believed to be around 15–18 years in the wild.\n\nThis slender loris is an endangered species. Habitat destruction is a major threat. It is widely trapped and killed for use in supposed remedies for eye diseases and get killed by snakes, dogs, and some fish. Other threats include: electrocution on live wires, road accidents and the pet trade.\n\nThe red slender loris was identified as one of the top-10 \"focal species\" in 2007 by the Evolutionarily Distinct and Globally Endangered (EDGE) project.\n\nOne early success has been the rediscovery of the virtually unknown Horton Plains slender loris (\"Loris tardigradus nycticeboides\"). Originally documented in 1937, there have only been four known encounters in the past 72 years, and for more than 60 years until 2002 the sub-species had been believed to be extinct. The sub-species was rediscovered in 2002 by a team led by Anna Nekaris in Horton Plains National Park. The late 2009 capture by a team working under the Zoological Society of London's EDGE programme has resulted in the first detailed physical examination of the Horton Plains sub-species and the first-ever photographs of it. The limited available evidence suggests there may be only about 100 animals still existing, which would make it among the top five most-threatened primates worldwide.\n\n"}
{"id": "1505896", "url": "https://en.wikipedia.org/wiki?curid=1505896", "title": "Rothera Research Station", "text": "Rothera Research Station\n\nThe Rothera Research Station is a British Antarctic Survey (BAS) base on the Antarctic Peninsula, located at Rothera Point, Adelaide Island. Rothera also serves as the capital of the British Antarctic Territory, a British Overseas Territory.\n\nRothera station was established in 1975 to replace Adelaide station (1961-1977) where the skiway had deteriorated.\n\nThe opening of the Bonner Laboratory in 1996/1997 marked the start of new activities in biological sciences in the Antarctic peninsula. These included scuba diving and experiments conducted in the Bonner Laboratory throughout the year. The first Bonner Lab burned down in the winter of 2001 after an electrical fault; it was rebuilt and opened in December 2003. Meteorological research using satellite data intercepted at the Rothera ground station also continues year round.\n\nIn January 2017, it was announced that the Rothera Research Station will receive £100m in funding from the government. The money will be used by the British Antarctic Survey to build new living quarters, storage and a new wharf. Tim Stockings, its director of operations called the investment “an exciting moment for polar science”. A portion of the money will also be used to fund the modernisation of facilities and buildings at the British Antarctic stations in Signy, Bird Island and at King Edward Point.\n\nFieldwork is concentrated in the summer months from November until March. Once in the field, the parties travel using skidoos and sledges for up to four months, and, being in daily HF radio communication with Rothera, they can be resupplied when necessary by air.\n\nThe station is open throughout the year with a maximum population of 130 in the summer and an average winter population of 22.\n\nIn 1998, 26 sounding rockets of \"Viper\"-type were launched from Rothera Research Station. They reached altitudes of 100 kilometres (over 60 mi).\n\nRothera has evolved from a small base (in its first winter it housed only four people) to the large complex it is today. As is the case everywhere in Antarctica, the buildings need constant repair, and eventual renewal, as the harsh environment takes its toll. Although some of the buildings are very new, some of the older ones still survive, often having undergone many different uses.\n\nThis two story building houses the communal dining area, bar, library, film/TV rooms, computer facilities, phone booth, some offices and the post office/ station shop. It was opened in 2008.\n\nThis was re-built from the original in 1985/1986, using parts of the old building. The building was the hub of the base, it has the bulk of the non-science offices, computer rooms, communication facilities, meteorological facilities, dried food storage and kitchen. The building was named after the former BAS ship \"\". There is a link corridor to the garage, and on one end is the operations tower, used during flight operations. Bransfield also produces all the fresh water for the base using a reverse osmosis plant. This was installed to replace old melt tanks, which were used to melt snow. \n\nAdmirals House was built over two seasons (1999/2000 and 2000/2001). It is a prefabricated unit from Top Housing AB of Sweden. The building has 44 two-person rooms, each with a shower and toilet facility. The building has washing facilities and heated boot rooms. It is named after a dog team that operated out of Rothera.\n\nThe Bonner Lab has been built twice, the first time in 1996/1997. A fire in winter 2001, caused by an electrical fault, destroyed the building, though nobody was hurt. The lab was then rebuilt in the 2002/2003 seasons and opened in the 2003/2004 season. The Bonner Lab is a state-of-the-art facility for terrestrial and marine biology. The dive facility (with decompression chamber, warming bath, and compressors) keeps diving safely going throughout the year. There are three dry labs, one wet lab, aquarium, library, microscope room and a small kitchen. During the winter this large facility is left in the hands of the dive officer, a terrestrial biologist and two marine biologists, although this can vary depending on the projects underway at the time. In the summer, as many as 30 science staff can occupy the building, and upwards of 10 divers can be using the facility. The lab was named after W. Nigel Bonner, head of biological science at BAS between 1953 and 1986, and deputy director of BAS from 1986 to 1988. The original lab was built in response to the base at Signy being down scaled to a summer only facility.\n\nAlso known as the Sledge Store, or Phase III. The building was erected in 1978/79, and originally housed the science offices, cold room and travel store. It is now used mainly as the travel store or sledge store. The huge amount of mountaineering or camping equipment for use in Antarctica is maintained and stored here. The cold store remains, with four large freezers storing all the base's frozen food. It was named after Sir Vivian Fuchs, BAS Director from 1958 to 1973.\n\nErected in 1996/1997 as transit accommodation, it contains eight rooms of four beds, and a toilet / shower facility. The building is only used in summer.\n\nBingham House was originally at Adelaide Island Base, and is as such the oldest building on site. It was pulled across from Base T in the winter of 1977. It was used as accommodation, but is now used as a building store. Bingham was named after E W Bingham, leader of FIDS 1945 to 1947 and FIDS surgeon commander.\n\nNext door is the Chippy Shop, which was the original Rothera Base, being built in the 1976/1977 season. This building housed the base kitchen and eating facilities until the original Bransfield was built some four years later. As suggested by its name it is now the carpentry workshop, and also houses the electricians' store and workshop.\n\nThe generator shed houses four Volvo Penta generators and has its own stores and workshop facilities.\n\nThe Span and Boat Shed were built at approximately the same time using similar techniques (interlocking steel archways on a concrete base), and are located at either end of site. The Span is a storage facility for vehicles, equipment and waste. It is called the Span as it was manufactured by the Miracle-Span company which specialises in these buildings. The Boat Shed is used to store, maintain and operate the boats for travel to the local islands and diving (all Ribs of various size). It is situated next to the Biscoe Wharf (named after the RRS John Biscoe). There is a slip (only usable in summer) and a hydraulic crane to lift the boats in and out of the water.\n\nThe STP was built in summer 2002/2003. The sewage is treated using bacteria, which leave a peat like substance that is dry and compact enough to remove from Antarctica. The Hangar was built at the same time as the runway and is big enough for three Twin Otters and the Dash 7.\n\nThere is a refuge hut on Lagoon. This was built out of the materials left over from the original Bransfield House at Rothera. The hut has a stove, fuel, food and four bunks. There is also spare dry clothing, bedding and a pyramid tent. Boat teams who get caught out by bad weather or sea ice can use the refuge. It is often used during the summer months as a place for BAS staff to go and relax. There is an Apple hut on the Leonie Island, which was provided by the Dutch Antarctic Division during a joint working programme. There was also a Melon Hut on Lagoon for the same purpose, but this has now been moved to Anchorage Island.\n\nNunatak was the BAS’s house band. The five person indie rock band is part of a science team investigating climate change and evolutionary biology on the Antarctic Peninsula. They are chiefly known for their participation in Live Earth in 2007, where they were the only band to play in the event's Antarctica concert.\n\nBeing located just South of the Antarctic Circle, the weather is cold year round. Temperatures in summer barely go over freezing.\n\nRothera Station is the BAS logistics centre for the Antarctic and home to well-equipped biological laboratories and facilities for a wide range of research. The station is situated on a rock and raised beach promontory at the southern extremity of Wormald Ice Piedmont, south-eastern Adelaide Island. The station has a 900 m (2,950 ft) crushed rock runway, with an associated hangar and bulk fuel storage facility, and a wharf for the discharge of cargo from supply ships. There is a transitory summer population of scientists and support staff who reach Rothera either by ship or through use of a de Havilland Canada Dash 7 aircraft flying from the Falkland Islands.\n\nFrom its inception until the 1991/1992 summer season BAS Twin Otter aircraft used the skiway 300 m (about 1000 ft) above the station on Wormald Ice Piedmont. With the commissioning of the gravel runway and hangar in 1991/1992 air operations became more reliable and access to Rothera was greatly improved through a direct airlink from the Falkland Islands. The Twin Otters mainly fly south of Rothera, via a network of fuel depots, most of which are manned. Heading south of Rothera, the first stop would be Fossil Bluff, then Sky Blu. The Dash 7 will make approximately 20 flights a season to Stanley during the summer, bringing in scientists, support staff, food and equipment. When not tasked for these flights, the Dash can fly to Sky Blu in one hop, landing on the Blue Ice runway, significantly enhancing the range of the Twin Otters by depositing fuel and equipment in much larger quantities.\n\n\n"}
{"id": "27064189", "url": "https://en.wikipedia.org/wiki?curid=27064189", "title": "Shrouds of the Night", "text": "Shrouds of the Night\n\nShrouds of the Night is a book from the astronomy genre.\n\nThis books is an education on dark matter, the invisible matter which comprises about 70% of space. The book was written at the Mount Stromlo Observatory in 2007 by David Block and Ken Freeman. It can be seen as a timetable presenting astronomical photography from 1826 to the present day.\n\nThe reader is exposed to information about galaxies in which challenges and concepts facing the astronomical community today have been explored.\n\nMuch of the content of this book is published here for the first time. Two examples of this are: one from Arizona's Lowell Observatory and another from the Royal Astronomical Society of London.\n"}
{"id": "31056458", "url": "https://en.wikipedia.org/wiki?curid=31056458", "title": "Sociological Perspectives", "text": "Sociological Perspectives\n\nSociological Perspectives is the official publication of the Pacific Sociological Association. It is a peer-reviewed quarterly academic journal published by University of California Press, in Berkeley, California. It was first published in 1957. Articles typically address social processes and are related to economic, political, anthropological and historical issues.\n\n\"Sociological Perspectives\" is abstracted and indexed in the Social Sciences Citation Index. According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 1.013, ranking it 84th out of 146 journals in the category \"Sociology\".\n\n"}
{"id": "84237", "url": "https://en.wikipedia.org/wiki?curid=84237", "title": "Space Race", "text": "Space Race\n\nThe Space Race refers to the 20th-century competition between two Cold War rivals, the Soviet Union (USSR) and the United States (US), for dominance in spaceflight capability. It had its origins in the missile-based nuclear arms race between the two nations that occurred following World War II, aided by captured German missile technology and personnel from the Aggregat program. \nThe technological superiority required for such dominance was seen as necessary for national security, and symbolic of ideological superiority. The Space Race spawned pioneering efforts to launch artificial satellites, uncrewed space probes of the Moon, Venus, and Mars, and human spaceflight in low Earth orbit and to the Moon.\n\nThe Space Race began on August 2, 1955, when the Soviet Union responded to the US announcement four days earlier of intent to launch artificial satellites for the International Geophysical Year, by declaring they would also launch a satellite \"in the near future\". The Soviet Union beat the US to the first successful launch, with the October 4, 1957, orbiting of \"Sputnik 1\", and later beat the US to have the first human in earth orbit, Yuri Gagarin, on April 12, 1961. The \"race\" peaked with the July 20, 1969, US landing of the first humans on the Moon with Apollo 11. The USSR attempted several crewed lunar missions but eventually canceled them and concentrated on Earth orbital space stations.\n\nA period of détente followed with the April 1972 agreement on a co-operative Apollo–Soyuz Test Project, resulting in the July 1975 rendezvous in Earth orbit of a US astronaut crew with a Soviet cosmonaut crew. The end of the Space Race is harder to pinpoint than its beginning, but it was over by the December 1991 dissolution of the Soviet Union, after which spaceflight cooperation between the US and Russia flourished.\n\nThe Space Race has left a legacy of Earth communications and weather satellites, and continuing human space presence on the International Space Station. It has also sparked increases in spending on education and research and development, which led to beneficial spin-off technologies.\n\nThe origins of the Space Race can be traced to Germany, beginning in the 1930s and continuing during World War II when Nazi Germany researched and built operational ballistic missiles capable of sub-orbital spaceflight. Starting in the early 1930s, during the last stages of the Weimar Republic, German aerospace engineers experimented with liquid-fueled rockets, with the goal that one day they would be capable of reaching high altitudes and traversing long distances. The head of the German Army's Ballistics and Munitions Branch, Lieutenant Colonel Karl Emil Becker, gathered a small team of engineers that included Walter Dornberger and Leo Zanssen, to figure out how to use rockets as long-range artillery in order to get around the Treaty of Versailles' ban on research and development of long-range cannons. Wernher von Braun, a young engineering prodigy, was recruited by Becker and Dornberger to join their secret army program at Kummersdorf-West in 1932. Von Braun dreamed of conquering outer space with rockets and did not initially see the military value in missile technology.\n\nDuring the Second World War, General Dornberger was the military head of the army's rocket program, Zanssen became the commandant of the Peenemünde army rocket center, and von Braun was the technical director of the ballistic missile program. They led the team that built the Aggregate-4 (A-4) rocket, which became the first vehicle to reach outer space during its test flight program in 1942 and 1943. By 1943, Germany began mass-producing the A-4 as the Vergeltungswaffe 2 (\"Vengeance Weapon\" 2, or more commonly, V2), a ballistic missile with a range carrying a warhead at . Its supersonic speed meant there was no defense against it, and radar detection provided little warning. Germany used the weapon to bombard southern England and parts of Allied-liberated western Europe from 1944 until 1945. After the war, the V-2 became the basis of early American and Soviet rocket designs.\n\nAt war's end, American, British, and Soviet scientific intelligence teams competed to capture Germany's rocket engineers along with the German rockets themselves and the designs on which they were based. Each of the Allies captured a share of the available members of the German rocket team, but the United States benefited the most with Operation Paperclip, recruiting von Braun and most of his engineering team, who later helped develop the American missile and space exploration programs. The United States also acquired a large number of complete V2 rockets.\n\nThe German rocket center in Peenemünde was located in the eastern part of Germany, which became the Soviet zone of occupation. On Stalin's orders, the Soviet Union sent its best rocket engineers to this region to see what they could salvage for future weapons systems. The Soviet rocket engineers were led by Sergei Korolev. He had been involved in space clubs and early Soviet rocket design in the 1930s, but was arrested in 1938 during Joseph Stalin's Great Purge and imprisoned for six years in Gulag. After the war, he became the USSR's chief rocket and spacecraft engineer, essentially the Soviet counterpart to von Braun. His identity was kept a state secret throughout the Cold War, and he was identified publicly only as \"the Chief Designer.\" In the West, his name was only officially revealed when he died in 1966.\n\nAfter almost a year in the area around Peenemünde, Soviet officials conducted Operation Osoaviakhim and later moved more than 170 of the top captured German rocket specialists to Gorodomlya Island on Lake Seliger, about northwest of Moscow. They were not allowed to participate in final Soviet missile design, but were used as problem-solving consultants to the Soviet engineers. They helped in the following areas: the creation of a Soviet version of the A-4; work on \"organizational schemes\"; research in improving the A-4 main engine; development of a 100-ton engine; assistance in the \"layout\" of plant production rooms; and preparation of rocket assembly using German components. With their help, particularly Helmut Gröttrup's group, Korolev reverse-engineered the A-4 and built his own version of the rocket, the R-1, in 1948. Later, he developed his own distinct designs, though many of these designs were influenced by the Gröttrup Group's G4-R10 design from 1949. The Germans were eventually repatriated in 1951–53.\n\nThe American professor Robert H. Goddard had worked on developing solid-fuel rockets since 1914, and demonstrated a light battlefield rocket to the US Army Signal Corps only five days before the signing of the armistice that ended World War I. He also started developing liquid-fueled rockets in 1921, yet he had not been taken seriously by the public.\n\nVon Braun and his team were sent to the United States Army's White Sands Proving Ground, located in New Mexico, in 1945. They set about assembling the captured V2s and began a program of launching them and instructing American engineers in their operation. These tests led to the first rocket to take photos from outer space, and the first two-stage rocket, the WAC Corporal-V2 combination, in 1949. The German rocket team was moved from Fort Bliss to the Army's new Redstone Arsenal, located in Huntsville, Alabama, in 1950. From here, von Braun and his team developed the Army's first operational medium-range ballistic missile, the Redstone rocket, that in slightly modified versions, launched both America's first satellite, and the first piloted Mercury space missions. It became the basis for both the Jupiter and Saturn family of rockets.\n\nThe Cold War (1947–1991) developed between two former allies, the Soviet Union and the United States, soon after the end of the Second World War. It involved a continuing state of political conflict, military tension, proxy wars, and economic competition, primarily between the Soviet Union and its satellite states (often referred to as the Eastern Bloc) and the powers of the Western world, particularly the United States. The primary participants' military forces never clashed directly, but expressed this conflict through military coalitions, strategic conventional force deployments, extensive aid to states deemed vulnerable, proxy wars, espionage, propaganda, a nuclear arms race, and economic and technological competitions, such as the Space Race.\n\nIn simple terms, the Cold War could be viewed as an expression of the ideological struggle between communism and capitalism. The United States faced a new uncertainty beginning in September 1949, when it lost its monopoly on the atomic bomb. American intelligence agencies discovered that the Soviet Union had exploded its first atomic bomb, with the consequence that the United States potentially could face a future nuclear war that, for the first time, might devastate its cities. Given this new danger, the United States participated in an arms race with the Soviet Union that included development of the hydrogen bomb, as well as intercontinental strategic bombers and intercontinental ballistic missiles (ICBMs) capable of delivering nuclear weapons. A new fear of communism and its sympathizers swept the United States during the 1950s, which devolved into paranoid McCarthyism. With communism spreading in China, Korea, and Eastern Europe, Americans came to feel so threatened that popular and political culture condoned extensive \"witch-hunts\" to expose communist spies. Part of the American reaction to the Soviet atomic and hydrogen bomb tests included maintaining a large Air Force, under the control of the Strategic Air Command (SAC). SAC employed intercontinental strategic bombers, as well as medium-bombers based close to Soviet airspace (in western Europe and in Turkey) that were capable of delivering nuclear payloads.\n\nFor its part, the Soviet Union harbored fears of invasion. Having suffered at least 27 million casualties during World War II after being invaded by Nazi Germany in 1941, the Soviet Union was wary of its former ally, the United States, which until late 1949 was the sole possessor of atomic weapons. The United States had used these weapons operationally during World War II, and it could use them again against the Soviet Union, laying waste to its cities and military centers. Since the Americans had a much larger air force than the Soviet Union, and the United States maintained advance air bases near Soviet territory, in 1947 Stalin ordered the development of intercontinental ballistic missiles (ICBMs) in order to counter the perceived American threat.\n\nIn 1953, Korolev was given the go-ahead to develop the R-7 Semyorka rocket, which represented a major advance from the German design. Although some of its components (notably boosters) still resembled the German G-4, the new rocket incorporated staged design, a completely new control system, and a new fuel. It was successfully tested on August 21, 1957, and became the world's first fully operational ICBM the following month. It was later used to launch the first satellite into space, and derivatives launched all piloted Soviet spacecraft.\n\nThe United States had multiple rocket programs divided among the different branches of the American armed services, which meant that each force developed its own ICBM program. The Air Force initiated ICBM research in 1945 with the MX-774. However, its funding was cancelled and only three partially successful launches were conducted in 1947. In 1950, von Braun began testing the Air Force PGM-11 Redstone rocket family at Cape Canaveral. In 1951, the Air Force began a new ICBM program called MX-1593, and by 1955 this program was receiving top-priority funding. The MX-1593 program evolved to become the Atlas-A, with its maiden launch occurring June 11, 1957, becoming the first successful American ICBM. Its upgraded version, the Atlas-D rocket, later served as a nuclear ICBM and as the orbital launch vehicle for Project Mercury and the remote-controlled Agena Target Vehicle used in Project Gemini.\n\nWith the Cold War as an engine for change in the ideological competition between the United States and the Soviet Union, a coherent space policy began to take shape in the United States during the late 1950s. Korolev took inspiration from the competition as well, achieving many firsts to counter the possibility that the United States might prevail.\n\nIn 1955, with both the United States and the Soviet Union building ballistic missiles that could be utilized to launch objects into space, the \"starting line\" was drawn for the Space Race. In separate announcements four days apart, both nations publicly announced that they would launch artificial Earth satellites by 1957 or 1958. On July 29, 1955, James C. Hagerty, president Dwight D. Eisenhower's press secretary, announced that the United States intended to launch \"small Earth circling satellites\" between July 1, 1957, and December 31, 1958, as part of their contribution to the International Geophysical Year (IGY). Four days later, at the Sixth Congress of International Astronautical Federation in Copenhagen, scientist Leonid I. Sedov spoke to international reporters at the Soviet embassy, and announced his country's intention to launch a satellite as well, in the \"near future\". On August 30, 1955, Korolev managed to get the Soviet Academy of Sciences to create a commission whose purpose was to beat the Americans into Earth orbit: this was the \"de facto\" start date for the Space Race. The Council of Ministers of the Soviet Union began a policy of treating development of its space program as a classified state secret.\n\nInitially, President Eisenhower was worried that a satellite passing above a nation at over , might be construed as violating that nation's sovereign airspace. He was concerned that the Soviet Union would accuse the Americans of an illegal overflight, thereby scoring a propaganda victory at his expense. Eisenhower and his advisors believed that a nation's airspace sovereignty did not extend into outer space, acknowledged as the Kármán line, and he used the 1957–58 International Geophysical Year launches to establish this principle in international law. Eisenhower also feared that he might cause an international incident and be called a \"warmonger\" if he were to use military missiles as launchers. Therefore, he selected the untried Naval Research Laboratory's Vanguard rocket, which was a research-only booster. This meant that von Braun's team was not allowed to put a satellite into orbit with their Jupiter-C rocket, because of its intended use as a future military vehicle. On September 20, 1956, von Braun and his team did launch a Jupiter-C that was capable of putting a satellite into orbit, but the launch was used only as a suborbital test of nose cone reentry technology.\n\nKorolev received word about von Braun's 1956 Jupiter-C test, but thinking it was a satellite mission that failed, he expedited plans to get his own satellite in orbit. Since his R-7 was substantially more powerful than any of the American boosters, he made sure to take full advantage of this capability by designing Object D as his primary satellite. It was given the designation 'D', to distinguish it from other R-7 payload designations 'A', 'B', 'V', and 'G' which were nuclear weapon payloads. Object D dwarfed the proposed American satellites, by having a weight of , of which would be composed of scientific instruments that would photograph the Earth, take readings on radiation levels, and check on the planet's magnetic field. However, things were not going along well with the design and manufacturing of the satellite, so in February 1957, Korolev sought and received permission from the Council of Ministers to create a \"prosteishy sputnik\" (PS-1), or simple satellite. The Council also decreed that Object D be postponed until April 1958. The new \"sputnik\" was a shiny sphere that would be a much lighter craft, weighing and having a diameter. The satellite would not contain the complex instrumentation that Object D had, but had two radio transmitters operating on different short wave radio frequencies, the ability to detect if a meteoroid were to penetrate its pressure hull, and the ability to detect the density of the Earth's thermosphere.\nKorolev was buoyed by the first successful launches of his R-7 rocket in August and September, which paved the way for him to launch his \"sputnik\". Word came that the Americans were planning to announce a major breakthrough at an International Geophysical Year conference at the National Academy of Sciences in Washington D.C., with a paper entitled \"Satellite Over the Planet\", on October 6, 1957. Korolev anticipated that von Braun might launch a Jupiter-C with a satellite payload on or around October 4 or 5, in conjunction with the paper. He hastened the launch, moving it to October 4. The launch vehicle for PS-1, was a modified R-7 – vehicle 8K71PS number M1-PS– without much of the test equipment and radio gear that was present in the previous launches. It arrived at the Soviet missile base Tyura-Tam in September and was prepared for its mission at launch site number one. On Friday, October 4, 1957, at exactly 10:28:34 pm Moscow time, the R-7, with the now named \"Sputnik 1\" satellite, lifted off the launch pad, and placed this artificial \"moon\" into an orbit a few minutes later. This \"fellow traveler,\" as the name is translated in English, was a small, beeping ball, less than two feet in diameter and weighing less than 200 pounds. But the celebrations were muted at the launch control center until the down-range far east tracking station at Kamchatka received the first distinctive beep ... beep ... beep sounds from \"Sputnik 1\"s radio transmitters, indicating that it was on its way to completing its first orbit. About 95 minutes after launch, the satellite flew over its launch site, and its radio signals were picked up by the engineers and military personnel at Tyura-Tam: that's when Korolev and his team celebrated the first successful artificial satellite placed into Earth-orbit.\n\nThe Soviet success raised a great deal of concern in the United States. For example, economist Bernard Baruch wrote in an open letter titled \"The Lessons of Defeat\" to the \"New York Herald Tribune\": \"While we devote our industrial and technological power to producing new model automobiles and more gadgets, the Soviet Union is conquering space. ... It is Russia, not the United States, who has had the imagination to hitch its wagon to the stars and the skill to reach for the moon and all but grasp it. America is worried. It should be.\"\n\nEisenhower ordered project Vanguard to move up its timetable and launch its satellite much sooner than originally planned. The December 6, 1957 Project Vanguard launch failure occurred at Cape Canaveral Air Force Station in Florida, broadcast live in front of a US television audience. It was a monumental failure, exploding a few seconds after launch, and it became an international joke. The satellite appeared in newspapers under the names Flopnik, Stayputnik, Kaputnik, and Dudnik. In the United Nations, the Soviet delegate offered the US representative aid \"under the Soviet program of technical assistance to backwards nations.\" Only in the wake of this very public failure did von Braun's Redstone team get the go-ahead to launch their Jupiter-C rocket as soon as they could. In Britain, the US's Western Cold War ally, the reaction was mixed: some celebrated the fact that the Soviets had reached space first, while others feared the destructive potential that military uses of spacecraft might bring.\n\nOn January 31, 1958, nearly four months after the launch of \"Sputnik 1\", von Braun and the United States successfully launched its first satellite on a four-stage Juno I rocket derived from the US Army's Redstone missile, at Cape Canaveral. The satellite \"Explorer 1\" was in mass. The payload of Explorer 1 weighed . It carried a micrometeorite gauge and a Geiger-Müller tube. It passed in and out of the Earth-encompassing radiation belt with its orbit, therefore saturating the tube's capacity and proving what Dr. James Van Allen, a space scientist at the University of Iowa, had theorized. The belt, named the Van Allen radiation belt, is a doughnut-shaped zone of high-level radiation intensity around the Earth above the magnetic equator. Van Allen was also the man who designed and built the satellite instrumentation of \"Explorer 1\". The satellite measured three phenomena: cosmic ray and radiation levels, the temperature in the spacecraft, and the frequency of collisions with micrometeorites. The satellite had no memory for data storage, therefore it had to transmit continuously. In March 1958 a second satellite was sent into orbit with augmented cosmic ray instruments.\n\nOn April 2, 1958, President Eisenhower reacted to the Soviet space lead in launching the first satellite by recommending to the US Congress that a civilian agency be established to direct nonmilitary space activities. Congress, led by Senate Majority Leader Lyndon B. Johnson, responded by passing the National Aeronautics and Space Act, which Eisenhower signed into law on July 29, 1958. This law turned the National Advisory Committee on Aeronautics into the National Aeronautics and Space Administration (NASA). It also created a Civilian-Military Liaison Committee, chaired by the President, responsible for coordinating the nation's civilian and military space programs.\n\nOn October 21, 1959, Eisenhower approved the transfer of the Army's remaining space-related activities to NASA. On July 1, 1960, the Redstone Arsenal became NASA's George C. Marshall Space Flight Center, with von Braun as its first director. Development of the Saturn rocket family, which when mature gave the US parity with the Soviets in terms of lifting capability, was thus transferred to NASA.\n\nIn 1958, Korolev upgraded the R-7 to be able to launch a payload to the Moon. Three secret 1958 attempts to launch Luna E-1-class impactor probes failed. The fourth attempt, Luna 1, launched successfully on January 2, 1959, but missed the Moon. The fifth attempt on June 18 also failed at launch. The Luna 2 successfully impacted the Moon on September 14, 1959. The Luna 3 successfully flew by the Moon and sent back pictures of its far side on October 6, 1959.\n\nThe US reacted to the Luna program by embarking on the Ranger program in 1959, managed by NASA's Jet Propulsion Laboratory. The Block I Ranger 1 and Ranger 2 suffered Atlas-Agena launch failures in August and November 1961.\nThe Block II Ranger 3 launched successfully on January 26, 1962, but missed the Moon. The Ranger 4 became the first US spacecraft to reach the Moon, but its solar panels and navigational system failed near the Moon and it impacted the far side without returning any scientific data. Ranger 5 ran out of power and missed the Moon by on October 21, 1962. The first successful Ranger mission was the Block III Ranger 7 which impacted on July 31, 1964.\n\nBy 1959, American observers believed that the Soviet Union would be the first to get a human into space, because of the time needed to prepare for Mercury's first launch. On April 12, 1961, the USSR surprised the world again by launching Yuri Gagarin into a single orbit around the Earth in a craft they called Vostok 1. They dubbed Gagarin the first cosmonaut, roughly translated from Russian and Greek as \"sailor of the universe\". Although he had the ability to take over manual control of his capsule in an emergency by opening an envelope he had in the cabin that contained a code that could be typed into the computer, it was flown in an automatic mode as a precaution; medical science at that time did not know what would happen to a human in the weightlessness of space. Vostok 1 orbited the Earth for 108 minutes and made its reentry over the Soviet Union, with Gagarin ejecting from the spacecraft at , and landing by parachute. The Fédération Aéronautique Internationale (International Federation of Aeronautics) credited Gagarin with the world's first human space flight, although their qualifying rules for aeronautical records at the time required pilots to take off and land with their craft. For this reason, the Soviet Union omitted from their FAI submission the fact that Gagarin did not land with his capsule. When the FAI filing for Gherman Titov's second Vostok flight in August 1961 disclosed the ejection landing technique, the FAI committee decided to investigate, and concluded that the technological accomplishment of human spaceflight lay in the safe launch, orbiting, and return, rather than the manner of landing, and revised their rules, keeping Gagarin's and Titov's records intact.\n\nGagarin became a national hero of the Soviet Union and the Eastern Bloc, and a worldwide celebrity. Moscow and other cities in the USSR held mass demonstrations, the scale of which was second only to the World War II Victory Parade of 1945. April 12 was declared Cosmonautics Day in the USSR, and is celebrated today in Russia as one of the official \"Commemorative Dates of Russia.\" In 2011, it was declared the International Day of Human Space Flight by the United Nations.\nThe radio communication between the launch control room and Gagarin included the following dialogue at the moment of rocket launch:\n\nGagarin's informal \"poyekhali!\" became a historical phrase in the Eastern Bloc, used to refer to the beginning of the human space flight era.\n\nThe US Air Force had been developing a program to launch the first man in space, named Man in Space Soonest. This program studied several different types of one-man space vehicles, settling on a ballistic re-entry capsule launched on a derivative Atlas missile, and selecting a group of nine candidate pilots. After NASA's creation, the program was transferred over to the civilian agency and renamed Project Mercury on November 26, 1958. NASA selected a new group of astronaut (from the Greek for \"star sailor\") candidates from Navy, Air Force and Marine test pilots, and narrowed this down to a group of seven for the program. Capsule design and astronaut training began immediately, working toward preliminary suborbital flights on the Redstone missile, followed by orbital flights on the Atlas. Each flight series would first start uncrewed, then carry a non-human primate, then finally humans.\n\nOn May 5, 1961, Alan Shepard became the first American in space, launched in a ballistic trajectory on Mercury-Redstone 3, in a spacecraft he named \"Freedom 7\". Though he did not achieve orbit like Gagarin, he was the first person to exercise manual control over his spacecraft's attitude and retro-rocket firing. After his successful return, Shepard was celebrated as a national hero, honored with parades in Washington, New York and Los Angeles, and received the NASA Distinguished Service Medal from President John F. Kennedy.\n\nBefore Gagarin's flight, US President John F. Kennedy's support for America's crewed space program was lukewarm. Jerome Wiesner of MIT, who served as a science advisor to presidents Eisenhower and Kennedy, and himself an opponent of crewed space exploration, remarked, \"If Kennedy could have opted out of a big space program without hurting the country in his judgment, he would have.\" As late as March 1961, when NASA administrator James E. Webb submitted a budget request to fund a Moon landing before 1970, Kennedy rejected it because it was simply too expensive. Some were surprised by Kennedy's eventual support of NASA and the space program because of how often he had attacked the Eisenhower administration's inefficiency during the election.\n\nGagarin's flight changed this; now Kennedy sensed the humiliation and fear on the part of the American public over the Soviet lead. Additionally, the Bay of Pigs invasion, planned before his term began but executed during it, was an embarrassment to his administration due to the colossal failure of the American forces. Looking for something to save political face, he sent a memo dated April 20, 1961, to Vice President Lyndon B. Johnson, asking him to look into the state of America's space program, and into programs that could offer NASA the opportunity to catch up. The two major options at the time seemed to be, either establishment of an Earth orbital space station, or a crewed landing on the Moon. Johnson, in turn, consulted with von Braun, who answered Kennedy's questions based on his estimates of US and Soviet rocket lifting capability. Based on this, Johnson responded to Kennedy, concluding that much more was needed to reach a position of leadership, and recommending that the crewed Moon landing was far enough in the future that the US had a fighting chance to achieve it first.\n\nKennedy ultimately decided to pursue what became the Apollo program, and on May 25 took the opportunity to ask for Congressional support in a Cold War speech titled \"Special Message on Urgent National Needs\". \nHe justified the program in terms of its importance to national security, and its focus of the nation's energies on other scientific and social fields. He rallied popular support for the program in his \"We choose to go to the Moon\" speech, on September 12, 1962, before a large crowd at Rice University Stadium, in Houston, Texas, near the construction site of the new Manned Spacecraft Center facility. \nKhrushchev responded to Kennedy's implicit challenge with silence, refusing to publicly confirm or deny the Soviets were pursuing a \"Moon race\". As later disclosed, the Soviet Union secretly pursued a crewed lunar program until 1974.\n\nAmerican Virgil \"Gus\" Grissom repeated Shepard's suborbital flight in \"Liberty Bell 7\" on July 21, 1961. Almost a year after the Soviet Union put a human into orbit, astronaut John Glenn became the first American to orbit the Earth, on February 20, 1962. His Mercury-Atlas 6 mission completed three orbits in the \"Friendship 7\" spacecraft, and splashed down safely in the Atlantic Ocean, after a tense reentry, due to what falsely appeared from the telemetry data to be a loose heat-shield. As the first American in orbit, Glenn became a national hero, and received a ticker-tape parade in New York City, reminiscent of that given for Charles Lindbergh. On February 23, 1962, President Kennedy escorted him in a parade at Cape Canaveral Air Force Station, where he awarded Glenn with the NASA service medal.\n\nThe United States launched three more Mercury flights after Glenn's: \"Aurora 7\" on May 24, 1962, duplicated Glenn's three orbits; \"Sigma 7\" on October 3, 1962, six orbits; and \"Faith 7\" on May 15, 1963, 22 orbits (32.4 hours), the maximum capability of the spacecraft. NASA at first intended to launch one more mission, extending the spacecraft's endurance to three days, but since this would not beat the Soviet record, it was decided instead to concentrate on developing Project Gemini.\n\nGherman Titov became the first Soviet cosmonaut to exercise manual control of his Vostok 2 craft on August 6, 1961. The Soviet Union demonstrated 24-hour launch pad turnaround and the capability to launch two piloted spacecraft, Vostok 3 and Vostok 4, in essentially identical orbits, on August 11 and 12, 1962. The two spacecraft came within approximately of one another, close enough for radio communication. Vostok 4 also set a record of nearly four days in space. Though the two craft's orbits were as nearly identical as possible given the accuracy of the launch rocket's guidance system, slight variations still existed which drew the two craft at first as close to each other as , then as far apart as . There were no maneuvering rockets on the Vostok to permit space rendezvous, required to keep two spacecraft a controlled distance apart.\n\nThe Soviet Union duplicated its dual-launch feat with Vostok 5 and Vostok 6 (June 16, 1963). This time they launched the first woman (also the first civilian), Valentina Tereshkova, into space on Vostok 6. Launching a woman was reportedly Korolev's idea, and it was accomplished purely for propaganda value. Tereshkova was one of a small corps of female cosmonauts who were amateur parachutists, but Tereshkova was the only one to fly. The USSR didn't again open its cosmonaut corps to women until 1980, two years after the United States opened its astronaut corps to women.\n\nThe Soviets kept the details and true appearance of the Vostok capsule secret until the April 1965 Moscow Economic Exhibition, where it was first displayed without its aerodynamic nose cone concealing the spherical capsule. The \"Vostok spaceship\" had been first displayed at the July 1961 Tushino air show, mounted on its launch vehicle's third stage, with the nose cone in place. A tail section with eight fins was also added, in an apparent attempt to confuse western observers. This spurious tail section also appeared on official commemorative stamps and a documentary.\n\nOn September 20, 1963, in a speech before the United Nations General Assembly, President Kennedy proposed that the United States and the Soviet Union join forces in an effort to reach the Moon. Kennedy thus changed his mind regarding the desirability of the space race, preferring instead to ease tensions with the Soviet Union by cooperating on projects such as a joint lunar landing. Soviet Premier Nikita Khrushchev initially rejected Kennedy's proposal. However, on October 2, 1997, it was reported that Khrushchev's son Sergei claimed Khrushchev was poised to accept Kennedy's proposal at the time of Kennedy's assassination on November 22, 1963. During the next few weeks he reportedly concluded that both nations might realize cost benefits and technological gains from a joint venture, and decided to accept Kennedy's offer based on a measure of rapport during their years as leaders of the world's two superpowers, but changed his mind and dropped the idea since he did not have the same trust for Kennedy's successor, Lyndon Johnson.\n\nAs President, Johnson steadfastly pursued the Gemini and Apollo programs, promoting them as Kennedy's legacy to the American public. One week after Kennedy's death, he issued an executive order renaming the Cape Canaveral and Apollo launch facilities after Kennedy.\n\nFocused by the commitment to a Moon landing, in January 1962 the US announced Project Gemini, a two-man spacecraft that would support the later three-man Apollo by developing the key spaceflight technologies of space rendezvous and docking of two craft, flight durations of sufficient length to simulate going to the Moon and back, and extra-vehicular activity to accomplish useful work outside the spacecraft.\n\nMeanwhile, Korolev had planned further, long-term missions for the Vostok spacecraft, and had four Vostoks in various stages of fabrication in late 1963 at his OKB-1 facilities. At that time, the Americans announced their ambitious plans for the Project Gemini flight schedule. These plans included major advancements in spacecraft capabilities, including a two-person spacecraft, the ability to change orbits, the capacity to perform an extravehicular activity (EVA), and the goal of docking with another spacecraft. These represented major advances over the previous Mercury or Vostok capsules, and Korolev felt the need to try to beat the Americans to many of these innovations. Korolev already had begun designing the Vostok's replacement, the next-generation Soyuz spacecraft, a multi-cosmonaut spacecraft that had at least the same capabilities as the Gemini spacecraft. Soyuz would not be available for at least three years, and it could not be called upon to deal with this new American challenge in 1964 or 1965. Political pressure in early 1964–which some sources claim was from Khrushchev while other sources claim was from other Communist Party officials—pushed him to modify his four remaining Vostoks to beat the Americans to new space firsts in the size of flight crews, and the duration of missions.\n\nThe greater advances of the Soviet space program at the time allowed their space program to achieve other significant firsts, including the first EVA \"spacewalk\" and the first mission performed by a crew in shirt-sleeves. Gemini took a year longer than planned to accomplish its first flight, allowing the Soviets to achieve another first, launching Voskhod 1 on October 12, 1964, the first spacecraft with a three-cosmonaut crew. The USSR touted another technological achievement during this mission: it was the first space flight during which cosmonauts performed in a shirt-sleeve-environment. However, flying without spacesuits was not due to safety improvements in the Soviet spacecraft's environmental systems; rather this innovation was accomplished because the craft's limited cabin space did not allow for spacesuits. Flying without spacesuits exposed the cosmonauts to significant risk in the event of potentially fatal cabin depressurization. This feat was not repeated until the US Apollo Command Module flew in 1968; this later mission was designed from the outset to safely transport three astronauts in a shirt-sleeve environment while in space.\n\nBetween October 14–16, 1964, Leonid Brezhnev and a small cadre of high-ranking Communist Party officials deposed Khrushchev as Soviet government leader a day after Voskhod 1 landed, in what was called the \"Wednesday conspiracy\".\nThe new political leaders, along with Korolev, ended the technologically troublesome Voskhod program, cancelling Voskhod 3 and 4, which were in the planning stages, and started concentrating on the race to the Moon. Voskhod 2 ended up being Korolev's final achievement before his death on January 14, 1966, as it became the last of the many space firsts that demonstrated the USSR's domination in spacecraft technology during the early 1960s. According to historian Asif Siddiqi, Korolev's accomplishments marked \"the absolute zenith of the Soviet space program, one never, ever attained since.\" There was a two-year pause in Soviet piloted space flights while Voskhod's replacement, the Soyuz spacecraft, was designed and developed.\n\nOn March 18, 1965, about a week before the first American piloted Project Gemini space flight, the USSR accelerated the competition, by launching the two-cosmonaut Voskhod 2 mission with Pavel Belyayev and Alexey Leonov. Voskhod 2's design modifications included the addition of an inflatable airlock to allow for extravehicular activity (EVA), also known as a spacewalk, while keeping the cabin pressurized so that the capsule's electronics would not overheat. Leonov performed the first-ever EVA as part of the mission. A fatality was narrowly avoided when Leonov's spacesuit expanded in the vacuum of space, preventing him from re-entering the airlock. In order to overcome this, he had to partially depressurize his spacesuit to a potentially dangerous level. He succeeded in safely re-entering the ship, but he and Belyayev faced further challenges when the spacecraft's atmospheric controls flooded the cabin with 45% pure oxygen, which had to be lowered to acceptable levels before re-entry. The reentry involved two more challenges: an improperly timed retrorocket firing caused the Voskhod 2 to land off its designated target area, the town of Perm; and the instrument compartment's failure to detach from the descent apparatus caused the spacecraft to become unstable during reentry.\n\nThough delayed a year to reach its first flight, Gemini was able to take advantage of the USSR's two-year hiatus after Voskhod, which enabled the US to catch up and surpass the previous Soviet lead in piloted spaceflight. Gemini achieved several significant firsts during the course of ten piloted missions:\n\nMost of the novice pilots on the early missions would command the later missions. In this way, Project Gemini built up spaceflight experience for the pool of astronauts for the Apollo lunar missions.\n\nKorolev's design bureau produced two prospectuses for circumlunar spaceflight (March 1962 and May 1963), the main spacecraft for which were early versions of his Soyuz design. Soviet Communist Party Central Committee Command 655-268 officially established two secret, competing crewed programs for circumlunar flights and lunar landings, on August 3, 1964. The circumlunar flights were planned to occur in 1967, and the landings to start in 1968.\nThe circumlunar program (Zond), created by Vladimir Chelomey's design bureau OKB-52, was to fly two cosmonauts in a stripped-down Soyuz 7K-L1, launched by Chelomey's Proton UR-500 rocket. The Zond sacrificed habitable cabin volume for equipment, by omitting the Soyuz orbital module. Chelomey gained favor with Khruschev by employing members of his family.\n\nKorolev's lunar landing program was designated N1/L3, for its N1 super rocket and a more advanced Soyuz 7K-L3 spacecraft, also known as the lunar orbital module (\"\"Lunniy Orbitalny Korabl\", LOK), with a crew of two. A separate lunar lander (\"Lunniy Korabl\"\", LK), would carry a single cosmonaut to the lunar surface.\n\nThe N1/L3 launch vehicle had three stages to Earth orbit, a fourth stage for Earth departure, and a fifth stage for lunar landing assist. The combined space vehicle was roughly the same height and takeoff mass as the three-stage US Apollo/ Saturn V and exceeded its takeoff thrust by 28%, but had only roughly half the translunar injection payload capability.\n\nFollowing Khruschev's ouster from power, Chelomey's Zond program was merged into the N1/L3 program.\n\nThe US and USSR began discussions on the peaceful uses of space as early as 1958, presenting issues for debate to the United Nations, which created a Committee on the Peaceful Uses of Outer Space in 1959.\n\nOn May 10, 1962, Vice President Johnson addressed the Second National Conference on the Peaceful Uses of Space revealing that the United States and the USSR both supported a resolution passed by the Political Committee of the UN General Assembly on December 1962, which not only urged member nations to \"extend the rules of international law to outer space,\" but to also cooperate in its exploration. Following the passing of this resolution, Kennedy commenced his communications proposing a cooperative American/Soviet space program.\n\nThe UN ultimately created a \"Treaty on Principles Governing the Activities of States in the Exploration and Use of Outer Space, including the Moon and Other Celestial Bodies\", which was signed by the United States, USSR, and the United Kingdom on January 27, 1967, and went into force the following October 10.\nThis treaty:\n\nThe treaty remains in force, signed by 107 member states. – \n\nIn 1967, both nations faced serious challenges that brought their programs to temporary halts. Both had been rushing at full-speed toward the first piloted flights of Apollo and Soyuz, without paying due diligence to growing design and manufacturing problems. The results proved fatal to both pioneering crews.\nOn January 27, 1967, the same day the US and USSR signed the Outer Space Treaty, the crew of the first crewed Apollo mission, Command Pilot Virgil \"Gus\" Grissom, Senior Pilot Ed White, and Pilot Roger Chaffee, were killed in a fire that swept through their spacecraft cabin during a ground test, less than a month before the planned February 21 launch. An investigative board determined the fire was probably caused by an electrical spark, and quickly grew out of control, fed by the spacecraft's pure oxygen atmosphere. Crew escape was made impossible by inability to open the plug door hatch cover against the greater-than-atmospheric internal pressure. The board also found design and construction flaws in the spacecraft, and procedural failings, including failure to appreciate the hazard of the pure-oxygen atmosphere, as well as inadequate safety procedures. All these flaws had to be corrected over the next twenty-two months until the first piloted flight could be made.\nMercury and Gemini veteran Grissom had been a favored choice of Deke Slayton, NASA's Director of Flight Crew Operations, to make the first piloted landing.\nOn April 24, 1967, the single pilot of Soyuz 1, Vladimir Komarov, became the first in-flight spaceflight fatality. The mission was planned to be a three-day test, to include the first Soviet docking with an unpiloted Soyuz 2, but the mission was plagued with problems. Early on, Komarov's craft lacked sufficient electrical power because only one of two solar panels had deployed. Then the automatic attitude control system began malfunctioning and eventually failed completely, resulting in the craft spinning wildly. Komarov was able to stop the spin with the manual system, which was only partially effective. The flight controllers aborted his mission after only one day. During the emergency re-entry, a fault in the landing parachute system caused the primary chute to fail, and the reserve chute became tangled with the drogue chute, causing descent speed to reach as high as 40 m/s (140 km/h; 89 mph). Shortly thereafter, Soyuz 1 impacted the ground 3 km (1.9 mi) west of Karabutak, exploding into a ball of flames. The official autopsy states Komarov died of blunt force trauma on impact, and that the subsequent heat mutilation of his corpse was a result of the explosive impact. Fixing the spacecraft's faults caused an eighteen-month delay before piloted Soyuz flights could resume.\n\nThe United States recovered from the Apollo 1 fire, fixing the fatal flaws in an improved version of the Block II command module. The US proceeded with unpiloted test launches of the Saturn V launch vehicle (Apollo 4 and Apollo 6) and the Lunar Module (Apollo 5) during the latter half of 1967 and early 1968. Apollo 1's mission to check out the Apollo Command/Service Module in Earth orbit was accomplished by Grissom's backup crew commanded by Walter Schirra on Apollo 7, launched on October 11, 1968. The eleven-day mission was a total success, as the spacecraft performed a virtually flawless mission, paving the way for the United States to continue with its lunar mission schedule.\n\nThe Soviet Union also fixed the parachute and control problems with Soyuz, and the next piloted mission Soyuz 3 was launched on October 26, 1968. The goal was to complete Komarov's rendezvous and docking mission with the un-piloted Soyuz 2. Ground controllers brought the two craft to within of each other, then cosmonaut Georgy Beregovoy took control. He got within of his target, but was unable to dock before expending 90 percent of his maneuvering fuel, due to a piloting error that put his spacecraft into the wrong orientation and forced Soyuz 2 to automatically turn away from his approaching craft. The first docking of Soviet spacecraft was finally realized in January 1969 by the Soyuz 4 and Soyuz 5 missions. It was the first-ever docking of two crewed spacecraft, and the first transfer of crew from one space vehicle to another.\n\nThe Soviet Zond spacecraft was not yet ready for piloted circumlunar missions in 1968, after five unsuccessful and partially successful automated test launches: Cosmos 146 on March 10, 1967; Cosmos 154 on April 8, 1967; Zond 1967A September 27, 1967; Zond 1967B on November 22, 1967. Zond 4 was launched on March 2, 1968, and successfully made a circumlunar flight. After its successful flight around the Moon, Zond 4 encountered problems with its Earth reentry on March 9, and was ordered destroyed by an explosive charge over the Gulf of Guinea. The Soviet official announcement said that Zond 4 was an automated test flight which ended with its intentional destruction, due to its recovery trajectory positioning it over the Atlantic Ocean instead of over the USSR.\n\nDuring the summer of 1968, the Apollo program hit another snag: the first pilot-rated Lunar Module (LM) was not ready for orbital tests in time for a December 1968 launch. NASA planners overcame this challenge by changing the mission flight order, delaying the first LM flight until March 1969, and sending Apollo 8 into lunar orbit without the LM in December. This mission was in part motivated by intelligence rumors the Soviet Union might be ready for a piloted Zond flight during late 1968. In September 1968, Zond 5 made a circumlunar flight with tortoises on board and returned to Earth, accomplishing the first successful water landing of the Soviet space program in the Indian Ocean. It also scared NASA planners, as it took them several days to figure out that it was only an automated flight, not piloted, because voice recordings were transmitted from the craft en route to the Moon. On November 10, 1968, another automated test flight, Zond 6, was launched. It encountered difficulties in Earth reentry, and depressurized and deployed its parachute too early, causing it to crash-land only from where it had been launched six days earlier. It turned out there was no chance of a piloted Soviet circumlunar flight during 1968, due to the unreliability of the Zonds.\n\nOn December 21, 1968, Frank Borman, James Lovell, and William Anders became the first humans to ride the Saturn V rocket into space, on Apollo 8. They also became the first to leave low-Earth orbit and go to another celestial body, entering lunar orbit on December 24. They made ten orbits in twenty hours, and transmitted one of the most watched TV broadcasts in history, with their Christmas Eve program from lunar orbit, which concluded with a reading from the biblical Book of Genesis. Two and a half hours after the broadcast, they fired their engine to perform the first trans-Earth injection to leave lunar orbit and return to the Earth. Apollo 8 safely landed in the Pacific Ocean on December 27, in NASA's first dawn splashdown and recovery.\n\nThe American Lunar Module was finally ready for a successful piloted test flight in low Earth orbit on Apollo 9 in March 1969. The next mission, Apollo 10, conducted a \"dress rehearsal\" for the first landing in May 1969, flying the LM in lunar orbit as close as above the surface, the point where the powered descent to the surface would begin. With the LM proven to work well, the next step was to attempt the landing.\n\nUnknown to the Americans, the Soviet Moon program was in deep trouble. After two successive launch failures of the N1 rocket in 1969, Soviet plans for a piloted landing suffered delay. The launch pad explosion of the N-1 on July 3, 1969, was a significant setback. The rocket hit the pad after an engine shutdown, destroying itself and the launch facility. Without the N-1 rocket, the USSR could not send a large enough payload to the Moon to land a human and return him safely.\n\nApollo 11 was prepared with the goal of a July landing in the Sea of Tranquility. The crew, selected in January 1969, consisted of commander (CDR) Neil Armstrong, Command Module Pilot (CMP) Michael Collins, and Lunar Module Pilot (LMP) Edwin \"Buzz\" Aldrin. They trained for the mission until just before the launch day. On July 16, 1969, at exactly 9:32 am EDT, the Saturn V rocket, AS-506, lifted off from Kennedy Space Center Launch Complex 39 in Florida.\n\nThe trip to the Moon took just over three days. After achieving orbit, Armstrong and Aldrin transferred into the Lunar Module, named \"Eagle\", and after a landing gear inspection by Collins remaining in the Command/Service Module \"Columbia\", began their descent. After overcoming several computer overload alarms caused by an antenna switch left in the wrong position, and a slight downrange error, Armstrong took over manual flight control at about , and guided the Lunar Module to a safe landing spot at 20:18:04 UTC, July 20, 1969 (3:17:04 pm CDT). The first humans on the Moon waited six hours before they left their craft. At 02:56 UTC, July 21 (9:56 pm CDT July 20), Armstrong became the first human to set foot on the Moon.\nThe first step was witnessed by at least one-fifth of the population of Earth, or about 723 million people. His first words when he stepped off the LM's landing footpad were, \"That's one small step for [a] man, one giant leap for mankind.\" Aldrin joined him on the surface almost 20 minutes later. Altogether, they spent just under two and one-quarter hours outside their craft. The next day, they performed the first launch from another celestial body, and rendezvoused back with \"Columbia\".\n\nApollo 11 left lunar orbit and returned to Earth, landing safely in the Pacific Ocean on July 24, 1969. When the spacecraft splashed down, 2,982 days had passed since Kennedy's commitment to landing a man on the Moon and returning him safely to the Earth before the end of the decade; the mission was completed with 161 days to spare. With the safe completion of the Apollo 11 mission, the Americans won the race to the Moon.\n\nNASA had ambitious follow-on human spaceflight plans as it reached its lunar goal, but soon discovered it had expended most of its political capital to do so.\n\nThe first landing was followed by another, precision landing on Apollo 12 in November 1969. NASA had achieved its first landing goal with enough Apollo spacecraft and Saturn V launchers left for eight follow-on lunar landings through Apollo 20, conducting extended-endurance missions and transporting the landing crews in Lunar Roving Vehicles on the last five. They also planned an Apollo Applications Program to develop a longer-duration Earth orbital workshop (later named Skylab) to be constructed in orbit from a spent S-IVB upper stage, using several launches of the smaller Saturn IB launch vehicle. But planners soon decided this could be done more efficiently by using the two live stages of a Saturn V to launch the workshop pre-fabricated from an S-IVB (which was also the Saturn V third stage), which immediately removed Apollo 20. Belt-tightening budget cuts soon led NASA to cut Apollo 18 and 19 as well, but keep three extended/Lunar Rover missions. Apollo 13 encountered an in-flight spacecraft failure and had to abort its lunar landing in April 1970, returning its crew safely but temporarily grounding the program again. It resumed with four successful landings on Apollo 14 (February 1971), Apollo 15 (July 1971), Apollo 16 (April 1972), and Apollo 17 (December 1972).\n\nIn February 1969, President Richard M. Nixon convened a Space Task Group to set recommendations for the future US civilian space program, headed by his Vice President Spiro T. Agnew. Agnew was an enthusiastic proponent of NASA's follow-on plans, and the STG recommended plans to develop a reusable Space Transportation System including a Space Shuttle, which would facilitate development of permanent space stations in Earth and lunar orbit, perhaps a base on the lunar surface, and the first human flight to Mars as early as 1986 or as late as 2000. Nixon had a better sense of the declining political support in Congress for a new Apollo-style program, which had disappeared with the achievement of the landing, and he intended to pursue detente with the USSR and China, which he hoped might ease Cold War tensions. He cut the spending proposal he sent to Congress to include funding for only the Space Shuttle, with perhaps an option to pursue the Earth orbital space station for the foreseeable future.\n\nThe USSR continued trying to perfect their N1 rocket, finally canceling it in 1976, after two more launch failures in 1971 and 1972.\n\nHaving lost the race to the Moon, the USSR decided to concentrate on orbital space stations. During 1969 and 1970, they launched six more Soyuz flights after Soyuz 3, then launched the first space station, the Salyut 1 laboratory designed by Kerim Kerimov, on April 19, 1971. Three days later, the Soyuz 10 crew attempted to dock with it, but failed to achieve a secure enough connection to safely enter the station. The Soyuz 11 crew of Vladislav Volkov, Georgi Dobrovolski and Viktor Patsayev successfully docked on June 7, and completed a record 22-day stay. The crew became the second in-flight space fatality during their reentry on June 30. They were asphyxiated when their spacecraft's cabin lost all pressure, shortly after undocking. The disaster was blamed on a faulty cabin pressure valve, that allowed all the air to vent into space. The crew was not wearing pressure suits and had no chance of survival once the leak occurred.\n\nSalyut 1's orbit was increased to prevent premature reentry, but further piloted flights were delayed while the Soyuz was redesigned to fix the new safety problem. The station re-entered the Earth's atmosphere on October 11, after 175 days in orbit. The USSR attempted to launch a second Salyut-class station designated Durable Orbital Station-2 (DOS-2) on July 29, 1972, but a rocket failure caused it to fail to achieve orbit. After the DOS-2 failure, the USSR attempted to launch four more Salyut-class stations up to 1975, with another failure due to an explosion of the final rocket stage, which punctured the station with shrapnel so that it would not hold pressure. All of the Salyuts were presented to the public as non-military scientific laboratories, but some of them were covers for the military Almaz reconnaissance stations.\n\nThe United States launched the orbital workstation Skylab 1 on May 14, 1973. It weighed , was long by in diameter, with a habitable volume of . Skylab was damaged during the ascent to orbit, losing one of its solar panels and a meteoroid thermal shield. Subsequent crewed missions repaired the station, and the final mission's crew, Skylab 4, set the Space Race endurance record with 84 days in orbit when the mission ended on February 8, 1974. Skylab stayed in orbit another five years before reentering the Earth's atmosphere over the Indian Ocean and Western Australia on July 11, 1979.\n\nIn May 1972, President Richard M. Nixon and Soviet Premier Leonid Brezhnev negotiated an easing of relations known as detente, creating a temporary \"thaw\" in the Cold War. In the spirit of good sportsmanship, the time seemed right for cooperation rather than competition, and the notion of a continuing \"race\" began to subside.\n\nThe two nations planned a joint mission to dock the last US Apollo craft with a Soyuz, known as the Apollo-Soyuz Test Project (ASTP). To prepare, the US designed a docking module for the Apollo that was compatible with the Soviet docking system, which allowed any of their craft to dock with any other (e.g. Soyuz/Soyuz as well as Soyuz/Salyut). The module was also necessary as an airlock to allow the men to visit each other's craft, which had incompatible cabin atmospheres. The USSR used the Soyuz 16 mission in December 1974 to prepare for ASTP.\n\nThe joint mission began when Soyuz 19 was first launched on July 15, 1975, at 12:20 UTC, and the Apollo craft was launched with the docking module six and a half hours later. The two craft rendezvoused and docked on July 17 at 16:19 UTC. The three astronauts conducted joint experiments with the two cosmonauts, and the crew shook hands, exchanged gifts, and visited each other's craft.\n\nIn the 1970s, the United States began developing a new generation of reusable orbital spacecraft known as the Space Shuttle, and launched a range of uncrewed probes. The USSR continued to develop space station technology with the Salyut program and Mir ('Peace' or 'World', depending on the context) space station, supported by Soyuz spacecraft. They developed their own large space shuttle under the Buran program. The USSR dissolved in 1991 and the remains of its space program mainly passed to Russia. The United States and Russia worked together in space with the Shuttle–Mir Program, and again with the International Space Station.\n\nThe Russian R-7 rocket family, which launched the first Sputnik at the beginning of the Space Race, is still in use today. It services the International Space Station (ISS) as the launcher for both the Soyuz and Progress spacecraft. It also ferries both Russian and American crews to and from the station.\n\n\n"}
{"id": "10469862", "url": "https://en.wikipedia.org/wiki?curid=10469862", "title": "Technological somnambulism", "text": "Technological somnambulism\n\nTechnological somnambulism is a concept used when talking about the philosophy of technology. The term was used by Langdon Winner in his essay \"Technology as forms of life\". Winner puts forth the idea that we are simply in a state of \"sleepwalking\" in our mediations with technology. This sleepwalking is caused by a number of factors. One of the primary causes is the way we view technology as tools, something that can be put down and picked up again. Because of this view of objects as something we can easily separate ourselves from technology, and so we fail to look at the long term implications of using that object. A second factor is the separation of those who make the technology and those who use the technology. This division causes there to be little thought and research going into the effects of using/developing that technology. The third and most important idea is the way in which technology seems to create new \"worlds\" in which we live. These worlds are created by the restructuring of the common and seemingly everyday things around us. In most situations the changes take place with little attention or care from us because we are more focused on the menial aspects of the technology (Winner 105-107).\n\nThe concept can be found in the earlier work of Marshall McLuhan, cf. \"Understanding Media\", where he refers to a comment made by David Sarnoff expressing a socially deterministic view of \"value free\" technology whose value is solely defined by its usage as representing, \"...the voice of the current somnambulism\". Given that this piece by McLuhan has become standard reading in Media Theory it is reasonable to suspect that Winner encountered the concept there or elsewhere and then went on to develop it further.\n"}
{"id": "341394", "url": "https://en.wikipedia.org/wiki?curid=341394", "title": "The Analyst", "text": "The Analyst\n\nThe Analyst, subtitled \"A DISCOURSE Addressed to an Infidel MATHEMATICIAN. WHEREIN It is examined whether the Object, Principles, and Inferences of the modern Analysis are more distinctly conceived, or more evidently deduced, than Religious Mysteries and Points of Faith\", is a book published by George Berkeley in 1734. The \"infidel mathematician\" is believed to have been Edmond Halley, though others have speculated Sir Isaac Newton was intended. See .\n\nFrom his earliest days as a writer, Berkeley had taken up his satirical pen to attack what were then called 'free-thinkers' (secularists, skeptics, agnostics, atheists, etc.—in short, anyone who doubted the truths of received Christian religion or called for a diminution of religion in public life). In 1732, in the latest installment in this effort, Berkeley published his \"Alciphron\", a series of dialogues directed at different types of 'free-thinkers'. One of the archetypes Berkeley addressed was the secular scientist, who discarded Christian mysteries as unnecessary superstitions, and declared his confidence in the certainty of human reason and science. Against his arguments, Berkeley mounted a subtle defense of the validity and usefulness of these elements of the Christian faith.\n\n\"Alciphron\" was widely read and caused a bit of a stir. But it was an offhand comment mocking Berkeley's arguments by the 'free-thinking' royal astronomer Sir Edmund Halley that prompted Berkeley to pick up his pen again and try a new tack. The result was \"The Analyst\", conceived as a satire attacking the foundations of mathematics with the same vigor and style as 'free-thinkers' routinely attacked religious truths.\n\nBerkeley sought to take mathematics apart, claimed to uncover numerous gaps in proof, attacked the use of infinitesimals, the diagonal of the unit square, the very existence of numbers, etc. The general point was not so much to mock mathematics or mathematicians, but rather to show that mathematicians, like Christians, relied upon incomprehensible 'mysteries' in the foundations of their reasoning. Moreover, the existence of these 'superstitions' was not fatal to mathematical reasoning, indeed it was an aid. So too with the Christian faithful and their 'mysteries'. Berkeley concluded that the certainty of mathematics is no greater than the certainty of religion.\n\n\"The Analyst\" was a direct attack on the foundations of calculus, specifically on Newton's notion of fluxions and on Leibniz's notion of infinitesimal change. In section 16, Berkeley criticizes\n...the fallacious way of proceeding to a certain Point on the Supposition of an Increment, and then at once shifting your Supposition to that of no Increment . . . Since if this second Supposition had been made before the common Division by \"o\", all had vanished at once, and you must have got nothing by your Supposition. Whereas by this Artifice of first dividing, and then changing your Supposition, you retain 1 and nx. But, notwithstanding all this address to cover it, the fallacy is still the same.\n\nIts most frequently quoted passage:\nAnd what are these Fluxions? The Velocities of evanescent Increments? And what are these same evanescent Increments? They are neither finite Quantities nor Quantities infinitely small, nor yet nothing. May we not call them the ghosts of departed quantities?\n\nBerkeley did not dispute the results of calculus; he acknowledged the results were true. The thrust of his criticism was that Calculus was not more logically rigorous than religion. He instead questioned whether mathematicians \"submit to Authority, take things upon Trust\" just as followers of religious tenets did. According to Burton, Berkeley introduced an ingenious theory of compensating errors that were meant to explain the correctness of the results of calculus. Berkeley contended that the practitioners of calculus introduced several errors which cancelled, leaving the correct answer. In his own words, \"by virtue of a two fold mistake you arrive, though not at science, yet truth.\"\n\nThe idea that Newton was the intended recipient of the discourse is put into doubt by a passage that appears toward the end of the book: \n\" \"Query 58: Whether it be really an effect of Thinking, that the same Men admire the great author for his Fluxions, and deride him for his Religion?\" \"\n\nHere Berkeley ridicules those who celebrate Newton (the inventor of \"fluxions\", roughly equivalent to the differentials of later versions of the differential calculus) as a genius while deriding his well-known religiosity. Since Berkeley is here explicitly calling attention to Newton's religious faith, that seems to indicate he did not mean his readers to identify the \"infidel (i.e., lacking faith) mathematician\" with Newton.\n\nMathematical historian Judith Grabiner comments, “Berkeley’s criticisms of the rigor of the calculus were witty, unkind, and — with respect to the mathematical practices he was criticizing — essentially correct” . While his critiques of the mathematical practices were sound, his essay has been criticized on logical and philosophical grounds.\n\nFor example, David Sherry argues that Berkeley's criticism of infinitesimal calculus consists of a logical criticism and a metaphysical criticism. The logical criticism is that of a \"fallacia suppositionis\", which means gaining points in an argument by means of one assumption and, while keeping those points, concluding the argument with a contradictory assumption. The metaphysical criticism is a challenge to the existence itself of concepts such as fluxions, moments, and infinitesimals, and is rooted in Berkeley's empiricist philosophy which tolerates no expression without a referent . Andersen (2011) showed that Berkeley's doctrine of the compensation of errors contains a logical circularity. Namely, Berkeley relies upon Apollonius's determination of the tangent of the parabola in Berkeley's own determination of the derivative of the quadratic function.\n\nTwo years after this publication, Thomas Bayes published anonymously \"An Introduction to the Doctrine of Fluxions, and a Defence of the Mathematicians Against the Objections of the Author of the Analyst\" (1736), in which he defended the logical foundation of Isaac Newton's calculus against the criticism outlined in \"The Analyst\". Colin Maclaurin's two-volume \"Treatise of Fluxions\" published in 1742 also began as a response to Berkeley attacks, intended to show that Newton's calculus was rigorous by reducing it to the methods of Greek geometry .\n\nDespite these attempts calculus continued to be developed using non-rigorous methods until around 1830 when Augustin Cauchy, and later Bernhard Riemann and Karl Weierstrass, redefined the derivative and integral using a rigorous definition of the concept of limit. The concept of using limits as a foundation for calculus had been suggested by d'Alembert, but d'Alembert's definition was not rigorous by modern standards . The concept of limits had already appeared in the work of Newton , but was not stated with sufficient clarity to hold up to the criticism of Berkeley .\n\nIn 1966, Abraham Robinson introduced \"Non-standard Analysis\", which provided a rigorous foundation for working with infinitely small quantities. This provided another way of putting calculus on a mathematically rigorous foundation that was in a similar spirit to the way calculus was done before the (ε, δ)-definition of limit had been fully developed.\n\nTowards the end of \"The Analyst,\" Berkeley addresses possible justifications for the foundations of calculus that mathematicians may put forward. In response to the idea fluxions could be defined using ultimate ratios of vanishing quantities , Berkeley wrote:\n\nIt must, indeed, be acknowledged, that [Newton] used Fluxions, like the Scaffold of a building, as things to be laid aside or got rid of, as soon as finite Lines were found proportional to them. But then these finite Exponents are found by the help of Fluxions. Whatever therefore is got by such Exponents and Proportions is to be ascribed to Fluxions: which must therefore be previously understood. And what are these Fluxions? The Velocities of evanescent Increments? And what are these same evanescent Increments? They are neither finite Quantities nor Quantities infinitely small, nor yet nothing. May we not call them the Ghosts of departed Quantities?\n\nEdwards describes this as the most memorable point of the book . Katz and Sherry argue that the expression was intended to address both infinitesimals and Newton's theory of fluxions. \n\nToday the phrase \"ghosts of departed quantities\" is also used when discussing Berkeley's attacks on other possible foundations of Calculus. In particular it is used when discussing infinitesimals , but it is also used when discussing differentials , and adequality .\n\nThe full text of can be read on Wikisource, as well as on David R. Wilkins' website, which includes some commentary and links to responses by Berkeley's contemporaries.\n\n\"The Analyst\" is also reproduced, with commentary, in recent works:\nEwald concludes that Berkeley's objections to the calculus of his day were mostly well taken at the time.\n\n\n"}
