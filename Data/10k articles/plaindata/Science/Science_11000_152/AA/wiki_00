{"id": "14725460", "url": "https://en.wikipedia.org/wiki?curid=14725460", "title": "An Inconvenient Book", "text": "An Inconvenient Book\n\nAn Inconvenient Book: Real Solutions to the World's Biggest Problems is a 2007 political narrative written and edited by conservative commentator Glenn Beck\n\nThe title of \"An Inconvenient Book\" is a parody of the title of Al Gore's 2006 documentary \"An Inconvenient Truth\". One section of the book provides a critical response to Gore's views on global warming.\n\nBeck discusses his political views on a number of subjects. Issues include Beck’s claims that the free market provides the best way to fight global warming, divorce rates, Beck's perceptions of liberal bias on school campuses, the income gap, perceived anti-Americanism of the United Nations, and illegal immigration.\n\nPublishers Weekly described \"An Inconvenient Book\" as \"a good read for conservatives,\" referring to Beck's often lighthearted tone, \"at his best when most absurd, and funniest when he's his own target.\" On the content, the reviewer says \"While often informative, as in his chapter on global warming, Beck is sometimes tedious, particularly when dealing with Islam and education (France is literally teetering on the edge, and our biggest ally, England, is about to be turned inside out as well).\"\n\n\"An Inconvenient Book\" entered The New York Times Best Seller List at Number 1 under the category Hardcover Nonfiction, and stayed in the list for 17 weeks.\n\n"}
{"id": "30914072", "url": "https://en.wikipedia.org/wiki?curid=30914072", "title": "Aquarius Stream", "text": "Aquarius Stream\n\nThe Aquarius Stream is a stellar stream located in the Milky Way Galaxy. It is so named because most of the stars in the stream lie in the direction of the Aquarius constellation. At its nearest point it is about 2000 light years from Earth; At its farthest it is about 30,000 light years away. It is the closest stellar stream to Earth yet found, and the youngest having formed about 700 million years ago. The stream was discovered in late 2010 by a team of astronomers involved in the RAdial Velocity Experiment (RAVE) survey led by New Zealander Mary Williams.\n\n\n"}
{"id": "479975", "url": "https://en.wikipedia.org/wiki?curid=479975", "title": "Attribute grammar", "text": "Attribute grammar\n\nAn attribute grammar is a formal way to define attributes for the productions of a formal grammar, associating these attributes with values. The evaluation occurs in the nodes of the abstract syntax tree, when the language is processed by some parser or compiler.\n\nThe attributes are divided into two groups: \"synthesized\" attributes and \"inherited\" attributes. The synthesized attributes are the result of the attribute evaluation rules, and may also use the values of the inherited attributes. The inherited attributes are passed down from parent nodes.\n\nIn some approaches, synthesized attributes are used to pass semantic information up the parse tree, while inherited attributes help pass semantic information down and across it. For instance, when constructing a language translation tool, such as a compiler, it may be used to assign semantic values to syntax constructions. Also, it is possible to validate semantic checks associated with a grammar, representing the rules of a language not explicitly imparted by the syntax definition.\n\nAttribute grammars can also be used to translate the syntax tree directly into code for some specific machine, or into some intermediate language.\n\nOne strength of attribute grammars is that they can transport information from anywhere in the abstract syntax tree to anywhere else, in a controlled and formal way.\n\nAttribute grammars were invented by Donald Knuth and Peter Wegner. While Donald Knuth is credited for the overall concept, Peter Wegner invented inherited attributes during a conversation with Knuth. Some embryonic ideas trace back to the work of Edgar T. \"Ned\" Irons, the author of IMP.\n\nThe following is a simple context-free grammar which can describe a language made up of multiplication and addition of integers.\n\nThe following attribute grammar can be used to calculate the result of an expression written in the grammar. Note that this grammar only uses synthesized values, and is therefore an S-attributed grammar.\n\nA synthesized attribute is computed from the values of attributes of the children. Since the values of the children must be computed first, this is an example of bottom-up propagation. To formally define a synthesized attribute, let formula_1 be a formal grammar, where\n\n\nThen, given a string of nonterminal symbols formula_6 and an attribute name formula_7, formula_8 is a synthesized attribute if all four of these conditions are met:\n\n\nAn \"inherited attribute\" at a node in parse tree is defined using the attribute values at the parent or siblings. Inherited attributes are convenient for expressing the dependence of a programming language construct on the context in which it appears. For example, we can use an inherited attribute to keep track of whether an identifier appears on the left or the right side of an assignment in order to decide whether the address or the value of the identifier is needed. In contrast to synthesized attributes, inherited attributes can take values from parent and/or siblings. As in the following production,\n\nwhere A can get values from S, B, and C. B can take values from S, A, and C. Likewise, C can take values from S, A, and B.\n\n\n\n"}
{"id": "2579958", "url": "https://en.wikipedia.org/wiki?curid=2579958", "title": "Augustinian hypothesis", "text": "Augustinian hypothesis\n\nThe Augustinian hypothesis is a solution to the synoptic problem, which concerns the origin of the Gospels of the New Testament. The hypothesis holds that Matthew was written first, by Matthew the Evangelist (see the Gospel According to the Hebrews and the Jewish-Christian Gospels). Mark the Evangelist wrote the Gospel of Mark second and used Matthew and the preaching of Peter as sources. Luke the Evangelist wrote the Gospel of Luke and was aware of the two Gospels that preceded him. Unlike some competing hypotheses, this hypothesis does not rely on, nor does it argue for, the existence of any document that is not explicitly mentioned in historical testimony. Instead, the hypothesis draws primarily upon historical testimony, rather than textual criticism, as the central line of evidence. The foundation of evidence for the hypothesis is the writings of the Church Fathers: historical sources dating back to as early as the first half of the 2nd century, which have been held as authoritative by most Christians for nearly two millennia. Adherents to the Augustinian hypothesis view it as a simple, coherent solution to the synoptic problem.\n\nThe Augustinian hypothesis addresses certain fundamental points of contention surrounding the synoptic problem, such as how reliable the early Christian tradition is, which gospel was written first, whether there were other unknown sources behind the gospels, to what extent, if any, the gospels were redacted, and to what extent the gospels were altered between the time they were originally written and the time the first surviving manuscripts appear. These and other matters are raised and alternate resolutions proposed by proponents of competing hypotheses, such as the Two-source hypothesis, its related Q hypothesis, the Farrer hypothesis, and others.\n\nThe main two areas of contention within the Augustinian community are whether Matthew was originally written in Aramaic using Hebrew script (see Aramaic primacy), or if the Greek text is the original, and whether it was Mark or Luke who wrote second. A modified version of the Augustinian hypothesis, known as the Griesbach hypothesis, agrees that Matthew wrote first and that Mark depended on Matthew, and does not dispute that the original text was in Hebrew thereafter translated into Greek, but argues that Mark also depended on Luke and therefore that Luke’s gospel precedes Mark's. Because of the similarity on primary points of contention, this hypothesis is also treated as a possible amendment to the Augustinian hypothesis. Another modified version of the Augustinian hypothesis is the hypothesis of Eta Linnemann and F. David Farnell that two Gospels for diaspora Jewish audiences are required by the Mosaic rule of \"two witnesses\".\n\nThe hypothesis takes its name from Augustine of Hippo, an early 5th century bishop and church father, who wrote: \"Now, those four evangelists whose names have gained the most remarkable circulation over the whole world, and whose number has been fixed as four, ...are believed to have written in the order which follows: first Matthew, then Mark, thirdly Luke, lastly John.\" And: \"Of these four, it is true, only Matthew is reckoned to have written in the Hebrew language; the others in Greek. And however they may appear to have kept each of them a certain order of narration proper to himself, this certainly is not to be taken as if each individual writer chose to write in ignorance of what his predecessor had done...\"\n\nMark was famously dubbed by Augustine as \"pedissequus et breviator Matthaei\", the attendant and abbreviator of Matthew, in direct contrast to the view most commonly held in academia today, that Mark's gospel was the earliest. Augustine also discussed the commonalities between the Synoptic Gospels, including the identical language found in Matthew, Mark, and Luke. Augustine was not the first to articulate this view, as Irenaeus and Origen, among others, shared this ordering. However, Augustine was the first author to give a detailed scholarly textual analysis of the three texts' interdependence, and to articulate a theory for the express purpose of explaining this fact.\n\nThe Church Fathers who wrote about the order and authorship of the canonical gospels all supported some basic ideas of the Augustinian hypothesis. The fathers whose writings survive and who wrote about authorship are almost unanimous in agreement that Matthew the apostle was the author, wrote first, and did so for the Hebrews in their language. A number of sources in antiquity asserted that Mark wrote his Gospel after Matthew based on the preaching of Peter. Various elements of this tradition are found in the writings of Irenaeus, Origen, Eusebius, and others.\n\nThe text of the Gospel itself circulated with a title \"According to Matthew\", a tradition indisputably acknowledged before the close of the 2nd century. In addition, the title \"According to Matthew\" is found in the earliest manuscripts. A number of scholars have argued that the title must be dated no later than 125. Many contemporary scholars, however, believe it was originally anonymous.\n\nThe earliest surviving references to the gospel tradition are quoted by Eusebius (lived c. 263–339 CE), and different but related traditions appear in the works of Papias (wrote during the first half of 2nd century CE) and the works of Clement. A third ancient source, Irenaeus, also provides further information about the traditions, especially that of Papias, and possibly adds a third related tradition to the sources. These related traditions generally agree on the primary points of contention within the Augustinian hypothesis, though not without discrepancies. Rather than seen as a refutation to the hypothesis, instead these discrepancies are often cited in defense of the hypothesis because they counter the argument that the entire tradition is merely a repetition of Papias's original assertion (therefore, if he were wrong, the great many historical sources supporting the theory would be inconsequential). Instead, slight disagreement is actually in favor of multiple, near identical traditions.\n\nAccording to Irenaeus, Papias was \"a hearer of John and a companion of Polycarp, a man of primitive times,\" who wrote a volume in \"five books.\" The benefit of historical immediacy, as argued by D. H. Fischer is one of the key determinants of historicity, and the church father Papias is a very early source in regard to testimony that the Matthew wrote his gospel first. Papias wrote that: \"Matthew compiled the sayings in the Hebrew language, and everyone translated them as well he could.\" (The 'Hebrew language' referred to by Papias has often been interpreted as Aramaic.)\n\nIt has been argued, because Papias does not cite an authority for his assertions concerning Matthew but does concerning Mark, that Matthew was already fully accepted at the time of his writings.\n\nEusebius also recorded an important tradition from Clement of Alexandria (died c. 213):\nThis source claims multiple authorities of antiquity, not merely Papias; this is taken as evidence against the view that the testimony of the Fathers is based solely upon the witness of Papias. Furthermore the tradition of Clement concurs with the significant point of contention: Matthean priority. However, Clement conflicts with the Augustinian hypothesis concerning the order of Mark and Luke. The Griesbach hypothesis attempts to resolve the difficulty concerning this secondary point of contention by stating Luke wrote before Mark.\n\nIrenaeus, who was familiar with the work of Papias and who knew Polycarp and possibly even the apostle John, wrote: \"Now Matthew published also a book of the Gospel among the Hebrews in their own dialect, while Peter and Paul were preaching the gospel in Rome and founding the Church.\"\n\nIrenaeus gives here another tradition in accord with Papias, though containing more information. This has been taken as evidence of a third, yet harmonious tradition. However, Irenaeus places the composition of Mark after Peter's death, while Clement (and others, such as Origen and Eusebius) claimed Peter was alive and approved the work. Nonetheless, because the Augustinian hypothesis does not address whether Peter was alive at the time of the composition of Mark or not, this discrepancy is not a basis for objection to the theory.\n\nAn original Aramaic version of Matthew does not exist in the sense that no copy survives in the original language today. Many proponents of the Augustinian hypothesis hold that the current Greek Matthew is a complete translation of the original Aramaic Matthew. This theory has strong support in a number of Church Fathers. Papias, Irenaeus, Origen, Eusebius, Epiphanius and Jerome all agree that the original Matthew was written in Hebrew. Jerome even claimed to have seen the original Aramaic Matthew in the library of Pamphilus the Martyr. Eusebius wrote in c. 325 that Pantaerus found a copy of the Gospel of Matthew written in Hebrew in India, and that it had been left there by Bartholomew. In c. 376, Epiphanius wrote there was \"no doubt\" that a sect in Palestine still used the original Hebrew text \"just as it was originally written.\" And, of course, Augustine also repeated this tradition. To these authors should be added Pantaenus, Athanasius, John Chrysostom, Cyril of Jerusalem, Gregory of Nazianzus, and others in agreement.\n\nThe Augustinian position, and the similar Griesbach hypothesis, has drawn recent interest, especially from B. C. Butler, John Wenham, W.R. Farmer, and others as an alternative solution to the synoptic problem, and has been employed as a scholarly refutation of Marcan priority, the Q hypothesis, and the two-source hypothesis. Butler argued that accepting the priority of Matthew rendered it possible to dispense with the hypothetical Q document altogether, a position he supported by arguments concerning the inadmissibility of appealing to Q as a sound explanation of the cases where Matthew appears to be more original than Mark. Farmer argued that a modification of the Augustinian hypothesis, ordering Matthew-Luke-Mark, eliminated all reasons for the existence of Q, a position whose credibility was conceded by W.C. Allen and others. Likewise it has been pointed out that differences between the Synoptic Gospels are as easily explained by differing purposes of the authors than by forced redactions or omissions due to ignorance. Furthermore, against certain arguments that the “primitiveness” of the ideas within the Gospels is the determining factor in their literary interdependence, it is observed that defining \"primitiveness\" carries obvious difficulties.\n\nRecently, modern scholars accepting some form of the Augustinian hypothesis have attempted to develop a detailed argument explaining the theoretical origin of the gospels. There was a perceived need for this in response to recent competing theories, expressed by Bernard Orchard: “the two-document hypothesis and the priority of Mark are still only hypotheses, not infallible dogmas, and they have stood secure for so long chiefly because no one has been able to offer any satisfactory alternative.\" Central to this process is the assumption that the gospel's development should be understood as a reaction to various developing needs of the early church.\n\nJohn Wenham argued that, in the early Jerusalem Church, there would have been an early need for the production of a written record to augment the \"atmosphere of spontaneity\" within which the apostles, disciples, and eyewitnesses would have given instruction. The reasons for this, he asserted, were: the need for instruction when no qualified teacher was available, the need for consistency and accuracy in what was taught as it spread throughout the first scattered Christian communities, and for the basic need of evangelization. Wenham also argued that Matthew was a natural choice since, as a tax collector, he would have had the requisite literacy, as well as his first hand memories, and perhaps even notes. Others have observed that persecutions in Palestine, threatening dispersion of the Christians, would have been a motivating factor for a text of the life of Jesus.\n\nThe majority Hebrew makeup of the primitive Church has been seen as support of Aramaic primacy. Besides the traditional material (see above), other support for an Aramaic Matthew advanced in recent years includes the theory that the Medieval Hebrew gospel of Matthew in Even Bohan could be a corrupted version of the original.\n\nBernard Orchard identified the above period as a \"first phase\" of the development of the Gospels, distinguished from the subsequent phase by the events of the year 42:\n\nCentral to Orchard's characterization of this new second phase is the distinction between a primarily Hebrew orientation and a primarily Greek orientation, focusing not only on the Jewish converts to Christianity, but to the gentile converts as well. This, he argues, resulted in three key events: the translation of the original Matthew into Greek, the production of the Gospel of Mark within the context of Peter's preaching to Greek speaking converts in Rome, and Luke's authorship of his Gospel under the instruction of Paul. Cited in support of this are the comments of Clement, Irenaeus, and others who state that the Gospel of Mark was written by Mark, a follower of the apostle Peter, based on his speeches. Orchard countered the claim that the Gospel of Mark must have been written first, since it contains less information than Matthew and Luke, by positing that Peter elected not to speak on certain subjects, such as the birth and resurrection narratives, since he had not been a direct witness of those events. The notion that Peter employed Matthew in his preaching was supported by B.C. Butler, but not by John Wenham, who instead explained the similar structure by arguing simply that Mark used both his recollection of his instruction from the Gospel of Matthew and his memory of the preaching of Peter to pen his own synthesis.\n\nThe association of the Gospel of Luke with Paul the apostle, which is witnessed by tradition, has led some to argue that Luke was with Paul during his imprisonment in Rome, or to at least place the date of composition prior to 70 and the fall of Jerusalem. The author of Luke also wrote in his prologue that he employed various sources in composing his work. Wenham argued that an excess of such material, along with the constraints of scroll length, was one cause of his noticeable omission of material found in Matthew and Mark.\n\nAn unusual modern scholar who supported the notion that the Synoptic Gospels were of an early date, specifically before 70, was John Robinson. Though generally considered a liberal theologian, his views in respect to the development of the Gospels were consistent with the Augustinian hypothesis. He wrote in his work \"Redating the New Testament\" that past scholarship was based on a \"tyranny of unexamined assumptions\" and an \"almost wilful blindness,\" concluding that New Testament was written before 64, and that there is no compelling evidence and little evidence of any kind that anything in the New Testament reflects knowledge of the Temple's destruction. Furthermore, in relation to the four gospels, according to Norman Geisler:\n\n\n"}
{"id": "3593875", "url": "https://en.wikipedia.org/wiki?curid=3593875", "title": "Business mathematics", "text": "Business mathematics\n\nBusiness mathematics is mathematics used by commercial enterprises to record and manage business operations. Commercial organizations use mathematics in accounting, inventory management, marketing, sales forecasting, and financial analysis. \n\nMathematics typically used in commerce includes elementary arithmetic, elementary algebra, statistics and probability. Business management can be done more effectively in some cases by use of more advanced mathematics such as calculus, matrix algebra and linear programming.\n\n\"Business mathematics, \" sometimes called \"commercial math\" or \"consumer math\", is a group of practical subjects used in commerce and everyday life. In schools, these subjects are often taught to students who are not planning a university education. In the United States, they are typically offered in high schools and in schools that grant associate's degrees; elsewhere they may be included under Business studies. The emphasis in these courses is on computational skills and their practical application, with practical application being predominant. \n\nA (U.S.) business math course might include a review of elementary arithmetic, including fractions, decimals, and percentages. Elementary algebra is often included as well, in the context of solving practical business problems. The practical applications typically include checking accounts, price discounts, markups and Markup, payroll calculations, simple and compound interest, consumer and business credit, and mortgages and revenues.\n\n\"Business Mathematics\" comprises mathematics courses taken at an undergraduate level by business students. The two most common here are \"Business Calculus\" and \"Business Statistics\". Programs often also cover matrix operations as above, and may include a separate module on interest calculations.\n\nThese courses are usually focused on problems from the business world, and the syllabus is adjusted correspondingly. Thus for example, whereas in a regular calculus course students would study trigonometric functions, courses here would not typically cover this area. Correspondingly, these courses typically do not go into the same depth as standard courses in the mathematics or science fields. (Although see Bachelor of Science in Business Administration and Bachelor of Business Science.) \n\nNote that economics majors, especially those planning to pursue graduate study in the field, are encouraged to instead take regular calculus, as well as linear algebra and other advanced math courses, especially real analysis. Some programs (instead) include a module in \"mathematics for economists\", providing a bridge between the above \"Business Mathematics\" courses and mathematical economics and econometrics.\nOperations management (and management accounting) may similarly include supplementary coursework in relevant quantitative techniques, generally linear programming as above, as well as other optimization methods.\n\nAt the postgraduate level, \"generalist\" management and finance programs include quantitative topics which are foundational for the study in question - often exempting students with an appropriate background. These are usually \"interest mathematics\" and statistics, both at the above level. MBA programs often also include basic operations research (linear programming, as above) with the emphasis on practice, and may combine the topics as \"quantitative analysis\"; MSF programs may similarly cover applied econometrics.\n\nMore technical Masters in these areas, such as those in management science and quantitative finance, will entail a deeper, more theoretical study of operations research and econometrics, and extend to further advanced topics such as mathematical optimization and stochastic calculus. These programs do not include \"Business mathematics\" \"per se\".\n\nWhere mathematical economics is not required, graduate economics programs often include \"quantitative techniques\", which covers (applied) linear algebra and multivariate calculus, and may include the above topics; regardless, econometrics is usually a separate course, and is dealt with in depth.\n\n\n"}
{"id": "2257544", "url": "https://en.wikipedia.org/wiki?curid=2257544", "title": "Carl August Dohrn", "text": "Carl August Dohrn\n\nCarl August Dohrn (27 June 1806 – 10 May 1892) was a German entomologist.\n\nBorn at Stettin (Szczecin, now Poland) Carl August was the son of Heinrich Dohrn, who was a wine and spice merchant, and had made the family fortune by trading in sugar. This wealth allowed Carl August to devote himself to his various hobbies; travelling, folk music and entomology.\n\nAlthough interested in all orders of insects Dohrn specialised in Coleoptera. His first published paper was in the \"Entomologische Zeitung\" for 1845 but he was an active entomologist long before this, since he had acted as Secretary to the Stettin Entomological Society from its foundation in 1839 and edited its journal. He was elected president of the society in 1843 retiring from the post in 1887.During this time he held together the rather fractious German entomologists and the society flourished. Dohrn had many contacts including Alexander Henry Haliday, with whom he wrote a paper on the Linnaean Diptera (See ) and the London entomologists. He was a personal friend of Henry Tibbats Stainton with whom he stayed in England.\n\nA frequent visitor to London he was accompanied by the Lepidopterist, Philipp Christoph Zeller in 1852, by Carl Henrik Boheman in 1854 and by Hermann August Hagen in 1857. He spent many summers in Italy, in Lucca with Haliday and in Turin with Maximilian Spinola. Dohrn was elected a fellow of the Entomological Society of London in 1855 and an honorary member in 1855. A classicist he was fluent in many European languages. His Coleoptera collection was very extensive and just prior to his death, at 86, he had received upwards of 1,000 beetles from Sumatra.\n\nDohrn's son Anton Dohrn, an ardent supporter of Charles Darwin, became a famous marine zoologist. A second son Heinrich Wolfgang Ludwig Dohrn was also an entomologist.\n\n\n"}
{"id": "3319516", "url": "https://en.wikipedia.org/wiki?curid=3319516", "title": "Chinese dormouse", "text": "Chinese dormouse\n\nThe Chinese dormouse or Sichuan dormouse (\"Chaetocauda sichuanensis\") is a species of dormouse found in subalpine mixed forests in northern Sichuan, China, where it is known from Jiuzhaigou and Wanglang Nature Reserves. It is known only from two captured female specimens taken in the Wanglang Natural Reserve, and was first described by Wang Youzhi in 1985 and relisted by Corbet and Hill (1991, 1992) under a new genus as \"Chaetocauda sichuanensis\". It is currently the only member of the genus \"Chaetocauda\". The two specimens had head and body lengths of 90mm and 91mm and tail lengths of 92mm and 102mm, respectively. They weighed 24.5 and 36.0 g. It is nocturnal and arboreal, nesting in trees around 3 metres above the ground, and was found above an altitude of 2500m above sea level. It is classified as endangered by the IUCN as of the 2004 Red List due to its small, isolated habitat. \n\n\n"}
{"id": "49795988", "url": "https://en.wikipedia.org/wiki?curid=49795988", "title": "Chintaman Govind Pandit", "text": "Chintaman Govind Pandit\n\nChintaman Govind Pandit, (25 July 1895 – 7 September 1991) was an Indian virologist, writer and the founder director of the Indian Council of Medical Research. He secured his doctoral degree (PhD) from the University of London in 1922, worked as the director of King Institute of Preventive Medicine and Research, Chennai, before becoming the founder director of the Indian Council of Medical Research when the institution was established in 1948. After his superannuation in 1964, he was made the Emeritus Scientist of the Council of Scientific and Industrial Research (CSIR).\n\nPandit, besides writing several medical articles, authored two books, \"Indian Research Fund Association and Indian Council of Medical Research, 1911-1961; fifty years of progress\" and \"Nutrition in India\". He served as the president of the Indian Science Congress of 1991 and was an elected Fellow of the Indian National Science Academy (1939) and a founder Fellow of the National Academy of Medical Sciences (India).\n\nIn the 1943 Birthday Honours list, Patel was appointed an Officer of the Order of the British Empire (OBE). He received the fourth-highest Indian civilian honour, the Padma Shri, in 1956. The Government of India awarded him the third highest civilian honour of the Padma Bhushan, in 1964, for his contributions to science. After his death on 7 September 1991, the Indian Council of Medical Research instituted a distinguished scientist chair, \"Dr. C. G. Pandit National Chair\", in his honour.\n"}
{"id": "1044752", "url": "https://en.wikipedia.org/wiki?curid=1044752", "title": "De Phenomenis in Orbe Lunae", "text": "De Phenomenis in Orbe Lunae\n\nDe Phenomenis in Orbe Lunae is a 1612 book by Collegio Romano philosophy professor Giulio Cesare de Galla that describes emission of light by a stone. De Galla's inspiration came from Galileo's debate with Vincenzo Casciarolo regarding a \"lapis solaris,\" a stone that emitted light seemingly on its own. In \"De Phenomenis\" de Galla asserts that the stone was only able to emit light after the stone itself had calcified. It released \"a certain quantity of fire and light\" that it had absorbed just as water would be absorbed by a sponge.\n"}
{"id": "6894881", "url": "https://en.wikipedia.org/wiki?curid=6894881", "title": "De facto monopoly", "text": "De facto monopoly\n\nA \"de facto\" monopoly is a monopoly that was not created by government. It is most often used in contrast to \"de jure\" monopoly, which is one that is protected from competition by government action.\n\nIn a free market without government intervention this kind of monopoly is theoretically unobtainable for any extended amount of time. A \"de facto\" monopoly is only able to be achieved by providing a far demanded product at all times compared to the competition, and even then there would not be a 100% market share.\n\n"}
{"id": "61345", "url": "https://en.wikipedia.org/wiki?curid=61345", "title": "Documentary hypothesis", "text": "Documentary hypothesis\n\nThe documentary hypothesis (DH) is one of four models used to explain the origins and composition of the first five books of the Bible, as well as the most widespread and commonly agreed upon of the various hypotheses. The first five books of the Bible are Genesis, Exodus, Leviticus, Numbers, and Deuteronomy, called collectively the Torah or Pentateuch. Two other theories are the supplementary hypothesis and the fragmentary hypothesis. The fourth model would be some variation of the traditional ideas of authorship (see Mosaic authorship).\n\nAll three agree that the Torah is not a unified work from a single author (traditionally Moses) but is made up of sources combined over many centuries by many hands. They differ on the nature of these sources and how they were combined. According to the documentary hypothesis there were four sources, each originally a separate and independent book (a \"document\"), joined together at various points in time by a series of editors (\"redactors\"). Fragmentary hypotheses see the Torah as a collection of small fragments, and supplementary hypotheses as a single core document supplemented by fragments taken from many sources.\n\nA version of the documentary hypothesis, frequently identified with the German scholar Julius Wellhausen, was almost universally accepted for most of the 20th century, but the consensus has now collapsed. As a result, there has been a revival of interest in fragmentary and supplementary approaches, frequently in combination with each other and with a documentary model, making it difficult to classify contemporary theories as strictly one or another.\n\nModern scholars increasingly see the completed Torah as a product of the time of the Achaemenid Empire (probably 450–350 BCE), although some would place its production in the Hellenistic period (333–164 BCE) or even the Hasmonean dynasty (140–37 BCE). Of its constituent sources, Deuteronomy is generally dated between the 7th and 5th centuries; there is much discussion of the unity, extent, nature, and date of the Priestly material. Deuteronomy continues to be seen as having had a history separate from the first four books, and there is a growing recognition that Genesis developed apart from the Exodus stories until joined to it by the Priestly writer.\n\nThe Torah (or Pentateuch) is the collective name for the first five books of the Bible: Genesis, Exodus, Leviticus, Numbers, and Deuteronomy. According to tradition they were dictated by God to Moses, but when modern critical scholarship began to be applied to the Bible it was discovered that the Pentateuch was not the unified text one would expect from a single author. As a result, the Mosaic authorship of the Torah had been largely rejected by leading scholars by the 17th century, and the modern consensus is that it is the product of a long evolutionary process.\n\nIn the mid-18th century, some scholars started a critical study of doublets (parallel accounts of the same incidents), inconsistencies, and changes in style and vocabulary in the Torah. In 1780 Johann Eichhorn, building on the work of the French doctor and exegete Jean Astruc's \"Conjectures\" and others, formulated the \"older documentary hypothesis\": the idea that Genesis was composed by combining two identifiable sources, the Jehovist (\"J\"; also called the Yahwist) and the Elohist (\"E\"). These sources were subsequently found to run through the first four books of the Torah, and the number was later expanded to three when Wilhelm de Wette identified the Deuteronomist as an additional source found only in Deuteronomy (\"D\"). Later still the Elohist was split into Elohist and Priestly (\"P\") sources, increasing the number to four.\n\nThese documentary approaches were in competition with two other models, the fragmentary and the supplementary. The fragmentary hypothesis argued that fragments of varying lengths, rather than continuous documents, lay behind the Torah; this approach accounted for the Torah's diversity but could not account for its structural consistency, particularly regarding chronology. The supplementary hypothesis was better able to explain this unity: it maintained that the Torah was made up of a central core document, the Elohist, supplemented by fragments taken from many sources. The supplementary approach was dominant by the early 1860s, but it was challenged by an important book published by Hermann Hupfeld in 1853, who argued that the Pentateuch was made up of four documentary sources, the Priestly, Yahwist, and Elohist intertwined in Genesis-Exodus-Leviticus-Numbers, and the stand-alone source of Deuteronomy. At around the same period Karl Heinrich Graf argued that the Yahwist and Elohist were the earliest sources and the Priestly source the latest, while Wilhelm Vatke linked the four to an evolutionary framework, the Yahwist and Elohist to a time of primitive nature and fertility cults, the Deuteronomist to the ethical religion of the Hebrew prophets, and the Priestly source to a form of religion dominated by ritual, sacrifice and law.\n\n\"The table is based on that in Walter Houston's \"The Pentateuch\", with expansions as indicated. Note that the three hypotheses are not mutually exclusive.\"\n\nIn 1878 Julius Wellhausen published \"Geschichte Israels, Bd 1\" (\"History of Israel, Vol 1\"); the second edition he printed as \"Prolegomena zur Geschichte Israels\" (\"Prolegomena to the History of Israel\"), in 1883, and the work is better known under that name. (The second volume, a synthetic history titled \"Israelitische und jüdische Geschichte\" [\"Israelite and Jewish History\"], did not appear until 1894 and remains untranslated.) Crucially, this historical portrait was based upon two earlier works of his technical analysis: \"Die Composition des Hexateuchs\" (\"The Composition of the Hexateuch\") of 1876/77 and sections on the \"historical books\" (Judges–Kings) in his 1878 edition of Friedrich Bleek's \"Einleitung in das Alte Testament\" (\"Introduction to the Old Testament\").\n\nWellhausen's documentary hypothesis owed little to Wellhausen himself but was mainly the work of Hupfeld, Eduard Eugène Reuss, Graf, and others, who in turn had built on earlier scholarship. He accepted Hupfeld's four sources and, in agreement with Graf, placed the Priestly work last. J was the earliest document, a product of the 900s and the court of Solomon; E was from the 8th century BCE in the northern Kingdom of Israel, and had been combined by a redactor (editor) with J to form a document JE; D, the third source, was a product of the 7th century BC, by 620 BCE, during the reign of King Josiah; P (what Wellhausen first named \"Q\") was a product of the priest-and-temple dominated world of the 6th century; and the final redaction, when P was combined with JED to produce the Torah as we now know it.\n\nWellhausen's explanation of the formation of the Torah was also an explanation of the religious history of Israel. The Yahwist and Elohist described a primitive, spontaneous and personal world, in keeping with the earliest stage of Israel's history; in Deuteronomy he saw the influence of the prophets and the development of an ethical outlook, which he felt represented the pinnacle of Jewish religion; and the Priestly source reflected the rigid, ritualistic world of the priest-dominated post-exilic period. His work, notable for its detailed and wide-ranging scholarship and close argument, entrenched the \"new documentary hypothesis\" as the dominant explanation of Pentateuchal origins from the late 19th to the late 20th centuries.\n\nThe consensus around the documentary hypothesis collapsed in the last decades of the 20th century. The groundwork was laid with the investigation of the origins of the written sources in oral compositions, implying that the creators of J and E were collectors and editors and not authors and historians. Rolf Rendtorff (1925–2014), building on this insight, argued that the basis of the Pentateuch lay in short, independent narratives, gradually formed into larger units and brought together in two editorial phases, the first Deuteronomic, the second Priestly. This led to the current position which sees only two major sources in the Pentateuch, the Deuteronomist (confined to the Book of Deuteronomy) and the Priestly (confined to the books Genesis-Exodus-Leviticus-Numbers).\n\nThe majority of scholars today continue to recognise Deuteronomy as a source, with its origin in the law-code produced at the court of Josiah as described by De Wette, subsequently given a frame during the exile (the speeches and descriptions at the front and back of the code) to identify it as the words of Moses. Most scholars also agree that some form of Priestly source existed, although its extent, especially its end-point, is uncertain. The remainder is called collectively non-Priestly, a grouping which includes both pre-Priestly and post-Priestly material. The final Torah is increasingly seen as a product of the Persian period (539–333 BCE, probably 450–350 BCE), although some would place it somewhat later, in the Hellenistic (333–164 BCE) or even Hasmonean (140–37 BCE) periods – the latter remains a minority view, but the Elephantine papyri, the records of a Jewish colony in Egypt dating from the last quarter of the 5th century BCE, show no knowledge of a Torah or of an exodus. There is also a growing recognition that Genesis developed separately from Exodus-Leviticus-Numbers, and was joined to the story of Moses by the Priestly writer.\n\nA revised neo-documentary hypothesis still has adherents, especially in North America and Israel. This distinguishes sources by means of plot and continuity rather than stylistic and linguistic concerns, and does not tie them to stages in the evolution of Israel's religious history. Its resurrection of an E source is probably the element most often criticised by other scholars, as it is rarely distinguishable from the classical J source and European scholars have largely rejected it as fragmentary or non-existent.\n\nWellhausen used the sources of the Torah as evidence of changes in the history of Israelite religion as it moved (in his opinion) from free, simple and natural to fixed, formal and institutional. Modern scholars of Israel's religion have become much more circumspect in how they use the Old Testament, not least because many have concluded that the Bible is not a reliable witness to the religion of ancient Israel and Judah, representing instead the beliefs of only a small segment of the ancient Israelite community centred in Jerusalem and devoted to the exclusive worship of the god Yahweh.\n\n\n"}
{"id": "38956017", "url": "https://en.wikipedia.org/wiki?curid=38956017", "title": "Einstein Wrote Back", "text": "Einstein Wrote Back\n\nEinstein Wrote Back is a memoir by Canadian physicist John Moffat which documents his encounters with various other famous physicists, including Niels Bohr, Albert Einstein, Erwin Schrödinger, Fred Hoyle, Wolfgang Pauli, Paul Dirac, Abdus Salam, and J. Robert Oppenheimer, as well as his work at Imperial College London, Princeton University, CERN, and the University of Toronto.\n\nThe title of the book comes from a series of letters Moffat exchanged with Einstein early in his life, which inspired Moffat to continue studying physics.\n\n"}
{"id": "1926015", "url": "https://en.wikipedia.org/wiki?curid=1926015", "title": "Electron paramagnetic resonance", "text": "Electron paramagnetic resonance\n\nElectron paramagnetic resonance (EPR) or electron spin resonance (ESR) spectroscopy\nis a method for studying materials with unpaired electrons. The basic concepts of EPR are analogous to those of nuclear magnetic resonance (NMR), but it is electron spins that are excited instead of the spins of atomic nuclei. EPR spectroscopy is particularly useful for studying metal complexes or organic radicals.\nEPR was first observed in Kazan State University by Soviet physicist Yevgeny Zavoisky in 1944, and was developed independently at the same time by Brebis Bleaney at the University of Oxford.\n\nEvery electron has a magnetic moment and spin quantum number formula_1, with magnetic components formula_2 and formula_3. In the presence of an external magnetic field with strength formula_4, the electron's magnetic moment aligns itself either parallel (formula_3) or antiparallel (formula_2) to the field, each alignment having a specific energy due to the Zeeman effect:\n\nwhere\n\nTherefore, the separation between the lower and the upper state is formula_11 for unpaired free electrons. This equation implies (since both formula_8and formula_10 are constant) that the splitting of the energy levels is directly proportional to the magnetic field's strength, as shown in the diagram below.\nAn unpaired electron can move between the two energy levels by either absorbing or emitting a photon of energy formula_14 such that the resonance condition, formula_15, is obeyed. This leads to the fundamental equation of EPR spectroscopy: formula_16.\n\nExperimentally, this equation permits a large combination of frequency and magnetic field values, but the great majority of EPR measurements are made with microwaves in the 9000–10000 MHz (9–10 GHz) region, with fields corresponding to about 3500 G (0.35 T). Furthermore, EPR spectra can be generated by either varying the photon frequency incident on a sample while holding the magnetic field constant or doing the reverse. In practice, it is usually the frequency that is kept fixed. A collection of paramagnetic centers, such as free radicals, is exposed to microwaves at a fixed frequency. By increasing an external magnetic field, the gap between the formula_2 and formula_3 energy states is widened until it matches the energy of the microwaves, as represented by the double arrow in the diagram above. At this point the unpaired electrons can move between their two spin states. Since there typically are more electrons in the lower state, due to the Maxwell–Boltzmann distribution (see below), there is a net absorption of energy, and it is this absorption that is monitored and converted into a spectrum. The upper spectrum below is the simulated absorption for a system of free electrons in a varying magnetic field. The lower spectrum is the first derivative of the absorption spectrum. The latter is the most common way to record and publish continuous wave EPR spectra.\n\nFor the microwave frequency of 9388.2 MHz, the predicted resonance occurs at a magnetic field of about formula_19 = 0.3350 teslas = 3350 gausses.\n\nBecause of electron-nuclear mass differences, the magnetic moment of an electron is substantially larger than the corresponding quantity for any nucleus, so that a much higher electromagnetic frequency is needed to bring about a spin resonance with an electron than with a nucleus, at identical magnetic field strengths. For example, for the field of 3350 G shown at the right, spin resonance occurs near 9388.2 MHz for an electron compared to only about 14.3 MHz for H nuclei. (For NMR spectroscopy, the corresponding resonance equation is formula_20 where formula_21 and formula_22 depend on the nucleus under study.)\n\nAs previously mentioned an EPR spectrum is usually directly measured as the first derivative of the absorption. This is accomplished by using field modulation. A small additional oscillating magnetic field is applied to the external magnetic field at a typical frequency of 100 kHz. By detecting the peak to peak amplitude the first derivative of the absorption is measured. By using phase sensitive detection only signals with the same modulation (100 kHz) are detected. This results in higher signal to noise ratios. Note field modulation is unique to continuous wave EPR measurements and spectra resulting from pulsed experiments are presented as absorption profiles.\n\nIn practice, EPR samples consist of collections of many paramagnetic species, and not single isolated paramagnetic centers. If the population of radicals is in thermodynamic equilibrium, its statistical distribution is described by the Maxwell–Boltzmann equation:\n\nwhere formula_24 is the number of paramagnetic centers occupying the upper energy state, formula_25 is the Boltzmann constant, and formula_26 is the thermodynamic temperature. At 298 K, X-band microwave frequencies (formula_27 ≈ 9.75 GHz) give formula_28 ≈ 0.998, meaning that the upper energy level has a slightly smaller population than the lower one. Therefore, transitions from the lower to the higher level are more probable than the reverse, which is why there is a net absorption of energy.\n\nThe sensitivity of the EPR method (i.e., the minimal number of detectable spins formula_29) depends on the photon frequency formula_27 according to\n\nwhere formula_32 is a constant, formula_33 is the sample's volume, formula_34 is the unloaded quality factor of the microwave cavity (sample chamber), formula_35 is the cavity filling coefficient, and formula_36 is the microwave power in the spectrometer cavity. With formula_35 and formula_36 being constants, formula_29 ~ formula_40, i.e., formula_29 ~ formula_42, where formula_43 ≈ 1.5. In practice, formula_43 can change varying from 0.5 to 4.5 depending on spectrometer characteristics, resonance conditions, and sample size.\n\nA great sensitivity is therefore obtained with a low detection limit formula_29 and a large number of spins. Therefore, the required parameters are:\n\nIn real systems, electrons are normally not solitary, but are associated with one or more atoms. There are several important consequences of this:\n\nKnowledge of the \"g\"-factor can give information about a paramagnetic center's electronic structure. An unpaired electron responds not only to a spectrometer's applied magnetic field formula_47 but also to any local magnetic fields of atoms or molecules. The effective field formula_48 experienced by an electron is thus written\n\nwhere formula_50 includes the effects of local fields (formula_51 can be positive or negative). Therefore, the formula_52 resonance condition (above) is rewritten as follows:\n\nThe quantity formula_54 is denoted formula_55 and called simply the \"g\"-factor, so that the final resonance equation becomes\n\nThis last equation is used to determine formula_55 in an EPR experiment by measuring the field and the frequency at which resonance occurs. If formula_55 does not equal formula_8, the implication is that the ratio of the unpaired electron's spin magnetic moment to its angular momentum differs from the free-electron value. Since an electron's spin magnetic moment is constant (approximately the Bohr magneton), then the electron must have gained or lost angular momentum through spin–orbit coupling. Because the mechanisms of spin–orbit coupling are well understood, the magnitude of the change gives information about the nature of the atomic or molecular orbital containing the unpaired electron.\n\nIn general, the \"g\" factor is not a number but a second-rank tensor represented by 9 numbers arranged in a 3×3 matrix. The principal axes of this tensor are determined by the local fields, for example, by the local atomic arrangement around the unpaired spin in a solid or in a molecule. Choosing an appropriate coordinate system (say, \"x\",\"y\",\"z\") allows one to \"diagonalize\" this tensor, thereby reducing the maximal number of its components from 9 to 3: \"g\", \"g\" and \"g\". For a single spin experiencing only Zeeman interaction with an external magnetic field, the position of the EPR resonance is given by the expression \"gB\" + \"gB\" + \"gB\". Here \"B\", \"B\" and \"B\" are the components of the magnetic field vector in the coordinate system (\"x\",\"y\",\"z\"); their magnitudes change as the field is rotated, so does the frequency of the resonance. For a large ensemble of randomly oriented spins, the EPR spectrum consists of three peaks of characteristic shape at frequencies \"gB\", \"gB\" and \"gB\": the low-frequency peak is positive in first-derivative spectra, the high-frequency peak is negative, and the central peak is bipolar. Such situations are commonly observed in powders, and the spectra are therefore called \"powder-pattern spectra\". In crystals, the number of EPR lines is determined by the number of crystallographically equivalent orientations of the EPR spin (called \"EPR center\").\n\nSince the source of an EPR spectrum is a change in an electron's spin state, the EPR spectrum for a radical (S = 1/2 system) would consist of one line. Greater complexity arises because the spin couples with nearby nuclear spins. The magnitude of the coupling is proportional to the magnetic moment of the coupled nuclei and depends on the mechanism of the coupling. Coupling is mediated by two processes, dipolar (through space) and isotropic (through bond).\n\nThis coupling introduces additional energy states and, in turn, multi-lined spectra. In such cases, the spacing between the EPR spectral lines indicates the degree of interaction between the unpaired electron and the perturbing nuclei. The hyperfine coupling constant of a nucleus is directly related to the spectral line spacing and, in the simplest cases, is essentially the spacing itself. \n\nTwo common mechanisms by which electrons and nuclei interact are the Fermi contact interaction and by dipolar interaction. The former applies largely to the case of isotropic interactions (independent of sample orientation in a magnetic field) and the latter to the case of anisotropic interactions (spectra dependent on sample orientation in a magnetic field). Spin polarization is a third mechanism for interactions between an unpaired electron and a nuclear spin, being especially important for formula_60-electron organic radicals, such as the benzene radical anion. The symbols \"\"a\" or \"A\" are used for isotropic hyperfine coupling constants, while \"B\" is usually employed for anisotropic hyperfine coupling constants.\n\nIn many cases, the isotropic hyperfine splitting pattern for a radical freely tumbling in a solution (isotropic system) can be predicted.\n\n\n\nWhile it is easy to predict the number of lines, the reverse problem, unraveling a complex multi-line EPR spectrum and assigning the various spacings to specific nuclei, is more difficult.\n\nIn the often encountered case of \"I\" = 1/2 nuclei (e.g., H, F, P), the line intensities produced by a population of radicals, each possessing \"M\" equivalent nuclei, will follow Pascal's triangle. For example, the spectrum at the right shows that the three H nuclei of the CH radical give rise to 2\"MI\" + 1 = 2(3)(1/2) + 1 = 4 lines with a 1:3:3:1 ratio. The line spacing gives a hyperfine coupling constant of \"a\" = 23 \"G\" for each of the three H nuclei. Note again that the lines in this spectrum are \"first derivatives\" of absorptions.\nAs a second example, the methoxymethyl radical, HCOCH the OC\"H\" center will give an overall 1:2:1 EPR pattern, each component of which is further split by the three methoxy hydrogens into a 1:3:3:1 pattern to give a total of 3×4 = 12 lines, a triplet of quartets. A simulation of the observed EPR spectrum is shown at the right and agrees with the 12-line prediction and the expected line intensities. Note that the smaller coupling constant (smaller line spacing) is due to the three methoxy hydrogens, while the larger coupling constant (line spacing) is from the two hydrogens bonded directly to the carbon atom bearing the unpaired electron. It is often the case that coupling constants decrease in size with distance from a radical's unpaired electron, but there are some notable exceptions, such as the ethyl radical (CHCH).\n\nResonance linewidths are defined in terms of the magnetic induction \"B\" and its corresponding units, and are measured along the \"x\" axis of an EPR spectrum, from a line's center to a chosen reference point of the line. These defined widths are called halfwidths and possess some advantages: for asymmetric lines, values of left and right halfwidth can be given. The halfwidth formula_61 is the distance measured from the line's center to the point in which absorption value has half of maximal absorption value in the center of resonance line. First inclination width formula_62 is a distance from center of the line to the point of maximal absorption curve inclination. In practice, a full definition of linewidth is used. For symmetric lines, halfwidth formula_63, and full inclination width formula_64.\nEPR/ESR spectroscopy is used in various branches of science, such as biology, chemistry and physics, for the detection and identification of free radicals and paramagnetic centers such as F-centers. EPR is a sensitive, specific method for studying both radicals formed in chemical reactions and the reactions themselves. For example, when ice (solid HO) is decomposed by exposure to high-energy radiation, radicals such as H, OH, and HO are produced. Such radicals can be identified and studied by EPR. Organic and inorganic radicals can be detected in electrochemical systems and in materials exposed to UV light. In many cases, the reactions to make the radicals and the subsequent reactions of the radicals are of interest, while in other cases EPR is used to provide information on a radical's geometry and the orbital of the unpaired electron. EPR/ESR spectroscopy is also used in geology and archaeology as a dating tool. It can be applied to a wide range of materials such as carbonates, sulfates, phosphates, silica or other silicates.\n\nMedical and biological applications of EPR also exist. Although radicals are very reactive, and so do not normally occur in high concentrations in biology, special reagents have been developed to spin-label molecules of interest. These reagents are particularly useful in biological systems. Specially-designed nonreactive radical molecules can attach to specific sites in a biological cell, and EPR spectra can then give information on the environment of these so-called spin labels or spin probes. Spin-labeled fatty acids have been extensively used to study dynamic organisation of lipids in biological membranes, lipid-protein interactions and temperature of transition of gel to liquid crystalline phases.\n\nA type of dosimetry system has been designed for reference standards and routine use in medicine, based on EPR signals of radicals from irradiated polycrystalline α-alanine (the alanine deamination radical, the hydrogen abstraction radical, and the (CO(OH))=C(CH)NH radical) . This method is suitable for measuring gamma and x-rays, electrons, protons, and high-linear energy transfer (LET) radiation of doses in the 1 Gy to 100 kGy range.\n\nEPR/ESR spectroscopy can be applied only to systems in which the balance between radical decay and radical formation keeps the free radicals concentration above the detection limit of the spectrometer used. This can be a particularly severe problem in studying reactions in liquids. An alternative approach is to slow down reactions by studying samples held at cryogenic temperatures, such as 77 K (liquid nitrogen) or 4.2 K (liquid helium). An example of this work is the study of radical reactions in single crystals of amino acids exposed to x-rays, work that sometimes leads to activation energies and rate constants for radical reactions.\n\nThe study of radiation-induced free radicals in biological substances (for cancer research) poses the additional problem that tissue contains water, and water (due to its electric dipole moment) has a strong absorption band in the microwave region used in EPR spectrometers.\n\nEPR/ESR also has been used by archaeologists for the dating of teeth. Radiation damage over long periods of time creates free radicals in tooth enamel, which can then be examined by EPR and, after proper calibration, dated. Alternatively, material extracted from the teeth of people during dental procedures can be used to quantify their cumulative exposure to ionizing radiation. People exposed to radiation from the Chernobyl disaster have been examined by this method.\n\nRadiation-sterilized foods have been examined with EPR spectroscopy, the aim being to develop methods to determine whether a particular food sample has been irradiated and to what dose. \n\nEPR/ESR spectroscopy has been used to measure properties of crude oil, in particular asphaltene and vanadium content. EPR measurement of asphaltene content is a function of spin density and solvent polarity. Prior work dating to the 1960s has demonstrated the ability to measure vanadium content to sub-ppm levels.\n\nIn the field of quantum computing, pulsed EPR is used to control the state of electron spin qubits in materials such as diamond, silicon and gallium arsenide.\n\nHigh-field high-frequency EPR measurements are sometimes needed to detect subtle spectroscopic details. However, for many years the use of electromagnets to produce the needed fields above 1.5 T was impossible, due principally to limitations of traditional magnet materials. The first multifunctional millimeter EPR spectrometer with a superconducting solenoid was described in the early 1970s by Prof. Y. S. Lebedev's group (Russian Institute of Chemical Physics, Moscow) in collaboration with L. G. Oranski's group (Ukrainian Physics and Technics Institute, Donetsk), which began working in the Institute of Problems of Chemical Physics, Chernogolovka around 1975. Two decades later, a W-band EPR spectrometer was produced as a small commercial line by the German Bruker Company, initiating the expansion of W-band EPR techniques into medium-sized academic laboratories.\n\nThe EPR waveband is stipulated by the frequency or wavelength of a spectrometer's microwave source (see Table).\n\nEPR experiments often are conducted at X and, less commonly, Q bands, mainly due to the ready availability of the necessary microwave components (which originally were developed for radar applications). A second reason for widespread X and Q band measurements is that electromagnets can reliably generate fields up to about 1 tesla. However, the low spectral resolution over \"g\"-factor at these wavebands limits the study of paramagnetic centers with comparatively low anisotropic magnetic parameters. Measurements at formula_27 > 40 GHz, in the millimeter wavelength region, offer the following advantages:\n\nThis was demonstrated experimentally in the study of various biological, polymeric and model systems at D-band EPR.\n\nThe microwave bridge contains both the microwave source and the detector. Older spectrometers used a vacuum tube called a klystron to generate microwaves, but modern spectrometers use a Gunn diode. Immediately after the microwave source there is an isolator which serves to attenuate any reflections back to the source which would result in fluctuations in the microwave frequency. The microwave power from the source is then passed through a directional coupler which splits the microwave power into two paths, one directed towards the cavity and the other the reference arm. Along both paths there is a variable attenuator that facilitates the precise control of the flow of microwave power. This in turn allows for accurate control over the intensity of the microwaves subjected to the sample. On the reference arm, after the variable attenuator there is a phase shifter that sets a defined phase relationship between the reference and reflected signal which permits phase sensitive detection.\n\nMost EPR machines are reflection spectrometers, meaning that the detector should only be exposed to microwave radiation coming back from the cavity. This is achieved by the use of a device known as the circulator which directs the microwave radiation (from the branch that is heading towards the cavity) into the cavity. Reflected microwave radiation (after absorption by the sample) is then passed through the circulator towards the detector, ensuring it does not go back to the microwave source. The reference signal and reflected signal are combined and passed to the detector diode which converts the microwave power into an electrical current.\n\nAt low energies (less than 1 μW) the diode current is proportional to the microwave power and the detector is referred to as a square law detector. At higher power levels (greater than 1 mW) the diode current is proportional to the square root of the microwave power and the detector is called a linear detector. In order to obtain optimal sensitivity as well as quantitative information the diode should be operating within the linear region. To ensure the detector is operating at that level the reference arm serves to provide a \"bias\".\n\nIn an EPR machine the magnetic assembly includes the magnetic with a dedicated power supply as well as a field sensor or regulator such as a Hall probe. EPR machines use one of two types of magnet which is determined by the operating microwave frequency (which determine the range of magnetic field strengths required). The first is an electromagnet which are generally capable of generating field strengths of up to 1.5 T making them suitable for measurements using the Q-band frequency. In order to generate field strengths appropriate for W-band and higher frequency operation superconducting magnets are employed. The magnetic field is homogeneous across the sample volume and has a high stability at static field.\n\nThe microwave resonator is designed to enhance the microwave magnetic field at the sample in order to induce EPR transitions. It is a metal box with a rectangular or cylindrical shape that resonates with microwaves (like an organ pipe with sound waves). At the resonance frequency of the cavity microwaves remain inside the cavity and are not reflected back. Resonance means the cavity stores microwave energy and its ability to do this is given by the quality factor (\"Q\"), defined by the following equation:\n\nformula_69\n\nThe higher the value of Q the higher the sensitivity of the spectrometer. The energy dissipated is the energy lost in one microwave period. Energy may be lost to the side walls of the cavity as microwaves may generate currents which in turn generate heat. A consequence of resonance is the creation of a standing wave inside the cavity. Electromagnetic standing waves have their electric and magnetic field components exactly out of phase. This provides an advantage as the electric field provides nonresonant absorption of the microwaves, which in turn increases the dissipated energy and reduces Q. To achieve the largest signals and hence sensitivity the sample is positioned such that it lies within the magnetic field maximum and the electric field minimum. When the magnetic field strength is such that an absorption event occurs, the value of Q will be reduced due to the extra energy loss. This results in a change of impedance which serves to stop the cavity from being critically coupled. This means microwaves will now be reflected back to the detector (in the microwave bridge) where an EPR signal is detected.\n\nThe dynamics of electron spins are best studied with pulsed measurements. Microwave pulses typically 10–100 ns long are used to control the spins in the Bloch sphere. The spin–lattice relaxation time can be measured with an inversion recovery experiment.\n\nAs with pulsed NMR, the Hahn echo is central to many pulsed EPR experiments. A Hahn echo decay experiment can be used to measure the dephasing time, as shown in the animation below. The size of the echo is recorded for different spacings of the two pulses. This reveals the decoherence, which is not refocused by the formula_60 pulse. In simple cases, an exponential decay is measured, which is described by the formula_71 time.\n\nPulsed electron paramagnetic resonance could be advanced into electron nuclear double resonance spectroscopy (ENDOR), which utilizes waves in the radio frequencies. Since different nuclei with unpaired electrons respond to different wavelengths, radio frequencies are required at times. Since the results of the ENDOR gives the coupling resonance between the nuclei and the unpaired electron, the relationship between them can be determined.\n\n"}
{"id": "25249846", "url": "https://en.wikipedia.org/wiki?curid=25249846", "title": "Functional beverage", "text": "Functional beverage\n\nA functional beverage is a drink typically intended to convey a health benefit. Some include ingredients like herbs, vitamins, minerals, nootropics, amino acids, or additional raw fruit or vegetables.\n\nExamples of functional beverages include sports and performance drinks, energy drinks, ready to drink (RTD) teas, smart drinks (shine+), enhanced fruit drinks, soy beverages, and enhanced water.\n\nFunctional beverages have become popular among people who want specific health benefits from their foods and beverages. Both convenience and health have been identified as important factors in consumers' decision-making about food and beverage purchases. Functional drinks are advertised as having various health benefits. For example, some claim to improve heart health, immunity, digestion, and joint health, while others promote themselves as satiating and energy-boosting.\n\nThe functional beverage industry is a subsector of the functional food and non-alcoholic beverage industry. It is the fastest-growing sector of the industry, partially due to the maturity of the carbonated soft drink sector and heavy investments by major food and beverage companies. Another reason for the industry's growth may be the consumer-oriented market scheme whereby innovative ideas come from consumers. By 2008, in the U.S., the market share of functional beverages accounted for 48.9% of the non-alcoholic industry, which is worth $118.3 billion.\n\nIn 2006, functional beverage consumption per capita rose to 66.4 gallons, while the carbonated soft drink sector saw a decline in per-capita consumption to 50.4 gallons (decreased from an average per-capita consumption of 192.5 gallons in 2006).\n\nFunctional beverage industry players are generally categorized into four types: \n\nThe functional beverage industry encompasses a wide range of varieties targeting different health-related concerns. One trend has been toward hybrid drinks, which are marketed as having benefits like thirst-quenching ability, with daily dosages of vitamins or other nutrients. Another trend is the rise of probiotics, exemplified by Activia yogurt, marketed for intestinal and immune health. Other beverages, like , a carambola punch energy drink in the Function Drinks line, advertise improved memory and mental sharpness. Functional drinks marketed to children have also been developed, and received attention with Nestlé's Boost.\n\nA 2005 trend toward single-serve functional beverages was fueled by consumers' demands for convenience. According to Campbell's director of single-serve beverages, \"People know they will be seen when they are drinking single-serving beverages, so the package is critical.\" Drinks marketed toward weight loss, health, and beauty (like Nova the Essential Drink) account for a considerable market share. Lastly, \"energy-boosting\" functional beverage products, such as Red Bull and 5-Hour Energy, have been rated fastest in growth in the functional beverage market.\n\nThe functional beverage industry generally competes using four primary strategies:\n\nMarket segments of the functional beverage industry are divided mainly into four parts. Those include hydration; energy/rejuvenation; health and wellness; and weight management. Each segment has its own target market and consumers. Overlapping of target consumers does occur—not because of undefined market needs, but due to consumer acceptance of functional beverages.\n\nRecently, there has been an increase in the promotion of hydration drinks. Fowhich it marketed as an exclusive hydration drink, sold only in Neiman Marcus stores. Among the drink's ingredient list were antioxidant vitamins and fruit extracts, which the company claims \"hydrate the inner and outer layer of the skin\" and protect drinkers from free radicals.\n\nGatorade has also created several drinks marketed as hydration beverages with various health benefits. Its \"Thirst Quencher\" drinks, according to advertisements, each contain an \"excellent source\" of various vitamins:\n\nIn tandem with these adjustments, a low-calorie version called G2 was also reformulated. According to Gatorade, G2 now also provides:\n\nGatorade's new products are a good example of the first major strategy of competition, listed in the \"Market\" section above. By reformulating its products, Gatorade's goals were to promote their own new products as healthier, and to emphasize the healthy ingredients in the drinks.\n\nHighly caffeinated, often highly sweetened \"energy drinks\" have become popular on the beverage market in the United States, as well as globally, in the past decade. Consumer demand has helped generate a new generation of \"energy drink\" brands containing similar amounts of caffeine, calories, and sugar.\n\nVarious stimulants found in energy drinks include taurine, glucoronolactone, caffeine, B vitamins, guarana, ginseng, ginkgo biloba, L-carnitine, sugars, antioxidants, yerba maté, creatine, and milk thistle. Although these ingredients have been approved by the FDA, health experts still recommend that consumers read their energy drinks' labels, as these ingredients may not improve health.\n\n\"Health-conscious\" individuals are among the target consumers of many functional beverage companies. To target these individuals, many companies have introduced functional beverages which contain less sugar, and therefore less calories. For example, VitaminWater 10 contains only 10 calories per serving (25 calories for a 351mL bottle, with 7.5 grams of sugar). An entire bottle also contains 250% of the RDI of vitamin C, and 25% of the RDI of vitamins B, B, B, and B. VitaminWater 10 lowered its calorie content by using an all-natural sweetener (stevia) extracted from the \"Stevia rebaudiana\" plant. This also makes it possible for the company to advertise VitaminWater 10 as \"natural\".\n\nWith increased worries about obesity and its implications on health, combined with consumer demand for convenience goods, consumer demand has increased for easy weight loss methods that can be easily integrated into daily lifestyles. Functional beverages are striving to market themselves as such, by adding ingredients that are claimed to promote weight loss.\n\nFor example:\n\nSome investigators report slimming actions induced by chlorogenic acids from green coffee, but further investigations need to be performed.\n\nAs of 2008, based on dollar sales, the most popular functional beverages, in order, were:\n\nAccording to a 2006 article, the functional beverage market's consumer group is characterized as well-educated females aged 35–55, belonging to the upper middle, middle, and lower middle classes. This is thought to result from this group's perceptions that functional drinks produce positive health beliefs, as well as their relatively high disposable income. A 2002 article stated that within the energy and stimulant drink sector, young adults aged 18 to 34 are considered to be the main target market, as evidenced by high consumption rates. However, due to constant changes in attitudes about different types of functional beverages, these target markets could change.\n\nHealth experts are concerned about the increased consumption and popularity of functional beverages. Although these beverages may serve to hydrate the individual, they may not mitigate or even address today's major health issues, such as obesity, heart disease, and cancer. Most functional beverages are sweetened, and consumption of sweetened beverages is associated with higher levels of obesity and heart disease. Most of these drinks contain significant amounts of sugars and hence calories, which would add to discretionary and total caloric intake. As such, these ingredients pose health risks because of what they contain (sugar and caffeine) or what they replace in the diet (vitamin and mineral-rich foods).\n\nAnother set of concerns is that some functional beverages contain ingredients that have not been sufficiently studied for health benefits, safety, and dosage. At the same time, many functional beverages have higher levels of a certain ingredient, like caffeine—which, when consumed in large amounts, is associated with heart disease and cancer. \n\nMany functional drinks have high levels of sugar, even if they have other \"healthy\" ingredients. For example, a 20oz bottle of Glacéau's VitaminWater has been reported to contain approximately 33 g of sugar, which is similar to the sugar content of a can of Coca-Cola. This prompted The Coca-Cola Company to be sued for claiming that VitaminWater was a healthy beverage.\n\nGiven their sugar content, many functional beverages may not be as healthy an alternative as other commonly consumed beverages. In addition, the sugar content of such beverages promotes dental cavities amongst frequent users.\n\nIn some functional beverages, particularly energy drinks, the caffeine content can be up to 141 milligrams per serving, more than an average 8-ounce cup of coffee containing 133 mg of caffeine. There have been reports to Health Canada of adverse reactions involving energy drinks.\n\n\n"}
{"id": "34355881", "url": "https://en.wikipedia.org/wiki?curid=34355881", "title": "George V. Eleftheriades", "text": "George V. Eleftheriades\n\nGeorge V. Eleftheriades does pioneering research in the field of metamaterials. He has been endowed with a Canada Research Chair at the University of Toronto and is a professor in the Department of Computer and Electrical Engineering there. He has received notable awards for his achievements, is a fellow of the IEEE and the Royal Society of Canada.\n\nAlso, at the University of Toronto, he heads a group for research in novel electromagnetic materials. He has also contributed book chapters to several books on antennas and transmission line theory that utilize metamaterials, along with other novel concepts, and is co-editor of one book in the same field. Professor Eleftheriades is also the author and co-author of a significant volume of published research in peer reviewed journals.\n\nProfessor Eleftheriades was elected IEEE fellow \"for contributions to conception, analysis and fabrication of electromagnetic materials and their applications.\"\n\nHe received the 2008 IEEE Kiyo Tomiyasu Award, a Technical Field Award conferred by the IEEE Board of Directors.\n\n"}
{"id": "6107158", "url": "https://en.wikipedia.org/wiki?curid=6107158", "title": "Gyula Farkas (natural scientist)", "text": "Gyula Farkas (natural scientist)\n\nFarkas Gyula, or Julius Farkas (March 28, 1847 – December 27, 1930) was a Hungarian mathematician and physicist.\n\nHe attended the gymnasium at Győr (Raab), and studied law and physics at Pest. After teaching in a secondary school at Székesfehérvár (Stuhlweissenburg), Farkas became in succession principal of the normal school at Pápa, privat-docent (1881) of mathematics at the University of Budapest, and professor of physics (1888) at Franz Joseph University of Kolozsvár (Klausenburg). He worked here up to 1915, when he retired and moved to Budapest.\n\nThe Hungarian Academy of Science elected him corresponding member May 6, 1898. He made a contribution to linear algebra with Farkas' lemma, which is named after him for his derivation of it.\n\nHis principal writings are embodied in the reports of the Academy of Science of Paris (1878–1884)\n\nHis separately published works are:\n\n\n"}
{"id": "4051863", "url": "https://en.wikipedia.org/wiki?curid=4051863", "title": "H6N2", "text": "H6N2\n\nH6N2 is an avian influenza virus with two forms: one has a low and the other a high pathogenicity. Avian derived from the Latin word \"avis\" for \"bird.\" It can cause a serious problem for poultry, and also infects ducks as well. H6N2 subtype is considered to be a non-pathogenic chicken virus, the host still unknown, but could strain from feral animals, and/or aquatic bird reservoirs. H6N2 along with H6N6 are viruses that are found to replicate in mice without preadaptation, and some have acquired the ability to bind to human-like receptors. Genetic markers for H6N2 include 22-amino acid stalk deletion in neuraminidase (NA) protein gene, increased N-glycosylation, and a D144 mutation of the Haemagglutinin (HA) protein gene. Transmission of avian influenza viruses from wild aquatic birds to domestic birds usually cause subclinical infections, and occasionally, respiratory disease and drops in egg production. Some histological features presented in chicken infected with H6N2 are fibrinous yolk peritonitis, salpingitis, oophoritis, nephritis, along with swollen kidneys as well.\n\n"}
{"id": "56734151", "url": "https://en.wikipedia.org/wiki?curid=56734151", "title": "HR 2562 b", "text": "HR 2562 b\n\nHR 2562b is a substellar companion of debris disk host star HR 2562. Initially categorised as brown dwarf, its exact mass is unknown, and is thought to be 30 ± 15 Jupiter masses, and its luminosity is about two one-thousandths of a percent of a solar luminosity. If classified as a brown dwarf, its spectral type of L7±3. It was first observed in 2016 using the Gemini Planet Imager. \n\nAccording to NASA Exoplanet Archive, with a mass of , it is listed as the most massive planet.\n"}
{"id": "39908986", "url": "https://en.wikipedia.org/wiki?curid=39908986", "title": "Ingemar Lundquist", "text": "Ingemar Lundquist\n\nIngemar Henry Lundquist (born in Stockholm, Sweden, October 19, 1921, died in Carmel Valley Village, California, February 25, 2007) was a prolific inventor and mechanical engineer.\n\nLundquist graduated from the Stockholm Institute of Technology in 1945 with a mechanical engineering degree. He migrated to the United States in 1948 and became an American citizen in 1950.\n\nHe worked for various medical technology companies in the San Francisco Bay Area, including Advanced Cardiovascular Systems and E.P. Technologies.\n\nLundquist had hundreds of inventions, typically working in his garage or basement. He held more than a hundred patents. His inventions included over the wire balloon angioplasty, T.U.N.A., and somnoplasty. He also worked on cardiac stem-cell therapy.\n\n"}
{"id": "22732293", "url": "https://en.wikipedia.org/wiki?curid=22732293", "title": "Internal model (motor control)", "text": "Internal model (motor control)\n\nIn the subject area of control theory, an internal model is a process that simulates the response of the system in order to estimate the outcome of a system disturbance. The internal model principle was first articulated in 1976 by B. A. Francis and W. M. Wonham as an explicit formulation of the Conant and Ashby good regulator theorem.\n\nThe internal model theory of motor control argues that the motor system is controlled by the constant interactions of the “plant” and the “controller.” The plant is the body part being controlled, while the internal model itself is considered part of the controller. Information from the controller, such as information from the central nervous system (CNS), feedback information, and the efference copy, is sent to the plant which moves accordingly.\n\nInternal models can be controlled through either feed-forward or feedback control. Feed-forward control computes its input into a system using only the current state and its model of the system. It does not use feedback, so it cannot correct for errors in its control. In feedback control, some of the output of the system can be fed back into the system’s input, and the system is then able to make adjustments or compensate for errors from its desired output. Two primary types of internal models have been proposed: forward models and inverse models. In simulations, models can be combined together to solve more complex movement tasks.\n\nIn their simplest form, forward models take the input of a motor command to the “plant” and output a predicted position of the body.\n\nThe motor command input to the forward model can be an efference copy, as seen in Figure 1. The output from that forward model, the predicted position of the body, is then compared with the actual position of the body. The actual and predicted position of the body may differ due to noise introduced into the system by either internal (e.g. body sensors are not perfect, sensory noise) or external (e.g. unpredictable forces from outside the body) sources. If the actual and predicted body positions differ, the difference can be fed back as an input into the entire system again so that an adjusted set of motor commands can be formed to create a more accurate movement.\n\nInverse models use the desired and actual position of the body as inputs to estimate the necessary motor commands which would transform the current position into the desired one. For example, in an arm reaching task, the desired position (or a trajectory of consecutive positions) of the arm is input into the postulated inverse model, and the inverse model generates the motor commands needed to control the arm and bring it into this desired configuration (Figure 2). Inverse internal models are also in close connection with the uncontrolled manifold hypothesis (UCM), see also here.\n\nTheoretical work has shown that in models of motor control, when inverse models are used in combination with a forward model, the efference copy of the motor command output from the inverse model can be used as an input to a forward model for further predictions. For example, if, in addition to reaching with the arm, the hand must be controlled to grab an object, an efference copy of the arm motor command can be input into a forward model to estimate the arm's predicted trajectory. With this information, the controller can then generate the appropriate motor command telling the hand to grab the object. It has been proposed that if they exist, this combination of inverse and forward models would allow the CNS to take a desired action (reach with the arm), accurately control the reach and then accurately control the hand to grip an object.\n\nWith the assumption that new models can be acquired and pre-existing models can be updated, the efference copy is important for the adaptive control of a movement task. Throughout the duration of a motor task, an efference copy is fed into a forward model known as a dynamics predictor whose output allows prediction of the motor output. When applying adaptive control theory techniques to motor control, efference copy is used in indirect control schemes as the input to the reference model.\n\nA wide range of scientists contribute to progress on the internal model hypothesis. Michael I. Jordan, Emmanuel Todorov and \nDaniel Wolpert contributed significantly to the mathematical formalization. Sandro Mussa-Ivaldi, Mitsuo Kawato, Claude Ghez, Reza Shadmehr, Randy Flanagan and Konrad Kording contributed with numerous behavioral experiments. The DIVA model of speech production developed by Frank H. Guenther and colleagues uses combined forward and inverse models to produce auditory trajectories with simulated speech articulators. Two interesting inverse internal models for the control of speech production were developed by Iaroslav Blagouchine & Eric Moreau. Both models combine the optimum principles and the equilibrium-point hypothesis (motor commands λ are taken as coordinates of the internal space). The input motor command λ is found by minimizing the length of the path traveled in the internal space, either under the acoustical constraint (the first model), or under the both acoustical and mechanical constraints (the second model). The acoustical constraint is related to the quality of the produced speech (measured in terms of formants), while the mechanical one is related to the stiffness of the tongue's body. The first model, in which the stiffness remains uncontrolled, is in agreement with the standard UCM hypothesis. In contrast, the second optimum internal model, in which the stiffness is prescribed, displays the good variability of speech (at least, in the reasonable range of stiffness) and is in agreement with the more recent versions of the uncontrolled manifold hypothesis (UCM). There is also a rich clinical literature on internal models including work from John Krakauer, Pietro Mazzoni, Maurice A. Smith, Kurt Thoroughman, Joern Diedrichsen, and Amy Bastian.\n"}
{"id": "9775965", "url": "https://en.wikipedia.org/wiki?curid=9775965", "title": "Jörundur Svavarsson", "text": "Jörundur Svavarsson\n\nJörundur Svavarsson is a professor in marine biology at the University of Iceland. His fields of research are marine invertebrates, marine biodiversity and ecotoxicology. According to Web of Science Prof. Svavarsson has published 49 papers in peer-reviewed journals, with 13 or them being cited more than 11 times. He is currently the head of the department of Biology at University of Iceland. Professor Svavarsson has spearheaded several cultural and historic projects, one of which was the establishment of an exhibition dedicated to the explorations of Jean-Baptiste Charcot whose ship Pourquoi pas ? was lost on the west coast of Iceland in 1936. In 2012 Professor Svavarsson was awarded the title of \"Chevalier des Palmes académiques\" for this work, a title which was created by Napoleon in 1808.\n\nThe most widely referred to are:\n\n"}
{"id": "248377", "url": "https://en.wikipedia.org/wiki?curid=248377", "title": "List of fictional dinosaurs", "text": "List of fictional dinosaurs\n\nThis list of fictional dinosaurs is subsidiary to the list of fictional animals and is a collection of various notable dinosaur characters that appear in various works of fiction. It is limited to well-referenced examples of dinosaurs in literature, film, television, comics, animation, video games and mythology, and applies only to non-avian dinosaur species that lived from the Triassic Period until the end of the Cretaceous.\n\n\n"}
{"id": "22688031", "url": "https://en.wikipedia.org/wiki?curid=22688031", "title": "List of gemstones in the Bible", "text": "List of gemstones in the Bible\n\nGemstones are referenced in multiple books of the Bible, particularly in the Old Testament and in the Book of Revelation. We know something about their origins, but since the 4th Century AD there has been considerable confusion about precise identification of all the stones.\n\nThe Hebrews obtained their gemstones from the Middle East, India, and Egypt. At the time of the Exodus, Ancient Egypt was flooded with riches, and the Israelites on leaving the land possessed themselves of many gemstones, according to the commandment of God (Book of Exodus, iii, 22; xii, 35-36). Later when they were settled in Palestine they could easily obtain stones from the merchant caravans travelling from Babylonia or Persia to Egypt and those from Saba and Raamah to Tyre (Book of Ezekiel, xxvii, 22). KSolomon even equipped a fleet which returned from Ophir laden with gems (Books of Kings, x, 11).\n\nThe gemstones of the Bible are mentioned in connection with the breastplate of the High Priest of Israel (Book of Exodus, xxviii, 17-20; xxxix, 10-13), the treasure of the King of Tyre (Book of Ezekiel, xxviii, 13), and the foundations of the New Jerusalem (Book of Tobit, xiii, 16-17, in the Greek text, and more fully, Book of Revelation, xxi, 18-21). The twelve stones of the breastplate and the two stones of the shoulder-ornaments were considered by the Jews to be the most\nprecious. Both Book of Ezekiel, xxviii, 13, and Book of Revelation, xxi, 18-21, are patterned after the model of the rational and further allude to the Twelve Tribes of Israel.\n\nThe stones' composition were the objects of a considerable amount of literature from the fourth century. That such a literature should have arisen is of itself convincing proof that the identification of the stones was no easy problem to solve. At the time of the Septuagint translation, the stones to which the Hebrew names apply could no longer be identified, and the translators rendered the same Hebrew name by different Greek words. So also did Josephus who, however, claimed he had seen the actual stones. This, coupled with the fact that the late Biblical lists, although visibly depending on that of Exodus, exhibit here and there notable changes, makes the task of identifying the stones a difficult one.\n\nThe ancients did not classify their gemstones by analyzing their composition and crystalline forms: names were given in accordance with their colour, use, or their country of origin. Therefore, stones of the same or nearly the same colour, but of different composition or crystalline form, bear identical names.\n\nAnother problem is nomenclature; names having changed in the course of time: thus the ancient chrysolite is our topaz, the sapphire is our lazuli, etc. However, we know most of the stones were precious in Egypt, Assyria, and Babylonia. Owing to the neighbourhood and to the influence of these countries on Palestine, it is highly probable that the score of substances referred to in the Bible as \"desirable stones\" (Is., liv, 12) must be contained in the\nfairly long list of the precious and ornamental stones of the Assyro-Babylonians and the Egyptians.\n\nThe list comprises comparative etymological origins and referential locations for each stone within the Bible. Where relevant, additional information concerning individual stones has also been included.\n\nAgate, Heb. \"shbw\"; Sept. \"achates\"; Vulg. \"achates\" (Ex., xxviii, 19; xxxix, 12, in Heb. and Vulg.; also Ezech., xxviii, 13, in Sept.). - This is the second stone of the third row of the rational, where it very probably represented the tribe of Asher. The etymological derivation of the Hebrew word is unclear, but the stone has generally been acknowledged to be the agate. The Hebraic derivation derives \"shbw\" from \"shbb\" \"to flame\"; it may also be related to Saba (\"shba\"), caravans having brought the stone to Palestine. The Greek and Latin names are taken from the river Achates, the modern Dirillo, in \"Sicily\", where this stone was first found (Theophrastus, \"\"De lapid\".\", 38; Pliny, \"Hist. nat.\", XXXVII, liv).\n\nThe stone belongs to the silex family (chalcedony species) and is formed by deposits of\nsiliceous beds in hollows of rocks. This mode of formation results in the bands of various colours which it contains. Its conchoidal cleavage makes it susceptible to a highly polished state.\n\nVarious medicinal powers were attributed until far into the Middle Ages. Agate was supposed to void the toxicity of all poisons and counteract the infection of contagious diseases; if held in the hand or in the mouth it was believed to alleviate fever. Within mythology the eagle placed an agate in its nest to guard its young against the bite of venomous animals and the red agate was credited with the power of sharpening vision.\n\nAt present agate and onyx differ only in the manner in which the stone is cut; if it is cut to show the layers of colour, it is called agate; if cut parallel to the lines, onyx. Formerly an agate that was banded with well-defined colours was the onyx. The banded agate is used for the manufacture of cameos.\n\nAmethyst, Heb. \"ahlmh\"; Sept. \"amethystos\", also Apoc., xxi, 20, where it is the twelfth and last stone of the foundation of the New Jerusalem. It is the third stone in the third row of the rational, representing the tribe of Issachar (Ex., xxviii, 19; xxxix, 12); the Septuagint enumerates it among the riches of the King of Tyre (Ezech., xxviii, 13). The Greek name alludes to the popular belief that amethyst prevented intoxication; hence drinking vessels were made of amethyst for festivities, and carousers wore amulets made of it to counteract the action of wine. Abenesra and Kimchi explain the Hebrew \"ahlmh\" in an analogous manner, deriving it from \"hlm\", to dream; \"hlm\" in its first meaning signifies \"to be hard\". A consensus exists regarding the accuracy of the translation among the various versions; Josephus (Ant. Jud., III, vii, 6) also has \"amethyst\"; the Targum of Onkelos and the Syriac Version have \"calf's eye\", indicating the colour.\n\nThe amethyst is a brilliant transparent stone of a purple colour and varying in shade from violet purple to rose. There are two kinds of amethysts: the oriental amethyst, a species of sapphire which is very hard (cf. Heb.,\"hlm\"), and when colourless is almost indistinguishable from the diamond. The occidental amethyst is of the silex family and different in composition from the oriental stone. But the identity of names is accounted for by the identity of colour. The occidental amethyst is easily engraved and is found in a variety of sizes. Its shape is different from the round pebble to the hexagonal, pyramid-capped crystal.\n\nBeryl, Heb. \"yhlm\"; Sept. \"beryllos\"; Vulg. \"beryllus\" occupied the third place of the second row and in the breastplate and was understood to represent Nephtali (Ex., xxviii, 19; xxxix, 13). According to the Septuagint it was the second of the fourth row, and third of the fourth according to the Vulgate. Ezech., xxviii, 13, mentions it in the third place and it is also cited in the Greek text of Tob., xiii, 17, but is missing in the Vulgate; Apoc., xxi, 20, gives it as the eighth stone of the foundation of the New Jerusalem.\n\nThe etymological debate indicates a difference of opinion regarding the exact Hebrew correlative of this word. The best supported is \"yhlm\", though \"shhm\" is also probable. \"shpht\" has also been suggested, but with little proof. Consequently, the Hebrew \"shpht\" must correspond to jasper, Gr. \"iaspis\" and Lat. \"jaspis\". This mistaken idea probably arose from the supposition that the translated words originally occupied the same position in the original. Comparative analysis of the Greek and Latin translations demonstrates this is not the case; in the Vulgate, jasper is in the same position as \"yshpht\", whereas the Greek \"beryllos\" does not correspond to the Latin \"beryllus\".\n\nThe same may have happened regarding the translation of the Hebrew into Greek, especially because the old manner of writing the two words \"yshlm\" and \"shlm\" might be easily confused. Josephus is not reliable in this instance as he most likely quoted from memory; the position of the words being at variance in his two lists (Bell. Jud., V, v, 7; Ant. Jud., III, vii).\n\nTherefore, the ultimate analysis is limited to the two words \"yshlm\" and \"shlm\". By comparing various texts of the Vulgate - the Greek is very inconsistent - we find that \"shlm\"\nalways translated to onyx. This alone seems sufficient to support the opinion that beryl corresponds to the Heb. \"yhlm\". That beryl was among the stones of the rational appears beyond doubt because all translations mention it and with the etymology giving us no special help, by elimination; we come to the generally accepted conclusion that beryl and \"yhlm\" stand for each other.\n\nBeryl is a stone composed of silica, alumina, and glucina with beryl and emerald being of the same species. The difference between beryl, aquamarine, and emerald is determined by the colouring and the peculiar shade of each. Beryl, though sometimes white, is usually of a light blue bordering on a yellowish green; emerald is more transparent and of a finer hue than beryl. As a gem, it is considered more beautiful, and therefore more expensive - aqua marine is a beautiful sea-green variety.\n\nEmerald derives its colour from a small quantity of chromium oxide; beryl and aqua marine from a small quantity of iron oxide. Beryl occurs in the shape of either a pebble or of an hexagonal prism. It is found in metamorphic limestone, slate, mica schist, gneiss and granite. In ancient times it was mined in Upper Egypt and is still found in the mica slate of Mt. Zaborah. The largest beryls known have been found in Acworth and Grafton, New Hampshire, and in Royalston, Massachusetts, United States of America; one weighs 2900 lb. and measures 51 inches in length by 32 inches by 22.\n\nAccording to John Aubrey in \"Miscellanies\" beryl has also been employed for mystical and cabalistic practices.\n\nCarbuncle, Heb., \"gphr\"; Sept. \"anthrax\" (Ex., xxviii, 18;\nxxxix, 11; Ezech., xxviii, 13; omitted in Ezech., xxvii, 16); Vulg., \"carbunculus\" (Ex., xxviii, 18; xxxix, 11; Ezech., xxviii, 13), \"gemma\" (Ezech., xxvii, 16). The carbuncle was the first stone of the second row of the rational and it represented Juda, and is also the eighth stone mentioned of the riches of the King of Tyre (Ezech., xxviii, 13). An imported object, not a native product, (Ezech., xxvii, 16); it is perhaps the third stone of the foundation of the celestial city (Apoc., xxi, 19).\n\nThe ancient authors are not in accordance on the precise nature of the carbuncle stone. It probably corresponded to the \"anthrax\" of Theophrastus (De lap., 18), the \"carbunculus\" of Pliny (Hist. nat., XXXVII, xxv), the \"charchedonius\" of Petronius, and the \"ardjouani\" of the Arabs. If so, it is a red glittering stone, probably the Oriental ruby, though the appellation may have been applied to a variety of other red gems. Theophrastus describes it as: \"Its colour is red and of such a kind that when it is held against the sun it resembles a burning coal.\" This description fits well with the Oriental ruby. He also relates that the most perfect carbuncles were brought from Carthage, Marseilles, Egypt, and the neighbourhood of Siena.\n\nCarbuncles were named differently according to their places of origin. Pliny (Hist. nat., XXXVII, xxv) cites the lithizontes, or Indian carbuncles, the amethystizontes, the colour of which resembled amethyst, and sitites. Carbuncle was therefore most probably a generic name which applied to several stones.\n\nCarnelian, Heb. \"arm\", to be red, especially \"red blooded\"; Sept. and Apoc. \"sardion\"; Vulg. \"sardius\"; the first stone of the breastplate (Ex., xxviii, 17; xxxix, 10) representing Ruben; also the first among the stones of the King of Tyre (Ezech., xxviii, 13); the sixth foundation stone of the celestial city (Apoc., xxi, 19). Also found in Noahs story is the unproven that the dove Noah sent down to the ground was actually a garnet used to light the ground.\n\nThe word \"sardion\" has sometimes been called \"sardonyx\". This is a mistake, for the same word is equivalent to carnelian in Theophrastus (De lap., 55) and Pliny (Hist. nat., XXXVII, xxxi), who derive the name from that of the city of Sardes where, they claim, it was first found. The carnelian is a siliceous stone and a species of chalcedony. Its colour is a flesh-hued red, varying from the palest flesh-colour to a deep blood-red. It is of a conchoidal structure. Normally its colour is without clouds or veins; but sometimes delicate veins of extremely light red or white are found arranged much like the rings of an agate. Carnelian is used for rings and seals. The finest carnelians are found in the East Indies.\n\nChalcedony, Apoc., xxi, 19, \"chalkedon\"; Vulg. \"chalcedonius\", the third foundation stone of the celestial Jerusalem. The view that the writing \"chalkedon\" is an error and that it should be \"charkedon\" (the carbuncle) is not without some reason. However, the other eleven stones correspond to a stone in the rational and this is the only exception. The ancients very often confounded the names of these two stones. Chalcedony is a siliceous stone. Its name is supposed to derive from Chalcedon, in Bithynia, where the ancients obtained the stone from. It is a species of agate and bears various names according to its colour. Chalcedony is usually made up of concentric circles of various colours and the most valuable of these stones are found in the East Indies. The gem is used for rings, seals and, in the East; drinking vessels.\n\nChodchod, \"kdkd\" (Is., liv, 12; Ezech., xxvii, 16); Sept.\"iaspis\" (Is., liv, 12), \"chorchor\" (Ezech., xxvii, 16); Vulg.\"jaspis\" (Is., liv, 12), \"chodchod\" (Ezech., xvii, 16). This word is used only twice in the Bible. Chodchod is generally identified with the\nOriental ruby. The translation of the word in Is. both by the Septuagint and the Vulgate is \"jasper\"; in Ezech. the word is merely transliterated; the Greek \"chorchor\" is explained by considering how easy it is to mistake a resh for a daleth.\n\n\"What chodchod signifies\", says St. Jerome, \"I have until now not been able to find\" (Comment. in Ezech., xxvii, 16, in P. L., XXV, 255). In Is. he follows the Septuagint and translates chodchod by \"jaspis\". The word is probably derived from \"phyr\", \"to throw fire\"; the stone was therefore brilliant and very likely red. This supposition is strengthened by the fact that the Arabic word \"kadzkadzat\", evidently derived from the same stem as chodchod, designates a bright red. It was therefore a kind of ruby, likely the Oriental ruby, perhaps also the carbuncle (see above).\n\nChrysolite, Heb. \"trshysh\" (Ex., xxviii, 20; xxxix, 13; Ezech., i, 16; x, 9; xxviii, 13; Cant., v, 14; Dan., x, 6); Sept., \"chrysolithos\" (Ex., xxviii, 20; xxxix, 13; Ezech., xxviii, 13); \"tharsis\" (Cant., v, 14; Dan., x, 6); \"tharseis\" (Ezech., 1, 16; x, 9); Vulg. \"chrysolithus\" (Ex., xxviii, 20; xxxix, 13; Ezech., x, 9; xxviii, 13; Dan., x, 6),\n\"hyacinthus\" (Cant., v, 14); \"quasi visio maris\" (Ezech., i, 16); Apoc., xxi, 20, \"chrysolithos\"; Vulg. \"chrysolithus\". This is the tenth stone of the rational, representing the tribe of Zebulun; it stands fourth in the enumeration of Ezech., xxviii, 13, and is given as the seventh foundation stone of the celestial city in Apoc., xxi, 20.\n\nNone of the Hebrew texts give any hint as to the nature of this stone. However, since the Septuagint repeatedly translates the Hebrew word by \"chrysolithos\", except where it merely transliterates it, and in Ezech., x, 9, since, moreover, the Vulgate follows this translation with very few exceptions, and Aquila, Josephus, and St. Epiphanius agree in their rendering, it can be assumed that the \"chrysolite\" of the ancients equates to our topaz.\n\nThe word \"tharsis\" very likely points to the origin of the gem (Tarshish). The modern chrysolite is a green oblong hexagonal prism of unequal sides terminated by two triangular pyramids. Topaz, or ancient chrysolite, is an octangular prism of an orange-yellow colour; it is composed of alumina, silica, hydrofluoric acid, and iron. it is found in Ceylon, Arabia, and Egypt. Several species were reported to exist (Pliny, \"Hist. nat.\", XXXVII, xlv) and during the Middle Ages it was believed to possess the power of relieving anxiety at night, driving away devils and to be an excellent cure for eye diseases.\n\nChrysophrasus, Greek \"chrysoprasos\", the tenth foundation stone of the celestial Jerusalem (Apoc., xxi, 20). This is perhaps the agate of Ex., xxviii, 20, and xxxix, 13, since the chrysoprasus was not very well known among the ancients. It is a type of green agate, composed mostly of silica and a small percentage of nickel.\n\nCoral, Heb. \"ramwt\" (Job, xxviii, 18; Prov., xxiv, 7; Ezech., xxvii, 16); Sept. \"meteora\", \"ramoth\"; Vulg. \"excelsa\", \"sericum\". The Hebrew word seems to derive from \"tas\", \"to be high\", probably pertaining to a tree. Another possibility is that the name originates from a strange country, as did the coral itself. It is apparent that the ancient\nversions have been prone to mis-interpretation. In one instance they even went so far as to\nsimply transliterate the Hebrew word.\n\nIn Ezech., xxvii, 16, coral is mentioned as one of the articles brought by the Syrians to Tyre. The Phoenicians mounted beads of coral on collars and garments. These corals were obtained by Babylonian pearl-flshers in the Red Sea and the Indian Ocean. The Hebrews apparently made very little use of this substance, and it is seldom mentioned in their writings. This also explains the difficulty experienced in scriptural translation.\n\nGesenius (Thesaurus, p. 1113) translates \"phnynys\" (Job, xxviii, 18; Prov., iii, 15; viii, 11; xx, 15; xxxi, 10; Lam., iv, 7) as \"red coral\". However, pearl has also been interpreted to be the meaning in these passages. The coral referred to in the Bible is the precious coral (\"corallum rubrum\"), the formation of which is well known. It is a calcareous secretion of certain polyps resulting in a tree-like formation. Presently coral is found in the Mediterranean, the northern coast of Africa furnishing the dark red, Sardinia the yellow or salmon-coloured, and the coast of Italy the rose-pink coral. One of the greatest coral-fisheries of the present day is Torre del Greco, near Naples.\n\nCrystal, Heb. \"ghbsh\" (Job, xxviii, 18), \"qrh\" (Ezech, i, 22): both words signify a glassy substance; Sept. \"gabis\"; Vulg. \"eminentia\" (Job, xxviii, 18); \"krystallos\", \"crystallus\" (Ezech., i, 22). Crystal is a transparent mineral resembling glass, most probably a variety of quartz. Job places it in the same category with gold, onyx, sapphire, glass, coral, topaz, etc. The Targum renders the \"qrt\" of\nEzech. as \"ice\"; the other versions translate it as \"crystal\". Crystal is again mentioned in Apoc., iv, 6; xxi, 11; xxii, 1. In Ps. cxlvii, 17, and Ecclus., xliii, 22, there can be no question that ice is indicated. The word \"zkwkyh\", Job, xxviii, 17, which can be translated as crystal, means glass.\n\nDiamond, Heb. \"shmyr\"; Sept. \"adamantinos\"; Vulg. \"adamas\", \"adamantinus\" (Ezech., iii, 9; Zach., vii, 12; Jer, xvii 1). Whether or not this stone is really diamond cannot be established. Many passages in Holy Scriptures point to the qualities of diamond, in particular its hardness (Ezech., iii, 9; Zach., vii, 12; Jer., xvii, 1). In the last citation\nJeremiah informs us of a diamond usage which is much the same as its usage today: \"The sin of Juda is written with a pen of iron, with the point of a diamond\". However, although diamond is used to engrave hard substances, other stones can serve the same purpose.\n\nThe Septuagint omits the passages of Ezech. and Zach., while the first five verses of Jer., xvii, are missing in the Cod. Vaticanus and Alexandrinus, but are found in the Complutensian edition and in the Syriac and Arabic Versions. Despite the qualities mentioned in the Bible, the stone referred to may be the limpid corindon; which exhibits the same qualities, and is used in India for the same purposes as the diamond.\n\nDiamond was not very well known among the ancients; and if we add to this the etymological similarity between the words \"smiris\", the Egyptian \"asmir\", \"emery\", a species of corindon used to polish gemstones, and \"shmyr\", the Hebrew word supposed to mean diamond; the conclusion to be drawn is that limpid corindon was intended.\n\nAben-Esra and Abarbanel translate \"yhlm\" as \"diamond\"; but \"yhlm\" was demonstrated above to be beryl. Diamond is made up of pure carbon, mostly of a white transparent colour, but sometimes tinted. White diamond is often regarded as the most precious because of its beauty and rarity.\n\nEmerald, Heb. \"brqm\"; Sept. \"smaragdos\"; Vulg. \"smaragdus\"; the third stone of the rational (Ex., xxviii, 17; xxxix, 10), representing the tribe of Levi; it is the ninth stone in Ezech., xxviii,13, and the fourth foundation stone of the celestial Jerusalem (Apoc., xxi, 19). The same stone is also mentioned in Tob., xiii, 16 (Vulg. 21); Jud., x, 21 (Vulg. 19); and in the Greek text of Ecclus., xxxii, 8, but there is no indication of it in the Manuscript B. of the Hebrew text, found in the Genizah of Cairo in 1896.\n\nPractically all versions, including Josephus (Ant. Jud., III, vii, 5; Bell. Jud., V, v, 7) translate \"brhm\" as \"emerald\". The Hebrew root \"brq\" (to glitter\"), from which it is probably derived, is agreed on by scholastic consensus. The word may also derive from the Sanskrit \"marakata\" which is certainly emerald nor is the Greek form \"smaragdos\" that different either. In Job, xiii, 21; Jud., x, 19; Ecclus., xxxii, 8; and Apoc., xxi, 19, the emerald is certainly the stone referred to. The word \"bphr\" also has sometimes been translated by \"smaragdus\" but this is a mistake as \"bphr\" signifies carbuncle.\n\nEmerald is a green variety of beryl and is composed of silicate of alumina and glucina. Structurally, it is a hexagonal crystal with a brilliant reflecting green colour. The emerald is highly polished and is found in metamorphic rocks, granites, and mica schist. Many of the finest specimens have been found in Muzo, Bogota, South America but the ancients obtained the stone from Egypt and India.\n\nAlthough claims have been made that the ancients knew nothing of the emerald - Pliny, Theophrastus and others clearly refute this even though the name may have been used possibly for other stones. In the Middle Ages miraculous healing powers were attributed to the emerald, among them; the power to preserve or heal visual problems.\n\nHyacinth, Greek \"hyakinthos\"; Vulg. \"hyacinthus\" (Apoc., xxi, 20); the eleventh stone of the foundation of the heavenly city. It very probably equates with Heb., the \"ligurius\" of Ex., xxviii, 19; xxxix, 12 (St. Epiphan., \"De duodecim gemmis\" in P. G., XLIII, 300). The stone referred to in Cant., v, 14, and called \"hyacinthus\" in the Vulgate is the Hebrew \"shoham\", which has been shown above to be chrysolite. The exact nature of hyacinth cannot be determined as the name was applied to several stones of similar colours and most probably designated stones reminiscent of the hyacinth flower.\n\nHyacinth is a zircon of a crimson, red, or orange colour. It is harder than quartz and its cleavage is undulating and sometimes lamellated. Its form is that of an oblong quadrangular prism terminated on both ends by a quadrangular pyramid. It was allegedly used as a talisman against tempests.\n\nJasper Heb. יָשְׁפֵ֑ה \"yashpeh\"; Sept. \"iaspis\"; Vulg. \"jaspis\"; the twelfth stone of the breastplate (Ex., xxviii, 18; xxxix, 11), representing Benjamin. In the Greek and Latin texts it comes sixth, and so also in Ezech., xxviii, 13; in the Apocalypse it is the first (xxi, 19). Despite this difference of position \"jaspis\" is undoubtedly the \"yshphh\" of the Hebrew text. The gem is an anhydrate quartz composed of silica, alumina, and iron and there are jaspers of nearly every colour. It is a completely opaque stone of a conchoidal cleavage. It seems to have been obtained by the Jews from India and Egypt.\n\nLigurus, Heb. \"lshs\"; Sept. \"ligyrion\"; Vulg. \"ligurius\"; the first stone of the third row of the rational (Ex., xxviii, 19; xxxix, 12), representing Gad. It is missing in the Hebrew of Ezech., xxviii, 13, but present in the Greek. This stone is probably the same as hyacinth (St. Epiphan., loc. cit.). This traditional identification, is based upon\nthe remark that the twelve foundation stones of the celestial city in Apoc., xxi, 19-20, correspond to the twelve stones of the rational. This alone is enough to equate ligurus with hyacinth although it has been identified with turmaline; though the latter view is rejected by most scholars.\n\nOnyx, Lat; Sept. \"onychion\"; Vulg. \"lapis onychinus\"; the eleventh stone of the breastplate in the Hebrew and the Vulgate (Ex., xxviii, 20; xxxix, 13), representing the tribe of Joseph. In the Sept. it is the twelfth stone and the fifth in Ezech., xxviii, 13, in the Heb., but the twelfth in the Greek; it is called \"sardonyx\" and comes in the fifth place in Apoc., xxi, 20.\n\nThe exact nature of this stone is disputed because the Greek word \"beryllos\" occurs instead of the Hebrew \"???\" thereby indicating beryl. However, this is not so (see Beryl above).\nThe Vulgate equates onyx with the Hebrew \"??? \" and although this alone would be a very weak argument; there are other, stronger testimonies to the fact that the Hebrew word occurs frequently in Holy Scripture: (Gen., ii, 12; Ex., xxv, 7; xxv, 9, 27; I Par., xxxix, 2; etc.) and on each occasion, except Job, xxviii, 16, the gem is translated in the Vulgate by \"lapis onychinus\" (\"lapis sardonychus\" in Job, xxviii, 16).\n\nThe Greek is very inconsistent in its translation, rendering \"shhs\" differently in various texts; therefore in Gen., ii, 12, it is \"lithos prasinos\", \"sardios\" in Ex. xxv, 7; xxxv, 9;\n\"smaragdos\" in Ex., xxviii, 9; xxxv, 27; xxxix, 6; \"soam\", a mere transcription of the Hebrew word in I Par., xxix, 2; and onyx in Job, xxviii, 16.\n\nOther Greek translators are more consistent: Aquila has \"sardonyx\" and Symmachus and Theodotion have onyx. The paraphrase of Onkelos had \"burla\", the Syriac \"berula\", both of which evidently are the Greek \"beryllos\"; \"beryl\". Since the translations do not observe the same order as the Hebrew in enumerating the stones of the rational (see Beryl above), it is not mandatory to accept the Greek \"beryllos\" as the translation of \"shhm\". Therefore, relying on the testimony of the various versions it can safely be assumed that onyx is the stone signified by \"shhm\".\n\nOnyx is a variety of quartz analogous to agate and other crypto-crystalline species. It is composed of different layers of variously coloured carnelian much like banded agate in structure, but the layers are in even or parallel planes. This makes it well adapted for the cutting of cameos and was much used by the ancients for that purpose. The colours of the best are perfectly well defined, and are either white and black, or white, brown, and\nblack. Some of the best specimens have been brought from India.\n\nSardonyx has a structure similar to onyx, but is composed usually of alternate layers of white chalcedony and carnelian, although carnelian may be associated with layers of white, brown, and black chalcedony. The ancients obtained onyx from Arabia, Egypt, and India.\n\nPearl. Although not a gemstone in the strictest sense we can apply the word \"stone\" in a broader context similar to that of coral. It is comparatively certain that pearl (Greek\n\"margarite\", Vulg. \"margarita\") was known among the Jews, at least after the time of Solomon, as it was among the Phoenicians. The exact etymology is uncertain but the following have been suggested: \"ghbysh\", which signified \"crystal\" (see above); \"phnynym\", which Gesenius renders by \"red coral\"; \"dr\", Esth., i, 6, which is translated in the Vulg. by \"lapis parius\", \"marble\"; the Arabic \"dar\" also signifies \"pearl\", and therefore Furst also renders the Hebrew word.\n\nIn the New Testament we find pearl mentioned in Matt., xiii, 45, 46; I Tim., ii, 9; etc. Pearl is a concretion consisting chiefly of lime carbonate found in several bivalve molluscs, but especially in \"avicula margaritifera\". Generally, it has a whitish blue hue, sometimes showing a tinge of pink; but there are also yellow pearls. This gem was considered the most precious of all among the ancients, and was obtained from the Red Sea,\nthe Indian Ocean, and the Persian Gulf.\n\nRuby. This stone may have been either the carbuncle or the chodchod (see above). There is, however, a choice between the oriental ruby and the spinel ruby; but the words may have been used indiscriminately for both. The former is extremely hard, almost as hard as the diamond, and is obtained from Ceylon, India, and China. It is considered one of the most precious gems.\n\nSapphire, Heb. \"mghry\" Septuag. \"sappheiron\"; Vulg. \"sapphirus\". Sapphire was the fifth stone of the rational (Ex., xxviii, 19; xxxix, 13), and represented the tribe of Issachar. It is the seventh stone in Ezech., xxviii, 14 (in the Hebrew text, for it occurs fifth in the Greek text); it is also the second foundation stone of the celestial Jerusalem (Apoc., xxi, 19).\n\nThe genuine sapphire is a beautiful blue hyaline corindon and is composed of nearly pure alumina, its colour resulting from the presence of iron oxide. The ancients also referred to lapis-lazuli as sapphire, which is likewise a blue stone, often speckled with shining\npyrites giving it the appearance of being sprinkled with gold dust. It is composed of silica, alumina, and alkali and is an opaque substance easily engraved. Debate still continues as to which stone is precisely referred to in the Bible. Both may be meant, but lapis-lazuli seems more probable as its qualities are better suited for the purposes of engraving (Lam., iv, 7; Ex., xxviii, 17; xxxix, 13). Sapphire was obtained from India.\n\nSardonyx; SARD. These two words are often confused by interpreters. Sard is carnelian, while sardonyx is a species of onyx.\n\nTopaz, Heb. \"ghtrh\"; Sept. \"topazion\"; Vulg. \"topazius\", the second stone of the rational (Ex., xxviii, 17; xxxix, 19), representing Simeon; also the second stone in Ezech., xxviii, 13; the ninth foundation stone of the celestial Jerusalem (Apoc., xxi, 20) and also mentioned in Job, xxviii, 19.\n\nThis topaz is generally believed to have been chrysolite rather than the more generally known topaz. Oriental topaz is composed of nearly pure alumina, silica, and fluoric acid; its shape is an orthorhombic prism with a cleavage transverse to its long axis. It is extremely hard and has a double refraction. When rubbed or heated it becomes highly electric.\n\nIt varies in colour according to the country of origin. Australian topaz is green or yellow; the Tasmanian clear, bright, and transparent; the Saxon pale violet; the Bohemian sea-green and the Brazilian red, varying from a pale red to a deep carmine. The ancients very probably obtained it from the East.\n\nPriestly breastplate\n\n"}
{"id": "38563792", "url": "https://en.wikipedia.org/wiki?curid=38563792", "title": "List of medical professionals who died during the SARS outbreak", "text": "List of medical professionals who died during the SARS outbreak\n\nThis is a list of medical professionals who died during the SARS outbreak in 2003.\n"}
{"id": "56312054", "url": "https://en.wikipedia.org/wiki?curid=56312054", "title": "List of original stand-up comedy specials distributed by Netflix", "text": "List of original stand-up comedy specials distributed by Netflix\n\nNetflix is an American global on-demand Internet streaming media provider, that has distributed a number of original programs, including original series, specials, miniseries, and documentaries and films.\n\nThe following projects have all been announced as being in development, but do not have a specific release date known at this time.\n\n"}
{"id": "31732740", "url": "https://en.wikipedia.org/wiki?curid=31732740", "title": "List of places used in the names of chemical elements", "text": "List of places used in the names of chemical elements\n\n40 of the 118 chemical elements have names associated with, or specifically named for, places around the world or among astronomical objects. 32 of these have names tied to the Earth and the other 8 have names connected to bodies in the Solar System. The first tables below list the terrestrial locations (excluding the entire Earth itself, taken as a whole) and the last table lists astronomical objects which the chemical elements are named after.\n\n<nowiki>*</nowiki> - The element mercury was named directly for the deity, with only indirect naming connection to the planet (see etymology of mercury).\n<br>\n\n"}
{"id": "41706691", "url": "https://en.wikipedia.org/wiki?curid=41706691", "title": "List of unsaturated fatty acids", "text": "List of unsaturated fatty acids\n\nThe following fatty acids have one unsaturated bond.\n\nCrotonic acid has 4 carbons, is included in croton oil, and is a \"trans\"-2-mono-unsaturated fatty acid. CH COH, IUPAC organization name (\"E\")-but-2-enoic acid, \" trans \" -but-2-enoic acid, numerical representation 4: 1, n-1, molecular weight 86.09, melting point 72-74 °C, boiling point 180-181 °C, specific gravity 1.027. CAS registry number 107-93-7.\n\nMyristoleic acid has 14 carbons, is found in whale blubber, and is a \"cis\"-9-monounsaturated fatty acid. CHCOH, IUPAC organization name (\"Z\")-tetradec-9-enoic acid, numerical representation 14:1, n-5, molecular weight 226.36, melting point of -4.5 -4 °C. CAS Registry Number 544-64-9.\n\nPalmitoleic acid has 16 carbons, is found in cod liver oil, sardine oil, and herring oil, and is a \"cis\"9-monounsaturated fatty acid. CHCOH, IUPAC organization name (\"Z\")-hexadec-9-enoic acid, n-7, numerical representation of 16: 1, molecular weight 254.41, melting point 5 °C, specific gravity 0.894. CAS Registry Number 373-49-9.\n\nSapienic acid has 16 carbons, is found in the skin, and is a \"cis\"6-mono-unsaturated fatty acid. CHCOH, IUPAC organization name (\"Z\")-6-Hexadecenoic acid, n-10, numerical expression 16: 1, molecular weight 254.41. CAS Registry Number 17004-51-2.\n\nOleic acid has 18 carbons, is found in most animal fats and olive oil, and is a \"cis\"-9-monounsaturated fatty acid. CHCOH, IUPAC organization name (\"Z\")-octadec-9-enoic acid, numerical representation 18:1 (9), n-9, molecular weight 282.46, melting point 13.4 °C, specific gravity 0.891. CAS Registry Number 112-80-1.\n\nElaidic acid has 18 carbons and is a \"trans\"-9-mono-unsaturated fatty acid. It is also a \"trans\" isomer of oleic acid. CHCOH, IUPAC organization name (\"E\")-octadec-9-enoic acid, numerical representation 18:1 (9), n-9, molecular weight 282.46, melting point 43-45 °C. CAS Registry Number 112-79-8.\n\nVaccenic acid has 18 carbons, is found in beef tallow, mutton, and butter, and is a \"trans\"11-mono-unsaturated fatty acid. CHCOH, IUPAC organization name (\"Z\")-octadec-11-enoic acid, numerical representation 18:1 (11) n-7, molecular weight 282.46. CAS Registry Number 506-17-2.\n\nGadoleic has 20 carbons, is found in cod liver oil and other marine animal oils, and is a \"cis\"9-mono-unsaturated fatty acid. CHCOH, IUPAC organization name (\"Z\")-icos-9-enoic acid, numerical representation 20:1 (9), n-11, molecular weight 310.51. CAS Registry Number 29204-02-2.\n\nEicosenoic acid has 20 carbons, is found in a wide variety of plant oils, and is a \"cis\"11-mono-unsaturated fatty acid. CHCOH, IUPAC organization name (\"Z\")-icos-11-enoic acid, numerical representation 20:1 (11), n-9, molecular weight 310.51. CAS Registry Number 5561-99-9.\n\nErucic acid has 22 carbons, is found in rapeseed oil and mustard oil, and is a \"cis\"13-monounsaturated is a fatty acid. CHCOH, IUPAC organization name (\"Z\")-docos-13-enoic acid, numerical representation 22:1, n-9, molecular weight 338.57, melting point 33-35 °C. CAS Registry Number 112-86-7.\n\nNervonic acid has 24 carbons, is found in brain glycolipids (Nervon) and sphingomyelin, and is a \"cis\"15-mono-unsaturated fatty acid. CHCOH, IUPAC organization name (\"Z\")-tetracos-15-enoic acid, numerical representation 24:1, n-9, molecular weight 366.62, melting point 42-43 °C. CAS Registry Number 506-37-6.\n\nThe following fatty acids have two unsaturated bonds.\n\nLinoleate has 18 carbons, is contained in many vegetable oils, particularly semi-drying oils, and is a \"cis\"-9-\"cis\"- 12-di-unsaturated fatty acid. CHCOH, IUPAC organization name (9\"Z \", 12\"Z\")-octadeca- 9,12-dienoic acid, numerical representation 18: 2 (9,12), n-6, molecular weight 280.45, melting point -5 °C, specific gravity 0.902. CAS Registry Number 60-33-3. There is a double bond is conjugated as an isomer conjugated linoleic acid.\n\nEicosadienoic acid (eicosadienoic's) has 20 carbons and is a \"cis\"-11-\"cis\"14-di-unsaturated fatty acid. CHCOH, IUPAC organization name (11\"Z \", 14\"Z\")-icosa- 11,14-dienoic acid, numerical representation 20: 2 (11,14), n-6, molecular weight 308.50.\n\nDocosadienoic acid (docosadienoic's) has 22 carbons and is a \"cis\"-13-\"cis\"16-di-unsaturated fatty acid. CHCOH, IUPAC organization name (13\"Z \", 16\"Z\")-docosa- 13,16-dienoic acid, numerical representation 22: 2 (13,16), n-6, molecular weight 336.55. CAS Registry Number 7370-49-2.\n\nThe following fatty acids have three unsaturated bonds.\n\nα-linolenic acid (alpha-linolenic's) has 18 carbons, is found in linseed oil and drying oil, and is a 9,12,15-tri-unsaturated fatty acid. CHCOH, IUPAC organization name (9\"Z \", 12\"Z \", 15 \" Z\")-octadeca-9,12,15-trienoic acid, numerical representation 18: 3 (9,12,15), n-3, molecular weight 278.43, melting point -11 °C, specific gravity 0.914. CAS Registry Number 463-40-1.\n\nΓ-linolenic acid (gamma-linolenic's) has 18 carbons, the structural isomers of α- linolenic acid. IUPAC organization name (6\"Z \", 9\"Z \", 12\"Z\")-octadeca-6,9,12-trienoic acid, numerical representation 18: 3 (6, 9, 12), n-6. CAS Registry Number 506-26-3.\n\nPinolenic acid (pinolenic's) has 18 carbons, is found in pine nuts, and is a 5,9,12-triunsaturated fatty acid. CHCOH, IUPAC organization name (5\"Z \", 9\"Z \", 12 \" Z\")-octadeca-5,9,12-trienoic acid, numerical representation 18: 3 (5,9,12), n-6, molecular weight 278.43. CAS Registry Number 16833-54-8.\n\nα-eleostearic acid (alpha-eleostearic's) has 18 carbons, is found in Kiri drying oil, and is a 9,11,13-triunsaturated fatty acid. CHCOH, IUPAC organization name (9\"E \", 11\"E \", 13 \" Z\")-octadeca-9,11,13-trienoic acid, numerical representation 18: 3 (9,11,13), n-5, molecular weight 278.43.\n\nβ-eleostearic acid (beta-eleostearic's, beta-eleostearic acid) is a geometric isomer of α- eleostearic acid. IUPAC organization name (9\"E \", 11\"E \", 13\"E\")-octadeca-9,11,13-trienoic acid, numerical representation 18: 3 (9, 11, 13), n-5.\n\nMead acid (Mead's) has 20 carbons, is a 5,8,11-tri-unsaturated fatty acid. CHCOH, IUPAC organization name (5\"Z \", 8\"Z \", 11 \" Z\")-icosa-5,8,11-trienoic acid, numerical representation 20: 3 (5,8,11), n-9, molecular weight 306.48. CAS Registry Number 20590-32-3.\n\nDihomo-γ-linolenic acid (dihomo-gamma-linolenic's, dihomo-gamma-linolenic acid, DGLA) has 20 carbons, and is an 8,11,14-tri-unsaturated fatty acid. CHCOH, IUPAC organization name (8\"Z \", 11\"Z \", 14 \" Z\")-icosa-8,11,14-trienoic acid, numerical representation 20: 3 (8,11,14), n-6, molecular weight 306.48. CAS Registry Number 1783-84-2.\n\nEicosatrienoic acid (eicosatrienoic's, eicosatrienoic acid) has 20 carbons and is a 11,14,17- tri unsaturated fatty acid. CHCOH, IUPAC organization name (11\"Z \", 14\"Z \", 17 \" Z\")-icosa-11,14,17-trienoic acid, numerical representation 20: 3 (11,14,17), n-3, molecular weight 306.48.\n\nThe following fatty acids have four unsaturated bonds.\n\nStearidonic acid (stearidonic's) has 18 carbons, is found in sardine oil and herring oil, and is a 6,9,12,15-tetraunsaturated fatty acid. CHCOH, IUPAC organization name (6\"Z \", 9\"Z \", 12 \" Z \", 15\"Z\")-octadeca-6,9,12,15-tetraenoic acid, numerical representation 18: 4 (6,9,12,15), n-3, molecular weight 276.41. CAS Registry Number 20290-75-9.\n\nArachidonic acid (arachidonic's) has 20 carbons, is present in animal visceral fat (brain, liver, kidney, lung, spleen), and is a 5,8,11,14-tetra-unsaturated fatty acid. I caused by the decomposition of cell membrane in the phospholipid. Prostaglandin, and important as starting materials for the thromboxane, leukotriene such as are known as a series of metabolic pathway to give eicosanoids, arachidonic acid cascade are compounds.\n\nCHCOH, IUPAC organization name (5\"Z \", 8\"Z \", 11 \" Z \", 14\"Z\")-icosa-5,8,11,14-tetraenoic acid, numerical representation 20: 4 (5,8,11,14), n-6, molecular weight 304.47, boiling point 169- 171 °C. CAS Registry Number 506-32-1.\n\nEicosatetraenoic acid (eicosatetraenoic's) has 20 carbons and is an 8,11,14,17-tetraunsaturated fatty acid. CHCOH, IUPAC organization name (8\"Z \", 11\"Z \", 14 \" Z \", 17\"Z\")-icosa-8,11,14,17-tetraenoic acid, numerical representation 20: 4 (8,11,14,17), n-3, molecular weight 304.47.\n\nAdrenic acid (adrenic'sd) has 22 carbons and is a 7,10,13,16-tetra-unsaturated fatty acid. CHCOH, IUPAC organization name (7\"Z \", 10\"Z \", 13 \" Z \", 16\"Z\")-docosa-7,10,13,16-tetraenoic acid, numerical representation 22: 4 (7,10,13,16), n-6, molecular weight 332.52. CAS Registry Number 28874-58-0.\n\nThe following fatty acids have five unsaturated bonds.\n\nBosseopentaenoic acid (Boseopentaen's), has 20 carbons and is a 5,8,10,12,14-pentaunsaturated fatty acid. CHCOH, IUPAC organization name (5\"Z \", 8\"Z \", 10 \" E \", 12\"E \", 14\"Z\")-eicosa-5,8,10,12,14-pentaenoic acid, numerical representation 20: 5 (5,8,10,12,14), n-6, molecular weight 302.46 g·mol−1.\n\nEicosapentaenoic acid (EPA) has 20 carbons, is found in fish oil, is a pentaunsaturated fatty acid. It is one of the essential fatty acids. The recommendation of ingesting fish oil supplements during pregnancy is said to help increase the cognitive ability at 6 months, but mercury concentration in fish products offsets the effect. In patients with hyperlipidemia and obstructive artery disease it can help lower triglycerides and also has an anti-platelet effect similar to other anti-platelet agents. It has also been shown to help in secondary prevention of ischemic heart disease as shown with the JELIS test.\n\nCHCOH, IUPAC organization name (5\"Z \", 8\"Z \", 11 \" Z \", 14\"Z \", 17\"Z\")-icosa-5,8,11,14,17-pentaenoic acid, numerical representation of 20: 5 (5,8,11,14,17), n-3, molecular weight 302.45, from melting point -54 -53 °C, specific gravity 0.943. CAS Registry Number 10417-94-4.\n\nOzubondo acid (Ozubondo's, Osbond acid), has 22 carbons, is a 4,7,10,13,16- pentaunsaturated fatty acid. CHCOH, IUPAC organization name (4\"Z \", 7\"Z \", 10\"Z \", 13\"Z \", 16\"Z\")-docosa-4,7,10,13,16-pentaenoic acid, numerical representation 22: 5 (4,7,10,13,16), n-6, molecular weight 330.50. CAS Registry Number 25182-74-5\n\nSardine acid (Clupanodonic acid) has 22 carbons, is found in sardine oil and herring oil, is a 7,10,13,16,19- pentaunsaturated fatty acid. CHCOH, IUPAC organization name (7\"Z \", 10\"Z \", 13 \" Z \", 16\"Z \", 19\"Z\")-docosa-7,10,13,16,19-pentaenoic acid, numerical representation 22: 5 (7,10,13,16,19), n-3, molecular weight 330.50.\n\nTetracosanolpentaenoic acid has 24 carbons, is a 9,12,15,18,21-penta unsaturated fatty acid. CHCOH, IUPAC organization name (9\"Z \", 12\"Z \", 15 \" Z \", 18\"Z \", 21\"Z\")-tetracosa-9,12,15,18,21-pentaenoic acid, numerical representation 24: 5 (9,12,15,18,21), n-3, molecular weight 358.56.\n\nThe following fatty acids have six unsaturated bonds.\n\nDocosahexaenoic acid (DHA) has 22 carbons, is found in fish oil, is a 4,7,10,13,16,19-hexa unsaturated fatty acid. In the human body it is generated from α-linolenic acid. CHCOH, IUPAC organization name (4\"Z\", 7\"Z\", 10\"Z\", 13\"Z\", 16\"Z\", 19\"Z\")-docosa-4,7,10,13,16,19-hexaenoic acid, numerical representation 22: 6 (4, 7,10,13,16,19), n-3, molecular weight 328.49, melting point -44 °C, specific gravity 0.950. CAS Registry Number 6217-54-5.\n\nHerring acid (Herring's, Nisinic acid) is a 6,9,12,15,18,21-hexa unsaturated fatty acid with 24 carbon atoms. CHCOH, IUPAC organization name (6\"Z \", 9\"Z \", 12 \" Z \", 15\"Z \", 18\"Z \", 21\"Z\")-tetracosa-6,9,12,15,18,21-hexaenoic acid, numerical representation 24: 5 (6, 9,12,15,18,21), n-3, molecular weight 356.54.\n\n"}
{"id": "16625060", "url": "https://en.wikipedia.org/wiki?curid=16625060", "title": "MS 1512-cB58", "text": "MS 1512-cB58\n\nMS 1512-cB58 is a galaxy in the Boötes constellation. It is a starburst galaxy that is being strongly gravitationally lensed, magnifying its apparent size by 30−50 times.\n\n"}
{"id": "13428834", "url": "https://en.wikipedia.org/wiki?curid=13428834", "title": "Marta Grandi", "text": "Marta Grandi\n\nMarta Grandi (3 July 1915, Bologna – Bologna, 6 October 2005, Bologna ) was an Italian entomologist who specialised in Ephemeroptera.\n\n\nSee Bibliography of Ephemeroptera\n\n"}
{"id": "12941686", "url": "https://en.wikipedia.org/wiki?curid=12941686", "title": "Medical laboratory", "text": "Medical laboratory\n\nA medical laboratory or clinical laboratory is a laboratory where clinical pathology tests are carried out on clinical specimens to obtain information about the health of a patient to aid in diagnosis, treatment, and prevention of disease. Clinical Medical laboratories are an example of applied science, as opposed to research laboratories that focus on basic science, such as found in some academic institutions. \n\nMedical laboratories vary in size and complexity and so offer a variety of testing services. More comprehensive services can be found in acute-care hospitals and medical centers, where 70% of clinical decisions are based on laboratory testing. Doctors offices and clinics, as well as skilled nursing and long-term care facilities, may have laboratories that provide more basic testing services. Commercial medical laboratories operate as independent businesses and provide testing that is otherwise not provided in other settings due to low test volume or complexity.\n\nIn hospitals and other patient-care settings, laboratory medicine is provided by the Department of Pathology, and generally divided into two sections, each of which will be subdivided into multiple specialty areas. The two sections are:\n\nLayouts of clinical laboratories in health institutions vary greatly from one facility to another. For instance, some health facilities have a single laboratory for the microbiology section, while others have a separate lab for each specialty area.\nThe following is an example of a typical breakdown of the responsibilities of each area:\n\nThe staff of clinical laboratories may include:\n\nIn the United States, there is a documented shortage of working laboratory professionals. For example, in 2016 vacancy rates for Medical Laboratory Scientists ranged from 5% to 9% for various departments. The decline is primarily due to retirements, and at-capacity educational programs that cannot expand and limits the number of new graduates. Professional organizations and some state educational systems are responding by developing ways to promote the lab professions in an effort to combat this shortage. The National Center For Workforce Analysis has estimated that by 2025, there will be a 24% increase in demand for lab professionals.\n\nIn most developed countries, there are two main types of lab processing the majority of medical specimens. Hospital laboratories are attached to a hospital, and perform tests on their patients. Private (or community) laboratories receive samples from general practitioners, insurance companies, clinical research sites and other health clinics for analysis. For extremely specialised tests, samples may go to a research laboratory. Some tests involve specimens sent between different labs for uncommon tests. For example, in some cases it may be more cost effective if a particular laboratory specializes in a less common tests, receiving specimens (and payment) from other labs, while sending other specimens to other labs for those tests they do not perform.\n\nIn many countries there are specialized types of Medical Laboratories according to the types of investigations carried out. Organisations that provide blood products for transfusion to hospitals, such as The Red Cross, will provide access to their reference laboratory for their customers. Some laboratories specialize in Molecular diagnostic and cytogenetic testing, in order to provide information regarding diagnosis and treatment of genetic or cancer-related disorders.\n\nIn a hospital setting, sample processing will usually start with a set of samples arriving with a test request, either on a form or electronically via the laboratory information system (LIS). Inpatient specimens will already be labeled with patient and testing information provided by the LIS. Entry of test requests onto the LIS system involves typing (or scanning where barcodes are used) in the laboratory number, and entering the patient identification, as well as any tests requested. This allows laboratory analyzers, computers and staff to recognize what tests are pending, and also gives a location (such as a hospital department, doctor or other customer) for results reporting.\n\nOnce the specimens are assigned a laboratory number by the LIS, a sticker is typically printed that can be placed on the tubes or specimen containers. This label has a barcode that can be scanned by automated analyzers and test requests uploaded to the analyzer from the LIS. \n\nSpecimens are prepared for analysis in various ways. For example, chemistry samples are usually centrifuged and the serum or plasma is separated and tested. If the specimen needs to go on more than one analyzer, it can be divided into separate tubes.\n\nMany specimens end up in one or more sophisticated automated analysers, that process a fraction of the sample to return one or more test results. Some laboratories use robotic sample handlers (Laboratory automation) to optimize the workflow and reduce the risk of contamination from sample handling by the staff.\n\nThe work flow in a hospital laboratory is usually heaviest from 2:00 am to 10:00 am. Nurses and doctors generally have their patients tested at least once a day with common tests such as complete blood counts and chemistry profiles. These orders are typically drawn during a morning run by phlebotomists for results to be available in the patient's charts for the attending physicians to consult during their morning rounds. Another busy time for the lab is after 3:00 pm when private practice physician offices are closing. Couriers will pick up specimens that have been drawn throughout the day and deliver them to the lab. Also, couriers will stop at outpatient drawing centers and pick up specimens. These specimens will be processed in the evening and overnight to ensure results will be available the following day.\n\nThe large amount if information processed in laboratories is managed by a system of software programs, computers, and terminology standards that exchange data about patients, test requests, and test results known as a Laboratory information system or LIS. The LIS is often interfaced with the hospital information system, EHR and/or Laboratory instruments. Formats for terminologies for test processing and reporting are being standardized with systems such as Logical Observation Identifiers Names and Codes (LOINC) and Nomenclature for Properties and Units terminology (NPU terminology).\n\nThese systems enable hospitals and labs to order the correct test requests for each patient, keep track of individual patient and specimen histories, and help guarantee a better quality of results. Results are made available to care providers electronically or by printed hard copies for patient charts.\n\nAccording to various regulations, such as the international ISO 15189 norm, all pathological laboratory results must be verified by a competent professional. In some countries, staffs composed of clinical scientists do the majority of this work inside the laboratory with certain abnormal results referred to the relevant pathologist. Clinical scientists have the responsibility for limited interpretation of testing results in their discipline in many countries. Interpretation of results can be assisted by some software in order to validate normal or non modified results.\n\nIn other testing areas, only professional medical staff (pathologist or clinical biologist) is involved with interpretation and consulting. Medical staff are sometimes also required in order to explain pathology results to physicians. For a simple result given by phone or to explain a technical problem, often a medical technologist or medical lab scientist can provide additional information.\n\nMedical Laboratory Departments in some countries are exclusively directed by a specialized pathologist. In others, a consultant, medical or non-medical, may be the head the department. In Europe and some other countries, Clinical Scientists with a Masters level education may be qualified to head the department. Others may have a PhD and can have an exit qualification equivalent to medical staff (e.g., FRCPath in the UK).\n\nIn France, only medical staff (Pharm.D. and M.D. specialized in anatomical pathology or clinical biology) can discuss pathological results.\n\nCredibility of medical laboratories is paramount to the health and safety of the patients relying on the testing services provided by these labs. Credentialing agencies vary by country. The international standard in use today for the accreditation of medical laboratories is ISO 15189 - \"Medical laboratories - Requirements for quality and competence.\n\nIn the United States, accreditation is performed by the Joint Commission, College of American Pathologists, AAB (American Association of Bioanalysts), and other state and federal agencies. Legislative guidelines are provided under CLIA 88 (Clinical Laboratory Improvement Amendments) which regulates Medical Laboratory testing and personnel.\n\nThe accrediting body in Australia is NATA, where all laboratories must be NATA accredited to receive payment from Medicare.\n\nIn France the accrediting body is COFRAC (). In 2010, modification of legislation established ISO 15189 accreditation as an obligation for all clinical laboratories.\n\nIn the United Arab Emirates, the Dubai Accreditation Department (DAC) is the accreditation body that is internationally recognised by the International Laboratory Accreditation Cooperation (ILAC) for many facilties and groups, including Medical Laboratories, Testing and Calibration Laboratories, and Inspection Bodies.\n\nIn Hong Kong, the accrediting body is Hong Kong Accreditation Service (HKAS). On 16 February 2004, HKAS launched it's medical testing accreditation programme.\n\nIn Canada, laboratory accreditation is not mandatory, but is becoming more and more popular. Accreditation Canada (AC) is the national reference. Different provincial oversight bodies mandate laboratories in EQA participations like LSPQ (Quebec), IQMH (Ontario) for example.\n\nThe laboratory industry is a part of the broader healthcare and health technology industry. Companies exist at various levels, including clinical laboratory services, suppliers of instrumentation equipment and consumable materials, and suppliers and developers of of diagnostic tests themselves (often by biotechnology companies). \n\nClinical laboratory services includes large multinational corporations such LabCorp, Quest Diagnostics, and Sonic Healthcare but a significant portion of revenue, estimated at 60% in the United States, is generated by hospital labs. In 2018, the total global revenue for these companies was estimated to reach $146 billion by 2024. Another estimate places the market size at $205 billion, reaching $333 billion by 2023. The American Association for Clinical Chemistry (AAAC) represents professionals in the field. \n\nClinical laboratories are supplied by other multinational companies which focus on materials and equipment, which can be used for both scientific research and medical testing. The largest of these is Thermo Fisher Scientific. In 2016, global life sciences instrumentation sales were around $47 billion, not including consumables, software, and services. In general, laboratory equipment includes lab centrifuges, transfection solutions, water purification systems, extraction techniques, gas generators, concentrators and evaporators, fume hoods, incubators, biological safety cabinets, bioreactors and fermenters, microwave-assisted chemistry, lab washers, and shakers and stirrers. \n\n, the in vitro diagnostics (IVD) market was estimated at a global value of around $45-50 billion, with six key companies: Roche Diagnostics, Abbott Diagnostics, Siemens, Johnson & Johnson Medical Devices and Diagnostics, Beckman Coulter and BioMerieux. Many of the companies sell capital equipment and supply consumables, and the devices are also used for industrial purposes such as food testing. Molecular diagnostics is estimated at 10% of total revenue, and half of that focused on infectious disease testing.\n\nIn the United States, estimated total revenue as of 2016 was $75 billion, about 2% of total healthcare spending. In 2016, an estimated 60% of revenue was done by hospital labs, with 25% done by two independent companies (LabCorp and Quest). Hospital labs may also outsource their lab, known as outreach, to run tests; however, health insurers may pay the hospitals more than they would pay a laboratory company for the same test, but as of 2016, the markups were questioned by insurers. Rural hospitals, in particular, can bill for lab outreach under the Medicare’s 70/30 shell rule.\n\nLaboratory developed tests are designed and developed inside a specific laboratory and do not require FDA approval; due to technological innovations, they have become more common and are estimated at a total value of $11 billion in 2016.\n\nDue to the rise of high-deductible health plans, laboratories have sometimes struggled to collect when billing patients; consequently, some laboratories have shifted to become more \"consumer-focused\".\n\n"}
{"id": "735303", "url": "https://en.wikipedia.org/wiki?curid=735303", "title": "Mercury-Redstone 2", "text": "Mercury-Redstone 2\n\nMercury-Redstone 2 (MR-2) was the penultimate test flight of the Mercury-Redstone Launch Vehicle prior to the first manned American space mission in Project Mercury. It was launched at 16:55 UTC on January 31, 1961 from LC-5 at Cape Canaveral, Florida. Mercury spacecraft No. 5 carried Ham the Chimp, a chimpanzee, on a suborbital flight, landing in the Atlantic Ocean 16 minutes and 39 seconds after launch.\n\nThe previous Mercury-Redstone mission, MR-1A, flew a trajectory that was too steep with accelerations too high for a human passenger. MR-1A had climbed to its programmed apogee of about 130 miles (209 km) and landed 235 miles (378 km) downrange. Mercury-Redstone 2 would follow a more flattened trajectory. Its planned flight path was an apogee of 115 miles (185 km) and a range of 290 miles (467 km).\n\nMercury spacecraft No. 5 contained six new systems that had not been on previous flights: environmental control system, attitude stabilization control system, live retrorockets, voice communications system, \"closed loop\" abort sensing system, and a pneumatic landing bag.\n\nSix chimpanzees (four female and two male) and 20 medical specialists and animal handlers from Holloman Air Force Base, New Mexico, where the chimps lived and were trained, were moved into quarters behind Hangar S at Cape Canaveral, Florida on January 2, 1961. The six chimps were trained in Mercury simulators for three weeks. The day before the flight, two chimps were chosen for the mission: one primary, Ham, and one backup, a female chimp named Minnie. The competition was fierce, but Ham was full of energy and good humor. Ham was named in honor of Holloman Aerospace Medical Center. Ham was from Cameroon, Africa, (original name Chang, Chimp No. 65) and was purchased by the USAF July 9, 1959. He was 3 years 8 months old at launch.\n\nAt 12:53 UTC, January 31, 1961, Ham was inserted into the spacecraft. The countdown was then delayed almost four hours because of a hot inverter, and several other minor problems.\n\nAt 16:55 UTC the MR-2 lifted off. Computers reported one minute after launch, the flight path angle was at least one degree too high and rising. At two minutes, the computers predicted a 17 \"g\" (167 m/s²) acceleration. At 2 minutes 17 seconds into the flight, the liquid oxygen supply was depleted. The closed-loop abort system sensed a change in engine chamber pressure when the LOX supply was depleted and fired the launch escape system. The abort signaled a Mayday message to the recovery forces.\n\nThe high flight angle, and the early abort, caused the maximum velocity of the spacecraft to be 7,540 ft/s (2,298 m/s) instead of the planned 6,465 ft/s (1,970 m/s). The retrorockets had been jettisoned during the abort and therefore could not be used to slow down the spacecraft. All of this added up to an overshoot of the planned landing area by 130 miles (209 km) and an apogee of 157 miles (253 km) instead of 115 miles (185 km).\n\nAnother problem occurred at 2 minutes and 18 seconds into the flight, when cabin pressure dropped from 5.5 to 1 lb/in² (38 to 7 kPa). This malfunction was traced later to the air inlet snorkel valve. Vibrations had loosened a pin in the snorkel valve and allowed the valve to open. Ham was safe in his own couch spacesuit and did not suffer any ill effects from the loss of cabin pressure. His couch spacesuit pressure remained normal, and suit temperature stayed well within the 60 to 80 degrees Fahrenheit (16 to 26 °C) optimum range.\n\nBecause of over-acceleration of the launch vehicle and the boost from the escape rocket, a speed of 5,857 mph (9,426 km/h) was reached instead of the 4,400 mph (7,081 km/h) planned. At apogee Ham's spacecraft was 48 miles (77 km) farther downrange than planned. Ham was weightless for 6.6 minutes instead of the 4.9 minutes that were planned. The spacecraft landed 422 miles (679 km) downrange after a 16.5-minute flight. He received 14.7 \"g\" (144 m/s²) during reentry, almost 3 \"g\" (29 m/s²) greater than planned.\n\nHam performed his tasks well, pushing levers about 50 times during the flight. Onboard cameras filming Ham's reaction to weightlessness showed a surprising amount of dust and debris floating around inside the capsule during apogee. \n\nThe spacecraft splashed down about 12:12 p.m. EST, out of sight from recovery forces. About 12 minutes later, the first recovery signal was received from the spacecraft. Tracking showed it was about 60 miles (96 km) from the nearest recovery ship. Twenty-seven minutes after landing, a search plane sighted the capsule floating upright in the Atlantic. The search plane requested that the Navy send its rescue helicopters from the closest ship carrying them.\n\nWhen helicopters arrived they found the spacecraft on its side, taking on water, and submerging. The beryllium heat shield upon water impact had bounced against the capsule bottom, punching two holes in the titanium pressure bulkhead. The landing bag had worn badly, and the heatshield was torn free from the spacecraft before recovery. After the craft capsized, the open snorkel valve let still more sea water enter the capsule. When the helicopter crew finally latched onto and picked up Ham's spacecraft at 18:52 UTC., they estimated there was about 800 pounds (360 kg) of sea water aboard. The spacecraft was flown to and lowered to the deck of . When the spacecraft was opened Ham appeared to be in good condition and readily accepted an apple and half an orange.\n\nWith the malfunctions during the flight, the Mercury-Redstone was still not ready for a human passenger planned for MR-3. It was postponed pending a final booster development flight, Mercury-Redstone BD.\n\nAfter his spaceflight, Ham was transferred to the National Zoo in Washington, D.C. for 17 years and then in 1981 was moved to a zoo in North Carolina to live with a colony of other chimps. He died on January 19, 1983, at the age 26. Ham is buried at the New Mexico Museum of Space History in Alamogordo, New Mexico. He was one of many animals in space.\n\nHam's backup, Minnie, was the only female chimp trained for the Mercury program. After her role in the Mercury program ended, Minnie became part of an Air Force chimp-breeding program, producing nine offspring and helping raise the offspring of several other members of the chimp colony. She was the last surviving \"astro-chimp\". She died at the age of 41 on March 14, 1998.\n\nMercury spacecraft No. 5, used in the Mercury-Redstone 2 mission, is currently displayed at the California Science Center, Los Angeles, California.\n\n\n\n"}
{"id": "2145168", "url": "https://en.wikipedia.org/wiki?curid=2145168", "title": "Metric tensor (general relativity)", "text": "Metric tensor (general relativity)\n\nIn general relativity, the metric tensor (in this context often abbreviated to simply the metric) is the fundamental object of study. It may loosely be thought of as a generalization of the gravitational potential of Newtonian gravitation. The metric captures all the geometric and causal structure of spacetime, being used to define notions such as time, distance, volume, curvature, angle, and separating the future and the past.\n\nThroughout this article we work with a metric signature that is mostly positive (); see sign convention. The gravitation constant formula_1 will be kept explicit. The Einstein summation convention, where repeated indices are automatically summed over, is employed.\n\nMathematically, spacetime is represented by a four-dimensional differentiable manifold formula_2 and the metric tensor is given as a covariant, second-degree, symmetric tensor on formula_2, conventionally denoted by formula_4. Moreover, the metric is required to be nondegenerate with signature . A manifold formula_2 equipped with such a metric is a type of Lorentzian manifold.\n\nExplicitly, the metric tensor is a symmetric bilinear form on each tangent space of formula_2 that varies in a smooth (or differentiable) manner from point to point. Given two tangent vectors formula_7 and formula_8 at a point formula_9 in formula_2, the metric can be evaluated on formula_7 and formula_8 to give a real number:\nThis is a generalization of the dot product of ordinary Euclidean space. Unlike Euclidean space – where the dot product is positive definite – the metric is indefinite and gives each tangent space the structure of Minkowski space.\n\nPhysicists usually work in local coordinates (i.e. coordinates defined on some local patch of formula_2). In local coordinates formula_15 (where formula_16 is an index that runs from 0 to 3) the metric can be written in the form\nThe factors formula_18 are one-form gradients of the scalar coordinate fields formula_15. The metric is thus a linear combination of tensor products of one-form gradients of coordinates. The coefficients formula_20 are a set of 16 real-valued functions (since the tensor formula_4 is a \"tensor field\", which is defined at all points of a spacetime manifold). In order for the metric to be symmetric we must have\ngiving 10 independent coefficients.\n\nIf the local coordinates are specified, or understood from context, the metric can be written as a symmetric matrix with entries formula_20. The nondegeneracy of formula_24 means that this matrix is non-singular (i.e. has non-vanishing determinant), while the Lorentzian signature of formula_4 implies that the matrix has one negative and three positive eigenvalues. Note that physicists often refer to this matrix or the coordinates formula_20 themselves as the metric (see, however, abstract index notation).\n\nWith the quantities formula_18 being regarded as the components of an infinitesimal coordinate displacement four-vector (not to be confused with the one-forms of the same notation above), the metric determines the invariant square of an infinitesimal line element, often referred to as an \"interval\". The interval is often denoted\n\nThe interval formula_29 imparts information about the causal structure of spacetime. When formula_30, the interval is timelike and the square root of the absolute value of formula_29 is an incremental proper time. Only timelike intervals can be physically traversed by a massive object. When formula_32, the interval is lightlike, and can only be traversed by light. When formula_33, the interval is spacelike and the square root of formula_29 acts as an incremental proper length. Spacelike intervals cannot be traversed, since they connect events that are outside each other's light cones. Events can be causally related only if they are within each other's light cones.\n\nThe components of the metric depend on the choice of local coordinate system. Under a change of coordinates formula_35, the metric components transform as\n\nThe simplest example of a Lorentzian manifold is flat spacetime, which can be given as R with coordinates formula_37 and the metric\nNote that these coordinates actually cover all of R. The flat space metric (or Minkowski metric) is often denoted by the symbol \"η\" and is the metric used in special relativity. In the above coordinates, the matrix representation of \"η\" is\n\nIn spherical coordinates formula_43, the flat space metric takes the form\nwhere\nis the standard metric on the 2-sphere.\n\nThe Schwarzschild metric describes an uncharged, non-rotating black hole. There are also metrics that describe rotating and charged metrics for black holes.\n\nBesides the flat space metric the most important metric in general relativity is the Schwarzschild metric which can be given in one set of local coordinates by \nwhere, again, formula_47 is the standard metric on the 2-sphere. Here, formula_1 is the gravitation constant and formula_2 is a constant with the dimensions of mass. Its derivation can be found here. The Schwarzschild metric approaches the Minkowski metric as formula_2 approaches zero (except at the origin where it is undefined). Similarly, when formula_51 goes to infinity, the Schwarzschild metric approaches the Minkowski metric.\n\nWith coordinates\nwe can write the metric as\n\nSeveral other systems of coordinates have been devised for the Schwarzschild metric: Eddington–Finkelstein coordinates, Gullstrand–Painlevé coordinates, Kruskal–Szekeres coordinates, and Lemaître coordinates.\n\nRotating and charged black holes are described by the Kerr metric, the Kerr–Newman metric, and the Reissner–Nordström metric.\n\nOther notable metrics are:\n\n\nSome of them are without the event horizon or can be without the gravitational singularity.\n\nThe metric \"g\" induces a natural volume form (up to a sign), which can be used to integrate over a region of a manifold. Given local coordinates formula_15 for the manifold, the volume form can be written\nwhere formula_56 is the determinant of the matrix of components of the metric tensor for the given coordinate system.\n\nThe metric formula_4 completely determines the curvature of spacetime. According to the fundamental theorem of Riemannian geometry, there is a unique connection ∇ on any semi-Riemannian manifold that is compatible with the metric and torsion-free. This connection is called the Levi-Civita connection. The Christoffel symbols of this connection are given in terms of partial derivatives of the metric in local coordinates formula_15 by the formula\n\nThe curvature of spacetime is then given by the Riemann curvature tensor which is defined in terms of the Levi-Civita connection ∇. In local coordinates this tensor is given by:\n\nThe curvature is then expressible purely in terms of the metric formula_4 and its derivatives.\n\nOne of the core ideas of general relativity is that the metric (and the associated geometry of spacetime) is determined by the matter and energy content of spacetime. Einstein's field equations:\nwhere the Ricci curvature tensor\nand the scalar curvature\nrelate the metric (and the associated curvature tensors) to the stress–energy tensor formula_65. This tensor equation is a complicated set of nonlinear partial differential equations for the metric components. Exact solutions of Einstein's field equations are very difficult to find.\n\n\n"}
{"id": "15736292", "url": "https://en.wikipedia.org/wiki?curid=15736292", "title": "Network of Concerned Anthropologists", "text": "Network of Concerned Anthropologists\n\nThe Network of Concerned Anthropologists (NCA) is an independent ad hoc network of anthropologists seeking to promote an ethical anthropology.\n\nThe network is concerned that the \"war on terror\" threatens to militarize anthropology in a way that undermines the integrity of the discipline. Therefore, the network offers the possibility to sign a pledge where it is stated what kinds of work anthropologists should not engage in.\n\nThe founding members of the Network of Concerned Anthropologists include Catherine Besteman, Andrew Bickford, Greg Feldman, Gustaaf Houtman, Roberto Gonzalez, Hugh Gusterson, Jean Jackson, Kanhong Lin, Catherine Lutz, David Price, and David Vine.\n\n"}
{"id": "7025186", "url": "https://en.wikipedia.org/wiki?curid=7025186", "title": "Operation Sunbeam", "text": "Operation Sunbeam\n\nOperation Sunbeam was a series of four nuclear tests conducted at the United States of America's Nevada Test Site in 1962. \"Operation Sunbeam\" tested small, \"tactical\" nuclear warheads; the most notable was the \"Davy Crockett\". \"Operation Sunbeam\" was also known as \"Operation Dominic II\".\n\nThe chief milestone of \"Operation Sunbeam\" was that it was the last nuclear test series on the Nevada Test Site conducted in the atmosphere by the United States. Since \"Operation Sunbeam\", specifically the \"Little Feller 1\" test of the \"Davy Crockett,\" all US nuclear tests on the Test Site have been carried out underground in accordance with the Partial Test Ban Treaty.\n"}
{"id": "1553317", "url": "https://en.wikipedia.org/wiki?curid=1553317", "title": "Optical medium", "text": "Optical medium\n\nAn optical medium is material through which electromagnetic waves propagate. It is a form of transmission medium. The permittivity and permeability of the medium define how electromagnetic waves propagate in it. The medium has an \"intrinsic impedance\", given by\nwhere formula_2 and formula_3 are the electric field and magnetic field, respectively.\nIn a region with no electrical conductivity, the expression simplifies to:\n\nFor example, in free space the intrinsic impedance is called the characteristic impedance of vacuum, denoted \"Z\", and\n\nWaves propagate through a medium with velocity formula_6, where formula_7 is the frequency and formula_8 is the wavelength of the electromagnetic waves. This equation also may be put in the form\nwhere formula_10 is the angular frequency of the wave and formula_11 is the wavenumber of the wave. In electrical engineering, the symbol formula_12, called the \"phase constant\", is often used instead of formula_11.\n\nThe propagation velocity of electromagnetic waves in free space, an idealized standard reference state (like absolute zero for temperature), is conventionally denoted by \"c\": \n\nFor a general introduction, see Serway For a discussion of man-made media, see Joannopoulus.\n\n\n"}
{"id": "266611", "url": "https://en.wikipedia.org/wiki?curid=266611", "title": "Optical telescope", "text": "Optical telescope\n\nAn optical telescope is a telescope that gathers and focuses light, mainly from the visible part of the electromagnetic spectrum, to create a magnified image for direct view, or to make a photograph, or to collect data through electronic image sensors.\n\nThere are three primary types of optical telescope:\n\nA telescope's light gathering power and ability to resolve small detail is directly related to the diameter (or aperture) of its objective (the primary lens or mirror that collects and focuses the light). The larger the objective, the more light the telescope collects and the finer detail it resolves.\n\nPeople use telescopes and binoculars for activities such as observational astronomy, ornithology, pilotage and reconnaissance, and watching sports or performance arts.\n\nThe telescope is more a discovery of optical craftsmen than an invention of a scientist. The lens and the properties of refracting and reflecting light had been known since antiquity and theory on how they worked were developed by ancient Greek philosophers, preserved and expanded on in the medieval Islamic world, and had reached a significantly advanced state by the time of the telescope's invention in early modern Europe. But the most significant step cited in the invention of the telescope was the development of lens manufacture for spectacles, first in Venice and Florence in the thirteenth century, and later in the spectacle making centers in both the Netherlands and Germany. It is in the Netherlands in 1608 where the first recorded optical telescopes (refracting telescopes) appeared. The invention is credited to the spectacle makers Hans Lippershey and Zacharias Janssen in Middelburg, and the instrument-maker and optician Jacob Metius of Alkmaar.\n\nGalileo greatly improved on these designs the following year, and is generally credited as the first to use a telescope for astronomy. Galileo's telescope used Hans Lippershey's design of a convex objective lens and a concave eye lens, and this design is now called a Galilean telescope. Johannes Kepler proposed an improvement on the design that used a convex eyepiece, often called the Keplerian Telescope.\n\nThe next big step in the development of refractors was the advent of the Achromatic lens in the early 18th century, which corrected the chromatic aberration in Keplerian telescopes up to that time—allowing for much shorter instruments with much larger objectives.\n\nFor reflecting telescopes, which use a curved mirror in place of the objective lens, theory preceded practice. The theoretical basis for curved mirrors behaving similar to lenses was probably established by Alhazen, whose theories had been widely disseminated in Latin translations of his work. Soon after the invention of the refracting telescope Galileo, Giovanni Francesco Sagredo, and others, spurred on by their knowledge that curved mirrors had similar properties as lenses, discussed the idea of building a telescope using a mirror as the image forming objective. The potential advantages of using parabolic mirrors (primarily a reduction of spherical aberration with elimination of chromatic aberration) led to several proposed designs for reflecting telescopes, the most notable of which was published in 1663 by James Gregory and came to be called the Gregorian telescope, but no working models were built. Isaac Newton has been generally credited with constructing the first practical reflecting telescopes, the Newtonian telescope, in 1668 although due to their difficulty of construction and the poor performance of the speculum metal mirrors used it took over 100 years for reflectors to become popular. Many of the advances in reflecting telescopes included the perfection of parabolic mirror fabrication in the 18th century, silver coated glass mirrors in the 19th century, long-lasting aluminum coatings in the 20th century, segmented mirrors to allow larger diameters, and active optics to compensate for gravitational deformation. A mid-20th century innovation was catadioptric telescopes such as the Schmidt camera, which uses both a lens (corrector plate) and mirror as primary optical elements, mainly used for wide field imaging without spherical aberration.\n\nThe late 20th century has seen the development of adaptive optics and space telescopes to overcome the problems of astronomical seeing.\n\nThe basic scheme is that the primary light-gathering element the objective (1) (the convex lens or concave mirror used to gather the incoming light), focuses that light from the distant object (4) to a focal plane where it forms a real image (5). This image may be recorded or viewed through an eyepiece (2), which acts like a magnifying glass. The eye (3) then sees an inverted magnified virtual image (6) of the object.\n\nMost telescope designs produce an inverted image at the focal plane; these are referred to as \"inverting telescopes\". In fact, the image is both turned upside down and reversed left to right, so that altogether it is rotated by 180 degrees from the object orientation. In astronomical telescopes the rotated view is normally not corrected, since it does not affect how the telescope is used. However, a mirror diagonal is often used to place the eyepiece in a more convenient viewing location, and in that case the image is erect, but still reversed left to right. In terrestrial telescopes such as spotting scopes, monoculars and binoculars, prisms (e.g., Porro prisms) or a relay lens between objective and eyepiece are used to correct the image orientation. There are telescope designs that do not present an inverted image such as the Galilean refractor and the Gregorian reflector. These are referred to as \"erecting telescopes\".\n\nMany types of telescope fold or divert the optical path with secondary or tertiary mirrors. These may be integral part of the optical design (Newtonian telescope, Cassegrain reflector or similar types), or may simply be used to place the eyepiece or detector at a more convenient position. Telescope designs may also use specially designed additional lenses or mirrors to improve image quality over a larger field of view. \nDesign specifications relate to the characteristics of the telescope and how it performs optically. Several properties of the specifications may change with the equipment or accessories used with the telescope; such as Barlow lenses, star diagonals and eyepieces. These interchangeable accessories don't alter the specifications of the telescope, however they alter the way the telescopes properties function, typically magnification, apparent field of view and FOV.\n\nThe smallest resolvable surface area of an object, as seen through an optical telescope, is the limited physical area that can be resolved. It is analogous to angular resolution, but differs in definition: instead of separation ability between point-light sources it refers to the physical area that can be resolved. A familiar way to express the characteristic is the resolvable ability of features such as Moon craters or Sun spots. Expression using the formula is given by the sum of twice the resolving power formula_1 over aperture diameter formula_2 multiplied by the objects diameter formula_3 multiplied by the constant formula_4 all divided by the objects apparent diameter formula_5.\n\n\"Resolving power formula_1 is derived from the wavelength formula_7 using the same unit as aperture; where 550 nm to mm is given by: formula_8.\"\n\"The constant formula_4 is derived from radians to the same unit as the objects apparent diameter; where the Moons apparent diameter of formula_10 radians to arcsecs is given by: formula_11.\"\n\nAn example using a telescope with an aperture of 130 mm observing the Moon in a 550 nm wavelength, is given by: formula_12\n\nThe unit used in the object diameter results in the smallest resolvable features at that unit. In the above example they are approximated in kilometers resulting in the smallest resolvable Moon craters being 3.22 km in diameter. The Hubble Space Telescope has a primary mirror aperture of 2400 mm that provides a surface resolvability of Moon craters being 174.9 meters in diameter, or sunspots of 7365.2 km in diameter.\n\nIgnoring blurring of the image by turbulence in the atmosphere (atmospheric seeing) and optical imperfections of the telescope, the angular resolution of an optical telescope is determined by the diameter of the primary mirror or lens gathering the light (also termed its \"aperture\").\n\nThe Rayleigh criterion for the resolution limit formula_13 (in radians) is given by\nwhere formula_15 is the wavelength and formula_2 is the aperture. For visible light (formula_15 = 550 nm) in the small-angle approximation, this equation can be rewritten:\nHere, formula_13 denotes the resolution limit in arcseconds and formula_2 is in millimeters. \nIn the ideal case, the two components of a double star system can be discerned even if separated by slightly less than formula_13. This is taken into account by the Dawes limit\nThe equation shows that, all else being equal, the larger the aperture, the better the angular resolution. The resolution is not given by the maximum magnification (or \"power\") of a telescope. Telescopes marketed by giving high values of the maximum power often deliver poor images.\n\nFor large ground-based telescopes, the resolution is limited by atmospheric seeing. This limit can be overcome by placing the telescopes above the atmosphere, e.g., on the summits of high mountains, on balloon and high-flying airplanes, or in space. Resolution limits can also be overcome by adaptive optics, speckle imaging or lucky imaging for ground-based telescopes.\n\nRecently, it has become practical to perform aperture synthesis with arrays of optical telescopes. Very high resolution images can be obtained with groups of widely spaced smaller telescopes, linked together by carefully controlled optical paths, but these interferometers can only be used for imaging bright objects such as stars or measuring the bright cores of active galaxies.\n\nThe focal length of an optical system is a measure of how strongly the system converges or diverges light. For an optical system in air, it is the distance over which initially collimated rays are brought to a focus. A system with a shorter focal length has greater optical power than one with a long focal length; that is, it bends the rays more strongly, bringing them to a focus in a shorter distance. In astronomy, the f-number is commonly referred to as the \"focal ratio\" notated as formula_23. The focal ratio of a telescope is defined as the focal length formula_24 of an objective divided by its diameter formula_2 or by the diameter of an aperture stop in the system. The focal length controls the field of view of the instrument and the scale of the image that is presented at the focal plane to an eyepiece, film plate, or CCD.\n\nAn example of a telescope with a focal length of 1200 mm and aperture diameter of 254 mm is given by:\nformula_26\n\nNumerically large Focal ratios are said to be \"long\" or \"slow\". Small numbers are \"short\" or \"fast\". There are no sharp lines for determining when to use these terms, and an individual may consider their own standards of determination. Among contemporary astronomical telescopes, any telescope with a focal ratio slower (bigger number) than f/12 is generally considered slow, and any telescope with a focal ratio faster (smaller number) than f/6, is considered fast. Faster systems often have more optical aberrations away from the center of the field of view and are generally more demanding of eyepiece designs than slower ones. A fast system is often desired for practical purposes in astrophotography with the purpose of gathering more photons in a given time period than a slower system, allowing time lapsed photography to process the result faster.\n\nWide-field telescopes (such as astrographs), are used to track satellites and asteroids, for cosmic-ray research, and for astronomical surveys of the sky. It is more difficult to reduce optical aberrations in telescopes with low f-ratio than in telescopes with larger f-ratio.\n\nThe light-gathering power of an optical telescope, also referred to as light grasp or aperture gain, is the ability of a telescope to collect a lot more light than the human eye. Its light-gathering power is probably its most important feature. The telescope acts as a \"light bucket\", collecting all of the photons that come down on it from a far away object, where a larger bucket catches more photons resulting in more received light in a given time period, effectively brightening the image. This is why the pupils of your eyes enlarge at night so that more light reaches the retinas. The gathering power formula_27 compared against a human eye is the squared result of the division of the aperture formula_2 over the observer's pupil diameter formula_29, with an average adult having a pupil diameter of 7mm. Younger persons host larger diameters, typically said to be 9mm, as the diameter of the pupil decreases with age.\n\nAn example gathering power of an aperture with 254 mm compared to an adult pupil diameter being 7 mm is given by: formula_30\n\nLight-gathering power can be compared between telescopes by comparing the areas formula_31 of the two different apertures.\n\nAs an example, the light-gathering power of a 10 meter telescope is 25x that of a 2 meter telescope: formula_32\n\nFor a survey of a given area, the field of view is just as important as raw light gathering power. Survey telescopes such as the Large Synoptic Survey Telescope try to maximize the product of mirror area and field of view (or etendue) rather than raw light gathering ability alone.\n\nThe magnification through a telescope magnifies a viewing object while limiting the FOV. Magnification is often misleading as the optical power of the telescope, its characteristic is the most misunderstood term used to describe the observable world. At higher magnifications the image quality significantly reduces, usage of a Barlow lens—which increases the effective focal length of an optical system—multiplies image quality reduction.\n\nSimilar minor effects may be present when using star diagonals, as light travels through a multitude of lenses that increase or decrease effective focal length. The quality of the image generally depends on the quality of the optics (lenses) and viewing conditions—not on magnification.\n\nMagnification itself is limited by optical characteristics. With any telescope or microscope, beyond a practical maximum magnification, the image looks bigger but shows no more detail. It occurs when the finest detail the instrument can resolve is magnified to match the finest detail the eye can see. Magnification beyond this maximum is sometimes called \"empty magnification\".\n\nTo get the most detail out of a telescope, it is critical to choose the right magnification for the object being observed. Some objects appear best at low power, some at high power, and many at a moderate magnification. There are two values for magnification, a minimum and maximum. A wider field of view eyepiece may be used to keep the same eyepiece focal length whilst providing the same magnification through the telescope. For a good quality telescope operating in good atmospheric conditions, the maximum usable magnification is limited by diffraction.\n\nThe visual magnification formula_33 of the field of view through a telescope can be determined by the telescopes focal length formula_24 divided by the eyepiece focal length formula_35 (or diameter). The maximum is limited by the focal length of the eyepiece.\n\nAn example of visual magnification using a telescope with a 1200 mm focal length and 3 mm eyepiece is given by: formula_36\n\nThere is a lowest usable magnification on a telescope. The increase in brightness with reduced magnification has a limit related to something called the exit pupil. The exit pupil is the cylinder of light coming out of the eyepiece, hence the lower the magnification, the larger the exit pupil. The minimum formula_37 can be calculated by dividing the telescope aperture formula_2 over the exit pupil diameter formula_39. Decreasing the magnification past this limit cannot increase brightness, at this limit there is no benefit for decreased magnification. Likewise calculating the exit pupil formula_39 is a division of the aperture diameter formula_2 and the visual magnification formula_33 used. The minimum often may not be reachable with some telescopes, a telescope with a very long focal length may require a \"longer-focal-length\" eyepiece than is possible.\n\nAn example of the lowest usable magnification using a 254 mm aperture and 7 mm exit pupil is given by: formula_43, whilst the exit pupil diameter using a 254 mm aperture and 36x magnification is given by: formula_44\n\nA useful reference is: \nOnly personal experience determines the best optimum magnifications for objects, relying on observational skills and seeing conditions.\n\nField of view is the extent of the observable world seen at any given moment, through an instrument (e.g., telescope or binoculars), or by naked eye. There are various expressions of field of view, being a specification of an eyepiece or a characteristic determined from and eyepiece and telescope combination. A physical limit derives from the combination where the FOV cannot be viewed larger than a defined maximum, due to diffraction of the optics.\n\nApparent FOV is the observable world observed through an ocular eyepiece without insertion into a telescope. It is limited by the barrel size used in a telescope, generally with modern telescopes that being either 1.25 or 2 inches in diameter. A wider FOV may be used to achieve a more vast observable world given the same magnification compared with a smaller FOV without compromise to magnification. Note that increasing the FOV lowers surface brightness of an observed object, as the gathered light is spread over more area, in relative terms increasing the observing area proportionally lowers surface brightness dimming the observed object. Wide FOV eyepieces work best at low magnifications with large apertures, where the relative size of an object is viewed at higher comparative standards with minimal magnification giving an overall brighter image to begin with.\n\nTrue FOV is the observable world observed though an ocular eyepiece inserted into a telescope. Knowing the true FOV of eyepieces is very useful since it can be used to compare what is seen through the eyepiece to printed or computerized star charts that help identify what is observed. True FOV formula_45 is the division of apparent FOV formula_46 over magnification formula_33.\n\nAn example of true FOV using an eyepiece with 52° apparent FOV used at 81.25x magnification is given by: formula_48\n\nMax FOV is a term used to describe the maximum useful true FOV limited by the optics of the telescope, it is a physical limitation where increases beyond the maximum remain at maximum. Max FOV formula_49 is the barrel size formula_50 over the telescopes focal length formula_24 converted from radian to degrees.\n\nAn example of max FOV using a telescope with a barrel size of 31.75 mm (1.25 inches) and focal length of 1200 mm is given by: formula_52\n\nThere are many properties of optical telescopes and the complexity of observation using one can be a daunting task; experience and experimentation are the major contributors to understanding how to maximize one's observations. In practice, only two main properties of a telescope determine how observation differs: the focal length and aperture. These relate as to how the optical system views an object or range and how much light is gathered through an ocular eyepiece. Eyepieces further determine how the field of view and magnification of the observable world change.\n\n\"Observable world\" describes what can be seen using a telescope, when viewing an object or range the observer may use many different techniques. Understanding what can be viewed and how to view it depends on the field of view. Viewing an object at a size that fits entirely in the field of view is measured using the two telescope properties—focal length and aperture, with the inclusion of an ocular eyepiece with suitable focal length (or diameter). Comparing the observable world and the angular diameter of an object shows how much of the object we see. However, the relationship with the optical system may not result in high surface brightness. Celestial objects are often dim because of their vast distance, and detail may be limited by diffraction or unsuitable optical properties.\n\nFinding what can be seen through the optical system begins with the eyepiece providing the field of view and magnification; the magnification is given by the division of the telescope and eyepiece focal lengths. Using an example of an amateur telescope such as a Newtonian telescope with an aperture formula_2 of 130 mm (5\") and focal length formula_24 of 650 mm (25.5 inches), one uses an eyepiece with a focal length formula_55 of 8 mm and apparent field of view formula_46 of 52°. The magnification at which the observable world is viewed is given by: formula_57. The true field of view formula_45 requires the magnification, which is formulated by its division over the apparent field of view: formula_59. The resulting true field of view is 0.64°, allowing an object such as the Orion nebula, which appears elliptical with an angular diameter of 65 × 60 arcminutes, to be viewable through the telescope in its entirety, where the whole of the nebula is within the observable world. Using methods such as this can greatly increase one's viewing potential ensuring the observable world can contain the entire object, or whether to increase or decrease magnification viewing the object in a different aspect.\n\nThe surface brightness at such a magnification significantly reduces, resulting in a far dimmer appearance. A dimmer appearance results in less visual detail of the object. Details such as matter, rings, spiral arms, and gases may be completely hidden from the observer, giving a far less \"complete\" view of the object or range. Physics dictates that at the theoretical minimum magnification of the telescope, the surface brightness is at 100%. Practically, however, various factors prevent 100% brightness; these include telescope limitations (focal length, eyepiece focal length, etc.) and the age of the observer.\n\nAge plays a role in brightness, as a contributing factor is the observer's pupil. With age the pupil naturally shrinks in diameter; generally accepted a young adult may have a 7 mm diameter pupil, an older adult as little as 5 mm, and a younger person larger at 9 mm. The minimum magnification formula_60 can be expressed as the division of the aperture formula_2 and pupil formula_62 diameter given by: formula_63. A problematic instance may be apparent, achieving a theoretical surface brightness of 100%, as the required effective focal length of the optical system may require an eyepiece with too large a diameter.\n\nSome telescopes cannot achieve the theoretical surface brightness of 100%, while some telescopes can achieve it using a very small-diameter eyepiece. To find what eyepiece is required to get minimum magnification one can rearrange the magnification formula, where it is now the division of the telescope's focal length over the minimum magnification: formula_64. An eyepiece of 35 mm is a non-standard size and would not be purchasable; in this scenario \nto achieve 100% one would require a standard manufactured eyepiece size of 40 mm. As the eyepiece has a larger focal length than the minimum magnification, an abundance of wasted light is not received through the eyes.\n\nThe increase in surface brightness as one reduces magnification is limited; that limitation is what is described as the exit pupil: a cylinder of light that projects out the eyepiece to the observer. An exit pupil must match or be smaller in diameter than one's pupil to receive the full amount of projected light; a larger exit pupil results in the wasted light. The exit pupil formula_65 can be derived with from division of the telescope aperture formula_2 and the minimum magnification formula_60, derived by: formula_68. The pupil and exit pupil are almost identical in diameter, giving no wasted observable light with the optical system. A 7 mm pupil falls slightly short of 100% brightness, where the surface brightness formula_50 can be measured from the product of the constant 2, by the square of the pupil formula_62 resulting in: formula_71. The limitation here is the pupil diameter; it's an unfortunate result and degrades with age. Some observable light loss is expected and decreasing the magnification cannot increase surface brightness once the system has reached its minimum usable magnification, hence why the term is referred to as \"usable\".\n\nWhen using a CCD to record observations, the CCD is placed in the focal plane. Image scale (sometimes called \"plate scale\") describes how the angular size of the object being observed is related to the physical size of the projected image in the focal plane\n\nformula_72\n\nwhere formula_73 is the image scale, formula_74 is the angular size of the observed object, and formula_75 is the physical size of the projected image. In terms of focal length image scale is\n\nformula_76\n\nwhere formula_73 is measured in radians per meter (rad/m), and formula_24 is measured in meters. Normally formula_73 is given in units of arcseconds per millimeter (\"/mm). So if the focal length is measured in millimeters, the image scale is\n\nformula_80\n\nThe derivation of this equation is fairly straightforward and the result is the same for reflecting or refracting telescopes. However, conceptually it is easier to derive by considering a reflecting telescope. If an extended object with angular size formula_74 is observed through a telescope, then due to the Laws of reflection and Trigonometry the size of the image projected onto the focal plane will be\n\nformula_82\n\nThefore, the image scale (angular size of object divided by size of projected image) will be\n\nformula_83\n\nand by using the small angle relation formula_84, when formula_85 (N.B. only valid if formula_86 is in radians), we obtain\n\nformula_87\n\nNo telescope can form a perfect image. Even if a reflecting telescope could have a perfect mirror, or a refracting telescope could have a perfect lens, the effects of aperture diffraction are unavoidable. In reality, perfect mirrors and perfect lenses do not exist, so image aberrations in addition to aperture diffraction must be taken into account. Image aberrations can be broken down into two main classes, monochromatic, and polychromatic. In 1857, Philipp Ludwig von Seidel (1821–1896) decomposed the first order monochromatic aberrations into five constituent aberrations. They are now commonly referred to as the five Seidel Aberrations.\n\n\nOptical defects are always listed in the above order, since this expresses their interdependence as first order aberrations via moves of the exit/entrance pupils. The first Seidel aberration, Spherical Aberration, is independent of the position of the exit pupil (as it is the same for axial and extra-axial pencils). The second, coma, changes as a function of pupil distance and spherical aberration, hence the well-known result that it is impossible to correct the coma in a lens free of spherical aberration by simply moving the pupil. Similar dependencies affect the remaining aberrations in the list.\n\nOptical telescopes have been used in astronomical research since the time of their invention in the early 17th century. Many types have been constructed over the years depending on the optical technology, such as refracting and reflecting, the nature of the light or object being imaged, and even where they are placed, such as space telescopes. Some are classified by the task they perform such as Solar telescopes.\n\nNearly all large research-grade astronomical telescopes are reflectors. Some reasons are: \n\nMost large research reflectors operate at different focal planes, depending on the type and size of the instrument being used. These including the prime focus of the main mirror, the cassegrain focus (light bounced back down behind the primary mirror), and even external to the telescope all together (such as the Nasmyth and coudé focus).\n\nA new era of telescope making was inaugurated by the Multiple Mirror Telescope (MMT), with a mirror composed of six segments synthesizing a mirror of 4.5 meters diameter. This has now been replaced by a single 6.5 m mirror. Its example was followed by the Keck telescopes with 10 m segmented mirrors.\n\nThe largest current ground-based telescopes have a primary mirror of between 6 and 11 meters in diameter. In this generation of telescopes, the mirror is usually very thin, and is kept in an optimal shape by an array of actuators (see active optics). This technology has driven new designs for future telescopes with diameters of 30, 50 and even 100 meters.\n\nRelatively cheap, mass-produced ~2 meter telescopes have recently been developed and have made a significant impact on astronomy research. These allow many astronomical targets to be monitored continuously, and for large areas of sky to be surveyed. Many are robotic telescopes, computer controlled over the internet (see \"e.g.\" the Liverpool Telescope and the Faulkes Telescope North and South), allowing automated follow-up of astronomical events.\n\nInitially the detector used in telescopes was the human eye. Later, the sensitized photographic plate took its place, and the spectrograph was introduced, allowing the gathering of spectral information. After the photographic plate, successive generations of electronic detectors, such as the charge-coupled device (CCDs), have been perfected, each with more sensitivity and resolution, and often with a wider wavelength coverage.\n\nCurrent research telescopes have several instruments to choose from such as: \n\nThe phenomenon of optical diffraction sets a limit to the resolution and image quality that a telescope can achieve, which is the effective area of the Airy disc, which limits how close two such discs can be placed. This absolute limit is called the diffraction limit (and may be approximated by the Rayleigh criterion, Dawes limit or Sparrow's resolution limit). This limit depends on the wavelength of the studied light (so that the limit for red light comes much earlier than the limit for blue light) and on the diameter of the telescope mirror. This means that a telescope with a certain mirror diameter can theoretically resolve up to a certain limit at a certain wavelength. For conventional telescopes on Earth, the diffraction limit is not relevant for telescopes bigger than about 10 cm. Instead, the seeing, or blur caused by the atmosphere, sets the resolution limit. But in space, or if adaptive optics are used, then reaching the diffraction limit is sometimes possible. At this point, if greater resolution is needed at that wavelength, a wider mirror has to be built or aperture synthesis performed using an array of nearby telescopes.\n\nIn recent years, a number of technologies to overcome the distortions caused by atmosphere on ground-based telescopes have been developed, with good results. See adaptive optics, speckle imaging and optical interferometry.\n\n"}
{"id": "417675", "url": "https://en.wikipedia.org/wiki?curid=417675", "title": "Philip Eaton", "text": "Philip Eaton\n\nPhilip E. Eaton (born 1936) is a Professor Emeritus of Chemistry at the University of Chicago. He and his fellow researchers were the first to synthesize the \"impossible\" cubane molecule in 1964.\n\nWorking with Mao-Xi Zhang he is reported as having been the first to make octanitrocubane (their paper was published in the year 2003) Because of its eight nitro groups and highly strained C-C bonds - octanitrocubane is a very powerful high explosive.\n\nPhilip E. Eaton was born in 1936 in Brooklyn, New York. When Eaton was seven his family relocated to Budd Lake, New Jersey. Here he began attending Roxbury Grammar School and later Roxbury High School. It was during these high school years that he began to find his passion for science. It was the support of his parents and teachers that made him decide to major in chemistry.\n\nEaton attended Princeton University seeking a major in chemistry. Eaton received his B.A. in 1957 before attending Harvard University and earning his M.A. in 1960 and Ph.D. in 1961. During his time in school he became familiar with cage chemistry, specifically Kepone.\n\nUpon graduating from Harvard Eaton accepted an assistant professorship position at the University of California, Berkeley. During this time he taught introductory organic chemistry. In 1962, he transferred to the University of Chicago where he remains today.\n\nAfter arriving at University of Chicago Eaton began his research which he is now most well known for, cubane synthesis. In 1964 Eaton and Thomas W. Cole Jr. synthesized the \"impossible\" cubane molecule. It was given this name because of its unusual cubic geometry. Many scientists believed that the 90 degree bond-angles would be too strained to allow this molecule to form. He later studied larger prismanes.\n\n\n"}
{"id": "48955231", "url": "https://en.wikipedia.org/wiki?curid=48955231", "title": "Plant press", "text": "Plant press\n\nA plant press is a set of equipment used by botanists to flatten and dry field samples so that they can be easily stored. A professional plant press is made to the standard maximum size for biological specimens to be filed in a particular herbarium. A flower press is a similar device of no standard size that is used to make flat dried flowers for pressed flower craft.\n\nSpecimens prepared in a plant press are later glued to archival-quality card stock top their labels, and are filed in a herbarium. Labels are made with archival ink (or pencil) and paper, and attached with archival-quality glue.\n\nA modern plant press consists of two strong outer boards with straps that can be tightened around them to exert pressure. Between the boards, fresh plant samples are placed, carefully labelled, between layers of paper. Further layers of absorbent paper and corrugated cardboard are usually added to help to dry the samples as quickly as possible, which prevents decay and improves colour retention. Layers of a sponge material can be used in order to prevent squashing parts of the specimens, such as fruit. Older plant presses and some modern flower presses have screws to supply the pressure, which often limits the thickness of the stack of samples that can be put into one press.\n\nLuca Ghini (1490—1556) Italian physician and botanist, created the first recorded herbarium, and is considered the first person to have used drying under pressure to prepare a plant collection.\n\nWilliam Withering English botanist, geologist, chemist and physician wrote popular books on British botany, and by describing the screw-down plant press (and the vasculum) he brought it to the attention of amateur naturalists in Britain around 1770.\n\n"}
{"id": "13937733", "url": "https://en.wikipedia.org/wiki?curid=13937733", "title": "Poikilohydry", "text": "Poikilohydry\n\nPoikilohydry is the lack of ability (structural or functional mechanism) to maintain and/or regulate water content to achieve homeostasis of cells and tissue connected with quick equilibration of cell/tissue water content to that of the environment. Frequently, it is coupled with the capacity to tolerate dehydration to low cell or tissue water content and to recover from it without physiological damage. This condition occurs in such organisms as the lichens and bryophytes that lack mechanisms such as a waterproofing cuticle or stomata that can help resist desiccation. Poikilohydry also occurs in many forms of algae, which may be able to survive desiccation between successive high tides, or during occasional stranding due to the drying of a lake or pond. Similarly, poikilohydry occurs in land plants which survive environmental conditions when water supplies are seasonal or intermittent, as in the liverwort genus \"Targionia\", which lives in Mediterranean habitats with hot dry summers.\n\nThe term is derived from Ancient Greek ποικίλος (poikílos, “spotted or variegate”). The antonym of poikilohydry is homoiohydry, a suite of morphological adaptations and strategies that enable plants to regulate or achieve homeostasis of cell and tissue water content. The vascular plants have largely lost the capacity to tolerate dehydration. Aside from most seeds and spores, only about 300 species of vascular plants are desiccation-tolerant, including resurrection plants (such as \"Selaginella lepidophylla\") and aerophytes (including some species of \"Tillandsia\").\n"}
{"id": "2311300", "url": "https://en.wikipedia.org/wiki?curid=2311300", "title": "Radio-activated guard box", "text": "Radio-activated guard box\n\nRadio activated guard (RAG) boxes are experimental pest control devices intended to deter wolves from preying on livestock. Specifically, they are designed to work against wolves which have been fitted with radio tracking collars prior to being re-released into the wild (and, by extension, the packs of which they are members). The device was conceived by Edward Cummings, a rancher from Montana, who suggested that a hazing device could be tuned to a radio collar's frequency; after discussions with ranchers, the specifications of the device were designed and prototyped by Dr. John Shivik, then with the United States Department of Agriculture Wildlife Services, National Wildlife Research Center. Very few of the devices have been produced for commercial sale. \n\nThe RAG box is a \"disruptive stimulus device.\" It uses a strobe light and two loudspeakers which emit an annoying noise; these are activated when the box detects the signal from a radio collar at short range, and scare off the wolf pack.\n\nThe boxes were subjected to limited testing on wolves in Idaho and researchers concluded that they are effective for\nprotecting livestock in small pastures; the technology is thought to be limited, however, because of the complexity of the device and its price.\n\n"}
{"id": "45492749", "url": "https://en.wikipedia.org/wiki?curid=45492749", "title": "Registered Science Technician", "text": "Registered Science Technician\n\nRegistered Science Technician (RSciTech) is a professional qualification for science technicians that was introduced in 2011 alongside Registered Scientist as an extension to the UK Science Council's existing professional register for Chartered Scientists. the Registered Science Technician (RSciTech) was developed with the support of the Gatsby Charitable Foundation, with the aim of increasing the professionalism and recognition of those working in technical roles in science. Holders of this qualification can use the post-nominal letters RSciTech. Registration as RSciTech has been encouraged by institutions such as Imperial College London, and the UK Government's \"Science manufacturing technician\" and \"Laboratory technician\" apprenticeship standards are designed to lead to registration as an RSciTech.\n\nThe Science Council licences its member bodies to award professional statuses. The professional bodies listed below are those licensed to award Registered Science Technician as of May 2017:\n"}
{"id": "18205351", "url": "https://en.wikipedia.org/wiki?curid=18205351", "title": "Sarah Featon", "text": "Sarah Featon\n\nSarah Ann Featon (née Porter, 1848 – 28 April 1927) was an accomplished botanical artist from New Zealand.\n\nThere are few records of Featon's early life. She was of English origin, the daughter of Henry William Porter. It is unclear when she arrived in New Zealand but she is recorded as having married Edward Featon at St Paul's, Auckland in 1870. In 1875 Featon and her husband moved to Gisborne, as Edward had been appointed as the area's first District Land Officer.\n\nDuring this time Featon and her husband began work on their seminal work \"The Art Album of New Zealand Flora\". Featon painted the watercolours for the plates while her husband wrote the text. The Featons set out to produce their album to debunk the widely held belief that there were no flowers in New Zealand.\nReverend William Colenso, a prominent early settler and noted expert on botany, and Archdeacon (later Bishop) W. L . Williams were keen supporters of the book and supplied specimens to Featon to paint. Colenso named a newly discovered species, Dracophyllum featonium, in her honour. Unfortunately this species is now regarded as being synonymous with Dracophyllum strictum and therefore the compliment became redundant.\n\nThe album was the first full-colour art book to be published in New Zealand. It contained systematic and popular descriptions of the native flowering plants of New Zealand and the adjacent islands. Featon created all of the artwork for the book and commissioned the chromolithography for the book plates from the workshop of Bock and Cousins, Wellington. The album was originally published in three parts, the first part being released in November 1887 and the next two in 1888. The three parts were issued as a single volume in 1889.\n\nA copy of the book was presented by the New Zealand Government to Queen Victoria in 1897 on the occasion of her diamond jubilee. That copy is now in the British Museum.\n\nFeaton suffered financial hardship later in life and sold the original artwork for the book to the Dominion Museum - now the Museum of New Zealand Te Papa Tongarewa where they continue to be held. She died in Gisborne on 28 April 1927, and was buried at Makaraka Cemetery.\n\n"}
{"id": "2255858", "url": "https://en.wikipedia.org/wiki?curid=2255858", "title": "Somatic marker hypothesis", "text": "Somatic marker hypothesis\n\nThe somatic marker hypothesis, formulated by Alice, proposes that emotional processes guide (or bias) behavior, particularly decision-making.\n\n\"Somatic markers\" are feelings in the body that are associated with emotions, such as the association of rapid heartbeat with anxiety or of nausea with disgust. According to the hypothesis, somatic markers strongly influence subsequent decision-making. Within the brain, somatic markers are thought to be processed in the ventromedial prefrontal cortex (VMPFC) and the amygdala. The hypothesis has been tested in experiments using the Iowa gambling task.\n\nIn economic theory, human decision-making is often modeled as being devoid of emotions, involving only logical reasoning based on cost-benefit calculations. In contrast, the somatic marker hypothesis proposes that emotions play a critical role in the ability to make fast, rational decisions in complex and uncertain situations.\n\nPatients with frontal lobe damage, such as Phineas Gage, provided the first evidence that the frontal lobes were associated with decision-making. Frontal lobe damage, particularly to the Ventromedial prefrontal cortex (vmPFC), results in impaired abilities to organize and plan behavior and learn from previous mistakes, without affecting intellect in terms of working memory, attention, and language comprehension and expression.\n\nvmPFC patients also have difficulty expressing and experiencing appropriate emotions. This led Antonio Damasio to hypothesize that decision-making deficits following vmPFC damage result from the inability to use emotions to help guide future behavior based on past experiences. Consequently, vmPFC damage forces those afflicted to rely on slow and laborious cost-benefit analyses for every given choice situation.\n\nWhen individuals make decisions, they must assess the incentive value of the choices available to them, using cognitive and emotional processes. When the individuals face complex and conflicting choices, they may be unable to decide using only cognitive processes, which may become overloaded. Emotions, consequently, are hypothesized to guide decision-making.\n\nEmotions, as defined by Damasio, are changes in both body and brain states in response to stimuli. Physiological changes (such as muscle tone, heart rate, endocrine activity, posture, facial expression, and so forth) occur in the body and are relayed to the brain where they are transformed into an emotion that tells the individual something about the stimulus that they have encountered. Over time, emotions and their corresponding bodily changes, which are called \"somatic markers\", become associated with particular situations and their past outcomes.\n\nWhen making subsequent decisions, these somatic markers and their evoked emotions are consciously or unconsciously associated with their past outcomes, and influence decision-making in favor of some behaviors instead of others. For instance, when a somatic marker associated with a positive outcome is perceived, the person may feel happy and thereby motivated to pursue that behavior. When a somatic marker associated with the negative outcome is perceived, the person may feel sad, which acts as an internal alarm to warn the individual to avoid that course of action. These situation-specific somatic states based on, and reinforced by, past experiences help to guide behavior in favor of more advantageous choices, and therefore are adaptive.\n\nAccording to the hypothesis, two distinct pathways reactivate somatic marker responses. In the first pathway, emotion can be evoked by changes in the body that are projected to the brain – called the \"body loop\". For instance, encountering a feared object like a snake may initiate the fight-or-flight response and cause fear. In the second pathway, cognitive representations of the emotions (imagining an unpleasant situation \"as-if\" you were in that particular situation) can be activated in the brain without being directly elicited by a sensory stimulus – called the \"as-if body loop\". Thus, the brain can anticipate expected bodily changes, which allows the individual to respond faster to external stimuli without waiting for an event to actually occur. The amygdala and VMPFC (a subsection of the orbitomedial prefrontal cortex or OMPFC) are essential components of this hypothesized mechanism, and therefore damage to either structure will disrupt decision-making.\n\nIn an effort to produce a simple neuropsychological tool that would assess deficits in emotional processing, decision-making, and social skills of OMPFC-lesioned individuals, Bechara and collaborators created the Iowa gambling task. The task measures a form of emotion-based learning. Studies using the gambling task have found deficits in various neurological (such as amygdala and OMPFC lesions) and psychiatric populations (such as schizophrenia, mania, and drug abusers).\n\nThe Iowa gambling task is a computerized test in which participants are presented with four decks of cards from which they repeatedly choose. Each deck contains various amounts of rewards of either $50 or $100, and occasional losses that are greater in the decks with higher rewards. The participants do not know where the penalty cards are located, and are told to pick cards that will maximize their winnings. The most profitable strategy turns out to be to choose cards only from the small reward/small penalty decks, because although the reward is smaller, the penalty is proportionally much smaller than in the high reward/high penalty decks. Over the course of a session, most healthy participants come to adopt the profitable low-penalty deck strategy. Participants with brain damage, however, are unable to determine the better deck to choose from, and continue to choose from the high reward/high penalty decks.\n\nSince the Iowa gambling task measures participants' quickness in \"developing anticipatory emotional responses to guide advantageous choices\", it is helpful in testing the somatic marker hypothesis. According to the hypothesis, somatic markers give rise to anticipation of the emotional consequences of a decision being made. Consequently, persons who perform well on the task are thought to be aware of the penalty cards and of the negative emotions associated with drawing such cards, and to realize which deck is less likely to yield a penalty.\n\nThis experiment has been used to analyze the impairments suffered by people with damage to the ventromedial prefrontal cortex, which has been known to affect neural signaling of prospective rewards or punishments. Such persons perform less well on the task. Functional magnetic resonance imaging (fMRI) has been used to analyze the brain during the Iowa gambling task. The brain regions that were activated during the Iowa gambling task were also the ones hypothesized to be triggered by somatic markers during decision-making.\n\nDamasio has posited that the ability of humans to perform abstract thinking quickly and efficiently coincides with both the development of the ventromedial (VM) cortex and with the use of somatic markers to guide human behavior during evolution. Patients with damage to the VM cortices are more likely to engage in behaviors that negatively impact personal relationships in the distant future, but they never engage in actions that would lead to immediate harm to themselves or others. The evolution of the prefrontal cortex was associated with the ability to represent events that may occur in the future.\n\nThe somatic marker hypothesis has been applied to trying to understand risky behaviors, such as risky sexual behavior and drug addiction.\n\nAccording to the hypothesis, riskier sexual behaviors are more exhilarating and pleasurable, and therefore they are more likely to stimulate repetitive engagement in such behaviors. When this idea was tested in individuals who were infected with HIV and were substance dependent, differences were found between persons who scored well in the Iowa gambling test, and those who scored poorly. The high scorers showed a correlation between the amount of distress they reported having over their HIV status, and their acceptance of risk during sexual behavior – the greater the distress, the greater the risk that these people would take. The low scorers, on the other hand, showed no such correlation. These results were interpreted as indicating that persons with intact decision-making abilities are better able to rely on past emotional experiences when weighing risks, than are persons who are deficient in such abilities, and that acceptance of risk serves to ameliorate emotional distress.\n\nDrug abusers are thought to ignore the negative consequences of addiction while seeking drugs. According to the somatic marker hypothesis, such abusers are impaired in their ability to recall and consider past unpleasant experiences when weighing whether to consider drug seeking behaviors. Researchers analyzed the neuroendocrine responses of substance-dependent individuals and healthy individuals after being shown pleasant or unpleasant images. In response to unpleasant images, drug users showed decreased levels of several neuroendocrine markers, including norepinephrine, cortisol, and adrenocorticotropic hormone. Addicts showed lesser responses to both pleasant and unpleasant images, suggesting that they may have a diminished emotional response. Neuroimaging studies utilizing fMRI indicate that drug-related stimuli have the ability to activate brain regions involved in emotional evaluation and reward processing. When shown a film of people smoking cocaine, cocaine users showed greater activation of the anterior cingulate cortex, the right inferior parietal lobe, and the caudate nucleus than did non-users. Conversely, the cocaine users showed lesser activation when viewing a sex film than did non-users.\n\nSome researchers believe that the use of somatic markers (i.e., afferent feedback) would be a very inefficient method of influencing behavior. Damasio's notion of the \"as-if\" experience dependent feedback route, whereby bodily responses are re-represented utilizing the somatosensory cortex (postcentral gyrus), also proposes an inefficient method of affecting explicit behavior. Rolls (1999) stated that; \"it would be very inefficient and noisy to place in the execution route a peripheral response, and transducers to attempt to measure that peripheral response, itself a notoriously difficult procedure\" (p. 73). Reinforcement association located in the orbitofrontal cortex and amygdala, where the incentive value of stimuli is decoded, is sufficient to elicit emotion-based learning and to affect behavior via, for example, the orbitofrontal-striatal pathway. This process can occur via implicit or explicit processes.\n\nThe somatic marker hypothesis represents a model of how feedback from the body may contribute to both advantageous and disadvantageous decision-making in situations of complexity and uncertainty. Much of its supporting data comes from data taken from the Iowa gambling task. While the Iowa gambling task has proven to be an ecologically valid measure of decision-making impairment, there exist three assumptions that need to hold true. \n\nFirst, the claim that it assesses implicit learning as the reward/punishment design is inconsistent with data showing accurate knowledge of the task possibilities and that mechanisms such as working-memory appear to have a strong influence. Second, the claim that this knowledge occurs through preventive marker signals is not supported by competing explanations of the psychophysiology generated profile. Lastly, the claim that the impairment is due to a 'myopia for the future' is undermined by more plausible psychological mechanisms explaining deficits on the tasks such as reversal learning, risk-taking, and working-memory deficits. There may also be more variability in control performance than previously thought, thus complicating the interpretation of the findings. \n\nFurthermore, although the somatic marker hypothesis has accurately identified many of the brain regions involved in decision-making, emotion, and body-state representation, it has failed to clearly demonstrate how these processes interact at a psychological and evolutionary level. There are many experiments that could be implemented to further test the somatic marker hypothesis. One way would be to develop variants of the Iowa gambling task that control some of the methodological issues and interpretation ambiguities generated. It may be a good idea to include removing the reversal learning confound, which would make the task more difficult to consciously comprehend. Additionally, causal tests of the somatic marker hypothesis could be practiced more insistently in a greater range of populations with altered peripheral feedback, like on patients with facial paralysis. \n\nIn conclusion, the somatic marker hypothesis needs to be tested in more experiments. Until a wider range of empirical approaches are employed in order to test the somatic marker hypothesis, it appears that the framework is simply an intriguing idea that is in need of some better supporting evidence. Despite these issues, the somatic marker hypothesis and the Iowa gambling task reestablish the notion that emotion has the potential to be a benefit as well as a problem during the decision-making process in humans.\n"}
{"id": "3538896", "url": "https://en.wikipedia.org/wiki?curid=3538896", "title": "Sphaerobacter", "text": "Sphaerobacter\n\nSphaerobacter is a genus of bacteria. When originally described it was placed in its own subclass (Spahaerobacteridae) within the class Actinobacteria. Subsequently, phylogenetic studies have now placed it in its own order Sphaerobacterales within the phylum Chloroflexi. Up to now there is only one species of this genus known (Sphaerobacter thermophilus).\n"}
{"id": "57426192", "url": "https://en.wikipedia.org/wiki?curid=57426192", "title": "Tamsin Edwards", "text": "Tamsin Edwards\n\nTamsin Edwards is a British climate scientist and lecturer at King's College London. She is a popular science communicator and writes for the Public Library of Science (PLOS).\n\nEdwards became interested in physics after reading \"A Brief History of Time\". The daughter of Michael Edwards, she completed A-Levels in Physics, Chemistry and Maths at St Margaret's School, in Exeter. She studied physics in the School of Physics and Astronomy at the University of Manchester. She completed a PhD in Particle Physics at the University of Manchester under the supervision of Brian Cox. Her thesis investigated the production of Z-Bosons, detected by their subsequent decay to muons, using data collected at the Tevatron.\n\nEdwards joined the Open University as a lecturer, working in the Palaeoenvironmental Change team. She uses computer models to predict and study climate change, with a particular interest in the impact on sea level rise of changes in the Antarctic ice sheet. She studied how a glacier's grounding line (the point at which is separates from a continent's bedrock and floats into the sea) affects the rate of flow of glaciers, and estimated the effects of positive feedback. In 2017 Edwards joined King's College London as a lecturer in geography. She will be a lead author for Chapter 9 (\"Ocean, cryosphere, and sea level change\") of the sixth assessment report of the Intergovernmental Panel on Climate Change.\n\nEdwards writes a popular science blog hosted by the Public Library of Science (PLOS). She has written for \"The Guardian\" and contributed chapters to books about climate change. Working with the Met Office, Edwards created educational resources about sea level rise for the 2017 United Nations Climate Change Conference (\"COP23\").\n\nIn 2014 she gave a TEDx talk at CERN, \"How to Love Uncertainty in Climate Science.\" After fights between climate scientists and sceptics on Twitter in 2014, Edwards was part of a dinner party discussing how they could calm the debate. The dinner included David Rose and Richard A. Betts, and Edwards was the only woman. In 2015 she was celebrated as one of twenty women \"making waves\" at the 2015 United Nations Climate Change Conference. She won the 2016 British Science Association Charles Lyell Award for Environmental Sciences. She discussed how computer models can be used to predict ice sheet collapse and how to communicate uncertainty. In 2017 she was profiled in the \"HuffPost\" Australia's \"Breaking The Ice\" series. She is a speaker at the 2018 Bluedot Festival.\n\nEdwards has acted as a scientific consultant for the BBC. She was a consultant on the BBC's \"Climate Change by Numbers\", which won an American Association for the Advancement of Science award for Science Journalism, and a 2015 award for \"Best Presentation of Science in an Environment Issue\" from EuroPAWS. She has appeared on BBC Radio 4 and BBC World Service.\n"}
{"id": "39701360", "url": "https://en.wikipedia.org/wiki?curid=39701360", "title": "Timeline of egg fossil research", "text": "Timeline of egg fossil research\n\nThis timeline of egg fossils research is a chronologically ordered list of important discoveries, controversies of interpretation, taxonomic revisions, and cultural portrayals of egg fossils. Humans have encountered egg fossils for thousands of years. In stone age Mongolia, local peoples fashioned fossil dinosaur eggshell into jewelry. In the Americas, fossil eggs may have inspired Navajo creation myths about the human theft of a primordial water monster's egg. Nevertheless, the scientific study of fossil eggs began much later. As reptiles, dinosaurs were presumed to have laid eggs from the 1820s on, when their first scientifically documented remains were being described in England. In 1859, the first scientifically documented dinosaur \"egg\" fossils were discovered in southern France by a Catholic priest and amateur naturalist named Father Jean-Jacques Poech, however he thought they were laid by giant birds.\n\nThe first scientifically recognized dinosaur egg fossils were discovered serendipitously in 1923 by an American Museum of Natural History crew while looking for evidence of early humans in Mongolia. These eggs were mistakenly attributed to the locally abundant herbivore \"Protoceratops\", but are now known to be \"Oviraptor\" eggs. Egg discoveries continued to mount all over the world, leading to the development of multiple competing classification schemes. In 1975 Chinese paleontologist Zhao Zi-Kui started a revolution in fossil egg classification by developing a system of \"parataxonomy\" based on the traditional Linnaean system to classify eggs based on their physical qualities rather than their hypothesized mothers. Zhao's new method of egg classification was hindered from adoption by Western scientists due to language barriers. However, in the early 1990s Russian paleontologist Konstantin Mikhailov brought attention to Zhao's work in the English language scientific literature.\n\nLate Paleolithic to early Neolithic\n\nPrecolumbian North America\n\n1859\n\n1869\n\n1913\n\n1919\n\n1922\n\n1923\n\n1939\n\n1946\n\n1957\n\n1964\n\n1966\n\n1969\n\n1970\n\nThey also found that the carbon in the eggshell is mostly the heavier Carbon 13 rather than the lighterCarbon 12. This means the dinosaur were primarily feeding on C3 plants which use 3 carbon atoms in their photosynthesis products rather than C4 plants that use four.\n1975\n\n1978\n\n1979\n\n1991\n\nEarly to mid-1990s\n\n2009\n\n\n"}
{"id": "2264196", "url": "https://en.wikipedia.org/wiki?curid=2264196", "title": "Timeline of psychotherapy", "text": "Timeline of psychotherapy\n\nThis article is a compiled timeline of psychotherapy. A more general description of the development of the subject of psychology can be found in the History of psychology article. For related overviews see the Timeline of psychology and Timeline of psychiatry articles.\n\n\n\n\n\n\n\ncredited as providing the first clinical/scientific mention of the unconscious in his work Von den Krankeiten.\nParacelsus called for the humane treatment of the mentally ill (but was ignored for several centuries) as he saw \nthem not to be possessed by evil spirits, but merely 'brothers' ensnared in a treatable malady.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "50571", "url": "https://en.wikipedia.org/wiki?curid=50571", "title": "Transportation engineering", "text": "Transportation engineering\n\nTransportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods (transport) . It is a sub-discipline of civil engineering. The importance of transportation engineering within the civil engineering profession can be judged by the number of divisions in ASCE (American Society of Civil Engineers) that are directly related to transportation. There are six such divisions (Aerospace; Air Transportation; Highway; Pipeline; Waterway, Port, Coastal and Ocean; and Urban Transportation) representing one-third of the total 18 technical divisions within the ASCE (1987).\n\nThe planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors. Technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (how many trips for what purpose), trip distribution (destination choice, where is the traveler going), mode choice (what mode is being taken), and route assignment (which streets or routes are being used). More sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). Passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system.\n\nA review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important to civil engineers, particularly to those working in highway and urban transportation. The National Council of Examiners for Engineering and Surveying (NCEES) list online the safety protocols, geometric design requirements, and signal timing.\n\nTransportation engineering, as practiced by civil engineers, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. The facilities support air, highway, railroad, pipeline, water, and even space transportation. The design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track).\n\nBefore any planning occurs the Engineer must take what is known as an inventory of the area or if it is appropriate, the previous system in place. This inventory or database must include information on (1)population, (2)land use, (3)economic activity, (4)transportation facilities and services, (5)travel patterns and volumes, (6)laws and ordinances, (7)regional financial resources, (8)community values and expectations. These inventories help the engineer create business models to complete accurate forecasts of the future conditions of the systemReview.\n\nOperations and management involve traffic engineering, so that vehicles move smoothly on the road or track. Older techniques include signs, signals, markings, and tolling. Newer technologies involve intelligent transportation systems, including advanced traveler information systems (such as variable message signs), advanced traffic control systems (such as ramp meters), and vehicle infrastructure integration. Human factors are an aspect of transportation engineering, particularly concerning driver-vehicle interface and user interface of road signs, signals, and markings.\n\nEngineers in this specialization:\n\nRailway engineers handle the design, construction, and operation of railroads and mass transit systems that use a fixed guideway (such as light rail or even monorails). Typical tasks would include determining horizontal and vertical alignment design, station location and design, and construction cost estimating. Railroad engineers can also move into the specialized field of train dispatching which focuses on train movement control.\n\nRailway engineers also work to build a cleaner and safer transportation network by reinvesting and revitalizing the rail system to meet future demands. In the United States, railway engineers work with elected officials in Washington, D.C. on rail transportation issues to make sure that the rail system meets the country's transportation needs.\n\nPort and harbor engineers handle the design, construction, and operation of ports, harbors, canals, and other maritime facilities.\n\nAirport engineers design and construct airports. Airport engineers must account for the impacts and demands of aircraft in their design of airport facilities. These engineers must use the analysis of predominant wind direction to determine runway orientation, determine the size of runway border and safety areas, different wing tip to wing tip clearances for all gates and must designate the clear zones in the entire port.\n\n"}
{"id": "807902", "url": "https://en.wikipedia.org/wiki?curid=807902", "title": "William Stephen Finsen", "text": "William Stephen Finsen\n\nWilliam Stephen Finsen (28 July 1905 – 16 May 1979) was a South African astronomer.\n\nHe discovered a number of double stars and took many photographs of Mars. He developed the Finsen eyepiece interferometer to measure very close double stars. He was the final director of Union Observatory in South Africa from 1957 until it closed in 1965 (it was renamed Republic Observatory in 1961); he had started there 55 years previous. \n\nThe asteroid 1794 Finsen is named after him, as is the geological feature Finsen Dorsum on the asteroid 433 Eros.\n\n"}
{"id": "549356", "url": "https://en.wikipedia.org/wiki?curid=549356", "title": "World Data Center", "text": "World Data Center\n\nThe World Data Centre (WDC) system was created to archive and distribute data collected from the observational programmes of the 1957-1958 International Geophysical Year by the International Council of Science (ICSU). The WDCs were funded and maintained by their host countries on behalf of the international science community.\n\nOriginally established in the United States, Europe, Soviet Union, and Japan, the WDC system expanded to other countries and to new scientific disciplines. The WDC system included up to 52 Centres in 12 countries. All data held in WDCs were available for the cost of copying and sending the requested information.\n\nAt the end of 2008, following the ICSU General Assembly in Maputo (Mozambique), the World Data Centres were reformed and a new ICSU World Data System (WDS) established in 2009 building on the 50-year legacy of the ICSU World Data Centre system (WDC) and the ICSU Federation of Astronomical and Geophysical data-analysis Services.\n\n\n"}
