{"id": "2760814", "url": "https://en.wikipedia.org/wiki?curid=2760814", "title": "Abeona Mons", "text": "Abeona Mons\n\nAbeona Mons is a mountain on Venus named after the goddess Abeona.\n"}
{"id": "13293216", "url": "https://en.wikipedia.org/wiki?curid=13293216", "title": "Alternate-Phase Return-to-Zero", "text": "Alternate-Phase Return-to-Zero\n\nAlternate-Phase Return-to-Zero (APRZ) is an optical line code.\n\nIn APRZ the field intensity drops to zero between consecutive bits (RZ), and the field phase alternates between neighbouring bits (AP), so that if the phase of the signal is, for example, 0 in even bits (bit number 2\"n\"), the phase in odd bit slots (bit number 2\"n\"+1) will be \"ΔΦ\", the phase alternation amplitude.\n\nReturn-to-zero (RZ) can be seen as a special case of APRZ in which \"ΔΦ\"=0, while Carrier-Suppressed Return-to-Zero (CSRZ) can be viewed as a special case of APRZ in which \"ΔΦ\"=π (and the duty cycle is 67%, at least in the standard form of CSRZ).\n\nAPRZ can be used to generate specific optical modulation formats, for example, APRZ-OOK, in which data is coded on the intensity of the signal using a binary scheme (light on=1, light off=0). APRZ is often used to designate APRZ-OOK.\n\nThe characteristic properties of an APRZ signal are those to have a spectrum similar to that of an RZ signal, except that frequency peaks at a spacing of \"B\"/2 as opposed to \"B\" are observed (where \"B\" is the bit rate).\n\nAPRZ-OOK is considered to be more tolerant to pulse-to-pulse (intra-channel) non-linear impairments, with respect to other OOK modulation formats.\n"}
{"id": "6077632", "url": "https://en.wikipedia.org/wiki?curid=6077632", "title": "Amaranthus brownii", "text": "Amaranthus brownii\n\nAmaranthus brownii is an annual herb in the Amaranthaceae family. The plant is found only on the small island of Nihoa in the Northwestern Hawaiian Islands, growing on rocky outcrops at altitudes of . It is one of nine species of \"Amaranthus\" in the Hawaiian Islands, but the only endemic Hawaiian species of the genus. It was first discovered during the Tanager Expedition in 1923 by botanist Edward Leonard Caum. \"A. brownii\" differs from other Hawaiian species of \"Amaranthus\" with its spineless leaf axils, linear leaves, and indehiscent fruits.\n\nIt is one of 26 vascular plants on Nihoa, 17 of which are indigenous, six alien, and three endemic only to Nihoa, including \"A. brownii\", the Nihoa fan palm or \"loulu\", and the Nihoa carnation. \"A. brownii\" is considered the rarest plant on Nihoa and has not been directly observed on the island since 1983. Past expeditions collected plant samples and seeds, but no specimens have managed to survive ex-situ conservation efforts outside of its native habitat. There are no known plants or seeds from \"A. brownii\" in any botanical gardens.\n\nConservation and recovery plans for \"A. brownii\" have been proposed by the United States Fish and Wildlife Service (FWS) which administers the island of Nihoa as part of the Hawaiian Islands National Wildlife Refuge in the Papahānaumokuākea Marine National Monument. In 1996, the plant was listed by the FWS as an endangered species. In 2003, the FWS designated the island of Nihoa as a critical habitat for the plant and it was classified as critically endangered on the IUCN Red List. The plant is one of 51 endangered or threatened plants in the Hawaiian Islands listed under the Endangered Species Act.\n\nThe species was first collected during a ten-day visit to the island of Nihoa by the Tanager Expedition. Botanist Edward Leonard Caum collected the first specimen on June 17, 1923, and a second was collected by cartographer Charles S. Judd on June 20, 1923. Forest B. H. Brown, botanist of the Bayard Dominick Expedition to the Marquesas Islands (1921–1922), helped provide descriptions and comments for some of the species described by Erling Christophersen and Caum. They named \"A. brownii\" after Brown in 1931 with the publication of their paper \"Vascular plants of the Leeward Islands, Hawaii\". In the paper they originally described \"A. brownii\" as one of 20 vascular plant species on the island of Nihoa. The FWS does not recognize a common name.\n\n\"Amaranthus brownii\" is the only endemic species of Hawaiian \"Amaranthus\" in the Hawaiian Islands. It is an herbaceous annual plant that grows to a height of and has narrow, linear leaves, small green flowers, and fruit that holds a single, dark red seed. \"A. brownii\" is monoecious; that is, the male and female flowers are found together on the same plant. It differs from other Hawaiian species of \"Amaranthus\" with its spineless leaf axils, linear leaves, and indehiscent fruits (fruit which does not open to release seeds when ripe). The fruits are ovoid and between 0.8–1 mm long and 0.6–0.8 mm wide. The plant is thought to be anemophilous (pollinated by wind).\n\n\"Amaranthus brownii\" has a very limited range; it is found only on the island of Nihoa, located northwest of Kauai. It is thought that this endemic plant has probably always been rare and restricted to Nihoa. Its habitat is managed by the United States Fish and Wildlife Service and protected as part of the Hawaiian Islands National Wildlife Refuge in the Papahānaumokuākea Marine National Monument. \"A. brownii\" is one of three endemic and endangered species only found on Nihoa, along with the Nihoa fan palm (\"Pritchardia remota\") and the Nihoa carnation (\"Schiedea verticillata\"). At least nine other native plant species can be found in its habitat, including Hawaiian goosefoot (\"Chenopodium oahuense\"), lovegrass (\"Eragrostis variablis\"), \"koali awa\" (\"Ipomoea indica\"), goat's foot (\"Ipomoea pes-caprae\" subsp. \"brasiliensis\"), \"Panicum torridum\", \"naupakas\" (\"Scaevola sericea\"), \"Sicyos pachycarpus\", \"ilima\" (\"Sida fallax\"), and Nelson's horsenettle (\"Solanum nelsonii\").\nThe plant grows during the moist, winter season from December through July in Nihoa's coastal dry shrubland habitat in shallow soil on rocky outcrops in exposed areas between . At the time of its discovery in 1923, botanists first observed the plant growing in great quantity on the ridges towards Miller Peak and on eastern ridges of the island. Expeditions in the early and mid-1960s failed to identify any specimens, but in 1969, ethnobotanist Douglas E. Yen of the Bishop Museum collected specimens near Miller Peak. Derral R. Herbst and Wayne Takeuchi of the FWS collected the last known specimen on July 27, 1980. Carl C. Christensen also visited Nihoa in 1980 to reevaluate endemic species last observed on the Tanager Expedition. Sheila Conant and Mark S. Collins visited Nihoa in 1980 as well; Conant returned twice in 1981, first with Mark J. Rauzon and later with Audrey L. Newman. In 1983, Conant visited the island with Wayne C. Gagné. Conant found \"A. brownii\" growing on the island in 1981 and 1983, by which time only two populations of 35 plants were thought to exist: 23 plants were found near Miller Peak and 12 plants in Middle Valley. The two plant populations are separated by a distance of approximately .\n\nPrehistoric Polynesian habitation on Nihoa may have initially led to a decrease in the plant population of \"A. brownii\". Major threats to the plant include invasive species, fire, and hybridization with other \"Amaranthus\" species. Inbreeding is a serious threat, as the small plant population must reproduce within its own circle resulting in genetic defects. \"A. brownii\" is also forced to compete with the non-native weed (\"Portulaca oleracea\"), the plant's main alien species threat. In 2002 and 2004, the invasive gray bird grasshopper (\"Schistocerca nitens\") presented an even larger threat to \"A. brownii\". First recorded on the island in 1977, the increasing population density of gray bird grasshoppers led to massive defoliation on the island, leaving \"A. brownii\" at greater risk of predation. In 2004 alone, an estimated 400,000 gray bird grasshoppers destroyed almost 90% of the vegetation on Nihoa.\n\nAccording to zoologist and conservationist Sheila Conant, \"A. brownii\" is important due to its uniqueness in the Northwestern Hawaiian Islands as \"the only Hawaiian endemic in this large genus which contains many economically and nutritionally important species.\" However, in more than a decade of field surveys on Nihoa, no living plants have been identified. Wildlife refuge staff have visited the island during the dry season at least 21 times between 1983 and 1996. The absence of the plant in recent field surveys might be explained by the time of visit. Because winter surveys of Nihoa tend to be difficult and dangerous due to poor landing conditions, surveys have not been conducted during the moist, winter growing season from December through March when the plant is easiest to find. Most of the surveys have been completed during the summer months, when it is easiest to visit Nihoa, but during this time, the stems of \"A. brownii\" dry up and cannot be distinguished from other herbaceous plants. A seven-day visit to the island in April 2006 still did not find any specimens but botanists are optimistic that the species has survived. Additional winter surveys are required to accurately assess the conservation status of the plant.\nEx-situ conservation efforts to propagate \"A. brownii\" by seed in botanical gardens have been unsuccessful. During the 1981 expedition, \"A. bronwii\" seeds were collected by Sheila Conant and presented to the Waimea Arboretum on the Hawaiian island of Oahu and the Kew Gardens in London, England. Although the seeds at the Waimea Arboretum germinated and grew for a while, no plants survived beyond the stage of seedling development. Information about the outcome of the seeds sent to Kew Gardens is unavailable.\n\nA proposal for listing \"A. brownii\" under the U.S. Endangered Species Act was originally submitted on June 16, 1976, but was withdrawn on December 10, 1979 as out of date and incomplete. It was proposed again on March 24, 1993, and was federally listed as an endangered species on August 21, 1996. On May 22, 2003, the FWS designated on the island of Nihoa as a critical habitat for \"A. brownii\", as well as \"Pritchardia remota\", \"Schiedea verticillata\", and two other species also found on Nihoa and other Hawaiian islands, \"ohai\" (\"Sesbania tomentosa\") and \"Mariscus pennatiformis\". In the same year, \"A. brownii\" was internationally classified as critically endangered on the IUCN Red List. As of 2010, \"A. brownii\" was one of 51 Hawaiian plant species listed as endangered or threatened under the Endangered Species Act.\n\na. Wagner and Herbst list five naturalized species of \"Amaranthus\" in Hawaii in addition to the endemic \"A. brownii\". The authors note that the information may be both inaccurate and incomplete due to errors caused by a lost collection and lack of data. \"A. graecizans\", \"A. retroflexus\", and a third unknown species (possibly extinct or reclassified) have been proposed as additional naturalized candidates.\n\n\"A. brownii\" is one of 12 endemic flowering plant species in the Amaranthaceae family found in the Hawaiian Islands:\nb. Nihoa: fl, fr, June 20, 1923, C. S. Judd No. 2\n\nc. Herbst & Takeuchi 6545; BISH. Also see the database record at the U.S. National Herbarium: Herbst, D.R.; Takeuchi, W. No. 6545; Collection Date: 27 Jul 1980; Hawaiian Islands, Nihoa, Middle Valley. Alt. 91 m.; Barcode: 00453038 USNM No.: 02921853.\n\nd. According to Mark J. Rauzon, anthropologist Kenneth Emory, a member of the Tanager Expedition, identified 66 archaeological sites on the island of Nihoa, and to date 86 sites have been found. Emory estimated that 7.7 percent of the island (12 out of 156 acres) was used for terraced, dry-land crop production, and along with fish and seafowl, Emory believed 100 people (or more) could have survived on a long-term basis. However, questions about good potable water sources and the fact that only six skeletons have been found cast doubt on this figure.\n\ne. Compare the destruction of vegetation on Nihoa in 2004 to that of Laysan island. In 1894, Max Schlemmer introduced rabbits to Laysan. Eventually, the rabbit problem and bird poaching led U.S. President Theodore Roosevelt to declare the Northwestern Hawaiian Islands a bird sanctuary in 1909. By 1918, 26 plant species had disappeared from the island and the Laysan millerbird had become extinct. The Tanager Expedition arrived in 1923 and exterminated the last of the surviving rabbits.\n\n"}
{"id": "1299149", "url": "https://en.wikipedia.org/wiki?curid=1299149", "title": "Amazonian manatee", "text": "Amazonian manatee\n\nThe Amazonian manatee (\"Trichechus inunguis\") is a species of manatee that lives in the Amazon Basin in Brazil, Peru, Bolivia, Colombia and Ecuador. It has thin, wrinkled skin, with fine hairs scattered over its body and a white chest patch. It is the smallest species of manatee.\n\nThe specific name, \"inunguis\" is Latin for \"nailless.\" The genus name \"Trichechus\", comes from Latin meaning \"hair\", referencing the whiskers around the manatee's mouth.\n\nRanges of body weight and size observed are 7.5–346.5 kg and 76.0 –255.5 cm for captive males, 8.1–379.5 kg and 71.0–266.5 cm for captive females, and 120.0–270.0 kg and 162.0 –230.0 cm for free-ranging manatees, respectively. The maximum actual Amazonian manatee weight reported is 379.5 kg. Calves of the species are born weighing 10–15 kg and 85 –105 cm long. The Amazonian Manatees increase in length approximately 1.6-2.0 mm per day. This length is measured along the curvature of the body so absolute length can differ between individuals. As calves, they gain an average of 1 kilogram per week.\n\nAmazonian manatees are large, cylindrically shaped mammals, with forelimbs modified into flippers, no free hind-limbs, and the rear of the body in the form of a flat, rounded, horizontal paddle. The flexible flippers are used for aiding motion over the bottom, scratching, touching and even embracing other manatees, and moving food into and cleaning the mouth. The manatee's upper lip is modified into a large bristly surface, which is deeply divided. It can move each side of the lips independently while feeding. The general coloration is grey, and most Amazonian manatees have a distinct white or bright pink patch on the breast.\n\nThe manatee does not have incisors or canine teeth, only cheek teeth (molars). Molars designed to crush vegetation form continuously at the back of the jaw and move forward as older ones wear down. The older ones eventually fall out, while new ones come in at the rear of the jaw to replace them.\n\nThe Amazonian manatee lacks nails on its flippers, setting it apart from other manatees. An almost unique feature (amongst mammals) of the manatee is the constant replacement of molar teeth; new teeth enter at the back of the jaw and replace old and worn teeth at the front. The order's closest relatives, the elephants, also have teeth that get replaced, but have only a limited set of these replacement teeth.\n\nThe Amazonian manatee is the only sirenian that lives exclusively in freshwater habitat. The species relies on changes in the peripheral circulation for its primary mechanism for thermoregulation by using sphincters to deflect blood flow from areas of the body in close contact with water. They also rely on subcutaneous fat to reduce heat loss.\n\nManatees have nostrils, not blowholes like other aquatic mammals, which close when under water to keep water out and open when above water to breath. Although manatees can remain under water for extended periods, surfacing for air about every five minutes is common. The longest documented submergence of an Amazonian manatee in captivity is 14 minutes.\n\nManatees make seasonal movements synchronized with the flood regime of the Amazon Basin. They are found in flooded forests and meadows during the flood season, when food is abundant. The Amazonian manatee has the smallest degree of rostral deflection (25° to 41°) among sirenians, an adaptation to feed closer to the water surface. It is both nocturnal and diurnal and lives its life almost entirely underwater. Only its nostrils protrude from the surface of the water while it searches river and lake bottoms for vegetation.\n\nThe Amazonian and Florida manatees are the only manatees known to vocalize. They have been observed vocalizing alone and with others, particularly between cows and their calves.\n\nThe manatees themselves feed on a variety of aquatic macrophytes, including aroids (especially \"Pistia\", aka \"water lettuce\"), grasses, bladderworts, hornworts, water lilies, and particularly, water hyacinths. They are also known to eat palm fruits that fall into the water. Maintaining a herbivorous diet, the manatee has a similar post-gastric digestive process to that of the horse. The manatee consumes approximately 8% of its body weight in food per day.\n\nDuring the July–August dry season when water levels begin to fall, some populations become restricted to the deep parts of large lakes, where they often remain until the end of the dry season in March. They are thought to fast during this period, their large fat reserves and low metabolic rates – only 36% of the usual placental mammal metabolic rate – allowing them to survive for up to seven months with little or no food.\n\nThe Amazonian manatee is a seasonal breeder with a gestational period of 12–14 months and a prolonged calving period. Most births take place between December and July, with about 63% between February and May, during a time of rising river levels in their native region. After the calf is born, it will begin to eat while staying with its mother for 12 – 18 months.\n\nTwo individuals lived 12.5 years in captivity. Wild individuals have a lifespan of about 30 years.\n\nAs of 1977 the population count of the Amazonian manatee was estimated to be around 10,000. As of now the total population count is undetermined, however the population trend seems to be decreasing. They are mainly distributed throughout the Amazon River Basin in northern South America, ranging from the Marajó Islands in Brazil through Colombia, Peru, and Ecuador. They are occasionally found overlapping with the West Indian manatee along the coasts of Brazil.\n\nAmazonian manatees occur through most the Amazon River drainage, from the headwaters, in Colombia, Ecuador and Peru to the mouth of the Amazon (close to the Marajó Island) in Brazil over an estimated seven million square kilometers. However, their distribution is patchy, concentrating in areas of nutrient-rich flooded forest, which covers around 300,000 km² They also inhabit environments in lowland tropical areas below 300 m asl, where there is large production of aquatic and semi-aquatic plants; they are also found in calm, shallow waters, away from human settlements\n\nThe Amazonian manatee is completely aquatic and never leaves the water. It is the only manatee to occur exclusively in freshwater environments. The Amazonian manatee favors backwater lakes, oxbows, and lagoons with deep connections to large rivers and abundant aquatic vegetation They are mainly solitary but sometimes they will gather in small groups consisting of up to eight individuals. They engage in long seasonal movements, moving from flooded areas during the wet season to deep water-bodies during the dry season\n\nNatural predators include jaguars, sharks, and crocodiles.\n\nThe IUCN red list ranks the Amazonian manatee as vulnerable. Population declines are primarily a result of hunting, as well as calf mortality, climate change, and habitat loss. However, due to their murky water habitat it is difficult to gain accurate population estimates.\n\nThere are no national management plans for the Amazonian Manatee, except in Colombia. As of 2008, the INPA takes care of 34 captive manatees and the CPPMA is caring for 31 manatees. The manatee has been protected by Peruvian law since 1973, via Supreme Decree 934-73-AG, prohibiting hunting and commercial use of the manatee.\n\nHunting remains the largest problem and continues in much of its range, even within reserves. In 1986, it was estimated that the hunting levels in Ecuador were unsustainable and it would be gone from this country within 10–15 years. While hunting still occurs, an increasing risk to its continued survival in Ecuador is now believed to be the risk of oil spills. The oil exploration also means an increase in boat traffic on the rivers.\n\nThe Amazonian manatees of Peru have experienced much of their decline due to hunting by human populations for meat, blubber, skin and other materials that can be collected from the manatee. Such hunting is carried out with harpoons, gillnets, and set traps. Much of this hunting occurs in the lakes and streams near the Pacaya-Samiria National Reserve in northeastern Peru. The species is slow-moving, docile, and is often found feeding at the surface of the lakes and rivers it inhabits. Manatees are also at risk from pollution, accidental drowning in commercial fishing nets, and the degradation of vegetation by soil erosion resulting from deforestation. Additionally, the indiscriminate release of mercury in mining activities threatens the entire aquatic ecosystem of the Amazon Basin.\n\n\nhttp://species-identification.org/species.php?species_group=marine_mammals&menuentry=soorten&id=150&tab=beschrijving\n\n"}
{"id": "5654189", "url": "https://en.wikipedia.org/wiki?curid=5654189", "title": "American Academy of Anti-Aging Medicine", "text": "American Academy of Anti-Aging Medicine\n\nThe American Academy of Anti-Aging Medicine (A4M) is a United States registered 501(c)(3) nonprofit organization that promotes the questionable field of anti-aging medicine and trains and certifies physicians in this specialty. As of 2011, approximately 26,000 practitioners had been given certificates. The field of anti-aging medicine is not recognized by established medical organizations, such as the American Board of Medical Specialties (ABMS) and the American Medical Association (AMA). The Academy's activities include lobbying and public relations. The A4M was founded in 1993 by osteopathic physicians Robert Goldman and Ronald Klatz, and has grown to 26,000 members from 110 countries. The organization sponsors several conferences, including the Annual World Congress on Anti-Aging Medicine.\n\nSeveral of the anti-aging methods recommended by the Academy have wide support among experts in the field, such as exercise and a healthy diet, but others, such as hormone treatments, do not have support from a consensus of the wider medical community. Many scientists studying aging dissociate themselves from the claims of A4M, and critics have accused the group of using misleading marketing to sell expensive and ineffective products. The A4M's founders and merchants who promote products through the organization have been involved in legal and professional disputes.\n\nThe activities of the A4M are controversial: in 2003 a commentary on the response of the scientific community to the promotion of anti-aging medicine noted that the activities of the A4M were seen as a threat to the credibility of serious scientific research on aging. According to MSNBC, anti-aging advocates have responded to such criticism by describing it as censorship perpetrated by a conspiracy of the US government, notably the Food and Drug Administration, the AMA, and the mainstream media, motivated by competing commercial interests. Thomas Perls of the Boston University School of Medicine, a prominent critic of the organization, has stated that claims of censorship and suppression are a common theme in what he calls \"anti-aging quackery\".\n\nAccording to \"The New York Times\", their co-founder and president Ronald Klatz stated that \"We're not about growing old gracefully. We're about never growing old.\" With Klatz being quoted in 2004 as stating that:\nWriting in the 2001 issue of the journal \"Generations\", historian Carole Haber of the University of Delaware, states that Klatz' aspirations and the rhetoric of the A4M \"reflect well-worn ideas and the often-enunciated hopes of the past\", drawing parallels with the ideas of the 19th century physiologists Charles-Édouard Brown-Séquard, Serge Voronoff and Eugen Steinach. Haber states that the current resurgence of these ideas may be due to their appeal to the aging Baby Boom Generation, in a culture that is focused on the ideal of youth. Haber has also discussed the strong continuities within the philosophy of the anti-aging movement, writing that \"For Steinach and Voronoff, as for the members of the A4M, old age was a \"grotesque\" disease that could be scientifically eradicated through the correct combination of hormones, diet, and surgery.\"\n\nThe chairman of the A4M is Robert Goldman and the president is Ronald Klatz. The senior vice president is Joseph Maroon of the University of Pittsburgh and Nicholas DiNubile of the University of Pennsylvania is the vice president. The Academy states that it has over 20,000 members from over 100 countries, and that this membership is made up of physicians, scientists, researchers, health practitioners and members of the public. In 2007, the organization reported just over seven million dollars in assets. However, a 2006 review of anti-aging medicine notes that of the researchers who are interested in this topic, the \"vast majority dissociate themselves from the A4M.\" The \"Los Angeles Times\" states that \"Many physicians, researchers and scientists, delving into the physiological aspects of human aging, view the Academy's activities with disdain, saying that the organization is an inappropriate blend of scientific and commercial interests.\"\n\nThe main activity of the A4M is PR and advocacy for its brand of anti-aging medicine. It does this through publications, on-line activity and sponsoring conferences including the World Anti-Aging Congress and Exposition and the Annual World Congress on Anti-Aging Medicine. Some of these conferences are in conjunction with the World Anti-Aging Academy of Medicine, an umbrella group for several national anti-aging organizations that is also headed by Goldman. The \"LA Times\" stated that the 2004 annual conference of the A4M at Las Vegas presented a mix of \"scientific and technical presentations\" and exhibitors selling \"wrinkle creams, hair-growing potions, sexual enhancement pills and hormone treatments\".\n\nAccording to a review of the anti-aging movement published in 2005, the A4M is one of the most prominent organizations that are making \"attempts at legitimizing anti-aging as a medical specialty\". The review notes that these efforts at legitimization are contentious and have been rebuffed by some academic scientists who work on aging, who instead attempt to portray the A4M as \"charlatans whose main goal is making money\". In a review of the history of anti-aging medicine published in 2004, Robert Binstock of Case Western Reserve University noted that A4M \"actively solicits and displays numerous advertisements on its website for products and services (such as cosmetics and alternative medicines and therapies), anti-aging clinics, and anti-aging physicians and practitioners.\" \"The Times\" reported in 2004 that Klatz professes outrage at suggestions that he is motivated by money, quoting him as insisting that \"The only thing that I sell are books...my website is non-commercial – we’re just trying to advance science.\" \"The Times\" went on to note a partnership between Klatz and Goldman and a business named Market America, which sells products that promise to \"slow the ageing process\". However, according to a 2005 article in the \"Chicago Tribune\", the company later pulled out of this contract.\n\nThe A4M's American Board of Anti-Aging Medicine (ABAAM) states that it offers anti-aging medicine as a specialty and gives educational credits to those who attend A4M conferences. \"The New York Times\" has reported that the American Board of Medical Specialties does not recognize this body as having professional standing. MSNBC noted that \"as far as the American Medical Association or the American Board of Medical Specialties is concerned, there is no such thing as an anti-aging specialty.\" Robert Binstock stated in a 2004 review article in \"The Gerontologist\" that \"Although the organization is not recognized by the American Medical Association, A4M has established three board-certification programs under its auspices—for physicians, chiropractors, dentists, naturopaths, podiatrists, pharmacists, registered nurses, nurse practitioners, nutritionists, dieticians, sports trainers and fitness consultants, and PhDs.\"\n\nThe A4M publishes \"Anti Aging Medical News\", a trade periodical which is their official magazine, as well as proceedings of its anti-aging conferences in a periodical called \"Anti-Aging Therapeutics\", this is edited by Klatz and Goldman.\n\nThe \"International Journal of Anti-Aging Medicine\" (IJAAM) was another periodical published by the A4M. Describing the intended scope of this publication, Klatz is quoted as stating, \"We hope to cover the waterfront of the entire field of anti-aging medicine, with a clinical focus.\" As of 2009, the A4M recommend this publication on their website as a good way of keeping up with recent developments in anti-aging medicine, stating that it \"report(s) on the latest anti-aging findings\". According to Ulrich's Periodicals Directory, IJAAM was published by Total Health Holdings, LLC from 1998 to 2001, on behalf of the A4M.\n\nThe contents of the \"International Journal of Anti-Aging Medicine\" have been strongly criticised. In a 2002 letter published in \"Science\", Aubrey de Grey described them as consisting of a set of advertisements for a \"pseudoscientific anti-aging industry\". According to Bruce Carnes of the University of Oklahoma:\nLeonard Hayflick of the University of California, San Francisco, a former editor of \"Experimental Gerontology\", writes:\n\nIn 2009 the A4M stated that it is no longer associated with the journal and that it had sold its interests in this publication in 1999. They also defended the scientific quality of its contents, writing that almost all of its articles were reviewed by an editorial board before publication. Robert Binstock of Case Western Reserve University stated in 2004 that this periodical is a \"nonrefereed publication\".\n\nAccording to a 2002 article in the \"Seattle Times\", there are two opposing viewpoints of anti-aging products. The article states that the first view is represented by scientists who publish their findings in the scientific literature and who believe that no currently available intervention can slow or prevent aging. The alternative viewpoint is represented by people who the article states have \"fewer credentials\" and who promote a range of products that claim to have anti-aging properties. A similar observation was made by \"Business Week\" in 2006, when they stated that although anti-aging medicine is increasingly popular, there is \"precious little scientific data to back up their claims that the potions extend life.\"\n\nAs an example of the first viewpoint, a 2004 review in \"Trends in Biotechnology\" written by Leigh Turner of the Institute for Advanced Study in Princeton, New Jersey stated that the products promoted by the A4M have \"no credible scientific basis\" and that \"there are no proven, scientifically established ‘anti-aging’ medications\". A 2006 review published in the \"Cleveland Clinic Journal of Medicine\" of the antioxidants and hormones that are promoted as anti-aging products by the A4M and the Life Extension Institute concluded that these products have \"minimal to no effect on improving longevity or functional abilities.\" In an editorial accompanying this study, Thomas Perls stated that although many unjustified claims were made about anti-ageing products, no substance had yet been shown to halt or slow the aging process. Similarly, the National Institute on Aging, who are part of the National Institutes of Health, published a general warning in 2009 against businesses that claim anti-aging benefits for their products, describing these as \"health scams\" and stating that \"no treatments have been proven to slow or reverse the aging process\".\n\nThe alternative view is held by the A4M, who argue that anti-aging medicine is \"evidence-based, clinically sound health care.\" and state that \"only those diagnostic and treatment elements which prove their validity through independent evaluations are embraced by the A4M.\" The \"Seattle Times\" quotes Klatz as describing those who doubt the validity of anti-aging medicine as \"flat-earthers\" who make unjustified criticisms that are not backed by scientific evidence, the article also states that Klatz \"sees the science and medical establishments as out to get him.\" Though he has been quoted as saying, \"I’m not against the AMA and I’m not against the establishment, I’m really for the establishment, I’m for technology I’m for science-based medicine. But the innovators are always 30 years ahead of the mainstream and that’s just the way it is with anti-aging medicine. We’re just ahead of the curve.\"\n\nThe American Academy of Anti-Aging Medicine was formed following a 1990 study on human growth hormone (hGH) that was published in the \"New England Journal of Medicine\". The study was performed by Daniel Rudman and colleagues at the Medical College of Wisconsin. Rudman had treated twelve men over 60 years of age with human growth hormone; after six months, these men had an increase in lean body mass and a decrease in adipose tissue mass when compared with a group of nine men who did not receive hormone. Members of the anti-aging movement have interpreted these results to support a role for growth hormone in slowing or reversing aging. A review in \"The Journal of Urology\" noted that this promotion of growth hormone as an anti-aging remedy is \"arguably similar\" to ideas that date back to the late 19th century, when the physiologist Charles-Édouard Brown-Séquard advocated rejuvenating hormone products prepared from animal testicles and stated that \"the injections have taken 30 years off my life\".\n\n\"The New York Times\" reports that the idea that growth hormone can improve \"health, energy level and sense of well-being.\" is a core belief of the A4M, with Klatz writing a book in 1998 entitled \"Grow Young with HGH: The Amazing Medically Proven Plan to Reverse Aging\" where he states \"The ‘Fountain of Youth’ lies within the cells of each of us. All you need to do is release it\". A 2005 review in the \"Journal of Endocrinological Investigation\" noted the long history of these ideas, but stated that the \"concept of a 'hormonal fountain of youth' is predominantly mythological.\" Nevertheless, Klatz maintains that growth hormone reverses aging as a physical process and has described growth hormone as \"the first medically proven age-reversal therapy.\" However, MSNBC reports that Daniel Rudman, the author of the 1990 study that sparked the movement, \"issued many caveats and cautions about using HGH and never recommended its use to delay aging. In fact, he was horrified his study was being used to support the industry especially since heavy use of growth hormone can have unwanted side effects\".\n\n\"The New York Times\" states that medical authorities not affiliated with the A4M question the safety and efficacy of the use of growth hormone in anti-aging medicine, quoting Michael Fossell of Michigan State University who stated that \"hormone therapies are the new patent medicines – cure-alls embraced by a too-trusting public.\" A 2003 review that was published in the \"Annual Review of Medicine\" noted that the long-term risks or benefits of this treatment are uncertain, that \"neither the benefits nor the dangers have been defined\" and advising that a \"prudent physician should not condone the use of GH for normal aging\".\n\nAs a result of the reactions to the 1990 article and its frequent citation by proponents of HGH as an anti-aging agent, in 2003 the \"New England Journal of Medicine\" published two articles that strongly and clearly stated that there was insufficient medical and scientific evidence to support use of HGH as anti-aging drug. One article was written by the \"Journal\"'s then-editor in chief, Jeffrey M. Drazen, M.D. and was entitled, \"Inappropriate Advertising of Dietary Supplements\". It focused mostly on the advertising of dietary supplements. The other article was written by the editor-in-chief at the time the 1990 article was published, Mary Lee Vance, M.D., and was entitled, \"Can Growth Hormone Prevent Aging?\"; it focused more on the medical issues around whether there was sufficient evidence to use HGH as an anti-aging agent.\n\nA 2007 review on the use of human growth hormone as an anti-aging treatment in healthy elderly people published in the \"Annals of Internal Medicine\" concluded the risks of HGH significantly outweigh the benefits, noted soft tissue edema as a common side effect and found no evidence that the hormone prolongs life. \"ABC News\" interviewed Hau Liu of Stanford University and lead author of the paper, who stated that people are paying thousands of dollars a year for a treatment that has not been proved to be beneficial and has many side effects. \"ABC News\" also reported that the A4A disputed the conclusions of this review, quoting from an A4A statement which maintained that growth hormone supplementation is beneficial in healthy adults and which described arguments against the use of the hormone as a \"heinous act of malpractice\".\n\nSome small studies have shown that low-dose GH treatment for adults with severe GH deficiency, such as that produced after surgical removal of the pituitary gland, produces positive changes in body composition by increasing muscle mass, decreasing fat mass, increasing bone density and muscle strength; improves cardiovascular parameters (i.e. decrease of LDL cholesterol), and improves quality of life without significant side effects. The extension of this approach to healthy elderly people is an area of current research, with a 2000 review in \"Hormone Research\" commenting that \"Clearly more studies are needed before GH replacement for the elderly becomes established.\" and noting that \"safety issues will require close scrutiny\".\n\nA 2008 review of the controversy surrounding the use of growth hormone in anti-aging medicine which published in \"Clinical Interventions in Aging\" noted the opinions of the A4A on this topic, but suggested that high levels of growth hormone might actually accelerate aging. This concern was repeated by the United States National Institute on Aging who stated in 2009 that:\n\nThe \"Clinical Interventions in Aging\" review also stated that although the decreasing levels of the hormone seen in the elderly might reduce quality of life, this change could protect from age-related diseases and cited evidence linking GH to cancer. This concern was mirrored in a 2008 review published in \"Clinical Endocrinology\", which stated that the risk of increasing the incidence of cancer was a strong argument against the use of this hormone as an \"elixir of youth\" in healthy adults.\n\nThe Academy's co-founders include Klatz and Goldman, who are licensed osteopathic physicians and have Doctor of Osteopathic Medicine degrees (D.O.). However, according to \"The New York Times\", they also received M.D. degrees as doctors of medicine from a university in Belize in 1988, although the paper notes that they had not studied in Belize. In 2009 Klatz and Goldman stated that these degrees involved eight years of medical and surgical training and a year of clinical rotations. \"The New York Times\" reported that the Illinois State Board of Medical Registration did not recognize these MD degrees, and stated that the Board fined the men for using MD after their names. Writing in 2004, \"The Times\" stated that Klatz and Goldman \"agreed to pay $5,000 penalties for allegedly identifying themselves as doctors of medicine in the state without being \"properly licensed\".\" The Illinois Division of Professional Regulation disciplinary records state that Klatz and Goldman \"agreed to cease and desist using the designation \"M.D.\" in addition to the appropriate \"D.O.\" title and fined $5,000. Both physicians did receive degrees as doctors of medicine, but were never properly licensed to use the title \"M.D.\" in Illinois\" In 2009, Klatz and Goldman stated that Illinois Department of Financial & Professional Regulation had determined that they are currently:\n\nThey go on to state that they have \"valid M.D. degrees from a recognized medical school\". Writing in 2004, the historian Carole Haber put this dispute into context, noting that \"like the gland doctors before them, the leaders of the A4M have had their practices and credentials assailed by the medical and legal communities\".\n\nTwo articles in the Journal of the American Medical Association have stated that the use of growth hormone as an anti-aging product is illegal. However, Klatz and Goldman dispute this, arguing that this use of growth hormone is legal. The United States Department of Justice states that growth hormone is a potentially dangerous drug and its supply \"for any use . . . other than the treatment of a disease or other recognized medical condition, where such use has been authorized by the Secretary of Human Services\" is a felony under the 1990 Anabolic Steroids Control Act. Similarly, the FDA has stated in a Warning Letter that no growth hormone products have been approved as anti-aging treatments and supply for this use is therefore illegal and an \"offense punishable by not more than 5 years in prison\". In 2007 \"The New York Times\" discussed ongoing federal and state investigations into illegal trafficking of human growth hormone and anabolic steroids, noting that \"many of the individuals and companies cited in the indictments have been involved with the academy and its conventions over the years\". However, the paper notes that the Academy is not accused of any wrongdoing as part of these investigations and quotes Klatz and Goldman as stating that \"they barely knew the suspects or the nature of their businesses\". A May 2000 article in the \"Los Angeles Times\" suggested that, from an examination of the disciplinary records of doctors in California, members of the A4M in this state were approximately ten times more likely to be disciplined than the national average. In the article, Klatz is quoted as commenting that:\n\nAccording to lawyers claiming to act for A4M and one or more people involved with it, their clients had initiated \"defamation actions in New York and Massachusetts\" against Wikipedia editors in 2009.\nAccording to Courthouse News Service, the A4M co-founders Ronald Klatz and Robert Goldman are pursuing legal action against the online encyclopedia Wikipedia in New York County Court, seeking damages for alleged defamation.\n\nIn 2002, A4M was a co-recipient of the first \"Silver Fleece Award,\" created to publicize \"the most ridiculous claims about antiaging medicine\" according to the award's inventor, S. Jay Olshansky. Heated legal and academic controversies ensued. Olshansky, a biodemographer at the University of Illinois at Chicago, described it as \"a lighthearted attempt to make the public aware of...anti-aging quackery\". This \"award\" was presented by Olshansky, who stated that in his opinion, a \"suite of anti-aging substances created by Ronald Klatz and Robert Goldman...and sold on the Internet by Market America, Inc.\" had made \"outrageous or exaggerated claims about slowing or reversing human aging\". Writing in \"Biogerontology\", anthropologist Courtney Mykytyn of the University of Southern California states that this award appears to have been an attempt by Olshansky to protect what he saw as \"'real' science from the taint of swindle.\" Mykytyn states that this involved Olshansky \"tagging the A4M as fraudulent and its principals as profiteers\". In response, the Academy filed defamation lawsuits, demanding $150 million in damages, with Klatz stating \"We take great exception to Mr Olshansky and his tactics which have finally compelled us to file suit for various unprofessional and improper actions\". Klatz and Goldman described this action as \"part of a larger campaign of disparagement by Olshansky and Perls aimed at discrediting A4M and its founders\". The \"Chicago Tribune\" quoted experts on libel law who stated that the action was an \"almost unheard-of attempt to punish academics for comments made in their professional capacity\". \"CNN\" states that Olshansky countersued and that \"both sides eventually agreed to drop their cases\". The \"Chicago Tribune\" states that the case \"ended in a settlement, with neither side paying damages or the other's costs.\"\n\nIn 2002, Olshansky, Hayflick, and Carnes published a position paper, endorsed by 51 scientists in the field of aging, stating that \"no currently marketed intervention has yet been proved to slow, stop or reverse human aging...The entrepreneurs, physicians and other health care practitioners who make these claims are taking advantage of consumers who cannot easily distinguish between the hype and reality of interventions designed to influence the aging process and age-related diseases,\". The A4M responded by publishing a critique of what it argued were biased statements in this paper.\n\nIn 2009, Imre Zs-Nagy of the University of Debrecen, Hungary, defended A4M from what he called the \"gerontological establishment\" in an editorial published in \"Archives of Gerontology and Geriatrics\", a journal Zs-Nagy founded and of which he is editor-in-chief. Zs-Nagy defended therapies promoted by A4M, which he states are related to his own \"membrane hypothesis of aging\", as theoretically feasible. He described the conflict between the scientific community and the Academy as one pitting government funds, \"personal gain\" and \"intellectual dishonesty\" against the \"independent, open-minded approach\" of A4M, calling the conflict one of the \"biggest scandals of the recent history of medicine\".\n\n\n\n"}
{"id": "5007888", "url": "https://en.wikipedia.org/wiki?curid=5007888", "title": "Anton Eduard van Arkel", "text": "Anton Eduard van Arkel\n\nAnton Eduard van Arkel, ('s-Gravenzande Netherlands, 19 November 1893 – Leiden, 14 March 1976) was a Dutch chemist.\n\nHe suggested the names \"pnictogen\" and \"pnictide\".\n\nVan Arkel became member of the Royal Netherlands Academy of Arts and Sciences in 1962.\n\n\n"}
{"id": "1461", "url": "https://en.wikipedia.org/wiki?curid=1461", "title": "Apollo program", "text": "Apollo program\n\nThe Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s, which he proposed in an address to Congress on May 25, 1961. It was the third US human spaceflight program to fly, preceded by the two-man Project Gemini conceived in 1961 to extend spaceflight capability in support of Apollo.\n\nKennedy's goal was accomplished on the Apollo 11 mission when astronauts Neil Armstrong and Buzz Aldrin landed their Lunar Module (LM) on July 20, 1969, and walked on the lunar surface, while Michael Collins remained in lunar orbit in the Command/Service Module (CSM), and all three landed safely on Earth on July 24. Five subsequent Apollo missions also landed astronauts on the Moon, the last in December 1972. In these six spaceflights, twelve men walked on the Moon.\n\nApollo ran from 1961 to 1972, with the first manned flight in 1968. It achieved its goal of manned lunar landing, despite the major setback of a 1967 Apollo 1 cabin fire that killed the entire crew during a prelaunch test. After the first landing, sufficient flight hardware remained for nine follow-on landings with a plan for extended lunar geological and astrophysical exploration. Budget cuts forced the cancellation of three of these. Five of the remaining six missions achieved successful landings, but the Apollo 13 landing was prevented by an oxygen tank explosion in transit to the Moon, which destroyed the Service Module's capability to provide electrical power, crippling the CSM's propulsion and life support systems. The crew returned to Earth safely by using the Lunar Module as a \"lifeboat\" for these functions. Apollo used Saturn family rockets as launch vehicles, which were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973–74, and the Apollo–Soyuz Test Project, a joint US-Soviet Union Earth-orbit mission in 1975.\n\nApollo set several major human spaceflight milestones. It stands alone in sending manned missions beyond low Earth orbit. Apollo 8 was the first manned spacecraft to orbit another celestial body, while the final Apollo 17 mission marked the sixth Moon landing and the ninth manned mission beyond low Earth orbit. The program returned of lunar rocks and soil to Earth, greatly contributing to the understanding of the Moon's composition and geological history. The program laid the foundation for NASA's subsequent human spaceflight capability and funded construction of its Johnson Space Center and Kennedy Space Center. Apollo also spurred advances in many areas of technology incidental to rocketry and manned spaceflight, including avionics, telecommunications, and computers.\n\nThe Apollo program was conceived during the Eisenhower administration in early 1960, as a follow-up to Project Mercury. While the Mercury capsule could only support one astronaut on a limited Earth orbital mission, Apollo would carry three astronauts. Possible missions included ferrying crews to a space station, circumlunar flights, and eventual manned lunar landings.\n\nThe program was named after Apollo, the Greek god of light, music, and the sun, by NASA manager Abe Silverstein, who later said that \"I was naming the spacecraft like I'd name my baby.\" Silverstein chose the name at home one evening, early in 1960, because he felt \"Apollo riding his chariot across the Sun was appropriate to the grand scale of the proposed program.\"\n\nIn July 1960, NASA Deputy Administrator Hugh L. Dryden announced the Apollo program to industry representatives at a series of Space Task Group conferences. Preliminary specifications were laid out for a spacecraft with a \"mission module\" cabin separate from the \"command module\" (piloting and re-entry cabin), and a \"propulsion and equipment module\". On August 30, a feasibility study competition was announced, and on October 25, three study contracts were awarded to General Dynamics/Convair, General Electric, and the Glenn L. Martin Company. Meanwhile, NASA performed its own in-house spacecraft design studies led by Maxime Faget, to serve as a gauge to judge and monitor the three industry designs.\n\nIn November 1960, John F. Kennedy was elected president after a campaign that promised American superiority over the Soviet Union in the fields of space exploration and missile defense. Up to the election of 1960, Kennedy had been speaking out against the \"missile gap\" that he and many other senators felt had formed between the Soviets and themselves due to the inaction of President Eisenhower. Beyond military power, Kennedy used aerospace technology as a symbol of national prestige, pledging to make the US not \"first but, first and, first if, but first period.\" Despite Kennedy's rhetoric, he did not immediately come to a decision on the status of the Apollo program once he became president. He knew little about the technical details of the space program, and was put off by the massive financial commitment required by a manned Moon landing. When Kennedy's newly appointed NASA Administrator James E. Webb requested a 30 percent budget increase for his agency, Kennedy supported an acceleration of NASA's large booster program but deferred a decision on the broader issue.\n\nOn April 12, 1961, Soviet cosmonaut Yuri Gagarin became the first person to fly in space, reinforcing American fears about being left behind in a technological competition with the Soviet Union. At a meeting of the US House Committee on Science and Astronautics one day after Gagarin's flight, many congressmen pledged their support for a crash program aimed at ensuring that America would catch up. Kennedy was circumspect in his response to the news, refusing to make a commitment on America's response to the Soviets.\nOn April 20, Kennedy sent a memo to Vice President Lyndon B. Johnson, asking Johnson to look into the status of America's space program, and into programs that could offer NASA the opportunity to catch up. Johnson responded approximately one week later, concluding that \"we are neither making maximum effort nor achieving results necessary if this country is to reach a position of leadership.\" His memo concluded that a manned Moon landing was far enough in the future that it was likely the United States would achieve it first.\n\nOn May 25, 1961, twenty days after the first US manned spaceflight \"Freedom 7\", Kennedy proposed the manned Moon landing in a \"Special Message to the Congress on Urgent National Needs\":\nNow it is time to take longer strides - time for a great new American enterprise - time for this nation to take a clearly leading role in space achievement, which in many ways may hold the key to our future on Earth.<br><br>...I believe that this nation should commit itself to achieving the goal, before this decade is out, of landing a man on the Moon and returning him safely to the Earth. No single space project in this period will be more impressive to mankind, or more important in the long-range exploration of space; and none will be so difficult or expensive to accomplish. \n\nAt the time of Kennedy's proposal, only one American had flown in space—less than a month earlier—and NASA had not yet sent an astronaut into orbit. Even some NASA employees doubted whether Kennedy's ambitious goal could be met. By 1963, Kennedy even came close to agreeing to a joint US-USSR Moon mission, to eliminate duplication of effort.\n\nWith the clear goal of a manned landing replacing the more nebulous goals of space stations and circumlunar flights, NASA decided that, in order to make progress quickly, it would discard the feasibility study designs of Convair, GE, and Martin, and proceed with Faget's command / service module design. The mission module was determined to be only useful as an extra room, and therefore deemed unnecessary. They used Faget's design as the specification for another competition for spacecraft procurement bids in October 1961. On November 28, 1961, it was announced that North American Aviation had won the contract, although its bid was not rated as good as Martin's. Webb, Dryden and Robert Seamans chose it in preference due to North American's longer association with NASA and its predecessor.\n\nLanding men on the Moon by the end of 1969 required the most sudden burst of technological creativity, and the largest commitment of resources ($25 billion; $ in dollars) ever made by any nation in peacetime. At its peak, the Apollo program employed 400,000 people and required the support of over 20,000 industrial firms and universities.\n\nOn July 1, 1960, NASA established the Marshall Space Flight Center (MSFC) in Huntsville, Alabama. MSFC designed the heavy lift-class Saturn launch vehicles, which would be required for Apollo.\n\nIt became clear that managing the Apollo program would exceed the capabilities of Robert R. Gilruth's Space Task Group, which had been directing the nation's manned space program from NASA's Langley Research Center. So Gilruth was given authority to grow his organization into a new NASA center, the Manned Spacecraft Center (MSC). A site was chosen in Houston, Texas, on land donated by Rice University, and Administrator Webb announced the conversion on September 19, 1961. It was also clear NASA would soon outgrow its practice of controlling missions from its Cape Canaveral Air Force Station launch facilities in Florida, so a new Mission Control Center would be included in the MSC.\nIn September 1962, by which time two Project Mercury astronauts had orbited the Earth, Gilruth had moved his organization to rented space in Houston, and construction of the MSC facility was under way, Kennedy visited Rice to reiterate his challenge in a famous speech:\nThe MSC was completed in September 1963. It was renamed by the US Congress in honor of Lyndon Johnson soon after his death in 1973.\n\nIt also became clear that Apollo would outgrow the Canaveral launch facilities in Florida. The two newest launch complexes were already being built for the Saturn I and IB rockets at the northernmost end: LC-34 and LC-37. But an even bigger facility would be needed for the mammoth rocket required for the manned lunar mission, so land acquisition was started in July 1961 for a Launch Operations Center (LOC) immediately north of Canaveral at Merritt Island. The design, development and construction of the center was conducted by Kurt H. Debus, a member of Dr. Wernher von Braun's original V-2 rocket engineering team. Debus was named the LOC's first Director. Construction began in November 1962. Upon Kennedy's death, President Johnson issued an executive order on November 29, 1963, to rename the LOC and Cape Canaveral in honor of Kennedy.\n\nThe LOC included Launch Complex 39, a Launch Control Center, and a 130 million cubic foot (3.7 million cubic meter) Vertical Assembly Building (VAB) in which the space vehicle (launch vehicle and spacecraft) would be assembled on a Mobile Launcher Platform and then moved by a transporter to one of several launch pads. Although at least three pads were planned, only two, designated A and B, were completed in October 1965. The LOC also included an Operations and Checkout Building (OCB) to which Gemini and Apollo spacecraft were initially received prior to being mated to their launch vehicles. The Apollo spacecraft could be tested in two vacuum chambers capable of simulating atmospheric pressure at altitudes up to , which is nearly a vacuum.\n\nAdministrator Webb realized that in order to keep Apollo costs under control, he had to develop greater project management skills in his organization, so he recruited Dr. George E. Mueller for a high management job. Mueller accepted, on the condition that he have a say in NASA reorganization necessary to effectively administer Apollo. Webb then worked with Associate Administrator (later Deputy Administrator) Seamans to reorganize the Office of Manned Space Flight (OMSF). On July 23, 1963, Webb announced Mueller's appointment as Deputy Associate Administrator for Manned Space Flight, to replace then Associate Administrator D. Brainerd Holmes on his retirement effective September 1. Under Webb's reorganization, the directors of the Manned Spacecraft Center (Gilruth), Marshall Space Flight Center (von Braun), and the Launch Operations Center (Debus) reported to Mueller.\n\nBased on his industry experience on Air Force missile projects, Mueller realized some skilled managers could be found among high-ranking officers in the United States Air Force, so he got Webb's permission to recruit General Samuel C. Phillips, who gained a reputation for his effective management of the Minuteman program, as OMSF program controller. Phillips' superior officer Bernard A. Schriever agreed to loan Phillips to NASA, along with a staff of officers under him, on the condition that Phillips be made Apollo Program Director. Mueller agreed, and Phillips managed Apollo from January 1964, until it achieved the first manned landing in July 1969, after which he returned to Air Force duty.\n\nOnce Kennedy had defined a goal, the Apollo mission planners were faced with the challenge of designing a spacecraft that could meet it while minimizing risk to human life, cost, and demands on technology and astronaut skill. Four possible mission modes were considered:\n\nIn early 1961, direct ascent was generally the mission mode in favor at NASA. Many engineers feared that a rendezvous—let alone a docking—neither of which had been attempted even in Earth orbit, would be extremely difficult in lunar orbit. Dissenters including John Houbolt at Langley Research Center emphasized the important weight reductions that were offered by the LOR approach. Throughout 1960 and 1961, Houbolt campaigned for the recognition of LOR as a viable and practical option. Bypassing the NASA hierarchy, he sent a series of memos and reports on the issue to Associate Administrator Robert Seamans; while acknowledging that he spoke \"somewhat as a voice in the wilderness,\" Houbolt pleaded that LOR should not be discounted in studies of the question.\nSeamans' establishment of an ad-hoc committee headed by his special technical assistant Nicholas E. Golovin in July 1961, to recommend a launch vehicle to be used in the Apollo program, represented a turning point in NASA's mission mode decision. This committee recognized that the chosen mode was an important part of the launch vehicle choice, and recommended in favor of a hybrid EOR-LOR mode. Its consideration of LOR—as well as Houbolt's ceaseless work—played an important role in publicizing the workability of the approach. In late 1961 and early 1962, members of the Manned Spacecraft Center began to come around to support LOR, including the newly hired deputy director of the Office of Manned Space Flight, Joseph Shea, who became a champion of LOR. The engineers at Marshall Space Flight Center (MSFC), which had much to lose from the decision, took longer to become convinced of its merits, but their conversion was announced by Wernher von Braun at a briefing on June 7, 1962.\n\nBut even after NASA reached internal agreement, it was far from smooth sailing. Kennedy's science advisor Jerome Wiesner, who had expressed his opposition to manned spaceflight to Kennedy before the President took office, and had opposed the decision to land men on the Moon, hired Golovin, who had left NASA, to chair his own \"Space Vehicle Panel\", ostensibly to monitor, but actually to second-guess NASA's decisions on the Saturn V launch vehicle and LOR by forcing Shea, Seamans, and even Webb to defend themselves, delaying its formal announcement to the press on July 11, 1962, and forcing Webb to still hedge the decision as \"tentative\".\n\nWiesner kept up the pressure, even making the disagreement public during a two-day September visit by the President to Marshall Space Flight Center. Wiesner blurted out \"No, that's no good\" in front of the press, during a presentation by von Braun. Webb jumped in and defended von Braun, until Kennedy ended the squabble by stating that the matter was \"still subject to final review\". Webb held firm, and issued a request for proposal to candidate Lunar Excursion Module (LEM) contractors. Wiesner finally relented, unwilling to settle the dispute once and for all in Kennedy's office, because of the President's involvement with the October Cuban Missile Crisis, and fear of Kennedy's support for Webb. NASA announced the selection of Grumman as the LEM contractor in November 1962.\n\nSpace historian James Hansen concludes that:\n\nThe LOR method had the advantage of allowing the lander spacecraft to be used as a \"lifeboat\" in the event of a failure of the command ship. Some documents prove this theory was discussed before and after the method was chosen. A 1964 MSC study concluded, \"The LM [as lifeboat] ... was finally dropped, because no single reasonable CSM failure could be identified that would prohibit use of the SPS.\" Ironically, just such a failure happened on Apollo 13 when an oxygen tank explosion left the CSM without electrical power. The Lunar Module provided propulsion, electrical power and life support to get the crew home safely.\n\nFaget's preliminary Apollo design employed a cone-shaped command module, supported by one of several service modules providing propulsion and electrical power, sized appropriately for the space station, cislunar, and lunar landing missions. Once Kennedy's Moon landing goal became official, detailed design began of a \"Command/Service Module\" (CSM) in which the crew would spend the entire direct-ascent mission and lift off from the lunar surface for the return trip, after being soft-landed by a larger landing propulsion module. The final choice of lunar orbit rendezvous changed the CSM's role to the translunar ferry used to transport the crew, along with a new spacecraft, the \"Lunar Excursion Module\" (LEM, later shortened to \"Lunar Module\", LM) which would take two men to the lunar surface and return them to the CSM.\n\nThe Command Module (CM) was the conical crew cabin, designed to carry three astronauts from launch to lunar orbit and back to an Earth ocean landing. It was the only component of the Apollo spacecraft to survive without major configuration changes as the program evolved from the early Apollo study designs. Its exterior was covered with an ablative heat shield, and had its own reaction control system (RCS) engines to control its attitude and steer its atmospheric entry path. Parachutes were carried to slow its descent to splashdown. The module was tall, in diameter, and weighed approximately .\n\nA cylindrical Service Module (SM) supported the Command Module, with a service propulsion engine and an RCS with propellants, and a fuel cell power generation system with liquid hydrogen and liquid oxygen reactants. A high-gain S-band antenna was used for long-distance communications on the lunar flights. On the extended lunar missions, an orbital scientific instrument package was carried. The Service Module was discarded just before re-entry. The module was long and in diameter. The initial lunar flight version weighed approximately fully fueled, while a later version designed to carry a lunar orbit scientific instrument package weighed just over .\n\nNorth American Aviation won the contract to build the CSM, and also the second stage of the Saturn V launch vehicle for NASA. Because the CSM design was started early before the selection of lunar orbit rendezvous, the service propulsion engine was sized to lift the CSM off the Moon, and thus was oversized to about twice the thrust required for translunar flight. Also, there was no provision for docking with the Lunar Module. A 1964 program definition study concluded that the initial design should be continued as Block I which would be used for early testing, while Block II, the actual lunar spacecraft, would incorporate the docking equipment and take advantage of the lessons learned in Block I development.\n\nThe Lunar Module (LM) was designed to descend from lunar orbit to land two astronauts on the Moon and take them back to orbit to rendezvous with the Command Module. Not designed to fly through the Earth's atmosphere or return to Earth, its fuselage was designed totally without aerodynamic considerations, and was of an extremely lightweight construction. It consisted of separate descent and ascent stages, each with its own engine. The descent stage contained storage for the descent propellant, surface stay consumables, and surface exploration equipment. The ascent stage contained the crew cabin, ascent propellant, and a reaction control system. The initial LM model weighed approximately , and allowed surface stays up to around 34 hours. An Extended Lunar Module weighed over , and allowed surface stays of over 3 days. The contract for design and construction of the Lunar Module was awarded to Grumman Aircraft Engineering Corporation, and the project was overseen by Thomas J. Kelly.\n\nBefore the Apollo program began, Wernher von Braun and his team of rocket engineers had started work on plans for very large launch vehicles, the Saturn series, and the even larger Nova series. In the midst of these plans, von Braun was transferred from the Army to NASA, and made Director of the Marshall Space Flight Center. The initial direct ascent plan to send the three-man Apollo Command/Service Module directly to the lunar surface, on top of a large descent rocket stage, would require a Nova-class launcher, with a lunar payload capability of over . The June 11, 1962, decision to use lunar orbit rendezvous enabled the Saturn V to replace the Nova, and the MSFC proceeded to develop the Saturn rocket family for Apollo.\n\nSince Apollo, like Mercury, would require a launch escape system (LES) in case of a launch failure, a relatively small rocket was required for qualification flight testing of this system. A size bigger than the NAA Little Joe would be required, so the Little Joe II was built by General Dynamics/Convair. After an August 1963 qualification test flight, four LES test flights (A-001 through 004) were made at the White Sands Missile Range between May 1964 and January 1966.\n\nSince Apollo, like Mercury, used more than one launch vehicle for space missions, NASA used spacecraft-launch vehicle combination series numbers: AS-10x for Saturn I, AS-20x for Saturn IB, and AS-50x for Saturn V (compare Mercury-Redstone 3, Mercury-Atlas 6) to designate and plan all missions, rather than numbering them sequentially as in Project Gemini. This was changed by the time manned flights began.\n\nSaturn I, the first US heavy lift launch vehicle, was initially planned to launch partially equipped CSMs in low Earth orbit tests. The S-I first stage burned RP-1 with liquid oxygen (LOX) oxidizer in eight clustered Rocketdyne H-1 engines, to produce of thrust. The S-IV second stage used six liquid hydrogen-fueled Pratt & Whitney RL-10 engines with of thrust. A planned Centaur (S-V) third stage with two RL-10 engines never flew on Saturn I.\n\nThe first four Saturn I test flights were launched from LC-34, with only live first stages, carrying dummy upper stages filled with water. The first flight with a live S-IV was launched from LC-37. This was followed by five launches of boilerplate CSMs (designated AS-101 through AS-105) into orbit in 1964 and 1965. The last three of these further supported the Apollo program by also carrying Pegasus satellites, which verified the safety of the translunar environment by measuring the frequency and severity of micrometeorite impacts.\n\nIn September 1962, NASA planned to launch four manned CSM flights on the Saturn I from late 1965 through 1966, concurrent with Project Gemini. The payload capacity would have severely limited the systems which could be included, so the decision was made in October 1963 to use the uprated Saturn IB for all manned Earth orbital flights.\n\nThe Saturn IB was an upgraded version of the Saturn I. The S-IB first stage increased the thrust to by uprating the H-1 engine. The second stage replaced the S-IV with the S-IVB-200, powered by a single J-2 engine burning liquid hydrogen fuel with LOX, to produce of thrust. A restartable version of the S-IVB was used as the third stage of the Saturn V. The Saturn IB could send over into low Earth orbit, sufficient for a partially fueled CSM or the LM. Saturn IB launch vehicles and flights were designated with an AS-200 series number, \"AS\" indicating \"Apollo Saturn\" and the \"2\" indicating the second member of the Saturn rocket family.\n\nSaturn V launch vehicles and flights were designated with an AS-500 series number, \"AS\" indicating \"Apollo Saturn\" and the \"5\" indicating Saturn V. The three-stage Saturn V was designed to send a fully fueled CSM and LM to the Moon. It was in diameter and stood tall with its lunar payload. Its capability grew to for the later advanced lunar landings. The S-IC first stage burned RP-1/LOX for a rated thrust of , which was upgraded to . The second and third stages burned liquid hydrogen, and the third stage was a modified version of the S-IVB, with thrust increased to and capability to restart the engine for translunar injection after reaching a parking orbit.\n\nNASA's Director of Flight Crew Operations during the Apollo program was Donald K. \"Deke\" Slayton, one of the original Mercury Seven astronauts who was medically grounded in September 1962 due to a heart murmur. Slayton was responsible for making all Gemini and Apollo crew assignments.\nThirty-two astronauts were assigned to fly missions in the Apollo program. Twenty-four of these left Earth's orbit and flew around the Moon between December 1968 and December 1972 (three of them twice). Half of the 24 walked on the Moon's surface, though none of them returned to it after landing once. One of the moonwalkers was a trained geologist. Of the 32, Gus Grissom, Ed White, and Roger Chaffee were killed during a ground test in preparation for the Apollo 1 mission.\n\nThe Apollo astronauts were chosen from the Project Mercury and Gemini veterans, plus from two later astronaut groups. All missions were commanded by Gemini or Mercury veterans. Crews on all development flights (except the Earth orbit CSM development flights) through the first two landings on Apollo 11 and Apollo 12, included at least two (sometimes three) Gemini veterans. Dr. Harrison Schmitt, a geologist, was the first NASA scientist astronaut to fly in space, and landed on the Moon on the last mission, Apollo 17. Schmitt participated in the lunar geology training of all of the Apollo landing crews.\n\nNASA awarded all 32 of these astronauts its highest honor, the Distinguished Service Medal, given for \"distinguished service, ability, or courage\", and personal \"contribution representing substantial progress to the NASA mission\". The medals were awarded posthumously to Grissom, White, and Chaffee in 1969, then to the crews of all missions from Apollo 8 onward. The crew that flew the first Earth orbital test mission Apollo 7, Walter M. Schirra, Donn Eisele, and Walter Cunningham, were awarded the lesser NASA Exceptional Service Medal, because of discipline problems with the Flight Director's orders during their flight. The NASA Administrator in October, 2008, decided to award them the Distinguished Service Medals, by this time posthumously to Schirra and Eisele.\n\nThe nominal planned lunar landing mission proceeded as follows:\n\n\nTwo Block I CSMs were launched from LC-34 on suborbital flights in 1966 with the Saturn IB. The first, AS-201 launched on February 26, reached an altitude of and splashed down downrange in the Atlantic Ocean. The second, AS-202 on August 25, reached altitude and was recovered downrange in the Pacific Ocean. These flights validated the Service Module engine and the Command Module heat shield.\n\nA third Saturn IB test, AS-203 launched from pad 37, went into orbit to support design of the S-IVB upper stage restart capability needed for the Saturn V. It carried a nosecone instead of the Apollo spacecraft, and its payload was the unburned liquid hydrogen fuel, the behavior of which engineers measured with temperature and pressure sensors, and a TV camera. This flight occurred on July 5, before AS-202, which was delayed because of problems getting the Apollo spacecraft ready for flight.\n\nTwo manned orbital Block I CSM missions were planned: AS-204 and AS-205. The Block I crew positions were titled Command Pilot, Senior Pilot, and Pilot. The Senior Pilot would assume navigation duties, while the Pilot would function as a systems engineer. The astronauts would wear a modified version of the Gemini spacesuit.\n\nAfter an unmanned LM test flight AS-206, a crew would fly the first Block II CSM and LM in a dual mission known as AS-207/208, or AS-278 (each spacecraft would be launched on a separate Saturn IB). The Block II crew positions were titled Commander (CDR) Command Module Pilot (CMP) and Lunar Module Pilot (LMP). The astronauts would begin wearing a new Apollo A6L spacesuit, designed to accommodate lunar extravehicular activity (EVA). The traditional visor helmet was replaced with a clear \"fishbowl\" type for greater visibility, and the lunar surface EVA suit would include a water-cooled undergarment.\n\nDeke Slayton, the grounded Mercury astronaut who became Director of Flight Crew Operations for the Gemini and Apollo programs, selected the first Apollo crew in January 1966, with Grissom as Command Pilot, White as Senior Pilot, and rookie Donn F. Eisele as Pilot. But Eisele dislocated his shoulder twice aboard the KC135 weightlessness training aircraft, and had to undergo surgery on January 27. Slayton replaced him with Chaffee. NASA announced the final crew selection for AS-204 on March 21, 1966, with the backup crew consisting of Gemini veterans James McDivitt and David Scott, with rookie Russell L. \"Rusty\" Schweickart. Mercury/Gemini veteran Wally Schirra, Eisele, and rookie Walter Cunningham were announced on September 29 as the prime crew for AS-205.\n\nIn December 1966, the AS-205 mission was canceled, since the validation of the CSM would be accomplished on the 14-day first flight, and AS-205 would have been devoted to space experiments and contribute no new engineering knowledge about the spacecraft. Its Saturn IB was allocated to the dual mission, now redesignated AS-205/208 or AS-258, planned for August 1967. McDivitt, Scott and Schweickart were promoted to the prime AS-258 crew, and Schirra, Eisele and Cunningham were reassigned as the Apollo 1 backup crew.\n\nThe spacecraft for the AS-202 and AS-204 missions were delivered by North American Aviation to the Kennedy Space Center with long lists of equipment problems which had to be corrected before flight; these delays caused the launch of AS-202 to slip behind AS-203, and eliminated hopes the first manned mission might be ready to launch as soon as November 1966, concurrently with the last Gemini mission. Eventually the planned AS-204 flight date was pushed to February 21, 1967.\n\nNorth American Aviation was prime contractor not only for the Apollo CSM, but for the Saturn V S-II second stage as well, and delays in this stage pushed the first unmanned Saturn V flight AS-501 from late 1966 to November 1967. (The initial assembly of AS-501 had to use a dummy spacer spool in place of the stage.)\n\nThe problems with North American were severe enough in late 1965 to cause Manned Space Flight Administrator George Mueller to appoint program director Samuel Phillips to head a \"tiger team\" to investigate North American's problems and identify corrections. Phillips documented his findings in a December 19 letter to NAA president Lee Atwood, with a strongly worded letter by Mueller, and also gave a presentation of the results to Mueller and Deputy Administrator Robert Seamans. Meanwhile, Grumman was also encountering problems with the Lunar Module, eliminating hopes it would be ready for manned flight in 1967, not long after the first manned CSM flights.\n\nGrissom, White, and Chaffee decided to name their flight Apollo 1 as a motivational focus on the first manned flight. They trained and conducted tests of their spacecraft at North American, and in the altitude chamber at the Kennedy Space Center. A \"plugs-out\" test was planned for January, which would simulate a launch countdown on LC-34 with the spacecraft transferring from pad-supplied to internal power. If successful, this would be followed by a more rigorous countdown simulation test closer to the February 21 launch, with both spacecraft and launch vehicle fueled.\n\nThe plugs-out test began on the morning of January 27, 1967, and immediately was plagued with problems. First the crew noticed a strange odor in their spacesuits, which delayed the sealing of the hatch. Then, communications problems frustrated the astronauts and forced a hold in the simulated countdown. During this hold, an electrical fire began in the cabin, and spread quickly in the high pressure, 100% oxygen atmosphere. Pressure rose high enough from the fire that the cabin inner wall burst, allowing the fire to erupt onto the pad area and frustrating attempts to rescue the crew. The astronauts were asphyxiated before the hatch could be opened.\n\nNASA immediately convened an accident review board, overseen by both houses of Congress. While the determination of responsibility for the accident was complex, the review board concluded that \"deficiencies existed in Command Module design, workmanship and quality control.\" At the insistence of NASA Administrator Webb, North American removed Harrison Storms as Command Module program manager. Webb also reassigned Apollo Spacecraft Program Office (ASPO) Manager Joseph Francis Shea, replacing him with George Low.\nTo remedy the causes of the fire, changes were made in the Block II spacecraft and operational procedures, the most important of which were use of a nitrogen/oxygen mixture instead of pure oxygen before and during launch, and removal of flammable cabin and space suit materials. The Block II design already called for replacement of the Block I plug-type hatch cover with a quick-release, outward opening door. NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights. Crew members would also exclusively wear modified, fire-resistant A7L Block II space suits, and would be designated by the Block II titles, regardless of whether a LM was present on the flight or not.\n\nOn April 24, 1967, Mueller published an official Apollo mission numbering scheme, using sequential numbers for all flights, manned or unmanned. The sequence would start with Apollo 4 to cover the first three unmanned flights while retiring the Apollo 1 designation to honor the crew, per their widows' wishes.\n\nIn September 1967, Mueller approved a sequence of mission types which had to be successfully accomplished in order to achieve the manned lunar landing. Each step had to be successfully accomplished before the next ones could be performed, and it was unknown how many tries of each mission would be necessary; therefore letters were used instead of numbers. The A missions were unmanned Saturn V validation; B was unmanned LM validation using the Saturn IB; C was manned CSM Earth orbit validation using the Saturn IB; D was the first manned CSM/LM flight (this replaced AS-258, using a single Saturn V launch); E would be a higher Earth orbit CSM/LM flight; F would be the first lunar mission, testing the LM in lunar orbit but without landing (a \"dress rehearsal\"); and G would be the first manned landing. The list of types covered follow-on lunar exploration to include H lunar landings, I for lunar orbital survey missions, and J for extended-stay lunar landings.\n\nThe delay in the CSM caused by the fire enabled NASA to catch up on man-rating the LM and Saturn V. Apollo 4 (AS-501) was the first unmanned flight of the Saturn V, carrying a Block I CSM on November 9, 1967. The capability of the Command Module's heat shield to survive a trans-lunar reentry was demonstrated by using the Service Module engine to ram it into the atmosphere at higher than the usual Earth-orbital reentry speed.\n\nApollo 5 (AS-204) was the first unmanned test flight of LM in Earth orbit, launched from pad 37 on January 22, 1968, by the Saturn IB that would have been used for Apollo 1. The LM engines were successfully test-fired and restarted, despite a computer programming error which cut short the first descent stage firing. The ascent engine was fired in abort mode, known as a \"fire-in-the-hole\" test, where it was lit simultaneously with jettison of the descent stage. Although Grumman wanted a second unmanned test, George Low decided the next LM flight would be manned.\n\nThis was followed on April 4, 1968, by Apollo 6 (AS-502) which carried a CSM and a LM Test Article as ballast. The intent of this mission was to achieve trans-lunar injection, followed closely by a simulated direct-return abort, using the Service Module engine to achieve another high-speed reentry. The Saturn V experienced pogo oscillation, a problem caused by non-steady engine combustion, which damaged fuel lines in the second and third stages. Two S-II engines shut down prematurely, but the remaining engines were able to compensate. The damage to the third stage engine was more severe, preventing it from restarting for trans-lunar injection. Mission controllers were able to use the Service Module engine to essentially repeat the flight profile of Apollo 4. Based on the good performance of Apollo 6 and identification of satisfactory fixes to the Apollo 6 problems, NASA declared the Saturn V ready to fly men, cancelling a third unmanned test.\n\nApollo 7, launched from LC-34 on October 11, 1968, was the C mission, crewed by Schirra, Eisele and Cunningham. It was an 11-day Earth-orbital flight which tested the CSM systems.\n\nApollo 8 was planned to be the D mission in December 1968, crewed by McDivitt, Scott and Schweickart, launched on a Saturn V instead of two Saturn IBs. In the summer it had become clear that the LM would not be ready in time. Rather than waste the Saturn V on another simple Earth-orbiting mission, ASPO Manager George Low suggested the bold step of sending Apollo 8 to orbit the Moon instead, deferring the D mission to the next mission in March 1969, and eliminating the E mission. This would keep the program on track. The Soviet Union had sent two tortoises, mealworms, wine flies, and other lifeforms around the Moon on September 15, 1968, aboard Zond 5, and it was believed they might soon repeat the feat with human cosmonauts. The decision was not announced publicly until successful completion of Apollo 7. Gemini veterans Frank Borman and Jim Lovell, and rookie William Anders captured the world's attention by making ten lunar orbits in 20 hours, transmitting television pictures of the lunar surface on Christmas Eve, and returning safely to Earth.\n\nThe following March, LM flight, rendezvous and docking were successfully demonstrated in Earth orbit on Apollo 9, and Schweickart tested the full lunar EVA suit with its Portable Life Support System (PLSS) outside the LM. The F mission was successfully carried out on Apollo 10 in May 1969 by Gemini veterans Thomas P. Stafford, John Young and Eugene Cernan. Stafford and Cernan took the LM to within of the lunar surface.\n\nThe G mission was achieved on Apollo 11 in July 1969 by an all-Gemini veteran crew consisting of Neil Armstrong, Michael Collins and Buzz Aldrin. Armstrong and Aldrin performed the first landing at the Sea of Tranquility at 20:17:40 UTC on July 20, 1969. They spent a total of 21 hours, 36 minutes on the surface, and spent 2 hours, 31 minutes outside the spacecraft, walking on the surface, taking photographs, collecting material samples, and deploying automated scientific instruments, while continuously sending black-and-white television back to Earth. The astronauts returned safely on July 24.\n\nIn November 1969, Gemini veteran Charles \"Pete\" Conrad and rookie Alan L. Bean made a precision landing on Apollo 12 within walking distance of the Surveyor 3 unmanned lunar probe, which had landed in April 1967 on the Ocean of Storms. The Command Module Pilot was Gemini veteran Richard F. Gordon Jr. Conrad and Bean carried the first lunar surface color television camera, but it was damaged when accidentally pointed into the Sun. They made two EVAs totaling 7 hours and 45 minutes. On one, they walked to the Surveyor, photographed it, and removed some parts which they returned to Earth.\n\nThe success of the first two landings allowed the remaining missions to be crewed with a single veteran as Commander, with two rookies. Apollo 13 launched Lovell, Jack Swigert, and Fred Haise in April 1970, headed for the Fra Mauro formation. But two days out, a liquid oxygen tank exploded, disabling the Service Module and forcing the crew to use the LM as a \"life boat\" to return to Earth. Another NASA review board was convened to determine the cause, which turned out to be a combination of damage of the tank in the factory, and a subcontractor not making a tank component according to updated design specifications. Apollo was grounded again, for the remainder of 1970 while the oxygen tank was redesigned and an extra one was added.\n\nThe contracted batch of 15 Saturn Vs were enough for lunar landing missions through Apollo 20. NASA publicized a preliminary list of eight more planned landing sites, with plans to increase the mass of the CSM and LM for the last five missions, along with the payload capacity of the Saturn V. These final missions would combine the I and J types in the 1967 list, allowing the CMP to operate a package of lunar orbital sensors and cameras while his companions were on the surface, and allowing them to stay on the Moon for over three days. These missions would also carry the Lunar Roving Vehicle (LRV) increasing the exploration area and allowing televised liftoff of the LM. Also, the Block II spacesuit was revised for the extended missions to allow greater flexibility and visibility for driving the LRV.\n\nAbout the time of the first landing in 1969, it was decided to use an existing Saturn V to launch the Skylab orbital laboratory pre-built on the ground, replacing the original plan to construct it in orbit from several Saturn IB launches; this eliminated Apollo 20. NASA's yearly budget also began to shrink in light of the successful landing, and NASA also had to make funds available for the development of the upcoming Space Shuttle. By 1971, the decision was made to also cancel missions 18 and 19. The two unused Saturn Vs became museum exhibits at the John F. Kennedy Space Center on Merritt Island, Florida, George C. Marshall Space Center in Huntsville, Alabama, Michoud Assembly Facility in New Orleans, Louisiana, and Lyndon B. Johnson Space Center in Houston, Texas.\n\nThe cutbacks forced mission planners to reassess the original planned landing sites in order to achieve the most effective geological sample and data collection from the remaining four missions. Apollo 15 had been planned to be the last of the H series missions, but since there would be only two subsequent missions left, it was changed to the first of three J missions.\n\nApollo 13's Fra Mauro mission was reassigned to Apollo 14, commanded in February 1971 by Mercury veteran Alan Shepard, with Stuart Roosa and Edgar Mitchell. This time the mission was successful. Shepard and Mitchell spent 33 hours and 31 minutes on the surface, and completed two EVAs totalling 9 hours 24 minutes, which was a record for the longest EVA by a lunar crew at the time.\n\nIn August 1971, just after conclusion of the Apollo 15 mission, President Richard Nixon proposed canceling the two remaining lunar landing missions, Apollo 16 and 17. Office of Management and Budget Deputy Director Caspar Weinberger was opposed to this, and persuaded Nixon to keep the remaining missions.\n\nApollo 15 was launched on July 26, 1971, with David Scott, Alfred Worden and James Irwin. Scott and Irwin landed on July 30 near Hadley Rille, and spent just under two days, 19 hours on the surface. In over 18 hours of EVA, they collected about of lunar material.\n\nApollo 16 landed in the Descartes Highlands on April 20, 1972. The crew was commanded by John Young, with Ken Mattingly and Charles Duke. Young and Duke spent just under three days on the surface, with a total of over 20 hours EVA.\n\nApollo 17 was the last of the Apollo program, landing in the Taurus-Littrow region in December 1972. Eugene Cernan commanded Ronald E. Evans and NASA's first scientist-astronaut, geologist Dr. Harrison H. Schmitt. Schmitt was originally scheduled for Apollo 18, but the lunar geological community lobbied for his inclusion on the final lunar landing. Cernan and Schmitt stayed on the surface for just over three days and spent just over 23 hours of total EVA.\n\nSource: \"Apollo by the Numbers: A Statistical Reference\" (Orloff 2004)\n\nThe Apollo program returned over of lunar rocks and soil to the Lunar Receiving Laboratory in Houston. Today, 75% of the samples are stored at the Lunar Sample Laboratory Facility built in 1979.\n\nThe rocks collected from the Moon are extremely old compared to rocks found on Earth, as measured by radiometric dating techniques. They range in age from about 3.2 billion years for the basaltic samples derived from the lunar maria, to about 4.6 billion years for samples derived from the highlands crust. As such, they represent samples from a very early period in the development of the Solar System, that are largely absent on Earth. One important rock found during the Apollo Program is dubbed the Genesis Rock, retrieved by astronauts David Scott and James Irwin during the Apollo 15 mission. This anorthosite rock is composed almost exclusively of the calcium-rich feldspar mineral anorthite, and is believed to be representative of the highland crust. A geochemical component called KREEP was discovered by Apollo 12, which has no known terrestrial counterpart. KREEP and the anorthositic samples have been used to infer that the outer portion of the Moon was once completely molten (see lunar magma ocean).\n\nAlmost all the rocks show evidence of impact process effects. Many samples appear to be pitted with micrometeoroid impact craters, which is never seen on Earth rocks, due to the thick atmosphere. Many show signs of being subjected to high pressure shock waves that are generated during impact events. Some of the returned samples are of \"impact melt\" (materials melted near an impact crater.) All samples returned from the Moon are highly brecciated as a result of being subjected to multiple impact events.\n\nAnalysis of composition of the lunar samples supports the giant impact hypothesis, that the Moon was created through impact of a large astronomical body with the Earth.\n\nWhen President Kennedy first chartered the Moon landing program, a preliminary cost estimate of $7 billion was generated, but this proved an extremely unrealistic guess of what could not possibly be determined precisely, and James Webb used his judgment as administrator to change the estimate to $20 billion before giving it to Vice President Johnson.\n\nWhen Kennedy made his 1962 speech at Rice University, the annual space budget was $5.4 billion, and he described this cost as 40 cents per person per week, \"somewhat less than we pay for cigarettes and cigars every year\", but that the Moon program would soon raise this to \"more than 50 cents a week for every man, woman and child in the United States\".\n\nWebb's estimate shocked many at the time (including the President) but ultimately proved accurate. In January 1969, NASA prepared an itemized estimate of the run-out cost of the Apollo program. The total came to $23.9 billion, itemized as follows:\n\nThe final cost of Apollo was reported to Congress as $25.4 billion in 1973, It took up the majority of NASA's budget while it was being developed. For example, in 1966 it accounted for about 60 percent of NASA's total $5.2 billion budget. That was one of the biggest investment of the US in science, research and development, and employed thousands of American scientists. A single Saturn V launch in 1969 cost up to $375 million, compared to the National Science Foundation's fiscal year 1970 budget of $440 million.\n\nIn 2009, NASA held a symposium on project costs which presented an estimate of the Apollo program costs in 2005 dollars as roughly $170 billion ($ in dollars). This included all research and development costs; the procurement of 15 Saturn V rockets, 16 Command/Service Modules, 12 Lunar Modules, plus program support and management costs; construction expenses for facilities and their upgrading, and costs for flight operations. This was based on a Congressional Budget Office report, \"A Budgetary Analysis of NASA's New Vision for Space\", September 2004. \"The Space Review\" estimated in 2010 the cost of Apollo from 1959 to 1973 as $20.4 billion, or $109 billion in 2010 dollars. ($ in dollars)\n\nLooking beyond the manned lunar landings, NASA investigated several post-lunar applications for Apollo hardware. The Apollo Extension Series (\"Apollo X\",) proposed up to 30 flights to Earth orbit, using the space in the Spacecraft Lunar Module Adapter (SLA) to house a small orbital laboratory (workshop). Astronauts would continue to use the CSM as a ferry to the station. This study was followed by design of a larger orbital workshop to be built in orbit from an empty S-IVB Saturn upper stage, and grew into the Apollo Applications Program (AAP). The workshop was to be supplemented by the Apollo Telescope Mount, which could be attached to the ascent stage of the lunar module via a rack. The most ambitious plan called for using an empty S-IVB as an interplanetary spacecraft for a Venus fly-by mission.\n\nThe S-IVB orbital workshop was the only one of these plans to make it off the drawing board. Dubbed Skylab, it was constructed complete on the ground rather than in space, and launched in 1973 using the two lower stages of a Saturn V. It was equipped with an Apollo Telescope Mount. Skylab's last crew departed the station on February 8, 1974, and the station itself re-entered the atmosphere in 1979.\n\nThe Apollo-Soyuz Test Project also used Apollo hardware for the first joint nation space flight, paving the way for future cooperation with other nations in the Space Shuttle and International Space Station programs.\n\nIn September 2007, the X PRIZE Foundation and Google announced the Google Lunar X Prize, to be awarded for a robotic lunar landing mission which transmits close-up images of the Apollo Lunar Modules and other artificial objects on the surface.\n\nIn 2008, Japan Aerospace Exploration Agency's SELENE probe observed evidence of the halo surrounding the Apollo 15 Lunar Module blast crater while orbiting above the lunar surface. In 2009, NASA's robotic Lunar Reconnaissance Orbiter, while orbiting above the Moon, began photographing the remnants of the Apollo program left on the lunar surface, and photographed each site where manned Apollo flights landed. All of the U. S. flags left on the Moon during the Apollo missions were found to still be standing, with the exception of the one left during the Apollo 11 mission, which was blown over during that mission's lift-off from the lunar surface and return to the mission Command Module in lunar orbit; the degree to which these flags retain their original colors remains unknown.\n\nIn a November 16, 2009, editorial, \"The New York Times\" opined:\n\nThe Apollo program has been called the greatest technological achievement in human history. Apollo stimulated many areas of technology, leading to over 1,800 spinoff products as of 2015. The flight computer design used in both the Lunar and Command Modules was, along with the Polaris and Minuteman missile systems, the driving force behind early research into integrated circuits (IC). By 1963, Apollo was using 60 percent of the United States' production of ICs. The crucial difference between the requirements of Apollo and the missile programs was Apollo's much greater need for reliability. While the Navy and Air Force could work around reliability problems by deploying more missiles, the political and financial cost of failure of an Apollo mission was unacceptably high.\n\nThe crew of Apollo 8 sent the first live televised pictures of the Earth and the Moon back to Earth, and read from the creation story in the Book of Genesis, on Christmas Eve 1968. An estimated one quarter of the population of the world saw—either live or delayed—the Christmas Eve transmission during the ninth orbit of the Moon, and an estimated one fifth of the population of the world watched the live transmission of the Apollo 11 moonwalk.\n\nThe Apollo program also affected environmental activism in the 1970s due to photos taken by the astronauts. The most famous, taken by the Apollo 17 astronauts, is \"The Blue Marble\". This image, which was released during a surge in environmentalism, became a symbol of the environmental movement, as a depiction of Earth's frailty, vulnerability, and isolation amid the vast expanse of space.\n\nAccording to \"The Economist\", Apollo succeeded in accomplishing President Kennedy's goal of taking on the Soviet Union in the Space Race, and beat it by accomplishing a singular and significant achievement, and thereby was meant to showcase the superiority of the free-market system as represented by the US. The publication noted the irony that in order to achieve the goal, the program required the organization of tremendous public resources within a vast, centralized government bureaucracy.\n\nAs part of Apollo 11's 40th anniversary in 2009, NASA spearheaded an effort to digitally restore the existing videotapes of the mission's live televised moonwalk. After an exhaustive three-year search for missing tapes of the original video of the Apollo 11 moonwalk, NASA concluded the data tapes had more than likely been accidentally erased.\n\nThe lunar EVA video was produced with a special Apollo TV camera which scanned the picture in a format incompatible with broadcast TV. This required conversion for live television broadcast, which due to the state of 1969 technology caused some degradation of picture quality. But the unconverted picture signal was recorded on magnetic telemetry tapes. In 2006, Stanley Lebar, who had led the team that designed and built the lunar television camera at Westinghouse Electric Corporation, worked with Nafzger to try to locate the missing tapes, with the goal of seeing if more modern technology could produce a broadcast-ready picture closer to the original quality. However, in the intervening years, a magnetic tape shortage prompted NASA to recall massive numbers of magnetic tapes from the National Archives and Records Administration to be reused to record newer satellite data.\n\nWith a budget of $230,000, the surviving original lunar broadcast data from Apollo 11 was compiled by Nafzger and assigned to Lowry Digital for restoration. The video was processed to remove random noise and camera shake without destroying historical legitimacy. The images were from tapes in Australia, the CBS News archive, and kinescope recordings made at Johnson Space Center. The restored video, remaining in black and white, contains conservative digital enhancements and did not include sound quality improvements.\n\nNumerous documentary films cover the Apollo program and the Space Race, including:\n\nThe Apollo program, or certain missions, have been dramatized in \"Apollo 13\" (1995), \"Apollo 11\" (1996), \"From the Earth to the Moon\" (1998), \"The Dish\" (2000), \"Space Race\" (2005), \"Moonshot\" (2009), and \"First Man\" (2018).\n\n\nNASA reports\n\nMultimedia\n"}
{"id": "4982458", "url": "https://en.wikipedia.org/wiki?curid=4982458", "title": "Audio Video Satellite", "text": "Audio Video Satellite\n\nAudio Video Satellite (AVS) is a monthly news magazine published from Karachi, Sindh, Pakistan. The magazine is published in Urdu language.\n\n"}
{"id": "644504", "url": "https://en.wikipedia.org/wiki?curid=644504", "title": "Background independence", "text": "Background independence\n\nBackground independence is a condition in theoretical physics, that requires the defining equations of a theory to be independent of the actual shape of the spacetime and the value of various fields within the spacetime. In particular this means that it must be possible not to refer to a specific coordinate system—the theory must be coordinate-free. In addition, the different spacetime configurations (or backgrounds) should be obtained as different solutions of the underlying equations.\n\nBackground independence is a loosely defined property of a theory of physics. Roughly speaking, it limits the number of mathematical structures used to describe space and time that are put in place \"by hand\". Instead, these structures are the result of dynamical equations, such as Einstein field equations, so that one can determine from first principles what form they should take. Since the form of the metric determines the result of calculations, a theory with background independence is more predictive than a theory without it, since the theory requires fewer inputs to make its predictions. This is analogous to desiring fewer free parameters in a fundamental theory.\n\nSo background independence can be seen as extending the mathematical objects that should be predicted from theory to include not just the parameters, but also geometrical structures. Summarizing this, Rickles writes: \"Background structures are contrasted with dynamical ones, and a background independent theory only possesses the latter type—obviously, background dependent theories are those possessing the former type in addition to the latter type.\"\n\nIn general relativity, background independence is identified with the property that the metric of space–time is the solution of a dynamical equation. In classical mechanics, this is not the case, the metric is fixed by the physicist to match experimental observations. This is undesirable, since the form of the metric impacts the physical predictions, but is not itself predicted by the theory.\n\nManifest background independence is primarily an aesthetic rather than a physical requirement. It is analogous and closely related to requiring in differential geometry that equations be written in a form that is independent of the choice of charts and coordinate embeddings. If a background-independent formalism is present, it can lead to simpler and more elegant equations. However, there is no physical content in requiring that a theory be manifestly background-independent – for example, the equations of general relativity can be rewritten in local coordinates without affecting the physical implications.\n\nAlthough making a property manifest is only aesthetic, it is a useful tool for making sure the theory actually has that property. For example, if a theory is written in a manifestly Lorentz-invariant way, one can check at every step to be sure that Lorentz invariance is preserved. Making a property manifest also makes it clear whether or not the theory actually has that property. The inability to make classical mechanics manifestly Lorentz-invariant does not reflect a lack of imagination on the part of the theorist, but rather a physical feature of the theory. The same goes for making classical mechanics or electromagnetism background-independent.\n\nBecause of the speculative nature of quantum-gravity research, there is much debate as to the correct implementation of background independence. Ultimately, the answer is to be decided by experiment, but until experiments can probe quantum-gravity phenomena, physicists have to settle for debate. Below is a brief summary of the two largest quantum-gravity approaches.\n\nPhysicists have studied models of 3D quantum gravity, which is a much simpler problem than 4D quantum gravity (this is because in 3D, quantum gravity has no local degrees of freedom). In these models, there are non-zero transition amplitudes between two different topologies, or in other words, the topology changes. This and other similar results lead physicists to believe that any consistent quantum theory of gravity should include topology change as a dynamical process.\n\nString theory is usually formulated with perturbation theory around a fixed background. While it is possible that the theory defined this way is locally background-invariant, if so, it is not manifest, and it is not clear what the exact meaning is. One attempt to formulate string theory in a manifestly background-independent fashion is string field theory, but little progress has been made in understanding it.\n\nAnother approach is the conjectured, but yet unproven AdS/CFT duality, which is believed to provide a full, non-perturbative definition of string theory in spacetimes with anti-de Sitter asymptotics. If so, this could describe a kind of superselection sector of the putative background-independent theory. But it would be still restricted to anti-de Sitter space asymptotics, which disagrees with the current observations of our Universe. A full non-perturbative definition of the theory in arbitrary space–time backgrounds is still lacking.\n\nTopology change is an established process in string theory.\n\nA very different approach to quantum gravity called loop quantum gravity is full non-perturbative, manifest background-independent: Geometric quantities, such as area, are predicted without reference to a background metric or asymptotics (e.g. no need for a background metric or an anti-de Sitter asymptotics), only a given topology.\n\n\n"}
{"id": "46509336", "url": "https://en.wikipedia.org/wiki?curid=46509336", "title": "Boost-glide", "text": "Boost-glide\n\nBoost-glide trajectories are a class of spacecraft guidance and reentry trajectories that extend the range of suborbital spaceplanes and reentry vehicles by employing aerodynamic lift in the high upper atmosphere. In most examples, boost-glide roughly doubles the range over the purely ballistic trajectory. In others, a series of \"skips\" allows range to be further extended, and leads to the alternate terms skip-glide and skip reentry.\n\nThe concept was first seriously studied as a way to extend the range of ballistic missiles, but has not been used operationally in this form. The underlying aerodynamic concepts have been used to produce maneuverable reentry vehicles, or MARV, to increase the accuracy of some missiles. More recently the traditional form with an extended gliding phase has been considered as a way to reach targets while flying below their radar coverage.\n\nThe concept has also been used to extend the reentry time for vehicles returning to Earth from the Moon, who would otherwise have to shed a large amount of velocity in a short time and thereby suffer very high heating loads. The Apollo Command Module used what is essentially a one-skip reentry (or partial skip), as did the Soviet Zond and Chinese Chang'e 5-T1. More complex multi-skip reentry is proposed for newer vehicles like the Orion spacecraft.\n\nThe conceptual basis for the boost-glide concept was first noticed by German artillery officers, who found that their \"Peenemünder Pfeilgeschosse\" arrow shells traveled much further when fired from higher altitudes. This was not entirely unexpected due to geometry and thinner air, but when these factors were accounted for, they still could not explain the much greater ranges being seen. Investigations at Peenemünde led them to discover that the longer trajectories in the thinner high-altitude air resulted in the shell having an angle of attack that produced aerodynamic lift at supersonic speeds. At the time this was considered highly undesirable because it made the trajectory very difficult to calculate, but its possible application for extending range was not lost on the observers.\n\nIn June 1939, Kurt Patt of Klaus Riedel's design office at Peenemünde proposed wings for converting rocket speed and altitude into aerodynamic lift and range. He calculated that this would roughly double range of the A-4 rockets from to about . Early development was considered under the A-9 name, although little work other than wind tunnel studies at the Zeppelin-Staaken company would be carried out during the next few years. Low-level research continued until 1942 when it was cancelled.\n\nThe earliest known use of the boost-glide concept for truly long-range use dates to the 1941 \"Silbervogel\" proposal by Eugen Sänger for a rocket powered bomber able to attack New York City from bases in Germany and then fly on for landing somewhere in the Pacific Ocean held by the Empire of Japan. The idea would be to use the vehicle's wings to generate lift and pull up into a new ballistic trajectory, exiting the atmosphere again and giving the vehicle time to cool off between the skips. It was later demonstrated that the heating load during the skips was much higher than initially calculated, and would have melted the spacecraft.\n\nIn 1943, the A-9 work was dusted off again, this time under the name A-4b. It has been suggested this was either because it was now based on an otherwise unmodified A-4, or because the A-4 program had \"national priority\" by this time, and placing the development under the A-4 name guaranteed funding. A-4b used swept wings in order to extend the range of the V2 enough to allow attacks on UK cities in The Midlands or to reach London from areas deeper within Germany. The A-9 was originally similar, but later featured long ogive delta shaped wings instead of the more conventional swept ones. This design was adapted as a manned upper stage for the A-9/A-10 intercontinental missile, which would glide from a point over the Atlantic with just enough range to bomb New York before the pilot bailed out.\n\nIn the immediate post-war era, Soviet rocket engineer Aleksei Isaev found a copy of an updated August 1944 report on the \"Silbervogel\" concept. He had the paper translated to Russian, and it eventually came to the attention of Joseph Stalin who was intensely interested in the concept of an antipodal bomber. In 1946, he sent his son Vasily Stalin and scientist Grigori Tokaty, who had also worked on winged rockets before the war, to visit Sänger and Irene Bredt in Paris and attempt to convince them to join a new effort in the Soviet Union. Sänger and Bredt turned down the invitation.\n\nIn November 1946, the Soviets formed the NII-1 design bureau under Mstislav Keldysh to develop their own version without Sänger and Bredt. Their early work convinced them to convert from a rocket powered hypersonic skip-glide concept to a ramjet powered supersonic cruise missile, not unlike the Navaho being developed in the United States during the same period. Development continued for a time as the Keldysh bomber, but improvements in conventional ballistic missiles ultimately rendered the project unnecessary.\n\nIn the United States, the skip-glide concept was advocated by many of the German scientists who moved there, primarily Walter Dornberger and Krafft Ehricke at Bell Aircraft. In 1952, Bell proposed a bomber concept that was essentially a vertical launch version of \"Silbervogel\" known as Bomi. This led to a number of follow-on concepts during the 1950s, including Robo, Hywards, Brass Bell, and ultimately the Boeing X-20 Dyna-Soar. Earlier designs were generally bombers, while later models were aimed at reconnaissance or other roles. Dornberger and Ehricke also collaborated on a 1955 \"Popular Science\" article pitching the idea for airliner use.\n\nThe introduction of successful intercontinental ballistic missiles (ICBMs) in the offensive role ended any interest in the skip-glide bomber concepts, as did the reconnaissance satellite for the spyplane roles. The X-20 space fighter saw continued interest through the 1960s, but was ultimately the victim of budget cuts; after another review in March 1963, Robert McNamara canceled the program in December, noting that after $400 million had been spent they still had no mission for it to fulfill.\n\nRussia in March 2018 unveiled hypersonic glide vehicle Avangard.\n\nThrough the 1960s, the skip-glide concept saw interest not as a way to extend range, which was no longer a concern with modern missiles, but as the basis for maneuverable reentry vehicles for ICBMs. The first known example was the Alpha Draco tests of 1959, followed by the Boost Glide Reentry Vehicle (BGRV) test series, ASSET and PRIME.\n\nThis research was eventually put to use in the Pershing II's MARV reentry vehicle. In this case, there is no extended gliding phase; the warhead uses lift only for short periods to adjust its trajectory. This is used late in the reentry process, combining data from a Singer Kearfott inertial navigation system with a Goodyear Aerospace active radar. Similar concepts have been developed for most nuclear-armed nation's theatre ballistic missiles.\n\nIn contrast to these maneuvering warhead concepts, there has been growing interest in the traditional boost-glide concept not to extend range \"per se\", but to allow it to reach a given range while flying at a much lower altitude. The goal, in this case, is to keep the reentry vehicle below radar coverage until it enters the terminal phase. Such a system is assumed to be used on the Chinese DF-21D anti-ship ballistic missile, which is also believed to maneuver during the terminal phase to make interception more difficult. The later DF-26, a development of the DP-21, may be armed with the WU-14, a hypersonic glide vehicle that has been successfully tested six times by the Chinese. Similar efforts by Russia led to the Kholod and Igla hypersonic test projects, and more recently the Yu-71 hypersonic glide vehicle which can be carried by RS-28 Sarmat.\n\nIn the early 21st century, boost-glide became the topic of some interest as a possible solution to the Prompt Global Strike (PGS), which seeks a weapon that can hit a target anywhere on the Earth within one hour of launch from the United States. PGS does not define the mode of operation, and current studies include Advanced Hypersonic Weapon boost-glide warhead, Falcon HTV-2 hypersonic aircraft, and submarine-launched missiles. The WU-14 would be similar to these weapons. Hypersonic Glide Vehicles could be used for delivering quick nuclear decapitating strikes.\n\nWhile flying below the operational envelope of Exoatmospheric Kill Vehicles, the tradeoff with HGVs in comparison to conventional MIRVs are many-fold, including no needle in a haystack protection from missile decoys and both less speed and altitude as they near the target, all of these characteristics result in HGVs having poorer survivability odds when placed against lower-tier interceptors. Some examples of which include the high thrust mach-10 Sprint missile, its US derivatives and the still operational mach-17 Russian 53T6, \"ABM-3 Gazelle\". Moreover, the possible re-emergence of nuclear or hit-to-kill stratosphere reaching guns, guided and triggered by forward operating flight-path sensors (such as the 2016 Hypervelocity Projectile (HVP) in development for the M109 howitzer) also will decrease HGV survivability odds.\n\nOther more speculative counter-hypersonic vehicle measures may involve laser or rail gun technologies.\n\nThe technique was used by the Soviet Zond series of circumlunar spacecraft, which used one skip before landing. In this case a true skip was required in order to allow the spacecraft to reach the higher-latitude landing areas. Zond 6, Zond 7 and Zond 8 made successful skip entries, although Zond 5 did not. The Chang'e 5-T1, which flew mission profiles similar to Zond, also used this technique.\n\nThe Apollo Command Module used a skip-like concept to lower the heating loads on the vehicle by extending the re-entry time, but the spacecraft did not leave the atmosphere again and there has been considerable debate whether this makes it a true skip profile. NASA referred to it simply as \"lifting entry\". A true multi-skip profile was considered as part of the Apollo Skip Guidance concept, but this was not used on any manned flights. The concept continues to appear on more modern vehicles like the Orion spacecraft, using onboard computers.\n\nUsing simplified equations of motion and assuming that during the atmospheric flight both drag and lift forces will be much larger than the gravity force acting on the vehicle, the following analytical relations for a skip reentry flight can be derived:\n\nformula_1\n\nWhere gamma is the flight path angle relative to the local horizontal, the subscript E indicates the conditions at the start of the entry and the subscript F indicates the conditions at the end of the entry flight.\n\nThe velocity V before and after the entry can be derived to relate as follows:\n\nformula_2\n\nWhere L/D equals the lift to drag ratio of the vehicle.\n\n\n"}
{"id": "236564", "url": "https://en.wikipedia.org/wiki?curid=236564", "title": "Calcium–aluminium-rich inclusion", "text": "Calcium–aluminium-rich inclusion\n\nA calcium–aluminium-rich inclusion or Ca–Al-rich inclusion (CAI) is a submillimeter- to centimeter-sized light-colored calcium- and aluminium-rich inclusion found in carbonaceous chondrite meteorites. They are probably the oldest substances in the Solar System. The oldest age was measured in an inclusion of the CV3 carbonaceous chondrite Northwest Africa (NWA) 2364 and was dated at .\n\nCAIs consist of minerals that are among the first solids condensed from the cooling protoplanetary disk. They are thought to have formed as fine-grained condensates from a high temperature (>1300 K) gas that existed in the protoplanetary disk at early stages of Solar System formations. Some of them were probably remelted later resulting in distinct coarser textures. The most common and characteristic minerals in CAIs include anorthite, melilite, perovskite, aluminous spinel, hibonite, calcic pyroxene, and forsterite-rich olivine.\n\nUsing lead (Pb–Pb) isotopic dating of a CAI from NWA 2364, an age of million years has been calculated, which can be interpreted as the beginning of the formation of the planetary system. Radiometric dating with Pb–Pb, Al–Mg and Cr–Mn chronometers shows that the CAIs formed up to 3 million years before the chondrules appeared, although some chondrules may have formed simultaneously with some CAIs.\n\n\n"}
{"id": "12510214", "url": "https://en.wikipedia.org/wiki?curid=12510214", "title": "Carbochemistry", "text": "Carbochemistry\n\nCarbochemistry is the branch of chemistry that studies the transformation of coals (bituminous coal, anthracite, lignite, graphite, and charcoal) into useful products and raw materials. The processes that are used in carbochemistry include degasification processes such as carbonization and coking, gasification processes, and liquefaction processes.\n\nThe beginning of carbochemistry goes back to the 16th century. At that time, large quantities of charcoal were needed for the smelting of iron ores. Since the production of charcoal required large amounts of slowly-regenerating wood, the use of coal was studied. The use of pure coal was difficult because of the amount of liquid and solid by-products that were generated. In order to improve the handling the coal was initially treated as wood in kilns to produce coke.\n\nAround 1684, John Clayton discovered that coal gas generated from coal was combustible. He described his discovery in the \"Philosophical Transactions of the Royal Society\".\n\n"}
{"id": "14757039", "url": "https://en.wikipedia.org/wiki?curid=14757039", "title": "Community indifference curve", "text": "Community indifference curve\n\nA community indifference curve is an illustration of different combinations of commodity quantities that would bring a whole community the same level of utility. The model can be used to describe any community, such as a town or an entire nation. In a community indifference curve, the indifference curves of all those individuals are aggregated and held at an equal and constant level of utility.\n\nInvented by Tibor Scitovsky, a Hungarian born economist, in 1941.\n\nA community indifference curve (CIC) provides the set of all aggregate endowments (x-bar, y-bar) = (x1 + x2, y1, + y2) needed to achieve a given distribution of utilities, (u1-bar, u2-bar). The community indifference curve can be found by solving for the following minimization problem:<br>\n<br>\n<br>\nCICs assume allocative efficiency amongst members of the community. Allocative Efficiency provides that MRS1xy = MRS2 xy.\nThe CIC comes from solving for y-bar in terms of x-bar, y-cic(x-bar).\n\nCommunity indifference curves are an aggregate of individual indifference curves.\n\n\nAlbouy, David. \"Welfare Economics with a Full Production Economy.\" Economics 481. Fall 2007.\n\nDeardorff's Glossary of International Economics.\n"}
{"id": "24640834", "url": "https://en.wikipedia.org/wiki?curid=24640834", "title": "David McFarland", "text": "David McFarland\n\nDavid McFarland is a scientist specialized in the field of animal behavior and more recently the broadening of this understanding to \"artificial ethology\" and robotics. He was educated at Leighton Park School, the Quaker school at Reading. He later taught at Balliol College Oxford. He is the author of a number of books, including \"Animal Behaviour: Psychobiology, Ethology, and Evolution\", and \"Companion to Animal Behaviour\", published by Oxford University Press. He is also the author of the \"Dictionary of Animal Behaviour\", published by Oxford Paperback Reference in 2006.\n\nNewest publications are listed first.\n\n"}
{"id": "3379184", "url": "https://en.wikipedia.org/wiki?curid=3379184", "title": "Dependency need", "text": "Dependency need\n\nDependency need is \"the vital, originally infantile needs for mothering, love, affection, shelter, protection, security, food, and warmth.\" (Segen, 1992) A dependency need is thought to be characterized by two components: (1) It is a real need of an organism, something that must be present in order for the organism to be able to thrive, (2) It is something that an individual cannot provide for him or herself. It is well known that infants have many dependency needs; some of these needs are obvious, others have only come to the attention of researchers as the result of epidemiological studies. The more obvious needs of infants include: adequate feeding, adequate watering, adequate cleaning, adequate shelter, and more specifically, keeping the infant's body temperature within the narrow range of normalcy. On the other hand, it was not well known until the middle of the 20th century that infants also required the presence of warmth and affection, known as \"maternal warmth\". The greatest number of dependency needs seem to be encompassed in infancy, however, dependency needs begin to change and decrease with age and maturity. This marked decrease in dependency needs as an individual gets older can be largely attributed to the notion that, as an individual gets older, he or she becomes capable of providing these things for him or herself. However, to some extent, these needs remain present even into adulthood. Even as adults, people have certain universal dependency needs that remain constant throughout the lifespan that they are not able to provide for themselves; these include: the need to belong, need for affection, as well as the need for emotional support. These needs can usually be met by partnership, in which both partners get used to depending on one another. If adults lack partnership, their needs can usually be met by family and/or friend relationships.\n\nDependency need is an important psychological concept, encompassing the fields of psychological, evolutionary, and ethological theory. Need, in general, is a concept greatly studied in varying psychological fields, by psychologists with varying specialties. Need is particularly important to the area of personality psychology. The concept of need can be defined as a \"state of tension within a person\", and as the need is satisfied, the state of tension is reduced. (Larsen & Buss, 2008) It is thought that all individuals have needs, and that needs organize perceptions, guiding individuals to see what they want (or need) to see (Larsen & Buss). A physical or psychological need is capable of organizing action by compelling an individual to do what is necessary in order to fulfill such a need. (Larsen & Buss, 2008) After action has been taken to fulfill the need, the need subsides until it is again desired and recurs.\n\nIt can be hypothesized that the concept of dependency need originated from the well-known psychologists Henry Murray and Abraham Maslow's ideas about needs. According to Murray, a need can be defined as a \"potentiality or readiness to respond in a certain way under certain circumstances.\" (Murray, 1938) Murray purposed a list of fundamental human needs. Each need was thought to be associated with \"(1) a specific desire or intention, (2) a particular set of emotions, and (3) specific action tendencies.\" (Larsen & Buss, 2008) Murray believed that human beings had their own hierarchy of needs, unique to each individual. (Larsen & Buss, 2008) It is thought that each individual's various needs exist at different levels of strength.\n\nAccording to Maslow, the concept of need can be defined primarily by an individual's goals. (Larsen & Buss, 2008) Maslow was a firm believer of self-actualization, the process of becoming \"more and more what one idiosyncratically is, becoming everything that one is capable of becoming.\" (Larson & Buss, 2008) Maslow believed that needs were hierarchically organized, with more basic needs found toward the bottom of the hierarchy and the self-actualization need at the top. The needs defined by Murray and Maslow (physiological needs, safety needs, belongingness needs, esteem needs, self-actualization needs) seem to correspond with the vital needs encompassed in the concept of dependency need.\n\nDependency needs can sometimes be associated with Attachment Theory. Attachment can be defined as a \"deep and enduring emotional bond that connects one person to another across time and space.\" (Ainsworth, 1973; Bowlby, 1969) Attachment is thought to first occur between an infant and his or her primary caregiver. (Hetherington & Parke, 1999) Harry Harlow and his research in developmental psychology showed that attachment between infant and caregiver is vitally important to the psychological development of the infant and requires physical contact with a warm and responsive mother. (Larsen & Buss, 2008) These early experiences and reactions of the infants to the primary caregivers become working models for later adult relationships. (Larsen & Buss, 2008) Therefore, conclusions can be visibly drawn linking attachment and dependency need; vital, infantile needs for mothering, love, affection, shelter, protection, security, food, and warmth (dependency need) can stem from the type of interaction between that of an infant and his or her primary caregiver. Three different types of attachment styles, securely attached, avoidantly attached, and ambivalent attached, can result from varying levels and styles of care a primary caregiver provides an infant.\n\nDependency needs can be categorized into biological needs and social needs.\n\nBiological needs are basic survival needs, such as protection from harm, consumption of food, and regulation of body temperature; they are mechanisms used to promote and maintain proper body functioning.\n\nSocial needs are \"acquired psychological processes that activate emotional responses to particular need-relevant incentives.\" (Reeve, 2009) Need for eye contact, expression of positive emotions by caretakers or loved ones, and cuddling—anything that fosters a sense of emotional security—can be defined as a social need. There are four basic social needs: power, achievement, intimacy, and affiliation. People who have power needs look to gain dominance, achieve high statuses, and/or achieve high positions in their occupations, households, or social groups or organizations. These people look for leadership roles, and are usually happy and content when they are in control. People who have achievement needs are willing to seek out and accomplish tasks. The strive for achievement can develop strongly in children when influenced by their parents. Intimacy needs are linked to affiliation needs. Intimacy needs can be sought out and met when in close, personal relationships with others. Fulfillment of intimacy needs can help decrease an individual's chance of developing onset of depression, as well as help reduce an individual's fears of being rejected. Affiliation needs are people's needs to feel a sense of involvement and \"belonging\" within a social group; affiliation needs have to do more with the acceptance of behavior. It is human nature for people to want to be liked by others and get approval from them. It is also innate for people to want to maintain healthy and positive relationships with others around them.\n\nThere have been numerous theorists who have done research in relation to dependency need.\n\nRobert Bornstein, a professor of psychology, researched certain levels of dependency needs, as well as personality disorders related to dependency needs, including dependent personality disorder (DPD) and histrionic personality disorder (HPD). Henry Murray's publication, Explorations in Personality (1938) describes differences and similarities between types of dependency needs.\n\nAs mentioned previously, Abraham Maslow was a key contributor to the establishment of a dependency need theory. His need theory, Maslow's Hierarchy of Needs, is thought to help a person achieve the unsatisfied needs of one's self. In his hierarchy, he outlined five needs crucial to human development and happiness across the lifespan; they are thought to occur in stages. The five stages include, physiological needs, safety needs, social needs, self-esteem needs, and self-actualization. Physiological needs are needs that everyone has to have in order to survive, such as air, food, water, and sleep. After a person has attained these physiological needs, he or she then focuses his or her attention to safety needs. Safety needs help an individual feel secure in order to make him or her feel safe, physically and emotionally. An example of this can be seen in people's choices of where they choose to live and work, and attaining medical insurance. After safety needs are met, social needs are the focus of attention. Social needs have to do with interactions with others (friends, family, romantic partners) and receiving love. After these needs have been met, one's self-esteem needs begin to arise. Esteem needs can be both internal and external. Having and achieving self-respect, receiving attention, and accomplishing achievements are examples of self-esteem needs. After one succeeds in the area of self-esteem, self-actualization needs are to be met. One's full potential as a person, his or her \"self-actualization\", helps him or her keep developing as a person throughout the lifespan. (Maslow, 2002)\n\nAnother key contributor to the establishment of dependency need theory was Sigmund Freud's theory of psychosexual development.\n\nFreud's theory of psychosexual development\n\nSigmund Freud came up with a five-stage theory that stated human beings are born with sexual energy; this energy was thought to develop in five stages (oral, anal, phallic, latency, and genital stages). The first stage, the oral stage, occurs from birth to 2 years of age. The key component of the oral stage is the child's fascination with his or her mouth, more specifically, putting items into his or her mouth, breast feeding, etc. The child is thought to get great pleasure from such objects being placed in his or her mouth. The second stage, the anal stage, occurs from 18 months of age to 3 years. In this stage, the child's main focus is on his or her anus, and the experience of toilet training is thought to be quite pleasurable. The third phase, the phallic stage, occurs from 3 years of age to 6 years of age. In this stage, the main focus of the child is on discovering his or her genital region. Children become more aware of their own bodies, the bodies of other children, as well as their parents' bodies. Children also become aware of anatomical sex differences between male and female genitalia. Freud believed during this stage, that boys had the idea that they needed to compete with their fathers in order to possess their mothers. He also believed that girls felt penis envy towards males, and therefore blamed their mothers for not having a penis; the girls then competed for psychosexual possession of their fathers. The fourth stage, the latency stage, occurs from 6 years of age until the child reaches puberty. In this stage, children are thought to be \"latent\" of sexual energies, however sexual urges still remain. During this time, children play with the same sex friends. The final stage in psychosexual development is the genital stage, occurring from puberty through the rest of adulthood. In this stage, individuals focus on detaching from their parents, doing their own thing and not relying as much on their parents. In this stage, genitalia is the main focus and sexual energy and urges are normal. The person's concern is now focused on mature, adult friendships and family relationships, as well as intimate relationships, and adult responsibilities. (Basic, 2011)\n\nThere are differences and similarities worth noting when it comes to biological needs and social needs. Biological needs are required for survival in everyday life, whereas social needs are acquired and learned. Similarities between the two include the relevance that an individual requires both types of needs in one's life in order to live happy, healthy lives in which he or she is able to thrive and succeed; if an individual's needs are not met, he or she may become sad and/or depressed.\n\nA person's vital needs for mothering, love, affection, shelter, protection, security, food, and warmth are ever so important to an individual. If these dependency needs are not met, particularly when an individual is younger, emotional, psychological, as well as physical problems may result down the road. Also, mental retardation or even death can result in extreme cases of neglect of dependency needs (usually in cases when biological needs are neglected). In general, depression, sadness, and loneliness are likely results if dependency needs are not met no matter what age an individual is.\n\nMany experiments relating the importance of dependency need have been conducted over the years; here, three influential experiments are outlined.\n\nWhen this study was conducted in 1957, Harry Harlow was questioning current theories about dependency and love. At this time, Harlow and his team stated that love began developing as a feeding bond between an infant and its mother; this notion applied to family members, as well. It was also thought that humans, along with other social animals, lived in organized societies in order to regularize sexual contact. Harlow, being fascinated with the concept of love and nurturing, worked with monkeys to test these theories. (Berger, 2005)\n\nIn Harlow's monkey experiment, newborn monkeys were separated from their mothers almost immediately after birth. They were then raised with substitute \"mothers\" made of either (1) wire or (2) wood covered with a soft cloth. In one of the experiments, both the wire and wood mothers were presented to the infant monkey in the same cage, and only one wore a nipple, which the infant was able to nurse from. Some of the monkeys nursed from the wire mother, and others from the cloth mother. According to Berger, Harlow found that even when the wire mother was the source of food the infant monkey spent more of its time with the cloth surrogate. Harlow also found that the cloth mother provided not only food to the infant monkeys, but also was able to provide comfort and security for them. The interpretation made by Harlow about this was that the liking for the cloth surrogate mother showed the importance of affection, emotion, nurturance, and dependency in mother child relationships.\n\nThe phenomenon of infant dependency need was first noticed in René Spitz's orphan study. It was during this study that researchers learned of the higher mortality rates for infants maintained in orphanages. When the obvious factors such as inadequate nutrition, contagious diseases, etc., were ruled out, researchers discovered that mortality rates could be greatly ameliorated by having the nurses in charge of the infants in the orphanages cuddle them in a way that approximated the amount of cuddling infants would normally receive from their own parents.\n\nSpitz's orphan study focused on two groups of children, starting when the children were infants, and continued all the way until they reached 12 years of age. The first group of children were raised in an orphanage; the children in this group only received minimal care, let alone any special, one-on-one time with a caregiver. In the second group, each infant received individual care from various women caregivers serving a prison sentence; these caregivers were with the children for the first year of their lives. When the children in each group turned two years old, dramatic differences occurred between them.\n\nResults from the experiment showed drastic differences. It was found that children raised in the orphanage, whom had received minimal care, had less developmental progress than children raised in the prison. Only 26 kids could walk, and only a few could talk. Some of the orphanage kids had signs of mental retardation and were psychologically and socially underdeveloped for their age. Whereas in the prison group, most all the children had reached the point of full development for their age division.\n\nDefined previously, dependency needs are \"The vital, originally infantile needs for mothering, love, affection, shelter, protection, security, food, and warmth\" (Segen, 1992). When those needs are not met psychological, emotional, physical and attachment problems can arise. This was shown in the Spitz orphan study. (Shepard, 2013)\n\n\"Attachment theory states that a child's first relationship is a love relationship that will have profound, long-lasting effects on an individual's subsequent development.\" (Colin, 1991) The closeness of the child to the person providing protection and a sense of security will becomes this figure to the child. This figure lays the foundation for the child to be able to form other secure relationships in the future. There are four main types of attachment, including: secure, anxious/avoidant, anxious/ambivalent and disorganized/disoriented.\n\nIn securely attached babies, the baby's attachment figure is an effective secure base from them. They are able to explore the world and can even handle brief separation in an environment unfamiliar to them. They are secure in the fact that they know the attachment figure will return and that there is little to no anxiety experienced. (Colin, 1991)\n\nIn anxious/avoidant attachment, the baby is anxious about the attachment figure's (typically a parent) responsiveness. The baby then develops a defensive strategy for managing his or her own anxiety. The baby experiencing this situation has very little, to no confidence in relation to being responded to; so when the baby seeks care, the child expects to be rejected. For example, if the baby is hungry, he or she will not tell anyone/cry because he or she doesn't expect anyone to meet this need. (Colin, 1991)\n\nIn anxious/ambivalent attachment, signs of anxiety, anger, and mixed feelings about the attachment figure are present in the baby; this is especially true after brief separations of the baby from the attachment figure, occurring in unfamiliar environments. (Colin, 1991)\n\nIn disorganized/disoriented babies, the baby does not have a consistent strategy for managing separation and reunion with his or her attachment figure. Some of the babies show to be depressed, demonstrate avoidant behavior, express anger, and/or show disturbing behaviors. Infants in this category have typically been maltreated. (Colin, 1991)\n\nThe research currently being done in the last few years in relation to dependency need relates largely to attachment theory. Most studies of attachment theory focus on how attachment relates to other aspects in the individual's life; for example, how an individual's attachment style (developed early on) affects his or her display of emotions after a break-up, or how an individual's attachment style affects his or her ability to make new friends in college.\n\nA study done in 2010 by Stephanie Parade, Esther Leerke, and Nayena Blankson looked at the correlation between an individual's attachment to his or her parents and how well he or she is able to make friends; this study specifically focused on college-age individuals. The study also looked at social anxiety as the mediator. The results of this study show that the more securely attached a person is to his or her parents; the easier it is for him or her to make friends. These findings are consistent to Bowlby's proposition. The authors also found that social anxiety only showed up in the minority. The participants in the minority group had less social anxiety when they had a secure relationship with their parents, which in turn helped them be more sociable overall. (Parade, Leerke, & Blankson, 2010)\n\nAnother current study done by Gnilka, Ashby, and Noble, looked at \"adult attachment styles and the psychological outcomes, like hopelessness and life satisfaction, using maladaptive and adaptive perfectionism as the mediators.\" (Gnika, Ashby, & Noble, 2013) The results show that having high levels of avoidant or anxiety attachment will cause low maladaptive perfectionism, which overall makes one more likely to have low life satisfaction and high levels of hopelessness. These findings can benefit counselors when working with patients who have problems with perfectionism.\n\nNot a whole lot of research is going on in this field today. One article, done in 1995, looked at female suicide and dependency needs. The focus of the article was on the dependency needs of female adolescent suicide attempters and non-attempters. It also looked at other social network and intimate relationships for both suicide attempters and non-attempters. The authors hypothesized that dependency needs would be higher in suicide attempters. The results displayed a similarity in both groups, although, the sample size was too small to generalize to attempters. The article suggests future research on the range of dependency and the effect dependency might have on suicide. (Beettridge & Favreau, 1995)\n\nIn the last decade, research on the topic of dependency need has been declining. The theory of dependency need has largely been incorporated into attachment theory.\n\nVital, originally infantile needs for mothering, love, affection, shelter, protection, security, food, and warmth, more specifically known as \"dependency needs\", change and decrease with age and maturity. As an individual gets older, these needs usually decrease in strength and can normally be met individually, or fulfilled by relationships among partners, family members or friends.\n\nBy looking at historical research pioneered by Murray, Maslow, Freud, and Harlow, as well as more recent concepts developed by Spritz, Colin, Parade et al., Beettridge & Favreau, one can determine that dependency need is an important psychological concept, encompassed in many areas of psychology. Throughout time, the basic needs related to safety, love, affection, and protection have seemed to be a subject of utmost importance. No matter the way one theorizes the concept of dependency need, it is well known that all humans seem to have these basics needs. It is also known that if these needs are somehow not adequately met, the person who has been neglected in this way, will likely suffer deep-seated emotional, psychological, and possibly even physical hardships.\n\nToday, the concept of dependency need is largely interrelated with the concept of attachment theory. Nowadays, a lot of attachment theory studies are interested in seeing if there is a relationship between a person's style of attachment (developed in infancy) and the way in which he or she deals with whatever life throws his or her way (years later); i.e., dealing with emotions after a break-up or making friendships in college. Conclusions can be visibly drawn linking attachment and dependency need; vital, infantile needs for mothering, love, affection, shelter, protection, security, food, and warmth (dependency need) can stem from the type of interaction between that of an infant and his or her primary caregiver. Attachment theory can also go beyond infancy to look at how early interactions have shaped an individual's personality, and how he or she deals with life situations years and years down the road. Dependency need continues to be a concept that is of great interest to psychologists (more specifically tied into attachment theory) and is ever-present in the everyday lives of all human beings.\n\n"}
{"id": "44879146", "url": "https://en.wikipedia.org/wiki?curid=44879146", "title": "Der Alte Schwede", "text": "Der Alte Schwede\n\nDer Alte Schwede or Alter Schwede (meaning (The) Old Swede in German) is a glacial erratic, found during dredging of the river Elbe near Hamburg in 1999, at a depth of 15 m. The rock has a circumference of 19.7 m, a height of 4.5 m and weighs 217 tons. During the Elster maximum glaciation of the ice age 400 000 years ago, it was carried from Småland to the site where it was found. In June 2000, it was given its current name. It is Germany's oldest glacial erratic.\n"}
{"id": "8640320", "url": "https://en.wikipedia.org/wiki?curid=8640320", "title": "Dioptric correction", "text": "Dioptric correction\n\nDioptric correction is the expression for the adjustment of the optical instrument to the varying visual acuity of a person's eyes. It is the adjustment of one lens to provide compatible focus when the viewer's eyes have differing visual capabilities. One result is less strain on the eyes that allow for optimal viewing and depth and contrast focusing when composing a photograph or viewing an item through a device made of lenses or lens elements.\n"}
{"id": "6049076", "url": "https://en.wikipedia.org/wiki?curid=6049076", "title": "Downward harmonization", "text": "Downward harmonization\n\nDownward harmonization is an econo-political term describing the act of adapting the trade laws of a country with an established economy \"downward\" to the trade laws of the country with a developing economy. This \"harmonizing\" may affect labor laws, human rights laws, minimum-wage, industry standards, quality control, anti-terrorism, etc.\n\n"}
{"id": "21743661", "url": "https://en.wikipedia.org/wiki?curid=21743661", "title": "Dressed particle", "text": "Dressed particle\n\nIn theoretical physics, the term dressed particle refers to a bare particle together with some excitations of other quantum fields that are physically inseparable from the bare particle. For example, a dressed electron includes the chaotic dynamics of electron-positron pairs and photons surrounding the original electron.\n\nA further noteworthy example is represented by polaritons\n\nIn radiobiology, a dressed particle is a bare particle together with its Debye sphere that neutralizes its electric charge.\nDressed particles are also often called clothed particles.\n\n"}
{"id": "6924337", "url": "https://en.wikipedia.org/wiki?curid=6924337", "title": "Ebullioscopic constant", "text": "Ebullioscopic constant\n\nIn thermodynamics, the ebullioscopic constant, formula_1, relates molality formula_2 to boiling point elevation. It is the ratio of the latter to the former:\n\nformula_3<br>\n\nA formula to compute the ebullioscopic constant is\nformula_6\n\nThrough the procedure called ebullioscopy, a known constant can be used to calculate an unknown molar mass. The term \"ebullioscopy\" comes from the Greek language and means \"boiling measurement.\" This is related to cryoscopy, which determines the same value from the cryoscopic constant (of freezing point depression).\n\nThis property of elevation of boiling point is a colligative property. It means that the property, in this case formula_11, depends on the number of particles dissolved into the solvent and not the nature of those particles.\n\n\n"}
{"id": "24073609", "url": "https://en.wikipedia.org/wiki?curid=24073609", "title": "Effective selfing model", "text": "Effective selfing model\n\nThe effective selfing model is a mathematical model that describes the mating system of a plant population in terms of the degree of self-fertilisation present.\n\nIt was developed in the 1980s by Kermit Ritland, as an alternative to the simplistic mixed mating model. The mixed mating model assumes that every fertilisation event may be classed as either self-fertilisation, or outcrossing with a completely random mate. That is, it assumes that inbreeding is caused solely by self-fertilisation. This assumption is often violated in wild plant populations, where inbreeding may be due to outcrossing between closely related plants. For example, in dense stands, mating often occurs between plants in close proximity; and in plants with short seed dispersal distances, plants are often closely related to their nearest neighbours. When both these criteria are met, plants will tend to be closely related to the near neighbours with which they mate, resulting in significant inbreeding. In such a scenario, the mixed mating model will attribute all inbreeding to self-fertilisation, and therefore overestimate the extent of self-fertilisation occurring. The effective selfing model takes into account the potential for inbreeding to occur as a result of outcrossing between closely related plants, by considering the extent of kinship between mates.\n\nUltimately, it is not possible to tease apart the two potential causes of inbreeding, and attributed the observed inbreeding to one cause or the other. Therefore, just as with the mixed mating model, in the effective selfing model there is only one parameter to be estimated. However this parameter, termed the effective selfing rate, is often a more accurate measure of the proportion of self-fertilisation than the corresponding parameter in the mixed mating model.\n"}
{"id": "9247154", "url": "https://en.wikipedia.org/wiki?curid=9247154", "title": "Franco-British Nuclear Forum", "text": "Franco-British Nuclear Forum\n\nThe first meeting of the Franco–British Nuclear Forum was held in Paris in November 2007, chaired by the Minister for Energy and the French Industry Minister. The working groups are focusing on specific areas for collaboration. A follow-up meeting on the issue in London was planned for March 2008, but did not take place.\n\n\n"}
{"id": "659899", "url": "https://en.wikipedia.org/wiki?curid=659899", "title": "Gravimetric analysis", "text": "Gravimetric analysis\n\nGravimetric analysis describes a set of methods used in analytical chemistry for the quantitative determination of an analyte (the ion being analyzed) based on its mass. The principle behind this type of analysis is that once an ion's mass has been determined as a unique compound, that known measurement can then be used to determine the same analyte's mass in a mixture, as long as the relative quantities of the other constituents are known.\n\nThe four main types of this method of analysis are \"precipitation\", \"volitilization\", \"electro-analytical\" and \"miscellaneous physical method\". The methods involve changing the phase of the analyte to separate it in its pure form from the original mixture and are quantitative measurements.\n\nThe precipitation method is the one used for the determination of the amount of calcium in water. Using this method, an excess of oxalic acid, HCO, is added to a measured, known volume of water. By adding a reagent, here ammonia, the calcium will precipitate as calcium oxalate. The proper reagent, when added to aqueous solution, will produce highly insoluble precipitates from the positive and negative ions that would otherwise be soluble with their counterparts (equation 1).\n\nThe reaction is:\n\nFormation of calcium oxalate:\n\nCa + CO → CaCO\n\nThe precipitate is collected, dried and ignited to high (red) heat which converts it entirely to calcium oxide.\n\nThe reaction is pure calcium oxide formed\n\nCaCO → CaO + CO+ CO\n\nThe pure precipitate is cooled, then measured by weighing, and the difference in weights before and after reveals the mass of analyte lost, in this case calcium oxide. That number can then be used to calculate the amount, or the percent concentration, of it in the original mix.\n\nIn volatilization methods, removal of the analyte involves separation by heating or chemically decomposing a volatile sample at a suitable temperature. In other words, thermal or chemical energy is used to precipitate a volatile species. For example, the water content of a compound can be determined by vaporizing the water using thermal energy (heat). Heat can also be used, if oxygen is present, for combustion to isolate the suspect species and obtain the desired results.\n\nThe two most common gravimetric methods using volatilization are those for water and carbon dioxide. An example of this method is the isolation of sodium hydrogen bicarbonate (the main ingredient in most antacid tablets) from a mixture of carbonate and bicarbonate. The total amount of this analyte, in whatever form, is obtained by addition of an excess of dilute sulfuric acid to the analyte in solution.\n\nIn this reaction, nitrogen gas is introduced through a tube into the flask which contains the solution. As it passes through, it gently bubbles. The gas then exits, first passing a drying agent (here CaSO, the common desiccant \"Drierite\"). It then passes a mixture of the drying agent \"and\" sodium hydroxide which lays on asbestos or \"Ascarite II\", a non-fibrous silicate containing sodium hydroxide The mass of the carbon dioxide is obtained by measuring the increase in mass of this absorbent. This is performed by measuring the difference in weight of the tube in which the ascarite contained before and after the procedure.\n\nThe calcium sulfate (CaSO) in the tube retains carbon dioxide selectively as it's heated, and thereby, removed from the solution. The drying agent absorbs any aerosolized water and/or water vapor (reaction 3.). The mix of the drying agent and NaOH absorbs the CO and any water that may have been produced as a result of the absorption of the NaOH (reaction 4.).\n\nThe reactions are:\n\nReaction 3 - absorption of water\n\nNaHCO + HSO → CO) + HO + NaHSO\n\nReaction 4. Absorption of CO and residual water\n\nCO + 2 NaOH NaCO + HO\n\nVolatilization methods can be either \"direct\" or \"indirect\". Water eliminated in a quantitative manner from many inorganic substances by ignition is an example of a direct determination. It is collected on a solid desiccant and its mass determined by the gain in mass of the desiccant.\n\nAnother direct volatilization method involves carbonates which generally decompose to release carbon dioxide when acids are used. Because carbon dioxide is easily evolved when heat is applied, its mass is directly established by the measured increase in the mass of the absorbent solid used.\n\nDetermination of the amount of water by measuring the loss in mass of the sample during heating is an example of an indirect method. It is well known that changes in mass occur due to decomposition of many substances when heat is applied, regardless of the presence or absence of water. Because one must make the assumption that water was the only component lost, this method is less satisfactory than direct methods.\n\nThis often fault and misleading assumption has proven to be wrong on more than a few occasions. There are many substances other than water loss that can lead to loss of mass with the addition of heat, as well as a number of other factors that may contribute to it. The widened margin of error created by this all-too-often false assumption is not one to be lightly disregarded as the consequences could be far-reaching.\n\nNevertheless, the indirect method, although less reliable than direct, is still widely used in commerce. For example, it's used to measure the moisture content of cereals, where a number of imprecise and inaccurate instruments are available for this purpose.\n\n\nA chunk of ore is to be analyzed for sulfur content. It is treated with concentrated nitric acid and potassium chlorate to convert all of the sulfur to sulfate (SO). The nitrate and chlorate are removed by treating the solution with concentrated HCl. The sulfate is precipitated with barium (Ba) and weighed as BaSO.\n\nGravimetric analysis, if methods are followed carefully, provides for exceedingly precise analysis. In fact, gravimetric analysis was used to determine the atomic masses of many elements to six figure accuracy. Gravimetry provides very little room for instrumental error and does not require a series of standards for calculation of an unknown. Also, methods often do not require expensive equipment. Gravimetric analysis, due to its high degree of accuracy, when performed correctly, can also be used to calibrate other instruments in lieu of reference standards.\n\nGravimetric analysis usually only provides for the analysis of a single element, or a limited group of elements, at a time. Comparing modern dynamic flash combustion coupled with gas chromatography with traditional combustion analysis will show that the former is both faster and allows for simultaneous determination of multiple elements while traditional determination allowed only for the determination of carbon and hydrogen. Methods are often convoluted and a slight mis-step in a procedure can often mean disaster for the analysis (colloid formation in precipitation gravimetry, for example). Compare this with hardy methods such as spectrophotometry and one will find that analysis by these methods is much more efficient.\n\nAfter appropriate dissolution of the sample the following steps should be followed for successful gravimetric procedure:\n\n1. Preparation of the Solution: This may involve several steps including adjustment of the pH of the solution in order for the precipitate to occur quantitatively and get a precipitate of desired properties, removing interferences, adjusting the volume of the sample to suit the amount of precipitating agent to be added.\n2. Precipitation: This requires addition of a precipitating agent solution to the sample solution. Upon addition of the first drops of the precipitating agent, supersaturation occurs, then nucleation starts to occur where every few molecules of precipitate aggregate together forming a nucleous. At this point, addition of extra precipitating agent will either form new nuclei or will build up on existing nuclei to give a precipitate. This can be predicted by Von Weimarn ratio where, according to this relation the particle size is inversely proportional to a quantity called the relative supersaturation where\nRelative supersaturation = (Q – S)/S\nThe Q is the concentration of reactants before precipitation, S is the solubility of precipitate in the medium from which it is being precipitated. Therefore, to get particle growth instead of further nucleation we must make the relative supersaturation ratio as small as possible. The optimum conditions for precipitation which make the supersaturation low are:\na. Precipitation using dilute solutions to decrease Q\nb. Slow addition of precipitating agent to keep Q as low as possible\nc. Stirring the solution during addition of precipitating agent to avoid concentration sites and keep Q low\nd. Increase solubility by precipitation from hot solution\ne. Adjust the pH to increase S, but not too much increase np as we do not want to lose precipitate by dissolution\nf. Usually add a little excess of the precipitating agent for quantitative precipitation and check for completeness of the precipitation\n3. Digestion of the precipitate: The precipitate is left hot (below boiling) for 30 min to one hour for the particles to be digested. Digestion involves dissolution of small particles and reprecipitation on larger ones resulting in particle growth and better precipitate characteristics. This process is called Ostwald ripening. An important advantage of digestion is observed for colloidal precipitates where large amounts of adsorbed ions cover the huge area of the precipitate. Digestion forces the small colloidal particles to agglomerate which decreases their surface area and thus adsorption. You should know that adsorption is a major problem in gravimetry in case of colloidal precipitate since a precipitate tends to adsorb its own ions present in excess, Therefore, forming what is called a primary ion layer which attracts ions from solution forming a secondary or counter ion layer. Individual particles repel each other keeping the colloidal properties of the precipitate. Particle coagulation can be forced by either digestion or addition of a high concentration of a diverse ions strong electrolytic solution in order to shield the charges on colloidal particles and force agglomeration. Usually, coagulated particles return to the colloidal state if washed with water, a process called peptization.\n\n4. Washing and Filtering the Precipitate: It is crucial to wash the precipitate thoroughly to remove all adsorbed species that would add to the weight of the precipitate. One should be careful nor to use too much water since part of the precipitate may be lost. Also, in case of colloidal precipitates we should not use water as a washing solution since peptization would occur. In such situations dilute nitric acid, ammonium nitrate, or dilute acetic acid may be used. Usually, it is a good practice to check for the presence of precipitating agent in the filtrate of the final washing solution. The presence of precipitating agent means that extra washing is required. Filtration should be done in appropriate sized Gooch or ignition filter paper.\n5. Drying and Ignition: The purpose of drying (heating at about 120-150 oC in an oven) or ignition in a muffle furnace at temperatures ranging from 600-1200 oC is to get a material with exactly known chemical structure so that the amount of analyte can be accurately determined.\n\n6. Precipitation from Homogeneous Solution: To make Q minimum we can, in some situations, generate the precipitating agent in the precipitation medium rather than adding it. For example, to precipitate iron as the hydroxide, we dissolve urea in the sample. Heating of the solution generates hydroxide ions from the hydrolysis of urea. Hydroxide ions are generated at all points in solution and thus there are no sites of concentration. We can also adjust the rate of urea hydrolysis and thus control the hydroxide generation rate. This type of procedure can be very advantageous in case of colloidal precipitates.\n\nAs expected from previous information, diverse ions have a screening effect on dissociated ions which leads to extra dissociation. Solubility will show a clear increase in presence of diverse ions as the solubility product will increase. Look at the following example:\n\nFind the solubility of AgCl (K = 1.0 x 10) in 0.1 M NaNO. The activity coefficients for silver and chloride are 0.75 and 0.76, respectively.\n\nWe can no longer use the thermodynamic equilibrium constant (i.e. in absence of diverse ions) and we have to consider the concentration equilibrium constant or use activities instead of concentration if we use Kth:\n\nWe have calculated the solubility of AgCl in pure water to be 1.0 x 10 M, if we compare this value to that obtained in presence of diverse ions we see % increase in solubility = {(1.3 x 10 – 1.0 x 10) / 1.0 x 10} x 100 = 30%\nTherefore, once again we have an evidence for an increase in dissociation or a shift of equilibrium to right in presence of diverse ions.\n\n"}
{"id": "30026014", "url": "https://en.wikipedia.org/wiki?curid=30026014", "title": "Itabirite", "text": "Itabirite\n\nItabirite, also known as banded-quartz hematite and hematite schist, is a laminated, metamorphosed oxide-facies iron formation in which the original chert or jasper bands have been recrystallized into megascopically distinguishable grains of quartz and the iron is present as thin layers of hematite, magnetite, or \"martite\" (pseudomorphs of hematite after magnetite).\n\nThe term was originally applied in Itabirito (Pico de Itabirito), in the state of Minas Gerais and southern part of Belo Horizonte, Brazil, to a high-grade, massive specular hematite ore (66% iron) associated with a schistose rock composed of granular quartz and scaly hematite. The term is now widely used outside Brazil.\n"}
{"id": "6956679", "url": "https://en.wikipedia.org/wiki?curid=6956679", "title": "Kvant (magazine)", "text": "Kvant (magazine)\n\nKvant ( for \"quantum\") is a popular science magazine in physics and mathematics for school students and teachers, issued since 1970 in Soviet Union and continued in Russia. Translation of selected articles from \"Kvant\" had been published in \"Quantum Magazine\" in 1990–2001, which in turn had been translated and published in Greece in 1994–2001. \n\nKvant was started as a joint project of the USSR Academy of Sciences and USSR Academy of Pedagogical Sciences. In Soviet time, it was published by Nauka publisher with circulation about 200,000.\n\nThe idea of the magazine was introduced by Pyotr Kapitsa. Its first chief editors were physicist Isaak Kikoin and mathematician Andrei Kolmogorov. In 1985, its editorial board had 18 Academicians and Corresponding Members of the USSR Academy of Sciences and USSR Academy of Pedagogical Sciences, 14 Doctors of Sciences and 20 Candidates of Science.\n\nAll published issues of \"Kvant\" are freely available online.\n\n\"Quantum Magazine\" was a US-based bimonthly magazine published by the National Science Teachers Association (NSTA) from 1990 to 2001. Some of its articles were translations from \"Kvant\".\n\nIn 1999, American Mathematical Society published translation of selected articles from Kvant on algebra and mathematical analysis as two volumes in the \"Mathematical World\" series. Yet another volume, published in 2002, included translation of selected articles on combinatorics. \n\nThere were two books with selected articles from Kvant published in France.\n\n"}
{"id": "44227211", "url": "https://en.wikipedia.org/wiki?curid=44227211", "title": "Lauge-Hansen classification", "text": "Lauge-Hansen classification\n\nThe Lauge-Hansen classification is a system of categorizing ankle fractures based on the foot position and the force applied.\n\n"}
{"id": "20207005", "url": "https://en.wikipedia.org/wiki?curid=20207005", "title": "List of A1 genes, proteins or receptors", "text": "List of A1 genes, proteins or receptors\n\nThis is a list of genes, proteins or receptors named A1 or Alpha-1 :\n\n\n\n\n"}
{"id": "3595147", "url": "https://en.wikipedia.org/wiki?curid=3595147", "title": "List of ERP software packages", "text": "List of ERP software packages\n\nThis is a list of enterprise resource planning (ERP) software. The first section is devoted to free and open-source software, and the second is for proprietary software.\n\n"}
{"id": "12747265", "url": "https://en.wikipedia.org/wiki?curid=12747265", "title": "List of Virginia state symbols", "text": "List of Virginia state symbols\n\nThis is a list of symbols of the United States Commonwealth of Virginia. The majority of the items in the list are officially recognized symbols created by an act of the Virginia General Assembly and signed into law by the governor. The state nickname, \"The Old Dominion\", is the oldest symbol. However, it is the only symbol that is not official. The other nickname, \"Mother of Presidents\", is also historic, as eight Virginians have served as President of the United States, including four of the first five: George Washington, Thomas Jefferson, James Madison, James Monroe, William Henry Harrison, John Tyler, Zachary Taylor, and Woodrow Wilson. Additionally, Sam Houston, president of the Republic of Texas, Fulwar Skipwith, the president of the Republic of West Florida, and Joseph Jenkins Roberts, the first president of Liberia were from Virginia.\n\nThe state motto and seal have been official since Virginia declared its independence from the Kingdom of Great Britain. Virginia is the only state to have the same plant for state flower and state tree, the Flowering Dogwood. The majority of the symbols were made official in the late 20th century.\n\nThe flag was adopted in 1861 after secession from the United States.\n\nThe Virginia Colony was nicknamed \"The Old Dominion\" by King Charles II for its perceived loyalty to the English monarchy during the English Civil War.\n\nPictures of Virginia license plates throughout the years can be found here.\n\nIn 1940, Virginia made \"Carry Me Back to Old Virginny\" the state song, but it was retired in 1997 and reclassified as the state song emeritus.\n\n"}
{"id": "11570854", "url": "https://en.wikipedia.org/wiki?curid=11570854", "title": "List of flags of Lithuania", "text": "List of flags of Lithuania\n\nThe following is a list of flags of Lithuania.\n\nEach county of Lithuania has adopted a flag, each of them conforming to a pattern: a blue rectangle, with ten instances of the Cross of Vytis appearing in gold, acts as a fringe to the central feature of the flag, which is chosen by the county itself. Most of the central designs were adapted from the counties' coat of arms.\n\n"}
{"id": "21119792", "url": "https://en.wikipedia.org/wiki?curid=21119792", "title": "List of inactive volcanoes in the Philippines", "text": "List of inactive volcanoes in the Philippines\n\nThis is a List of inactive volcanoes in the Philippines. Volcanoes with no record of eruptions are considered as extinct or inactive. Their physical form since their last activity has been altered by agents of weathering and erosion with the formation of deep and long gullies. Inactive does not necessarily indicate the volcano will not erupt again. Mount Pinatubo had no recorded historical eruption before its cataclysmic 1991 eruption.\n\nThe Philippine Institute of Volcanology and Seismology (PHIVOLCS) currently lists 355 volcanoes in the Philippines as inactive. The PHIVOLCS listing is the basis of this list, but with additional information, some were reclassified in the active list or the potentially active list. Volcanoes with solfataric or fumarolic activity indicating active magma supply such as Pocdol Mountains, are placed in the List of potentially active volcanoes in the Philippines. This list shows 339 inactive volcanoes in the Philippines, listed by volcanic region.\n\n \n\n"}
{"id": "54390921", "url": "https://en.wikipedia.org/wiki?curid=54390921", "title": "List of people in blockchain technology", "text": "List of people in blockchain technology\n\nThis is a list of people in blockchain technology, people who do work in the area of Blockchain and Cryptocurrency, in particular researchers, business people and authors.\n\nSome persons notable as programmers are included here because they work in research as well as programming. A few of these people pre-date the invention of this technology; they are now regarded as people in blockchain technology because their work can be seen as leading to the invention of this technology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "49681713", "url": "https://en.wikipedia.org/wiki?curid=49681713", "title": "List of rectores magnifici of Leiden University", "text": "List of rectores magnifici of Leiden University\n\nThis is a list of chancellors (\"rectores magnifici\") of Leiden University, as from 1575. Three Nobel laureates are among these chancellors: Hendrik Lorentz, Heike Kamerlingh Onnes and Willem Einthoven.\n\n\n\n"}
{"id": "37678417", "url": "https://en.wikipedia.org/wiki?curid=37678417", "title": "MACS J0647+7015", "text": "MACS J0647+7015\n\nMACS J0647.7+7015 is a galaxy cluster with a redshift \"z\" = 0.592, located at J2000.0 right ascension declination . It lies between the Big Dipper and Little Dipper in the constellation Camelopardalis. It is part of a sample of 12 extreme galaxy clusters at \"z\" > 0.5 discovered by the MAssive Cluster Survey (MACS).\n\nDuring 2012 the galaxy cluster was announced as gravitationally lensing the most distant galaxy (MACS0647-JD), then ever imaged (\"z\" = 11).\n"}
{"id": "1982496", "url": "https://en.wikipedia.org/wiki?curid=1982496", "title": "Nuclear force", "text": "Nuclear force\n\nThe nuclear force (or nucleon–nucleon interaction or residual strong force) is a force that acts between the protons and neutrons of atoms. Neutrons and protons, both nucleons, are affected by the nuclear force almost identically. Since protons have charge +1 \"e\", they experience an electric force that tends to push them apart, but at short range the attractive nuclear force is strong enough to overcome the electromagnetic force. The nuclear force binds nucleons into atomic nuclei.\n\nThe nuclear force is powerfully attractive between nucleons at distances of about 1 femtometre (fm, or 1.0 × 10 metres), but it rapidly decreases to insignificance at distances beyond about 2.5 fm. At distances less than 0.7 fm, the nuclear force becomes repulsive. This repulsive component is responsible for the physical size of nuclei, since the nucleons can come no closer than the force allows. By comparison, the size of an atom, measured in angstroms (Å, or 1.0 × 10 m), is five orders of magnitude larger. The nuclear force is not simple, however, since it depends on the nucleon spins, has a tensor component, and may depend on the relative momentum of the nucleons. The strong nuclear force is one of the fundamental forces of nature.\n\nThe nuclear force plays an essential role in storing energy that is used in nuclear power and nuclear weapons. Work (energy) is required to bring charged protons together against their electric repulsion. This energy is stored when the protons and neutrons are bound together by the nuclear force to form a nucleus. The mass of a nucleus is less than the sum total of the individual masses of the protons and neutrons. The difference in masses is known as the mass defect, which can be expressed as an energy equivalent. Energy is released when a heavy nucleus breaks apart into two or more lighter nuclei. This energy is the electromagnetic potential energy that is released when the nuclear force no longer holds the charged nuclear fragments together.\n\nA quantitative description of the nuclear force relies on equations that are partly empirical. These equations model the internucleon potential energies, or potentials. (Generally, forces within a system of particles can be more simply modeled by describing the system's potential energy; the negative gradient of a potential is equal to the vector force.) The constants for the equations are phenomenological, that is, determined by fitting the equations to experimental data. The internucleon potentials attempt to describe the properties of nucleon–nucleon interaction. Once determined, any given potential can be used in, e.g., the Schrödinger equation to determine the quantum mechanical properties of the nucleon system.\n\nThe discovery of the neutron in 1932 revealed that atomic nuclei were made of protons and neutrons, held together by an attractive force. By 1935 the nuclear force was conceived to be transmitted by particles called mesons. This theoretical development included a description of the Yukawa potential, an early example of a nuclear potential. Mesons, predicted by theory, were discovered experimentally in 1947. By the 1970s, the quark model had been developed, by which the mesons and nucleons were viewed as composed of quarks and gluons. By this new model, the nuclear force, resulting from the exchange of mesons between neighboring nucleons, is a residual effect of the strong force.\n\nWhile the nuclear force is usually associated with nucleons, more generally this force is felt between hadrons, or particles composed of quarks. At small separations between nucleons (less than ~ 0.7 fm between their centers, depending upon spin alignment) the force becomes repulsive, which keeps the nucleons at a certain average separation, even if they are of different types. This repulsion arises from the Pauli exclusion force for identical nucleons (such as two neutrons or two protons). A Pauli exclusion force also occurs between quarks of the same type within nucleons, when the nucleons are different (a proton and a neutron, for example).\n\nAt distances larger than 0.7 fm the force becomes attractive between spin-aligned nucleons, becoming maximal at a center–center distance of about 0.9 fm. Beyond this distance the force drops exponentially, until beyond about 2.0 fm separation, the force is negligible. Nucleons have a radius of about 0.8 fm.\n\nAt short distances (less than 1.7 fm or so), the attractive nuclear force is stronger than the repulsive Coulomb force between protons; it thus overcomes the repulsion of protons within the nucleus. However, the Coulomb force between protons has a much greater range as it varies as the inverse square of the charge separation, and Coulomb repulsion thus becomes the only significant force between protons when their separation exceeds about 2 to 2.5 fm.\n\nThe nuclear force has a spin-dependent component. The force is stronger for particles with their spins aligned than for those with their spins anti-aligned. If two particles are the same, such as two neutrons or two protons, the force is not enough to bind the particles, since the spin vectors of two particles of the same type must point in opposite directions when the particles are near each other and are (save for spin) in the same quantum state. This requirement for fermions stems from the Pauli exclusion principle. For fermion particles of different types, such as a proton and neutron, particles may be close to each other and have aligned spins without violating the Pauli exclusion principle, and the nuclear force may bind them (in this case, into a deuteron), since the nuclear force is much stronger for spin-aligned particles. But if the particles' spins are anti-aligned the nuclear force is too weak to bind them, even if they are of different types.\n\nThe nuclear force also has a tensor component which depends on the interaction between the nucleon spins and the angular momentum of the nucleons, leading to deformation from a simple spherical shape.\n\nTo disassemble a nucleus into unbound protons and neutrons requires work against the nuclear force. Conversely, energy is released when a nucleus is created from free nucleons or other nuclei: the nuclear binding energy. Because of mass–energy equivalence (i.e. Einstein's famous formula ), releasing this energy causes the mass of the nucleus to be lower than the total mass of the individual nucleons, leading to the so-called \"mass defect\".\n\nThe nuclear force is nearly independent of whether the nucleons are neutrons or protons. This property is called \"charge independence\". The force depends on whether the spins of the nucleons are parallel or antiparallel, as it has a non-central or \"tensor\" component. This part of the force does not conserve orbital angular momentum, which under the action of central forces is conserved.\n\nThe symmetry resulting in the strong force, proposed by Werner Heisenberg, is that protons and neutrons are identical in every respect, other than their charge. This is not completely true, because neutrons are a tiny bit heavier, but it is an approximate symmetry. Protons and neutrons are therefore viewed as the same particle, but with different isospin quantum numbers; conventionally, the proton is \"isospin up,\" while the neutron is \"isospin down.\" The strong force is invariant under SU(2) isospin transformations, just as other interactions between particles are invariant under SU(2) transformations of intrinsic spin. In other words, both isospin and intrinsic spin transformations are isomorphic to the SU(2) symmetry group. There are only strong attractions when the total isospin of the set of interacting particles is 0, which is confirmed by experiment.\n\nOur understanding of the nuclear force is obtained by scattering experiments and the binding energy of light nuclei.\nThe nuclear force occurs by the exchange of virtual light mesons, such as the virtual pions, as well as two types of virtual mesons with spin (vector mesons), the rho mesons and the omega mesons. The vector mesons account for the spin-dependence of the nuclear force in this \"virtual meson\" picture.\n\nThe nuclear force is distinct from what historically was known as the weak nuclear force. The weak interaction is one of the four fundamental interactions, and plays a role in such processes as beta decay. The weak force plays no role in the interaction of nucleons, though it is responsible for the decay of neutrons to protons and vice versa.\n\nThe nuclear force has been at the heart of nuclear physics ever since the field was born in 1932 with the discovery of the neutron by James Chadwick. The traditional goal of nuclear physics is to understand the properties of atomic nuclei in terms of the 'bare' interaction between pairs of nucleons, or nucleon–nucleon forces (NN forces).\n\nWithin months after the discovery of the neutron, Werner Heisenberg and Dmitri Ivanenko had proposed proton–neutron models for the nucleus. Heisenberg approached the description of protons and neutrons in the nucleus through quantum mechanics, an approach that was not at all obvious at the time. Heisenberg's theory for protons and neutrons in the nucleus was a \"major step toward understanding the nucleus as a quantum mechanical system.\" Heisenberg introduced the first theory of nuclear exchange forces that bind the nucleons. He considered protons and neutrons to be different quantum states of the same particle, i.e., nucleons distinguished by the value of their nuclear isospin quantum numbers.\n\nOne of the earliest models for the nucleus was the liquid drop model developed in the 1930s. One property of nuclei is that the average binding energy per nucleon is approximately the same for all stable nuclei, which is similar to a liquid drop. The liquid drop model treated the nucleus as a drop of incompressible nuclear fluid, with nucleons behaving like molecules in a liquid. The model was first proposed by George Gamow and then developed by Niels Bohr, Werner Heisenberg and Carl Friedrich von Weizsäcker. This crude model did not explain all the properties of the nucleus, but it did explain the spherical shape of most nuclei. The model also gave good predictions for the nuclear binding energy of nuclei.\n\nIn 1934, Hideki Yukawa made the earliest attempt to explain the nature of the nuclear force. According to his theory, massive bosons (mesons) mediate the interaction between two nucleons. Although, in light of quantum chromodynamics (QCD), meson theory is no longer perceived as fundamental, the meson-exchange concept (where hadrons are treated as elementary particles) continues to represent the best working model for a quantitative \"NN\" potential. The Yukawa potential (also called a screened Coulomb potential) is a potential of the form\n\nwhere \"g\" is a magnitude scaling constant, i.e., the amplitude of potential, formula_2 is the Yukawa particle mass, \"r\" is the radial distance to the particle. The potential is monotone increasing, implying that the force is always attractive. The constants are determined empirically. The Yukawa potential depends only on the distance between particles, \"r\", hence it models a central force.\n\nThroughout the 1930s a group at Columbia University led by I. I. Rabi developed magnetic resonance techniques to determine the magnetic moments of nuclei. These measurements led to the discovery in 1939 that the deuteron also possessed an electric quadrupole moment. This electrical property of the deuteron had been interfering with the measurements by the Rabi group. The deuteron, composed of a proton and a neutron, is one of the simplest nuclear systems. The discovery meant that the physical shape of the deuteron was not symmetric, which provided valuable insight into the nature of the nuclear force binding nucleons. In particular, the result showed that the nuclear force was not a central force, but had a tensor character. Hans Bethe identified the discovery of the deuteron's quadrupole moment as one of the important events during the formative years of nuclear physics.\n\nHistorically, the task of describing the nuclear force phenomenologically was formidable. The first semi-empirical quantitative models came in the mid-1950s, such as the Woods–Saxon potential (1954). There was substantial progress in experiment and theory related to the nuclear force in the 1960s and 1970s. One influential model was the Reid potential (1968).\n\nIn recent years, experimenters have concentrated on the subtleties of the nuclear force, such as its charge dependence, the precise value of the π\"NN\" coupling constant, improved phase shift analysis, high-precision \"NN\" data, high-precision \"NN\" potentials, \"NN\" scattering at intermediate and high energies, and attempts to derive the nuclear force from QCD.\n\nThe nuclear force is a residual effect of the more fundamental strong force, or strong interaction. The strong interaction is the attractive force that binds the elementary particles called quarks together to form the nucleons (protons and neutrons) themselves. This more powerful force is mediated by particles called gluons. Gluons hold quarks together with a force like that of electric charge, but of far greater strength. Quarks, gluons and their dynamics are mostly confined within nucleons, but residual influences extend slightly beyond nucleon boundaries to give rise to the nuclear force.\n\nThe nuclear forces arising between nucleons are analogous to the forces in chemistry between neutral atoms or molecules called London forces. Such forces between atoms are much weaker than the attractive electrical forces that hold the atoms themselves together (i.e., that bind electrons to the nucleus), and their range between atoms is shorter, because they arise from small separation of charges inside the neutral atom. Similarly, even though nucleons are made of quarks in combinations which cancel most gluon forces (they are \"color neutral\"), some combinations of quarks and gluons nevertheless leak away from nucleons, in the form of short-range nuclear force fields that extend from one nucleon to another nearby nucleon. These nuclear forces are very weak compared to direct gluon forces (\"color forces\" or strong forces) inside nucleons, and the nuclear forces extend only over a few nuclear diameters, falling exponentially with distance. Nevertheless, they are strong enough to bind neutrons and protons over short distances, and overcome the electrical repulsion between protons in the nucleus.\n\nSometimes, the nuclear force is called the residual strong force, in contrast to the strong interactions which arise from QCD. This phrasing arose during the 1970s when QCD was being established. Before that time, the \"strong nuclear force\" referred to the inter-nucleon potential. After the verification of the quark model, \"strong interaction\" has come to mean QCD.\n\nTwo-nucleon systems such as the deuteron, the nucleus of a deuterium atom, as well as proton–proton or neutron–proton scattering are ideal for studying the \"NN\" force. Such systems can be described by attributing a \"potential\" (such as the Yukawa potential) to the nucleons and using the potentials in a Schrödinger equation. The form of the potential is derived phenomenologically (by measurement), although for the long-range interaction, meson-exchange theories help to construct the potential. The parameters of the potential are determined by fitting to experimental data such as the deuteron binding energy or \"NN\" elastic scattering cross sections (or, equivalently in this context, so-called \"NN\" phase shifts).\n\nThe most widely used \"NN\" potentials are the Paris potential, the Argonne AV18 potential\n, the CD-Bonn potential and the Nijmegen potentials.\n\nA more recent approach is to develop effective field theories for a consistent description of nucleon–nucleon and three-nucleon forces. Quantum hadrodynamics is an effective field theory of the nuclear force, comparable to QCD for color interactions and QED for electromagnetic interactions. Additionally, chiral symmetry breaking can be analyzed in terms of an effective field theory (called chiral perturbation theory) which allows perturbative calculations of the interactions between nucleons with pions as exchange particles.\n\nThe ultimate goal of nuclear physics would be to describe all nuclear interactions from the basic interactions between nucleons. This is called the \"microscopic\" or \"ab initio\" approach of nuclear physics. There are two major obstacles to overcome before this dream can become reality:\n\nThis is an active area of research with ongoing advances in computational techniques leading to better first-principles calculations of the nuclear shell structure. Two- and three-nucleon potentials have been implemented for nuclides up to \"A\" = 12.\n\nA successful way of describing nuclear interactions is to construct one potential for the whole nucleus instead of considering all its nucleon components. This is called the \"macroscopic\" approach. For example, scattering of neutrons from nuclei can be described by considering a plane wave in the potential of the nucleus, which comprises a real part and an imaginary part. This model is often called the optical model since it resembles the case of light scattered by an opaque glass sphere.\n\nNuclear potentials can be \"local\" or \"global\": local potentials are limited to a narrow energy range and/or a narrow nuclear mass range, while global potentials, which have more parameters and are usually less accurate, are functions of the energy and the nuclear mass and can therefore be used in a wider range of applications.\n\n\n\n"}
{"id": "185820", "url": "https://en.wikipedia.org/wiki?curid=185820", "title": "Physics and Beyond", "text": "Physics and Beyond\n\nPhysics and Beyond () is a book by Werner Heisenberg, the German physicist who discovered the uncertainty principle. It tells, from his point of view, the history of exploring atomic science and quantum mechanics in the first half of the 20th century.\n\nAs the subtitle \"Encounters and Conversations\" suggests, the core part of this book takes the form of discussions between himself and other scientists. Heisenberg says: \"I wanted to show that science is done by people, and the most wonderful ideas come from dialog\".\n\nWith chapters like \"The first encounter with the science about atoms\", \"Quantum mechanics and conversations with Einstein\", \"Conversation about the relation between biology, physics and chemistry\" or \"Conversations about language\" and \"The behavior of an individual during a political disaster\", dated 1937-1941, a reader can hear speaking such persons as Erwin Schrödinger, Niels Bohr, Albert Einstein or Max Planck, not only about physics, but also about many other questions related to biology, humans, philosophy, and politics.\n\nNot only that, these conversations are often situated in detailed description of the historical atmosphere and a beautiful scenery, as many of them were led in nature during the many journeys they made, backpacking or sailing. \"'Do you see whales, Heisenberg?', 'Yes, I see only whales, but I hope they are only big waves.'\", is one of humorous scenes when the author, Bohr and other friends were sailing in a dark night.\n\nThe book provides a first-hand account about how science is done and how quantum physics, especially the Copenhagen interpretation, emerged.\n\n\"Nobody can reproduce these conversations verbatim, but I believe that the spirit of what the people said, and how they did, is conserved,\" the author tries to explain in the preface.\n\nMany believe that the golden years of physics around 1925, when \"even small people could do big things\" are gone. But the people who had been there continue to speak to us through this book.\n\nThe book was published first in German 1969, in English as \"Physics and Beyond\" (1971) and in French in 1972 (La partie et le tout).\n"}
{"id": "45239565", "url": "https://en.wikipedia.org/wiki?curid=45239565", "title": "Purple Crow Lidar", "text": "Purple Crow Lidar\n\nThe Purple Crow Lidar is considered to be a powerful laser radar (lidar) that emits pulses of light. The light brightness of this laser is equal to having 1 million 75 W light bulbs on. Such bright light scatters off air molecules, and its reflection gets collected by a large telescope.\n\nThe telescope is formed by rotating liquid mercury at 10 r.p.m. in a 2.65-m diameter container. This liquid mirror technology has been made and developed at Université Laval in Québec City. Such rotating measurement allow air density, pressure, temperature and composition to be measured. Which can be useful to collect data for global warming and weather prediction.\n\nThis chart is used to determine and predict the weather in Purple Crow Lidar Facility in London Ontario for astronomical observing.\n\nThe Purple Crow operates from the Echo Base Observatory located at Western's Environmental Science Field Station located near London, Ontario, Canada.\n\nThe Purple Crow Lidar research project is headed by Professor Robert J. Sica in Physics Department at the UWO London Ontario, and students (primarily graduate students), can help and participate in astronmical research.\nIts main support comes from the Natural Science and Engineering Research Council of Canada (NSERC). \n\nLarge Zenith Telescope\n\nLiquid Mirror Telescope\n"}
{"id": "12818849", "url": "https://en.wikipedia.org/wiki?curid=12818849", "title": "René Lavocat", "text": "René Lavocat\n\nRené Lavocat was a French paleontologist who described several genera of African dinosaurs including the sauropod \"Rebbachisaurus\", as well as several extinct mammals such as the family Kenyamyidae. The mammal \"Lavocatia\", the Notosuchian \"Lavocatchampsa\" and phorusrhacid \"Lavocatavis\" are named after him.\n\nEager to try paleontological research in Africa to find Oligocene mammals, Lavocat was strongly endorsed by Camille Arambourg. In 1947, he obtained leadership of a research mission in the Algerian-Moroccan desert.\n\nHe did not find any Oligocene mammals, but instead came across a rich fauna of Cretaceous vertebrates. His first notes on this subject were made in 1948 entitled \"les Comptes Rendus Sommaires de la Société géologique de France\" (\"English: Report Summary to the Geological Society of France\") in which Lavocat explains the discovery of a large number of Cretaceous reptiles (dinosaurs and crocodiles) and fish in the bedrock of the desert. A year later, a second note appeared in the same journal and extends his discoveries to the southwestern Kem Kem.\n\nIn 1954, Lavocat described a new species of sauropod, Rebbachisaurus, discovered in the region. In addition, in 1955 he described a new genus of theropod, Majungasaurus. In 1960, Lavocat returned to Africa and described a second species of Rebbachisaurus, \"R. tamesnensis\". In 1973, Lavocat discovered two genera and three species of Miocene rodent, which he placed in the family Kenyamyidae.\n"}
{"id": "57814490", "url": "https://en.wikipedia.org/wiki?curid=57814490", "title": "Sarah Tuttle", "text": "Sarah Tuttle\n\nSarah Tuttle is a Professor of Astrophysics and Science Communicator based at the University of Washington. She designs experimental equipment to detect nearby galaxies. She was the instrument scientist of McDonald Observatory's Hobby–Eberly Telescope dark energy study VIRUS.\n\nTuttle was born and raised in Santa Cruz. She studied physics at the University of California, Santa Cruz (UCSC) and graduated in 2001. After graduating Tuttle joined Add Vision as a research scientist. She was part of the team who built the first screen-printed polymer light emitting diodes. She moved to Columbia University for her Master's and PhD, working with David Schiminovich on the Faint Intergalactic medium Redshifted Emission Balloon (FIREBall). FIREBall is a balloon-borne telescope that is coupled to a Ultraviolet–visible spectrograph.\n\nTuttle was appointed as the lead for the Hobby–Eberly Telescope's VIRUS detector. In 2016 she joined University of Washington as an Assistant Professor. She is leading a spectrography project for the Apache Point Observatory.\n\nShe appeared on the podcast 365 Days of Astronomy. Tuttle contributed to American Astronomical Society workshops and supported new guidelines to build a more diverse and inclusive environment.\n"}
{"id": "3410567", "url": "https://en.wikipedia.org/wiki?curid=3410567", "title": "Similarity Matrix of Proteins", "text": "Similarity Matrix of Proteins\n\nSimilarity Matrix of Proteins (SIMAP) is a database of protein similarities created using distributed computing. It is freely accessible for scientific purposes. SIMAP uses the FASTA algorithm to precalculate protein similarity, while another application uses hidden Markov models to search for protein domains. SIMAP is a joint project of the Technical University of Munich, the Helmholtz Zentrum München, and the University of Vienna.\n\nThe project usually got new work units at the beginning of each month. More recently, (2010), inclusion of environmental sequences into the database has required longer periods of activity, several months of continuous work for example. Typically, these updates occurred twice each year.\n\nIn the fourth quarter of 2010, the project relocated to the University of Vienna due to the failing electrical infrastructure at the Technical University of Munich. Part of this exercise involved the creation of a project specific URL requiring existing volunteers and users to detach/reattach to the project.\n\nOn May 30, 2014, it was announced by project administrators that after a 10-year history, SIMAP would be leaving BOINC by the end of 2014. SIMAP research, however, will go forward with the use of local hardware consisting of \"ordinary multi-core CPUs (some hundreds), crunching a SSE-optimized version of the Smith-Waterman algorithm.\"\n\nSIMAP used the Berkeley Open Infrastructure for Network Computing (BOINC) distributed computing platform.\n\nApplication performance notes. Work unit CPU times varied widely, ranging between 15 minutes and 3 hours. Work units varied in size from 1.5 to 2.2 MB each, averaging around 2 MB. SIMAP provided client software optimized for SSE enabled processors and x86-64 processors. For older processors non SSE applications are provided but require manual installation steps to be taken. Operating Systems supported by SIMAP are Linux, Windows, Mac OS, Android, and other UNIX platforms. Since the database had sometimes been completed with all publicly known protein sequences and metagenomes having been precalculated by the project, the work available consisted of newly published protein sequences and metagenomes that needed to be precomputed for SIMAP.\n\n\n"}
{"id": "45311025", "url": "https://en.wikipedia.org/wiki?curid=45311025", "title": "Spectrometer", "text": "Spectrometer\n\nA spectrometer () is a scientific instrument used to separate and measure spectral components of a physical phenomenon. Spectrometer is a broad term often used to describe instruments that measure a continuous variable of a phenomenon where the spectral components are somehow mixed. In visible light a spectrometer can for instance separate white light and measure individual narrow bands of color, called a spectrum, while a mass spectrometer measures the spectrum of the masses of the atoms or molecules present in a gas. The first spectrometers were used to split light into an array of separate colors. Spectrometers were developed in early studies of physics, astronomy, and chemistry. The capability of spectroscopy to determine chemical composition drove its advancement and continues to be one of its primary uses. Spectrometers are used in astronomy to analyze the chemical composition of stars and planets, and spectrometers gather data on the origin of the universe. \n\nExamples of spectrometers are devices that separate particles, atoms, and molecules by their mass, momentum, or energy. These types of spectrometers are used in chemical analysis and particle physics.\n\nOptical spectrometers (often simply called \"spectrometers\"), in particular, show the intensity of light as a function of wavelength or of frequency. The deflection is produced either by refraction in a prism or by diffraction in a diffraction grating. \n\nThese spectrometers utilize the phenomenon of optical dispersion. The light from a source can consist of a continuous spectrum, an emission spectrum (bright lines), or an absorption spectrum (dark lines). Because each element leaves its spectral signature in the pattern of lines observed, a spectral analysis can reveal the composition of the object being analyzed.\n\nA mass spectrometer is an analytical instrument that is used to identify the amount and type of chemicals present in a sample by measuring the mass-to-charge ratio and abundance of gas-phase ions.\n\nThe energy spectrum of particles of known mass can also be measured by determining the time of flight between two detectors (and hence, the velocity) in a time-of-flight spectrometer. Alternatively, if the velocity is known, masses can be determined in a time-of-flight mass spectrometer.\n\nWhen a fast charged particle (charge \"q\", mass \"m\") enters a constant magnetic field \"B\" at right angles, it is deflected into a circular path of radius \"r\", due to the Lorentz force. The momentum \"p\" of the particle is then given by\n\nwhere \"m\" and \"v\" are mass and velocity of the particle. The focusing principle of the oldest and simplest magnetic spectrometer, the semicircular spectrometer, invented by J. K. Danisz, is shown on the left. A constant magnetic field is perpendicular to the page. Charged particles of momentum \"p\" that pass the slit are deflected into circular paths of radius \"r = p/qB\". It turns out that they all hit the horizontal line at nearly the same place, the focus; here a particle counter should be placed. Varying \"B\", this makes possible to measure the energy spectrum of alpha particles in an alpha particle spectrometer, of beta particles in a beta particle spectrometer, of particles (e.g., fast ions) in a particle spectrometer, or to measure the relative content of the various masses in a mass spectrometer.\n\nSince Danysz' time, many types of magnetic spectrometers more complicated than the semicircular type have been devised.\n\nGenerally, the resolution of an instrument tells us how well two close-lying energies (or wavelengths, or frequencies, or masses) can be resolved. Generally, for an instrument with mechanical slits, higher resolution will mean lower intensity.\n\n"}
{"id": "40599510", "url": "https://en.wikipedia.org/wiki?curid=40599510", "title": "The Birth Order Book", "text": "The Birth Order Book\n\nThe Birth Order Book: Why You Are the Way You Are is a 1982 non-fiction book by Dr. Kevin Leman on birth order and its potential influence on personality and development. An updated and revised version of the book was published in 1998 through Baker Publishing Group. Leman first began studying birth orders while a student at the University of Arizona and noticing that several elements of personality were associated with a person's birth order rank.\n\nIn the book Leman details four types of personality based upon the individual's birth order: First Born, Only Child, Middle Child, and Last Born. Only Child types are considered to be a form of the First Born personality types, but \"in triplicate\".\n\n\nReception for \"The Birth Order Book\" and the theories espoused in the book has been mixed, with some commenting that there are \"many variables\" that have an effect on the personality aside from birth order. In a 1995 article in the Los Angeles Times, University of Texas professor Toni Falbo commented that the modern family dynamic is \"quite complex\" and that \"[relying] too heavily on birth order for answers is a mistake\" due to families being \"much more complicated now\" due to the addition of step-siblings, half-siblings, and other various factors.\n"}
{"id": "3989011", "url": "https://en.wikipedia.org/wiki?curid=3989011", "title": "The Music of the Primes", "text": "The Music of the Primes\n\nThe Music of the Primes (British subtitle: \"Why an Unsolved Problem in Mathematics Matters\"; American subtitle: \"Searching to Solve the Greatest Mystery in Mathematics\") is a 2003 book by Marcus du Sautoy, a professor in mathematics at the University of Oxford, on the history of prime number theory. In particular he examines the Riemann hypothesis, the proof of which would revolutionize our understanding of prime numbers. He traces the prime number theorem back through history, highlighting the work of some of the greatest mathematical minds along the way.\n\nThe cover design for the hardback version of the book contains several pictorial depictions of prime numbers, such as the number 73 bus. It also has an image of a clock, referring to clock arithmetic, which is a significant theme in the text.\n"}
{"id": "1239214", "url": "https://en.wikipedia.org/wiki?curid=1239214", "title": "Thermal fluids", "text": "Thermal fluids\n\nThermofluids is a branch of science and engineering encompassing four intersecting fields:\n\nThe term is a combination of \"thermo\", referring to heat, and \"fluids\", which refers to liquids, gases and vapors. Temperature, pressure, equations of state, and transport laws all play an important role in thermofluid problems. Phase transition and chemical reactions may also be important in a thermofluid context. The subject is sometimes also referred to as \"thermal fluids\".\n\nHeat transfer is a discipline of thermal engineering that concerns the transfer of thermal energy from one physical system to another. Heat transfer is classified into various mechanisms, such as heat conduction, convection, thermal radiation, and phase-change transfer. Engineers also consider the transfer of mass of differing chemical species, either cold or hot, to achieve heat transfer. \n\nSections include :\n\n\nThermodynamics is the science of energy conversion involving heat and other forms of energy, most notably mechanical work. It studies and interrelates the macroscopic variables, such as temperature, volume and pressure, which describe physical, thermodynamic systems.\n\nFluid Mechanics the study of the physical forces at work during fluid flow. Fluid mechanics can be divided into fluid kinematics, the study of fluid motion, and fluid dynamics, the study of the effect of forces on fluid motion, which can further be divided into fluid statics, the study of fluids at rest, and fluid kinetics, the study of fluids in motion. Some of its more interesting concepts include momentum and reactive forces in fluid flow and fluid machinery theory and performance.\n\nSections include:\n\n\nCombustion is the sequence of exothermic chemical reactions between a fuel and an oxidant accompanied by the production of heat and conversion of chemical species. The release of heat can result in the production of light in the form of either glowing or a flame. Fuels of interest often include organic compounds (especially hydrocarbons) in the gas, liquid or solid phase.\n\n"}
{"id": "1822111", "url": "https://en.wikipedia.org/wiki?curid=1822111", "title": "Time discipline", "text": "Time discipline\n\nIn sociology and anthropology, time discipline is the general name given to social and economic rules, conventions, customs, and expectations governing the measurement of time, the social currency and awareness of time measurements, and people's expectations concerning the observance of these customs by others.\n\nThe concept of \"time discipline\" as a field of special attention in sociology and anthropology was pioneered by E. P. Thompson in \"Time, Work-Discipline, and Industrial Capitalism\", published in 1967. Coming from a Marxist viewpoint, Thompson argued that observance of clock-time is a consequence of the European industrial revolution, and that neither industrial capitalism nor the creation of the modern state would have been possible without the imposition of synchronic forms of time and work discipline. The new clock time imposed by government and capitalist interests replaced earlier, collective perceptions of time that Thompson believed flowed from the collective wisdom of human societies. While in fact it appears likely that earlier views of time were imposed instead by religious and other social authorities prior to the industrial revolution, Thompson's work identified time discipline as an important concept for study within the social sciences. \n\nIn societies based around agriculture, hunting, and other pursuits that involve human interaction with the natural world, time discipline is a matter governed by astronomical and biological factors. Specific times of day or seasons of the year are defined by reference to these factors, and measured, to the extent that they need measuring, by observation. Different peoples' needs with respect to these things mean sharply differing cultural perceptions of time. For example, it surprises many non-Muslims that the Islamic calendar is entirely lunar and makes no reference at all to the seasons; the desert-dwelling Arabs who devised it were nomads rather than agriculturalists, and a calendar that made no reference to the seasons was no inconvenience for most of them.\n\nIn more urban societies, some of these natural phenomena were no longer at hand, and most were of much less consequence to the inhabitants. Artificial means of dividing and measuring time were needed. Plautus complained of the social effect of the invention of such divisions in his lines complaining of the sundial:\n\nPlautus's protagonist here complains about the social discipline and expectations that arose when these measurements of time were introduced. The invention of artificial units of time measurement made the introduction of time management possible, and time management was not universally appreciated by those whose time was managed.\n\nIn western Europe, the practice of Christian monasticism introduced new factors into the time discipline observed by members of religious communities. The rule of Saint Benedict introduced canonical hours; these were religious observances that were held on a daily basis, and based on factors again mostly unrelated to natural phenomena. It is no surprise, then, that religious communities were likely the inventors, and certainly the major consumers, of early clocks. The invention of the mechanical clock in western Europe, and its subsequent technical developments, enabled a public time discipline even less related to natural phenomena. (Highly sophisticated clepsydras existed in China, where they were used by astrologers connected with the imperial court; these water clocks were quite large, and their use limited to those who were professionally interested in precise timekeeping.)\n\nThe English word \"clock\" comes from an Old French word for \"bell,\" for the striking feature of early clocks was a greater concern than their dials. Shakespeare's Sonnet XII begins, \"When I do count the clock that tells the time.\" Even after the introduction of the clock face, clocks were costly, and found mostly in the homes of aristocrats. The vast majority of urban dwellers had to rely on clock towers, and outside the sight of their dials or the sound of their bells, clock time held no sway. Clock towers did define the time of day, at least for those who could hear and see them. As the saying goes, \"a person with a clock always knows what time it is; a person with two clocks is never sure.\"\n\nThe discipline imposed by these public clocks still remained lax by contemporary standards. A clock that only strikes the hours can only record the nearest hour that has passed; most early clocks had only hour hands in any case. Minute hands did not come into widespread use until the pendulum enabled a large leap in the accuracy of clocks; for watches, a similar leap in accuracy was not made possible before the invention of the balance spring. Before these improvements, the equation of time, the difference between apparent and mean solar time, was not even noticed.\n\nDuring the 17th and 18th centuries, private ownership of clocks and watches became more common, as their improved manufacture made them available for purchase by at least the bourgeoisie of the cities. Their proliferation had many social and even religious consequences for those who could afford and use them.\n\nBefore time became standardized, clock masters used “True Time”. The day work began and ended with the sun. This time period was divided into 12 equal hours. This meant that these hours would vary with the seasons, as the length of daylight changed. Each town would have their own variance of this “True Time”. Eventually, cities adopted “Mean Time”, which is how we think of time nowadays. Astronomers used the Earth’s rotation and the stars to calculate the time, and divided the day into 24 uniform and equal hours. Geneva was the first city to adopt mean time in 1780, followed by London in 1792, Berlin in 1810, Paris in 1816, and Vienna in 1823.\n\nReligious texts of the period make many more references to the irreversible passage of time, and artistic themes appeared at this time such as \"Vanitas\", a reminder of death in the form of a still life, which always included a watch, clock, or some other timepiece. The relentless ticking of a clock or watch, and the slow but certain movement of its hands, functioned as a visible and audible \"memento mori\". Clocks and sundials would be decorated with mottos such as \"ultima forsan\" (\"perhaps the last\" [hour]) or \"vulnerant omnes, ultima necat\" (\"they all wound, and the last kills\"). Even today, clocks often carry the motto \"tempus fugit\", \"time flies.\" Mary, Queen of Scots was said to have owned a large watch made in the shape of a silver skull.\n\nEconomically, their impact was even greater; an awareness that \"time is money\", a limited commodity not to be wasted, also appears during this period. Because Protestantism was at this time chiefly a religion of literate city dwellers, the so-called \"Protestant work ethic\" came to be associated with this newly fashioned time discipline. Production of clocks and watches during this period shifted from Italy and Bavaria to Protestant areas such as Geneva, the Netherlands, and England; the names of French clockmakers during this time disclose a large number of commonly Huguenot names from the Old Testament.\n\nIn the nineteenth century, the introduction of standard time and time zones divorced the \"time of day\" from local mean solar time and any links to astronomy. Time signals, like the bells and dials of public clocks, once were relatively local affairs; the ball that is dropped in Times Square on New Year's Eve in New York City once served as a time signal whose original purpose was for navigators to check their marine chronometers. However, when the railroads began running trains on complex schedules, keeping a schedule that could be followed over distances of hundreds of miles required synchronization on a scale not attempted before. Telegraphy and later shortwave radio were used to broadcast time signals from the most accurate clocks available. Radio and television broadcasting schedules created a further impetus to regiment everyone's clock so that they all told the same time within a very small tolerance; the broadcasting of time announcements over radio and television enabled all the households in their audience to get in synch with the clocks at the network.\n\nThe mass production of clocks and watches further tightened time discipline in the Western world; before these machines were made, and made to be more accurate, it would be pointless to complain about someone's being fifteen, or five, minutes late. For many employees, the time clock was the clock that told the time that mattered: it was the clock that recorded their hours of work. By the time that time clocks became commonplace, public, synchronized clock time was considered a fact of life. Uniform, synchronized, public clock time did not exist until the nineteenth century.\n\nWhen one speaks about the intellectual history of time, one essentially is stating that changes have occurred in the way humans experience and measure time. Our conceived abstract notions of time have presumably developed in accordance with our art, our science, and our social infrastructure. (See also horology.)\n\nThe units of time first developed by humans would likely have been days and months (moons). In some parts of the world the cycle of seasons is apparent enough to lead to people speaking about years and seasons (e.g. 4 summers ago, or 4 floods ago). With the invention of agriculture in the 3rd millennium BC, people relied heavily on the cycle of the seasons for planting and harvesting crops. Most humans came to live in settled societies and the whole community relied upon accurate predictions of the seasonal cycle. This led to the development of calendars. Over time, some people came to recognize patterns of the stars with the seasons. Learning astronomy became an assigned duty for certain people so they could coordinate the lunar and solar calendars by adding days or months to the year.\n\nAt about the same time, sundials were developed, likely marked first at noon, sunrise and sunset. In ancient Sumer and Egypt, numbers were soon used to divide the day into 12 hours; the night was similarly divided. In Egypt there is not as much seasonal variation in the length of the day, but those further from the equator would need to make many more modifications in calibrating their sundials to deal with these differences. Ancient traditions did \"not\" begin the day at midnight, some starting at dawn instead, others at dusk (both being more obvious).\n\nSince a sundial has only one \"hand,\" a minute probably only meant \"a short time.\" It took centuries for technology to make measurements precise enough for minutes (and later seconds) to become fixed meaningful units—longer still for milliseconds, nanoseconds, and further subdivisions.\n\nWhen the water clock was invented, time could also be measured at night—though there was significant variation in flow rate and less accuracy and precision. With water clocks, and also candle clocks, modifications were made to have them make sounds on a regular basis.\n\nWith the invention of the hourglass (perhaps as early as the 11th century), hours and units of time smaller than an hour could be measured much more reliably than with water clocks and candle clocks.\n\nThe earliest reasonably accurate mechanical clocks are the 13th century tower clocks probably developed for (and perhaps by) monks in Northern Italy. Using gears and gradually falling weights, these were adjusted to conform with canonical hours—which varied with the length of the day. As these were used primarily to ring bells for prayer, the clock dial likely only came later. When dials were eventually incorporated into clocks, they were analogous to the dials on sundials, and, like a sundial, the clocks themselves had only one hand.\n\nA possible explanation for the shift from having the first hour being the one after dawn, to having the hour after noon being designated as 1 pm (post meridiem), is that these clocks would likely regularly be reset at local high noon each day. This, of course, results in midnight becoming 12 o'clock.\n\nPeter Henlein, a locksmith and burgher of Nuremberg, Germany, invented a spring-powered clock around 1510. It had only one hand, had no glass cover, and was rather imprecise because it slowed down as the spring unwound. In fact, Henlein went so far as to develop the first portable watch; it was six inches high. People usually carried it by hand, or wore it around their necks or in large pockets. The first reported person to actually wear a watch on the wrist was the French mathematician and philosopher, Blaise Pascal (1623–1662). He attached his pocket watch to his wrist with a piece of string.\n\nIn 1577, the minute hand was added by a Swiss clock maker, Jost Burgi (who also is a contender for the invention of logarithms), and was incorporated into a clock Burgi made for astronomer Tycho Brahe, who had a need for more accuracy as he charted the heavens.\n\nWith invention of the pendulum clock in 1656 by Christiaan Huygens, came isochronous time, with a fixed pace of 3600 seconds per hour. By 1680, both a minute hand and then a second hand were added. Some of the first of these had a separate dial for the minute hand (turning counter-clockwise), and a \"second\" hand that took 5 minutes per cycle. Even as late as 1773, towns were content to order clocks without minute hands.\n\nBut the clocks were still aligned with the local noonday sun. Following the invention of the locomotive in 1830, time had to be synchronized across vast distances in order to organize the train schedules. This eventually led to the development of time zones, and, thus, global isochronous time. These time changes were not accepted everywhere right away, because many people's lives were still tied closely to the length of the daytime. With the invention in 1879 of the light bulb, that changed too.\n\nThe isochronous clock changed lives. Appointments are rarely \"within the hour,\" but at quarter hours (and being five minutes late is often considered being tardy). People often eat, drink, sleep, and even go to the bathroom in adherence to some time-dependent schedule.\n\nWhile Thompson's theory of industrial time-discipline has dominated the field for more than 40 years, critics of his work have emerged. \n\nPaul Glennie and Nigel Thrift posit an alternative perspective on the development of time-consciousness in \"Reworking E. P. Thompson's 'Time, Work-Discipline and Industrial Capitalism'\" (1996). According to Glennie and Thrift, Thompson and subsequent theorists on modern time competence in England have theorized that industrial work-discipline centered on the clock is responsible for spreading a unitary concept of time rooted in materialist realities. In contrast, Glennie and Thrift explore the role of symbolic, qualitative, and multiple time-senses in the West. Different kinds of work and multiple means of measuring time problematize the centrality of factory work and the clock. Generally, they argue that time-discipline was evident before the spread of industrialization and that it did not trigger a significant change in time-sense. Because it rests on the argument that disparate, spatial temporalities can not be unified, critics have argued that their analysis seems incomplete. In short, they offer poignant critiques of the dominant theory without positing a stronger theory in its place.\n\nMichael J. Sauter argues that Thompson's approach to time discipline is \"gendered and Eurocentric\". Time discipline did not arise because of the Industrial Revolution, but had been a phenomenon since the Middle Ages as the government, religion, and economics played larger roles in day-to-day life. In Sauter's article \"Clockwaters and Stargazers: Time Discipline in Early Modern Berlin\", he argues that time discipline came from the streets, and was part of the rise of \"local knowledge\" as public clocks were used by public event planners. People began to learn where clocks were located and which social groups used which ones. Furthermore, Sauter argues that time discipline is not \"externally imposed\" on people, but \"a standard that is determined by people with specialized knowledge and skills\". Prior to the rise of mechanical timekeeping, clocks were based on the easily accessed sun, and after 1800 precise timekeeping again returned to the Earth's position in relationship to the stars, as measured by scientists using specialized instruments.\n\n\n\n"}
{"id": "852567", "url": "https://en.wikipedia.org/wiki?curid=852567", "title": "Triarchic theory of intelligence", "text": "Triarchic theory of intelligence\n\nThe triarchic theory of intelligence was formulated by Robert J. Sternberg, a prominent figure in research of human intelligence. The theory by itself was among the first to go against the psychometric approach to intelligence and take a more cognitive approach. The three meta components are also called triarchic components.\n\nSternberg's definition of human intelligence is \"(a) mental activity directed toward purposive adaptation to, selection and shaping of, real-world environments relevant to one's life\" (Sternberg, 1985, p. 45). Thus, Sternberg viewed intelligence as how well an individual deals with environmental changes throughout their lifespan. Sternberg's theory comprises three parts: componential, experiential, and practical.\n\nSternberg associated the workings of the mind with a series of components. These components he labeled the metacomponents, performance components, and knowledge-acquisition components (Sternberg, 1985).\n\nThe \"metacomponents\" are executive processes used in problem solving and decision making that involve the majority of managing our mind. They tell the mind how to act. Metacomponents are also sometimes referred to as a homunculus. A homunculus is a fictitious or metaphorical \"person\" inside our head that controls our actions, and which is often seen to invite an infinite regress of homunculi controlling each other (Sternberg, 1985).\n\nSternberg's next set of components, \"performance components\", are the processes that actually carry out the actions the metacomponents dictate. These are the basic processes that allow us to do tasks, such as perceiving problems in our long-term memory, perceiving relations between objects, and applying relations to another set of terms (Sternberg, 1997).\n\nThe last set of components, \"knowledge-acquisition components\", are used in obtaining new information. These components complete tasks that involve selectively choosing relevant information from a mix of information, some of it relevant and some of it irrelevant. These components can also be used to selectively combine the various pieces of information they have gathered. Gifted individuals are proficient in using these components because they are able to learn new information at a greater rate (Sternberg, 1997).\n\nWhereas Sternberg explains that the basic information processing components underlying the three parts of his triarchic theory are the same, different contexts and different tasks require different kinds of intelligence (Sternberg, 2001).\n\nSternberg associated the componential subtheory with analytical giftedness. This is one of three types of giftedness that Sternberg recognizes. Analytical giftedness is influential in being able to take apart problems and being able to see solutions not often seen. Unfortunately, individuals with only this type are not as adept at creating unique ideas of their own. This form of giftedness is the type that is tested most often (Sternberg, 1997).\n\nSternberg's 2nd stage of his theory is his experiential subtheory. This stage deals mainly with how well a task is performed with regard to how familiar it is. Sternberg splits the role of experience into two parts: novelty and automation.\n\nA \"novel\" situation is one that you have never experienced before. People that are adept at managing a novel situation can take the task and find new ways of solving it that the majority of people would not notice (Sternberg, 1997).\n\nA process that has been \"automated\" has been performed multiple times and can now be done with little or no extra thought. Once a process is automatized, it can be run in parallel with the same or other processes. The problem with novelty and automation is that being skilled in one component does not ensure that you are skilled in the other (Sternberg, 1997).\n\nThe experiential subtheory also correlates with another one of Sternberg's proposed types of giftedness. Synthetic giftedness is seen in creativity, intuition, and a study of the arts. People with synthetic giftedness are not often seen with the highest IQ's because there are not currently any tests that can sufficiently measure these attributes, but synthetic giftedness is especially useful in creating new ideas to create and solve new problems. Sternberg also associated another one of his students, \"Barbara\", to the synthetic giftedness. Barbara did not perform as well as Alice on the tests taken to get into school, but was recommended to Yale University based on her exceptional creative and intuitive skills. Barbara was later very valuable in creating new ideas for research (Sternberg, 1997).\n\nSternberg's third subtheory of intelligence, called practical or contextual, \"deals with the mental activity involved in attaining fit to context\" (Sternberg, 1985, p. 45). Through the three processes of adaptation, shaping, and selection, individuals create an ideal fit between themselves and their environment. This type of intelligence is often referred to as \"street smarts.\"\n\n\"Adaptation\" occurs when one makes a change within oneself in order to better adjust to one's surroundings (Sternberg, 1985). For example, when the weather changes and temperatures drop, people adapt by wearing extra layers of clothing to remain warm.\n\n\"Shaping\" occurs when one changes their environment to better suit one's needs (Sternberg, 1985). A teacher may invoke the new rule of raising hands to speak to ensure that the lesson is taught with least possible disruption.\n\nThe process of \"selection\" is undertaken when a completely new alternate environment is found to replace the previous, unsatisfying environment to meet the individual's goals (Sternberg, 1985). For instance, immigrants leave their lives in their homeland countries where they endure economical and social hardships and go to other countries in search of a better and less strained life.\n\nThe effectiveness with which an individual fits to his or her environment and contends with daily situations reflects degree of intelligence. Sternberg's third type of giftedness, called practical giftedness, involves the ability to apply synthetic and analytic skills to everyday situations. Practically gifted people are superb in their ability to succeed in any setting (Sternberg, 1997). An example of this type of giftedness is \"Celia\". Celia did not have outstanding analytical or synthetic abilities, but she \"was highly successful in figuring out what she needed to do in order to succeed in an academic environment. She knew what kind of research was valued, how to get articles into journals, how to impress people at job interviews, and the like\" (Sternberg, 1997, p. 44). Celia's contextual intelligence allowed her to use these skills to her best advantage. \nSternberg also acknowledges that an individual is not restricted to having excellence in only one of these three intelligences. Many people may possess an integration of all three and have high levels of all three intelligences.\n\nPractical intelligence is also a topic covered by Malcolm Gladwell in his book \"\".\n\nPsychologist Linda Gottfredson criticises the unempirical nature of triarchic theory. Further, she argues it is absurd to assert that traditional intelligence tests are not measuring practical intelligence, given that they show a moderate correlation with income, especially at middle age when individuals have had a chance to reach their maximum career potential, and an even higher correlation with occupational prestige, and that IQ tests predict the ability to stay out of jail and stay alive (all of which qualifies as practical intelligence or \"street smarts\"). Gottfredson claims that what Sternberg calls practical intelligence is not a broad aspect of cognition at all but simply a specific set of skills people learn to cope with a specific environment (task specific knowledge).\n\nThere is evidence to suggest that certain aspects of creativity (i.e. divergent thinking) are separable from analytical intelligence, and are better accounted for by the cognitive process of executive functioning. More specifically, task-switching and interference management are suggested to play an important role in divergent thinking. A more recent meta-analysis found only small correlations between IQ and creativity (Kim, 2005).\n\n\n"}
{"id": "38193228", "url": "https://en.wikipedia.org/wiki?curid=38193228", "title": "White–Juday warp-field interferometer", "text": "White–Juday warp-field interferometer\n\nThe White–Juday warp-field interferometer is an experiment designed to detect a microscopic instance of a warping of spacetime. If such a warp is detected, it is hoped that more research into creating an Alcubierre warp bubble will be inspired. A research team led by Harold \"Sonny\" White in collaboration with Dr. Richard Juday at the NASA Johnson Space Center and Dakota State University are conducting experiments, but results so far have been inconclusive.\n\nThe NASA research team led by Harold White and their university partners currently aim to experimentally evaluate several concepts, especially a redesigned energy-density topology, as well as an implication of brane cosmology theory. If space actually were to be embedded in higher dimensions, the energy requirements could be decreased dramatically and a comparatively small energy density could already lead to a measurable (i.e. using an interferometer) curvature of spacetime. The theoretical framework for the experiment dates back to work by Harold White from 2003, as well as work by White and Eric W. Davis from 2006 that was published in the AIP, where they also consider how baryonic matter could, at least mathematically, adopt characteristics of dark energy (see section below). In the process, they described how a toroidal positive energy density may result in a spherical negative-pressure region, possibly eliminating the need for actual exotic matter.\n\nThe metric derived by Alcubierre was mathematically motivated by cosmological inflation.\n\nThe original device proposed by White after finding the energy-decreasing possibilities (see theoretical framework) is a modified Michelson interferometer that uses a λ = 633 nm beam from a helium–neon laser. The beam is split into two paths, with the space-warping device placed in or near one beam path. The space warp would induce a relative phase shift between the split beams that should be detectable, provided that the magnitude of the phase shift created by the change in apparent path length is sufficient. By using 2D analytic signal processing, the magnitude and phase of the field can be extracted for study and comparison to theoretical models. The researchers first tried to see whether the space warping by the electric-field energy of a high-voltage (up to 20 kV) ring (0.5 cm radius) of high-κ barium titanate ceramic capacitors could be detected. After the first tests the experiment was moved to a seismically isolated lab due to very high interference caused by people walking outside the room. The goals circa 2013 were to increase sensitivity up to 1/100 of a wavelength and implement the oscillating field in order to get definite results.\n\nWhite announced the first results of his interferometer experiment at a 2013 space conference. According to White, these results showed a vanishing but non-zero difference between charged and uncharged states after signal processing, but this difference remains inconclusive due to external interference and limits in the computational processing. It is now clear that no exotic matter is involved in such an experiment, but some other concept is being used.\n\nDuring the first two weeks of April 2015, scientists fired lasers through a microwave cavity and noticed highly significant variations in the path time. The readings indicated that some of the laser pulses traveled longer, possibly pointing to a slight warp bubble inside the resonance chamber of the device. However, a small rise in ambient air temperature inside the chamber was also recorded, which could possibly have caused the recorded fluctuation in speeds of the laser pulses. According to Paul March, a NASA JSC researcher, the experiment was to be verified inside a vacuum chamber to remove all interference of air. This was done at the end of April 2015. White does not think, however, that the measured change in path length is due to transient air heating, because the visibility threshold is 40 times larger than the predicted effect from air.\n\nThe experiment used a short, cylindrical, aluminum resonant cavity (and not a tapered resonant cavity or EmDrive) excited at a natural frequency of 1.48 GHz with an input power of 30 watts, over 27000 cycles of data (each 1.5 s cycle energizing the system for 0.75 s and de-energizing it for 0.75 s), which were averaged to obtain a power spectrum that revealed a signal frequency of 0.65 Hz with amplitude clearly above system noise. Four additional tests were successfully conducted that demonstrated repeatability.\n\nThe NASA research team has postulated that their findings could reduce the energy requirements for a spaceship moving at ten times the speed of light (\"warp 2\") from the mass–energy equivalent of the planet Jupiter to that of the Voyager 1 spacecraft (~700 kg) or less.\n\nBy harnessing the physics of cosmic inflation, future spaceships crafted to satisfy the laws of these mathematical equations might actually be able to get somewhere unthinkably fast and without adverse effects.\n\nPhysicist and EarthTech CEO Harold E. Puthoff has explained that, contrary to widespread belief, even the highly blue-shifted light seen on board such a spaceship would not fry its crew, being bathed in strong UV light and X-rays. It would, however, be dangerous to anyone seeing it fly by closely.\n\nResearch on this device and other proposed devices is notable as the original newsletter from the NASA center and later presentations at a NASA conference detailed NASA funding of research in advanced concepts and in this particular case in the work proposed by Miguel Alcubierre, physical effects that have potential applications to space travel. In addition, these news releases included the researchers' enthusiastic descriptions of the potentials with statements such as: \"... while this would be a very modest instantiation of the phenomenon, it would likely be a Chicago pile moment for this area of research...\" Several space technology newsletters and space society organizations have further publicized these claims. Keith Cowing of the blog NASA Watch questions the oversight of this line of research by NASA and requested an explanation. Another journalist says that although a practical warp drive is a long way off, serious efforts to learn more about it are being undertaken now. At the second 100 Year Starship Symposium, White told Space.com, \"We're trying to see if we can generate a very tiny instance of this in a tabletop experiment\", the project is a \"humble experiment\" but said it represents a promising first step: \"The findings I presented today change it from impractical to plausible and worth further investigation.\"\n\n\n"}
{"id": "57531353", "url": "https://en.wikipedia.org/wiki?curid=57531353", "title": "Women in archaeology", "text": "Women in archaeology\n\nWomen in archaeology is an aspect of the history of archaeology and the topic of women in science more generally.\n\nAs a professional field of study, archaeology was initially established as an academic discipline in the nineteenth century and typically developed from people engaged in the study of antiquities. Prior to the Victorian era, women in Canada, the United Kingdom and the United States were rarely engaged in professional archaeology. Participation by women in the field was discouraged, both by men and societal pressure, as the occupation masculinized the accepted view of women as homemakers and nurturers. Even after they began to enter the field, the reluctance of male colleagues to accept them in fieldwork, led many women to choose roles outside of academia, seeking positions in museums or in cultural preservation associations. In Europe, women often entered the discipline as research partners with their husbands or to learn about the cultures when their spouses were posted to Colonial outposts or missionary fields. From the mid-1850s women's higher education facilities began offering separate courses for women and in the 1870s several European countries opened university curricula to women. Though women were accepted into the study of archaeology, they were rarely considered equals and often were not admitted to prestigious societies, or allowed to complete training in the field. Swedish archaeologist Hanna Rydh was one exception, as was French archaeologist Madeleine Colani, but more typical were the hard-fought battles of women such as Edith Hall, Harriet Boyd Hawes, , Eugénie Sellers Strong, and Blanche E. Wheeler to undertake excavation projects. More typically, women such as German archaeologist Johanna Mestorf, who worked as a museum curator and academic; writers such as British Egyptologist Amelia Edwards and Persianist Gertrude Bell, and French Persianist Jean Dieulafoy, who traveled and wrote about excavations during their travels; and women like Tessa Wheeler, who assisted her husband by compiling reports and raising money, were the pioneers of women archaeologists.\n\nAt the turn of the twentieth century, British women such as Eugénie Sellers Strong, who taught at the Archaeological Institute of America and British School at Rome and Margaret Murray, who lectured at University College London, began to join the ranks of university faculty. By the time of World War I, the majority of women working in the archaeology were employed in museums. Noted women archaeological curators or museum directors include Dane Maria Mogensen, Greek Semni Karouzou and Spaniards and To carve out their own niches, women typically focused on research close to where they lived or from their native cultures, or undertook studies researching household items typically ignored by men. For example, Marija Gimbutas focused on Eastern European topics even after relocating to the United States; Lanier Simmons, who wanted to study Maya culture, ended up researching closer to home because of family obligations; and Harriet Boyd focused on domestic objects and utensils. Greek Anna Apostolaki, Dane Margrethe Hald, Spaniard and Swede Agnes Geijer became experts on textiles; Dane Elisabeth Munksgaard focused on clothing, while Norwegian Charlotte Blindheim studied Viking costumes and jewelry. Pottery and art were also topics on which women focused. \n\nPrior to the 1970s, even women like Gertrude Caton-Thompson, Hilda Petrie, and Elizabeth Riefstahl, pioneers in Egyptology who had made distinguished contributions to the field, were omitted from compilations of experts working in the field. If women were mentioned at all, their roles were trivialized. During the New Deal, the Works Progress Administration sponsored excavations at mound sites in Alabama, Georgia and North Carolina, which allowed women of color and working-class women to participate in archaeological work; however, class- and race-based definitions of femininity curtailed broad participation by white women, who tended to focus on participating in amateur organizations.\n\nSue Hamilton, the director of the UCL Institute of Archaeology, noted in 2014 that 60–70% of the Institute's undergraduate and postgraduate students were women, as were the majority of its postdoctoral researchers. However, the proportion of women amongst permanent academic staff has never been more than 31%. Women are progressively further under-represented in each academic rank at the Institute: 38% of lecturers are female, 41% of senior lecturers, 17% of readers, and just 11% of professors. A 2016 study found a similar pattern in Australian universities. Whilst 41% of academic archaeologists were women, there was an imbalance in female representation in research fellowships (67%) compared to higher-ranked lecturing posts (31%). This study identified a \"two-tiered\" glass ceiling: women were less likely to obtain permanent tenure-track positions, and those that did also found it more difficult to advance to senior ranks.\n\nIn 2014, the Survey Academic Field Experiences (SAFE) surveyed nearly 700 scientists on their experiences of sexual harassment and sexual assault during fieldwork. The survey was aimed at field researchers across a range of disciplines (e.g. anthropologists, biologists), but archaeologists constituted the largest group of respondents. The survey confirmed that sexual harassment and assault were \"systemic\" problems at field sites, with 64% of respondents reporting that they had personally experienced harassment and 20% that they had personally experienced sexual assault. Women, who made up the majority of the respondents (77.5%), were significantly more likely to have experienced both and were also more likely to report that such experiences were occurred \"regularly\" or \"frequently\". The targets were almost always students or early career researchers, and the perpetrators were most likely to be more senior members of the research team, although harassment and assault from peers and members of local communities were also relatively common. The experiences reported ranged from \"inadvertent alienating behavior\" to unwanted sexual advances, sexual assault and rape. Few respondents found that there were adequate codes of conduct or reporting procedures in place. The authors of the SAFE survey emphasised the significant negative impacts that such experiences of have on victims' job satisfaction, performance, career progression, and physical and mental health.\n\nIn 1994, around 15% of the archaeologists working in the top 30 academic institutions for the field were women.\n\n\n\n\n"}
{"id": "46981202", "url": "https://en.wikipedia.org/wiki?curid=46981202", "title": "World Solar Challenge 2011", "text": "World Solar Challenge 2011\n\nThe 2011 World Solar Challenge was a race from Darwin, Northern Territory to Adelaide, South Australia in Australia. 37 vehicles were entered in the race, and the event was won by a car from Tokai University, Tokyo, Japan.\n\nResults:\n"}
