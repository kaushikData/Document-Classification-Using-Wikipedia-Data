{"id": "47059593", "url": "https://en.wikipedia.org/wiki?curid=47059593", "title": "19th century in ichnology", "text": "19th century in ichnology\n\nThe 19th century in ichnology refers to advances made between the years 1800 and 1899 in the scientific study of trace fossils, the preserved record of the behavior and physiological processes of ancient life forms, especially fossil footprints. The 19th century was notably the first century in which fossil footprints received scholarly attention. British paleontologist William Buckland performed the first true scientific research on the subject during the early .\n\nA slab of Permian-aged sandstone had been discovered in Scotland which preserved a series of unusual footprints. After acquiring the specimen, Buckland experimented with modern animals to ascertain the trackmaker and concluded that the Scottish footprints were made by tortoises. Later in the century famed advocate of evolution Thomas Henry Huxley would refute this attribution and these footprints, called \"Chelichnus\", would remain without an identified trackmaker until scientists recognized that they were actually made by an evolutionary precursor to mammals.\n\nThe 1830s also saw the discovery and investigation of unusual hand-shaped footprints from Triassic rocks in Germany that were later named \"Chirotherium\". The identification of the \"Chirotherium\" trackmaker proved elusive and suggestions from researchers included everything from monkeys to giant toads and kangaroos. \"Chirotherium\" proved to be an enduring ichnological mystery that would not be solved until long into the 20th century.\n\nSome of the most important ichnological research of the 19th century occurred across the Atlantic in the United States. Dinosaur footprints were first discovered there in 1802 when a Massachusetts farm boy stumbled upon bird-like footprints in sandstone that the local clergy mistakenly attributed to the raven that Noah released from his ark during the Biblical Flood. The region's footprints came to the attention of scholars during the mid 1830s when further bird-like dinosaur tracks were discovered elsewhere in the state. These became the lifelong preoccupation of prominent ichnologist Edward Hitchcock. Hitchcock thought the tracks were made by giant flightless birds.\n\nLate in the 19th century prisoners in Nevada discovered a major Ice Age track site at what was once an ancient lake shore. Many of the trackmakers were familiar animals like mammoths or even more modern animals like deer and wolves, but this track site also seemed to preserve the tracks of a sandal-wearing giant. The tracks received significant scholarly and popular attention like satire by Mark Twain who attributed the giant tracks to primitive Nevadan legislators. However, the true identity of the \"giant\" trackmaker was recognized by paleontologists Joseph Le Conte and Othniel Charles Marsh as a giant ground sloth, possibly of the genus \"Mylodon\".\n\n1802\n\n1820s\n\nLate 1820s\n\n1828\n\n1831\n\n1834\n\n\n1835\n\n1836\n\n1842\n\n1843\n\n1845\n\n1846\n\n1847\n\n1848\n\n1850\n\n1851\n\n1852\n\n1854\n\n1858\n\n1860\n\n1862\n\n1864\n\n1865\n\n1877\n\n1879\n\n1880\n\n1884\n\n1886\n\n1889\n\n[[:Category:1890s in paleontology|1890s]]\n\n[[1894 in paleontology|1894]]\n\n[[1895 in paleontology|1895]]\n\n\n\n[[Category:19th century in paleontology]]\n[[Category:Ichnology]]\n[[Category:Paleontology timelines|Ichnology]]"}
{"id": "5784784", "url": "https://en.wikipedia.org/wiki?curid=5784784", "title": "Bach tensor", "text": "Bach tensor\n\nIn differential geometry and general relativity, the Bach tensor is a trace-free tensor of rank 2 which is conformally invariant in dimension . Before 1968, it was the only known conformally invariant tensor that is algebraically independent of the Weyl tensor. In abstract indices the Bach tensor is given by"}
{"id": "642398", "url": "https://en.wikipedia.org/wiki?curid=642398", "title": "Bluedating", "text": "Bluedating\n\nWireless dating, Widating or Bluedating (from \"Bluetooth\") is a form of dating which makes use of mobile phone and Bluetooth technologies. Subscribers to the service enter details about themselves and about their ideal partner, as they would for other on-line dating services. When their mobile phone comes in the vicinity of that of another subscriber (a radius of about 10 meters) the phones exchange details of the two people. If there is a match, then both users are alerted and can seek each other out and directly chat using Bluetooth (bluechat). Settings can include an option which restricts alerts to subscribers who have a friend in common.\n\nA group of researchers at the Swiss Federal Institute of Technology (ETH Zurich) have developed a dating application called Bluedating running on mobile phones with Bluetooth. There is also an implementation of wireless dating called Serendipity being pioneered by MIT's Media Lab in Cambridge, Massachusetts (Reported in \"New Scientist Magazine\" 20 March 2004.).\n\n\n"}
{"id": "12889951", "url": "https://en.wikipedia.org/wiki?curid=12889951", "title": "Cash flow loan", "text": "Cash flow loan\n\nCash flow loan is a type of debt financing, in which a bank lends funds, generally for working capital, using the expected cash flows that a borrowing company generates as collateral for the loan. \n\nTo secure repayment the bank covenants a borrower on such levels and ratios as enterprise value, EBITDA, total interest coverage ratio, total debt/EBITDA, and so on. They will also take a charge over the assets of the business to provide the lender with the ability to take control of the cash flows in the event of default.\n\nIn contrast, an asset-based loan is lent against company's assets. Senior stretch loan is the mix of the two.\n\nCashflow loans are usually senior term loans or subordinated debt, being used for funding growth or financing an acquisition. \n\n\n"}
{"id": "6857991", "url": "https://en.wikipedia.org/wiki?curid=6857991", "title": "Charles Baker Adams", "text": "Charles Baker Adams\n\nCharles Baker Adams (January 11, 1814 – January 19, 1853) was an American educator and naturalist.\n\nHe was born in Dorchester, Massachusetts in 1814, the son of Charles J. Adams.\nHe graduated from Amherst College in 1834 with high honors (having transferred from Yale University to Amherst in 1832), and became an assistant to Edward Hitchcock in the Geological Survey of New York in 1836. In 1837, he became a tutor and a lecturer in geology at Amherst College. He left to become professor of chemistry and natural history at Middlebury College in 1838, remaining in that position through 1847.\n\nHe served as the first state geologist of Vermont from 1845 through 1848. In 1847, he left Middlebury to become professor of astronomy, zoology, and natural history at Amherst College, a position he retained till his death in 1853, aged 39. He visited the West Indies several times in the interest of science, and wrote on conchology. He was elected a Fellow of the American Academy of Arts and Sciences in 1849.\n\nWith the assistance of Alonzo Gray of Brooklyn, New York, he published an elementary work on geology.\n\nHe was the author of eleven numbers of \"Contributions to Conchology\", monographs on \"Stoastoma\" and \"Vitrinella\", and \"Catalogue of Shells Collected in Panama\" (New York, 1852).\n\n\n"}
{"id": "26288139", "url": "https://en.wikipedia.org/wiki?curid=26288139", "title": "Coefficient of fractional parentage", "text": "Coefficient of fractional parentage\n\nIn physics, coefficients of fractional parentage (cfp's) can be used to obtain anti-symmetric many-body states for like particles. In a jj-coupling scheme they are defined by the following relation.\n\nThe state formula_2 is normalized and totally anti-symmetric with respect to permutations of all its particles, while the state formula_3 is normalized and totally anti-symmetric with respect to all its particles.\n"}
{"id": "954231", "url": "https://en.wikipedia.org/wiki?curid=954231", "title": "Cuspy halo problem", "text": "Cuspy halo problem\n\nThe cuspy halo problem (also known as the core-cusp problem) refers to a discrepancy between the inferred dark matter density profiles of low-mass galaxies and the density profiles predicted by cosmological N-body simulations. Nearly all simulations form dark matter halos which have \"cuspy\" dark matter distributions, with density increasing steeply at small radii, while the rotation curves of most observed dwarf galaxies suggest that they have flat central dark matter density profiles (\"cores\").\n\nSeveral possible solutions to the core-cusp problem have been proposed. Many recent studies have shown that including baryonic feedback (particularly feedback from supernovae and active galactic nuclei) can \"flatten out\" the core of a galaxy's dark matter profile, since feedback-driven gas outflows produce a time-varying gravitational potential that transfers energy to the orbits of the collisionless dark matter particles. Other works have shown that the core-cusp problem can be solved outside of the most widely accepted Cold Dark Matter (CDM) paradigm: simulations with warm or self-interacting dark matter also produce dark matter cores in low-mass galaxies.\n\nAccording to W.J.G. de Blok \"The presence of a cusp in the centers of CDM halos is one of the earliest and strongest results derived from N-body cosmological simulations.\" Numerical simulations for CDM structure formation predict some structure properties that conflict with astronomical observations.\n\nThe discrepancies range from galaxies to clusters of galaxies. \"The main one that has attracted a lot of attention is the cuspy halo problem, namely that CDM models predict halos that have a high density core or have an inner profile that is too steep compared to observations.\"\n\nThe conflict between numerical simulations and astronomical observations creates numerical constraints related to the core/cusp problem. Observational constraints on halo concentrations imply the existence of theoretical constraints on cosmological parameters. According to McGaugh, Barker, and de Blok, there might be 3 basic possibilities for interpreting the halo concentration limits stated by them or anyone else:\n\nOne approach to solving the cusp-core problem in galactic halos is to consider models that modify the nature of dark matter; theorists have considered warm, fuzzy, self-interacting, and meta-cold dark matter, among other possibilities.\n\n"}
{"id": "1586849", "url": "https://en.wikipedia.org/wiki?curid=1586849", "title": "Custos Messium", "text": "Custos Messium\n\nCustos Messium (Latin for \"harvest-keeper\") was a constellation created by Jérôme Lalande in 1775 to honor Charles Messier. It was located between the constellations of Camelopardalis, Cassiopeia and Cepheus. It is no longer recognized.\n\n\n"}
{"id": "176622", "url": "https://en.wikipedia.org/wiki?curid=176622", "title": "Degrees of freedom", "text": "Degrees of freedom\n\nIn many scientific fields, the degrees of freedom of a system is the number of parameters of the system that may vary independently. For example, a point in the plane has two degrees of freedom for translation: its two coordinates; a non-infinitesimal object on the plane might have additional degrees of freedoms related to its orientation.\n\nIn mathematics, this notion is formalized as the dimension of a manifold or an algebraic variety. When \"degrees of freedom\" is used instead of \"dimension\", this usually means that the manifold or variety that models the system is only implicitly defined.\nSee:\n\n"}
{"id": "48602317", "url": "https://en.wikipedia.org/wiki?curid=48602317", "title": "Domeyko Fault", "text": "Domeyko Fault\n\nThe Domeyko Fault () or Precordilleran Fault System is a geological fault located in Northern Chile. The fault is of the strike-slip type and runns parallel to the Andes, the coast and the nearby Atacama Fault. The fault originated in the Eocene. Along its length the Domeyko Fault hosts several porphyry copper deposits including Chuquicamata, Collahuasi, El Abra, El Salvador, La Escondida and Potrerillos. The fault is named after 19th century geologist Ignacy Domeyko.\n\n"}
{"id": "6317", "url": "https://en.wikipedia.org/wiki?curid=6317", "title": "Earth (classical element)", "text": "Earth (classical element)\n\nEarth is one of the classical elements, in some systems numbering four along with air, fire, and water.\n\nEarth is one of the four classical elements in ancient Greek philosophy and science. It was commonly associated with qualities of heaviness, matter and the terrestrial world. Due to the hero cults, and chthonic underworld deities, the element of \"earth\" is also associated with the sensual aspects of both life and death in later occultism.\n\nEmpedocles of Acragas proposed four \"archai\" by which to understand the cosmos: \"fire\",\" air\", \"water\", and \"earth\". Plato believed the elements were geometric forms (the platonic solids) and he assigned the cube to the element of \"earth\" in his dialogue \"Timaeus\". Aristotle (384–322 BCE) believed \"earth\" was the heaviest element, and his theory of \"natural place\" suggested that any \"earth–laden\" substances, would fall quickly, straight down, towards the center of the \"cosmos\".\n\nIn Classical Greek and Roman myth, various goddesses \nrepresented the Earth, seasons, crops and fertility, including Demeter and Persephone; Ceres; the Horae (goddesses of the seasons), and Proserpina; and Hades (Pluto) who ruled the souls of dead in the Underworld.\n\nIn ancient Greek medicine, each of the four humours became associated with an element. Black bile was the humor identified with earth, since both were cold and dry. Other things associated with earth and black bile in ancient and medieval medicine included the season of fall, since it increased the qualities of cold and aridity; the melancholic temperament (of a person dominated by the black bile humour); the feminine; and the southern point of the compass.\n\nIn alchemy, earth was believed to be primarily dry, and secondarily cold, (as per Aristotle). Beyond those classical attributes, the chemical substance salt, was associated with earth and its alchemical symbol was a downward-pointing triangle, bisected by a horizontal line.\n\nPrithvi (Sanskrit: ', also ') is the Hindu \"earth\" and mother goddess. According to one such tradition, she is the personification of the Earth itself; according to another, its actual mother, being \"Prithvi Tattwa\", the essence of the element earth.\n\nAs \"Prithvi Mata\", or \"Mother Earth\", she contrasts with \"Dyaus Pita\", \"father sky\". In the Rigveda, \"earth\" and sky are frequently addressed as a duality, often indicated by the idea of two complementary \"half-shells.\" In addition, the element Earth is associated with Budha or Mercury who represents communication, business, mathematics and other practical matters.\n\nEarth and the other Greek classical elements were incorporated into the Golden Dawn system. Zelator is the elemental grade attributed to earth; this grade is also attributed to the Qabbalistic sphere Malkuth. The elemental weapon of earth is the Pentacle. Each of the elements has several associated spiritual beings. The archangel of earth is Uriel, the angel is Phorlakh, the ruler is Kerub, the king is Ghob, and the earth elementals (following Paracelsus) are called gnomes. Earth is considered to be passive; it is represented by the symbol for Taurus, and it is referred to the lower left point of the pentagram in the Supreme Invoking Ritual of the Pentagram. Many of these associations have since spread throughout the occult community.\n\nIt is sometimes represented by its Tattva or by a downward pointing triangle with a horizontal line through it.\n\nEarth is one of the five elements that appear in most Wiccan and Pagan traditions. Wicca in particular was influenced by the Golden Dawn system of magic, and Aleister Crowley's mysticism which was in turn inspired by the Golden Dawn.\n\nIn East Asia, metal is sometimes seen as the equivalent of \"earth\" and is represented by the White Tiger (Chinese constellation), known as 白虎 (\"Bái Hǔ\") in Chinese, \"Byakko\" in Japanese, \"Bạch Hổ\" in Vietnamese and \"Baekho\" (백호, Hanja:白虎) in Korean. \"Earth\" is represented in the Aztec religion by a house; to the Hindus, a lotus; to the Scythians, a plough; to the Greeks, a wheel; and in Christian iconography; bulls and birds.\n\n\n"}
{"id": "35016889", "url": "https://en.wikipedia.org/wiki?curid=35016889", "title": "Environmental Change and Security Program", "text": "Environmental Change and Security Program\n\nThe Environmental Change and Security Program (ECSP) is one of several programs and projects that make up the Global Resilience and Sustainability Program at the Woodrow Wilson International Center for Scholars. ECSP was founded in 1994 to study the connections among environmental, health, and population dynamics and their links to conflict, human insecurity, and foreign policy.\n\nECSP holds events and publishes research and multimedia content with the aim of connecting scholars, policymakers, the media, and practitioners. The program currently has three primary topical focus areas:\n\n\nECSP produces a series of program reports as well as the \"FOCUS\" series of short briefs on integrated population, health, and environment programs. Previous occasional publications include \"Navigating Peace: Forging New Water Partnerships\" and \"Water Stories: Expanding Opportunities in Small-Scale Water and Sanitation Projects\". The program also maintains a daily blog, \"New Security Beat\", and a YouTube channel with speaker interviews.\n\nECSP is supported by grants from the U.S. Agency for International Development, under the Health, Environment, Livelihoods, Population, and Security (HELPS) Project and the Resources for Peace Project (RFPP).\n\n\n"}
{"id": "33390716", "url": "https://en.wikipedia.org/wiki?curid=33390716", "title": "Filter theory (sociology)", "text": "Filter theory (sociology)\n\nFilter theory is a sociological theory concerning dating and mate selection. It proposes that social structure limits the number of eligible candidates for a mate. Most often, this takes place due to homogamy, as people seek to date and marry only those similar to them (characteristics that are often taken into account are age, race, social status and religion). Homogamy is the idea of marriage between spouses who share similar characteristics, where heterogamy denotes marriage between spouses of different characteristics. The idea of \"opposites attract: is heterogamous as well as the idea that one spouse has complementing, not similar characteristics to the other.\n\n\n\n\n\n"}
{"id": "55339410", "url": "https://en.wikipedia.org/wiki?curid=55339410", "title": "Georgia Tech Online Master of Science in Computer Science", "text": "Georgia Tech Online Master of Science in Computer Science\n\nGeorgia Tech Online Master of Science in Computer Science is a master of science degree offered by the College of Computing at Georgia Tech. The program is offered in partnership with Udacity and AT&T and delivered through the massive open online course format.\nThe course has received attention for offering a full master's degree program for under $7,000 that gives students from all over the world the opportunity to enroll in a top 10-ranked computer science program. The program has been recognized by the University Professional and Continuing Education Association and \"Fast Company\" for excellence and innovation.\n\nThe College of Computing at the Georgia Institute of Technology launched its online Master of Science in Computer Science degree in January 2014. The program was conceived by John P. Imlay Jr., Dean of Computing Zvi Galil, and Udacity founder Sebastian Thrun. It was offered in partnership with Udacity and AT&T and delivered through the massive open online course format, and was designed to provide a low-cost, high-quality alternative to traditional master's degree programs by delivering instructional content and academic support via the massive open online course format. Charles Isbell, a senior associate dean at the College of Computing, helped lead the effort.\n\nAs of Spring 2018, the program has 6,365 enrolled students located in 110 countries. It admits all applicants deemed to possess a reasonable chance of success—almost two-thirds of applicants to date—which is significantly higher than the university’s on-campus graduate admissions rate. From its creation in 2014 until the spring semester of 2018, the program has graduated nearly 900 students and received nearly 20,000 applications. The program has received significant media attention since its announcement in May 2013, including a front-page story in \"The New York Times\" and a segment on the PBS NewsHour series \"Rethinking Education.\"\n\nThe online master's program currently offers 30 courses and four specializations—Computational Perception and Robotics, Computing Systems, Interactive Intelligence, and Machine Learning.\n\nA study entitled “Can Online Delivery Increase Access to Education,” by John F. Kennedy School of Government at Harvard University Associate Professor Joshua Goodman, Ivan Allen College of Liberal Arts at the Georgia Institute of Technology Associate Professor Julia Melkers, and Harvard Faculty of Arts and Sciences Associate Professor Amanda Pallais, explored the structure and industry impact of the online master's program and concluded that it supplies the need of “a vast untapped market for highly affordable degrees from prestigious colleges.”\n\nDue to the online format of the program, social media has played a significant role in the development of robust student communities on social media, including a student-led community on Google+ with about 7,500 members. The program also utilizes an artificial intelligence teaching assistant called “Jill” Watson, built using IBM’s Watson platform. Jill is able to answer questions posed in natural language and assists students enrolled in the program's Knowledge-Based Artificial Intelligence course, led by Professor Ashok Goel.\n\nFormer President Barack Obama publicly praised Georgia Tech's online master's program on two occasions, as providing a model to both address the STEM worker shortage and control the costs of higher education. The program was the recipient of University Professional and Continuing Education Association’s Outstanding Program Award in the credit category. and was cited as the reason Georgia Tech was named to Fast Company’s 2017 list of Most Innovative Companies in the World, third in the education sector.\n\nOther universities have modeled similar programs after Georgia Tech's, including The University of Illinois at Urbana-Champaign, which has launched three such master’s programs in business administration, data science, and accounting. In early 2018, the University of Colorado Boulder announced a comparable program in electrical engineering. The success of the online model also led Georgia Tech to create a second program with this structure, the Online Master of Science in Analytics (OMS Analytics), which launched in 2017. In March 2018, Coursera announced six new, MOOC-based degree programs in collaboration with the University of Michigan, Arizona State University, the University of London, and Imperial College London.\n"}
{"id": "42943848", "url": "https://en.wikipedia.org/wiki?curid=42943848", "title": "Grapevine leafroll-associated virus", "text": "Grapevine leafroll-associated virus\n\nGrapevine leafroll-associated virus (GLRaV) is a name for a group of viruses infecting grapevine in the genus \"Closterovirus\".\n\nObscure mealybugs (\"Pseudococcus viburni\") feed on the phloem of vines and woody-stemmed plants, especially pear and apple trees and grape vines. Some individuals are vectors for infectious pathogens and can transmit them from plant to plant while feeding; mealybug-spread \"grapevine leafroll associated virus type III\" (GRLaV-3), in particular, has wreaked havoc among the grapes of New Zealand, reducing the crop yield of infected vineyards by up to 60%.\n\nLeafroll viruses are associated with rugose wood condition of grapevine.\n"}
{"id": "7362118", "url": "https://en.wikipedia.org/wiki?curid=7362118", "title": "Hen's Teeth and Horse's Toes", "text": "Hen's Teeth and Horse's Toes\n\nHen's Teeth and Horse's Toes (1983) is Stephen Jay Gould's third volume of collected essays reprinted from his monthly columns for \"Natural History\" magazine titled \"This view of life\". Three essays appeared elsewhere. \"Evolution as Fact and Theory\" first appeared in \"Discover\" magazine in May 1981; \"Phyletic size decrease in Hershey bars\" appeared in C. J. Rubins's \"Junk Food\", 1980; and his \"Reply to critics\", was written specifically for this volume as a commentary upon criticism of essay 16, \"The Piltdown Conspiracy\".\n\n\n\nThe book was awarded the 1983 Phi Beta Kappa Award for Science from the Phi Beta Kappa Society.\n\n"}
{"id": "21369463", "url": "https://en.wikipedia.org/wiki?curid=21369463", "title": "Highlands J virus", "text": "Highlands J virus\n\nThe Highlands J (HJ) virus is a zoonotic alphavirus native to North and South America. It maintains a natural reservoir in the songbird population of freshwater swamps (generally scrub jays and blue jays) and is transmitted by the bite of the female \"Culiseta melanura\" mosquito.\n\nThough nearly identical in structure and natural cycle to the Eastern equine encephalitis virus, it is considerably less virulent than its cousin, causing relatively mild symptoms in its primary avian reservoir and only nominally capable of transmission to mammals. A 1995 study conducted in Florida swampland found that 15% of swamp-dwelling jays tested positive for HJ antibodies, all of which were asymptomatic and in apparent good health. Recorded bird deaths from HJ infection are uncommon but not rare, and include several domestic turkeys at a commercial facility and young broiler chickens in an experimental setting.\n\nTransmission to equines or humans via mosquito is also possible, though even more rare. During the 1990-1991 St. Louis encephalitis outbreak in Missouri, 4 patients were found to be comorbidly infected with SLE and HJ, though no harmful effects were attributed to the HJ alone. A limited survey of swamp-dwelling rodents in Florida found one cotton mouse and one cotton rat with antibodies to HJ, both asymptomatic. The sole mammalian fatality attributed to HJ was a Florida horse originally diagnosed with Western equine encephalitis in 1964, which was later redetermined in 1989 to have been caused by HJ.\n\nDespite its negligible virulence in humans, it is often tested for in US domestic mosquito control programs as an indicator of fruitful conditions for other mosquito-borne zoonoses to multiply.\n\n"}
{"id": "16593931", "url": "https://en.wikipedia.org/wiki?curid=16593931", "title": "History of manufactured fuel gases", "text": "History of manufactured fuel gases\n\nThe history of gaseous fuel, important for lighting, heating, and cooking purposes throughout most of the 19th century and the first half of the 20th century, began with the development of analytical and pneumatic chemistry in the 18th century. The manufacturing process for \"synthetic fuel gases\" (also known as \"manufactured fuel gas\", \"manufactured gas\" or simply \"gas\") typically consisted of the gasification of combustible materials, usually coal, but also wood and oil. The coal was gasified by heating the coal in enclosed ovens with an oxygen-poor atmosphere. The fuel gases generated were mixtures of many chemical substances, including hydrogen, methane, carbon monoxide and ethylene, and could be burnt for heating and lighting purposes. Coal gas, for example, also contains significant quantities of unwanted sulfur and ammonia compounds, as well as heavy hydrocarbons, and so the manufactured fuel gases needed to be purified before they could be used.\n\nThe first attempts to manufacture fuel gas in a commercial way were made in the period 1795–1805 in France by Philippe LeBon, and in England by William Murdoch. Although precursors can be found, it was these two engineers who elaborated the technology with commercial applications in mind. Frederick Winsor was the key player behind the creation of the first gas utility, the London-based Gas Light and Coke Company, incorporated by royal charter in April 1812.\n\nManufactured gas utilities were founded first in England, and then in the rest of Europe and North America in the 1820s. The technology increased in scale. After a period of competition, the business model of the gas industry matured in monopolies, where a single company provided gas in a given zone. The ownership of the companies varied from outright municipal ownership, such as in Manchester, to completely private corporations, such as in London and most North American cities. Gas companies thrived during most of the nineteenth century, usually returning good profits to their shareholders, but were also the subject of many complaints over price.\n\nThe most important use of manufactured gas in the early 19th century was for gas lighting, as a convenient substitute for candles and oil lamps in the home. Gas lighting became the first widespread form of street lighting. For this use, gases that burned with a highly luminous flame, \"illuminating gases\", were needed, in contrast to other uses (e.g. as fuel) where the heat output was the main consideration. Accordingly some gas mixtures of low intrinsic luminosity, such as blue water gas, were enriched with oil to make them more suitable for street lighting. \n\nIn the second half of the 19th century, the manufactured fuel gas industry diversified out of lighting and into heat and cooking. The threat from electrical light in the later 1870s and 1880s drove this trend strongly. The gas industry did not cede the gas lighting market to electricity immediately, as the invention of the Welsbach mantle, a refractory mesh bag heated to incandescence by a mostly non-luminous flame within, dramatically increased the efficiency of gas lighting. Acetylene was also used from about 1898 for gas cooking and gas lighting (see Carbide lamp) on a smaller scale, although its use too declined with the advent of electric lighting, and LPG for cooking. Other technological developments in the late nineteenth century include the use of water gas and machine stoking, although these were not universally adopted.\n\nIn the 1890s, pipelines from natural gas fields in Texas and Oklahoma were built to Chicago and other cities, and natural gas was used to supplement manufactured fuel gas supplies, eventually completely displacing it. Gas ceased to be manufactured in North America by 1966 (with the exception of Indianapolis and Honolulu), while it continued in Europe until the 1980s. \"Manufactured gas\" is again being evaluated as a fuel source, as energy utilities look towards coal gasification once again as a potentially cleaner way of generating power from coal, although nowadays such gases are likely to be called \"synthetic natural gas\".\n\nPneumatic chemistry developed in the eighteenth century with the work of scientists such as Stephen Hales, Joseph Black, Joseph Priestley, and Antoine-Laurent Lavoisier, and others. Until the eighteenth century, gas was not recognized as a separate state of matter. Rather, while some of the mechanical properties of gases were understood, as typified by Robert Boyle's experiments and the development of the air pump, their chemical properties were not. Gases were regarded in keeping the Aristotelean tradition of four elements as being air, one of the four fundamental elements. The different sorts of airs, such as putrid airs or inflammable air, were looked upon as atmospheric air with some impurities, much like muddied water.\n\nAfter Joseph Black realized that carbon dioxide was in fact a different sort of gas altogether from atmospheric air, other gases were identified, including hydrogen by Henry Cavendish in 1766. Alessandro Volta expanded the list with his discovery of methane in 1776. It had also been known for a long time that inflammable gases could be produced from most combustible materials, such as coal and wood, through the process of distillation. Stephen Hales, for example, had written about the phenomenon in the \"Vegetable Staticks\" in 1722. In the last two decades of the eighteenth century, as more gases were being discovered and the techniques and instruments of pneumatic chemistry became more sophisticated, a number of natural philosophers and engineers thought about using gases in medical and industrial applications. One of the first such uses was ballooning beginning in 1783, but other uses soon followed.\n\nOne of the results of the ballooning craze of 1783–1784 was the first implementation of lighting by manufactured gas. A professor of natural philosophy at the University of Louvain Jan Pieter Minckeleers and two of his colleagues were asked by their patron, the Duke of Arenberg, to investigate ballooning. They did so, building apparatus to generate lighter than air inflammable gases from coal and other inflammable substances. In 1785 Minckeleers used some of this apparatus to gasify coal to light his lecture hall at the university. He did not extend gas lighting much beyond this, and when he was forced to flee Leuven during the Brabant Revolution, he abandoned the project altogether.\n\nPhilippe LeBon was a French civil engineer working in the public engineering corps who became interested while at university in distillation as an industrial process for the manufacturing of materials such as tar and oil. He graduated from the engineering school in 1789, and was assigned to Angoulême. There, he investigated distillation, and became aware that the gas produced in the distillation of wood and coal could be useful for lighting, heating, and as an energy source in engines. He took out a patent for distillation processes in 1794, and continued his research, eventually designing a distillation oven known as the \"thermolamp\". He applied for and received a patent for this invention in 1799, with an addition in 1801. He launched a marketing campaign in Paris in 1801 by printing a pamphlet and renting a house where he put on public demonstrations with his apparatus. His goal was to raise sufficient funds from investors to launch a company, but he failed to attract this sort of interest, either from the French state or from private sources. He was forced to abandon the project and return to the civil engineering corps. Although he was given a forest concession by the French government to experiment with the manufacture of tar from wood for naval use, he never succeed with the thermolamp, and died in uncertain circumstances in 1805.\n\nAlthough the thermolamp received some interest in France, in Germany interest was the greatest. A number of books and articles were written on the subject in the period 1802–1812. There were also thermolamps designed and built in Germany, the most important of which were by Zachaus Winzler, an Austrian chemist who ran a saltpetre factory in Blansko. Under the patronage of the aristocratic zu Salm family, he built a large one in Brno. He moved to Vienna to further his work. The thermolamp, however, was used primarily for making charcoal and not for the production of gases.\n\nWilliam Murdoch (sometimes Murdock) (1754–1839) was an engineer working for the firm of Boulton & Watt, when, while investigating distillation processes sometime in 1792–1794, he began using coal gas for illumination. He was living in Redruth in Cornwall at the time, and made some small scale experiments with lighting his own house with coal gas. He soon dropped the subject until 1798, when he moved to Birmingham to work at Boulton & Watt's home base of Soho. Boulton & Watt then instigated another small-scale series of experiments. With ongoing patent litigation and their main business of steam engines to attend to, the subject was dropped once again. Gregory Watt, James Watt's second son, while traveling in Europe saw Lebon's demonstrations and wrote a letter to his brother, James Watt Jr., informing him of this potential competitor. This prompted James Watt Jr. to begin a gaslight development program at Boulton & Watt that would scale up the technology and lead to the first commercial applications of gaslight.\n\nAfter an initial installation at the Soho Foundry in 1803–1804, Boulton & Watt prepared an apparatus for the textile firm of Philips & Lee in Salford near Manchester in 1805–1806. This was to be their only major sale until late 1808. George Augustus Lee was a major motivating force behind the development of the apparatus. He had an avid interest in technology, and had introduced a series of technological innovations at the Salford Mill, such as iron frame construction and steam heating. He continued to encourage the development of gaslight technology at Boulton & Watt.\n\nThe first company to provide manufactured gas to consumer as a utility was the London-based Gas Light and Coke Company. It was founded through the efforts of a German émigré, Frederick Winsor, who had witnessed Lebon's demonstrations in Paris. He had tried unsuccessfully to purchase a thermolamp from Lebon, but remained taken with the technology, and decided to try his luck, first in his hometown of Brunswick, and then in London in 1804. Once in London, Winsor began an intense campaign to find investors for a new company that would manufacture gas apparatus and sell gas to consumers. He was successful in finding investors, but the legal form of the company was a more difficult problem. By the Bubble Act of 1720, all joint-stock companies above a certain number of shareholders in England needed to receive a royal charter to incorporate, which meant that an act of Parliament was required.\n\nWinsor waged his campaign intermittently to 1807, when the investors constituted a committee charged with obtaining an act of Parliament. They pursued this task over the next three years, encountering adversities en route, the most important of which was the resistance by Boulton & Watt in 1809. In that year, the committee made a serious attempt to get the House of Commons to pass a bill empowering the king to grant the charter, but Boulton & Watt felt their gaslight apparatus manufacturing business was threatened and mounted an opposition through their allies in Parliament. Although a parliamentary committee recommended approval, it was defeated on the third reading.\n\nThe following year, the committee tried again, succeeding with the acquiescence of Boulton & Watt because they renounced all powers to manufacture apparatus for sale. The act required that the company raise £100,000 before they could request a charter, a condition it took the next two years to fill. George III granted the charter in 1812.\n\nFrom 1812 to approximately 1825, manufactured gas was predominantly an English technology. A number of new gas utilities were founded to serve London and other cities in the UK after 1812. Liverpool, Exeter, and Preston were the first in 1816. Others soon followed; by 1821, no town with population over 50,000 was without gaslight. Five years later, there were only two towns over 10,000 that were without gaslight.\nIn London, the growth of gaslight was rapid. New companies were founded within a few years of the Gas Light and Coke Company, and a period of intense competition followed as companies competed for consumers on the boundaries of their respective zones of operations. Frederick Accum, in the various editions of his book on gaslight, gives a good sense of how rapidly the technology spread in the capital. In 1815, he wrote that there were 4000 lamps in the city, served by 26 miles (42 km) of mains. In 1819, he raised his estimate to 51,000 lamps and 288 miles (463 km) of mains. Likewise, there were only two gasworks in London in 1814, and by 1822, there were seven and by 1829, there were 200 companies. The government did not regulate the industry as a whole until 1816, when an act of Parliament created and a post of inspector for gasworks, the first holder of which was Sir William Congreve. Even then, no laws were passed regulating the entire industry until 1847, although a bill was proposed in 1822, which failed due to opposition from gas companies. The charters approved by Parliament did, however, contain various regulations such as how the companies could break up the pavement, etc.\n\nFrance's first gas company was also promoted by Frederick Winsor after he had to flee England in 1814 due to unpaid debts and tried to found another gas company in Paris, but it failed in 1819. The government was also interested in promoting the industry, and in 1817 commissioned Chabrol de Volvic to study the technology and build a prototype plant, also in Paris. The plant provided gas for lighting the hôpital Saint Louis, and the experiment was judged successful. King Louis XVIII then decided to give further impulse to the development of the French industry by sending people to England to study the situation there, and to install gaslight at a number of prestigious buildings, such as the Opera building, the national library, etc. A public company was created for this purpose in 1818. Private companies soon followed, and by 1822, when the government moved to regulate the industry, there were four in operation in the capital. The regulations passed then prevented the companies from competing, and Paris was effectively divided between the various companies operating as monopolies in their own zones.\n\nGaslight spread to other European countries. In 1817, a company was founded in Brussels by P. J. Meeus-Van der Maelen, and began operating the following year. By 1822, there were companies in Amsterdam and Rotterdam using English technology. In Germany, gaslight was used on a small scale from 1816 onwards, but the first gaslight utility was founded by English engineers and capital. In 1824, the Imperial Continental Gas Association was founded in London to establish gas utilities in other countries. Sir William Congreve, one if its leaders, signed an agreement with the government in Hanover, and the gas lamps were used on streets for the first time in 1826.\n\nGaslight was first introduced to the US in 1816 in Baltimore by Rembrandt and Rubens Peale, who lit their museum with gaslight, which they had seen on a trip to Europe. The brothers convinced a group of wealthy people to back them in a larger enterprise. The local government passed a law allowing the Peales and their associates to lay mains and light the streets. A company was incorporated for this purpose in 1817. After some difficulties with the apparatus and financial problems, the company hired an English engineer with experience in gaslight. It began to flourish, and by the 1830s, the company was supplying gas to 3000 domestic customers and 100 street lamps. Companies in other cities followed, the second being Boston Gas Light in 1822 and New York Gas Light Company in 1825. A gas works was built in Philadelphia in 1835.\n\nGas lighting was one of the most debated technologies of the first industrial revolution. In Paris, as early as 1823, controversy forced the government to devise safety standards (Fressoz, 2007). The residues produced from distilled coal were either drained into rivers or stored in basins which polluted (and still pollute) the soil.\n\nCase law in the UK and the US clearly held though, the construction and operation of a gas-works was not the creation of a public nuisance \"in se\", due to the reputation of gas-works as highly undesirable neighbors, and the noxious pollution known to issue from such, especially in the early days of manufactured gas, gas-works were on extremely short notice from the courts that (detectable) contamination outside of their grounds – especially in residential districts – would be severely frowned upon. Indeed, many actions for the abatement of nuisances brought before the courts did result in unfavorable verdicts for gas manufacturers – in one study on early environmental law, actions for nuisance involving gas-works resulted in findings for the plaintiffs 80% of the time, compared with an overall plaintiff victory rate of 28.5% in industrial nuisance cases.\n\nInjunctions both preliminary and permanent could and were often issued in cases involving gas works. For example, the ill reputation of gas-works became so well known that in \"City of Cleveland vs. Citizens' Gas Light Co.\", 20 N. J. Eq. 201, a court went so far as to enjoin a \"future\" gas-works not yet even built – preventing it from causing \"annoying and offensive vapours and odors\" in the first place. The injunction not only regulated the gas manufacturing process – forbidding the use of lime purification – but also provided that if nuisances of any sort were to issue from the works – a permanent injunction forbidding the production of gas would issue from the court. Indeed, as the Master of the Rolls, Lord Langdale, once remarked in his opinion in \"Haines v. Taylor\", 10 Beavan 80, that \"I have been rather astonished to hear the effects of gas works treated as nothing...every man, in these days, must have sufficient experience, to enable him to come to the conclusion, that, whether a nuisance or not, a gas manufactory is a very disagreeable thing. Nobody can doubt that the volatile products which arise from the distillation of coal are extremely offensive. It is quite contrary to common experience to say they are not so...every man knows it.\"\nHowever, as time went by, gas-works began to be seen as more as a double-edged sword – and eventually as a positive good, as former nuisances were abated by technological improvements, and the full benefits of gas became clear. There were several major impetuses that drove this phenomenon:\n\n\nBoth the era of consolidation of gas-works through high-pressure distribution systems (1900s–1930s) and the end of the era of manufactured gas (1955–1975) saw gas-works being shut down due to redundancies. What brought about the end of manufactured gas was that pipelines began to be built to bring natural gas directly from the well to gas distribution systems. Natural gas was superior to the manufactured gas of that time, being cheaper – extracted from wells rather than manufactured in a gas-works – more user friendly – coming from the well requiring little, if any, purification – and safer – due to the lack of carbon monoxide in the distributed product. Upon being shut down, few former manufactured gas plant sites were brought to an acceptable level of environmental cleanliness to allow for their re-use, at least by contemporary standards. In fact, many were literally abandoned in place, with process wastes left \"in situ\", and never adequately disposed of.\n\nAs the wastes produced by former manufactured gas plants were persistent in nature, they often (as of 2009) still contaminate the site of former manufactured gas plants: the waste causing the most concern today is primarily coal tar (mixed long-chain aromatic and aliphatic hydrocarbons, a byproduct of coal carbonization), while \"blue billy\" (a noxious byproduct of lime purification contaminated with cyanides) as well as other lime and coal tar residues are regarded as lesser, though significant environmental hazards. Some former manufactured gas plants are owned by gas utilities today, often in an effort to prevent contaminated land from falling into public use, and inadvertently causing the release of the wastes therein contained. Others have fallen into public use, and without proper reclamation, have caused – often severe – health hazards for their users. When and where necessary, former manufactured gas plants are subject to environmental remediation laws, and can be subject to legally mandated cleanups.\n\nThe basic design of gaslight apparatus was established by Boulton & Watt and Samuel Clegg in the period 1805–1812. Further improvements were made at the Gas Light and Coke Company, as well as by the growing number of gas engineers such as John Malam and Thomas Peckston after 1812. Boulton & Watt contributed the basic design of the retort, condenser, and gasometer, while Clegg improved the gasometer and introduced lime purification and the hydraulic main, another purifier.\n\nThe retort bench was the construction in which the retorts were located for the carbonization (synonymous with pyrolysis) of the coal feedstock and the evolution of coal gas. Over the years of manufactured gas production, advances were made that turned the retort-bench from little more than coal-containing iron vessels over an open fire to a massive, highly efficient, partially automated, industrial-scale, capital-intensive plant for the carbonization of large amounts of coal. Several retort benches were usually located in a single \"retort house\", which there was at least one of in every gas works.\n\nInitially, retort benches were of many different configurations due to the lack of long use and scientific and practical understanding of the carbonization of coal. Some early retorts were little more than iron vessels filled with coal and thrust upon a coal fire with pipes attached to their top ends. Though practical for the earliest gas works, this quickly changed once the early gas-works served more than a few customers. As the size of such vessels grew – the need became apparent for efficiency in refilling retorts – and it was apparent that filling one-ended vertical retorts was easy; removing the coke and residues from them after the carbonization of coal was far more difficult. Hence, gas retorts transitioned from vertical vessels to horizontal tubular vessels.\n\nRetorts were usually made of cast iron during the early days. Early gas engineers experimented extensively with the best shape, size, and setting. No one form of retort dominated, and many different cross-sections remained in use. After the 1850s, retorts generally became made of fire clay due to greater heat retention, greater durability, and other positive qualities. Cast-iron retorts were used in small gas works, due to their compatibility with the demands there, with the cast-iron retort's lower cost, ability to heat quickly to meet transient demand, and \"plug and play\" replacement capabilities. This outweighed the disadvantages of shorter life, lower temperature margins, and lack of ability to be manufactured in non-cylindrical shapes. Also, general gas-works practice following the switch to fire-clay retorts favored retorts that were shaped like a \"D\" turned 90 degrees to the left, sometimes with a slightly pitched bottom section.\n\nWith the introduction of the fire-clay retort, higher heats could be held in the retort benches, leading to faster and more complete carbonization of the coal. As higher heats became possible, advanced methods of retort bench firing were introduced, catalyzed by the development of the open hearth furnace by Siemens, circa 1855–1870, leading to a revolution in gas-works efficiency.\n\nSpecifically, the two major advances were:\n\nThese two advances turned the old, \"directly fired\" retort bench into the advanced, \"indirectly fired\", \"regenerative\" or \"generative\" retort bench, and lead coke usage within the retort benches (in the larger works) to drop from upwards of 40% of the coke made by the retorts to factors as low as 15% of the coke made by the retorts, leading to an improvement in efficiency of an order of magnitude. These improvements imparted an additional capital cost to the retort bench, which caused them to be slowly incorporated in the smaller gas-works, if they were incorporated at all.\n\nFurther increases in efficiency and safety were seen with the introduction of the \"through\" retort, which had a door at front and rear. This provided for greater efficiency and safety in loading and unloading the retorts, which was a labor-intensive and often dangerous process. Coal could now be pushed out of the retort – rather than pulled out of the retort. One interesting modification of the \"through\" retort was the \"inclined\" retort – coming into its heyday in the 1880s – a retort set on a moderate incline, where coal was poured in at one end, and the retort sealed; following pyrolysis, the bottom was opened and the coke poured out by gravity. This was adopted in some gas-works, but the savings in labor were often offset by the uneven distribution and pyrolysis of the coal as well as clumping problems leading to failure of the coal to pour out of the bottom following pyrolysis that were exacerbated in certain coal types. As such, inclined retorts were rendered obsolescent by later advances, including the retort-handling machine and the vertical retort system.\n\nSeveral advanced retort-house appliances were introduced for improved efficiency and convenience. The compressed-air or steam-driven clinkering pick was found to be especially useful in removing clinker from the primary combustion area of the indirectly fired benches – previously clinkering was an arduous and time-consuming process that used large amounts of retort house labor. Another class of appliances introduced were apparatuses – and ultimately, machines – for retort loading and unloading. Retorts were generally loaded by using an elongated scoop, into which the coal was loaded – a gang of men would then lift the scoop and ram it into the retort. The coal would then be raked by the men into a layer of even thickness and the retort sealed. Gas production would then ensue – and from 8 – 12 hours later, the retort would be opened, and the coal would be either pulled (in the case of \"stop-ended\" retorts) or pushed (in the case of \"through\" retorts) out of the retort. Thus, the retort house had heavy manpower requirements – as many men were often required to bear the coal-containing scoop and load the retort.\n\n\"(TBD: Brief description of advanced retort loading apparatus; more detailed description of retort-handling machine.)\"\n\n\"Coming soon: The introduction of the coke-oven system, and, finally, the vertical retort system.\"\n\nFrom the retort, the gas would first pass through a tar/water \"trap\" (similar to a trap in plumbing) called a hydraulic main, where a considerable fraction of coal tar was given up and the gas was significantly cooled. Then, it would pass through the main out of the retort house into an atmospheric or water-cooled condenser, where it would be cooled to the temperature of the atmosphere or the water used. At this point, it enters the exhauster house and passes through an \"exhauster\", an air pump which maintains the hydraulic mains and, consequently, the retorts at a negative pressure (with a zero pressure being atmospheric). It would then be washed in a \"washer\" by bubbling it through water, to extract any remaining tars. After this, it would enter a purifier. The gas would then be ready for distribution, and pass into a gasholder for storage.\n\nWithin each retort-house, the retort benches would be lined up next to one another in a long row. Each retort had a loading and unloading door. Affixed to each door was an ascension pipe, to carry off the gas as it was evolved from the coal within. These pipes would rise to the top of the bench where they would terminate in an inverted \"U\" with the leg of the \"U\" disappearing into a long, trough-shaped structure (with a covered top) made of cast iron called a hydraulic main that was placed atop the row of benches near their front edge. It ran continuously along the row of benches within the retort house, and each ascension pipe from each retort descended into it.\n\nThe hydraulic main had a level of a liquid mixture of (initially) water, but, following use, also coal tar, and ammoniacal liquor. Each retort ascension pipe dropped under the water level by at least a small amount, perhaps by an inch, but often considerably more in the earlier days of gas manufacture. The gas evolved from each retort would thus bubble through the liquid and emerge from it into the void above the liquid, where it would mix with the gas evolved from the other retorts and be drawn off through the foul main to the condenser.\n\nThere were two purposes to the liquid seal: first, to draw off some of the tar and liquor, as the gas from the retort was laden with tar, and the hydraulic main could rid the gas of it, to a certain degree; further tar removal would take place in the condenser, washer/scrubber, and the tar extractor. Still, there would be less tar to deal with later. Second, the liquid seal also provided defense against air being drawn into the hydraulic main: if the main had no liquid within, and a retort was left open with the pipe not shut off, and air were to combine with the gas, the main could explode, along with nearby benches.\n\nHowever, after the early years of gas, research proved that a very deep, excessive seal on the hydraulic main threw a backpressure upon all the retorts as the coal within was gasifying, and this had deleterious consequences; carbon would likely deposit onto the insides of retorts and ascension pipes; and the bottom layer of tar with which the gas would have to travel through in a deeply sealed main robbed the gas of some of its illuminating value. As such, after the 1860s, hydraulic mains were run at around 1 inch of seal, and no more.\n\nLater retort systems (many types of vertical retorts, especially ones in continuous operation) which had other anti-oxygen safeguards, such as check valves, etc., as well as larger retorts, often omitted the hydraulic main entirely and went straight to the condensers – as other apparatus and buildings could be used for tar extraction, the main was unnecessary for these systems.\n\nAir Cooled Condensers\n\nCondensers were either air cooled or water cooled. Air cooled condensers were often made up from odd lengths of pipe and connections. The main varieties in common use were classified as follows:\n(a) Horizontal types \n\n(b) Vertical types\n\n(c) Annular types \n\n(d) The battery condenser.\n\nThe horizontal condenser was an extended foul main with the pipe in a zigzag pattern from end to end of one of the retort-house walls. Flange connections were essential as blockages from naphthalene or pitchy deposits were likely to occur. The condensed liquids flowed down the sloping pipes in the same direction as the gas. As long as gas flow was slow, this was an effective method for the removal of naphthalene. Vertical air condensers had gas and tar outlets.\n\nThe annular atmospheric condenser was easier to control with respect to cooling rates. The gas in the tall vertical cylinders was annular in form and allowed an inside and outside surface to be exposed to cooling air. The diagonal side pipes conveyed the warm gas to the upper ends of each annular cylinder. Butterfly valves or dampers were fitted to the top of each vertical air pipe, so that the amount of cooling could be regulated.\n\nThe battery condenser was a long and narrow box divided internally by baffle-plates which cause the gas to take a circuitous course. The width of the box was usually about 2 feet, and small tubes passed from side to side form the chief cooling surface. The ends of these tubes were left open to allow air to pass through. The obstruction caused by the tubes played a role in breaking up and throwing down the tars suspended in the gas.\n\nTypically, plants using cast-iron mains and apparatus allowed 5 square feet of superficial area per 1,000 cubic feet of gas made per day. This could be slightly reduced when wrought iron or mild steel was used.\n\nWater Cooled Condensers\n\nWater cooled condensers were almost constructed from riveted mild steel plates (which form the outer shell) and steel or wrought-iron tubes. There were two distinct types used: \n\n(a) Multitubular condensers. \n(b) Water-tube condensers.\n\nUnless the cooling water was exceptionally clean, the water-tube condenser was preferred. The major difference between the multitubular and water-tube condenser was that in the former the water passed outside and around the tubes which carry the hot gas, and in the latter type, the opposite was the case. Thus when only muddy water pumped from rivers or canals was available; the water-tube condenser was used. When the incoming gas was particularly dirty and contained an undesirable quantity of heavy tar, the outer chamber was liable to obstruction from this cause.\n\nThe hot gas was saturated with water vapor and accounted for the largest portion of the total work of condensation. Water vapor has to lose large quantities of heat, as did any liquefiable hydrocarbon. Of the total work of condensation, 87% was accounted for in removing water vapor and the remainder was used to cool permanent gases and to condensing liquefiable hydrocarbon.\n\nAs extremely finely divided particles were also suspended in the gas, it was impossible to separate the particulate matter solely by a reduction of vapor pressure. The gas underwent processes to remove all traces of solid or liquid matter before it reached the wet purification plant. Centrifugal separators, such as the Colman Cyclone apparatus were utilized for this process in some plants.\n\nThe hydrocarbon condensates removed in the order heavy tars, medium tars and finally light tars and oil fog. About 60-65% of the tars would be deposited in the hydraulic main. Most of this tar was heavy tars. The medium tars condensed out during the passage of the products between the hydraulic and the condenser. The lighter tars oil fog would travel considerably further.\n\nIn general, the temperature of the gas in the hydraulic main varies between 140-160 F. The constituents most liable to be lost were benzene, toluene, and, to some extent, xylene, which had an important effect on the ultimate illuminating power of the gas. Tars were detrimental for the illuminating power and were isolated from the gas as rapidly as possible.\n\nMaintained hydraulic main and condenser at negative pressure.\n\nThere were several types of exhausters.\n\n\nFinal extractions of minor deleterious fractions. \nScrubbers which utilized water were designed in the 25 years after the foundation of the industry. It was discovered that the removal of ammonia from the gas depended upon the way in which the gas to be purified was contacted by water. This was found to be best performed by the Tower Scrubber. This scrubber consisted of a tall cylindrical vessel, which contained trays or bricks which were supported on grids. The water, or weak gas liquor, trickled over these trays, thereby keeping the exposed surfaces thoroughly wetted. The gas to be purified was run through the tower to be contacted with the liquid. In 1846 George Lowe patented a device with revolving perforated pipes for supplying water or purifying liquor. At a later date, the Rotary Washer Scrubber was introduced by Paddon, who used it at Brighton about 1870. This prototype machine was followed by others of improved construction ; notably by Kirkham, Hulett, and Chandler, who introduced the well-known Standard Washer Scrubber, Holmes, of Huddersfield, and others. The Tower Scrubber and the Rotary Washer Scrubber made it possible to completely remove ammonia from the gas.\n\nCoal gas coming directly from the bench was a noxious soup of chemicals, and removal of the most deleterious fractions was important, for improving the quality of the gas, for preventing damage to equipment or premises, and for recovering revenues from the sale of the extracted chemicals. Several offensive fractions being present in a distributed gas might lead to problems – Tar in the distributed gas might gum up the pipes (and could be sold for a good price), ammoniacal vapours in the gas might lead to corrosion problems (and the extracted ammonium sulfate was a decent fertilizer), naphthalene vapours in the gas might stop up the gas-mains, and even carbon dioxide in the gas was known to decrease illumination; thus various facilities within the gas-works were tasked with the removal of these deleterious effluents. But these do not compare to the most hazardous contaminant in the raw coal gas: the sulfuret of hydrogen (hydrogen sulfide, HS). This was regarded as utterly unacceptable for several reasons:\n\nAs such, the removal of the sulfuret of hydrogen was given the highest level of priority in the gas-works. A special facility existed to extract the sulfuret of hydrogen – known as the purifier. The purifier was arguably the most important facility in the gas-works, if the retort-bench itself is not included.\n\nOriginally, purifiers were simple tanks of lime-water, also known as cream or milk of lime, where the raw gas from the retort bench was bubbled through to remove the sulfuret of hydrogen. This original process of purification was known as the \"wet lime\" process. The lime residue left over from the \"wet lime\" process was one of the first true \"toxic wastes\", a material called \"blue billy\". Originally, the waste of the purifier house was flushed into a nearby body of water, such as a river or a canal. However, after fish kills, the nauseating way it made the rivers stink, and the truly horrendous stench caused by exposure of residuals if the river was running low, the public clamoured for better means of disposal. Thus it was piled into heaps for disposal. Some enterprising gas entrepreneurs tried to sell it as a weed-killer, but most people wanted nothing to do with it, and generally, it was regarded as waste which was both smelly and poisonous, and gas-works could do little with, except bury. But this was not the end of the \"blue billy\", for after burying it, rain would often fall upon its burial site, and leach the poison and stench from the buried waste, which could drain into fields or streams. Following countless fiascoes with \"blue billy\" contaminating the environment, a furious public, aided by courts, juries, judges, and masters in chancery, were often very willing to demand that the gas-works seek other methods of purification – and even pay for the damages caused by their old methods of purification.\n\nThis led to the development of the \"dry lime\" purification process, which was less effective than the \"wet lime\" process, but had less toxic consequences. Still, it was quite noxious. Slaked lime (calcium hydroxide) was placed in thick layers on trays which were then inserted into a square or cylinder-shaped purifier tower which gas was then passed through, from the bottom to the top. After the charge of slaked lime had lost most of its absorption effectiveness, the purifier was then shut off from the flow of gas, and either was opened, or air was piped in. Immediately, the sulfur-impregnated slaked lime would react with the air to liberate large concentrations of sulfuretted hydrogen, which would then billow out of the purifier house, and make the gas-works, and the district, stink of sulfuretted hydrogen. Though toxic in sufficient concentrations or long exposures, the sulfuret was generally just nauseating for short exposures at moderate concentrations, and was merely a health hazard (as compared to the outright danger of \"blue billy\") for the gas-works employees and the neighbors of the gas-works. The sulfuretted lime was not toxic, but not greatly wanted, slightly stinking of the odor of the sulfuret, and was spread as a low grade fertilizer, being impregnated with ammonia to some degree. The outrageous stinks from many gas-works led many citizens to regard them as public nuisances, and attracted the eye of regulators, neighbors, and courts.\n\nThe \"gas nuisance\" was finally solved by the \"iron ore\" process. Enterprising gas-works engineers discovered that bog iron ore could be used to remove the sulfuretted hydrogen from the gas, and not only could it be used for such, but it could be used in the purifier, exposed to the air, whence it would be rejuvenated, without emitting noxious sulfuretted hydrogen gas, the sulfur being retained in the iron ore. Then it could be reinserted into the purifier, and reused and rejuvenated multiple times, until it was thoroughly embedded with sulfur. It could then be sold to the sulfuric acid works for a small profit. Lime was sometimes still used after the iron ore had thoroughly removed the sulfuret of hydrogen, to remove carbonic acid (carbon dioxide, CO), the bisulfuret of carbon (carbon disulfide, CS), and any ammonia still aeroform after its travels through the works. But it was not made noxious as before, and usually could fetch a decent rate as fertilizer when impregnated with ammonia. This finally solved the greatest pollution nuisances of the gas-works, but still lesser problems remained – not any that the purifier house could solve, though.\n\nPurifier designs also went through different stages throughout the years.\n\n Gasholders were constructed of a variety of materials, brick, stone, concrete, steel, or wrought iron. The holder or floating vessel is the storage reservoir for the gas, and it serves the purpose of equalizing the distribution of the gas under pressure, and ensures a continuity of supply, while gas remains in the holder. They are cylindrical like an inverted beaker and work up and down in the tank. In order to maintain a true vertical position, the vessel has rollers which work on guide-rails attached to the tank sides and to the columns surrounding the holder. \nGasholders may be either single or telescopic in two or more lifts. When it is made in the telescopic form, its capacity could be increased to as much as four times the capacity of the single-lift holder for equal dimensions of tank. The telescopic versions were found to be useful as they conserved ground space and capital.\n\nThe gasworks had numerous small appertunances and facilities to aid with divers gas management tasks or auxiliary services.\n\nAs the years went by, boilers (for the raising of steam) became extremely common in most gas-works above those small in size; the smaller works often used gas-powered internal combustion engines to do some of the tasks that steam performed in larger workings.\n\nSteam was in use in many areas of the gasworks, including:\nFor the operation of the exhauster;\nFor scurfing of pyrolysis char and slag from the retorts and for clinkering the producer of the bench;\nFor the operation of engines used for conveying, compressing air, charging hydraulics, or the driving of dynamos or generators producing electric current;\nTo be injected under the grate of the producer in the indirectly fired bench, so as to prevent the formation of clinker, and to aid in the water-gas shift reaction, ensuring high-quality secondary combustion;\nAs a reactant in the (carburetted) water gas plant, as well as driving the equipment thereof, such as the numerous blowers used in that process, as well as the oil spray for the carburettor;\nFor the operation of fire, water, liquid, liquor, and tar pumps;\nFor the operation of engines driving coal and coke conveyor-belts;\nFor clearing of chemical obstructions in pipes, including naphthalene & tar as well as general cleaning of equipment;\nFor heating cold buildings in the works, for maintaining the temperature of process piping, and preventing freezing of the water of the gasholder, or congealment of various chemical tanks and wells.\n\nHeat recovery appliances could also be classed with boilers. As the gas industry applied scientific and rational design principles to its equipment, the importance of thermal management and capture from processes became common. Even the small gasworks began to use heat-recovery generators, as a fair amount of steam could be generated for \"free\" simply by capturing process thermal waste using water-filled metal tubing inserted into a strategic flue.\n\nAs the electric age came into being, the gas-works began to use electricity – generated on site – for many of the smaller plant functions previously performed by steam or gas-powered engines, which were impractical and inefficient for small, sub-horsepower uses without complex and failure-prone mechanical linkages. As the benefits of electric illumination became known, sometimes the progressive gasworks diversified into electric generation as well, as coke for steam-raising could be had on-site at low prices, and boilers were already in the works.\n\nAccording to Meade, the gasworks of the early 20th century generally kept on hand several weeks of coal. This amount of coal could cause major problems, because coal was liable to spontaneous combustion when in large piles, especially if they were rained upon, due to the protective dust coating of the coal being washed off, exposing the full porous surface area of the coal of slightly to highly activated carbon below; in a heavy pile with poor heat transfer characteristics, the heat generated could lead to ignition. But storage in air-entrained confined spaces was not highly looked upon either, as residual heat removal would be difficult, and fighting a fire if it was started could result in the formation of highly toxic carbon monoxide through the water-gas reaction, caused by allowing water to pass over extremely hot carbon (HO + C = H + CO), which would be dangerous outside, but deadly in a confined space.\n\nCoal storage was designed to alleviate this problem. Two methods of storage were generally used; underwater, or outdoor covered facilities. To the outdoor covered pile, sometimes cooling appurtenances were applied as well; for example, means to allow the circulation of air through the depths of the pile and the carrying off of heat. Amounts of storage varied, often due to local conditions. Works in areas with industrial strife often stored more coal, while nations whose proletariat was under \"control\" stored less. Other variables included national security; for instance, the gasworks of Tegel in Berlin had some 1 million tons of coal (6 months of supply) in gigantic underwater bunker facilities half a mile long (Meade 2e, p. 379); as Berlin is on the North German Plain, perhaps this was due to what happened to Paris in the Franco-Prussian War of 1870–1871.\n\nMachine stoking or power stoking was used to replace labor and minimize disruptions due to labor disputes. Each retort typically required two sets of three stokers. Two of the stokers were required to lift the point of the scoop into the retort, while the third would push it in and turn it over. Coal would be introduced from each side of the retort. The coke produced would be removed from both sides also. Gangs of stokers worked 12-hour shifts, although the labor was not continuous. The work was also seasonal, with extra help being required in the winter time. Machine stoking required more uniform placement of the retorts. Increasing cost of labor increased the profit margin in experimenting with and instituting machine stoking.\n\nThe chemical industries demanded coal tar, and the gas-works could provide it for them; and so the coal tar was stored on site in large underground tanks. As a rule, these were single wall metal tanks – that is, if they were not porous masonry. In those days, underground tar leaks were seen as merely a waste of tar; out of sight was truly out of mind; and such leaks were generally addressed only when the loss of revenue from leaking tar \"wells\", as these were sometimes called, exceeded the cost of repairing the leak. This practice of bygone days has caused representatives of present-day gas utilities to dive under tables and utter minced oaths when terms like \"purportedly responsible party\", \"BTEX\", \"aquifer plume\", or \"Superfund\" are mentioned.\n\nAmmoniacal liquor was stored on site as well, in similar tanks. Sometimes the more progressive gasworks would have an ammonium sulfate plant, to convert the liquor into fertilizer, which was sold to farmers.\n\nThis large-scale gas meter precisely measured gas as it issued from the works into the mains. It was of the utmost importance, as the gasworks balanced the account of issued gas versus the amount of paid for gas, and strived to detect why and how they varied from one another. Often it was coupled with a dynamic regulator to keep pressure constant, or even to modulate the pressure at specified times (a series of rapid pressure spikes was sometimes used with appropriately equipped street-lamps to automatically ignite or extinguish such remotely).\n\nThis device injected a fine mist of naphtha into the outgoing gas so as to avoid the crystallization of naphthalene in the mains, and their consequent blockage. Naphtha was found to be a rather effective solvent for these purposes, even in small concentrations. Where troubles with naphthalene developed, as it occasionally did even after the introduction of this minor carburettor, a team of workers was sent out to blow steam into the main and dissolve the blockage; still, prior to its introduction, naphthalene was a very major annoyance for the gasworks.\n\nThis steam or gas engine powered device compressed the gas for injection into the high-pressure mains, which in the early 1900s began to be used to convey gas over greater distances to the individual low pressure mains, which served the end-users. This allowed the works to serve a larger area and achieve economies of scale.\n\n"}
{"id": "50040583", "url": "https://en.wikipedia.org/wiki?curid=50040583", "title": "Horizon (Online magazine)", "text": "Horizon (Online magazine)\n\nHorizon is an online-only, open-access magazine covering research and innovation, published in Brussels since 2013 by the European Commission. It covers a wide range of topics, including agriculture, energy, environment, frontier research, health, ICT, industry, policy, science in society, security, social sciences, space and transport.\n\n\"Horizon\" publishes three to five articles per week and in English only and normally covers research projects which were funded by the European Union (EU) through its Framework Programmes for Research and Technological Development, such as FP7 and Horizon 2020, and through the European Research Council. Occasionally, Horizon also publishes policy announcements from the European Commission’s Directorate-General for Research and Innovation.\n\nArticles from Horizon Magazine can be republished under a license which requires simple attribution. Horizon articles have been shared or re-published, among others, by the European Space Agency, by the University of Oxford, by the Hebrew University of Jerusalem, by the University of Trento and by the Welfare State Futures Coordination Office at Humboldt University of Berlin and by the BBC's The Naked Scientists podcast.\n\nHorizon is produced, on the European Commission’s behalf, by ICF Mostra, a Brussels-based communications division of ICF International.\n\n"}
{"id": "13965672", "url": "https://en.wikipedia.org/wiki?curid=13965672", "title": "Iaso Tholus", "text": "Iaso Tholus\n\nIaso Tholus is a feature on Venus.\n"}
{"id": "39823176", "url": "https://en.wikipedia.org/wiki?curid=39823176", "title": "International Society of Genetic Genealogy", "text": "International Society of Genetic Genealogy\n\nThe International Society of Genetic Genealogy (ISOGG) is an independent non-commercial nonprofit organization of genetic genealogists run by volunteers. It was founded by a group of surname DNA project administrators in 2005 to promote DNA testing for genealogy. It advocates the use of genetics in genealogical research, provides educational resources for genealogists interested in DNA testing, and facilitates networking among genetic genealogists. , it comprises over 8,000 members in 70 countries. , regional meetings are coordinated by 20 volunteer regional coordinators located in the United States, Australia, Brazil, Canada, England, Egypt, Ireland and Russia.\n\nISOGG hosts the ISOGG Wiki, a free online encyclopedia maintained by ISOGG members which contains a wide variety of educational resources and guidance for genetic genealogy consumers and DNA project administrators. The ISOGG Wiki contains ethical guidelines for DNA project administrators and ISOGG members perform peer reviews of DNA project websites of other members on request, following which the websites may display the ISOGG Peer Reviewed graphic.\n\nIn 2006 ISOGG co-founder and director Katherine Borges explained there was interest in testing as \"people want to connect\", and in 2010 she estimated one million people had taken DTC genetic genealogy tests since they became available in 2000.\n\nIn 2008 ISOGG supported the passing of the Genetic Information Nondiscrimination Act designed to prohibit the improper use of genetic information in health insurance and employment in the United States. In July 2010 Borges represented ISOGG at an FDA public meeting on oversight of laboratory developed tests, where she spoke against FDA regulation preventing consumer access to DTC testing: \n\nAn article published in \"Genetics in Medicine\" in March 2012 provides an overview of the diverse array of tests and practices in the emerging DTC genetic genealogy industry. In the article, the authors highlight ISOGG's potential role in developing industry best practice guidelines and consumer guidance: \n\nThe increasing affordability and popularity of DTC genetic genealogy testing has also raised ethical concerns about genealogists testing the DNA of others without consent. According to Borges, \"People who realize the potential of DNA will go to great lengths to get it.\" The ISOGG Wiki contains a selection of external resources on ethics for genetic genealogists.\n\nISOGG promotes the adoption of voluntary industry Y-STR nomenclature standards developed by NIST and published in the \"Journal of Genetic Genealogy\" in 2008. Borges explains ISOGG's rationale as follows: \n\nISOGG members such as Leo Little, Roberta Estes, Rebekah Canada and Bonnie Schrack have been involved in important citizen science discoveries regarding human phylogeny and ethnic origins. The broader ISOGG membership participates in and supports the Genographic Project, a genetic anthropology study that uses crowdsourcing to facilitate new discoveries about human genetic history, and other genetic databases where broader and larger databases aid the identification of participants' ancestral origins.\n\nSince 2006 ISOGG has hosted the regularly updated online ISOGG Y-chromosome phylogenetic tree. ISOGG aims to keep the tree as up-to-date as possible, incorporating new SNPs which are being discovered frequently. The ISOGG tree has been described by academics as using the accepted nomenclature for human Y-chromosome DNA haplogroups and subclades in that it follows the Y Chromosome Consortium nomenclature as described in Karafet et al. 2008, and being \"one of the most up-to-date, if not completely academically verified, phylogenetic trees of Y chromosome haplogroups\". The ISOGG tree is widely cited in peer reviewed academic literature.\n\n\n\n"}
{"id": "40324370", "url": "https://en.wikipedia.org/wiki?curid=40324370", "title": "Jumping-Jupiter scenario", "text": "Jumping-Jupiter scenario\n\nThe jumping-Jupiter scenario specifies an evolution of giant-planet migration described by the Nice model, in which an ice giant (Uranus, Neptune, or an additional Neptune-mass planet) is scattered inward by Saturn and outward by Jupiter, causing the step-wise separation of their orbits. The jumping-Jupiter scenario was proposed by Ramon Brasser, Alessandro Morbidelli, Rodney Gomes, Kleomenis Tsiganis, and Harold Levison after their studies revealed that the smooth divergent migration of Jupiter and Saturn resulted in an inner Solar System significantly different from the current Solar System. The sweeping of secular resonances through the inner Solar System during the migration excited the eccentricities of the terrestrial planets beyond current values and left an asteroid belt with an excessive ratio of high- to low-inclination objects. The step-wise separation of Jupiter and Saturn described in the jumping-Jupiter scenario can allow these resonances to quickly cross the inner Solar System without altering orbits excessively, although the terrestrial planets remain sensitive to its passage. The jumping-Jupiter scenario also results in a number of other differences with the original Nice model. The fraction of lunar impactors from the core of the asteroid belt during the Late Heavy Bombardment is significantly reduced, most of the Jupiter trojans are captured during Jupiter's encounters with the ice giant, as are Jupiter's irregular satellites. In the jumping-Jupiter scenario, the likelihood of preserving four giant planets on orbits resembling their current ones appears to increase if the early Solar System originally contained an additional ice giant, which was later ejected by Jupiter into interstellar space. However, this remains an atypical result, as is the preservation of the current orbits of the terrestrial planets.\n\nIn the original Nice model a resonance crossing results in a dynamical instability that rapidly alters the orbits of the giant planets. The original Nice model begins with the giant planets in a compact configuration with nearly circular orbits. Initially, interactions with planetesimals originating in an outer disk drive a slow divergent migration of the giant planets. This planetesimal-driven migration continues until Jupiter and Saturn cross their mutual 2:1 resonance. The resonance crossing excites the eccentricities of Jupiter and Saturn. The increased eccentricities create perturbations on Uranus and Neptune, increasing their eccentricities until the system becomes chaotic and orbits begin to intersect. Gravitational encounters between the planets then scatter Uranus and Neptune outward into the planetesimal disk. The disk is disrupted, scattering many of the planetesimals onto planet-crossing orbits. A rapid phase of divergent migration of the giant planets is initiated and continues until the disk is depleted. Dynamical friction during this phase dampens the eccentricities of Uranus and Neptune stabilizing the system. In numerical simulations of the original Nice model the final orbits of the giant planets are similar to the current Solar System.\n\nLater versions of the Nice model begin with the giant planets in a series of resonances. This change reflects some hydrodynamic models of the early Solar System. In these models, interactions between the giant planets and the gas disk result in the giant planets migrating toward the central star, in some cases becoming hot Jupiters. However, in a multiple-planet system, this inward migration may be halted or reversed if a more rapidly migrating smaller planet is captured in an outer orbital resonance. The Grand Tack hypothesis, which posits that Jupiter's migration is reversed at 1.5 AU following the capture of Saturn in a resonance, is an example of this type of orbital evolution. The resonance in which Saturn is captured, a 3:2 or a 2:1 resonance, and the extent of the outward migration (if any) depends on the physical properties of the gas disk and the amount of gas accreted by the planets. The capture of Uranus and Neptune into further resonances during or following this outward migration results in a quadruply resonant system, with several stable combinations having been identified. Following the dissipation of the gas disk, the quadruple resonance is eventually broken due to interactions with planetesimals from the outer disk. Evolution from this point resembles the original Nice model with an instability beginning either shortly after the quadruple resonance is broken or after a delay during which planetesimal-driven migration drives the planets across a different resonance. However, there is no slow approach to the 2:1 resonance as Jupiter and Saturn either begin in this resonance or cross it rapidly during the instability.\n\nThe stirring of the outer disk by massive planetesimals can trigger a late instability in a multi-resonant planetary system. As the eccentricities of the planetesimals are excited by gravitational encounters with Pluto-mass objects, an inward migration of the giant planets occurs. The migration, which occurs even if there are no encounters between planetesimals and planets, is driven by a coupling between the average eccentricity of the planetesimal disk and the semi-major axes of the outer planets. Because the planets are locked in resonance, the migration also results in an increase in the eccentricity of the inner ice giant. The increased eccentricity changes the precession frequency of the inner ice giant, leading to the crossing of secular resonances. The quadruple resonance of the outer planets can be broken during one of these secular-resonance crossings. Gravitational encounters begin shortly afterward due to the close proximity of the planets in the previously resonant configuration. The timing of the instability caused by this mechanism, typically occurring several hundred million years after the dispersal of the gas disk, is fairly independent of the distance between the outer planet and the planetesimal disk. In combination with the updated initial conditions, this alternative mechanism for triggering a late instability has been called the Nice 2 model.\n\nEncounters between Jupiter and an ice giant during the giant planet migration are required to reproduce the current Solar System. In a series of three articles Ramon Brasser, Alessandro Morbidelli, Rodney Gomes, Kleomenis Tsiganis, and Harold Levison analyzed the orbital evolution of the Solar System during giant planet migration. The first article demonstrated that encounters between an ice giant and at least one gas giant were required to reproduce the oscillations of the eccentricities of the gas giants. The other two demonstrated that if Jupiter and Saturn underwent a smooth planetesimal-driven separation of their orbits the terrestrial planets would have orbits that are too eccentric and too many of the asteroids would have orbits with large inclinations. They proposed that the ice giant encountered both Jupiter and Saturn, causing the rapid separation of their orbits, thereby avoiding the secular resonance sweeping responsible for the excitation of orbits in the inner Solar System.\n\nExciting the oscillations of the eccentricities of the giant planets requires encounters between planets. Jupiter and Saturn have modest eccentricities that oscillate out of phase, with Jupiter reaching maximum eccentricity when Saturn reaches its minimum and vice versa. A smooth migration of the giant planets without resonance crossings results in very small eccentricities. Resonance crossings excite their mean eccentricities, with the 2:1 resonance crossing reproducing Jupiter's current eccentricity, but these do not generate the oscillations in their eccentricities. Recreating both requires either a combination of resonance crossings and an encounter between Saturn and an ice giant, or multiple encounters of an ice giant with one or both gas giants.\n\nDuring the smooth migration of the giant planets the ν5 secular resonance sweeps through the inner Solar System, exciting the eccentricities of the terrestrial planets. When planets are in a secular resonance the precessions of their orbits are synchronized, keeping their relative orientations and the average torques exerted between them fixed. The torques transfer angular momentum between the planets causing changes in their eccentricities and, if the orbits are inclined relative to one another, their inclinations. If the planets remain in or near secular resonances these changes can accumulate resulting in significant changes in eccentricity and inclination. During a ν5 secular resonance crossing this can result in the excitation of the terrestrial planet's eccentricity, with the magnitude of the increase depending on the eccentricity of Jupiter and the time spent in the secular resonance. For the original Nice model the slow approach to Jupiter's and Saturn's 2:1 resonance results in an extended interaction of the ν5 secular resonance with Mars, driving its eccentricity to levels that can destabilize the inner Solar System, potentially leading to collisions between planets or the ejection of Mars. In later versions of the Nice model Jupiter's and Saturn's divergent migration across (or from) the 2:1 resonance is more rapid and the nearby ν5 resonance crossings of Earth and Mars are brief, thus avoiding the excessive excitation of their eccentricities in some cases. Venus and Mercury, however, reach significantly higher eccentricities than are observed when the ν5 resonance later crosses their orbits.\n\nA smooth planetesimal-driven migration of the giant planets also results in an asteroid belt orbital distribution unlike that of the current asteroid belt. As it sweeps across the asteroid belt the ν16 secular resonance excites asteroid inclinations. It is followed by the ν6 secular resonance which excites the eccentricities of low-inclination asteroids. If the secular resonance sweeping occurs during a planetesimal driven migration, which has a timescale of 5 million years or longer, the remaining asteroid belt is left with a significant fraction of asteroids with inclinations greater than 20°, which are relatively rare in the current asteroid belt. The interaction of the ν6 secular resonance with the 3:1 mean-motion resonance also leaves a prominent clump in the semi-major-axis distribution that is not observed. The secular resonance sweeping would also leave too many high inclination asteroids if the giant planet migration occurred early, with all of the asteroids initially in low eccentricity and inclination orbits, and also if the orbits of the asteroids were excited by Jupiter's passage during the Grand Tack.\n\nEncounters between an ice giant and both Jupiter and Saturn accelerate the separation of their orbits, limiting the effects of secular resonance sweeping on the orbits of the terrestrial planets and the asteroids. To prevent the excitation of orbits of the terrestrial planets and asteroids the secular resonances must sweep rapidly through the inner Solar System. The small eccentricity of Venus indicates that this occurred on a timescale of less than 150,000 years, much shorter than in a planetesimal driven migration. The secular resonance sweeping can be largely avoided, however, if the separation of Jupiter and Saturn was driven by gravitational encounters with an ice giant. These encounters must drive the Jupiter–Saturn period ratio quickly from below 2.1 to beyond 2.3, the range where the secular resonance crossings occur. This evolution of the giant planets orbits has been named the jumping-Jupiter scenario after a similar process proposed to explain the eccentric orbits of some exoplanets.\n\nThe jumping-Jupiter scenario replaces the smooth separation of Jupiter and Saturn with a series of jumps, thereby avoiding the sweeping of secular resonances through the inner Solar System as their period ratio crosses from 2.1–2.3. In the jumping-Jupiter scenario an ice giant is scattered inward by Saturn onto a Jupiter-crossing orbit and then scattered outward by Jupiter. Saturn's semi-major axis is increased in the first gravitational encounter and Jupiter's reduced by the second with the net result being an increase in their period ratio. In numerical simulations the process can be much more complex: while the trend is for Jupiter's and Saturn's orbits to separate, depending on the geometry of the encounters, individual jumps of Jupiter's and Saturn's semi-major axes can be either up and down. In addition to numerous encounters with Jupiter and Saturn, the ice giant can encounter other ice giant(s) and in some cases cross significant parts of the asteroid belt. The gravitational encounters occur over a period of 10,000–100,000 years, and end when dynamical friction with the planetesimal disk dampens the ice giant's eccentricity, raising its perihelion beyond Saturn's orbit; or when the ice giant is ejected from the Solar System. A jumping-Jupiter scenario occurs in a subset of numerical simulations of the Nice model, including some done for the original Nice model paper. The chances of Saturn scattering an ice giant onto a Jupiter-crossing orbit increases when the initial Saturn–ice giant distance is less than 3 AU, and with the 35-Earth-mass planetesimal belt used in the original Nice model, typically results in the ejection of the ice giant.\n\nThe frequent loss of the giant planet encountering Jupiter in simulations has led some to propose that the early Solar System began with five giant planets. In numerical simulations of the jumping-Jupiter scenario the ice giant is often ejected following its gravitational encounters with Jupiter and Saturn, leaving planetary systems that begin with four giant planets with only three. Although beginning with a higher-mass planetesimal disk was found to stabilize four-planet systems, the massive disk either resulted in excess migration of Jupiter and Saturn after the encounters between an ice giant and Jupiter or prevented these encounters by damping eccentricities. This problem led David Nesvorný to investigate planetary systems beginning with five giant planets. After conducting thousands of simulations he reported that simulations beginning with five giant planets were 10 times as likely to reproduce the current orbits of the outer planets. A follow-up study by David Nesvorny and Alessandro Morbidelli sought initial resonant configurations that would reproduce the semi-major axis of the four outer planets, Jupiter's eccentricity, and a jump from <2.1 to >2.3 in Jupiter's and Saturn's period ratio. While less than 1% of the best four-planet models met these criteria roughly 5% of the best five-planet models were judged successful, with Jupiter's eccentricity being the most difficult to reproduce. A separate study by Konstantin Batygin and Michael Brown found similar probabilities (4% vs 3%) of reproducing the current outer Solar System beginning with four or five giant planets using the best initial conditions. Their simulations differed in that the planetesimal disk was placed close to the outer planet resulting in a period of migration before planetary encounters began. Criteria included reproducing the oscillations of Jupiter's and Saturn's eccentricities, a period when Neptune's eccentricity exceeded 0.2 during which hot classical Kuiper belt objects were captured, and the retention of a primordial cold classical Kuiper belt, but not the jump in Jupiter's and Saturn's period ratio. Their results also indicate that if Neptune's eccentricity exceeded 0.2, preserving a cold classical belt may require the ice giant to be ejected in as little as 10,000 years.\n\nNeptune's migration into the planetesimal disk before planetary encounters begin allows Jupiter to retain a significant eccentricity and limits its migration after the ejection of the fifth ice giant. Jupiter's eccentricity is excited by resonance crossings and gravitational encounters with the ice giant and is damped due to secular friction with the planetesimal disk. Secular friction occurs when the orbit of a planet suddenly changes and results in the excitation of the planetesimals' orbits and the reduction of the planet's eccentricity and inclination as the system relaxes. If gravitational encounters begin shortly after the planets leave their multi-resonant configuration, this leaves Jupiter with a small eccentricity. However, if Neptune first migrates outward disrupting the planetesimal disk, its mass is reduced and the eccentricities and inclinations of the planetesimals are excited. When planetary encounters are later triggered by a resonance crossing this lessens the impact of secular friction allowing Jupiter's eccentricity to be maintained. The smaller mass of the disk also reduces the divergent migration of Jupiter and Saturn following the ejection of the fifth planet. This can allow Jupiter's and Saturn's period ratio to jump beyond 2.3 during the planetary encounters without exceeding the current value once the planetesimal disk is removed. Although this evolution of the outer planet's orbits can reproduce the current Solar System, it is not the typical result in simulations that begin with a significant distance between the outer planet and the planetesimal disk as in the Nice 2 model. An extended migration of Neptune into the planetesimal disk before planetary encounters begin can occur if the disk's inner edge was within 2 AU of Neptune's orbit. This migration begins soon after the protoplanetary disk dissipates, resulting in an early instability, and is most likely if the giant planets began in a 3:2, 3:2, 2:1, 3:2 resonance chain.\n\nA late instability can occur if Neptune first underwent a slow dust-driven migration towards a more distant planetesimal disk. For a five planet system to remain stable for 400 million years the inner edge of the planetesimal disk must be several AU beyond Neptune's initial orbit. Collisions between planetesimals in this disk creates debris that is ground down to dust in a collisional cascade. The dust drifts inward due to Poynting–Robertson drag, eventually reaching the orbits of the giant planets. Gravitational interactions with the dust causes the giant planets to escape from their resonance chain roughly 10 million years after the dissipation of the gas disk. The gravitational interactions then result in a slow dust-driven migration of the planets until Neptune approaches the inner edge of the disk. A more rapid planetesimal-driven migration of Neptune into the disk then ensues until the orbits of the planets are destabilized following a resonance crossing. The dust driven migration requires 7–22 Earth-masses of dust, depending on the initial distance between Neptune's orbit and the inner edge of the dust disk. The rate of the dust-driven migration slows with time as the amount of dust the planets encounters declines. As a result, the timing of the instability is sensitive to the factors that control the rate of dust generation such as the size distribution and the strength of the planetesimals.\n\nThe jumping-Jupiter scenario results in a number of differences with the original Nice model.\n\nThe rapid separation of Jupiter's and Saturn's orbits causes the secular resonances to quickly cross the inner Solar System. The number of asteroids removed from the core of the asteroid belt is reduced, leaving an inner extension of the asteroid belt as the dominant source of rocky impactors. The likelihood of preserving the low eccentricities of the terrestrial planets increases to above 20% in a selected jumping-Jupiter model. Since the modification of orbits in the asteroid belt is limited, its depletion and the excitement of its orbits must have occurred earlier. However, asteroid orbits are modified enough to shift the orbital distribution produced by a grand tack toward that of the current asteroid belt, to disperse collisional families, and to remove fossil Kirkwood gaps. The ice giant crossing the asteroid belt allows some icy planetesimals to be implanted into the inner asteroid belt.\n\nIn the outer Solar System icy planetesimals are captured as Jupiter trojans when Jupiter's semi-major axis jumps during encounters with the ice giant. Jupiter also captures irregular satellites via three body interactions during these encounters. The orbits of Jupiter's regular satellites are perturbed, but in roughly half of simulations remain in orbits similar to those observed. Encounters between an ice giant and Saturn perturb the orbit of Iapetus and may be responsible for its inclination. The dynamical excitement of the outer disk by Pluto-massed objects and its lower mass reduces the bombardment of Saturn's moons. Saturn's tilt is acquired when it is captured in a spin-orbit resonance with Neptune. A slow and extended migration of Neptune into the planetesimal disk before planetary encounters begin leaves the Kuiper belt with a broad inclination distribution. When Neptune's semi-major axis jumps outward after it encounters the ice giant objects captured in its 2:1 resonance during its previous migration escape, leaving a clump of low inclination objects with similar semi-major axes. The outward jump also releases objects from the 3:2 resonance, reducing the number of low inclination plutinos remaining at the end of Neptune's migration. \n\nMost of the rocky impactors of the Late Heavy Bombardment originate from an inner extension of the asteroid belt yielding a smaller but longer lasting bombardment. The innermost region of the asteroid belt is currently sparsely populated due to the presence of the ν6 secular resonance. In the early Solar System, however, this resonance was located elsewhere and the asteroid belt extended farther inward, ending at Mars-crossing orbits. During the giant planet migration the ν6 secular resonance first rapidly traversed the asteroid belt removing roughly half of its mass, much less than in the original Nice model. When the planets reached their current positions the ν6 secular resonance destabilized the orbits of the innermost asteroids. Some of these quickly entered planet crossing orbit initiating the Late Heavy Bombardment. Others entered quasi-stable higher inclination orbits, later producing an extended tail of impacts, with a small remnant surviving as the Hungarias. The increase in the orbital eccentricities and inclinations of the destabilized objects also raised impact velocities, resulting in a change the size distribution of lunar craters, and in the production of impact melt in the asteroid belt. The innermost (or E-belt) asteroids are estimated to have produced nine basin-forming impacts on the Moon between 4.1 and 3.7 billion years ago with three more originating from the core of the asteroid belt. The pre-Nectarian basins, part of the LHB in the original Nice model, are thought to be due to the impacts of leftover planetesimals from the inner Solar System.\n\nThe magnitude of the cometary bombardment is also reduced. The giant planets outward migration disrupts the outer planetesimal disk causing icy planetesimals to enter planet crossing orbits. Some of them are then perturbed by Jupiter onto orbits similar to those of Jupiter-family comets. These spend a significant fraction of their orbits crossing the inner Solar System raising their likelihood of impacting the terrestrial planets and the moon. In the original Nice model this results in a cometary bombardment with a magnitude similar to the asteroid bombardment. However, while low levels of iridium detected from rocks dating from this era have been cited as evidence of a cometary bombardment, other evidence such as the mix of highly siderophile elements in lunar rocks, and oxygen isotope ratios in the fragments of impactors are not consistent with a cometary bombardment. The size distribution of lunar craters is also largely consistent with that of the asteroids, leading to the conclusion the bombardment was dominated by asteroids. The bombardment by comets may have been reduced by a number of factors. The stirring of the orbits by Pluto-massed objects excites of the inclinations of the orbits of the icy planetimals, reducing the fraction of objects entering Jupiter-family orbits from 1/3 to 1/10. The mass of the outer disk in the five-planet model is roughly half that of the original Nice model. The magnitude of the bombardment may have been reduced further due to the icy planetesimals undergoing significant mass loss, or their having broken up as the entered the inner Solar System. The combination of these factors reduces the estimated largest impact basin to the size of Mare Crisium, roughly half the size of the Imbrium basin. Evidence of this bombardment may have been destroyed by later impacts by asteroids.\n\nA number of issues have been raised regarding the connection between the Nice model and the Late Heavy Bombardment. Crater counts using topographic data from the Lunar Reconnaissance Orbiter find an excess of small craters relative to large impact-basins when compared to the size distribution of the asteroid belt. However, if the E-belt was the product of collisions among a small number of large asteroids, it may have had a size distribution that differed from that of the asteroid belt with a larger fraction of small bodies. A recent work has found that the bombardment originating from the inner band of asteroids would yield only two lunar basins and would be insufficient to explain ancient impact spherule beds. It suggests instead that debris from a massive impact was the source, noting that this would better match the size distribution of impact craters. A second work concurs, finding that the asteroid belt was probably not the source of the Late Heavy Bombardment. Noting the lack of direct evidence of cometary impactors, it proposes that leftover planetesimals were the source of most impacts and that Nice model instability may have occurred early. If a different crater scaling law is used, however, the Nice model is more likely to produce the impacts attributed to the Late Heavy bombardment and more recent impact craters.\n\nA giant-planet migration in which the ratio of the periods of Jupiter and Saturn quickly cross from below 2.1 to greater than 2.3 can leave the terrestrial planets with orbits similar to their current orbits. The eccentricities and inclinations of a group of planets can be represented by the angular momentum deficit (AMD), a measure of the differences of their orbits from circular coplanar orbits. A study by Brasser, Walsh, and Nesvorny found that when a selected jumping-Jupiter model was used, the current angular momentum deficit has a reasonable chance (~20%) of being reproduced in numerical simulations if the AMD was initially between 10% and 70% of the current value. The orbit of Mars is largely unchanged in these simulations indicating that its initial orbit must have been more eccentric and inclined than those of the other planets. The jumping-Jupiter model used in this study was not typical, however, being selected from among only 5% with Jupiter and Saturn's period ratio jumped to beyond 2.3 while reproducing other aspects of the outer Solar System.\n\nThe overall success rate of jumping-Jupiter models with a late instability reproducing both the inner and outer Solar System is small. When Kaib and Chambers conducted a large number of simulations starting with five giant planets in a resonance chain and Jupiter and Saturn in a 3:2 resonance, 85% resulted in the loss of a terrestrial planet, less than 5% reproduce the current AMD, and only 1% reproduce both the AMD and the giant planet orbits. In addition to the secular-resonance crossings, the jumps in Jupiter's eccentricity when it encounters an ice giant can also excite the orbits of the terrestrial planets. This led them to propose that the Nice model migration occurred before the formation of the terrestrial planets and that the LHB had another cause. However, the advantage of an early migration is significantly reduced by the requirement that the Jupiter–Saturn period ratio jump to beyond 2.3 to reproduce the current asteroid belt.\n\nThe jumping-Jupiter model can reproduce the eccentricity and inclination of Mercury's orbit. Mercury's eccentricity is excited when it crosses a secular resonance with Jupiter. When relativistic effects are included, Mercury's precession rate is faster, which reduces the impact of this resonance crossing, and results in a smaller eccentricity similar to its current value. Mercury's inclination may be the result of it or Venus crossing a secular resonance with Uranus.\n\nThe rapid traverse of resonances through the asteroid belt can leave its population and the overall distribution of its orbital elements largely preserved. In this case the asteroid belt's depletion, the mixing of its taxonomical classes, and the excitation of its orbits, yielding a distribution of inclinations peaked near 10° and eccentricities peaked near 0.1, must have occurred earlier. These may be the product of Jupiter's Grand Tack, provided that an excess of higher eccentricity asteroids is removed due to interactions with the terrestrial planets. Gravitational stirring by planetary embryos embedded in the asteroid belt could also produce its depletion, mixing, and excitation. However, most if not all of the embryos must have been lost before the instability. A mixing of asteroids types could be the product of asteroids being scattered into the belt during the formation of the planets. An initially small mass asteroid belt could have its inclinations and eccentricities excited by secular resonances that hopped across the asteroid belt if Jupiter's and Saturn's orbits became chaotic while in resonance. \n\nThe orbits of the asteroids could be excited during the instability if the ice giant spent hundreds of thousands of years on a Jupiter crossing orbit. Numerous gravitational encounters between the ice giant and Jupiter during this period would cause frequent variations in Jupiter's semi-major axis, eccentricity and inclination. The forcing exerted by Jupiter on the orbits of the asteroids and the semi-major axes where it was strongest, would also vary, causing a chaotic excitation of the asteroids orbits that could reach or exceed their present level. The highest eccentricity asteroids would be later be removed by encounters with the terrestrial planets. The eccentricities of the terrestrial planets are excited beyond the current values during this process, however, requiring that the instability occur before their formation in this case.\n\nThe sweeping of resonances and the penetration of the ice giant into the asteroid belt results in the dispersal of asteroid collisional families formed during or before the Late Heavy Bombardment. A collisional family's inclinations and eccentricities are dispersed due to the sweeping secular resonances, including those inside mean motion resonances, with the eccentricities being most affected. Perturbations by close encounters with the ice giant result in the spreading of a family's semi-major axes. Most collisional families would thus become unidentifiable by techniques such as the hierarchical clustering method, and V-type asteroids originating from impacts on Vesta could be scattered to the middle and outer asteroid belt. However, if the ice giant spent a short time crossing the asteroid belt, some collisional families may remain recognizable by identifying the V-shaped patterns in plots of semi-major axes vs absolute magnitude produced by the Yarkovsky effect. The survival of the Hilda collisional family, a subset of the Hilda group thought to have formed during the LHB because of the current low collision rate, may be due to its creation after Hilda's jump-capture in the 3:2 resonance as the ice giant was ejected.\nThe stirring of semi-major axes by the ice giant may also remove fossil Kirkwood gaps formed before the instability.\n\nPlanetesimals from the outer disc are embedded in all parts of the asteroid belt, remaining as P- and D-type asteroids. While Jupiter's resonances sweep across the asteroid belt, outer disk planetesimals are captured by its inner resonances, evolve to lower eccentricities via secular resonances with in these resonances, and are released onto stable orbits as Jupiter's resonances move on. Other planetesimals are implanted in the asteroid belt during encounters with the ice giant, either directly leaving them with aphelia higher than that of the ice giant's perihelia, or by removing them from a resonance. Jumps in Jupiter's semi-major axis during its encounters with the ice giant shift the locations of its resonances, releasing some objects and capturing others. Many of those remaining after its final jump, along with others captured by the sweeping resonances as Jupiter migrates to its current location, survive as parts of the resonant populations such as the Hildas, Thule, and those in the 2:1 resonance. Objects originating in the asteroid belt can also be captured in the 2:1 resonance, along with a few among the Hilda population. The excursions the ice giant makes into the asteroid belt allows the icy planetesimals to be implanted farther into the asteroid belt, with a few reaching the inner asteroid belt with semi-major axis less than 2.5 AU. Some objects later drift into unstable resonances due to diffusion or the Yarkovsky effect and enter Earth-crossing orbits, with the Tagish Lake meteorite representing a possible fragment of an object that originated in the outer planetesimal disk. Numerical simulations of this process can roughly reproduce the distribution of P- and D-type asteroids and the size of the largest bodies, with differences such as an excess of objects smaller than 10 km being attributed to losses from collisions or the Yarkovsky effect, and the specific evolution of the planets in the model.\n\nMost of the Jupiter trojans are jump-captured shortly after a gravitational encounters between Jupiter and an ice giant. During these encounters Jupiter's semi-major axis can jump by as much as 0.2 AU, displacing the L4 and L5 points radially, and releasing many existing Jupiter trojans. New Jupiter trojans are captured from the population of planetesimals with semi-major axes similar to Jupiter's new semi-major axis. The captured trojans have a wide range of inclinations and eccentricities, the result of their being scattered by the giant planets as they migrated from their original location in the outer disk. Some additional trojans are captured, and others lost, during weak-resonance crossings as the co-orbital regions becomes temporarily chaotic. Following its final encounters with Jupiter the ice giant may pass through one of Jupiter's trojan swarms, scattering many, and reducing its population. In simulations, the orbital distribution of Jupiter trojans captured and the asymmetry between the L4 and L5 populations is similar to that of the current Solar System and is largely independent of Jupiter's encounter history. Estimates of the planetesimal disk mass required for the capture of the current population of Jupiter trojans range from 15-20 Earth masses, consistent with the mass required to reproduce other aspects of the outer Solar System.\n\nJupiter captures a population of irregular satellites and the relative size of Saturn's population is increased.\nDuring gravitational encounters between planets, the hyperbolic orbits of unbound planetesimals around one giant planet are perturbed by the presence of the other. If the geometry and velocities are right, these three-body interactions leave the planetesimal in a bound orbit when planets separate. Although this process is reversible, loosely bound satellites including possible primordial satellites can also escape during these encounters, tightly bound satellites remain and the number of irregular satellites increases over a series of encounters. Following the encounters, the satellites with inclinations between 60° and 130° are lost due the Kozai resonance and the more distant prograde satellites are lost to the evection resonance. Collisions among the satellites result in the formation of families, in a significant loss of mass, and in a shift of their size distribution. The populations and orbits of Jupiter's irregular satellites captured in simulations are largely consistent with observations. Himalia, which has a spectra similar to asteroids in the middle of the asteroid belt, is somewhat larger than the largest captured in simulations. If it was a primordial object its odds of surviving the series of gravitational encounters range from 0.01 - 0.3, with the odds falling as the number increases. Saturn has more frequent encounters with the ice giant in the jumping-Jupiter scenario, and Uranus and Neptune have fewer encounters if that was a fifth giant planet. This increases the size of Saturn's population relative to Uranus and Neptune when compared to the original Nice model, producing a closer match with observations.\n\nThe orbits of Jupiter's regular satellites can remain dynamically cold despite encounters between the giant planets. Gravitational encounters between planets perturb the orbits of their satellites, exciting inclinations and eccentricities, and altering semi-major axes. If these encounters would lead to results inconsistent with the observations, for example, collisions between or the ejections of satellites or the disruption of the Laplace resonance of Jupiter's moons Io, Europa and Ganymede, this could provide evidence against jumping-Jupiter models. In simulations, collisions between or the ejection of satellites was found to be unlikely, requiring an ice giant to approach within 0.02 AU of Jupiter. More distant encounters that disrupted the Laplace resonance were more common, though tidal interactions often lead to their recapture. A sensitive test of jumping-Jupiter models is the inclination of Callisto's orbit, which isn't damped by tidal interactions. Callisto's inclination remained small in six out of ten 5-planet models tested in one study (including some where Jupiter acquired irregular satellites consistent with observations), and another found the likelihood of Jupiter ejecting a fifth giant planet while leaving Callisto's orbit dynamically cold at 42%. Callisto is also unlikely to have been part of the Laplace resonance, because encounters that raise it to its current orbit leave it with an excessive inclination.\n\nThe encounters between planets also perturb the orbits of the moons of the other outer planets. Saturn's moon Iapetus could have been excited to its current inclination, if the ice giant's closest approach was out of the plane of Saturn's equator. If Saturn acquired its tilt before the encounters, Iapetus's inclination could also be excited due to multiple changes of its semi-major axis, because the inclination of Saturn's Laplace plane would vary with the distance from Saturn. In simulations, Iapetus was excited to its current inclination in five of ten of the jumping-Jupiter models tested, though three left it with excessive eccentricity. The preservation of Oberon's small inclination favors the 5-planet models, with only a few encounters between Uranus and an ice giant, over 4-planet models in which Uranus encounters Jupiter and Saturn. The low inclination of Uranus's moon Oberon, 0.1°, was preserved in nine out of ten of five planet models, while its preservation was found to be unlikely in four planet models. The encounters between planets may have also be responsible for the absence of regular satellites of Uranus beyond the orbit of Oberon.\n\nThe loss of ices from the inner satellites due to impacts is reduced. Numerous impacts of planetesimals onto the satellites of the outer planets occur during the Late Heavy Bombardment. In the bombardment predicted by the original Nice model, these impacts generate enough heat to vaporize the ices of Mimas, Enceladus and Miranda. The smaller mass planetesimal belt in the five planet models reduces this bombardment. Furthermore, the gravitational stirring by Pluto-massed objects in the Nice 2 model excites the inclinations and eccentricities of planetesimals. This increases their velocities relative to the giant planets, decreasing the effectiveness of gravitational focusing, thereby reducing the fraction of planetesimals impacting the inner satellites. Combined these reduce the bombardment by an order of magnitude. Estimates of the impacts on Iapetus are also less than 20% of that of the original Nice model.\n\nSome of the impacts are catastrophic, resulting in the disruption of the inner satellites. In the bombardment of the original Nice model this may result in the disruption of several of the satellites of Saturn and Uranus. An order of magnitude reduction in the bombardment avoids the destruction of Dione and Ariel; but Miranda, Mimas, Enceladus, and perhaps Tethys would still be disrupted. These may be second generation satellites formed from the re-accretion of disrupted satellites. In this case Mimas would not be expected to be differentiated and the low density of Tethys may be due to it forming primarily from the mantle of a disrupted progenitor. Alternatively they may have accreted later from a massive Saturnian ring, or even as recently as 100 Myr ago after the last generation of moons were destroyed in an orbital instability.\n\nJupiter's and Saturn's tilts can be produced by spin-orbit resonances. A spin-orbit resonance occurs when the precession frequency of a planet's spin-axis matches the precession frequency of another planet's ascending node. These frequencies vary during the planetary migration with the semi-major axes of the planets and the mass of the planetesimal disk. Jupiter's small tilt may be due to a quick crossing of a spin-orbit resonance with Neptune while Neptune's inclination was small, for example, during Neptune's initial migration before planetary encounters began. Alternatively, if that crossing occurred when Jupiter's semi-major axis jumped, it may be due to its current proximity to spin-orbit resonance with Uranus. Saturn's large tilt can be acquired if it is captured in a spin-orbit resonance with Neptune as Neptune slowly approached its current orbit at the end of the migration. The final tilts of Jupiter and Saturn are very sensitive to the final positions of the planets: Jupiter's tilt would be much larger if Uranus migrated beyond its current orbit, Saturn's would be much smaller if Neptune's migration ended earlier or if the resonance crossing was more rapid. Even in simulations where the final position of the giant planets are similar to the current Solar System, Jupiter's and Saturn's tilt are reproduced less than 10% of the time.\n\nA slow migration of Neptune covering several AU results in a Kuiper belt with a broad inclination distribution. As Neptune migrates outward it scatters many objects from the planetesimal disk onto orbits with larger semi-major axes. Some of these planetesimals are then captured in mean-motion resonances with Neptune. While in a mean-motion resonance, their orbits can evolve via processes such as the Kozai mechanism, reducing their eccentricities and increasing their inclinations; or via apsidal and nodal resonances, which alter eccentricities and inclinations respectively. Objects that reach low-eccentricity high-perihelion orbits can escape from the mean-motion resonance and are left behind in stable orbits as Neptune's migration continues. The inclination distribution of the hot classical Kuiper belt objects is best reproduced in numerical simulations where Neptune migrated smoothly from 24 AU to 28 AU with an exponential timescale of 10 million years before jumping outward when it encounters with a fifth giant planet and with a 30 million years exponential timescale thereafter. The slow pace and extended distance of this migration provides sufficient time for inclinations to be excited before the resonances reach the region of Kuiper belt where the hot classical objects are captured and later deposited. If Neptune reaches an eccentricity greater than 0.12 following its encounter with the fifth giant planet hot classical Kuiper belt objects can also be captured due to secular forcing. Secular forcing causes the eccentricities of objects to oscillate, allowing some to reach smaller eccentricity orbits that become stable once Neptune reaches a low eccentricity.\n\nThe objects that remain in the mean-motion resonances at the end of Neptune's migration form the resonant populations such as the plutinos. The slow migration allows these objects to reach large inclinations before capture and to evolve to lower eccentricities without escaping from resonance. Few low inclination objects resembling the cold classical objects remain among the plutinos at the end of the Neptune's migration. The outward jump in Neptune's semi-major axes releases the low-inclination low-eccentricity objects captured as Neptune's 3:2 resonance initially swept outward. Afterwards, their captures are largely prevented due to the excitation of inclinations and eccentricities as secular resonances slowly sweep ahead of it. The number of planetesimals with initial semi-major axes beyond 30 AU must have been small to avoid an excess of objects in Neptune's 5:4 and 4:3 resonances.\n\nEncounters between Neptune and Pluto-massed objects reduce the fraction of Kuiper belt objects in resonances. Velocity changes during the gravitational encounters with planetesimals that drive Neptune's migration cause small jumps in its semi-major axis, yielding a migration that is grainy instead of smooth. The shifting locations of the resonances produced by this rough migration increases the libration amplitudes of resonant objects, causing many to become unstable and escape from resonances. The observed ratio of hot classical objects to plutinos is best reproduced in simulations that include 1000–4000 Pluto-massed objects (i.e. large dwarf planets) or about 1000 bodies twice as massive as Pluto, making up 10–40% of the 20-Earth-mass planetesimal disk, with roughly 0.1% of this initial disk remaining in various parts of the Kuiper belt. The grainy migration also reduces the number of plutinos relative to objects in the 2:1 and 5:2 resonances with Neptune, and results in a population of plutinos with a narrower distribution of libration amplitudes. A large number of Pluto-massed objects would requires the Kuiper belt's size distribution to have multiple deviations from a constant slope.\n\nThe kernel of the cold classical Kuiper belt objects is left behind when Neptune encounters the fifth giant planet. The kernel is a concentration of Kuiper belt objects with small eccentricities and inclinations, and with semi-major axes of 44–44.5 AU identified by the Canada–France Ecliptic Plane Survey. As Neptune migrates outward low-inclination low-eccentricity objects are captured by its 2:1 mean-motion resonance. These objects are carried outward in this resonance until Neptune reaches 28 AU. At this time Neptune encounters the fifth ice giant, which has been scattered outward by Jupiter. The gravitational encounter causes Neptune's semi-major axis to jump outward. The objects that were in the 2:1 resonance, however, remain in their previous orbits and are left behind as Neptune's migration continues. Those objects that have been pushed-out a short distance have small eccentricities and are added to the local population of cold classical KBOs. Others that have been carried longer distances have their eccentricities excited during this process. While most of these are released on higher eccentricity orbits a few have their eccentricities reduced due to a secular resonance within the 2:1 resonance and released as part of the kernel or earlier due to Neptune's grainy migration. Among these are objects from regions no longer occupied by dynamically cold objects that formed in situ, such as between 38 and 40 AU. Pushing out in resonance allows these loosely bound, neutrally colored or 'blue' binaries binaries to be implanted without encountering Neptune. The kernel has also been reproduced in a simulation in which a more violent instability occurred without a preceding migration of Neptune and the disk was truncated at ~44.5 AU.\n\nThe low eccentricities and inclinations of the cold classical belt objects places some constraints on the evolution of Neptune's orbit. They would be preserved if the eccentricity and inclination of Neptune following its encounter with another ice giant remained small (e < 0.12 and i < 6°) or was damped quickly. This constraint may be relaxed somewhat if Neptune's precession is rapid due to strong interactions with Uranus or a high surface density disk. A combination of these may allow the cold classical belt to be reproduced even in simulations with more violent instabilities. If Neptune's rapid precession rate drops temporarily, a 'wedge' of missing low eccentricity objects can form beyond 44 AU. The appearance of this wedge can also be reproduced if the size of objects initially beyond 45 AU declined with distance. A more extended period of Neptune's slow precession could allow low eccentricity objects to remain in the cold classical belt if its duration coincided with that of the oscillations of the objects' eccentricities. A slow sweeping of resonances, with an exponential timescale of 100 million years, while Neptune has a modest eccentricity can remove the higher-eccentricity low-inclination objects, truncating the eccentricity distribution of the cold classical belt objects and leaving a step near the current position of Neptune's 7:4 resonance.\n\nIn the scattered disk, a slow and grainy migration of Neptune leaves detached objects with perihelia greater than 40 AU clustered near its resonances. Planetesimals scattered outward by Neptune are captured in resonances, evolve onto lower-eccentricity higher-inclination orbits, and are released onto stable higher perihelion orbits. Beyond 50 AU this process requires a slower migration of Neptune for the perihelia to be raised above 40 AU. As a result, in the scattered disk fossilized high-perihelion objects are left behind only during the latter parts of Neptune's migration, yielding short trails (or fingers) on a plot of eccentricity vs. semi-major axis, near but just inside the current locations of Neptune's resonances. The extent of these trails is dependent on the timescale of Neptune's migration and extends farther inward if the timescale is longer. The release of these objects from resonance is aided by a grainy migration of Neptune which may be necessary for an object like to have escaped from Neptune's 8:3 resonance. If the encounter with the fifth planet leaves Neptune with a large eccentricity the semi-major axes of the high perihelion objects would be distributed more symmetrically about Neptune's resonances, unlike the objects observed by OSSOS.\n\nThe dynamics of the scattered disk left by Neptune's migration varies with distance. During Neptune's outward migration many objects are scattered onto orbits with semi-major axes greater than 50 AU. Similar to in the Kuiper belt, some of these objects are captured by and remain in a resonance with Neptune, while others escape from resonance onto stable orbits after their perihelia are raised. Other objects with perihelia near Neptune's also remain at the end of Neptune's migration. The orbits of these scattering objects vary with time as they continue to interact with Neptune, with some of them entering planet crossing orbits, briefly becoming centaurs or comets before they are ejected from the Solar System. Roughly 80% of the objects between 50 and 200 AU have stable, resonant or detached, orbits with semi-major axes that vary less than 1.5 AU per billion years. The remaining 20% are actively scattering objects with semi-major axes that vary significantly due to interactions with Neptune. Beyond 200 AU most objects in the scattered disc are actively scattering. The total mass deposited in the scattered disk is about twice that of the classical Kuiper belt, with roughly 80% of the objects surviving to the present having semi-major axes less than 200 AU. Lower inclination detached objects become scarcer with increasing semi-major axis, possible due to stable mean motion resonances, or the Kozai resonance within these resonances, requiring a minimum inclination that increases with semi-major axis.\n\nIf the hypothetical Planet Nine exists and was present during the giant planet migration a cloud of objects with similar semi-major axes would be formed. Objects scattered outward to semi-major axes greater than 200 AU would have their perihelia raised by the dynamical effects of Planet Nine decoupling them from the influence of Neptune. The semi-major axes the objects dynamically controlled by Planet Nine would be centered on its semi-major axis, ranging from 200 AU to ~2000 AU, with most objects having semi-major axes greater than that of Planet Nine. Their inclinations would be roughly isotropic, ranging up to 180 degrees. The perihelia of these object would cycle over periods of over 100 Myr, returning many to the influence of the Neptune. The estimated mass remaining at the current time is 0.3 – 0.4 Earth masses.\n\nSome of the objects scattered onto very distant orbits during the giant planet migration are captured in the Oort cloud. The outer Oort cloud, semi-major axes greater than 20,000 AU, forms quickly as the galactic tide raises the perihelion of object beyond the orbits of the giant planets. The inner Oort cloud forms more slowly, from the outside in, due to the weaker effect of the galactic tide on objects with smaller semi-major axes. Most objects captured in the outer Oort cloud are scattered outward by Saturn, without encountering Jupiter, with some being scattered outward by Uranus and Neptune. Those captured in the inner Oort cloud are primarily scattered outward by Neptune. Roughly 6.5% of the planetesimals beyond Neptune's initial orbit, approximately 1.3 Earth masses, are captured in the Oort cloud with roughly 60% in the inner cloud.\n\nObjects may also have been captured earlier and from other sources. As the sun left its birth cluster objects could have been captured in the Oort cloud from other stars. If the gas disk extended beyond the orbits of the giant planets when they cleared their neighborhoods comet-sized object are slowed by gas drag preventing them from reaching the Oort cloud. However, if Uranus and Neptune formed late, some of the objects cleared from their neighborhood after the gas disk dissipates may be captured in the Oort cloud. If the Sun remained in its birth cluster at this time, or during the planetary migration if that occurred early, the Oort cloud formed would be more compact.\n\n"}
{"id": "416787", "url": "https://en.wikipedia.org/wiki?curid=416787", "title": "Kensington Runestone", "text": "Kensington Runestone\n\nThe Kensington Runestone is a slab of greywacke covered in runes on its face and side. \nA Swedish immigrant, Olof Ohman, reported that he discovered it in 1898 in the largely rural township of Solem, Douglas County, Minnesota, and named it after the nearest settlement, Kensington.\n\nThe inscription purports to be a record left behind by Scandinavian explorers in the 14th century (internally dated to the year 1362). \nThere has been a drawn-out debate on the stone's authenticity, but the scholarly consensus has classified it as a 19th-century hoax since it was first examined in 1910, with some critics directly charging the purported discoverer Ohman with fabricating the inscription.\nNevertheless there remains a community convinced of the stone's authenticity.\n\nSwedish immigrant Olof Ohman said that he found the stone late in 1898 while clearing land he had recently acquired of trees and stumps before plowing. The stone was said to be near the crest of a small knoll rising above the wetlands, lying face down and tangled in the root system of a stunted poplar tree, estimated to be from less than 10 to about 40 years old. The artifact is about 30 × 16 × 6 inches (76 × 41 × 15 cm) in size and weighs . Ohman's ten-year-old son, Edward Ohman, noticed some markings, and the farmer later said he thought they had found an \"Indian almanac.\"\n\nDuring this period the journey of Leif Ericson to Vinland (North America) was being widely discussed and there was renewed interest in the Vikings throughout Scandinavia, stirred by the National Romanticism movement. Five years earlier Norway had participated in the World's Columbian Exposition by sending the \"Viking\", a replica of the \"Gokstad ship\", to Chicago. There was also friction between Sweden and Norway (which ultimately led to Norway's independence from Sweden in 1905). Some Norwegians claimed the stone was a Swedish hoax and there were similar Swedish accusations because the stone references a joint expedition of Norwegians and Swedes. It is thought to be more than coincidental that the stone was found among Scandinavian newcomers in Minnesota, still struggling for acceptance and quite proud of their Nordic heritage.\n\nA copy of the inscription made its way to the University of Minnesota. Olaus J. Breda (1853–1916), professor of Scandinavian Languages and Literature in the Scandinavian Department, declared the stone to be a forgery and published a discrediting article which appeared in \"Symra\" during 1910. Breda also forwarded copies of the inscription to fellow linguists and historians in Scandinavia, such as Oluf Rygh, Sophus Bugge, Gustav Storm, Magnus Olsen and Adolf Noreen. They \"unanimously pronounced the Kensington inscription a fraud and forgery of recent date\".\nThe stone was then sent to Northwestern University in Evanston, Illinois. Scholars either dismissed it as a prank or felt unable to identify a sustainable historical context and the stone was returned to Ohman. Hjalmar Holand, a Norwegian-American historian and author, claimed Ohman gave him the stone. However, the Minnesota Historical Society has a bill of sale showing Ohman sold them the stone for $10 in 1911. Holand renewed public interest with an article enthusiastically summarizing studies that were made by geologist Newton Horace Winchell (Minnesota Historical Society) and linguist George T. Flom (Philological Society of the University of Illinois), who both published opinions in 1910.\n\nAccording to Winchell, the tree under which the stone was found had been destroyed before 1910. Several nearby poplars that witnesses estimated as being about the same size were cut down and, by counting their rings, it was determined they were around 30–40 years old. One member of the team who had excavated at the find site in 1899, county schools superintendent Cleve Van Dyke, later recalled the trees being only ten or twelve years old. The surrounding county had not been settled until 1858, and settlement was severely restricted for a time by the Dakota War of 1862 (although it was reported that the best land in the township adjacent to Solem, Holmes City, was already taken by 1867, by a mixture of Swedish, Norwegian and \"Yankee\" settlers.)\n\nWinchell estimated that the inscription was roughly 500 years old, by comparing its weathering with the weathering on the backside, which he assumed was glacial and 8000 years old. Winchell also mentions in the same report that Prof. W. O. Hotchkiss, state geologist of Wisconsin, estimated that the runes were \"at least 50 to 100 years.\" Meanwhile, Flom found a strong apparent divergence between the runes used in the Kensington inscription and those in use during the 14th century. Similarly, the language of the inscription was modern compared to the Nordic languages of the 14th century.\n\nThe Kensington Runestone is on display at the n Alexandria, Minnesota.\n\nThe text consists of 9 lines on the face of the stone, and 3 lines on the edge, read as follows:\n\nFront:\nSide:\n\nThe sequences \"rr\", \"ll\" and \"gh\" represent actual digraphs. The \"AVM\" is written in Latin capitals.\nThe numbers given in Arabic numerals in the above transcription are given in the pentimal system.\nAt least seven of the runes, including those transcribed \"a, d, v, j, ä, ö\" above, are not in any standard known from the medieval period (see below for details).\nThe language of the inscription is close to modern Swedish, the transliterated text being quite easily comprehensible to \nany speaker of a modern Scandinavian language. The language being closer to the Swedish of the 19th than of the 14th century is one of the main reasons for the scholarly consensus dismissing it as a hoax.\n\nThe text translates to:\n\n\"Eight Geats and twenty-two Norwegians on an exploration journey from Vinland to the west. We had camp by two skerries one day's journey north from this stone. We were [out] to fish one day. After we came home [we] found ten men red of blood and dead. AVM (Ave Virgo Maria) save [us] from evil.\"\n\n\"[We] have ten men by the sea to look after our ships, fourteen days' travel from this island. [In the] year 1362.\"\n\nHoland took the stone to Europe and, while newspapers in Minnesota carried articles hotly debating its authenticity, the stone was quickly dismissed by Swedish linguists.\n\nFor the next 40 years, Holand struggled to sway public and scholarly opinion about the Runestone, writing articles and several books. He achieved brief success in 1949, when the stone was put on display at the Smithsonian Institution, and scholars such as William Thalbitzer and S. N. Hagen published papers supporting its authenticity.\nAt nearly the same time, Scandinavian linguists Sven Jansson, Erik Moltke, Harry Anderson and K. M. Nielsen, along with a popular book by Erik Wahlgren, again questioned the Runestone's authenticity.\n\nAlong with Wahlgren, historian Theodore C. Blegen flatly asserted Ohman had carved the artifact as a prank, possibly with help from others in the Kensington area. Further resolution seemed to come with the 1976 published transcript of an interview of Frank Walter Gran conducted by Paul Carson, Jr. on August 13, 1967 that had been recorded on audio tape. In it, Gran said his father John confessed in 1927 that Ohman made the inscription. John Gran's story however was based on second-hand anecdotes he had heard about Ohman, and although it was presented as a dying declaration, Gran lived for several more years, saying nothing more about the stone.\n\nThe possibility of the runestone being an authentic 14th-century artefact was again raised in 1982 by Robert Hall, an emeritus professor of Italian language and literature at Cornell University, who published a book (and a follow up in 1994) questioning the methodology of its critics. Hall asserted that the odd philological problems in the Runestone could be the result of normal dialectal variances in Old Swedish of the period. He further contended that critics had failed to consider the physical evidence, which he found leaning heavily in favour of authenticity.\n\nIn \"The Vikings and America\" (1986), Wahlgren again stated that the text bore linguistic abnormalities and spellings that he thought suggested the Runestone was a forgery.\n\nOne of the main linguistic arguments for the rejection of the text as genuine Old Swedish is the term \n\"opthagelse farth\" (\"updagelsefard\") \"journey of discovery\". \nThis lexeme is unattested in either Scandinavian, Low Franconian or Low German before the 16th century.\nThe term exists in modern Scandinavian (Norwegian \"oppdagingsferd\" or \"oppdagelsesferd\", Swedish \"upptäcktsfärd\")\nIt is a loan from Low German \"*updagen\", Dutch \"opdagen\", which are in turn from High German \"aufdecken\", ultimately loan-translated from French \"découvrir\" \"to discover\" in the 16th century.\nThe Norwegian historian Gustav Storm often used the modern Norwegian lexeme in late 19th-century articles on Viking exploration, creating a plausible incentive for the manufacturer of the inscription to use this word.\n\nAnother characteristic pointed out by skeptics is the text's lack of cases.\nEarly Old Swedish (14th century) still retained the four cases of Old Norse, but Late Old Swedish (15th century) reduced its case structure to two cases, so that the absence of inflection in a Swedish text of the 14th century would be an irregularity. \nSimilarly, the inscription text does not use the plural verb forms that were common in the 14th century and have only recently disappeared: for example, (plural forms in parenthesis) \"wi war\" (wörum), \"hathe\" (höfuðum), \"[wi] fiske\" (fiskaðum), \"kom\" (komum), \"fann\" (funnum) and \"wi hathe\" (hafdum).\n\nProponents of the stone's authenticity pointed to sporadic examples of these simpler forms in some 14th-century texts and to the great changes of the morphological system of the Scandinavian languages that began during the latter part of that century.\n\nThe inscription contains \"pentadic\" numerals. Such numerals are known in Scandinavia, but nearly always from relatively recent times, not from verified medieval runic monuments, on which numbers were usually spelled out as words.\n\nS. N. Hagen stated \"The Kensington alphabet is a synthesis of older unsimplified runes, later dotted runes, and a number of Latin letters ... The runes for a, n, s and t are the old Danish unsimplified forms which should have been out of use for a long time [by the 14th century]...I suggest that [a posited 14th century] creator must at some time or other in his life have been familiar with an inscription (or inscriptions) composed at a time when these unsimplified forms were still in use\" and that he \"was not a professional runic scribe before he left his homeland\".\n\nA possible origin for the irregular shape of the runes was discovered in 2004, in the 1883 notes of a then-16-year-old journeyman tailor with an interest in folk music, Edward Larsson. Larsson's aunt had migrated with her husband and son from Sweden to Crooked Lake, just outside Alexandria, in 1870.\nLarsson's sheet lists two different Futharks. The first Futhark consists of 22 runes, the last two of which are bind-runes, representing the letter-combinations EL and MW. His second Futhark consists of 27 runes, where the last 3 are specially adapted to represent the letters å, ä, and ö of the modern Swedish alphabet. The runes in this second set correspond closely to the non-standard runes in the Kensington inscription.\n\nThe abbreviation for \"Ave Maria\" consists of the Latin letters \"AVM\".\nWahlgren (1958) noted that the carver had incised a notch on the upper right hand corner of the letter V. The Massey Twins in their 2004 paper argued that this notch is consistent with a scribal abbreviation for a final \"-e\" used in the 14th century.\n\nThere is some limited historical evidence for possible 14th-century Scandinavian expeditions to North America. In a letter by Gerardus Mercator to John Dee, dated 1577, Mercator refers to a Jacob Cnoyen, who had learned that eight men returned to Norway from an expedition to the Arctic islands in 1364. One of the men, a priest, provided the King of Norway with a great deal of geographical information. \nCarl Christian Rafn in the early 19th century mentions a priest named Ivar Bardarsson, who had previously been based in Greenland and turns up in Norwegian records from 1364 onward.\n\nFurthermore, in 1354, King Magnus Eriksson of Sweden and Norway had issued a letter appointing a law officer named Paul Knutsson as leader of an expedition to the colony of Greenland, to investigate reports that the population was turning away from Christian culture.\n\nAnother of the documents reprinted by the 19th century scholars was a scholarly attempt by Icelandic Bishop Gisli Oddsson, in 1637, to compile a history of the Arctic colonies. He dated the Greenlanders' fall away from Christianity to 1342, and claimed that they had turned instead to America. Supporters of a 14th-century origin for the Kensington runestone argue that Knutson may therefore have travelled beyond Greenland to North America, in search of renegade Greenlanders, most of his expedition being killed in Minnesota and leaving just the eight voyagers to return to Norway.\n\nHowever, there is no evidence that the Knutson expedition ever set sail (the government of Norway went through considerable turmoil in 1355) and the information from Cnoyen as relayed by Mercator states specifically that the eight men who came to Norway in 1364 were not survivors of a recent expedition, but descended from the colonists who had settled the distant lands several generations earlier. Also, those early 19th century books, which aroused a great deal of interest among Scandinavian Americans, would have been available to a late 19th-century hoaxer.\nHjalmar Holand adduced the \"blond\" Indians among the Mandan on the Upper Missouri River as possible descendants of the Swedish and Norwegian explorers. This was dismissed as \"tangential\" to the Runestone issue by Alice Beck Kehoe (2004), in her book \"The Kensington Runestone, Approaching a Research Question Holistically.\"\nA possible route of such an expedition connecting the Hudson Bay with Kensington would lead up either Nelson River or Hayes River, through Lake Winnipeg, then up the Red River of the North.\nThe northern waterway begins at Traverse Gap, on the other side of which is the source of the Minnesota River, flowing to join the great Mississippi River at Saint Paul/Minneapolis.\nThis route was examined by Flom (1910), who found that explorers and traders had come from Hudson Bay to Minnesota by this route decades before the area was officially settled.\n\n\n\n"}
{"id": "3935006", "url": "https://en.wikipedia.org/wiki?curid=3935006", "title": "Kosmos 496", "text": "Kosmos 496\n\nKosmos 496 ( meaning \"Cosmos 496\") was an unmanned test of the redesigned Soyuz ferry. It did not dock with any space station. After the Soyuz 11 disaster the third seat was removed because the space was need for the two crewmen in space suits and their equipment. Kosmos 496 retained its solar arrays.\n\n\n"}
{"id": "38894374", "url": "https://en.wikipedia.org/wiki?curid=38894374", "title": "List of Legionnaires' disease outbreaks", "text": "List of Legionnaires' disease outbreaks\n\nThis is a list of Legionnaires' disease outbreaks; Legionnaire's is a potentially fatal infectious disease caused by gram negative, aerobic bacteria belonging to the genus \"Legionella\". The first reported outbreak was in Philadelphia, Pennsylvania in 1976 during a Legionnaires Convention at the Bellevue-Stratford Hotel.\n\nThe guidance issued by the UK government's Health and Safety Executive (HSE) now recommends that microbiological monitoring for wet cooling systems, using a dipslide, should be performed weekly. The guidance now also recommends that routine testing for legionella bacteria in wet cooling systems be carried out at least quarterly, and more frequently when a system is being commissioned, or if the bacteria have been identified on a previous occasion.\nFurther non-statutory UK guidance from the Water Regulations Advisory Scheme now exists for pre-heating of water in applications such as solar water heating systems.\n\nThe City of Garland, Texas requires yearly testing for legionella bacteria at cooling towers at apartment buildings.\n\nMalta requires twice yearly testing for \"Legionella\" bacteria at cooling towers and water fountains. Malta prohibits the installation of new cooling towers and evaporative condensers at health care facilities and schools.\n\nThe Texas Department of State Health Services has provided guidelines for hospitals to detect and prevent the spread of nosocomial infection due to legionella.\nThe European Working Group for Legionella Infections (EWGLI) was established in 1986 within the European Union framework to share knowledge and experience about potential sources of \"Legionella\" and their control. This group has published guidelines about the actions to be taken to limit the number of colony forming units (i.e., the \"aerobic count\") of micro-organisms per mL at 30 °C (minimum 48 hours incubation):\n\nAlmost all natural water sources contain \"Legionella\" and their presence should not be taken as an indication of a problem. The tabled figures are for total aerobic plate count, cfu/ml at 30 °C (minimum 48 hours incubation) with colony count determined by the pour plate method according to ISO 6222(21) or spread plate method on yeast extract agar. Legionella isolation can be conducted using the method developed by the US Center for Disease Control using buffered charcoal yeast extract agar with antibiotics.\n\nMany other governmental agencies, cooling tower manufacturers, and industrial trade organizations have developed design and maintenance guidelines for preventing or controlling the growth of \"Legionella\" in cooling towers. However, in the US, there are no regulations requiring testing or maintaining any specified levels in these facilities.\n\nThe bacteria grow best in warm water, like the kind found in hot tubs, cooling towers, hot water tanks, large plumbing systems, or parts of the air-conditioning systems of large buildings. Indoor ornamental fountains have been confirmed as a cause of Legionnaires' disease outbreaks, in which submerged lighting as a heat source was attributed to the outbreak in all documented cases. Controlling the growth of \"Legionella\" in ornamental fountains is touched on in many of the listed guidelines, especially for solar water heating systems.\n\nAdding an antibacterial agent to the automobiles' windshield system's reservoir is also recommended Legionellae have been discovered in up to 40% of freshwater environments and have been in up to 80% of freshwater sites by PCR hybridization assay.\n\n\"Legionella\" bacteria themselves can be inactivated by UV light. However, \"Legionella\" bacteria that grow and reproduce in amoebae or that are sheltered in corrosion particles cannot be killed by UV light alone.\n\n\"Legionella\" will grow in water at temperatures from . However, the bacteria reproduce at the greatest rate in stagnant water at temperatures of .\n\nCopper-Silver ionization is an effective industrial control and prevention process to eradicate \"Legionella\" in potable water distribution systems and cooling towers found in health facilities, hotels, nursing homes and most large buildings. In 2003, ionization became the first such hospital disinfection process to have fulfilled a proposed four-step modality evaluation; by then it had been adopted by over 100 hospitals. Additional studies indicate ionization is superior to thermal eradication.\n\nA 2011 study by Lin, Stout and Yu found Copper-Silver ionization to be the only Legionella control technology which has been validated through a 4-step scientific approach.\n\nA recent research study provided evidence that \"Legionella pneumophila\", the causative agent of Legionnaires' disease, can travel airborne at least 6 km from its source. In 2000, ASHRAE issued guidelines to maintain water systems and to decrease the chances of Legionnaires' disease transmission. The guidelines were not valued because legionella multiply in such temperatures. On the other hand, a lot of states had regulations that limited temperatures in health care facilities in order to reduce scalding injuries.\n\nIt was previously believed that transmission of the bacterium was restricted to much shorter distances. A team of French scientists reviewed the details of an epidemic of Legionnaires' disease that took place in Pas-de-Calais in northern France in 2003–2004. There were 86 confirmed cases during the outbreak, of whom 18 died. The source of infection was identified as a cooling tower in a petrochemical plant, and an analysis of those affected in the outbreak revealed that some infected people lived as far as 6–7 km from the plant.\n\nA study of Legionnaires' disease cases in May 2005 in Sarpsborg, Norway concluded that: \"The high velocity, large drift, and high humidity in the air scrubber may have contributed to the wide spread of \"Legionella\" species, probably for >10 km.\"\n\nIn 2010 a study by the UK Health Protection Agency reported that 20% of cases may be caused by infected windscreen washer systems filled with pure water. The finding came after researchers spotted that professional drivers are five times more likely to contract the disease. No cases of infected systems were found whenever a suitable washer fluid was used.\n\nTemperature affects the survival of \"Legionella\" as follows:\n\nRemoving slime, which can carry legionellae when airborne, may be an effective control process.\n\n\n"}
{"id": "3395591", "url": "https://en.wikipedia.org/wiki?curid=3395591", "title": "List of Lepidoptera that feed on Viburnum", "text": "List of Lepidoptera that feed on Viburnum\n\nViburnum species are used as food plants by the larvae of a number of Lepidoptera species including:\n\nSpecies which feed exclusively on \"Viburnum\"\n\n\nSpecies which feed on \"Viburnum\" among other plants\n\n\n"}
{"id": "6064211", "url": "https://en.wikipedia.org/wiki?curid=6064211", "title": "List of U.S. state poems", "text": "List of U.S. state poems\n\n"}
{"id": "343426", "url": "https://en.wikipedia.org/wiki?curid=343426", "title": "List of craters on Europa", "text": "List of craters on Europa\n\nThis is a list of craters on Europa. The surface of Jupiter's moon Europa is very young, geologically speaking, and as a result there are very few craters. Furthermore, as Europa's surface is potentially made of weak water ice over a liquid ocean, most surviving craters have slumped so that their structure is very low in relief. Most of the craters that are large enough to have names are named after prominent figures in Celtic myths and folklore.\n\n"}
{"id": "18883674", "url": "https://en.wikipedia.org/wiki?curid=18883674", "title": "List of people with tinnitus", "text": "List of people with tinnitus\n\nThis is a list of notable people that have been diagnosed with tinnitus.\n"}
{"id": "42403016", "url": "https://en.wikipedia.org/wiki?curid=42403016", "title": "Logology (science of science)", "text": "Logology (science of science)\n\nLogology (\"the science of science\") is the study of all aspects of science and of its practitioners—aspects philosophical, biological, psychological, societal, historical, political, institutional, financial. The term \"logology\" is used here as a synonym for the equivalent term \"science of science\" and the semi-equivalent term \"sociology of science\".\n\nThe term \"logology\" is back-formed from \"-logy\" (as in \"geo\"logy\"\", \"anthropo\"logy\"\", \"socio\"logy\"\", etc.) in the sense of the \"study of study\" or the \"science of science\"—or, more plainly, the \"study of science\". \nThe word \"logology\" provides grammatical variants not available with the earlier terms \"science of science\" and \"sociology of science\"—\"logologist\", \"to logologize\", \"logological\", \"logologically\".\n\nThe early 20th century brought calls, initially from sociologists, for the creation of a new, empirically-based science that would study the scientific enterprise itself. The early proposals were put forward with some hesitancy and tentativeness. The new meta-science would be given a variety of names, including \"science of knowledge\", \"science of science\", \"sociology of science\", and \"logology\".\n\nFlorian Znaniecki, who is considered to be the founder of Polish academic sociology, and who in 1954 also served as the 44th president of the American Sociological Association, opened a 1923 article:\n\n\"[T]hough theoretical reflection on knowledge — which arose as early as Heraclitus and the Eleatics — stretches... unbroken... through the history of human thought to the present day... we are now witnessing the creation of a new \"science of knowledge\" [author's emphasis] whose relation to the old inquiries may be compared with the relation of modern physics and chemistry to the 'natural philosophy' that preceded them, or of contemporary sociology to the 'political philosophy' of antiquity and the Renaissance. [T]here is beginning to take shape a concept of a single, general theory of knowledge... permitting of empirical study... This theory... is coming to be distinguished clearly from epistemology, from normative logic, and from a strictly descriptive history of knowledge.\"\n\nA dozen years later, Polish husband-and-wife sociologists Stanisław Ossowski and Maria Ossowska (the \"Ossowscy\") took up the same subject in an article on \"The Science of Science\" whose 1935 English-language version first introduced the term \"science of science\" to the world. The article postulated that the new discipline would subsume such earlier ones as epistemology, the philosophy of science, the \"psychology of science\", and the \"sociology of science\". \"Science of science\" would also concern itself with questions of a practical character such as social and state policy in relation to science; the organization of institutions of higher learning, of research institutes, and of scientific expeditions; the protection of scientific workers, etc. It would concern itself as well with historical questions: the history of the conception of science, of the scientist, of the various disciplines, and of learning in general.\n\nIn their 1935 paper, the \"Ossowscy\" mentioned the German philosopher (1899–1953) who, in fragmentary 1931 remarks, had enumerated some possible types of research in the science of science and had proposed his own name for the new discipline: \"scientiology\". The \"Ossowscy\" took issue with the name: \"Those who wish to replace the expression 'science of science' by a one-word term [that] sound[s] international, in the belief that only after receiving such a name [will] a given group of [questions be] officially dubbed an autonomous discipline, [might] be reminded of the name \"mathesiology\", proposed long ago for similar purposes [by the French mathematician and physicist André-Marie Ampère (1775–1836)].\"\n\nYet, before long, in Poland, the unwieldy three-word term \"nauka o nauce\" (\"science of science\") was replaced by the more versatile one-word term \"naukoznawstwo\" (\"logology\") and its natural variants: \"naukoznawca\" (\"logologist\"), \"naukoznawczy\" (\"logological\"), and \"naukoznawczo\" (\"logologically\"). And just after World War II, only 11 years after the \"Ossowscy\"'s landmark 1935 paper, the year 1946 saw the founding of the Polish Academy of Sciences' quarterly \"Zagadnienia Naukoznawstwa\" (Logology) — long before similar journals in many other countries. \n\nThe new discipline also took root elsewhere — in English-speaking countries, without the benefit of a one-word name.\n\nThe term \"science\" (from the Latin \"scientia\", \"knowledge\") means somewhat different things in different languages. In the English language, \"science\", when unqualified, generally refers to the \"natural\", \"exact\", or \"hard sciences\". The corresponding terms in other languages, for example , German, and , refer to a broader domain that includes not only the exact sciences (logic and mathematics) and the natural sciences (physics, chemistry, biology, medicine, Earth sciences, geography, astronomy, etc.) but also the engineering sciences, social sciences (history, geography, psychology, physical anthropology, sociology, political science, economics, international relations, pedagogy, etc.), and humanities (philosophy, history, cultural anthropology, linguistics, etc.).\n\nUniversity of Amsterdam humanities professor Rens Bod points out that science—defined as a set of methods that describes and interprets observed or inferred phenomena, past or present, aimed at testing hypotheses and building theories—applies to such humanities fields as philology, art history, musicology, linguistics, archaeology, historiography, and literary studies.\n\nBod gives a historic example of scientific textual analysis. in 1440 the Italian philologist Lorenzo Valla exposed the Latin document \"Donatio Constantini\" (The Donation of Constantine)—which was used by the Catholic Church to legitimize its claim to lands in the Western Roman Empire—as a forgery. Valla used historical, linguistic, and philological evidence, including counterfactual reasoning, to rebut the document. Valla found words and constructions in the document that could not have been used by anyone in the time of Emperor Constantine I, at the beginning of the fourth century A.D. For example, the late Latin word \"feudum\" (\"fief\") referred to the feudal system, a medieval invention that did not exist before the seventh century A.D. Valla's methods were those of science, and inspired the later scientifically-minded work of Dutch humanist Erasmus of Rotterdam (1466–1536), Leiden University professor Joseph Justus Scaliger (1540–1609), and philosopher Baruch Spinoza (1632–77).\n\nScience's search for the truth about various aspects of reality entails the question of the very \"knowability\" of reality. Philosopher Thomas Nagel writes: \"[In t]he pursuit of scientific knowledge through the interaction between theory and observation... we test theories against their observational consequences, but we also question or reinterpret our observations in light of theory. (The choice between geocentric and heliocentric theories at the time of the Copernican revolution is a vivid example.) ... \nHow things seem is the starting point for all knowledge, and its development through further correction, extension, and elaboration is inevitably the result of more seemings—considered judgments about the plausibility and consequences of different theoretical hypotheses. The only way to pursue the truth is to consider what seems true, after careful reflection of a kind appropriate to the subject matter, in light of all the relevant data, principles, and circumstances.\"\n\nThe question of knowability is approached from a different perspective by physicist-astronomer Marcelo Gleiser: \"What we observe is not nature itself but nature as discerned through data we collect from machines. In consequence, the scientific worldview depends on the information we can acquire through our instruments. And given that our tools are limited, our view of the world is necessarily myopic. We can see only so far into the nature of things, and our ever shifting scientific worldview reflects this fundamental limitation on how we perceive reality.\" Gleiser cites the condition of biology before and after the invention of the microscope or gene sequencing; of astronomy before and after the telescope; of particle physics before and after colliders or fast electronics. \"[T]he theories we build and the worldviews we construct change as our tools of exploration transform. This trend is the trademark of science.\"\n\nWrites Gleiser: \"There is nothing defeatist in understanding the limitations of the scientific approach to knowledge... What should change is a sense of scientific triumphalism – the belief that no question is beyond the reach of scientific discourse.\n\n\"There are clear unknowables in science – reasonable questions that, unless currently accepted laws of nature are violated, we cannot find answers to. One example is the multiverse: the conjecture that our universe is but one among a multitude of others, each potentially with a different set of laws of nature. Other universes lie outside our causal horizon, meaning that we cannot receive or send signals to them. Any evidence for their existence would be circumstantial: for example, scars in the radiation permeating space because of a past collision with a neighboring universe.\"\n\nGleiser gives three further examples of unknowables, involving the origins of the universe; of life; and of mind:\n\n\"Scientific accounts of the origin of the universe are incomplete because they must rely on a conceptual framework to even begin to work: energy conservation, relativity, quantum physics, for instance. Why does the universe operate under these laws and not others?\n\n\"Similarly, unless we can prove that only one or very few biochemical pathways exist from nonlife to life, we cannot know for sure how life originated on Earth.\n\n\"For consciousness, the problem is the jump from the material to the subjective – for example, from firing neurons to the experience of pain or the color red. Perhaps some kind of rudimentary consciousness could emerge in a sufficiently complex machine. But how could we tell? How do we establish – as opposed to conjecture – that something is conscious?\" Paradoxically, writes Gleiser, it is through our consciousness that we make sense of the world, even if imperfectly. \"Can we fully understand something of which we are a part?\"\n\nTheoretical physicist and mathematician Freeman Dyson explains that \"[s]cience consists of facts and theories\":\n\n\"Facts are supposed to be true or false. They are discovered by observers or experimenters. A scientist who claims to have discovered a fact that turns out to be wrong is judged harshly... \n\n\"Theories have an entirely different status. They are free creations of the human mind, intended to describe our understanding of nature. Since our understanding is incomplete, theories are provisional. Theories are tools of understanding, and a tool does not need to be precisely true in order to be useful. Theories are supposed to be more-or-less true... A scientist who invents a theory that turns out to be wrong is judged leniently.\"\n\nDyson cites a psychologist's description of how theories are born: \"We can't live in a state of perpetual doubt, so we make up the best story possible and we live as if the story were true.\" Dyson writes: \"The inventor of a brilliant idea cannot tell whether it is right or wrong.\" The passionate pursuit of wrong theories is a normal part of the development of science. Dyson cites, after Mario Livio, five famous scientists who made major contributions to the understanding of nature but also believed firmly in a theory that proved wrong.\n\nCharles Darwin explained the evolution of life with his theory of natural selection of inherited variations, but he believed in a theory of blending inheritance that made the propagation of new variations impossible. He never read Gregor Mendel's studies that showed that the laws of inheritance would become simple when inheritance was considered as a random process. Though Darwin in 1866 did the same experiment that Mendel had, Darwin did not get comparable results because he failed to appreciate the statistical importance of using very large experimental samples. Eventually, Mendelian inheritance by random variation would, no thanks to Darwin, provide the raw material for Darwinian selection to work on.\n\nWilliam Thomson (Lord Kelvin) discovered basic laws of energy and heat, then used these laws to calculate an estimate of the age of the earth that was too short by a factor of fifty. He based his calculation on the belief that the earth's mantle was solid and could transfer heat from the interior to the surface only by conduction. It is now known that the mantle is partly fluid and transfers most of the heat by the far more efficient process of convection, which carries heat by a massive circulation of hot rock moving upward and cooler rock moving downward. Kelvin could see the eruptions of volcanoes bringing hot liquid from deep underground to the surface; but his skill in calculation blinded him to processes, such as volcanic eruptions, that could not be calculated.\n\nLinus Pauling discovered the chemical structure of protein and proposed a completely wrong structure for DNA, which carries hereditary information from parent to offspring. Pauling guessed a wrong structure for DNA because he assumed that a pattern that worked for protein would also work for DNA. He overlooked the gross chemical differences between protein and DNA. Francis Crick and James Watson paid attention to the differences and found the correct structure for DNA that Pauling had missed a year earlier.\n\nAstronomer Fred Hoyle discovered the process by which the heavier elements essential to life are created by nuclear reactions in the cores of massive stars. He then proposed a theory of the history of the universe known as steady-state cosmology, which has the universe existing forever without an initial Big Bang (as Hoyle derisively dubbed it). He held his belief in the steady state long after observations proved that the Big Bang had happened.\n\nAlbert Einstein discovered the theory of space, time, and gravitation known as general relativity, and then added a cosmological constant, later known as dark energy. Subsequently Einstein withdrew his proposal of dark energy, believing it unnecessary. Long after his death, observations suggested that dark energy really exists, so that Einstein's addition to the theory may have been right; and his withdrawal, wrong.\n\nTo Mario Livio's five examples of scientists who blundered, Dyson adds a sixth: himself. Dyson had concluded, on theoretical principles, that what was to become known as the W-particle, a charged weak boson, could not exist. An experiment conducted at CERN, in Geneva, later proved him wrong. \"With hindsight I could see several reasons why my stability argument would not apply to W-particles. [They] are too massive and too short-lived to be a constituent of anything that resembles ordinary matter.\"\n\nSteven Weinberg, 1979 Nobel laureate in physics, and a historian of science, writes that the core goal of science has always been the same: \"to explain the world\"; and in reviewing earlier periods of scientific thought, he concludes that only since Isaac Newton has that goal been pursued more or less correctly. He decries the \"intellectual snobbery\" that Plato and Aristotle showed in their disdain for science's practical applications, and he holds Francis Bacon and René Descartes to have been the \"most overrated\" among the forerunners of modern science (they tried to prescribe rules for conducting science, which \"never works\").\n\nWeinberg draws parallels between past and present science, as when a scientific theory is \"fine-tuned\" (adjusted) to make certain quantities equal, without any understanding of why they \"should\" be equal. Such adjusting vitiated the celestial models of Plato's followers, in which different spheres carrying the planets and stars were assumed, with no good reason, to rotate in exact unison. But, Weinberg writes, a similar fine-tuning also besets current efforts to understand the \"dark energy\" that is speeding up the expansion of the universe.\n\nAncient science has been described as having gotten off to a good start, then faltered. The doctrine of atomism, propounded by the pre-Socratic philosophers Leucippus and Democritus, was naturalistic, accounting for the workings of the world by impersonal processes, not by divine volitions. Nevertheless these pre-Socratics come up short for Weinberg as proto-scientists, in that they apparently never tried to justify their speculations or to test them against evidence.\n\nWeinberg believes that science faltered early on due to Plato's suggestion that scientific truth could be attained by reason alone, disregarding empirical observation, and due to Aristotle's attempt to explain nature teleologically—in terms of ends and purposes. Plato's ideal of attaining knowledge of the world by unaided intellect was \"a false goal inspired by mathematics\"—one that for centuries \"stood in the way of progress that could be based only on careful analysis of careful observation.\" And it \"never was fruitful\" to ask, as Aristotle did, \"what is the purpose of this or that physical phenomenon.\"\n\nA scientific field in which the Greek and Hellenistic world did make progress was astronomy. This was partly for practical reasons: the sky had long served as compass, clock, and calendar. Also, the regularity of the movements of heavenly bodies made them simpler to describe than earthly phenomena. But not \"too\" simple: though the sun, moon and \"fixed stars\" seemed regular in their celestial circuits, the \"wandering stars\"—the planets—were puzzling; they seemed to move at variable speeds, and even to reverse direction. Writes Weinberg: \"Much of the story of the emergence of modern science deals with the effort, extending over two millennia, to explain the peculiar motions of the planets.\"\n\nThe challenge was to make sense of the apparently irregular wanderings of the planets on the assumption that all heavenly motion is actually circular and uniform in speed. Circular, because Plato held the circle to be the most perfect and symmetrical form; and therefore circular motion, at uniform speed, was most fitting for celestial bodies. Aristotle agreed with Plato. In Aristotle's cosmos, everything had a \"natural\" tendency to motion that fulfilled its inner potential. For the cosmos' sublunary part (the region below the moon), the natural tendency was to move in a straight line: downward, for earthen things (such as rocks) and water; upward, for air and fiery things (such as sparks). But in the celestial realm things were not composed of earth, water, air, or fire, but of a \"fifth element\", or \"quintessence,\" which was perfect and eternal. And its natural motion was uniformly circular. The stars, the sun, the moon, and the planets were carried in their orbits by a complicated arrangement of crystalline spheres, all centered around an immobile earth.\n\nThe Platonic-Aristotelian conviction that celestial motions must be circular persisted stubbornly. It was fundamental to the astronomer Ptolemy's system, which improved on Aristotle's in conforming to the astronomical data by allowing the planets to move in combinations of circles called \"epicycles\".\n\nIt even survived the Copernican revolution. Copernicus was conservative in his Platonic reverence for the circle as the heavenly pattern. According to Weinberg, Copernicus was motivated to dethrone the earth in favor of the sun as the immobile center of the cosmos largely by aesthetic considerations: he objected to the fact that Ptolemy, though faithful to Plato's requirement that heavenly motion be circular, had departed from Plato's other requirement that it be of uniform speed. By putting the sun at the center—actually, somewhat off-center—Copernicus sought to honor circularity while restoring uniformity. But to make his system fit the observations as well as Ptolemy's system, Copernicus had to introduce still more epicycles. That was a mistake that, writes Weinberg, illustrates a recurrent theme in the history of science: \"A simple and beautiful theory that agrees pretty well with observation is often closer to the truth than a complicated ugly theory that agrees better with observation.\"\n\nThe planets, however, do not move in perfect circles but in ellipses. It was Johannes Kepler, about a century after Copernicus, who reluctantly (for he too had Platonic affinities) realized this. Thanks to his examination of the meticulous observations compiled by astronomer Tycho Brahe, Kepler \"was the first to understand the nature of the departures from uniform circular motion that had puzzled astronomers since the time of Plato.\"\n\nThe replacement of circles by supposedly ugly ellipses overthrew Plato's notion of perfection as the celestial explanatory principle. It also destroyed Aristotle's model of the planets carried in their orbits by crystalline spheres; writes Weinberg, \"there is no solid body whose rotation can produce an ellipse.\" Even if a planet were attached to an ellipsoid crystal, that crystal's rotation would still trace a circle. And if the planets were pursuing their elliptical motion through empty space, then what was holding them in their orbits?\n\nScience had reached the threshold of explaining the world not geometrically, according to shape, but dynamically, according to force. It was Isaac Newton who finally crossed that threshold. He was the first to formulate, in his \"laws of motion\", the concept of force. He demonstrated that Kepler's ellipses were the very orbits the planets would take if they were attracted toward the sun by a force that decreased as the square of the planet's distance from the sun. And by comparing the moon's motion in its orbit around the earth to the motion of, perhaps, an apple as it falls to the ground, Newton deduced that the forces governing them were quantitatively the same. \"This,\" writes Weinberg, \"was the climactic step in the unification of the celestial and terrestrial in science.\"\n\nBy formulating a unified explanation of the behavior of planets, comets, moons, tides, and apples, writes Weinberg, Newton \"provided an irresistible model for what a physical theory should be\"—a model that fit no preexisting metaphysical criterion. In contrast to Aristotle, who claimed to explain the falling of a rock by appeal to its inner striving, Newton was unconcerned with finding a deeper cause for gravity. He declared in his \"Philosophiæ Naturalis Principia Mathematica\": \"I do not 'feign' hypotheses.\" What mattered were his mathematically stated principles describing this force, and their ability to account for a vast range of phenomena.\n\nAbout two centuries later, in 1915, a deeper explanation for Newton's law of gravitation was found in Albert Einstein's general theory of relativity: gravity could be explained as a manifestation of the curvature in spacetime resulting from the presence of matter and energy. Successful theories like Newton's, writes Weinberg, may work for reasons that their creators do not understand—reasons that deeper theories will later reveal. Scientific progress is not a matter of building theories on a foundation of reason, but of unifying a greater range of phenomena under simpler and more general principles.\n\nSince 1950, when Alan Turing proposed what has come to be called the “Turing test,” there has been speculation whether machines such as computers can possess intelligence; and, if so, whether intelligent machines could become a threat to human intellectual and scientific ascendancy—or even an existential threat to humanity. John Searle points out common confusion about the correct interpretation of computation and information technology. \"For example, one routinely reads that in exactly the same sense in which Garry Kasparov… beat Anatoly Karpov in chess, the computer called Deep Blue played and beat Kasparov... [T]his claim is [obviously] suspect. In order for Kasparov to play and win, he has to be conscious that he is playing chess, and conscious of a thousand other things... Deep Blue is conscious of none of these things because it is not conscious of anything at all. Why is consciousness so important? You cannot literally play chess or do much of anything else cognitive if you are totally disassociated from consciousness.\"\n\nSearle explains that, \"in the literal, real, observer-independent sense in which humans compute, mechanical computers do not compute. They go through a set of transitions in electronic states that we can interpret computationally. The transitions in those electronic states are absolute or observer-independent, but \"the computation is observer-relative\". The transitions in physical states are just electrical sequences unless some conscious agent can give them a computational interpretation... There is no psychological reality at all to what is happening in the [computer].\"\n\n\"[A] digital computer\", writes Searle, \"is a syntactical machine. It manipulates symbols and does nothing else. For this reason, the project of creating human intelligence by designing a computer program that will pass the Turing Test... is doomed from the start. The appropriately programmed computer has a syntax [rules for constructing or transforming the symbols and words of a language] but no semantics [comprehension of meaning]... Minds, on the other hand, have mental or semantic content.\"\n\nProfessor of psychology and neural science Gary Marcus points out a so far insuperable stumbling block to artificial intelligence: an incapacity for reliable disambiguation. \"[V]irtually every sentence [that people generate] is ambiguous, often in multiple ways. Our brain is so good at comprehending language that we do not usually notice.\" A prominent example is known as the \"pronoun disambiguation problem\" (\"PDP\"): a machine has no way of determining to whom or what a pronoun in a sentence—such as \"he\", \"she\" or \"it\"—refers.\n\nComputer scientist Pedro Domingos writes: \"AIs are like autistic savants and will remain so for the foreseeable future... AIs lack common sense and can easily make errors that a human never would... They are also liable to take our instructions too literally, giving us precisely what we asked for instead of what we actually wanted.\n\nKai-Fu Lee, a Beijing-based venture capitalist, artificial-intelligence (AI) expert with a Ph.D. in computer science from Carnegie Mellon University, and author of the 2018 book, \"AI Superpowers: China, Silicon Valley, and the New World Order\", emphasized in a 2018 PBS \"Amanpour\" interview with Hari Sreenivasan that AI, with all its capabilities, will never be capable of creativity or empathy.\n\nFifty years before Florian Znaniecki published his 1923 paper proposing the creation of an empirical field of study to study the field of science, Aleksander Głowacki (better known by his pen name, Bolesław Prus) had made the same proposal. In an 1873 public lecture \"On Discoveries and Inventions\", Prus said:\nPrus defines \"discovery\" as \"the finding out of a thing that has existed and exists in nature, but which was previously unknown to people\"; and \"invention\" as \"the making of a thing that has not previously existed, and which nature itself cannot make.\"\n\nHe illustrates the concept of \"discovery\":\n\nPrus illustrates the concept of \"invention\":\n\nAccording to Prus, \"inventions and discoveries are natural phenomena and, as such, are subject to certain laws.\" Those are the laws of \"gradualness\", \"dependence\", and \"combination\".\n\nEach of Prus' three \"laws\" entails important corollaries. The law of gradualness implies the following:\n\nFrom the law of dependence flow the following corollaries:\n\nFinally, Prus' corollaries to his law of combination:\n\nBut, asks Prus, \"What force drives [the] toilsome, often frustrated efforts [of the investigators]? What thread will clew these people through hitherto unexplored fields of study?\"\n\nPrus holds that, over time, the multiplication of discoveries and inventions has improved the quality of people's lives and has expanded their knowledge. \"This gradual advance of civilized societies, this constant growth in knowledge of the objects that exist in nature, this constant increase in the number of tools and useful materials, is termed \"progress\", or the \"growth of civilization.\"\" Conversely, Prus warns, \"societies and people that do not make inventions or know how to use them, lead miserable lives and ultimately perish.\"\n\nA fundamental feature of the scientific enterprise is reproducibility of results. \"For decades\", writes Shannon Palus, \"it has been... an open secret that a [considerable part] of the literature in some fields is plain wrong.\" This effectively sabotages the scientific enterprise and costs the world many billions of dollars annually in wasted resources. Militating against reproducibility is scientists' reluctance to share techniques, for fear of forfeiting one's advantage to other scientists. Also, scientific journals and tenure committees tend to prize impressive new results rather than gradual advances that systematically build on existing literature. Scientists who quietly fact-check others' work or spend extra time ensuring that their own protocols are easy for other researchers to understand, gain little for themselves.\n\nWith a view to improving reproducibility of scientific results, it has been suggested that research-funding agencies finance only projects that include a plan for making their work transparent. In 2016 the U.S. National Institutes of Health introduced new application instructions and review questions to encourage scientists to improve reproducibility. The NIH requests more information on how the study builds on previous work, and a list of variables that could affect the study, such as the sex of animal subjects—a previously overlooked factor that led many studies to describe phenomena found in male animals as universal.\n\nLikewise, the questions that a funder can ask in advance could be asked by journals and reviewers. One solution is \"registered reports\", a preregistration of studies whereby a scientist submits, for publication, research analysis and design plans before actually doing the study. Peer reviewers then evaluate the methodology, and the journal promises to print the results, no matter what they are. In order to prevent over-reliance on preregistered studies—which could encourage safer, less venturesome research, thus over-correcting the problem—the preregistered-studies model could be operated in tandem with the traditional results-focused model, which may sometimes be more friendly to serendipitous discoveries.\n\nA 2016 \"Scientific American\" report highlights the role of \"rediscovery\" in science. Indiana University Bloomington researchers combed through 22 million scientific papers published over the previous century and found dozens of \"Sleeping Beauties\"—studies that lay dormant for years before getting noticed. The top finds, which languished longest and later received the most intense attention from scientists, came from the fields of chemistry, physics, and statistics. The dormant findings were wakened by scientists from other disciplines, such as medicine, in search of fresh insights, and by the ability to test once-theoretical postulations. Sleeping Beauties will likely become even more common in the future because of increasing accessibility of scientific literature. The \"Scientific American\" report lists the top 15 Sleeping Beauties: 7 in chemistry, 5 in physics, 2 in statistics, and 1 in metallurgy. Examples include:\n\nHerbert Freundlich's \"Concerning Adsorption in Solutions\" (1906), the first mathematical model of adsorption, when atoms or molecules adhere to a surface. Today both environmental remediation and decontamination in industrial settings rely heavily on adsorption.\n\nA. Einstein, B. Podolsky and N. Rosen, \"Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?\" \"Physical Review\", vol. 47 (May 15, 1935), pp. 777–780. This famous thought experiment in quantum physics—now known as the EPR paradox, after the authors' surname initials—was discussed \"theoretically\" when it first came out. It was not until the 1970s that physics had the experimental means to test quantum entanglement.\n\nJ[ohn] Turkevich, P. C. Stevenson, J. Hillier, \"A Study of the Nucleation and Growth Processes in the Synthesis of Colloidal Gold\", \"Discuss. Faraday. Soc.\", 1951, 11, pp. 55–75, explains how to suspend gold nanoparticles in liquid. It owes its awakening to medicine, which now employs gold nanoparticles to detect tumors and deliver drugs.\n\nWilliam S. Hummers and Richard E Offeman, \"Preparation of Graphitic Oxide\", \"Journal of the American Chemical Society\", vol. 80, no. 6 (March 20, 1958), p. 1339, introduced Hummers' Method, a technique for making graphite oxide. Recent interest in graphene's potential has brought the 1958 paper to attention. Graphite oxide could serve as a reliable intermediate for the 2-D material.\n\nHistorians and sociologists have remarked the occurrence, in science, of \"multiple independent discovery\". Sociologist Robert K. Merton defined such \"multiples\" as instances in which similar discoveries are made by scientists working independently of each other. \"Sometimes the discoveries are simultaneous or almost so; sometimes a scientist will make a new discovery which, unknown to him, somebody else has made years before.\" Commonly cited examples of multiple independent discovery are the 17th-century independent formulation of calculus by Isaac Newton, Gottfried Wilhelm Leibniz, and others; the 18th-century independent discovery of oxygen by Carl Wilhelm Scheele, Joseph Priestley, Antoine Lavoisier, and others; and the 19th-century independent formulation of the theory of evolution of species by Charles Darwin and Alfred Russel Wallace. \n\nMerton contrasted a \"multiple\" with a \"singleton\" — a discovery that has been made uniquely by a single scientist or group of scientists working together. He believed that it is multiple discoveries, rather than unique ones, that represent the \"common\" pattern in science.\n\nMultiple discoveries in the history of science provide evidence for evolutionary models of science and technology, such as memetics (the study of self-replicating units of culture), evolutionary epistemology (which applies the concepts of biological evolution to study of the growth of human knowledge), and cultural selection theory (which studies sociological and cultural evolution in a Darwinian manner). A recombinant-DNA-inspired \"paradigm of paradigms\", describing a mechanism of \"recombinant conceptualization\", predicates that a new concept arises through the crossing of pre-existing concepts and facts. This is what is meant when one says that a scientist, scholar, or artist has been \"influenced by\" another — etymologically, that a concept of the latter's has \"flowed into\" the mind of the former.\n\nThe phenomenon of multiple independent discoveries and inventions can be viewed as a consequence of Bolesław Prus' three laws of gradualness, dependence, and combination (see \"Discoveries and inventions\", above). The first two laws may, in turn, be seen as corollaries to the third law, since the laws of gradualness and dependence imply the impossibility of certain scientific or technological advances pending the availability of certain theories, facts, or technologies that must be combined to produce a given scientific or technological advance.\n\nA practical question concerns the traits that enable some individuals to achieve extraordinary results in their fields of work – and how such creativity can be fostered. Melissa Schilling, a student of innovation strategy, has identified some traits shared by eight major innovators in natural science or technology: Benjamin Franklin (1706–90), Thomas Edison (1847–1931), Nikola Tesla (1856–1943), Maria Skłodowska Curie (1867–1934), Dean Kamen (born 1951), Steve Jobs (1955–2011), Albert Einstein (1879–1955), and Elon Musk (born 1971).\n\nSchilling chose innovators in natural science and technology rather than in other fields because she found much more consensus about important contributions to natural science and technology than, for example, to art or music. She further limited the set to individuals associated with \"multiple\" innovations. \"When an individual is associated with only a single major invention, it is much harder to know whether the invention was caused by the inventor's personal characteristics or by simply being at the right place at the right time.\"\n\nThe eight individuals were all extremely intelligent, but \"that is not enough to make someone a serial breakthrough innovator.\" Nearly all these innovators showed very high levels of social detachment, or separateness (a notable exception being Benjamin Franklin). \"Their isolation meant that they were less exposed to dominant ideas and norms, and their sense of not belonging meant that even when exposed to dominant ideas and norms, they were often less inclined to adopt them.\" From an early age, they had all shown extreme faith in their ability to overcome obstacles – what psychology calls \"self-efficacy\".\n\n\"Most [of them, writes Schilling] were driven by idealism, a superordinate goal that was more important than their own comfort, reputation, or families. Nikola Tesla wanted to free mankind from labor through unlimited free energy and to achieve international peace through global communication. Elon Musk wants to solve the world's energy problems and colonize Mars. Benjamin Franklin was seeking greater social harmony and productivity through the ideals of egalitarianism, tolerance, industriousness, temperance, and charity. Marie Curie had been inspired by Polish Positivism's argument that Poland, which was under Tsarist Russian rule, could be preserved only through the pursuit of education and technological advance by all Poles – \"including women\".\"\n\nMost of the innovators also worked hard and tirelessly because they found work extremely rewarding. Some had an extremely high need for achievement. Many also appeared to find work autotelic – rewarding for its own sake. A surprisingly large portion of the breakthrough innovators have been autodidacts – self-taught persons – and excelled much more outside the classroom than inside.\n\n\"Almost all breakthrough innovation,\" writes Schilling, \"starts with an unusual idea or with beliefs that break with conventional wisdom... However, creative ideas alone are almost never enough. Many people have creative ideas, even brilliant ones. But usually we lack the time, knowledge, money, or motivation to act on those ideas.\" It is generally hard to get others' help in implementing original ideas because the ideas are often initially hard for others to understand and value. Thus each of Schilling's breakthrough innovators showed \"extraordinary\" effort and persistence. Even so, writes Schilling, \"being at the right place at the right time still matter[ed].\"\n\nWhen Swiss botanist Simon Schwendener discovered in the 1860s that lichens were a symbiotic partnership between a fungus and an alga, his finding at first met with resistance from the scientific community. After his discovery that the fungus--which cannot make its own food--provides the lichen's structure, while the alga's contribution is its photosynthetic production of food, it was found that in some lichens a cyanobacterium provides the food—and a handful of lichen species contain \"both\" an alga and a cyanobacterium, along with the fungus.\n\nA self-taught naturalist, Trevor Goward, has helped create a paradigm shift in the study of lichens and perhaps of all life-forms by doing something that people did in pre-scientific times: going out into nature and closely observing. His essays about lichens were largely ignored by most researchers because Goward has no scientific degrees and because some of his radical ideas are not supported by rigorous data.\n\nWhen Goward told Toby Spribille, who at the time lacked a high-school education, about some of his lichenological ideas, Goward recalls, \"He said I was delusional.\" Ultimately Spribille passed a high-school equivalency examination, obtained a Ph.D. in lichenology at the University of Graz in Austria, and became an assistant professor of the ecology and evolution of symbiosis at the University of Alberta. In July 2016 Spribille and his co-authors published a ground-breaking paper in \"Science\" revealing that many lichens contain a second fungus. \n\nSpribille credits Goward with having \"a huge influence on my thinking. [His essays] gave me license to think about lichens in [an unorthodox way] and freed me to see the patterns I worked out in \"Bryoria\" with my co-authors.\" Even so, \"one of the most difficult things was allowing myself to have an open mind to the idea that 150 years of literature may have entirely missed the theoretical possibility that there would be more than one fungal partner in the lichen symbiosis.\" Spribille says that academia's emphasis on the canon of what others have established as important is inherently limiting.\n\nContrary to previous studies indicating that higher intelligence makes for better leaders in various fields of endeavor, later research suggests that, at a certain point, a higher IQ can be viewed as harmful. Decades ago, psychologist Dean Simonton suggested that brilliant leaders' words may go over people's heads, their solutions could be more complicated to implement, and followers might find it harder to relate to them. At last, in the July 2017 \"Journal of Applied Psychology\", he and two colleagues published the results of actual tests of the hypothesis. \n\nStudied were 379 men and women business leaders in 30 countries, including the fields of banking, retail, and technology. The managers took IQ tests--an imperfect but robust predictor of performance in many areas--and each was rated on leadership style and effectiveness by an average of 8 co-workers. IQ correlated positively with ratings of leadership effectiveness, strategy formation, vision, and several other characteristics—up to a point. The ratings peaked at an IQ of about 120, which is higher than some 80% of office workers. Beyond that, the ratings declined. The researchers suggested that the ideal IQ could be higher or lower in various fields, depending on whether technical or social skills are more valued in a given work culture. \n\nPsychologist Paul Sackett, not involved in the research, comments: \"To me, the right interpretation of the work would be that it highlights a need to understand what high-IQ leaders do that leads to lower perceptions by followers. The wrong interpretation would be,'Don't hire high-IQ leaders.'\" The study's lead author, psychologist John Antonakis, suggests that leaders should use their intelligence to generate creative metaphors that will persuade and inspire others. \"I think the only way a smart person can signal their intelligence appropriately and still connect with the people,\" says Antonakis, \"is to speak in charismatic ways.\"\n\nAcademic specialization produces great benefits for science and technology by focusing effort on discrete disciplines. But over-narrow specialization can act as a roadblock to progress in areas that overlap established disciplines. \n\nIn 2017, in Manhattan, James Harris Simons, a noted mathematician and retired founder of one of the world's largest hedge funds, inaugurated the Flatiron Institute, a nonprofit enterprise whose goal is to apply his hedge fund's analytical strategies to projects dedicated to expanding knowledge and helping humanity. He has established computational divisions for research in astrophysics, biology, and quantum physics, and an interdisciplinary division for climate modelling that interfaces geology, oceanography, atmospheric science, biology, and climatology.\n\nThe latter, fourth Flatiron Institute division was inspired by a 2017 presentation to the Institute's leadership by John Grotzinger, a \"bio-geoscientist\" from the California Institute of Technology, who explained the challenges of climate modelling. Grotzinger was a specialist in historical climate change—specifically, what had caused the great Permian extinction, during which virtually all species died. To properly assess this cataclysm, one had to understand both the rock record and the ocean's composition, but geologists did not interact much with physical oceanographers. Grotzinger's own best collaboration had resulted from a fortuitous lunch with an oceanographer. Climate modelling was an intrinsically difficult problem made worse by academia's structural divisions. \"If you had it all under one umbrella... it could result [much sooner] in a major breakthrough.\" Simons and his team found Grotzinger's presentation compelling, and the Flatiron Institute decided to establish its fourth and final computational division.\n\nSociologist Harriet Zuckerman, in her 1977 study of natural-science Nobel laureates in the United States, was struck by the fact that more than half (48) of the 92 laureates who did their prize-winning research in the U.S. by 1972 had worked either as students, postdoctorates, or junior collaborators under older Nobel laureates. Furthermore, those 48 future laureates had worked under a total of 71 laureate masters.\n\nSocial viscosity ensures that not every qualified novice scientist attains access to the most productive centers of scientific thought. Nevertheless, writes Zuckerman, \"To some extent, students of promise can choose masters with whom to work and masters can choose among the cohorts of students who present themselves for study. This process of bilateral assortative selection is conspicuously at work among the ultra-elite of science. Actual and prospective members of that elite select their scientist parents and therewith their scientist ancestors just as later they select their scientist progeny and therewith their scientist descendants.\"\n\nZuckerman writes: \"[T]he lines of elite apprentices to elite masters who had themselves been elite apprentices, and so on indefinitely, often reach far back into the history of science, long before 1900, when [Alfred] Nobel's will inaugurated what now amounts to the International Academy of Sciences. As an example of the many long historical chains of elite masters and apprentices, consider the German-born English laureate Hans Krebs (1953), who traces his scientific lineage [...] back through his master, the 1931 laureate Otto Warburg. Warburg had studied with Emil Fis[c]her [1852–1919], recipient of a prize in 1902 at the age of 50, three years before it was awarded [in 1905] to \"his\" teacher, Adolf von Baeyer [1835–1917], at age 70. This lineage of four Nobel masters and apprentices has its own pre-Nobelian antecedents. Von Baeyer had been the apprentice of F[riedrich] A[ugust] Kekulé [1829–96], whose ideas of structural formulae revolutionized organic chemistry and who is perhaps best known for the often retold story about his having hit upon the ring structure of benzene in a dream (1865). Kekulé himself had been trained by the great organic chemist Justus von Liebig (1803–73), who had studied at the Sorbonne with the master J[oseph] L[ouis] Gay-Lussac (1778–1850), himself once apprenticed to Claude Louis Berthollet (1748–1822). Among his many institutional and cognitive accomplishments, Berthollet helped found the \"École Polytechnique\", served as science advisor to Napoleon in Egypt, and, more significant for our purposes here, worked with [Antoine] Lavoisier [1743–94] to revise the standard system of chemical nomenclature.\"\n\nWhat has been dubbed \"Big Science\" emerged from the United States' World War II Manhattan Project that produced the world's first nuclear weapons; and Big Science has since been associated with physics, which requires massive particle accelerators. In biology, Big Science debuted in 1990 with the Human Genome Project to sequence human DNA. In 2013 neuroscience became a Big Science domain when the U.S. announced a BRAIN Initiative and the European Union announced a Human Brain Project. Major new brain-research initiatives were also announced by Israel, Canada, Australia, New Zealand, Japan, and China.\n\nEarlier successful Big Science projects had habituated politicians, mass media, and the public to view Big Science programs with sometimes uncritical favor.\n\nThe U.S.'s BRAIN Initiative was inspired by concern about the spread and cost of mental disorders and by excitement about new brain-manipulation technologies such as optogenetics. After some early false starts, the U.S. National Institute of Mental Health let the country's brain scientists define the BRAIN Initiative, and this led to an ambitious interdisciplinary program to develop new technological tools to better monitor, measure, and simulate the brain. Competition in research was ensured by the National Institute of Mental Health's peer-review process.\n\nIn the European Union, the European Commission's Human Brain Project got off to a rockier start because political and economic considerations obscured questions concerning the feasibility of the Project's initial scientific program, based principally on computer modeling of neural circuits. Four years earlier, in 2009, fearing that the European Union would fall further behind the U.S. in computer and other technologies, the European Union had begun creating a competition for Big Science projects, and the initial program for the Human Brain Project seemed a good fit for a European program that might take a lead in advanced and emerging technologies. Only in 2015, after over 800 European neuroscientists threatened to boycott the European-wide collaboration, were changes introduced into the Human Brain Project, supplanting many of the original political and economic considerations with scientific ones.\n\nNathan Myhrvold, former Microsoft chief technology officer and founder of Microsoft Research, argues that the funding of basic science cannot be left to the private sector—that \"without government resources, basic science will grind to a halt.\" He notes that Albert Einstein's general theory of relativity, published in 1915, did not spring full-blown from his brain in a eureka moment; he worked at it for years—finally driven to complete it by a rivalry with mathematician David Hilbert. The history of almost any iconic scientific discovery or technological invention—the lightbulb, the transistor, DNA, even the Internet—shows that the famous names credited with the breakthrough \"were only a few steps ahead of a pack of competitors.\" Some writers and elected officials have used this phenomenon of \"parallel innovation\" to argue against public financing of basic research: government, they assert, should leave it to companies to finance the research they need.\n\nMyhrvold writes that such arguments are dangerously wrong: without government support, most basic scientific research will never happen. \"This is most clearly true for the kind of pure research that has delivered... great intellectual benefits but no profits, such as the work that brought us the Higgs boson, or the understanding that a supermassive black hole sits at the center of the Milky Way, or the discovery of methane seas on the surface of Saturn's moon Titan. Company research laboratories used to do this kind of work: experimental evidence for the big bang was discovered at AT&T's Bell Labs, resulting in a Nobel Prize. Now those days are gone.\"\n\nEven in applied fields such as materials science and computer science, writes Myhrvold, \"companies now understand that basic research is a form of charity—so they avoid it.\" Bell Labs scientists created the transistor, but that invention earned billions for Intel and Microsoft. Xerox PARC engineers invented the modern graphical user interface, but Apple and Microsoft profited most. IBM researchers pioneered the use of giant magnetoresistance to boost hard-disk capacity but soon lost the disk-drive business to Seagate and Western Digital.\n\nCompany researchers now have to focus narrowly on innovations that can quickly bring revenue; otherwise the research budget could not be justified to the company's investors. \"Those who believe profit-driven companies will altruistically pay for basic science that has wide-ranging benefits—but mostly to others and not for a generation—are naive... If government were to leave it to the private sector to pay for basic research, most science would come to a screeching halt. What research survived would be done largely in secret, for fear of handing the next big thing to a rival.\"\n\nA complementary perspective on the funding of scientific research is given by D.T. Max, writing about the Flatiron Institute, a computational center set up in 2017 in Manhattan to provide scientists with mathematical assistance. The Flatiron Institute was established by James Harris Simons, a mathematician who had used mathematical algorithms to make himself a Wall Street billionaire. The Institute has three computational divisions dedicated respectively to astrophysics, biology, and quantum physics, and is working on a fourth division for climate modeling that will involve interfaces of geology, oceanography, atmospheric science, biology, and climatology.\n\nThe Flatiron Institute is part of a trend in the sciences toward privately-funded research. In the United States, basic science has traditionally been financed by universities or the government, but private institutes are often faster and more focused. Since the 1990s, when Silicon Valley began producing billionaires, private institutes have sprung up across the U.S. In 1997 Larry Ellison launched the Ellison Medical Foundation to study the biology of aging. In 2003 Paul Allen founded the Allen Institute for Brain Science. In 2010 Eric Schmidt founded the Schmidt Ocean Institute.\n\nThese institutes have done much good, partly by providing alternatives to more rigid systems. But private foundations also have liabilities. Wealthy benefactors tend to direct their funding toward their personal enthusiasms. And foundations are not taxed; much of the money that supports them would otherwise have gone to the government.\n\nJohn P.A. Ioannidis, of Stanford University Medical School, writes that \"There is increasing evidence that some of the ways we conduct, evaluate, report and disseminate research are miserably ineffective. A series of papers in 2014 in the \"Lancet\"... estimated that 85 percent of investment in biomedical research is wasted. Many other disciplines have similar problems.\" Ioannidis identifies some science-funding biases that undermine the efficiency of the scientific enterprise, and proposes solutions:\n\nFunding too few scientists: \"[M]ajor success [in scientific research] is largely the result of luck, as well as hard work. The investigators currently enjoying huge funding are not necessarily genuine superstars; they may simply be the best connected.\" Solutions: \"Use a lottery to decide which grant applications to fund (perhaps after they pass a basic review)... Shift... funds from senior people to younger researchers...\"\n\nNo reward for transparency: \"Many scientific protocols, analysis methods, computational processes and data are opaque. [M]any top findings cannot be reproduced. That is the case for two out of three top psychology papers, one out of three top papers in experimental economics and more than 75 percent of top papers identifying new cancer drug targets. [S]cientists are not rewarded for sharing their techniques.\" Solutions: \"Create better infrastructure for enabling transparency, openness and sharing. Make transparency a prerequisite for funding. [P]referentially hire, promote or tenure... champions of transparency.\"\n\nNo encouragement for replication: Replication is indispensable to the scientific method. Yet, under pressure to produce new discoveries, researchers tend to have little incentive, and much counterincentive, to try replicating results of previous studies. Solutions: \"Funding agencies must pay for replication studies. Scientists' advancement should be based not only on their discoveries but also on their replication track record.\"\n\nNo funding for young scientists: \"Werner Heisenberg, Albert Einstein, Paul Dirac and Wolfgang Pauli made their top contributions in their mid-20s.\" But the average age of biomedical scientists receiving their first substantial grant is 46. The average age for a full professor in the U.S. is 55. Solutions: \"A larger proportion of funding should be earmarked for young investigators. Universities should try to shift the aging distribution of their faculty by hiring more young investigators.\"\n\nBiased funding sources: \"Most funding for research and development in the U.S. comes not from the government but from private, for-profit sources, raising unavoidable conflicts of interest and pressure to deliver results favorable to the sponsor.\" Solutions: \"Restrict or even ban funding that has overt conflicts of interest. Journals should not accept research with such conflicts. For less conspicuous conflicts, at a minimum ensure transparent and thorough disclosure.\"\n\nFunding the wrong fields: \"Well-funded fields attract more scientists to work for them, which increases their lobbying reach, fueling a vicious circle. Some entrenched fields absorb enormous funding even though they have clearly demonstrated limited yield or uncorrectable flaws.\" Solutions: \"Independent, impartial assessment of output is necessary for lavishly funded fields. More funds should be earmarked for new fields and fields that are high risk. Researchers should be encouraged to switch fields, whereas currently they are incentivized to focus in one area.\"\n\nNot spending enough: The U.S. military budget ($886 billion) is 24 times the budget of the National Institutes of Health ($37 billion). \"Investment in science benefits society at large, yet attempts to convince the public often make matters worse when otherwise well-intentioned science leaders promise the impossible, such as promptly eliminating all cancer or Alzheimer's disease.\" Solutions: \"We need to communicate how science funding is used by making the process of science clearer, including the number of scientists it takes to make major accomplishments... We would also make a more convincing case for science if we could show that we do work hard on improving how we run it.\"\n\nRewarding big spenders: \"Hiring, promotion and tenure decisions primarily rest on a researcher's ability to secure high levels of funding. But the expense of a project does not necessarily correlate with its importance. Such reward structures select mostly for politically savvy managers who know how to absorb money.\" Solutions: \"We should reward scientists for high-quality work, reproducibility and social value rather than for securing funding. Excellent research can be done with little to no funding other than protected time. Institutions should provide this time and respect scientists who can do great work without wasting tons of money.\"\n\nNo funding for high-risk ideas: \"The pressure that taxpayer money be 'well spent' leads government funders to back projects most likely to pay off with a positive result, even if riskier projects might lead to more important, but less assured, advances. Industry also avoids investing in high-risk projects... Innovation is extremely difficult, if not impossible, to predict...\" Solutions: \"Fund excellent scientists rather than projects and give them freedom to pursue research avenues as they see fit. Some institutions such as Howard Hughes Medical Institute already use this model with success.\" It must be communicated to the public and to policy-makers that science is a cumulative investment, that no one can know in advance which projects will succeed, and that success must be judged on the total agenda, not on a single experiment or result.\n\nLack of good data: \"There is relatively limited evidence about which scientific practices work best. We need more research on research ('meta-research') to understand how to best perform, evaluate, review, disseminate and reward science.\" Solutions: \"We should invest in studying how to get the best science and how to choose and reward the best scientists.\"\n\nClaire Pomeroy, president of the Lasker Foundation, which is dedicated to advancing medical research, points out that women scientists continue to be subjected to discrimination in professional advancement.\n\nThough the percentage of doctorates awarded to women in life sciences in the United States increased from 15 to 52 percent between 1969 and 2009, only a third of assistant professors and less than a fifth of full professors in biology-related fields in 2009 were women. Women make up only 15 percent of permanent department chairs in medical schools and barely 16 percent of medical-school deans.\nThe problem is a culture of unconscious bias that leaves many women feeling demoralized and marginalized. In one study, science faculty were given identical résumés in which the names and genders of two applicants were interchanged; both male \"and\" female faculty judged the male applicant to be more competent and offered him a higher salary.\n\nUnconscious bias also appears as \"microassaults\" against women scientists: purportedly insignificant sexist jokes and insults that accumulate over the years and undermine confidence and ambition. Writes Claire Pomeroy: \"Each time it is assumed that the only woman in the lab group will play the role of recording secretary, each time a research plan becomes finalized in the men's lavatory between conference sessions, each time a woman is not invited to go out for a beer after the plenary lecture to talk shop, the damage is reinforced.\"\n\n\"When I speak to groups of women scientists,\" writes Pomeroy, \"I often ask them if they have ever been in a meeting where they made a recommendation, had it ignored, and then heard a man receive praise and support for making the same point a few minutes later. Each time the majority of women in the audience raise their hands. Microassaults are especially damaging when they come from a high-school science teacher, college mentor, university dean or a member of the scientific elite who has been awarded a prestigious prize—the very people who should be inspiring and supporting the next generation of scientists.\"\n\nSexual harassment is more prevalent in academia than in any other social sector except the military. A June 2018 report by the National Academies of Sciences, Engineering, and Medicine states that sexual harassment hurts individuals, diminishes the pool of scientific talent, and ultimately damages the integrity of science.\n\nPaula Johnson, co-chair of the committee that drew up the report, describes some measures for preventing sexual harassment in science. One would be to replace trainees' individual mentoring with group mentoring, and to uncouple the mentoring relationship from the trainee's financial dependence on the mentor. Another way would be to prohibit the use of confidentiality agreements in connection with harassment cases.\n\nA novel approach to the reporting of sexual harassment, dubbed \"Callisto\", that has been adopted by some institutions of higher education, lets aggrieved persons record experiences of sexual harassment, date-stamped, without actually formally reporting them. This program lets people see if others have recorded experiences of harassment from the same individual, and share information anonymously.\n\nPsychologist Andrei Cimpian and philosophy professor Sarah-Jane Leslie have proposed a theory to explain why American women and African-Americans are often subtly deterred from seeking to enter certain academic fields by a misplaced emphasis on genius. Cimpian and Leslie had noticed that their respective fields are similar in their substance but hold different views on what is important for success. Much more than psychologists, philosophers value a certain \"kind of person\": the \"brilliant superstar\" with an exceptional mind. Psychologists are more likely to believe that the leading lights in psychology grew to achieve their positions through hard work and experience. In 2015, women accounted for less than 30% of doctorates granted in philosophy; African-Americans made up only 1% of philosophy Ph.D.s. Psychology, on the other hand, has been successful in attracting women (72% of 2015 psychology Ph.D.s) and African-Americans (6% of psychology Ph.D.s).\n\nAn early insight into these disparities was provided to Cimpian and Leslie by the work of psychologist Carol Dweck. She and her colleagues had shown that a person's beliefs about ability matter a great deal for that person's ultimate success. A person who sees talent as a stable trait is motivated to \"show off this aptitude\" and to avoid making mistakes. By contrast, a person who adopts a \"growth mindset\" sees his or her current capacity as a work in progress: for such a person, mistakes are not an indictment but a valuable signal highlighting which of their skills are in need of work. Cimpian and Leslie and their collaborators tested the hypothesis that attitudes, about \"genius\" and about the unacceptability of making mistakes, within various academic fields may account for the relative attractiveness of those fields for American women and African-Americans. They did so by contacting academic professionals from a wide range of disciplines and asking them whether they thought that some form of exceptional intellectual talent was required for success in their field. The answers received from almost 2,000 academics in 30 fields matched the distribution of Ph.D.s in the way that Cimpian and Leslie had expected: fields that placed more value on brilliance also conferred fewer Ph.D.s on women and African-Americans. The proportion of women and African-American Ph.D.s in psychology, for example, was higher than the parallel proportions for philosophy, mathematics, or physics.\n\nFurther investigation showed that non-academics share similar ideas of which fields require brilliance. Exposure to these ideas at home or school could discourage young members of stereotyped groups from pursuing certain careers, such as those in the natural sciences or engineering. To explore this, Cimpian and Leslie asked hundreds of five-, six-, and seven-year-old boys and girls questions that measured whether they associated being \"really, really smart\" (i.e., \"brilliant\") with their sex. The results, published in January 2017 in \"Science\", were consistent with scientific literature on the early acquisition of sex stereotypes. Five-year-old boys and girls showed no difference in their self-assessment; but by age six, girls were less likely to think that girls are \"really, really smart.\" The authors next introduced another group of five-, six-, and seven-year-olds to unfamiliar gamelike activities that the authors described as being \"for children who are really, really smart.\" Comparison of boys' and girls' interest in these activities at each age showed no sex difference at age five but significantly greater interest from boys at ages six and seven—exactly the ages when stereotypes emerge.\n\nCimpian and Leslie conclude that, \"Given current societal stereotypes, messages that portray [genius or brilliance] as singularly necessary [for academic success] may needlessly discourage talented members of stereotyped groups.\"\n\nLargely as a result of his growing popularity, astronomer and science popularizer Carl Sagan, creator of the 1980 PBS TV \"\" series, came to be ridiculed by scientist peers and failed to receive tenure at Harvard University in the 1960s and membership in the National Academy of Sciences in the 1990s. The eponymous \"Sagan effect\" persists: as a group, scientists still discourage individual investigators from engaging with the public unless they are already well-established senior researchers.\n\nThe operation of the Sagan effect deprives society of the full range of expertise needed to make informed decisions about complex questions, including genetic engineering, climate change, and energy alternatives. Fewer scientific voices mean fewer arguments to counter antiscience or pseudoscientific discussion. The Sagan effect also creates the false impression that science is the domain of older white men (who dominate the senior ranks), thereby tending to discourage women and minorities from considering science careers.\n\nA number of factors contribute to the Sagan effect's durability. At the height of the Scientific Revolution in the 17th century, many researchers emulated the example of Isaac Newton, who dedicated himself to physics and mathematics and never married. These scientists were viewed as pure seekers of truth who were not distracted by more mundane concerns. Similarly, today anything that takes scientists away from their research, such as having a hobby or taking part in public debates, can undermine their credibility as researchers.\n\nAnother, more prosaic factor in the Sagan effect's persistence may be professional jealousy.\n\nHowever, there appear to be some signs that engaging with the rest of society is becoming less hazardous to a career in science. So many people have social-media accounts now that becoming a public figure is not as unusual for scientists as previously. Moreover, as traditional funding sources stagnate, going public sometimes leads to new, unconventional funding streams. A few institutions such as Emory University and the Massachusetts Institute of Technology may have begun to appreciate outreach as an area of academic activity, in addition to the traditional roles of research, teaching, and administration. Exceptional among federal funding agencies, the National Science Foundation now officially favors popularization.\n\n\n"}
{"id": "4086673", "url": "https://en.wikipedia.org/wiki?curid=4086673", "title": "Minnesota Texas Adoption Research Project", "text": "Minnesota Texas Adoption Research Project\n\nThe Minnesota / Texas Adoption Research Project (MTARP) is a longitudinal research study that focuses on the consequences of variations in openness in adoption arrangements for all members of the adoptive kinship network: birthmothers, adoptive parents, and adopted children, and for the relationships within these family systems.\n\nMTARP is a joint project between the University of Minnesota and University of Texas at Austin and involves interviews with adoptive parents and birth mothers. The Principal Investigators of the study are Harold D. Grotevant, University of Minnesota, and Ruth G. McRoy, University of Texas at Austin. Co-investigators include Gretchen Wrobel, Bethel University, St. Paul, MN, Martha Rueter, University of Minnesota, Susan Ayers-Lopez, University of Texas at Austin, and Sarah Friese, University of Minnesota. It is funded primarily by the National Institute of Child Health and Human Development, National Science Foundation, William T. Grant Foundation, Office of Population Affairs, and Rudd Family Foundation Chair in Psychology at the University of Massachusetts Amherst. It was also funded in part by the Pioneer Fund.\n\nEach of the families in the project adopted a child in the late 1970s or early 1980s. Families and birthmothers were first interviewed between 1987 and 1992 and again between 1996 and 2000. Grotevant and colleagues at the Minnesota site have followed the adopted children and their adoptive parents (e.g., Grotevant, Ross, Marchel, & McRoy, 1999; Dunbar & Grotevant, 2004); McRoy and colleagues at the University of Texas at Austin have followed the children’s birthmothers (e.g., Christian, et al., 1997; Fravel, et al., 2000). Adoptive families and birthmothers were recruited for the study through 35 adoption agencies located across the United States.\n\nFamilies where there was at least one adopted child (the \"target child\") between the ages of 4 and 12 at the time of the interview, who was adopted through an agency before his or her first birthday, and in which both adoptive parents were married to the partner they had at the time of the adoption were selected for the study. Transracial, international, or \"special needs\" adoptees were not included. Participants in the study were located in 23 different states from all regions of the U.S., making this study the only nationwide one of its kind.\n\n"}
{"id": "97536", "url": "https://en.wikipedia.org/wiki?curid=97536", "title": "Modern synthesis (20th century)", "text": "Modern synthesis (20th century)\n\nThe modern synthesis was the early 20th-century synthesis reconciling Charles Darwin's theory of evolution and Gregor Mendel's ideas on heredity in a joint mathematical framework. Julian Huxley coined the term in his 1942 book, \"\".\n\nThe 19th century ideas of natural selection and Mendelian genetics were put together with population genetics, early in the twentieth century. The modern synthesis also addressed the relationship between the broad-scale changes of macroevolution seen by palaeontologists and the small-scale microevolution of local populations of living organisms. The synthesis was defined differently by its founders, with Ernst Mayr in 1959, G. Ledyard Stebbins in 1966 and Theodosius Dobzhansky in 1974 offering differing numbers of basic postulates, though they all included natural selection, working on heritable variation supplied by mutation. Other major figures in the synthesis included E. B. Ford, Bernhard Rensch, Ivan Schmalhausen, and George Gaylord Simpson. An early event in the modern synthesis was R. A. Fisher's 1918 paper on mathematical population genetics, but William Bateson, and separately Udny Yule, were already starting to show how Mendelian genetics could work in evolution in 1902.\n\nDifferent syntheses followed, accompanying the gradual breakup of the early 20th century synthesis, including with social behaviour in E. O. Wilson's sociobiology in 1975, evolutionary developmental biology's integration of embryology with genetics and evolution, starting in 1977, and Massimo Pigliucci's proposed extended evolutionary synthesis of 2007. In the view of the evolutionary biologist Eugene Koonin in 2009, the modern synthesis will be replaced by a 'post-modern' synthesis that will include revolutionary changes in molecular biology, the study of prokaryotes and the resulting tree of life, and genomics.\n\nCharles Darwin's 1859 book \"On the Origin of Species\" was successful in convincing most biologists that evolution had occurred, but was less successful in convincing them that natural selection was its primary mechanism. In the 19th and early 20th centuries, variations of Lamarckism (inheritance of acquired characteristics), orthogenesis (progressive evolution), saltationism (evolution by jumps) and mutationism (evolution driven by mutations) were discussed as alternatives. Alfred Russel Wallace advocated a selectionist version of evolution, and unlike Darwin completely rejected Lamarckism. In 1880, Wallace's view was labelled neo-Darwinism by Samuel Butler. \n\nFrom the 1880s onwards, there was a widespread belief among biologists that Darwinian evolution was in deep trouble. This eclipse of Darwinism (in Julian Huxley's phrase) grew out of the weaknesses in Darwin's account, written with an incorrect view of inheritance. Darwin himself believed in blending inheritance, which implied that any new variation, even if beneficial, would be weakened by 50% at each generation, as the engineer Fleeming Jenkin correctly noted in 1868. This in turn meant that small variations would not survive long enough to be selected. Blending would therefore directly oppose natural selection. In addition, Darwin and others considered Lamarckian inheritance of acquired characteristics entirely possible, and Darwin's 1868 theory of pangenesis, with contributions to the next generation (gemmules) flowing from all parts of the body, actually implied Lamarckism as well as blending.\n\nAugust Weismann's idea, set out in his 1892 book \"Das Keimplasma: eine Theorie der Vererbung\" (The Germ Plasm: a Theory of Inheritance), was that the hereditary material, which he called the germ plasm, and the rest of the body (the soma) had a one-way relationship: the germ-plasm formed the body, but the body did not influence the germ-plasm, except indirectly in its participation in a population subject to natural selection. If correct, this made Darwin's pangenesis wrong, and Lamarckian inheritance impossible. His experiment on mice, cutting off their tails and showing that their offspring had normal tails, demonstrated that inheritance was 'hard'. He argued strongly and dogmatically for Darwinism and against Lamarckism, polarising opinions among other scientists. This increased anti-Darwinian feeling, contributing to its eclipse.\n\nWhile carrying out breeding experiments to clarify the mechanism of inheritance in 1900, Hugo de Vries and Carl Correns independently rediscovered Gregor Mendel's work. News of this reached William Bateson in England, who reported on the paper during a presentation to the Royal Horticultural Society in May 1900. In Mendelian inheritance, the contributions of each parent retain their integrity rather than blending with the contribution of the other parent. In the case of a cross between two true-breeding varieties such as Mendel's round and wrinkled peas, the first-generation offspring are all alike, in this case all round. Allowing these to cross, the original characteristics reappear (segregation): about 3/4 of their offspring are round, 1/4 wrinkled. There is a discontinuity between the appearance of the offspring; de Vries coined the term allele for a variant form of an inherited characteristic. This reinforced a major division of thought, already present in the 1890s, between gradualists who followed Darwin, and saltationists such as Bateson.\n\nThe two schools were the Mendelians, such as Bateson and de Vries, who favoured mutationism, evolution driven by mutation, based on genes whose alleles segregated discretely like Mendel's peas; and the biometric school, led by Karl Pearson and Walter Weldon. The biometricians argued vigorously against mutationism, saying that empirical evidence indicated that variation was continuous in most organisms, not discrete as Mendelism seemed to predict; they wrongly believed that Mendelism inevitably implied evolution in discontinuous jumps.\n\nA traditional view is that the biometricians and the Mendelians rejected natural selection and argued for their separate theories for 20 years, the debate only resolved by the development of population genetics.\nA more recent view is that Bateson, de Vries, Thomas Hunt Morgan and Reginald Punnett had by 1918 formed a synthesis of Mendelism and mutationism. The understanding achieved by these geneticists spanned the action of natural selection on alleles (alternative forms of a gene), the Hardy-Weinberg equilibrium, the evolution of continuously-varying traits (like height), and the probability that a new mutation will become fixed. In this view, the early geneticists accepted natural selection but rejected Darwin's non-Mendelian ideas about variation and heredity, and the synthesis began soon after 1900. The traditional claim that Mendelians rejected the idea of continuous variation is false; as early as 1902, Bateson and Saunders wrote that \"If there were even so few as, say, four or five pairs of possible allelomorphs, the various homo- and hetero-zygous combinations might, on seriation, give so near an approach to a continuous curve, that the purity of the elements would be unsuspected\". Also in 1902, the statistician Udny Yule showed mathematically that given multiple factors, Mendel's theory enabled continuous variation. Yule criticised Bateson's approach as confrontational, but failed to prevent the Mendelians and the biometricians from falling out.\n\nStarting in 1906, William Castle carried out a long study of the effect of selection on coat colour in rats. The piebald or hooded pattern was recessive to the grey wild type. He crossed hooded rats with the black-backed Irish type, and then back-crossed the offspring with pure hooded rats. The dark stripe on the back was bigger. He then tried selecting different groups for bigger or smaller stripes for 5 generations, and found that it was possible to change the characteristics way beyond the initial range of variation. This effectively refuted de Vries's claim that continuous variation was caused by the environment and could not be inherited. By 1911 Castle noted that the results could be explained by Darwinian selection on heritable variation of a sufficient number of Mendelian genes.\n\nThomas Hunt Morgan began his career in genetics as a saltationist, and started out trying to demonstrate that mutations could produce new species in fruit flies. However, the experimental work at his lab with the fruit fly, \"Drosophila melanogaster\" demonstrated that rather than creating new species in a single step, mutations increased the supply of genetic variation in the population. By 1912, after years of work on the genetics of fruit flies, Morgan showed that these insects had many small Mendelian factors (discovered as mutant flies) on which Darwinian evolution could work as if variation was fully continuous. The way was open for geneticists to conclude that Mendelism supported Darwinism.\n\nThe theoretical biologist and philosopher of biology Joseph Henry Woodger led the introduction of positivism into biology with his 1929 book \"Biological Principles\". He saw a mature science as being characterised by a framework of hypotheses that could be verified by facts established by experiments. He criticised the traditional natural history style of biology, including the study of evolution, as immature science, since it relied on narrative. Woodger set out to play for biology the role of Robert Boyle's 1661 \"Sceptical Chymist\", intending to convert the subject into a formal, unified science, and ultimately, following the Vienna Circle of logical positivists like Otto Neurath and Rudolf Carnap, to reduce biology to physics and chemistry. His efforts stimulated the biologist J. B. S. Haldane to push for the axiomatisation of biology, and by influencing thinkers such as Huxley, helped to bring about the modern synthesis. The positivist climate made natural history unfashionable, and in America, research and university-level teaching on evolution declined almost to nothing by the late 1930s. The Harvard physiologist William John Crozier told his students that evolution was not even a science: \"You can't experiment with two million years!\"\n\nThe tide of opinion turned with the adoption of mathematical modelling and controlled experimentation in population genetics, combining genetics, ecology and evolution in a framework acceptable to positivism.\n\nIn 1918, R. A. Fisher wrote the paper \"The Correlation between Relatives on the Supposition of Mendelian Inheritance,\" which showed mathematically how continuous variation could result from a number of discrete genetic loci. In this and subsequent papers culminating in his 1930 book \"The Genetical Theory of Natural Selection\", Fisher showed how Mendelian genetics was consistent with the idea of evolution driven by natural selection.\n\nDuring the 1920s, a series of papers by J. B. S. Haldane applied mathematical analysis to real-world examples of natural selection, such as the evolution of industrial melanism in peppered moths. Haldane established that natural selection could work even faster than Fisher had assumed. Both workers, and others such as Dobzhansky and Wright, explicitly intended to bring biology up to the philosophical standard of the physical sciences, making it firmly based in mathematical modelling, its predictions confirmed by experiment. Natural selection, once considered hopelessly unverifiable speculation about history, was becoming predictable, measurable, and testable.\n\nThe traditional view is that developmental biology played little part in the modern synthesis, but in his 1930 book \"Embryos and Ancestors\", the evolutionary embryologist Gavin de Beer anticipated evolutionary developmental biology by showing that evolution could occur by heterochrony, such as in the retention of juvenile features in the adult. This, de Beer argued, could cause apparently sudden changes in the fossil record, since embryos fossilise poorly. As the gaps in the fossil record had been used as an argument against Darwin's gradualist evolution, de Beer's explanation supported the Darwinian position.\nHowever, despite de Beer, the modern synthesis largely ignored embryonic development to explain the form of organisms, since population genetics appeared to be an adequate explanation of how forms evolved.\n\nThe population geneticist Sewall Wright focused on combinations of genes that interacted as complexes, and the effects of inbreeding on small relatively isolated populations, which could be subject to genetic drift. In a 1932 paper, he introduced the concept of an adaptive landscape in which phenomena such as cross breeding and genetic drift in small populations could push them away from adaptive peaks, which would in turn allow natural selection to push them towards new adaptive peaks. Wright's model would appeal to field naturalists such as Theodosius Dobzhansky and Ernst Mayr who were becoming aware of the importance of geographical isolation in real world populations. The work of Fisher, Haldane and Wright helped to found the discipline of theoretical population genetics.\n\nTheodosius Dobzhansky, an emigrant from the Soviet Union to the United States, who had been a postdoctoral worker in Morgan's fruit fly lab, was one of the first to apply genetics to natural populations. He worked mostly with \"Drosophila pseudoobscura\". He says pointedly: \"Russia has a variety of climates from the Arctic to sub-tropical... Exclusively laboratory workers who neither possess nor wish to have any knowledge of living beings in nature were and are in a minority.\" Not surprisingly, there were other Russian geneticists with similar ideas, though for some time their work was known to only a few in the West. His 1937 work \"Genetics and the Origin of Species\" was a key step in bridging the gap between population geneticists and field naturalists. It presented the conclusions reached by Fisher, Haldane, and especially Wright in their highly mathematical papers in a form that was easily accessible to others. Further, Dobzhansky asserted that evolution was based on material genes, arranged in a string on physical hereditary structures, the chromosomes, and linked more or less strongly to each other according to their physical distances from each other on the chromosomes. As with Haldane and Fisher, Dobzhansky's \"evolutionary genetics\" was a genuine science, now unifying cell biology, genetics, and both micro- and macroevolution. His work emphasized that real world populations had far more genetic variability than the early population geneticists had assumed in their models, and that genetically distinct sub-populations were important. Dobzhansky argued that natural selection worked to maintain genetic diversity as well as driving change. He was influenced by his exposure in the 1920s to the work of Sergei Chetverikov, who had looked at the role of recessive genes in maintaining a reservoir of genetic variability in a population before his work was shut down by the rise of Lysenkoism in the Soviet Union. By 1937, Dobzhansky was able to argue that mutations were the main source of evolutionary changes and variability, along with chromosome rearrangements, effects of genes on their neighbours during development, and polyploidy. Next, genetic drift (he used the term in 1941), selection, migration, and geographical isolation could change gene frequencies. Thirdly, mechanisms like ecological or sexual isolation and hybrid sterility could fix the results of the earlier processes.\n\nE. B. Ford was an experimental naturalist who wanted to test natural selection in nature, virtually inventing the field of ecological genetics. His work on natural selection in wild populations of butterflies and moths was the first to show that predictions made by R. A. Fisher were correct. In 1940, he was the first to describe and define genetic polymorphism, and to predict that human blood group polymorphisms might be maintained in the population by providing some protection against disease. His 1949 book \"Mendelism and Evolution\" helped to persuade Dobzhansky to change the emphasis in the third edition of his famous textbook \"Genetics and the Origin of Species\" from drift to selection.\n\nIvan Schmalhausen developed the theory of stabilizing selection, the idea that selection can preserve a trait at some value, publishing a paper in Russian titled \"Stabilizing selection and its place among factors of evolution\" in 1941 and a monograph \"Factors of Evolution: The Theory of Stabilizing Selection\" in 1945. He developed it from J. M. Baldwin's 1902 concept that changes induced by the environment will ultimately be replaced by hereditary changes (including the Baldwin effect on behaviour), following that theory's implications to their Darwinian conclusion, and bringing him into conflict with Lysenkoism. Schmalhausen observed that stabilizing selection would remove most variations from the norm, most mutations being harmful. Dobzhansky called the work \"an important missing link in the modern view of evolution\".\n\nIn 1942, Julian Huxley's serious but popularising \"\" introduced a name for the synthesis and intentionally set out to promote a \"synthetic point of view\" on the evolutionary process. He imagined a wide synthesis of many sciences: genetics, developmental physiology, ecology, systematics, palaeontology, cytology, and mathematical analysis of biology, and assumed that evolution would proceed differently in different groups of organisms according to how their genetic material was organised and their strategies for reproduction, leading to progressive but varying evolutionary trends. His vision was of an \"evolutionary humanism\", with a system of ethics and a meaningful place for \"Man\" in the world grounded in a unified theory of evolution which would demonstrate progress leading to man at its summit. Natural selection was in his view a \"fact of nature capable of verification by observation and experiment\", while the \"period of synthesis\" of the 1920s and 1930s had formed a \"more unified science\", rivalling physics and enabling the \"rebirth of Darwinism\".\n\nHowever, the book was not the research text that it appeared to be. In the view of the philosopher of science Michael Ruse, and in Huxley's own opinion, Huxley was \"a generalist, a synthesizer of ideas, rather than a specialist\". Ruse observes that Huxley wrote as if he were adding empirical evidence to the mathematical framework established by Fisher and the population geneticists, but that this was not so. Huxley avoided mathematics, for instance not even mentioning Fisher's fundamental theorem of natural selection. Instead, Huxley used a mass of examples to demonstrate that natural selection is powerful, and that it works on Mendelian genes. The book was successful in its goal of persuading readers of the reality of evolution, effectively illustrating topics such as island biogeography, speciation, and competition. Huxley further showed that the appearance of long-term orthogenetic trends – predictable directions for evolution – in the fossil record were readily explained as allometric growth (since parts are interconnected). All the same, Huxley did not reject orthogenesis out of hand, but maintained a belief in progress all his life, with \"Homo sapiens\" as the end point, and he had since 1912 been influenced by the vitalist philosopher Henri Bergson, though in public he maintained an atheistic position on evolution. Huxley's belief in progress within evolution and evolutionary humanism was shared in various forms by Dobzhansky, Mayr, Simpson and Stebbins, all of them writing about \"the future of Mankind\". Both Huxley and Dobzhansky admired the palaeontologist priest Pierre Teilhard de Chardin, Huxley writing the introduction to Teilhard's 1955 book on orthogenesis, \"The Phenomenon of Man\". This vision required evolution to be seen as the central and guiding principle of biology.\n\nErnst Mayr's key contribution to the synthesis was \"Systematics and the Origin of Species\", published in 1942. It asserted the importance of and set out to explain population variation in evolutionary processes including speciation. He analysed in particular the effects of polytypic species, geographic variation, and isolation by geographic and other means. Mayr emphasized the importance of allopatric speciation, where geographically isolated sub-populations diverge so far that reproductive isolation occurs. He was skeptical of the reality of sympatric speciation believing that geographical isolation was a prerequisite for building up intrinsic (reproductive) isolating mechanisms. Mayr also introduced the biological species concept that defined a species as a group of interbreeding or potentially interbreeding populations that were reproductively isolated from all other populations. Before he left Germany for the United States in 1930, Mayr had been influenced by the work of the German biologist Bernhard Rensch, who in the 1920s had analyzed the geographic distribution of polytypic species, paying particular attention to how variations between populations correlated with factors such as differences in climate.\n\nGeorge Gaylord Simpson was responsible for showing that the modern synthesis was compatible with palaeontology in his 1944 book \"Tempo and Mode in Evolution\". Simpson's work was crucial because so many palaeontologists had disagreed, in some cases vigorously, with the idea that natural selection was the main mechanism of evolution. It showed that the trends of linear progression (in for example the evolution of the horse) that earlier palaeontologists had used as support for neo-Lamarckism and orthogenesis did not hold up under careful examination. Instead the fossil record was consistent with the irregular, branching, and non-directional pattern predicted by the modern synthesis.\n\nDuring the war, Mayr edited a series of bulletins of the Committee on Common Problems of Genetics, Paleontology, and Systematics, formed in 1943, reporting on discussions of a \"synthetic attack\" on the interdisciplinary problems of evolution. In 1946, the committee became the Society for the Study of Evolution, with Mayr, Dobzhansky and Sewall Wright the first of the signatories. Mayr became the editor of its journal, \"Evolution\". From Mayr and Dobzhansky's point of view, suggests the historian of science Betty Smocovitis, Darwinism was reborn, evolutionary biology was legitimised, and genetics and evolution were synthesised into a newly unified science. Everything fitted in to the new framework, except \"heretics\" like Richard Goldschmidt who annoyed Mayr and Dobzhansky by insisting on the possibility of speciation by macromutation, creating \"hopeful monsters\". The result was \"bitter controversy\".\n\nThe botanist G. Ledyard Stebbins extended the synthesis to encompass botany. He described the important effects on speciation of hybridization and polyploidy in plants in his 1950 book \"Variation and Evolution in Plants\". These permitted evolution to proceed rapidly at times, polyploidy in particular evidently being able to create new species effectively instantaneously.\n\nThe modern synthesis was defined differently by its various founders, with differing numbers of basic postulates, as shown in the table.\n\nAfter the synthesis, evolutionary biology continued to develop with major contributions from workers including W. D. Hamilton, George C. Williams, E. O. Wilson, Edward B. Lewis and others.\n\nIn 1964, W. D. Hamilton published two papers on \"The Genetical Evolution of Social Behaviour\". These defined inclusive fitness as the number of offspring equivalents an individual rears, rescues or otherwise supports through its behaviour. This was contrasted with personal reproductive fitness, the number of offspring that the individual directly begets. Hamilton, and others such as John Maynard Smith, argued that a gene's success consisted in maximising the number of copies of itself, either by begetting them or by indirectly encouraging begetting by related individuals who shared the gene, the theory of kin selection.\n\nIn 1966, George C. Williams published \"Adaptation and Natural Selection\", outlined a gene-centred view of evolution following Hamilton's concepts, disputing the idea of evolutionary progress, and attacking the then widespread theory of group selection. Williams argued that natural selection worked by changing the frequency of alleles, and could not work at the level of groups. Gene-centred evolution was popularised by Richard Dawkins in his 1976 book \"The Selfish Gene\" and developed in his more technical writings.\n\nIn 1975, E. O. Wilson published his controversial book \"\", the subtitle alluding to the modern synthesis as he attempted to bring the study of animal society into the evolutionary fold. This appeared radically new, although Wilson was following Darwin, Fisher, Dawkins and others. Critics such as Gerhard Lenski noted that he was following Huxley, Simpson and Dobzhansky's approach, which Lenski considered needlessly reductive as far as human society was concerned. By 2000, the proposed discipline of sociobiology had morphed into the relatively well-accepted discipline of evolutionary psychology.\n\nIn 1977, recombinant DNA technology enabled biologists to start to explore the genetic control of development. The growth of evolutionary developmental biology from 1978, when Edward B. Lewis discovered homeotic genes, showed that many so-called toolkit genes act to regulate development, influencing the expression of other genes. It also revealed that some of the regulatory genes are extremely ancient, so that animals as different as insects and mammals share control mechanisms; for example, the \"Pax6\" gene is involved in forming the eyes of mice and of fruit flies. Such deep homology provided strong evidence for evolution and indicated the paths that evolution had taken.\n\nIn 1982, a historical note on a series of evolutionary biology books could state without qualification that evolution is the central organizing principle of biology. Smocovitis commented on this that \"What the architects of the synthesis had worked to construct had by 1982 become a matter of fact\", adding in a footnote that \"the centrality of evolution had thus been rendered tacit knowledge, part of the received wisdom of the profession\".\n\nBy the late 20th century, however, the modern synthesis was showing its age, and fresh syntheses to remedy its defects and fill in its gaps were proposed from different directions. These have included such diverse fields as the study of society, developmental biology, epigenetics, molecular biology, microbiology, genomics, symbiogenesis, and horizontal gene transfer. The physiologist Denis Noble argues that these additions render neo-Darwinism in the sense of the early 20th century's modern synthesis \"at the least, incomplete as a theory of evolution\", and one that has been falsified by later biological research.\n\nMichael Rose and Todd Oakley note that evolutionary biology, formerly divided and \"Balkanized\", has been brought together by genomics. It has in their view discarded at least five common assumptions from the modern synthesis, namely that the genome is always a well-organised set of genes; that each gene has a single function; that species are well adapted biochemically to their ecological niches; that species are the durable units of evolution, and all levels from organism to organ, cell and molecule within the species are characteristic of it; and that the design of every organism and cell is efficient. They argue that the \"new biology\" integrates genomics, bioinformatics, and evolutionary genetics into a general-purpose toolkit for a \"Postmodern Synthesis\".\n\nIn 2007, more than half a century after the modern synthesis, Massimo Pigliucci called for an extended evolutionary synthesis to incorporate aspects of biology that had not been included or had not existed in the mid-20th century. It revisits the relative importance of different factors, challenges assumptions made in the modern synthesis, and adds new factors such as multilevel selection, transgenerational epigenetic inheritance, niche construction, and evolvability.\n\nIn 2009, Darwin's 200th anniversary, the \"Origin of Species\" 150th, and the 200th of Lamarck's \"early evolutionary synthesis\", \"Philosophie Zoologique\", the evolutionary biologist Eugene Koonin stated that while \"the edifice of the [early 20th century] Modern Synthesis has crumbled, apparently, beyond repair\", a new 21st century synthesis could be glimpsed. Three interlocking revolutions had, he argued, taken place in evolutionary biology: molecular, microbiological, and genomic. The molecular revolution included the neutral theory, that most mutations are neutral and that purifying selection happens more often than the positive form, and that all current life evolved from a single common ancestor. In microbiology, the synthesis has expanded to cover the prokaryotes, using ribosomal RNA to form a tree of life. Finally, genomics brought together the molecular and microbiological syntheses, noting that a molecular view shows that the tree of life is problematic. In particular, horizontal gene transfer between bacteria means that prokaryotes freely share genes, challenging Mayr's foundational definition of species. Further, horizontal gene transfer, gene duplication, and \"momentous events\" like endosymbiosis enable evolution to proceed in sudden jumps, ending the old gradualist-saltationist debate by showing that on this point Darwin's gradualism was wrong. The idea of progress in biology, too, is seen to be wrong, along with the modern synthesis belief in pan-adaptationism, that everything is optimally adapted: genomes plainly are not. Many of these points had already been made by other researchers such as Ulrich Kutschera and Karl J. Niklas.\n\nBiologists, alongside scholars of the history and philosophy of biology, have continued to debate the need for, and possible nature of, a replacement synthesis. For example, in 2017 Philippe Huneman and Denis M. Walsh stated in their book \"Challenging the Modern Synthesis\" that numerous theorists had pointed out that the disciplines of embryological developmental theory, morphology, and ecology had been omitted. They noted that all such arguments amounted to a continuing desire to replace the modern synthesis with one that united \"all biological fields of research related to evolution, adaptation, and diversity in a single theoretical frame.\" They observed further that there are two groups of challenges to the way the modern synthesis viewed inheritance. The first is that other modes such as epigenetic inheritance, phenotypic plasticity, the Baldwin effect, and the maternal effect allow new characteristics to arise and be passed on, and for the genes to catch up with the new adaptations later. The second is that all such mechanisms are part, not of an inheritance system, but a developmental system: the fundamental unit is not a discrete selfishly competing gene, but a collaborating system that works at all levels from genes and cells to organisms and cultures to guide evolution.\n\nLooking back at the conflicting accounts of the modern synthesis, the historian Betty Smocovitis notes in her 1996 book \"Unifying Biology: The Evolutionary Synthesis and Evolutionary Biology\" that both historians and philosophers of biology have attempted to grasp its scientific meaning, but have found it \"a moving target\"; the only thing they agreed on was that it was a historical event. In her words \"by the late 1980s the notoriety of the evolutionary synthesis was recognized . . . So notorious did 'the synthesis' become, that few serious historically minded analysts would touch the subject, let alone know where to begin to sort through the interpretive mess left behind by the numerous critics and commentators\".\n\n\n"}
{"id": "244680", "url": "https://en.wikipedia.org/wiki?curid=244680", "title": "Multiple cropping", "text": "Multiple cropping\n\nIn agriculture, multiple cropping is the practice of growing two or more crops in the same piece of land in same growing seasons instead of one crop. It is a form of polyculture. It can take the form of double-cropping, in which a second crop is planted after the first has been harvested, or relay cropping, in which the second crop is started amidst the first crop before it has been harvested. A related practice, companion planting, is sometimes used in gardening and intensive cultivation of vegetables and fruits. One example of multi-cropping is tomatoes + onions + marigold; the marigolds repel some tomato pests.\nMixed cropping is found in many agricultural traditions. In the Garhwal Himalaya of India, a practice called baranaja involves sowing 12 or more crops on the same plot, including various types of beans, grains, and millets, and harvesting them at different times.\n\nIn the cultivation of rice, multiple cropping requires effective irrigation, especially in areas with a dry season. Rain that falls during the wet season permits the cultivation of rice during that period, but during the other half of the year, water cannot be channeled into the rice fields without an irrigation system. The Green Revolution in Asia led to the development of high-yield varieties of rice, which required a substantially shorter growing season of 100 days, as opposed to traditional varieties, which needed 150 to 185 days. Due to this, multiple cropping became more prevalent in Asian countries.\n\n\n"}
{"id": "8243614", "url": "https://en.wikipedia.org/wiki?curid=8243614", "title": "Oregon pioneer history", "text": "Oregon pioneer history\n\nOregon pioneer history (1806—1890) is the period in the history of Oregon Country and Oregon Territory, in the present day state of Oregon and Northwestern United States.\n\nIt was the era when pioneers and mountain men, primarily of European descent, traveled west across North America to explore and settle the lands west of the Rocky Mountains and north of California. Some also arrived via the Pacific Ocean, traveling by ship either around Cape Horn or by changing ships at Panama. The period begins after the explorations of the lower Columbia River by Robert Gray and George Vancouver in 1792, along with the 1804-1806 Lewis and Clark Expedition to Oregon Country, and runs until circa 1890 when railroads and urban centers created a more settled state.\n\nAt the beginning of the pioneer period the Oregon Country was the homeland of numerous tribes of Native Americans. Regardless, portions of the area were claimed by the United States, Great Britain, Spain, and Russia. From 1818 to the mid-19th century, several treaties were signed that would set the current political boundaries. In 1818, the United States and Great Britain signed the Treaty of 1818 that led to what has been termed a \"joint occupation\" of the Oregon Country. Also in 1818 the U.S. resolved its claims with Spain regarding the western Louisiana Purchase lands, limiting Spanish claims to colonial Alta California south of the 42nd degree of latitude. This was followed by the Russo-American Treaty in 1825 that removed all remaining Russian-American Company claims south of 54' 40\" (previously resolved with the British, regarding claims with Great Britain only). The remaining territorial conflict between British and U.S. claims continued until 1846 when the Oregon Treaty settled the boundary issue with the 49th degree of latitude set as the international boundary between the United States and Britain's North American possessions. However, due to some ambiguity in the treaty, future conflict did arise and ended with the bloodless Pig War over the San Juan Islands.\nThe portion that became part of the United States in 1846 remained unorganized until Congress created the Oregon Territory in August 1848. In 1853, the northern boundary of the current state of Oregon was defined, with roughly half of the original Oregon Territory becoming the Washington Territory. The boundaries were finalized for Oregon upon entering the Union as the 33rd state on February 14, 1859. The remaining northeastern portion of the territory became part of the Washington Territory. In 1863, Idaho Territory was created from the eastern section of the Washington Territory, with other former eastern portions of the Oregon Territory becoming parts of Montana Territory and Wyoming Territory. Washington became a state in 1889, followed by Idaho in 1890.\n\nVarious Native American tribes inhabited the region at the beginning of the pioneer settlement period. Each tribe had their own forms of government, but no modern nation existed. The first formal government in the region came in the form of the Hudson's Bay Company, who were granted the authority by their charter to in effect rule over the region's British subjects. Thus the HBC was the de facto government for much of the region until U.S. settlers eventually outnumbered the British in the region.\n\nBeginning in 1841 with the death of Ewing Young, settlers in the Willamette Valley held a series of meetings at Champoeg, Oregon. Eventually, in 1843 the majority of participants voted to create a government to rule over the pioneers until the boundary question would be settled. This temporary government had a supreme judge, a legislature, and at first an executive committee followed later by a governor. This government remained in control of portions of the region until 1849 when the United States' territorial government arrived.\n\nBeginning in 1849, the Oregon Territorial Legislature began meeting and passing laws, with the Provisional Government's laws remaining in effect unless a new law was passed (except a law allowing for minting of money that was set aside by the first territorial governor). In 1857, the people of the territory passed a resolution to hold a convention to draft a constitution in order to achieve statehood. The Oregon Constitutional Convention was held in Salem during the summer of 1857, and created Oregon's first constitution. Oregon submitted the constitution to Congress, and on February 14, 1859, Oregon became a state.\n\nThe Lewis and Clark Expedition helped expand interest in the Pacific Northwest. Although seaborne traders had been engaged in the fur trade along the coastline for many years, Lewis and Clark's news and descriptions of the region spurred others in the United States to seek fortunes in the fur trade business in Oregon Country. The first Americans to return were members of John Jacob Astor's Pacific Fur Company as part of an expedition that established Fort Astoria at the mouth of the Columbia River in 1811. However, some of the first British traders overland include members of the North West Company that crossed the Rocky Mountains in 1808 and traveled down what they named the Fraser River in modern British Columbia. The fur trade envisioned by the American Pacific Fur Company and put into practice by the North West Company, and later Hudson's Bay Company, was a triangular trade that sent furs to China, Chinese goods such as tea to England, and manufactured goods to the Pacific Northwest for trade with the Native Americans.\n\nIn 1813, during the War of 1812, the Pacific Fur Company representatives at Fort Astoria sold the fort and all other company assets in Oregon to the British-owned North West Company. This happened under threat from a British warship and without the confirmation of John Jacob Astor. Fort Astoria was renamed Fort George. When Astor tried to regain the post, the British insisted their takeover was a business deal, not an act of war. Astor lost his investment. Some years after the war, U.S. diplomats interpreted the Treaty of Ghent, which ended the war, as including the return of the fur trade post. Although returned to American ownership, the site of Fort Astoria was not reoccupied for many years. The North West Company built a new Fort George adjacent to the old one. In 1821 the North West Company and the Hudson's Bay Company were merged by an act of Parliament with the name of the Hudson's Bay Company (HBC) retained for the combined entity. The HBC then named Dr. John McLoughlin as Chief Factor for the region the HBC called the Columbia District which encompassed much of the drainage of the Columbia River. In 1822 McLoughlin had a new post built near where the confluence of the Willamette River and the Columbia. On the north shore of the Columbia a new headquarters, Fort Vancouver, became the centerpiece of a multi-post system where furs and supplies were funneled in and out of Fort Vancouver. Brigades of fur trappers would spend months in the wilderness trapping animals, then return with the pelts to fur posts such as Fort George, Fort Umpqua, Fort Walla Walla, Fort Nisqually, Fort Okanogan, and Fort Boise. Later the HBC would start the Puget Sound Agricultural Company to supply food staples to the venture. By the 1830s the Hudson's Bay Company was worried about American expansion into the region and, in an attempt to forestall it, made a policy that fur trapping brigades operating south of the Columbia River, especially in the drainages of the Snake River and Willamette River, would work to create so-called \"fur deserts\", where beaver stocks were rapidly and deliberately depleted. This policy, although successful in making beaver rare in the Willamette Valley, did not prevent American settlement.\n\nThe next player in the fur trade was American Nathaniel Jarvis Wyeth who had made a fortune in the ice business in New England. In 1832 he led a new expedition to establish a fur trading empire through his new Pacific Trading Company. After returning from Oregon Country, Wyeth set out again in 1834 to set up the trading posts. His expedition established Fort Hall (on the Snake River) and Fort William (on Wapatoo Island), but the venture was a failure due to the dominance of the HBC in the region and the American Fur Company's control of the trade in the Rocky Mountains. In 1836 Wyeth sold his two posts to the HBC.\n\nBeginning in the early 1840s the fur trade began to decline as fashion tastes shifted away from beaver pelt hats and the numbers of beavers declined due to over harvesting. Then beginning in the mid-1830s missionaries and settlers began to arrive in the region Also the majority of the Native Americans in many areas were killed off by diseases introduced by Euro-Americans, including up to 70 percent in the Willamette Valley and Lower Columbia valley by 1830. Mass migration began in 1842 when a wagon train of around 100 wagons came overland along the Oregon Trail. In 1846 McLoughlin retired from leading the HBC in the region. Then in 1849 the United States Army arrived after the creation of the Oregon Territory and set up adjacent to Fort Vancouver. In June 1860 the Hudson's Bay Company closed the fort and withdrew to Fort Victoria, essentially ending the systematic fur trade in the region.\n\nEarly travel to the region was mainly by ship, with overland transportation developing later. By the 1830s a steady stream of travelers entered Oregon from the south through California and from the east over the Rocky Mountains. Many of these people were involved with the fur trade and would use the well worn trails of the Native Americans. Travel overland was mainly by horse, mule, and foot until the later 1830s when wagons slowly worked their way into the region. The Oregon Trail began seeing mass migration involving wagon trains in 1843. Boats were used extensively to haul cargo in the region, including steamboats, with the SS Beaver as the first steamboat in Oregon.\n\nAs more settlers arrived in the area, further transportation infrastructure was developed. Roads such as the Barlow Road, Canyon Road, and the Applegate Trail were created and small bridges built. Ferries also began to appear in the 1840s at many river crossings in the region. As the population grew, steamboats began regular service on the rivers, and later railroads were developed. The Oregon Steam Navigation Company and other smaller carriers developed transportation networks. The first railroad came in 1858 with the Cascade Railroad Company operating a line in the Columbia River Gorge, followed by the Oregon and California Railroad and eventually connections to the transcontinental rail lines in 1883.\n\nIn 1873, at Willamette Falls a lock and canal were completed to allow vessels to pass the waterfall and continue upriver on the Willamette River. Construction on a lock to bypass a set of cascades on the Columbia River began in 1878, but were not completed until 1896. Other canals were also built, including the Tualatin Canal at Oswego Lake. In 1887, the Morrison Bridge was completed as the first bridge over the Willamette River in Portland.\n\nIn January 1837, thirteen pioneer settlers formed the Willamette Cattle Company to travel to Mexican-owned California and purchase cattle. The settlers were urged on by United States Navy officer William A. Slacum, who was on a mission from the United States President. Slacum provided some financing and the transportation to California aboard the vessel \"Loriot\". The settlers were led by American Ewing Young, with others such as Jason Lee of the Methodist Mission and John McLoughlin of the Hudson's Bay Company providing additional investment.\n\nYoung led a small group to California, sailing from the Willamette River to San Francisco Bay. There the group procured about 630 head of cattle, which they drove north to the Willamette Valley. In Oregon, the cattle were divided between the investors, making Young one of the richest settlers in Oregon and helping to break the dependence of the settlers on the cattle of the HBC.\n\nAlthough the Willamette Cattle Company brought some cattle to Oregon Country, the demand exceeded the supply. Beginning in 1840, another group of pioneers began building a ship to sail south to California where they would trade the ship for more livestock. This operation ended in 1843 when the group returned to the Willamette Valley settlements with a variety of livestock. They left California with 1,250 head of cattle, 600 horses and mules, and 3,000 sheep.\n\n"}
{"id": "82249", "url": "https://en.wikipedia.org/wiki?curid=82249", "title": "Pioneer P-31", "text": "Pioneer P-31\n\nPioneer P-31 (also known as Atlas-Able 5B or Pioneer Z) was intended to be a lunar orbiter probe, but the mission failed shortly after launch. The objectives were to place a highly instrumented probe in lunar orbit, to investigate the environment between the Earth and Moon, and to develop technology for controlling and maneuvering spacecraft from Earth. It was equipped to take images of the lunar surface with a television-like system, estimate the Moon's mass and topography of the poles, record the distribution and velocity of micrometeorites, and study radiation, magnetic fields, and low frequency electromagnetic waves in space. A midcourse propulsion system and injection rocket would have been the first United States self-contained propulsion system capable of operation many months after launch at great distances from Earth and the first U.S. tests of maneuvering a satellite in space.\n\nThe spacecraft was launched on Atlas vehicle 91D coupled to Thor-Able upper stages including an Able solid propellant third stage on December 15, 1960. The launch was uneventful until T+66 seconds when a severe axial disturbance was recorded, followed by rapid loss of LOX tank pressure and changes in the Atlas's engine exhaust indicative of oxidizer starvation. At T+73 seconds, the Atlas experienced total structural breakup and loss of telemetry. The upper stages continued transmitting data until impact with the ocean. The payload fell into the Atlantic Ocean 12 to 20 km (7 to 12 miles) from Cape Canaveral in about 20 meter (65 feet) deep water. A Navy salvage operation recovered parts of the launch vehicle and the payload. The immediate cause of the failure was unclear, but thought to be related to either the adapter mating the Able stages to the Atlas coming loose and being rammed into the LOX tank or else aerodynamic buffeting on the launch vehicle. The recovered Able second stage showed no sign that engine ignition or operation had taken place, and the most probable cause of the failure was believed to be aerodynamic flexing of the Able adapter which then ruptured the Atlas's LOX tank. The crippled booster continued to fly for a few seconds afterwards, but the structural collapse of the Atlas's forward section combined with the loss of LOX pressure to the propellant feed system resulted in engine shutdown and vehicle self-destruction.\n\nAs a result of this failure and Mercury-Atlas 1 five months earlier due to a similar episode of aerodynamic bending in the forward portion of the LOX tank, GD/A began requiring that all Atlas upper stage/payload combinations undergo proper structural dynamics testing.\n\nThe failure was described as \"especially disappointing\" since it was the final launch in the Able probe series as its successor, the Ranger program, was in the works. In the end, the US space program would not see a completely successful lunar probe until Ranger 7 four years later. It also marked the final launch in the first generation of lunar probes, which used direct ascent trajectories and would give way to the second generation probes which had parking orbits.\n\nPioneer P-31 was virtually identical to the earlier Pioneer P-30 satellite which failed, a 1-meter diameter sphere with a propulsion system mounted on the bottom giving a total length of 1.4 meters. The mass of the structure and aluminum alloy shell was about 30 kg and the propulsion units roughly 90 kg. Four solar panels, each 60 x 60 cm and containing 2200 solar cells in 22 100-cell nodules, extended from the sides of the spherical shell in a \"paddle-wheel\" configuration with a total span of about 2.7 meters. The solar panels charged nickel-cadmium batteries. Inside the shell, a large spherical hydrazine tank made up most of the volume, topped by two smaller spherical nitrogen tanks and a 90 N injection rocket to slow the spacecraft down to go into lunar orbit, which was designed to be capable of firing twice during the mission. Attached to the bottom of the sphere was a 90 N vernier rocket for mid-course propulsion and lunar orbit maneuvers which could be fired four times.\n\nAround the upper hemisphere of the hydrazine tank was a ring-shaped instrument platform which held the batteries in two packs, two 1.5 W UHF transmitters and diplexers, logic modules for scientific instruments, two command receivers, decoders, a buffer/amplifier, three converters, a telebit, a command box, and most of the scientific instruments. Two dipole UHF antennas protruded from the top of the sphere on either side of the injection rocket nozzle. Two dipole UHF antennas and a long VLF antenna protruded from the bottom of the sphere. The transmitters operated on a frequency of 378 MHz.\n\nThermal control was planned to be achieved by 50 small \"propeller blade\" devices on the surface of the sphere. The blades themselves were made of reflective material and consisted of four vanes which were flush against the surface, covering a black heat-absorbing pattern painted on the sphere. A thermally sensitive coil was attached to the blades in such a way that low temperatures within the satellite would cause the coil to contract and rotate the blades and expose the heat absorbing surface, and high temperatures would cause the blades to cover the black patterns. Square heat-sink units were also mounted on the surface of the sphere to help dissipate heat from the interior.\n\nThe scientific instruments consisted of an ion chamber and Geiger-Müller tube to measure total radiation flux, a proportional radiation counter telescope to measure high energy radiation, a scintillation counter to monitor low-energy radiation, a scintillation spectrometer to study the Earth's (and possible lunar) radiation belts, a VLF receiver for natural radio waves, a transponder to study electron density, and part of the flux-gate and search coil magnetometers mounted on the instrument platform. A plasma probe was mounted on the sphere to measure energy and momentum distribution of protons above a few kilovolts to study the radiation effect of solar flares. The micrometeorite detector and sun scanner were mounted on the sphere as well. The only difference between Pioneer P-31 and the earlier Pioneer P-30 was the addition of a solid state detector sensitive to low energy protons on the satellite and an STL-designed rubidium frequency standard experiment placed on a pod attached to the booster. The total mass of the science package including electronics and power supply was roughly 60 kg. Total cost of the mission was estimated at 9–10 million dollars.\n\n"}
{"id": "56373939", "url": "https://en.wikipedia.org/wiki?curid=56373939", "title": "Priscilla Baker", "text": "Priscilla Baker\n\nPriscilla Baker is a professor of analytical chemistry at the University of the Western Cape. She is the co-leader of SensorLab, a research platform in electrochemistry that deals with the electrodynamics of materials and sensors. She is an active member of the Academy of Science of South Africa, European Scientific Network for Artificial Muscles (ESNAM) and the Marie Curie International staff exchange scheme (IRSES).\n\nBaker obtained her BSc at the University of Cape Town and majored in Ocean and Atmospheric Science as the only black female in her class. She then completed her National Diploma in Analytical Chemistry, at the Cape Peninsula University of Technology. After getting interested in electrochemistry, she did her BSc Honours (Chemistry) and successfully completed her MSc dissertation (Chemistry) on the evaluation of trace metals in the atmosphere at University of the Western Cape. In 2004, she received her PhD (Chemistry) in the area of novel metal tin oxide composites as anodes for phenol degradation, at the University of Stellenbosch.\n"}
{"id": "51320802", "url": "https://en.wikipedia.org/wiki?curid=51320802", "title": "Proton radius puzzle", "text": "Proton radius puzzle\n\nThe proton radius puzzle is an unanswered problem in physics relating to the size of the proton. Historically the proton radius was measured via two independent methods, which converged to a value of about 0.8768 femtometres (1 fm = 10 m). This value was challenged by a 2010 experiment utilizing a third method, which produced a radius about 5% smaller than this. The discrepancy remains unresolved, and is a topic of ongoing research.\n\nPrior to 2010, the proton radius was measured using one of two methods: one relying on spectroscopy, and one relying on nuclear scattering.\n\nThe spectroscopy method uses the energy levels of electrons orbiting the nucleus. The exact values of the energy levels is sensitive to the nuclear radius (see Lamb Shift). For hydrogen, whose nuclei consists only of one proton, this indirectly measures the proton radius. Measurements of hydrogen's energy levels are now so precise that the proton radius is the limiting factor when comparing experimental results to theoretical calculations. This method produces a proton radius of about (or ), with approximately 1% relative uncertainty.\n\nThe nuclear method is similar to Rutherford's scattering experiments that established the existence of the nucleus. Small particles such as electrons can be fired at a proton, and by measuring how the electrons are scattered, the size of the proton can be inferred. Consistent with the spectroscopy method, this produces a proton radius of about .\n\nIn 2010, Pohl et al. published the results of an experiment relying on muonic hydrogen as opposed to normal hydrogen. Conceptually, this is similar to the spectroscopy method. However, the much higher mass of a muon causes it to orbit 207 times closer than an electron to the hydrogen nucleus, where it is consequently much more sensitive to the size of the proton. The resulting radius was recorded as , 5 standard deviations (5σ) smaller than the prior measurements. The newly measured radius is 4% smaller than the prior measurements, which were believed to be accurate within 1%. (The new measurement's uncertainty limit of only 0.1% makes a negligible contribution to the discrepancy.)\n\nSince 2010, additional measurements using electrons have slightly reduced the estimated radius to (), but by reducing the uncertainty even more the disagreement has worsened to over 7σ.\n\nA follow-up experiment by Pohl et al. in August 2016 used a deuterium atom to create muonic deuterium and measured the deuteron radius. This experiment allowed the measurements to be 2.7 times more accurate, but also found a discrepancy of 7.5 standard deviations smaller than the expected value. In 2017 Pohl's group performed yet another experiment, this time using hydrogen atoms that had been excited by two different lasers. By measuring the energy released when the excited electrons fell back to lower-energy states, the Rydberg constant could be calculated, and from this the proton radius inferred. The result is again ~5% smaller than the generally-accepted proton radius.\n\nThe anomaly remains unresolved and is an active area of research. There is as yet no conclusive reason to doubt the validity of the old data. The immediate concern is for other groups to reproduce the anomaly. \n\nThe uncertain nature of the experimental evidence has not stopped theorists from attempting to explain the conflicting results. Among the postulated explanations are the three-body force, interactions between gravity and the weak force or a flavour-dependent interaction, higher dimension gravity, a new boson, and the quasi-free hypothesis.\n\nRandolf Pohl, the original investigator of the puzzle, stated that while it would be \"fantastic\" if the puzzle led to a discovery, the most likely explanation is not new physics but some measurement artefact. His personal assumption is that past measurements have misgauged the Rydberg constant and that the current official proton size is inaccurate. \n\nIn one of the most recent attempts of resolve the puzzle without new physics, Alarcón, et al. (2018), at Jefferson Labs, have proposed that a different technique to fit the experimental scattering data to a conclusion regarding the proton charge radius that gives more weight to a larger number of higher energy data points in an analytically well justified manner produces a proton charge radius determination from the existing electron scattering data that is consistent with the muonic hydrogen measurement. Effectively, this approach attributes the cause of the proton radius puzzle to a failure to account for significant theoretical uncertainty introduced in previous determinations by the process used to fit the experimental observations made to a determination of the proton charge radius. Other investigators have suggested that the analysis used for the electron based proton charge radius may not be properly considering the rest frames of the different components of the experiments in light of special relativity. Consideration of polarization factors in the muonic hydrogen case that aren't material in ordinary hydrogen has also been proposed as a possible solution.\n"}
{"id": "3342537", "url": "https://en.wikipedia.org/wiki?curid=3342537", "title": "Pwpaw", "text": "Pwpaw\n\nPWPAW A Projector Augmented Wave (PAW) code for electronic structure calculation. It is a free software package, distributed under the copyleft GNU General Public License. It is a plane wave implementation of the projector augmented wave (PAW) method developed by Peter E Blochl for electronic structure calculations within the framework of density functional theory. In addition to the self-consistent calculation of the electronic structure of a periodic solid, the program has a number of other capabilities, including structural geometry optimization and molecular dynamics simulations within the Born–Oppenheimer approximation.\n\n"}
{"id": "48457228", "url": "https://en.wikipedia.org/wiki?curid=48457228", "title": "Rachel Chan", "text": "Rachel Chan\n\nRachel Chan is an Argentine biologist from the Santa Fe Province in Argentina. She was named one of the ten most outstanding scientists in Latin America by the BBC.\n\nChan did her undergraduate study at the Hebrew University of Jerusalem, Israel. She received her Ph.D. degree in Biochemistry from the National University of Rosario, Argentina, in 1988.\n\nChumino held positions at the National University of Litoral, the National Research Council (Conicet), and the Agrobiotechnology Institute of Santa Fe (IAL). The IAL researches biotechnology and plant molecular biology. Chan’s research was concentrating on photosynthesis. After returning to Argentina in 1992, she began a project to understand how plants are affected by environmental conditions. Chan's team of scientific researchers created more drought resistant seeds.\n\nAbout her work Chan stated, \n"}
{"id": "57785164", "url": "https://en.wikipedia.org/wiki?curid=57785164", "title": "Sentinel outlet", "text": "Sentinel outlet\n\nA sentinel outlet in occupational safety and health is a water outlet that is chosen to have its temperature monitored so that risk from \"Legionella\" can be controlled. This is typically chosen to be the closest and furthest outlets from the water tank.\n"}
{"id": "4136119", "url": "https://en.wikipedia.org/wiki?curid=4136119", "title": "Special Engineer Detachment", "text": "Special Engineer Detachment\n\nSpecial Engineer Detachment (SED) was a US Army program that identified enlisted personnel with technical skills, such as machining, or who had some science education beyond high school. Those identified were organized into the Special Engineer Detachment, or SED. SED personnel began arriving at Los Alamos in October 1943. By August 1945, 1800 SED personnel worked at Los Alamos. These troops worked in all areas and activities of the Laboratory, including the Trinity Test, and were involved in overseas operations on Tinian.\n\n"}
{"id": "48661", "url": "https://en.wikipedia.org/wiki?curid=48661", "title": "Statistician", "text": "Statistician\n\nA statistician is a person who works with theoretical or applied statistics. The profession exists in both the private and public sectors. It is common to combine statistical knowledge with expertise in other subjects, and statisticians may work as employees or as statistical consultants.\n\nAccording to the United States Bureau of Labor Statistics, as of 2014, 26,970 jobs were classified as \"statistician\" in the United States. Of these people, approximately 30 percent worked for governments (federal, state, or local). As of May 2016, the median pay for statisticians in the United States was $80,500. Additionally, there is a substantial number of people who use statistics and data analysis in their work but have job titles other than \"statistician\", such as actuaries, applied mathematicians, economists, data scientists, data analysts (predictive analytics) financial analysts, psychometricians, sociologists, epidemiologists, and quantitative psychologists. Statisticians are included with the professions in various national and international occupational classifications. According to the BLS, \"Overall employment is projected to grow 33% from 2016 to 2026, much faster than average for all occupations. Businesses will need these workers to analyze the increasing volume of digital and electronic data.\" \n\nIn the United States most employment in the field requires either a masters degree in statistics or a related field or a PhD. \"Typical work includes collaborating with scientists, providing mathematical modeling, simulations, designing randomized experiments and randomized sampling plans, analyzing experimental or survey results, and forecasting future events (such as sales of a product).\"\n\n\n"}
{"id": "30823998", "url": "https://en.wikipedia.org/wiki?curid=30823998", "title": "Sump (aquarium)", "text": "Sump (aquarium)\n\nIn fishkeeping, a sump is an accessory aquarium tank in which mechanical equipment is kept. A remote sump allows for a clutter-free display tank.\n\nIt is found mainly in a reef aquarium or marine aquarium. The sump sits below, behind, or (less typically) on a support above the main tank, and is used as a filter, as well as a holding place for unsightly, miscellaneous equipment such as protein skimmers, calcium reactors, and heaters. The main advantage of having a sump plumbed into an aquarium is the increase of water volume in the system, making it more stable and less prone to fluctuations of pH and salinity, and also mitigating the effects of nutrient buildup or the unintentional introduction of foreign substances. In addition, some sumps have a compartment that can be converted into a refugium, helping to filter out excess nutrients such as nitrates.\n\nA sump can also improve aeration of the water in the aquarium. Water movement between the sump and the display tank helps with gas exchange between the water and air. Increased dissolved oxygen is beneficial to fish and can also aid in avoiding \"Cyanobacteria\" outbreaks.\n"}
{"id": "46619066", "url": "https://en.wikipedia.org/wiki?curid=46619066", "title": "Timeline of Permian research", "text": "Timeline of Permian research\n\nThis timeline of Permian research is a chronological listing of events in the history of geology and paleontology focused on the study of earth during the span of time lasting from 298.9–252.17 million years ago and the legacies of this period in the rock and fossil records.\n\n1828\n\n1853\n\n1864 - 1866\n\n1870\n\n1877\n\n1877\n\n1882\n\n1887\n\n1899\n\n1904\n\n1911\n\n1922\n\n1937\n\n1948\n\n"}
{"id": "41996593", "url": "https://en.wikipedia.org/wiki?curid=41996593", "title": "University of Chicago Institute of Politics", "text": "University of Chicago Institute of Politics\n\nThe Institute of Politics is an extracurricular, nonpartisan institute at the University of Chicago designed to inspire students to pursue careers in politics and public service. The Institute accomplishes its goals through four major avenues: A civic engagement program, where students take part in community service projects and gain leadership skills, a fellows program that hosts a group of political and policy professionals to lead seminars for an academic quarter, a speaker series featuring public events with a diverse array of political figures, and a career development program featuring hundreds of internships in government, politics and policy. It was formally established in 2013 with David Axelrod, who was President Barack Obama's chief campaign advisor, as its director. \n\nSince its inception, the IOP has hosted prominent speakers including Joe Biden, Mitt Romney, Rand Paul, Al Gore, Rick Santorum, John McCain, Newt Gingrich, Bernie Sanders, John Brennan, Frank Bruni, Edward Snowden (via videochat), Jon Stewart and Chance the Rapper; hosted fellows such as Beth Myers, Michael Steele, Roger Simon, Husain Haqqani, Matthew Dowd, Howard Wolfson, Mark Udall, Tom Harkin, Michael Morell, Jeff Roe and Bakari Sellers. It has arranged over 250 student internships to institutions like the U.S. Capitol, the Brookings Institution, and the White House, and placed over 300 students in civic engagement projects.\n"}
{"id": "624976", "url": "https://en.wikipedia.org/wiki?curid=624976", "title": "V-3 cannon", "text": "V-3 cannon\n\nThe V-3 (, \"Retribution Weapon 3\") was a German World War II supergun working on the multi-charge principle whereby secondary propellant charges are fired to add velocity to a projectile.\n\nThe weapon was planned to be used to bombard London from two large bunkers in the Pas-de-Calais region of northern France, but they were rendered unusable by Allied bombing raids before completion. Two similar guns were used to bombard Luxembourg from December 1944 to February 1945.\n\nThe V-3 was also known as the \"Hochdruckpumpe\" (\"High Pressure Pump,\" HDP for short), which was a code name intended to hide the real purpose of the project. It was also known as \"Fleißiges Lieschen\" (\"Busy Lizzie\").\n\nThe gun used multiple propellant charges placed along the barrel's length and timed to fire as soon as the projectile passed them in order to provide an additional boost. Solid-fuel rocket boosters were used instead of explosive charges because of their greater suitability and ease of use. These were arranged in symmetrical pairs along the length of the barrel, angled to project their thrust against the base of the projectile as it passed. This layout spawned the German codename \"Tausendfüßler\" (\"millipede\"). The barrel and side chambers were designed as identical sections to simplify production and allow damaged sections to be replaced. The entire gun would use multiple such sections bolted together. The smoothbore gun fired a fin-stabilized shell that depended upon aerodynamic forces rather than gyroscopic forces to prevent tumbling (distinct from conventional rifled weapons which cause the projectile to spin); this resulted in a lower drag coefficient.\n\nThe origin of the multi-chamber gun dates back to the 19th century. In 1857, U.S. arms expert Azel Storrs Lyman (1815–1885) was granted a patent on \"Improvement in accelerating fire-arms\", and he built a prototype in 1860 which proved to be unsuccessful. Lyman then modified the design in collaboration with James Richard Haskell, who had been working for years on the same principle.\n\nHaskell and Lyman reasoned that subsidiary propellant charges could increase the muzzle velocity of a projectile if the charges were spaced at intervals along the barrel of a gun in side chambers and ignited an instant after a shell had passed them. The \"Lyman-Haskell multi-charge gun\" was constructed on the instructions of the U.S. Army's Chief of Ordnance, but it did not resemble a conventional artillery piece. The barrel was so long that it had to be placed on an inclined ramp, and it had pairs of chambers angled back at 45 degrees discharging into it. It was test fired at the Frankford Arsenal at Philadelphia in 1880 and was unsuccessful. The flash from the original propellant charge bypassed the projectile due to faulty obturation and prematurely ignited the subsidiary charges before the shell passed them, slowing the shell down. The best velocity that could be obtained from it was , inferior to the performance of a conventional RBL 7 inch Armstrong gun of the same period. New prototypes of multi-charge guns were built and tested, but Lyman and Haskell abandoned the idea.\n\nDuring the same period, French engineer Louis-Guillaume Perreaux, one of the pioneers of the motorcycle, had been working on a similar project since before 1860. Perreaux was granted a patent in 1864 for a multi-chamber gun. In 1878, Perreaux presented his invention at the World Exhibition of Paris.\n\nIn 1918, the French Army made plans for a very long range multi-chamber gun in response to the German Paris Gun. The Paris Gun was built by Friedrich Krupp AG and could bombard Paris from German lines over a distance of no less than . However, the French initiative did not reach the prototype stage, as it was discontinued when the retreat of the German armies and the armistice put an end to the bombardment. The plans for the multi-chamber gun were archived, as they had been envisioned to counter the German fire.\n\nFrance collapsed in June 1940 at the beginning of World War II, and German troops acquired the plans of this long-range gun. In 1942, this patent attracted the attention of August Cönders, developer of the Röchling shell and chief engineer of the plants \"Röchling Stahlwerk AG\" in Wetzlar, Germany. Cönders thought that the gradual acceleration of the shell by a series of small charges spread over the length of the barrel might be the solution to the problem of designing very long range guns. The very strong explosive charge needed to project shells at a high speed were causing very rapid degradation of the gun tubes of conventional guns.\n\nCönders proposed the use of electrically activated charges to eliminate the problem of the premature ignition of the subsidiary charges, the problem experienced by the Lyman-Haskell gun. Cönders built a prototype of a 20 mm multi-chamber gun using machinery readily available at the Wetzlar plant, machinery that was producing tubes of this caliber for the Flak 38 anti-aircraft guns of 20 mm. The first tests were encouraging, but to get the support of the Ministry of arms, Hermann Röchling had to present to Albert Speer Cönders' project of a cannon capable of firing on London from the coast of the Pas-de-Calais. The project intended to use two batteries to crush London under a barrage of hundreds of shells per hour, shells of with an explosive charge of .\n\nSpeer told Adolf Hitler about the proposal in May 1943. After the Royal Air Force (RAF) bombed the Peenemünde rocket center on 17 August, Hitler agreed to Speer's suggestion that the gun be built without more tests. Cönders constructed a full-calibre gun at the Hillersleben proving ground near Magdeburg but, by the end of 1943, he had encountered severe problems both in putting the gun's basic principle into operation and in producing a feasible design for the shells that it was to fire. Even when everything worked, the muzzle velocity was just over , which was nowhere near what had been promised. Nonetheless, a proposal was made to build a single full-sized gun with a barrel at Misdroy on the Baltic island of Wolin, near Peenemünde, while construction went ahead at the Mimoyecques site in France (which had already been attacked by the USAAF and the RAF). The \"Heereswaffenamt\" (Weapon Procurement Office) took control of the project by March 1944, with no good news from Misdroy, and Cönders became one of the engineers working on the three chief problems: projectile design, obturation, and ignition of the secondary charges.\n\nSix different companies produced satisfactory designs for projectiles, including Krupp and Škoda Works. Obturation problems were solved by placing a sealing piston between the projectile and the initial propellant charge, which in turn prevented the flash from the charge from getting ahead of the projectile and solved the problem of controlling the initiation of the secondary charges. By the end of May 1944, there were four designs for the 150-mm finned projectile, one manufactured by Fasterstoff (designed by Füstenberg) and three others by Röchling (Cönders), Bochumer (Verein-Haack), and Witkowitz Ironworks (Athem).\n\nTrials were held at Misdroy from 20–24 May 1944 with ranges of up to being attained. On 4 July 1944, the Misdroy gun was test-fired with 8 rounds (one of the long shells travelled ). The gun burst during the testing, putting an end to the tests.\n\nMajor Bock of Festung Pioneer-Stab 27 (the fortification regiment of LVII Corps, Fifteenth Army, at the time based in the Dieppe area) was given the task of finding a suitable site for the HDP batteries following Hitler's decision that HDP guns should be sited in northern France to bombard London. A study in early 1943 concluded that a hill with a rock core would be most suitable, as the gun tubes could be placed in drifts (inclined tunnels) and support equipment and supplies located in adjacent tunnels. The guns would not be movable and would be permanently aimed at London.\n\nA suitable site was selected at a limestone hill about north of the Hidrequent quarries, near Mimoyecques in the Pas-de-Calais region of northern France behind Cap Gris Nez, very close to the French end of the present day Channel tunnel, where V-1 and V-2 launch sites were already under construction. The site was from the sea and from London. It was code-named \"Wiese\" (meadow) and \"Bauvorhaben 711\" (Construction Project 711), and Organisation Todt began construction in September 1943 with the building of railway lines to support the work, and began to excavate the gun shafts in October. The initial layout comprised two parallel facilities approximately apart, each with five drifts which were to hold a stacked cluster of five HDP gun tubes, for a total of 50 guns. Both facilities were served by an underground railway tunnel and underground ammunition storage galleries.\n\nThe eastern complex consisted of five drifts angled at 50 degrees reaching below the hilltop. The five drifts exited the hilltop through a concrete slab wide and thick. Large steel plates protected the five openings and each drift had a special armoured door. Extensive tunnels and elevator shafts supported the guns and, if the site had become operational, about 1,000 troops from Artillerie Abteilung 705 and supporting units would have been deployed at Mimoyecques. Artillerie Abteilung 705 had been organised in January 1944 under Oberstleutnant Georg Borttscheller to operate the Wiese gun complex.\n\nThe plans were to have the first battery of five gun tubes ready for March 1944, and the full complex of 25 gun tubes by 1 October 1944. A failure occurred, however, at the Misdroy proving ground in April 1944 after only 25 rounds had been fired and, as a result, the project was further cut back from five drifts to three, even though work had begun on some of the other drifts. The site was finally put out of commission on 6 July 1944, when bombers of RAF Bomber Command's 617 Squadron (the famous \"Dambusters\") attacked using \"Tallboy\" deep-penetration bombs.\n\nThe project eventually came under the control of the SS, and SS General Hans Kammler ordered it to be ready for action in late 1944, assisted by Walter Dornberger. A battery was constructed of two shorter V-3 guns approximately long with 12 side-chambers, and it was placed in the hands of the army artillery unit \"Artillerie Abteilung 705\" under the command of Hauptmann (Captain) Patzig. These were sited in a wooded ravine of the Ruwer River at Lampaden about southeast of Trier in Germany.\n\nThe two guns were aimed west, resting on 13 steel support structures on solid wooden bases on a 34 degree slope. The city of Luxembourg (which had been liberated in September 1944) was at a range of about and was designated Target No. 305. Concrete blockhouses were constructed between the two gun tubes, as well as ten smaller bunkers to hold projectiles and propellant charges.\n\nThe assembly and mounting of the Lampaden guns coincided with the final preparations for the Battle of the Bulge. However, the supply of ammunition became problematic due to the state of the German railway network. Time had become critical, and it was decided to use a finned projectile with a discarding sabot, weighing and carrying a explosive charge. The propellant comprised a main charge and 24 subsidiary charges for a total of .\n\nBy the time that the Ardennes offensive began on 16 December 1944, Kammler received orders from OB West (German Army Command in the West) to begin firing at the end of the month, and the first gun tube was ready for action on 30 December 1944. Two warm-up rounds were initially fired, followed by five high-explosive shells which were fired in sequence, attended by Kammler. The muzzle velocity was approximately .\n\nThe second gun tube was brought into operation on 11 January 1945 and some 183 rounds in total were fired until 22 February 1945, with 44 confirmed hits in the urban area. The guns were not particularly effective; from the 142 rounds that struck Luxembourg, total casualties were 10 dead and 35 wounded.\n\nOne of the two Lampaden guns was dismantled on 15 February, and firing ceased on 22 February, when US Army units had advanced to within of the Lampaden site.\n\nA second battery of guns began to be deployed in January 1945 at Buhl, aimed at Belfort in support of the Operation Nordwind offensive. One gun was erected before the failure of the Nordwind offensive put the site at risk, and the equipment was removed before firing could begin.\n\nThere were other proposals to deploy batteries to bombard London, Paris, Antwerp and other cities but these were not implemented due to the poor state of the German railway network and a lack of ammunition. All four HDP guns were eventually abandoned at the Röchling works in Wetzlar and Artillerie Abteilung 705 was re-equipped with conventional artillery. The disassembled gun tubes, spare parts, and remaining ammunition were later captured by the US Army and shipped to the United States where they were tested and evaluated at the Aberdeen Proving Ground, Maryland, and finally scrapped there in 1948.\n\nThe Mimoyecques museum allows visitors to view the galleries (in various stages of construction and bombing damage), remains of the guns, a small scale V-3 replica, and examples of machinery, rail systems and tools employed. The site also contains memorials to the slave labourers who were employed by the Nazis to construct it and to the airmen killed in action during the destruction of the base.\n\nThe Misdroy site also has a museum.\n\nHugh Hunt of Cambridge University, together with explosives engineer Charlie Adcock created a working scale model of the V-3 gun and was able to prove the ignition of the propellants was done by the advancing gas behind the projectile.\n\n\nNotes\n\nCitations\n\nBibliography\n"}
{"id": "53549788", "url": "https://en.wikipedia.org/wiki?curid=53549788", "title": "Valerie Belton", "text": "Valerie Belton\n\nValerie \"Val\" Belton is a British professor of management science and operations research at the University of Strathclyde.\n\nBelton obtained degrees in Mathematics and operations research for her Bachelor's and master's degrees from Durham University and Lancaster University respectively. Furthermore, she bagged a doctorate degree in multi-criteria decision analysis from Cambridge University. She is an expert in multi-criteria decision analysis (MCDA) and problem structuring. She was elected to the Council of the UK OR Society in 1980, their first female member. She was President of the Association of European Operational Research Societies from 2009–10, President of the UK OR Society from 2004–2006, and President of the International Society for MCDM from 2000-2004.\n\n\n\n"}
{"id": "29319582", "url": "https://en.wikipedia.org/wiki?curid=29319582", "title": "William Didusch", "text": "William Didusch\n\nWilliam P. Didusch(1895-1981) was a scientific illustrator known for his work for the American Urological Association.\n\n"}
{"id": "16342329", "url": "https://en.wikipedia.org/wiki?curid=16342329", "title": "Zhizn' Natsional'nostei", "text": "Zhizn' Natsional'nostei\n\nZhizn' Natsional'nostei (Жизнь национальностей, \"Life of the Nationalities\") was a journal published in Moscow from 1918 to 1924.\n\nMany senior figures in Narkomnats contributed to it.\n\nPublication was resumed in 1992, whereon it was circulated through the Commonwealth of Independent States.\n\n"}
