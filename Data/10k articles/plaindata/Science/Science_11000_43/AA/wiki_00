{"id": "40489234", "url": "https://en.wikipedia.org/wiki?curid=40489234", "title": "Agricultural Research Station, Parsabad", "text": "Agricultural Research Station, Parsabad\n\nAgricultural Research Station ( – \"Īstgāh Taḥqīqāt Keshāvarzy Maghān\") is a research station and village in Qeshlaq-e Shomali Rural District, in the Central District of Parsabad County, Ardabil Province, Iran. At the 2006 census, its population was 139, in 41 families.\n"}
{"id": "243062", "url": "https://en.wikipedia.org/wiki?curid=243062", "title": "American Association for the Advancement of Science", "text": "American Association for the Advancement of Science\n\nThe American Association for the Advancement of Science (AAAS) is an American international non-profit organization with the stated goals of promoting cooperation among scientists, defending scientific freedom, encouraging scientific responsibility, and supporting scientific education and science outreach for the betterment of all humanity. It is the world's largest general scientific society, with over 120,000 members, and is the publisher of the well-known scientific journal \"Science,\" which had a weekly circulation of 138,549 in 2008.\n\nThe American Association for the Advancement of Science was created on September 20, 1848 at the Academy of Natural Sciences in Philadelphia, Pennsylvania. It was a reformation of the Association of American Geologists and Naturalists. The society chose William Charles Redfield as their first president because he had proposed the most comprehensive plans for the organization. According to the first constitution which was agreed to at the September 20 meeting, the goal of the society was to promote scientific dialogue in order to allow for greater scientific collaboration. By doing so the association aimed to use resources to conduct science with increased efficiency and allow for scientific progress at a greater rate. The association also sought to increase the resources available to the scientific community through active advocacy of science. There were only 78 members when the AAAS was formed. As a member of the new scientific body, Matthew Fontaine Maury, USN was one of those who attended the first 1848 meeting.\n\nAt a meeting held on Friday afternoon, September 22, 1848, Redfield presided, and Matthew Fontaine Maury gave a full scientific report on his \"Wind and Current Charts\". Maury stated that hundreds of ship navigators were now sending abstract logs of their voyages to the United States Naval Observatory. He added, \"Never before was such a corps of observers known.\" But, he pointed out to his fellow scientists, his critical need was for more \"simultaneous observations.\" \"The work,\" Maury stated, \"is not exclusively for the benefit of any nation or age.\" The minutes of the AAAS meeting reveal that because of the universality of this \"view on the subject, it was suggested whether the states of Christendom might not be induced to cooperate with their Navies in the undertaking; at least so far as to cause abstracts of their log-books and sea journals to be furnished to Matthew F. Maury, USN, at the Naval Observatory at Washington.\"\n\nWilliam Barton Rogers, professor at the University of Virginia and later founder of the Massachusetts Institute of Technology, offered a resolution: \"Resolved that a Committee of five be appointed to address a memorial to the Secretary of the Navy, requesting his further aid in procuring for Matthew Maury the use of the observations of European and other foreign navigators, for the extension and perfecting of his charts of winds and currents.\" The resolution was adopted and, in addition to Rogers, the following members of the association were appointed to the committee: Professor Joseph Henry of Washington; Professor Benjamin Peirce of Cambridge, Massachusetts; Professor James H. Coffin of Easton, Pennsylvania, and Professor Stephen Alexander of Princeton, New Jersey. This was scientific cooperation, and Maury went back to Washington with great hopes for the future.\n\nBy 1860, membership increased to over 2,000. The AAAS became dormant during the American Civil War; their August 1861 meeting in Nashville, Tennessee was postponed indefinitely after the outbreak of the first major engagement of the war at Bull Run. The AAAS did not become a permanent casualty of the war.\n\nIn 1866, Frederick Barnard presided over the first meeting of the resurrected AAAS at a meeting in New York City. Following the revival of the AAAS, the group had considerable growth. The AAAS permitted all people, regardless of scientific credentials, to join. The AAAS did, however, institute a policy of granting the title of \"Fellow of the AAAS\" to well-respected scientists within the organization. The years of peace brought the development and expansion of other scientific-oriented groups. The AAAS's focus on the unification of many fields of science under a single organization was in contrast to the many new science organizations founded to promote a single discipline. For example, the American Chemical Society, founded in 1876, promotes chemistry.\n\nIn 1863, the US Congress established the National Academy of Sciences, another multidisciplinary sciences organization. It elects members based on recommendations from colleagues and the value of published works.\n\nAlan I. Leshner, AAAS CEO from 2001 until 2015, published many op-ed articles discussing how many people integrate science and religion in their lives. He has opposed the insertion of non-scientific content, such as creationism or intelligent design, into the scientific curriculum of schools.\n\nIn December 2006, the AAAS adopted an official statement on climate change, in which they stated, \"The scientific evidence is clear: global climate change caused by human activities is occurring now, and it is a growing threat to society...The pace of change and the evidence of harm have increased markedly over the last five years. The time to control greenhouse gas emissions is now.\"\n\nIn February 2007, the AAAS used satellite images to document human rights abuses in Burma. The next year, AAAS launched the Center for Science Diplomacy to advance both science and the broader relationships among partner countries, by promoting science diplomacy and international scientific cooperation.\n\nIn 2012, AAAS published op-eds, held events on Capitol Hill and released analyses of the U.S. federal research-and-development budget, to warn that a budget sequestration would have severe consequences for scientific progress.\n\nAAAS covers various areas of sciences and engineering. It has twelve sections, each with a committee and its chair. These committees are also entrusted with the annual evaluation and selection of Fellows (see: Fellow of the American Association for the Advancement of Science). The sections are:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe most recent Constitution of the AAAS, enacted on January 1, 1973, establishes that the governance of the AAAS is accomplished through four entities: a President, a group of administrative officers, a Council, and a Board of Directors.\n\nIndividuals elected to the presidency of the AAAS hold a three-year term in a unique way. The first year is spent as President-elect, the second as President and the third as Chairperson of the Board of Directors. In accordance with the convention followed by the AAAS, presidents are referenced by the year in which they left office.\n\nGeraldine Richmond is the President of AAAS for 2015–16; Phillip Sharp is the Board Chair; and Barbara A. Schaal is the President-Elect. Each took office on the last day of the 2015 AAAS Annual Meeting in February 2015. On the last day of the 2016 AAAS Annual Meeting, February 15, 2016, Richmond will become the Chair, Schaal will become the President, and a new President-Elect will take office.\n\nPast presidents of AAAS have included some of the most important scientific figures of their time. Among them: explorer and geologist John Wesley Powell (1888); astronomer and physicist Edward Charles Pickering (1912); anthropologist Margaret Mead (1975); and biologist Stephen Jay Gould (2000).\n\nNotable Presidents of the AAAS, 1848–2005\n\nThere are three classifications of high-level administrative officials that execute the basic, daily functions of the AAAS. These are the Executive Officer, the Treasurer and then each of the AAAS's section secretaries. The current CEO of AAAS and executive publisher of \"Science\" magazine is Rush D. Holt.\n\nThe AAAS has 24 \"sections\" with each section being responsible for a particular concern of the AAAS. There are sections for agriculture, anthropology, astronomy, atmospheric science, biological science, chemistry, dentistry, education, engineering, general interest in science and engineering, geology and geography, the history and philosophy of science, technology, computer science, linguistics, mathematics, medical science, neuroscience, pharmaceutical science, physics, psychology, science and human rights, social and political science, the social impact of science and engineering, and statistics.\n\nAAAS affiliates include 262 societies and academies of science, serving more than 10 million members, from the Acoustical Society of America to the Wildlife Society, as well as non-mainstream groups like the Parapsychological Association.\n\nThe Council is composed of the members of the Board of Directors, the retiring section chairmen, elected delegates and affiliated foreign council members. Among the elected delegates there are always at least two members from the National Academy of Sciences and one from each region of the country. The President of the AAAS serves as the Chairperson of the Council. Members serve the Council for a term of three years.\n\nThe council meets annually to discuss matters of importance to the AAAS. They have the power to review all activities of the Association, elect new fellows, adopt resolutions, propose amendments to the Association's constitution and bylaws, create new scientific sections, and organize and aid local chapters of the AAAS. The Council recently has new additions to it from different sections which include many youngsters as well. John Kerry of Chicago is the youngest American in the council and Akhil Ennamsetty of India is the youngest foreign council member.\n\nThe board of directors is composed of a chairperson, the president, and the president-elect along with eight elected directors, the executive officer of the association and up to two additional directors appointed by elected officers. Members serve a four-year term except for directors appointed by elected officers, who serve three-year terms.\n\nThe current chairman is Gerald Fink, Margaret and Herman Sokol Professor at Whitehead Institute, MIT. Fink will serve in the post until the end of the 2016 AAAS Annual Meeting, 15 February 2016. (The chairperson is always the immediate past-president of AAAS.)\n\nThe board of directors has a variety of powers and responsibilities. It is charged with the administration of all association funds, publication of a budget, appointment of administrators, proposition of amendments, and determining the time and place of meetings of the national association. The board may also speak publicly on behalf of the association. The board must also regularly correspond with the council to discuss their actions.\n\nThe AAAS council elects every year, its members who are distinguished scientifically, to the grade of fellow (FAAAS). Election to AAAS is an honor bestowed by their peers and elected fellows are presented with a certificate and rosette pin.\n\nFormal meetings of the AAAS are numbered consecutively, starting with the first meeting in 1848. Meetings were not held 1861–1865 during the American Civil War, and also 1942–1943 during World War II. Since 1946, one meeting has occurred annually, now customarily in February.\n\nEach year, the AAAS gives out a number of honorary awards, most of which focus on science communication, journalism, and outreach — sometimes in partnership with other organizations. The awards recognize \"scientists, journalists, and public servants for significant contributions to science and to the public’s understanding of science.” The awards are presented each year at the association’s annual meeting.\n\n\nThe society's flagship publication is \"Science\", a weekly interdisciplinary scientific journal. Other peer-reviewed journals published by the AAAS are \"Science Signaling\", \"Science Translational Medicine\", \"Science Immunology\", \"Science Robotics\" and the interdisciplinary \"Science Advances\". They also publish the non-peer-reviewed \"Science & Diplomacy\".\n\nIn 1996, AAAS launched EurekAlert! website, an editorially independent, non-profit news release distribution service covering all areas of science, medicine and technology. Eurekalert! provides news in English, Spanish, French, German, Portuguese, Japanese. In 2007, EurekAlert! Chinese was launched.\n\nWorking staff journalists and freelancers who meet eligibility guidelines can access the latest studies before publication and obtain embargoed information in compliance with the U.S. Securities and Exchange Commission's Regulation Fair Disclosure policy. By early 2018, more than 14,000 reporters from more than 90 countries have registered for free access to embargoed materials. More than 5,000 active public information officers from 2,300 universities, academic journals, government agencies, and medical centers are credentialed to provide new releases to reporters and the public through the system.\n\nIn 1998, European science organizations countered Eurekalert! with a press release distribution service AlphaGalileo.\n\nEurekalert! has fallen under criticism for lack of press release standards and as generating churnalism.\n\n\n"}
{"id": "1657023", "url": "https://en.wikipedia.org/wiki?curid=1657023", "title": "Basic needs", "text": "Basic needs\n\nThe basic needs approach is one of the major approaches to the measurement of absolute poverty in developing countries. It attempts to define the absolute minimum resources necessary for long-term physical well-being, usually in terms of consumption goods. The poverty line is then defined as the amount of income required to satisfy those needs. The 'basic needs' approach was introduced by the International Labour Organization's World Employment Conference in 1976. \"Perhaps the high point of the WEP was the World Employment Conference of 1976, which proposed the satisfaction of basic human needs as the overriding objective of national and international development policy. The basic needs approach to development was endorsed by governments and workers’ and employers’ organizations from all over the world. It influenced the programmes and policies of major multilateral and bilateral development agencies, and was the precursor to the human development approach.\"\n\nA traditional list of immediate \"basic needs\" is food (including water), shelter and clothing. Many modern lists emphasize the minimum level of consumption of 'basic needs' of not just food, water, clothing and shelter, but also sanitation, education, healthcare, and internet. Different agencies use different lists.\n\nThe basic needs approach has been described as consumption-oriented, giving the impression \"that poverty elimination is all too easy.\" Amartya Sen focused on 'capabilities' rather than consumption.\n\nIn the development discourse, the basic needs model focuses on the measurement of what is believed to be an eradicable level of poverty. Development programs following the basic needs approach do not invest in economically productive activities that will help a society carry its own weight in the future, rather it focuses on allowing the society to consume just enough to rise above the poverty line and meet its basic needs. These programs focus more on subsistence than fairness. Nevertheless, in terms of \"measurement\", the basic needs or absolute approach is important. The 1995 world summit on social development in Copenhagen had, as one of its principal declarations that all nations of the world should develop measures of both absolute and relative poverty and should gear national policies to \"eradicate absolute poverty by a target date specified by each country in its national context.\"\n\nProfessor Chris Sarlo, an economist at Nipissing University in North Bay, Ontario, Canada and a senior fellow of the Fraser Institute, uses Statistics Canada's socio-economic databases, particularly the \"Survey of Household Spending\" to determine the cost of a list of household necessities. The list includes food, shelter, clothing, health care, personal care, essential furnishings, transportation and communication, laundry, home insurance, and miscellaneous; it assumes that education is provided freely to all residents of Canada. This is calculated for various communities across Canada and adjusted for family size. With this information, he determines the proportion of Canadian households that have insufficient income to afford those necessities. Based on his basic needs poverty threshold, the poverty rate in Canada, the poverty rate has declined from about 12% of Canadian households to about 5% since the 1970s. This is in sharp contrast to the results of Statistic Canada, Conference Board of Canada, the Organisation for Economic Co-operation and Development (OECD) and UNESCO reports using the relative poverty measure considered to the most useful for advanced industrial nations like Canada, which Sarlo rejects.\n\nOECD and UNICEF rate Canada's poverty rate much higher using a relative poverty threshold. Statistics Canada's LICO, which Sarlo also rejects, also result in higher poverty rates. According to a 2008 report by the Organisation for Economic Co-operation and Development (OECD), the rate of poverty in Canada, is among the highest of the OECD member nations, the world's wealthiest industrialized nations. There is no official government definition and therefore, measure, for poverty in Canada. However, Dennis Raphael, author of \"Poverty in Canada: Implications for Health and Quality of Life\" reported that the United Nations Development Program (UNDP), the United Nations Children’s Fund (UNICEF), the Organisation for Economic Co-operation and Development (OECD) and Canadian poverty researchers find that relative poverty is the \"most useful measure for ascertaining poverty rates in wealthy developed nations such as Canada.\" In its report released the Conference Board \n\nThe Municipality of Rosario, Batangas, Philippines implemented its Aksyon ng Bayan Rosario 2001 And Beyond Human and Ecological Security Plan using this concept as a core strategy through the \"Minimum Basic Needs Approach to Improved Quality of Life - Community-Based Information System (MBN-CBIS)\" prescribed by the Philippine Government. This approach helped the municipal government identify priority families and communities for intervention, as well as rationalize the allocation of its social development funds.\n\nIn the United States, the equivalent measures are called \"self-sufficiency standards\" or \"living income standards\". Unlike the federal poverty level (FPL), which is calculated from a single, national variable (cost of food), these models assume that different households have different needs, based on factors such as the number and age of children in the household, and the cost of housing in the particular area (usually a county) that they live in. In keeping with the principles of basic needs, these measurements do not include any extra money for entertainment, savings, debt payment, or unusual or avoidable expenses, such as vehicle repairs. It assumes that adults will be working and pay taxes; it also includes costs of all government, charitable, and family subsidies, such as free medical care through Medicaid, free food from the USDA food stamps program or a food bank, or free childcare from a grandparent. All of these costs are ignored by the official FPL measurement, but included in a self-sufficiency standard.\n\nMinimum expenses vary by region. For housing, child care, food, transportation, health care, and other necessary expenses, plus net taxes, a family in middle-class Warren County in northwestern Pennsylvania of one adult and two children (one preschooler, one school-aged) needed a minimum income of $30,269 to pay its own way in 2006. Child care is the largest expense in this budget, followed by housing, taxes, and food. The same family, living in the wealthy Seattle region of Washington would need to earn $48,269 to be self-sufficient while remaining in that location. These figures contrast sharply with the FPL for that year, which was just $16,600 for any three-person household.\n\n\nBasic Needs in Development Planning, Michael Hopkins and Rolph Van Der Hoeven (Gower, Aldershot, UK, 1983)\n\n"}
{"id": "54948386", "url": "https://en.wikipedia.org/wiki?curid=54948386", "title": "British degree nicknames", "text": "British degree nicknames\n\nBased on colloquial rhyming slang, British undergraduate honours degree classifications have gained a number of common nicknames, based on the names of various well known current and historical figures. The following is a summary of these.\n"}
{"id": "56828702", "url": "https://en.wikipedia.org/wiki?curid=56828702", "title": "Caenorhabditis elegans Cer13 virus", "text": "Caenorhabditis elegans Cer13 virus\n\nCaenorhabditis elegans Cer13 virus is a species of virus in the genus \"Semotivirus\" and the family Belpaoviridae. It exists as retrotransposons in the \"Caenorhabditis elegans\" genome.\n\n\n"}
{"id": "226412", "url": "https://en.wikipedia.org/wiki?curid=226412", "title": "Caloric theory", "text": "Caloric theory\n\nThe caloric theory is an obsolete scientific theory that heat consists of a self-repellent fluid called caloric that flows from hotter bodies to colder bodies. Caloric was also thought of as a weightless gas that could pass in and out of pores in solids and liquids. The \"caloric theory\" was superseded by the mid-19th century in favor of the mechanical theory of heat, but nevertheless persisted in some scientific literature—particularly in more popular treatments—until the end of the 19th century.\n\nIn the history of thermodynamics, the initial explanations of heat were thoroughly confused with explanations of combustion. After J. J. Becher and Georg Ernst Stahl introduced the phlogiston theory of combustion in the 17th century, phlogiston was thought to be the \"substance of heat.\"\n\nThere is one version of the caloric theory that was introduced by Antoine Lavoisier. Lavoisier developed the explanation of combustion in terms of oxygen in the 1770s. In his paper \"Réflexions sur le phlogistique\" (1783), Lavoisier argued that phlogiston theory was inconsistent with his experimental results, and proposed a 'subtle fluid' called caloric as the \"substance of heat\". According to this theory, the quantity of this substance is constant throughout the universe, and it flows from warmer to colder bodies. Indeed, Lavoisier was one of the first to use a calorimeter to measure the heat changes during chemical reaction.\n\nIn the 1780s, some believed that cold was a fluid, \"frigoric\". Pierre Prévost argued that cold was simply a lack of caloric.\n\nSince heat was a material substance in caloric theory, and therefore could neither be created nor destroyed, conservation of heat was a central assumption.\n\nThe introduction of the caloric theory was also influenced by the experiments of Joseph Black related to the thermal properties of materials. Besides the caloric theory, another theory existed in the late eighteenth century that could explain the phenomenon of heat: the kinetic theory. The two theories were considered to be equivalent at the time, but kinetic theory was the more modern one, as it used a few ideas from atomic theory and could explain both combustion and calorimetry.\n\nQuite a number of successful explanations can be, and were, made from these hypotheses alone. We can explain the cooling of a cup of tea in room temperature: caloric is self-repelling, and thus slowly flows from regions dense in caloric (the hot water) to regions less dense in caloric (the cooler air in the room).\n\nWe can explain the expansion of air under heat: caloric is absorbed into the air, which increases its volume. If we say a little more about what happens to caloric during this absorption phenomenon, we can explain the radiation of heat, the state changes of matter under various temperatures, and deduce nearly all of the gas laws.\n\nSadi Carnot developed his principle of the Carnot cycle, which still forms the basis of heat engine theory, solely from the caloric viewpoint.\n\nHowever, one of the greatest apparent confirmations of the caloric theory was Pierre-Simon Laplace's theoretical correction of Sir Isaac Newton’s calculation of the speed of sound. Newton had assumed an isothermal process, while Laplace, a calorist, treated it as adiabatic. This addition not only substantially corrected the theoretical prediction of the speed of sound, but also continued to make even more accurate predictions for almost a century afterward, even as measurements became more precise.\n\nIn 1798, Count Rumford published \"An Experimental Enquiry Concerning the Source of the Heat which is Excited by Friction\", a report on his investigation of the heat produced while manufacturing cannons. He had found that boring a cannon repeatedly does not result in a loss of its ability to produce heat, and therefore no loss of caloric. This suggested that caloric could not be a conserved \"substance\" though the experimental uncertainties in his experiment were widely debated.\n\nHis results were not seen as a \"threat\" to caloric theory at the time, as this theory was considered to be equivalent to the alternative kinetic theory. In fact, to some of his contemporaries, the results added to the understanding of caloric theory.\n\nRumford's experiment inspired the work of James Prescott Joule and others towards the middle of the 19th century. In 1850, Rudolf Clausius published a paper showing that the two theories were indeed compatible, as long as the calorists' principle of the conservation of heat was replaced by a principle of conservation of energy. In this way, the caloric theory was absorbed into the annals of physics, and evolved into modern thermodynamics, in which heat is the kinetic energy of some particles (atoms, molecules) of the substance. \n\nIn later combination with the law of energy conservation, the caloric theory still shows a very valuable physical insight into some aspects of heat, for example, the emergence of Laplace's equation and Poisson's equation in the problems of spatial distribution of heat and temperature. The caloric theory is now also remembered for the naming of the calorie.\n\n"}
{"id": "5233", "url": "https://en.wikipedia.org/wiki?curid=5233", "title": "Carl Linnaeus", "text": "Carl Linnaeus\n\nCarl Linnaeus (; 23 May 1707 – 10 January 1778), also known after his ennoblement as Carl von Linné (), was a Swedish botanist, physician, and zoologist who formalised binomial nomenclature, the modern system of naming organisms. He is known as the \"father of modern taxonomy\". Many of his writings were in Latin, and his name is rendered in Latin as (after 1761 Carolus a Linné).\n\nLinnaeus was born in the countryside of Småland in southern Sweden. He received most of his higher education at Uppsala University and began giving lectures in botany there in 1730. He lived abroad between 1735 and 1738, where he studied and also published the first edition of his \"\" in the Netherlands. He then returned to Sweden where he became professor of medicine and botany at Uppsala. In the 1740s, he was sent on several journeys through Sweden to find and classify plants and animals. In the 1750s and 1760s, he continued to collect and classify animals, plants, and minerals, while publishing several volumes. He was one of the most acclaimed scientists in Europe at the time of his death.\n\nPhilosopher Jean-Jacques Rousseau sent him the message: \"Tell him I know no greater man on earth.\" Johann Wolfgang von Goethe wrote: \"With the exception of Shakespeare and Spinoza, I know no one among the no longer living who has influenced me more strongly.\" Swedish author August Strindberg wrote: \"Linnaeus was in reality a poet who happened to become a naturalist\". Linnaeus has been called \"\" (Prince of Botanists) and \"The Pliny of the North\". He is also considered as one of the founders of modern ecology.\n\nIn botany, the abbreviation L. is used to indicate Linnaeus as the authority for a species' name. In older publications, the abbreviation \"Linn.\" is found. Linnaeus' remains comprise the type specimen for the species \"Homo sapiens\" following the International Code of Zoological Nomenclature, since the sole specimen that he is known to have examined was himself.\n\nLinnæus was born in the village of Råshult in Småland, Sweden, on 23 May 1707. He was the first child of Nicolaus (Nils) Ingemarsson (who later adopted the family name Linnæus) and Christina Brodersonia. His siblings were Anna Maria Linnæa, Sofia Juliana Linnæa, Samuel Linnæus (who would eventually succeed their father as rector of Stenbrohult and write a manual on beekeeping), and Emerentia Linnæa. His family spoke so much Latin at home, that Linnæus learned Latin before he learned Swedish.\n\nOne of a long line of peasants and priests, Nils was an amateur botanist, a Lutheran minister, and the curate of the small village of Stenbrohult in Småland. Christina was the daughter of the rector of Stenbrohult, Samuel Brodersonius.\n\nA year after Linnæus' birth, his grandfather Samuel Brodersonius died, and his father Nils became the rector of Stenbrohult. The family moved into the rectory from the curate's house.\n\nEven in his early years, Linnæus seemed to have a liking for plants, flowers in particular. Whenever he was upset, he was given a flower, which immediately calmed him. Nils spent much time in his garden and often showed flowers to Linnaeus and told him their names. Soon Linnæus was given his own patch of earth where he could grow plants.\n\nCarl's father was the first in his ancestry to adopt a permanent surname. Before that, ancestors had used the patronymic naming system of Scandinavian countries: his father was named Ingemarsson after his father Ingemar Bengtsson. When Nils was admitted to the University of Lund, he had to take on a family name. He adopted the Latinate name Linnæus after a giant linden tree (or lime tree), \"\" in Swedish, that grew on the family homestead. This name was spelled with the æ ligature. When Carl was born, he was named Carl Linnæus, with his father's family name. The son also always spelled it with the æ ligature, both in handwritten documents and in publications. Carl's patronymic would have been Nilsson, as in Carl Nilsson Linnæus.\n\nLinnaeus' father began teaching him basic Latin, religion, and geography at an early age. When Linnaeus was seven, Nils decided to hire a tutor for him. The parents picked Johan Telander, a son of a local yeoman. Linnaeus did not like him, writing in his autobiography that Telander \"was better calculated to extinguish a child's talents than develop them.\"\n\nTwo years after his tutoring had begun, he was sent to the Lower Grammar School at Växjö in 1717. Linnaeus rarely studied, often going to the countryside to look for plants. He reached the last year of the Lower School when he was fifteen, which was taught by the headmaster, Daniel Lannerus, who was interested in botany. Lannerus noticed Linnaeus' interest in botany and gave him the run of his garden.\n\nHe also introduced him to Johan Rothman, the state doctor of Småland and a teacher at Katedralskolan (a gymnasium) in Växjö. Also a botanist, Rothman broadened Linnaeus' interest in botany and helped him develop an interest in medicine. By the age of 17, Linnaeus had become well acquainted with the existing botanical literature. He remarks in his journal that he \"read day and night, knowing like the back of my hand, Arvidh Månsson's Rydaholm Book of Herbs, Tillandz's Flora Åboensis, Palmberg's Serta Florea Suecana, Bromelii Chloros Gothica and Rudbeckii Hortus Upsaliensis...\"\n\nLinnaeus entered the Växjö Katedralskola in 1724, where he studied mainly Greek, Hebrew, theology and mathematics, a curriculum designed for boys preparing for the priesthood. In the last year at the gymnasium, Linnaeus' father visited to ask the professors how his son's studies were progressing; to his dismay, most said that the boy would never become a scholar. Rothman believed otherwise, suggesting Linnaeus could have a future in medicine. The doctor offered to have Linnaeus live with his family in Växjö and to teach him physiology and botany. Nils accepted this offer.\n\nRothman showed Linnaeus that botany was a serious subject. He taught Linnaeus to classify plants according to Tournefort's system. Linnaeus was also taught about the sexual reproduction of plants, according to Sébastien Vaillant. In 1727, Linnaeus, age 21, enrolled in Lund University in Skåne. He was registered as \"\", the Latin form of his full name, which he also used later for his Latin publications.\n\nProfessor Kilian Stobæus, natural scientist, physician and historian, offered Linnaeus tutoring and lodging, as well as the use of his library, which included many books about botany. He also gave the student free admission to his lectures. In his spare time, Linnaeus explored the flora of Skåne, together with students sharing the same interests.\n\nIn August 1728, Linnaeus decided to attend Uppsala University on the advice of Rothman, who believed it would be a better choice if Linnaeus wanted to study both medicine and botany. Rothman based this recommendation on the two professors who taught at the medical faculty at Uppsala: Olof Rudbeck the Younger and Lars Roberg. Although Rudbeck and Roberg had undoubtedly been good professors, by then they were older and not so interested in teaching. Rudbeck no longer gave public lectures, and had others stand in for him. The botany, zoology, pharmacology and anatomy lectures were not in their best state. In Uppsala, Linnaeus met a new benefactor, Olof Celsius, who was a professor of theology and an amateur botanist. He received Linnaeus into his home and allowed him use of his library, which was one of the richest botanical libraries in Sweden.\n\nIn 1729, Linnaeus wrote a thesis, ' on plant sexual reproduction. This attracted the attention of Rudbeck; in May 1730, he selected Linnaeus to give lectures at the University although the young man was only a second-year student. His lectures were popular, and Linnaeus often addressed an audience of 300 people. In June, Linnaeus moved from Celsius' house to Rudbeck's to become the tutor of the three youngest of his 24 children. His friendship with Celsius did not wane and they continued their botanical expeditions. Over that winter, Linnaeus began to doubt Tournefort's system of classification and decided to create one of his own. His plan was to divide the plants by the number of stamens and pistils. He began writing several books, which would later result in, for example, ' and '. He also produced a book on the plants grown in the Uppsala Botanical Garden, '.\n\nRudbeck's former assistant, Nils Rosén, returned to the University in March 1731 with a degree in medicine. Rosén started giving anatomy lectures and tried to take over Linnaeus' botany lectures, but Rudbeck prevented that. Until December, Rosén gave Linnaeus private tutoring in medicine. In December, Linnaeus had a \"disagreement\" with Rudbeck's wife and had to move out of his mentor's house; his relationship with Rudbeck did not appear to suffer. That Christmas, Linnaeus returned home to Stenbrohult to visit his parents for the first time in about three years. His mother had disapproved of his failing to become a priest, but she was pleased to learn he was teaching at the University.\n\nDuring a visit with his parents, Linnaeus told them about his plan to travel to Lapland; Rudbeck had made the journey in 1695, but the detailed results of his exploration were lost in a fire seven years afterwards. Linnaeus' hope was to find new plants, animals and possibly valuable minerals. He was also curious about the customs of the native Sami people, reindeer-herding nomads who wandered Scandinavia's vast tundras. In April 1732, Linnaeus was awarded a grant from the Royal Society of Sciences in Uppsala for his journey.\n\nLinnaeus began his expedition from Uppsala on 12 May 1732, just before he turned 25. He travelled on foot and horse, bringing with him his journal, botanical and ornithological manuscripts and sheets of paper for pressing plants. Near Gävle he found great quantities of \"Campanula serpyllifolia\", later known as \"Linnaea borealis\", the twinflower that would become his favourite. He sometimes dismounted on the way to examine a flower or rock and was particularly interested in mosses and lichens, the latter a main part of the diet of the reindeer, a common and economically important animal in Lapland.\n\nLinnaeus travelled clockwise around the coast of the Gulf of Bothnia, making major inland incursions from Umeå, Luleå and Tornio. He returned from his six-month-long, over expedition in October, having gathered and observed many plants, birds and rocks. Although Lapland was a region with limited biodiversity, Linnaeus described about 100 previously unidentified plants. These became the basis of his book \"\". However, on the expedition to Lapland, Linnaeus used Latin names to describe organisms because he had not yet developed the binomial system.\n\nIn ' Linnaeus' ideas about nomenclature and classification were first used in a practical way, making this the first proto-modern Flora. The account covered 534 species, used the Linnaean classification system and included, for the described species, geographical distribution and taxonomic notes. It was Augustin Pyramus de Candolle who attributed Linnaeus with ' as the first example in the botanical genre of Flora writing. Botanical historian E. L. Greene described \"\" as \"the most classic and delightful\" of Linnaeus's works.\n\nIt was also during this expedition that Linnaeus had a flash of insight regarding the classification of mammals. Upon observing the lower jawbone of a horse at the side of a road he was travelling, Linnaeus remarked: \"If I only knew how many teeth and of what kind every animal had, how many teats and where they were placed, I should perhaps be able to work out a perfectly natural system for the arrangement of all quadrupeds.\"\n\nIn 1734, Linnaeus led a small group of students to Dalarna. Funded by the Governor of Dalarna, the expedition was to catalogue known natural resources and discover new ones, but also to gather intelligence on Norwegian mining activities at Røros.\n\nHis relations with Nils Rosén having worsened, Linnaeus accepted an invitation from Claes Sohlberg, son of a mining inspector, to spend the Christmas holiday in Falun, where Linnaeus was permitted to visit the mines.\n\nIn April 1735, at the suggestion of Sohlberg's father, Linnaeus and Sohlberg set out for the Dutch Republic, where Linnaeus intended to study medicine at the University of Harderwijk while tutoring Sohlberg in exchange for an annual salary. At the time, it was common for Swedes to pursue doctoral degrees in the Netherlands, then a highly revered place to study natural history.\n\nOn the way, the pair stopped in Hamburg, where they met the mayor, who proudly showed them a supposed wonder of nature in his possession: the taxidermied remains of a seven-headed hydra. Linnaeus quickly discovered the specimen was a fake cobbled together from the jaws and paws of weasels and the skins of snakes. The provenance of the hydra suggested to Linnaeus that it had been manufactured by monks to represent the Beast of Revelation. Even at the risk of incurring the mayor's wrath, Linnaeus made his observations public, dashing the mayor's dreams of selling the hydra for an enormous sum. Linnaeus and Sohlberg were forced to flee from Hamburg.\n\nLinnaeus began working towards his degree as soon as he reached Harderwijk, a university known for awarding degrees in as little as a week. He submitted a dissertation, written back in Sweden, entitled \"Dissertatio medica inauguralis in qua exhibetur hypothesis nova de febrium intermittentium causa\", in which he laid out his hypothesis that malaria arose only in areas with clay-rich soils. Although he failed to identify the true source of disease transmission, (\"i.e.\", the \"Anopheles\" mosquito), he did correctly predict that \"Artemisia annua\" (wormwood) would become a source of antimalarial medications.\n\nWithin two weeks he had completed his oral and practical examinations and was awarded a doctoral degree.\n\nThat summer Linnaeus reunited with Peter Artedi, a friend from Uppsala with whom he had once made a pact that should either of the two predecease the other, the survivor would finish the decedent's work. Ten weeks later, Artedi drowned in the canals of Amsterdam, leaving behind an unfinished manuscript on the classification of fish.\n\nOne of the first scientists Linnaeus met in the Netherlands was Johan Frederik Gronovius to whom Linnaeus showed one of the several manuscripts he had brought with him from Sweden. The manuscript described a new system for classifying plants. When Gronovius saw it, he was very impressed, and offered to help pay for the printing. With an additional monetary contribution by the Scottish doctor Isaac Lawson, the manuscript was published as \" (1735).\n\nLinnaeus became acquainted with one of the most respected physicians and botanists in the Netherlands, Herman Boerhaave, who tried to convince Linnaeus to make a career there. Boerhaave offered him a journey to South Africa and America, but Linnaeus declined, stating he would not stand the heat. Instead, Boerhaave convinced Linnaeus that he should visit the botanist Johannes Burman. After his visit, Burman, impressed with his guest's knowledge, decided Linnaeus should stay with him during the winter. During his stay, Linnaeus helped Burman with his '. Burman also helped Linnaeus with the books on which he was working: ' and \"\".\n\nIn August 1735, during Linnaeus' stay with Burman, he met George Clifford III, a director of the Dutch East India Company and the owner of a rich botanical garden at the estate of Hartekamp in Heemstede. Clifford was very impressed with Linnaeus' ability to classify plants, and invited him to become his physician and superintendent of his garden. Linnaeus had already agreed to stay with Burman over the winter, and could thus not accept immediately. However, Clifford offered to compensate Burman by offering him a copy of Sir Hans Sloane's \"Natural History of Jamaica\", a rare book, if he let Linnaeus stay with him, and Burman accepted. On 24 September 1735, Linnaeus moved to Hartekamp to become personal physician to Clifford, and curator of Clifford's herbarium. He was paid 1,000 florins a year, with free board and lodging. Though the agreement was only for a winter of that year, Linnaeus practically stayed there until 1738. It was here that he wrote a book \"Hortus Cliffortianus\", in the preface of which he described his experience as \"the happiest time of my life.\" (A portion of Hartekamp was declared as public garden in April 1956 by the Heemstede local authority, and was named \"Linnaeushof\". It eventually became, as it is claimed, the biggest playground in Europe.)\n\nIn July 1736, Linnaeus travelled to England, at Clifford's expense. He went to London to visit Sir Hans Sloane, a collector of natural history, and to see his cabinet, as well as to visit the Chelsea Physic Garden and its keeper, Philip Miller. He taught Miller about his new system of subdividing plants, as described in \"\". Miller was in fact reluctant to use the new binomial nomenclature, preferring the classifications of Joseph Pitton de Tournefort and John Ray at first. Linnaeus, nevertheless, applauded Miller's \"Gardeners Dictionary\", The conservative Scot actually retained in his dictionary a number of pre-Linnaean binomial signifiers discarded by Linnaeus but which have been retained by modern botanists. He only fully changed to the Linnaean system in the edition of \"The Gardeners Dictionary\" of 1768. Miller ultimately was impressed, and from then on started to arrange the garden according to Linnaeus' system.\n\nLinnaeus also travelled to Oxford University to visit the botanist Johann Jacob Dillenius. He failed to make Dillenius publicly fully accept his new classification system, though the two men remained in correspondence for many years afterwards. Linnaeus dedicated his \"Critica botanica\" to him, as \"opus botanicum quo absolutius mundus non vidit\". Linnaeus would later name a genus of tropical tree Dillenia in his honour. He then returned to Hartekamp, bringing with him many specimens of rare plants. The next year, he published ', in which he described 935 genera of plants, and shortly thereafter he supplemented it with ', with another sixty (\"sexaginta\") genera.\n\nHis work at Hartekamp led to another book, \"\", a catalogue of the botanical holdings in the herbarium and botanical garden of Hartekamp. He wrote it in nine months (completed in July 1737), but it was not published until 1738. It contains the first use of the name \"Nepenthes\", which Linnaeus used to describe a genus of pitcher plants.\n\nLinnaeus stayed with Clifford at Hartekamp until 18 October 1737 (new style), when he left the house to return to Sweden. Illness and the kindness of Dutch friends obliged him to stay some months longer in Holland. In May 1738, he set out for Sweden again. On the way home, he stayed in Paris for about a month, visiting botanists such as Antoine de Jussieu. After his return, Linnaeus never left Sweden again.\n\nWhen Linnaeus returned to Sweden on 28 June 1738, he went to Falun, where he entered into an engagement to Sara Elisabeth Moræa. Three months later, he moved to Stockholm to find employment as a physician, and thus to make it possible to support a family. Once again, Linnaeus found a patron; he became acquainted with Count Carl Gustav Tessin, who helped him get work as a physician at the Admiralty. During this time in Stockholm, Linnaeus helped found the Royal Swedish Academy of Science; he became the first Praeses in the academy by drawing of lots.\n\nBecause his finances had improved and were now sufficient to support a family, he received permission to marry his fiancée, Sara Elisabeth Moræa. Their wedding was held 26 June 1739. Seven months later, Sara gave birth to their first son, Carl. Two years later, a daughter, Elisabeth Christina, was born, and the subsequent year Sara gave birth to Sara Magdalena, who died when 15 days old. Sara and Linnaeus would later have four other children: Lovisa, Sara Christina, Johannes and Sophia.\nIn May 1741, Linnaeus was appointed Professor of Medicine at Uppsala University, first with responsibility for medicine-related matters. Soon, he changed place with the other Professor of Medicine, Nils Rosén, and thus was responsible for the Botanical Garden (which he would thoroughly reconstruct and expand), botany and natural history, instead. In October that same year, his wife and nine-year-old son followed him to live in Uppsala.\n\nTen days after he was appointed Professor, he undertook an expedition to the island provinces of Öland and Gotland with six students from the university, to look for plants useful in medicine. First, they travelled to Öland and stayed there until 21 June, when they sailed to Visby in Gotland. Linnaeus and the students stayed on Gotland for about a month, and then returned to Uppsala. During this expedition, they found 100 previously unrecorded plants. The observations from the expedition were later published in ', written in Swedish. Like ', it contained both zoological and botanical observations, as well as observations concerning the culture in Öland and Gotland.\n\nDuring the summer of 1745, Linnaeus published two more books: ' and '. ' was a strictly botanical book, while ' was zoological. Anders Celsius had created the temperature scale named after him in 1742. Celsius' scale was inverted compared to today, the boiling point at 0 °C and freezing point at 100 °C. In 1745, Linnaeus inverted the scale to its present standard.\n\nIn the summer of 1746, Linnaeus was once again commissioned by the Government to carry out an expedition, this time to the Swedish province of Västergötland. He set out from Uppsala on 12 June and returned on 11 August. On the expedition his primary companion was Erik Gustaf Lidbeck, a student who had accompanied him on his previous journey. Linnaeus described his findings from the expedition in the book \"\", published the next year. After returning from the journey the Government decided Linnaeus should take on another expedition to the southernmost province Scania. This journey was postponed, as Linnaeus felt too busy.\n\nIn 1747, Linnaeus was given the title archiater, or chief physician, by the Swedish king Adolf Frederick—a mark of great respect. The same year he was elected member of the Academy of Sciences in Berlin.\n\nIn the spring of 1749, Linnaeus could finally journey to Scania, again commissioned by the Government. With him he brought his student, Olof Söderberg. On the way to Scania, he made his last visit to his brothers and sisters in Stenbrohult since his father had died the previous year. The expedition was similar to the previous journeys in most aspects, but this time he was also ordered to find the best place to grow walnut and Swedish whitebeam trees; these trees were used by the military to make rifles. The journey was successful, and Linnaeus' observations were published the next year in \"\".\n\nIn 1750, Linnaeus became rector of Uppsala University, starting a period where natural sciences were esteemed. Perhaps the most important contribution he made during his time at Uppsala was to teach; many of his students travelled to various places in the world to collect botanical samples. Linnaeus called the best of these students his \"apostles\". His lectures were normally very popular and were often held in the Botanical Garden. He tried to teach the students to think for themselves and not trust anybody, not even him. Even more popular than the lectures were the botanical excursions made every Saturday during summer, where Linnaeus and his students explored the flora and fauna in the vicinity of Uppsala.\n\nLinnaeus published \"Philosophia Botanica\" in 1751. The book contained a complete survey of the taxonomy system he had been using in his earlier works. It also contained information of how to keep a journal on travels and how to maintain a botanical garden.\n\nDuring Linnaeus' time it was normal for upper class women to have wet nurses for their babies. Linnaeus joined an ongoing campaign to end this practice in Sweden and promote breast-feeding by mothers. In 1752 Linnaeus published a thesis along with Frederick Lindberg, a physician student, based on their experiences. In the tradition of the period, this dissertation was essentially an idea of the presiding reviewer (\"prases\") expounded upon by the student. Linnaeus' dissertation was translated into French by J.E. Gilibert in 1770 as \"La Nourrice marâtre, ou Dissertation sur les suites funestes du nourrisage mercénaire\". Linnaeus suggested that children might absorb the personality of their wet nurse through the milk. He admired the child care practices of the Lapps and pointed out how healthy their babies were compared to those of Europeans who employed wet nurses. He compared the behaviour of wild animals and pointed out how none of them denied their newborns their breastmilk. It is thought that his activism played a role in his choice of the term \"Mammalia\" for the class of organisms.\n\nLinnaeus published \"Species Plantarum\", the work which is now internationally accepted as the starting point of modern botanical nomenclature, in 1753. The first volume was issued on 24 May, the second volume followed on 16 August of the same year. The book contained 1,200 pages and was published in two volumes; it described over 7,300 species. The same year the king dubbed him knight of the Order of the Polar Star, the first civilian in Sweden to become a knight in this order. He was then seldom seen not wearing the order's insignia.\n\nLinnaeus felt Uppsala was too noisy and unhealthy, so he bought two farms in 1758: Hammarby and Sävja. The next year, he bought a neighbouring farm, Edeby. He spent the summers with his family at Hammarby; initially it only had a small one-storey house, but in 1762 a new, larger main building was added. In Hammarby, Linnaeus made a garden where he could grow plants that could not be grown in the Botanical Garden in Uppsala. He began constructing a museum on a hill behind Hammarby in 1766, where he moved his library and collection of plants. A fire that destroyed about one third of Uppsala and had threatened his residence there necessitated the move.\n\nSince the initial release of ' in 1735, the book had been expanded and reprinted several times; the tenth edition was released in 1758. This edition established itself as the starting point for zoological nomenclature, the equivalent of '.\n\nThe Swedish King Adolf Frederick granted Linnaeus nobility in 1757, but he was not ennobled until 1761. With his ennoblement, he took the name Carl von Linné (Latinised as \"\"), 'Linné' being a shortened and gallicised version of 'Linnæus', and the German nobiliary particle 'von' signifying his ennoblement. The noble family's coat of arms prominently features a twinflower, one of Linnaeus' favourite plants; it was given the scientific name \"Linnaea borealis\" in his honour by Gronovius. The shield in the coat of arms is divided into thirds: red, black and green for the three kingdoms of nature (animal, mineral and vegetable) in Linnaean classification; in the centre is an egg \"to denote Nature, which is continued and perpetuated \"in ovo\".\" At the bottom is a phrase in Latin, borrowed from the Aeneid, which reads \"Famam extendere factis\": we extend our fame by our deeds. Linnaeus inscribed this personal motto in books that were gifted to him by friends.\n\nAfter his ennoblement, Linnaeus continued teaching and writing. His reputation had spread over the world, and he corresponded with many different people. For example, Catherine II of Russia sent him seeds from her country. He also corresponded with Giovanni Antonio Scopoli, \"the Linnaeus of the Austrian Empire\", who was a doctor and a botanist in Idrija, Duchy of Carniola (nowadays Slovenia). Scopoli communicated all of his research, findings, and descriptions (for example of the olm and the dormouse, two little animals hitherto unknown to Linnaeus). Linnaeus greatly respected Scopoli and showed great interest in his work. He named a solanaceous genus, \"Scopolia\", the source of scopolamine, after him, but because of the great distance between them, they never met.\n\nLinnaeus was relieved of his duties in the Royal Swedish Academy of Science in 1763, but continued his work there as usual for more than ten years after. He stepped down as rector at Uppsala University in December 1772, mostly due to his declining health.\n\nLinnaeus' last years were troubled by illness. He had suffered from a disease called the Uppsala fever in 1764, but survived thanks to the care of Rosén. He developed sciatica in 1773, and the next year, he had a stroke which partially paralysed him. He suffered a second stroke in 1776, losing the use of his right side and leaving him bereft of his memory; while still able to admire his own writings, he could not recognise himself as their author.\n\nIn December 1777, he had another stroke which greatly weakened him, and eventually led to his death on 10 January 1778 in Hammarby. Despite his desire to be buried in Hammarby, he was buried in Uppsala Cathedral on 22 January.\n\nHis library and collections were left to his widow Sara and their children. Joseph Banks, an English botanist, wanted to buy the collection, but his son Carl refused and moved the collection to Uppsala. In 1783 Carl died and Sara inherited the collection, having outlived both her husband and son. She tried to sell it to Banks, but he was no longer interested; instead an acquaintance of his agreed to buy the collection. The acquaintance was a 24-year-old medical student, James Edward Smith, who bought the whole collection: 14,000 plants, 3,198 insects, 1,564 shells, about 3,000 letters and 1,600 books. Smith founded the Linnean Society of London five years later.\n\nThe von Linné name ended with his son Carl, who never married. His other son, Johannes, had died aged 3. There are over two hundred descendants of Linnaeus through two of his daughters.\n\nDuring Linnaeus' time as Professor and Rector of Uppsala University, he taught many devoted students, 17 of whom he called \"apostles\". They were the most promising, most committed students, and all of them made botanical expeditions to various places in the world, often with his help. The amount of this help varied; sometimes he used his influence as Rector to grant his apostles a scholarship or a place on an expedition. To most of the apostles he gave instructions of what to look for on their journeys. Abroad, the apostles collected and organised new plants, animals and minerals according to Linnaeus' system. Most of them also gave some of their collection to Linnaeus when their journey was finished. Thanks to these students, the Linnaean system of taxonomy spread through the world without Linnaeus ever having to travel outside Sweden after his return from Holland. The British botanist William T. Stearn notes without Linnaeus' new system, it would not have been possible for the apostles to collect and organise so many new specimens. Many of the apostles died during their expeditions.\n\nChristopher Tärnström, the first apostle and a 43-year-old pastor with a wife and children, made his journey in 1746. He boarded a Swedish East India Company ship headed for China. Tärnström never reached his destination, dying of a tropical fever on Côn Sơn Island the same year. Tärnström's widow blamed Linnaeus for making her children fatherless, causing Linnaeus to prefer sending out younger, unmarried students after Tärnström. Six other apostles later died on their expeditions, including Pehr Forsskål and Pehr Löfling.\n\nTwo years after Tärnström's expedition, Finnish-born Pehr Kalm set out as the second apostle to North America. There he spent two-and-a-half years studying the flora and fauna of Pennsylvania, New York, New Jersey and Canada. Linnaeus was overjoyed when Kalm returned, bringing back with him many pressed flowers and seeds. At least 90 of the 700 North American species described in \"Species Plantarum\" had been brought back by Kalm.\n\nDaniel Solander was living in Linnaeus' house during his time as a student in Uppsala. Linnaeus was very fond of him, promising Solander his oldest daughter's hand in marriage. On Linnaeus' recommendation, Solander travelled to England in 1760, where he met the English botanist Joseph Banks. With Banks, Solander joined James Cook on his expedition to Oceania on the \"Endeavour\" in 1768–71. Solander was not the only apostle to journey with James Cook; Anders Sparrman followed on the \"Resolution\" in 1772–75 bound for, among other places, Oceania and South America. Sparrman made many other expeditions, one of them to South Africa.\n\nPerhaps the most famous and successful apostle was Carl Peter Thunberg, who embarked on a nine-year expedition in 1770. He stayed in South Africa for three years, then travelled to Japan. All foreigners in Japan were forced to stay on the island of Dejima outside Nagasaki, so it was thus hard for Thunberg to study the flora. He did, however, manage to persuade some of the translators to bring him different plants, and he also found plants in the gardens of Dejima. He returned to Sweden in 1779, one year after Linnaeus' death.\n\nThe first edition of \"\" was printed in the Netherlands in 1735. It was a twelve-page work. By the time it reached its 10th edition in 1758, it classified 4,400 species of animals and 7,700 species of plants. People from all over the world sent their specimens to Linnaeus to be included. By the time he started work on the 12th edition, Linnaeus needed a new invention - the index card - to track classifications.\n\nIn \"Systema Naturae\", the unwieldy names mostly used at the time, such as \"\", were supplemented with concise and now familiar \"binomials\", composed of the generic name, followed by a specific epithet – in the case given, \"Physalis angulata\". These binomials could serve as a label to refer to the species. Higher taxa were constructed and arranged in a simple and orderly manner. Although the system, now known as binomial nomenclature, was partially developed by the Bauhin brothers (see Gaspard Bauhin and Johann Bauhin) almost 200 years earlier, Linnaeus was the first to use it consistently throughout the work, including in monospecific genera, and may be said to have popularised it within the scientific community.\n\nAfter the decline in Linnaeus' health in the early 1770s, publication of editions of \"Systema Naturae\" went in two different directions. Another Swedish scientist, Johan Andreas Murray issued the \"Regnum Vegetabile\" section separately in 1774 as the \"Systema Vegetabilium\", rather confusingly labelled the 13th edition. Meanwhile a 13th edition of the entire \"Systema\" appeared in parts between 1788 and 1793. It was through the \"Systema Vegetabilium\" that Linnaeus' work became widely known in England, following its translation from the Latin by the Lichfield Botanical Society as \"A System of Vegetables\" (1783–1785).\n\n' (or, more fully, ') was first published in 1753, as a two-volume work. Its prime importance is perhaps that it is the primary starting point of plant nomenclature as it exists today.\n\n\" was first published in 1737, delineating plant genera. Around 10 editions were published, not all of them by Linnaeus himself; the most important is the 1754 fifth edition. In it Linnaeus divided the plant Kingdom into 24 classes. One, Cryptogamia, included all the plants with concealed reproductive parts (algae, fungi, mosses and liverworts and ferns).\n\n' (1751) was a summary of Linnaeus' thinking on plant classification and nomenclature, and an elaboration of the work he had previously published in ' (1736) and ' (1737). Other publications forming part of his plan to reform the foundations of botany include his ' and ': all were printed in Holland (as were ' (1737) and \" (1735)), the \"Philosophia\" being simultaneously released in Stockholm.\n\nAt the end of his lifetime the Linnean collection in Uppsala was considered one of the finest collections of natural history objects in Sweden. Next to his own collection he had also built up a museum for the university of Uppsala, which was supplied by material donated by Carl Gyllenborg (in 1744–1745), crown-prince Adolf Fredrik (in 1745), Erik Petreus (in 1746), Claes Grill (in 1746), Magnus Lagerström (in 1748 and 1750) and Jonas Alströmer (in 1749). The relation between the museum and the private collection was not formalised and the steady flow of material from Linnean pupils were incorporated to the private collection rather than to the museum. Linnaeus felt his work was reflecting the harmony of nature and he said in 1754 'the earth is then nothing else but a museum of the all-wise creator's masterpieces, divided into three chambers'. He had turned his own estate into a microcosm of that 'world museum'.\n\nIn April 1766 parts of the town were destroyed by a fire and the Linnean private collection was subsequently moved to a barn outside the town, and shortly afterwards to a single-room stone building close to his country house at Hammarby near Uppsala. This resulted in a physical separation between the two collections, the museum collection remained in the botanical garden of the university. Some material which needed special care (alcohol specimens) or ample storage space was moved from the private collection to the museum.\n\nIn Hammarby the Linnean private collections suffered seriously from damp and the depredations by mice and insects. Carl von Linné's son (Carl Linnaeus) inherited the collections in 1778 and retained them until his own death in 1783. Shortly after Carl von Linné's death his son confirmed that mice had caused \"horrible damage\" to the plants and that also moths and mould had caused considerable damage. He tried to rescue them from the neglect they had suffered during his father's later years, and also added further specimens. This last activity however reduced rather than augmented the scientific value of the original material.\n\nIn 1784 the young medical student James Edward Smith purchased the entire specimen collection, library, manuscripts, and correspondence of Carl Linnaeus from his widow and daughter and transferred the collections to London. Not all material in Linné's private collection was transported to England. Thirty-three fish specimens preserved in alcohol were not sent and were later lost.\n\nIn London Smith tended to neglect the zoological parts of the collection, he added some specimens and also gave some specimens away. Over the following centuries the Linnean collection in London suffered enormously at the hands of scientists who studied the collection, and in the process disturbed the original arrangement and labels, added specimens that did not belong to the original series and withdrew precious original type material.\n\nMuch material which had been intensively studied by Linné in his scientific career belonged to the collection of Queen Lovisa Ulrika (1720–1782) (in the Linnean publications referred to as \"Museum Ludovicae Ulricae\" or \"M. L. U.\"). This collection was donated by his grandson King Gustav IV Adolf (1778–1837) to the museum in Uppsala in 1804. Another important collection in this respect was that of her husband King Adolf Fredrik (1710–1771) (in the Linnean sources known as \"Museum Adolphi Friderici\" or \"Mus. Ad. Fr.\"), the wet parts (alcohol collection) of which were later donated to the Royal Swedish Academy of Sciences, and is today housed in the Swedish Museum of Natural History at Stockholm. The dry material was transferred to Uppsala.\n\nThe establishment of universally accepted conventions for the naming of organisms was Linnaeus' main contribution to taxonomy—his work marks the starting point of consistent use of binomial nomenclature. During the 18th century expansion of natural history knowledge, Linnaeus also developed what became known as the \"Linnaean taxonomy\"; the system of scientific classification now widely used in the biological sciences. A previous zoologist Rumphius (1627–1702) had more or less approximated the Linnaean system and his material contributed to the later development of the binomial scientific classification by Linnaeus.\n\nThe Linnaean system classified nature within a nested hierarchy, starting with three kingdoms. Kingdoms were divided into classes and they, in turn, into orders, and thence into genera (\"singular:\" genus), which were divided into species (\"singular:\" species). Below the rank of species he sometimes recognised taxa of a lower (unnamed) rank; these have since acquired standardised names such as \"variety\" in botany and \"subspecies\" in zoology. Modern taxonomy includes a rank of family between order and genus and a rank of phylum between kingdom and class that were not present in Linnaeus' original system.\n\nLinnaeus' groupings were based upon shared physical characteristics, and not simply upon differences. Of his higher groupings, only those for animals are still in use, and the groupings themselves have been significantly changed since their conception, as have the principles behind them. Nevertheless, Linnaeus is credited with establishing the idea of a hierarchical structure of classification which is based upon observable characteristics and intended to reflect natural relationships. While the underlying details concerning what are considered to be scientifically valid \"observable characteristics\" have changed with expanding knowledge (for example, DNA sequencing, unavailable in Linnaeus' time, has proven to be a tool of considerable utility for classifying living organisms and establishing their evolutionary relationships), the fundamental principle remains sound.\n\nLinnaeus' system of taxonomy was especially noted as the first to include humans (\"Homo\") taxonomically grouped with apes (\"Simia\"), under the header of \"Anthropomorpha\".\nGerman biologist Ernst Haeckel speaking in 1907 noted this as the \"most important sign of Linnaeus' genius\".\nLinnaeus classified humans among the primates (as they were later called) beginning with the first edition of \"\". \nDuring his time at Hartekamp, he had the opportunity to examine several monkeys and noted similarities between them and man. He pointed out both species basically have the same anatomy; except for speech, he found no other differences. Thus he placed man and monkeys under the same category, \"Anthropomorpha\", meaning \"manlike.\" This classification received criticism from other biologists such as Johan Gottschalk Wallerius, Jacob Theodor Klein and Johann Georg Gmelin on the ground that it is illogical to describe a human as 'like a man'. In a letter to Gmelin from 1747, Linnaeus replied:\n\nIt does not please [you] that I've placed Man among the Anthropomorpha, perhaps because of the term 'with human form', but man learns to know himself. Let's not quibble over words. It will be the same to me whatever name we apply. But I seek from you and from the whole world a generic difference between man and simian that [follows] from the principles of Natural History. I absolutely know of none. If only someone might tell me a single one! If I would have called man a simian or vice versa, I would have brought together all the theologians against me. Perhaps I ought to have by virtue of the law of the discipline.\n\nThe theological concerns were twofold: first, putting man at the same level as monkeys or apes would lower the spiritually higher position that man was assumed to have in the great chain of being, and second, because the Bible says man was created in the image of God (theomorphism), if monkeys/apes and humans were not distinctly and separately designed, that would mean monkeys and apes were created in the image of God as well. This was something many could not accept. The conflict between world views that was caused by asserting man was a type of animal would simmer for a century until the much greater, and still ongoing, creation–evolution controversy began in earnest with the publication of \"On the Origin of Species\" by Charles Darwin in 1859.\n\nAfter such criticism, Linnaeus felt he needed to explain himself more clearly. The 10th edition of ' introduced new terms, including \"Mammalia\" and \"Primates\", the latter of which would replace \"Anthropomorpha\" as well as giving humans the full binomial \"Homo sapiens\". The new classification received less criticism, but many natural historians still believed he had demoted humans from their former place of ruling over nature and not being a part of it. Linnaeus believed that man biologically belongs to the animal kingdom and had to be included in it. In his book ', he said, \"One should not vent one's wrath on animals, Theology decree that man has a soul and that the animals are mere 'aoutomata mechanica,' but I believe they would be better advised that animals have a soul and that the difference is of nobility.\"\n\nLinnaeus added a second species to the genus \"Homo\" in \"\" based on a figure and description by Jacobus Bontius from a 1658 publication: \"Homo troglodytes\" (\"caveman\") and published a third in 1771: \"Homo lar\". Swedish historian Gunnar Broberg states that the new human species Linnaeus described were actually simians or native people clad in skins to frighten colonial settlers, whose appearance had been exaggerated in accounts to Linnaeus.\n\nIn early editions of \"\", many well-known legendary creatures were included such as the phoenix, dragon, manticore, and satyrus, which Linnaeus collected into the catch-all category \"Paradoxa\". Broberg thought Linnaeus was trying to offer a natural explanation and demystify the world of superstition. Linnaeus tried to debunk some of these creatures, as he had with the hydra; regarding the purported remains of dragons, Linnaeus wrote that they were either derived from lizards or rays. For \"Homo troglodytes\" he asked the Swedish East India Company to search for one, but they did not find any signs of its existence. \"Homo lar\" has since been reclassified as \"Hylobates lar\", the lar gibbon.\n\nIn the first edition of \"\", Linnaeus subdivided the human species into four varieties based on continent and skin colour: \"Europæus albus\" (white European), \"Americanus rubescens\" (red American), \"Asiaticus fuscus\" (brown Asian) and \"Africanus niger\" (black African). \nIn the tenth edition of Systema Naturae he further detailed phenotypical characteristics for each variety, based on the concept of the four temperaments from classical antiquity, and changed the description of Asians' skin tone to \"luridus\" (yellow). \nAdditionally, Linnaeus created a wastebasket taxon \"monstrosus\" for \"wild and monstrous humans, unknown groups, and more or less abnormal people\".\n\nIn 1959, W. T. Stearn designated Linnaeus to be the lectotype of \"H. sapiens\".\n\nLinnaeus' applied science was inspired not only by the instrumental utilitarianism general to the early Enlightenment, but also by his adherence to the older economic doctrine of Cameralism. Additionally, Linnaeus was a state interventionist. He supported tariffs, levies, export bounties, quotas, embargoes, navigation acts, subsidised investment capital, ceilings on wages, cash grants, state-licensed producer monopolies, and cartels.\n\nAnniversaries of Linnaeus' birth, especially in centennial years, have been marked by major celebrations. Linnaeus has appeared on numerous Swedish postage stamps and banknotes. There are numerous statues of Linnaeus in countries around the world. The Linnean Society of London has awarded the Linnean Medal for excellence in botany or zoology since 1888. Following approval by the Riksdag of Sweden, Växjö University and Kalmar College merged on 1 January 2010 to become Linnaeus University. Other things named after Linnaeus include the twinflower genus \"Linnaea\", the crater Linné on the Earth's moon, a street in Cambridge, Massachusetts, and the cobalt sulfide mineral Linnaeite.\n\nAndrew Dickson White wrote in \"\" (1896):\n\nLinnaeus ... was the most eminent naturalist of his time, a wide observer, a close thinker; but the atmosphere in which he lived and moved and had his being was saturated with biblical theology, and this permeated all his thinking. ... Toward the end of his life he timidly advanced the hypothesis that all the species of one genus constituted at the creation one species; and from the last edition of his \"Systema Naturæ\" he quietly left out the strongly orthodox statement of the fixity of each species, which he had insisted upon in his earlier works. ... warnings came speedily both from the Catholic and Protestant sides.\nThe mathematical PageRank algorithm, applied to 24 multilingual Wikipedia editions in 2014, published in \"PLOS ONE\" in 2015, placed Carl Linnaeus at the top historical figure, above Jesus, Aristotle, Napoleon, and Adolf Hitler (in that order).\n\n\n\n\n\n"}
{"id": "24626874", "url": "https://en.wikipedia.org/wiki?curid=24626874", "title": "Carlyle A. Luer", "text": "Carlyle A. Luer\n\nCarlyle A. Luer (born August 23, 1922) is a botanist specializing in the Orchidaceae. His specialty interest is the Pleurothallidinae (Genus \"Pleurothallis\") and allied species.\n\nBorn to Carl & Vera Luer, he was raised in Alton, Illinois and later attended Washington University, St. Louis School of Medicine, graduating in 1946. From there he went on to be a surgeon in Sarasota, Florida and upon retirement in 1975 took up the study and botanical illustration of Orchids.\n\nHe aided in the foundation of the Marie Selby Botanical Gardens and was the first editor of their research journal \"Selbyana\". He has also been named a senior curator at the Missouri Botanical Garden and has published numerous articles and two books related to Orchid taxonomy.\n"}
{"id": "9756354", "url": "https://en.wikipedia.org/wiki?curid=9756354", "title": "ChIP-on-chip", "text": "ChIP-on-chip\n\nChIP-on-chip (also known as ChIP-chip) is a technology that combines chromatin immunoprecipitation ('ChIP') with DNA microarray (\"chip\"). Like regular ChIP, ChIP-on-chip is used to investigate interactions between proteins and DNA \"in vivo\". Specifically, it allows the identification of the cistrome, the sum of binding sites, for DNA-binding proteins on a genome-wide basis. Whole-genome analysis can be performed to determine the locations of binding sites for almost any protein of interest. As the name of the technique suggests, such proteins are generally those operating in the context of chromatin. The most prominent representatives of this class are transcription factors, replication-related proteins, like Origin Recognition Complex Protein (ORC), histones, their variants, and histone modifications.\n\nThe goal of ChIP-on-chip is to locate protein binding sites that may help identify functional elements in the genome. For example, in the case of a transcription factor as a protein of interest, one can determine its transcription factor binding sites throughout the genome. Other proteins allow the identification of promoter regions, enhancers, repressors and silencing elements, insulators, boundary elements, and sequences that control DNA replication. If histones are subject of interest, it is believed that the distribution of modifications and their localizations may offer new insights into the mechanisms of regulation.\n\nOne of the long-term goals ChIP-on-chip was designed for is to establish a catalogue of (selected) organisms that lists all protein-DNA interactions under various physiological conditions. This knowledge would ultimately help in the understanding of the machinery behind gene regulation, cell proliferation, and disease progression. Hence, ChIP-on-chip offers both potential to complement our knowledge about the orchestration of the genome on the nucleotide level and information on higher levels of information and regulation as it is propagated by research on epigenetics.\n\nThe technical platforms to conduct ChIP-on-chip experiments are DNA microarrays, or \"chips\". They can be classified and distinguished according to various characteristics:\n\n\"Probe type\": DNA arrays can comprise either mechanically spotted cDNAs or PCR-products, mechanically spotted oligonucleotides, or oligonucleotides that are synthesized \"in situ\". The early versions of microarrays were designed to detect RNAs from expressed genomic regions (open reading frames aka ORFs). Although such arrays are perfectly suited to study gene expression profiles, they have limited importance in ChIP experiments since most \"interesting\" proteins with respect to this technique bind in intergenic regions. Nowadays, even custom-made arrays can be designed and fine-tuned to match the requirements of an experiment. Also, any sequence of nucleotides can be synthesized to cover genic as well as intergenic regions.\n\n\"Probe size\": Early version of cDNA arrays had a probe length of about 200bp. Latest array versions use oligos as short as 70- (Microarrays, Inc.) to 25-mers (Affymetrix). (Feb 2007)\n\n\"Probe composition\": There are tiled and non-tiled DNA arrays. Non-tiled arrays use probes selected according to non-spatial criteria, i.e., the DNA sequences used as probes have no fixed distances in the genome. Tiled arrays, however, select a genomic region (or even a whole genome) and divide it into equal chunks. Such a region is called tiled path. The average distance between each pair of neighboring chunks (measured from the center of each chunk) gives the resolution of the tiled path. A path can be overlapping, end-to-end or spaced.\n\n\"Array size\": The first microarrays used for ChIP-on-Chip contained about 13,000 spotted DNA segments representing all ORFs and intergenic regions from the yeast genome. Nowadays, Affymetrix offers whole-genome tiled yeast arrays with a resolution of 5bp (all in all 3.2 million probes). Tiled arrays for the human genome become more and more powerful, too. Just to name example, Affymetrix offers a set of seven arrays with about 90 million probes, spanning the complete non-repetitive part of the human genome with about 35bp spacing. (Feb 2007)\nBesides the actual microarray, other hard- and software equipment is necessary to run ChIP-on-chip experiments. It is generally the case that one company’s microarrays can not be analyzed by another company’s processing hardware. Hence, buying an array requires also buying the associated workflow equipment. The most important elements are, among others, hybridization ovens, chip scanners, and software packages for subsequent numerical analysis of the raw data.\n\nStarting with a biological question, a ChIP-on-chip experiment can be divided into three major steps: The first is to set up and design the experiment by selecting the appropriate array and probe type. Second, the actual experiment is performed in the wet-lab. Last, during the dry-lab portion of the cycle, gathered data are analyzed to either answer the initial question or lead to new questions so that the cycle can start again.\n\nIn the first step, the protein of interest (POI) is cross-linked with the DNA site it binds to in an \"in vitro\" environment. Usually this is done by a gentle formaldehyde fixation that is reversible with heat.\n\nThen, the cells are lysed and the DNA is sheared by sonication or using micrococcal nuclease. This results in double-stranded chunks of DNA fragments, normally 1 kb or less in length. Those that were cross-linked to the POI form a POI-DNA complex.\n\nIn the next step, only these complexes are filtered out of the set of DNA fragments, using an antibody specific to the POI. The antibodies may be attached to a solid surface, may have a magnetic bead, or some other physical property that allows separation of cross-linked complexes and unbound fragments. This procedure is essentially an immunoprecipitation (IP) of the protein. This can be done either by using a tagged protein with an antibody against the tag (ex. FLAG, HA, c-myc) or with an antibody to the native protein.\n\nThe cross-linking of POI-DNA complexes is reversed (usually by heating) and the DNA strands are purified. For the rest of the workflow, the POI is no longer necessary.\n\nAfter an amplification and denaturation step, the single-stranded DNA fragments are labeled with a fluorescent tag such as Cy5 or Alexa 647.\n\nFinally, the fragments are poured over the surface of the DNA microarray, which is spotted with short, single-stranded sequences that cover the genomic portion of interest. Whenever a labeled fragment \"finds\" a complementary fragment on the array, they will hybridize and form again a double-stranded DNA fragment.\n\nAfter a sufficiently large time frame to allow hybridization, the array is illuminated with fluorescent light. Those probes on the array that are hybridized to one of the labeled fragments emit a light signal that is captured by a camera. This image contains all raw data for the remaining part of the workflow.\n\nThis raw data, encoded as false-color image, needs to be converted to numerical values before the actual analysis can be done. The analysis and information extraction of the raw data often remains the most challenging part for ChIP-on-chip experiments. Problems arise throughout this portion of the workflow, ranging from the initial chip read-out, to suitable methods to subtract background noise, and finally to appropriate algorithms that normalize the data and make it available for subsequent statistical analysis, which then hopefully lead to a better understanding of the biological question that the experiment seeks to address. Furthermore, due to the different array platforms and lack of standardization between them, data storage and exchange is a huge problem. Generally speaking, the data analysis can be divided into three major steps:\n\nDuring the first step, the captured fluorescence signals from the array are normalized, using control signals derived from the same or a second chip. Such control signals tell which probes on the array were hybridized correctly and which bound nonspecifically.\n\nIn the second step, numerical and statistical tests are applied to control data and IP fraction data to identify POI-enriched regions along the genome. The following three methods are used widely: Median percentile rank, Single-array error, and Sliding-window. These methods generally differ in how low-intensity signals are handled, how much background noise is accepted, and which trait for the data is emphasized during the computation. In the recent past, the sliding-window approach seems to be favored and is often described as most powerful.\n\nIn the third step, these regions are analyzed further. If, for example, the POI was a transcription factor, such regions would represent its binding sites. Subsequent analysis then may want to infer nucleotide motifs and other patterns to allow functional annotation of the genome.\n\nUsing tiled arrays, ChIP-on-chip allows for high resolution of genome-wide maps. These maps can determine the binding sites of many DNA-binding proteins like transcription factors and also chromatin modifications.\n\nAlthough ChIP-on-chip can be a powerful technique in the area of genomics, it is very expensive. Most published studies using ChIP-on-chip repeat their experiments at least three times to ensure biologically meaningful maps. The cost of the DNA microarrays is often a limiting factor to whether a laboratory should proceed with a ChIP-on-chip experiment. Another limitation is the size of DNA fragments that can be achieved. Most ChIP-on-chip protocols utilize sonication as a method of breaking up DNA into small pieces. However, sonication is limited to a minimal fragment size of 200 bp. For higher resolution maps, this limitation should be overcome to achieve smaller fragments, preferably to single nucleosome resolution. As mentioned previously, the statistical analysis of the huge amount of data generated from arrays is a challenge and normalization procedures should aim to minimize artifacts and determine what is really biologically significant. So far, application to mammalian genomes has been a major limitation, for example, due to the significant percentage of the genome that is occupied by repeats. However, as ChIP-on-chip technology advances, high resolution whole mammalian genome maps should become achievable.\n\nAntibodies used for ChIP-on-chip can be an important limiting factor. ChIP-on-chip requires highly specific antibodies that must recognize its epitope in free solution and also under fixed conditions. If it is demonstrated to successfully immunoprecipitate cross-linked chromatin, it is termed \"ChIP-grade\". Companies that provide ChIP-grade antibodies include Abcam, Cell Signaling Technology, Santa Cruz, and Upstate. To overcome the problem of specificity, the protein of interest can be fused to a tag like FLAG or HA that are recognized by antibodies. An alternative to ChIP-on-chip that does not require antibodies is DamID.\n\nAlso available are antibodies against a specific histone modification like H3 tri methyl K4. As mentioned before, the combination of these antibodies and ChIP-on-chip has become extremely powerful in determining whole genome analysis of histone modification patterns and will contribute tremendously to our understanding of the histone code and epigenetics.\n\nA study demonstrating the non-specific nature of DNA binding proteins has been published in PLoS Biology. This indicates that alternate confirmation of functional relevancy is a necessary step in any ChIP-chip experiment.\n\nA first ChIP-on-chip experiment was performed in 1999 to analyze the distribution of cohesin along budding yeast chromosome III. Although the genome was not completely represented, the protocol in this study remains equivalent as those used in later studies. The ChIP-on-chip technique using all of the ORFs of the genome (that nevertheless remains incomplete, missing intergenic regions) was then applied successfully in three papers published in 2000 and 2001. The authors identified binding sites for individual transcription factors in the budding yeast \"Saccharomyces cerevisiae\". In 2002, Richard Young’s group determined the genome-wide positions of 106 transcription factors using a c-Myc tagging system in yeast. The first demonstration of the mammalian ChIp-on-chip technique reported the isolation of nine chromatin fragments containing weak and strong E2F binding site was done by Peggy Farnham's lab in collaboration with Michael Zhang's lab and published in 2001. This study was followed several months later in a collaboration between the Young lab with the laboratory of Brian Dynlacht which used the ChIP-on-chip technique to show for the first time that E2F targets encode components of the DNA damage checkpoint and repair pathways, as well as factors involved in chromatin assembly/condensation, chromosome segregation, and the mitotic spindle checkpoint Other applications for ChIP-on-chip include DNA replication, recombination, and chromatin structure. Since then, ChIP-on-chip has become a powerful tool in determining genome-wide maps of histone modifications and many more transcription factors. ChIP-on-chip in mammalian systems has been difficult due to the large and repetitive genomes. Thus, many studies in mammalian cells have focused on select promoter regions that are predicted to bind transcription factors and have not analyzed the entire genome. However, whole mammalian genome arrays have recently become commercially available from companies like Nimblegen. In the future, as ChIP-on-chip arrays become more and more advanced, high resolution whole genome maps of DNA-binding proteins and chromatin components for mammals will be analyzed in more detail.\n\nChip-Sequencing is a recently developed technology that still uses chromatin immunoprecipitation to crosslink the proteins of interest to the DNA but then instead of using a micro-array, it uses the more accurate, higher throughput method of sequencing to localize interaction points.\n\nDamID is an alternative method that does not require antibodies.\n\nChIP-exo uses exonuclease treatment to achieve up to single base pair resolution.\n\n\n\n"}
{"id": "37700226", "url": "https://en.wikipedia.org/wiki?curid=37700226", "title": "Charles George Johnson", "text": "Charles George Johnson\n\nCharles George Johnson (188614 October 1950) was a chemist, businessman and political figure in Adelaide, South Australia.\n\nJohnson was born in Essex, England in 1886 and led an adventurous life, including a spell of boxing professionally, before arriving in Adelaide in 1925. He first came to public notice as an Labor Party (ALP) candidate for a North Adelaide seat in South Australia's House of Assembly in 1933.\n\nIn August 1934, Johnson founded the cleaning products company Jasol Chemical Products. \"Jasol\" was an acronym for \"Johnson's Antiseptic Soluble Oils Limited.\" Early deliveries were done by Johnson on a pushbike.\n\nIn 1945, when he was well known for his association with the Jasol firm, Johnson was elected as Councillor on the Adelaide City Council. He resigned from the ALP in 1947. He consistently voted against generosity towards retiring councillors, mayors and employees, even when he acknowledged that the person had served the council exceptionally well. He resigned in 1949 to stand as an alderman but was defeated.\n\nHe was vice-president of the Adelaide sub-branch of the Returned Sailors Soldiers and Airmens Imperial League of Australia.\n\nJohnson lived on South Terrace, Adelaide. He died in the Repatriation General Hospital, Daw Park in 1950. He was survived by his wife Millie Johnson.\n"}
{"id": "19135050", "url": "https://en.wikipedia.org/wiki?curid=19135050", "title": "Compressed hydrogen", "text": "Compressed hydrogen\n\nCompressed hydrogen (CH, CGH or CGH2) is the gaseous state of the element hydrogen kept under pressure. Compressed hydrogen in hydrogen tanks at 350 bar (5,000 psi) and 700 bar (10,000 psi) is used for mobile hydrogen storage in hydrogen vehicles. It is used as a fuel gas.\n\nCompressed hydrogen is used in hydrogen pipeline transport and in compressed hydrogen tube trailer transport.\n\n\n"}
{"id": "42882", "url": "https://en.wikipedia.org/wiki?curid=42882", "title": "Cosmogony", "text": "Cosmogony\n\nCosmogony is any model concerning the origin of either the cosmos or universe. Developing a complete theoretical model has implications in both the philosophy of science and epistemology.\n\nThe word comes from the Koine Greek κοσμογονία (from κόσμος \"cosmos, the world\") and the root of γί(γ)νομαι / γέγονα (\"come into a new state of being\"). In astronomy, cosmogony refers to the study of the origin of particular astrophysical objects or systems, and is most commonly used in reference to the origin of the Universe, the Solar System, or the Earth–Moon system.\n\nThe Big Bang theory is the prevailing cosmological model of the early development of the universe. \nThe most commonly held view is that the universe originates in a gravitational singularity, which expanded extremely rapidly from its hot and dense state. \nCosmologist and science communicator Sean M. Carroll explains two competing types of explanations for the origins of the singularity which is the main disagreement between the scientists who study cosmogony and centers on the question of whether time existed \"before\" the emergence of our universe or not. One cosmogonical view sees time as fundamental and even eternal: The universe could have contained the singularity because the universe evolved or changed from a prior state (the prior state was \"empty space\", or maybe a state that could not be called \"space\" at all). The other view, held by proponents like Stephen Hawking, says that there was no change through time because \"time\" itself emerged along with this universe (in other words, there can be no \"prior\" to the universe). Thus, it remains unclear what combination of \"stuff\", space, or time emerged with the singularity and this universe.\n\nOne problem in cosmogony is that there is currently no theoretical model that explains the earliest moments of the universe's existence (during the Planck time) because of a lack of a testable theory of quantum gravity. Researchers in string theory and its extensions (for example, M theory), and of loop quantum cosmology, have nevertheless proposed solutions of the type just discussed.\n\nCosmogony can be distinguished from cosmology, which studies the universe at large and throughout its existence, and which technically does not inquire directly into the source of its origins. There is some ambiguity between the two terms. For example, the cosmological argument from theology regarding the existence of God is technically an appeal to cosmogonical rather than cosmological ideas. In practice, there is a scientific distinction between cosmological and cosmogonical ideas. Physical cosmology is the science that attempts to explain all observations relevant to the development and characteristics of the universe as a whole. Questions regarding why the universe behaves in such a way have been described by physicists and cosmologists as being extra-scientific (i.e., metaphysical), though speculations are made from a variety of perspectives that include extrapolation of scientific theories to untested regimes (i.e., at Planck scales), and philosophical or religious ideas.\n\nCosmogonists have only tentative theories for the early stages of the universe and its beginning. , no accelerator experiments probe energies of sufficient magnitude to provide any experimental insight into the behavior of matter at the energy levels that prevailed shortly after the Big Bang. \n\nProposed theoretical scenarios differ radically, and include string theory and M-theory, the Hartle–Hawking initial state, string landscape, brane inflation, the Big Bang, and the ekpyrotic universe. Some of these models are mutually compatible, whereas others are not.\n"}
{"id": "48707305", "url": "https://en.wikipedia.org/wiki?curid=48707305", "title": "Cuboid conjectures", "text": "Cuboid conjectures\n\nThree cuboid conjectures are three mathematical propositions claiming irreducibility of three univariate polynomials with integer coefficients depending on several integer parameters. They are neither proved nor disproved.\n\nCuboid conjecture 1. \"For any two positive coprime integer numbers \nformula_1 the eighth degree polynomial\"\n\n\"is irreducible over the ring of integers formula_2\".\n\nCuboid conjecture 2. \"For any two positive coprime integer numbers \nformula_3 the tenth-degree polynomial\"\n\n\"is irreducible over the ring of integers formula_2\".\n\nCuboid conjecture 3. \"For any three positive coprime integer numbers formula_5, formula_6, formula_7 such that none of the conditions\"\n\n\"is fulfilled the twelfth degree polynomial\"\n\n\"is irreducible over the ring of integers formula_2\".\n\nThe conjectures 1, 2, and 3 are related to the perfect cuboid problem. Though they are not equivalent to the perfect cuboid problem, if all of these three conjectures are valid, then no perfect cuboids exist.\n"}
{"id": "46504", "url": "https://en.wikipedia.org/wiki?curid=46504", "title": "DARPA TIDES program", "text": "DARPA TIDES program\n\nTIDES is an ambitious technology development effort, funded by DARPA. It stands for Translingual Information Detection, Extraction and Summarization. It is focused on the automated processing and understanding of a variety of human language data. The primary goal is to make it possible for English speakers to find and interpret needed information quickly and effectively regardless of language or medium.\n\nTo provide that overall capability, TIDES is intended to develop a suite of robust, powerful, broadly useful component capabilities; integrate those components effectively in technology demonstration systems; and experiment with the systems on real-world problems. These are all high-risk research activities.\n\nThe four component capabilities are\n\nDetection, extraction, and summarization must work within a language (monolingually) and across languages (translingually) to aid people who speak only English.\n\nIn addition to creating effective technology, TIDES aims to develop methods for porting these capabilities rapidly and inexpensively to other languages, including languages having severely limited linguistic resources.\n\nTIDES will integrate its component capabilities with one another and with other technologies to produce synergistic, effective, end-to-end, technology demonstration systems able to address multiple real-world applications.\n\nThe FBI's Investigative Data Warehouse contains an \"Open Source News Library\". This library contains news gathered by the TIDES program. The information is collected from dozens of public websites all over the world, such as Ha'aretz, Pravda, the Jordan Times, The People's Daily, \"The Washington Post\", and others. It uses the MiTAP (Mitre Text and Audio Processing) system.\n\n\n\n"}
{"id": "17970972", "url": "https://en.wikipedia.org/wiki?curid=17970972", "title": "Disneyland model", "text": "Disneyland model\n\nThe Disneyland model is a proposed system in which users of a service would bear no risk for damage or injuries they sustain that are caused by others, as full liability would be imposed upon the responsible party (and/or their insurers). It is in contrast to the ballpark model, under which people use a service at their own risk. The Disneyland model is frequently advocated as a method by which licensure of motorists and their vehicles could be privatized. Before a person would be granted a license plate, they would need to obtain liability insurance without any caps on coverage amount. The name comes from the fact that at Disneyland, the company is liable for any accidents that befall a customer if they, for instance, ride a ride they were too short for.\n"}
{"id": "36380355", "url": "https://en.wikipedia.org/wiki?curid=36380355", "title": "DistrRTgen", "text": "DistrRTgen\n\nDistributed Free Rainbow Tables (or DistrRTgen) was a distributed computing project for making rainbow tables for password cracking. By using distributed computing, DistrRTgen can generate rainbow tables that are able to crack long passwords. DistrRtgen was used to generate LM, NTLM, MD5 and MYSQLSHA1 rainbow tables. All of the rainbow tables are downloadable at Free Rainbow Tables.\n"}
{"id": "2709521", "url": "https://en.wikipedia.org/wiki?curid=2709521", "title": "Donchian channel", "text": "Donchian channel\n\nThe Donchian channel is an indicator used in market trading developed by Richard Donchian. It is formed by taking the highest high and the lowest low of the last \"n\" periods. The area between the high and the low is the channel for the period chosen.\n\nIt is commonly available on most trading platforms. On a charting program, a line is marked for the high and low values visually demonstrating the channel on the markets price (or other) values.\n\nThe Donchian channel is a useful indicator for seeing the volatility of a market price. If a price is stable the Donchian channel will be relatively narrow. If the price fluctuates a lot the Donchian channel will be wider. Its primary use, however, is for providing signals for long and short positions. If a security trades above its highest \"n\" periods high, then a long is established. If it trades below its lowest \"n\" periods low, then a short is established.\n\nOriginally the \"n\" periods were based upon daily values. With today's trading platforms, the period may be of the value desired by the investor. i.e.: day, hour, minute, ticks, etc.\n\n\n"}
{"id": "58666815", "url": "https://en.wikipedia.org/wiki?curid=58666815", "title": "Elisabeth Boyko", "text": "Elisabeth Boyko\n\nElisabeth Boyko (24 September 1892 - 14 December 1985) was an Austrian-Israeli botanist noted for pioneering the use of salt water for irrigation of desert plants in Israel, alongside her husband Hugo Boyko. She received the William F. Petersen Award from the International Society of Biometeorology.\n\n"}
{"id": "19278848", "url": "https://en.wikipedia.org/wiki?curid=19278848", "title": "European decency threshold", "text": "European decency threshold\n\nThe Social Charter initially defined what many UK campaigning groups termed the Council of Europe decency threshold in the 1960s as 68% of average earnings within a national economy. The definition was modified to that of 60% of net earnings () in order to take account of the difficulties experienced in taking into account initiatives such as redistributive tax systems when calculating adequate incomes. \n\nThere are a number of anomalies between the previous use of this threshold by UK campaigning groups and the way in which it is understood by the Secretariat of the European Social Charter. The exact origins of the term 'Council of Europe Decency Threshold' are vague, but it is said to be an incorrect term as the Council of Europe did not create it. It is therefore now more commonly referred to as the 'European Social Charter Adequate Remuneration Threshold' or ESCART.\n\nMany pressure groups in the UK used the original method of calculation to call for a higher minimum wage. Before its closure, the Low Pay Unit used this threshold in campaigning in addition to calling for a minimum wage of half male median earnings, rising to 2/3 over the next few years. However, it (like many other organisations) had expressed reservations about the usefulness of the Threshold following the move to a definition of 60% of net average earnings, primarily because this was a far lower monetary amount than the Threshold as previously defined.\n\nThe Scottish Low Pay Unit, an independent organisation with similar aims to the now-defunct London based Low Pay Unit, continue to campaign for a higher National Minimum Wage in this way although they do not use the ESCART due to difficulties in obtaining accurate net earnings figures for the UK. They have also produced a briefing outlining the advantages and disadvantages of the calculation.\n"}
{"id": "17824815", "url": "https://en.wikipedia.org/wiki?curid=17824815", "title": "Galactic anticenter", "text": "Galactic anticenter\n\nThe galactic anticenter is a direction in space directly opposite to the Galactic Center, as viewed from Earth. This direction corresponds to a point on the celestial sphere. From the perspective of an observer on Earth, the galactic anticenter is located in the constellation Auriga, and Beta Tauri is the bright star that appears nearest this point.\n\nIn terms of the galactic coordinate system, the Galactic Center (in Sagittarius) corresponds to a longitude of 0°, while the anticenter is located exactly at 180°. In the equatorial coordinate system, the anticenter is found at roughly RA 05h 46m, dec +28° 56'.\n\n"}
{"id": "58866707", "url": "https://en.wikipedia.org/wiki?curid=58866707", "title": "Geraldine Harriman", "text": "Geraldine Harriman\n\nGeraldine C. Bussolaria Harriman is a chemist and biotech entrepreneur with leadership roles in multiple scientific companies.\n\nHarriman received a B.S. degree in chemistry from American International College. She studied Biomedical and Pharmaceutical Sciences with Prof. Elie Abushanab at the University of Rhode Island, receiving her doctorate in 1991. Harriman completed postdoctoral studies with Prof. Gunda Georg at the University of Kansas, focusing on the medicinal chemistry of paclitaxel analogs.\n\nHarriman's first industrial position was at the immunology start-up LeukoSite, which was acquired by Millennium Pharmaceuticals in 1999. Over the next seven years, Harriman rose to become the Head of Chemistry at Millennium, when she was recruited by Galenea in 2007. She served as VP and Head of Chemistry there until 2010, when she was appointed VP at Nimbus Therapeutics, formed by Atlas Ventures in 2009. Nimbus specialized in computational and outsourced drug discovery. The program Harriman oversaw - \"Nimbus Apollo\" - focused on targeting the acetyl-coA carboxylase pathway to treat fatty liver diseases such as Non-Alcoholic Steatohepatitis (NASH). Gilead acquired Nimbus Apollo for $1.2 billion USD in 2016.\n\nIn 2017, Harriman co-founded Atlas-backed HotSpot Therapeutics, which will seek for allosteric inhibitors to potentially treat autoimmune and metabolic disorders. Harriman is a co-inventor on over twenty small-molecule use patents and 30 scientific publications.\n\n"}
{"id": "47010541", "url": "https://en.wikipedia.org/wiki?curid=47010541", "title": "Ginger Fraser", "text": "Ginger Fraser\n\nPaul F. \"Ginger\" Fraser was an American football player, coach, and military officer. He was considered to be one of Maine's all-time greatest college football players.\n\nFraser was born in Boston's Roxbury neighborhood. He attended Boston Latin School in 1907 and 1908, where he played end, center, and halfback on the football team and outfield on the baseball team. He then attended Dorchester High School in 1909 and 1910, where he played in the backfield on the football team and second base on the baseball team. He was a named to the greater Boston all-scholastic football team two years.\n\nFraser attended Colby College, where he played four years for the Colby Mules football team. He was the team captain in 1914. That year, Colby outscored its three in-state rivals Bowdoin College, the University of Maine, and Bates College 123 to 0 to win the series title. Colby also gained national recognition for its game against the star–studded Navy Midshipmen. Colby led Navy 21–10 at the half, but due to Navy's superior manpower, it was able to use a great number of substitutes in the second half, which helped the team score 21 unanswered points and win the game 31–21. After the game, a \"New York Times\" sportswriter wrote \"It was one of the finest exhibitions of football ever seen at Annapolis. In the first half the brilliant running of [Edward] Cawley, [John] Lowney, and Fraser swept the Midshipmen off their feet.\"\n\nAfter graduating in 1915, Fraser served as sub-master and athletic director at Waterville High School in Waterville, Maine. On April 27, 1916, the Everett, Massachusetts School Committee unanimously voted to hire Fraser to coach football and baseball and teach science. He succeeded Cleo O'Donnell, who became head coach of the Purdue Boilermakers football team.\n\nDuring World War I, Fraser tried to enter government service, but was initially rejected due to poor eyesight. He instead joined the YMCA, which maintained camps at the rear of the fighting lines. On April 30, 1917 he was ordered to report to his YMCA in Portland, Maine. Everett High granted him an indefinite leave of absence. That December, Fraser was selected for the Officers' Training Corps and assigned to Fort Oglethorpe in Georgia. He was later transferred to Camp Wadsworth in Spartanburg, South Carolina, where he excelled in wrestling. He was eventually assigned to the 1st Maine Heavy Artillery Regiment as a First Lieutenant.\n\nAfter the War, Fraser became athletic director at the Coburn Classical Institute. In 1922 he became community athletic director for Westbrook, Maine and head football coach of the town's high school. He also served as a teacher and secretary of the Westbrook Community Association. In 1926 his team won the Maine state championship. That same year he was the referee for a football game between Bowdoin and Boston University. The game was the first in Maine and one of the first in the United States to be played with a limited number of plays (each team had thirty plays per quarter). Bowdoin won the game 6–0. In 1928 and 1929, Fraser was granted a leave of absence to serve as an assistant football coach at Bowdoin. He retired as Westbrook football coach and athletic director in June 1932, but stayed on as a teacher and community association secretary.\n\nOn April 11, 1938, Fraser died of a heart attack following a game of badminton. He was survived by his wife, four daughters, and two sons. In 1939, Colby College's annual \"Colby Night\" was renamed \"Ginger Fraser Night\" to honor Fraser. All of the living members of the 1914 team returned to Colby for the school's homecoming celebration.\n"}
{"id": "17426234", "url": "https://en.wikipedia.org/wiki?curid=17426234", "title": "Green Canyon", "text": "Green Canyon\n\nGreen Canyon is an area in the Gulf of Mexico that is rich in oil fields and under the control of the Bureau of Ocean Energy Management. Among other oil fields Green Canyon consist of Atlantis (blocks 699, 700, 742, 743, and 744) operated by BP, Marco Polo (block 608) and K2 (blocks 518 and 562) operated by Anadarko Petroleum, Manatee (block 155) operated by Shell, Shenzi (blocks 609, 610, 653, and 654) operated by BHP Billiton, Droshky (block 244) operated by Marathon Oil, Tahiti (blocks 596, 597, 640 and 641) operated by Chevron Corporation.\nGreen Canyon, block 185 is known as Bush Hill and is a well known cold seep with a wide array of tube worms. In this area, the first time in Gulf of Mexico gas hydrates were recovered in piston cores.\nAdditionally, like most of the Gulf of Mexico, there is a significant amount of salt tectonics.\n\nBlocks in this protraction area are defined in UTM zone 15 north in feet.\n"}
{"id": "27880038", "url": "https://en.wikipedia.org/wiki?curid=27880038", "title": "Gustav Eberhard", "text": "Gustav Eberhard\n\nGustav E. Eberhard (10 August 1867 – 3 January 1940) German astrophysicist.\n\nEberhard published numerous investigations on spectroscopy and on photographic photogrametry.\n\nThe photographic \"Eberhard effect\" (belonging to the edge effects family) is named after him and was published in 1926.\n"}
{"id": "10049973", "url": "https://en.wikipedia.org/wiki?curid=10049973", "title": "Gyromax", "text": "Gyromax\n\nThe Gyromax is the trade name for an adjustable mass (or adjustable inertia) balance wheel used in Patek Philippe wristwatches. Instead of weight adjustment screws on the outside of the rim, as in traditional balances, the Gyromax has turnable weights recessed into the top of the rim. The advantages claimed for this design are that, without projecting weight screws, the diameter of the balance can be increased, giving it a larger moment of inertia, and that it has less air resistance.\n\nThe Gyromax balance has six to eight small turnable weights that fit on pins located in recesses around the top of the balance wheel rim. Each of the weights, called \"collets\", has a cutout making it heavier on one side. When the collet's cutout points to the outside of the balance wheel, the heavier side is toward the center which decreases the wheel's moment of inertia, increasing its speed. When the collet's cutout points toward the center, the weight moves outward and the balance wheel turns more slowly. A watchmaker can turn an individual collet to adjust the wheel's balance, referred to as 'poise', or pairs of opposing collets to adjust the wheel's rotation speed. The Gyromax is a 'free sprung' balance, meaning there is no regulator on the wheel's balance spring for adjusting the watch's rate, so the collets are used for adjusting both poise and rate.\n\nSwiss patents were granted to Patek Phillipe for the Gyromax balance on May 15, 1949 and December 31, 1951, and the balance was first used in watches in 1952. Another balance wheel with a similar design is the Rolex Microstella.\n\n"}
{"id": "49278031", "url": "https://en.wikipedia.org/wiki?curid=49278031", "title": "Helmut Gams", "text": "Helmut Gams\n\nHelmut Gams (1893–1976) was a central European botanist. Born in Brno, he moved to Zürich as a child. He studied at the University of Zurich, being awarded a PhD in 1918. During his career, he worked at the University of Munich and the University of Innsbruck. His research saw him pursue fieldwork around Europe and Asia. He specialized in mosses and lichens.\n"}
{"id": "19731062", "url": "https://en.wikipedia.org/wiki?curid=19731062", "title": "Ibsen (crater)", "text": "Ibsen (crater)\n\nIbsen is a crater on Mercury. It is located near the antipode of Caloris Basin.\n"}
{"id": "513747", "url": "https://en.wikipedia.org/wiki?curid=513747", "title": "Imperial Prize of the Japan Academy", "text": "Imperial Prize of the Japan Academy\n\nThe is a prestigious honor conferred to two of the recipients of the Japan Academy Prize. It is awarded in two categories: humanities and natural sciences. The Emperor and Empress visit the awarding ceremony and present a vase to the awardees.\n\n\n\n"}
{"id": "33656853", "url": "https://en.wikipedia.org/wiki?curid=33656853", "title": "Information history", "text": "Information history\n\nInformation history may refer to the history of each of the categories listed below (or to combinations of them). It should be recognized that the understanding of, for example, libraries as information systems only goes back to about 1950. The application of the term \"information\" for earlier systems or societies is a retronym.\n\nThe Latin roots and Greek origins of the word \"information\" is presented by Capurro & Hjørland (2003). References on \"formation or molding of the mind or character, training, instruction, teaching\" date from the 14th century in both English (according to Oxford English Dictionary) and other European languages. \nIn the transition from Middle Ages to Modernity the use of the concept of information reflected a fundamental turn in epistemological basis – from \"giving a (substantial) form to matter\" to \"communicating something to someone\". Peters (1988, pp. 12–13) concludes:\n\nIn the modern era, the most important influence on the concept of information is derived from the Information theory developed by Shannon and others. This theory, however, reflects a fundamental contradiction. Qvortrup (1993) wrote:\n\nIn their seminal book \"The Study of Information: Interdisciplinary Messages\", Machlup and Mansfield (1983) collected key views on the interdisciplinary controversy in computer science, artificial intelligence, library and information science, linguistics, psychology, and physics, as well as in the social sciences. Machlup (1983, p. 660) himself disagrees with the use of the concept of information in the context of signal transmission, the basic senses of information in his view al referring \"to telling something or to the something that is being told. Information is addressed to human minds and is received by human minds.\" All other senses, including its use with regard to nonhuman organisms as well to society as a whole, are, according to Machlup, metaphoric and, as in the case of cybernetics, anthropomorphic.\n\nHjørland (2007) describes the fundamental difference between objective and subjective views of information and argues that the subjective view has been supported by, among others, Bateson, Yovits, Spang-Hanssen, Brier, Buckland, Goguen, and Hjørland. Hjørland provided the following example: \n\nInformation history is an emerging discipline related to, but broader than, library history. An important introduction and review was made by Alistair Black (2006).\nA prolific scholar in this field is also Toni Weller, for example, Weller (2007, 2008, 2010a and 2010b). As part of her work Toni Weller has argued that there are important links between the modern information age and its historical precedents. A description from Russia is Volodin (2000).\n\nAlistair Black (2006, p. 445) wrote: \"This chapter explores issues of discipline definition and legitimacy by segmenting information history into its various components: \n\n\"Bodies influential in the field include the American Library Association’s Round Table on Library History, the Library History Section of the International Federation of Library Associations and Institutions (IFLA), and, in the U.K., the Library and Information History Group of the Chartered Institute of Library and Information Professionals (CILIP). Each of these bodies has been busy in recent years, running conferences and seminars, and initiating scholarly projects. Active library history groups function in many other countries, including Germany (The Wolfenbuttel Round Table on Library History, the History of the Book and the History of Media, located at the Herzog August Bibliothek), Denmark (The Danish Society for Library History, located at the Royal School of Library and Information Science), Finland (The Library History Research Group, University of Tamepere), and Norway (The Norwegian Society for Book and Library History). Sweden has no official group dedicated to the subject, but interest is generated by the existence of a museum of librarianship in Bods, established by the Library Museum Society and directed by Magnus Torstensson. Activity in Argentina, where, as in Europe and the U.S., a \"new library history\" has developed, is described by Parada (2004).\" (Black (2006, p. 447).\n\n\nThe term IT is ambiguous although mostly synonym with computer technology. Haigh (2011, pp. 432-433) wrote \n\nSome people use the term \"information technology\" about technologies used before the development of the computer. This is however to use the term as a retronym.\n\n\n\"It is said that we live in an \"Age of Information,\" but it is an open scandal that there is no theory, nor even definition, of information that is both broad and precise enough to make such an assertion meaningful.\" (Goguen, 1997).\n\nThe Danish Internet researcher Niels Ole Finnemann (2001) developed a general history of media. He wrote: \"A society cannot exist in which the production and exchange of information are of only minor significance. For this reason one cannot compare industrial societies to information societies in any consistent way. Industrial societies are necessarily also information societies, and information societies may also be industrial societies.\" He suggested the following media matrix:\n\n\nMany information science historians cite Paul Otlet and Henri La Fontaine as the fathers of information science with the founding of the International Institute of Bibliography (IIB) in 1895 Institutionally, information science emerged in the last part of the 19th century as documentation science which in general shifted name to information science in the 1960s.\n\nHeting Chu (2010) classified the history and development of information representation and retrieval (IRR) in four phases. \"The history of IRR is not long. A retrospective look at the field identifies increased demand, rapid growth, the demystification phase, and the networked era as the four major stages IRR has experienced in its development:\" \n\n\n\n"}
{"id": "48589000", "url": "https://en.wikipedia.org/wiki?curid=48589000", "title": "Joyanti Chutia", "text": "Joyanti Chutia\n\nJoyanti Chutia is an Indian physicist. She was the among the first women who have headed scientific institutions in India when she became the Director of the Institute of Advanced Study in Science and Technology in Guwahati, Assam the first major research institution in North East India. She is a fellow of National Academy of Sciences. She is an Emeritus Scientist at the Department of Science & Technology in the Government of India \n\nJoyanti became involved in physics when she was a student in the Cotton College in Assam. After finishing her fellowship given by the Japanese Government in 1988 to work in the Plasma Laboratory of the Institute of Space and Astronautical Science, Tokyo, in 2005 she became the Director of the Institute of Advanced Study in Science and Technology.\n"}
{"id": "6978390", "url": "https://en.wikipedia.org/wiki?curid=6978390", "title": "Korean decimal classification", "text": "Korean decimal classification\n\nThe Korean decimal classification (KDC) is a system of library classification used in South Korea. The main classes are the same as in the Dewey Decimal Classification but these are in a different order: Natural sciences 400; Technology and engineering 500; Arts 600; Language 700.\n\n"}
{"id": "41617045", "url": "https://en.wikipedia.org/wiki?curid=41617045", "title": "Leopoldo López Escobar", "text": "Leopoldo López Escobar\n\nÁngel Leopoldo López-Escobar (1940 – June 29, 2013) was a Chilean geochemistry academic. His scientific career begun by studying Biology and Chemistry at the Pontifical Catholic University of Chile. Later he worked at the Austral University of Chile and the University of Chile. In 1996 he started a metallogeny research group at the University of Concepción.\n\n"}
{"id": "21564241", "url": "https://en.wikipedia.org/wiki?curid=21564241", "title": "Linear model of innovation", "text": "Linear model of innovation\n\nThe Linear Model of Innovation was an early model designed for to understand the relationship of science and technology that begins with basic research that flows into applied research, development and diffusion \n\nIt prioritizes scientific research as the basis of innovation, and plays down the role of later players in the innovation process.\n\nCurrent models of innovation derive from approaches such as Actor-Network Theory, Social shaping of technology and social learning,provide a much richer picture of the way innovation works. Current ideas in Open Innovation and User innovation derive from these later ideas. \n\nIn the 'Phase Gate Model' , the product or services concept is frozen at an early stage to minimize risk. Through enterprise, the innovation process involves a series of sequential phases arranged in a manner that the preceding phase muse be cleared before movie to the next phase. Therefore a project must pass through a gate with the permission of the gatekeeper before moving to the next succeeding phase.\n\nCriteria for passing through each gate is defined beforehand. The gatekeeper examines whether the stated objectives for the preceding phase have been properly met or not and whether desired development has taken place during the preceding phase or not.\n\nTwo versions of the linear model of innovation are often presented:\n\n\nFrom the 1950s to the Mid-1960s, the industrial innovation process was generally perceived as a linear progression from scientific discovery, through technological development in firms, to the marketplace. The stages of the \"Technology Push\" model are: \n\nFrom the Mid 1960s to the Early 1970s, emerges the second-generation Innovation model, referred to as the \"market pull\" model of innovation. According to this simple sequential model, the market was the source of new ideas for directing R&D, which had a reactive role in the process. The stages of the \"market pull \" model are:\n\nThe linear models of innovation supported numerous criticisms concerning the linearity of the models. These models ignore the many feedbacks and loops that occur between the different \"stages\" of the process. Shortcomings and failures that occur at various stages may lead to a reconsideration of earlier steps and this may result in an innovation. A history of the linear model of innovation may be found in Godin \"The Linear Model of Innovation: The Historical Construction of an Analytical Framework\".\n\n\n"}
{"id": "599970", "url": "https://en.wikipedia.org/wiki?curid=599970", "title": "List of lemmas", "text": "List of lemmas\n\nThis following is a list of lemmas (or, \"lemmata\", i.e. minor theorems, or sometimes intermediate technical results factored out of proofs). See also list of axioms, list of theorems and list of conjectures.\n"}
{"id": "43943566", "url": "https://en.wikipedia.org/wiki?curid=43943566", "title": "Malignant: How Cancer Becomes Us", "text": "Malignant: How Cancer Becomes Us\n\nMalignant: How Cancer Becomes Us is a 2013 book by S. Lochlann Jain, published by University of California Press.\n\nIt won an Edelstein Prize, and Victor Turner Prize.\n\n"}
{"id": "7598228", "url": "https://en.wikipedia.org/wiki?curid=7598228", "title": "Natural transfer", "text": "Natural transfer\n\nThe natural transfer (hypothesis or theory), in reference to the HIV/AIDS pandemic, states that humans first received HIV by contact with primates, presumably from a fight with a Chimpanzee during hunting or consumption of primate meat, and became contaminated with simian immunodeficiency virus (SIV). According to the 'Hunter Theory', the virus was transmitted from a chimpanzee to a human when a bushmeat hunter was bitten or cut while hunting or butchering an animal. The resulting exposure of the hunter to blood or other bodily fluids of the chimpanzee could have resulted in infection. A contrasting hypothesis regarded as disproven is the oral polio vaccine (OPV) AIDS hypothesis.\n"}
{"id": "1419397", "url": "https://en.wikipedia.org/wiki?curid=1419397", "title": "Newton's Apple", "text": "Newton's Apple\n\nNewton's Apple is an American educational television program produced and developed by KTCA of Minneapolis–Saint Paul, and distributed to PBS stations in the United States that ran from October 15, 1983, to January 3, 1998, with reruns continued until October 31, 1999. The show's title is based on the legend of Isaac Newton sitting under a tree and an apple falling near him — or, more popularly, on his head — prompting him to ponder what makes things fall, leading to the development of his theory of gravitation (an event often loosely described as him \"discovering\" gravity). The show was produced by Twin Cities Public Television (tpt). For most of the run, the show's theme song was \"Ruckzuck\" by Kraftwerk, later remixed by Absolute Music. Earlier- and later- episodes of the show featured an original song.\n\nIra Flatow was the show's first host, later succeeded by David Heil, then assistant director of the Oregon Museum of Science and Industry (OMSI). Peggy Knapp was a longtime field reporter and served as co-host in the 14th season. The last season was hosted by the team of David Heil, Dave Huddleston, actress and voiceover artist Eileen Galindo, Brian Hackney and SuChin Pak. An occasional short feature called \"Science of the Rich and Famous\" featured celebrities explaining a scientific principle or natural or physical phenomenon; for example, rock star Ted Nugent explained guitar feedback, Olympic Gold Medalist skater Scott Hamilton demonstrated angular momentum in the context of a skater's spin, Monty Hall explained principles of probability, and Betty White showed how cats purr.\n\n\"Newton's Apple\" won numerous national awards including the American Association for the Advancement of Science Science Journalism Award, the Parent's Choice Award, and the 1989 Daytime Emmy Award for Outstanding Children's Series. \n\nA segment in the early years, titled \"Newton's Lemons,\" used 1950s-era newsreels of a device that was considered \"futuristic\" at the time of its introduction but had long since been forgotten.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "23177673", "url": "https://en.wikipedia.org/wiki?curid=23177673", "title": "Nukespeak", "text": "Nukespeak\n\nNukespeak: Nuclear Language, Visions and Mindset is a 1982 book by Stephen Hilgartner, Richard C. Bell and Rory O'Connor. This book is a concise history of nuclear weapons and nuclear power in the United States, with special emphasis on the language of the \"nuclear mindset\".\n\nThe National Council of Teachers of English gave the book's authors an Orwell Award in 1982.\n\n"}
{"id": "34560068", "url": "https://en.wikipedia.org/wiki?curid=34560068", "title": "Orbit-Vis", "text": "Orbit-Vis\n\nOrbit-Vis is an orbit simulation program which is designed to allow users to simulate a satellite in any orbit around the Earth, and to give the user data on the position and motion of the satellite in both visual and numerical form. It is based on information from the book 'Satellite Orbits - Models, Methods and Applications', and is an example of an orbit propagation program utilising the Kepler Two-body problem for solving elliptical orbits.\n\nIt uses the Graphics Device Interface to display the calculated orbit as it progresses.\n\nThe program features two major parts - the input and numerical display window, in which the user can enter a desired Apogee and Perigee and press the 'Launch!' button to begin the simulation, and which displays the numerical properties of the orbit in real time, and also the visual display, which consists of a sizeable window displaying the Earth and the orbiting object to scale. Also featured is a control panel which allows the user to manipulate the display window in real time.\n\nThe graphical display features double-buffering and the ability to update the inputs while the simulation is running, along with a moveable viewpoint and the ability to pause the simulation.\n\nVersion 2.0 also fixed various bugs and cleared up the interface.\n\n\n"}
{"id": "17144791", "url": "https://en.wikipedia.org/wiki?curid=17144791", "title": "Pattern-oriented modeling", "text": "Pattern-oriented modeling\n\nPattern-oriented modeling (POM) is an approach to bottom-up complex systems analysis that was developed to model complex ecological and agent-based systems. A goal of POM is to make ecological modeling more rigorous and comprehensive.\n\nA traditional ecosystem model attempts to approximate the real system as closely as possible. POM proponents posit that an ecosystem is so information-rich that an ecosystem model will inevitably either leave out relevant information or become over-parameterized and lose predictive power. Through a focus on only the relevant patterns in the real system, POM offers a meaningful alternative to the traditional approach. \n\nAn attempt to mimic the scientific method, POM requires the researcher to begin with a pattern found in the real system, posit hypotheses to explain the pattern, and then develop predictions that can be tested. A model used to determine the original pattern may not be used to test the researcher’s predictions. Through this focus on the pattern, the model can be constructed to include only information relevant to the question at hand.\n\nPOM is also characterized by an effort to identify the appropriate temporal and spatial scale at which to study a pattern, and to avoid the assumption that a single process might explain a pattern at multiple temporal or spatial scales. It does, however, offer the opportunity to look explicitly at how processes at multiple scales might be driving a particular pattern.\n\nA look at the trade-offs between model complexity and payoff can be considered in the framework of the Medawar zone. The model is considered too simple if it addresses a single problem (e.g., the explanation behind a single pattern), whereas it will be considered too complex if it incorporates all the available biological data. The Medawar zone, where the payoff in what is learned is greatest, is at an intermediate level of model complexity.\n\nPattern-oriented modeling has been used to test a priori hypotheses on how herdsman decide which farmers to contract with when grazing their cattle. Herdsman behavior followed the pattern predicted by a 'friend' rather than 'cost' priority hypothesis.\n"}
{"id": "45312187", "url": "https://en.wikipedia.org/wiki?curid=45312187", "title": "Plymouth Coastal Observatory", "text": "Plymouth Coastal Observatory\n\nThe Plymouth Coastal Observatory (PCO) is the data management centre for the South West Regional Coastal Monitoring Programme. It is managed and led by Teignbridge District Council in partnership with other south west of England local authorities and the Environment Agency.\n\nThe South West Regional Coastal Programme is part of a nationwide network of regional coastal monitoring programmes. The focus of the ongoing programme is collecting data on waves, tides, LiDAR, Aerial Photography, topographic beach surveys, storm response and ecological mapping Data is published through its website, and is freely available for public use.\n\nThe PCO office is situated on the Campus of Plymouth University, located in the city centre of Plymouth, England.\n\nThe first phase of the programme was set up in 2006, with an initial grant of £7.2 million from the Department for Environment, Food and Rural Affairs (DEFRA). The funding was split into two with £4.1 million used by Teignbridge to deliver, Bathymetric and Topographic, Hydrographic surveys and £3.1 million managed by the Environment Agency to deliver Aerial photography, LiDAR and Ecological mapping. Phase 2 of the programme commenced in 2011 with 100% DEFRA funding, and will run until 2016. Phase 2 of the Programme is solely managed by Teignbridge District Council.\n\nThe PCO has a network of wave buoys around the south west coastline collecting data on wave height, direction and sea temperature. In July 2014 the PCO wave buoys recorded the highest sea temperatures seen for 7 years around the south west coastline. In 2011 the PCO buoy network also detected a 0.5 - 0.8m tsunami along south west coast of England.\n\nThe PCO has four tide gauges situated around the south west coast, collecting real time tidal and surge data. A notable addition to the tidal gauge network was the Port Isaac Step gauge, which was installed in 2010. The installation at Port Isaac filled a 'notable gap in measured tide data along the north Cornwall coastline' \n\nAfter the 2013/2014 winter storms Natural Environment Research Council (NERC) awarded a £50,000 emergency project grant to coastal researchers at the Plymouth University in conjunction with PCO and the Met Office. The project will run from the 1st March 2014 for 1 year and assess the coastal response to the extreme winter storms. Collaboration between the PCO and the Plymouth University Coastal Process Research Group (CPRG) is ongoing.\n\nData collected and provided by the PCO is used by local authority coastal engineers and planners to inform decisions on coastal policy, defence and maintenance.\n\nThe PCO currently employs 4 members of full-time staff. It has close links with the Channel Coastal Observatory, and academics at Plymouth University. Phase 3 of the programme is expected to commence in March 2016 pending DEFRA funding.\n\n"}
{"id": "977923", "url": "https://en.wikipedia.org/wiki?curid=977923", "title": "Ridge push", "text": "Ridge push\n\nRidge push or sliding plate force is a proposed driving force for plate motion in plate tectonics that occurs at mid-ocean ridges as the result of the rigid lithosphere sliding down the hot, raised asthenosphere below mid-ocean ridges. Although it is called ridge push, the term is somewhat misleading; it is actually a body force that acts throughout an ocean plate, not just at the ridge, as a result of gravitational pull. The name comes from earlier models of plate tectonics in which ridge push was primarily ascribed to upwelling magma at mid-ocean ridges pushing or wedging the plates apart.\n\nRidge push is the result of gravitational forces acting on the young, raised oceanic lithosphere around mid-ocean ridges, causing it to slide down the similarly raised but weaker asthenosphere and push on lithospheric material farther from the ridges.\n\nMid-ocean ridges are long underwater mountain chains that occur at divergent plate boundaries in the ocean, where new oceanic crust is formed by upwelling mantle material as a result of tectonic plate spreading and relatively shallow (above ~60 km) decompression melting. The upwelling mantle and fresh crust are hotter and less dense than the surrounding crust and mantle, but cool and contract with age until reaching equilibrium with older crust at around 90 Ma. This produces an isostatic response that causes the young regions nearest the plate boundary to rise above older regions and gradually sink with age, producing the mid-ocean ridge morphology. The greater heat at the ridge also weakens rock closer to the surface, raising the boundary between the brittle lithosphere and the weaker, ductile asthenosphere to create a similar elevated and sloped feature underneath the ridge.\n\nThese raised features produce ridge push; gravity pulling down on the lithosphere at the mid-ocean ridge is mostly opposed by the normal force from the underlying rock, but the remainder acts to push the lithosphere down the sloping asthenosphere and away from the ridge. Because the asthenosphere is weak, ridge push and other driving forces are enough to deform it and allow the lithosphere to slide over it, opposed by drag at the Lithosphere-Asthenosphere boundary and resistance to subduction at convergent plate boundaries. Ridge push is mostly active in lithosphere younger than 90 Ma, after which it has cooled enough to reach thermal equilibrium with older material and the slope of the Lithosphere-Asthenosphere boundary becomes effectively zero.\n\nDespite its current status as one of the driving forces of plate tectonics, ridge push was not included in any of Alfred Wegener's 1912-1930 proposals of continental drift, which were produced before the discovery of mid-ocean ridges and lacked any concrete mechanisms by which the process might have occurred. Even after the development of acoustic depth sounding and the discovery of global mid-ocean ridges in the 1930s, the idea of a spreading force acting at the ridges was not mentioned in scientific literature until Harry Hess's proposal of seafloor spreading in 1960, which included a pushing force at mid-ocean ridges as a result of upwelling magma wedging the lithosphere apart.\n\nIn 1964 and 1965, Egon Orowan proposed the first gravitational mechanism for spreading at mid-ocean ridges, postulating that spreading can be derived from the principles of isostasy. In Orowan's proposal, pressure within and immediately under the elevated ridge is greater than the pressure in the oceanic crust to either side due to the greater weight of overlying rock, forcing material away from the ridge, while the lower density of the ridge material relative to the surrounding crust would gradually compensate for the greater volume of rock down to the depth of isostatic compensation. Similar models were proposed by Lliboutry in 1969, Parsons and Richer in 1980, and others. In 1969, Hales proposed a model in which the raised lithosphere of the mid-ocean ridges slid down the elevated ridge, and in 1970 Jacoby proposed that the less dense material and isostasy of Orowan and others' proposals produced uplift which resulted in sliding similar to Hales' proposal. The term \"ridge push force\" was coined by Forsyth and Uyeda in 1975.\n\nEarly models of plate tectonics, such as Harry Hess's seafloor spreading model, assumed that the motions of plates and the activity of mid-ocean ridges and subduction zones were primarily the result of convection currents in the mantle dragging on the crust and supplying fresh, hot magma at mid-ocean ridges. Further developments of the theory suggested that some form of ridge push helped supplement convection in order to keep the plates moving, but in the 1990s, calculations indicated that slab pull, the force that a subducted section of plate exerts on the attached crust on the surface, was an order of magnitude stronger than ridge push. As of 1996, slab pull was generally considered the dominant mechanism driving plate tectonics. Modern research, however, indicates that the effects of slab pull are mostly negated by resisting forces in the mantle, limiting it to only 2-3 times the effective strength of ridge push forces in most plates, and that mantle convection is probably much too slow for drag between the lithosphere and the asthenosphere to account for the observed motion of the plates. This restores ridge push as one of the dominant factors in plate motion.\n\nRidge push is primarily opposed by plate drag, which is the drag force of the rigid lithosphere moving over the weaker, ductile asthenosphere. Models estimate that ridge push is probably just sufficient to overcome plate drag and maintain the motion of the plate in most areas. Slab pull is similarly opposed by resistance to the subduction of the lithosphere into the mantle at convergent plate boundaries.\n\nResearch by Rezene Mahatsente indicates that the driving stresses caused by ridge push would be dissipated by faulting and earthquakes in plate material containing large quantities of unbound water, but they conclude that ridge push is still a significant driving force in existing plates because of the rarity of intraplate earthquakes in the ocean.\n\nIn plates with particularly small or young subducting slabs, ridge push may be the predominant driving force in the plate's motion. According to Stefanick and Jurdy, the ridge push force acting on the South American plate is approximately 5 times the slab pull forces acting at its subducting margins because of the small size of the subducting slabs at the Scotia and Caribean margins. The Nazca plate also experiences relatively small slab pull, approximately equal to its ridge push, because the plate material is young (no more than 50 million years old) and therefore less dense, with less tendency to sink into the mantle. This also causes the subducting Nazca slab to experience flat slab subduction, one of the few places in the world where this currently occurs.\n"}
{"id": "34403339", "url": "https://en.wikipedia.org/wiki?curid=34403339", "title": "Sage Bionetworks", "text": "Sage Bionetworks\n\nSage Bionetworks is a nonprofit organization in Seattle that promotes open science and patient engagement in the research process. It is led by Lara Mangravite. It was co-founded by Stephen Friend and Eric Schadt.\n\nSage Bionetworks is notable for being an early advocate of open science. The company operates a software platform for collaborative data analysis called Synapse that allows researchers to work together on data curation and computational modeling asynchronously in a manner inspired by GitHub. Synapse also serves as the software infrastructure for running computational challenges. Sage is also developing a citizen-science platform called Bridge.\n\nThe bulk of Sage's scientific results emerge from cancer and neurosciences, with notable contributions to the Cancer Genome Atlas Pan-Cancer project. Another Sage initiative, \"The Resilience Project\" describes itself as a search for individuals who have genetic changes expected to cause severe illness but who remain perfectly healthy. The hope is to yield insight into factors that protect these individuals from disease.\n\nSage Bionetworks was founded in 2009 as a spinout of Merck & Co., who released software, hardware, intellectual property, and staff connected to its Rosetta Inpharmatics unit. A donation from Quintiles provided early funding.\n\nIn March 2011 Sage partnered with CHDI Foundation to develop computer simulations for studying Huntington's disease. At the same time Sage also announced a partnership with Takeda Pharmaceutical Company wherein Sage would do research to identify biological targets for central nervous system diseases.\n\nIn February 2013, Sage Bionetworks partnered with the Dialogue on Reverse Engineering Assessment and Methods (DREAM) project to provide expertise and infrastructure for DREAM Challenges on the Synapse.org platform.\n\n\n"}
{"id": "16022841", "url": "https://en.wikipedia.org/wiki?curid=16022841", "title": "Shaft passer", "text": "Shaft passer\n\nA shaft passer is a hypothetical device that allows a spoked wheel to rotate despite having a shaft (such as the axle of another wheel) passing between its spokes. The device is usually mentioned as a joke between nerds, in the manner of a fool's errand, since there is no evidence of one ever having been constructed until very recently.\n\nOne of the earliest printed references to these devices was made by Richard Feynman, who was told by a colleague at Frankford Arsenal in Philadelphia that the cable-passing version of the device had been used during both world wars on German naval mine mooring cables, to prevent the mines from being caught by British cables swept along the sea bottom.\n\nThe device was supposed to work using a spoked, rimless wheel that allows cables to pass through as it rotates. The ends of the spokes are widened, and the cable is held together by a short curved sleeve through which these spoke ends slide.\n"}
{"id": "140586", "url": "https://en.wikipedia.org/wiki?curid=140586", "title": "Specification language", "text": "Specification language\n\nA specification language is a formal language in computer science used during systems analysis, requirements analysis and systems design to describe a system at a much higher level than a programming language, which is used to produce the executable code for a system.\n\nSpecification languages are generally not directly executed. They are meant to describe the \"what\", not the \"how\". Indeed, it is considered as an error if a requirement specification is cluttered with unnecessary implementation detail.\n\nA common fundamental assumption of many specification approaches is that programs are modelled as algebraic or model-theoretic structures that include a collection of sets of data values together with functions over those sets. This level of abstraction coincides with the view that the correctness of the input/output behaviour of a program takes precedence over all its other properties.\n\nIn the \"property-oriented\" approach to specification (taken e.g. by CASL), specifications of programs consist mainly of logical axioms, usually in a logical system in which equality has a prominent role, describing the properties that the functions are required to satisfy - often just by their interrelationship.\nThis is in contrast to so-called model-oriented specification in frameworks like VDM and Z, which consist of a simple realization of the required behaviour.\n\nSpecifications must be subject to a process of \"refinement\" (the filling-in of implementation detail) before they can actually be implemented. The result of such a refinement process is an executable algorithm, which is either formulated in a programming language, or in an executable subset of the specification language at hand. For example, Hartmann pipelines, when\nproperly applied, may be considered a dataflow specification which \"is\" directly executable. Another example is the Actor model which has no specific application content and must be \"specialized\" to be executable.\n\nAn important use of specification languages is enabling the creation of proofs of program correctness (\"see theorem prover\").\n\n\n"}
{"id": "2576109", "url": "https://en.wikipedia.org/wiki?curid=2576109", "title": "Sports rating system", "text": "Sports rating system\n\nA sports rating system is a system that analyzes the results of sports competitions to provide ratings for each team or player. Common systems include polls of expert voters, crowdsourcing non-expert voters, betting markets, and computer systems. Ratings, or power ratings, are numerical representations of competitive strength, often directly comparable so that the game outcome between any two teams can be predicted. Rankings, or power rankings, can be directly provided (e.g., by asking people to rank teams), or can be derived by sorting each team's ratings and assigning an ordinal rank to each team, so that the highest rated team earns the #1 rank. Rating systems provide an alternative to traditional sports standings which are based on win-loss-tie ratios.\nIn the United States, the biggest use of sports ratings systems is to rate NCAA college football teams in Division I FBS, choosing teams to play in the College Football Playoff. Sports ratings systems are also used to help determine the field for the NCAA men's and women's basketball tournaments, men's professional golf tournaments, professional tennis tournaments, and NASCAR. They are often mentioned in discussions about the teams that could or should receive invitations to participate in certain contests, despite not earning the most direct entrance path (such as a league championship).\n\nComputer rating systems can tend toward objectivity, without specific player, team, regional, or style bias. Ken Massey writes that an advantage of computer rating systems is that they can \"objectively track all\" 351 college basketball teams, while human polls \"have limited value\". Computer ratings are verifiable and repeatable, and are comprehensive, requiring assessment of all selected criteria. By comparison, rating systems relying on human polls include inherent human subjectivity; this may or may not be an attractive property depending on system needs.\n\nSports ratings systems have been around for almost 80 years, when ratings were calculated on paper rather than by computer, as most are today. Some older computer systems still in use today include: Jeff Sagarin's systems, the \"New York Times\" system, and the Dunkel Index, which dates back to 1929. Before the advent of the college football playoff, the Bowl Championship Series championship game participants were determined by a combination of expert polls and computer systems.\n\nSports ratings systems use a variety of methods for rating teams, but the most prevalent method is called a power rating. The power rating of a team is a calculation of the team's strength relative to other teams in the same league or division. The basic idea is to maximize the amount of transitive relations in a given data set due to game outcomes. For example, if A defeats B and B defeats C, then one can safely say that A>B>C.\n\nThere are obvious problems with basing a system solely on wins and losses. For example, if C defeats A, then an intransitive relation is established (A > B > C > A) and a ranking violation will occur if this is the only data available. Scenarios such as this happen fairly regularly in sports—for example, in the 2005 NCAA Division I-A football season, Penn State beat Ohio State, Ohio State beat Michigan, and Michigan beat Penn State. To address these logical breakdowns, rating systems usually consider other criteria such as the game's score and where the match was held (for example, to assess a home field advantage). In most cases though, each team plays a sufficient amount of other games during a given season, which lessens the overall effect of such violations.\n\nFrom an academic perspective, the use of linear algebra and statistics are popular among many of the systems' authors to determine their ratings. Some academic work is published in forums like the MIT Sloan Sports Analytics Conference, others in traditional statistics, mathematics, psychology, and computer science journals.\n\nIf sufficient \"inter-divisional\" league play is not accomplished, teams in an isolated division may be artificially propped up or down in the overall ratings due to a lack of correlation to other teams in the overall league. This phenomenon is evident in systems that analyze historical college football seasons, such as when the top Ivy League teams of the 1970s, like Dartmouth, were calculated by some rating systems to be comparable with accomplished powerhouse teams of that era such as Nebraska, USC, and Ohio State. This conflicts with the subjective opinion that claims that while good in their own right, they were not nearly as good as those top programs. However, this may be considered a \"pro\" by non-BCS teams in Division I-A college football who point out that ratings systems have proven that their top teams belong in the same strata as the BCS teams. This is evidenced by the 2004 Utah team that went undefeated in the regular season and earned a BCS bowl bid due to the bump in their overall BCS ratings via the computer ratings component. They went on to play and defeat the Big East Conference champion Pittsburgh in the 2005 Fiesta Bowl by a score of 35-7. A related example occurred during the 2006 NCAA Men's Basketball Tournament where George Mason were awarded an at-large tournament bid due to their regular season record and their RPI rating and rode that opportunity all the way to the Final Four.\n\nGoals of some rating systems differ from one another. For example, systems may be crafted to provide a perfect retrodictive analysis of the games played to-date, while others are predictive and give more weight to future trends rather than past results. This results in the potential for misinterpretation of rating system results by people unfamiliar with these goals; for example, a rating system designed to give accurate point spread predictions for gamblers might be ill-suited for use in selecting teams most deserving to play in a championship game or tournament.\n\nWhen two teams of equal quality play, the team at home tends to win more often. The size of the effect changes based on the era of play, game type, season length, sport, even number of time zones crossed. But across all conditions, \"simply playing at home increases the chances of winning.\" A win away from home is therefore seen more favorably than a win at home, because it was more challenging. Home advantage (which, for sports played on a pitch, is almost always called \"home field advantage\") is also based on the qualities of the individual stadium and crowd; the advantage in the NFL can be more than a 4-point difference from the stadium with the least advantage to the stadium with the most.\n\nStrength of schedule refers to the quality of a team's opponents. A win against an inferior opponent is usually seen less favorably than a win against a superior opponent. Often teams in the same league, who are compared against each other for championship or playoff consideration, have not played the same opponents. Therefore, judging their relative win-loss records is complicated.\n\nThe college football playoff committee uses a limited strength-of-schedule algorithm that only considers opponents' records and opponents' opponents' records (much like RPI).\n\nA key dichotomy among sports rating systems lies in the representation of game outcomes. Some systems store final scores as ternary discrete events: wins, draws, and losses. Other systems record the exact final game score, then judge teams based on margin of victory. Rating teams based on margin of victory is often criticized as creating an incentive for coaches to run up the score, an \"unsportsmanlike\" outcome.\n\nStill other systems choose a middle ground, reducing the marginal value of additional points as the margin of victory increases. Sagarin chose to clamp the margin of victory to a predetermined amount. Other approaches include the use of a decay function, such as a logarithm or placement on a cumulative distribution function.\n\nBeyond points or wins, some system designers choose to include more granular information about the game. Examples include time of possession of the ball, individual statistics, and lead changes. Data about weather, injuries, or \"throw-away\" games near season's end may affect game outcomes but are difficult to model. \"Throw-away games\" are games where teams have already earned playoff slots and have secured their playoff seeding before the end of the regular season, and want to rest/protect their starting players by benching them for remaining regular season games. This usually results in unpredictable outcomes and may skew the outcome of rating systems.\n\nTeams often shift their composition between and within games, and players routinely get injured. Rating a team is often about rating a specific collection of players. Some systems assume parity among all members of the league, such as each team being built from an equitable pool of players via a draft or free agency system as is done in many major league sports such as the NFL, MLB, NBA, and NHL. This is certainly not the case in collegiate leagues such as Division I-A football or men's and women's basketball.\n\nAt the beginning of a season, there have been no games from which to judge teams' relative quality. Solutions to the cold start problem often include some measure of the previous season, perhaps weighted by what percent of the team is returning for the new season. ARGH Power Ratings is an example of a system that uses multiple previous years plus a percentage weight of returning players.\n\nSeveral methods offer some permutation of traditional standings. This search for the \"real\" win-loss record often involves using other data, such as point differential or identity of opponents, to alter a team's record in a way that is easily understandable. Sportswriter Gregg Easterbrook created a measure of Authentic Games, which only considers games played against opponents deemed to be of sufficiently high quality. The consensus is that all wins are not created equal.\n\nPythagorean expectation, or Pythagorean projection, calculates a percentage based on the number of points a team has scored and allowed. Typically the formula involves the number of points scored, raised to some exponent, placed in the numerator. Then the number of points the team allowed, raised to the same exponent, is placed in the denominator and added to the value in the numerator. Football Outsiders has used\nThe resulting percentage is often compared to a team's true winning percentage, and a team is said to have \"overachieved\" or \"underachieved\" compared to the Pythagorean expectation. For example, Bill Barnwell calculated that before week 9 of the 2014 NFL season, the Arizona Cardinals had a Pythagorean record two wins lower than their real record. Bill Simmons cites Barnwell's work before week 10 of that season and adds that \"any numbers nerd is waving a “REGRESSION!!!!!” flag right now.\" In this example, the Arizona Cardinals' regular season record was 8-1 going into the 10th week of the 2014 season. The Pythagorean win formula implied a winning percentage of 57.5%, based on 208 points scored and 183 points allowed. Multiplied by 9 games played, the Cardinals' Pythagorean expectation was 5.2 wins and 3.8 losses. The team had \"overachieved\" at that time by 2.8 wins, derived from their actual 8 wins less the expected 5.2 wins, an increase of 0.8 overachieved wins from just a week prior.\n\nOriginally designed by Arpad Elo as a method for ranking chess players, several people have adapted the Elo rating system for team sports such as basketball, soccer and American football. For instance, Jeff Sagarin and FiveThirtyEight publish NFL football rankings using Elo methods. Elo ratings initially assign strength values to each team, and teams trade points based on the outcome of each game.\n\nResearchers like Matt Mills use Markov chains to model college football games, with team strength scores as outcomes. Algorithms like Google's PageRank have also been adapted to rank football teams.\n\n\nIn collegiate American football, the following people's systems were used to choose teams to play in the national championship game.\n\n\n"}
{"id": "36218534", "url": "https://en.wikipedia.org/wiki?curid=36218534", "title": "Starch production", "text": "Starch production\n\nStarch production is an isolation of starch from plant sources. It takes place in starch plants. Starch industry is a part of food processing which is using starch as a starting material for production of starch derivatives, hydrolysates, dextrins.\n\nAt first, the raw material for the preparation of the starch was wheat. Currently main starch sources are: \n\nThe production of potato starch comprises the steps such as delivery and unloading potatoes, cleaning, rasping of tubers, potato juice separation, starch extraction, starch milk rafination, dewatering of refined starch milk and starch drying.\n\nThe potato starch production supply chain varies significantly by region. For example, potato starch in Europe is produced from potatoes grown specifically for this purpose. However, in the US, potatoes are not grown for starch production and manufacturers must source raw material from food processor waste streams. The characteristics of these waste streams can vary significantly and require further processing by the US potato starch manufacturer to ensure the end-product functionality and specifications are acceptable.\n\nPotatoes are delivered to the starch plants via road or rail transport. Unloading of potatoes could be done in two ways: \n\nCoarsely cleaning of potatoes takes place during the transport of potatoes to the scrubber by channel. In addition, before the scrubber, straw and stones separators are installed. The main cleaning is conducted in scrubber (different kinds of high specialized machines are used). The remaining stones, sludge and light wastes are removed at this step. Water used for washing is then purified and recycled back into the process.\n\nMost often the rasping of potato tubers is carried out with a rotary grater. The purpose of this stage is disruption of cell walls, which therefore release the starch. In practice, potato cells are not entirely destroyed and part of the starch remains in the mash.\nPotato pulp rapidly turns dark, because tyrosine presented in the potato is oxidised by polyphenol oxidase, which is located in the cellular juice. Therefore, cellular juice must be separated as soon as possible.\n\nThis allows the recovery full-value protein from juice and reduces the onerousness of water juice as a sludge.\n\nAfter separation of potato juice the pulp is directed to the washing starch station, to isolate the starch. Most used are stream-oriented washers. In these machines pulp diluted with water is washed with a strong stream of water to flush out the milk starch. The mash smuggling with water is a waste product – dewatered potato pulp. Starch milk is contaminated by small fiber particles (potato tissue fragments) and the remaining components of the potato juice – that’s why it is called raw starch milk.\n\nRaw starch milk is purified in the refining process. This involves the removal of small fibers from the starch milk and then the removal of juice water and starch milk condensation. For this purpose, the screens and hydrocyclones are commonly used. Hydrocyclones due to the low output (approximately 0.3 cubic meter per hour) are connected in parallel and works as multihydrocyclones. For the starch milk desanding bihydrocyclones are used. In order to prevent enzymatic darkening of potato juice the chemical refining of starch is carried out using sulfurous acid. Refined starch milk has a density of about 22° Be, which is about 38% of starch.\n\nIt is a suspension of starch in water, which needs dewatering up to 20% of moisture.\nThis is equivalent to the moisture content of commercial starch when stored. High temperatures cannot be used in this process because of the danger of starch gelatinization which destroys granular structure. It may result in significant changes of the functional starch properties. Therefore, removal of excess water from milk shall be done only under conditions that prevent the gelatinization of starch.\n\nDewatering of refined starch milk is carried out in two stages. In the first stage the excess water is removed by means of a rotary vacuum filter. Secondly moist starch is dried, without starch pasting. For this purpose a pneumatic dryier is used. In this device moist starch (with water content 36 – 40%) is floating in strong and hot (160 °C) air flow and then dried during 2 – 3 seconds. Then, the starch is separated from hot air in cyclones. Due to short time of high temperature drying and intensive water evaporation from the starch granules, its surface is heated only to 40 °C.\n\nDried starch contains about 21% of water. During the pneumatic transport starch loses additional 1% of water.\nReceived starch is storing separately in silos, in jute bags (100 kg) or paper bags (50 kg). There are three kinds of starch: superior extra, superior and prima. Different sorts of starch depend on degree of purity and whiteness. The differences between them are in an acidity and content of mineral substances.\n\nTable 1. Potato starch production characteristic.\n\nThe water which is used in starch production (dirty water) for transportation and cleaning doesn’t have to be totally clean. That’s why clarifier usage enables application of closed cycle which noticeably reduces amount of cleaning water that is needed.\nOn the other hand, requirements for quality of technological water are the same as for drinking water (microbiological and chemical contamination). In addition, this type of water should contain low amounts of metals such as: Ca, Mg, Fe, Mn; which has bad influence on starch properties.\n\n- Potato juice is a liquid waste product separated from the potatoes pulp after the rasping, using centrifuges or decanters. It contains about 5% of dry substance, including about 2% valuable protein of the potato of high nutritional value, minerals, vitamins and other. Modern starch plants separate the juice from the mash.\n\n- Potato protein can be extrated from the juice by coagulation with heat at low pH. About 600 kg of coagulated protein from each 1000 kg of potato protein can be recovered this way.\nThe coagulated protein product contains about 80% protein (with the digestibility of about 90%), 2.5% minerals, 1.5% fat, 6% nitrogen-free substances and 10% water. Because of the full range of the exogenic amino acids the formulation is a valuable protein feed. The remaining potota Juice is used as fertilizer.\n\n- Potato pulp is a side product of washing the starch from the mash. It contains all non-starchy substances insoluble in water (fragments of wall of cells) fibres and bounded starch which cannot be mechanically separated from the blended parties of potato. The dry substance of the pulp contains 30% of starch, which makes it a good source for animal feed.\nThe pulp contains a lot of water, dry solids about 16%, which is inconvenient in transportation and storage. That’s why it is really often dried and dehydrated.\n\n- Juicy water is a liquid side product obtained after refining of starch milk. It is ten times diluted potato juice. As it is a sludge and it cannot be discharged to open water. It must be treated as waste water or is used as fertilizer.\n\nThe usage of raw starch is relatively small. Starch is mainly used as material both in the manufacture of food and non-food products. In food processing a lot of starch is converted to starch hydrolysates. Also it is used to receive puddings, jellies, desserts, caramel and other food products. In addition, starch is processed to modified starch in order to change material properties. The modified starches have a wide range of applications in many industries. Applications of starch in non-food sectors include: production of dextrin and adhesives, drilling fluids, biodegradable plastics, gypsum binders and many other. In addition modified starches are used as fillers, emulsion stabilizers, consistency modifiers etc. Other important fields of starch application are textiles, cosmetics, pharmaceuticals and paints.\n\n\nMieczysław Pałasiński, 2005. Technology of Carbohydrate Processing (in Polish). Polish Society of Food Technologists, Małopolska Branch, Kraków, p. 63 - 76.\n"}
{"id": "19989523", "url": "https://en.wikipedia.org/wiki?curid=19989523", "title": "TMRO", "text": "TMRO\n\nTMRO (pronounced \"tomorrow\", formerly known as Spacevidcast), is a network of live and on-demand HD webcasts about technology, science, space, and human society. TMRO was established as Spacevidcast in 2008 by husband and wife team Benjamin and Cariann Higginbotham.\n\n\"TMRO\" stated goal is \n\nIn contrast to most space-related podcasts, \"TMRO\" is produced independently and is not associated with any governmental or private organization, which sets it apart from other shows like \"NASA Edge\" or \"Hubblecast\". In 2011 the show hosts were hired by SpaceX, however there is no tie between SpaceX and \"TMRO\".\n\nIn addition to traditional episodes, \"TMRO\" also offers some audio and text content as well as content provided through the crowdfunding platform Patreon, including Google Hangouts and early access to \"TMRO After Dark\". There is also a \"TMRO\" subreddit for viewers to discuss space and show topics. The program was awarded the Best Presentation of Space award during NewSpace 2010.\n\nThe show is divided into an opener and three main segments. The first segment is Space News, discussing launches, technology, and spaceflight events that have occurred since the previous episode. Podcaster Michael Clark often talks about SpaceX news and provides input on other space topics. The second segment includes that episode's featured topic, which may be an interview or a discussion on a particular topic. During the third segment, viewer comments relating to the previous episode are read and discussed.\n\nTMRO Space was the original TMRO (and Spacevidcast) show before new shows were added.\nIn 2015 TMRO began producing Space Pods, which are short videos about various space-related topics. Space Pods are typically presented by either Michael Clark or Jared Head, with others occasionally presenting, including Benjamin Higginbotham and Tim Dodd. Lisa Stojanovski and Ariel Waldman formerly hosted Space Pods.\n\nTMRO Science was launched with once monthly shows hosted by Lisa Stojanovski, Athena Brensberger, and Jade Kim.\nTMRO Cities premiered with one episode in 2017, but was immediately placed on hiatus.\n\"Spacevidcast\" has covered a number of live events such as Shuttle and Soyuz launches, the Phoenix Mars landing, the launch of Jules Verne ATV, the GLAST Space Telescope, Chandrayaan-1, Shenzhou 7, the Kepler Mission as well as commercial satellite launches. The website was one of many outlets to broadcast a live feed of the lunar impact by NASA's LCROSS spacecraft and participated in Guy Laliberté's \"Poetic Social Mission Event: Moving Stars and Earth for Water\". These events are usually summed up in short daily clips dedicated to one or two events.\n\nAdditionally, \"Spacevidcast\" streamed NASA TV with an accompanying chat room through UStream live 24/7 with an HD video feed during launches and other significant events.\n\nThe \"Spacevidcast\" team was the official event webcaster for the X Prize Foundation's 2008 Northrop Grumman Lunar Lander Challenge. \"Spacevidcast\" streamed continuously from Las Cruces, New Mexico on October 24 and 25. The hosts were on site to interview team leaders and VIP guests. The podcast was also the only media outlet to cover the \"Rocket Racing League at Spaceport America\" announcement by the governor of New Mexico, Bill Richardson.\n\n\"Spacevidcast\" was the exclusive webcaster of the 2009 International Space Development Conference, an annual event hosted and organized by the National Space Society. As part of its coverage, the hosts provided live video webcasts of the main presentations as well as an option for viewers to send questions via the chat room. The videos were also made available on demand. In addition to direct coverage of the conference, \"Spacevidcast\" also broadcast a regular weekly show from the ISDC 2009 setting and the hosts participated in a panel discussion on May 29 entitled \"Space Media 2.0\".\n\n\"Spacevidcast\" was the exclusive webcaster of the 2010 SpaceUp San Diego unconference, the first major space BarCamp-like event hosted by the San Diego Space Society. As part of their coverage, the hosts provided live video webcasts of many of the presentations as well as much of the Ignite talks. The videos are now available on demand. The quality of these videos is low only because of a lack of internet connections at the conference and spotty cell phone coverage.\n\n\"Spacevidcast\" was the exclusive webcaster of the 2010 Space Frontier Foundation NewSpace Conference, a yearly conference for NewSpace leaders to meet and organize the future of the NewSpace movement. Benjamin and Cariann provided live coverage of each of the talks during the weekend. The videos are now available on demand (some are Epic subscriber-only).\n\n\"Spacevidcast\" was the exclusive webcaster of the 2010 SpaceUp Washington D.C. unconference, the second space BarCamp-like event hosted by Evadot founder Michael Doornbos. As part of their coverage, the hosts provided live video webcasts of many of the presentations and Ignite talks. The videos are now all available to watch on their YouTube channel.\n\n\"Spacevidcast\" is the exclusive webcaster of the 2011 SpaceUp San Diego unconference, the third space BarCamp-like event hosted by the San Diego Space Society and the first repeat SpaceUp, held on February 12–13, 2011. Cariann Higginbotham, co-host of \"Spacevidcast\", is attending the event and, assisted by corporate and private sponsors, will be providing live video webcasts of many of the presentations and T-5 talks (similar to Ignite talks). The videos will be available to watch on their YouTube channel and will be viewable live on their website.\n\n\"Spacevidcast\" is the exclusive webcaster of the 2011 SpaceUp Houston unconference, the fourth space BarCamp-like event hosted by the Clear Lake NSS/Moon Society and the Lunar and Planetary Institute on February 12–13, 2011. Benjamin Higginbotham, co-host of \"Spacevidcast\", is attending the event and will be providing live video webcasts of many of the presentations and demonstrations. The videos will be available to watch on their YouTube channel and will be viewable live on their website.\n\n\"Spacevidcast\" was one of the first sources to get High Definition Space Shuttle launches online. These shows progressed until \"Spacevidcast\" was granted media access to the Kennedy Space Center Press site for STS-132 through the end of the program. The \"Spacevidcast\" shuttle coverage was marked as Wired Magazine's three best ways to watch a shuttle launch online.\n\nDuring season 7, \"TMRO\" left YouTube streaming and moved to Livestream for more stable live streaming.\n\nDuring season 7, \"TMRO\" transitioned from its previous donor on-demand service, called Epic, to the crowdfunding website Patreon.\n\n"}
{"id": "1117979", "url": "https://en.wikipedia.org/wiki?curid=1117979", "title": "VSEPR theory", "text": "VSEPR theory\n\nValence shell electron pair repulsion (VSEPR) theory is a model used in chemistry to predict the geometry of individual molecules from the number of electron pairs surrounding their central atoms. It is also named the Gillespie-Nyholm theory after its two main developers, Ronald Gillespie and Ronald Nyholm. The acronym \"VSEPR\" is pronounced either \"\"ves\"-pur\" or \"vuh-\"seh\"-per\".\n\nThe premise of VSEPR is that the valence electron pairs surrounding an atom tend to repel each other and will, therefore, adopt an arrangement that minimizes this repulsion, thus determining the molecule's geometry. Gillespie has emphasized that the electron-electron repulsion due to the Pauli exclusion principle is more important in determining molecular geometry than the electrostatic repulsion.\n\nVSEPR theory is based on observable electron density rather than mathematical wave functions and hence unrelated to orbital hybridisation, although both address molecular shape. While it is mainly qualitative, VSEPR has a quantitative basis in quantum chemical topology (QCT) methods such as the electron localization function (ELF) and the quantum theory of atoms in molecules (QTAIM).\n\nThe idea of a correlation between molecular geometry and number of valence electron pairs (both shared and unshared pairs) was originally proposed in 1939 by Ryutaro Tsuchida in Japan, and was independently presented in a Bakerian Lecture in 1940 by Nevil Sidgwick and Herbert Powell of the University of Oxford. In 1957, Ronald Gillespie and Ronald Sydney Nyholm of University College London refined this concept into a more detailed theory, capable of choosing between various alternative geometries.\n\nIn recent years, VSEPR theory has been criticized as an outdated model from the standpoint of both scientific accuracy and pedagogical value. In particular, the equivalent lone pairs of water and carbonyl compounds in VSEPR theory neglect fundamental differences in the symmetries (σ vs. π) of molecular orbitals and natural bond orbitals that correspond to them, a difference that is sometimes chemically significant. Furthermore, there is little evidence, computational or experimental, proposing that lone pairs are \"bigger\" than bonding pairs. It has been suggested that Bent's rule is capable of replacing VSEPR as a simple model for explaining molecular structure. Nevertheless, VSEPR theory captures many of the essential features of the structure and electron distribution of simple molecules, and most undergraduate general chemistry courses continue to teach it.\n\nVSEPR theory is used to predict the arrangement of electron pairs around non-hydrogen atoms in molecules, especially simple and symmetric molecules, where these key, central atoms participate in bonding to two or more other atoms; the geometry of these key atoms and their non-bonding electron pairs in turn determine the geometry of the larger whole.\n\nThe number of electron pairs in the valence shell of a central atom is determined after drawing the Lewis structure of the molecule, and expanding it to show all bonding groups and lone pairs of electrons. In VSEPR theory, a double bond or triple bond are treated as a single bonding group. The sum of the number of atoms bonded to a central atom and the number of lone pairs formed by its nonbonding valence electrons is known as the central atom's steric number.\n\nThe electron pairs (or groups if multiple bonds are present) are assumed to lie on the surface of a sphere centered on the central atom and tend to occupy positions that minimize their mutual repulsions by maximizing the distance between them. The number of electron pairs (or groups), therefore, determines the overall geometry that they will adopt. For example, when there are two electron pairs surrounding the central atom, their mutual repulsion is minimal when they lie at opposite poles of the sphere. Therefore, the central atom is predicted to adopt a \"linear\" geometry. If there are 3 electron pairs surrounding the central atom, their repulsion is minimized by placing them at the vertices of an equilateral triangle centered on the atom. Therefore, the predicted geometry is \"trigonal\". Likewise, for 4 electron pairs, the optimal arrangement is \"tetrahedral\".\n\nThe overall geometry is further refined by distinguishing between \"bonding\" and \"nonbonding\" electron pairs. The bonding electron pair shared in a sigma bond with an adjacent atom lies further from the central atom than a nonbonding (lone) pair of that atom, which is held close to its positively charged nucleus. VSEPR theory therefore views repulsion by the lone pair to be greater than the repulsion by a bonding pair. As such, when a molecule has 2 interactions with different degrees of repulsion, VSEPR theory predicts the structure where lone pairs occupy positions that allow them to experience less repulsion. Lone pair–lone pair (lp–lp) repulsions are considered stronger than lone pair–bonding pair (lp–bp) repulsions, which in turn are considered stronger than bonding pair–bonding pair (bp–bp) repulsions, distinctions that then guide decisions about overall geometry when 2 or more non-equivalent positions are possible. For instance, when 5 valence electron pairs surround a central atom, they adopt a trigonal bipyramidal molecular geometry with two collinear \"axial\" positions and three \"equatorial\" positions. An electron pair in an axial position has three close equatorial neighbors only 90° away and a fourth much farther at 180°, while an equatorial electron pair has only two adjacent pairs at 90° and two at 120°. The repulsion from the close neighbors at 90° is more important, so that the axial positions experience more repulsion than the equatorial positions; hence, when there are lone pairs, they tend to occupy equatorial positions as shown in the diagrams of the next section for steric number five.\n\nThe difference between lone pairs and bonding pairs may also be used to rationalize deviations from idealized geometries. For example, the HO molecule has four electron pairs in its valence shell: two lone pairs and two bond pairs. The four electron pairs are spread so as to point roughly towards the apices of a tetrahedron. However, the bond angle between the two O–H bonds is only 104.5°, rather than the 109.5° of a regular tetrahedron, because the two lone pairs (whose density or probability envelopes lie closer to the oxygen nucleus) exert a greater mutual repulsion than the two bond pairs.\n\nAn advanced-level explanation replaces the above distinction with two rules:\n\nThe \"AXE method\" of electron counting is commonly used when applying the VSEPR theory. The electron pairs around a central atom are represented by a formula AXE, where \"A\" represents the central atom and always has an implied subscript one. Each \"X\" represents a ligand (an atom bonded to A). Each \"E\" represents a \"lone pair\" of electrons on the central atom. The total number of \"X\" and \"E\" is known as the steric number. For example in a molecule AXE, the atom A has a steric number of 5.\n\nBased on the steric number and distribution of \"X\"s and \"E\"s, VSEPR theory makes the predictions in the following tables. Note that the geometries are named according to the atomic positions only and not the electron arrangement. For example, the description of AXE as a bent molecule means that the three atoms AX are not in one straight line, although the lone pair helps to determine the geometry.\n\nWhen the substituent (X) atoms are not all the same, the geometry is still approximately valid, but the bond angles may be slightly different from the ones where all the outside atoms are the same. For example, the double-bond carbons in alkenes like CH are AXE, but the bond angles are not all exactly 120°. Likewise, SOCl is AXE, but because the X substituents are not identical, the X–A–X angles are not all equal.\n\nAs a tool in predicting the geometry adopted with a given number of electron pairs, an often used physical demonstration of the principle of minimal electron pair repulsion utilizes inflated balloons. Through handling, balloons acquire a slight surface electrostatic charge that results in the adoption of roughly the same geometries when they are tied together at their stems as the corresponding number of electron pairs. For example, five balloons tied together adopt the trigonal bipyramidal geometry, just as do the five bonding pairs of a PCl molecule (AX) or the two bonding and three non-bonding pairs of a XeF molecule (AXE). The molecular geometry of the former is also trigonal bipyramidal, whereas that of the latter is linear.\n\nPossible geometries for steric numbers of 10, 11, 12, or 14 are bicapped square antiprismatic (or bicapped dodecadeltahedral), octadecahedral, icosahedral, and bicapped hexagonal antiprismatic, respectively. No compounds with steric numbers this high involving monodentate ligands exist, and those involving multidentate ligands can often be analysed more simply as complexes with lower steric numbers when some multidentate ligands are treated as a unit.\n\nThe methane molecule (CH) is tetrahedral because there are four pairs of electrons. The four hydrogen atoms are positioned at the vertices of a tetrahedron, and the bond angle is cos(−) ≈ 109° 28′. This is referred to as an AX type of molecule. As mentioned above, A represents the central atom and X represents an outer atom.\n\nThe ammonia molecule (NH) has three pairs of electrons involved in bonding, but there is a lone pair of electrons on the nitrogen atom. It is not bonded with another atom; however, it influences the overall shape through repulsions. As in methane above, there are four regions of electron density. Therefore, the overall orientation of the regions of electron density is tetrahedral. On the other hand, there are only three outer atoms. This is referred to as an AXE type molecule because the lone pair is represented by an E. By definition, the molecular shape or geometry describes the geometric arrangement of the atomic nuclei only, which is trigonal-pyramidal for NH.\n\nSteric numbers of 7 or greater are possible, but are less common. The steric number of 7 occurs in iodine heptafluoride (IF); the base geometry for a steric number of 7 is pentagonal bipyramidal. The most common geometry for a steric number of 8 is a square antiprismatic geometry. Examples of this include the octacyanomolybdate () and octafluorozirconate () anions. The nonahydridorhenate ion () in potassium nonahydridorhenate is a rare example of a compound with a steric number of 9, which has a tricapped trigonal prismatic geometry.\n\nThere are groups of compounds where VSEPR fails to predict the correct geometry.\n\nThe gas phase structures of the triatomic halides of the heavier members of group 2, (i.e., calcium, strontium and barium halides, MX), are not linear as predicted but are bent, (approximate X–M–X angles: CaF, 145°; SrF, 120°; BaF, 108°; SrCl, 130°; BaCl, 115°; BaBr, 115°; BaI, 105°). It has been proposed by Gillespie that this is caused by interaction of the ligands with the electron core of the metal atom, polarising it so that the inner shell is not spherically symmetric, thus influencing the molecular geometry. Ab initio calculations have been cited to propose that contributions from d orbitals in the shell below the valence shell are responsible. Disilynes are also bent, despite having no lone pairs.\n\nOne example of the AXE geometry is molecular lithium oxide, LiO, a linear rather than bent structure, which is ascribed to its bonds being essentially ionic and the strong lithium-lithium repulsion that results. Another example is O(SiH) with an Si–O–Si angle of 144.1°, which compares to the angles in ClO (110.9°), (CH)O (111.7°), and N(CH) (110.9°). Gillespie and Robinson rationalize the Si–O–Si bond angle based on the observed ability of a ligand's lone pair to most greatly repel other electron pairs when the ligand electronegativity is greater than or equal to that of the central atom. In O(SiH), the central atom is more electronegative, and the lone pairs are less localized and more weakly repulsive. The larger Si–O–Si bond angle results from this and strong ligand-ligand repulsion by the relatively large -SiH ligand. Burford et al showed through X-ray diffraction studies that ClAl–O–PCl has a linear Al–O–P bond angle and is therefore a non-VSEPR molecule.\n\nSome AXE molecules, e.g. xenon hexafluoride (XeF) and the Te(IV) and Bi(III) anions, , , , and , are octahedra, rather than pentagonal pyramids, and the lone pair does not affect the geometry to the degree predicted by VSEPR. One rationalization is that steric crowding of the ligands allows little or no room for the non-bonding lone pair; another rationalization is the inert pair effect. Similarly, the octafluoroxenate ion () in nitrosonium octafluoroxenate(VI) is a square antiprism and not a distorted square antiprism (as predicted by VSEPR theory for an AXE molecule), despite having a lone pair.\n\nMany transition metal compounds have unusual geometries, which can be ascribed to ligand bonding interaction with the d subshell and to absence of valence shell lone pairs. Gillespie suggested that this interaction can be weak or strong. Weak interaction is dealt with by the Kepert model, while strong interaction produces bonding pairs that also occupy the respective antipodal points of the sphere. This is similar to predictions based on sd hybrid orbitals using the VALBOND theory. The repulsion of these bidirectional bonding pairs leads to a different prediction of shapes.\n\nThe Kepert model cannot explain the formation of square planar complexes or distorted structures.\n\nThe VSEPR theory can be extended to molecules with an odd number of electrons by treating the unpaired electron as a \"half electron pair\" — for example, Gillespie and Nyholm suggested that the decrease in the bond angle in the series (180°), NO (134°), (115°) indicates that a given set of bonding electron pairs exert a weaker repulsion on a single non-bonding electron than on a pair of non-bonding electrons. In effect, they considered nitrogen dioxide as an AXE molecule, with a geometry intermediate between and . Similarly, chlorine dioxide (ClO) is an AXE molecule, with a geometry intermediate between and .\n\nFinally, the methyl radical (CH) is predicted to be trigonal pyramidal like the methyl anion (), but with a larger bond angle (as in the trigonal planar methyl cation ()). However, in this case, the VSEPR prediction is not quite true, as CH is actually planar, although its distortion to a pyramidal geometry requires very little energy.\n\n\n"}
{"id": "4842312", "url": "https://en.wikipedia.org/wiki?curid=4842312", "title": "X-ray absorption near edge structure", "text": "X-ray absorption near edge structure\n\nX-ray absorption near edge structure (XANES), also known as near edge X-ray absorption fine structure (NEXAFS), is a type of absorption spectroscopy that indicates the features in the X-ray absorption spectra (XAS) of condensed matter due to the photoabsorption cross section for electronic transitions from an atomic core level to final states in the energy region of 50–100 eV above the selected atomic core level ionization energy, where the wavelength of the photoelectron is larger than the interatomic distance between the absorbing atom and its first neighbour atoms.\n\nBoth XANES and NEXAFS are acceptable terms for the same technique. XANES name was invented in 1980 by Antonio Bianconi to indicate strong absorption peaks in X-ray absorption spectra in condensed matter due to multiple scattering resonances above the ionization energy. The name NEXAFS was introduced in 1983 by Jo Stohr and is synonymous with XANES, but is generally used when applied to surface and molecular science.\n\nThe fundamental phenomenon underlying XANES is the absorption of an x-ray photon by condensed matter with the formation of many body excited states characterized by a core hole in a selected atomic core level. (see the first Figure). In the single particle theory approximation, the system is separated into one electron in the core levels of the selected atomic species of the system and N-1 passive electrons. In this approximation the final state is described by a core hole in the atomic core level and an excited photoelectron. The final state has a very short life time because of the short life-time of the core hole and the short mean free path of the excited photoelectron with kinetic energy in the range around 20-50 eV. The core hole is filled either via an Auger process or by capture of an electron from another shell followed by emission of a fluorescent photon. The difference between NEXAFS and traditional photoemission experiments is that in photoemission, the initial photoelectron itself is measured, while in NEXAFS the fluorescent photon or Auger electron or an inelastically scattered photoelectron may also be measured. The distinction sounds trivial but is actually significant: in photoemission the final state of the emitted electron captured in the detector must be an extended, free-electron state. By contrast in NEXAFS the final state of the photoelectron may be a bound state such as an exciton since the photoelectron itself need not be detected. The effect of measuring fluorescent photons, Auger electrons, and directly emitted electrons is to sum over all possible final states of the photoelectrons, meaning that what NEXAFS measures is the total joint density of states of the initial core level with all final states, consistent with conservation rules. The distinction is critical because in spectroscopy final states are more susceptible to many-body effects than initial states, meaning that NEXAFS spectra are more easily calculable than photoemission spectra. Due to the summation over final states, various sum rules are helpful in the interpretation of NEXAFS spectra. When the x-ray photon energy resonantly connects a core level with a narrow final state in a solid, such as an exciton, readily identifiable characteristic peaks will appear in the spectrum. These narrow characteristic spectral peaks give the NEXAFS technique a lot of its analytical power as illustrated by the B 1s π* exciton shown in the second Figure.\n\nSynchrotron radiation has a natural polarization that can be utilized to great advantage in NEXAFS studies. The commonly studied molecular adsorbates have sigma and pi bonds that may have a particular orientation on a surface. The angle dependence of the x-ray absorption tracks the orientation of resonant bonds due to dipole selection rules.\n\nSoft x-ray absorption spectra are usually measured either through the \"fluorescent yield,\" in which emitted photons are monitored, or \"total electron yield,\" in which the sample is connected to ground through an ammeter and the neutralization current is monitored. Because NEXAFS measurements require an intense tunable source of soft x-rays, they are performed at synchrotrons. Because soft x-rays are absorbed by air, the synchrotron radiation travels from the ring in an evacuated beam-line to the end-station where the specimen to be studied is mounted. Specialized beam-lines intended for NEXAFS studies often have additional capabilities such as heating a sample or exposing it to a dose of reactive gas.\n\nIn the absorption edge region of metals, the photoelectron is excited to the first unoccupied level above the Fermi level. Therefore, its mean free path in a pure single crystal at zero temperature is as large as infinite, and it remains very large, increasing the energy of the final state up to about 5 eV above the Fermi level. Beyond the role of the unoccupied density of states and matrix elements in single electron excitations, many-body effects appear as an \"infrared singularity\" at the absorption threshold in metals.\n\nIn the absorption edge region of insulators the photoelectron is excited to the first unoccupied level above the chemical potential but the unscreened core hole forms a localized bound state called core exciton.\n\nThe fine structure in the x-ray absorption spectra in the high energy range extending from about 150 eV beyond the ionization potential is a powerful tool to determine the atomic pair distribution (i.e. interatomic distances) with a time scale of about 10 s.\nIn fact the final state of the excited photoelectron in the high kinetic energy range (150-2000 eV ) is determined only by single backscattering events due to the low amplitude photoelectron scattering.\n\nIn the NEXAFS region, starting about 5 eV beyond the absorption threshold, because of the low kinetic energy range (5-150 eV) the photoelectron backscattering amplitude by neighbor atoms is very large so that multiple scattering events become dominant in the NEXAFS spectra.\n\nThe different energy range between NEXAFS and EXAFS can be also explained in a very simple manner by the comparison between the photoelectron wavelength formula_1 and the interatomic distance of the photoabsorber-backscatterer pair. The photoelectron kinetic energy is connected with the wavelength formula_1 by the following relation:\n\nwhich means that for high energy the wavelength is shorter than interatomic distances and hence the NEXAFS region corresponds to a single scattering regime; while for lower E, formula_1 is larger than interatomic distances and the XANES region is associated with a multiple scattering regime.\n\nThe absorption peaks of NEXAFS spectra are determined by multiple scattering resonances of the photoelectron excited at the atomic absorption site and scattered by neighbor atoms.\nThe local character of the final states is determined by the short photoelectron mean free path, that is strongly reduced (down to about 0.3 nm at 50 eV) in this energy range because of inelastic scattering of the photoelectron by electron-hole excitations (excitons) and collective electronic oscillations of the valence electrons called plasmons.\n\nThe great power of NEXAFS derives from its elemental specificity. Because the various elements have different core level energies, NEXAFS permits extraction of the signal from a surface monolayer or even a single buried layer in the presence of a huge background signal. Buried layers are very important in engineering applications, such as magnetic recording media buried beneath a surface lubricant or dopants below an electrode in an integrated circuit. Because NEXAFS can also determine the chemical state of elements which are present in bulk in minute quantities, it has found widespread use in environmental chemistry and geochemistry. The ability of NEXAFS to study buried atoms is due to its integration over all final states including inelastically scattered electrons, as opposed to photoemission and Auger spectroscopy, which study atoms only with a layer or two of the surface.\n\nMuch chemical information can be extracted from the NEXAFS region: formal valence (very difficult to experimentally determine in a nondestructive way); coordination environment (e.g., octahedral, tetrahedral coordination) and subtle geometrical distortions of it.\n\nTransitions to bound vacant states just above the Fermi level can be seen. Thus NEXAFS spectra can be used as a probe of the unoccupied band structure of a material.\n\nThe near-edge structure is characteristic of an environment and valence state hence one of its more common uses is in fingerprinting: if you have a mixture of sites/compounds in a sample you can fit the measured spectra with a linear combinations of NEXAFS spectra of known species and determine the proportion of each site/compound in the sample. One example of such a use is the determination of the oxidation state of the plutonium in the soil at Rocky Flats.\n\nThe acronym XANES was first used in 1980 during interpretation of multiple scattering resonances spectra measured at the Stanford Synchrotron Radiation Laboratory (SSRL) by A. Bianconi. In 1982 the first paper on the application of XANES for determination of local structural geometrical distortions using multiple scattering theory was published by A. Bianconi, P. J. Durham and J. B. Pendry. In 1983 the first NEXAFS paper examining molecules adsorbed on surfaces appeared. The first XAFS paper, describing the intermediate region between EXAFS and XANES, appeared in 1987.\n\nADF Calculation of NEXAFS using spin-orbit coupling TDDFT or the Slater-TS method.\n\nFDMNES Calculation of NEXAFS using finite difference method and full multiple scattering theory.\n\nFEFF8 Calculation of NEXAFS using full multiple scattering theory.\n\nMXAN NEXAFS fitting using full multiple scattering theory.\n\nFitIt NEXAFS fitting using multidimensional interpolation approximation.\n\nPARATEC NEXAFS calculation using plane-wave pseudopotential approach\n\nWIEN2k NEXAFS calculation on the basis of full-potential (linearized) augmented plane-wave approach.\n\n\n\n"}
