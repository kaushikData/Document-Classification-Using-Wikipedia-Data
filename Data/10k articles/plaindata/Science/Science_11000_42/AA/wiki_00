{"id": "2575969", "url": "https://en.wikipedia.org/wiki?curid=2575969", "title": "ALICE experiment", "text": "ALICE experiment\n\nALICE (A Large Ion Collider Experiment) is one of seven detector experiments at the Large Hadron Collider at CERN. The other six are: ATLAS, CMS, TOTEM, LHCb, LHCf and MoEDAL.\n\nALICE is optimized to study heavy-ion (Pb-Pb nuclei) collisions at a centre of mass energy of 2.76 TeV per nucleon pair. The resulting temperature and energy density are expected to be high enough to produce quark–gluon plasma, a state of matter wherein quarks and gluons are freed. Similar conditions are believed to have existed a fraction of the second after the Big Bang before quarks and gluons bound together to form hadrons and heavier particles.\n\nALICE is focusing on the physics of strongly interacting matter at extreme energy densities. The existence of the quark–gluon plasma and its properties are key issues in quantum chromodynamics for understanding color confinement and chiral symmetry restoration. Recreating this primordial form of matter and understanding how it evolves is expected to shed light on questions about how matter is organized, the mechanism that confines quarks and gluons and the nature of strong interactions and how they result in generating the bulk of the mass of ordinary matter.\n\nQuantum chromodynamics (QCD) predicts that at sufficiently high energy densities there will be a phase transition from conventional hadronic matter, where quarks are locked inside nuclear particles, to a plasma of deconfined quarks and gluons. The reverse of this transition is believed to have taken place when the universe was just 10 s old, and may still play a role today in the hearts of collapsing neutron stars or other astrophysical objects.\n\nThe idea of building a dedicated heavy-ion detector for the LHC was first aired at the historic Evian meeting \"Towards the LHC experimental Programme\" in March 1992. From the ideas presented there, the ALICE collaboration was formed and in 1993, a LoI was submitted.\n\nALICE was first proposed as a central detector in 1993 and later complemented by an additional forward muon spectrometer designed in 1995. In 1997, ALICE received the green light from the LHC Committee to proceed towards final design and construction.\n\nThe first ten years were spent on design and an extensive R&D effort. Like for all other LHC experiments, it became clear from the outset that also the challenges of heavy ion physics at LHC could not be really met (nor paid for) with existing technology. Significant advances, and in some cases a technological break-through, would be required to build on the ground what physicists had dreamed up on paper for their experiments. The initially very broad and later more focused, well organised and well supported R&D effort, which was sustained over most of the 1990s, has led to many evolutionary and some revolutionary advances in detectors, electronics and computing.\n\nDesigning a dedicated heavy-ion experiment in the early '90s for use at the LHC some 15 years later posed some daunting challenges. The detector had to be general purpose - able to measure most signals of potential interest, even if their relevance may only become apparent later - and flexible, allowing additions and modifications along the way as new avenues of investigation would open up. In both respects ALICE did quite well, as it included a number of observables in its initial menu whose importance only became clear later. Various major detection system were added, from the muon spectrometer in 1995, the transition radiation detectors in 1999 to a large jet calorimeter added in 2007.\n\nALICE recorded data from the first lead-lead collisions at the LHC in 2010. Data sets taken during heavy-ion periods in 2010 and 2011 as well as proton-lead data from 2013 have provided an excellent basis for an in-depth look at the physics of quark–gluon plasma.\n\nSearches for Quark Gluon plasma and a deeper understanding of the QCD started at CERN and Brookhaven with lighter ions in the 1980s. Today's programme at these laboratories has moved on to ultrarelativistic collisions of heavy ions, and is just reaching the energy threshold at which the phase transition is expected to occur. The LHC, with a centre-of-mass energy around 5.5 TeV/nucleon, will push the energy reach even further.\n\nDuring head-on collisions of lead ions at the LHC, hundreds of protons and neutrons smash into one another at energies of upwards of a few TeVs. Lead ions are accelerated to more than 99.9999% of the speed of light and collisions at the LHC are 100 times more energetic than those of protons - heating up matter in the interaction point to a temperature almost 100,000 times higher than the temperature in the core of the sun.\n\nWhen the two lead nuclei slam into each other, matter undergoes a transition to form for a brief instant a droplet of primordial matter, the so-called quark–gluon plasma which is believed to have filled the universe a few microseconds after the Big Bang.\n\nThe quark–gluon plasma is formed as protons and neutrons \"melt\" into their elementary constituents, quarks and gluons become asymptotically free. The droplet of QGP instantly cools, and the individual quarks and gluons (collectively called partons) recombine into a blizzard of ordinary matter that speeds away in all directions. The debris contains particles such as pions and kaons, which are made of a quark and an antiquark; protons and neutrons, made of three quarks; and even copious antiprotons and antineutrons, which may combine to form the nuclei of antiatoms as heavy as helium. Much can be learned by studying the distribution and energy of this debris.\n\nThe Large Hadron Collider smashed its first lead ions in 2010, on 7 November at around 12:30 a.m. CET.\n\nThe first collisions in the center of the ALICE, ATLAS and CMS detectors took place less than 72 hours after the LHC ended its first run of protons and switched to accelerating lead-ion beams. Each lead nucleus contains 82 protons, and the LHC accelerates each proton to an energy of 3.5 TeV, thus resulting in an energy of 287 TeV per beam, or a total collision energy of 574 TeV.\n\nUp to 3,000 charged particles were emitted from each collision, shown here as lines radiating from the collision point. The colors of the lines indicate how much energy each particle carried away from the collision.\n\nIn 2013, the LHC collided protons with lead ions for the LHC's first physics beams of 2013. The experiment was conducted by counter-rotating beams of \"protons\" and \"lead ions\", and begun with centred orbits with different revolution frequencies, and then separately ramped to the accelerator's maximum collision energy.\n\nThe first lead-proton run at the LHC lasted for one month and data help ALICE physicists to decouple the effects of the plasma from effects that stem from cold nuclear matter effects and shed more light on the study of the Quark-Gluon plasma.\n\nIn the case of lead-lead collisions, the configurations of the quarks and gluons that make up the protons and neutrons of the incoming lead nucleus can be somewhat different of those in the incoming protons. In order to study if part of the effects we see when comparing lead-lead and proton-proton collisions is due to this configuration difference rather than the formation of the plasma. Proton-lead collisions are an ideal tool for this study.\n\nA key design consideration of ALICE is the ability to study QCD and quark (de)confinement under these extreme conditions. This is done by using particles, created inside the hot volume as it expands and cools down, that live long enough to reach the sensitive detector layers situated around the interaction region. ALICE's physics programme relies on being able to identify all of them, i.e. to determine if they are electrons, photons, pions, etc. and to determine their charge. This involves making the most of the (sometimes slightly) different ways that particles interact with matter.\n\nIn a \"traditional\" experiment, particles are identified or at least assigned to families (charged or neutral hadrons), by the characteristic signatures they leave in the detector. The experiment is divided into a few main components and each component tests a specific set of particle properties. These components are stacked in layers and the particles go through the layers sequentially from the collision point outwards: first a tracking system, then an electromagnetic (EM) and a hadronic calorimeter and finally a muon system. The detectors are embedded in a magnetic field in order to bend the tracks of charged particles for momentum and charge determination. This method for particle identification works well only for certain particles, and is used for example by the large LHC experiments ATLAS and CMS. However, this technique is not suitable for hadron identification as it doesn't allow distinguishing the different charged hadrons that are produced in Pb-Pb collisions.\n\nIn order to identify all the particles that are coming out of the system of the QGP ALICE is using a set of 18 detectors that give information about the mass, the velocity and the electrical sign of the particles.\n\nAn ensemble of cylindrical barrel detectors that surround the nominal interaction point is used to track all the particles that fly out of the hot, dense medium. The Inner Tracking System(ITS) (consisting of three layers of detectors: Silicon Pixel Detector(SPD), Silicon Drift Detector(SDD), Silicon Strip Detector(SSD)), the Time Projection Chamber(TPC) and the Transition Radiation Detector(TRD) measure at many points the passage of each particle carrying an electric charge and give precise information about the particle's trajectory. The ALICE barrel tracking detectors are embedded in a magnetic field of 0.5 Tesla produced by a huge magnetic solenoid bending the trajectories of the particles. From the curvature of the tracks one can derive their momentum. The ITS is so precise that particles which are generated by the decay of other particles with a long(~.1 mm before decay) life time can be identified by seeing that they do not originate from the point where the interaction has taken place (the \"vertex\" of the event) but rather from a point at a distance of as small as a tenth of a millimeter. This allows us to measure, for example, bottom quarks which decay into a relatively long-lived B-meson through \"topological\" cuts.\n\nThe short-living heavy particles cover a very small distance before decaying. This system aims at identifying these phenomena of decay by measuring the location where it occurs with a precision of a tenth of millimetre.\n\nThe Inner Tracking System (ITS) consists of six cylindrical layers of silicon detectors. The layers surround the collision point and measure the properties of the particles emerging from the collisions, pin-pointing their position of passage to a fraction of a millimetre. With the help of the ITS, particles containing heavy quarks (charm and beauty) can be identified by reconstructing the coordinates at which they decay.\n\nITS layers (counting from the interaction point):\n\nThe ITS was inserted at the heart of the ALICE experiment in March 2007 following a large phase of R&D. Using the smallest amounts of the lightest material, the ITS has been made as lightweight and delicate as possible. With almost 5 m of double-sided silicon strip detectors and more than 1 m of silicon drift detectors, it is the largest system using both types of silicon detector.\n\nALICE has recently presented plans for an upgraded Inner Tracking System, mainly based on building a new silicon tracker with greatly improved features in terms of determination of the impact parameter (d0) to the primary vertex, tracking efficiency at low pT and readout rate capabilities. The upgraded ITS will open new channels in the study of the Quark Gluon Plasma formed at LHC which are necessary in order to understand the dynamics of this condensed phase of the QCD.\n\nIt will allow the study of the process of thermalization of heavy quarks in the medium by measuring heavy flavour charmed and beauty baryons and extending these measurements down to very low p for the first time. It will also give a better understanding of the quark mass dependence of in-medium energy loss and offer a unique capability of measuring the beauty quarks while also improving the beauty decay vertex reconstruction. Finally, the upgraded ITS will give us the chance to characterize the thermal radiation coming from the QGP and the in-medium modification of hadronic spectral functions as related to chiral symmetry restoration.\n\nThe upgrade project requires an extensive R&D effort by our researchers and collaborators all over the world on cutting-edge technologies: silicon sensors, low-power electronics, interconnection and packaging technologies, ultra-light mechanical structures and cooling units.\n\nThe ALICE Time Projection Chamber (TPC) is a large volume filled with a gas as detection medium and is the main particle tracking device in ALICE.\n\nCharged particles crossing the gas of the TPC ionize the gas atoms along their path, liberating electrons that drift towards the end plates of the detector. The characteristics of the ionization process caused by fast charged particles passing through a medium can be used for particle identification. The velocity dependence of the ionization strength is connected to the well-known Bethe-Bloch formula , which describes the average energy loss of charged particles through inelastic Coulomb collisions with the atomic electrons of the medium.\n\nMultiwire proportional counters or solid-state counters are often used as detection medium, because they provide signals with pulse heights proportional to the ionization strength. An avalanche effect in the vicinity of the anode wires strung in the readout chambers, gives the necessary signal amplification. The positive ions created in the avalanche induce a positive current signal on the pad plane. The readout is performed by the 557 568 pads that form the cathode plane of the multi-wire proportional chambers (MWPC) located at the end plates. This gives the radial distance to the beam and the azimuth. The last coordinate, z along the beam direction, is given by the drift time. Since energy-loss fluctuations can be considerable, in general many pulse-height measurements are performed along the particle track in order to optimize the resolution of the ionization measurement.\n\nAlmost all of the TPC's volume is sensitive to the traversing charged particles, but it features a minimum material budget. The straightforward pattern recognition (continuous tracks) make TPCs the perfect choice for high-multiplicity environments, such as in heavy-ion collisions, where thousands of particles have to be tracked simultaneously. Inside the ALICE TPC, the ionization strength of all tracks is sampled up to 159 times, resulting in a resolution of the ionization measurement as good as 5%.\n\nElectrons and positrons can be discriminated from other charged particles using the emission of transition radiation, X-rays emitted when the particles cross many layers of thin materials.\n\nThe identification of electrons and positrons is achieved using a transition radiation detector (TRD). In a similar manner to the muon spectrometer, this system enables detailed studies of the production of vector-meson resonances, but with extended coverage down to the light vector-meson ρ and in a different rapidity region. Below 1 GeV/c, electrons can be identified via a combination of particle identification detector (PID) measurements in the TPC and time of flight (TOF). In the momentum range 1–10 GeV/c, the fact that electrons may create TR when travelling through a dedicated \"radiator\" can be exploited. Inside such a radiator, fast charged particles cross the boundaries between materials with different dielectric constants, which can lead to the emission of TR photons with energies in the X-ray range. The effect is tiny and the radiator has to provide many hundreds of material boundaries to achieve a high enough probability to produce at least one photon. In the ALICE TRD, the TR photons are detected just behind the radiator using MWPCs filled with a xenon-based gas mixture, where they deposit their energy on top of the ionization signals from the particle's track.\n\nThe ALICE TRD was designed to derive a fast trigger for charged particles with high momentum and can significantly enhance the recorded yields of vector mesons. For this purpose, 250,000 CPUs are installed right on the detector to identify candidates for high-momentum tracks and analyse the energy deposition associated with them as quickly as possible (while the signals are still being created in the detector). This information is sent to a global tracking unit, which combines all of the information to search for electron–positron track pairs within only 6 μs.\n\nTo develop such a Transition Radiation Detector (TRD) for ALICE many detector prototypes were tested in mixed beams of pions and electrons.\n\nALICE also wants to know the identity of each particle, whether it is an electron, or a proton, a kaon or a pion.\n\nCharged hadrons (in fact, all stable charged particles) are unambiguously identified if their mass and charge are determined. The mass can be deduced from measurements of the momentum and of the velocity. Momentum and the sign of the charge are obtained by measuring the curvature of the particle's track in a magnetic field. To obtain the particle velocity there exist four methods based on measurements of time-of-flight and ionization, and on detection of transition radiation and Cherenkov radiation. Each of these methods works well in different momentum ranges or for specific types of particle. In ALICE all of these methods may be combined in order to measure, for instance, particle spectra.\n\nIn addition to the information given by ITS and TPC, more specialized detectors are needed: the TOF measures, with a precision better than a tenth of a billionth of a second, the time that each particle takes to travel from the vertex to reach it, so that one can measure its speed. The high momentum particle identification detector (HMPID) measures the faint light patterns generated by fast particles and the TRD measures the special radiation very fast particles emit when crossing different materials, thus allowing to identify electrons. Muons are measured by exploiting the fact that they penetrate matter more easily than most other particles: in the forward region a very thick and complex absorber stops all other particles and muons are measured by a dedicated set of detectors: the muon spectrometer.\n\nCharged particles are identified in ALICE by Time-Of-Flight (TOF). TOF measurements yield the velocity of a charged particle by measuring the flight time over a given distance along the track trajectory. Using the tracking information from other detectors every track firing a sensor is identified. Provided the momentum is also known, the mass of the particle can then be derived from these measurements. The ALICE TOF detector is a large-area detector based on multigap resistive plate chambers (MRPCs) that cover a cylindrical surface of 141 m, with an inner radius of 3.7 m. There are approximately 160 000 MRPC pads with time resolution of about 100 ps distributed over the large surface of 150 m.\n\nThe MRPCs are parallel-plate detectors built of thin sheets of standard window glass to create narrow gas gaps with high electric fields. These plates are separated using fishing lines to provide the desired spacing; 10 gas gaps per MRPC are needed to arrive at a detection efficiency close to 100%.\n\nThe simplicity of the construction allows a large system to be built with an overall TOF resolution of 80 ps at a relatively low cost (CERN Courier November 2011 p8). This performance allows the separation of kaons, pions and protons up to momenta of a few GeV/c. Combining such a measurement with the PID information from the ALICE TPC has proved useful in improving the separation between the different particle types, as figure 3 shows for a particular momentum range.\n\nThe High Momentum Particle Identification Detector (HMPID) is a RICH detector to determine the speed of particles beyond the momentum range available through energy loss (in ITS and TPC, \"p\" = 600 MeV) and through time-of-flight measurements (in TOF, \"p\" = 1.2–1.4 GeV).\n\nCherenkov radiation is a shock wave resulting from charged particles moving through a material faster than the velocity of light in that material. The radiation propagates with a characteristic angle with respect to the particle track, which depends on the particle velocity. Cherenkov detectors make use of this effect and in general consist of two main elements: a radiator in which Cherenkov radiation is produced and a photon detector. Ring imaging Cherenkov (RICH) detectors resolve the ring-shaped image of the focused Cherenkov radiation, enabling a measurement of the Cherenkov angle and thus the particle velocity. This in turn is sufficient to determine the mass of the charged particle.\n\nIf a dense medium (large refractive index) is used, only a thin radiator layer of the order of a few centimetres is required to emit a sufficient number of Cherenkov photons. The photon detector is then located at some distance (usually about 10 cm) behind the radiator, allowing the cone of light to expand and form the characteristic ring-shaped image. Such a proximity-focusing RICH is installed in the ALICE experiment.\n\nALICE HMPID's momentum range is up to 3 GeV for pion/kaon discrimination and up to 5 GeV for kaon/proton discrimination. It is the world's largest caesium iodide RICH detector, with an active area of 11 m². A prototype was successfully tested at CERN in 1997 and currently takes data at the Relativistic Heavy Ion Collider at the Brookhaven National Laboratory in the US.\n\nCalorimeters measure the energy of particles, and determine whether they have electromagnetic or hadronic interactions. Particle Identification in a calorimeter is a destructive measurement. All particles except muons and neutrinos deposit all their energy in the calorimeter system by production of electromagnetic or hadronic showers. Photons, electrons and positrons deposit all their energy in an electromagnetic calorimeter. Their showers are indistinguishable, but a photon can be identified by the non-existence of a track in the tracking system that is associated to the shower.\n\nThe photons (particles of light), like the light emitted from a hot object, tell us about the temperature of the system. To measure them, special detectors are necessary: the crystals of the PHOS, which are as dense as lead and as transparent as glass, will measure them with fantastic precision in a limited region, while the PMD and in particular the EMCal will measure them over a very wide area. The EMCal will also measure groups of close particles (called \"jets\") which have a memory of the early phases of the event.\n\nPHOS is a high-resolution electromagnetic calorimeter installed in ALICE to provide data to test the thermal and dynamical properties of the initial phase of the collision. This is done by measuring photons emerging directly from the collision. PHOS covers a limited acceptance domain at central rapidity. It is made of lead tungstate crystals, similar to the ones used by CMS, read out using Avalanche Photodiodes (APD).\n\nWhen high energy photons strike lead tungstate, they make it glow, or scintillate, and this glow can be measured. Lead tungstate is extremely dense (denser than iron), stopping most photons that reach it. The crystals are kept at a temperature of 248 K, which helps to minimize the deterioration of the energy resolution due to noise and to optimize the response for low energies.\n\nThe EMCal is a lead-scintillator sampling calorimeter comprising almost 13,000 individual towers that are grouped into ten super-modules. The towers are read out by wavelength-shifting optical fibers in a shashlik geometry coupled to an avalanche photodiode. The complete EMCal will contain 100,000 individual scintillator tiles and 185 kilometers of optical fiber, weighing in total about 100 tons.\n\nThe EMCal covers almost the full length of the ALICE Time Projection Chamber and central detector, and a third of its azimuth placed back-to-back with the ALICE Photon Spectrometer – a smaller, highly granular lead-tungstate calorimeter.\n\nThe super-modules are inserted into an independent support frame situated within the ALICE magnet, between the time-of-flight counters and the magnet coil. The support frame itself is a complex structure: it weighs 20 tons and must support five times its own weight, with a maximum deflection between being empty and being fully loaded of only a couple of centimeters. Installation of the eight-ton super-modules requires a system of rails with a sophisticated insertion device to bridge across to the support structure.\n\nThe Electro-Magnetic Calorimeter (EM-Cal) will add greatly to the high momentum particle measurement capabilities of ALICE. It will extend ALICE's reach to study jets and other hard processes.\n\nThe Photon Multiplicity Detector (PMD) is a Particle shower detector which measures the multiplicity and spatial distribution of photons produced in the collisions. It utilizes as a first layer a veto detector to reject charged particles. Photons on the other hand pass through a converter, initiating an electromagnetic shower in a second detector layer where they produce large signals on several cells of its sensitive volume. Hadrons on the other hand normally affect only one cell and produce a signal representing minimum-ionizing particles.\n\nThe Forward Multiplicity Detector (FMD) extends the coverage for multiplicity of charge particles into the forward regions - giving ALICE the widest coverage of the 4 LHC experiments for these measurements.\n\nThe FMD consist of 5 large silicon discs with each 10 240 individual detector channels to measure the charged particles emitted at small angles relative to the beam. FMD provides an independent measurement of the orientation of the collisions in the vertical plane, which can be used with measurements from the barrel detector to investigate flow, jets, etc.\n\nThe ALICE forward muon spectrometer studies the complete spectrum of heavy quarkonia (J/Ψ, Ψ′, ϒ, ϒ′, ϒ′′) via their decay in the μ+μ– channel. Heavy quarkonium states, provide an essential tool to study the early and hot stage of heavy-ion collisions. In particular they are expected to be sensitive to Quark-Gluon Plasma formation. In the presence of a deconfined medium (i.e. QGP) with high enough energy density, quarkonium states are dissociated because of colour screening. This leads to a suppression of their production rates. At the high LHC collision energy, both the charmonium states (J/Ψ and Ψ′) as well as the bottomonium states (ϒ, ϒ′ and ϒ′′) can be studied. The Dimuon spectrometer is optimized for the detection of these heavy quark resonances.\n\nMuons may be identified using the just described technique by using the fact that they are the only charged particles able to pass almost undisturbed through any material. This behaviour is connected to the fact that muons with momenta below a few hundred GeV/c do not suffer from radiative energy losses and so do not produce electromagnetic showers. Also, because they are leptons, they are not subject to strong interactions with the nuclei of the material they traverse. This behaviour is exploited in muon spectrometers in high-energy physics experiments by installing muon detectors behind the calorimeter systems or behind thick absorber materials. All charged particles other than muons are completely stopped, producing electromagnetic (and hadronic) showers.\n\nThe muon spectrometer in the forward region of ALICE features a very thick and complex front absorber and an additional muon filter consisting of an iron wall 1.2 m thick. Muon candidates selected from tracks penetrating these absorbers are measured precisely in a dedicated set of tracking detectors. Pairs of muons are used to collect the spectrum of heavy-quark vector-meson resonances (J/Psi). Their production rates can be analysed as a function of transverse momentum and collision centrality in order to investigate dissociation due to colour screening. The acceptance of the ALICE Muon Spectrometer covers the pseudorapidity interval 2.5 ≤ η ≤ 4 and the resonances can be detected down to zero transverse momentum.\n\nFinally, we need to know how powerful the collision was: this is done by measuring the remnants of the colliding nuclei in detectors made of high density materials located about 110 meters on both sides of ALICE (the ZDCs) and by measuring with the FMD, V0 and T0 the number of particles produced in the collision and their spatial distribution. T0 also measures with high precision the time when the event takes place.\n\nThe ZDCs are calorimeters which detect the energy of the spectator nucleons in order to determine the overlap region of the two colliding nuclei. It is composed of four calorimeters, two to detect protons (ZP) and two to detect neutrons (ZN). They are located 115 meters away from the interaction point on both sides, exactly along the beam line. The ZN is placed at zero degree with respect to the LHC beam axis, between the two beam pipes. That is why we call them Zero Degree Calorimeters (ZDC).The ZP is positioned externally to the outgoing beam pipe. The spectator protons are separated from the ion beams by means of the dipole magnet D1.\n\nThe ZDCs are \"spaghetti calorimeters\", made by a stack of heavy metal plates grooved to allocate a matrix of quartz fibres. Their principle of operation is based on the detection of Cherenkov light produced by the charged particles of the shower in the fibers.\n\nV0 is made of two arrays of scintillator counters set on both sides of the ALICE interaction point, and called V0-A and V0-C. The V0-C counter is located upstream of the dimuon arm absorber and cover the spectrometer acceptance while the V0-A counter will be located at around 3.5 m away from the collision vertex, on the other side.\n\nIt is used to estimate the centrality of the collision by summing up the energy deposited in the two disks of V0. This observable scales directly with the number of primary particles generated in the collision and therefore to the centrality.\n\nV0 is also used as reference in Van Der Meer scans that give the size and shape of colliding beams and therefore the luminosity delivered to the experiment.\n\nALICE T0 serves as a start, trigger and luminosity detector for ALICE. The accurate interaction time (START) serves as the reference signal for the Time-of-Flight detector that is used for particle identification. T0 supplies five different trigger signals to the Central Trigger Processor. The most important of these is the T0 vertex providing prompt and accurate confirmation of the location of the primary interaction point along the beam axis within the set boundaries. The detector is also used for online luminosity monitoring providing fast feedback to the accelerator team.\n\nThe T0 detector consists of two arrays of Cherenkov counters (T0-C and T0-A) positioned at the opposite sides of the interaction point (IP). Each array has 12 cylindrical counters equipped with a quartz radiator and a photomultiplier tube.\n\nThe ALICE underground cavern provides an ideal place for the detection of high energy atmospheric muons coming from cosmic ray showers. ACORDE detects cosmic ray showers by triggering the arrival of muons to the top of the ALICE magnet.\n\nThe ALICE cosmic ray trigger is made of 60 scintillator modules distributed on the 3 upper faces of the ALICE magnet yoke. The array can be configured to trigger on single or multi-muon events, from 2-fold coincidences up to the whole array if desired. ACORDE's high luminosity allows the recording of cosmic events with very high multiplicity of parallel muon tracks, the so-called muon bundles.\n\nWith ACORDE, the ALICE Experiment has been able to detect muon bundles with the highest multiplicity ever registered as well as to indirectly measure very high energy primary cosmic rays .\n\nALICE had to design a data acquisition system that operates efficiently in two widely different running modes: the very frequent but small events, with few produced particles encountered during proton-proton collisions and the relatively rare, but extremely large events, with tens of thousands of new particles produced in lead-lead collisions at the LHC (L = 10 cm s in Pb-Pb with 100 ns bunch crossings and L = 10-10 cm s in pp with 25 ns bunch crossings).\n\nThe ALICE data acquisition system needs to balance its capacity to record the steady stream of very large events resulting from central collisions, with an ability to select and record rare cross-section processes. These requirements result in an aggregate event building bandwidth of up to 2.5 GByte/s and a storage capability of up to 1.25 GByte/s, giving a total of more than 1 PByte of data every year. As shown in the figure, ALICE needs a data storage capacity that by far exceeds that of the current generation of experiments. This data rate is equivalent to six times the contents of the Encyclopædia Britannica every second.\n\nThe hardware of the ALICE DAQ system is largely based on commodity components: PC's running Linux and standard Ethernet switches for the eventbuilding network. The required performances are achieved by the interconnection of hundreds of these PC's into a large DAQ fabric. The software framework of the ALICE DAQ is called DATE (ALICE Data Acquisition and Test Environment). DATE is already in use today, during the construction and testing phase of the experiment, while evolving gradually towards the final production system. Moreover, AFFAIR (A Flexible Fabric and Application Information Recorder) is the performance monitoring software developed by the ALICE Data Acquisition project. AFFAIR is largely based on open source code and is composed of the following components: data gathering, inter-node communication employing DIM, fast and temporary round robin database storage, and permanent storage and plot generation using ROOT.\n\nFinally. the ALICE experiment Mass Storage System (MSS) combines a very high bandwidth (1.25 GByte/s) and every year stores huge amounts of data, more than 1 Pbytes. The mass storage system is made of: a) Global Data Storage (GDS) performing the temporary storage of data at the experimental pit; b) Permanent Data Storage (PDS) for long-term archive of data in the CERN Computing Center and finally from The Mass Storage System software managing the creation, the access and the archive of data.\n\nThe physics programme of ALICE includes the following main topics: i) the study of the thermalization of partons in the QGP with focus on the massive charming beauty quarks and understanding the behaviour of these heavy quarks in relation to the stroungly-coupled medium of QGP, ii) the study of the mechanisms of energy loss that occur in the medium and the dependencies of energy loss on the parton species, iii) the dissociation of quarkonium states which can be a probe of deconfinement and of the temperature of the medium and finally the production of thermal photons and low-mass dileptons emitted by the QGP which is about assessing the initial temperature and degrees of freedom of the systems as well as the chiral nature of the phase transition.\n\nThe ALICE collaboration presented its first results from LHC proton collisions at a centre-of-mass energy of 7 TeV in March 2010. The results confirmed that the charged-particle multiplicity is rising with energy faster than expected while the shape of the multiplicity distribution is not reproduced well by standard simulations. The results were based on the analysis of a sample of 300,000 proton–proton collisions the ALICE experiment collected during the first runs of the LHC with stable beams at a centre-of-mass energy, √s, of 7 TeV,\n\nIn 2011, the ALICE Collaboration measured the size of the system created in Pb-Pb collisions at a centre-of-mass energy of 2.76 TeV per nucleon pair. ALICE confirmed that the QCD matter created in Pb-Pb collisions behaves like a fluid, with strong collective motions that are well described by hydrodynamic equations. The fireball formed in nuclear collisions at the LHC is hotter, lives longer and expands to a larger size than the medium that was formed in heavy-ion collisions at RHIC. Multiplicity measurements by the ALICE experiment show that the system initially has much higher energy density and is at least 30% hotter than at RHIC, resulting in about double the particle multiplicity for each colliding nucleon pair (Aamodt et al. 2010a). Further analyses, in particular including the full dependence of these observables on centrality, will provide more insights into the properties of the system – such as initial velocities, the equation of state and the fluid viscosity – and strongly constrain the theoretical modelling of heavy-ion collisions.\n\nOff-centre nuclear collisions, with a finite impact parameter, create a strongly asymmetric \"almond-shaped\" fireball. However, experiments cannot measure the spatial dimensions of the interaction (except in special cases, for example in the production of pions, see). Instead, they measure the momentum distributions of the emitted particles. A correlation between the measured azimuthal momentum distribution of particles emitted from the decaying fireball and the initial spatial asymmetry can arise only from multiple interactions between the constituents of the created matter; in other words it tells us about how the matter flows, which is related to its equation of state and its thermodynamic transport properties.\n\nThe measured azimuthal distribution of particles in momentum space can be decomposed into Fourier coefficients. The second Fourier coefficient (v2), called elliptic flow, is particularly sensitive to the internal friction or viscosity of the fluid, or more precisely, η/s, the ratio of the shear viscosity (η) to entropy (s) of the system. For a good fluid such as water, the η/s ratio is small. A \"thick\" liquid, such as honey, has large values of η/s.\n\nIn heavy-ion collisions at the LHC, the ALICE collaboration found that the hot matter created in the collision behaves like a fluid with little friction, with η/s close to its lower limit (almost zero viscosity). With these measurements, ALICE has just begun to explore the temperature dependence of η/s and we anticipate many more in-depth flow-related measurements at the LHC that will constrain the hydrodynamic features of the QGP even further.\n\nIn August 2012 ALICE scientists announced that their experiments produced quark–gluon plasma with temperature at around 5.5 trillion kelvins, the highest temperature mass achieved in any physical experiments thus far. This temperature is about 38% higher than the previous record of about 4 trillion kelvins, achieved in the 2010 experiments at the Brookhaven National Laboratory.\n\nThe ALICE results were announced at the August 13 \"Quark Matter 2012\" conference in Washington, D.C.. The quark–gluon plasma produced by these experiments approximates the conditions in the universe that existed microseconds after the Big Bang, before the matter coalesced into atoms.\n\nA basic process in QCD is the energy loss of a fast parton in a medium composed of colour charges. This phenomenon, \"jet quenching\", is especially useful in the study of the QGP, using the naturally occurring products (jets) of the hard scattering of quarks and gluons from the incoming nuclei. A highly energetic parton (a colour charge) probes the coloured medium rather like an X-ray probes ordinary matter. The production of these partonic probes in hadronic collisions is well understood within perturbative QCD. The theory also shows that a parton traversing the medium will lose a fraction of its energy in emitting many soft (low energy) gluons. The amount of the radiated energy is proportional to the density of the medium and to the square of the path length travelled by the parton in the medium. Theory also predicts that the energy loss depends on the flavour of the parton.\n\nJet quenching was first observed at RHIC by measuring the yields of hadrons with high transverse momentum. These particles are produced via fragmentation of energetic partons. The yields of these high-pT particles in central nucleus–nucleus collisions were found to be a factor of five lower than expected from the measurements in proton–proton reactions. ALICE has recently published the measurement of charged particles in central heavy-ion collisions at the LHC. As at RHIC, the production of high-pT hadrons at the LHC is strongly suppressed. However, the observations at the LHC show qualitatively new features. The observation from ALICE is consistent with reports from the ATLAS and CMS collaborations on direct evidence for parton energy loss within heavy-ion collisions using fully reconstructed back-to-back jets of particles associated with hard parton scatterings. The latter two experiments have shown a strong energy imbalance between the jet and its recoiling partner (G Aad et al. 2010 and CMS collaboration 2011). This imbalance is thought to arise because one of the jets traversed the hot and dense matter, transferring a substantial fraction of its energy to the medium in a way that is not recovered by the reconstruction of the jets.\n\nQuarkonia are bound states of heavy flavour quarks (charm or bottom) and their antiquarks. Two types of quarkonia have been extensively studied: charmonia, which consist of a charm quark and an anti-charm, and bottomonia made of a bottom and an anti-bottom quark. Charm and anticharm quarks in the presence of the Quark Gluon Plasma, in which there are many free colour charges, are not able to see each other any more and therefore they cannot form bound states. The \"melting\" of quarkonia into the QGP manifests itself in the suppression of the quarkonium yields compared to the production without the presence of the QGP. The search for quarkonia suppression as a QGP signature started 25 years ago. The first ALICE results for charm hadrons in PbPb collisions at a centre-of-mass energy √sNN = 2.76 TeV indicate strong in-medium energy loss for charm and strange quarks that is an indication of the formation of the hot medium of QGP.\n\nAs the temperature increases so does the colour screening resulting in greater suppression of the quarkonium states as it is more difficult for charm – anticharm or bottom – antibottom to form new bound states. At very high temperatures no quarkonium states are expected to survive; they melt in the QGP. Quarkonium sequential suppression is therefore considered as a QGP thermometer, as states with different masses have different sizes and are expected to be screened and dissociated at different temperatures. However - as the collision energy increases - so does the number of charm-anticharm quarks that can form bound states, and a balancing mechanism of recombination of quarkonia may appear as we move to higher energies.\n\nThe results from the first ALICE run are rather striking, when compared with the observations from lower energies. While a similar suppression is observed at LHC energies for peripheral collisions, when moving towards more head-on collisions – as quantified by the increasing number of nucleons in the lead nuclei participating in the interaction – the suppression no longer increases. Therefore, despite the higher temperatures attained in the nuclear collisions at the LHC, more J/ψ mesons are detected by the ALICE experiment in Pb–Pb with respect to p–p. Such an effect is likely to be related to a regeneration process occurring at the temperature boundary between the QGP and a hot gas of hadrons\n\nThe suppression of charmonium states was also observed in proton-lead collisions at the LHC, in which Quark Gluon Plasma is not formed. This suggests that the observed suppression in proton-nucleus collisions (pA) is due to cold nuclear matter effects. Grasping the wealth of experimental results requires understanding the medium modification of quarkonia and disentangling hot and cold-matter effects. Today there is a large amount of data available from RHIC and LHC on charmonium and bottomonium suppression and ALICE tries to distinguish between effects due to the formation of the QGP and those from cold nuclear matter effects.\n\nThe analysis of the data from the p-Pb collisions at the LHC revealed a completely unexpected double-ridge structure with so far unknown origin. The proton–lead (pPb) collisions in 2013, two years after its heavy-ion collisions opened a new chapter in exploration of the properties of the deconfined, chirally symmetrical state of the QGP. A surprising near-side, long-range (elongated in pseudorapidity) correlation, forming a ridge-like structure observed in high-multiplicity pp collisions, was also found in high-multiplicity pPb collisions, but with a much larger amplitude (). However, the biggest surprise came from the observation that this near-side ridge is accompanied by an essentially symmetrical away-side ridge, opposite in azimuth (CERN Courier March 2013 p6). This double ridge was revealed after the short-range correlations arising from jet fragmentation and resonance decays were suppressed by subtracting the correlation distribution measured for low-multiplicity events from the one for high-multiplicity events.\n\nSimilar long-range structures in heavy-ion collisions have been attributed to the collective flow of particles emitted from a thermalized system undergoing a collective hydrodynamic expansion. This anisotropy can be characterized by means of the vn (n = 2, 3, ...) coefficients of a Fourier decomposition of the single-particle azimuthal distribution. To test the possible presence of collective phenomena further, the ALICE collaboration has extended the two-particle correlation analysis to identified particles, checking for a potential mass ordering of the v2 harmonic coefficients. Such an ordering in mass was observed in heavy-ion collisions, where it was interpreted to arise from a common radial boost – the so-called radial flow – coupled to the anisotropy in momentum space. Continuing the surprises, a clear particle-mass ordering, similar to the one observed in mid-central PbPb collisions (CERN Courier, September 2013), has been measured in high-multiplicity pPb collisions.\n\nThe final surprise, so far, comes from the charmonium states. Whereas J/ψ production does not reveal any unexpected behaviour, the production of the heavier and less-bound (2S) state indicates a strong suppression (0.5–0.7) with respect to J/ψ, when compared with pp collisions. Is this a hint of effects of the medium? Indeed, in heavy-ion collisions, such a suppression has been interpreted as a sequential melting of quarkonia states, depending on their binding energy and the temperature of the QGP created in these collisions.\n\nThe first pPb measurement campaign, expected results were widely accompanied by unanticipated observations. Among the expected results is the confirmation that proton–nucleus collisions provide an appropriate tool to study the partonic structure of cold nuclear matter in detail. The surprises have come from the similarity of several observables between pPb and PbPb collisions, which hint at the existence of collective phenomena in pPb collisions with high particle multiplicity and, eventually, the formation of QGP.\n\nThe main upgrade activity on ALICE during LHC's Long Shutdown 1 was the installation of the dijet calorimeter (DCAL), an extension of the existing EMCAL system that adds 60° of azimuthal acceptance opposite the existing 120° of the EMCAL's acceptance. This new subdetector will be installed on the bottom of the solenoid magnet, which currently houses three modules of the photon spectrometer (PHOS). Moreover, an entirely new rail system and cradle will be installed to support the three PHOS modules and eight DCAL modules, which together weigh more than 100 tones. The installation of five modules of the TRD will follow and so complete this complex detector system, which consists of 18 units,\n\nIn addition to these mainstream detector activities, all of the 18 ALICE subdetectors underwent major improvements during LS1 while the computers and discs of the online systems are replaced, followed by upgrades of the operating systems and online software.\n\nAll of these efforts are to ensure that ALICE is in good shape for the three-year LHC running period after LS1, when the collaboration looks forward to heavy-ion collisions at the top LHC energy of 5.5 TeV/nucleon at luminosities in excess of 1027 Hz/cm.\n\nThe ALICE collaboration has plans for a major upgrade during the next long shutdown, LS2, currently scheduled for 2018. Then the entire silicon tracker will be replaced by a monolithic-pixel tracker system built from ALPIDE chips; the time-projection chamber will be upgraded with gaseous electron-multiplier (GEM) detectors for continuous read-out and the use of new microelectronics; and all of the other subdetectors and the online systems will prepare for a 100-fold increase in the number of events written to tape.\n\n"}
{"id": "30387605", "url": "https://en.wikipedia.org/wiki?curid=30387605", "title": "Aaron Glacier", "text": "Aaron Glacier\n\nAaron Glacier () is a long Antarctic glacier which drains between the Janulis Spur and the Gray Spur in the Thiel Mountains. The name was proposed by Peter Bermel and Arthur B. Ford, co-leaders of the U.S. Geological Survey (USGS) Thiel Mountains party from 1960 to 1961. It was named for Johm M. Aaron, a USGS geologist and member of the 1960–61 and 1961–62 field parties to the Thiel Mountains.\n\n"}
{"id": "5761568", "url": "https://en.wikipedia.org/wiki?curid=5761568", "title": "Attachment in children", "text": "Attachment in children\n\nAttachment in children is \"a biological instinct in which proximity to an attachment figure is sought when the child senses or perceives threat or discomfort. Attachment behaviour anticipates a response by the attachment figure which will remove threat or discomfort\". Attachment also describes the function of availability, which is the degree to which the authoritative figure is responsive to the child's needs and shares communication with them. Childhood attachment can define characteristics that will shape the child's sense of self, their forms of emotion-regulation, and how they carry out relationships with others. Attachment is found in all mammals to some degree, especially nonhuman primates.\n\nAttachment theory has led to a new understanding of child development. Children develop different patterns of attachment based on experiences and interactions with their caregivers at a young age. Four different attachment classifications have been identified in children: secure attachment, anxious-ambivalent attachment, anxious-avoidant attachment, and disorganized attachment. Attachment theory has become the dominant theory used today in the study of infant and toddler behavior and in the fields of infant mental health, treatment of children, and related fields.\n\nAttachment theory (Bowlby 1969, 1973, 1980) is rooted in the ethological notion that a newborn child is biologically programmed to seek proximity with caregivers, and this proximity-seeking behavior is naturally selected. Through repeated attempts to seek physical and emotional closeness with a caregiver and the responses the child gets, the child develops an internal working model (IWM) that reflects the response of the caregiver to the child. According to Bowlby, attachment provides a secure base from which the child can explore the environment, a haven of safety to which the child can return when he or she is afraid or fearful. Bowlby's colleague Mary Ainsworth identified that an important factor which determines whether a child will have a secure or insecure attachment is the degree of sensitivity shown by their caregiver:\n\nThe sensitive caregiver responds socially to attempts to initiate social interaction, playfully to his attempts to initiate play. She picks him up when he seems to wish it, and puts him down when he wants to explore. When he is distressed, she knows what kinds and degree of soothing he requires to comfort him – and she knows that sometimes a few words or a distraction will be all that is needed. On the other hand, the mother who responds inappropriately tries to socialize with the baby when he is hungry, play with him when he is tired, or feed him when he is trying to initiate social interaction.\n\nHowever, it should be recognized that \"even sensitive caregivers get it right only about 50 percent of the time. Their communications are either out of synch, or mismatched. There are times when parents feel tired or distracted. The telephone rings or there is breakfast to prepare. In other words, attuned interactions rupture quite frequently. But the hallmark of a sensitive caregiver is that the ruptures are managed and repaired.\"\n\nThe most common and empirically supported method for assessing attachment in infants (12 months – 20 months) is the Strange Situation Protocol, developed by Mary Ainsworth as a result of her careful in-depth observations of infants with their mothers in Uganda(see below). The Strange Situation Protocol is a research, not a diagnostic, tool and the resulting attachment classifications are not 'clinical diagnoses.' While the procedure may be used to supplement clinical impressions, the resulting classifications should not be confused with the clinically diagnosed 'Reactive Attachment Disorder (RAD).' The clinical concept of RAD differs in a number of fundamental ways from the theory and research driven attachment classifications based on the Strange Situation Procedure. The idea that insecure attachments are synonymous with RAD is, in fact, not accurate and leads to ambiguity when formally discussing attachment theory as it has evolved in the research literature. This is not to suggest that the concept of RAD is without merit, but rather that the clinical and research conceptualizations of insecure attachment and attachment disorder are not synonymous.\n\nThe 'Strange Situation' is a laboratory procedure used to assess infant patterns of attachment to their caregiver. In the procedure, the mother and infant are placed in an unfamiliar playroom equipped with toys while a researcher observes/records the procedure through a one-way mirror. The procedure consists of eight sequential episodes in which the child experiences both separation from and reunion with the mother as well as the presence of an unfamiliar stranger. The protocol is conducted in the following format unless modifications are otherwise noted by a particular researcher:\n\n\nMainly on the basis of their reunion behaviours (although other behaviours are taken into account) in the Strange Situation Paradigm (Ainsworth et al., 1978; see below), infants can be categorized into three 'organized' attachment categories: Secure (Group B); Avoidant (Group A); and Anxious/Resistant (Group C). There are subclassifications for each group (see below). A fourth category, termed Disorganized (D), can also be assigned to an infant assessed in the Strange Situation although a primary 'organized' classification is always given for an infant judged to be disorganized. Each of these groups reflects a different kind of attachment relationship with the mother. A child may have a different type of attachment to each parent as well as to unrelated caregivers. Attachment style is thus not so much a part of the child's thinking, but is characteristic of a specific relationship. However, after about age five the child exhibits one primary consistent pattern of attachment in relationships.\n\nThe pattern the child develops after age five demonstrates the specific parenting styles used during the developmental stages within the child. These attachment patterns are associated with behavioural patterns and can help further predict a child's future personality.\n\n\"The strength of a child's attachment behaviour in a given circumstance does not indicate the 'strength' of the attachment bond. Some insecure children will routinely display very pronounced attachment behaviours, while many secure children find that there is no great need to engage in either intense or frequent shows of attachment behaviour\".\n\nA toddler who is securely attached to its parent (or other familiar caregiver) will explore freely while the caregiver is present, typically engages with strangers, is often visibly upset when the caregiver departs, and is generally happy to see the caregiver return. The extent of exploration and of distress are affected by the child's temperamental make-up and by situational factors as well as by attachment status, however. A child's attachment is largely influenced by their primary caregiver's sensitivity to their needs. Parents who consistently (or almost always) respond to their child's needs will create securely attached children. Such children are certain that their parents will be responsive to their needs and communications.\n\nIn the traditional Ainsworth et al. (1978) coding of the Strange Situation, secure infants are denoted as \"Group B\" infants and they are further subclassified as B1, B2, B3, and B4. Although these subgroupings refer to different stylistic responses to the comings and goings of the caregiver, they were not given specific labels by Ainsworth and colleagues, although their descriptive behaviours led others (including students of Ainsworth) to devise a relatively 'loose' terminology for these subgroups. B1's have been referred to as 'secure-reserved', B2's as 'secure-inhibited', B3's as 'secure-balanced,' and B4's as 'secure-reactive.' In academic publications however, the classification of infants (if subgroups are denoted) is typically simply \"B1\" or \"B2\" although more theoretical and review-oriented papers surrounding attachment theory may use the above terminology.\n\nSecurely attached children are best able to explore when they have the knowledge of a secure base to return to in times of need. When assistance is given, this bolsters the sense of security and also, assuming the parent's assistance is helpful, educates the child in how to cope with the same problem in the future. Therefore, secure attachment can be seen as the most adaptive attachment style. According to some psychological researchers, a child becomes securely attached when the parent is available and able to meet the needs of the child in a responsive and appropriate manner. At infancy and early childhood, if parents are caring and attentive towards their children, those children will be more prone to secure attachment.\n\nAnxious-resistant insecure attachment is also called ambivalent attachment. In general, a child with an anxious-resistant attachment style will typically explore little (in the Strange Situation) and is often wary of strangers, even when the caregiver is present. When the caregiver departs, the child is often highly distressed. The child is generally ambivalent when they return. The Anxious-Ambivalent/Resistant strategy is a response to unpredictably responsive caregiving, and that the displays of anger or helplessness towards the caregiver on reunion can be regarded as a conditional strategy for maintaining the availability of the caregiver by preemptively taking control of the interaction.\n\nThe C1 subtype is coded when:\n\n\"...resistant behavior is particularly conspicuous. The mixture of seeking and yet resisting contact and interaction has an unmistakably angry quality and indeed an angry tone may characterize behavior in the preseparation episodes...\"\n\nThe C2 subtype is coded when:\n\n\"Perhaps the most conspicuous characteristic of C2 infants is their passivity. Their exploratory behavior is limited throughout the SS and their interactive behaviors are relatively lacking in active initiation. Nevertheless, in the reunion episodes they obviously want proximity to and contact with their mothers, even though they tend to use signalling rather than active approach, and protest against being put down rather than actively resisting release...In general the C2 baby is not as conspicuously angry as the C1 baby.\"\n\nA child with the anxious-avoidant insecure attachment style will avoid or ignore the caregiver – showing little emotion when the caregiver departs or returns. The child will not explore very much regardless of who is there. Infants classified as anxious-avoidant (A) represented a puzzle in the early 1970s. They did not exhibit distress on separation, and either ignored the caregiver on their return (A1 subtype) or showed some tendency to approach together with some tendency to ignore or turn away from the caregiver (A2 subtype). Ainsworth and Bell theorised that the apparently unruffled behaviour of the avoidant infants is in fact as a mask for distress, a hypothesis later evidenced through studies of the heart-rate of avoidant infants.\n\nInfants are depicted as anxious-avoidant insecure when there is:\n\n\"...conspicuous avoidance of the mother in the reunion episodes which is likely to consist of ignoring her altogether, although there may be some pointed looking away, turning away, or moving away...If there is a greeting when the mother enters, it tends to be a mere look or a smile...Either the baby does not approach his mother upon reunion, or they approach in 'abortive' fashions with the baby going past the mother, or it tends to only occur after much coaxing...If picked up, the baby shows little or no contact-maintaining behavior; he tends not to cuddle in; he looks away and he may squirm to get down.\"\n\nAinsworth's narrative records showed that infants avoided the caregiver in the stressful Strange Situation Procedure when they had a history of experiencing rebuff of attachment behaviour. The child's needs are frequently not met and the child comes to believe that communication of needs has no influence on the caregiver. Ainsworth's student Mary Main theorised that avoidant behaviour in the Strange Situational Procedure should be regarded as 'a conditional strategy, which paradoxically permits whatever proximity is possible under conditions of maternal rejection' by de-emphasising attachment needs. Main proposed that avoidance has two functions for an infant whose caregiver is consistently unresponsive to their needs. Firstly, avoidant behaviour allows the infant to maintain a conditional proximity with the caregiver: close enough to maintain protection, but distant enough to avoid rebuff. Secondly, the cognitive processes organising avoidant behaviour could help direct attention away from the unfulfilled desire for closeness with the caregiver – avoiding a situation in which the child is overwhelmed with emotion ('disorganised distress'), and therefore unable to maintain control of themselves and achieve even conditional proximity.\n\nAinsworth herself was the first to find difficulties in fitting all infant behaviour into the three classifications used in her Baltimore study. Ainsworth and colleagues sometimes observed 'tense movements such as hunching the shoulders, putting the hands behind the neck and tensely cocking the head, and so on. It was our clear impression that such tension movements signified stress, both because they tended to occur chiefly in the separation episodes and because they tended to be prodromal to crying. Indeed, our hypothesis is that they occur when a child is attempting to control crying, for they tend to vanish if and when crying breaks through'. Such observations also appeared in the doctoral theses of Ainsworth's students. Crittenden, for example, noted that one abused infant in her doctoral sample was classed as secure (B) by her undergraduate coders because her strange situation behaviour was \"without either avoidance or ambivalence, she did show stress-related stereotypic headcocking throughout the strange situation. This pervasive behaviour, however, was the only clue to the extent of her stress\".\n\nDrawing on records of behaviours discrepant with the A, B, and C classifications, a fourth classification was added by Ainsworth's colleague Mary Main. In the Strange Situation, the attachment system is expected to be activated by the departure and return of the caregiver. If the behaviour of the infant does not appear to the observer to be coordinated in a smooth way across episodes to achieve either proximity or some relative proximity with the caregiver, then it is considered 'disorganised' as it indicates a disruption or flooding of the attachment system (e.g. by fear). Infant behaviours in the Strange Situation Protocol coded as disorganised/disoriented include overt displays of fear; contradictory behaviours or affects occurring simultaneously or sequentially; stereotypic, asymmetric, misdirected or jerky movements; or freezing and apparent dissociation. Lyons-Ruth has urged, however, that it should be wider 'recognized that 52% of disorganized infants continue to approach the caregiver, seek comfort, and cease their distress without clear ambivalent or avoidant behavior.'\n\nThere is 'rapidly growing interest in disorganized attachment' from clinicians and policy-makers as well as researchers. Yet the Disorganized/disoriented attachment (D) classification has been criticised by some for being too encompassing. In 1990, Ainsworth put in print her blessing for the new 'D' classification, though she urged that the addition be regarded as 'open-ended, in the sense that subcategories may be distinguished', as she worried that the D classification might be too encompassing and might treat too many different forms of behaviour as if they were the same thing. Indeed, the D classification puts together infants who use a somewhat disrupted secure (B) strategy with those who seem hopeless and show little attachment behaviour; it also puts together infants who run to hide when they see their caregiver in the same classification as those who show an avoidant (A) strategy on the first reunion and then an ambivalent-resistant (C) strategy on the second reunion. Perhaps responding to such concerns, George and Solomon have divided among indices of Disorganized/disoriented attachment (D) in the Strange Situation, treating some of the behaviours as a 'strategy of desperation' and others as evidence that the attachment system has been flooded (e.g. by fear, or anger). Crittenden also argues that some behaviour classified as Disorganized/disoriented can be regarded as more 'emergency' versions of the avoidant and/or ambivalent/resistant strategies, and function to maintain the protective availability of the caregiver to some degree. Sroufe et al. have agreed that 'even disorganised attachment behaviour (simultaneous approach-avoidance; freezing, etc.) enables a degree of proximity in the face of a frightening or unfathomable parent'. However, 'the presumption that many indices of \"disorganisation\" are aspects of organised patterns does not preclude acceptance of the notion of disorganisation, especially in cases where the complexity and dangerousness of the threat are beyond children's capacity for response'. For example, 'Children placed in care, especially more than once, often have intrusions. In videos of the Strange Situation Procedure, they tend to occur when a rejected/neglected child approaches the stranger in an intrusion of desire for comfort, then loses muscular control and falls to the floor, overwhelmed by the intruding fear of the unknown, potentially dangerous, strange person'.\n\nMain and Hesse found that most of the mothers of these children had suffered major losses or other trauma shortly before or after the birth of the infant and had reacted by becoming severely depressed. In fact, 56% of mothers who had lost a parent by death before they completed high school subsequently had children with disorganized attachments. Subsequently, studies, whilst emphasising the potential importance of unresolved loss, have qualified these findings. For example, Solomon and George found that unresolved loss in the mother tended to be associated with disorganised attachment in their infant primarily when they had also experienced an unresolved trauma in their life prior to the loss.\n\nStudies of older children have identified further attachment classifications. Main and Cassidy observed that disorganized behaviour in infancy can develop into a child using caregiving-controlling or punitive behaviour in order to manage a helpless or dangerously unpredictable caregiver. In these cases, the child's behaviour is organised, but the behaviour is treated by researchers as a form of 'disorganization' (D) since the hierarchy in the family is no longer organised according to parenting authority.\n\nPatricia McKinsey Crittenden has elaborated classifications of further forms of avoidant and ambivalent attachment behaviour. These include the caregiving and punitive behaviours also identified by Main and Cassidy (termed A3 and C3 respectively), but also other patterns such as compulsive compliance with the wishes of a threatening parent (A4).\n\nCrittenden's ideas developed from Bowlby's proposal that 'given certain adverse circumstances during childhood, the selective exclusion of information of certain sorts may be adaptive. Yet, when during adolescence and adult the situation changes, the persistent exclusion of the same forms of information may become maladaptive'.\n\nCrittenden She proposed that the basic components of human experience of danger are two kinds of information:\n\n1. 'Affective information' – the emotions provoked by the potential for danger, such as anger or fear. Crittenden terms this 'affective information'. In childhood this information would include emotions provoked by the unexplained absence of an attachment figure. Where an infant is faced with insensitive or rejecting parenting, one strategy for maintaining the availability of their attachment figure is to try to exclude from consciousness or from expressed behaviour any emotional information that might result in rejection.\n\n2. Causal or other sequentially-ordered knowledge about the potential for safety or danger. In childhood this would include knowledge regarding the behaviours that indicate an attachment figure's availability as a secure haven. If knowledge regarding the behaviours that indicate an attachment figure's availability as a secure haven is subject to segregation, then the infant can try to keep the attention of their caregiver through clingy or aggressive behaviour, or alternating combinations of the two. Such behaviour may increase the availability of an attachment figure who otherwise displays inconsistent or misleading responses to the infant's attachment behaviours, suggesting the unreliability of protection and safety.\n\nCrittenden proposes that both kinds of information can be split off from consciousness or behavioural expression as a 'strategy' to maintain the availability of an attachment figure: 'Type A strategies were hypothesized to be based on reducing perception of threat to reduce the disposition to respond. Type C was hypothesized to be based on heightening perception of threat to increase the disposition to respond' Type A strategies split off emotional information about feeling threatened and type C strategies split off temporally-sequenced knowledge about how and why the attachment figure is available. By contrast, type B strategies effectively use both kinds of information without much distortion. For example: a toddler may have come to depend upon a type C strategy of tantrums in working to maintain the availability of an attachment figure whose inconsistent availability has led the child to distrust or distort causal information about their apparent behaviour. This may lead their attachment figure to get a clearer grasp on their needs and the appropriate response to their attachment behaviours. Experiencing more reliable and predictable information about the availability of their attachment figure, the toddler then no longer needs to use coercive behaviours with the goal of maintaining their caregiver's availability and can develop a secure attachment to their caregiver since they trust that their needs and communications will be heeded.\n\nResearch based on data from longitudinal studies, such as the National Institute of Child Health and Human Development Study of Early Child Care and the Minnesota Study of Risk and Adaption from Birth to Adulthood, and from cross-sectional studies, consistently shows associations between early attachment classifications and peer relationships as to both quantity and quality. Lyons-Ruth, for example, found that 'for each additional withdrawing behavior displayed by mothers in relation to their infant's attachment cues in the Strange Situation Procedure, the likelihood of clinical referral by service providers was increased by 50%.'\n\nSecure children have more positive and fewer negative peer reactions and establish more and better friendships. Insecure-ambivalent children have a tendency to anxiously but unsuccessfully seek positive peer interaction whereas insecure-avoidant children appear aggressive and hostile and may actively repudiate positive peer interaction. On only a few measures is there any strong direct association between early experience and a comprehensive measure of social functioning in early adulthood but early experience significantly predicts early childhood representations of relationships, which in turn predicts later self and relationship representations and social behaviour.\n\nStudies have suggested that infants with a high-risk for Autism Spectrum Disorders (ASD) may express attachment security differently from infants with a low-risk for ASD. Behavioural problems and social competence in insecure children increase or decline with deterioration or improvement in quality of parenting and the degree of risk in the family environment.\n\nMichael Rutter describes the procedure in the following terms:\n\"It is by no means free of limitations (see Lamb, Thompson, Gardener, Charnov & Estes, 1984). To begin with, it is very dependent on brief\nseparations and reunions having the same meaning for all children. This may be a major constraint when applying the procedure in cultures, such as that in Japan (see Miyake et al., 1985), where infants are rarely separated from their mothers in ordinary circumstances. Also, because older children have a cognitive capacity to maintain relationships when the older person is not present, separation may not provide the same stress for them. Modified procedures based on the Strange Situation have been developed for older preschool children (see Belsky et al., 1994; Greenberg et al., 1990) but it is much more dubious whether the same approach can be used in middle childhood. Also, despite its manifest strengths, the procedure is based on just 20 minutes of behaviour. It can be scarcely expected to tap all the relevant qualities of a child's attachment relationships. Q-sort procedures based on much longer naturalistic observations in the home, and interviews with the mothers have developed in order to extend the data base (see Vaughn & Waters, 1990). A further constraint is that the coding procedure results in discrete categories rather than continuously distributed dimensions. Not only is this likely to provide boundary problems, but also it is not at all obvious that discrete categories best represent the concepts that are inherent in attachment security. It seems much more likely that infants vary in their degree of security and there is need for a measurement systems that can quantify individual variation\".\nWith respect to the ecological validity of the Strange Situation, a meta-analysis of 2,000 infant-parent dyads, including several from studies with non-Western language and/or cultural bases found the global distribution of attachment categorizations to be A (21%), B (65%), and C (14%). This global distribution was generally consistent with Ainsworth et al.'s (1978) original attachment classification distributions.\n\nHowever, controversy has been raised over a few cultural differences in these rates of 'global' attachment classification distributions. In particular, two studies diverged from the global distributions of attachment classifications noted above. One study was conducted in North Germany in which more avoidant (A) infants were found than global norms would suggest, and the other in Sapporo, Japan, where more resistant (C) infants were found. Of these two studies, the Japanese findings have sparked the most controversy as to the meaning of individual differences in attachment behaviour as originally identified by Ainsworth et al. (1978).\n\nIn a recent study conducted in Sapporo, Behrens et al. (2007) found attachment distributions consistent with global norms using the six-year Main & Cassidy scoring system for attachment classification. In addition to these findings supporting the global distributions of attachment classifications in Sapporo, Behrens et al. also discuss the Japanese concept of amae and its relevance to questions concerning whether the insecure-resistant (C) style of interaction may be engendered in Japanese infants as a result of the cultural practice of amae.\n\nA separate study was conducted in Korea, to help determine if mother-infant attachment relationships are universal or culture-specific. The results of the study of infant-mother attachment were compared to a national sample and showed that the four attachment patterns, secure, avoidance, ambivalent, and disorganized, exist in Korea as well as other varying cultures.\n\nVan IJzendoorn and Kroonenberg conducted a meta-analysis of various countries, including Japan, Israel, Germany, China, the UK and the USA using the Strange Situation. The research showed that though there were cultural differences, the four basic patterns, secure, avoidance, ambivalent, and disorganized can be found in every culture in which studies have been undertaken, even where communal sleeping arrangements are the norm. Selection of the secure pattern is found in the majority of children across cultures studied. This follows logically from the fact that attachment theory provides for infants to adapt to changes in the environment, selecting optimal behavioural strategies. How attachment is expressed shows cultural variations which need to be ascertained before studies can be undertaken.\n\nRegarding the issue of whether the breadth of infant attachment functioning can be captured by a categorical classification scheme, it should be noted that continuous measures of attachment security have been developed which have demonstrated adequate psychometric properties. These have been used either individually or in conjunction with discrete attachment classifications in many published reports. The original Richter's et al. (1998) scale is strongly related to secure versus insecure classifications, correctly predicting about 90% of cases. Readers further interested in the categorical versus continuous nature of attachment classifications (and the debate surrounding this issue) should consult a paper by Fraley and Spieker and the rejoinders in the same issue by many prominent attachment researchers including J. Cassidy, A. Sroufe, E. Waters & T. Beauchaine, and M. Cummings.\n\n"}
{"id": "57656591", "url": "https://en.wikipedia.org/wiki?curid=57656591", "title": "Australian Geographer", "text": "Australian Geographer\n\nAustralian Geographer (The Australian Geographer until 1975) is a quarterly peer-reviewed academic journal published by the Geographical Society of New South Wales since August 1928. Covering all aspects of Australian geography, it is currently copublished with Taylor & Francis.\n"}
{"id": "21689605", "url": "https://en.wikipedia.org/wiki?curid=21689605", "title": "Avalency", "text": "Avalency\n\nAvalency refers to the property of a verb of taking no arguments. Avalent verbs are verbs which have no valency, i.e. they have no logical arguments, such as subject, object, etc. A common example of such verbs in many languages is the set of verbs describing weather.\n\n\"It rains.\"<br>\n\"It snows.\"<br>\n\"It is freezing.\"<br>\n\"It is snowing.\"\n\nAvalency is more clearly demonstrated in pro-drop languages such as Spanish, which do not grammatically require a dummy pronoun as English does. For instance, the Spanish equivalent of \"it's raining\" is \"llueve\".\n\nAlthough in English these verbs do have what seems to be a subject, \"it\", it is arguably completely devoid of semantic meaning and merely a syntactic placeholder, a dummy subject. Contrary views of this use of \"it\" do exist, however, making it here potentially a \"quasi-(verb) argument\" or simply a normal subject.\n"}
{"id": "56065871", "url": "https://en.wikipedia.org/wiki?curid=56065871", "title": "Berkeley SETI Research Center", "text": "Berkeley SETI Research Center\n\nThe Berkeley SETI Research Center (BSRC) conducts experiments searching for optical and electromagnetic transmissions from intelligent extraterrestrial civilizations. The center is based at the University of California, Berkeley. \n\nThe Berkeley SETI Research Center has several SETI searches operating at various wavelengths, from radio, through infrared, to visible light. These include SERENDIP, SEVENDIP, NIROSETI, Breakthrough Listen, and SETI@home. The research center is also involved in the development of new telescopes and instrumentation. \n\nThe Berkeley SETI Research Center is independent of, but collaborates with, researchers at the SETI Institute. No unambiguous signals from extraterrestrial intelligence have been found. \n\nThe Berkeley SETI Research Center also hosts the Breakthrough Listen program, which is a ten-year initiative with $100 million funding begun in July 2015 to actively search for intelligent extraterrestrial communications in the universe, in a substantially expanded way, using resources that had not previously been extensively used for the purpose. It has been described as the most comprehensive search for alien communications to date. Announced in July 2015, the project is observing for thousands of hours every year on two major radio telescopes, the Green Bank Observatory in West Virginia, the Parkes Observatory in Australia, and the Automated Planet Finder telescope.\n\nThe center also created the SETI@home, an Internet-based public volunteer computing project employing the BOINC software platform, hosted by their Space Sciences Laboratory. Its purpose is to analyze radio data from radio telescopes for signs of extraterrestrial intelligence.\n\nThe SERENDIP program takes advantage of ongoing \"mainstream\" radio telescope observations and analyzes deep space radio telescope data that it obtains while other astronomers are using the telescope. SERENDIP observations have been conducted at frequencies between 400 MHz and 5 GHz, with most observations near the so-called Cosmic Water Hole (1.42 GHz (21 cm) neutral hydrogen and 1.66 GHz hydroxyl transitions).\n\nSEVENDIP, which stands for Search for Extraterrestrial Visible Emissions from Nearby Developed Intelligent Populations, was a project using visible wavelengths to search for extraterrestrial life's intelligent signals from outer space.\n\nThe NIROSETI (Near-InfraRed Optical Search for Extraterrestrial Intelligence) program searches for artificial signals in the optical (visible) and near infrared (NIR) wavebands of the electromagnetic spectrum. It uses the Nickel 1-m telescope at the Lick Observatory in California, USA. The instrument saw its first light on 15 March 2015 and was commissioned on January 2016.\n\nThe NIROSETI instrument employs a new generation of near-infrared (900 to 1700 nm) detectors, cooled at -25°C, that have a high speed response (>1 GHz) and gain comparable to photomultiplier tubes, while also producing very low noise, and significantly reducing false positives. Its field-of-view is 2.5\"x2.5\" each, and focuses\non detecting short (nanosecond) pulsed laser emissions. The NIROSETI instrument is also being used to study variability of very short natural near-infrared transient stars.\n\n"}
{"id": "6738308", "url": "https://en.wikipedia.org/wiki?curid=6738308", "title": "Borderless selling", "text": "Borderless selling\n\nBorderless selling is the process of selling services to clients outside the country of origin of services through modern methods which eliminate the actions specifically designed to hinder international trade. International trade through \"borderless selling\" is a new phenomenon born in the current \"globalization\" era.\n\nBorderless selling is defined as the process of performing sales transaction between two or more parties from different countries (an exporter and an importer) which is free from actions specifically designed to hinder international trade, such as tariff barriers, currency restrictions, and import quotas.\n\nInternational trade which is the exchange of goods and services across international borders has been present throughout much of history of economics, society and politics. \n\nIt is assumed that offshore outsourcing gave birth to \"borderless selling\". The selling of services by offshore outsourcing service providers to foreign clients is free from actions specifically designed to hinder international trade, such as tariff barriers, currency restrictions, and import quotas. This is largely because most of the services are sold or delivered electronically from the offshore service provider to the foreign client. This phenomenon gave birth to borderless selling.\n\nThere is a high correlation between outsourcing and exporting activity. However, borderless selling is different from free international trade or selling. Under the belief in Mercantilism, most nations had high tariffs and many restrictions on international trade for centuries. In the 19th century, a belief in free trade became paramount in west, especially in Britain and this outlook has since then dominated the thinking of western nations. Traditionally international trade was possible between only those countries which regulated international trade through bilateral treaties. Borderless selling is possible between any two countries of the world because services can be exported using modern telecommunication networks without the need to regulate trade.\n\nThe \"borderless selling\" term was originated as part of the research carried out by team led by Paramjeev Singh Sethi.\n\n\n\nMany services can be sold through borderless selling, popularly including:\n\nDifferent means used for borderless selling:\n\n"}
{"id": "1586839", "url": "https://en.wikipedia.org/wiki?curid=1586839", "title": "Cerberus (constellation)", "text": "Cerberus (constellation)\n\nCerberus is an obsolete constellation created by Hevelius, whose stars are now included in the constellation Hercules. It was depicted as a three-headed snake that Hercules is holding in his hand. The constellation is no longer in use. This constellation \"figure typified the serpent ... infesting the country around Taenarum the Μέτωπον of Greece, the modern Cape Matapan.\" The presence of Cerberus (Kerberos) at Taenarum (Tainaron) is mentioned by Strabo, Statius, and Seneca the Younger. John Senex combined this constellation with the likewise obsolete constellation Ramus Pomifer, an apple branch held by Hercules, in his 1721 star map to create \"Cerberus et Ramus\".\n\n"}
{"id": "3450602", "url": "https://en.wikipedia.org/wiki?curid=3450602", "title": "Chicago Botanic Garden", "text": "Chicago Botanic Garden\n\nThe Chicago Botanic Garden is a living plant museum situated on nine islands in the Cook County Forest Preserves. It features 27 display gardens in four natural habitats: McDonald Woods, Dixon Prairie, Skokie River Corridor, and Lakes and Shores. The address for the garden is 1000 Lake Cook Road, Glencoe, Illinois. The garden is open every day of the year. Admission is free, but parking is $30 per car (free for garden members).\n\nThe Chicago Botanic Garden is owned by the Forest Preserve District of Cook County, and managed by the Chicago Horticultural Society. It opened to the public in 1972, and is home to the Joseph Regenstein Jr. School of the Chicago Botanic Garden, offering a number of classes and certificate programs.\n\nThe Chicago Botanic Garden is accredited by the American Alliance of Museums and is a member of the American Public Gardens Association (APGA).\n\nThe Chicago Botanic Garden has 50,000 members, the largest membership of any U.S. public garden, and is Chicago's 7th largest cultural institution and 12th-ranking tourist attraction.\n\nThe 25 display gardens and four natural habitats include:\n\nThe architectural design for the Chicago Botanic Garden began with the creation of the master plan by John O. Simonds and Geoffrey Rausch. Several famous buildings have been designed by well-known architects since 1976.\n\n\nThe Chicago Botanic Garden opened the Daniel F. and Ada L. Rice Plant Conservation Science Center, located at the south end of the garden, to the public on September 23, 2009. In September 2010, the Plant Science Center earned a GOLD LEED (Leadership in Energy and Environmental Design) rating from the U.S. Green Building Council because of its sustainable design.\n\nThe Chicago Botanic Garden conserves rare plant species, and is working with regional, national and international organizations on behalf of plant conservation. The garden is a partner in the Seeds of Success project, a branch of the Millennium Seed Bank Partnership managed by the Royal Botanic Gardens, Kew. The goal is to collect 10,000 seeds from each of 1,500 native species of the Midwest for conservation and restoration efforts. The garden is also a partner in the Plants of Concern initiative to monitor rare species in Northeastern Illinois.\n\nThe garden is a member of Chicago Wilderness, a consortium of 200 local institutions dedicated to preserving and restoring Chicago's natural areas, as well as the Center for Plant Conservation, a group of 30 other botanic gardens and arboreta committed to conserving rare plants from their regions.\n\nDegree programs offered at the School of the Chicago Botanic Garden:\n\nCertificate programs offered at the School of the Chicago Botanic Garden:\n\nOther educational programs available at the garden include the Green Youth Farm, the Windy City Harvest, and the Cook County Sheriff's Vocational Rehabilitation Impact Center.\n\nIn 2008, the Chicago Botanic Garden was chosen by the United Nations Environment Programme (UNEP) as the sole North American host for World Environment Day with the theme \"CO—Kick the Habit!: Towards a Low Carbon Economy\".\n\nOver 30 nonprofit, academic, cultural, and environmental organizations participated in the \"Knowledge and Action Marketplace\" on the garden's Esplanade. Displays and representatives discussed products to help green homes, local carpools, volunteer and community conservation programs, classes on green gardening, the use of CFL light bulbs, vehicles that run on used vegetable oil, and appliances that pop popcorn using solar energy.\n\nOrganizations participating in the event included:\n\n\nThe garden hosted its first International Climate Change Forum on that day, featuring national and international experts, including Dr. Ashok Khosla, former chairman on the UNEP; Fred Krupp, president of the Environmental Defense Fund; Mary Grade, former regional administrator for EPA region 5; Suzanne Malec-McKenna, commissioner of the Department of the Environment for the City of Chicago; John Rowe, chief executive officer of the Exelon Corporation; Arthur J. Gibson, vice president of environment, health & safety for Baxter International; and Arthur Armishaw, chief technology and services officer for HSBC—North America.\n\nOn June 5 of each year, the garden and other venues around the world highlight resources and initiatives that promote low carbon economies and lifestyles, such as improved energy efficiency, alternative energy sources, forest conservation, and eco-friendly consumption.\n\nThe first generation of sustainable gardens at the Chicago Botanic Garden were the victory gardens of World Wars I and II. Today’s gardens incorporate food and paper scrap composting, sustainable irrigation, and a minimal use of fertilizer and pesticides. The Chicago Botanic Garden also encourages others to garden sustainably by composting food waste, installing backyard rain barrels, using native plants, removing invasive species, and establishing perennials. The Windy City Harvest program offers workshops in sustainable urban horticulture and urban agriculture.\n\nIn 2010, the Corporate Roundtable on Sustainability was established to encourage companies to act sustainably.\n\nProject BudBurst is a nationwide initiative to help scientists understand the effects of climate change on plants by recording the timing of leafing, flowering, and fruiting (also known as phenological events), of a variety of plant species. The project started as a three-month pilot program in 2007. Thousands of observations have been amassed in subsequent years from students, gardeners, and others citizen scientists in all 50 states. When combined with other studies on insects, birds, and other pollinators, Project BudBurst aims to help scientists measure asynchronous plant-pollinator activities in light of climate change.\n\nProject BudBurst is co-managed by the National Ecological Observatory Network, Inc. (NEON), and the Chicago Botanic Garden. It is funded by the National Ecological Observatory Network and the National Geographic Education Foundation.\n\nIn 2006, the Chicago Botanic Garden received the 'Award for Garden Excellence', given yearly by the APGA and \"Horticulture\" magazine to a public garden that exemplifies the highest standards of horticultural practices and has shown a commitment to supporting and demonstrating best gardening practices.\n\nIn 2012, The Chicago Botanic Garden was chosen as one of 10 \"Great Place\" (Public Space) for providing food locally, excellence in design, education and outreach, and sustainability by the American Planning Association, which selects \"Great Places\" in the United States annually to highlight good places for people to work and to live, representing a \"true sense of place, cultural and historical interest\".\n\n\n\n"}
{"id": "3340637", "url": "https://en.wikipedia.org/wiki?curid=3340637", "title": "Clinamen", "text": "Clinamen\n\nClinamen (; plural \"clinamina\", derived from \"clīnāre\", to incline) is the Latin name Lucretius gave to the unpredictable swerve of atoms, in order to defend the atomistic doctrine of Epicurus. In modern English it has come more generally to mean an inclination or a bias.\n\nAccording to Lucretius, the unpredictable swerve occurs \"at no fixed place or time\":\nWhen atoms move straight down through the void by their own weight, they deflect a bit in space at a quite uncertain time and in uncertain places, just enough that you could say that their motion has changed. But if they were not in the habit of swerving, they would all fall straight down through the depths of the void, like drops of rain, and no collision would occur, nor would any blow be produced among the atoms. In that case, nature would never have produced anything.\n\nThis swerving, according to Lucretius, provides the \"free will which living things throughout the world have\". Lucretius never gives the primary cause of the deflections. \n\nIn modern English clinamen is defined as an inclination or a bias. It implies that one is inclined or biased towards introducing a plausible but unprovable \"clinamen\" when a specific mechanism cannot be found to refute a credible argument against one's hypothesis or theory. The OED gives its first recorded use in English by Jonathan Swift in his 1704 \"Tale of a Tub\" ix.166, satirizing the atomistic theory of Epicurus:\nEpicurus modestly hoped that one time or other, a certain fortuitous concourse of all men's opinions—after perpetual justlings, the sharp with the smooth, the light and the heavy, the round and the square—would, by certain clinamina, unite in the notions of atoms and void, as these did in the originals of all things.\n\nThe term has been taken up by Harold Bloom to describe the inclinations of writers to \"swerve\" from the influence of their predecessors; it is the first of his \"Ratios of Revision\" as described in \"The Anxiety of Influence\".\n\nIn \"Difference and Repetition\", Gilles Deleuze employs the term in his description of \"multiplicities\". In addition, other French writers such as Simone de Beauvoir, Jacques Lacan, Jacques Derrida, Jean-Luc Nancy, Alain Badiou, and Michel Serres have made extensive use of the word 'clinamen' in their writings, albeit with very different meanings.\n\nLucretius' concept is central to the book \"\", written by Stephen Greenblatt.\n\n\"Clinamen\" is defined by Alfred Jarry in Chapter 33 of his \"Exploits and Opinions of Dr. Faustroll, Pataphysician\". The notion later figured in the imaginary science of the Jarry-inspired College of Pataphysics and the experimental literature of OuLiPo.\n\n"}
{"id": "5951578", "url": "https://en.wikipedia.org/wiki?curid=5951578", "title": "Community Coordinated Modeling Center", "text": "Community Coordinated Modeling Center\n\nThe Community Coordinated Modeling Center (CCMC) is a collaborative effort between multiple organizations to provide information and models relating to space weather research. The partnership includes resources from NASA, the Air Force Materiel Command, Air Force Research Laboratory (AFRL), Air Force Weather Agency (AFWA), NOAA, NSF, and ONR. Quoted from the site's main page, the CCMC is \"a multi-agency partnership to enable, support and perform the research and development for next generation space science and space weather models.\" The CCMC is based at the NASA Goddard Space Flight Center in Greenbelt, Maryland.\n\n"}
{"id": "6884590", "url": "https://en.wikipedia.org/wiki?curid=6884590", "title": "Cyclol", "text": "Cyclol\n\nThe cyclol hypothesis is the first structural model of a folded, globular protein. It was developed by Dorothy Wrinch in the late 1930s, and was based on three assumptions. Firstly, the hypothesis assumes that two peptide groups can be crosslinked by a cyclol reaction (Figure 1); these crosslinks are \"covalent\" analogs of \"non-covalent\" hydrogen bonds between peptide groups. These reactions have been observed in the ergopeptides and other compounds. Secondly, it assumes that, under some conditions, amino acids will naturally make the maximum possible number of cyclol crosslinks, resulting in cyclol molecules (Figure 2) and cyclol fabrics (Figure 3). These cyclol molecules and fabrics have never been observed. Finally, the hypothesis assumes that globular proteins have a tertiary structure corresponding to Platonic solids and semiregular polyhedra formed of cyclol fabrics with no free edges. Such \"closed cyclol\" molecules have not been observed either.\n\nAlthough later data demonstrated that this original model for the structure of globular proteins needed to be amended, several elements of the cyclol model were verified, such as the cyclol reaction itself and the hypothesis that hydrophobic interactions are chiefly responsible for protein folding. The cyclol hypothesis stimulated many scientists to research questions in protein structure and chemistry, and was a precursor of the more accurate models hypothesized for the DNA double helix and protein secondary structure. The proposal and testing of the cyclol model also provides an excellent illustration of empirical falsifiability acting as part of the scientific method.\n\nBy the mid-1930s, analytical ultracentrifugation studies by Theodor Svedberg had shown that proteins had a well-defined chemical structure, and were not aggregations of small molecules. The same studies appeared to show that the molecular weight of proteins fell into a few well-defined classes related by integers, such as \"M\" = 23 Da, where \"p\" and \"q\" are nonnegative integers. However, it was difficult to determine the exact molecular weight and number of amino acids in a protein. Svedberg had also shown that a change in solution conditions could cause a protein to disassemble into small subunits, now known as a change in quaternary structure.\n\nThe chemical structure of proteins was still under debate at that time. The most accepted (and ultimately correct) hypothesis was that proteins are linear polypeptides, i.e., unbranched polymers of amino acids linked by peptide bonds. However, a typical protein is remarkably long—hundreds of amino-acid residues—and several distinguished scientists were unsure whether such long, linear macromolecules could be stable in solution. Further doubts about the polypeptide nature of proteins arose because some enzymes were observed to cleave proteins but not peptides, whereas other enzymes cleave peptides but not folded proteins. Attempts to synthesize proteins in the test tube were unsuccessful, mainly due to the chirality of amino acids; naturally occurring proteins are composed of only \"left-handed\" amino acids. Hence, alternative chemical models of proteins were considered, such as the diketopiperazine hypothesis of Emil Abderhalden. However, no alternative model had yet explained why proteins yield only amino acids and peptides upon hydrolysis and proteolysis. As clarified by Linderstrøm-Lang, these proteolysis data showed that denatured proteins were polypeptides, but no data had yet been obtained about the structure of folded proteins; thus, denaturation could involve a chemical change that converted folded proteins into polypeptides.\n\nThe process of protein denaturation (as distinguished from coagulation) had been discovered in 1910 by Harriette Chick and Charles Martin, but its nature was still mysterious. Tim Anson and Alfred Mirsky had shown that denaturation was a \"reversible, two-state process\" that results in many chemical groups becoming available for chemical reactions, including cleavage by enzymes. In 1929, Hsien Wu hypothesized correctly that denaturation corresponded to protein unfolding, a purely conformational change that resulted in the exposure of amino-acid side chains to the solvent. Wu's hypothesis was also advanced independently in 1936 by Mirsky and Linus Pauling. Nevertheless, protein scientists could not exclude the possibility that denaturation corresponded to a \"chemical\" change in the protein structure, a hypothesis that was considered a (distant) possibility until the 1950s.\n\nX-ray crystallography had just begun as a discipline in 1911, and had advanced relatively rapidly from simple salt crystals to crystals of complex molecules such as cholesterol. However, even the smallest proteins have over 1000 atoms, which makes determining their structure far more complex. In 1934, Dorothy Crowfoot Hodgkin had taken crystallographic data on the structure of the small protein, insulin, although the structure of that and other proteins were not solved until the late 1960s. However, pioneering X-ray fiber diffraction data had been collected in the early 1930s for many natural fibrous proteins such as wool and hair by William Astbury, who proposed rudimentary models of secondary structure elements such as the alpha helix and the beta sheet.\n\nSince protein structure was so poorly understood in the 1930s, the physical interactions responsible for stabilizing that structure were likewise unknown. Astbury hypothesized that the structure of fibrous proteins was stabilized by hydrogen bonds in β-sheets. The idea that globular proteins are also stabilized by hydrogen bonds was proposed by Dorothy Jordan Lloyd in 1932, and championed later by Alfred Mirsky and Linus Pauling. At a 1933 lecture by Astbury to the Oxford Junior Scientific Society, physicist Frederick Frank suggested that the fibrous protein α-keratin might be stabilized by an alternative mechanism, namely, \"covalent\" crosslinking of the peptide bonds by the cyclol reaction above. The cyclol crosslink draws the two peptide groups close together; the N and C atoms are separated by ~1.5 Å, whereas they are separated by ~3 Å in a typical hydrogen bond. The idea intrigued J. D. Bernal, who suggested it to the mathematician Dorothy Wrinch as possibly useful in understanding protein structure.\n\nWrinch developed this suggestion into a full-fledged model of protein structure. The basic cyclol model was laid out in her first paper (1936). She noted the possibility that polypeptides might cyclize to form closed rings (true) and that these rings might form internal crosslinks through the cyclol reaction (also true, although rare). Assuming that the cyclol form of the peptide bond could be more stable than the amide form, Wrinch concluded that certain cyclic peptides would naturally make the maximal number of cyclol bonds (such as cyclol 6, Figure 2). Such cyclol molecules would have hexagonal symmetry, if the chemical bonds were taken as having the same length, roughly 1.5 Å; for comparison, the N-C and C-C bonds have the lengths 1.42 Å and 1.54 Å, respectively.\n\nThese rings can be extended indefinitely to form a cyclol fabric (Figure 3). Such fabrics exhibit a long-range, quasi-crystalline order that Wrinch felt was likely in proteins, since they must pack hundreds of residues densely. Another interesting feature of such molecules and fabrics is that their amino-acid side chains point axially upwards from only one face; the opposite face has no side chains. Thus, one face is completely independent of the primary sequence of the peptide, which Wrinch conjectured might account for sequence-independent properties of proteins.\n\nIn her initial article, Wrinch stated clearly that the cyclol model was merely a \"working hypothesis\", a potentially valid model of proteins that would have to be checked. Her goals in this article and its successors were to propose a well-defined testable model, to work out the consequences of its assumptions and to make predictions that could be tested experimentally. In these goals, she succeeded; however, within a few years, experiments and further modeling showed that the cyclol hypothesis was untenable as a model for globular proteins.\n\nIn two tandem Letters to the Editor (1936), Wrinch and Frank addressed the question of whether the cyclol form of the peptide group was indeed more stable than the amide form. A relatively simple calculation showed that the cyclol form is significantly \"less\" stable than the amide form. Therefore, the cyclol model would have to be abandoned unless a compensating source of energy could be identified. Initially, Frank proposed that the cyclol form might be stabilized by better interactions with the surrounding solvent; later, Wrinch and Irving Langmuir hypothesized that hydrophobic association of nonpolar sidechains provides stabilizing energy to overcome the energetic cost of the cyclol reactions.\n\nThe lability of the cyclol bond was seen as an \"advantage\" of the model, since it provided a natural explanation for the properties of denaturation; reversion of cyclol bonds to their more stable amide form would open up the structure and allows those bonds to be attacked by proteases, consistent with experiment. Early studies showed that proteins denatured by pressure are often in a different state than the same proteins denatured by high temperature, which was interpreted as possibly supporting the cyclol model of denaturation.\n\nThe Langmuir-Wrinch hypothesis of hydrophobic stabilization shared in the downfall of the cyclol model, owing mainly to the influence of Linus Pauling, who favored the hypothesis that protein structure was stabilized by hydrogen bonds. Another twenty years had to pass before hydrophobic interactions were recognized as the chief driving force in protein folding.\n\nIn her third paper on cyclols (1936), Wrinch noted that many \"physiologically active\" substances such as steroids are composed of fused hexagonal rings of carbon atoms and, thus, might be sterically complementary to the face of cyclol molecules without the amino-acid side chains. Wrinch proposed that steric complementarity was one of chief factors in determining whether a small molecule would bind to a protein.\n\nWrinch speculated that proteins are responsible for the synthesis of all biological molecules. Noting that cells digest their proteins only under extreme starvation conditions, Wrinch further speculated that life could not exist without proteins.\n\nFrom the beginning, the cyclol reaction was considered as a covalent analog of the hydrogen bond. Therefore, it was natural to consider hybrid models with both types of bonds. This was the subject of Wrinch's fourth paper on the cyclol model (1936), written together with Dorothy Jordan Lloyd, who first proposed that globular proteins are stabilized by hydrogen bonds. A follow-up paper was written in 1937 that referenced other researchers on hydrogen bonding in proteins, such as Maurice Loyal Huggins and Linus Pauling.\n\nWrinch also wrote a paper with William Astbury, noting the possibility of a keto-enol isomerization of the >CH and an amide carbonyl group >C=O, producing a crosslink >C-C(OH)< and again converting the oxygen to a hydroxyl group. Such reactions could yield five-membered rings, whereas the classic cyclol hypothesis produces six-membered rings. This keto-enol crosslink hypothesis was not developed much further.\n\nIn her fifth paper on cyclols (1937), Wrinch identified the conditions under which two planar cyclol fabrics could be joined to make an angle between their planes while respecting the chemical bond angles. She identified a mathematical simplification, in which the non-planar six-membered rings of atoms can be represented by planar \"median hexagon\"s made from the midpoints of the chemical bonds. This \"median hexagon\" representation made it easy to see that the cyclol fabric planes can be joined correctly if the dihedral angle between the planes equals the tetrahedral bond angle δ = arccos(-1/3) ≈ 109.47°.\n\nA large variety of closed polyhedra meeting this criterion can be constructed, of which the simplest are the truncated tetrahedron, the truncated octahedron, and the octahedron, which are Platonic solids or semiregular polyhedra. Considering the first series of \"closed cyclols\" (those modeled on the truncated tetrahedron), Wrinch showed that their number of amino acids increased quadratically as 72\"n\", where \"n\" is the index of the closed cyclol \"C\". Thus, the \"C\" cyclol has 72 residues, the \"C\" cyclol has 288 residues, etc. Preliminary experimental support for this prediction came from Max Bergmann and Carl Niemann, whose amino-acid analyses suggested that proteins were composed of integer multiples of 288 amino-acid residues (\"n\"=2). More generally, the cyclol model of globular proteins accounted for the early analytical ultracentrifugation results of Theodor Svedberg, which suggested that the molecular weights of proteins fell into a few classes related by integers.\n\nThe cyclol model was consistent with the general properties then attributed to folded proteins. (1) Centrifugation studies had shown that folded proteins were significantly denser than water (~1.4 g/mL) and, thus, tightly packed; Wrinch assumed that dense packing should imply \"regular\" packing. (2) Despite their large size, some proteins crystallize readily into symmetric crystals, consistent with the idea of symmetric faces that match up upon association. (3) Proteins bind metal ions; since metal-binding sites must have specific bond geometries (e.g., octahedral), it was plausible to assume that the entire protein also had similarly crystalline geometry. (4) As described above, the cyclol model provided a simple \"chemical\" explanation of denaturation and the difficulty of cleaving folded proteins with proteases. (5) Proteins were assumed to be responsible for the synthesis of all biological molecules, including other proteins. Wrinch noted that a fixed, uniform structure would be useful for proteins in templating their own synthesis, analogous to the Watson-Francis Crick concept of DNA templating its own replication. Given that many biological molecules such as sugars and sterols have a hexagonal structure, it was plausible to assume that their synthesizing proteins likewise had a hexagonal structure. Wrinch summarized her model and the supporting molecular-weight experimental data in three review articles.\n\nHaving proposed a model of globular proteins, Wrinch investigated whether it was consistent with the available structural data. She hypothesized that bovine tuberculin protein (523) was a \"C\" closed cyclol consisting of 72 residues and that the digestive enzyme pepsin was a \"C\" closed cyclol of 288 residues. These residue-number predictions were difficult to verify, since the methods then available to measure the mass of proteins were inaccurate, such as analytical ultracentrifugation and chemical methods.\n\nWrinch also predicted that insulin was a \"C\" closed cyclol consisting of 288 residues. Limited X-ray crystallographic data were available for insulin which Wrinch interpreted as \"confirming\" her model. However, this interpretation drew rather severe criticism for being premature. Careful studies of the Patterson diagrams of insulin taken by Dorothy Crowfoot Hodgkin showed that they were roughly consistent with the cyclol model; however, the agreement was not good enough to claim that the cyclol model was confirmed.\n\nThe cyclol fabric was shown to be implausible for several reasons. Hans Neurath and Henry Bull showed that the dense packing of side chains in the cyclol fabric was inconsistent with the experimental density observed in protein films. Maurice Huggins calculated that several non-bonded atoms of the cyclol fabric would approach more closely than allowed by their van der Waals radii; for example, the inner H and C atoms of the lacunae would be separated by only 1.68 Å (Figure 5). Haurowitz showed chemically that the outside of proteins could not have a large number of hydroxyl groups, a key prediction of the cyclol model, whereas Meyer and Hohenemser showed that cyclol condensations of amino acids did not exist even in minute quantities as a transition state. More general chemical arguments against the cyclol model were given by Bergmann and Niemann and by Neuberger. Infrared spectroscopic data showed that the number of carbonyl groups in a protein did not change upon hydrolysis, and that intact, folded proteins have a full complement of amide carbonyl groups; both observations contradict the cyclol hypothesis that such carbonyls are converted to hydroxyl groups in folded proteins. Finally, proteins were known to contain proline in significant quantities (typically 5%); since proline lacks the amide hydrogen and its nitrogen already forms three covalent bonds, proline seems incapable of the cyclol reaction and of being incorporated into a cyclol fabric. An encyclopedic summary of the chemical and structural evidence against the cyclol model was given by Pauling and Niemann. Moreover, a supporting piece of evidence—the result that all proteins contain an integer multiple of 288 amino-acid residues—was likewise shown to be incorrect in 1939.\n\nWrinch replied to the steric-clash, free-energy, chemical and residue-number criticisms of the cyclol model. On steric clashes, she noted that small deformations of the bond angles and bond lengths would allow these steric clashes to be relieved, or at least reduced to a reasonable level. She noted that distances between non-bonded groups within a single molecule can be shorter than expected from their van der Waals radii, e.g., the 2.93 Å distance between methyl groups in hexamethylbenzene. Regarding the free-energy penalty for the cyclol reaction, Wrinch disagreed with Pauling's calculations and stated that too little was known of intramolecular energies to rule out the cyclol model on that basis alone. In reply to the chemical criticisms, Wrinch suggested that the model compounds and simple bimolecular reactions studied need not pertain to the cyclol model, and that steric hindrance may have prevented the surface hydroxyl groups from reacting. On the residue-number criticism, Wrinch extended her model to allow for other numbers of residues. In particular, she produced a \"minimal\" closed cyclol of only 48 residues, and, on that (incorrect) basis, may have been the first to suggest that the insulin monomer had a molecular weight of roughly 6000 Da.\n\nTherefore, she maintained that the cyclol model of globular proteins was still potentially viable and even proposed the cyclol fabric as a component of the cytoskeleton. However, most protein scientists ceased to believe in it and Wrinch turned her scientific attention to mathematical problems in X-ray crystallography, to which she contributed significantly. One exception was physicist Gladys Anslow, Wrinch's colleague at Smith College, who studied the ultraviolet absorption spectra of proteins and peptides in the 1940s and allowed for the possibility of cyclols in interpreting her results. As the sequence of insulin began to be determined by Frederick Sanger, Anslow published a three-dimensional cyclol model with sidechains, based on the backbone of Wrinch's 1948 \"minimal cyclol\" model.\n\nThe downfall of the overall cyclol model generally led to a rejection of its elements; one notable exception was J. D. Bernal's short-lived acceptance of the Langmuir-Wrinch hypothesis that protein folding is driven by hydrophobic association. Nevertheless, cyclol bonds were identified in small, naturally occurring cyclic peptides in the 1950s.\n\nClarification of the modern terminology is appropriate. The classic cyclol reaction is the addition of the NH amine of a peptide group to the C=O carbonyl group of another; the resulting compound is now called an azacyclol. By analogy, an oxacyclol is formed when an OH hydroxyl group is added to a peptidyl carbonyl group. Likewise, a thiacyclol is formed by adding an SH thiol moiety to a peptidyl carbonyl group.\n\nThe oxacyclol alkaloid ergotamine from the fungus \"Claviceps purpurea\" was the first identified cyclol. The cyclic depsipeptide serratamolide is also formed by an oxacyclol reaction. Chemically analogous cyclic thiacyclols have also been obtained. Classic azacyclols have been observed in small molecules and tripeptides. Peptides are naturally produced from the reversion of azacylols, a key prediction of the cyclol model. Hundreds of cyclol molecules have now been identified, despite Linus Pauling's calculation that such molecules should not exist because of their unfavorably high energy.\n\nAfter a long hiatus during which she worked mainly on the mathematics of X-ray crystallography, Wrinch responded to these discoveries with renewed enthusiasm for the cyclol model and its relevance in biochemistry. She also published two books describing the cyclol theory and small peptides in general.\n\nThe cyclol model of protein structure is an example of empirical falsifiability acting as part of the scientific method. An original hypothesis is made that accounts for unexplained experimental observations; the consequences of this hypothesis are worked out, leading to predictions that are tested by experiment. In this case, the key hypothesis was that the cyclol form of the peptide group could be favored over the amide form. This hypothesis led to the predictions of the cyclol-6 molecule and the cyclol fabric, which in turn suggested the model of semi-regular polyhedra for globular proteins. A key testable prediction was that a folded protein's carbonyl groups should be largely converted to hydroxyl groups; however, spectroscopic and chemical experiments showed that this prediction was incorrect. The cyclol model also predicts a high lateral density of amino acids in folded proteins and in films that does not agree with experiment. Hence, the cyclol model could be rejected and the search begun for new hypotheses of protein structure, such as the models of the alpha helix proposed in the 1940s and 1950s.\n\nIt is sometimes argued that the cyclol hypothesis should never have been advanced, because of its \"a priori\" flaws, e.g., its steric clashes, its inability to accommodate proline, and the high free energy disfavoring the cyclol reaction itself. Although such flaws rendered the cyclol hypothesis \"implausible\", they did not make it \"impossible\". The cyclol model was the first well-defined structure proposed for globular proteins, and too little was then known of intramolecular forces and protein structure to reject it immediately. It neatly explained several general properties of proteins and accounted for then-anomalous experimental observations. Although generally incorrect, some elements of the cyclol theory were eventually verified, such as the cyclol reactions and the role of hydrophobic interactions in protein folding. A useful comparison is the Bohr model of the hydrogen atom, which was considered implausible from its inception, even by its creator, yet led the way to the ultimately correct theory of quantum mechanics. Similarly, Linus Pauling proposed a well-defined model of DNA that was likewise implausible yet thought-provoking to other investigators.\n\nConversely, the cyclol model is an example of an incorrect scientific theory of great symmetry and beauty, two qualities that can be regarded as signs of \"obviously true\" scientific theories. For example, the Watson-Crick double helix model of DNA is sometimes said to be \"obvious\" because of its plausible hydrogen bonding and symmetry; nevertheless, other, less symmetrical structures of DNA are favored under different conditions. Similarly, the beautiful theory of general relativity was considered by Albert Einstein as not needing experimental verification; yet even this theory will require revision for consistency with quantum field theory.\n\n"}
{"id": "14230641", "url": "https://en.wikipedia.org/wiki?curid=14230641", "title": "Douglas Emlong", "text": "Douglas Emlong\n\nDouglas Ralph Emlong (April 17, 1942–June 1980) was an amateur fossil collector from the Oregon Coast in the northwestern United States. His collections contributed to the discovery and description of numerous extinct marine mammal species, many of which are ancestral to extant groups. Described as an 'indefatigable' fossil collector with 'Promethian prowess in discovery of unprecedented vertebrate fossils', he contributed substantially to the field from the age of fourteen. The ancestral pinniped \"Enaliarctos emlongi\" was named in his honor by Annalisa Berta in 1991.\n\nFossils discovered by Douglas Emlong include:\n\nMarine Mammals:\nBirds:\n\n"}
{"id": "13566030", "url": "https://en.wikipedia.org/wiki?curid=13566030", "title": "Edith Durham", "text": "Edith Durham\n\nMary Edith Durham (8 December 1863 – 15 November 1944) was a British traveller, artist and writer who became famous for her anthropological accounts of life in Albania in the early 20th century.\n\nDurham was the eldest of nine children; her father, Arthur Edward Durham, was a distinguished London surgeon.She attended Bedford College (1878–82) followed by the Royal Academy of Arts to train as an artist. She exhibited widely and contributed a number of detailed drawings to the amphibia and reptiles volume of the \"Cambridge Natural History\" (published 1899).\n\nAfter the death of her father, Durham took on the responsibilities of caring for her sick mother for several years. It proved an exhausting experience; when she was 37, her doctor recommended that she should undertake a foreign vacation to recuperate. She took a trip by sea down the coast of Dalmatia, travelling from Trieste to Kotor and then overland to Cetinje, the capital of Montenegro. It gave her a taste for southern Balkan life that she was to retain for the rest of her life.\n\nDurham travelled extensively in the Balkans over the next twenty years, focusing particularly on Albania, which then was one of the most isolated and undeveloped areas of Europe. She worked in a variety of relief organisations, painted and wrote, and collected folklore and folk art. Her work was of genuine anthropological significance; she contributed frequently to the journal \"Man\" and became a Fellow of the Royal Anthropological Institute. Her writings, however, were to earn her particular fame. She wrote seven books on Balkan affairs, of which \"High Albania\" (1909) is the best known. It is still regarded as the pre-eminent guide to the customs and society of the highlands of northern Albania.\n\nAfter her pro-Serb phase, Durham came to identify closely with the Albanian cause and championed the unity and independence of the Albanian people. She earned a reputation as a difficult and eccentric person, and was strongly criticised by - and criticised in turn - advocates of a Yugoslav state, who supported the incorporation of Albanian-populated parts of Kosovo into Yugoslavia. She became increasingly anti-Serb, denouncing what she termed \"Serb vermin\" for having \"not created a Jugoslavia but have carried out their original aim of making Great Serbia ... Far from being liberated the bulk of people live under a far harsher rule than before.\"\n\nOther, more pro-Serb British intellectuals sharply criticised her views. Rebecca West included Durham in her description of the sort of traveller who came back \"with a pet Balkan people established in their hearts as suffering and innocent, eternally the massacree and never the massacrer,\" (Durham sued West over this) and then went on to say \"the Albanians, as championed by Miss Durham, strongly resembled Sir Joshua Reynolds's picture of the Infant Samuel.\" R.W. Seton-Watson commented that \"the fact is that while always denouncing 'Balkan mentality', she is herself exactly what she means by the word.\"\n\nFor their part, however, the Albanians held Durham in high regard. They dubbed her \"Mbretëresha e Malësoreve\" – the \"Queen of the Highlanders.\" She was well received in the Albanian highlands and passed unmolested despite being a lone female traveller. She benefited from the Albanian tradition of insuring a guest's safety, and from an ancient Albanian custom, the tradition of \"Sworn virgins\" – women who wore men's clothes and were regarded as protected individuals. When she died in 1944 she received high praise for her work from the exiled King Zog, who wrote: \"She gave us her heart and she won the ear of our mountaineers.\" She is still regarded as something of a national heroine; in 2004, Albanian President Alfred Moisiu described her as \"one of the most distinguished personalities of the Albanian world during the last century\"\n\nMuch of Durham's work was donated to academic collections following her death. Her papers are held by the Royal Anthropological Institute, London, her diaries are in the Bankfield Museum, Halifax along with her collections of Balkan costume and jewellery given in 1935. Further gifts of mostly Balkan artefacts were given to the British Museum in 1914 and to the Pitt Rivers Museum, Oxford and the Horniman Museum, London.\n\n\n\n"}
{"id": "25211625", "url": "https://en.wikipedia.org/wiki?curid=25211625", "title": "Electric dipole transition", "text": "Electric dipole transition\n\nElectric dipole transition is the dominant effect of an interaction of an electron in an atom with the electromagnetic field.\n\nFollowing, consider an electron in an atom with quantum Hamiltonian formula_1, interacting with a plane electromagnetic wave\n\nWrite the Hamiltonian of the electron in this electromagnetic field as\n\nformula_3\n\nTreating this system by means of time-dependent perturbation theory, one finds that the most likely transitions of the electron from one state to the other occur due to the summand of formula_4 written as\n\nElectric dipole transitions are the transitions between energy levels in the system with the Hamiltonian formula_6.\n\nBetween certain electron states the electric dipole transition rate may be zero due to one or more selection rules, particularly the angular momentum selection rule. In such a case, the transition is termed \"electric dipole forbidden\", and the transitions between such levels must be approximated by \"higher-order transitions\".\n\nThe next order summand in formula_4 is written as\n\nand describes magnetic dipole transitions.\n\nEven smaller contributions to transition rates are given by higher electric and magnetic multipole transitions.\n\n"}
{"id": "14949505", "url": "https://en.wikipedia.org/wiki?curid=14949505", "title": "Entrainment (biomusicology)", "text": "Entrainment (biomusicology)\n\nEntrainment in the biomusicological sense refers to the synchronization of organisms (only humans as a whole, with some particular instances of a particular animal) to an external perceived rhythm, such as human music and dance such as foot tapping.\n\nBeat induction is the process in which a regular isochronous pulse is activated while one listens to music (i.e. the beat to which one would tap one's foot). It was thought that the cognitive mechanism that allows us to infer a beat from a sound pattern, and to synchronize or dance to it, was uniquely human. No primate tested so far—with exception of the human species—can dance or collaboratively clap to the beat of the music. Humans know when to start, when to stop, when to speed up or to slow down, in synchronizing with their fellow dancers or musicians. Although primates do not appear to display beat induction, some parrots do. The most famous example, Snowball was shown to display genuine dance, including changing his movements to a change in tempo (Patel et al., 2009)\n\nBeat induction can be seen as a fundamental cognitive skill that allows for music (e.g., Patel, 2008; Honing, 2007; 2012). We can hear a pulse in a rhythmic pattern while it might not even be explicitly in there: The pulse is being induced (hence the name) while listening—like a perspective can be induced by looking at an arrangement of objects in a picture.\n\nNeuroscientist Ani Patel proposes beat induction—referring to it as \"beat-based rhythm processing\"—as a key area in music-language research, suggesting beat induction \"a fundamental aspect of music cognition that is not a byproduct of cognitive mechanisms that also serve other, more clearly adaptive, domains (e.g. auditory scene analysis or language)\" (Patel, 2008).\n\nJoseph Jordania recently suggested that the human ability to be entrained was developed by the forces of natural selection as an important part of achieving the specific altered state of consciousness, battle trance. Achieving this state, in which humans lose their individuality, do not feel fear and pain, are united in a shared collective identity, and act in the best interests of the group, was crucial for the physical survival of our ancestors against the big African predators, after hominids descended from the safer trees to the dangerous ground and became terrestrial.\n\n\n\n"}
{"id": "5708736", "url": "https://en.wikipedia.org/wiki?curid=5708736", "title": "Feedback linearization", "text": "Feedback linearization\n\nFeedback linearization is a common approach used in controlling nonlinear systems. The approach involves coming up with a transformation of the nonlinear system into an equivalent linear system through a change of variables and a suitable control input. Feedback linearization may be applied to nonlinear systems of the form\n\nwhere formula_2 is the state vector, formula_3 is the vector of inputs, and formula_4 is the vector of outputs. The goal is to develop a control input\nthat renders a linear input–output map between the new input formula_6 and the output. An outer-loop control strategy for the resulting linear control system can then be applied.\n\nHere, consider the case of feedback linearization of a single-input single-output (SISO) system. Similar results can be extended to multiple-input multiple-output (MIMO) systems. In this case, formula_7 and formula_8. The objective is to find a coordinate transformation formula_9 that transforms the system (1) into the so-called normal form which will reveal a feedback law of the form\nthat will render a linear input–output map from the new input formula_11 to the output formula_12. To ensure that the transformed system is an equivalent representation of the original system, the transformation must be a diffeomorphism. That is, the transformation must not only be invertible (i.e., bijective), but both the transformation and its inverse must be smooth so that differentiability in the original coordinate system is preserved in the new coordinate system. In practice, the transformation can be only locally diffeomorphic, but the linearization results only hold in this smaller region.\n\nSeveral tools are required to solve this problem.\n\nThe goal of feedback linearization is to produce a transformed system whose states are the output formula_12 and its first formula_14 derivatives. To understand the structure of this target system, we use the Lie derivative. Consider the time derivative of (2), which can be computed using the chain rule,\n\nNow we can define the Lie derivative of formula_16 along formula_17 as,\n\nand similarly, the Lie derivative of formula_16 along formula_20 as,\n\nWith this new notation, we may express formula_22 as,\n\nNote that the notation of Lie derivatives is convenient when we take multiple derivatives with respect to either the same vector field, or a different one. For example,\n\nand\n\nIn our feedback linearized system made up of a state vector of the output formula_12 and its first formula_14 derivatives, we must understand how the input formula_28 enters the system. To do this, we introduce the notion of relative degree. Our system given by (1) and (2) is said to have relative degree formula_29 at a point formula_30 if,\n\nConsidering this definition of relative degree in light of the expression of the time derivative of the output formula_12, we can consider the relative degree of our system (1) and (2) to be the number of times we have to differentiate the output formula_12 before the input formula_28 appears explicitly. In an LTI system, the relative degree is the difference between the degree of the transfer function's denominator polynomial (i.e., number of poles) and the degree of its numerator polynomial (i.e., number of zeros).\n\nFor the discussion that follows, we will assume that the relative degree of the system is formula_38. In this case, after differentiating the output formula_38 times we have,\n\nwhere the notation formula_41 indicates the formula_38th derivative of formula_12. Because we assumed the relative degree of the system is formula_38, the Lie derivatives of the form formula_45 for formula_46 are all zero. That is, the input formula_28 has no direct contribution to any of the first formula_14th derivatives.\n\nThe coordinate transformation formula_49 that puts the system into normal form comes from the first formula_14 derivatives. In particular,\n\ntransforms trajectories from the original formula_52 coordinate system into the new formula_53 coordinate system. So long as this transformation is a diffeomorphism, smooth trajectories in the original coordinate system will have unique counterparts in the formula_53 coordinate system that are also smooth. Those formula_53 trajectories will be described by the new system,\n\nHence, the feedback control law\n\nrenders a linear input–output map from formula_6 to formula_59. The resulting linearized system\n\nis a cascade of formula_38 integrators, and an outer-loop control formula_6 may be chosen using standard linear system methodology. In particular, a state-feedback control law of\n\nwhere the state vector formula_53 is the output formula_12 and its first formula_14 derivatives, results in the LTI system\n\nwith,\n\nSo, with the appropriate choice of formula_69, we can arbitrarily place the closed-loop poles of the linearized system.\n\nFeedback linearization can be accomplished with systems that have relative degree less than formula_38. However, the normal form of the system will include zero dynamics (i.e., states that are not observable from the output of the system) that may be unstable. In practice, unstable dynamics may have deleterious effects on the system (e.g., it may be dangerous for internal states of the system to grow unbounded). These unobservable states may be controllable or at least stable, and so measures can be taken to ensure these states do not cause problems in practice. Minimum phase systems provide some insight on zero dynamics.\n\n\n"}
{"id": "2589751", "url": "https://en.wikipedia.org/wiki?curid=2589751", "title": "Fluorescence correlation spectroscopy", "text": "Fluorescence correlation spectroscopy\n\nFluorescence correlation spectroscopy (FCS) is a correlation analysis of fluctuation of the fluorescence intensity. The analysis provides parameters of the physics under the fluctuations. One of the interesting applications of this is an analysis of the concentration fluctuations of fluorescent particles (molecules) in solution. In this application, the fluorescence emitted from a very tiny space in solution containing a small number of fluorescent particles (molecules) is observed. The fluorescence intensity is fluctuating due to Brownian motion of the particles. In other words, the number of the particles in the sub-space defined by the optical system is randomly changing around the average number. The analysis gives the average number of fluorescent particles and average diffusion time, when the particle is passing through the space. Eventually, both the concentration and size of the particle (molecule) are determined. Both parameters are important in biochemical research, biophysics, and chemistry.\n\nFCS is such a sensitive analytical tool because it observes a small number of molecules (nanomolar to picomolar concentrations) in a small volume (~1μm). In contrast to other methods (such as HPLC analysis) FCS has no physical separation process; instead, it achieves its spatial resolution through its optics. Furthermore, FCS enables observation of fluorescence-tagged molecules in the biochemical pathway in intact living cells. This opens a new area, \"in situ or in vivo biochemistry\": tracing the biochemical pathway in intact cells and organs.\n\nCommonly, FCS is employed in the context of optical microscopy, in particular Confocal microscopy or two-photon excitation microscopy. In these techniques light is focused on a sample and the measured fluorescence intensity fluctuations (due to diffusion, physical or chemical reactions, aggregation, etc.) are analyzed using the temporal autocorrelation. Because the measured property is essentially related to the magnitude and/or the amount of fluctuations, there is an optimum measurement regime at the level when individual species enter or exit the observation volume (or turn on and off in the volume). When too many entities are measured at the same time the overall fluctuations are small in comparison to the total signal and may not be resolvable – in the other direction, if the individual fluctuation-events are too sparse in time, one measurement may take prohibitively too long. FCS is in a way the fluorescent counterpart to dynamic light scattering, which uses coherent light scattering, instead of (incoherent) fluorescence.\n\nWhen an appropriate model is known, FCS can be used to obtain quantitative information such as \n\nBecause fluorescent markers come in a variety of colors and can be specifically bound to a particular molecule (e.g. proteins, polymers, metal-complexes, etc.), it is possible to study the behavior of individual molecules (in rapid succession in composite solutions). With the development of sensitive detectors such as avalanche photodiodes the detection of the fluorescence signal coming from individual molecules in highly dilute samples has become practical. With this emerged the possibility to conduct FCS experiments in a wide variety of specimens, ranging from materials science to biology. The advent of engineered cells with genetically tagged proteins (like green fluorescent protein) has made FCS a common tool for studying molecular dynamics in living cells.\n\nSignal-correlation techniques were first experimentally applied to fluorescence in 1972 by Magde, Elson, and Webb, who are therefore commonly credited as the \"inventors\" of FCS. The technique was further developed in a group of papers by these and other authors soon after, establishing the theoretical foundations and types of applications. See Thompson (1991) for a review of that period.\n\nBeginning in 1993, a number of improvements in the measurement techniques—notably using confocal microscopy, and then two-photon microscopy—to better define the measurement volume and reject background—greatly improved the signal-to-noise ratio and allowed single molecule sensitivity. Since then, there has been a renewed interest in FCS, and as of August 2007 there have been over 3,000 papers using FCS found in Web of Science. See Krichevsky and Bonnet for a recent review. In addition, there has been a flurry of activity extending FCS in various ways, for instance to laser scanning and spinning-disk confocal microscopy (from a stationary, single point measurement), in using cross-correlation (FCCS) between two fluorescent channels instead of autocorrelation, and in using Förster Resonance Energy Transfer (FRET) instead of fluorescence.\n\nThe typical FCS setup consists of a laser line (wavelengths ranging typically from 405–633 nm (cw), and from 690–1100 nm (pulsed)), which is reflected into a microscope objective by a dichroic mirror. The laser beam is focused in the sample, which contains fluorescent particles (molecules) in such high dilution, that only a few are within the focal spot (usually 1–100 molecules in one fL). When the particles cross the focal volume, they fluoresce. This light is collected by the same objective and, because it is red-shifted with respect to the excitation light it passes the dichroic mirror reaching a detector, typically a photomultiplier tube, an avalanche photodiode detector or a superconducting nanowire single-photon detector. The resulting electronic signal can be stored either directly as an intensity versus time trace to be analyzed at a later point, or computed to generate the autocorrelation directly (which requires special acquisition cards). The FCS curve by itself only represents a time-spectrum. Conclusions on physical phenomena have to be extracted from there with appropriate models. The parameters of interest are found after fitting the autocorrelation curve to modeled functional forms.\n\nThe measurement volume is a convolution of illumination (excitation) and detection geometries, which result from the optical elements involved. The resulting volume is described mathematically by the point spread function (or PSF), it is essentially the image of a point source. The PSF is often described as an ellipsoid (with unsharp boundaries) of few hundred nanometers in focus diameter, and almost one micrometer along the optical axis. The shape varies significantly (and has a large impact on the resulting FCS curves) depending on the quality of the optical elements (it is crucial to avoid astigmatism and to check the real shape of the PSF on the instrument). In the case of confocal microscopy, and for small pinholes (around one Airy unit), the PSF is well approximated by Gaussians:\n\nwhere formula_2 is the peak intensity, r and z are radial and axial position, and formula_3 and formula_4 are the radial and axial radii, and formula_5. This Gaussian form is assumed in deriving the functional form of the autocorrelation.\n\nTypically formula_3 is 200–300 nm, and formula_4 is 2–6 times larger. One common way of calibrating the measurement volume parameters is to perform FCS on a species with known diffusion coefficient and concentration (see below). Diffusion coefficients for common fluorophores in water are given in a later section.\n\nThe Gaussian approximation works to varying degrees depending on the optical details, and corrections can sometimes be applied to offset the errors in approximation.\n\nThe (temporal) autocorrelation function is the correlation of a time series with itself shifted by time formula_8, as a function of formula_8:\n\nwhere formula_11 is the deviation from the mean intensity. The normalization (denominator) here is the most commonly used for FCS, because then the correlation at formula_12, \"G\"(0), is related to the average number of particles in the measurement volume.\n\nAs an example, raw FCS data and its autocorrelation for freely diffusing Rhodamine 6G are shown in the figure to the right. The plot on top shows the fluorescent intensity versus time. The intensity fluctuates as Rhodamine 6G moves in and out of the focal volume. In the bottom plot is the autocorrelation on the same data. Information about the diffusion rate and concentration can be obtained using one of the models described below.\n\nFor a Gaussian illumination profile formula_13, the autocorrelation function is given by the general master formula\nwhere the vector formula_15 denotes the stochastic displacement in space of a fluorophore after time formula_16.\nThe expression is valid if the average number formula_17 of fluorophores in the focal volume is low and if dark states, etc., of the fluorophore can be ignored. It particular, no assumption was made on the type of diffusive motion under investigation. The formula allows for an interpretation of formula_18 as (i) a return probability for small beam parameters formula_19 and (ii) the moment-generating function of formula_20 if formula_19 are varied.\n\nTo extract quantities of interest, the autocorrelation data can be fitted, typically using a nonlinear least squares algorithm. The fit's functional form depends on the type of dynamics (and the optical geometry in question).\n\nThe fluorescent particles used in FCS are small and thus experience thermal motions in solution. The simplest FCS experiment is thus normal 3D diffusion, for which the autocorrelation is:\n\nwhere formula_23 is the ratio of axial to radial formula_24 radii of the measurement volume, and formula_25 is the characteristic residence time. This form was derived assuming a Gaussian measurement volume. Typically, the fit would have three free parameters—G(0), formula_26, and formula_27—from which the diffusion coefficient and fluorophore concentration can be obtained.\n\nWith the normalization used in the previous section, \"G\"(0) gives the mean number of diffusers in the volume <N>, or equivalently—with knowledge of the observation volume size—the mean concentration:\n\nwhere the effective volume is found from integrating the Gaussian form of the measurement volume and is given by:\n\nIf the diffusing particles are hindered by obstacles or pushed by a force (molecular motors, flow, etc.) the dynamics is often not sufficiently well-described by the normal diffusion model, where the mean squared displacement (MSD) grows linearly with time. Instead the diffusion may be better described as anomalous diffusion, where the temporal dependence of the MSD is non-linear as in the power-law:\n\nwhere formula_32 is an anomalous diffusion coefficient. \"Anomalous diffusion\" commonly refers only to this very generic model, and not the many other possibilities that might be described as anomalous. Also, a power law is, in a strict sense, the expected form only for a narrow range of rigorously defined systems, for instance when the distribution of obstacles is fractal. Nonetheless a power law can be a useful approximation for a wider range of systems.\n\nThe FCS autocorrelation function for anomalous diffusion is:\n\nwhere the anomalous exponent formula_34 is the same as above, and becomes a free parameter in the fitting.\n\nUsing FCS, the anomalous exponent has been shown to be an indication of the degree of molecular crowding (it is less than one and smaller for greater degrees of crowding).\n\nIf there are diffusing particles with different sizes (diffusion coefficients), it is common to fit to a function that is the sum of single component forms:\n\nwhere the sum is over the number different sizes of particle, indexed by i, and formula_36 gives the weighting, which is related to the quantum yield and concentration of each type. This introduces new parameters, which makes the fitting more difficult as a higher-dimensional space must be searched. Nonlinear least square fitting typically becomes unstable with even a small number of formula_37s. A more robust fitting scheme, especially useful for polydisperse samples, is the Maximum Entropy Method.\n\nWith diffusion together with a uniform flow with velocity formula_38 in the lateral direction, the autocorrelation is:\n\nwhere formula_40 is the average residence time if there is only a flow (no diffusion).\n\nA wide range of possible FCS experiments involve chemical reactions that continually fluctuate from equilibrium because of thermal motions (and then \"relax\"). In contrast to diffusion, which is also a relaxation process, the fluctuations cause changes between states of different energies. One very simple system showing chemical relaxation would be a stationary binding site in the measurement volume, where particles only produce signal when bound (e.g. by FRET, or if the diffusion time is much faster than the sampling interval). In this case the autocorrelation is:\n\nwhere\n\nis the relaxation time and depends on the reaction kinetics (on and off rates), and:\n\nis related to the equilibrium constant \"K\".\n\nMost systems with chemical relaxation also show measurable diffusion as well, and the autocorrelation function will depend on the details of the system. If the diffusion and chemical reaction are decoupled, the combined autocorrelation is the product of the chemical and diffusive autocorrelations.\n\nThe autocorrelations above assume that the fluctuations are not due to changes in the fluorescent properties of the particles. However, for the majority of (bio)organic fluorophores—e.g. green fluorescent protein, rhodamine, Cy3 and Alexa Fluor dyes—some fraction of illuminated particles are excited to a triplet state (or other non-radiative decaying states) and then do not emit photons for a characteristic relaxation time formula_44. Typically formula_44 is on the order of microseconds, which is usually smaller than the dynamics of interest (e.g. formula_27) but large enough to be measured. A multiplicative term is added to the autocorrelation to account for the triplet state. For normal diffusion:\n\nwhere formula_48 is the fraction of particles that have entered the triplet state and formula_49 is the corresponding triplet state relaxation time. If the dynamics of interest are much slower than the triplet state relaxation, the short time component of the autocorrelation can simply be truncated and the triplet term is unnecessary.\n\nThe fluorescent species used in FCS is typically a biomolecule of interest that has been tagged with a fluorophore (using immunohistochemistry for instance), or is a naked fluorophore that is used to probe some environment of interest (e.g. the cytoskeleton of a cell). The following table gives diffusion coefficients of some common fluorophores in water at room temperature, and their excitation wavelengths.\nFCS almost always refers to the single point, single channel, temporal autocorrelation measurement, although the term \"fluorescence correlation spectroscopy\" out of its historical scientific context implies no such restriction. FCS has been extended in a number of variations by different researchers, with each extension generating another name (usually an acronym).\n\nWhereas FCS is a point measurement providing diffusion time at a given observation volume, svFCS is a technique where the observation spot is varied in order to measure diffusion times at different spot sizes. The relationship between the diffusion time and the spot area is linear and could be plotted in order to decipher the major contribution of confinement. The resulting curve is called the diffusion law.\nThis technique is used in Biology to study the plasma membrane organization on living cells.\nwhere formula_51 is the y axis intercept. In case of Brownian diffusion, formula_52. In case of a confinement due to isolated domains, formula_53 whereas in case of isolated domains, formula_54.\n\nsvFCS studies on living cells and simulation papers\n\nSampling-Volume-Controlled Fluorescence Correlation Spectroscopy (SVC-FCS):\n\nz-scan FCS\n\nFCS with Nano-apertures: breaking the diffraction barrier\n\nSTED-FCS:\n\nFCS is sometimes used to study molecular interactions using differences in diffusion times (e.g. the product of an association reaction will be larger and thus have larger diffusion times than the reactants individually); however, FCS is relatively insensitive to molecular mass as can be seen from the following equation relating molecular mass to the diffusion time of globular particles (e.g. proteins):\n\nwhere formula_56 is the viscosity of the sample and formula_57 is the molecular mass of the fluorescent species. In practice, the diffusion times need to be sufficiently different—a factor of at least 1.6—which means the molecular masses must differ by a factor of 4. Dual color fluorescence cross-correlation spectroscopy (FCCS) measures interactions by cross-correlating two or more fluorescent channels (one channel for each reactant), which distinguishes interactions more sensitively than FCS, particularly when the mass change in the reaction is small.\n\nFluorescence cross correlation spectroscopy overcomes the weak dependence of diffusion rate on molecular mass by looking at multicolor coincidence. What about homo-interactions? The solution lies in brightness analysis. These methods use the heterogeneity in the intensity distribution of fluorescence to measure the molecular brightness of different species in a sample. Since dimers will contain twice the number of fluorescent labels as monomers, their molecular brightness will be approximately double that of monomers. As a result, the relative brightness is sensitive a measure of oligomerization. The average molecular brightness (formula_58) is related to the variance (formula_59) and the average intensity (formula_60) as follows:\n\nHere formula_62 and formula_63 are the fractional intensity and molecular brightness, respectively, of species formula_64.\n\nAnother FCS based approach to studying molecular interactions uses fluorescence resonance energy transfer (FRET) instead of fluorescence, and is called FRET-FCS. With FRET, there are two types of probes, as with FCCS; however, there is only one channel and light is only detected when the two probes are very close—close enough to ensure an interaction. The FRET signal is weaker than with fluorescence, but has the advantage that there is only signal during a reaction (aside from autofluorescence).\n\nIn Scanning fluorescence correlation spectroscopy (sFCS) the measurement volume is moved across the sample in a defined way. The introduction of scanning is motivated by its ability to alleviate or remove several distinct problems often encountered in standard FCS, and thus, to extend the range of applicability of fluorescence correlation methods in biological systems.\n\nSome variations of FCS are only applicable to serial scanning laser microscopes. Image Correlation Spectroscopy and its variations all were implemented on a scanning confocal or scanning two photon microscope, but transfer to other microscopes, like a spinning disk confocal microscope. Raster ICS (RICS), and position sensitive FCS (PSFCS) incorporate the time delay between parts of the image scan into the analysis. Also, low-dimensional scans (e.g. a circular ring)—only possible on a scanning system—can access time scales between single point and full image measurements. Scanning path has also been made to adaptively follow particles.\n\nAny of the image correlation spectroscopy methods can also be performed on a spinning disk confocal microscope, which in practice can obtain faster imaging speeds compared to a laser scanning confocal microscope. This approach has recently been applied to diffusion in a spatially varying complex environment, producing a pixel resolution map of a diffusion coefficient. The spatial mapping of diffusion with FCS has subsequently been extended to the TIRF system. Spatial mapping of dynamics using correlation techniques had been applied before, but only at sparse points or at coarse resolution.\n\nWhen the motion is slow (in biology, for example, diffusion in a membrane), getting adequate statistics from a single-point FCS experiment may take a prohibitively long time. More data can be obtained by performing the experiment in multiple spatial points in parallel, using a laser scanning confocal microscope. This approach has been called Image Correlation Spectroscopy (ICS). The measurements can then be averaged together.\n\nAnother variation of ICS performs a spatial autocorrelation on images, which gives information about the concentration of particles. The correlation is then averaged in time. While camera white noise does not autocorrelate over time, it does over space - this creates a white noise amplitude in the spatial autocorrelation function which must be accounted for when fitting the autocorrelation amplitude in order to find the concentration of fluorescent molecules.\n\nA natural extension of the temporal and spatial correlation versions is spatio-temporal ICS (STICS). In STICS there is no explicit averaging in space or time (only the averaging inherent in correlation). In systems with non-isotropic motion (e.g. directed flow, asymmetric diffusion), STICS can extract the directional information. A variation that is closely related to STICS (by the Fourier transform) is \"k\"-space Image Correlation Spectroscopy (kICS).\n\nThere are cross-correlation versions of ICS as well, which can yield the concentration, distribution and dynamics of co-localized fluorescent molecules. Molecules are considered co-localized when individual fluorescence contributions are indistinguishable due to overlapping point-spread functions of fluorescence intensities.\n\nPICS is a powerful analysis tool that resolves correlations on the nanometer length and millisecond timescale. Adapted from methods of spatio-temporal image correlation spectroscopy, it exploits the high positional accuracy of single-particle tracking. While conventional tracking methods break down if multiple particle trajectories intersect, this method works in principle for arbitrarily large molecule densities and dynamical parameters (e.g. diffusion coefficients, velocities) as long as individual molecules can be identified. It is computationally cheap and robust and allows one to identify and quantify motions (e.g. diffusion, active transport, confined diffusion) within an ensemble of particles, without any a priori knowledge about the dynamics.\n\nA particle image cross-correlation spectroscopy (PICCS) extension is available for biological processes that involve multiple interaction partners, as can observed by two-color microscopy.\n\nSuper-resolution optical fluctuation imaging (SOFI) is a super-resolution technique that achieves spatial resolutions below the diffraction limit by post-processing analysis with correlation equations, similar to FCS. While original reports of SOFI used fluctuations from stationary, blinking of fluorophores, FCS has been combined with SOFI where fluctuations are produced from diffusing probes to produce super-resolution spatial maps of diffusion coefficients. This has been applied to understand diffusion and spatial properties of porous materials, including agarose hydrogels and liquid crystals.\n\nTotal internal reflection fluorescence (TIRF) is a microscopy approach that is only sensitive to a thin layer near the surface of a coverslip, which greatly minimizes background fluorescence. FCS has been extended to that type of microscope, and is called TIR-FCS. Because the fluorescence intensity in TIRF falls off exponentially with distance from the coverslip (instead of as a Gaussian with a confocal), the autocorrelation function is different.\n\nLight sheet fluorescence microscopy or selective plane imaging microscopy (SPIM) uses illumination that is done perpendicularly to the direction of observation, by using a thin sheet of (laser) light. Under certain conditions, this illumination principle can be combined with fluorescence correlation spectroscopy, to allow spatially resolved imaging of the mobility and interactions of fluorescing particles such as GFP labelled proteins inside living biological samples.\n\nThere are two main non-correlation alternatives to FCS that are widely used to study the dynamics of fluorescent species.\n\nIn FRAP, a region is briefly exposed to intense light, irrecoverably photobleaching fluorophores, and the fluorescence recovery due to diffusion of nearby (non-bleached) fluorophores is imaged. A primary advantage of FRAP over FCS is the ease of interpreting qualitative experiments common in cell biology. Differences between cell lines, or regions of a cell, or before and after application of drug, can often be characterized by simple inspection of movies. FCS experiments require a level of processing and are more sensitive to potentially confounding influences like: rotational diffusion, vibrations, photobleaching, dependence on illumination and fluorescence color, inadequate statistics, etc. It is much easier to change the measurement volume in FRAP, which allows greater control. In practice, the volumes are typically larger than in FCS. While FRAP experiments are typically more qualitative, some researchers are studying FRAP quantitatively and including binding dynamics. A disadvantage of FRAP in cell biology is the free radical perturbation of the cell caused by the photobleaching. It is also less versatile, as it cannot measure concentration or rotational diffusion, or co-localization. FRAP requires a significantly higher concentration of fluorophores than FCS.\n\nIn particle tracking, the trajectories of a set of particles are measured, typically by applying particle tracking algorithms to movies. Particle tracking has the advantage that all the dynamical information is maintained in the measurement, unlike FCS where correlation averages the dynamics to a single smooth curve. The advantage is apparent in systems showing complex diffusion, where directly computing the mean squared displacement allows straightforward comparison to normal or power law diffusion. To apply particle tracking, the particles have to be distinguishable and thus at lower concentration than required of FCS. Also, particle tracking is more sensitive to noise, which can sometimes affect the results unpredictably.\n\nSeveral advantages in both spatial resolution and minimizing photodamage/photobleaching in organic and/or biological samples are obtained by two-photon or three-photon excitation FCS.\n\n\n"}
{"id": "18427873", "url": "https://en.wikipedia.org/wiki?curid=18427873", "title": "Focus recovery based on the linear canonical transform", "text": "Focus recovery based on the linear canonical transform\n\nFocus recovery from a defocused image is an ill-posed problem since it loses the component of high frequency. Most of the methods for focus recovery are based on depth estimation theory. The Linear canonical transform (LCT) gives a scalable kernel to fit many well-known optical effects. Using LCTs to approximate an optical system for imaging and inverting this system, theoretically permits recovery of a defocused image.\n\nIn photography, depth of field (DOF) means an effective focal length. It is usually used for stressing an object and deemphasizing the background (and/or the foreground). The important measure related to DOF is the lens aperture. Decreasing the diameter of aperture increases focus and lowers resolution and vice versa.\n\nThe Huygens–Fresnel principle describes diffraction of wave propagation between two fields. It belongs to Fourier optics rather than geometric optics.The disturbance of diffraction depends on two circumstance parameters, the size of aperture and the interfield distance.\n\nConsider a source field and a destination field, field 1 and field 0, respectively. P(x,y) is the position in the source field, P(x,y) is the position in the destination field. The Huygens–Fresnel principle gives the diffraction formula for two fields U(x,y), U(x,y) as following:\n\nwhere θ denotes the angle between formula_2 and formula_3. Replace cosθ by formula_4 and formula_2 by \nformula_6\n\nwe get\n\nThe further distance \"z\" or the smaller aperture \"(x,y)\" causes a greater diffraction. A larger DOF can lead to a more effective focused wave distribution. This seems to be a conflict. Here are the notations:\nIn conclusion, diffraction explains a micro behavior whereas DOF shows a macro behavior. Both of them are related to aperture size.\n\nAs the meaning of \"canonical\", the linear canonical transform (LCT) is a scalable transform that connects to lots of important kernels such as the Fresnel transform, Fraunhofer transform and the fractional Fourier transform. It can be easily controlled by its four parameters, \"a\", \"b\", \"c\", \"d\" (3 degrees of freedom). The definition:\n\nwhere\n\nConsider a general imaging system with object distance \"z\", focal length of the thin lens \"f\" and an imaging distance \"z\". The effect of the propagation in freespace acts as nearly a chirp convolution, that is, the formula of diffraction. Besides, the effect of the propagation in thin lens acts as a chirp multiplication. The parameters are all simplified as paraxial approximations while meeting the freespace propagation. It does not consider aperture size.\n\nFrom the properties of the LCT, it is possible to obtain those 4 parameters for this optical system as:\n\nOnce the values of \"z\", \"z\" and \"f\" are known, the LCT can simulate any optical system.\n\n"}
{"id": "36675565", "url": "https://en.wikipedia.org/wiki?curid=36675565", "title": "Glossary of biology", "text": "Glossary of biology\n\n\"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\"\n\nThis glossary of biology terms is a list of definitions of fundamental terms and concepts of biology, its sub-disciplines, and related fields. For more specific definitions from other glossaries related to biology, see Glossary of ecology, Glossary of botany, Glossary of genetics, and Glossary of speciation.\n\n"}
{"id": "1071088", "url": "https://en.wikipedia.org/wiki?curid=1071088", "title": "High Accuracy Radial Velocity Planet Searcher", "text": "High Accuracy Radial Velocity Planet Searcher\n\nThe High Accuracy Radial velocity Planet Searcher (HARPS) is a high-precision echelle planet finding spectrograph installed in 2002 on the ESO's 3.6m telescope at La Silla Observatory in Chile. The first light was achieved in February 2003. HARPS has discovered over 130 exoplanets to date, with the first one in 2004, making it the most successful planet finder behind the Kepler space observatory. It is a second-generation radial-velocity spectrograph, based on experience with the ELODIE and CORALIE instruments.\n\nHARPS can attain a precision of 0.97 m/s (3.5 km/h), with an effective precision of the order of 30 cm s, making it one of only two instruments worldwide with such accuracy. This is due to a design in which the target star and a reference spectrum from a thorium lamp are observed simultaneously using two identical optic fibre feeds, and to careful attention to mechanical stability: the instrument sits in a vacuum vessel which is temperature-controlled to within 0.01 kelvins. The precision and sensitivity of the instrument is such that it incidentally produced the best available measurement of the thorium spectrum. Planet-detection is in some cases limited by the seismic pulsations of the star observed rather than by limitations of the instrument.\n\nThe principal investigator on HARPS is Michel Mayor who, along with Didier Queloz and Stéphane Udry have used the instrument to characterize the Gliese 581 planetary system, home to one of the smallest known exoplanet orbiting a normal star, and two super-Earths whose orbits lie in the star's habitable zone.\n\nIt was initially used for a survey of a thousand stars.\n\nSince October 2012 the HARPS spectrograph has the precision to detect a new category of planets: Habitable super-Earths. This sensitivity was expected from simulations of stellar intrinsic signals, and actual observations of planetary systems. Currently, HARPS can detect habitable super-Earth only around low-mass stars as these are more affected by gravitational tug from planets and have habitable zones close to the host star.\n\nThis is an incomplete list of exoplanets discovered by HARPS. The list is sorted by the date of the discovery's announcement. As of December 2017, the list contains 134 exoplanets.\n\n\nSimilar instruments:\n\nSpace based detectors :\n\n"}
{"id": "34953080", "url": "https://en.wikipedia.org/wiki?curid=34953080", "title": "Illusion of validity", "text": "Illusion of validity\n\nIllusion of validity is a cognitive bias in which a person overestimates his or her ability to interpret and predict accurately the outcome when analyzing a set of data, in particular when the data analyzed show a very consistent pattern—that is, when the data \"tell\" a coherent story.\n\nThis effect persists even when the person is aware of all the factors that limit the accuracy of his or her predictions, that is when the data and/or methods used to judge them lead to highly fallible predictions.\n\nDaniel Kahneman, Paul Slovic, and Amos Tversky explain the illusion as follows: \"people often predict by selecting the output...that is most representative of the input...The confidence they have in their prediction depends primarily on the degree of representativeness...with little or no regard for the factors that limit predictive accuracy. Thus, people express great confidence in the prediction that a person is a librarian when given a description of his personality which matches the stereotype of librarians, even if the description is scanty, unreliable, or outdated. The unwarranted confidence which is produced by a good fit between the predicted outcome and the input information may be called the illusion of validity.\"\n\nIn one study, for example, subjects reported higher confidence in a prediction of the final grade point average of a student after seeing a first-year record of consistent \"B\"&apos;s than a first-year record of an even number of \"A\"&apos;s and \"C\"&apos;s. Consistent patterns may be observed when input variables are highly redundant or correlated, which may increase subjective confidence. However, a number of highly correlated inputs should not increase confidence much more than only one of the inputs; instead higher confidence should be merited when a number of highly \"independent\" inputs show a consistent pattern.\n\nThis bias was first described by Amos Tversky and Daniel Kahneman in their 1973 paper \"On the Psychology of Prediction\".\n\nIn a 2011 article, Kahneman recounted the story of his discovery of the illusion of validity. After completing an undergraduate psychology degree and spending a year as an infantry officer in the Israeli Army, he was assigned to the army's Psychology Branch, where he helped evaluate candidates for officer training using a test called the Leaderless Group Challenge. Candidates were taken to an obstacle field and assigned a group task so that Kahneman and his fellow evaluators could discern their individual leadership qualities or lack thereof.\n\nBut although Kahneman and his colleagues emerged from the exercise with very clear judgments as to who was and wasn't a potential leader, their forecasts proved \"largely useless\" in the long term. Comparing their original evaluations of candidates with the judgments of their officer-training school commanders months later, Kahneman and his colleagues found that their own \"ability to predict performance at the school was negligible. Our forecasts were better than blind guesses, but not by much.\"\n\nYet when asked to again to assess yet another group of candidates, their judgments were as clear as before. \"The dismal truth about the quality of our predictions,\" recalled Kahneman, \"had no effect whatsoever on how we evaluated new candidates and very little effect on the confidence we had in our judgments and predictions.\" Kahneman found this striking: \"The statistical evidence of our failure should have shaken our confidence in our judgments of particular candidates, but it did not. It should also have caused us to moderate our predictions, but it did not.\" Kahneman named this cognitive fallacy \"the illusion of validity\".\n\nDecades later, Kahneman reflected that at least part of the reason for his and his colleagues' failure in assessing the officer candidates was that they had been confronted with a difficult question but had instead, without realizing it, answered an easier one instead. \"We were required to predict a soldier's performance in officer training and in combat, but we did so by evaluating his behavior over one hour in an artificial situation. This was a perfect instance of a general rule that I call WYSIATI, 'What you see is all there is.' We had made up a story from the little we knew but had no way to allow for what we did not know about the individual’s future, which was almost everything that would actually matter.\"\n\nComparing the results of 25 wealth advisers over an eight-year period, Kahneman found that none of them stood out consistently as better or worse than the others. \"The results,\" as he put it, \"resembled what you would expect from a dice-rolling contest, not a game of skill.\" Yet at the firm for which all these advisers worked, no one seemed to be aware of this: \"The advisers themselves felt they were competent professionals performing a task that was difficult but not impossible, and their superiors agreed.\" Kahneman informed the firm's directors that they were \"rewarding luck as if it were skill.\" The directors believed this, yet \"life in the firm went on just as before.\" The directors clung to the \"illusion of skill,\" as did the advisers themselves.\n\nThe scientist Freeman Dyson has recalled his experience as a British Army statistician during World War II, performing an analysis of the operations of the Bomber Command. At the time, an officer argue that because of the heavy gun turrets they carried, the bombers were too slow and could not fly high enough to avoid being shot down. He suggested they remove the turrets and gunners. But the commander in chief rejected the suggestion – because, said Dyson, \"he was blinded by the illusion of validity.\" He was not alone: everyone in the command \"saw every bomber crew as a tightly knit team of seven, with the gunners playing an essential role defending their comrades against fighter attack.\" Part of this illusion \"was the belief that the team learned by experience. As they became more skillful and more closely bonded, their chances of survival would improve.\" Yet statistics, Dyson found, proved that all this was an illusion: deaths occurred randomly, having nothing to do with experience. Members of the bomber command, he realized, were dying unnecessarily because everyone was taken in by an illusion.\n\nIn 2014, an article in \"Rolling Stone\" presented as fact an accusation of rape at the University of Virginia that proved to be false. Rolling Stone's writers and editors, the university president and other administrators, and many U.Va. students were quick to believe the false charges. Harlan Loeb later explained this as an example of the illusion of validity in action.\n\nIn 2012, a sportswriter who described Kahneman as his \"favorite scientist\" wrote: \"The illusion of validity is why I get deeply suspicious whenever a fan, sportswriter, coach, or GM says anything to the effect of 'the numbers don’t tell the whole story.' This is, in fact, true, but what the person saying this usually means is 'I don't care what the numbers say because I am convinced that what I have seen is correct.' Which is, thanks to this illusion, almost never true. If I make an argument that the data says a player isn't good, and someone points out 'Yes, but if you watch the games you will notice that this year they are only shooting threes from the slot, and rarely from the corner, where he used to excel,' then that person is pointing out a hole in the data that's worth investigating. If the argument is along the lines of 'anyone who's watching him can clearly see he's much better than that,' then I'm certain the illusion of validity is doing its dirty work.\"\n\nIn a 1981 paper, J.B. Bushyhead and J.J. Christensen-Szalanski studied data from an outpatient clinic showing that doctors there ordered chest radiographs only on patients who manifested clinical attributes linked to some pneumonia cases, rather than on patients manifesting clinical attributes associated with all pneumonia cases. They attributed this behavior to the illusion of validity.\n\nOther cases where this phenomenon appears include job interviews, wine tasting, stock markets, political strategy.\n\nThe illusion of validity may be caused in part by confirmation bias and/or the representativeness heuristic, and could in turn cause the overconfidence effect.\n\nAmong the factors contributing to the illusion of validity, according to Meinolf Dierkes, Ariane Berthoin Antal, John Child, and Ikujiro Nonaka, are \"a person's tendency to register the frequency of events more than their probability\"; \"the impossibility of gathering information about alternative assumptions if action is based on a hypothesis\"; a \"disregard of base-rate information\"; and \"the self-fulfilling prophecy,\" or \"a behavior manifested in individuals or groups because it was expected.\"\n\nIf one wishes to try to avoid being traduced by the illusion of validity, according to Kahneman, one should ask two questions: \"Is the environment in which the judgment is made sufficiently regular to enable predictions from the available evidence? The answer is yes for diagnosticians, no for stock pickers. Do the professionals have an adequate opportunity to learn the cues and the regularities? The answer here depends on the professionals' experience and on the quality and speed with which they discover their mistakes.\" While many professionals \"easily pass both tests,\" meaning that their \"off-the-cuff judgments\" are of value, in general judgments by \"assertive and confident people\" should be taken with a grain of salt \"unless you have independent reason to believe that they know what they are talking about.\" This can be difficult, however, because \"overconfident professionals...act as experts and look like experts,\" and it can be a \"struggle to remind yourself that they may be in the grip of an illusion.\"\n\nIn his article on the false rape case at the University of Virginia, Harlan Loeb outlined an approach to avoiding the illusion of validity in cases which, like that one, involve “a highly emotional and personal issue that has national resonance, high-profile media coverage and an organization already on the defensive with recent issues.” He advised, first: \"Always challenge (appropriately, of course) facts and assumptions that many rely on to inform their thinking and decision-making about risk and crisis management.\" Second: \"In situations with palpable unknowns, where the illusion of validity in decision-making is a material threat, push hard to do research, polling and active listening to help identify the levers and pulleys that shape the operating and environmental realities of the present risk.\" Third: \"Determine existing organizational challenges that will prevent leadership from making decisions consistently, effectively, and quickly in the face of uncertainty.\" Fourth: \"Be aware of how current actions could dictate future strategy.\" Fifth: \"Be ready to take on current risk to manage future risk.\"\n\nPhil Thornton has offered the following advice for avoiding the illusion of validity in the financial sector. First, \"remember that just because previous generations were successful following certain approaches, replicating their actions may not necessarily be a good idea.\" Second, \"remember that the consequences of decisions being wrong can be more important than the probability of them being correct.\"\n\n\n"}
{"id": "35558740", "url": "https://en.wikipedia.org/wiki?curid=35558740", "title": "International charter for walking", "text": "International charter for walking\n\nThe International charter for walking is an initiative undertaken by 'Walk21' to encourage walking in urban areas for benefits to health, the environment and the economy.\n\n\n\n"}
{"id": "19852895", "url": "https://en.wikipedia.org/wiki?curid=19852895", "title": "Introduction to evolution", "text": "Introduction to evolution\n\nEvolution is the process of change in all forms of life over generations, and evolutionary biology is the study of how evolution occurs. Biological populations evolve through genetic changes that correspond to changes in the organisms' observable traits. Genetic changes include mutations, which are caused by damage or replication errors in organisms' DNA. As the genetic variation of a population drifts randomly over generations, natural selection gradually leads traits to become more or less common based on the relative reproductive success of organisms with those traits.\n\nThe age of the Earth is about 4.54 billion years. The earliest undisputed evidence of life on Earth dates at least from 3.5 billion years ago, during the Eoarchean Era after geological crust started to solidify, following the earlier molten Hadean Eon. Microbial mat fossils in 3.48 billion-year-old sandstone have been found in Western Australia. Other early physical evidence of life includes graphite, a biogenic substance, in 3.7 billion-year-old metasedimentary rocks in western Greenland and, in 2015, \"remains of biotic life\" found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, \"If life arose relatively quickly on Earth ... then it could be common in the universe.\" It is estimated that more than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described. More recently, in May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.\n\nEvolution does not attempt to explain the origin of life (covered instead by abiogenesis), but it does explain how early lifeforms evolved into the complex ecosystem that we see today. Based on the similarities between all present-day organisms, all life on Earth is assumed to have originated through common descent from a last universal ancestor from which all known species have diverged through the process of evolution. All individuals have hereditary material in the form of genes received from their parents, which they pass on to any offspring. Among offspring there are variations of genes due to the introduction of new genes via random changes called mutations or via reshuffling of existing genes during sexual reproduction. The offspring differs from the parent in minor random ways. If those differences are helpful, the offspring is more likely to survive and reproduce. This means that more offspring in the next generation will have that helpful difference and individuals will not have equal chances of reproductive success. In this way, traits that result in organisms being better adapted to their living conditions become more common in descendant populations. These differences accumulate resulting in changes within the population. This process is responsible for the many diverse life forms in the world.\n\nThe forces of evolution are most evident when populations become isolated, either through geographic distance or by other mechanisms that prevent genetic exchange. Over time, isolated populations can branch off into new species.\n\nThe majority of genetic mutations neither assist, change the appearance of, nor bring harm to individuals. Through the process of genetic drift, these mutated genes are neutrally sorted among populations and survive across generations by chance alone. In contrast to genetic drift, natural selection is not a random process because it acts on traits that are necessary for survival and reproduction. Natural selection and random genetic drift are constant and dynamic parts of life and over time this has shaped the branching structure in the tree of life.\n\nThe modern understanding of evolution began with the 1859 publication of Charles Darwin's \"On the Origin of Species\". In addition, Gregor Mendel's work with plants helped to explain the hereditary patterns of genetics. Fossil discoveries in paleontology, advances in population genetics and a global network of scientific research have provided further details into the mechanisms of evolution. Scientists now have a good understanding of the origin of new species (speciation) and have observed the speciation process in the laboratory and in the wild. Evolution is the principal scientific theory that biologists use to understand life and is used in many disciplines, including medicine, psychology, conservation biology, anthropology, forensics, agriculture and other social-cultural applications.\n\nThe main ideas of evolution may be summarized as follows:\n\n\nIn the 19th century, natural history collections and museums were popular. The European expansion and naval expeditions employed naturalists, while curators of grand museums showcased preserved and live specimens of the varieties of life. Charles Darwin was an English graduate educated and trained in the disciplines of natural history. Such natural historians would collect, catalogue, describe and study the vast collections of specimens stored and managed by curators at these museums. Darwin served as a ship's naturalist on board HMS \"Beagle\", assigned to a five-year research expedition around the world. During his voyage, he observed and collected an abundance of organisms, being very interested in the diverse forms of life along the coasts of South America and the neighboring Galápagos Islands.\n\nDarwin gained extensive experience as he collected and studied the natural history of life forms from distant places. Through his studies, he formulated the idea that each species had developed from ancestors with similar features. In 1838, he described how a process he called natural selection would make this happen.\n\nThe size of a population depends on how much and how many resources are able to support it. For the population to remain the same size year after year, there must be an equilibrium, or balance between the population size and available resources. Since organisms produce more offspring than their environment can support, not all individuals can survive out of each generation. There must be a competitive struggle for resources that aid in survival. As a result, Darwin realised that it was not chance alone that determined survival. Instead, survival of an organism depends on the differences of each individual organism, or \"traits,\" that aid or hinder survival and reproduction. Well-adapted individuals are likely to leave more offspring than their less well-adapted competitors. Traits that hinder survival and reproduction would \"disappear\" over generations. Traits that help an organism survive and reproduce would \"accumulate\" over generations. Darwin realised that the unequal ability of individuals to survive and reproduce could cause gradual changes in the population and used the term \"natural selection\" to describe this process.\n\nObservations of variations in animals and plants formed the basis of the theory of natural selection. For example, Darwin observed that orchids and insects have a close relationship that allows the pollination of the plants. He noted that orchids have a variety of structures that attract insects, so that pollen from the flowers gets stuck to the insects' bodies. In this way, insects transport the pollen from a male to a female orchid. In spite of the elaborate appearance of orchids, these specialised parts are made from the same basic structures that make up other flowers. In his book, \"Fertilisation of Orchids\" (1862), Darwin proposed that the orchid flowers were adapted from pre-existing parts, through natural selection.\n\nDarwin was still researching and experimenting with his ideas on natural selection when he received a letter from Alfred Russel Wallace describing a theory very similar to his own. This led to an immediate joint publication of both theories. Both Wallace and Darwin saw the history of life like a family tree, with each fork in the tree’s limbs being a common ancestor. The tips of the limbs represented modern species and the branches represented the common ancestors that are shared amongst many different species. To explain these relationships, Darwin said that all living things were related, and this meant that all life must be descended from a few forms, or even from a single common ancestor. He called this process \"descent with modification\".\n\nDarwin published his theory of evolution by natural selection in \"On the Origin of Species\" in 1859. His theory means that all life, including humanity, is a product of continuing natural processes. The implication that all life on Earth has a common ancestor has met with objections from some religious groups. Their objections are in contrast to the level of support for the theory by more than 99 percent of those within the scientific community today.\n\nNatural selection is commonly equated with \"survival of the fittest\", but this expression originated in Herbert Spencer's \"Principles of Biology\" in 1864, five years after Charles Darwin published his original works. \"Survival of the fittest\" describes the process of natural selection incorrectly, because natural selection is not only about survival and it is not always the fittest that survives.\n\nDarwins theory of natural selection laid the groundwork for modern evolutionary theory, and his experiments and observations showed that the organisms in populations varied from each other, that some of these variations were inherited, and that these differences could be acted on by natural selection. However, he could not explain the source of these variations. Like many of his predecessors, Darwin mistakenly thought that heritable traits were a product of use and disuse, and that features acquired during an organism's lifetime could be passed on to its offspring. He looked for examples, such as large ground feeding birds getting stronger legs through exercise, and weaker wings from not flying until, like the ostrich, they could not fly at all. This misunderstanding was called the inheritance of acquired characters and was part of the theory of transmutation of species put forward in 1809 by Jean-Baptiste Lamarck. In the late 19th century this theory became known as Lamarckism. Darwin produced an unsuccessful theory he called pangenesis to try to explain how acquired characteristics could be inherited. In the 1880s August Weismann's experiments indicated that changes from use and disuse could not be inherited, and Lamarckism gradually fell from favor.\n\nThe missing information needed to help explain how new features could pass from a parent to its offspring was provided by the pioneering genetics work of Gregor Mendel. Mendel's experiments with several generations of pea plants demonstrated that inheritance works by separating and reshuffling hereditary information during the formation of sex cells and recombining that information during fertilisation. This is like mixing different hands of playing cards, with an organism getting a random mix of half of the cards from one parent, and half of the cards from the other. Mendel called the information \"factors\"; however, they later became known as genes. Genes are the basic units of heredity in living organisms. They contain the information that directs the physical development and behavior of organisms.\n\nGenes are made of DNA. DNA is a long molecule made up of individual molecules called nucleotides. Genetic information is encoded in the sequence of nucleotides, that make up the DNA, just as the sequence of the letters in words carries information on a page. The genes are like short instructions built up of the \"letters\" of the DNA alphabet. Put together, the entire set of these genes gives enough information to serve as an \"instruction manual\" of how to build and run an organism. The instructions spelled out by this DNA alphabet can be changed, however, by mutations, and this may alter the instructions carried within the genes. Within the cell, the genes are carried in chromosomes, which are packages for carrying the DNA. It is the reshuffling of the chromosomes that results in unique combinations of genes in offspring. Since genes interact with one another during the development of an organism, novel combinations of genes produced by sexual reproduction can increase the genetic variability of the population even without new mutations. The genetic variability of a population can also increase when members of that population interbreed with individuals from a different population causing gene flow between the populations. This can introduce genes into a population that were not present before.\n\nEvolution is not a random process. Although mutations in DNA are random, natural selection is not a process of chance: the environment determines the probability of reproductive success. Evolution is an inevitable result of imperfectly copying, self-replicating organisms reproducing over billions of years under the selective pressure of the environment. The outcome of evolution is not a perfectly designed organism. The end products of natural selection are organisms that are adapted to their present environments. Natural selection does not involve progress towards an ultimate goal. Evolution does not strive for more advanced, more intelligent, or more sophisticated life forms. For example, fleas (wingless parasites) are descended from a winged, ancestral scorpionfly, and snakes are lizards that no longer require limbs—although pythons still grow tiny structures that are the remains of their ancestor's hind legs. Organisms are merely the outcome of variations that succeed or fail, dependent upon the environmental conditions at the time.\n\nRapid environmental changes typically cause extinctions. Of all species that have existed on Earth, 99.9 percent are now extinct. Since life began on Earth, five major mass extinctions have led to large and sudden drops in the variety of species. The most recent, the Cretaceous–Paleogene extinction event, occurred 66 million years ago.\n\nGenetic drift is a cause of allelic frequency change within populations of a species. Alleles are different variations of specific genes. They determine things like hair color, skin tone, eye color and blood type; in other words, all the genetic traits that vary between individuals. Genetic drift does not introduce new alleles to a population, but it can reduce variation within a population by removing an allele from the gene pool. Genetic drift is caused by random sampling of alleles. A truly random sample is a sample in which no outside forces affect what is selected. It is like pulling marbles of the same size and weight but of different colors from a brown paper bag. In any offspring, the alleles present are samples of the previous generations alleles, and chance plays a role in whether an individual survives to reproduce and to pass a sample of their generation onward to the next. The allelic frequency of a population is the ratio of the copies of one specific allele that share the same form compared to the number of all forms of the allele present in the population.\n\nGenetic drift affects smaller populations more than it affects larger populations.\n\nThe Hardy–Weinberg principle states that a large population in Hardy–Weinberg equilibrium will have no change in the frequency of alleles as generations pass. It is impossible for a population of any considerable size to reach this equilibrium because of the five requirements that must be met. A population must be infinite in size. There must be a zero percent mutation rate between generations, because mutations can alter existing alleles or create new ones. There can be no immigration or emigration in the population, because individuals arriving and leaving directly change allelic frequencies. There can be no selective pressures of any kind on the population, meaning that no individual is more likely than any other to survive and reproduce. Finally, mating must be totally random, with all males (or females in some cases) being equally desirable mates. This ensures a true random mixing of alleles.\n\nA population that is in Hardy–Weinberg equilibrium is analogous to a deck of cards; no matter how many times the deck is shuffled, no new cards are added and no old ones are taken away. Cards in the deck represent alleles in a population’s gene pool.\n\nA population bottleneck occurs when the population of a species is reduced drastically over a short period of time due to external forces. In a true population bottleneck, the reduction does not favor any combination of alleles; it is totally random chance which individuals survive. A bottleneck can reduce or eliminate genetic variation from a population. Further drift events after the bottleneck event can also reduce the population's genetic diversity. The lack of diversity created can make the population at risk to other selective pressures.\n\nA common example of a population bottleneck is the Northern elephant seal. Due to excessive hunting throughout the 19th century, the population of the northern elephant seal was reduced to 30 individuals or less. They have made a full recovery, with the total number of individuals at around 100,000 and growing. The effects of the bottleneck are visible, however. The seals are more likely to have serious problems with disease or genetic disorders, because there is almost no diversity in the population.\n\nThe founder effect occurs when a small group from one population splits off and forms a new population, often through geographic isolation. This new population's allelic frequency is probably different from the original population's, and will change how common certain alleles are in the populations. The founders of the population will determine the genetic makeup, and potentially the survival, of the new population for generations.\n\nOne example of the founder effect is found in the Amish migration to Pennsylvania in 1744. Two of the founders of the colony in Pennsylvania carried the recessive allele for Ellis–van Creveld syndrome. Because the Amish tend to be religious isolates, they interbreed, and through generations of this practice the frequency of Ellis–van Creveld syndrome in the Amish people is much higher than the frequency in the general population.\n\nThe modern evolutionary synthesis is based on the concept that populations of organisms have significant genetic variation caused by mutation and by the recombination of genes during sexual reproduction. It defines evolution as the change in allelic frequencies within a population caused by genetic drift, gene flow between sub populations, and natural selection. Natural selection is emphasised as the most important mechanism of evolution; large changes are the result of the gradual accumulation of small changes over long periods of time.\n\nThe modern evolutionary synthesis is the outcome of a merger of several different scientific fields to produce a more cohesive understanding of evolutionary theory. In the 1920s, Ronald Fisher, J.B.S. Haldane and Sewall Wright combined Darwin's theory of natural selection with statistical models of Mendelian genetics, founding the discipline of population genetics. In the 1930s and 1940s, efforts were made to merge population genetics, the observations of field naturalists on the distribution of species and sub species, and analysis of the fossil record into a unified explanatory model. The application of the principles of genetics to naturally occurring populations, by scientists such as Theodosius Dobzhansky and Ernst Mayr, advanced the understanding of the processes of evolution. Dobzhansky's 1937 work \"Genetics and the Origin of Species\" helped bridge the gap between genetics and field biology by presenting the mathematical work of the population geneticists in a form more useful to field biologists, and by showing that wild populations had much more genetic variability with geographically isolated subspecies and reservoirs of genetic diversity in recessive genes than the models of the early population geneticists had allowed for. Mayr, on the basis of an understanding of genes and direct observations of evolutionary processes from field research, introduced the biological species concept, which defined a species as a group of interbreeding or potentially interbreeding populations that are reproductively isolated from all other populations. Both Dobzhansky and Mayr emphasised the importance of subspecies reproductively isolated by geographical barriers in the emergence of new species. The paleontologist George Gaylord Simpson helped to incorporate paleontology with a statistical analysis of the fossil record that showed a pattern consistent with the branching and non-directional pathway of evolution of organisms predicted by the modern synthesis. \n\nScientific evidence for evolution comes from many aspects of biology and includes fossils, homologous structures, and molecular similarities between species' DNA.\n\nResearch in the field of paleontology, the study of fossils, supports the idea that all living organisms are related. Fossils provide evidence that accumulated changes in organisms over long periods of time have led to the diverse forms of life we see today. A fossil itself reveals the organism's structure and the relationships between present and extinct species, allowing paleontologists to construct a family tree for all of the life forms on Earth.\n\nModern paleontology began with the work of Georges Cuvier. Cuvier noted that, in sedimentary rock, each layer contained a specific group of fossils. The deeper layers, which he proposed to be older, contained simpler life forms. He noted that many forms of life from the past are no longer present today. One of Cuvier’s successful contributions to the understanding of the fossil record was establishing extinction as a fact. In an attempt to explain extinction, Cuvier proposed the idea of \"revolutions\" or catastrophism in which he speculated that geological catastrophes had occurred throughout the Earth’s history, wiping out large numbers of species. Cuvier's theory of revolutions was later replaced by uniformitarian theories, notably those of James Hutton and Charles Lyell who proposed that the Earth’s geological changes were gradual and consistent. However, current evidence in the fossil record supports the concept of mass extinctions. As a result, the general idea of catastrophism has re-emerged as a valid hypothesis for at least some of the rapid changes in life forms that appear in the fossil records.\n\nA very large number of fossils have now been discovered and identified. These fossils serve as a chronological record of evolution. The fossil record provides examples of transitional species that demonstrate ancestral links between past and present life forms. One such transitional fossil is \"Archaeopteryx\", an ancient organism that had the distinct characteristics of a reptile (such as a long, bony tail and conical teeth) yet also had characteristics of birds (such as feathers and a wishbone). The implication from such a find is that modern reptiles and birds arose from a common ancestor.\n\nThe comparison of similarities between organisms of their form or appearance of parts, called their morphology, has long been a way to classify life into closely related groups. This can be done by comparing the structure of adult organisms in different species or by comparing the patterns of how cells grow, divide and even migrate during an organism's development.\n\nTaxonomy is the branch of biology that names and classifies all living things. Scientists use morphological and genetic similarities to assist them in categorising life forms based on ancestral relationships. For example, orangutans, gorillas, chimpanzees, and humans all belong to the same taxonomic grouping referred to as a family—in this case the family called Hominidae. These animals are grouped together because of similarities in morphology that come from common ancestry (called \"homology\").\nStrong evidence for evolution comes from the analysis of homologous structures: structures in different species that no longer perform the same task but which share a similar structure. Such is the case of the forelimbs of mammals. The forelimbs of a human, cat, whale, and bat all have strikingly similar bone structures. However, each of these four species' forelimbs performs a different task. The same bones that construct a bat's wings, which are used for flight, also construct a whale's flippers, which are used for swimming. Such a \"design\" makes little sense if they are unrelated and uniquely constructed for their particular tasks. The theory of evolution explains these homologous structures: all four animals shared a common ancestor, and each has undergone change over many generations. These changes in structure have produced forelimbs adapted for different tasks.\nHowever, anatomical comparisons can be misleading, as not all anatomical similarities indicate a close relationship. Organisms that share similar environments will often develop similar physical features, a process known as \"convergent evolution\". Both sharks and dolphins have similar body forms, yet are only distantly related—sharks are fish and dolphins are mammals. Such similarities are a result of both populations being exposed to the same selective pressures. Within both groups, changes that aid swimming have been favored. Thus, over time, they developed similar appearances (morphology), even though they are not closely related.\n\nIn some cases, anatomical comparison of structures in the embryos of two or more species provides evidence for a shared ancestor that may not be obvious in the adult forms. As the embryo develops, these homologies can be lost to view, and the structures can take on different functions. Part of the basis of classifying the vertebrate group (which includes humans), is the presence of a tail (extending beyond the anus) and pharyngeal slits. Both structures appear during some stage of embryonic development but are not always obvious in the adult form.\n\nBecause of the morphological similarities present in embryos of different species during development, it was once assumed that organisms re-enact their evolutionary history as an embryo. It was thought that human embryos passed through an amphibian then a reptilian stage before completing their development as mammals. Such a reenactment, often called \"recapitulation theory\", is not supported by scientific evidence. What does occur, however, is that the first stages of development are similar in broad groups of organisms. At very early stages, for instance, all vertebrates appear extremely similar, but do not exactly resemble any ancestral species. As development continues, specific features emerge from this basic pattern.\n\nHomology includes a unique group of shared structures referred to as \"vestigial structures\". \"Vestigial\" refers to anatomical parts that are of minimal, if any, value to the organism that possesses them. These apparently illogical structures are remnants of organs that played an important role in ancestral forms. Such is the case in whales, which have small vestigial bones that appear to be remnants of the leg bones of their ancestors which walked on land. Humans also have vestigial structures, including the ear muscles, the wisdom teeth, the appendix, the tail bone, body hair (including goose bumps), and the semilunar fold in the corner of the eye.\n\nBiogeography is the study of the geographical distribution of species. Evidence from biogeography, especially from the biogeography of oceanic islands, played a key role in convincing both Darwin and Alfred Russel Wallace that species evolved with a branching pattern of common descent. Islands often contain endemic species, species not found anywhere else, but those species are often related to species found on the nearest continent. Furthermore, islands often contain clusters of closely related species that have very different ecological niches, that is have different ways of making a living in the environment. Such clusters form through a process of adaptive radiation where a single ancestral species colonises an island that has a variety of open ecological niches and then diversifies by evolving into different species adapted to fill those empty niches. Well-studied examples include Darwin's finches, a group of 13 finch species endemic to the Galápagos Islands, and the Hawaiian honeycreepers, a group of birds that once, before extinctions caused by humans, numbered 60 species filling diverse ecological roles, all descended from a single finch like ancestor that arrived on the Hawaiian Islands some 4 million years ago. Another example is the Silversword alliance, a group of perennial plant species, also endemic to the Hawaiian Islands, that inhabit a variety of habitats and come in a variety of shapes and sizes that include trees, shrubs, and ground hugging mats, but which can be hybridised with one another and with certain tarweed species found on the west coast of North America; it appears that one of those tarweeds colonised Hawaii in the past, and gave rise to the entire Silversword alliance.\nEvery living organism (with the possible exception of RNA viruses) contains molecules of DNA, which carries genetic information. Genes are the pieces of DNA that carry this information, and they influence the properties of an organism. Genes determine an individual's general appearance and to some extent their behavior. If two organisms are closely related, their DNA will be very similar. On the other hand, the more distantly related two organisms are, the more differences they will have. For example, brothers are closely related and have very similar DNA, while cousins share a more distant relationship and have far more differences in their DNA. Similarities in DNA are used to determine the relationships between species in much the same manner as they are used to show relationships between individuals. For example, comparing chimpanzees with gorillas and humans shows that there is as much as a 96 percent similarity between the DNA of humans and chimps. Comparisons of DNA indicate that humans and chimpanzees are more closely related to each other than either species is to gorillas.\n\nThe field of molecular systematics focuses on measuring the similarities in these molecules and using this information to work out how different types of organisms are related through evolution. These comparisons have allowed biologists to build a \"relationship tree\" of the evolution of life on Earth. They have even allowed scientists to unravel the relationships between organisms whose common ancestors lived such a long time ago that no real similarities remain in the appearance of the organisms.\n\n\"Artificial selection\" is the controlled breeding of domestic plants and animals. Humans determine which animal or plant will reproduce and which of the offspring will survive; thus, they determine which genes will be passed on to future generations. The process of artificial selection has had a significant impact on the evolution of domestic animals. For example, people have produced different types of dogs by controlled breeding. The differences in size between the Chihuahua and the Great Dane are the result of artificial selection. Despite their dramatically different physical appearance, they and all other dogs evolved from a few wolves domesticated by humans in what is now China less than 15,000 years ago.\n\nArtificial selection has produced a wide variety of plants. In the case of maize (corn), recent genetic evidence suggests that domestication occurred 10,000 years ago in central Mexico. Prior to domestication, the edible portion of the wild form was small and difficult to collect. Today \"The Maize Genetics Cooperation • Stock Center\" maintains a collection of more than 10,000 genetic variations of maize that have arisen by random mutations and chromosomal variations from the original wild type.\n\nIn artificial selection the new breed or variety that emerges is the one with random mutations attractive to humans, while in natural selection the surviving species is the one with random mutations useful to it in its non-human environment. In both natural and artificial selection the variations are a result of random mutations, and the underlying genetic processes are essentially the same. Darwin carefully observed the outcomes of artificial selection in animals and plants to form many of his arguments in support of natural selection. Much of his book \"On the Origin of Species\" was based on these observations of the many varieties of domestic pigeons arising from artificial selection. Darwin proposed that if humans could achieve dramatic changes in domestic animals in short periods, then natural selection, given millions of years, could produce the differences seen in living things today.\n\nCoevolution is a process in which two or more species influence the evolution of each other. All organisms are influenced by life around them; however, in coevolution there is evidence that genetically determined traits in each species directly resulted from the interaction between the two organisms.\n\nAn extensively documented case of coevolution is the relationship between \"Pseudomyrmex\", a type of ant, and the \"acacia\", a plant that the ant uses for food and shelter. The relationship between the two is so intimate that it has led to the evolution of special structures and behaviors in both organisms. The ant defends the acacia against herbivores and clears the forest floor of the seeds from competing plants. In response, the plant has evolved swollen thorns that the ants use as shelter and special flower parts that the ants eat.\nSuch coevolution does not imply that the ants and the tree choose to behave in an altruistic manner. Rather, across a population small genetic changes in both ant and tree benefited each. The benefit gave a slightly higher chance of the characteristic being passed on to the next generation. Over time, successive mutations created the relationship we observe today.\n\nGiven the right circumstances, and enough time, evolution leads to the emergence of new species. Scientists have struggled to find a precise and all-inclusive definition of \"species\". Ernst Mayr defined a species as a population or group of populations whose members have the potential to interbreed naturally with one another to produce viable, fertile offspring. (The members of a species cannot produce viable, fertile offspring with members of \"other\" species). Mayr's definition has gained wide acceptance among biologists, but does not apply to organisms such as bacteria, which reproduce asexually.\n\nSpeciation is the lineage-splitting event that results in two separate species forming from a single common ancestral population. A widely accepted method of speciation is called \"allopatric speciation\". Allopatric speciation begins when a population becomes geographically separated. Geological processes, such as the emergence of mountain ranges, the formation of canyons, or the flooding of land bridges by changes in sea level may result in separate populations. For speciation to occur, separation must be substantial, so that genetic exchange between the two populations is completely disrupted. In their separate environments, the genetically isolated groups follow their own unique evolutionary pathways. Each group will accumulate different mutations as well as be subjected to different selective pressures. The accumulated genetic changes may result in separated populations that can no longer interbreed if they are reunited. Barriers that prevent interbreeding are either \"prezygotic\" (prevent mating or fertilisation) or \"postzygotic\" (barriers that occur after fertilisation). If interbreeding is no longer possible, then they will be considered different species. The result of four billion years of evolution is the diversity of life around us, with an estimated 1.75 million different species in existence today.\n\nUsually the process of speciation is slow, occurring over very long time spans; thus direct observations within human life-spans are rare. However speciation has been observed in present-day organisms, and past speciation events are recorded in fossils. Scientists have documented the formation of five new species of cichlid fishes from a single common ancestor that was isolated fewer than 5,000 years ago from the parent stock in Lake Nagubago. The evidence for speciation in this case was morphology (physical appearance) and lack of natural interbreeding. These fish have complex mating rituals and a variety of colorations; the slight modifications introduced in the new species have changed the mate selection process and the five forms that arose could not be convinced to interbreed.\n\nThe theory of evolution is widely accepted among the scientific community, serving to link the diverse specialty areas of biology. Evolution provides the field of biology with a solid scientific base. The significance of evolutionary theory is summarised by Theodosius Dobzhansky as \"nothing in biology makes sense except in the light of evolution.\" Nevertheless, the theory of evolution is not static. There is much discussion within the scientific community concerning the mechanisms behind the evolutionary process. For example, the rate at which evolution occurs is still under discussion. In addition, there are conflicting opinions as to which is the primary unit of evolutionary change—the organism or the gene.\n\nDarwin and his contemporaries viewed evolution as a slow and gradual process. Evolutionary trees are based on the idea that profound differences in species are the result of many small changes that accumulate over long periods.\n\nGradualism had its basis in the works of the geologists James Hutton and Charles Lyell. Hutton's view suggests that profound geological change was the cumulative product of a relatively slow continuing operation of processes which can still be seen in operation today, as opposed to catastrophism which promoted the idea that sudden changes had causes which can no longer be seen at work. A uniformitarian perspective was adopted for biological changes. Such a view can seem to contradict the fossil record, which often shows evidence of new species appearing suddenly, then persisting in that form for long periods. In the 1970s paleontologists Niles Eldredge and Stephen Jay Gould developed a theoretical model that suggests that evolution, although a slow process in human terms, undergoes periods of relatively rapid change (ranging between 50,000 and 100,000 years) alternating with long periods of relative stability. Their theory is called \"punctuated equilibrium\" and explains the fossil record without contradicting Darwin's ideas.\n\nA common unit of selection in evolution is the organism. Natural selection occurs when the reproductive success of an individual is improved or reduced by an inherited characteristic, and reproductive success is measured by the number of an individual's surviving offspring. The organism view has been challenged by a variety of biologists as well as philosophers. Richard Dawkins proposes that much insight can be gained if we look at evolution from the gene's point of view; that is, that natural selection operates as an evolutionary mechanism on genes as well as organisms. In his 1976 book, \"The Selfish Gene\", he explains:\nOthers view selection working on many levels, not just at a single level of organism or gene; for example, Stephen Jay Gould called for a hierarchical perspective on selection.\n\n\n\n\n"}
{"id": "2024999", "url": "https://en.wikipedia.org/wiki?curid=2024999", "title": "Josef Augusta (paleontologist)", "text": "Josef Augusta (paleontologist)\n\nJosef Augusta (March 17, 1903, Boskovice, Moravia – February 4, 1968, Prague) was a Czechoslovak paleontologist, geologist, and science popularizer.\n\nFrom 1921 to 1925 Augusta studied at the university in Brno. Between 1933 and 1968 he held posts at the Charles University in Prague as lecturer, professor, and dean of the faculty.\n\nIn addition to his scientific work (about 120 publications), Augusta wrote about twenty books popularizing his profession, mostly targeted to the youth. He is best known for his reconstructions of fossil flora and fauna, together with the painter Zdeněk Burian (1905–1981). He also participated in the famous movie \"Cesta do pravěku\" (\"Journey to the Beginning of Time\") (1954). \n\n\n"}
{"id": "6436836", "url": "https://en.wikipedia.org/wiki?curid=6436836", "title": "List of African-American inventors and scientists", "text": "List of African-American inventors and scientists\n\nThis list of black inventors and scientists documents many of the African Americans who have invented a multitude of items or made discoveries in the course of their lives. These have ranged from practical everyday devices to applications and scientific discoveries in diverse fields, including physics, biology, mathematics, plus the medical, nuclear and space sciences.\n\nAmong the earliest was George Washington Carver, whose reputation was based on his research into and promotion of alternative crops to cotton, which aided in nutrition for farm families. He wanted poor farmers to grow alternative crops both as a source of their own food and as a source of other products to improve their way of life. The most popular of his 44 practical bulletins for farmers contained 105 food recipes using peanuts. He also developed and promoted about 100 products made from peanuts that were useful for the house and farm. He received numerous honors for his work, including the Spingarn Medal of the NAACP.\n\nA later renowned scientist was Percy Lavon Julian, a research chemist and a pioneer in the chemical synthesis of medicinal drugs from plants. He was the first to synthesize the natural product physostigmine, and a pioneer in the industrial large-scale chemical synthesis of the human hormones, steroids, progesterone, and testosterone, from plant sterols such as stigmasterol and sitosterol. His work would lay the foundation for the steroid drug industry's production of cortisone, other corticosteroids, and birth control pills.\n\nA contemporary example of a modern-day inventor is Lonnie George Johnson, an engineer. Johnson invented the Super Soaker water gun, which was the top-selling toy in the United States in 1991 and 1992. In 1980 Johnson formed his own law firm and licensed the Super Soaker water gun to Larami Corporation. Two years later the Super Soaker generated over $200 million in retail sales and became the best selling toy in America. Larami Corporation was eventually purchased by Hasbro, the second largest toy manufacturer in the world. Over the years, Super Soaker sales have totaled close to one billion dollars. Johnson reinvested a majority of his earnings from the Super Soaker into research and development for his energy technology companies – \"It's who I am, it's what I do.\" Currently, Johnson holds over 80 patents, with over 20 more pending, and is the author of several publications on spacecraft power systems.\n\n\n"}
{"id": "5450573", "url": "https://en.wikipedia.org/wiki?curid=5450573", "title": "List of Space Shuttle crews", "text": "List of Space Shuttle crews\n\nThis is a list of persons who served aboard Space Shuttle crews, arranged in chronological order by Space Shuttle missions.\n\nAbbreviations:\n\nNames of astronauts returning from the Mir or ISS on the Space Shuttle are shown in \"italics\". They did not have specific crew roles, but are listed in the Payload Specialist columns for reasons of space.\n\nSTS-61-A in 1985 is the only flight to have launched with a crew of more than seven.\n\n<nowiki>*</nowiki> Note 1 - In this year, Approach and Landing Tests (ALT) were accomplished. These were atmospheric only, non-spaceflight drop-tests from a Boeing 747 Shuttle Carrier Aircraft.<br>\n<nowiki>**</nowiki> Note 2 - The durations listed count only the orbiter free-flight time, and not total time aloft along with airborne time atop of the 747 SCA.\n\n\n"}
{"id": "34928774", "url": "https://en.wikipedia.org/wiki?curid=34928774", "title": "List of freeware geophysics software", "text": "List of freeware geophysics software\n\nThis is a list of freeware for geophysical data processing and interpretation.\n"}
{"id": "13200698", "url": "https://en.wikipedia.org/wiki?curid=13200698", "title": "List of historical Gnutella clients", "text": "List of historical Gnutella clients\n\nMany projects have attempted to use the Gnutella network, since its introduction in early 2000. This list enumerates abandoned or discontinued projects.\n\nSoftware that still work but dropped the GNUtella protocol.\n\n\nMutella was a Gnutella client developed by Max Zaitsev and Gregory Block. It had two user interfaces, one for textmode use and another called \"remote control\", which ran on an integrated web server and was used by a web browser. The first public version of Mutella was published on October 6, 2001.\n\nThe Mutella logo was changed into a squid somewhere around version 4.1. Before this change the logo used to be an Ouroboros. There was a blue and a black version of the ouroboros logo.\n\nSlashdot reports that LimeWire and SwapNut used the same code. The website was www.swapnut.com.\n\nXoloX was a Gnutella-based peer-to-peer file sharing application for Windows. It advertised having no spyware, adware, or hijackware. However, upon installation, it prompted the user to install programs suspected to be of that kind. Also, Microsoft Anti-Spyware detected adware programs when you started to install the program.\n\n\n\n"}
{"id": "234960", "url": "https://en.wikipedia.org/wiki?curid=234960", "title": "List of integrals of exponential functions", "text": "List of integrals of exponential functions\n\nThe following is a list of integrals of exponential functions. For a complete list of integral functions, please see the list of integrals.\n\nIndefinite integrals are antiderivative functions. A constant (the constant of integration) may be added to the right hand side of any of these formulas, but has been suppressed here in the interest of brevity.\n\nIn the following formulas, is the error function and is the exponential integral.\n\nThe last expression is the logarithmic mean.\n\n\n\n"}
{"id": "7120173", "url": "https://en.wikipedia.org/wiki?curid=7120173", "title": "List of volcanoes in Papua New Guinea", "text": "List of volcanoes in Papua New Guinea\n\nThis is a list of active and extinct volcanoes in Papua New Guinea.\n"}
{"id": "1697378", "url": "https://en.wikipedia.org/wiki?curid=1697378", "title": "Localization of organelle proteins by isotope method tagging", "text": "Localization of organelle proteins by isotope method tagging\n\nLocalization of organelle proteins by isotope method tagging (or LOPIT) is a method for determining the subcellular localization of membrane proteins.\n\nsee Dunkley et al. (2006) Proc. Natl. Acad. Sci 103 (17) p 6518\n"}
{"id": "47372902", "url": "https://en.wikipedia.org/wiki?curid=47372902", "title": "M85-HCC1", "text": "M85-HCC1\n\nM85-HCC1 is an ultracompact dwarf galaxy with a star density 1,000,000 times that of the solar neighbourhood, lying near the galaxy Messier 85. , it is the densest galaxy known.\n\n"}
{"id": "2486949", "url": "https://en.wikipedia.org/wiki?curid=2486949", "title": "MELCOR", "text": "MELCOR\n\nMELCOR is a fully integrated, engineering-level computer code developed by Sandia National Laboratories for the U.S. Nuclear Regulatory Commission to model the progression of severe accidents in nuclear power plants. A broad spectrum of severe accident phenomena in both boiling and pressurized water reactors is treated in MELCOR in a unified framework. MELCOR applications include estimation of severe accident source terms, and their sensitivities and uncertainties in a variety of applications.\n\n\n"}
{"id": "33331852", "url": "https://en.wikipedia.org/wiki?curid=33331852", "title": "Mycoplasma meleagridis", "text": "Mycoplasma meleagridis\n\nAlso known as: Mycoplasma air sacculitis − Mycoplasma infectious stunting − Mycoplasmosis\n\nMycoplasma meleagridis is a small bacteria responsible for air sacculitis and disorders of the musculoskeletal and reproductive systems in turkeys.\n\nThe disease is more severe in young birds and occurs globally wherever turkeys are reared intensively.\n\nTransmission is mainly vertical via the egg but ticks such as the Ixodes species can also be vectors.\n\nThe disease is often subclinical but may appear as mild stunting.\n\nAir sacculitis appears as tachypnoea, nasal discharge and sneezing.\n\nMusculoskeletal problems can lead to lameness, swelling and crooked necks.\n\nThere may be a drop in egg production.\n\nThe bacteria can be cultured from tissue samples or swabs and identified with PCR or immunofluorescence.\n\nELISA and the Slide Agglutination Test are used for serological diagnosis.\n\nAntibiotics are effective at treating and preventing the disease, especially Tylosin but also Lincomycin and Spectinomycin.\n\nVertical transmission is difficult to prevent, but males can be tested before breeding and eggs can be dipped in a Tylosin bath.\n\n\n"}
{"id": "381782", "url": "https://en.wikipedia.org/wiki?curid=381782", "title": "NumPy", "text": "NumPy\n\nNumPy (pronounced () or sometimes ()) is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. The ancestor of NumPy, Numeric, was originally created by Jim Hugunin with contributions from several other developers. In 2005, Travis Oliphant created NumPy by incorporating features of the competing Numarray into Numeric, with extensive modifications. NumPy is open-source software and has many contributors.\n\nThe Python programming language was not initially designed for numerical computing, but attracted the attention of the scientific and engineering community early on, so that a special interest group called matrix-sig was founded in 1995 with the aim of defining an array computing package. Among its members was Python designer and maintainer Guido van Rossum, who implemented extensions to Python's syntax (in particular the indexing syntax) to make array computing easier.\n\nAn implementation of a matrix package was completed by Jim Fulton, then generalized by Jim Hugunin to become \"Numeric\", also variously called Numerical Python extensions or NumPy.\nHugunin, a graduate student at Massachusetts Institute of Technology (MIT), joined the Corporation for National Research Initiatives (CNRI) to work on JPython in 1997 leaving Paul Dubois of Lawrence Livermore National Laboratory (LLNL) to take over as maintainer. Other early contributors include David Ascher, Konrad Hinsen and Travis Oliphant.\n\nA new package called \"Numarray\" was written as a more flexible replacement for Numeric. Like Numeric, it is now deprecated. Numarray had faster operations for large arrays, but was slower than Numeric on small ones, so for a time both packages were used for different use cases. The last version of Numeric v24.2 was released on 11 November 2005 and numarray v1.5.2 was released on 24 August 2006.\n\nThere was a desire to get Numeric into the Python standard library, but Guido van Rossum decided that the code was not maintainable in its state then.\n\nIn early 2005, NumPy developer Travis Oliphant wanted to unify the community around a single array package and ported Numarray's features to Numeric, releasing the result as NumPy 1.0 in 2006. This new project was part of SciPy. To avoid installing the large SciPy package just to get an array object, this new package was separated and called NumPy. Support for Python 3 was added in 2011 with NumPy version 1.5.0.\n\nIn 2011, PyPy started development on an implementation of the NumPy API for PyPy. It is not yet fully compatible with NumPy.\n\nNumPy targets the CPython reference implementation of Python, which is a non-optimizing bytecode interpreter. Mathematical algorithms written for this version of Python often run much slower than compiled equivalents. NumPy addresses the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays, requiring rewriting some code, mostly inner loops using NumPy.\n\nUsing NumPy in Python gives functionality comparable to MATLAB since they are both interpreted, and they both allow the user to write fast programs as long as most operations work on arrays or matrices instead of scalars. In comparison, MATLAB boasts a large number of additional toolboxes, notably Simulink, whereas NumPy is intrinsically integrated with Python, a more modern and complete programming language. Moreover, complementary Python packages are available; SciPy is a library that adds more MATLAB-like functionality and Matplotlib is a plotting package that provides MATLAB-like plotting functionality. Internally, both MATLAB and NumPy rely on BLAS and LAPACK for efficient linear algebra computations.\n\nPython bindings of the widely used computer vision library OpenCV utilize NumPy arrays to store and operate on data.\nSince images with multiple channels are simply represented as three-dimensional arrays, indexing, or masking with other arrays are very efficient ways to access specific pixels of an image.\nThe NumPy array as universal data structure in OpenCV for images, extracted feature points, filter kernels and many more vastly simplifies the programming workflow and debugging.\n\nThe core functionality of NumPy is its \"ndarray\", for \"n\"-dimensional array, data structure. These arrays are strided views on memory. In contrast to Python's built-in list data structure (which, despite the name, is a dynamic array), these arrays are homogeneously typed: all elements of a single array must be of the same type.\n\nSuch arrays can also be views into memory buffers allocated by C/C++, Cython, and Fortran extensions to the CPython interpreter without the need to copy data around, giving a degree of compatibility with existing numerical libraries. This functionality is exploited by the SciPy package, which wraps a number of such libraries (notably BLAS and LAPACK). NumPy has built-in support for memory-mapped ndarrays.\n\nInserting or appending entries to an array is not as trivially possible as it is with Python's lists.\nThe np.pad(...) routine to extend arrays actually creates new arrays of the desired shape and padding values, copies the given array into the new one and returns it.\nNumPy's np.concatenate([a1,a2]) operation does not actually link the two arrays but returns a new one, filled with the entries from both given arrays in sequence.\nReshaping the dimensionality of an array with np.reshape(...) is only possible as long as the number of elements in the array does not change.\nThese circumstances originate from the fact that NumPy's arrays must be views on contiguous memory buffers. A replacement package called Blaze attempts to overcome this limitation.\n\nAlgorithms that are not expressible as a vectorized operation will typically run slowly because they must be implemented in \"pure Python\", while vectorization may increase memory complexity of some operations from constant to linear, because temporary arrays must be created that are as large as the inputs. Runtime compilation of numerical code has been implemented by several groups to avoid these problems; open source solutions that interoperate with NumPy include codice_1, numexpr and Numba. Cython and Pythran are static-compiling alternatives to these.\n\n\n\n\n"}
{"id": "1028314", "url": "https://en.wikipedia.org/wiki?curid=1028314", "title": "Penman equation", "text": "Penman equation\n\nThe Penman equation describes evaporation (\"E\") from an open water surface, and was developed by Howard Penman in 1948. Penman's equation requires daily mean temperature, wind speed, air pressure, and solar radiation to predict E. Simpler Hydrometeorological equations continue to be used where obtaining such data is impractical, to give comparable results within specific contexts, e.g. humid vs arid climates.\n\nNumerous variations of the Penman equation are used to estimate evaporation from water, and land. Specifically the Penman-Monteith equation refines weather based potential evapotranspiration (PET) estimates of vegetated land areas. It is widely regarded as one of the most accurate models, in terms of estimates. \n\nThe original equation was developed by Howard Penman at the Rothamsted Experimental Station, Harpenden, UK.\n\nThe equation for evaporation given by Penman is:\nwhere:\n\nwhich (if the SI units in parentheses are used) will give the evaporation \"E\" in units of kg/(m²·s), kilograms of water evaporated every second for each square meter of area. \n\nRemove λ to obviate that this is fundamentally an energy balance. Replace \"λ\" with L to get familiar precipitation units \"ET\", where \"L\"=\"λ\"\"ρ\". This has units of m/s, or more commonly mm/day, because it is flux m/s per m=m/s. \n\nThis equation assumes a daily time step so that net heat exchange with the ground is insignificant, and a unit area surrounded by similar open water or vegetation so that net heat & vapor exchange with the surrounding area cancels out. Some times people replace \"R\" with and \"A\" for total net available energy when a situation warrants account of additional heat fluxes.\n\ntemperature, wind speed, relative humidity impact the values of \"m\", \"g\", \"c\", \"ρ\", and δ\"e\".\n\nIn 1993, W.Jim Shuttleworth modified and adapted the Penman equation to use SI, which made calculating evaporation simpler. The resultant equation is:\n\nwhere:\n\nNote: this formula implicitly includes the division of the numerator by the density of water (1000 kg m) to obtain evaporation in units of mm d\n\nTherefore formula_4, mmHg/K\n\n\n"}
{"id": "12437622", "url": "https://en.wikipedia.org/wiki?curid=12437622", "title": "Problems in Latin squares", "text": "Problems in Latin squares\n\nIn mathematics, the theory of Latin squares is an active research area with many open problems. As in other areas of mathematics, such problems are often made public at professional conferences and meetings. Problems posed here appeared in, for instance, the \"Loops (Prague)\" conferences and the \"Milehigh (Denver)\" conferences.\n\nA \"transversal\" in a Latin square of order n is a set S of n cells such that every row and every column contains exactly one cell of S, and such that the symbols in S form {1...,n}. Let T(n) be the maximum number of transversals in a Latin square of order n. Estimate T(n).\n\nDescribe how all Latin subsquares in multiplication tables of Moufang loops arise.\n\nA partial Latin square has \"Blackburn property\" if whenever the cells (i,j) and (k,l) are occupied by the same symbol, the opposite corners (i,l) and (k,j) are empty. What is the highest achievable density of filled cells in a partial Latin square with the Blackburn property? In particular, is there some constant c > 0 such that we can always fill at least c n cells?\n\nLet formula_1 be the number of Latin squares of order n. What is the largest integer formula_2 such that formula_3 divides formula_1? Does formula_2 grow quadratically in n?\n\n\n\n\n"}
{"id": "4741467", "url": "https://en.wikipedia.org/wiki?curid=4741467", "title": "Reliable Replacement Warhead", "text": "Reliable Replacement Warhead\n\nThe Reliable Replacement Warhead (RRW) was a proposed new American nuclear warhead design and bomb family that was intended to be simple, reliable and to provide a long-lasting, low-maintenance future nuclear force for the United States. Initiated by the United States Congress in 2004, it became a centerpiece of the plans of the National Nuclear Security Administration (NNSA) to remake the nuclear weapons complex.\n\nIn 2008, Congress denied funding for the program, and in 2009 the Obama administration called for work on the program to cease.\n\nDuring the Cold War, the United States, in an effort to achieve and maintain an advantage in the nuclear arms race with the Soviet Union, invested large amounts of money and technical resources into nuclear weapons design, testing, and maintenance. Many of the weapons designed required high upkeep costs, justified primarily by their Cold War context and the specific and technically sophisticated applications they were created for. With the end of the Cold War, however, nuclear testing has ceased in the United States, and new warhead development has been significantly reduced. As a result, the need for high technical performance of warheads has decreased considerably, and the need for a longer-lasting and reliable stockpile has taken a high priority.\n\nPrior nuclear weapons produced by the U.S. had historically become extremely compact, low weight, highly integrated, and low-margin designs which used exotic materials. In many cases the components were toxic and/or unstable. A number of older US designs used high explosive types which degraded over time, some of which became dangerously unstable in short lifetimes (PBX 9404 and LX-09).\nSome of these explosives have cracked in warheads in storage, resulting in dangerous storage and disassembly conditions.\n\nMost experts believe that the insensitive explosives (PBX 9502, LX-17) currently in use are highly stable and may even become more stable over time.\n\nThe use of beryllium and highly toxic beryllium oxide material as neutron reflector layers was a major health hazard to bomb manufacturer and maintenance staff. The long term stability of plutonium metal, which may lose strength, crack, or otherwise degrade over time is also a concern. (See Nuclear weapons design and Teller-Ulam design for technical context.)\n\nThe question of whether the plutonium-gallium alloy used in the cores of the weapons suffered from aging has been a major topic of research at the weapons laboratories in recent decades. Though many at the labs still insist on scientific uncertainty on the question, a study commissioned by the National Nuclear Security Administration to the independent JASON group concluded in November 2006 that \"most plutonium pits have a credible lifetime of at least 100 years\". The oldest pits currently in the US arsenal are still less than 50 years old.\n\nThe concept underlying the RRW program is that the US weapons laboratories can design new nuclear weapons that are highly reliable and easy and safe to manufacture, monitor, and test. If that proves to be possible, designers could adapt a common set of core design components to various use requirements, such as different sized missile warheads, different nuclear bomb types, etc.\n\nNNSA officials believe the program is needed to maintain nuclear weapons expertise in order to rapidly adapt, repair, or modify existing weapons or develop new weapons as requirements evolve. They see the ability to adapt to changing military needs rather than maintain additional forces for unexpected contingencies as a key program driver. However, Congress has rejected the notion that the RRW is needed to meet new military requirements. In providing funds for 2006, the Appropriations Committee specified, \"any weapons design under the RRW program must stay within the military requirements of the existing deployed stockpile and any new weapon design must stay within the design parameters validated by past nuclear tests\".\n\nAccording to a Task Force of the Secretary of Energy's Advisory Board (SEAB), the RRW program and weapon designs should have the following characteristics:\n\nHowever, the full SEAB disavowed the Task Force's recommendations regarding the RRW, because the Task Force did not consider the program's potentially adverse impacts on U.S. nonproliferation objectives, which were beyond its expertise.\n\nThe RRW program has not to date publicly announced that it has developed any new nuclear weapon designs which are intended to be placed into production. Presumably, once that occurs, the weapons will receive numbers in the US warhead designation sequence, which currently runs from the Mark 1 nuclear bomb (aka Little Boy) to the W91 nuclear warhead, which was cancelled in the 1990s. RRW designs would presumably receive designations after that number, though new RNEP nuclear bunker buster weapons could conceivably be type-standardized and numbered prior to any RRW reaching that point, if the RNEP program does proceed.\n\nOn March 2, 2007, the NNSA announced that the Lawrence Livermore National Laboratory RRW design had been selected for the initial RRW production version.\n\nOne of the selection reasons given was that the LLNL proposed design was more closely tied to historical underground tested warhead designs. It was described by Thomas P. D'Agostino, acting head of the National Nuclear Security Administration, as having been based on a design which was test fired in the 1980s, but never entered service.\n\nLLNL staff have previously hinted in the press that LLNL was considering a design entry based on the tested but never deployed W89 design.\nThis warhead had been proposed as a W88 warhead replacement as early as 1991. The W89 design was already equipped with all then-current safety features, including insensitive high explosives, fire-resistant pits, and advanced detonator safety systems. The W89 was also reportedly designed using recycled pits from the earlier W68 nuclear weapon program, recoated in vanadium to provide the temperature resistance. The W89 warhead was test fired in the 1980s. It had entered Phase 2A technical definition and cost study in November, 1986, and Phase 3 development engineering and was assigned the numerical designation W89 in January 1988. The lead designer, Bruce Goodwin, referred to the primary as the \"SKUA9\" design which he said had been tested a number of times.\n\nThe W89 warhead design was a by weapon, with a weight of and yield of . As noted above, major safety features inherent in the tested W89 design include:\n\nModifications for the RRW design would probably have included replacing beryllium neutron reflector layers with another material, and increased performance margins throughout the design, possibly including more fissile material in the pit and a thicker radiation case or hohlraum (see Teller-Ulam design: Basic design).\n\nIn an April 15, 2006, article by Walter Pincus in the Washington Post, Linton F. Brooks, administrator of the US National Nuclear Safety Administration, the US nuclear weapon design agency within the United States Department of Energy, announced that two competing designs for the Reliable Replacement Warhead were being finalized by Lawrence Livermore National Laboratory and Los Alamos National Laboratory, and that a selection of one of those designs would be made by November 2006, to allow the RRW development program to be included in the Fiscal 2008 US government budget.\n\nThe article confirmed prior descriptions of the RRW, describing the weapons in the following terms:\n\nBased on prior weapons programs, the RRW should be assigned a numerical weapon designation when the design selection is made.\n\nOn December 1, 2006, the NNSA announced that it had decided to move forwards with the RRW program after analyzing the initial LLNL and LANL RRW proposals. At that time, NNSA's Nuclear Weapons Council had not selected which of the two designs to proceed forwards with.\n\nAccording to the FY 2008 NNSA budget (pp 88), the RRW program is described as:\n\nAnd (pp 94)\n\nFunding is listed as $25 million for FY 2006, $28 million for FY 2007, and $89 million for FY 2008.\n\nAs defined in an earlier UC report, nuclear weapons engineering phases are:\n\nThe FY08 RRW budget therefore indicates that one of the RRW designs has been approved and is entering the design definition and cost study phase. The document does not state which of the RRW designs has been selected.\n\nHistorically, the weapon's nuclear series identification is assigned at the entrance to phase 3, and if the design proceeds forwards to complete phase 2 and enter phase 3 this can be expected in 1–2 years.\n\nThe design is intended for first production unit (FPU) delivery by the end of 2012.\n\nOn March 2, 2007, the NNSA announced that the Lawrence Livermore National Laboratory RRW design had been selected for the initial RRW production version.\n\nThe National Defense Authorization Act for Fiscal Year 2008, H.R. 4986, Section 3111, forbids the expenditure of funds for the RRW program beyond Phase 2A; in effect, this prevents the RRW program from going forward without explicit Congressional authorization. Section 3121 Subsection 1 requires the study of the reuse of previously manufactured plutonium cores in any RRW warheads, so as to avoid the manufacture of additional plutonium cores. Section 3124 reaffirms the commitment of the U.S. to the Treaty on the Non-Proliferation of Nuclear Weapons and encourages the mutual reduction in armament of the U.S. and Russia through negotiation.\n\nPresident Obama's 2009 Department of Energy budget calls for development work on the Reliable Replacement Warhead project to cease.\n\nOpponents of the RRW program believe it has nothing to do with making US weapons safer or more reliable, but is merely an excuse for designing new weapons and maintaining jobs at the weapons laboratories. They note that the Secretaries of Defense and Energy have certified that the existing nuclear weapons stockpile is safe and reliable in each of the last nine years. The existing stockpile was extensively tested before the US entered the moratorium on nuclear weapons tests. According to Sidney Drell and Ambassador James Goodby, \"It takes an extraordinary flight of imagination to postulate a modern new arsenal composed of such untested designs that would be more reliable, safe and effective than the current U.S. arsenal based on more than 1,000 tests since 1945\".\n\nCritics maintain that this innocuous-sounding program could significantly damage US national security. Critics believe an expansive RRW program would anger US allies as well as hostile nations. They worry it would disrupt the global cooperation in nonproliferation that is vital to diplomacy with emerging nuclear powers such as Iran and North Korea and to controlling clandestine trafficking in nuclear materials and equipment.\n\nAdditionally, critics question whether or not the RRW program would force the United States to once again resume nuclear testing, as the US is unlikely to consider the new warheads \"reliable\" enough unless they have been tested at least once.\n\n\n"}
{"id": "199121", "url": "https://en.wikipedia.org/wiki?curid=199121", "title": "Rydberg constant", "text": "Rydberg constant\n\nThe Rydberg constant, symbol \"R\" for heavy atoms or \"R\" for hydrogen, named after the Swedish physicist Johannes Rydberg, is a physical constant relating to atomic spectra, in the science of spectroscopy. The constant first arose as an empirical fitting parameter in the Rydberg formula for the hydrogen spectral series, but Niels Bohr later showed that its value could be calculated from more fundamental constants, explaining the relationship via his \"Bohr model\". , \"R\" and electron spin g-factor are the most accurately measured fundamental physical constants.\n\nThe Rydberg constant represents the limiting value of the highest wavenumber (the inverse wavelength) of any photon that can be emitted from the hydrogen atom, or, alternatively, the wavenumber of the lowest-energy photon capable of ionizing the hydrogen atom from its ground state. The spectrum of hydrogen can be expressed simply in terms of the Rydberg constant, using the Rydberg formula.\n\nThe Rydberg unit of energy, symbol Ry, is closely related to the Rydberg constant. It corresponds to the energy of the photon whose wavenumber is the Rydberg constant, i.e. the ionization energy of the hydrogen atom.\n\nAccording to the 2014 CODATA, the constant is:\n\nwhere formula_2 is the rest mass of the electron, formula_3 is the elementary charge, formula_4 is the permittivity of free space, formula_5 is the Planck constant, and formula_6 is the speed of light in vacuum.\n\nThis constant is often used in atomic physics in the form of the Rydberg unit of energy: \n\nThe Bohr model explains the atomic spectrum of hydrogen (see hydrogen spectral series) as well as various other atoms and ions. It is not perfectly accurate, but is a remarkably good approximation in many cases, and historically played an important role in the development of quantum mechanics. The Bohr model posits that electrons revolve around the atomic nucleus in a manner analogous to planets revolving around the sun.\n\nIn the simplest version of the Bohr model, the mass of the atomic nucleus is considered to be infinite compared to the mass of the electron, so that the center of mass of the system, the barycenter, lies at the center of the nucleus. This infinite mass approximation is what is alluded to with the formula_8 subscript. The Bohr model then predicts that the wavelengths of hydrogen atomic transitions are (see Rydberg formula):\nwhere \"n\" and \"n\" are any two different positive integers (1, 2, 3, ...), and formula_10 is the wavelength (in vacuum) of the emitted or absorbed light.\n\nA refinement of the Bohr model takes into account the fact that the mass of the atomic nucleus is not actually infinite compared to the mass of the electron. Then the formula is:\nwhere formula_12 and \"M\" is the total mass of the nucleus. This formula comes from substituting the reduced mass for the mass of the electron.\n\nA generalization of the Bohr model describes a hydrogen-like ion; that is, an atom with atomic number \"Z\" that has only one electron, such as C. In this case, the wavenumbers and photon energies are scaled up by a factor of \"Z\" in the model.\n\nThe Rydberg constant is one of the most well-determined physical constants, with a relative experimental uncertainty of fewer than 7 parts in 10. The ability to measure it to such a high precision constrains the proportions of the values of the other physical constants that define it. \"See\" precision tests of QED.\n\nSince the Bohr model is not perfectly accurate, due to fine structure, hyperfine splitting, and other such effects, the Rydberg constant formula_13 cannot be \"directly\" measured at very high accuracy from the atomic transition frequencies of hydrogen alone. Instead, the Rydberg constant is inferred from measurements of atomic transition frequencies in three different atoms (hydrogen, deuterium, and antiprotonic helium). Detailed theoretical calculations in the framework of quantum electrodynamics are used to account for the effects of finite nuclear mass, fine structure, hyperfine splitting, and so on. Finally, the value of formula_13 comes from the best fit of the measurements to the theory.\n\nThe Rydberg constant can also be expressed as in the following equations.\n\nand\nwhere\nThe last expression in the first equation shows that the wavelength of light needed to ionize a hydrogen atom is 4π/α times the Bohr radius of the atom.\n\nThe second equation is relevant because its value is the coefficient for the energy of the atomic orbitals of a hydrogen atom: formula_29.\n\n"}
{"id": "18211613", "url": "https://en.wikipedia.org/wiki?curid=18211613", "title": "Script theory", "text": "Script theory\n\nScript theory is a psychological theory which posits that human behaviour largely falls into patterns called \"scripts\" because they function analogously to the way a written script does, by providing a program for action. Silvan Tomkins created script theory as a further development of his affect theory, which regards human beings' emotional responses to stimuli as falling into categories called \"affects\": he noticed that the purely biological response of affect may be followed by awareness and by what we cognitively do in terms of acting on that affect so that more was needed to produce a complete explanation of what he called \"human being theory\".\n\nIn script theory, the basic unit of analysis is called a \"scene\", defined as a sequence of events linked by the affects triggered during the experience of those events. Tomkins recognized that our affective experiences fall into patterns that we may group together according to criteria such as the types of persons and places involved and the degree of intensity of the effect experienced, the patterns of which constitute scripts that inform our behavior in an effort to maximize positive affect and to minimize negative affect.\n\nRoger Schank, Robert P. Abelson and their research group, extended Tomkins' scripts and used them in early artificial intelligence work as a method of representing procedural knowledge. In their work, scripts are very much like frames, except the values that fill the slots must be ordered. A script is a structured representation describing a stereotyped sequence of events in a particular context. Scripts are used in natural language understanding systems to organize a knowledge base in terms of the situations that the system should understand.\n\nThe classic example of a script involves the typical sequence of events that occur when a person drinks in a restaurant: \"finding a seat, reading the menu, ordering drinks from the waitstaff...\" In the script form, these would be decomposed into conceptual transitions, such as MTRANS and PTRANS, which refer to \"mental transitions [of information]\" and \"physical transitions [of things]\".\n\nSchank, Abelson and their colleagues tackled some of the most difficult problems in artificial intelligence (i.e., story understanding), but ultimately their line of work ended without tangible success. This type of work received little attention after the 1980s, but it is very influential in later knowledge representation techniques, such as case-based reasoning.\n\nScripts can be inflexible. To deal with inflexibility, smaller modules called memory organization packets (MOP) can be combined in a way that is appropriate for the situation.\n\n"}
{"id": "38476817", "url": "https://en.wikipedia.org/wiki?curid=38476817", "title": "Secondary city", "text": "Secondary city\n\nA secondary city often follows after a primate city and can be seen in the urban hierarchy.\nSecondary cities have between “500,000 to 3 million inhabitants, but are often unknown outside of their national or regional context. Many secondary cities in the Global South are expected to undergo massive expansions in the next few decades, comparable to city growth in Europe and North America over the past two centuries.\n\n\"A secondary city is largely determined by population, size, function, and economic status. Commonly, secondary cities are geographically defined urban jurisdictions or centres performing vital governance, logistical, and production functions at a sub-national or sub-metropolitan region level within a system of cities in a country. In some cases, their role and functions may expand to a geographic region of the global realm. The population of secondary-cities range between 10 and 50% of a country's largest city, although some can be smaller than this. They will likely constitute a sub-national or sub-metropolitan second-tier level of government, acting as centres for public administration and delivery of education, knowledge, health, community, and security services; an industrial centre or development growth pole; a new national capital; or a large city making up a cluster of smaller cities in a large metropolitan region.\"\n\nFurthermore, secondary cities “usually form more recent poles of growth, often also with a more diffuse genealogy, than larger metropoles. The ambivalent situation of these towns (in the periphery of the center and in the center of the periphery, in so far as these notions still retain their meaning) generates a particular, and by definition highly hybrid, socio-cultural urban dynamic which in turn influences the outlook of social, political and economic life in the more visible national metropoles.”\n\nSecondary cities have their own socio-economic and political culture that may differ from other cities such as Primate cites. Moreover, in the secondary city (and more generally in the margin of the state) there often is more room for improvisation. Local commerce, trading routes and smuggling networks determine the economic sphere in important ways; local forms of associational life (the middle ground of ‘civil society’) has a far greater influence on local politics than is the case in larger urban centers, and the functioning of local, decentralized political authorities is often shaped and cross-cut to a far greater extent by constantly shifting alliances between local stakeholders.”\n\nAccording to UN-HABITAT, \"slum population now expands annually by 25 million. In 1950 there were 86 cities in the world with a population over one million, today there are 400, and by 2015, there will be at least 550.\"\n\nMoreover, cities have absorbed nearly two-thirds of the global population explosion since 1950 and are currently growing by a million babies and migrants each week.\n\nConsequently, countries like India are developing secondary cities to absorb informals—as India's chief economic planner, Montek singh Ahluwalia in 2007, observes: \"One hundred million people are moving to cities in the next 10 years, and it’s important that these 100 million are absorbed into second-tier cities instead of showing up in Delhi or Mumbai.\" In Latin America, where primary cities long monopolized growth, secondary cities like Tijuana, Curitiba, Temuco, Salvador and Belem are now booming, \"with the fastest growth of all occurring cities with between 100,000 and 500,000 inhabitants.\"\n\nFurthermore, China \"may still be under the radar for many westerners but China's second and third-level cities will rise in profile over the next few years.\" New city developments by governments and other organizations are now focusing on secondary cities. Sociologist Saskia Sassen suggests that “small cities can be a global platform for companies' global expansion.”\n\nAlso in China, \"[t]he world's top hotel companies are clambering to open properties in China's second- and third-tier cities across all their brands. The sheer size of even tertiary cities, coupled with the growth in domestic travel and the potential gains of modern, international meeting facilities, has led to hotel development at unprecedented levels.\" These new developments are tying to integrate secondary cities into the global and local economy as well from the examples that are seen from China. Another development is that China Eastern Airlines has increased its number of flights to secondary cities in China.\n\nAccording to the World Bank, secondary Cities make up almost 40% of the world cities population. Nearly two-thirds of these are located in Africa and Asia: \"They form an important part of an emerging global system of cities.\" While the large cities play a significant role in shaping the new economic geography of cities in fostering global trade, travel and investment, it is secondary cities which will have a much stronger influence in the future upon the economic development of countries.\"\n\nSecondary cities fall into three categories or typologies:\n\n\n\"While the industrial revolution and 20th century national self-sufficiency industrialization policies, were to have a profound impact in shaping the development of countries, the current thrust of globalization is changing the dynamics and development of cities, especially developing cities, in ways few would have envisaged half a century ago. There is now growing levels of functional specialization and linkages occurring within the system of secondary cities. The new economic geography has increased competition in small, medium sized cities from international producers and markets. Growing specialization of production systems, supply chains, SMART logistics and inter-modal transfer systems, and the externalization of decision making by expanded consultation, decision-making and investment decisions well outside local government jurisdictions.\" If the efficiency of secondary cities were to improve this could double or triple the GDP of many poor cities and rural regions. In the countries where there is a less distorted system of secondary cities, countries that are not dominated by one mega city, there is generally lower levels of regional development disparities, higher levels of national productivity and income per capita\"\n\n"}
{"id": "47826256", "url": "https://en.wikipedia.org/wiki?curid=47826256", "title": "Sense of direction", "text": "Sense of direction\n\nSense of direction is the ability to know one's location and perform wayfinding. It is related to cognitive maps, spatial awareness, and spatial cognition.\nSense of direction can be impaired by brain damage, such as in the case of topographical disorientation.\n\nHumans create spatial maps whenever they go somewhere. Neurons called place cells inside the hippocampus fire individually while a person makes their way through an environment. This was first discovered in rats, when the neurons of the hippocampus were recorded. Certain neurons fired whenever the rat was in a certain area of its environment. These neurons form a grid when they are all put together on the same plane. \nWe get our sense of direction when we match up spatial maps we have stored in the hippocampus, to the pattern of firing neurons when we are trying to find our way back or trying to find our car in the parking lot.\n\nSense of direction can be measured with the Santa Barbara Sense-of-Direction Scale, a self-assessed psychometric test designed in 2002. This scale has been used to study sense of direction in many contexts, such as driving.\n"}
{"id": "33692814", "url": "https://en.wikipedia.org/wiki?curid=33692814", "title": "Servo (radio control)", "text": "Servo (radio control)\n\nServos (also RC servos) are small, cheap, mass-produced servomotors or other actuators used for radio control and small-scale robotics.\n\nMost servos are rotary actuators although other types are available. Linear actuators are sometimes used, although it is more common to use a rotary actuator with a bellcrank and pushrod. Some types, originally used as sail winches for model yachting, can rotate continuously.\n\nA typical servo consists of a small electric motor driving a train of reduction gears. A potentiometer is connected to the output shaft. Some simple electronics provide a closed-loop servomechanism.\n\nThe position of the output, measured by the potentiometer, is continually compared to the commanded position from the control (i.e., the radio control). Any difference gives rise to an error signal in the appropriate direction, which drives the electric motor either forwards or backwards, and moving the output shaft to the commanded position. When the servo reaches this position, the error signal reduces and then becomes zero, at which point the servo stops moving.\n\nIf the servo position changes from that commanded, whether this is because the command changes, \"or\" because the servo is mechanically pushed from its set position, the error signal will re-appear and cause the motor to restore the servo output shaft to the position needed.\n\nAlmost all modern servos are \"proportional servos\", where this commanded position can be anywhere within the range of movement. Early servos, and a precursor device called an \"escapement\", could only move to a limited number of set positions.\n\nRadio control servos are connected through a standard three-wire connection: two wires for a DC power supply and one for control, carrying a pulse-width modulation (PWM) signal. Each servo has a separate connection and PWM signal from the radio control receiver. This signal is easily generated by simple electronics, or by microcontrollers such as the Arduino. This, together with their low-cost, has led to their wide adoption for robotics and physical computing.\n\nRC servos use a three-pin 0.1\" spacing jack (female) which mates to standard 0.025\" square pins. The most common order is signal, +voltage, ground. The standard voltage is 4.8 V DC, however 6 V and 12 V is also used on a few servos. The control signal is a digital PWM signal with a 50 Hz frame rate. Within each 20 ms timeframe, an active-high digital pulse controls the position. The pulse nominally ranges from 1.0 ms to 2.0 ms with 1.5 ms always being center of range. Pulse widths outside this range can be used for \"overtravel\" - moving the servo beyond its normal range.\n\nThere are two general types of PWM. Each PWM defines a value that is used by the servo to determine its expected position. The first type is \"absolute\" and defines the value by the width of the active-high time pulse with an arbitrarily long period of low time. The second type is \"relative\" and defines the value by the percentage of time the control is active-high versus low-time. The \"absolute\" type allows up to eight servos to share one communication channel by multiplexing control signals using relatively simple electronics and is the basis of modern RC servos. The \"relative\" type is the more traditional usage of PWM whereby a simple low-pass filter converts a \"relative\" PWM signal into an analog voltage. The two types are both PWM because the servo responds to the width of the pulse. However, in the first case a servo may also be sensitive to pulse order.\n\nThe servo is controlled by three wires: ground, power, and control. The servo will move based on the pulses sent over the control wire, which set the angle of the actuator arm. The servo expects a pulse every 20 ms in order to gain correct information about the angle. The width of the servo pulse dictates the range of the servo's angular motion.\n\nA servo pulse of 1.5 ms width will typically set the servo to its \"neutral\" position (typically half of the specified full range), a pulse of 1.0 ms will set it to 0°, and a pulse of 2.0 ms to 90° (for a 90° servo). The physical limits and timings of the servo hardware varies between brands and models, but a general servo's full angular motion will travel somewhere in the range of 90° – 180° and the neutral position (45° or 90°) is almost always at 1.5 ms. This is the \"standard pulse servo mode\" used by all hobby analog servos.\n\nA hobby digital servo is controlled by the same \"standard pulse servo mode\" pulses as an analog servo. Some hobby digital servos can be set to another mode that allows a robot controller to read back the actual position of the servo shaft. Some hobby digital servos can optionally be set to another mode and \"programmed\", so it has the desired PID controller characteristics when it is later driven by a standard RC receiver.\n\nRC servos are usually powered by the receiver, which in turn is powered by battery packs or an electronic speed controller (ESC) with an integrated or a separate battery eliminator circuit (BEC). Common battery packs are either NiCd, NiMH or lithium-ion polymer battery (LiPo) type. Voltage ratings vary, but most receivers are operated at 5 V or 6 V.\n\nManufacturers and distributors of hobby RC servos often use a specific shorthand notation of mechanical properties of the servos. Two figures are typically stated: angular speed of servo shaft rotation and mechanical torque produced on the shaft. Speed is expressed as a time interval that a servo requires in order to rotate the shaft for 60° angle. Torque is expressed as weight that can be pulled up by the servo if it hangs from a pulley with a certain radius mounted on the shaft.\n\nFor example, if a servo model is described as \"0.2 s / 2 kg\", that should be interpreted as \"This servo rotates the shaft for 60° in 0.2 seconds, and it is able to pull up 2 kg weight using a 1 cm radius pulley\". That is, that particular servo model rotates the shaft with the angular speed of (2π / 6) / 0.2 s = 5.2 rad/s while producing 2 kg × 9.81 m/s = 19.6 N force at 1 cm distance, i.e. it produces 19.6 N × 0.01 m = 0.196 N m torque.\n\nAlthough not in accordance with either the SI or Imperial unit system, the shorthand notation is in fact quite useful, as 60° shaft rotation commands, 1 cm long shaft cranks, as well as control rod \"forces\" in kilogram-force range are typical in hobby RC world.\n\nContinuous-rotation servos are servos that do not have a limited travel angle, instead they can rotate continuously. They can be thought of as a motor and gearbox with servo input controls. In such servos the input pulse results in a rotational speed, and the typical 1.5 ms center value is the stop position. A smaller value should turn the servo clockwise and a higher one counterclockwise.\n\nThe earliest form of sequential (although not proportional) actuator for radio control was the \"escapement\". Like the device used in clocks, this escapement controls the release of stored energy from a spring or rubber band. Each signal from the transmitter operates a small solenoid that then allows a two- or four-lobed pawl to rotate. The pawl, like a clock, has two pallets so that the pawl can only rotate by one lobe's position, per signal pulse. This mechanism allows a simple keyed transmitter to give sequential control, i.e. selection between a number of defined positions at the model.\n\nA typical four-lobe escapement used for rudder control is arranged so that the first and third positions are \"straight ahead\", with positions two and four as \"left\" and \"right\" rudder. A single pulse from the first straight-ahead position allows it to move to left, or three pulses would select right. A further single pulse returns to straight-ahead. Such a system is difficult to use, as it requires the operator to remember which position the escapement is in, and so whether the next turn requires one or three pulses from the current position. A development of this was the two-lobe pawl, where keying the transmitter continuously (and thus holding the solenoid pallet in place) could be used to select the turn positions with the same keying sequence, no matter what the previous position.\n\nEscapements were low-powered, but light-weight. They were thus more popular for model aircraft than model boats. Where a transmitter and receiver had multiple control channels (e.g., a frequency-keyed reed receiver), then multiple escapements could be used together, one for each channel. Even with single channel radios, a sequence of escapements could sometimes be cascaded. Moving one escapement gave pulses that in turn drove a second, slower speed, escapement. Escapements were disappearing from radio control, in favour of servos, by the early 1970s.\n\n\n"}
{"id": "26592470", "url": "https://en.wikipedia.org/wiki?curid=26592470", "title": "The Periodic Table (Basher book)", "text": "The Periodic Table (Basher book)\n\nThe Periodic Table: Elements with Style is a 2007 children's science book created by Simon Basher and written by Adrian Dingle. It is the first book in Basher's science series, which includes \"Physics: Why Matter Matters!\", \"Biology: Life As We Know It\", \"Astronomy: Out of this World!\", \"Rocks and Minerals: A Gem of a Book\", and \"Planet Earth: What Planet Are You On?\", each of which is 128 pages long.\n\nThe book is arranged in eleven chapters plus an introduction, and includes a poster in the back of the book. Each chapter is on a different group of the periodic table (hydrogen, the alkali metals, the alkaline earth metals, the transition metals, the boron elements, the carbon elements, the nitrogen elements, the oxygen elements, the halogen elements, the noble gases, the lanthanides and actinides, and the transactinides). For every type of then known atom, Basher has created a \"manga-esque\" cartoon, and for many types of atoms, Dingle, a high-school chemistry teacher who also developed an award-winning chemistry website has written a couple paragraphs of facts to go with the cartoon. Dingle, who says that \"[s]cience is a serious business\", wanted in writing the book \"to get people engaged is to make it accessible while still presenting hard facts and knowledge,\" while Basher was concerned that the book's design be \"sharp and focused\" in order to \"connect with today's visually advanced young audience.\"\n\n\"Publishers Weekly\" said that the book was a \"lively introduction to the chart that has been the bane of many a chemistry student\", and in a review in \"New Scientist\", Vivienne Greig called \"The Periodic Table\" \"an engrossing read and an ideal way to painlessly impart a great deal of science history to seen-it-all-before teenagers.\" A review on the Royal Society of Chemistry website had some minor reservations about the book, but said it was \"endearing\" and succeeded in making learning chemistry easier and more fun.\n\n\"The Periodic Table: Elements with Style\" has also been reviewed in the \"Bulletin of the Center for Children's Books\" and the \"Journal of Chemical Education\".\n\n"}
{"id": "13484433", "url": "https://en.wikipedia.org/wiki?curid=13484433", "title": "The Politically Incorrect Guide to Science", "text": "The Politically Incorrect Guide to Science\n\nThe Politically Incorrect Guide to Science is a 2005 book by journalist Tom Bethell, in which the author addresses issues including HIV/AIDS denialism, intelligent design, and the relationship between science and Christianity. It was published by Regnery Publishing.\n\nThe book received negative reviews, and Bethell was criticized for misrepresenting science for political purposes.\n\nBethell, a senior editor at \"American Spectator\", and a former editor of the \"Washington Monthly\" discusses what conservatives have seen as the politicization of science. He addresses a number of issues, including global warming, nuclear power, DDT and control of malaria, HIV/AIDS denialism, cloning, genetic engineering, intelligent design, the trial of Galileo and the relationship between science and Christianity. On all these topics, Bethell argues that the Left have distorted scientific facts in order to advance their political agenda and to increase the size of government, often through scare campaigns like the risk of runaway climate change. He also states that the Left have tried to censor those scientists who disagree with their viewpoints, regardless of what the best scientific evidence might say.\n\n\"The Politically Incorrect Guide to Science\" was first published in 2005 by Regnery Publishing.\n\nThe book received a positive review from columnist William A. Rusher in \"The MetroWest Daily News\", a mixed review from Carl Grant in \"New Oxford Review\", and negative reviews from the journalist Chris Mooney in \"Skeptical Inquirer\" and Lisa Simpson Strange in the \"Glasgow Daily Times\". It was also reviewed by TB West in the \"Journal of American Physicians and Surgeons\", and discussed by George Neumayr in \"American Thinker\", Allan H. Ryskind in \"Human Events\", and in \"Nuclear News\". An overview of the book written by Bethell appeared in \"The American Spectator\" and Bethell was interviewed about the work in \"Human Events\" and LewRockwell.com. Bethell also discussed the work in \"National Review\".\n\nRusher credited Bethell with showing that the misuse of science to reinforce political viewpoints is a major political problem and with exposing \"liberal myths\" such as global warming and evolution, as well as beliefs about the dangers of nuclear power and DDT. He endorsed Bethell's view that federal funding provides scientists with an incentive to exaggerate such \"alleged dangers\".\n\nGrant credited Bethell with making important criticisms of the way in which science is done. He agreed with Bethell that scientists often have biases and conflicts of interest, and also expressed agreement with many of Bethell's views on the relationship of religion and science, writing that the evidence for naturalistic evolution was \"underwhelming\" and that, \"Much evolutionary theory is only a series of \"ad hoc\" explanations to cover the poor fit between Darwin’s theory and actual fact.\" However, he criticized Bethell for his dismissal of theistic evolution, for sometimes failing to \"provide reference where the context requires them\", such as in his discussions of the AIDS epidemic and the Catholic Church's treatment of Galileo Galilei, and for sometimes overstating his case, or alternately conceding too much to his opponents. Overall, he concluded that the book was \"moderately useful\".\n\nMooney argued that Bethell \"misrepresents the state of scientific knowledge on issues ranging from global warming to the vulnerability of endangered species to evolution\". However, he observed that Bethell's book was \"getting plenty of attention\" and selling well, that The Heritage Foundation had sponsored an event to promote it, and that it was \"likely to be read by a lot of people\". He considered its publication \"a highly significant development\", since it took the \"war on scientific knowledge from the political right\" in the United States \"to a new level of intensity\" and exposed the \"anti-science sentiments\" of many conservative Republicans. He wrote that Bethell \"provides a useful service\" by presenting \"discredited arguments\" often used to undermine well-established scientific conclusions. He accused Bethell of \"compiling scientific-sounding arguments to bolster a political conclusion\", misrepresenting some sources, presenting problematic \"general science policy arguments\", misguidedly encouraging journalists to criticize science, wrongly dismissing scientific consensus, and \"whipping up resentment of the scientific community among rank-and-file political conservatives.\" He found the book \"a very saddening and depressing read.\"\n\nStrange described the book as a \"tome of utter disinformation\" and Bethell as \"an ultra-conservative, right-wing religious zealot\" who \"takes the research actual scientists have worked on for years and either twists the findings to fit his own narrow-minded agenda\" or \"simply announces to the world that the efforts of dedicated, trained men and women in the fields of medicine, chemistry, molecular biology, genetics, etc.\" are nothing but \"junk science.\" She also charged Bethell with producing \"reams of type about subjects of which he has no clear understanding\" and of making \"no effort to educate himself on matters pertaining to actual scientific method and study.\" She also characterized Bethell's work as \"junk\".\n\nNeumayr credited Bethell with exposing global warming as propaganda. Ryskind welcomed Bethell's discussion of Darwinian theory, and maintained that Bethell had good credentials to discuss science. \"Nuclear News\" focused on Bethell's discussion of nuclear power. In LewRockwell.com, Bethell was interviewed by Ryan Setliff, who prefaced his interview by noting that Bethell had impressive credentials, noting that he was a senior editor with \"The American Spectator\", was \"an Oxford graduate with degrees in philosophy, physiology, and psychology\", and had also \"contributed to magazines and writes often on the discipline of science.\"\n\n\n"}
{"id": "34307401", "url": "https://en.wikipedia.org/wiki?curid=34307401", "title": "Thinking, Fast and Slow", "text": "Thinking, Fast and Slow\n\nThinking, Fast and Slow is a best-selling book published in 2011 by Nobel Memorial Prize in Economic Sciences laureate Daniel Kahneman. It was the 2012 winner of the National Academies Communication Award for best creative work that helps the public understanding of topics in behavioral science, engineering and medicine.\n\nThe book summarizes research that Kahneman conducted over decades, often in collaboration with Amos Tversky. It covers all three phases of his career: his early days working on cognitive biases, his work on prospect theory, and his later work on happiness.\n\nThe central thesis is a dichotomy between two modes of thought: \"System 1\" is fast, instinctive and emotional; \"System 2\" is slower, more deliberative, and more logical. The book delineates cognitive biases associated with each type of thinking, starting with Kahneman's own research on loss aversion. From framing choices to people's tendency to replace a difficult question with one which is easy to answer, the book highlights several decades of academic research to suggest that people place too much confidence in human judgment.\n\nIn the book's first section, Kahneman describes two different ways the brain forms thoughts:\n\n\nKahneman covers a number of experiments which purport to highlight the differences between these two thought systems and how they arrive at different results even given the same inputs. Terms and concepts include coherence, attention, laziness, association, jumping to conclusions, WYSIATI (What you see is all there is), and how one forms judgments. The System 1 vs. System 2 debate dives into the reasoning or lack thereof for human decision making, with big implications for many areas including law and market research.\n\nThe second section offers explanations for why humans struggle to think statistically. It begins by documenting a variety of situations in which we either arrive at binary decisions or fail to precisely associate reasonable probabilities with outcomes. Kahneman explains this phenomenon using the theory of heuristics. Kahneman and Tversky originally covered this topic in their landmark 1974 article titled Judgment under Uncertainty: Heuristics and Biases.\n\nKahneman uses heuristics to assert that System 1 thinking involves associating new information with existing patterns, or thoughts, rather than creating new patterns for each new experience. For example, a child who has only seen shapes with straight edges would experience an octagon rather than a triangle when first viewing a circle. In a legal metaphor, a judge limited to heuristic thinking would only be able to think of similar historical cases when presented with a new dispute, rather than seeing the unique aspects of that case. In addition to offering an explanation for the statistical problem, the theory also offers an explanation for human biases.\nThe \"anchoring effect\" names our tendency to be influenced by irrelevant numbers. Shown higher/lower numbers, experimental subjects gave higher/lower responses.\n\nThis is an important concept to have in mind when navigating a negotiation or considering a price. As an example, most people, when asked whether Gandhi was more than 114 years old when he died, will provide a much larger estimate of his age at death than others who were asked whether Gandhi was more or less than 35 years old. Experiments show that our behavior is influenced, much more than we know or want, by the environment of the moment.\nThe availability heuristic is a mental shortcut that occurs when people make judgments about the probability of events on the basis of how easy it is to think of examples. The availability heuristic operates on the notion that, \"if you can think of it, it must be important.\" The availability of consequences associated with an action is positively related to perceptions of the magnitude of the consequences of that action. In other words, the easier it is to recall the consequences of something, the greater we perceive these consequences to be. Sometimes, this heuristic is beneficial, but the frequencies at which events come to mind are usually not accurate reflections of the probabilities of such events in real life.\nSystem 1 is prone to substituting a difficult question with a simpler one. In what Kahneman calls their \"best-known and most controversial\" experiment, \"the Linda problem,\" subjects were told about an imaginary Linda, young, single, outspoken, and very bright, who, as a student, was deeply concerned with discrimination and social justice. They asked whether it was more probable that Linda is a bank teller or that she is a bank teller and an active feminist. The overwhelming response was that \"feminist bank teller\" was more likely than \"bank teller,\" violating the laws of probability. (Every feminist bank teller is a bank teller.) In this case System 1 substituted the easier question, \"Is Linda a feminist?\", dropping the occupation qualifier. An alternative view is that the subjects added an unstated cultural implicature to the effect that the other answer implied an exclusive or (xor), that Linda was not a feminist.\nKahneman writes of a \"pervasive optimistic bias\", which \"may well be the most significant of the cognitive biases.\" This bias generates the illusion of control, that we have substantial control of our lives.\n\nA natural experiment reveals the prevalence of one kind of unwarranted optimism. The planning fallacy is the tendency to overestimate benefits and underestimate costs, impelling people to take on risky projects. In 2002, American kitchen remodeling was expected on average to cost $18,658, but actually cost $38,769.\n\nTo explain overconfidence, Kahneman introduces the concept he labels \"What You See Is All There Is\" (WYSIATI). This theory states that when the mind makes decisions, it deals primarily with \"Known Knowns\", phenomena it has already observed. It rarely considers \"Known Unknowns\", phenomena that it knows to be relevant but about which it has no information. Finally it appears oblivious to the possibility of \"Unknown Unknowns\", unknown phenomena of unknown relevance.\n\nHe explains that humans fail to take into account complexity and that their understanding of the world consists of a small and necessarily un-representative set of observations. Furthermore, the mind generally does not account for the role of chance and therefore falsely assumes that a future event will mirror a past event.\nFraming is the context in which choices are presented. Experiment: subjects were asked whether they would opt for surgery if the \"survival\" rate is 90 percent, while others were told that the mortality rate is 10 percent. The first framing increased acceptance, even though the situation was no different.\nRather than consider the odds that an incremental investment would produce a positive return, people tend to \"throw good money after bad\" and continue investing in projects with poor prospects that have already consumed significant resources. In part this is to avoid feelings of regret.\n\nThis section of the book is dedicated to the undue confidence in what the mind believes it knows. It suggests that people often overestimate how much they understand about the world and underestimate the role of chance in particular. This is related to the excessive certainty of hindsight, when an event appears to be understood after it has occurred or developed. Kahneman's views on overconfidence are influenced by Nassim Nicholas Taleb.\n\nIn this section Kahneman returns to economics and expands his seminal work on Prospect Theory. He discusses the tendency for problems to be addressed in isolation and how, when other reference points are considered, the choice of that reference point (called a frame) has a disproportionate impact on the outcome. This section also offers advice on how some of the shortcomings of System 1 thinking can be avoided.\nKahneman developed prospect theory, the basis for his Nobel prize, to account for experimental errors he noticed in Daniel Bernoulli's traditional utility theory. According to Kahneman, Utility Theory makes logical assumptions of economic rationality that do not reflect people's actual choices, and does not take into account cognitive biases.\n\nOne example is that people are loss-averse: they are more likely to act to avert a loss than to achieve a gain. Another example is that the value people place on a change in probability (e.g., of winning something) depends on the reference point: people appear to place greater value on a change from 0% to 10% (going from impossibility to possibility) than from, say, 45% to 55%, and they place the greatest value of all on a change from 90% to 100% (going from possibility to certainty). This occurs despite the fact that under traditional utility theory all three changes give the same increase in utility. Consistent with loss-aversion, the order of the first and third of those is reversed when the event is presented as losing rather than winning something: there, the greatest value is placed on eliminating the probability of a loss to 0.\n\nAfter the book's publication, the \"Journal of Economic Literature\" published a thorough discussion of its take on prospect theory, as well as an analysis of the four fundamental factors that it rests on.\n\nThe fifth part of the book describes recent evidence which introduces a distinction between two selves, the 'experiencing self' and 'remembering self'.\nKahneman proposed an alternative measure that assessed pleasure or pain sampled from moment to moment, and then summed over time. Kahneman called this \"experienced\" well-being and attached it to a separate \"self.\" He distinguished this from the \"remembered\" well-being that the polls had attempted to measure. He found that these two measures of happiness diverged. \nThe author's significant discovery was that the remembering self does not care about the duration of a pleasant or unpleasant experience. Instead, it retrospectively rates an experience by the peak (or valley) of the experience, and by the way it ends. The remembering self dominated the patient's ultimate conclusion.\nKahneman first took up the study of well-being in the 1990s. At the time most happiness research relied on polls about life satisfaction. Having arrived at the subject from previously studying unreliable memories, the author was doubtful of the question of life satisfaction as a good indicator of happiness. He designed a question that focused instead on the well-being on the experiencing self. The author proposed that \"Helen was happy in the month of March\" if she spent most of her time engaged in activities that she would rather continue than stop, little time in situations that she wished to escape, and not too much time in a neutral state that wouldn't prefer continuing or stopping the activity either way.\n\n\nKahneman suggests that focusing on a life event such as a marriage or a new car can provide a distorted illusion of its true value. This \"focusing illusion\" revisits earlier ideas of substituting difficult questions and WYSIATI.\n\n\nSince the book's publication it has sold over 1.5 million copies worldwide. On the year of its publication, it was on the New York Times Bestseller List. The book was reviewed in media including the \"Huffington Post\", \"The Guardian\", \"The New York Times\", \"The Financial Times\", \"The Independent\", \"Bloomberg\" and \"The New York Review of Books\".\n\nThe book was widely reviewed in specialist journals, including the \"Journal of Economic Literature\", \"American Journal of Education\", \"The American Journal of Psychology\", \"Planning Theory\", \"The American Economist\", \"The Journal of Risk and Insurance\", \"The Michigan Law Review\", \"American Scientist\", \"Contemporary Sociology\", \"Science\", \"Contexts\", \"The Wilson Quarterly\", \"Technical Communication\", \"The University of Toronto Law Journal\", \"A Review of General Semantics\" and \"Scientific American Mind\".\n\nThe book was also reviewed in an annual magazine by \"The Association of Psychological Science\".\n\n\n"}
{"id": "14750722", "url": "https://en.wikipedia.org/wiki?curid=14750722", "title": "Thomas Mudge (horologist)", "text": "Thomas Mudge (horologist)\n\nThomas Mudge (1715 – 14 November 1794, London) was an English horologist who invented the lever escapement, the greatest single improvement ever applied to pocket watches.\n\nThomas Mudge was the second son of Zachariah Mudge, headmaster and clergyman, and his wife, Mary Fox. He was born in Exeter, but when he was young, the family moved to Bideford, where his father became headmaster of the grammar school. Thomas attended the same school and, when 14 or 15, was sent to London to be apprenticed to George Graham, the eminent clock and watch maker who had trained under Thomas Tompion. Graham’s business was situated in Water Lane, Fleet Street. \n\nWhen Mudge qualified as a watchmaker in 1738 he began to be employed by a number of important London retailers. Whilst making a most complicated equation watch for the eminent John Ellicott FRS, Mudge was discovered to be the actual maker of the watch and was subsequently directly commissioned to supply watches for Ferdinand VI of Spain. He is known to have made at least five watches for Ferdinand, including a watch that repeated the minutes as well as the quarters and hours.\nIn 1748 Mudge set himself up in business at 151 Fleet Street, and began to advertise for work as soon as his old master, George Graham, died in 1751. He rapidly acquired a reputation as one of England’s outstanding watchmakers, and is now rightly considered one of the greatest and most influential watch and clock makers of the period. In 1753 he married Abigail Hopkins of Oxford, with whom he had two sons.\n\nAround 1755, if not earlier, Mudge invented the detached lever escapement, which he first applied to a clock, but which, in watches, can be considered the greatest single improvement ever applied to them, and which remains a feature in almost every pocket timekeeper made up to and including the present day.\n\nIn 1765 he published the book, \"Thoughts on the Means of Improving Watches, Particularly those for Use at Sea\".\n\nIn 1770, due to ill-health, Mudge quit active business and left London to live in Plymouth with his brother Dr John Mudge. From that date Mudge worked on the development of a marine chronometer that would satisfy the rigorous requirements of the Board of Longitude, which had been amended after the earlier work of John Harrison. He sent the first of these for trial in 1774, and was awarded 500 guineas for his design. \n\nHe completed two others in 1779 in the continuing attempt to satisfy the increasingly difficult requirements set by the Board of Longitude. They were tested by the Astronomer Royal, Nevil Maskelyne, and declared as being unsatisfactory. There followed a controversy in which it was claimed that Maskelyne had not given them a fair trial. \n\nA similar controversy had arisen when John Harrison had been denied the full amount of the 1714 prize by the Board of Longitude. Eventually, in 1792, two years before his death, Mudge was awarded £2,500 by a Committee of the House of Commons who decided for Mudge and against the Board of Longitude, then headed by Sir Joseph Banks.\nIn 1770 George III purchased a large gold watch produced by Mudge, that incorporated his lever escapement. This he presented to his wife, Queen Charlotte, and it still remains in the Royal Collection at Windsor Castle. In 1776 Mudge was appointed watchmaker to the king.\n\nIn 1789 his wife, Abigail died. Thomas Mudge died at the home of his elder son, Thomas, at Newington Butts, south London on 14 November 1794. He was buried at St Dunstan-in-the-West, Fleet Street.\n"}
{"id": "14842567", "url": "https://en.wikipedia.org/wiki?curid=14842567", "title": "Whitelee Wind Farm", "text": "Whitelee Wind Farm\n\nWhitelee Wind Farm is the largest on-shore wind farm in the United Kingdom (second in Europe to Fântânele-Cogealac, in Romania) with 215 Siemens and Alstom wind turbines and a total capacity of 539 megawatts (MW), with the average of 2.5MW per turbine. Whitelee was developed and is operated by ScottishPower Renewables, which is part of the Spanish company Iberdrola.\n\nThe Scottish government had a target of generating 31% of Scotland's electricity from renewable energy by 2011 and 100% by 2020. The majority of this is likely to come from wind power.\n\nPositioned 300 metres (985 feet) above sea level and outside Glasgow, Scotland’s largest city, the wind farm has over half a million people living within a 30 km radius. This makes Whitelee one of the first large-scale wind farms to be developed close to a centre of population. In May 2009, Whitelee was officially opened to the public by Alex Salmond MSP, First Minister for Scotland. However, Whitelee was generating power more than a year before this with the first phase of the wind farm supplying power to the electricity grid in January 2008.\n\nIn May 2009, the Scottish Government granted permission for an extension to the wind farm to produce up to a further 130 megawatts of power, which would increase the total generating capacity of Whitelee to 452 MW. There is also the potential to increase the generating capacity once again by 140 megawatts. This would give Whitelee the potential to generate almost 600 megawatts of renewable energy.\n\nOn 19 March 2010 a blade snapped off a turbine, resulting in temporary suspension of operations until safety checks were completed. Following the accident Keith Anderson, managing director of ScottishPower Renewables, said: \"This type of incident is exceptionally rare and highly unusual.\"\n\nIn 2011 Scottish Power Renewables appointed the joint venture of John Sisk and Son Limited and Roadbridge, as the main contractor for an extension to Whitelee. Located to the east of the existing Whitelee Windfarm, the 75 turbine extension was planned to increase the generating capacity of the windfarm to 539MW. Turbine deliveries were planned for July 2011, with the windfarm expected to be complete in May 2012.\n\nWhitelee has become an eco-tourist attraction aided by an on site visitor centre. The visitor centre is host to an interactive exhibition room, cafe, shop and education hub. It was officially opened to the public in September 2009. The visitor centre also gives access to a network of over 90 km of paths for cyclists, ramblers and horse riders. The visitor centre is managed by Glasgow Science Centre and offers activities for education and community groups.\nThere is also a dedicated - free - electric vehicle charging station.\n\nWhitelee wind farm has a Countryside Ranger Service operated jointly by East Renfrewshire and South Lanarkshire councils that works to promote and develop access opportunities for the public within the wind farm and wider area, as well as operating an annual program of free activities and events open to the public. The Whitelee Countryside Ranger Service also works to encourage and assist community and charity organisations to use of access opportunities within the wind farm for fundraising and charity events. The Ranger Service forms part of the Whitelee Access Planning Group which is made up of the wind farm operators, land owners, the three local authorities the wind farm comes within, local community groups and other interested parties to the site.\n\nIn January 2014 work began on a purpose built single track mountain bike course at the windfarm, within a hollow created by a former borrow pit that was used to supply stone during construction of the windfarm. This development is being led by East Renfrewshire Council on behalf of the Whitelee Access Planning Group. The track has been designed by Phil Saxena of Architrail Ltd – designer of the 2008 Beijing Olympic and 2014 Glasgow Commonwealth Games XC courses.\n\nThe project has followed strong public demand for more technical MTB facilities at Whitelee. Its setting within the UK’s largest onshore windfarm will make it unique amongst trail centres. The plans will provide graded trails to suit a wide range of users, from beginners to more experienced riders. The course will offer a mix of route options, technical sections and challenges, as well as a large picnic and viewing area, with wet weather shelters for use by families, clubs, schools etc. The facilities will extend across an area of approximately 12 hectares and will be free to use, 7 days a week.\n\nIn June 2012, Whitelee wind farm became the first wind energy project in Scotland to join the Association of Scottish Visitor Attractions. The management took the decision after nearly 250,000 people had visited the site since its opening since July 2009. ScottishPower Renewables said that nearly 10,000 pupils had so far visited Whitelee on school trips. In addition, at least \"another 100,000 people had accessed the wind farm's 90km (56 miles) of trails for recreational purposes such as jogging and cycling\".\n\nThe wind farm area includes Lochgoin Farm, the home of the Howie family which in the 17th century was a noted refuge for Covenanters, and was searched multiple times by government soldiers. In the 18th century John Howie became a biographer who recorded the lives of Covenanting martyrs in books published from 1775 onwards. In 1896 a stone obelisk was erected nearby as a monument \"in memory of John Howie, author of the \"Scots Worthies\"\"; this is accessible by the tracks leading from the visitor centre. A small museum at the farm holds relics of Covenanters, check for opening arrangements.\n\nWhitelee Windfarm currently has a 75 turbine extension under construction. This will add an additional 217 MW of capacity, enough to power the equivalent of over 124,000 homes. This will bring the total generating capacity of the wind farm up to 539 MW. Work on this new development commenced in November 2010, with the completion date extended from December 2012 to March 2013. Additionally, the new extension will add a further 44 km of trails to the site. \nJohn Sisk and Son Limited and Roadbridge were jointly appointed as Principal Contractors for the site during construction with Alstom Limited erecting and commissioning the wind turbines.\n\nIn August 2012 Scottish Power announced that it was applying for a further small extension of five turbines on the west of the existing site, adding 12 MW of capacity. This was refused by the DPEA on 19 Oct 2016. \n\nOn 19 March 2010 a blade snapped off a turbine which led to Whitelee wind farm being temporarily shut down while safety checks were completed on all other turbines on the site. \n\nIn March 2017 a turbine lost its nose cone, the entire site was closed until such time as the remaining turbines could be checked and tested. \n\nOn 29 March 2017 a worker fell from a turbine while it was undergoing maintenance, though that was not linked to his death. The Police are not treating his death as suspicious. \n\n\n"}
{"id": "11450751", "url": "https://en.wikipedia.org/wiki?curid=11450751", "title": "Wizard: The Life and Times of Nikola Tesla", "text": "Wizard: The Life and Times of Nikola Tesla\n\nThe book Wizard, the Life and Times of Nikola Tesla is a biography of Nikola Tesla by Marc J. Seifer published in 1996.\n\nSeifer follows the life of Nikola Tesla, the Serbian American inventor, electrical engineer, mechanical engineer, physicist, and futurist. He covers the high points of the inventors life through his designs used in the modern alternating current system, experimentation with high frequency current and wireless power transmission, wireless remote control, X-ray imaging, and Tesla's \"death ray\". Seifer goes on to cover Tesla's downfall, attributing it to Tesla's megalomaniacal, neurotic, self-destructive tendencies, and Tesla's interactions with financier J. Pierpont Morgan.\n\nSeifer based his book largely on primary sourced documents including Tesla's writings and patents. Also including papers of Tesla's in the Tesla archives in Yugoslavia as well as manuscripts in the United States. Seifer filed Freedom of Information Act requests to get documents from the FBI and other United States government agencies.\n\n\n\n"}
