{"id": "35046050", "url": "https://en.wikipedia.org/wiki?curid=35046050", "title": "Abell 1795", "text": "Abell 1795\n\nAbell 1795 is a galaxy cluster in the Abell catalogue.\n\nIn January 2014, Chandra X-Ray Observatory claimed to have made discovery of a new supermassive black hole candidate disrupting star in the Abell 1795.\n\n"}
{"id": "7229349", "url": "https://en.wikipedia.org/wiki?curid=7229349", "title": "Argus laser", "text": "Argus laser\n\nArgus was a two-beam high power infrared neodymium doped silica glass laser with a output aperture built at Lawrence Livermore National Laboratory in 1976 for the study of inertial confinement fusion. Argus advanced the study of laser-target interaction and paved the way for the construction of its successor, the 20 beam Shiva laser.\n\nIt was known from some of the earlier experiments in ICF that when large laser systems amplified their beams beyond a certain point (typically around the gigawatt level), nonlinear optical effects would begin to appear due to the very intense nature of the light. The most serious effect among these was \"Kerr lensing\", where, because the beam is so intense, that during its passage through either air or glass the electric field of the light actually alters the index of refraction of the material and causes the beam at the most intense points to \"self focus\" down to filament like structures of extremely high intensity. When a beam collapses into extremely high intensity filaments like this, it can easily exceed the optical damage threshold of laser glass and other optics, severely damaging them by creating pits, cracks and grey tracks through the glass. These effects became so severe after just the first few amplification stages of early lasers, that it was seen as essentially impossible to exceed the gigawatt level for ICF lasers without destroying the laser itself after just a few shots.\n\nIn order to improve the quality of the amplified beams, LLNL had started experimenting with the use of spatial filters in the single-beam Cyclops laser, built the previous year. The basic idea was to extend the laser device into a very long \"beamline\", over which any imperfections that accumulated in the beam would be successively removed after every amplification stage. A series of tubes with lenses on either end would focus the light down to a point (the focal point) where it would pass through a pinhole which would reject stray unfocused light, smoothing the beam and eliminating the high intensity spots which would have otherwise been further amplified causing damage to down-beam optics. The technique was so successful on Argus it was often referred to as being \"the savior of laser ICF\".\n\nAfter the success of Cyclops in beam smoothing, the next step was to further increase the energy and power in the resulting beams. Argus used a series of five groups of amplifiers and spatial filters arranged along the beamlines, each one boosting power until it reached a total of about 1 kilojoule and 1-2 terawatts per beam. These intensities would have been impossible to achieve without the use of spatial filtering. Argus was designed primarily to characterize large laser beamlines and laser-target interactions, there was no attempt to actually achieve the fusion ignition state in the device as this was understood to be impossible at the energies Argus was capable of delivering. Argus however, was used to further explore higher yields of the so-called \"exploding pusher\" type targets and to develop x-ray diagnostic cameras to view the hot plasma in such targets, a technique crucial to characterization of target performance on later ICF lasers.\n\nArgus was capable of producing a total of about 4 terawatts of power in short pulses of up to about 100 picoseconds, or about 2 terawatts of power in a longer 1 nanosecond pulse (~2 kilojoules) on a 100 micrometer diameter fusion fuel capsule target. It became the first laser to perform experiments using X-rays produced by irradiating a hohlraum. The reduced production of hard X-ray energy via the production of hot electrons while using frequency doubled and tripled laser light (as opposed to the infrared light directly produced by the laser itself) was first noticed on Argus. This technique would also be later validated in the direct drive mode (at both the LLE and Novette laser) and subsequently used to enhance laser energy to target plasma coupling efficiency in experiments on nearly all subsequent laser inertial confinement devices. Argus was shut down and dismantled in September 1981. Maximum fusion yield for target implosions on Argus was about 10 neutrons per shot.\n\n\n"}
{"id": "25789394", "url": "https://en.wikipedia.org/wiki?curid=25789394", "title": "Arturo Sanchez-Azofeifa", "text": "Arturo Sanchez-Azofeifa\n\nGerardo Arturo Sánchez-Azofeifa is a full professor at the University of Alberta, the director of the university's Center for Earth Observation Sciences, and the director of Tropi-Dry, a research group focusing on land use/policy studies in tropical dry regions of the Americas. His research is related to the study of impacts of land use/cover change (LUCC) on biodiversity loss and habitat fragmentation in tropical dry forest environments.\n\nSánchez-Azofeifa uses advanced technology, including remote sensing and phenology towers, to evaluate land use and cover change in Mesoamerica. His research focuses on the efficacy of creating protected areas (National Parks and Biological Reserves) and use of Environmental Services payment methods to control tropical deforestation. In addition, his research examines remote sensing (multispectral and hyperspectral) in connection with Primary Productivity (PP), Leaf Area Index (LAI), Photosynthetic Active Radiation (PAR) and biodiversity, particularly in tropical secondary dry forests. He is currently developing techniques to identify and understand the relationship of lianas (non-self supporting tropical plants) and tropical hardwood species by means of hyperspectral remote sensing to generate data reflecting detailed leaf and canopy level information. Sánchez-Azofeifa's most cited paper is his 2006 Nature article, \"Widespread amphibian extinctions from epidemic disease driven by global warming.\"\n\n"}
{"id": "974163", "url": "https://en.wikipedia.org/wiki?curid=974163", "title": "Backstaff", "text": "Backstaff\n\nThe backstaff is a navigational instrument that was used to measure the altitude of a celestial body, in particular the Sun or Moon. When observing the Sun, users kept the Sun to their back (hence the name) and observed the shadow cast by the upper vane on a horizon vane. It was invented by the English navigator John Davis who described it in his book \"Seaman's Secrets\" in 1594.\n\nBackstaff is the name given to any instrument that measures the altitude of the sun by the projection of a shadow. It appears that the idea for measuring the sun's altitude using back observations originated with Thomas Harriot. Many types of instruments evolved from the cross-staff that can be classified as backstaves. Only the Davis quadrant remains dominant in the history of navigation instruments. Indeed, the Davis quadrant is essentially synonymous with backstaff. However, Davis was neither the first nor the last to design such an instrument and others are considered here as well.\n\nCaptain John Davis invented a version of the backstaff in 1594. Davis was a navigator who was quite familiar with the instruments of the day such as the mariner's astrolabe, the quadrant and the cross-staff. He recognized the inherent drawbacks of each and endeavoured to create a new instrument that could reduce those problems and increase the ease and accuracy of obtaining solar elevations.\n\nOne early version of the quadrant staff is shown in \"Figure 1\". It had an arc affixed to a staff so that it could slide along the staff (the shape is not critical, though the curved shape was chosen). The arc (A) was placed so that it would cast its shadow on the \"horizon vane\" (B). The navigator would look along the staff and observe the horizon through a slit in the horizon vane. By sliding the arc so that the shadow aligned with the horizon, the angle of the sun could be read on the graduated staff. This was a simple quadrant, but it was not as accurate as one might like. The accuracy in the instrument is dependent on the length of the staff, but a long staff made the instrument more unwieldy. The maximum altitude that could be measured with this instrument was 45°.\n\nThe next version of his quadrant is shown in \"Figure 2\". The arc on the top of the instrument in the previous version was replaced with a \"shadow vane\" placed on a transom. This transom could be moved along a graduated scale to indicate the angle of the shadow above the staff. Below the staff, a 30° arc was added. The horizon, seen through the \"horizon vane\" on the left, is aligned with the shadow. The \"sighting vane\" on the arc is moved until it aligns with the view of the horizon. The angle measured is the sum of the angle indicated by the position of the transom and the angle measured on the scale on the arc.\n\nThe instrument that is now identified with Davis is shown in \"Figure 3\". This form evolved by the mid-17th century. The quadrant arc has been split into two parts. The smaller radius arc, with a span of 60°, was mounted above the staff. The longer radius arc, with a span of 30° was mounted below. Both arcs have a common centre. At the common centre, a slotted \"horizon vane\" was mounted (B). A moveable \"shadow vane\" was placed on the upper arc so that its shadow was cast on the horizon vane. A moveable \"sight vane\" was mounted on the lower arc (C).\n\nIt is easier for a person to place a vane at a specific location than to read the arc at an arbitrary position. This is due to Vernier acuity, the ability of a person to align two line segments accurately. Thus an arc with a small radius, marked with relatively few graduations, can be used to place the shadow vane accurately at a specific angle. On the other hand, moving the sight vane to the location where the line to the horizon meets the shadow requires a large arc. This is because the position may be at a fraction of a degree and a large arc allows one to read smaller graduations with greater accuracy. The large arc of the instrument, in later years, was marked with transversals to allow the arc to be read to greater accuracy than the main graduations allow.\n\nThus Davis was able to optimize the construction of the quadrant to have both a small and a large arc, allowing the effective accuracy of a single arc quadrant of large radius without making the entire instrument so large. This form of the instrument became synonymous with the backstaff. It was one of the most widely used forms of the backstaff. Continental European navigators called it the English Quadrant.\n\nA later modification of the Davis quadrant was to use a \"Flamsteed glass\" in place of the shadow vane; this was suggested by John Flamsteed. This placed a lens on the vane that projected an image of the sun on the horizon vane instead of a shadow. It was useful under conditions where the sky was hazy or lightly overcast; the dim image of the sun was shown more brightly on the horizon vane where a shadow could not be seen.\n\nIn order to use the instrument, the navigator would place the shadow vane at a location anticipating the altitude of the sun. Holding the instrument in front of him, with the sun at his back, he holds the instrument so that the shadow cast by the shadow vane falls on the horizon vane at the side of the slit. He then moves the sight vane so that he observes the horizon in a line from the sight vane through the horizon vane's slit while simultaneously maintaining the position of the shadow. This permits him to measure the angle between the horizon and the sun as the sum of the angle read from the two arcs.\n\nSince the shadow's edge represents the limb of the sun, he must correct the value for the semidiameter of the sun.\n\nThe Elton's quadrant derived from the Davis quadrant. It added an index arm with spirit levels to provide an artificial horizon.\n\nThe demi-cross was an instrument that was contemporary with the Davis quadrant. It was popular outside England.\n\nThe vertical transom was like a half-transom on a cross-staff, hence the name \"demi-cross\". It supported a \"shadow vane\" (A in \"Figure 4\") that could be set to one of several heights (three according to May, four according to de Hilster). By setting the shadow vane height, the range of angles that could be measured was set. The transom could be slid along the staff and the angle read from one of the graduated scales on the staff.\n\nThe \"sight vane\" (C) and \"horizon vane\" (B) were aligned visually with the horizon. With the shadow vane's shadow cast on the horizon vane and aligned with the horizon, the angle was determined. In practice, the instrument was accurate but more unwieldy than the Davis quadrant.\n\nThe plough was the name given to an unusual instrument that existed for a short time. It was part cross-staff and part backstaff. In \"Figure 5\", \"A\" is the transom that casts its shadow on the horizon vane at \"B\". It functions in the same manner as the staff in \"Figure 1\". \"C\" is the sighting vane. The navigator uses the sighting vane and the horizon vane to align the instrument horizontally. The sighting vane can be moved left to right along the staff. \"D\" is a transom just as one finds on a cross-staff. This transom has two vanes on it that can be moved closer or farther from the staff to emulate different-length transoms. The transom can be moved on the staff and used to measure angles.\n\nThe Almucantar staff is a device specifically used for measuring the altitude of the sun at low altitudes.\n\nThe cross-staff was normally a direct observation instrument. However, in later years it was modified for use with back observations.\n\nThere was a variation of the quadrant – the Back observation quadrant – that was used for measuring the sun's altitude by observing the shadow cast on a horizon vane.\n\nThomas Hood invented this cross-staff in 1590. It could be used for surveying, astronomy or other geometric problems.\n\nIt consists of two components, a transom and a yard. The transom is the vertical component and is graduated from 0° at the top to 45° at the bottom. At the top of the transom, a vane is mounted to cast a shadow. The yard is horizontal and is graduated from 45° to 90°. The transom and yard are joined by a special fitting (the \"double socket\" in \"Figure 6\") that permits independent adjustments of the transom vertically and the yard horizontally.\n\nIt was possible to construct the instrument with the yard at the top of the transom rather than at the bottom.\n\nInitially, the transom and yard are set so that the two are joined at their respective 45° settings. The instrument is held so that the yard is horizontal (the navigator can view the horizon along the yard to assist in this). The socket is loosened so that the transom is moved vertically until the shadow of the vane is cast at the yard's 90° setting. If the movement of just the transom can accomplish this, the altitude is given by the transom's graduations. If the sun is too high for this, the yard horizontal opening in the socket is loosened and the yard is moved to allow the shadow to land on the 90° mark. The yard then yields the altitude.\n\nIt was a fairly accurate instrument, as the graduations were well spaced compared to a conventional cross-staff. However, it was a bit unwieldy and difficult to handle in wind.\n\nA late addition to the collection of backstaves in the navigation world, this device was invented by Benjamin Cole in 1748.\n\nThe instrument consists of a staff with a pivoting quadrant on one end. The quadrant has a \"shadow vane\", which can optionally take a lens like the Davis quadrant's Flamsteed glass, at the upper end of the graduated scale (A in \"Figure 7\"). This casts a shadow or projects an image of the sun on the \"horizon vane\" (B). The observer views the horizon through a hole in the \"sight vane\" (D) and a slit in the horizon vane to ensure the instrument is level. The quadrant component is rotated until the horizon and the sun's image or shadow are aligned. The altitude can then be read from the quadrant's scale. In order to refine the reading, a circular vernier is mounted on the staff (C).\n\nThe fact that such an instrument was introduced in the middle of the 18th century shows that the quadrant was still a viable instrument even in the presence of the octant.\n\nGeorge Adams Sr. created a very similar backstaff at the same time. Adam's version ensured that the distance between the Flamsteed glass and horizon vane was the same as the distance from the vane to the sight vane.\n\nEdmund Gunter invented the cross bow quadrant, also called the mariner's bow, around 1623. It gets its name from the similarity to the archer's crossbow.\n\nThis instrument is interesting in that the arc is 120° but is only graduated as a 90° arc. As such, the angular spacing of a degree on the arc is slightly greater than one degree. Examples of the instrument can be found with a 0° to 90° graduation or with two mirrored 0° to 45° segments centred on the midpoint of the arc.\n\nThe instrument has three vanes, a \"horizon vane\" (A in \"Figure 8\") which has an opening in it to observe the horizon, a \"shadow vane\" (B) to cast a shadow on the horizon vane and a \"sighting vane\" (C) that the navigator uses to view the horizon and shadow at the horizon vane. This serves to ensure the instrument is level while simultaneously measuring the altitude of the sun. The altitude is the difference in the angular positions of the shadow and sighting vanes.\n\nWith some versions of this instrument, the sun's declination for each day of the year was marked on the arc. This permitted the navigator to set the shadow vane to the date and the instrument would read the altitude directly.\n\n\n\n"}
{"id": "31154709", "url": "https://en.wikipedia.org/wiki?curid=31154709", "title": "Berisad Glacier", "text": "Berisad Glacier\n\nBerisad Glacier (, ‘Lednik Berisad’ \\'led-nik 'be-ri-sad\\) is a glacier long and wide in Veregava Ridge, central Sentinel Range in Ellsworth Mountains, Antarctica. It flows north-northwestwards from Kushla Peak to join Dater Glacier northeast of Sipey Peak.\n\nThe glacier is named after the Thracian King Berisad, 358-352 BC.\n\nBerisad Glacier is centred at . US mapping in 1961, updated in 1988.\n\n\n\n"}
{"id": "13958057", "url": "https://en.wikipedia.org/wiki?curid=13958057", "title": "Berkeley Physics Course", "text": "Berkeley Physics Course\n\nThe Berkeley Physics Course is a series of college-level physics textbooks written mostly by UC Berkeley professors.\n\nThe series consists of the following five volumes, each of which was originally used in a one-semester course at Berkeley:\n\nVolume 2, \"Electricity and Magnetism\", by Purcell (Harvard), is particularly well known, and was influential for its use of relativity in the presentation of the subject at the introductory college level. Half a century later the book is still in print, in an updated version by authors Purcell and Morin. The third edition of the text, published by Cambridge University Press in 2013, was completely revised and updated to SI units.\n\nA Sputnik-era project funded by a National Science Foundation grant, the course arose from discussions between Philip Morrison (then at Cornell University) and Charles Kittel (Berkeley) in 1961, and was published by McGraw-Hill starting in 1965. The Berkeley course was contemporary with \"The Feynman Lectures on Physics\" (a college course at a similar mathematical level), and PSSC Physics (a high school introductory course). These physics courses were all developed in the atmosphere of urgency about science education created in the West by Sputnik.\n\nBecause of the government support received, the original editions contained notices on their copyright pages stating that the books were to be available royalty-free after five years. The authors got lump-sum payments but did not receive royalties. There was a parallel series of laboratory courses developed by Alan Portis.\n\nThe series was translated into a number of foreign languages. Although the course was influential in physics education worldwide, the book series sold better in foreign markets than in the US, possibly because students in other countries specialized earlier and were therefore better prepared mathematically than US students. It was felt to be too advanced for typical engineering students at Berkeley, but continued to be used there in honors courses for physics majors. Adoption may have also been hindered by the choice of Gaussian units of measurement, and later editions of volumes 1 and 2 were eventually published with the Gaussian system replaced by SI units.\n\n"}
{"id": "34999450", "url": "https://en.wikipedia.org/wiki?curid=34999450", "title": "Bert Sperling", "text": "Bert Sperling\n\nBertrand T. Sperling (born 1950 in Brooklyn, New York) is an author and researcher. His books and studies on quality of life in America have made him \"an internationally recognized expert on cities.\"\n\nSperling is commissioned to carry out demographic studies which highlight a particular aspect of American life. Past studies have included \"Funnest Cities to Live\", \"Best Cities for Singles\", \"Best Cities to Retire\", and \"Best Cities for Women’s Health\".\n\nIn 2004 Sperling released the 832-page \"Cities Ranked and Rated\", co-authored by Peter Sander and published by John Wiley and Sons. Sperling and Sander appeared on The Today Show in support of the book.\n\nThe 864-page Second Edition of \"Cities Ranked and Rated\" was published in 2007.\n\nThe 464-page \"Best Places to Raise Your Family\" was released in 2006 and again co-authored by Sander.\n\nSperling's interactive \"Places, U.S.A.\" computer program evolved into the Sperling's BestPlaces (BestPlaces.net) website. Using quarterly-updated statistics from dozens of sources, the site is a free resource for people across the country seeking to relocate. The site is also used as a recreational learning tool and academic reference.\n\nSperling has been a guest on the Today Show and featured in the New York Times. The results of his studies have been mentioned in \"The Simpsons\" animated TV show, \"The Tonight Show\", as well as in clues in the popular trivia game show \"Jeopardy!\"\n\nThe Simpsons episode \"They Saved Lisa’s Brain\" (Season 10, Episode 22) generated controversy for its mention of East St. Louis, Illinois as America's least livable city.\n\nIn the episode, Comic Book Guy announces that Springfield is 299th on a list of the United States' 300 most livable cities. East St. Louis is in last place. A journalist for a \"local East St Louis [news]paper\" noticed this, and called writer Matt Selman to ask him why they were \"taking a shot at East St Louis.\" Selman jokingly replied: \"because it's a crack-ridden slum.\" The Simpsons staff received several angry letters from East St Louis' residents, demanding an apology.\n\nSperling founded Dataccount in the early 1980s, a company specializing in model-specific software for the emerging laptop market. For example, he entered the United States Tax Code into programmable calculators, which were then used by tax professionals for estate planning.\n\nSperling was quoted in the May 7, 1984 issue of InfoWorld magazine for this expertise on portable computer programming. InfoWorld also turned to Sperling for his insight on the downfall of early laptop manufacturer Gavilan Computer in the magazine’s December 3, 1984 issue.\n\nSperling started Fast Forward, Inc. in 1984 and wrote an interactive software program called \"Places USA.\" It was the first application of its kind, allowing users to weigh their own criteria for what kind of lifestyle, amenities and demographic features were important for them. It then used these criteria and weightings to produce a list of the user’s ideal place to live, from a pool of the 300 largest metropolitan areas (MSA’s) in the U.S.\n\nAn article featuring the \"Places USA\" program appeared in the newspaper USA Today. In 1987 the editors of Money Magazine discovered his work and they commissioned him to compile their \"Best Cities\" lists. He continued to author these Money lists for nearly 20 years.\n\nSperling served as a consultant to author William G. Zikmund in the writing of the 1989 book \"Exploring Marketing Research\".\n\nThe \"Places USA\" software application continued to be used by researchers as late as 1996, when William Seavey cited the program in his book \"Moving to Small Town America\".\n\n\nRichard Florida, professor at University of Toronto and head of the Rotman School of Management Martin Prosperity Institute said, \"you need information based on life stage, job and a cluster and bundle of amenities, such as schools, health care, culture,\" adding that Sperling \"tends to be good at assessing most, if not all, of that.\"\n\nThomas Wetzel, president of the Retirement Living Information Center in Redding, Connecticut believes the BestPlaces.net website is \"the only one that does the comparisons people are looking for.\"\n\nFor the Bestplaces.net website, as well as the rankings in his books and media studies, Sperling uses a wide variety of data sources. Most of this data is public domain and compiled by government organizations, providing objectivity and third-party accountability.\n\nSources include the U.S. Census Bureau, the FBI, the Centers for Disease Control and Prevention, the Bureau of Labor Statistics, the National Oceanic and Atmospheric Administration, and the U.S. Department of Health and Human Services.\n\nSperling lived in Brooklyn for about a year following his birth. He grew up in San Diego, Oslo, Norway, Key West, Florida, and Carmel, California.\n\nHe graduated from Oregon State University in 1974 with a degree in Engineering. He was recently featured in an article in his alma mater’s alumni magazine.\n\nAfter college Sperling moved to Portland, Oregon. He currently splits his time between Portland and Depoe Bay, Oregon. He has two adult sons with his wife Gretchen.\n\n"}
{"id": "3229529", "url": "https://en.wikipedia.org/wiki?curid=3229529", "title": "Bioshelter", "text": "Bioshelter\n\nA bioshelter is a solar greenhouse managed as an indoor ecosystem. The word bioshelter was coined by the New Alchemy Institute and solar designers Sean Wellesley-Miller and Day Chahroudi. The term was created to distinguish their work in greenhouse design and management from twentieth century petro-chemical fuelled monoculture greenhouses. \n\nNew Alchemy's pioneering work in ecological design is documented in their published Journals and Reports. In 1976 the Alchemists built the Cape Cod Ark bioshelter and her sister The Prince Edward Island Ark. For the next 15 years the New Alchemy Institute studied and reported on the use of these prototype food producing ecosystems.\n\nA bioshelter (life-shelter) involves two fields of knowledge and design. The first is architecture designed to nurture an ecosystem within. A bioshelter structure uses glazing to contain and protect the living biology inside, control air exchange and absorb energy. The building exchanges nutrients, gases and energy with the surrounding environment, produces crops, and recycles waste organic material into the soil. Solar energy is stored as heat energy in thermal mass such as water, stone, masonry, soil and plant biomass. \n\nThe second is the biology inside the bioshelter. Earle Barnhart of the New Alchemy Institute has compared a bioshelter to a contained ecosystem. Solar heat is absorbed and stored in thermal mass to moderate air temperatures and provide heat for later use. Water moves from rainfall to fishponds to soil to plants and finally to water vapor. Year-round habitat is provided for beneficial insects . Ecological relationships between pests and their predators reduce the number of pests. Gases are exchanged among the animals, insects, micro-organisms, soil and plants. Nutrient cycles are developed between fish, plant & soil. Within the bioshelter are a variety of microclimates. The south areas receive the most direct sunlight. The east and west areas can be shaded for a portion of the day. Higher levels in a growing space will be warmer. A well-designed bioshelter, managed by human intelligence, can shelter a community of people, food crops, edible fish, and a diverse ecosystem of plants, animals and soil life.\n\n"}
{"id": "1115098", "url": "https://en.wikipedia.org/wiki?curid=1115098", "title": "Business ecosystem", "text": "Business ecosystem\n\nStarting in the early 1990s, James F. Moore originated the strategic planning concept of a business ecosystem, now widely adopted in the high tech community. The basic definition comes from Moore's book, \"\".\n\nThe concept first appeared in Moore's May/June 1993 \"Harvard Business Review\" article, titled \"Predators and Prey: A New Ecology of Competition\", and won the McKinsey Award for article of the year.\n\nMoore defined \"business ecosystem\" as:\nAn economic community supported by a foundation of interacting organizations and individuals—the organisms of the business world. The economic community produces goods and services of value to customers, who are themselves members of the ecosystem. The member organisms also include suppliers, lead producers, competitors, and other stakeholders. Over time, they coevolve their capabilities and roles, and tend to align themselves with the directions set by one or more central companies. Those companies holding leadership roles may change over time, but the function of ecosystem leader is valued by the community because it enables members to move toward shared visions to align their investments, and to find mutually supportive roles.\nMoore used several ecological metaphors, suggesting that the firm is embedded in a (business) environment, that it needs to coevolve with other companies, and that “the particular niche a business occupies is challenged by newly arriving species.” This meant that companies need to become proactive in developing mutually beneficial (\"symbiotic\") relationships with customers, suppliers, and even competitors.\n\nUsing ecological metaphors to describe business structure and operations is increasingly common especially within the field of information technology (IT). For example, J. Bradford DeLong, a professor of economics at the University of California, Berkeley, has written that \"business ecosystems\" describe “the pattern of launching new technologies that has emerged from Silicon Valley”. He defines business ecology as “a more productive set of processes for developing and commercializing new technologies” that is characterized by the “rapid prototyping, short product-development cycles, early test marketing, options-based compensation, venture funding, early corporate independence”. DeLong also has expressed that the new way is likely to endure “because it's a better business ecology than the legendarily lugubrious model refined at Xerox Parc—a more productive set of processes for rapidly developing and commercializing new technologies”.\n\nMangrove Software, The Montague Institute, Kenneth L. Kraemer, director of the University of California, Irvine’s Center for Research on Information Technology and Organizations and Stephen Abram, Vice President of Micromedia, Ltd., Tom Gruber, co-founder and CTO of Intraspect Software, Vinod K. Dar, Managing Director of Dar & Company, have all advocated this approach.\n\nGruber explains that over a century ago, Ford Motors did well using methods of mass production, an assembly line, and insourcing. However, Ford began to outsource its production “[w]hen the ecology evolved.” Gruber (n.d.) has stated that such evolution in the ecology of the business world is “punctuated now and then by radical changes in the environment” and that “globalization and the Internet are the equivalents of large-scale climate change. Globalization is eliminating the traditional advantages of the large corporation: access to capital, access to markets, and economies of scale”.\n\nThe application service provider (ASP) industry is moving toward relationship networks and focusing on core competencies. “According to the gospel of Cisco Systems, companies inclined to exist together within an “ecosystem” facilitate the imminence of Internet-based application delivery”.\n\nBooks also use natural systems metaphors without discussing the interfaces between human business and biological ecosystems.\n\nAnother work defines business ecology as “a new field for sustainable organizational management and design,” one “that is based on the principle that organizations, as living organisms, are most successful when their development and behavior are aligned with their core purpose and values – what we call “social DNA’”.\n\nThe need for companies to attend to ecological health is indicated by the following: “Business ecology is based on the elegant structure and principles of natural systems. It recognizes that to develop healthy business ecosystems, leaders and their organizations must see themselves, and their environments, through an “ecological lens”.\n\nSome environmentalists have used \"business ecosystems\" as a way to talk about environmental issues as they relate to business rather than as a metaphor to describe the increasing complexity of relationships among companies. According to Townsend, business ecology is the study of the reciprocal relationship between business and organisms and their environments. The goal of this \"business ecology\" is sustainability through the complete ecological synchronization and integration of a business with the sites that it inhabits, uses, and affects.\n\nOther environmentalists believe that the ecosystem metaphor is just a way for business to appear 'Green'. According to author Alan Marshall, the metaphor is used to make out that somehow business operates using natural principles which should be left to run without interference by governments.\n\n"}
{"id": "46759963", "url": "https://en.wikipedia.org/wiki?curid=46759963", "title": "Catherine Gage", "text": "Catherine Gage\n\nCatherine Gage (18 May 1815 – 16 February 1892) was an Irish botanist, botanical and ornithological illustrator.\n\nCatherine Gage was born in County Down on 18 May 1815, the daughter of Rev. Robert Gage and Catherine Boyd. Gage lived her entire life in the family home, Manor House on Rathlin Island. Gage died 16 February 1892 and was buried on Rathlin Island.\n\nGage seems to have devoted a large portion of her life to illustrating a book by her brother Robert Gage on the birds of Rathlin Island that was never published. The book was styled on that of John James Audubon's \"The Birds of America\". During the course of this work she produced over five hundred watercolours of birds. She also illustrated local plants, creating a list for the Botanical Society of Edinburgh, the abstract for which was published in the 1850 \"Annals and Magazine of Natural History\". She also worked with her sister, Barbara Gage, illustrating the local flora as well as the fauna.\n\nWhen the folio of bird illustrations was auctioned in 2010, they were sold for €13,500.\n"}
{"id": "42583131", "url": "https://en.wikipedia.org/wiki?curid=42583131", "title": "Deep-sea wood", "text": "Deep-sea wood\n\nFar from the most common energy supply, the sun, and from many nutrient supplies closely tied to the surface, the deep sea is still home to a unique ecosystem. Deep-sea wood is the term for wood which sinks to the ocean floor. All organisms of the ocean floor face unique challenges in synthesizing ATP/GTP needed for cellular function and replication. In this case, deep-sea wood supports a unique form of deep sea community life including chemo-synthetic bacteria. Sources of carbon for these organisms are not limited to wood, but also include kelp and the remains of whales. As it is difficult and very costly to simply discover logs that have fallen to the ocean floor, much of what is known about deep-sea wood is obtained from experiments by marine biologists, in which wood is forced to the bottom of the ocean for a set amount of time and is then collected later for sampling.\n\nColonization experiments revealed the presence of wood boring bivalves that belong to the subfamily Xylophagainae, such as \"Xylophaga dorsalis\", or other species recently described from deep-sea canyons. They range in shell size from 1-10mm. These bivalves are able to digest wood with the help of symbiotic bacteria in their gills.\n\nChemosyntheic muscles identified as \"Idas modiolaeformis\" were also found in deep sea wood when organic matter settled for at least one year. They are slightly smaller than the bivalves found and range in length from 1-6mm.\n\nA variety of deep-sea crabs and sea urchins seemed to also be chemically attracted to the wood. There are numerous species of snail that have been discovered on the wood, along with predatory worms and small crustaceans. Their attraction to the wood may be attributed to its bacterial inhabitants serving as a base organism for deep-sea life, with the potential to feed on microorganisms, or other inhabitants of the wood.\n\nFungi are the major degraders of lignocellulose in aquatic environments. In aerobic terrestrial environments, a majority of cellulose breakdown is broken down by wood-decay fungi commonly and collectively known as white rot and soft rot. Complex enzymes are secreted by the various fungi, converting cellulose into a carbon form that can be utilized by the fungus, and subsequently any organism up the food chain.\n\nBacteria also contribute to the digestion of deep-sea wood, using an alternative method from that of fungi. In order to classify bacteria present on deep-sea wood, a variety of different techniques are employed. First, biomass allows scientists to quantify the amount of bacterial growth on a sample. Then DNA extraction and Automated Ribosomal Intergenic Spacer analysis (ARISA) can be used to identify the strains of bacteria present that are most dominant, and the ones that are present.\n\nWhile \"Gammaproteobacteria\" dominated the composition of bacteria found on freshly submerged wood, many other bacterial strains populated in response to colonization of the aforementioned wood-boring \"Xylophaga\", which take the large chunks of wood and convert them into fine chips and fecal matter. These processed forms of carbon lead to the growth of many other marine bacteria including \"Alphaproteobacteria\", \"Flavobacteria\", \"Actinobacteria\", \"Clostridia\", and \"Bacteroidetes\".\n\nThe presence of \"Clostridia\", obligate anaerobes suggests that the process of degrading the deep-sea wood may create oxygen-free environments for these bacteria to survive in.\n\nMany bacterial strains that were found on deep-sea wood were sulfur-reducing bacteria, meaning they obtain energy from reducing elemental sulfur, instead of traditionally using the sun for energy like almost all other organisms. Marine biologists suggest they may contribute to the break down of cellulose from the wood.\n\nThe species of wood that falls to the ocean floor produces variability in the organisms present on it. There is also natural viability between organisms found on the same species of tree, which promotes to deep-sea diversity. If fact, a study by marine biologists showed bacterial communities were approximately 75% dissimilar, even as similar logs of the same tree species were placed within the same 500m area.\n"}
{"id": "20330321", "url": "https://en.wikipedia.org/wiki?curid=20330321", "title": "Dynamic global vegetation model", "text": "Dynamic global vegetation model\n\nA Dynamic Global Vegetation Model (DGVM) is a computer program that simulates shifts in potential vegetation and its associated biogeochemical and hydrological cycles as a response to shifts in climate. DGVMs use time series of climate data and, given constraints of latitude, topography, and soil characteristics, simulate monthly or daily dynamics of ecosystem processes. DGVMs are used most often to simulate the effects of future climate change on natural vegetation and its carbon and water cycles.\n\nDGVMs generally combine biogeochemistry, biogeography, and disturbance submodels. Disturbance is often limited to wildfires, but in principle could include any of: forest/land management decisions, windthrow, insect damage, ozone damage etc. DGVMs usually \"spin up\" their simulations from bare ground to equilibrium vegetation (e.g. climax community) to establish realistic initial values for their various \"pools\": carbon and nitrogen in live and dead vegetation, soil organic matter, etc. corresponding to a documented historical vegetation cover.\n\nDGVMs are usually run in a spatially distributed mode, with simulations carried out for thousands of \"cells\", geographic points which are assumed to have homogeneous conditions within each cell. Simulations are carried out across a range of spatial scales, from global to landscape. Cells are usually arranged as lattice points; the distance between adjacent lattice points may be as coarse as a few degrees of latitude or longitude, or as fine as 30 arc-seconds. Simulations of the conterminous United States in the first DGVM comparison exercise (LPJ and MC1) called the VEMAP project in the 1990s used a lattice grain of one-half degree. Global simulations by the PIK group and collaborators using 6 different DGVMs (HYBRID, IBIS, LPJ, SDGVM, TRIFFID, and VECODE) used the same resolution as the general circulation model (GCM) that provided the climate data, 3.75 deg longitude x 2.5 deg latitude, a total of 1631 land grid cells. Sometimes lattice distances are specified in kilometers rather than angular measure, especially for finer grains, so a project like VEMAP is often referred to as 50 km grain.\n\nSeveral DGVMs appeared in the middle 1990s. The first was apparently IBIS (Foley et al., 1996), VECODE (Brovkin et al., 1997), followed by several others described below:\n\nSeveral DGVMs have been developed by various research groups around the world:\n\n\n\nThe next generation of models – earth system models (ex. CCSM, ORCHIDEE, JULES, CTEM ) – now includes the important feedbacks from the biosphere to the atmosphere so that vegetation shifts and changes in the carbon and hydrological cycles affect the climate.\n\nDGVMs commonly simulate a variety of plant and soil physiological processes. The processes simulated by various DGVMs are summarized in the table below. \nAbbreviations are: NPP, net primary production; PFT, plant functional type; SAW, soil available water; LAI, leaf area index; I, solar radiation; T, air temperature; Wr, root zone water supply; PET, potential evapotranspiration; vegc, total live vegetation carbon.\nReferences:\n"}
{"id": "15718387", "url": "https://en.wikipedia.org/wiki?curid=15718387", "title": "Eduard Honrath", "text": "Eduard Honrath\n\nEduard Gustav Honrath (11 August 1837, Coblenz – 19 April 1893, Berlin) was a German entomologist who specialised in Lepidoptera, particularly \"Parnassius\".\nHonrath was a well-known art dealer in Berlin. Among his entomological achievements, he described \"Parnassius graeseri\" (1885) (now \"Parnassius bremeri graeseri\" (a subspecies), \"Parnassius stenosemus\" and \"Papilio neumoegeni\" (both 1890) in the \"Berliner Entomologische Zeitschrift\". He was a member of the Entomological Society of Berlin, and its president for many years.\n\n\n"}
{"id": "57096460", "url": "https://en.wikipedia.org/wiki?curid=57096460", "title": "Elsevier Foundation Award", "text": "Elsevier Foundation Award\n\nThe Elsevier Foundation Award is awarded annually to young women scientists in the developing countries of Africa, the Middle East, Asia, Latin America and the Caribbean. In collaboration with the World Academy of Sciences and the Organization for Women in Science for the Developing World, Elsevier has celebrated evolving women scientists since the award was launched in 2012 as Elsevier Foundation Awards for Early Career Women Scientists in the Developing World. It is open to female scientists who live and work in one of the 81 countries with low scientific output. Nominations should be submitted within ten years of their earning a PhD.\n\nAnnounced by Elsevier, winners have included:\n\n\n\n\n\n\n"}
{"id": "3674853", "url": "https://en.wikipedia.org/wiki?curid=3674853", "title": "Ethnomycology", "text": "Ethnomycology\n\nEthnomycology is the study of the historical uses and sociological impact of fungi and can be considered a subfield of ethnobotany or ethnobiology. Although in theory the term includes fungi used for such purposes as tinder, medicine (medicinal mushrooms) and food (including yeast), it is often used in the context of the study of psychoactive mushrooms such as psilocybin mushrooms, the \"Amanita muscaria\" mushroom, and the ergot fungus.\n\nAmerican banker Robert Gordon Wasson pioneered interest in this field of study in the late 1950s, when he and his wife became the first Westerners on record allowed to participate in a mushroom \"velada\", held by the Mazatec \"curandera\" María Sabina. The biologist Richard Evans Schultes is also considered an ethnomycological pioneer. Later researchers in the field include Terence McKenna, Albert Hofmann, Ralph Metzner, Carl Ruck, Blaise Daniel Staples, Giorgio Samorini, Keewaydinoquay Peschel, John Marco Allegro, Clark Heinrich, Jonathan Ott, and Paul Stamets.\n\nBesides mycological determination in the field, ethnomycology depends to a large extent on anthropology and philology. One of the major debates among ethnomycologists is Wasson's theory that the Soma mentioned in the Rigveda of the Indo-Aryans was the \"Amanita muscaria\" mushroom. Following his example similar attempts have been made to identify psychoactive mushroom usage in many other (mostly) ancient cultures, with varying degrees of credibility. Another much written about topic is the content of the Kykeon, the sacrament used during the Eleusinian mysteries in ancient Greece between approximately 1500 BCE and 396 CE. Although not an ethnomycologist as such, philologist John Allegro has made an important contribution suggesting, in a book controversial enough to have his academic career destroyed, that \"Amanita muscaria\" was not only consumed as a sacrament but was the main focus of worship in the more esoteric sects of Sumerian religion, Judaism and early Christianity. Clark Heinrich claims that \"Amanita muscaria\" use in Europe was not completely wiped out by Orthodox Christianity but continued to be used (either consumed or merely symbolically) by individuals and small groups such as medieval Holy Grail myth makers, alchemists and Renaissance artists.\n\nWhile Wasson views historical mushroom use primarily as a facilitator for the shamanic or spiritual experiences core to these rites and traditions, McKenna takes this further, positing that the ingestion of psilocybin was perhaps primary in the formation of language and culture and identifying psychedelic mushrooms as the original \"Tree of Knowledge\". There is indeed some research supporting the theory that psilocybin ingestion temporarily increases neurochemical activity in the language centers of the brain, indicating a need for more research into the uses of psychoactive plants and fungi in human history.\n\nThe 1990s saw a surge in the recreational use of psilocybin mushrooms due to a combination of a psychedelic revival in the rave culture, improved and simplified cultivation techniques, and the distribution of both the mushrooms themselves and information about them via the Internet. This \"mushrooming of mushroom use\" has also caused an increased popularization of ethnomycology itself as there are many websites and Internet forums where mushroom references in Christmas and fairy tale symbolism are discussed. It remains open to interpretation what effect this popularization has on ethnomycology in the academic world, where the lack of verifiable evidence has kept its theories with their often far-reaching implications shrouded in controversy.\n\n\n"}
{"id": "981153", "url": "https://en.wikipedia.org/wiki?curid=981153", "title": "Exergonic process", "text": "Exergonic process\n\nAn exergonic process is one in which there is a positive flow of energy from the system to the surroundings. This is in contrast with an endergonic process. Constant pressure, constant temperature reactions are exergonic if and only if the Gibbs free energy change is negative (∆\"G\" < 0). \"Exergonic\" (from the prefix exo-, derived for the Greek word ἔξω \"exō\", \"outside\" and the suffix -ergonic, derived from the Greek word ἔργον \"ergon\", \"work\") means \"releasing energy in the form of work\". In thermodynamics, work is defined as the energy moving from the system (the internal region) to the surroundings (the external region) during a given process.\n\nAll physical and chemical systems in the universe follow the second law of thermodynamics and proceed in a downhill, i.e., \"exergonic\", direction. Thus, left to itself, any physical or chemical system will proceed, according to the second law of thermodynamics, in a direction that tends to lower the free energy of the system, and thus to expend energy in the form of work. These reactions occur spontaneously.\n\nA chemical reaction is also exergonic when spontaneous. Thus in this type of reactions the Gibbs free energy decreases. The entropy is included in any change of the Gibbs free energy. This differs from a exothermic reaction or a endothermic reaction where the entropy is not included. The Gibbs free energy is calculated with the Gibbs–Helmholtz equation:\n\nwhere:\n\nA chemical reaction progresses only spontaneously when the Gibbs free energy decreases, in that case the Δ\"G\" is negative. In exergonic reactions the Δ\"G\" is negative and in endergonic reactions the Δ\"G\" is positive:\n\nwhere: \n\n"}
{"id": "45254767", "url": "https://en.wikipedia.org/wiki?curid=45254767", "title": "Expression Atlas", "text": "Expression Atlas\n\nThe Expression Atlas is a database that provides information on gene expression patterns. The Expression Atlas allows searches by gene, splice variant and protein attribute. Individual genes or gene sets can be searched for. There are two components to the Expression Atlas, the Baseline Atlas and the Differential Atlas:\n\nThe Baseline Atlas provides information about which gene products are present (and at what abundance) under \"normal\" conditions. This component of the Expression Atlas consists of highly curated and quality-checked RNA-seq experiments from ArrayExpress. It aims to answer questions such as:\n\n\nThe Differential Atlas allows users to identify genes that are up- or down-regulated in different experimental conditions.\n\n\n\n"}
{"id": "7206612", "url": "https://en.wikipedia.org/wiki?curid=7206612", "title": "GRAPES-3", "text": "GRAPES-3\n\nThe GRAPES-3 experiment (or Gamma Ray Astronomy PeV EnergieS phase-3) located at Ooty in India started as a collaboration of the Indian Tata Institute of Fundamental Research and the Japanese Osaka City University, and now also includes the Japanese Nagoya Women's University.\n\nGRAPES-3 is designed to study cosmic rays with an array of air shower detectors and a large area muon detector. It aims to probe acceleration of cosmic rays in the following four astrophysical settings. These include acceleration of particles to, (i) ~100 MeV in atmospheric electric fields through muons, (ii) ~10 GeV in the Solar System through muons, (iii) ~1 PeV in our galaxy, (iv) ~100 EeV in the nearby universe through measurement of diffuse gamma ray flux.\n\nThe GRAPES-3 is located at N11.4, E76.7, 2200m above mean sea level. The observations began with 217 plastic scintillators and a 560 m area muon detector in 2000. The scintillators detect charged particles contained in extensive air showers produced by interaction of high energy cosmic rays in the atmosphere. At present the array is operating with ~400 scintillators that are spread over an area of 25,000 m. The energy threshold of muon detectors is 1 GeV.\n\nStudy of\n\nThe first cosmic ray experiment was started in 1955 by B. V. Sreekantan by setting up cloud chambers that heralded the beginning of research at the Cosmic Ray Laboratory (CRL) in Ooty. The next decade witnessed a variety of experiments involving high energy interactions and extensive air shower studies in this laboratory. The world's largest multiplate cloud chamber was operated here as part of an air shower array and significant results on the high energy nuclear interactions and cores of extensive air showers were obtained. A triple set-up comprising an air Cerenvok counter, a multiplate cloud chamber and a total absorption spectrometer was operated in the early seventies to study the differences in the characteristics of interactions with nuclei of protons and pions in the energy range 10-40 GeV. This enabled the time structure study of nuclear active components of air showers and led to the discovery that the nucleon-anti-nucleon production cross-section considerably increases with energy.\n\nIn continuation of the work on cosmic ray research at CRL, GRAPES-1 experiment was upgraded in various stages to GRAPES-2. However, due to the technical and administrative problem in its further expansion, a new experiment was set up at the RAC site 8 km from the old site which is called GRAPES-3. The GRAPES-3 experiment at present is operating with ~400 (each 1 m) plastic scintillator detectors with a separation of 8 meters, to record the density and arrival time of particles in cosmic ray showers, and in continuous operation. At present, GRAPES-3 array is the highest density conventional EAS array in the world, and also, this experiment associated with a huge 560 m area tracking muon detector, is also the largest area tracking detector anywhere.\n\nSeveral results have recently been obtained from the GRAPES-3 experiment on a variety of topics, a few of which are listed below.\n\n\n"}
{"id": "35326347", "url": "https://en.wikipedia.org/wiki?curid=35326347", "title": "Google Glass", "text": "Google Glass\n\nGoogle Glass is a brand of smart glasses—an optical head-mounted display designed in the shape of a pair of eyeglasses. It was developed by X (previously Google X) with the mission of producing a ubiquitous computer. Google Glass displayed information in a smartphone-like, hands-free format. Wearers communicated with the Internet via natural language voice commands. \n\nGoogle started selling a prototype of Google Glass to qualified \"Glass Explorers\" in the US on April 15, 2013, for a limited period for $1,500, before it became available to the public on May 15, 2014. It had an integral 5 megapixel still/720p video camera. The headset received a great deal of criticism and legislative action due to privacy and safety concerns. \n\nOn January 15, 2015, Google announced that it would stop producing the Google Glass prototype, to be continued in 2017 tentatively. In July 2017, it was announced that the Google Glass Enterprise Edition would be released.\n\nGoogle Glass was developed by Google X, the facility within Google devoted to technological advancements such as driverless cars.\n\nThe Google Glass prototype resembled standard eyeglasses with the lens replaced by a head-up display. In mid-2011, Google engineered a prototype that weighed ; by 2013 they were lighter than the average pair of sunglasses.\n\nIn April 2013, the Explorer Edition was made available to Google I/O developers in the United States for $1,500.\nThe product was publicly announced in April 2012. Sergey Brin wore a prototype of the Glass to an April 5, 2012, Foundation Fighting Blindness event in San Francisco. In May 2012, Google demonstrated for the first time how Google Glass could be used to shoot videos.\n\nGoogle provided four prescription frame choices for $225 and free with the purchase of any new Glass unit. Google entered in a partnership with the Italian eyewear company Luxottica, owners of the Ray-Ban, Oakley, and other brands, to offer additional frame designs.\nIn June 2014, Nepal government adopted Google Glass for tackling poachers of wild animals and herbs of Chitwan International Park and other parks listed under World heritage sites. \nIn January 2015, Google ended the beta period of Glass (the \"Google Glass Explorer\" program).\n\nIn early 2013, interested potential Glass users were invited to use a Twitter message, with hashtag #IfIHadGlass, to qualify as an early user of the product. The qualifiers, dubbed \"Glass Explorers\" and numbering 8,000 individuals, were notified in March 2013, and were later invited to pay $1,500 and visit a Google office in Los Angeles, New York or San Francisco, to pick up their unit following \"fitting\" and training from Google Glass guides. On May 13, 2014, Google announced a move to a \"more open beta\", via its Google Plus page.\n\nIn February 2015, \"The New York Times\" reported that Google Glass was being redesigned by former Apple executive Tony Fadell, and that it would not be released until he deemed it to be \"perfect\".\n\nIn July 2017 it was announced that the second iteration, the Google Glass Enterprise Edition, would be released in the US for companies such as Boeing.. Google Glass Enterprise Edition has already been successfully used by Dr. Ned Sahin to help children with autism learn social skills.\n\n\nGoogle Glass applications are free applications built by third-party developers. Glass also uses many existing Google applications, such as Google Now, Google Maps, Google+, and Gmail. Many developers and companies have built applications for Glass, including news apps, facial recognition, exercise, photo manipulation, translation, and sharing to social networks, such as Facebook and Twitter. Third-party applications announced at South by Southwest (SXSW) include Evernote, Skitch, \"The New York Times\", and Path.\n\nOn March 23, 2013, Google released the Mirror API, allowing developers to start making apps for Glass. In the terms of service, it was stated that developers may not put ads in their apps or charge fees; a Google representative told The Verge that this might change in the future.\n\nOn May 16, 2013, Google announced the release of seven new programs, including reminders from Evernote, fashion news from \"Elle\", and news alerts from CNN. Following Google's XE7 Glass Explorer Edition update in early July 2013, evidence of a \"Glass Boutique\", a store that will allow synchronization to Glass of Glassware and APKs, was noted.\n\nVersion XE8 made a debut for Google Glass on August 12, 2013. It brings an integrated video player with playback controls, the ability to post an update to Path, and lets users save notes to Evernote. Several other minute improvements include volume controls, improved voice recognition, and several new Google Now cards.\n\nOn November 19, 2013, Google unveiled its Glass Development Kit, showcasing a translation tool Word Lens, a cooking program AllTheCooks, and an exercise program Strava among others as successful examples. Google announced three news programs in May 2014 – TripIt, FourSquare and OpenTable – in order to entice travelers. On June 25, 2014, Google announced that notifications from Android Wear would be sent to Glass.\n\nThe European University Press published the first book to be read with Google Glass on October 8, 2014, as introduced at the Frankfurt Book Fair. The book can be read as a normal paper book or – enriched with multimedia elements – with Google Glass, Kindle, on Smartphone and Pads on the platforms iOS and Android.\n\nGoogle offers a companion Android and iOS app called MyGlass, which allows the user to configure and manage the device.\n\nOther than the touchpad, Google Glass can be controlled using just \"voice actions\". To activate Glass, wearers tilt their heads 30° upward (which can be altered for preference) or simply tap the touchpad, and say \"O.K., Glass.\" Once Glass is activated, wearers can say an action, such as \"Take a picture\", \"Record a video\", \"Hangout with [person/Google+ circle]\", \"Google 'What year was Wikipedia founded?'\", \"Give me directions to the Eiffel Tower\", and \"Send a message to John\" (many of these commands can be seen in a product video released in February 2013). For search results that are read back to the user, the voice response is relayed using bone conduction through a transducer that sits beside the ear, thereby rendering the sound almost inaudible to other people.\n\nAugmedix developed an app for the wearable device that allows physicians to live-stream the patient visit and claims it will eliminate electronic health record problems, possibly saving them up to 15 hours a week and improving record quality. The video stream is passed to remote scribes in HIPAA secure rooms where the doctor-patient interaction is transcribed. Ultimately, allowing physicians to focus on the patient. Hundreds of users were evaluating the app as of mid-2015.\n\nIn July 2013, Lucien Engelen commenced research on the usability and impact of Google Glass in the health care field. As of August 2013, Engelen, based at Singularity University and in Europe at Radboud University Nijmegen Medical Centre, was the first healthcare professional in Europe to participate in the Glass Explorer program. His research on Google Glass (starting August 9, 2013) was conducted in operating rooms, ambulances, a trauma helicopter, general practice, and home care as well as the use in public transportation for visually or physically impaired. Research included taking pictures, videos streaming to other locations, dictating operative log, having students watch the procedures and tele-consultation through Hangout. Engelen documented his findings in blogs, videos, pictures, on Twitter, and on Google+, with research ongoing as of that date. \n\nIn June 2014, Google Glass' ability to acquire images of a patient's retina (\"Glass Fundoscopy\") was publicly demonstrated for the first time at the Wilmer Clinical Meeting at Johns Hopkins University School of Medicine by Dr. Aaron Wang and Dr. Allen Eghrari. This technique was featured on the cover of the Journal for Mobile Technology in Medicine for January 2015. Doctors Phil Haslam and Sebastian Mafeld demonstrated the first application of Google Glass in the field of interventional radiology. They demonstrated how Google Glass could assist a liver biopsy and fistulaplasty, and the pair stated that Google Glass has the potential to improve patient safety, operator comfort, and procedure efficiency in the field of interventional radiology.\n\nOn June 20, 2013, Rafael J. Grossmann, a Venezuelan doctor practicing in the U.S., was the first surgeon to demonstrate the use of Google Glass during a live surgical procedure. In August 2013, Google Glass was used at Wexner Medical Center at Ohio State University. Surgeon Dr. Christopher Kaeding used Google Glass to consult with a distant colleague in Columbus, Ohio. A group of students at The Ohio State University College of Medicine also observed the operation on their laptop computers. Following the procedure, Kaeding stated, \"To be honest, once we got into the surgery, I often forgot the device was there. It just seemed very intuitive and fit seamlessly.\" \n\nOn June 21, 2013, Spanish doctor Pedro Guillen, chief of trauma service of Clínica CEMTRO of Madrid, also broadcast a surgery using Google Glass. In July 2014, the startup company Surgery Academy, in Milan, Italy, launched a remote training platform for medical students. The platform is a MOOC that allows students to join any operating theater thanks to Google Glass worn by surgeon. Also in July 2014, This Place released an app, MindRDR, to connect Glass to a Neurosky EEG monitor to allow people to take photos and share them to Twitter or Facebook using brain signals. It is hoped this will allow people with severe physical disabilities to engage with social media.\nIn Australia, during January 2014, Melbourne tech startup Small World Social collaborated with the Australian Breastfeeding Association to create the first hands-free breastfeeding Google Glass application for new mothers. The application, named Google Glass Breastfeeding app trial, allows mothers to nurse their baby while viewing instructions about common breastfeeding issues (latching on, posture etc.) or call a lactation consultant via a secure Google Hangout, who can view the issue through the mother's Google Glass camera. The trial was successfully concluded in Melbourne in April 2014, with 100% of participants breastfeeding confidently.\n\nSeveral groups began developing Google Glass based technologies to help children with autism learn about emotion and facial expressions. The first of these was developed by Brain Power who published the first academic paper on the use of Google Glass technology in children with autism. Brain Power launched a clinical trial in 2017 known as the \"Be Yourself.\" Brain Power was founded by Dr. Ned T. Sahin. Dr. Ned T. Sahin based the entire clinical trial upon the concept that every child or adult with autism is different. In order to gather this research he travelled into neighborhoods and visited schools. There, he was able to develop a more visual understanding of just a few of the daily challenges children with autism face. Dr. Ned T. Sahin states that he’s \"not anticipating that children will use the solution 24/7. Rather, just like a parent and child may read together for an hour, they’ll spend an hour a day interacting with each other while the child wears customized Google Glass\".\n\nThe basis of Brain Power is to create a classroom that would be wearable, using the Empower Me software. The trial is based on self sufficiency and focuses on \"skills including emotion decoding, eye contact, speech, conversation skills, behavior and self-regulation, avoiding stress and meltdowns\". A category formation tool on this device, which rewards autistic individuals for properly labeling images that are shown to them. These could include images such as an apple or various animals. Another tool for these images could be a consistent image that represents a calm or soothing emotion, while could result in stress levels decreasing. Many families have had positive experiences with this technology, but has not proven to be a permanent solution for any trial member.\n\nRecently Stanford Research has developed the Google Glass Project there is a team working on finding a way to utilize Google Glass as a tool for behavioral therapy for individuals with autism. This research has been controversial given a lack of reproducibility and transparency . As team manager Dennis Wall stated,\"We have developed a system using machine learning and artificial intelligence to automate facial expression recognition that runs on wearable glasses and delivers real-time social cues.\" An outward-facing camera visually displays facial expressions and developed a system to calculate eye contact. Catalin Voss, founder of the Autism Glass Project stated how flashcards are utilized,\"But that doesn't always translate to real-life situations,\" he said. \"Our idea was to try to build a more holistic aid that enables the user to recognize social cues when they actually need to receive those cues right then and there.\" \nCurrently the trial is simply testing the facial expression identification theory. Two participants have publicly identified in the media where they have declared some positive and encouraging experiences. One of the participants, an American teenage girl with autism, said, “It’s helped me to understand some people’s emotions. I can tell when a friend is upset better now than I could before. \n\nAs of February 2018, Google Glass released the public statement regarding the newly designed timeline of the program. This statement declared \"over the course of four months, the kids and their families will participate in the therapy. During three 20-minute sessions per day, anything a child sees while wearing Glass is recorded and saved onto a smartphone app developed by the lab. Kids and parents can then review the footage together, and the parents can point out the emotions they were feeling at specific moments. As this occurs, corresponding color-coded bars at the bottom of the screen are linked to those feelings. A red bar at the bottom of the screen means someone is upset, for example, and a yellow bar indicates they are happy … These color-coded videos help kids remember what emotions they saw and in what context.\" Overall, this statement highlights that the Google Glass journey is very family oriented, which in most cases leads to the best results. Google glass is proving to be a powerful tool for social interaction improvements due to the strong technology and versatile presentation either smartphones or as glasses.\n\nIn 2014, Voice of America Television Correspondent Carolyn Presutti and VOA Electronics Engineer Jose Vega began a web project called \"VOA & Google Glass,\" which explored the technology's potential uses in journalism. This series of news stories examined the technology's live reporting applications, including conducting interviews and covering stories from the reporter's point of view. On March 29, 2014, American a cappella group Pentatonix partnered with Voice of America when lead singer Scott Hoying wore Glass in the band's performance at DAR Constitution Hall in Washington, D.C., during the band's worldwide tour – the first use of Glass by a lead singer in a professional concert.\n\nIn the fall of 2014, The University of Southern California conducted a course called \"Glass Journalism,\" which explored the device's application in journalism.\n\nThe WWF as of mid-2014 used Google Glass and UAVs to track various animals and birds in the jungle, which may be the first use of the device by a non-profit, Non-governmental Organization (NGO).\n\nIn 2014, the International Olympic Committee Young Reporters program took Google Glass to the Nanjing 2014 Youth Olympic Games and put them on a number of athletes from different disciplines to explore novel point of view filmmaking.\n\nA visually impaired dancer, Benjamin Yonattan, used Google Glass to overcome his chronic vision condition. In 2015, Yonattan performed on the reality television program \"America's Got Talent\".\n\nConcerns have been raised by various sources regarding the intrusion of privacy, and the etiquette and ethics of using the device in public and recording people without their permission. Google co-founder, Sergey Brin, claims that Glass could be seen as a way to become even more isolated in public, but the intent was quite the opposite: Brin views checking social media as a constant \"nervous tic,\" which is why Glass can notify the user of important notifications and updates and does not obstruct the line of sight.\n\nAdditionally, there is controversy that Google Glass would cause security problems and violate privacy rights. Organizations like the FTC Fair Information Practice work to uphold privacy rights through Fair Information Practice Principles (FIPPS), which are guidelines representing concepts that concern fair information practice in an electronic marketplace.\n\nPrivacy advocates are concerned that people wearing such eyewear may be able to identify strangers in public using facial recognition, or surreptitiously record and broadcast private conversations. The \"Find my Face\" feature on Google+ functions to create a model of your face, and of people you know, in order to simplify tagging photos. However, the only current app that can identify strangers is called MORIS (Mobile Offender Recognition and Identification System), and is a $3,000 iPhone app used by police officers.\n\nSome companies in the US have posted anti-Google Glass signs in their establishments. In July 2013, prior to the official release of the product, Stephen Balaban, co-founder of software company Lambda Labs, circumvented Google’s facial recognition app block by building his own, non-Google-approved operating system. Balaban then installed face-scanning Glassware that creates a summary of commonalities shared by the scanned person and the Glass wearer, such as mutual friends and interests. Also created was Winky, a program that allows a Google Glass user to take a photo with a wink of an eye, while Marc Rogers, a principal security researcher at Lookout, discovered that Glass can be hijacked if a user could be tricked into taking a picture of a malicious QR code, demonstrating the potential to be used as a weapon in cyberwarfare.\n\nIn February 2013, a Google+ user noticed legal issues with Glass and posted in the Glass Explorers community about the issues, stating that the device may be illegal to use according to the current legislation in Russia and Ukraine, which prohibits use of spy gadgets that can record video, audio or take photographs in an inconspicuous manner.\n\nConcerns were also raised in regard to the privacy and security of Glass users in the event that the device is stolen or lost, an issue that was raised by a US congressional committee. As part of its response to the committee, Google stated that a locking system for the device is in development. Google also reminded users that Glass can be remotely reset. Police in various states have also warned Glass wearers to watch out for muggers and street robbers.\n\nLisa A. Goldstein, a freelance journalist who was born deaf, tested the product on behalf of people with disabilities and published a review on August 6, 2013. In her review, Goldstein states that Google Glass does not accommodate hearing aids and is not suitable for people who cannot understand speech. Goldstein also explained the limited options for customer support, as telephone contact was her only means of communication.\n\nSeveral facilities have banned the use of Google Glass before its release to the general public, citing concerns over potential privacy-violating capabilities. Other facilities, such as Las Vegas casinos, banned Google Glass, citing their desire to comply with Nevada state law and common gaming regulations which ban the use of recording devices near gambling areas. On October 29, 2014, the Motion Picture Association of America (MPAA) and the National Association of Theatre Owners (NATO) announced a ban on wearable technology including Google Glass, placing it under the same rules as mobile phones and video cameras.\n\nThere have also been concerns over potential eye pain caused by users new to Glass. These concerns were validated by Google's optometry advisor Dr. Eli Peli of Harvard, though he later partly backtracked due to the controversy which ensued from his remarks.\n\nConcerns have been raised by cyber forensics experts at the University of Massachusetts who have developed a way to steal smartphone and tablet passwords using Google Glass. The specialists developed a software program that uses Google Glass to track finger shadows as someone types in their password. Their program then converts the touchpoints into the keys they were touching, allowing them to catch the passcodes.\n\nAnother concern regarding the camera application raises controversy to privacy. Some people are concerned about how the product has the capability of recording during events such as conversations. The device sets off a light to indicate that it is recording but many speculate that there will be an app to disable this.\n\nConcerns have also been raised on operating motor vehicles while wearing the device. On July 31, 2013 it was reported that driving while wearing Google Glass was likely to be banned in the UK, being deemed careless driving, therefore a fixed penalty offense, following a decision by the Department for Transport.\n\nIn the US, West Virginia state representative Gary G. Howell introduced an amendment in March 2013 to the state's law against texting while driving that would include bans against \"using a wearable computer with head mounted display.\" In an interview, Howell stated, \"The primary thing is a safety concern, it [the glass headset] could project text or video into your field of vision. I think there's a lot of potential for distraction.\"\n\nIn October 2013, a driver in California was ticketed for \"driving with monitor visible to driver (Google Glass)\" after being pulled over for speeding by a San Diego Police Department officer. The driver was reportedly the first to be fined for driving while wearing a Google Glass. While the judge noted that \"Google Glass fell under 'the purview and intent' of the ban on driving with a monitor\", the case was thrown out of court due to lack of proof the device was on at the time.\n\nIn November 2014, Sawyer et al., from the University of Central Florida and the US Air Force Research Laboratory, published the results of comparative study in a driving simulator. Subjects were asked to use either Google Glass or a smartphone-based messaging interface and were then interrupted with an emergency event. The Glass-delivered messages served to moderate but did not eliminate distracting cognitive demands. A potential passive cost to drivers merely wearing the Glass was also observed. Messaging using either device impaired driving as compared to driving without multi-tasking.\n\nIn February 2014, a woman wearing Google Glass claimed she was verbally and physically assaulted at a bar in San Francisco after a patron confronted her while she was showing off the device, allegedly leading a man accompanying her to physically retaliate. Witnesses suggested that patrons were upset over the possibility of being recorded.\n\nUnder the Google Glass terms of service for the Glass Explorer pre-public release program, it specifically states, \"You may not resell, loan, transfer, or give your device to any other person. If you resell, loan, transfer, or give your device to any other person without Google's authorization, Google reserves the right to deactivate the device, and neither you nor the unauthorized person using the device will be entitled to any refund, product support, or product warranty.\"\n\"Wired\" commented on this policy of a company claiming ownership of its product after it had been sold, saying: \"Welcome to the New World, one in which companies are retaining control of their products even after consumers purchase them.\" Others pointed out that Glass was not for public sale at all, but rather in private testing for selected developers, and that not allowing developers in a closed beta to sell to the public is not the same as banning consumers from reselling a publicly released device.\n\nFor the developer Explorer units version 1:\nFor the developer Explorer units version 2, RAM was expanded to 2 GB and prescription frames were made available:\nThe new Google Glass Enterprise Edition improves upon previous editions with the following\n\n\n\n"}
{"id": "42956713", "url": "https://en.wikipedia.org/wiki?curid=42956713", "title": "Grapevine virus A", "text": "Grapevine virus A\n\nGrape vine virus A (GVA) is a plant virus and the type species in the genus \"Vitivirus\".\n\n\n"}
{"id": "24374745", "url": "https://en.wikipedia.org/wiki?curid=24374745", "title": "Hakuto", "text": "Hakuto\n\nIn 2008, \"White Label Space\" was founded in the Netherlands. In 2009, the team registered with the Google Lunar X PRIZE. On September 10, 2010 the team also established White Label Space Japan LLC, a Limited Liability Company registered in Japan. On June 11, 2012, it was decided to name unit 2 Rover prototype (PM-2) \"White Rabbit\" (「はくと」). On January 30, 2013, When the European teammates dropped out, the Japan-based members continued the work and changed their name to team Hakuto, and re-named their parent company ispace Inc. The change included a change of leadership from Steve Allen to co-founder Takeshi Hakamada, who has been leading operations in Japan. On July 15, 2013 the team officially changed the team name to Hakuto (「ハクト」). On December 4, 2013, the team successfully crowd-funded the development of unit 3 Rover prototype (PM-3). On February 19, 2014, Hakuto was nominated to the Milestone Prize, as one of five teams, in the mobility subsystem section.\n\nThe team is based in Japan and is led by Takeshi Hakamada. The lead engineer is Kazuya Yoshida, a professor of aerospace engineering at Tohoku University in Japan. Hakuto is operated by ispace Inc. and is supported by the Space Robotics Lab of Tohoku University.\n\n, the team had nine official partners:\n\nIn December, 2016 Hakuto decided to share some costs with Team Indus and launch their two rovers together on the proven PSLV launcher of ISRO. The launch was initially scheduled for 28 December 2017, and then delayed to March 2018. The media reported in 9 January 2018 that ISRO cancelled the launch contract with TeamIndus and Hakuto.\n\nOn January 23, 2018, X Prize founder and chairman Peter Diamandis stated \"After close consultation with our five finalist Google Lunar X Prize teams over the past several months, we have concluded that no team will make a launch attempt to reach the moon by the March 31, 2018, deadline... and the US$30 million Google Lunar XPRIZE will go unclaimed.\"\n\nIn April 2018, it was reported that launch is now planned for 2020 on the \"Peregrine\" lander by Astrobotic, to be launched on an Atlas V rocket. \"Sorato\" rover will be deployed on the lunar surface along with other smaller rovers, including Chile's Team AngelicvM's rover, and a set of mini-rovers from the Mexican Space Agency. The selected landing site is Lacus Mortis.\n\n\n"}
{"id": "12095821", "url": "https://en.wikipedia.org/wiki?curid=12095821", "title": "Igor Akimushkin", "text": "Igor Akimushkin\n\nIgor Ivanovich Akimushkin () (May 1, 1929 – 1993) was a Soviet zoologist and writer.\n\nBorn in Moscow, he graduated the biological faculty of Moscow State University in 1952.\nHis first books, \"Tracks of beast you never met\" and \"Following the Legends\", were published in 1961.\n\nIgor Akimushkin wrote a large number of popular science books and made a significant contribution to science and made some discoveries by exploring the marine life. Most of his works were translated to other languages. The squid \"Cycloteuthis akimushkini\" was named in honour of Igor Akimushkin in 1968 by fellow zoologist Filippova.\n\nHis most well known work is the six volume \"World of Animals\".\n\nHe worked at the Shirshov Institute of Oceanology, Russian Academy of Sciences.\n\n"}
{"id": "28059473", "url": "https://en.wikipedia.org/wiki?curid=28059473", "title": "Invasive species in the United States", "text": "Invasive species in the United States\n\nInvasive species are a significant threat to many native habitats and species of the United States and a significant cost to agriculture, forestry, and recreation. The term \"invasive species\" can refer to introduced or naturalized species, feral species, or introduced diseases. There are many species that are invasive. Some species, such as the dandelion, while non-native, do not cause significant economic or ecologic damage and are not widely considered as invasive. Overall, it is estimated that 50,000 non-native species have been introduced to the United States, including livestock, crops, pets, and other non-invasive species. Economic damages associated with invasive species' effects and control costs are estimated at $120 billion per year.\n\nFor a more complete list of invasive species, see List of invasive species in North America\n\nThe economic impacts of invasive species can be difficult to estimate, especially when an invasive species does not affect economically important native species. This is due in part to the difficulty in determining the non-use value of native habitats damaged by invasive species, and in part to incomplete knowledge of the effects of all of the invasive species present in the U.S. Estimates for the damages caused by well-known species can vary as well. The Office of Technology Assessment (OTA) has estimated zebra mussel economic effects at $300,000 a year, while an ACoE study put the number at $1 billion. The United States government spends an estimated $1 billion to recover from the invasive Formosan termite, investing $300 million of this budget is spent in areas surround New Orleans, a major port city. Estimates of total yearly costs due to invasive species range from $1.1 billion per year to $137 billion per year.\n\nIn 1993, the OTA estimated that a total of $100 million is invested annually in invasive species aquatic weed control in the US. Introduced rats cause more than $19 billion per year in damages, exotic fish cause up to $5.4 billion annually, and the total costs of introduced weeds are estimated at around $27 billion annually. The total damage to the U.S. native bird population due to invasive species is approximately $17 billion per year. Approximately $2.1 billion in forest products are lost each year to invasive plant pathogens in the United States, and a conservative estimate of the losses to U.S. livestock from exotic microbes and parasites was $9 billion per year in 2001.\n\nThe federal government has historically promoted the introduction and widespread distribution of species that would become invasive, including multiflora rose, kudzu, and others for numerous reasons. Before the 20th century, numerous species were imported and released without government oversight, such as the gypsy moth and house sparrow. Over 50% of flora recognized as invasive or noxious weeds were deliberately introduced to the United States, by either government policy or individuals. Current government policy can be broadly separated into two categories: preventing entry of a potential invasive species and controlling the spread of species already present. This is carried out by different government agencies, depending on what types of damage a species can cause. \n\nThe Lacey Act of 1900, originally designed to protect game wildlife, its role has increased to prohibit parties from bringing non-native species that have the potential to become invasive into the United States. The Lacey Act gives the FWS the power to list a species as \"injurious\" and regulate or prohibit its entry into the U.S. The Alien Species Prevention and Enforcement Act of 1992 makes it illegal to transport a plant or animal deemed injurious into the United States through the mail. The FWS concerns itself mostly with the invasive species likely to threaten sensitive habitats or endangered species.\n\nThe USDA is also involved in preventing the introduction of invasive species, largely through the Animal and Plant Health Inspection Service, or APHIS. APHIS was originally tasked with preventing damage to agriculture and forestry from alien species, pests, or diseases, but has had its mission expanded to include preventing invasive species spread as well. This includes identifying potential pests and diseases, assisting in international and domestic eradication efforts, and the Smuggling Interdiction and Trade Compliance Program, designed initially to deal with illegally imported produce, but now tasked with preventing the entry of exotic pests, diseases, and potentially invasive species. APHIS also enforces bans against interstate transport of pests, diseases, and species listed as injurious, noxious weeds, or nuisance species. An example of the USDA banning imports is the ban on fresh mangosteen fruit due to concerns about fruit flies from southeast Asia. This ban originally allowed only frozen or canned fruit, but now allows for fresh irradiated fruit to enter.\n\nMany invasive species are spread inadvertently by human activities, such as seeds stuck to clothing or mud transporting firewood, or through ballast water. The government has instituted several different policies related to different pathways the invasive species may be spread. For example, quarantines on a federal and state level exist for firewood across the Eastern United States in an attempt to halt the spread of the emerald ash borer, gypsy moth, oak wilt, and others. Transporting firewood out of quarantine zones can result in a fine of up to $1,000,000 and 25 years in jail, but punishments are usually much lower.\n\nThe techniques available for controlling the spread of invasive species can be broadly defined into 6 categories:\n\nAn integrated pest management (IPM) approach, as defined by the National Invasive Species Council, uses scientific data and population monitoring to help determine the most efficient control strategy, which is usually a combination of several of the methods listed above. Agencies are encouraged to use an adaptive management strategy, involving regular reviews on the efficiency of their policies and conduct research into better methods.\n\nInvasive species control is not overseen by one government agency. Eight agencies (divisions) of the United States Department of Agriculture (USDA) work on invasive species issues that include, Agricultural Research Service (ARS), Animal and Plant Health Inspection Service (APHIS), Cooperative State Research, Education, and Extension Service (CSREES), Economic Research Service (ERS), Farm Service Agency (FSA), Foreign Agricultural Service (FAS), United States Forest Service (FS), and the Natural Resources Conservation Service (NRCS). Different invasive species are controlled by different agencies. For example, policies aimed at controlling the emerald ash borer are undertaken by the USDA, because National Forests, the body coordinating emerald ash borer control efforts, are within the USDA. The National Invasive Species Council was created by executive order in 1999 and charged with promoting efficiency and coordination between the numerous federal invasive species prevention and control policies. The NISC is co-chaired by the secretaries of the three federal departments that are charged with invasive species control: Interior, Agriculture, and Commerce.\n\nMany of the policies used to contain invasive species, such as firewood transport bans or cleaning shoes and clothes after hiking are effective only when the general public knows of their existence and importance. Because of this numerous programs to inform the public about invasive species. This includes placing signs at boat ramps, campsites, state borders, hiking trails, and numerous other locations as reminders of policies and potential fines associated with breaking policies. There are also numerous government programs aimed at educating children, promoting volunteer efforts at removal, and the many ways citizens can prevent the spread of invasive species.\n\nCurrent efforts in the Great Lakes ecoregion focus on measures that prevent the introduction of invasive species. As a major transport area, a number of invasive species have already been established within the Great Lakes. In 1998, the United States Coast Guard, in accordance with the National Invasive Species Act of 1996, established a set of voluntary ballast water management program. In 2004 this voluntary program became mandatory for every ship entering US controlled waters. Current measures are among the most stringent in the world and require ships entering from outside the Exclusive Economic Zone to flush ballast water in open seas or retain their ballast water for the length of their stay in the Great Lakes. Failure to comply with the US Coast Guards regulations can result in a class C felony.\n\nAnother preventative measure in the Great Lakes region is the presence of an electrified barrier in the Chicago Sanitary and Ship Canal. The barrier is meant to keep Asian carp from reaching Lake Michigan and the other Great Lakes. On December 2, 2010, Michigan, Ohio, Pennsylvania and Wisconsin were denied their request to force the closing the Canal by Judge Robert Dow of the United States District Court for Northern District of Illinois. The closing of the Canal would have once again permanently separated Lake Michigan and the Mississippi river system. States argued that the canal, and the Asian carp in it, posed a risk to $7 billion worth of industry. Currently the electric barrier is the only preventative measure and some question its effectiveness, particularly following the discovery of Asian carp DNA past the barrier. The discovery of DNA of Asian carp could be linked to live bait used around the Great Lakes region. The method for identifying the DNA is called environmental DNA (eDNA) surveillance. This method uses DNA, that is left in the environment, to identify species in low abundances.\n\nThe USDA Rocky Mountain Research Station (RMRS) has a specific Invasive Species Working Group to do the research about invasive species in Rocky Mountain region. The Invasive Species Working Group focuses on four key areas: (1) prediction and prevention, (2) early detection and rapid response, (3) control and management, and (4) restoration and rehabilitation. Specific approaches include prioritizing of invasive species problems, increased collaboration among agencies regarding those problems, and accountability for the responsible use of the limited resources available for invasive control.\n\nInvasive species of particular concern in the Rocky Mountain region include: cheatgrass; leafy spurge; tansy ragwort; spotted knapweed; bufflegrass; saltcedar; white pine blister rust; armillaria root rot; introduced trout species; golden algae; spruce aphid; and banded elm bark beetle.\n\nAlready stressed by water management and damming, the Colorado River of Western United States is losing its big-river fish community to combined effects of predation and competition by introduced non-native fishes. This fish community includes four large fishes that are listed as endangered by the U.S. Fish and Wildlife Service. One of these, the Colorado pikeminnow (AKA white or Colorado river salmon, Ptychocheilus lucius) is the largest minnow native to North America, and it is well known for its spectacular fresh water spawning migrations and homing ability. Despite a massive recovery effort, its numbers decline. Hampered by a loss of about 80% of its habitat, the young of this once abundant fish is overwhelmed in its nursery habitat by invasive small fishes (such as red shiner and fathead minnow), whose numbers are as high as 90% of the standing stocks. Its juveniles and adults now must also compete with and are preyed upon by introduced northern pike, channel and flathead catfishes, large and smallmouth basses, common carp, and other fishes. However, the listing of these non native sport fish as invasive is controversial, as the fish are popular among anglers, who criticize the science used by government agencies and assert that nonnative species are largely a scapegoat in the decline of endemic Colorado River basin fish, blaming changes to the environment in the forms of dams and water diversions as being the main cause \n\nCalifornia has created a policy system towards invasive species, including Invasive Species Council of California (ISCC), California Invasive Species Advisory Committee (CISAC) and California Invasive Plant Council (Cal-IPC), a non-profit organization. The ISCC represents the highest level of leadership and authority in state government regarding invasive species. The ISCC is an inter-agency council that helps to coordinate and ensure complementary, cost-efficient, environmentally sound and effective state activities regarding invasive species. CISAC advises the ISCC, and created the California list of invasive species California has many diverse ecoregions, and numerous endemic species that are at risk from invasive species.\n\nInvasive species in Florida currently make up more than 26% of the animal population and a full one third of the flora population. In 1994, the Everglades Forever Act of 1994 was passed to help in controlling Florida's water supply, recreation areas, and diverse flora and fauna. In addition to control and prevention measures the act also calls for efforts to monitor the distribution of known invasive species.\n\nOne invasive species occurring in the Everglades that can have serious consequences is the Burmese python. Between 2000 and 2010, approximately 1,300 of the snakes were removed from the Everglades. Currently the National Park Service is researching control measures for the Burmese python in order to limit the species effects on the delicate Everglades ecosystem.\n\nIn 2015, the presence of the invasive land planarian \"Platydemus manokwari\" was recorded from several gardens in Miami. \"Platydemus manokwari\" is a predator of land snails and is considered a danger to endemic snails wherever it has been introduced.\n\nIn Hawaii measures to prevent the introduction and spread of invasive species are coordinated by the Hawaii Invasive Species Council. Currently the council is broken into five committees which focus on different areas of invasive species control. These focus areas are (1) prevention (2) management of established pests (3) increased public awareness (4) research and technology and (5) monetary resources.\n\nCurrently, Hawaii requires inspection of any and all plant, animal and microorganism transports. This includes transports from the mainland in addition to transports occurring between islands. Travelers are required to fill out a declaration form for each journey. Failure to declare these transports can result in up to one year imprisonment or a $250,000 fine. Many potential invasives or carriers for invasives require permits and quarantine periods before entry to the state is allowed.\n\nIn addition, there are other preventative measures such as a hotline for reporting sightings of known potential invaders like the brown tree snake.\n\nThe Idaho Department of Agriculture (U.S.) has around 300 introduced or exotic species listed with 36 classified as noxious weeds (invasive species). The legal designation of noxious weed for a plant species can use these four criteria:\n\n\nSome of the plants on Idaho's noxious weed list that are harmful or poisonous are:\nLouisiana\n\nThe city of New Orleans, the \"gateway to the Mississippi\", is a porous port city with rich soils. In turn, many aquatic plants are introduced to the region, making Louisiana the state with the second largest list of invasive aquatic species, second to Florida.\n\nThe \"Dirty Dozen\" details a list of the United States' most destructive invasive species. Of the twelve, four are identified in the state, including the zebra mussel, tamarisk, hydrilla, and Chinese tallow.\n\nInvasive species pose a threat to wildlife, habitat, waterways, economy, and the health of humans in New York State. The New York State Department of Environmental Conservation (NYSDEC) works with stewards of natural resources, non-profits and citizen scientists to detect, record and manage invasive species. These collaborations are organized into 8 Partnerships for Regional Invasive Species Management (PRISMs) throughout NYS. THE PRISMS were formed under Title 17, Environmental Conservation Law 9-1705(5)(g). \n\nAccording to NYSDEC PRISMS perform the following tasks: plan regional invasive species management;develop early detection and rapid response capacity;implement eradication projects; educate the public in cooperation with DEC contracted Education and Outreach providers; coordinate PRISM partners; recruit and train volunteers; support research through citizen science.\nTerrestrial species of high concern in New York include \n\nAquatic species of high concern in New York include\n\nInsects\n\n\n\n"}
{"id": "57559457", "url": "https://en.wikipedia.org/wiki?curid=57559457", "title": "Kevin Lynch Award", "text": "Kevin Lynch Award\n\nThe Kevin Lynch Award of the Massachusetts Institute of Technology's Department of Urban Studies and Planning, established in 1988, is named in honor of the urban planner and author Kevin A. Lynch. It is given to individuals or organizations which contribute to research in city form.\n\nStarting in 2014, the awards have been presented in five categories. Earlier awards have been retroactively assigned to these categories.\n\nPast winners:\n\n\n\n"}
{"id": "6828719", "url": "https://en.wikipedia.org/wiki?curid=6828719", "title": "Lee–Boot effect", "text": "Lee–Boot effect\n\nThe Lee–Boot effect is a phenomenon concerning the suppression or prolongation of oestrous cycles of mature female mice (and other rodents), when females are housed in groups and isolated from males. It is caused by the effects of an oestrogen-dependent pheromone, released via the urine, that acts on the vomeronasal organ of recipients. This pheromone lowers the concentration of luteinizing hormone and elevates prolactin levels, synchronising or stopping the recipient’s cycle. This effect goes some way to explain why spontaneous pseudopregnancy can occur in mice. The same response is invoked from isolated females when brought into contact with urine-soaked bedding from other females’ cages. Removing the vomeronasal organ of recipients causes an ineffective response – indicating that the cues are not mediated by the vomeronasal system.\n\n"}
{"id": "7296395", "url": "https://en.wikipedia.org/wiki?curid=7296395", "title": "Lens speed", "text": "Lens speed\n\nLens speed refers to the maximum aperture diameter, or minimum f-number, of a photographic lens. A lens with a larger maximum aperture (that is, a smaller minimum f-number) is called a \"fast lens\" because it can achieve the same exposure with a faster shutter speed. Conversely, a smaller maximum aperture (larger minimum f-number) is \"slow\" because it delivers less light intensity and requires a slower (longer) shutter speed.\n\nA fast lens speed is desirable in taking pictures in dim light, or with long telephoto lenses and for controlling depth of field and bokeh, especially in portrait photography, and for sports photography and photojournalism.\n\nLenses may also be referred to as being \"faster\" or \"slower\" than one another; so an lens can be described as faster than an 5.6.\n\nAttaining maximum lens speed requires engineering tradeoffs, and as such, \"prime\" (fixed focal length) lenses are generally faster than zoom lenses, and modern manual-focus lenses are generally faster than their autofocus counterparts.\n\nWith 35mm cameras, the fastest lenses are typically in the \"normal lens\" range near 50mm and there are several high-quality fast lenses available that are relatively inexpensive. For example, the Canon EF 50mm 1.8 II or Nikon AF Nikkor 50mm 1.8D are very inexpensive, but quite fast and optically well-regarded. Old fast manual focus lenses, just as the Nikkor-S(C) or Nikkor AI-S 50mm 1.4, were historically produced abundantly, and are thus sold relatively inexpensively on the used lens market.\n\nEspecially outside of the \"normal lenses\", lens speed also tends to correlate with the price and/or quality of the lens. This is because lenses with larger maximum apertures require greater care with regard to design, precision of manufacture, special coatings and quality of glass. At wide apertures, spherical aberration becomes more significant and must be corrected. Faster telephoto and wide-angle retrofocus designs tend to be much more expensive.\n\nThe fastest lenses in general production now are 1.2 or 1.4, with more at 1.8 and 2.0, and many at 2.8 or slower. What is considered \"fast\" has evolved to lower f-numbers over the years, due to advances in lens design, optical manufacturing, quality of glass, optical coatings, and the move toward smaller imaging formats. For example, the 1911 Encyclopædia Britannica states that \"...[Lenses] are also sometimes classified according to their rapidity, as expressed by their effective apertures, into extra rapid, with apertures larger than 6; rapid, with apertures from 6 to 8; slow, with apertures less than 11.\"\nFor scale, note that 0.5, 0.7, 1.0, 1.4, and 2.0 are each 1 f-stop apart (2× as fast), as an f-stop corresponds to a factor of square root of 2, about 1.4. Thus around 1.0, a change of 0.1 corresponds to about 1/4 of an f-stop (by linear approximation): 1.0 is about 50% faster than 1.2, which is about 50% faster than 1.4.\n\n, Canon, Nikon, Pentax and Sony all make an autofocus 50mm 1.4 lens. These are not unusual lenses and are relatively inexpensive. Canon also makes autofocus 50mm and 85mm 1.2 lenses, while Nikon makes a manual focus 50mm 1.2 lens and an autofocus 85mm 1.4; see Canon EF 50mm lenses and Canon EF 85mm lenses for details. Pentax makes a 50mm 1.4 lens and 55mm 1.4 lens for APS-C cameras; see Pentax lenses. Sony makes a 50mm 1.4 lens which is a continuation of the Minolta AF 50mm 1.4 lens, and two lenses with Carl Zeiss: a 50mm 1.4 and 85mm 1.4.\n\nThe maximum exposure time in free-hand photography can be enhanced even more also for fast lenses, if the camera is equipped with an image stabilisation system. In 2014 Panasonic introduced the fastest lens with in-built stabilisation, the Leica Nocticron 42.5 mm f/1.2, which in the meantime even can be operated with dual image stabilisation (Dual I.S.), provided that the camera body has an additional stabilising system at the image sensor, too.\n\nIn the mid 60s there was something of a fad for fast lenses among the major manufacturers. In 1966 in response to the trend Carl Zeiss displayed a prop lens christened the Super-Q-Gigantar 40mm 0.33 at photokina. Made from various parts found around the factory (the lenses came from a darkroom condenser enlarger), the claimed speed and focal lengths were purely nominal and it wasn't usable for photography.\n\nUltimately, the speed of a lens is limited by mechanical constraints of the camera system (shutter or mirror clearance, mount diameter). The smallest possible working f-number is\n\nwhere\n\nThis sets a limit close to 1.0 to 1.2 for most SLR mounts, whereas lenses for rangefinder and mirrorless cameras can be faster, as they can be brought closer to the image plane. Reproduction lenses incapable of infinity focus can have nominal f-numbers smaller than this limit, as the limit applies to the \"working\" f-number (the f-number corrected by the bellows factor), not to the nominal f-number. It should be noted that only the working f-number correctly assesses the light gathering power of the lens.\n\nSince , it follows that no lens can be faster than 0.5 if it operates in air. Lenses can be made faster than this by requiring the film emulsion to be in physical contact with the rear element, thus eliminating the air gap between the lens and the emulsion. Another option is to use oil immersion techniques.\n\nSome of the fastest camera lenses in production were as follows:\n\n\nThe following camera lenses are no longer in production :\n\n\nMany very fast lenses exist in C-mount (such as used by 16mm film cameras, CCTV, medical & scientific imaging systems), including:\n\nVery fast lenses in D-mount for 8mm movie use on H8 cameras:\n\nVery fast lenses used in x-ray machines:\n\n"}
{"id": "39999171", "url": "https://en.wikipedia.org/wiki?curid=39999171", "title": "Level (logarithmic quantity)", "text": "Level (logarithmic quantity)\n\nIn the International System of Quantities, the level of a quantity is the logarithm of the ratio of the value of that quantity to a reference value of the same quantity. Examples are the various types of sound level: sound power level (literally, the level of the sound power, abbreviated SWL), sound exposure level (SEL), sound pressure level (SPL) and particle velocity level (SVL).\n\nLevel and its units are defined in ISO 80000-3.\n\nLevel of a quantity \"Q\", denoted \"L\", is defined by\nwhere\n\nThe level of a \"root-power\" quantity (also known as a \"field\" quantity), denoted \"L\", is defined by \nwhere\nFor the level of a root-power quantity, the base of the logarithm is .\n\nLevel of a \"power\" quantity, denoted \"L\", is defined by\nwhere\nFor the level of a power quantity, the base of the logarithm is .\n\nThe neper, bel, and decibel (one tenth of a bel) are units of level that are often applied to such quantities as power, intensity, or gain. The neper, bel, and decibel are defined by\n\nIf \"F\" is a root-power quantity:\n\nIf \"P\" is a power quantity:\n\nIf the power quantity \"P\" is proportional to \"F\", and if the reference value of the power quantity, \"P\", is in the same proportion to \"F\", the levels \"L\" and \"L\" are equal.\n\nThe octave is a unit of level (specifically \"frequency level\", for ) though that concept is seldom seen outside of the ANSI standard that defines it. A semitone is one twelfth of an octave. A cent is one hundredth of a semitone.\n\n"}
{"id": "30110467", "url": "https://en.wikipedia.org/wiki?curid=30110467", "title": "List of Moroccan flags", "text": "List of Moroccan flags\n\nList of Moroccan flags This is a list of flags used in Morocco. For more information about the national flag, visit the article Flag of Morocco.\n\n"}
{"id": "42500102", "url": "https://en.wikipedia.org/wiki?curid=42500102", "title": "List of column-oriented DBMSes", "text": "List of column-oriented DBMSes\n\nThis article is a list of column-oriented database management system software.\n\n\n"}
{"id": "28848563", "url": "https://en.wikipedia.org/wiki?curid=28848563", "title": "List of parks and gardens in Pakistan", "text": "List of parks and gardens in Pakistan\n\nThis is a list of parks and gardens in Pakistan.\n\n\n\n\n- Lady Garden Park\n\n- Company Bagh\n\n- Wild life Park\n\n- Shimla Hill park\n\n\n\n\n\n\n\nThe great fiesta water park,\nSunway Lagoon water park,\nCosy water park,\n\n\n\n\n"}
{"id": "7119985", "url": "https://en.wikipedia.org/wiki?curid=7119985", "title": "List of volcanoes in the Comoros", "text": "List of volcanoes in the Comoros\n\nThis is a list of active and extinct volcanoes in the Comoros. \n"}
{"id": "39310227", "url": "https://en.wikipedia.org/wiki?curid=39310227", "title": "Low-information rationality", "text": "Low-information rationality\n\nLow information rationality is a social theory that states that people are information consumers with limited benefits and time for processing and understanding information. Due to the limited benefits and time individuals have for learning new information, individuals use various shortcuts and heuristics to understand information quicker. Simply put, it does not make sense for the average individual to develop in depth understandings of most issues. The theory is often used to explain the limited understanding of politics and scientific technologies by the general public.\n\nThe concept of low-information rationality is based on the assumption that human beings are cognitive misers and minimize the economic costs of making decisions and forming attitudes. Most citizens will therefore not bother to develop an in-depth understanding of political or scientific issues, which would require significant time and effort. Rather, they collect only as much information as they think is necessary to make any given decision.\n\nThe reason this theory comes about in today's age is due to vast increase in amount of information that each of us is exposed to through the internet, smart phones, and TV. Therefore, these patterns of information processing make perfect sense for citizens who have to deal with thousands of pieces of new information every day, and we all use them. We spend less cognitive effort in buying toothpaste than we do when picking a new car. And that difference in information-seeking is largely a function of the costs.\n\nThere are a variety of shortcuts individuals use to process information quickly and more efficiently. This does not mean, however, that these methods always lead to accurate and reliable conclusions. Common shortcuts include, stereotypes, opinions of others, interpersonal influences, news frames, heuristics and political ideology.\n\nAmerican pollster and political scientist Samuel Popkin coined the term \"low-information\" in 1991 when he used the phrase \"low-information signaling\" in his book \"The Reasoning Voter: Communication and Persuasion in Presidential Campaigns.\" Popkin relies on a theory of low information rationality to explain how voters are able to make rational choices between candidates. Voters do this by using information shortcuts that they receive during campaigns, usually using something like a \"drunkard's search.\" Voters use small amounts of personal information to construct a narrative about candidates. Essentially, they ask themselves this: \"Based on what I know about the candidate personally, what is the probability that this presidential candidate was a good governor? What is the probability that he will be a good president?\" Popkin's analysis is based on one main premise: voters use low information rationality gained in their daily lives, through the media and through personal interactions, to evaluate candidates and facilitate electoral choices.\n\nThe science Literacy/knowledge deficit model states that the public is willing and able to process information if it is available. Therefore, a lack of public support or participation is caused by a lack of information available to the public.\n\nAs mentioned above, scientists who use the knowledge deficit model face great difficulty conveying information to the lay public when overwhelming amount of psychology and political science studies show that the public uses the low information rationality model. Some examples of this are in the cutting-edge sciences of nanotechnology and biotechnology.\n\nFor issues such as agricultural biotechnology, for example, where developing an in-depth understanding would require significant efforts on the part of ordinary citizens, the pay-offs in terms of being able to make informed policy judgments may simply not be enough. As a result, it makes perfect sense for citizens to rely on shortcuts such as opinions of others when forming their own opinions and trying to make sense of different policy positions.\n\n\n"}
{"id": "5008317", "url": "https://en.wikipedia.org/wiki?curid=5008317", "title": "Mariano de la Paz Graëlls y de la Aguera", "text": "Mariano de la Paz Graëlls y de la Aguera\n\nMariano de la Paz Graëlls y de la Aguera (1809 – 1898) was a Spanish entomologist notable for pioneering work on the insects of corpses.\n\nGraëlls was born in Tricio, in the Province of Logroño. He died in Madrid where he had been Professor of Zoology.\n\nThe \"Graellsia\" genus of moths and the Graells's tamarin are named after him.\nHe also identified the Iberian subspecies (\"Meles meles marianensis\") of the badger.\n\n\n"}
{"id": "994704", "url": "https://en.wikipedia.org/wiki?curid=994704", "title": "Mental model", "text": "Mental model\n\nA mental model is an explanation of someone's thought process about how something works in the real world. It is a representation of the surrounding world, the relationships between its various parts and a person's intuitive perception about his or her own acts and their consequences. Mental models can help shape behaviour and set an approach to solving problems (similar to a personal algorithm) and doing tasks.\n\nA mental model is a kind of internal symbol or representation of external reality, hypothesized to play a major role in cognition, reasoning and decision-making. Kenneth Craik suggested in 1943 that the mind constructs \"small-scale models\" of reality that it uses to anticipate events.\n\nJay Wright Forrester defined general mental models as:\nThe image of the world around us, which we carry in our head, is just a model. Nobody in his head imagines all the world, government or country. He has only selected concepts, and relationships between them, and uses those to represent the real system (Forrester, 1971).\n\nIn psychology, the term \"mental models\" is sometimes used to refer to mental representations or mental simulation generally. At other times it is used to refer to and to the mental model theory of reasoning developed by Philip Johnson-Laird and Ruth M.J. Byrne.\n\nThe term \"mental model\" is believed to have originated with Kenneth Craik in his 1943 book \"The Nature of Explanation\". in \"Le dessin enfantin\" (Children's drawings), published in 1927 by Alcan, Paris, argued that children construct internal models, a view that influenced, among others, child psychologist Jean Piaget.\n\nPhilip Johnson-Laird published \"Mental Models: Towards a Cognitive Science of Language, Inference and Consciousness\" in 1983. In the same year, Dedre Gentner and Albert Stevens edited a collection of chapters in a book also titled \"Mental Models\". The first line of their book explains the idea further: \"One function of this chapter is to belabor the obvious; people's views of the world, of themselves, of their own capabilities, and of the tasks that they are asked to perform, or topics they are asked to learn, depend heavily on the conceptualizations that they bring to the task.\" (see the book: \"Mental Models\").\n\nSince then, there has been much discussion and use of the idea in human-computer interaction and usability by researchers including Donald Norman and Steve Krug (in his book \"Don't Make Me Think\"). Walter Kintsch and Teun A. van Dijk, using the term \"situation model\" (in their book \"Strategies of Discourse Comprehension\", 1983), showed the relevance of mental models for the production and comprehension of discourse.\n\nOne view of human reasoning is that it depends on mental models. In this view, mental models can be constructed from perception, imagination, or the comprehension of discourse (Johnson-Laird, 1983). Such mental models are similar to architects' models or to physicists' diagrams in that their structure is analogous to the structure of the situation that they represent, unlike, say, the structure of logical forms used in formal rule theories of reasoning. In this respect, they are a little like pictures in the picture theory of language described by philosopher Ludwig Wittgenstein in 1922. Philip Johnson-Laird and Ruth M.J. Byrne developed a theory of mental models which makes the assumption that reasoning depends, not on logical form, but on mental models (Johnson-Laird and Byrne, 1991).\n\nMental models are based on a small set of fundamental assumptions (axioms), which distinguish them from other proposed representations in the psychology of reasoning (Byrne and Johnson-Laird, 2009). Each mental model represents a possibility. A mental model represents one possibility, capturing what is common to all the different ways in which the possibility may occur (Johnson-Laird and Byrne, 2002). Mental models are iconic, i.e., each part of a model corresponds to each part of what it represents (Johnson-Laird, 2006). Mental models are based on a principle of truth: they typically represent only those situations that are possible, and each model of a possibility represents only what is true in that possibility according to the proposition. However, mental models can represent what is false, temporarily assumed to be true, for example, in the case of counterfactual conditionals and counterfactual thinking (Byrne, 2005).\n\nPeople infer that a conclusion is valid if it holds in all the possibilities. Procedures for reasoning with mental models rely on counter-examples to refute invalid inferences; they establish validity by ensuring that a conclusion holds over all the models of the premises. Reasoners focus on a subset of the possible models of multiple-model problems, often just a single model. The ease with which reasoners can make deductions is affected by many factors, including age and working memory (Barrouillet, et al., 2000). They reject a conclusion if they find a counterexample, i.e., a possibility in which the premises hold, but the conclusion does not (Schroyens, et al. 2003; Verschueren, et al., 2005).\n\nScientific debate continues about whether human reasoning is based on mental models, versus formal rules of inference (e.g., O'Brien, 2009), domain-specific rules of inference (e.g., Cheng & Holyoak, 2008; Cosmides, 2005), or probabilities (e.g., Oaksford and Chater, 2007). Many empirical comparisons of the different theories have been carried out (e.g., Oberauer, 2006).\n\nA mental model is generally:\n\nMental models are a fundamental way to understand organizational learning. Mental models, in popular science parlance, have been described as \"deeply held images of thinking and acting\". Mental models are so basic to understanding the world that people are hardly conscious of them.\n\nS.N. Groesser and M. Schaffernicht (2012) describe three basic methods which are typically used:\nThese methods allow showing a mental model of a dynamic system, as an explicit, written model about a certain system based on internal beliefs. Analyzing these graphical representations has been an increasing area of research across many social science fields. Additionally software tools that attempt to capture and analyze the structural and functional properties of individual mental models such as Mental Modeler, \"a participatory modeling tool based in fuzzy-logic cognitive mapping\", have recently been developed and used to collect/compare/combine mental model representations collected from individuals for use in social science research, collaborative decision-making, and natural resource planning.\n\nIn the simplification of reality, creating a model can find a sense of reality, seeking to overcome systemic thinking and system dynamics.\n\nThese two disciplines can help to construct a better coordination with the reality of mental models and simulate it accurately. They increase the probability that the consequences of how to decide and act in accordance with how to plan.\n\n\nAfter analyzing the basic characteristics, it is necessary to bring the process of changing the mental models, or the process of learning. Learning is a back-loop process, and feedback loops can be illustrated as: single-loop learning or double-loop learning.\n\nMental models affect the way that people work with information, and also how they determine the final decision. The decision itself changes, but the mental models remain the same. It is the predominant method of learning, because it is very convenient.\n\nDouble-loop learning (\"see diagram below\") is used when it is necessary to change the mental model on which a decision depends. Unlike single loops, this model includes a shift in understanding, from simple and static to broader and more dynamic, such as taking into account the changes in the surroundings and the need for expression changes in mental models.\n\n\n\n"}
{"id": "20937590", "url": "https://en.wikipedia.org/wiki?curid=20937590", "title": "NS4A", "text": "NS4A\n\nNonstructural protein 4A (NS4A) is a viral protein found in the hepatitis C virus. It acts as a cofactor for the enzyme NS3.\n"}
{"id": "53383534", "url": "https://en.wikipedia.org/wiki?curid=53383534", "title": "Native Girls Code", "text": "Native Girls Code\n\nNative Girls Code (NGC) is a Seattle-based program that focuses on providing computer coding skills with grounding in traditional Indigenous knowledge for Native American girls aged 12–18 through workshops, coaching, teaching and role modeling. It is organized by the non-profit organization Na'ah Illahee Fund (Mother Earth in the Chinook language), in partnership with University of Washington Information School Digital Youth Lab and the Washington NASA Space Consortium, as a way to support and perpetuate traditional knowledge, build leadership of women and encourage greater participation of Native American students in STEM fields.\n\nThe program was designed specifically to give Native girls from tribes throughout the United States a place to develop a strong foundation in Native culture, Native science, and build the skills needed to use modern computer technologies, resulting in the creation of websites, online games and virtual worlds. Leaders hope Native Girls Code will enrich both the girls and their communities.\n\nIn 2016 NGC was awarded a grant through the City of Seattle's Technology Matching Fund, aimed at increasing digital equity among underrepresented Seattle citizens. Google has been a major funder of the program and Facebook has donated laptops and filming equipment to NGC.\n\n"}
{"id": "58959176", "url": "https://en.wikipedia.org/wiki?curid=58959176", "title": "Negarnaviricota", "text": "Negarnaviricota\n\nThe phylum Negarnaviricota includes all negative-sense single-stranded RNA viruses (group V in Baltimore classification) except \"Hepatitis delta virus\". It is divided into the subphyla \"Haploviricotina\" and \"Polyploviricotina\". it is the sole viral phylum to be established, though it does not include all viruses as many have not been assigned to a phylum.\n\nThe taxonomy of the \"Negarnaviricota\" down to the rank of order is as follows:\n\n"}
{"id": "16639136", "url": "https://en.wikipedia.org/wiki?curid=16639136", "title": "Operation Flintlock (nuclear test)", "text": "Operation Flintlock (nuclear test)\n\nThe United States's Flintlock nuclear test series was a group of 47 nuclear tests conducted in 1965-1966. These tests followed the \"Operation Whetstone\" series and preceded the \"Operation Latchkey\" series.\n"}
{"id": "1107059", "url": "https://en.wikipedia.org/wiki?curid=1107059", "title": "Operation Hardtack II", "text": "Operation Hardtack II\n\nOperation Hardtack II was a series of 37 nuclear tests conducted by the United States in 1958 at the Nevada Test Site. These tests followed the \"Operation Argus\" series and preceded the \"Operation Nougat\" series.\n\nWith test moratoriums on the horizon, American weapons labs rushed out many new designs. A hard deadline for testing was set at midnight (0000 hrs), October 31, 1958, as negotiations were set to start that day, and the schedule shows it, with 29 tests executed in October, four of them on the last day. One other test was cancelled because weather delays postponed it across the midnight deadline. After the conclusion of Hardtack II, the United States announced a unilateral testing moratorium, which the Soviet Union joined after two last tests on November 1 and 3. In September 1961, the Soviet Union resumed nuclear testing — this period included the test of the most powerful nuclear device ever designed, the \"Tsar Bomba\" on October 30, 1961 — and the United States followed suit with \"Operation Nougat\".\n\n"}
{"id": "18908074", "url": "https://en.wikipedia.org/wiki?curid=18908074", "title": "Public participation geographic information system", "text": "Public participation geographic information system\n\nA public participation geographic information system (PPGIS) is meant to bring the academic practices of GIS and mapping to the local level in order to promote knowledge production by local and non-governmental groups. The idea behind PPGIS is empowerment and inclusion of marginalized populations, who have little voice in the public arena, through geographic technology education and participation. PPGIS uses and produces digital maps, satellite imagery, sketch maps, and many other spatial and visual tools, to change geographic involvement and awareness on a local level. The term was coined in 1996 at the meetings of the National Center for Geographic Information and Analysis (NCGIA).\n\nAttendees to the \"Mapping for Change International Conference on Participatory Spatial Information Management and Communication\" conferred to at least three potential implications of PPGIS; it can: (1) enhance capacity in generating, managing, and communicating spatial information; (2) stimulate innovation; and ultimately; (3) encourage positive social change. This reflects on the rather nebulous definition of PPGIS as referenced in the \"Encyclopedia of GIS\" which describes PPGIS as having a definition problem.\n\nThere are a range of applications for PPGIS. The potential outcomes can be applied from community and neighborhood planning and development to environmental and natural resource management. Marginalized groups, be they grassroots organizations to indigenous populations could benefit from GIS technology.\n\nGovernments, non-government organizations and non-profit groups are a big force behind many programs. The current extent of PPGIS programs in the US has been evaluated by Sawicki and Peterman. They catalog over 60 PPGIS programs who aid in \"public participation in community decision making by providing local-area data to community groups,\" in the United States (Craig et al., 2002:24). The organizations providing these programs are mostly universities, local chambers of commerce, non-profit foundations.\n\nIn general, neighborhood empowerment groups can form and gain access to information that is normally very easy for the official government and planning offices to obtain. It is easier for this to happen than for individuals of lower-income neighborhoods just working by themselves. There have been several projects where university students help implement GIS in neighborhoods and communities. It is believed that access to information is the doorway to more effective government for everybody and community empowerment. In a case study of a group in Milwaukee, residents of an inner city neighborhood became active participants in building a community information system, learning to access public information and create and analyze new databases derived from their own surveys, all with the purpose of making these residents useful actors in city management and in the formation of public policy. In many cases, there are providers of data for community groups, but the groups may not know that such entities exist. Getting the word out would be beneficial.\n\nSome of the spatial data that the neighborhood wanted was information on abandoned or boarded-up buildings and homes, vacant lots, and properties that contained garbage, rubbish and debris that contributed to health and safety issues in the area. They also appreciated being able to find landlords that were not keeping up the properties. The university team and the community were able to build databases and make maps that would help them find these areas and perform the spatial analysis that they needed. Community members learned how to use the computer resources, ArcView 1.0, and build a theme or land use map of the surrounding area. They were able to perform spatial queries and analyze neighborhood problems. Some of these problems included finding absentee landlords and finding code violations for the buildings on the maps (Ghose 2001).\n\nThere are two approaches to PPGIS use and application. These two perspectives, top–down and bottom–up, are the currently debated schism in PPGIS.\n\nAccording to Sieber (2006), PPGIS was first envisioned as a means of mapping individuals by many social and economic demographic factors in order to analyze the spatial differences in access to social services. She refers to this kind of PPGIS as \"top-down\", being that it is less hands on for the public, but theoretically serves the public by making adjustments for the deficiencies, and improvements in public management.\n\nA current trend with academic involvement in PPGIS, is researching existing programs, and or starting programs in order to collect data on the effectiveness of PPGIS. Elwood (2006) in \"The Professional Geographer\", talks in depth about the \"everyday inclusions, exclusions, and contradictions of Participatory GIS research.\" The research is being conducted in order to evaluate if PPGIS is involving the public equally. In reference to Sieber's top-down PPGIS, this is a counter method of PPGIS, rightly referred to as \"bottom-up\" PPGIS. Its purpose is to work with the public to let them learn the technologies, then producing their own GIS.\n\nPublic participation GIS is defined by Sieber as the use of geographic information systems to broaden public involvement in policymaking as well as to the value of GIS to promote the goals of nongovernmental organizations, grassroots groups and community-based organizations (Sieber 2006). It would seem on the surface that PPGIS, as it is commonly referred to, in this sense would be of a beneficial nature to those in the community or area that is being represented. But in truth only certain groups or individuals will be able to obtain the technology and use it. Is PPGIS becoming more available to the underprivileged sector of the community? The question of \"who benefits?\" should always be asked, and does this harm a community or group of individuals.\n\nThe local, participatory management of urban neighborhoods usually follows on from 'claiming the territory', and has to be made compatible with national or local authority regulations on administering, managing and planning urban territory (McCall 2003). PPGIS applied to participatory community/neighborhood planning has been examined by, among many others, [Howard (1999)], [Carver, Evans, Kingston, and Turton (1999)], [Leitner, McMaster, Elwood, McMaster, and Sheppard (2002)], and [Talen (1999)]. Specific attention has been given to applications such as housing issues (e.g. [Elwood (2002)]) or neighborhood revitalization (e.g. [Craig & Elwood (1998)]). Spatial databases along with the P-mapping are used to maintain a public records GIS or community land information systems (e.g. [Ventura, Niemann, Sutphin, & Chenoweth (2002)]). These are just a few of the uses of GIS in the community.\n\nPublic Participation in decision making processes works not only to identify areas of common values or variability, but also as an illustrative and instructional tool. One example of effective dialogue and building trust between the community and decision makers comes from pre-planning for development in the United Kingdom. It involves using GIS and multi-criteria decision analysis (MCDA) to make a decision about wind farm siting. This method hinges upon taking all stakeholder perspectives into account to improve chances of reaching consensus . This also creates a more transparent process and adds weight to the final decision by building upon traditional methods such as public meetings and hearings, surveys, focus groups, and deliberative processes enabling participants more insights and more informed opinions on environmental issues.\n\nCollaborative processes that consider objective and subjective inputs have the potential to efficiently address some of the conflict between development and nature as they involve a fuller justification by wind farm developers for location, scale, and design. Spatial tools such as creation of 3D view sheds offer participants new ways of assessing visual intrusion to make a more informed decision. Higgs et al. make a very telling statement when analyzing the success of this project – \"the only way of accommodating people's landscape concerns is to site wind farms in places that people find more acceptable\". This implies that developers recognize the validity of citizens' concerns and are willing to compromise in identifying sites where wind farms will not only be successful financially, but also successful politically and socially. This creates greater accountability and facilitates the incorporation of stakeholder values to resolve differences and gain public acceptance for vital development projects.\n\nIn another planning example, Simao et al. analyzed how to create sustainable development options with widespread community support. They determined that stakeholders need to learn likely outcomes that result from stated preferences, which can be supported through enhanced access to information and incentives to increase public participation. Through a multi-criteria spatial decision support system stakeholders were able to voice concerns and work on a compromise solution to have final outcome accepted by majority when siting wind farms. This differs from the work of Higgs et al. in that the focus was on allowing users to learn from the collaborative process, both interactively and iteratively about the nature of the problem and their own preferences for desirable characteristics of solution.\n\nThis stimulated sharing of opinions and discussion of interests behind preferences. After understanding the problem more fully, participants could discuss alternative solutions and interact with other participants to come to a compromise solution. Similar work has been done to incorporate public participation in spatial planning for transportation system development, and this method of two-way benefits is even beginning to move towards web-based mapping services to further simplify and extend the process into the community.\n\n\n\n\n"}
{"id": "732362", "url": "https://en.wikipedia.org/wiki?curid=732362", "title": "Reino Antero Hirvonen", "text": "Reino Antero Hirvonen\n\nReino Antero Hirvonen (1908–1989) was a famous\nFinnish physical geodesist, also well known for\ncontributions in mathematical and astronomical geodesy.\n\nHe worked at first at the Finnish Geodetic Institute under W.A. Heiskanen on gravimetric geoid determination, publishing his dissertation \"The Continental Undulations of the Geoid\" in 1934 on the determination of a global geoid model from only 4500 data points.\n\nIn 1950 he succeeded Heiskanen as Professor of Geodesy at the Helsinki University of Technology.\n\nHe also took an active interest in astronomy, acting from 1956 to 1964 as a\nvice president of the Finnish amateur astronomical society Ursa.\n\nR.A. Hirvonen participated in the 1930s in the construction of triangulation towers, and measurements, for the first order triangulation of Finland. He also developed over the years many new mathematical algorithms for manual calculations (before the computer era), e.g., for calculating the Gauss-Krüger map projection.\n\nIn 1947 he led a team of Finnish scientists to Brazil to measure the distance between South America and Africa. (T.J. Kukkamäki was the leader of the team sent to, then, Gold Coast, Africa.) They succeed with measurements using a method based on observing from points in Brazil and Africa. They used the solar eclipse happening that year, which was visible in both Africa and South America. Using long focus film cameras and the most accurate available radio time signals for the solar eclipse measurements, they were able to calculate the distance between Africa and South America to a higher accuracy than ever before: 141 m.\n\n1951–1952 and 1954–1955 Hirvonen lectured in the Department of Geodetic Science at The Ohio State University, in Columbus, Ohio (USA). He educated the students about navigation using the stars as reference points. Later it helped the USA to fly to the moon.\n\nIn 1967 he received the Kaarina and Weikko A. Heiskanen Award (Ohio State University).\n\n"}
{"id": "11097321", "url": "https://en.wikipedia.org/wiki?curid=11097321", "title": "Shackleton (TV serial)", "text": "Shackleton (TV serial)\n\nShackleton is a 2001 British television film written and directed by Charles Sturridge and starring Kenneth Branagh as explorer Sir Ernest Shackleton. The film tells the true story of Shackleton's 1914 Antarctic expedition on the ship \"Endurance\". The cast includes Kevin McNally, Lorcan Cranitch, Embeth Davidtz, Danny Webb, Matt Day and Phoebe Nicholls (also the director's wife) as Lady Shackleton. It was filmed in the UK, Iceland and Greenland. The film used first-hand accounts by the men on the expedition to retell the story. Shackleton biographer Roland Huntford was a production advisor.\n\n\"Shackleton\" was first broadcast in two parts by Channel 4 in January 2002. In North America the film was first broadcast by the A&E Network in April 2002. The film was nominated for seven Emmy Awards, six BAFTA Awards, and a Golden Globe Award.\n\nThe films tells the true story of explorer Sir Ernest Shackleton (Kenneth Branagh) and his 1914 Antarctic expedition on the ship \"Endurance\". The story begins with him planning the expedition and finding sponsors, particularly Sir James Caird. Shackleton's goal is to drive dog sled teams from one side of Antarctica to the other, which would make Britain the first nation to undertake such a trans-continental journey.\n\nOnce the expedition is underway, trouble arises due to thick sea ice and low temperatures. \"Endurance\" becomes trapped and eventually crushed by pack ice. Shackleton vows to find a way to rescue the men. He undertakes an epic journey across the ice, followed by 800 miles of the Southern Ocean and then an uncharted mountain range on South Georgia Island. He finds a whaling station from which rescue parties are sent to collect his entire shipwrecked crew. The otherwise failed expedition is made famous for every crew member surviving despite insurmountable odds.\n\n\n\n "}
{"id": "222390", "url": "https://en.wikipedia.org/wiki?curid=222390", "title": "Table of prime factors", "text": "Table of prime factors\n\nThe tables contain the prime factorization of the natural numbers from 1 to 1000.\n\nWhen \"n\" is a prime number, the prime factorization is just \"n\" itself, written in bold below.\n\nThe number 1 is called a unit. It has no prime factors and is neither prime nor composite.\n\n\"See also: Table of divisors\" (prime and non-prime divisors for 1 to 1000)\nMany properties of a natural number \"n\" can be seen or directly computed from the prime factorization of \"n\".\nThe divisors of \"n\" are all products of some or all prime factors of \"n\" (including the empty product 1 of no prime factors).\nThe number of divisors can be computed by increasing all multiplicities by 1 and then multiplying them.\nDivisors and properties related to divisors are shown in table of divisors.\n\n"}
{"id": "13530107", "url": "https://en.wikipedia.org/wiki?curid=13530107", "title": "Technological momentum", "text": "Technological momentum\n\nTechnological momentum is a theory about the relationship between technology and society over time. The term, which is considered a fourth technological determinism variant, was originally developed by the historian of technology Thomas P. Hughes. The idea is that relationship between technology and society is reciprocal and time-dependent so that one does not determine the changes in the other but both influence each other.\n\nHughes's thesis is a synthesis of two separate models for how technology and society interact. One, \"technological determinism\", claims that society itself is modified by the introduction of a new technology in an irreversible and irreparable way—for example, the introduction of the automobile has influenced the manner in which American cities are designed, a change that can clearly be seen when comparing the pre-automobile cities on the East Coast to the post-automobile cities on the West Coast. Technology, under this model, self-propagates as well—there is no turning back once the adoption has taken place, and the very existence of the technology means that it will continue to exist in the future.\n\nThe other model, \"social determinism\", claims that society itself controls how a technology is used and developed—for example, the rejection of nuclear power technology in the USA amid the public fears after the Three Mile Island incident.\n\n\"Technological momentum\" takes the two models and adds \"time\" as the unifying factor. In Hughes's theory, when a technology is young, deliberate control over its use and scope is possible and enacted by society. However, as a technology matures, and becomes increasingly enmeshed in the society where it was created, its own deterministic force takes hold, achieving technological momentum in the process. According to Hughes this inertia, which is particularly the case for large technological systems with their technological and social components, makes them difficult to influence and steer as they start to go more on their own way, assuming deterministic traits in the process. In other words, Hughes's says that the relationship between technology and society always starts with a \"social determinism\" model, but evolves into a form of \"technological determinism\" over time and as its use becomes more prevalent and important.\n\nSince its introduction by Hughes, the \"technological momentum\" concept has been applied by a number of other historians of technology. For instance, it is considered an effective approach to reconciling the apparently opposite perspectives of the autonomy of technology and the social and political motivations behind technological choices. It is able to describe how socially and politically conditioned technological institutions become independent and autonomous over time.\n\n"}
{"id": "39197931", "url": "https://en.wikipedia.org/wiki?curid=39197931", "title": "The Stars in their Courses", "text": "The Stars in their Courses\n\nThe Stars in their Courses is a collection of seventeen scientific essays by Isaac Asimov. It is the eighth in a series of books collecting his essays from \"The Magazine of Fantasy & Science Fiction\" (May 1969 to September 1970). Doubleday & Company first published the collection in 1971.\n\n\n"}
{"id": "54351634", "url": "https://en.wikipedia.org/wiki?curid=54351634", "title": "Torc Robotics", "text": "Torc Robotics\n\nTorc Robotics (Torc) is an American autonomous vehicle company headquartered in Blacksburg, Virginia. Torc produces unmanned and autonomous technology that retrofits to existing machinery and vehicles. Its custom products, software and automation kits have been used on vehicles in several industries, including military, mining agriculture, and automotive over the last decade.\n\nThrough 2014, 50 percent of Torc’s revenue came from defense work and the rest from commercial customers. Some of its end users include the United States Department of Defense, the U.S. Marines, Air Force Research Labs (AFRL), Caterpillar, and DCD Protected Mobility.\n\nIn 2005, the company was founded by a group of Virginia Tech graduate students including Michael Fleming, the current CEO.\n\nTorc partnered with Virginia Tech to compete in the 2007 Urban Challenge, hosted by the Department of Defense Advanced Research Projects Agency (DARPA). Teams were challenged to build a fully autonomous vehicle that could travel 60 miles of urban and off-road environments in less than six hours.\n\nTorc’s Ford Escape vehicle, named Odin, placed third out of 35 competing teams, winning the $500,000 prize. The other top-2 finishers included Tartan Racing, of Carnegie Mellon University and General Motors Corp., and the Stanford Racing Team.\n\nIn 2010, Torc partnered with a robotics team at Virginia Tech to develop a vehicle for the National Federation of the Blind’s (NFB) Blind Driver Challenge. The team received the National Instruments’ 2010 Application of the Year for the project.\n\nUsing a Ford Escape, Torc implemented its ByWire drive-by-wire conversion modules, Safestop wireless emergency stop system, and PowerHub distribution modules on the vehicle.\n\nOn January 29, 2011, a blind driver independently drove Torc’s vehicle down the main straightaway, onto the road course at the Daytona Speedway.\n\nIn 2012, Torc researchers participated in the DARPA Robotics Challenge (DRC) with Team ViGIR (Virginia-Germany Interdisciplinary Robotics Team). The program challenged teams to develop robotic software and hardware capabilities to support first responders. The team made it to the finale.\n\nTorc’s technology is currently being used in military applications in various parts of the world. One such vehicle is GUSS (Ground Unmanned Support Surrogate) an autonomous vehicle used to carry equipment for the Marines. Torc has also developed an advanced sensor fusion system for the Department of Defense that is used to increase high-speed obstacle detection, classification and prediction.\n\nIn 2012, Torc’s autonomous vehicle for Air Force Research Labs (AFRL) demonstrated its ability to perform expedient runway surveys, collect soil hardness measurements, provide terrain date and report hazards to flight.\n\nTorc developed a remote control solution for hazardous mining areas. The team replicated the cab of a 240-ton haul truck and developed a teleoperated control system to allow operators to teleoperate the haul truck from a safe distance away.\n\nTorc participated in a successful five-vehicle platoon test in 2015 with the U.S. Federal Highway Administration (FHWA)’s Saxton Transportation Operations Laboratory. The test was conducted on an inactive naval air base in Willow Grove, Pennsylvania.\n\nIn July, 2017, Torc was the first company that registered with Washington state’s Autonomous Vehicle Pilot Program permit to perform a certified test with its self-driving car in Washington.\n\nOn July 26, 2017, one of Torc’s self-driving cars completed a cross country trip, with over 4,300 miles driven autonomously. The team completed the trip in Richmond, Virginia, where they were greeted by Virginia Governor Terry McAuliffe.\n"}
{"id": "400598", "url": "https://en.wikipedia.org/wiki?curid=400598", "title": "Undergraduate Research Opportunities Program", "text": "Undergraduate Research Opportunities Program\n\nAn Undergraduate Research Opportunities Program provides funding and/or credit to undergraduate students who volunteer for faculty-mentored research projects pertaining to all academic disciplines at universities such as The University of Queensland, Boston University, the Georgia Institute of Technology (Georgia Tech), the University of California, Irvine, California State University, Long Beach, the Massachusetts Institute of Technology (MIT), the University of Michigan, the University of Michigan-Flint, Florida State University, the Hong Kong University of Science and Technology (HKUST), the University of Minnesota, the University of Illinois, Urbana-Champaign, the RWTH Aachen University, Imperial College London, the University of New Hampshire, the Nanyang Technological University and the University of Oregon.\n\nThe MIT program was founded in 1969, and the program at the University of Michigan was founded in the 1989 (primarily to help minorities and women break into science and mathematics). The University of California, Irvine founded its program in 1995. In the United Kingdom, Imperial College London provides a program for doing research in most of its departments for undergraduates at Imperial. Furthermore, the UROP scheme is also available at the University of Cambridge, the University of Reading, the University of Essex and the National University of Singapore. In 2008 the RWTH Aachen University started the first international UROP program for students from the United States and Canada in Germany. The University of Sussex runs a similar scheme, across all of its departments, where undergraduates are funded for 6 weeks as Junior Research Associates. The California State University, Long Beach program does not serve transfer students.\n\nStudents can apply for funding by submitting a proposal during \"Calls for Proposals\", which occur throughout the school year.\n\nThe Calls are announced on the school web sites and distributed to faculty members and the undergraduate counseling offices. Students have approximately one month from the announcement to submit their proposals. All undergraduate students from all disciplines and majors who are participating in a research project with a faculty advisor (usually a full-time professor at the university) eligible to respond to the current Call for Proposals if they are in good academic standing with the school.\n\nProposals are prepared by the student applicant and jointly submitted by the student and the faculty advisors. UROP funds a variety of projects with grants depending upon the merits of the project. At UC Irvine, an individual student may receive up to $1,000 through UROP for projects during the school year, to help pay for supplies and other necessary materials. Alternatively, UC Irvine students may also apply for a prestigious summer fellowship, by which they may be awarded as much as $3,000 per project, which is meant to reward them for their time and efforts. Higher amounts may be awarded for group projects. The UROP Faculty Advisory Board reviews proposals and decides whether the project merits funding. The UROP Faculty Advisory Board also makes the funding recommendations.\n\nUpon completion of the year-long research project, students are expected to present their findings at the school's research symposium. Although not a requirement, some students may submit their written reports (ranging from 15-25 pages) to the faculty committee in order to be considered for publication in the university's research journals. Publication in the UROP undergraduate research journal is known to be extremely competitive. In many cases, students are asked to submit numerous drafts to the faculty committee for review, only to find out that their papers are later denied publication. In the past, a mere 5% of all submissions were accepted for publication in the UC Irvine and MIT journals.\n\nIn addition, the University of Michigan housing also offers a UROP in residence option, called the Michigan Research Community, or MRC. This is an option for UROP students to live with each other and get an in-depth education in the University of Michigan's research. Additionally, students can also apply for a summer scholarship fellowship, in which students are given up to $3,000 to fund their own project.\n\n"}
{"id": "19132648", "url": "https://en.wikipedia.org/wiki?curid=19132648", "title": "WISE Campaign", "text": "WISE Campaign\n\nThe WISE Campaign (Women into Science and Engineering) encourages women and girls to value and pursue science, technology, engineering and maths-related courses in school or college and move on into related careers and progress. Its mission statement aims to facilitate understanding of these disciplines among women and girls and the opportunities which they present at a professional level. It is operated by UKRC trading as WISE (company number 07533934).\n\nThe campaign began on 17 January 1984, headed by The Baroness Platt of Writtle, a qualified mechanical engineer, at which time women made up 7% of graduate engineers and 3% of professional engineers in the UK. It was a collaboration between the Engineering Council and the Equal Opportunities Commission, originally viewed as a one-year campaign \"Women into Science and Engineering\" \"WISE'84\".\n\nOne of WISE's main objectives is to listen to students and women qualified or working in these sectors, and understand and voice their opinions to academic institutions, policy-makers and employers. It then works creatively with delivery agencies and others, offering models, tools and approaches to support them in challenging traditional approaches, so as to demonstrate equitable involvement. WISE combats gender stereotypes to get more girls and women involved in careers where female participation was once considered near impossible.\n\nWISE operates throughout the UK, with specialist committees in Wales, Northern Ireland and Scotland. Volunteers, from industry and relevant organisations, attend the various WISE committee meetings, and undertake projects with WISE.\n\nIn 2011 the UKRC - an organisation specialising in gender equality in science, engineering and technology - became part of WISE. Trudy Norris-Grey, the Chair of UKRC since 2007 then became Chair of WISE. WISE counts The Princess Royal, Dame Julia Higgins, Kate Bellingham and Joanna Kennedy as its patrons. The Founding Chair and Patron The Baroness Platt of Writtle died on 1 February 2015, aged 91. Jess Wade sits on the WISE Board and has been instrumental in increasing the representation of Women in STEM and promoting early career researchers.\n\nIt is headquartered at Leeds College of Building, though has been based at the UKRC (UK Resource Centre for Women in Science, Engineering, and Technology) in Bradford.\n\n"}
{"id": "4230480", "url": "https://en.wikipedia.org/wiki?curid=4230480", "title": "Wideband materials", "text": "Wideband materials\n\nWideband material refers to material that can convey Microwave signals (light/sound) over a variety of wavelengths. These materials possess exemplary attenuation and dielectric constants, and are excellent dielectrics for semiconductor gates. Examples of such material include gallium nitride (GaN) and silicon carbide (SiC). \n\nSiC has been used extensively in the creation of lasers for several years. However, it performs poorly (providing limited brightness) because it has an indirect band gap. GaN has a wide band gap (~3.4 eV), which usually results in high energies for structures which possess electrons in the conduction band.\n\n"}
