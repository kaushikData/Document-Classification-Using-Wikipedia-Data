{"id": "16081497", "url": "https://en.wikipedia.org/wiki?curid=16081497", "title": "3DSlicer", "text": "3DSlicer\n\n3D Slicer (Slicer) is a free and open source software package for image analysis and scientific visualization. Slicer is used in a variety of medical applications, including autism, multiple sclerosis, systemic lupus erythematosus, prostate cancer, schizophrenia, orthopedic biomechanics, COPD, cardiovascular disease and neurosurgery.\n\n3D Slicer is a free open source software (BSD-style license) that is a flexible, modular platform for image analysis and visualization. 3D Slicer are extended to enable development of both interactive and batch processing tools for a variety of applications.\n\n3D Slicer provides image registration, processing of DTI (diffusion tractography), an interface to external devices for image guidance support, and GPU-enabled volume rendering, among other capabilities. 3D Slicer has a modular organization that allows the addition of new functionality and provides a number of generic features not available in competing tools.\n\nThe interactive visualization capabilities of 3D Slicer include the ability to display arbitrarily oriented image slices, build surface models from image labels, and hardware accelerated volume rendering. 3D Slicer also supports a rich set of annotation features (fiducials and measurement widgets, customized colormaps).\n\nSlicer's capabilities include:\n\n\nSlicer is compiled for use on multiple computing platforms, including Windows, Linux, and macOS.\n\nSlicer is distributed under a BSD style, free, open source license. The license has no restrictions on use of the software in academic or commercial projects. However, no claims are made on the software being useful for any particular task. It is entirely the responsibility of the user to ensure compliance with local rules and regulations.The slicer has not been formally approved for clinical use by the FDA in the US or by any other regulatory body elsewhere.\n\nSlicer started as a masters thesis project between the Surgical Planning Laboratory at the Brigham and Women's Hospital and the MIT Artificial Intelligence Laboratory in 1998. 3D Slicer version 2 has been downloaded several thousand times. In 2007 a completely revamped version 3 of Slicer was released. The next major refactoring of Slicer was initiated in 2009, which aims to transition the GUI of Slicer from using KWWidgets to Qt. Qt-enabled Slicer version 4 has been released in 2011.\n\nSlicer software has enabled a variety of research publications, all aimed at improving image analysis.\n\nThis significant software project has been enabled by the participation of several large-scale NIH funded efforts, including the NA-MIC, NAC, BIRN, CIMIT, Harvard Catalyst and NCIGT communities. The funding support comes from several federal funding sources, including NCRR, NIBIB, NIH Roadmap, NCI, NSF and the DOD.\n\nSlicer's platform provides functionalities for segmentation, registration and three-dimensional visualization of multimodal image data, as well as advanced image analysis algorithms for diffusion tensor imaging, functional magnetic resonance imaging and image-guided radiation therapy. Standard image file formats are supported, and the application integrates interface capabilities to biomedical research software.\n\nSlicer has been used in a variety of clinical research. In image-guided therapy research, Slicer is frequently used to construct and visualize collections of MRI data that are available pre- and intra-operatively to allow for the acquiring of spatial coordinates for instrument tracking. In fact, Slicer has already played such a pivotal role in image-guided therapy, it can be considered as growing up alongside that field, with over 200 publications referencing Slicer since 1998.\n\nIn addition to producing 3D models from conventional MRI images, Slicer has also been used to present information derived from fMRI (using MRI to assess blood flow in the brain related to neural or spinal cord activity), DTI (using MRI to measure the restricted diffusion of water in imaged tissue), and electrocardiography. For example, Slicer's DTI package allows the conversion and analysis of DTI images. The results of such analysis can be integrated with the results from analysis of morphologic MRI, MR angiograms and fMRI. Other uses of Slicer include paleontology and neurosurgery planning.\n\nThe Slicer Developer Orientation offers resources for developers new to the platform. Slicer development is coordinated on the slicer-devel mailing list, and a summary of development statistics is available on Ohloh.\n\n3D Slicer is built on VTK, a pipeline-based graphical library that is widely used in scientific visualization and ITK, a framework widely used for the development of image segmentation and image registration. In version 4, the core application is implemented in C++, and the API is available through a Python wrapper to facilitate rapid, iterative development and visualization in the included Python console. The user interface is implemented in Qt, and may be extended using either C++ or Python.\n\nSlicer supports several types of modular development. Fully interactive, custom interfaces may be written in C++ or Python. Command-line programs in any language may be wrapped using a light-weight XML specification, from which a graphical interface is automatically generated.\n\nFor modules that are not distributed in the Slicer core application, a system is available to automatically build and distribute for selective download from within Slicer. This mechanism facilitates the incorporation of code with different license requirements from the permissive BSD-style license used for the Slicer core.\n\nThe Slicer build process utilizes CMake to automatically build prerequisite and optional libraries (excluding Qt). The core development cycle incorporates automatic testing, as well as incremental and nightly builds on all platforms, monitored using an online dashboard.\n\n\n\n"}
{"id": "21543302", "url": "https://en.wikipedia.org/wiki?curid=21543302", "title": "Aestivation (botany)", "text": "Aestivation (botany)\n\nAestivation or estivation refers to the positional arrangement of the parts of a flower within a flower bud before it has opened. Aestivation is also sometimes referred to as praefoliation or prefoliation, but these terms may also mean vernation: the arrangement of leaves within a vegetative bud.\n\nAestivation can be an important taxonomic diagnostic; for example Malvaceae flower buds have valvate sepals, with the exception of the genera \"Fremontodendron\" and \"Chiranthodendron\", which have sometimes been misplaced as a result.\n\nThe terms used to describe aestivation are the same as those used to describe leaf vernation.\nClasses of aestivation include:\n\nVexillary aestivation is a special type of aestivation occurring in plants like pea. In this type of aestivation a large petal called standard encloses two smaller petals called \n"}
{"id": "26013720", "url": "https://en.wikipedia.org/wiki?curid=26013720", "title": "Albert Einstein: Creator and Rebel", "text": "Albert Einstein: Creator and Rebel\n\nAlbert Einstein: Creator and Rebel is a book by Banesh Hoffmann with the collaboration of Helen Dukas. The book is a biography of Albert Einstein, including both his personal and academic life with references to specific events and people. The book also presents Einstein's photographs from his archives.\n\nThe work was published in 1972 by Viking Press, , held in 1175 WorldCat libraries.\nIt was reviewed by many general newspapers and magazines, including \"The New York Times\" and the \"Los Angeles Times\". It was also reviewed in academic periodicals, including \"ISIS\" \n\nIt was translated into Russian, as \"Al'bert Ejnštejn – tvorec i buntar\"(1975) ; into French, as \"Albert Einstein : créateur et rebelle\" (1975) ; into Slovenian, as \"Albert Einstein, ustvarjalec in upornik\" (1980); into German, as \"Albert Einstein, Schöpfer und Rebell\" in 1978; Italian, as \"Albert Einstein, creatore e ribelle\" (1977, reprinted 1988); Dutch, as \"Albert Einstein : schepper en rebel\" (1975); Spanish, as \"Einstein\" (1984); Japanese, as \"Ainshutain, sozo to hankotsu no hito\" (1974), Georgian, as \"Albert Einsteini : Shemokmedi Da Meambokhe\", and Greek, as \"Αϊνστάιν, δημιουργός και επαναστάτης\" (1982).\n"}
{"id": "41789711", "url": "https://en.wikipedia.org/wiki?curid=41789711", "title": "Audience cost", "text": "Audience cost\n\nAn audience cost is a term in international relations theory that describes the penalty a leader incurs from his or her constituency if they escalate a foreign policy crisis and are then seen as backing down. The term was popularized in a 1994 academic article by James Fearon.\n"}
{"id": "1159513", "url": "https://en.wikipedia.org/wiki?curid=1159513", "title": "Augmentation Research Center", "text": "Augmentation Research Center\n\nSRI International's Augmentation Research Center (ARC) was founded in the 1960s by electrical engineer Douglas Engelbart to develop and experiment with new tools and techniques for collaboration and information processing.\n\nThe main product to come out of ARC was the revolutionary oN-Line System, better known by its abbreviation, NLS. ARC is also known for the invention of the \"computer mouse\" pointing device, and its role in the early formation of the Internet.\n\nEngelbart recruited workers and ran the organization until the late 1970s when the project was commercialized and sold to Tymshare, which was eventually purchased by McDonnell Douglas.\n\nSome early ideas by Douglas Engelbart were developed in 1959 funded by the Air Force Office of Scientific Research (now Rome Laboratory).\nBy 1962, a framework document was published.\n\nJ. C. R. Licklider, the first director of the United States Department of Defense's Advanced Research Project Agency (DARPA) Information Processing Techniques Office (IPTO), funded the project in early 1963. First experiments were done trying to connect a display at SRI to the massive one-of-a-kind AN/FSQ-32 computer at the System Development Corporation in Santa Monica, California.\n\nNASA began to provide major funding at the behest of Robert Taylor in 1964. A custom graphical workstation was built around a commercial computer, the CDC 160A, and a CDC 3100, which handled a single user at a time. In 1965, Taylor became IPTO director, leading to increased funding. In 1968 an SDS 940 computer running the Berkeley Timesharing System allowed multiple users.\n\nThe project was first called ARNAS after the sponsors. For a few years it was then called the Augmented Human Intellect Research Center, which got shortened to the Augmentation Research Center around 1969.\n\nDuring a 90-minute session at the Fall Joint Computer Conference in December 1968, Engelbart, Bill English, Jeff Rulifson and other ARC staffers presented their work in a live demonstration, including real-time video conferencing and interactive editing in an era when batch processing was still the paradigm for using computers. This was later called \"the Mother of All Demos\".\n\nEngelbart had volunteered ARC to provide the first reference library service on the ARPANET while it was being designed. The first message sent on ARPANET was between the ARC computer and UCLA. Larry Roberts continued to fund the ARC through DARPA IPTO until he left in 1974. The library service evolved into the Internet Network Information Center managed by Elizabeth J. Feinler. Bertram Raphael was put in charge of the project in 1976.\n\nThe technology was sold to Tymshare in 1977, with 20 members of the former SRI group (including Engelbart) becoming Tymshare employees. Only about three or four people were left to continue the NIC, although this group grew quickly along with the Internet. Jon Postel left in 1977 to join the Information Sciences Institute. A number of early participants moved on to careers at Xerox, Hewlett-Packard, Apple Computer, Sun Microsystems, and other leading computer companies.\n\nTymshare renamed the software \"Augment\" and offered it as a commercial service via its new Office Automation Division. At Tymshare, Engelbart soon found himself marginalized and relegated to obscurity. Operational concerns at Tymshare overrode Engelbart's desire to do further research. Various executives, first at Tymshare and later at McDonnell Douglas, which acquired Tymshare in 1984, expressed interest in his ideas, but never committed the funds or the people to further develop them. His interest inside of McDonnell Douglas was focused on the enormous knowledge management and IT requirements involved in the life cycle of an aerospace program, which served to strengthen Engelbart's resolve to motivate the information technology arena toward global interoperability and an open hyperdocument system. Engelbart retired from McDonnell Douglas in 1986, determined to pursue his work free from commercial pressure.\n\nThe complex story of the rise and fall of ARC has been documented in a book by sociologist Thierry Bardini. From the perspective of the 1960s counter-culture revolution, John Markoff, in his book \"What the Dormouse Said\", also follows Englebart's persistence in creating ARC as not only a collection of talented off-beat engineers working in direct contrast to the Stanford Artificial Intelligence Laboratory nearby, but also as a sociological experiment that constructed and tested methods for group creation and design.\n\nARC was also indirectly covered in many other books about Xerox PARC, since that is where many ARC employees later fled to (and brought some of Engelbart's ideas with them). Taylor had founded the Computer Systems Laboratory at PARC in 1970.\n\n\n"}
{"id": "16487634", "url": "https://en.wikipedia.org/wiki?curid=16487634", "title": "Austreskorve Glacier", "text": "Austreskorve Glacier\n\nAustreskorve Glacier () is a broad glacier in the Mühlig-Hofmann Mountains which drains north from a position just east of the head of Vestreskorve Glacier and passes along the east side of Breplogen Mountain. It was mapped and named from surveys and from air photos by the Sixth Norwegian Antarctic Expedition (1956–60).\n\n"}
{"id": "6917217", "url": "https://en.wikipedia.org/wiki?curid=6917217", "title": "Autoinoculation", "text": "Autoinoculation\n\nAutoinoculation is derived from the Latin root words \"autos\" and \"inoculate\" that mean \"self implanting\" or \"self infection\" or \"implanting something from oneself\". Autoinoculation can refer to both beneficial medical procedures (e.g. vaccination) as well as non-beneficial or harmful natural processes (e.g. infection or disease). One beneficial autoinoculation medical procedure is when cells are removed from a person's body, medically altered then reinserted (\"implanted\" or \"infected\") into the same organism or person again to achieve some diagnostic or treatment aim. For example, stem cell treatments involve the harvesting of stem cells from one's own bone marrow and reintroduction (autoinoculation) of those cells at a later date, sometimes after altering those stem cells. Autoinoculation may also be used for the transplantation of a patient's own healthy bone marrow after recovering from a condition afflicting the tissue. Autoinoculation can also refer to the process by which viruses reproduce themselves within an organism by implanting themselves in an organism's cells, altering the metabolism, DNA repair, and replication processes of those cells, using those processes to reproduce and transmit itself throughout the organism. For example, warts and Molluscum contagiosum can be spread by this method if wart tissue cells (skin cells altered by a papillomavirus) are mechanically transported to another part of the body. This transmission or autoinoculation of the wart can occur by mechanical touching of one part of the organism to another, friction that removes a portion of the infected cells to an external surface (or another organism) and then reintroduces those cells upon contact with the body elsewhere, or when wart cells or tissue are transported though the blood stream of an organism.\n\n"}
{"id": "57216163", "url": "https://en.wikipedia.org/wiki?curid=57216163", "title": "BEnd.3", "text": "BEnd.3\n\nbEnd.3 is a mouse brain cell line derived from BALB/c mice. The cell line is commonly used in vascular research and studies of endothelial brain tissue. In particular, bEnd.3 cells can serve as blood-brain barrier models for ischemia.\n\n"}
{"id": "42136291", "url": "https://en.wikipedia.org/wiki?curid=42136291", "title": "Baba Shiv", "text": "Baba Shiv\n\nBaba Shiv is an American marketing professor and an expert in the area of neuroeconomics. He is the Sanwa Bank, Limited, Professor of Marketing at Stanford Graduate School of Business, Stanford University. His work has been featured in \"The Tonight Show with Jay Leno\", CNN, Fox Business, \"Financial Times\", \"The New York Times\" and \"Wall Street Journal\". Shiv received his PhD from Duke University.\n\nProfessor Shiv researches how decision making and economic behavior are effected by neural structures and how brains create creativity. His public speaking events and extensive library of published research have been highly influential in the general understanding of the brain functions of creativity and motivation. \n\nIn 2010, Shiv won the American Marketing Association William F. O'Dell Award which \"recognizes the Journal of Marketing Research article that has made the most significant, long-term contribution to marketing theory, methodology, and/or practice.\" He won the award for his article \"Placebo Effects of Marketing Actions: Consumers May Get What They Pay For\"\n\n\n\n"}
{"id": "58914925", "url": "https://en.wikipedia.org/wiki?curid=58914925", "title": "Buried rupture earthquake", "text": "Buried rupture earthquake\n\nA buried rupture earthquake, or blind earthquake (c.f., surface rupture earthquake) is an earthquake with non-visible offset of the ground surface when an earthquake rupture along a fault does not affect the Earth's surface. See also blind thrust earthquake, a close concept.\n\nRecorded ground motions of large surface-rupture earthquakes are weaker than the ground motions from buried rupture earthquakes.\n\nThe asperity for a buried rupture earthquakes is in area deeper than roughly . Examples are the Loma Prieta earthquake, Northridge earthquake, and the Noto Hanto earthquake.\n\nUplifted water outside the fault plane in the buried rupture case, as compared to the seabed surface rupture case, makes for large tsunami waves.\n\n\n"}
{"id": "3094621", "url": "https://en.wikipedia.org/wiki?curid=3094621", "title": "Charge conservation", "text": "Charge conservation\n\nIn physics, charge conservation is the principle that the total electric charge in an isolated system never changes. \nThe net quantity of electric charge, the amount of positive charge minus the amount of negative charge in the universe, is always \"conserved\". This does not mean that individual positive and negative charges cannot be created or destroyed. Electric charge is carried by subatomic particles such as electrons and protons. Charged particles can be created and destroyed in elementary particle reactions. In reactions that create charged particles, equal numbers of positive and negative particles are always created, keeping the net amount of charge unchanged. Similarly, when particles are destroyed, equal numbers of positive and negative charges are destroyed. This property—supported without exception by all empirical observations so far—is described as charge conservation.\n\nCharge conservation, considered as a physical conservation law, implies that the change in the amount of electric charge in any volume of space is exactly equal to the amount of charge flowing into the volume minus the amount of charge flowing out of the volume. In essence, charge conservation is an accounting relationship between the amount of charge in a region and the flow of charge into and out of that region, given by a continuity equation between charge density formula_1 and current density formula_2.\n\nAlthough conservation of charge requires that the total quantity of charge in the universe is constant, it leaves open the question of what that quantity is. Most evidence indicates that the net charge in the universe is zero; that is, there are equal quantities of positive and negative charge.\n\nCharge conservation was first proposed by British scientist William Watson in 1746 and American statesman and scientist Benjamin Franklin in 1747, although the first convincing proof was given by Michael Faraday in 1843.\n\nMathematically, we can state the law of charge conservation as a continuity equation:\n\nwhere formula_4 is the electric charge accumulation rate in a specific volume at time , formula_5 is the amount of charge flowing into the volume and formula_6 is the amount of charge flowing out of the volume; both amounts are regarded as generic functions of time. \n\nThe integrated continuity equation between two time values reads:\n\nThe general solution is obtained by fixing the initial condition time formula_8, leading to the integral equation:\n\nThe condition formula_10 corresponds to the absence of charge quantity change in the control volume: the system has reached a steady state. From the above condition, the following must hold true:\n\ntherefore, formula_5 and formula_6 are equal (not necessarily constant) over time, then the overall charge inside the control volume does not change. This deduction could be derived directly from the continuity equation, since at steady state formula_14 holds, and implies formula_15.\n\nIn electromagnetic field theory, vector calculus can be used to express the law in terms of charge density (in coulombs per cubic meter) and electric current density (in amperes per square meter). This is called the charge density continuity equation\n\nThe term on the left is the rate of change of the charge density at a point. The term on the right is the divergence of the current density at the same point. The equation equates these two factors, which says that the only way for the charge density at a point to change is for a current of charge to flow into or out of the point. This statement is equivalent to a conservation of four-current.\n\nThe net current into a volume is\nwhere is the boundary of oriented by outward-pointing normals, and is shorthand for , the outward pointing normal of the boundary . Here is the current density (charge per unit area per unit time) at the surface of the volume. The vector points in the direction of the current.\n\nFrom the Divergence theorem this can be written\n\nCharge conservation requires that the net current into a volume must necessarily equal the net change in charge within the volume.\n\nThe total charge \"q\" in volume \"V\" is the integral (sum) of the charge density in \"V\"\nSo, by the Leibniz integral rule\nEquating (1) and (2) gives\nSince this is true for every volume, we have in general\n\nCharge conservation can also be understood as a consequence of symmetry through Noether's theorem, a central result in theoretical physics that asserts that each conservation law is associated with a symmetry of the underlying physics. The symmetry that is associated with charge conservation is the global gauge invariance of the electromagnetic field. This is related to the fact that the electric and magnetic fields are not changed by different choices of the value representing the zero point of electrostatic potential formula_24. However the full symmetry is more complicated, and also involves the vector potential formula_25. The full statement of gauge invariance is that the physics of an electromagnetic field are unchanged when the scalar and vector potential are shifted by the gradient of an arbitrary scalar field formula_26:\n\nIn quantum mechanics the scalar field is equivalent to a phase shift in the wavefunction of the charged particle:\n\nso gauge invariance is equivalent to the well known fact that changes in the phase of a wavefunction are unobservable, and only changes in the magnitude of the wavefunction result in changes to the probability function formula_29. This is the ultimate theoretical origin of charge conservation.\n\nGauge invariance is a very important, well established property of the electromagnetic field and has many testable consequences. The theoretical justification for charge conservation is greatly strengthened by being linked to this symmetry. For example, gauge invariance also requires that the photon be massless, so the good experimental evidence that the photon has zero mass is also strong evidence that charge is conserved.\n\nEven if gauge symmetry is exact, however, there might be apparent electric charge non-conservation if charge could leak from our normal 3-dimensional space into hidden extra dimensions.\n\nSimple arguments rule out some types of charge nonconservation. For example, the magnitude of the elementary charge on positive and negative particles must be extremely close to equal, differing by no more than a factor of 10 for the case of protons and electrons. Ordinary matter contains equal numbers of positive and negative particles, protons and electrons, in enormous quantities. If the elementary charge on the electron and proton were even slightly different, all matter would have a large electric charge and would be mutually repulsive.\n\nThe best experimental tests of electric charge conservation are searches for particle decays that would be allowed if electric charge is not always conserved. No such decays have ever been seen.\nThe best experimental test comes from searches for the energetic photon from an electron decaying into a neutrino and a single photon:\n\nbut there are theoretical arguments that such single-photon decays will never occur even if charge is not conserved. \nCharge disappearance tests are sensitive to decays without energetic photons, other unusual charge violating processes such as an electron spontaneously changing into a positron, \nand to electric charge moving into other dimensions.\nThe best experimental bounds on charge disappearance are:\n\n"}
{"id": "29506526", "url": "https://en.wikipedia.org/wiki?curid=29506526", "title": "Citrus gummy bark viroid", "text": "Citrus gummy bark viroid\n\nThe citrus gummy bark viroid (abbreviated CGBVd) is a sub-species of the hop stunt viroid, and thus is a member of the genus Hostuviroid.\n\nAs the name suggests, the citrus gummy bark viroid causes problems in the bark of the sweet orange tree.\n\nScrapping the bark exposes localized spots or a line of reddish-brown, gum-impregnated tissue around the scion circumference especially visible near the bud union. The discoloration and gumming may extend above the bud union to the main branches of the sweet orange while in severe infection dark streaks of gum-impregnated tissue may also be observed in longitudinal sections.\n"}
{"id": "11307533", "url": "https://en.wikipedia.org/wiki?curid=11307533", "title": "City of Light Development", "text": "City of Light Development\n\nThe Kabul - City Of Light Development is an urban reconstruction plan, first proposed by urban planner and architect Hisham N. Ashkouri to revitalize the capital city of Afghanistan. The plan targets an area just south of the Kabul River for redevelopment. This area, approximately 3.5km long and 1.75 km wide, still hosts residences, commercial and retail activity, despite the fact that it has been largely reduced to rubble after years of occupation and civil war, and many of the collapsed structures have become temporary shelters constructed of whatever is available, without building codes or standards. The plan will revitalize Meywand Avenue, one of the main avenues of commerce in the city and part of the historic Silk Route, between the Shah Do Shamshera Mosque and the Id Gah Mosque. Retail, business, and residential areas are planned, alongside preserved and restored structures of historic value. Also incorporated into this project is the new Afghan National Museum.\n\nThe design of the City Of Light is based on an \"Arid Region Design Technique\" that has proven itself over the past decades in cities such as Istanbul, Baghdad, Isfahan, and Kabul. The project will rely on extensive use of concrete, in high-rise and national structures including all-exterior solar screen work. \n\n10% of the development's profit will be set aside for seed money to encourage refurbishment of nearby homes and businesses. \n\nThe bases of the aesthetic designs to be used are rooted in the rich history of Afghan jewelry and rug designs. The buildings assembled in this project are most adaptable to various design themes, especially with the exterior glass skin being set within an outside screen of colored concrete in the shape of women's jewelry and rug patterns, extrapolated from culturally significant Afghan images. Green space in the plan is also incorporated in the shape of Necklace Park, a greenway running through the development.\n\nA Memorandum of Understanding has been signed with Afghan Ambassador to the United States Said Tayeb Jawad for greater development of this plan, and it was presented to President Hamid Karzai in May 2005.\n\n\n"}
{"id": "24894365", "url": "https://en.wikipedia.org/wiki?curid=24894365", "title": "Clean Energy Project", "text": "Clean Energy Project\n\nThe Clean Energy Project (CEP) is a virtual high-throughput discovery and design effort for the next generation of plastic solar cell materials. It studies millions of candidate structures to identify suitable compounds for the harvesting of renewable energy from the sun and for other organic electronic applications. It runs on the BOINC platform.\n\nThe project searches for the most suitable organic compounds with which to make solar cells, the best polymeric membranes with which to make fuel cells, and how best to assemble the molecules for such devices.\n\nOn June 24, 2013, the Clean Energy Project released its database to the public and the research community. The release was featured on the White House Blog and by several news organizations including the MIT Technology Review. The database contains 150 million density functional theory calculations on 2.3 million molecules.\n\n\n\n"}
{"id": "2760958", "url": "https://en.wikipedia.org/wiki?curid=2760958", "title": "Critical distance", "text": "Critical distance\n\nCritical distance is, in acoustics, the distance at which the sound pressure level of the direct sound D and the reverberant sound R are equal when dealing with a directional source. In other words, it is the point in space at which the combined amplitude of all the reflected echoes are the same as the amplitude of the sound coming directly from the source (D = R). This distance, called the critical distance formula_1, is dependent on the geometry and absorption of the space in which the sound waves propagate, as well as the dimensions and shape of the sound source.\n\nA reverberant room generates a short critical distance and an acoustically dead (anechoic) room generates a longer critical distance.\n\nThe calculation of the critical distance for a diffuse approximation of the reverberant field:\nwhere formula_3 is the degree of directivity of the source (formula_4 for an omnidirectional source), formula_5 the equivalent absorption surface, formula_6 the room volume in m and \nformula_7 the reverberation time of room in seconds. The latter approximation is using Sabine's reverberation formula formula_8.\n"}
{"id": "14595743", "url": "https://en.wikipedia.org/wiki?curid=14595743", "title": "Derain quadrangle", "text": "Derain quadrangle\n\nThe Derain quadrangle (H-10) is one of fifteen quadrangles on Mercury. It runs from 288 to 360° longitude and from -25 to 25° latitude. Named after the Derain crater, it was mapped in detail for the first time after \"MESSENGER\" entered orbit around Mercury in 2011. It had not been mapped prior to that point because it was one of the six quadrangles that was not illuminated when \"Mariner 10\" made its flybys in 1974 and 1975. These six quadrangles continued to be known by their albedo feature names, with this one known as the Pieria quadrangle.\n\nThe massive Skinakas Basin overlaps the border of this quadrangle and neighboring Eminescu quadrangle.\n"}
{"id": "3088469", "url": "https://en.wikipedia.org/wiki?curid=3088469", "title": "Dermo-optical perception", "text": "Dermo-optical perception\n\nDermo-optical perception (DOP) — also known as dermal vision, dermo-optics, eyeless sight, eyeless vision, skin vision, skin reading, finger vision, paroptic vision, para-optic perception, cutaneous perception, digital sight, and bio-introscopy — is a term that is used in parapsychological literature to denote the alleged capability to perceive colors, differences in brightness, and/or formed images through the skin (without using the eyes, as distinct from blindsight), especially upon touching with the fingertips.\n\nTypically, people who claim to have dermo-optical perception claim to be able to see using the skin of their fingers or hands. People who claim to have DOP often demonstrate it by reading while blindfolded. The effect has not been demonstrated scientifically.\n\nThe first Western scientific reports are from the 17th century. Scattered cases kept being reported over the years, but scientific interest didn't pick up until the 20th century. ESP researchers enthusiastically studied DOP, hoping that it was an example of extra-sensory perception, but they could only conclude that some of the results couldn't be explained by cheating.\n\nAccording to Joe Nickell, a noted skeptic, many circus entertainers and magicians have utilized tricks to perform eyeless-sight feats. In the 1880s Washington Irving Bishop performed the \"blindfold drive\" with a horse-drawn carriage. In the early 20th century Joaquín María Argamasilla known as the \"Spaniard with X-ray Eyes\" claimed to be able to read handwriting or numbers on dice through closed metal boxes. Argamasilla managed to fool Gustav Geley and Charles Richet into believing he had genuine psychic powers. In 1924 he was exposed by Harry Houdini as a fraud. Argamasilla peeked through his simple blindfold and lifted up the edge of the box so he could look inside it without others noticing.\n\nA teenager from America named Pat Marquis known as \"the boy with X-ray eyes\" was tested by J. B. Rhine and was caught peeking through the blindfold down his nose. Science writer Martin Gardner has written that the ignorance of blindfold deception methods has been widespread in investigations into objects at remote locations from persons who claim to possess eyeless vision. Gardner documented various conjuring techniques psychics such as Rosa Kuleshova, Linda Anderson and Nina Kulagina have used to peek from their blindfolds to deceive investigators into believing they used eyeless vision.\n\n\"Life\" magazine reported on several cases on June 12, 1964, and on April 19, 1937, calling them \"X-ray wonders\", but all of them were found to be cheating when tested under controlled conditions.\n\nIn 2010, an Italian lady known as R. G. who claimed she could peer inside sealed boxes with X-ray vision to describe what is inside was tested at the University of Pavia by Massimo Polidoro, chemist Luigi Garlaschelli and physicist Adalberto Piazzoli. Twelve objects were selected and placed in wooden boxes. She failed the test, getting only one object correct.\n\nJoe Nickell who has studied DOP has written \"To date, no one has demonstrated convincingly, under suitably controlled conditions, the existence of X-ray sight or any other form of clairvoyance or ESP.\"\n\nExperiments into DOP by scientists have shown no effect. Alleged positive results haven't been accepted by the mainstream scientific community due to procedures not being tight enough to prevent cheating by participants, problems with replicating the effect reliably, and concerns about the colors being recognized by the texture of the ink on the paper (people who are blind from an early age can recognize Braille patters that only have .2 millimeters of elevation above the paper, and the limit of relief distinction in fingers is still unknown). In summary, DOP has not been demonstrated scientifically.\n\nMost of DOP positive results have been explained as cheating by participants, either via the use of magicians' tricks, or via \"peeking down the nose\" (cheating by participants) In recent years, DOP has been the object of mainstream research that had no links with ESP.\n\nApart from trickery, there are several hypotheses about how fingers could \"see\" radiation emitted by the colors in the paper, but none have been tested successfully. For example, people can hold their fingers near to painted and non-painted surfaces, and distinguish them by how much corporal heat is radiated back to their fingers. While it has not been verified if fingers can be sensitive enough to detect heat radiation from different inks in paper, it is theorized that blind people could plausibly do it.\n\n\n"}
{"id": "43800109", "url": "https://en.wikipedia.org/wiki?curid=43800109", "title": "Differential refractometer", "text": "Differential refractometer\n\nA differential refractometer (DRI), or refractive index detector (RI or RID) is a detector that measures the refractive index of an analyte relative to the solvent. They are often used as detectors for high-performance liquid chromatography and size exclusion chromatography. They are considered to be universal detectors because they can detect anything with a refractive index different from the solvent, but they have low sensitivity.\n\nWhen light leaves one material and enters another it bends, or refracts. The refractive index of a material is a measure of how much light bends when it enters. Differential refractometers contain a flow cell with two parts: one for the sample and one for the reference solvent. The detector measures the refractive index of both components. When only solvent is passing through the sample component the measured refractive index of both components is the same, but when an analyte passes through the flow cell the two measured refractive index are different. The difference appears as a peak in the chromatogram.\n\nDifferential refractometers are often used for the analysis of polymer samples in size exclusion chromatography.\n"}
{"id": "19176602", "url": "https://en.wikipedia.org/wiki?curid=19176602", "title": "Double mass analysis", "text": "Double mass analysis\n\nDouble mass analysis is a commonly used data analysis approach for investigating the behaviour of records made of hydrological or meteorological data at a number of locations. It is used to determine whether there is a need for corrections to the data - to account for changes in data collection procedures or other local conditions. Such changes may result from a variety of things including changes in instrumentation, changes in observation procedures, or changes in gauge location or surrounding conditions. Double mass analysis for checking consistency of a hydrological or meteorological record is considered to be an essential tool before taking it for analysis purpose. This method is based on the hypothesis that each item of the recorded data of a population is consistent.\n\nAn example of a double mass analysis is a \"double mass plot\", or \"double mass curve\". For this, points and/or a joining line are plotted where the x- and y- coordinates are determined by the running totals of the values observed at two stations. If both stations are affected to the same extent by the same trends then a double mass curve should follow a straight line. A break in the slope of the curve would indicate that conditions have changed at one location but not at another.\nThis technique is based on the principle that when each recorded data comes from the same parent population, they are consistent.\n\n\n"}
{"id": "39086776", "url": "https://en.wikipedia.org/wiki?curid=39086776", "title": "Dwarfism in chickens", "text": "Dwarfism in chickens\n\nDwarfism in chickens is an inherited condition found in chickens consisting of a significant delayed growth, resulting in adult individuals with a distinctive small size in comparison with normal specimens of the same breed or population.\nThe affected birds show no signs of dwarfism in the first weeks of age. Differences in size due to dwarfism appear slow and progressively along the growing stage. Poultry breeders begin to distinguish gradually dwarfs from normal birds by their shortest shanks and smallest body size. Depending on the breed, most types of dwarfism in chickens begin to be recognized when the birds reach 8–10 weeks of age, but classification is more precise when the chickens are five months old or more. At this point differences between normal and dwarf sibs is evident in all males and in 98% of the females. Dwarfs chickens reach sexual maturity and reproduce normally.\n\nDwarfism in chickens has been found to be controlled by several simple genetic factors. Some types are autosomic while others are sex-linked, but when poultry breeders make reference to 'dwarf chickens' they usually refer implicitly to sex-linked recessive dwarfism due to the recessive gene dw, located on the Z chromosome.\nAs sex-linked dwarf broiler breeder hens can bring about normal sized broiler chickens, sex-linked recessive dwarfism found application in poultry industry since the last decades of the 20th century.\n\nThese hens require less food and less housing space. Their feed intake do not need to be restricted. They also have more tolerance to heat (see: Advantages...). So the use of sex-linked dwarf broiler female parent stocks helps to save costs and to improve animal welfare and economic efficiency in European broiler industry (see: Use of...). But in spite of the proven advantages of raising dwarf breeder hens, their use is not generalized in broiler industry.\n\nTwo different types of autosomal dwarfisms have been found in chickens. These types of dwarfism are controlled by genes located on the autosomal chromosomes so inherits the same way in both sexes.\n\nIn 1929 a type of dwarfism was described in different breeds of Rhode Island Red chickens. This type of dwarfism produced individuals which showed a general growth delay, which was recognizable since two or three weeks of age. The outer toe was curved backwards. The skull was high and wide in relation to its length and the upper beak was curved downwards. Tongue was shortened and tumescent. Legs shortened, more in the matatarsal than in the femur. The condition was semi-lethal. None of the affected birds reached sexual maturity. It was considered as the result of a dysfunction of the thyroid gland, similar to the human pathological condition known as \"myxoedema infantilis\". Dwarfs of this type were homozygotes for an autosomal recessive gene \"td\" (thyrogenous dwarfism).\n\nAnother body size mutation was found in the experimental Cornell K-strain of chickens. Body size was reduced by about 30% and the affected birds were recognizable by 6 to 8 weeks of age. Their sexual maturity was somewhat delayed and rate of lay was about 90% that of the K-strain. Viability of the carriers was good but hatchability was poor. The condition was due to an autosomal recessive gene designed \"adw\".\n\nThe ultimate goal of the modern genetic studies is to find out the underlying genes involved in these traits. To achieve this, the so-called positional candidate gene approach is gaining in importance. This approach is based on the genetic localization of a trait using genetic linkage analysis. Subsequent comparative mapping of the trait locus with the gene-rich maps of the human and the mouse may reveal candidate genes for the trait in question. Comparative mapping revealed that autosomal dwarfism in the chicken (\"adw\") is located in a chromosomal region that is conserved between chicken, human and mouse. In the mouse the phenotype \"Pygmy\", similar to chicken \"adw\" is also located in that region. The Pygmy mouse phenotype arises from the inactivation of the High Mobility Group I-C (\"HMGCI-C\"). In the human the \"HMGCI-C\" gene is also located in the same conserved chromosomal segment. Fluorescent in situ hybridization of chicken metaphase chromosomes using the chicken \"HMGI-C\" gene as a probe, showed that the chicken \"HMGI-C\" gene is indeed located in the region of the \"adw\" locus. Insulin-like growth factor 1 (\"IGF1\") is another candidate for \"adw\" in the chicken.\n\nIn birds, female is the heterogametic sex, that is, it has one \"Z\" and one \"W\" sexual chromosome (genotype ZW), while male (the homogametic sex) carries two \"Z\" chromosomes (genotype ZZ). Thus, reciprocal crosses between normal and dwarf specimens may give rather different results.\n\nHutt studied in the 1940s a remarkable type of dwarfism caused by only one sex-linked recessive gene to which he assigned the symbol \"dw\".\n\nThis mutation reduces body weight in females by 26 to 32%, but the effect is still greater in homozygous males, by about 42-43%. Chicks are normal size. This is the best studied type of dwarfism in chickens. Sex-linked dwarfism in meat type breeds are first recognized by the shortening of the shanks than by the lowering body weight in the rearing period\n\nThere are no signs of sex-linked dwarfism in the first weeks of age. Some individuals can be identified as dwarfs at 8–10 weeks of age, but classification is more precise when the chickens are five months old or more. At this point differences between normal and dwarf sibs is evident in all males and in 98% of the females. These dwarfs reach sexual maturity and reproduce normally.\n\nNormal females are always of genotype \"Dw/-\", while dwarf females are always of genotype \"dw/-\", because female is the heterogametic sex having only one \"Z\" chromosome. That is, females carrying a sex-linked gene of dwarfism are always pure and exhibit the trait. On the other hand, normal males may be either homozygous \"Dw/Dw\" or heterozygous \"Dw/dw\", but dwarf males are always homozygous \"dw/dw\".\n\nDouble dose of dwarf gene causes the dwarfism to be much more evident in males than in females. The above picture illustrates the comparative size of two full-sib roosters born the same day: Left: Normal sibling of genotype \"Dw/dw\". Right: Dwarf sibling of genotype \"dw/dw\".\n\nAmong the main factors involved in growth regulation, thyroid hormones tiroxine (T4) triiodothyronine (T3), growth hormone (GH), and its related growth factor, Insulin-like growth factor-I (IGF-I), were the most studied in dwarfs.\n\nSex-linked dwarf chickens are characterized by low circulating levels of T3 and IGF-I in spite of normal or even increased levels of T4 and GH. The T3 deficiency is explained by a lower peripheral activity of T4 monodeiodination which could be related to an abnormal T4 uptake by the cell, particularly the hepatocyte. The low production of IGF-I could be related to a deficient GH receptor, as suggested by the decreased GH binding observed in the liver of dwarf birds. Both T3 and IGF-I synthesis may share common pathways since thyroidectomy also decreases IGF-I level while a GH injection stimulates the T4 to T3 monodeiodination in the normal embryo but not in the dwarf. Further studies are needed on the GH receptor and the T4 uptake in the hepatic cell to identify the common point where the dwarf gene could act. Ovulation rate and lipomobilisation are decreased in adult dwarfs but these findings are not yet easily related to the endocrinological changes observed during growth\n\nAdministration of triiodothyronine (T3) in the diet, from day of hatch until 8 weeks of age, to sex-linked dwarf chickens stimulates growth but can not fully restore a normal growth rate.\n\nSex-linked dwarfism in chickens is a form of growth hormone resistance that resembles the Laron syndrome in humans, characterized by reductions in stature and plasma insulin-like growth factor-I (IGF-I) levels.\n\nVariants in chicken growth hormone receptor (GHR) gene lead to sex-linked dwarf chickens, but effects of different variants are distinct.\n\nBantam dwarfism is a variety of dwarfism existing in many breeds of bantam chickens . Bantam chickens are also called miniatures. These birds are popular as pets, but Bantam hens are also renowned for hatching and brooding because they are very protective mothers and will attack anything that gets near their young.\n\nThe reciprocal crosses made between normal sized chickens with Bantams revealed that Bantams carry one or several sex-linked dominant genes that reduce body size. This mutation is present in Sebright Bantams and probably other bantams. This mutation is thought to be an allele at the \"Dw\" locus and to be different from the allele \"dw\".\n\nIn genetics, the common convention is that most dominant alleles are written as capital letters and recessive alleles as lower-case letters (see: Dominance (genetics)). In spite of this, literature refers to the gene of Bantam dwarfism with the lower-case symbol \"dw\".\n\nA second type of sex-linked recessive dwarfism was found in a sex-linked dwarf chicken population. This mutation is thought to be an allele or the \"Dw\" locus and to be different from the \"dw\" allele. This conclusion is based on the fact that males heterozygous for \"dw\"/\"dw\" produce female offspring which fall into two populations with respect to shank length. The evidence is inconclusive as to whether this allele is different or the same as the \"dw\" allele.\n\nUp to the present level of knowledge the dominance order of the alleles of locus \"Dw\" is: \"dw\" > \"Dw\" > \"dw\".\n\nIn other words, allele \"dw\" for Bantam dwarfism is dominant over the normal allele \"Dw\", and the last is dominant over the sex-linked recessive allele \"dw\". The existence of a second recessive allele has not yet been confirmed.\n\nSex-linked recessive dwarfism found application in poultry industry in the last decades of the 20th century. The application in broiler production is based on the known fact that dwarf female parents give 100% normal progeny when they are mated with normal male parents. The resulting progeny males are 100% heterozigotes \"Dw\"/\"dw\" and the females are hemizygotes \"Dw\"/-.\n\nIn commercial broiler chicken production the use of dwarf female parents acquired a great importance. Nowadays, the majority of broiler breeders in Europe are the standard, fast growing genotype, but 18-20% of the broiler breeders are dwarf parental females that produce standard and alternative (medium or slow growing) broilers. The use of dwarfism also found application in commercial egg production. Dwarf Leghorn layers need less housing space, so the main advantage is to allow a more efficient use of housing space producing more eggs per unit of surface, but the smallest size of their eggs imposes a serious drawback to this purpose.\n\nUnder current practice, normal parent poultry breeding stock potentially face welfare problems. Intensive selection for production traits, especially growth rate, is associated with increased nutritious requirement and thus feed consumption, but also reproductive dysfunctions and decreased sexual activity in broiler breeders. A first resulting serious welfare problem is the subsequent severe feed restriction which is applied during rearing, in order to prevent health problems and to reach better egg production. This severe feed restriction has negative effects on bird welfare as it causes chronic stress resulting from hunger. The use of normal fast growing broiler breeder hens require dedicated programmes of feed restriction, both to maximise egg and chick production and secondly to avoid metabolic disorders and mortality in broiler breeders. The negative correlation between muscle growth and reproduction effectiveness is known as the \"broiler breeder paradox\". Using dwarf broiler breeder hens is a good alternative, because dwarf hens combine relatively good reproductive fitness with \"ad libitum\" feeding.\n\nWith respect to bird welfare the use of slow growing birds is a viable alternative to reduce the negative effects of feed restriction. Dwarf broiler breeders do not need to be (severely) feed restricted.\nThe resulting male progeny grow as fast as progeny from normal female parents. The use of female broiler female parent breeders improves feed efficiency and allows a reduction of feed costs up to 33% They take up 20-30% less housing space and have more tolerance to heat. Comparative performance tests proved that these are important advantages in tropical environments.\n\n"}
{"id": "14848511", "url": "https://en.wikipedia.org/wiki?curid=14848511", "title": "Dynamical lifetime", "text": "Dynamical lifetime\n\nIn statistical orbital mechanics, a body's dynamical lifetime refers to the mean time that a small body can be expected to remain in its current mean motion resonance. Classic examples are comets and asteroids which evolve from the 7:3 resonance to the 5:2 resonance with Jupiter's orbit with dynamical lifetimes of 1-100 Ma.\n"}
{"id": "54530744", "url": "https://en.wikipedia.org/wiki?curid=54530744", "title": "European Skills, Competences, Qualifications and Occupations", "text": "European Skills, Competences, Qualifications and Occupations\n\nEuropean Skills, Competences, Qualifications and Occupations (ESCO) is a multilingual classification of identifies and categorises skills, competences, qualifications and occupations relevant for the EU labour market and education. ESCO has been developed by The European Commission since 2010.\n\nESCO v1 contains about 2 950 occupations, each occupation is classified according to International Standard Classification of Occupations (version 2008). \n\nESCO is published in the 24 official languages of the European Union as well as in Norwegian and in Icelandic.\n\n\n"}
{"id": "51887625", "url": "https://en.wikipedia.org/wiki?curid=51887625", "title": "Fritz Karl Preikschat", "text": "Fritz Karl Preikschat\n\nFritz Karl Preikschat (September 11, 1910 – September 2, 1994) was a German, later American, electrical and telecommunications engineer and inventor. He had more than three German patents and more than 23 U.S. patents, including a dot matrix teletypewriter (Germany, 1957), a blind-landing system for airports (1965), a phased array system for satellite communications (1971), a hybrid car system (1982), and a scanning laser diode microscope for particle analysis (1989).\n\nIn 1934, he graduated from Hindenburg Polytechnic in Oldenburg, Germany with a degree in \"Elektrotechnik\" (electrical engineering). He then served in a minesweeper unit of Kriegsmarine (German Navy).\n\nFrom 1940–1945 (during WW2), he worked as an engineer at , mostly in the radar group (see Radar in World War II). At the end of WW2, his family fled to Dresden and survived (except for one relative) the Bombing of Dresden in World War II. His family then resettled as refugees in the Bavarian town of Amberg.\n\nIn 1946, he was one of the more than two thousand German specialists forcibly (at gunpoint) brought to the Soviet Union under Operation Osoaviakhim. He was then one of the more than 170 German specialists – headed by Helmut Gröttrup - brought to Branch 1 of NII-88 on Gorodomlya Island in Lake Seliger. From 1946–1952, he was an engineer and head of the high frequency lab, working on electronic instruments for the early Soviet rocket program. He also worked on a design for a 6-dish (array) deep-space tracking station for the early Soviet space program. In 1960, the Soviet Union implemented the full 8-dish (with 52-foot diameter dishes) deep-space tracking station called Pluton in the Crimea.\n\nIn June 1952, he was released from the Soviet Union and returned to East Germany. The Berlin Wall had not yet been constructed, so he was able to cross the border via the Berlin U-Bahn from East Berlin, East Germany, to West Berlin (Western Zone). He quickly met an American MP, who put him in a safe house where he spent two months getting debriefed by the U.S. Air Force on the Soviet Union's rocket program. He was also interviewed over several months by Reinhard Gehlen. Later, he published a 114-page report (in German; he finalized the report in April 1954) for the U.S. Air Force on the Soviet Union's \"Microwave-based Control System for Long-Distance Rockets\". In September 1952, he was flown by the U.S. Air Force from West Berlin (Western Zone) to Frankfurt, West Germany, where he was reunited with his family (his wife, daughter and son), finally ending a difficult, six-year separation.\n\nIn 1952–1954, he filed five patent applications for a dot matrix teletypewriter (aka \"teletype writer 7 stylus 35 dot matrix\"), later granted in 1957 (see German patent #1,006,007). In April 1953, he was hired by Telefonbau und Normalzeit GmbH (TuN, later called Tenovis). In 1956, TuN introduced the device to the Deutsche Bundespost (German Post Office), which did not show interest. In his final contract with TuN (dated May 31, 1957), he sold the five patent applications to TuN for 12,000 Deutsche Marks and 50% of the device's net future profits (while retaining rights for the U.S. market). Photos and working papers of the dot matrix teletypewriter prototype were submitted to his first U.S. employer, General Mills, in 1957. A set of working papers for the dot matrix teletypewriter were published in 1961. At Boeing in 1966–1967, the dot matrix teletypewriter design was the basis for a portable facsimile machine (using dot matrix), which was prototyped and evaluated for military use by teams at Boeing, including sales.\n\nOn June 28, 1957, he emigrated to the United States via Operation Paperclip, sponsored by a U.S. Air Force contract with General Mills. The contract was cancelled shortly afterwards, so he hired on as principal scientist at the Johns Hopkins Applied Physics Laboratory, where he worked on satellite transponder communications. He became a U.S. citizen in 1962.\n\nFrom 1959–1970, he mostly worked as lead engineer in the Space Division of Boeing (near Seattle). He also had a stint in the Military Products Group at Honeywell (in Seattle).\n\nIn 1965, while at Boeing, he invented a blind-landing system for airports. It was an automated blind-landing system and featured a 3D-display showing the virtual landing strip overlaid on the actual visual display. The system was not implemented.\n\nIn 1971, while lead engineer in the telecommunications group of the space division of Boeing in Kent, Washington, he, along with Orral Ritchey and John Nitardy, invented a phased array system for satellite communications. The patent was assigned to Boeing and a technical paper was written. The invention won Boeing's Technical Paper Award for 1970.\n\nIn 1968–1974 (several patents granted), he invented a new moisture meter for pulp and paper mills. He co-founded F.P. Research Lab to commercialize the moisture meter. In 1979, F.P. Research Lab was acquired by BTG AB (Sweden), a technology company serving the global pulp and paper industry.\n\nIn 1982, he invented an electric propulsion and braking system for cars. The system allows for significant improvement of fuel efficiency by recycling energy from the car's braking system (regenerative braking). While clearly not the only patent relating to the hybrid electric vehicle, the patent was important based on more than 120 subsequent patents directly citing it. The system was only patented in the U.S. and not prototyped or commercialized. In 1997, with the introduction of the Prius in Japan, Toyota was one of the first companies to commercialize a hybrid electric vehicle (i.e. using regenerative braking technology). In July 2000 - the same month the patent expired - Toyota introduced the Prius globally (see Toyota press releases). The Prius became America's best selling hybrid electric car.\n\nIn 1989, he, with son Ekhard Preikschat, invented a scanning laser diode microscope for particle-size analysis. He and Ekhard Preikschat co-founded Lasentec to commercialize it. In 2001, Lasentec was acquired by Mettler Toledo (NYSE: MTD). About ten thousand systems have been installed globally - over $1 Billion in cumulative sales - mostly in the pharmaceutical industry to provide in-situ control of the crystallization process in large purification systems.\n"}
{"id": "1958097", "url": "https://en.wikipedia.org/wiki?curid=1958097", "title": "Fuzzy electronics", "text": "Fuzzy electronics\n\nFuzzy electronics is an electronic technology that uses fuzzy logic, instead of the two-state Boolean logic more commonly used in digital electronics. Fuzzy electronics is fuzzy logic implemented on dedicated hardware. This is to be compared with fuzzy logic implemented in software running on a conventional processor. Fuzzy electronics has a wide range of applications, including control systems and artificial intelligence.\n\nThe first fuzzy electronic circuit was built by Takeshi Yamakawa \"et al.\" in 1980 using discrete bipolar transistors. The first industrial fuzzy application was in a cement kiln in Denmark in 1982. The first VLSI fuzzy electronics was by Masaki Togai and Hiroyuki Watanabe in 1984. In 1987, Yamakawa built the first analog fuzzy controller. The first digital fuzzy processors came in 1988 by Togai (Russo, pp. 2-6).\n\n\n\n\n"}
{"id": "8018271", "url": "https://en.wikipedia.org/wiki?curid=8018271", "title": "Fuzzy subalgebra", "text": "Fuzzy subalgebra\n\nFuzzy subalgebras theory is a chapter of fuzzy set theory. It is obtained from an interpretation in a multi-valued logic of axioms usually expressing the notion of subalgebra of a given algebraic structure.\n\nConsider a first order language for algebraic structures with a monadic predicate symbol S. Then a \"fuzzy subalgebra\" is a fuzzy model of a theory containing, for any \"n\"-ary operation h, the axioms\n\nformula_1\n\nand, for any constant c, S(c).\n\nThe first axiom expresses the closure of S with respect to the operation h, and the second expresses the fact that c is an element in S. As an example, assume that the valuation structure is defined in [0,1] and denote by formula_2 the operation in [0,1] used to interpret the conjunction. Then a fuzzy subalgebra of an algebraic structure whose domain is D is defined by a fuzzy subset of D such that, for every d...,d in D, if h is the interpretation of the n-ary operation symbol h, then\n\n\nMoreover, if c is the interpretation of a constant c such that s(c) = 1.\n\nA largely studied class of fuzzy subalgebras is the one in which the operation formula_2 coincides with the minimum. In such a case it is immediate to prove the following proposition.\n\nProposition. A fuzzy subset s of an algebraic structure defines a fuzzy subalgebra if and only if for every λ in [0,1], the closed cut {x ∈ D : s(x)≥ λ} of s is a subalgebra.\n\nThe fuzzy subgroups and the fuzzy submonoids are particularly interesting classes of fuzzy subalgebras. In such a case a fuzzy subset \"s\" of a monoid (M,•,u) is a fuzzy submonoid if and only if\n\n\nwhere u is the neutral element in A.\n\nGiven a group G, a fuzzy subgroup of G is a fuzzy submonoid s of G such that \n\nIt is possible to prove that the notion of fuzzy subgroup is strictly related with the notions of fuzzy equivalence. In fact, assume that S is a set, G a group of transformations in S and (G,s) a fuzzy subgroup of G. Then, by setting \n\n\nwe obtain a fuzzy equivalence. Conversely, let e be a fuzzy equivalence in S and, for every transformation h of S, set \n\n\nThen s defines a fuzzy subgroup of transformation in S. In a similar way we can relate the fuzzy submonoids with the fuzzy orders.\n\n"}
{"id": "2440046", "url": "https://en.wikipedia.org/wiki?curid=2440046", "title": "Giuseppe Campani", "text": "Giuseppe Campani\n\nGiuseppe Campani (1635–July 28, 1715) was an Italian optician and astronomer who lived in Rome during the latter half of the 17th century.\n\nGiuseppe Campani was born in 1635.\nHe was an Umbrian from Castel San Felice near Spoleto.\nHis lenses and telescopes, made in Rome, were sent as far as Florence and Paris.\nCampani was known as the best maker of optical instruments of his age.\nHis brother, Matteo Campani-Alimenis, and he were experts in grinding and polishing lenses, especially for very long focal length aerial telescope objectives.\nHis brother is also noted as a mechanician for his work on clocks. He was a priest in charge of a parish in Rome.\n\nLouis XIV of France ordered several long-focus lenses (86, 100, 136 feet respectively) for the astronomer Giovanni Domenico Cassini.\nWith these Cassini found several moons of the planet Saturn, among other discoveries.\nConstantijn Huygens, Jr., brother of Christiaan Huygens, acquired one of Campani's telescopes.\nWhile in London in 1689 he ordered a new tube for the instrument from the instrument maker John Marshall.\n\nCampani made many observations himself. Cassini called his attention to the spots on Jupiter, and he disputed with Eustachio Divini, an Italian optician, the priority of their discovery. His astronomical observations and his descriptions of his telescopes are detailed in the following papers: \"Ragguaglio di due nuovi osservazioni, una celeste in ordine alla stella di Saturno, e terrestre l'altra in ordine agl' instrumenti\" (Rome, 1664, and again in 1665); \"Lettere di G. C. al sig. Giovanni Domenico Cassini intorno alle ombre delle stelle Medicee nel volto di Giove, ed altri nuovi fenomeni celesti scoperti co' suoi occhiali\" (Rome, 1666).\n\nCampani's entire workshop was donated to the \"Gabinetto di Fisica\" of the Academy of Sciences of Bologna Institute in 1747.\nHis telescopes were in length.\nA telescope by Campani was tested in 1871 and was found to provide good definition and a flat field, with a magnification of about 20 times. \nA tripod compound monocular microscope made by Campani is held in the Billings microscope collection at Walter Reed Army Medical Center in Washington, D.C..\n\nCitations\n\nSources\n\nFurther reading\n"}
{"id": "26736573", "url": "https://en.wikipedia.org/wiki?curid=26736573", "title": "Glossary of mammalian dental topography", "text": "Glossary of mammalian dental topography\n\nMany different terms have been proposed for features of the tooth crown in mammals.\n\n"}
{"id": "42963391", "url": "https://en.wikipedia.org/wiki?curid=42963391", "title": "Grapevine leafroll-associated virus 1", "text": "Grapevine leafroll-associated virus 1\n\nGrapevine leafroll-associated virus 1 (GRLaV-1) is a virus infecting grapevine in the genus \"Closterovirus\".\n\n\n"}
{"id": "31950695", "url": "https://en.wikipedia.org/wiki?curid=31950695", "title": "Interessement", "text": "Interessement\n\nThe term 'interessement' is French-English, and is synonymous with the word 'interposition'. It was first used by Michel Callon. It is used within the scientific tradition known as actor-network theory, in association with translation and the formation of networks. Various devices can be used in the interessement phase of a translation process, to strengthen the association between actors, and support the structure of the network.\n"}
{"id": "53836796", "url": "https://en.wikipedia.org/wiki?curid=53836796", "title": "Jiří Bičák", "text": "Jiří Bičák\n\nJiří Bičák (born 1942) is a Czech physicist, currently at Charles University and formerly a President of the Learned Society of the Czech Republic.\n"}
{"id": "40844467", "url": "https://en.wikipedia.org/wiki?curid=40844467", "title": "Kenneth Kermack", "text": "Kenneth Kermack\n\nKenneth A. Kermack (1919 – 2000) was a British palaeontologist at University College London most notable for his work on early mammals with his wife, Doris Mary Kermack.\n\nAmong Kermack's other significant contributions was the observation that \"Diplodocus\" could not have had an aquatic lifestyle because sheer water pressure alone on its chest would have prevented it breathing whilst submerged.\n\nHe first described the early mammal \"Aegialodon dawsoni\" from a molar tooth and the docodont \"Simpsonodon oxfordensis\".\nHe was also interested in astronomy, elected a member of the British Astronomical Association 1966 February 23, a member until his death.\n\n"}
{"id": "849815", "url": "https://en.wikipedia.org/wiki?curid=849815", "title": "Kepler (spacecraft)", "text": "Kepler (spacecraft)\n\nKepler is a retired space observatory launched by NASA to discover Earth-size planets orbiting other stars. Named after astronomer Johannes Kepler, the spacecraft was launched on March 7, 2009, into an Earth-trailing heliocentric orbit. The principal investigator was William J. Borucki. After nine years of operation, the telescope's reaction control system fuel was depleted, and NASA announced its retirement on October 30, 2018.\n\nDesigned to survey a portion of Earth's region of the Milky Way to discover Earth-size exoplanets in or near habitable zones and estimate how many of the billions of stars in the Milky Way have such planets, \"Kepler\" sole scientific instrument is a photometer that continually monitored the brightness of approx 150,000 main sequence stars in a fixed field of view. These data are transmitted to Earth, then analyzed to detect periodic dimming caused by exoplanets that cross in front of their host star. Only planets whose orbits are seen edge-on from Earth can be detected. During its over nine years of service, Kepler observed 530,506 stars and detected 2,662 planets.\n\"Kepler\" was part of NASA's Discovery Program of relatively low-cost science missions. The telescope's construction and initial operation were managed by NASA's Jet Propulsion Laboratory, with Ball Aerospace responsible for developing the \"Kepler\" flight system. The Ames Research Center is responsible for the ground system development, mission operations since December 2009, and scientific data analysis. The initial planned lifetime was 3.5 years, but greater-than-expected noise in the data, from both the stars and the spacecraft, meant additional time was needed to fulfill all mission goals. Initially, in 2012, the mission was expected to be extended until 2016, but on July 14, 2012, one of the spacecraft's four reaction wheels used for pointing the spacecraft stopped turning, and completing the mission would only be possible if all other reaction wheels remained reliable. Then, on May 11, 2013, a second reaction wheel failed, disabling the collection of science data and threatening the continuation of the mission.\n\nOn August 15, 2013, NASA announced that they had given up trying to fix the two failed reaction wheels. This meant the current mission needed to be modified, but it did not necessarily mean the end of planet hunting. NASA had asked the space science community to propose alternative mission plans \"potentially including an exoplanet search, using the remaining two good reaction wheels and thrusters\". On November 18, 2013, the \"K2\" \"Second Light\" proposal was reported. This would include utilizing the disabled \"Kepler\" in a way that could detect habitable planets around smaller, dimmer red dwarfs. On May 16, 2014, NASA announced the approval of the \"K2\" extension.\n\nBy January 2015, \"Kepler\" and its follow-up observations had found 1,013 confirmed exoplanets in about 440 star systems, along with a further 3,199 unconfirmed planet candidates. Four planets have been confirmed through \"Kepler\" K2 mission. In November 2013, astronomers estimated, based on \"Kepler\" space mission data, that there could be as many as 40 billion rocky Earth-size exoplanets orbiting in the habitable zones of Sun-like stars and red dwarfs within the Milky Way. It is estimated that 11 billion of these planets may be orbiting Sun-like stars. The nearest such planet may be away, according to the scientists.\n\nOn January 6, 2015, NASA announced the 1,000th confirmed exoplanet discovered by the \"Kepler\" space telescope. Four of the newly confirmed exoplanets were found to orbit within habitable zones of their related stars: three of the four, Kepler-438b, Kepler-442b and Kepler-452b, are almost Earth-size and likely rocky; the fourth, Kepler-440b, is a super-Earth. On May 10, 2016, NASA verified 1,284 new exoplanets found by \"Kepler\", the single largest finding of planets to date.\n\n\"Kepler\" data has also helped scientists observe and understand supernovae; measurements were collected every half-hour so the light curves were especially useful for studying these types of astronomical events.\n\nOn October 30, 2018, after the spacecraft ran out of fuel, NASA announced that the telescope would be retired. The telescope was shut down the same day, bringing an end to its nine-year service. \"Kepler\" observed 530,506 stars and discovered 2,662 exoplanets over its lifetime. A newer NASA mission, TESS, launched in 2018, is continuing the search for exoplanets.\n\nThe spacecraft has a mass of and contains a Schmidt camera with a front corrector plate (lens) feeding a primary mirror—at the time of its launch this was the largest mirror on any telescope outside Earth orbit, though the Herschel Space Observatory took this title a few months later. Its telescope has a 115 deg (about 12-degree diameter) field of view (FOV), roughly equivalent to the size of one's fist held at arm's length. Of this, 105 deg is of science quality, with less than 11% vignetting. The photometer has a soft focus to provide excellent photometry, rather than sharp images. The mission goal was a combined differential photometric precision (CDPP) of 20 ppm for a \"m\"(V)=12 Sun-like star for a 6.5-hour integration, though the observations fell short of this objective (see mission status).\n\nThe focal plane of the spacecraft's camera is made out of forty-two CCDs at 2200×1024 pixels each, possessing a total resolution of 94.6 megapixels, which at the time made it the largest camera system launched into space. The array was cooled by heat pipes connected to an external radiator. The CCDs were read out every 6.5 seconds (to limit saturation) and co-added on board for 58.89 seconds for short cadence targets, and 1765.5 seconds (29.4 minutes) for long cadence targets.<ref name=\"Kepler/K2 Data Products\"></ref> Due to the larger bandwidth requirements for the former, these were limited in number to 512 compared to 170,000 for long cadence. However, even though at launch \"Kepler\" had the highest data rate of any NASA mission, the 29-minute sums of all 95 million pixels constituted more data than could be stored and sent back to Earth. Therefore, the science team pre-selected the relevant pixels associated with each star of interest, amounting to about 6 percent of the pixels (5.4 megapixels). The data from these pixels was then requantized, compressed and stored, along with other auxiliary data, in the on-board 16 gigabyte solid-state recorder. Data that was stored and downlinked includes science stars, p-mode stars, smear, black level, background and full field-of-view images.\n\nThe \"Kepler\" primary mirror is in diameter. Manufactured by glass maker Corning using ultra-low expansion (ULE) glass, the mirror is specifically designed to have a mass only 14% that of a solid mirror of the same size. In order to produce a space telescope system with sufficient sensitivity to detect relatively small planets, as they pass in front of stars, a very high reflectance coating on the primary mirror was required. Using ion assisted evaporation, Surface Optics Corp. applied a protective nine-layer silver coating to enhance reflection and a dielectric interference coating to minimize the formation of color centers and atmospheric moisture absorption.\n\nIn terms of photometric performance, \"Kepler\" has worked well, much better than any Earth-bound telescope, but short of design goals. The objective was a combined differential photometric precision (CDPP) of 20 parts per million (PPM) on a magnitude 12 star for a 6.5-hour integration. This estimate was developed allowing 10 ppm for stellar variability, roughly the value for the Sun. The obtained accuracy for this observation has a wide range, depending on the star and position on the focal plane, with a median of 29 ppm. Most of the additional noise appears to be due to a larger-than-expected variability in the stars themselves (19.5 ppm as opposed to the assumed 10.0 ppm), with the rest due to instrumental noise sources slightly larger than predicted.\n\nBecause decrease in brightness from an Earth-size planet transiting a Sun-like star is so small, only 80 ppm, the increased noise means each individual transit is only a 2.7 σ event, instead of the intended 4 σ. This, in turn, means more transits must be observed to be sure of a detection. Scientific estimates indicated that a mission lasting 7 to 8 years, as opposed to the originally planned 3.5 years, would be needed to find all transiting Earth-sized planets. On April 4, 2012, the \"Kepler\" mission was approved for extension through the fiscal year 2016, but this also depended on all remaining reaction wheels staying healthy, which turned out not to be the case (see Spacecraft history below).\n\n\"Kepler\" orbits the Sun, which avoids Earth occultations, stray light, and gravitational perturbations and torques inherent in an Earth orbit.\n\nNASA has characterized \"Kepler\" orbit as \"Earth-trailing\". With an orbital period of 372.5 days, \"Kepler\" is slowly falling farther behind Earth (about 16 million miles per annum). , the distance to \"Kepler\" from Earth was about .\nThis means that after about 26 years \"Kepler\" will reach the other side of the sun and will get back to the neighborhood of the earth after 51 years.\n\nUntil 2013 the photometer pointed to a field in the northern constellations of Cygnus, Lyra and Draco, which is well out of the ecliptic plane, so that sunlight never enters the photometer as the spacecraft orbits. This is also the direction of the Solar System's motion around the center of the galaxy. Thus, the stars which \"Kepler\" observed are roughly the same distance from the galactic center as the Solar System, and also close to the galactic plane. This fact is important if position in the galaxy is related to habitability, as suggested by the Rare Earth hypothesis.\n\nOrientation is 3-axis stabilised by sensing rotations using fine-guidance sensors located on the instrument focal plane (instead of rate sensing gyroscopes, eg. as used on Hubble). and using reaction wheels to control the orientation.\n\n\"Kepler\" is operated out of Boulder, Colorado, by the Laboratory for Atmospheric and Space Physics (LASP) under contract to Ball Aerospace & Technologies. The spacecraft's solar array is rotated to face the Sun at the solstices and equinoxes, so as to optimize the amount of sunlight falling on the solar array and to keep the heat radiator pointing towards deep space. Together, LASP and Ball Aerospace control the spacecraft from a mission operations center located on the research campus of the University of Colorado. LASP performs essential mission planning and the initial collection and distribution of the science data. The mission's initial life-cycle cost was estimated at US$600 million, including funding for 3.5 years of operation. In 2012, NASA announced that the \"Kepler\" mission would be funded until 2016 at a cost of about $20 million per year.\n\nNASA contacts the spacecraft using the X band communication link twice a week for command and status updates. Scientific data are downloaded once a month using the K band link at a maximum data transfer rate of approximately 550 kB/s. The high gain antenna is not steerable so data collection is interrupted for a day to reorient the whole spacecraft and the high gain antenna for communications to Earth. \n\nThe \"Kepler\" spacecraft conducts its own partial analysis on board and only transmits scientific data deemed necessary to the mission in order to conserve bandwidth.\n\nScience data telemetry collected during mission operations at LASP is sent for processing to the \"Kepler\" Data Management Center (DMC) which is located at the Space Telescope Science Institute on the campus of Johns Hopkins University in Baltimore, Maryland. The science data telemetry is decoded and processed into uncalibrated FITS-format science data products by the DMC, which are then passed along to the Science Operations Center (SOC) at NASA Ames Research Center, for calibration and final processing. The SOC at NASA Ames Research Center (ARC) develops and operates the tools needed to process scientific data for use by the \"Kepler\" Science Office (SO). Accordingly, the SOC develops the pipeline data processing software based on scientific algorithms developed jointly by the SO and SOC. During operations, the SOC:\nThe SOC also evaluates the photometric performance on an ongoing basis and provides the performance metrics to the SO and Mission Management Office. Finally, the SOC develops and maintains the project's scientific databases, including catalogs and processed data. The SOC finally returns calibrated data products and scientific results back to the DMC for long-term archiving, and distribution to astronomers around the world through the Multimission Archive at STScI (MAST).\n\nOn July 14, 2012, one of the four reaction wheels used for fine pointing of the spacecraft failed. While \"Kepler\" requires only three reaction wheels to accurately aim the telescope, another failure would leave the spacecraft unable to aim at its original field.\n\nAfter showing some problems in January 2013, a second reaction wheel failed on May 11, 2013, ending \"Kepler\" primary mission. The spacecraft was put into safe mode, then from June to August 2013 a series of engineering tests were done to try to recover either failed wheel. By August 15, 2013, it was decided that the wheels were unrecoverable, and an engineering report was ordered to assess the spacecraft's remaining capabilities.\n\nThis effort ultimately led to the \"K2\" follow-on mission observing different fields near the ecliptic.\n\nIn January 2006, the project's launch was delayed eight months because of budget cuts and consolidation at NASA. It was delayed again by four months in March 2006 due to fiscal problems. At this time, the high-gain antenna was changed from a gimbal-led design to one fixed to the frame of the spacecraft to reduce cost and complexity, at the cost of one observation day per month.\n\nThe \"Kepler\" observatory was launched on March 7, 2009, at 03:49:57 UTC aboard a Delta II rocket from Cape Canaveral Air Force Station, Florida. The launch was a success and all three stages were completed by 04:55 UTC. The cover of the telescope was jettisoned on April 7, 2009, and the first light images were taken on the next day.\n\nOn April 20, 2009, it was announced that the \"Kepler\" science team had concluded that further refinement of the focus would dramatically increase the scientific return. On April 23, 2009, it was announced that the focus had been successfully optimized by moving the primary mirror 40 micrometers (1.6 thousandths of an inch) towards the focal plane and tilting the primary mirror 0.0072 degree.\n\nOn May 13, 2009, at 00:01 UTC, \"Kepler\" successfully completed its commissioning phase and began its search for planets around other stars.\n\nOn June 19, 2009, the spacecraft successfully sent its first science data to Earth. It was discovered that \"Kepler\" had entered safe mode on June 15. A second safe mode event occurred on July 2. In both cases the event was triggered by a \"processor reset\". The spacecraft resumed normal operation on July 3 and the science data that had been collected since June 19 was downlinked that day. On October 14, 2009, the cause of these safing events was determined to be a low voltage power supply that provides power to the RAD750 processor. On January 12, 2010, one portion of the focal plane transmitted anomalous data, suggesting a problem with focal plane MOD-3 module, covering two out of \"Kepler\" 42 CCDs. , the module was described as \"failed\", but the coverage still exceeded the science goals.\n\n\"Kepler\" downlinked roughly twelve gigabytes of data about once per month—an example of such a downlink was on November 22–23, 2010.\n\n\"Kepler\" has a fixed field of view (FOV) against the sky. The diagram to the right shows the celestial coordinates and where the detector fields are located, along with the locations of a few bright stars with celestial north at the top left corner. The mission website has a calculator that will determine if a given object falls in the FOV, and if so, where it will appear in the photo detector output data stream. Data on exoplanet candidates is submitted to the \"Kepler\" Follow-up Program, or KFOP, to conduct follow-up observations.\n\n\"Kepler\" field of view covers 115 square degrees, around 0.25 percent of the sky, or \"about two scoops of the Big Dipper\". Thus, it would require around 400 \"Kepler\"-like telescopes to cover the whole sky. The \"Kepler\" field contains portions of the constellations Cygnus, Lyra, and Draco.\n\nThe nearest star system in \"Kepler\"'s field of view is the trinary star system Gliese 1245, 15 light years from the Sun. The brown dwarf WISE J2000+3629, 22.8 ± 1 light years from the Sun is also in the field of view, but is invisible to Kepler due to emitting light primarily in infrared wavelengths.\n\nThe scientific objective of \"Kepler\" is to explore the structure and diversity of planetary systems. This spacecraft observes a large sample of stars to achieve several key goals:\n\n\nMost of the exoplanets previously detected by other projects were giant planets, mostly the size of Jupiter and bigger. \"Kepler\" is designed to look for planets 30 to 600 times less massive, closer to the order of Earth's mass (Jupiter is 318 times more massive than Earth). The method used, the transit method, involves observing repeated transit of planets in front of their stars, which causes a slight reduction in the star's apparent magnitude, on the order of 0.01% for an Earth-size planet. The degree of this reduction in brightness can be used to deduce the diameter of the planet, and the interval between transits can be used to deduce the planet's orbital period, from which estimates of its orbital semi-major axis (using Kepler's laws) and its temperature (using models of stellar radiation) can be calculated.\n\nThe probability of a random planetary orbit being along the line-of-sight to a star is the diameter of the star divided by the diameter of the orbit. For an Earth-size planet at 1 AU transiting a Sun-like star the probability is 0.47%, or about 1 in 210. For a planet like Venus orbiting a Sun-like star the probability is slightly higher, at 0.65%; If the host star has multiple planets, the probability of additional detections is higher than the probability of initial detection assuming planets in a given system tend to orbit in similar planes—an assumption consistent with current models of planetary system formation. For instance, if a \"Kepler\"-like mission conducted by aliens observed Earth transiting the Sun, there is a 12% chance that it would also see Venus transiting.\n\n\"Kepler\" 115 deg field of view gives it a much higher probability of detecting Earth-sized planets than the Hubble Space Telescope, which has a field of view of only 10 sq. arc-minutes. Moreover, \"Kepler\" is dedicated to detecting planetary transits, while the Hubble Space Telescope is used to address a wide range of scientific questions, and rarely looks continuously at just one starfield. Of the approximately half-million stars in \"Kepler\" field of view, around 150,000 stars were selected for observation. More than 90,000 are G-type stars on, or near, the main sequence. Thus, \"Kepler\" was designed to be sensitive to wavelengths of 400–865 nm where brightness of those stars peaks. Most of the stars observed by \"Kepler\" have apparent visual magnitude between 14 and 16 but the brightest observed stars have apparent visual magnitude of 8 or lower. Most of the planet candidates were initially not expected to be confirmed due to being too faint for follow-up observations. All the selected stars are observed simultaneously, with the spacecraft measuring variations in their brightness every thirty minutes. This provides a better chance for seeing a transit. The mission was designed to maximize the probability of detecting planets orbiting other stars.\n\nBecause \"Kepler\" must observe at least three transits to confirm that the dimming of a star was caused by a transiting planet, and because larger planets give a signal that is easier to check, scientists expected the first reported results to be larger Jupiter-size planets in tight orbits. The first of these were reported after only a few months of operation. Smaller planets, and planets farther from their sun would take longer, and discovering planets comparable to Earth were expected to take three years or longer.\n\nData collected by \"Kepler\" is also being used for studying variable stars of various types and performing asteroseismology, particularly on stars showing solar-like oscillations.\n\nOnce \"Kepler\" has collected and sent back the data, raw light curves are constructed. Brightness values are then adjusted to take the brightness variations due to the rotation of the spacecraft into account. The next step is processing (folding) light curves into a more easily observable form and letting software select signals that seem potentially transit-like. At this point, any signal that shows potential transit-like features is called a threshold crossing event. These signals are individually inspected in two inspection rounds, with the first round taking only a few seconds per target. This inspection eliminates erroneously selected non-signals, signals caused by instrumental noise and obvious eclipsing binaries.\n\nThreshold crossing events that pass these tests are called \"Kepler\" Objects of Interest (KOI), receive a KOI designation and are archived. KOIs are inspected more thoroughly in a process called dispositioning. Those which pass the dispositioning are called \"Kepler\" planet candidates. The KOI archive is not static, meaning that a \"Kepler\" candidate could end up in the false-positive list upon further inspection. In turn, KOIs that were mistakenly classified as false positives could end up back in the candidates list.\n\nNot all the planet candidates go through this process. Circumbinary planets do not show strictly periodic transits, and have to be inspected through other methods. In addition, third-party researchers use different data-processing methods, or even search planet candidates from the unprocessed light curve data. As a consequence, those planets may be missing KOI designation.\n\nOnce suitable candidates have been found from \"Kepler\" data, it is necessary to rule out false positives with follow-up tests.\n\nUsually, \"Kepler\" candidates are imaged individually with more-advanced ground-based telescopes in order to resolve any background objects which could contaminate the brightness signature of the transit signal. Another method to rule out planet candidates is astrometry for which \"Kepler\" can collect good data even though doing so was not a design goal. While \"Kepler\" cannot detect planetary-mass objects with this method, it can be used to determine if the transit was caused by a stellar-mass object.\n\nThere are a few different exoplanet detection methods which help to rule out false positives by giving further proof that a candidate is a real planet. One of the methods, called doppler spectroscopy, requires follow-up observations from ground-based telescopes. This method works well if the planet is massive or is located around a relatively bright star. While current spectrographs are insufficient for confirming planetary candidates with small masses around relatively dim stars, this method can be used to discover additional massive non-transiting planet candidates around targeted stars.\n\nIn multiplanetary systems, planets can often be confirmed through transit timing variation by looking at the time between successive transits, which may vary if planets are gravitationally perturbed by each other. This helps to confirm relatively low-mass planets even when the star is relatively distant. Transit timing variations indicate that two or more planets belong to the same planetary system. There are even cases where a non-transiting planet is also discovered in this way.\n\nCircumbinary planets show much larger transit timing variations between transits than planets gravitationally disturbed by other planets. Their transit duration times also vary significantly. Transit timing and duration variations for circumbinary planets are caused by the orbital motion of the host stars, rather than by other planets. In addition, if the planet is massive enough, it can cause slight variations of the host stars' orbital periods. Despite being harder to find circumbinary planets due to their non-periodic transits, it is much easier to confirm them, as timing patterns of transits cannot be mimicked by an eclipsing binary or a background star system.\n\nIn addition to transits, planets orbiting around their stars undergo reflected-light variations—like the Moon, they go through phases from full to new and back again. Because \"Kepler\" cannot resolve the planet from the star, it sees only the combined light, and the brightness of the host star seems to change over each orbit in a periodic manner. Although the effect is small—the photometric precision required to see a close-in giant planet is about the same as to detect an Earth-sized planet in transit across a solar-type star—Jupiter-sized planets with an orbital period of a few days or less are detectable by sensitive space telescopes such as \"Kepler\". In the long run, this method may help find more planets than the transit method, because the reflected light variation with orbital phase is largely independent of the planet's orbital inclination, and does not require the planet to pass in front of the disk of the star. In addition, the phase function of a giant planet is also a function of its thermal properties and atmosphere, if any. Therefore, the phase curve may constrain other planetary properties, such as the particle size distribution of the atmospheric particles.\n\n\"Kepler\" photometric precision is often high enough to observe a star's brightness changes caused by doppler beaming or a star's shape deformation by a companion. These can sometimes be used to rule out hot Jupiter candidates as false positives caused by a star or a brown dwarf when these effects are too noticeable. However, there are some cases where such effects are detected even by planetary-mass companions such as TrES-2b.\n\nIf a planet cannot be detected through at least one of the other detection methods, it can be confirmed by determining if the possibility of a \"Kepler\" candidate being a real planet is significantly larger than any false-positive scenarios combined. One of the first methods was to see if other telescopes can see the transit as well. The first planet confirmed through this method was Kepler-22b which was also observed with a Spitzer space telescope in addition to analyzing any other false-positive possibilities. Such confirmation is costly, as small planets can generally be detected only with space telescopes.\n\nIn 2014, a new confirmation method called \"validation by multiplicity\" was announced. From the planets previously confirmed through various methods, it was found that planets in most planetary systems orbit in a relatively flat plane, similar to the planets found in the Solar System. This means that if a star has multiple planet candidates, it is very likely a real planetary system. Transit signals still need to meet several criteria which rule out false-positive scenarios. For instance, it has to have considerable signal-to-noise ratio, it has at least three observed transits, orbital stability of those systems have to be stable and transit curve has to have a shape that partly eclipsing binaries could not mimic the transit signal. In addition, its orbital period needs to be 1.6 days or longer to rule out common false positives caused by eclipsing binaries. Validation by multiplicity method is very efficient and allows to confirm hundreds of \"Kepler\" candidates in a relatively short amount of time.\n\nA new validation method using a tool called PASTIS has been developed. It makes it possible to confirm a planet even when only a single candidate transit event for the host star has been detected. A drawback of this tool is that it requires a relatively high signal-to-noise ratio from \"Kepler\" data, so it can mainly confirm only larger planets or planets around quiet and relatively bright stars. Currently, the analysis of \"Kepler\" candidates through this method is underway. PASTIS was first successful for validating the planet Kepler-420b.\n\nThe \"Kepler\" observatory was in active operation from 2009 through 2013, with the first main results announced on January 4, 2010. As expected, the initial discoveries were all short-period planets. As the mission continued, additional longer-period candidates were found. , \"Kepler\" has discovered 5,011 exoplanet candidates and 2,512 confirmed exoplanets.\n\nNASA held a press conference to discuss early science results of the \"Kepler\" mission on August 6, 2009. At this press conference, it was revealed that \"Kepler\" had confirmed the existence of the previously known transiting exoplanet HAT-P-7b, and was functioning well enough to discover Earth-size planets.\n\nBecause \"Kepler\" detection of planets depends on seeing very small changes in brightness, stars that vary in brightness by themselves (variable stars) are not useful in this search. From the first few months of data, \"Kepler\" scientists determined that about 7,500 stars from the initial target list are such variable stars. These were dropped from the target list, and replaced by new candidates. On November 4, 2009, the \"Kepler\" project publicly released the light curves of the dropped stars.\n\nThe first six weeks of data revealed five previously unknown planets, all very close to their stars. Among the notable results are one of the least dense planets yet found, two low-mass white dwarfs that were initially reported as being members of a new class of stellar objects, and Kepler-16b, a well-characterized planet orbiting a binary star.\n\nOn June 15, 2010, the \"Kepler\" mission released data on all but 400 of the ~156,000 planetary target stars to the public. 706 targets from this first data set have viable exoplanet candidates, with sizes ranging from as small as Earth to larger than Jupiter. The identity and characteristics of 306 of the 706 targets were given. The released targets included five candidate multi-planet systems, including six extra exoplanet candidates. Only 33.5 days of data were available for most of the candidates. NASA also announced data for another 400 candidates were being withheld to allow members of the \"Kepler\" team to perform follow-up observations. The data for these candidates was published February 2, 2011. (See the \"Kepler\" results for 2011 below.)\n\nThe \"Kepler\" results, based on the candidates in the list released in 2010, implied that most candidate planets have radii less than half that of Jupiter. The results also imply that small candidate planets with periods less than thirty days are much more common than large candidate planets with periods less than thirty days and that the ground-based discoveries are sampling the large-size tail of the size distribution. This contradicted older theories which had suggested small and Earth-size planets would be relatively infrequent. Based on extrapolations from the \"Kepler\" data, an estimate of around 100 million habitable planets in the Milky Way may be realistic. Some media reports of the TED talk have led to the misunderstanding that \"Kepler\" had actually found these planets. This was clarified in a letter to the Director of the NASA Ames Research Center, for the \"Kepler\" Science Council dated August 2, 2010 states, \"Analysis of the current \"Kepler\" data does not support the assertion that \"Kepler\" has found any Earth-like planets.\"\n\nIn 2010, \"Kepler\" identified two systems containing objects which are smaller and hotter than their parent stars: KOI 74 and KOI 81. These objects are probably low-mass white dwarfs produced by previous episodes of mass transfer in their systems.\n\nOn February 2, 2011, the \"Kepler\" team announced the results of analysis of the data taken between 2 May and September 16, 2009. They found 1235 planetary candidates circling 997 host stars. (The numbers that follow assume the candidates are really planets, though the official papers called them only candidates. Independent analysis indicated that at least 90% of them are real planets and not false positives). 68 planets were approximately Earth-size, 288 super-Earth-size, 662 Neptune-size, 165 Jupiter-size, and 19 up to twice the size of Jupiter. In contrast to previous work, roughly 74% of the planets are smaller than Neptune, most likely as a result of previous work finding large planets more easily than smaller ones.\n\nThat February 2, 2011 release of 1235 exoplanet candidates included 54 that may be in the \"habitable zone\", including five less than twice the size of Earth. There were previously only two planets thought to be in the \"habitable zone\", so these new findings represent an enormous expansion of the potential number of \"Goldilocks planets\" (planets of the right temperature to support liquid water). All of the habitable zone candidates found thus far orbit stars significantly smaller and cooler than the Sun (habitable candidates around Sun-like stars will take several additional years to accumulate the three transits required for detection). Of all the new planet candidates, 68 are 125% of Earth's size or smaller, or smaller than all previously discovered exoplanets. \"Earth-size\" and \"super-Earth-size\" is defined as \"less than or equal to 2 Earth radii (Re)\" [(or, Rp ≤ 2.0 Re) – Table 5]. Six such planet candidates [namely: KOI 326.01 (Rp=0.85), KOI 701.03 (Rp=1.73), KOI 268.01 (Rp=1.75), KOI 1026.01 (Rp=1.77), KOI 854.01 (Rp=1.91), KOI 70.03 (Rp=1.96) – Table 6] are in the \"habitable zone.\" A more recent study found that one of these candidates (KOI 326.01) is in fact much larger and hotter than first reported.\n\nThe frequency of planet observations was highest for exoplanets two to three times Earth-size, and then declined in inverse proportionality to the area of the planet. The best estimate (as of March 2011), after accounting for observational biases, was: 5.4% of stars host Earth-size candidates, 6.8% host super-Earth-size candidates, 19.3% host Neptune-size candidates, and 2.55% host Jupiter-size or larger candidates. Multi-planet systems are common; 17% of the host stars have multi-candidate systems, and 33.9% of all the planets are in multiple planet systems.\n\nBy December 5, 2011, the \"Kepler\" team announced that they had discovered 2,326 planetary candidates, of which 207 are similar in size to Earth, 680 are super-Earth-size, 1,181 are Neptune-size, 203 are Jupiter-size and 55 are larger than Jupiter. Compared to the February 2011 figures, the number of Earth-size and super-Earth-size planets increased by 200% and 140% respectively. Moreover, 48 planet candidates were found in the habitable zones of surveyed stars, marking a decrease from the February figure; this was due to the more stringent criteria in use in the December data.\n\nOn December 20, 2011, the \"Kepler\" team announced the discovery of the first Earth-size exoplanets, Kepler-20e and Kepler-20f, orbiting a Sun-like star, Kepler-20.\n\nBased on \"Kepler\" findings, astronomer Seth Shostak estimated in 2011 that \"within a thousand light-years of Earth\", there are \"at least 30,000\" habitable planets. Also based on the findings, the \"Kepler\" team has estimated that there are \"at least 50 billion planets in the Milky Way\", of which \"at least 500 million\" are in the habitable zone. In March 2011, astronomers at NASA's Jet Propulsion Laboratory (JPL) reported that about \"1.4 to 2.7 percent\" of all Sun-like stars are expected to have Earth-size planets \"within the habitable zones of their stars\". This means there are \"two billion\" of these \"Earth analogs\" in the Milky Way alone. The JPL astronomers also noted that there are \"50 billion other galaxies\", potentially yielding more than one sextillion \"Earth analog\" planets if all galaxies have similar numbers of planets to the Milky Way.\n\nIn January 2012, an international team of astronomers reported that each star in the Milky Way may host \"on average...at least 1.6 planets\", suggesting that over 160 billion star-bound planets may exist in the Milky Way. \"Kepler\" also recorded distant stellar super-flares, some of which are 10,000 times more powerful than the 1859 Carrington event. The superflares may be triggered by close-orbiting Jupiter-sized planets. The Transit Timing Variation (TTV) technique, which was used to discover Kepler-9d, gained popularity for confirming exoplanet discoveries. A planet in a system with four stars was also confirmed, the first time such a system had been discovered.\n\n, there were a total of 2,321 candidates. Of these, 207 are similar in size to Earth, 680 are super-Earth-size, 1,181 are Neptune-size, 203 are Jupiter-size and 55 are larger than Jupiter. Moreover, 48 planet candidates were found in the habitable zones of surveyed stars. The \"Kepler\" team estimated that 5.4% of all stars host Earth-size planet candidates, and that 17% of all stars have multiple planets.\n\nAccording to a study by Caltech astronomers published in January 2013, the Milky Way contains at least as many planets as it does stars, resulting in 100–400 billion exoplanets. The study, based on planets orbiting the star Kepler-32, suggests that planetary systems may be common around stars in the Milky Way. The discovery of 461 more candidates was announced on January 7, 2013. The longer \"Kepler\" watches, the more planets with long periods it can detect.\n\nA candidate, newly announced on January 7, 2013, was Kepler-69c (formerly, \"KOI-172.02\"), an Earth-size exoplanet orbiting a star similar to the Sun in the habitable zone and possibly habitable.\n\nIn April 2013, a white dwarf was discovered bending the light of its companion red dwarf in the KOI-256 star system.\n\nIn April 2013, NASA announced the discovery of three new Earth-size exoplanets—Kepler-62e, Kepler-62f, and Kepler-69c—in the habitable zones of their respective host stars, Kepler-62 and Kepler-69. The new exoplanets are considered prime candidates for possessing liquid water and thus a habitable environment. A more recent analysis has shown that Kepler-69c is likely more analogous to Venus, and thus unlikely to be habitable.\n\nOn May 15, 2013, NASA announced the spacecraft had been crippled by failure of a reaction wheel that keeps it pointed in the right direction. A second wheel had previously failed, and the spacecraft requires three wheels (out of four total) to be operational for the instrument to function properly. Further testing in July and August determined that while \"Kepler\" was capable of using its damaged reaction wheels to prevent itself from entering safe mode and of downlinking previously collected science data it was not capable of collecting further science data as previously configured. Scientists working on the \"Kepler\" project said there was a backlog of data still to be looked at, and that more discoveries would be made in the following couple of years, despite the setback.\n\nAlthough no new science data from \"Kepler\" field had been collected since the problem, an additional sixty-three candidates were announced in July 2013 based on the previously collected observations.\n\nIn November 2013, the second \"Kepler\" science conference was held. The discoveries included the median size of planet candidates getting smaller compared to early 2013, preliminary results of the discovery of a few circumbinary planets and planets in the habitable zone.\n\nOn February 13, over 530 additional planet candidates were announced residing around single planet systems. Several of them were nearly Earth-sized and located in the habitable zone. This number was further increased by about 400 in June 2014.\n\nOn February 26, scientists announced that data from \"Kepler\" had confirmed the existence of 715 new exoplanets. A new statistical method of confirmation was used called \"verification by multiplicity\" which is based on how many planets around multiple stars were found to be real planets. This allowed much quicker confirmation of numerous candidates which are part of multiplanetary systems. 95% of the discovered exoplanets were smaller than Neptune and four, including Kepler-296f, were less than 2 1/2 the size of Earth and were in habitable zones where surface temperatures are suitable for liquid water.\n\nIn March, a study found that small planets with orbital periods of less than 1 day are usually accompanied by at least one additional planet with orbital period of 1–50 days. This study also noted that ultra-short period planets are almost always smaller than 2 Earth radii unless it is a misaligned hot Jupiter.\n\nOn April 17, the \"Kepler\" team announced the discovery of Kepler-186f, the first nearly Earth-sized planet located in the habitable zone. This planet orbits around a red dwarf.\n\nIn May 2014, K2 observations fields 0 to 13 were announced and described in detail. K2 observations began in June 2014.\n\nIn July 2014, the first discoveries from K2 field data were reported in the form of eclipsing binaries. Discoveries were derived from a \"Kepler\" engineering data set which was collected prior to campaign 0 in preparation to the main \"K2\" mission.\n\nOn September 23, 2014, NASA reported that the \"K2\" mission had completed campaign 1, the first official set of science observations, and that campaign 2 was underway.\n\nCampaign 3 lasted from November 14, 2014 to February 6, 2015 and included \"16,375 standard long cadence and 55 standard short cadence targets\".\n\n\nBy May 10, 2016, the Kepler mission had verified 1,284 new planets. Based on their size, about 550 could be rocky planets. Nine of these orbit in their stars' habitable zone:\n\"Kepler\" was launched in 2009. It was very successful at finding exoplanets, but failures in two of four reaction wheels crippled its extended mission in 2013. Without three functioning wheels, the telescope could not be pointed accurately. On October 30, 2018, NASA announced that the spacecraft was out of fuel and its mission was officially ended. \n\nIn April 2012, an independent panel of senior NASA scientists recommended that the \"Kepler\" mission be continued through 2016. According to the senior review, \"Kepler\" observations needed to continue until at least 2015 to achieve all the stated scientific goals. On November 14, 2012, NASA announced the completion of \"Kepler\" primary mission, and the beginning of its extended mission, which may last as long as four years.\n\nIn July 2012, one of \"Kepler\" four reaction wheels (wheel 2) failed. On May 11, 2013, a second wheel (wheel 4) failed, jeopardizing the continuation of the mission, as three wheels are necessary for its planet hunting. \"Kepler\" had not collected science data since May because it was not able to point with sufficient accuracy. On July 18 and 22 reaction wheels 4 and 2 were tested respectively; wheel 4 only rotated counter-clockwise but wheel 2 ran in both directions, albeit with significantly elevated friction levels. A further test of wheel 4 on July 25 managed to achieve bi-directional rotation. Both wheels, however, exhibited too much friction to be useful. On August 2, NASA put out a call for proposals to use the remaining capabilities of \"Kepler\" for other scientific missions. Starting on August 8, a full systems evaluation was conducted. It was determined that wheel 2 could not provide sufficient precision for scientific missions and the spacecraft was returned to a \"rest\" state to conserve fuel. Wheel 4 was previously ruled out because it exhibited higher friction levels than wheel 2 in previous tests. Sending astronauts to fix \"Kepler\" is not an option because it orbits the Sun and is millions of kilometers from Earth.\n\nOn August 15, 2013, NASA announced that \"Kepler\" would not continue searching for planets using the transit method after attempts to resolve issues with two of the four reaction wheels failed. An engineering report was ordered to assess the spacecraft's capabilities, its two good reaction wheels and its thrusters. Concurrently, a scientific study was conducted to determine whether enough knowledge can be obtained from \"Kepler\" limited scope to justify its $18 million per year cost.\n\nPossible ideas included searching for asteroids and comets, looking for evidence of supernovas, and finding huge exoplanets through gravitational microlensing. Another proposal was to modify the software on \"Kepler\" to compensate for the disabled reaction wheels. Instead of the stars being fixed and stable in \"Kepler\" field of view, they will drift. However, proposed software was to track this drift and more or less completely recover the mission goals despite being unable to hold the stars in a fixed view.\n\nPreviously collected data continues to be analyzed.\n\nIn November 2013, a new mission plan named \"K2\" \"Second Light\" was presented for consideration. \"K2\" would involve using \"Kepler\" remaining capability, photometric precision of about 300 parts per million, compared with about 20 parts per million earlier, to collect data for the study of \"supernova explosions, star formation and Solar-System bodies such as asteroids and comets, ... \" and for finding and studying more exoplanets. In this proposed mission plan, \"Kepler\" would search a much larger area in the plane of Earth's orbit around the Sun.\n\nIn early 2014, the spacecraft underwent successful testing for the \"K2\" mission. From March to May 2014, data from a new field called Field 0 was collected as a testing run. On May 16, 2014, NASA announced the approval of extending the \"Kepler\" mission to the \"K2\" mission. \"Kepler\" photometric precision for the \"K2\" mission was estimated to be 50 ppm on a magnitude 12 star for a 6.5-hour integration. In February 2014, photometric precision for the \"K2\" mission using two-wheel, fine-point precision operations was measured as 44 ppm on magnitude 12 stars for a 6.5-hour integration. The analysis of these measurements by NASA suggests the \"K2\" photometric precision approaches that of the \"Kepler\" archive of three-wheel, fine-point precision data.\n\nOn May 29, 2014, campaign fields 0 to 13 were reported and described in detail.\n\nField 1 of the \"K2\" mission is set towards the Leo-Virgo region of the sky, while Field 2 is towards the \"head\" area of Scorpius and includes two globular clusters, Messier 4 and Messier 80, and part of the Scorpius–Centaurus Association, which is only about 11 million years old and distant with probably over 1,000 members.\n\nOn December 18, 2014, NASA announced that the \"K2\" mission had detected its first confirmed exoplanet, a super-Earth named HIP 116454 b. Its signature was found in a set of engineering data meant to prepare the spacecraft for the full \"K2\" mission. Radial velocity follow-up observations were needed as only a single transit of the planet was detected.\n\nDuring a scheduled contact on April 7, 2016, \"Kepler\" was found to be operating in emergency mode, the lowest operational and most fuel intensive mode. Mission operations declared a spacecraft emergency, which afforded them priority access to NASA's Deep Space Network. By the evening of April 8 the spacecraft had been upgraded to safe mode, and on April 10 it was placed into point-rest state, a stable mode which provides normal communication and the lowest fuel burn. At that time, the cause of the emergency was unknown, but it was not believed that \"Kepler\" reaction wheels or a planned maneuver to support \"K2\" Campaign 9 were responsible. Operators downloaded and analyzed engineering data from the spacecraft, with the prioritization of returning to normal science operations. \"Kepler\" was returned to science mode on April 22. The emergency caused the first half of Campaign 9 to be shortened by two weeks.\n\nIn June 2016, NASA announced a K2 mission extension of three additional years, beyond the expected exhaustion of on-board fuel in 2018. In August 2018, NASA roused the spacecraft from sleep mode, applied a modified configuration to deal with thruster problems that degraded pointing performance, and began collecting scientific data for the 19th observation campaign, finding that the onboard fuel was not yet utterly exhausted.\n\nThe \"Kepler\" team originally promised to release data within one year of observations. However, this plan was changed after launch, with data being scheduled for release up to three years after its collection. This resulted in considerable criticism, leading the \"Kepler\" science team to release the third quarter of their data one year and nine months after collection. The data through September 2010 (quarters 4, 5, and 6) was made public in January 2012.\n\nPeriodically, the \"Kepler\" team releases a list of candidates (\"Kepler\" Objects of Interest, or KOIs) to the public. Using this information, a team of astronomers collected radial velocity data using the SOPHIE échelle spectrograph to confirm the existence of the candidate KOI-428b in 2010, later named Kepler-40b. In 2011, the same team confirmed candidate KOI-423b, later named Kepler-39b.\n\nSince December 2010, \"Kepler\" mission data has been used for the Planet Hunters project, which allows volunteers to look for transit events in the light curves of \"Kepler\" images to identify planets that computer algorithms might miss. By June 2011, users had found sixty-nine potential candidates that were previously unrecognized by the \"Kepler\" mission team. The team has plans to publicly credit amateurs who spot such planets.\n\nIn January 2012, the BBC program \"Stargazing Live\" aired a public appeal for volunteers to analyse Planethunters.org data for potential new exoplanets. This led two amateur astronomers—one in Peterborough, England—to discover a new Neptune-sized exoplanet, to be named Threapleton Holmes B. One hundred thousand other volunteers were also engaged in the search by late January, analyzing over one million \"Kepler\" images by early 2012. One such exoplanet, PH1b (or Kepler-64b from its Kepler designation), was discovered in 2012. A second exoplanet, PH2b (Kepler-86b) was discovered in 2013.\n\nIn April 2017, ABC \"Stargazing Live\", a variation of BBC \"Stargazing Live\", launched the Zooniverse project \"Exoplanet Explorers\". While Planethunters.org worked with archived data, Exoplanet Explorers used recently downlinked data from the K2 mission. On the first day of the project, 184 transit candidates were identified that passed simple tests. On the second day, the research team identified a star system, later named K2-138, with a Sun-like star and four super-Earths in a tight orbit. In the end, volunteers helped to identify 90 exoplanet candidates. The citizen scientists that helped discover the new star system will be added as co-authors in the research paper when published.\n\n In addition to discovering hundreds of exoplanet candidates, the \"Kepler\" spacecraft has also reported twenty-six exoplanets in eleven systems that have not yet been added to the Extrasolar Planet Database. Exoplanets discovered using \"Kepler\" data, but confirmed by outside researchers, include KOI-423b, KOI-428b, KOI-196b, KOI-135b, KOI-204b, Kepler-45 (formerly KOI-254b), KOI-730, and Kepler-42 (formerly KOI-961). The \"KOI\" acronym indicates that the star is a \"K\"epler \"O\"bject of \"I\"nterest.\n\nBoth Corot and \"Kepler\" measured the reflected light from planets. However, these planets were already known, because they transit their star. \"Kepler\" data allowed the first discovery of planets by this method, Kepler-70b and Kepler-70c.\n\nThe Kepler Input Catalog is a publicly searchable database of roughly 13.2 million targets used for the Kepler Spectral Classification Program and the \"Kepler\" mission. The catalog alone is not used for finding \"Kepler\" targets, because only a portion of the listed stars (about one-third of the catalog) can be observed by the spacecraft.\n\n\"Kepler\" has been assigned an observatory code in order to report its astrometric observations of small Solar System bodies to the Minor Planet Center. In 2013 the alternative \"NEOKepler\" mission was proposed, a search for near-Earth objects, in particular potentially hazardous asteroids (PHAs). Its unique orbit and larger field of view than existing survey telescopes allow it to look for objects inside Earth's orbit. It was predicted a 12-month survey could make a significant contribution to the hunt for PHAs as well as potentially locating targets for NASA's Asteroid Redirect Mission. \"Kepler\" first discovery in the Solar System, however, was , a 200-kilometer cold classical Kuiper belt object located beyond the orbit of Neptune.\n\nOn October 30, 2018, NASA announced that the Kepler space telescope, having run out of fuel, and after nine years of service and the discovery of over 2,600 exoplanets, has been officially retired, and will maintain its current, safe orbit, away from Earth. The spacecraft was deactivated with a \"goodnight\" command sent from the mission's control center at the Laboratory for Atmospheric and Space Physics on November 15, 2018. \"Kepler\"s retirement coincides with the 388th anniversary of Johannes Kepler's death in 1630.\n\n\n\n\n\n"}
{"id": "1342072", "url": "https://en.wikipedia.org/wiki?curid=1342072", "title": "King Edward Point", "text": "King Edward Point\n\nKing Edward Point (also known as KEP) is a permanent British Antarctic Survey research station on South Georgia and is the capital of the British Overseas Territory of South Georgia and the South Sandwich Islands, on the northeastern coast of the island of South Georgia. It is located at in Cumberland East Bay. It is sometimes confusingly referred to as Grytviken, which is the site of the disused whaling station, nearby at the head of King Edward Cove.\n\nThe area was explored by the Swedish Antarctic Expedition of 1901-04 under Otto Nordenskiöld. It was named around 1906 after King Edward VII of the United Kingdom.\n\nSince 1909, King Edward Point has been the residence of a British Magistrate administering the island.\n\nIn 1925, the government of the United Kingdom established a marine laboratory there as part of the Discovery Investigations.\n\nOn 1 January 1950, the station ownership was assumed by the Falkland Islands Dependencies Survey. The station was manned from 1 January 1952 to 13 November 1969.\n\nThe British Antarctic Survey provided the British presence at the station until 1982.\n\nAt the beginning of the Falklands War on 3 April 1982, Argentine forces occupied South Georgia and closed the station. They were soon expelled during Operation Paraquet and British military forces occupied the point.\n\nA series of civilian Marine Officers was appointed to carry out customs and fisheries duties for the South Georgia Government from 1991, and were billeted with the small garrison.\n\nOn 22 March 2001, the British Antarctic Survey reopened the station on behalf of the Government of South Georgia and the South Sandwich Islands (GSGSSI). Most of the old, dilapidated (and arguably historic) buildings were destroyed to make way for new ones, with the exception of Discovery House (1925) and the Gaol (1912).\nCurrently twelve BAS personnel overwinter at the station, rising to around 22 in summer. Two Government Officers plus partners are stationed on KEP, overlapping by about three months during the busy winter fishing season. Summer staff from the Museum at Grytviken are also accommodated at KEP.\n\nThe continued occupation of the station serves a political purpose as well: it helps to maintain British sovereignty against Argentina's claim for ownership of the territory.\nThe chief activities of the station are applied fisheries research on behalf of the Government of South Georgia and South Sandwich Islands, to assist its policies for sustainable management of the commercial fishery, and to provide logistic support for the Government Officer(s).\n\n\n"}
{"id": "53349635", "url": "https://en.wikipedia.org/wiki?curid=53349635", "title": "Lilabati Bhattacharjee", "text": "Lilabati Bhattacharjee\n\nLilabati Bhattacharjee (née Ray) was a mineralogist, crystallographer and a physicist. She studied with scientist Satyendra Nath Bose and completed her MSc in Physics from the University of Calcutta in 1951. Mrs Bhattacharjee specialised in the fields of structural crystallography, optical transform methods, computer programming, phase transformations, crystal growth, topography, instrumentation and made important contributions to these fields. She served as a Senior Mineralogist at the Geological Survey of India and later went on to become the Director (Mineral Physics) of the organisation. Bhattacherjee was married to Professor Siva Brata Bhattacherjee, a physicist and is survived by their son Dr Subrata Bhattacherjee and daughter Mrs Sonali Karmakar.\n"}
{"id": "26476828", "url": "https://en.wikipedia.org/wiki?curid=26476828", "title": "List of West Virginia state symbols", "text": "List of West Virginia state symbols\n\nThe following is a list of symbols of the U.S. state of West Virginia.\n"}
{"id": "51097602", "url": "https://en.wikipedia.org/wiki?curid=51097602", "title": "List of off-season Australian region tropical cyclones", "text": "List of off-season Australian region tropical cyclones\n\nAn off-season Australian tropical cyclone is a tropical cyclone that existed in the Australian Region, between 90°E and 160°E, outside of the official season. The World Meteorological Organization currently defines the season as occurring between 1 November and 30 April, of the following year, which is when the majority of all tropical cyclones exist. During the off-season, systems are more likely to either develop during or persist until May, with approximately 52% of such storms occurring during that month. Occasionally, however, storms develop in October, with approximately 34% of such storms occurring during that month. As of 2017, there have been 86 tropical cyclones known to have occurred off-season.\n\nOff-season cyclones are most likely to occur in the Coral Sea, with most affecting land in some way. Cumulatively, at least 4 deaths occurred due to the storms, The most recent off-season storm was Tropical Cyclone Greg in May 2017.\n\nThe wind speeds listed are maximum ten-minute average sustained winds, while the pressure is the minimum barometric pressure, both of which are estimates taken from the archives of either Meteo France, the Australian Bureau of Meteorology, the Fiji Meteorological Service, and New Zealand's MetService. If there are no known estimates of either the winds or pressure then the system is listed as \"Not specified\" under winds or pressure, if there is no known estimated winds or pressure. For deaths and damages \"None\" indicates that there were no reports of fatalities, although such storms may have impacted land. The damage totals are the United States dollar of the year of the storm.\n\nThe charts below both show during which month a tropical cyclone developed during the off-season. The statistics for April are not complete and only show those systems that formed during the month and either dissipated on 30 April, or persisted into the off-season.\n\n"}
{"id": "34534637", "url": "https://en.wikipedia.org/wiki?curid=34534637", "title": "MODELISAR", "text": "MODELISAR\n\nMODELISAR was an ITEA 2 (Information Technology for European Advancement) European project aiming to improve the design of systems and of embedded software in vehicles. The MODELISAR goals were to:\n\nThe MODELISAR project started in 2008 , ended Dec 2011 to define the Functional Mock-up Interface specifications, deliver technology studies, prove the FMI concepts through Use Cases elaborated by the consortium partners and enable tool vendors to build advanced prototypes or in some cases even products.\nMODELISAR has been awarded during ITEA 2 & ARTEMIS co summit with a Silver award. See ITEA 2 achievement award\n\nAlthough in MODELISAR the focus was driven by automotive Use Cases, the FMI specifications could be independently defined. Therefore the FMI can now address others industrial domains such as Energy, Aerospace, Rail.\n\n\n"}
{"id": "21503500", "url": "https://en.wikipedia.org/wiki?curid=21503500", "title": "Malay Falls, Nova Scotia", "text": "Malay Falls, Nova Scotia\n"}
{"id": "13064721", "url": "https://en.wikipedia.org/wiki?curid=13064721", "title": "Minimalism (technical communication)", "text": "Minimalism (technical communication)\n\nMinimalism in structured writing, topic-based authoring, and technical writing in general is based on the ideas of John Millar Carroll and others. Minimalism strives to reduce interference of information delivery with the user's sense-making process. It does not try to eliminate any chance of the user making a mistake, but regards an error as a teachable moment that content can exploit.\n\nLike Robert E. Horn's work on information mapping, John Carroll's principles of Minimalism were based in part on cognitive studies and learning research at Harvard and Columbia University, by Jerome Bruner, Jerome Kagan, B.F. Skinner, George A. Miller, and others. Carroll argues that training materials should present short task-oriented chunks, not lengthy, monolithic documentation that tries to explain everything in a long narrative.\n\nA historian of technical communication, R. John Brockmann, points out that Fred Bethke and others at IBM enunciated task orientation as a principle a decade earlier in a report on IBM Publishing Guidelines.\n\nCarroll observes that modern users are often already familiar with much of what a typical long manual describes. What they need is information to solve a task at hand. He feels that documentation should encourage them to do this with a minimum of systematic instruction.\n\nDarwin Information Typing Architecture (DITA) is built on Carroll's theories of Minimalism and Horn's theories of Information Mapping.\n\nMinimalism is a large part of JoAnn Hackos' recent workshops and books on information development using structured writing and the DITA XML standard.\n\nGood writing means that the message is directly clear to the projected audience. Adopting a minimalist method may appear, in the short-term, to cost more, as writers must cut up and rephrase content into single free-standing chunks. However, the longer-term brings cost-saving benefits, particularly in translation and localization, where often sum is on a ‘per word’ basis. But the greatest advantage for companies is user fulfillment. The less time a customer spends working out how to do something, the more likely they are to purchase again.\n\n"}
{"id": "20371327", "url": "https://en.wikipedia.org/wiki?curid=20371327", "title": "Nérée Boubée", "text": "Nérée Boubée\n\nNérée Boubée (1806 in Toulouse – 1862 in Luchon) was a naturalist, entomologist, geologist, author and a professor at the University of Paris. He was a Member of the Société entomologique de France.\n\nIn 1845 he established a natural history dealership and publishing house, the still very successful Maison d'édition Boubée.\n\n"}
{"id": "1107058", "url": "https://en.wikipedia.org/wiki?curid=1107058", "title": "Operation Hardtack I", "text": "Operation Hardtack I\n\nOperation Hardtack I was a series of 35 nuclear tests conducted by the United States from April 28 to August 18 in 1958 at the Pacific Proving Grounds. At the time of testing, the Operation Hardtack I test series included more nuclear detonations than all prior nuclear explosions in the Pacific Ocean put together. These tests followed the \"Project 58/58A\" series, which occurred from 1957 December 6 to 1, 958, March 14, and preceded the Operation Argus series, which took place in 1958 from August 27 to September 6.\n\nOperation Hardtack I was directed by Joint Task Force 7 (JTF 7). JTF-7 was a collaboration between the military and many civilians, but was structured like a military organization. Its 19,100 personnel were composed of members of the US military, Federal civilian employees, as well as workers affiliated with the Department of Defense (DOD) and the Atomic Energy Commission (AEC).\n\nThere were three main research directions. The first was the development of new types of nuclear weapons. This was undertaken by detonating experimental devices created by the AEC's Los Alamos Scientific Laboratory and the University of California Radiation Laboratory. The DOD performed experiments and tests on these detonations that did not hamper the AEC's research. The second research direction was to examine how underwater explosions affected materiel, especially Navy ships, and was performed by the DOD. The tests were named Wahoo (open ocean) and Umbrella (lagoon); the former was conducted in the open ocean whereas the latter in a lagoon. The final avenue of study was to analyze high-altitude nuclear tests in order to refine the detection of high-altitude nuclear tests and investigate defensive practices for combatting ballistic missiles. This research direction was composed of three individual tests and were the first high altitude tests. The individual tests in the series were Orange, Teak, and Yucca. Orange and Teak were known collectively as Operation Newsreel and were rocket boosted. Yucca, however, reached its altitude by being raised by balloons.\n\nMany events and proceedings leading up to Operation Hardtack I, such as previous nuclear testing results and the global political atmosphere, influenced its creation and design. One such historical circumstance was that nuclear radiation concerns were mounting publicly and abroad by 1956. During the 1956 Presidential Election, ending nuclear testing was a campaign issue and nuclear safety was one part of that discussion. At the same time, the \"Union of Soviet Socialist Republics\" (USSR) was publicly proposing a moratorium on testing.\n\nIn 1956 during June, the National Academy of Sciences (NAS) recommended new radiation exposure limits for the general public in a report entitled The Biological Effects of Atomic Radiation. The AEC, which was the body that developed nuclear weapons, accepted the NAS recommended radiation exposure limits. Some AEC members asserted that the limits were reached incorrectly and should be reviewed in the future. Additionally, Charles L. Dunham, the AEC Director of the Division of Biology and Medicine (DBM) said that the new limits of nuclear radiation exposure would prevent them from continuing testing in Nevada. Dunham, in association with other AEC offices and officials, made recommendations to move further tests to the Pacific in order to eliminate the need to determine radiological fallout safety.\n\nAlso in 1956, the AEC was designing a test series that included nuclear detonations which would release significant amounts of nuclear fallout. The series came to be known as Operation Plumbbob and took place in 1957 from April 24 to October 7. Operation Plumbbob was followed by Project 58/58A and Operation Hardtack I, respectively. At the time of testing, Operation Plumbbob was the most extensive nuclear test series held at the Nevada Test Site. President Eisenhower was very cautious in approving Plumbbob due to public concern. Consequently, the AEC held brief but fruitless discussions of moving some of the tests in the Plumbbob series to the next planned Pacific Ocean series, Operation Hardtack I, to minimize radioactive fallout in and around the Nevada Test Site.\n\nWhen Operation Plumbbob began in 1957 during the Spring, the planning process of Operation Hardtack I was well underway and the number of nuclear detonations planned were more than those in the Plumbbob series. At the same time, radiation and nuclear proliferation concerns around the world had led to formal discussions between the US, USSR, and other countries on the topic of instituting a global ban on nuclear testing as a path to disarmament. In 1957 on August 9 AEC chairman Lewis L. Strauss proposed to President Eisenhower a preliminary plan of Operation Hardtack I. President Eisenhower objected to the length of the four month testing period and that the plan called for 25 shots, which was one more than in Plumbbob. As a result of the discussion Eisenhower consented to yields no larger than 15 Megatons, and ordered that the testing period be as brief as possible.\n\nProject 58/58A followed Operation Plumbbob and began almost two months later starting in 1957 on December 6. Project 58/58A was composed of four safety tests; they were not supposed to result in nuclear radiation. All four took place at the Nevada Test Site. The safety tests were intended to ensure that the bombs would not malfunction. However, the test named Coulomb-C which took place in 1957 on December 9 did malfunction. It resulted in an unanticipated 500 ton blast. The blast released a nuclear radiation fallout cloud that traveled toward Los Angeles and resulted in low levels of nuclear radiation. The bomb failure added to public concern over nuclear testing's safety.\n\nSome tests were cut in an effort to meet Eisenhower's demands for a shorter testing period, but new tests quickly filled their place. AEC chairman Strauss remarked that the large number of shots was due to the \"DOD's requirement for an increasing number of different nuclear weapons types.\" Eisenhower eventually approved the plan for Operation Hardtack I in late January, even though it still contained twenty-five shots. In 1958 on March 31, the USSR announced a suspension of all tests and called on the US to do the same. On May 9 the leader of the Soviet Union, Nikita Khrushchev, accepted Eisenhower's invitation for technical discussions on a nuclear testing moratorium and negotiations began on July 1 of the same year. Eisenhower announced to the US in 1958 on August 22 that the ban would begin October 31. US scientists responded by seeking to add more tests to the Hardtack Series in case it turned out to be the last chance. Consequently, Operation Hardtack I came to consist of 35 tests.\n\nThere was public health concern with the number of shots in Operation Hardtack I. Furthermore, studies released in 1958 during March indicated that people as far as 400 miles (644 kilometers) away could have severe retinal burns from for two of the three high altitude tests, Teak and Orange. It was then decided that they would be moved to Johnston Island which was 538 miles (866 kilometers) from the nearest inhabited island. The high-altitude nuclear tests delivered the first openly reported man-made high-altitude electromagnetic pulses (EMP). Teak, which was detonated at 252,000 feet and was 3.8 megatons, produced an Aurora like effect that could be seen from Hawaii, which was 700 nautical miles (806 miles, 1297 kilometers) displaced from the detonation. Most radio communications immediately dropped across the Pacific Ocean; the blackout in Australia lasted for 9 hours and Hawaii's lasted for no less than 2 hours.\n\nOperation Hardtack I contained three high altitude tests that were designed to study many effects that a nuclear explosion would have on materials and electronic systems. They were also used to test the energy of the explosion and what forms of energy they would produce. Yucca was the name of the first high-altitude test and it was performed near the Enewetak Atoll in the Marshall Islands. The other two high-altitude tests, Orange and Teak, were performed near the Johnston Atoll in the South Pacific Ocean about 1,300 kilometers South West of the Hawaiian Islands.\n\nThere was some uncertainty on whether detonating a nuclear weapon at such high altitudes would cause a hole in the ozone layer. After multiple underwater tests there was evidence to show the energy created by the blast would create ozone. Project leads determined some destruction of ozone would occur which would then be replaced by the ozone created from the explosion, it was agreed that if this theory were wrong, the hole created in the ozone layer would be minuscule enough to cause no harm. Upon completion of Operation Newsreel, it was determined that there was very little evidence of any harm being caused to the natural ozone layer.\n\nYucca was the first high altitude test performed during Operation Hardtack I and was detonated on April 28, 1958. It was lifted by a balloon to an altitude of 86,000 feet (26.2 km) and had a yield of 1.7 kilotons.To achieve the altitude needed for detonation the device was attached to a large helium filled balloon which carried it to the detonation altitude. Due to issues with high winds on Enewetak Island the balloon launches from ground were unsatisfactory thus creating the need for a new method to be developed. The balloons were then deployed from an aircraft carrier which helped not only with deployment but also with inflation. While the balloons were inflated they were unable to take too much force from wind or the plastic material the balloons were made of would tear. While the balloons were inflating on the aircraft carrier, the ship could match and oppose the wind speed allowing for the balloon to be inflated in still air. To make sure the launch would be successful 86 balloon launches were tested.\n\nDue to concerns of failures, many safety measures were put into place. While on ground the bomb used multiple pins to keep certain safety features of the bomb from being deployed before liftoff could take place. One such pin was used to stop the electrical systems such that the bomb would be unable to be armed. In case of a dud the bomb would detach from the balloon and be allowed to drop into the ocean where many more safety features would come into action. Some of these features included probes which could detect saltwater which would destroy the electrical system so the conductive seawater could not cause a short which would detonate the bomb. Another worry was having a nuclear device floating in the ocean if a misfire did occur. To combat this inserts were created which would dissolve over a course of a few hours which would sink the device.\n\nOn the day of the launch many meteorological aspects were taken into account and calculations were done to make sure the bomb would reach its correct altitude. The bomb was loaded onto an aircraft carrier and preparations began for the test fire. As the balloon was inflated on the aircraft carrier multiple testing devices were connected to the balloon. The readings from these devices would be sent to multiple ships and aircraft so nothing would have to be recovered after the explosion. After all the preparations were done and the balloon was inflated the device was released and began to climb. After almost three and a half hours of climbing the bomb was detonated at an altitude of 26.2 kilometers. The aircraft carrier which carried Yucca was around 45 kilometers from the detonation site when the explosion occurred. The shock wave from Yucca reached the carrier 3 minutes and 16 seconds after detonation.\n\nThe Yucca test had many Department of Defense projects attached to it for research purposes. Alongside testing the use of a balloon carrier the Department of Defense wanted to research the electromagnetic waves emitted from a nuclear explosion. This test would be used to see the impact a nuclear explosion would have on electronic devices. All of the data from these Department of Defense projects would be stored on recording devices on surrounding islands and in aircraft so data would not have to be recovered after the test. However, Yucca was carrying five different transmitting devices in canisters to help with research. While the balloon proved to be a great success, no data from the canisters were able to be retrieved due to equipment issues onboard one of the aircraft carriers.\n\nThe Teak test was launched from Johnston Island on July 31, 1958 and carried a payload of 3.8 megatons. Teak was the second high-altitude test after the success of Yucca. Instead of a balloon, the warhead for the Teak test would be carried by a missile. The Redstone missile which would carry Teak had been used to launch Explorer I in January of the same year. Other nuclear weapons had been tested using the Redstone missile but up until this point the highest payload to have been detonated was 3 kilotons during Operation Teapot. Teak would be the first high altitude test to have a payload in the megaton range. The test was scheduled to take place from the Bikini Atoll. However, due to the fact that the payloads of Teak and Orange were much larger than previous high altitude shots the test was moved to Johnston Island as to protect nearby native islanders from any retinal damage. As both Teak and Orange had been relocated these two tests were named Operation Newsreel. The name Newsreel came from the fact that these two test were being moved to Johnston Island.\n\nThe safety precautions taken by the teams involved in these tests were precisely detailed. The day before the launch 187 team members would evacuate Johnston Island with 727 men the day of the test. This was to keep as few men on the island while still being able to operate the airfield and critical data instruments. Another problem the team members were concerned about was the issue of retinal damage. Since the payload of the bomb was so large aircraft were scheduled to keep any civilian ships out of a 760 kilometer radius of Johnston Island. Additionally, the Civil Aeronautics Authority was informed that it would be dangerous for any aircraft to fly within 965 kilometers of Johnston Island. On the day of the test only about 175 men remained on Johnston Island to prepare for Teak to be launched and other various duties needed after.\n\nAt 11:47 PM on July 31 Teak was launched and after 3 minutes was detonated. Due to programming issues, the warhead detonated directly above Johnston Island. At time of detonation the rocket had flown to an altitude of 76.2 kilometers. The explosion could be seen from Hawaii 1,297 kilometers away and was said to be visible for almost half an hour. After the explosion, high frequency long distance communication was interrupted across the Pacific. Due to this communication failure Johnston Island was unable to contact their superiors in the US to let them know the results of the test until about eight hours after the detonation. Thirty minutes after detonation, a crew was sent out to collect the pod which had detached from the missile carrying the warhead. The pod had been irradiated and to handle it the crew members used disposable gloves in an attempt to protect themselves from beta radiation.\n\nDuring the Teak test all crew on and around Johnston Island were given protective eyewear to prevent flash blindness once the explosion took place. After the explosion it was found that besides the hazard of blindness, thermal radiation was another concern—even at an altitude of 76 kilometers. A crew member who was on Johnston Island at the time was said to have received a slight sunburn from the amount of thermal radiation which had reached the island. While only slight to the crew member it created issues for the local fauna. Many birds were seen on Johnston Island in distress. Unsure if this was caused by blindness or thermal radiation, the project members decided to take precautions to protect local wildlife during the Orange test.\n\nOrange was launched twelve days after Teak on August 11, 1958. Orange, like Teak, was launched using a Redstone Missile and had a yield of 3.8 megatons. The same safety precautions used by Teak were implemented again for the Orange launch. Seeing how smoothly the evacuation for the Teak launch went it was decided that the evacuation did not need to occur the day before the launch and eight hundred and eight men were evacuated on August 11 to an aircraft carrier about 70 kilometers northeast of the island. Along with protection for the project crew, it was decided after Teak that Sand Island, a local bird refuge, would need protection from the blast as well. To make sure that most of the wildlife was safe a smoke screen was created over Sand Island. Due to interest in Hawaii, it was announced on August 11 that there would be a nuclear test sometime between 10 PM and 6 AM.\n\nThe rocket carrying the warhead was launched at 11:27 from Johnston Island and traveled south. Like Teak, the flight lasted 3 minutes and was detonated at 11:30 PM about 41 kilometers south of Johnston Island at an altitude of about 43 kilometers.The trajectory of Orange was a major success after the incident with Teak being detonated directly over the island. The recovery crew for the pod that was with Orange was unable to locate the research pod which had been launched with the rocket. Although Orange was visible from Hawaii it was not as great of a spectacle as Teak had been. The light from the blast was only visible for about 5 minutes. The explosion had also been slightly obscured to the crew at Johnston Island from cloud coverage that night. The blast from Orange did not come with large communication interruption that Teak had caused, but some commercial flights to Hawaii were said to have lost contact with air traffic controllers for a short period of time.\n\nOf the 35 nuclear tests in Operation Hardtack I, four were surface burst shots: Cactus, Koa, Quince and Fig. These tests took place from May to August 1958, all at the Enewetak Atoll. Surface tests inherently present the potential for more radioactive exposure issues than the high-altitude or underwater detonations. This is because there is more material present to be converted to radioactive debris by excess neutrons due to the proximity to the Earth's surface, and due to the soil and other minerals excavated from the craters created by these blasts. The existence of this extra material allows for larger radioactive particles to be created and lifted into the blast cloud, falling back to the surface as fallout.Though surface and near-surface tests have a higher probability of radioactive exposure problems, the radioactive elements have significantly shorter residence times when injected into the atmosphere. As radioactive clouds from surface-type tests reach heights of around 20 kilometers at maximum, and thus cannot extend higher than the lower stratosphere, the residence times can be up to 13 years less than the high-altitude blasts. During original concept planning in 1954, Enewetak was supposed to be the location of the smaller tests conducted during Operation Hardtack I. Due to poor weather conditions and policy changes in 1958, five of the UCRL tests which were planned to be conducted at the Bikini Atoll were moved to Enewetak. This included the later two surface blast devices in the Quince and Fig tests.\n\nThe Cactus test took place May 6, 1958 at approximately 0615. An 18 Kiloton land-surface type shot was detonated on a platform at the northern tip of Runit, Enewetak in the second of the 35 tests for Operation Hardtack I.The initial cloud from the explosion reached as high as 19,000 feet (5.8 km) within the first ten minutes, and settled at around 15,000 feet (4.6 km) by 20 minutes after detonation. The nuclear fallout prediction map proved to be accurate in determining the span and the intensity of the resultant fallout.Measured peak intensity of fallout reached 440 R at hour three directly above blast site on the North end of the Atoll. At mid-island the radiation was measured to be 1.7 R. The southern tip received a very small amount of radiation, 0.005 R due to the easterly winds.\n\nOut of the eight Programs listed, DOD-affiliated Projects 1.4, 1.7, 1.8, 1.9, 1.12, 2.8, 3.2, 5.2, 5.3, 6.4, 6.5 and 6.6 involved the Cactus test.\n\nOne of the goals of this test was to study the physical characteristics of the crater and the surrounding area pre- and post-detonation associated with Project 1.4. A camera was mounted to a RB-50 aircraft and the resultant crater was mapped using photogrammetry. Measurements were taken from 500 feet (152 meters) down to ground zero for both pre- and post detonation. Survey measurements of the Cactus test could not be made until the resultant radiation amounts had lowered to safer levels. Along with photogrammetric radial measurements, Airblast measurements were also recorded near surface zero for Cactus in accordance with Project 1.7. Similar to Project 1.8, Project 1.9 sought to determine transmission of blast pressure through the ground soil. Forty-three drums were buried at varying depths 600 feet (183 meters) from ground zero of the Cactus detonation. Individuals from the Air Force Ballistic Missile Division – TRW Space Technologies Laboratory sought to the Shock Spectra Studies of Project 1.12. Measurement gauges were located between 625 and 965 feet (191 to 294 meters) from the blast site. Fallout measurements and samples were taken by aircraft in the aftermath of the detonation, as part of Project 2.8. These surveys were designed in part to determine the impact of specific radionuclides to overall nuclear fallout. Samples were taken early post-blast using a new rocket sampler, the UCRL, which were then followed by B-57D and WB-50 aircraft. Between four and twenty-four hours post-detonation, the WB-50 collected several samples at 1,000 feet (305 meters) in altitude. Project 3.2 involved structural tests of corrugated steel arches. One of these arches was placed 980 feet (299 meters) away from the Cactus blast site. Eight days after the test, a crew of 13 men were allowed near the surface zero in order to extract the arch. The extraction took around twelve hours, and the radioactivity levels reached a maximum at 0.420 R/h during this time. Project 5.2 sought to determine the effects of the blast on two A4D-1 aircraft, both in thermal radiation and pressure. Nine film badges were placed throughout each aircraft in order to measure radiation. The Gamma dosimeter worn by the pilot in aircraft 827 indicated a neutron exposure level of 0.105 rem. There is not information available for the dosimeter worn by the pilot in aircraft 831. Radiation recorded by six of the nine film badges ranged from 0.49-1.74R. Information from the remaining three film badges, located at the Pilot's right leg, left sleeve, and left vest is not available. The pilot's exposures were both greater than 3R. Project 5.3 was very similar to 5.2, focusing on the effects of the blast on the structure of two test FJ-4 aircraft. Radiation from film badges on aircraft 467 ranged from 0.52-3.71 R, and 1.23-5.06 R for aircraft number 310. In Project 6.4, the Army Signal Research and Development Laboratory (SRDL) investigated the electromagnetic pulses post-detonation, utilizing two instruments, one at Wotho (roughly 240 nautical miles from Enewetak) and one at Kusaie (around 440 nautical miles from Enewetak). Though Cactus is mentioned in Project 6.5, whose goal was to study the radar echoes from the fireball, it is not clear what destroyers were involved if any. Destroyers for the Teak, Fir and Yucca tests are explicitly stated. Project 6.6, the lasts of the projects that involved the Cactus test, sought to measure the physical properties of the stabilized radioactive cloud following the blast.\n\nAt 0630 on May 13, the Koa surface device was detonated at the western side of Dridrilbwij. The size of the blast was 1.37 MT, around 76 times larger than the yield of the previous surface test, Cactus. The test was conducted in a large water tank. Within 17 minutes of the blast, surface clouds reached approximately 60,000 feet (18.3 kilometers). Nuclear fallout predictions for Koa were larger and covered a broader range than Cactus, with significantly higher radiation levels, an order of magnitude larger, in the immediate region surrounding the blast. There had been a larger nuclear barge detonation, the 1.85 MT Apache test, off the island of Dridrilbwij two years prior in 1956, during which the island survived. The detonation of the Koa device, however, caused complete destruction of the island.\n\nThe Department of Defense (DOD) sponsored a series of experiments for Koa: Projects 1.4, 1.7, 1.8, 1.9, 1.12, 2.9, 3.2, 3.6, 5.1, 5.3. 6.4, 6.5, 6.6, 6.9. and 6.11. Project 1.4, which sought to study land craters post detonation for several shots, took ground survey measurements from surface zero out to 2,500 feet (762 meters). Detailed measurements could not be made until four days after the blast due to radiation levels. Using a boat, the majority of the crater was mapped, though some measurements could not be made until 1959 due to radioactivity levels around the Koa blast site. In Project 1.9, the blast-induced pressure through the soil was measured by burying 43 drums at depths ranging from 0 to 20 feet and around 3,000 feet from the Koa blast site. The early radioactive cloud samples, normally collected post-shot as a part of Project 2.8 using a UCRL rocket sampler, were not collected after the Koa test due to technical difficulties. Fallout at high and low altitudes utilizing B-57D and WB-50 aircraft were measured with no technical problems. Project 3.6, which was conducted only on Koa, tested the effects of the blast on reinforced-concrete slabs buried close to surface zero. One station was located at 1,830 feet (558 meters) from the blast site, and the other was located 3,100 feet (945 meters) away. Project 5.2 was designed to measure the radiation levels using film badges on different areas of the aircraft and on the pilot directly. Koa had the lowest average radiation measurements of the eight shots on this project, with aircraft 827 ranging from 0.01-0.02R and aircraft 831 reporting levels between 0.03-0.19R. Unlike in the Cactus test, all nine film badges from both aircraft were successfully recovered and reported.\n\nQuince and Fig were a series of surface shots that occurred in early August at the center of Runit. They were the last of the Hardtack tests conducted at the Enewetak Atoll. Quince was detonated at 2:15pm on August 6, 1958. Twelve days later, the Fig device was detonated on August 18, 1958 at 4:00pm. Quince and Fig were UCRL devices with co-sponsorship through the DOD and the AEC, though the main focus of these tests was weapons development. One of the main differences between these two tests was that over 130 tons of soil from the Nevada Test Site had been shipped in and was placed at surface zero of the Fig test. The radioactive cloud from the Quince blast rose to 1,500 feet (457 meters). As Runit was to also be the detonation site for Fig, the area had to be decontaminated after the Quince test. Around three to five inches of contaminated topsoil was removed from an area 75 feet by 25 feet upwind of ground zero. An area of 60 square feet at the blast site was scraped to remove the top three inches of soil as well. Even with these precautions, the measured alpha activity was around 20,000 counts per minute (CPM), and the area closest to surface zero was roped off to prevent personnel from entering. Upon detonation, the mushroom cloud produced by the Fig device rose to approximately 6,000 feet (1.8 kilometers), with a base of roughly 4,300 feet (1.3 kilometers). 30 minutes post detonation, radiation measurements at the blast site reached over 10,000 R/hr.\n\nThe DOD experiments for Quince and Fig included Projects 1.4, 1.7. 2.4, 2.9, 2.10, 2.11, 2.14. and 8.7.\n\nBarge mounted test detonations was a technique first used in 1954, and also compensated for the lack of land at the HP. When testing returned to Bikini atoll the use of barges as the shot point had begun, with one notable advantage, no radioactive surface zero areas were developed. Two large underwater craters were formed in 1954 and these were used as subsequent surface zeros for detonations fired from barges. This allowed the available land area to be used for the placement of measurement instrumentation and reuse of the same burst point without the long delays required for radiological cooling by natural decay or expensive and long decontamination procedures. Reuse of zero points also allowed reuse of instrument locations and recording shelters for several tests saving construction costs and time and Increasing test-scheduling flexibility. In HARDTACK the 26 barge events used only five detonation areas.\n\nFIR predicted fallout, surface radiological exclusion (radex) areas, ship position, and aircraft participation. FIR was the first Bikini detonation of the Hardtack series. Detonated at 0550 on May 12, 1958. FIR was detonated on a barge in the BRAVO crater, producing a 1.36 Mt yield range. After detonation, the cloud rose to 60.000 to 90.000 feet (18.3 to 27.4 km). The FIR detonation was shortly followed by BUTTERNUT detonation on Enewetak 25 minutes later. DOD-sponsored experiments for FIR included Projects 3.7, 5.1, 6.4, 6.5, 6.6 and 6.11.\n\nBUTTERNUT predicted fallout, surface radiological exclusion areas (radex), ship positions, and aircraft participation. Butternut was detonated at 0615 on May 12, 1958, 25 minutes after FIR had been detonated at Enewetak. BUTTERNUT was detonated on a barge 4.000 feet (1.22 km) west of Runit, producing a yield range of 81 kt. The cloud rose to 35,000 feet (10.7 km), stabilizing at 30,000 feet (9.1 km). The DOD-sponsored experiments for BUTTERNUT were Projects 5.1, 5.2, 5.3, 6.5, 6.6, and 6.9.\n\nHOLLY predicted fallout, surface radiological exclusion (radex) areas, ship positions, and aircraft participation. HOLLY was detonated at 0630 on May 21, 1958 on a barge west of Runit, 4,000 feet (1.22 km) from the nearest edge of the island, producing a 5.9 kt yield range. The detonation produced a 15,000 ft. (4.6 km) cloud that stabilized at an altitude of 12,000 feet (2.7 km) at the top and 7,500 ft. (2.3 km) at its base. Project 6.6, was the only DOD-sponsored experiment for HOLLY, and was conducted on Enewetak Island.\n\nNUTMEG predicted fallout, surface radiological exclusion area (radex), ship positions, and aircraft participation. The second Bikini shot, NUTMEG, was detonated at 0920 on May 22, 1958. NUTMEG was detonated on a barge in the ZUNI crater, and produced a 25.1 kt yield range. The detonation cloud stabilized at 20,000 feet (6.1 km) by 0926. DOD-sponsored experiments for NUTMEG were Projects 6.3, 6.3a, 6.4, 6.5, 6.6, and 6.11. Projects 6.3 and 6.3a had stations near the burst point on Eneman Island.\n\nYELLOWWOOD predicted fallout, surface radiological (radix) exclusion area, and ship positions. YELLOWWOOD was detonated at 1400 on May 26, 1958. YELLOWOOD was detonated on a barge 5,000 feet (1.52 km) southwest of Enjebi, and produced a 330 kt yield range. The DOD sponsored 13 experiments for YELLOWWOOD: Projects 2.4, 2.8, 3.7, 5.1, 5.2, 5.3, 6.4, 6.5, 6.6, 6.8, 6.9, 6.11, and 8.1. Instrument stations for Project 2.4 were located on buoys in the lagoon. The remaining projects had more distant stations or were primarily airborne.\n\nPredicted fallout, surface radiological exclusion (radix) area, ship positions, and aircraft participation. MAGNOLIA was detonated at 0600 on May 27, 1958. MAGONLIA was detonated on a barge 3.000 feet (914 meters) southwest of the center of Runit, and produced a 57 kt yield range. The detonation produced a 44,000-foot (13.4-kim) cloud that stabilized at 41,000 feet (12.5 km) with its base at 15,000 feet (4.6 km). The DOD-sponsored experiments for MAGNOLIA were Projects: 3.7, 5.2, and 5.3. Instruments for Project 3.7 were on Boken, Enjebi, and Runit.\n\nTOBACCO predicted fallout, surface radiological exclusion (radix) area, ship positions, and aircraft participation. TOBACCO was detonated at 1415 on May 30, 1958. TOBACCO was detonated on a barge 3.000 feet (914 meters) northwest of Enjebi, and produced an 11.6 kt yield range. The detonation produced an 18,000-foot (5.5-km) cloud that stabilized at 16,000 feet (4.9 km) by 1430. The DOD-sponsored experiments for TOBACCO were Projects 3.7, 5.1, 5.2, 5.3, and 6.8. The project activity for Project 3.7 was on Boken, Enjebi, and Runit. Project 6.8 simply monitored TOBACCO from stations off Enewetak and Parry islands.\n\nSYCAMORE predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. SYCAMORE was detonated at 1500 on May 31, 1958. SYCAMORE was detonated on a barge moored in the BRAVO crater, and produced a 92 kt yield range. DOD-sponsored experiments for SYCAMORE were Projects: 3.7, 5.1, 6.4, 6.5, 6.6, and 6.11. Only Project 6.11 had an instrument site at the Bikini Atoll.\n\nMAPLE predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. MAPLE was detonated on June 11 at 0530 1958. MAPLE was detonated on a barge just south of Lomilik, and produced a 213 kt yield range. The 40,000-foot (12.2-kim) cloud produced by the detonation was tracked by radar from Benner. The DOD-sponsored experiments for MAPLE were Projects 5.1, 6.3, and 6.3a. Projects 6.3 and 6.3a shared the same, rather close-in sites on Lomilik.\n\nASPEN predicted fallout, surface radiological exclusion (radex) areas, ship positions, and aircraft participation. ASPEN was detonated on 15 June 58 at 0530. ASPEN was detonated on a barge in the BRAVO crater of Bikini, 4,000 feet (1.22 km) southwest of Nam. ASPEN produced a 319 kt yield range. The detonation produced a 48.600-foot (14.8-k1m) cloud as measured by radar from Benner. No experiments were sponsored by DOD for this shot.\n\nWALNUT predicted fallout, surface radiological exclusion (radex) areas, ship positions, and aircraft participation. WALNUT was detonated on June 15, 1958 at 0630, 1 hour after the ASPEN event at Bikini. WALNUT was detonated on a barge 5,000 feet (1.52 km) southwest of Enjebi Island in Enewetak. The detonation produced a 61.000-foot (18.6-kim) cloud, and produced a 1.45 Mt yield range. The DOD-sponsored experiments for WALNUT included Projects 2.4, 2.8, 3.7, 5.1, 5.2, 5.3, and 8.1.\n\nLINDEN predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. LINDEN was detonated on June 18, 1958 at 1500. LINDEN was detonated on a barge 4,000 feet (1.22 km) west of the center of Runit Island at Enewetak. The detonation produced a 20,000-foot cloud (6.1-k1m) with a 7.000-foot (2.1-kim) base, and produced an 11 kt yield range. No DOD-sponsored experiments were conducted during LINDEN.\n\nREDWOOD predicted fallout, surface radiological exclusion (radex) areas, ship positions, and aircraft participation. REDWOOD was detonated on June 28, 1958 at 0530. REDWOOD was detonated on a barge south of Lomilik in Bikini. REDWOOD was followed by the ELDER detonation at Enewetak Atoll 1 hour later. The detonation cloud base was at 28,000 feet (8.5 km) and the top stabilized at 55,000 feet (16.8 km), and produced a 412 kt yield range. The only DOD-sponsored experiment for REDWOOD was Project 5.1.\n\nELDER predicted fallout, surface radiological exclusion (radex) areas, ship positions, and aircraft participation. ELDER, detonated on June 28, 1958 at 0630. ELDER was the second of a tandem shot with REDWOOD (detonated an hour earlier on Bikini). ELDER was detonated on a barge 1 nmi (1.85 km) southeast of Enjebi Island in Enewetak. Initial cloud height had been well over 65.000 feet (19.8 km), and produced an 880 kt yield range. The only DOD-sponsored experiment for ELDER, was project 5.1.\n\nOAK predicted fallout, surface radiological exclusion (radex) area, and ship positions. OAK, one of the largest detonations at Enewetak Atoll, and was fired at 0730 on June 29, 1958. OAK was detonated on a barge moored on the reef 21,000 feet (6.40 km) southwest of Bokoluo Island. It was followed at noon by the HICKORY shot at Bikini. The Initial height of the cloud was estimated to be 78,000 feet (23.8 km), and produced an 8.9 MT yield range. Two DOD-sponsored experiments were included for OAK: Projects 2.8 and 5.1.\n\nHICKORY predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. HICKORY was detonated on June 29, 1958 at 1200. HICKORY was detonated on a barge off the west end of Enema at Bikini, 4-1/2 hours after the OAK shot at Enewetak Atoll. The detonation cloud rose to 24,200 feet (7.4 km) with an estimated 12.000-foot (3.7-km) base, and produced a 14 kt yield range. Projects 6.3 and 6.3a were the only DOD-sponsored experiments for HICKORY.\n\nSEQUOIA predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. SEQUOIA was detonated on July 2, 1958 at 0630. SEQUOIA was detonated on a barge 2,000 feet (5i0 meters) west-northwest of Runit Island at Enewetak. The cloud stabilized at 15,000 feet (4.6 k1m), and produced a 52 kt yield range. SEQUIA had no DOD-sponsored experiments.\n\nCEDAR predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. CEDAR was detonated on July 3, 1958 at 0530. CEDAR was detonated on a barge in the BRAVO crater southwest of Nam, 4,000 feet (1.22 km) from the edge of the island at Bikini. The CEDAR detonation produced a 50,000 foot (15.2) cloud, and produced a 220 kt yield range. The only DOD-sponsored experiment for CEDAR was Project 5.1.\n\nDOGWOOD predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. DOGWOOD was detonated on July 6, 1958 at 0630. DOGWOOD was detonated on a barge southwest of Enjebi, 4,000 feet (1.22 km) from the edge of the island at Enewetak. The cloud rose to 58,000 feet (17.7 kin), stabilizing at 54.000 feet (16.5 km) with a 35.000-foot (10.7-km) base, and produced a 397 kt yield range. The only DOD-sponsored experiment for DOGWOOD Was Project 5.1.\n\nPOPLAR predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. POPLAR was detonated on July 12, 1958 at 1530. POPLAR was detonated on a barge southwest of Nam, at Bikini. The detonation cloud quickly rose above the tracking radar limits of 61,000 feet (18.6 km), and the base was established at 42.000 feet (12.8 km) at 1540, and produced a 9300 kt yield range. The only DOD-sponsored experiment for POPLAR was Project 3.7.\n\nSCAEVOLA predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. Scaevola was detonated on July 14, 1958 at 1600. Scaevola was detonated on a barge west of Runit Island at Enewetak. Its yield was low and the explosion did not destroy the shot barge but only damaged it, producing a 0 kt yield range. No DOD-sponsored experiments were scheduled for Scaevola.\n\nPISONIA predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. PISONIA was detonated on July 18, 1958 at 1100. PISONIA was detonated on a barge 10,000 feet (3.05 km) west of Runit Island at Enewetak. The cloud rose immediately to 55,000 feet (16.8 km), and produced a 255 kt yield range. No DOD-sponsored experiments were scheduled for PISONIA.\n\nJUNIPER predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. JUNIPER, the last nuclear detonation to occur at Bikini, was detonated on July 22, 1958 at 1620. JUNIPER was detonated on a barge 4,000 feet (1.22 km) from the west end of Eneman in the ZUINI crater at Bikini. The detonation cloud rose to 40,000 feet (12.2 km) with an estimated 24,000-foot (7.3 km) base, and produced a 65 kt yield range. No DOD-sponsored experiments were scheduled for JUNIPER.\n\nOLIVE predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. OLIVE was detonated on July 23, 1958 at 0830. OLIVE was detonated on a barge southwest of Enjebi Island, at Enewetak. 4,000 feet (1.22 km) from the nearest land at Enewetak. The cloud rose to 50,000 feet (15.2 km) with an estimated 15,000-foot (4.6 km) base, and produced a 202 kt yield range. No DOD-sponsored experiments were scheduled for OLIVE.\n\nPINE predicted fallout, surface radiological exclusion (radex) area, ship positions, and aircraft participation. PINE was detonated on July 27, 1958 at 0830. PINE was detonated on a barge southwest of Enjebi Island, 8,500 feet (2.59 km) from the nearest land, at Enewetak. The cloud rose quickly to 66,000 feet (20.1 km) with an estimated 38,000-foot (11.6-km) base as measured by radar, and produced a 2 Mt yield range. No DOD-sponsored experiments were scheduled for PINE.\n\nUnderwater tests were conducted to assess the damages to Navy boats and materials. The location for these tests was Enewetak due to the uniformity of the sea bottom in the area. This is critical for the tests so that proper moorings of the target ships can be secured on the sea floor. The underwater explosions create a bubble from the expended energy of the blast. This bubble is due to the vaporization of water that directly absorbs the heat of the blast. This bubble may break the surface depending on how much energy is dispersed and the depth of the nuclear device. The bubble displaces large amounts of water that then collapses in on the bubble after all the energy is expended. The water collapsing in on the cavity is called the \"radioactive pool\" and has the highest concentration of radioactive material.\n\nMany different projects were put in place to test the two underwater blasts. Wave generation, hydrodynamic variables, and energy released were all studied using multiple sensors. These sensors were placed in multiple areas such as target ships and floating balloons. Oceanographic, seismographic and hydrographic surveys were completed after each nuclear blast. A project that used a nuclear blast to clear minefields was studied using the Umbrella blast. 120 inert mines were placed at different distances ranging from 1,500 feet to 8,000. These mines were then picked out of the water to study the effects of the blast at each interval.\n\nAnother reason for the underwater tests was to detect the radiation contamination of the ships after an underwater nuclear explosion. This meant that the contaminated ships required unique \"radsafe\" preparations so that the data could be quickly accounted for. After the data on the damage and radiation was recorded, the ships needed decontamination work completed before repairs could be completed. The Radiological and Decontamination Unit consisting of 200 enlisted and one officer was created for the task of decontamination. After decontamination, the target ships were repaired for use in a second underwater nuclear weapons test. The amount of radiation exposure was debated for the safety of the crews. Roentgens (R) were used to gauge the gamma radiation that affected the units working on the nuclear tests. \n\nFor the underwater test crew, a limit of 5 R per blast along with 10 R per operation was referenced as the limit for exposure. These initial limits were rejected and maximum permissible exposure standards (MPE) were solidified by commanders for the crews working on the decontamination unit. Due to the fast recovery of ships, high contamination values were expected by commanders and set the standard to four roentgens per hour (R/hr). An excess of 4 R/hr was designated off limits of recovery units. Personnel were given special breathing equipment to work in the interior of ships with an R/hr higher than one. High numbers of personnel working the tests were used to keep individuals within the MPE. Times were calculated to keep the total exposure below two roentgens for the personnel working on the ships. A floating decontamination facility was created on board a transport ship for the Wahoo and Umbrella nuclear tests. This allowed important data analysis and an increase in recovery efficiency.\n\nThe nuclear explosion was carried out in open ocean outside of Enewetak. This nuclear test codenamed Wahoo was the first underwater test in the Operation Hardtack series. This test could be considered a continuation of the Wigwam nuclear blast (a deep water nuclear test 500 nautical miles off the coast of San Diego). Like its predecessor, the Wahoo shot was a scientific program that studied the effects of an underwater nuclear blast on Navy systems. The nuclear device was positioned deep in the Pacific Ocean. This deep water test required precise targeting arrays to be set up around the blast location, which presented unique problems for the arrays as the winds, sea currents and tides needed to be within certain limits for the data to be accurate. Testing arrays were placed at depths of 400 to 800 fathoms (2400 to 4800 feet, 732 to 1463 metres) around the nuclear device. The deep water mooring systems needed for the test proved to be difficult to position, leading many analysts to believe the data skewed. The target ships to be used for this program consisted of three destroyers, an active submarine, a submarine mockup used in Wigwam tests, and a merchant marine ship. It was predicted that no air blast or thermal effects would be presented from the underwater blast. The fallout from the blast was also predicted to stay within the target array due to the southwest surface winds.\n\nThe test conditions were met on May 16 allowing for the nuclear device to be detonated. Within a second of detonation, a spray dome was created that reached a height of 840 feet after seven seconds. The overall shape of the spray dome resembled a cone with 45 degree sloped sides. Plumes were seen breaking through the spray dome after six seconds in every direction. The vertical plume continued rising until 12 seconds after the blast while the lateral plumes traveled for 20 seconds before collapsing. The diameter of the spray dome was approximately 3,800 feet at the 20 second mark. The base surge reached a radius of 8,000 feet in the downwind direction after 1.7 seconds. The downwind surge aided by a 15 knot wind reached speeds of 21 knots (24.2 mph). This base surge could be seen for three and a half minutes and for longer from the air as it continued to move across the ocean. When the spray dome and base surge had dissipated, a foam patch could be seen spreading from the surface zero water to reach over . The nuclear blast was calculated to be nine kilotons. All fallout stayed within the predicted fallout area with a maximum of 0.030 R/hr. The target ship at was directly hit by the shockwave, vibrating the entire ship and shaking it violently. The Moran merchant marine ship moored at 2,346 feet away was immobilized due to shock damage to its main and auxiliary equipment while also attaining minor hull damage. One hour and ten minutes after detonation, a five-gallon water sample was taken directly above the blast location showing 5 R/hr. The retrieval team entered a 3.8 R/hr field after an hour and thirty five minutes.\n\nThe second underwater explosion of the Operation Hardtack series was code-named Umbrella. This test was conducted in the lagoon inside Enewetak. This test could be considered a continuation of underwater test Baker, which was conducted in Bikini Lagoon. Explosions in shallow water can create an underwater crater if close enough to the bottom. The target ships from the Wahoo test were towed into the lagoon to be used again for the Umbrella test. The damage from Wahoo was minor in all cases with the exception of the Moran which required extra refurbishing of the hull. These ships had to be washed down and have the measuring equipment replaced before being put into use. The mooring for Umbrella was considerably easier than for Wahoo due to the location and depth of the lagoon. Information gathered from the Wahoo shot directly affected the Umbrella plans with regard to target array placement and decontamination times. The device was placed with a buoy at a depth of 150 feet.\n\nJune 9 saw the second underwater nuclear test of the Hardtack I series. The detonation occurred at 1115 with clear skies and a 15 to 17 knot wind from the east-northeast. Within one-tenth of a second, the spray dome had broken through the surface. The overall shape of the spray dome resembled a vertical column with the plumes helping create the shape. Within 20 seconds of detonation the maximum height of the column reached 5,000 feet. The base surge reached a distance around 10,000 feet downwind, reaching out approximately 6,000 feet in all directions. This base surge could be seen from the air for about 25 minutes and the foam patch over the blast location lasting even longer. The blast yield of the nuclear device was measured to be eight kilotons. The Umbrella blast created a crater 3,000 feet (914 m) in diameter and 20 feet (6 m) in depth in the lagoon. The maximum radiation reading was found to be 0.350 R/hr by an aircraft flying over the blast location. Radiation was reported to be of small portions, and re-entry was granted 30 minutes after detonation. Out of the major target ships, the highest radiation reading was logged at 0.0015 R/hr of the stern of the Moran. After all scientific data had been gathered from the target ships, they were restored to be seaworthy and towed back to Pearl Harbor. The Moran was concluded to be unseaworthy due to the blast and was sunk by naval gunfire near Ikuren Island.\n\nThe underwater nuclear tests (Wahoo and Umbrella) gathered scientific data that helped the Department of Defense understand the effects of nuclear weapons on Navy ships. Some of the key points found were that underwater nuclear blasts create less fallout due to the absorption of radioactive material in the water and vaporized droplets.(76) Direct gamma radiation was found to be extremely low, but the base surge was found to be highly radioactive as it passed measurement devices. Smaller radiation concentrations followed in waves mimicking the initial base surge. Almost all free-field radiation activity was found to disappear after fifteen minutes allowing for return to normal operations. The impact of shock waves was less than previously expected. The damage from the shockwave was found to be negligible on the destroyers at the ranges tested, but equipment failure from the blast could immobilize the ships. The preliminary tests of explosive tapered charges were proven to simulate the shock waves of a nuclear device according to the data gathered.\n\n"}
{"id": "22832114", "url": "https://en.wikipedia.org/wiki?curid=22832114", "title": "Radioactive Substances Act 1993", "text": "Radioactive Substances Act 1993\n\nThe Radioactive Substances Act 1993 (RSA93) deals with the control of radioactive material and disposal of radioactive waste in the United Kingdom.\n\nOn 6 April 2010 the Environmental Permitting (England and Wales) Regulations\n2010 came into force. These new regulations repeal, amend and replace much of Radioactive Substances Act 1993 in England and Wales.\n\n"}
{"id": "11005376", "url": "https://en.wikipedia.org/wiki?curid=11005376", "title": "Rotational modulation collimator", "text": "Rotational modulation collimator\n\nRotational modulation collimators (or RMCs) are a specialization of the modulation collimator, an imaging device invented by Minoru Oda. Devices of this type create images of high energy X-rays (or other radiations that cast shadows). Since high energy X-rays are not easily focused, such optics have found applications in various instruments. RMCs selectively block and unblock X-rays in a way which depends on their incoming direction, converting image information into time variations. Various mathematical transformations can then reconstitute the image of the source.\n\nThe Small Astronomy Satellite 3, launched in 1975, was one orbiting experiment that used RMCs. A more recent satellite that used RMCs was\nRHESSI.\n\n\n"}
{"id": "28033632", "url": "https://en.wikipedia.org/wiki?curid=28033632", "title": "S-45A", "text": "S-45A\n\nS-45A was an American satellite, which was lost in a launch failure in 1961. The satellite was intended to operate in a highly elliptical orbit, from which it was to have provided data on the shape of the ionosphere, and on the Earth's magnetic field. It was part of the Explorer programme, and would have been designated Explorer 12 had it reached orbit. It was the second of two identical satellites to be launched; the first, S-45, had also been lost in a launch failure, earlier in the year.\n\nS-45A was launched aboard a Juno II rocket, serial number AM-19G. It was the final flight of the Juno II. The launch took place from Launch Complex 26B at the Cape Canaveral Air Force Station at 19:48:05 UTC on 24 May 1961. The system which was intended to ignite the second stage malfunctioned, and as a result that stage failed to ignite. The rocket failed to achieve orbit.\n"}
{"id": "36931267", "url": "https://en.wikipedia.org/wiki?curid=36931267", "title": "Space Solar Telescope", "text": "Space Solar Telescope\n\nThe Space Solar Telescope (SST) is a planned Chinese optical space solar telescope. It was first proposed in the 1990s, and is intended to be a telescope.\n"}
{"id": "306755", "url": "https://en.wikipedia.org/wiki?curid=306755", "title": "Spitzer Space Telescope", "text": "Spitzer Space Telescope\n\nThe Spitzer Space Telescope (SST), formerly the Space Infrared Telescope Facility (SIRTF), is an infrared space telescope launched in 2003 and still operating as of 2018. It is the fourth and final of the NASA Great Observatories program.\n\nThe planned mission period was to be 2.5 years with a pre-launch expectation that the mission could extend to five or slightly more years until the onboard liquid helium supply was exhausted. This occurred on 15 May 2009. Without liquid helium to cool the telescope to the very low temperatures needed to operate, most of the instruments are no longer usable. However, the two shortest-wavelength modules of the IRAC camera are still operable with the same sensitivity as before the cryogen was exhausted, and have continued to be used to the present in the Spitzer Warm Mission. All \"Spitzer\" data, from both the primary and warm phases, are archived at the Infrared Science Archive (IRSA).\n\nIn keeping with NASA tradition, the telescope was renamed after its successful demonstration of operation, on 18 December 2003. Unlike most telescopes that are named after famous deceased astronomers by a board of scientists, the new name for SIRTF was obtained from a contest open to the general public.\n\nThe contest led to the telescope being named in honor of astronomer Lyman Spitzer, who had promoted the concept of space telescopes in the 1940s. Spitzer wrote a 1946 report for RAND Corporation describing the advantages of an extraterrestrial observatory and how it could be realized with available or upcoming technology. He has been cited for his pioneering contributions to rocketry and astronomy, as well as \"his vision and leadership in articulating the advantages and benefits to be realized from the Space Telescope Program.\"\n\nThe \"Spitzer\" was launched on 25 August 2003 at 05:35:39 UTC from Cape Canaveral SLC-17B aboard a Delta II 7920H rocket.\n\nIt follows a heliocentric instead of geocentric orbit, trailing and drifting away from Earth's orbit at approximately 0.1 astronomical units per year (a so-called \"earth-trailing\" orbit). The primary mirror is in diameter, , made of beryllium and was cooled to . The satellite contains three instruments that allow it to perform astronomical imaging and photometry from 3.6 to 160 micrometers, spectroscopy from 5.2 to 38 micrometers, and spectrophotometry from 5 to 100 micrometers.\n\nBy the early 1970s, astronomers began to consider the possibility of placing an infrared telescope above the obscuring effects of Earth's atmosphere.\nIn 1979, a report from the National Research Council of the National Academy of Sciences, \"A Strategy for Space Astronomy and Astrophysics for the 1980s\", identified a Space Infrared Telescope Facility (SIRTF) as \"one of two major astrophysics facilities [to be developed] for Spacelab\", a Shuttle-borne platform. Anticipating the major results from an upcoming Explorer satellite and from the Shuttle mission, the report also favored the \"study and development of ... long-duration spaceflights of infrared telescopes cooled to cryogenic temperatures.\"\n\nThe launch in January 1983 of the Infrared Astronomical Satellite, jointly developed by the United States, the Netherlands, and the United Kingdom, to conduct the first infrared survey of the sky, whetted the appetites of scientists worldwide for follow-up space missions capitalizing on the rapid improvements in infrared detector technology.\n\nEarlier infrared observations had been made by both space-based and ground-based observatories. Ground-based observatories have the drawback that at infrared wavelengths or frequencies, both the Earth's atmosphere and the telescope itself will radiate (glow) strongly. Additionally, the atmosphere is opaque at most infrared wavelengths. This necessitates lengthy exposure times and greatly decreases the ability to detect faint objects. It could be compared to trying to observe the stars at noon. Previous space observatories (such as IRAS, the Infrared Astronomical Satellite, and ISO, the Infrared Space Observatory) were launched during the 1980s and 1990s and great advances in astronomical technology have been made since then.\n\nMost of the early concepts envisioned repeated flights aboard the NASA Space Shuttle. This approach was developed in an era when the Shuttle program was expected to support weekly flights of up to 30 days duration. A May 1983 NASA proposal described SIRTF as a Shuttle-attached mission, with an evolving scientific instrument payload. Several flights were anticipated with a probable transition into a more extended mode of operation, possibly in association with a future space platform or space station. SIRTF would be a 1-meter class, cryogenically cooled, multi-user facility consisting of a telescope and associated focal plane instruments. It would be launched on the Space Shuttle and remain attached to the Shuttle as a Spacelab payload during astronomical observations, after which it would be returned to Earth for refurbishment prior to re-flight. The first flight was expected to occur about 1990, with the succeeding flights anticipated beginning approximately one year later. However, the Spacelab-2 flight aboard STS-51-F showed that the Shuttle environment was poorly suited to an onboard infrared telescope due to contamination from the relatively \"dirty\" vacuum associated with the orbiters. By September 1983 NASA was considering the \"possibility of a long duration [free-flyer] SIRTF mission\".\n\n\"Spitzer\" is the only one of the Great Observatories not launched by the Space Shuttle, as was originally intended. However, after the 1986 Challenger disaster, the Centaur LH2–LOX upper stage, which would have been required to place it in its final orbit, was banned from Shuttle use. The mission underwent a series of redesigns during the 1990s, primarily due to budget considerations. This resulted in a much smaller but still fully capable mission that could use the smaller Delta II expendable launch vehicle.\n\nOne of the most important advances of this redesign was an Earth-trailing orbit. Cryogenic satellites that require liquid helium (LHe, T ≈ 4 K) temperatures in near-Earth orbit are typically exposed to a large heat load from the Earth, and consequently require large amounts of LHe coolant, which then tends to dominate the total payload mass and limits mission life. Placing the satellite in solar orbit far from Earth allowed innovative passive cooling such as the sun shield, against the single remaining major heat source to drastically reduce the total mass of helium needed, resulting in an overall smaller lighter payload, with major cost savings. This orbit also simplifies telescope pointing, but does require the NASA Deep Space Network for communications.\n\nThe primary instrument package (telescope and cryogenic chamber) was developed by Ball Aerospace & Technologies, in Boulder, Colorado. The individual instruments were developed jointly by industrial, academic, and government institutions, the principals being Cornell, the University of Arizona, the Smithsonian Astrophysical Observatory, Ball Aerospace, and Goddard Spaceflight Center. The shorter-wavelength infrared detectors were developed by Raytheon in Goleta, California. Raytheon used indium antimonide and a doped silicon detector in the creation of the infrared detectors. It is stated that these detectors are 100 times more sensitive than what was once available in the beginning of the project during the 1980s. The far-IR detectors (70 - 160 micrometers) were developed jointly by the University of Arizona and Lawrence Berkeley National Laboratory using Gallium-doped Germanium. The spacecraft was built by Lockheed Martin. The mission is operated and managed by the Jet Propulsion Laboratory and the \"Spitzer Science Center\", located on the Caltech campus in Pasadena, California.\n\n\"Spitzer\" ran out of liquid helium coolant on 15 May 2009, which stopped far-IR observations. Only the IRAC instrument remains in use, and only at the two shorter wavelength bands (3.6 µm and 4.5 µm). The telescope equilibrium temperature is now around , and IRAC continues to produce valuable images at those wavelengths as the \"Spitzer Warm Mission\".\n\n\"Spitzer\" carries three instruments on-board:\n\nThe first images taken by SST were designed to show off the abilities of the telescope and showed a glowing stellar nursery; a big swirling, dusty galaxy; a disc of planet-forming debris; and organic material in the distant universe. Since then, many monthly press releases have highlighted \"Spitzer\" capabilities, as the NASA and ESA images do for the Hubble Space Telescope.\n\nAs one of its most noteworthy observations, in 2005, SST became the first telescope to directly capture light from exoplanets, namely the \"hot Jupiters\" HD 209458 b and TrES-1b, although it did not resolve that light into actual images. This was the first time extrasolar planets had actually been visually seen; earlier observations had been indirectly made by drawing conclusions from behaviors of the stars the planets were orbiting. The telescope also discovered in April 2005 that Cohen-kuhi Tau/4 had a planetary disk that was vastly younger and contained less mass than previously theorized, leading to new understandings of how planets are formed.\n\nWhile some time on the telescope is reserved for participating institutions and crucial projects, astronomers around the world also have the opportunity to submit proposals for observing time. Important targets include forming stars (young stellar objects, or YSOs), planets, and other galaxies. Images are freely available for educational and journalistic purposes.\n\nIn 2004, it was reported that \"Spitzer\" had spotted a faintly glowing body that may be the youngest star ever seen. The telescope was trained on a core of gas and dust known as L1014 which had previously appeared completely dark to ground-based observatories and to ISO (Infrared Space Observatory), a predecessor to \"Spitzer\". The advanced technology of \"Spitzer\" revealed a bright red hot spot in the middle of L1014.\n\nScientists from the University of Texas at Austin, who discovered the object, believe the hot spot to be an example of early star development, with the young star collecting gas and dust from the cloud around it. Early speculation about the hot spot was that it might have been the faint light of another core that lies 10 times further from Earth but along the same line of sight as L1014. Follow-up observation from ground-based near-infrared observatories detected a faint fan-shaped glow in the same location as the object found by \"Spitzer\". That glow is too feeble to have come from the more distant core, leading to the conclusion that the object is located within L1014. (Young \"et al.\", 2004)\n\nIn 2005, astronomers from the University of Wisconsin at Madison and Whitewater determined, on the basis of 400 hours of observation on the Spitzer Space Telescope, that the Milky Way galaxy has a more substantial bar structure across its core than previously recognized.\n\nAlso in 2005, astronomers Alexander Kashlinsky and John Mather of NASA's Goddard Space Flight Center reported that one of \"Spitzer\" earliest images may have captured the light of the first stars in the universe. An image of a quasar in the Draco constellation, intended only to help calibrate the telescope, was found to contain an infrared glow after the light of known objects was removed. Kashlinsky and Mather are convinced that the numerous blobs in this glow are the light of stars that formed as early as 100 million years after the Big Bang, redshifted by cosmic expansion.\n\nIn March 2006, astronomers reported an nebula near the center of the Milky Way Galaxy, the Double Helix Nebula, which is, as the name implies, twisted into a double spiral shape. This is thought to be evidence of massive magnetic fields generated by the gas disc orbiting the supermassive black hole at the galaxy's center, from the nebula and from Earth. This nebula was discovered by \"Spitzer\" and published in the magazine \"Nature\" on 16 March 2006.\n\nIn May 2007, astronomers successfully mapped the atmospheric temperature of HD 189733 b, thus obtaining the first map of some kind of an extrasolar planet.\n\nSince September 2006 the telescope participates in a series of surveys called the Gould Belt Survey, observing the Gould's Belt region in multiple wavelengths. The first set of observations by the Spitzer Space Telescope were completed from 21 September 2006 through 27 September. Resulting from these observations, the team of astronomers led by Dr. Robert Gutermuth, of the Harvard–Smithsonian Center for Astrophysics reported the discovery of Serpens South, a cluster of 50 young stars in the Serpens constellation.\n\nScientists have long wondered how tiny silicate crystals, which need high temperatures to form, have found their way into frozen comets, born in the very cold environment of the Solar System's outer edges. The crystals would have begun as non-crystallized, amorphous silicate particles, part of the mix of gas and dust from which the Solar System developed. This mystery has deepened with the results of the \"Stardust\" sample return mission, which captured particles from Comet Wild 2. Many of the Stardust particles were found to have formed at temperatures in excess of 1000 K.\n\nIn May 2009, \"Spitzer\" researchers from Germany, Hungary and the Netherlands found that amorphous silicate appears to have been transformed into crystalline form by an outburst from a star. They detected the infrared signature of forsterite silicate crystals on the disk of dust and gas surrounding the star EX Lupi during one of its frequent flare-ups, or outbursts, seen by \"Spitzer\" in April 2008. These crystals were not present in \"Spitzer\" previous observations of the star's disk during one of its quiet periods. These crystals appear to have formed by radiative heating of the dust within 0.5 AU of EX Lupi.\n\nIn August 2009, the telescope found evidence of a high-speed collision between two burgeoning planets orbiting a young star.\n\nIn October 2009, astronomers Anne J. Verbiscer, Michael F. Skrutskie, and Douglas P. Hamilton published findings of the \"Phoebe ring\" of Saturn, which was found with the telescope; the ring is a huge, tenuous disc of material extending from 128 to 207 times the radius of Saturn.\n\nGLIMPSE, the \"Galactic Legacy Infrared Mid-Plane Survey Extraordinaire\", is a survey spanning 300° of the inner Milky Way galaxy. It consists of approximately 444,000 images taken at four separate wavelengths using the Infrared Array Camera.\n\nMIPSGAL is a similar survey covering 278° of the galactic disk at longer wavelengths.\n\nOn 3 June 2008, scientists unveiled the largest, most detailed infra-red portrait of the Milky Way, created by stitching together more than 800,000 snapshots, at the 212th meeting of the American Astronomical Society in St. Louis, Missouri. This composite survey is now viewable with the GLIMPSE/MIPSGAL Viewer.\n\n\"Spitzer\" observations, announced in May 2011, indicate that tiny forsterite crystals might be falling down like rain on to the protostar HOPS-68. The discovery of the forsterite crystals in the outer collapsing cloud of the protostar is surprising, because the crystals form at lava-like high temperatures, yet they are found in the molecular cloud where the temperatures are about . This led the team of astronomers to speculate that the bipolar outflow from the young star may be transporting the forsterite crystals from near the star's surface to the chilly outer cloud.\n\nIn January 2012, it was reported that further analysis of the \"Spitzer\" observations of EX Lupi can be understood if the forsterite crystalline dust was moving away from the protostar at a remarkable average speed of . It would appear that such high speeds can arise only if the dust grains had been ejected by a bipolar outflow close to the star. Such observations are consistent with an astrophysical theory, developed in the early 1990s, where it was suggested that bipolar outflows garden or transform the disks of gas and dust that surround protostars by continually ejecting reprocessed, highly heated material from the inner disk, adjacent to the protostar, to regions of the accretion disk further away from the protostar.\n\nIn April 2015, \"Spitzer\" and the Optical Gravitational Lensing Experiment were reported as co-discovering one of the most distant planets ever identified: a gas giant about away from Earth.\nIn June and July 2015, the brown dwarf was discovered using the gravitational microlensing detection method in a joint effort between \"Swift\", \"Spitzer\", and the ground-based Optical Gravitational Lensing Experiment, the first time two space telescopes have observed the same microlensing event. This method was possible because of the large separation between the two spacecraft: \"Swift\" is in low-Earth orbit while \"Spitzer\" is more than one AU distant in an Earth-trailing heliocentric orbit. This separation provided significantly different perspectives of the brown dwarf, allowing for constraints to be placed on some of the object's physical characteristics.\n\nReported in March 2016, \"Spitzer\" and \"Hubble\" were used to discover the most distant-known galaxy, GN-z11. This object was seen as it appeared 13.4 billion years ago.\n\nOn 1 October 2016, \"Spitzer\" began its Observation Cycle 13, a year extended mission nicknamed \"Beyond\". One of the goals of this extended mission is to help prepare for the James Webb Space Telescope, also an infrared telescope, by identifying candidates for more detailed observations.\n\nAnother aspect of the \"Beyond\" mission are the engineering challenges of operating \"Spitzer\" in its progressing orbital phase. As the spacecraft moves farther from Earth on the same orbital path from the Sun, its antenna must point at increasingly higher angles to communicate with ground stations; this change in angle imparts more and more solar heating on the vehicle while its solar panels receive less sunlight.\n\n\"Spitzer\" has been put to work studying exoplanets thanks to creatively tweaking its hardware. This included doubling its stability by modifying its heating cycle, finding a new use for the \"peak-up\" camera, and analyzing the sensor at a sub-pixel level. Although in its \"warm\" mission, the spacecraft's passive cooling system keeps the sensors at . \"Spitzer\" can use the transit photometry and gravitational microlensing techniques to perform these observations. According to NASA's Sean Carey, \"We never even considered using Spitzer for studying exoplanets when it launched. ... It would have seemed ludicrous back then, but now it's an important part of what Spitzer does.\"\n\nExamples of exoplanets discovered using \"Spitzer\" include HD 219134 b in 2015, which was shown to be a rocky planet about 1.5 times as large as Earth in a three-day orbit around its star; and an unnamed planet found using microlensing located about from Earth.\n\nIn September–October 2016, \"Spitzer\" was used to discover five of a total of seven known planets around the star TRAPPIST-1, all of which are approximately Earth sized and likely rocky. Three of the discovered planets are located in the habitable zone, which means they are capable of supporting liquid water given sufficient parameters. Using the transit method, \"Spitzer\" helped measure the sizes of the seven planets and estimate the mass and density of the inner six. Further observations will help determine if there is liquid water on any of the planets.\n\n\n"}
{"id": "6669640", "url": "https://en.wikipedia.org/wiki?curid=6669640", "title": "The Control of Nature", "text": "The Control of Nature\n\nThe Control of Nature is a 1989 book by John McPhee that chronicles three attempts (with varying success) to control natural processes. It is divided into three long essays, \"Atchafalaya\", \"Cooling the Lava\", and \"Los Angeles Against the Mountains\". The Army Corps of Engineers prevents the Mississippi River from changing course, but has had less success in controlling flooding along the river. The residents of Heimaey, Iceland saved their harbor by spraying water on the volcanic lava flow threatening to close it off. The residents of the San Gabriel Mountains have had little success in preventing debris flows from destroying their houses.\n\nIn 1980, McPhee traveled with his daughter on a canoe trip on the Atchafalaya River due to her fascination with the novelist Walker Percy. He had conversations with natives about the efforts made by the Army Corps of Engineers in monitoring riverflow in the area. In Vicksburg, Mississippi, a man recommended to McPhee that he research the efforts being undergone to control the debris sliding down from mountains into Los Angeles. When he visited California, a geologist informed him about lava in Iceland.\n\nThe book's title is derived from a sign on the engineering building at the University of Wyoming. Though he believes nature will win, \"my book is not an editorial,\" McPhee said. \"It is a description of people defying nature. They may have no choice.\" Like all of McPhee's books, \"The Control of Nature\" started out as an outline that he proceeded to fill in.\n\nThe book begins by describing how the Atchafalaya River drains 30 percent of the Mississippi River at its source 300 miles upriver from New Orleans. Thanks to its steeper gradient and more direct route, the Atchafalaya seeks to change the course of the Mississippi as has happened in its long geological history. Due to the Mississippi's vital importance to industry, the Army Corps of Engineers constructed a control structure at the Atchafalaya's source to prevent this from occurring and to maintain the 30 percent drainage. McPhee explains how Morgan City, Louisiana would be destroyed if the river's banks increase. Three million cubic feet of water would inundate the town in the case of a hundred-year flood, though the Corps of Engineers has been trying its hardest to build a more stable flood structure.\n\nAll three essays that comprise \"The Control of Nature\" originally appeared in \"The New Yorker\". After its publication as a book in 1989, \"The Control of Nature\" was McPhee's second-best selling book, after \"Coming into the Country\". It received generally positive reviews from book critics.\n\n\"Los Angeles Times\" critic Jack Miles praised McPhee's \"knack of presenting even the most ordinary folks in their best, most ingenious moments.\" He liked \"Los Angeles Against the Mountains\" the most out of the three essays. He enjoyed how McPhee explained scientific and engineering concepts so a layperson could understand them.\n\nMartin Ruess, writing in \"Technology and Culture\", thought that McPhee should have added more interpretation to his descriptions. \"McPhee has the poet's knack for the telling point, the metaphor that incisively leads to greater understanding,\" Ruess wrote. He believed that McPhee underestimated humanity's ability to exert its control on natural settings, since the U.S. infrastructure effectively did this. Ruess concluded that \"McPhee's reports from the battlefronts are not as valuable as their implicit message that the control of nature is not nearly as important as knowing one's place in it.\"\n"}
{"id": "1625082", "url": "https://en.wikipedia.org/wiki?curid=1625082", "title": "Weak isospin", "text": "Weak isospin\n\nIn particle physics, weak isospin is a quantum number relating to the weak interaction, and parallels the idea of isospin under the strong interaction. Weak isospin is usually given the symbol or with the third component written as formula_1, formula_2, formula_3 or formula_4. It can be understood as the eigenvalue of a charge operator.\n\nThe weak isospin conservation law relates the conservation of formula_2; all weak interactions must preserve formula_2. It is also conserved by the electromagnetic, and strong interactions. However, one of the interactions is with the Higgs field. Since the Higgs field vacuum expectation value is nonzero, particles interact with this field all the time even in vacuum. This changes their weak isospin (and weak hypercharge). Only a specific combination of them, formula_7 (electric charge), is conserved. formula_2 is more important than and often the term \"weak isospin\" refers to the \"3rd component of weak isospin\".\n\nFermions with negative chirality (also called “left-handed” fermions) have formula_9 and can be grouped into doublets with formula_10 that behave the same way under the weak interaction. For example, up-type quarks (u, c, t) have formula_11 and always transform into down-type quarks (d, s, b), which have formula_12, and vice versa. On the other hand, a quark never decays weakly into a quark of the same formula_2. Something similar happens with left-handed leptons, which exist as doublets containing a charged lepton (, , ) with formula_12 and a neutrino (, , ) with formula_11. In all cases, the corresponding \"anti\"-fermion has reversed chirality (“right-handed” antifermion) and sign reversed formula_2.\n\nFermions with positive chirality (“right-handed” fermions) and \"anti\"-fermions with negative chirality (“left-handed” anti-fermions) have formula_17 and form singlets that \"do not undergo weak interactions\".\n\nElectric charge, formula_18, is related to weak isospin, formula_2, and weak hypercharge, formula_20, by\n\nThe symmetry associated with weak isospin is SU(2) and requires gauge bosons with integral formula_2 (, and ) to mediate transformations between fermions with half-integer weak isospin charges. This implies that bosons must have formula_23, with three different values of formula_2:\n\nUnder electroweak unification, the boson mixes with the weak hypercharge gauge boson , resulting in the observed boson and the photon of Quantum Electrodynamics. However, the resulting and the both have weak isospin 0. As a consequence of their weak isospin values and charges, all the electroweak bosons have weak hypercharge formula_33, so unlike gluons and the color force, the electroweak bosons are unaffected by the force they mediate.\n\n"}
{"id": "28222222", "url": "https://en.wikipedia.org/wiki?curid=28222222", "title": "William Matthew Hart", "text": "William Matthew Hart\n\nWilliam Matthew Hart (1830-1908) was an English bird illustrator and lithographer who worked for John Gould.\n\nHart started medical training, but was unable to complete his studies for financial reasons. He began working for Gould in 1851, beginning an association that was to last thirty years. Early during this period he made the patterns for the lithographic plates for Gould’s work on hummingbirds, as well as working on \"The Birds of Great Britain\" with Henry Constantine Richter. By 1870 Hart had become Gould's chief artist and lithographer. After Gould's death in 1881, Hart was employed by Richard Bowdler-Sharpe of the British Museum to complete Gould's work on the birds of New Guinea and to produce illustrations for Sharpe’s monograph on the birds-of-paradise.\n\n"}
{"id": "3788537", "url": "https://en.wikipedia.org/wiki?curid=3788537", "title": "WormBook", "text": "WormBook\n\nWormBook is an open access, comprehensive collection of original, peer-reviewed chapters covering topics related to the of the nematode worm \"Caenorhabditis elegans (C. elegans)\". WormBook also includes WormMethods, an up-to-date collection of methods and protocols for \"C. elegans\" researchers.\n\nWormBook is the online text companion to WormBase, the \"C. elegans\" model organism database. Capitalizing on the World Wide Web, WormBook links in-text references (e.g. genes, alleles, proteins, literature citations) with primary biological databases such as WormBase and PubMed. \"C. elegans\" was the first multicellular organism to have its genome sequenced and is a model organism for studying developmental genetics and neurobiology.\n\nThe content of WormBook is categorized into the sections listed below, each filled with a variety of relevant chapters. These sections include:\n\n"}
