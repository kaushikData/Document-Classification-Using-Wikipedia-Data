{"id": "21123525", "url": "https://en.wikipedia.org/wiki?curid=21123525", "title": "AAAS Award for Scientific Freedom and Responsibility", "text": "AAAS Award for Scientific Freedom and Responsibility\n\nThe AAAS Award for Scientific Freedom and Responsibility is given by the American Association for the Advancement of Science and honours scientists and engineers whose exemplary actions, often taken at significant personal cost, have served to foster scientific freedom and responsibility and increased scientific awareness throughout the world. The establishment of this new Award for Scientific Freedom and Responsibility was announced by AAAS executive officer William D. Carey on 23 October 1980. The award, which will be presented for the first time at the 1982 AAAS Annual Meeting in Washington, DC, will consist of a plaque and a cash prize of $1,000. According to the AAAS, these types of exemplary actions include \"acting to protect the public's health, safety or welfare; focusing public attention on important potential impacts of science and technology on society by their responsible participation in public policy debates.\" The 2018 recipient of the AAAS Award for Scientific Freedom and Responsibility was civil and environmental engineer Marc Edwards. The AAAS stated that he was given the award for his ability to \"to apply his engineering expertise to revealing dangerous levels of lead contamination in water supplies\" in the area of Flint Michigan. According to Marc, Flint Michigan \"represents misconduct by local and federal government engineers and scientists... and allowed an unprecedented exposure to the best known nerve toxin in the most powerful city in America, and perhaps even the world\".\n\n\n\n"}
{"id": "7250189", "url": "https://en.wikipedia.org/wiki?curid=7250189", "title": "Adventure playground", "text": "Adventure playground\n\nAn adventure playground is a specific type of playground for children. Adventure playgrounds can take many forms, ranging from \"natural playgrounds\" to \"junk playgrounds\", and are typically defined by an ethos of unrestricted play, the presence of playworkers (or \"wardens\"), and the absence of adult-manufactured or rigid play-structures. Adventure playgrounds are frequently defined in contrast to playing fields, contemporary-design playgrounds made by adult architects, and traditional-equipment play areas containing adult-made rigid play-structures like swings, slides, seesaws, and climbing bars.\n\nHarry Shier, in \"Adventure Playgrounds: an introduction\" (1984) defines an Adventure Playground this way:\n\nThe first \"planned\" playground of this type, the Emdrup Junk Playground, opened in Emdrup, Denmark, in 1943. In 1948, an adventure playground opened in Camberwell, England. The term \"junk playground\" is a calque from the Danish term \"skrammellegeplads\". Early examples of adventure playgrounds in the UK were known as \"junk playgrounds\", \"waste material playgrounds\", or \"bomb-site adventure playgrounds\". The term \"adventure playground\" was first adopted in the United Kingdom to describe waste material playgrounds \"in an effort to make the ‘junk’ playground concept more palatable to local authorities\". \n\nThe first junk playgrounds were based on the ideas of Carl Theodor Sørensen, a Danish landscape architect, who noticed that children preferred to play everywhere but in the playgrounds that he designed. In 1931, inspired by the sight of children playing in a construction site, he imagined \"A junk playground in which children could create and shape, dream and imagine a reality\". His aim was to provide children living in cities the same opportunities for play that were enjoyed by children living in rural areas. The first adventure playground was set up by a Workers Cooperative Housing Association in Emdrup, Denmark, during the German occupation of the 1940s. The playground at Emdrup grew out of the spirit of resistance to Nazi occupation and parents' fears that \"their children's play might be mistaken for acts of sabotage by soldiers\". Play advocates sometimes emphasize the importance of adventure playgrounds for children of color in the United States, where policing \"can feel like a kind of occupation\".\n\nMarjory Allen, an English landscape architect and child welfare advocate, visited and subsequently wrote a widely-read article about the Emdrup Adventure playground titled \"Why Not Use Our Bomb Sites Like This?\" and published in the Picture Post in 1946. While Marjory Allen's article is often credited with the introduction into the UK of \"the idea of transforming bomb sites into 'junk playgrounds', historians of the Adventure playground movement have pointed to the role played by other experiments carried out by youth workers in the UK. For example, \"Marie Paneth, an art therapist heavily influenced by Freud, independently developed the concept of permissive play as a tool for ameliorating childhood aggression in her work running a blitz-era play centre in London although not specifically incorporating the elements of a Junk/Adventure playground pointing to her role in the history of UK specific Playwork development.\"\n\nTo date, there are approximately 1,000 adventure playgrounds in Europe, most of them in England, Denmark, France, Germany, The Netherlands and Switzerland. Japan also has a significant number of adventure playgrounds.\n\n\n\n\n\n\nDenmark has several adventure playgrounds, now known as \"Byggelegeplads\" (Building-playground) and formerly as \"Skrammellegeplads\" (Junk-playground). From the first site in Emdrup, the idea spread across the country and at the height of the popularity in the 1960s, there were about 100 adventure playgrounds in the country. Present active adventure playgrounds in Denmark includes:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "751106", "url": "https://en.wikipedia.org/wiki?curid=751106", "title": "Anchoring", "text": "Anchoring\n\nAnchoring or focalism is a cognitive bias for an individual to rely too heavily on an initial piece of information offered (known as the \"anchor\") when making decisions.\n\nThe original description of the anchoring effect came from psychophysics where it was noticed when judging stimuli along a continuum that the first and last stimuli were used to compare the other stimuli. This is often referred to as end anchoring. This was applied to attitudes by Sherif et al. in 1958 in their article \"Assimilation and contrast effects of anchoring stimuli on judgments\".\n\nDuring decision making, anchoring occurs when individuals use this initial piece of information to make subsequent judgments. Those objects near the anchor tend to be assimilated toward it and those further away tend to be displaced in the other direction. Once the value of this anchor is set, all future negotiations, arguments, estimates, etc. are discussed in relation to the anchor. This bias occurs when interpreting future information using this anchor. For example, the initial price offered for a used car, set either before or at the start of negotiations, sets an arbitrary focal point for all following discussions. Prices discussed in negotiations that are lower than the anchor may seem reasonable, perhaps even cheap to the buyer, even if said prices are still relatively higher than the actual market value of the car.\n\nThe focusing effect (or focusing illusion) is a cognitive bias that occurs when people place too much importance on one aspect of an event, causing an error in accurately predicting the utility of a future outcome.\n\nPeople focus on notable differences, excluding those that are less conspicuous, when making predictions about happiness or convenience. For example, when people were asked how much happier they believe Californians are compared to Midwesterners, Californians and Midwesterners both said Californians must be considerably happier, when, in fact, there was no difference between the actual happiness rating of Californians and Midwesterners. The bias lies in that most people focused on and overweighed the sunny weather and ostensibly easy-going lifestyle of California and devalued and underrated other aspects of life and determinants of happiness, such as low crime rates and safety from natural disasters like earthquakes (both of which large parts of California lack).\n\nA rise in income has only a small and transient effect on happiness and well-being, but people consistently overestimate this effect. Academics Daniel Kahneman, along with A. Krueger, D. Schkade, N. Schwarz & A. Stone proposed that this is a result of a focusing illusion, with people focusing on conventional measures of achievement rather than on everyday routine.\n\nAnchoring and adjustment is a psychological heuristic that influences the way people intuitively assess probabilities. According to this heuristic, people start with an implicitly suggested reference point (the \"anchor\") and make adjustments to it to reach their estimate. A person begins with a first approximation (anchor) and then makes incremental adjustments based on additional information. These adjustments are usually insufficient, giving the initial anchor a great deal of influence over future assessments.\nThe anchoring and adjustment heuristic was first theorized by Amos Tversky and Daniel Kahneman. In one of their first studies, participants were asked to compute, within 5 seconds, the product of the numbers one through eight, either as formula_1 or reversed as formula_2. Because participants did not have enough time to calculate the full answer, they had to make an estimate after their first few multiplications. When these first multiplications gave a small answer – because the sequence started with small numbers – the median estimate was 512; when the sequence started with the larger numbers, the median estimate was 2,250. (The correct answer is 40,320.) In another study by Tversky and Kahneman, participants observed a roulette wheel that was predetermined to stop on either 10 or 65. Participants were then asked to guess the percentage of the United Nations that were African nations. Participants whose wheel stopped on 10 guessed lower values (25% on average) than participants whose wheel stopped at 65 (45% on average). The pattern has held in other experiments for a wide variety of different subjects of estimation.\n\nAs a second example, in a study by Dan Ariely, an audience is first asked to write the last two digits of their social security number and consider whether they would pay this number of dollars for items whose value they did not know, such as wine, chocolate and computer equipment. They were then asked to bid for these items, with the result that the audience members with higher two-digit numbers would submit bids that were between 60 percent and 120 percent higher than those with the lower social security numbers, which had become their anchor.\n\nVarious studies have shown that anchoring is very difficult to avoid. For example, in one study students were given anchors that were obviously wrong. They were asked whether Mahatma Gandhi died before or after age 9, or before or after age 140. Clearly neither of these anchors can be correct, but when the two groups were asked to suggest when they thought he had died, they guessed significantly differently (average age of 50 vs. average age of 67).\n\nOther studies have tried to eliminate anchoring much more directly. In a study exploring the causes and properties of anchoring, participants were exposed to an anchor and asked to guess how many physicians were listed in the local phone book. In addition, they were explicitly informed that anchoring would \"contaminate\" their responses, and that they should do their best to correct for that. A control group received no anchor and no explanation. Regardless of how they were informed and whether they were informed correctly, all of the experimental groups reported higher estimates than the control group. Thus, despite being expressly aware of the anchoring effect, participants were still unable to avoid it. A later study found that even when offered monetary incentives, people are unable to effectively adjust from an anchor.\n\nSeveral theories have been put forth to explain what causes anchoring, although some explanations are more popular than others, there is no consensus as to which is best. In a study on possible causes of anchoring, two authors described anchoring as easy to demonstrate, but hard to explain. At least one group of researchers has argued that multiple causes are at play, and that what is called \"anchoring\" is actually several different effects.\n\nIn their original study, Tversky and Kahneman put forth a view later termed anchoring-as-adjustment. According to this theory, once an anchor is set, people adjust away from it to get to their final answer; however, they adjust insufficiently, resulting in their final guess being closer to the anchor than it would be otherwise. Other researchers also found evidence supporting the anchoring-and-adjusting explanation.\n\nHowever, later researchers criticized this model, because it is only applicable when the initial anchor is outside the range of acceptable answers. To use an earlier example, since Mahatma Gandhi obviously did not die at age 9, then people will adjust from there. If a reasonable number were given, though, there would be no adjustment. Therefore, this theory cannot, according to its critics, explain the anchoring effect.\n\nAnother study found that the anchoring effect holds even when the anchor is subliminal. According to Tversky and Kahneman's theory, this is impossible, since anchoring is only the result of conscious adjustment. Because of arguments like these, anchoring-and-adjusting has fallen out of favor.\n\nIn the same study that criticized anchoring-and-adjusting, the authors proposed an alternate explanation regarding selective accessibility, which is derived from a theory called \"confirmatory hypothesis testing\". In short, selective accessibility proposes that when given an anchor, a judge (i.e. a person making some judgment) will evaluate the hypothesis that the anchor thinks is a suitable answer. Assuming it is not, the judge moves on to another guess, but not before accessing all the relevant attributes of the anchor itself. Then, when evaluating the new answer, the judge looks for ways in which it is similar to the anchor, resulting in the anchoring effect. Various studies have found empirical support for this hypothesis. This explanation assumes that the judge considers the anchor to be a plausible value so that it is not immediately rejected, which would preclude considering its relevant attributes.\n\nMore recently, a third explanation of anchoring has been proposed concerning attitude change. According to this theory, providing an anchor changes someone's attitudes to be more favorable to the particular attributes of that anchor, biasing future answers to have similar characteristics as the anchor. Leading proponents of this theory consider it to be an alternate explanation in line with prior research on anchoring-and-adjusting and selective accessibility.\n\nA wide range of research has linked sad or depressed moods with more extensive and accurate evaluation of problems. As a result of this, earlier studies hypothesized that people with more depressed moods would tend to use anchoring less than those with happier moods. However, more recent studies have shown the opposite effect: sad people are \"more\" likely to use anchoring than people with happy or neutral mood.\n\nEarly research found that experts (those with high knowledge, experience, or expertise in some field) were more resistant to the anchoring effect. Since then, however, numerous studies have demonstrated that while experience can sometimes reduce the effect, even experts are susceptible to anchoring. In a study concerning the effects of anchoring on judicial decisions, researchers found that even experienced legal professionals were affected by anchoring. This remained true even when the anchors provided were arbitrary and unrelated to the case in question.\n\nResearch has correlated susceptibility to anchoring with most of the Big Five personality traits. People high in agreeableness and conscientiousness are more likely to be affected by anchoring, while those high in extraversion are less likely to be affected. Another study found that those high in openness to new experiences were more susceptible to the anchoring effect.\n\nThe impact of cognitive ability on anchoring is contested. A recent study on willingness to pay for consumer goods found that anchoring decreased in those with greater cognitive ability, though it did not disappear. Another study, however, found that cognitive ability had no significant effect on how likely people were to use anchoring.\n\nIn negotiations, anchoring is setting a boundary that outlines the basic constraints for a negotiation. The anchoring effect is where we set our estimation for the true value of the item at hand. In addition to the initial research conducted by Tversky and Kahneman, multiple other studies have shown that anchoring can greatly influence the estimated value of an object. For instance, although negotiators can generally appraise an offer based on multiple characteristics, studies have shown that they tend to focus on only one aspect. In this way, a deliberate starting point can strongly affect the range of possible counteroffers. The process of offer and counteroffer results in a mutually beneficial arrangement. However, multiple studies have shown that initial offers have a stronger influence on the outcome of negotiations than subsequent counteroffers.\n\nAn example of the power of anchoring has been conducted during the Strategic Negotiation Process Workshops. During the workshop, a group of participants is divided into two sections: buyers and sellers. Each side receives identical information about the other party before going into a one-on-one negotiation. Following this exercise, both sides debrief about their experiences. The results show that where the participants anchor the negotiation had a significant effect on their success.\n\nAnchoring affects everyone, even people who are highly knowledgeable in a field. Northcraft and Neale conducted a study to measure the difference in the estimated value of a house between students and real-estate agents. In this experiment, both groups were shown a house and then given different listing prices. After making their offer, each group was then asked to discuss what factors influenced their decisions. In the follow-up interviews, the real-estate agents denied being influenced by the initial price, but the results showed that both groups were equally influenced by that anchor.\n\nAnchoring can have more subtle effects on negotiations as well. Janiszewski and Uy investigated the effects of precision of an anchor. Participants read an initial price for a beach house, then gave the price they thought it was worth. They received either a general, seemingly nonspecific anchor (e.g., $800,000) or a more precise and specific anchor (e.g., $799,800). Participants with a general anchor adjusted their estimate more than those given a precise anchor ($751,867 vs $784,671). The authors propose that this effect comes from difference in scale; in other words, the anchor affects not only the starting \"value\", but also the starting \"scale\". When given a general anchor of $20, people will adjust in large increments ($19, $21, etc.), but when given a more specific anchor like $19.85, people will adjust on a lower scale ($19.75, $19.95, etc.). Thus, a more specific initial price will tend to result in a final price closer to the initial one.\n\n"}
{"id": "47350206", "url": "https://en.wikipedia.org/wiki?curid=47350206", "title": "Andromeda XVIII", "text": "Andromeda XVIII\n\nAndromeda XVIII, discovered in 2008, is a dwarf spheroidal galaxy (has no rings, low luminosity, much dark matter, little gas or dust), which is a satellite of the Andromeda Galaxy (M31). It is one of the 14 known dwarf galaxies orbiting M31. It was announced in 2010 that the orbiting galaxies lie close to a plane running through M31's center.\n\n"}
{"id": "28914230", "url": "https://en.wikipedia.org/wiki?curid=28914230", "title": "Barrett Glacier", "text": "Barrett Glacier\n\nBarrett Glacier () is a glacier draining from the north slopes of the Prince Olav Mountains, about long, flowing between the Longhorn Spurs and the Gabbro Hills to the Ross Ice Shelf. It was named by the Southern Party of the New Zealand Geological Survey Antarctic Expedition (1963–64) for Peter J. Barrett, a geologist with that party.\n\n"}
{"id": "29332921", "url": "https://en.wikipedia.org/wiki?curid=29332921", "title": "Borchgrevinkisen", "text": "Borchgrevinkisen\n\nBorchgrevinkisen () is a glacier flowing northward to the west of Taggen Nunatak, at the west end of the Sør Rondane Mountains. It was mapped by Norwegian cartographers in 1957 from air photos taken by U.S. Navy Operation Highjump, 1946–47, and named for Carsten E. Borchgrevink, Norwegian leader of the British Antarctic Expedition, 1898–1900.\n\n"}
{"id": "4765289", "url": "https://en.wikipedia.org/wiki?curid=4765289", "title": "Bovine herpesvirus 2", "text": "Bovine herpesvirus 2\n\nBovine herpesvirus 2 (BoHV2) is a virus of the family \"Herpesviridae\" that causes two diseases in cattle, bovine mammillitis and pseudo-lumpy skin disease. BoHV2 is similar in structure to human herpes simplex virus.\n\nPseudo-lumpy skin disease was originally discovered in South Africa where a similar but more serious disease caused by a poxvirus, lumpy skin disease, is also prevalent. Symptoms include fever and skin nodules on the face, back, and perineum. The disease heals within a few weeks. Bovine mammillitis is characterized by lesions restricted to the teats and udder. BoHV-2 probably spreads through an arthropod vector, but can also be spread through milkers and milking machines.\n\nA review publication from 2011 presents a series of controversial but scientifically based conclusions concerning the pathogenesis and epidemiology of the infection (summary: https://sites.google.com/site/bovineherpesvirus2/), among these that spread among cattle is preferably by the respiratory route, and that skin lesions result from viremic spread to epidermal foci and inflammation due to complement activation by the classical pathway at sites of virus propagation after formation of early antibody to BoHV2. Lesions may be aggravated by low skin temperature (e.g. in edematic or hairless skin areas) causing reduced blood circulation and hampered removal of cell-toxic inflammatory substances.\n"}
{"id": "18506619", "url": "https://en.wikipedia.org/wiki?curid=18506619", "title": "Capitalist Patriarchy and the Case for Socialist Feminism", "text": "Capitalist Patriarchy and the Case for Socialist Feminism\n\nCapitalist Patriarchy and the Case for Socialist Feminism is a 1978 anthology about socialist feminism edited by Zillah R. Eisenstein.\nThe sociologist Rhonda F. Levine cites Eisenstein's work as a \"superb discussion of the socialist-feminist position\". Levine goes on to describe the book as \"one of the earliest statements of how a Marxist class analysis can combine with a feminist analysis of patriarchy to produce a theory of how gender and class intersect as systems of inequality\".\n\n\"Eisenstein defines the term 'capitalist patriarchy' as descriptive of the 'mutually reinforcing dialectical relationship between capitalist class structure and hierarchical sexual structuring.\"\n\nShe believes that \"The recognition of women as a sexual class lays the subversive quality of feminism for liberalism because liberalism is premised upon women's exclusion from public life on this very class basis. The demand for real equality of women with men, if taken to its logical conclusion, would dislodge the patriarchal structure necessary to a liberal society.\"\n"}
{"id": "23433075", "url": "https://en.wikipedia.org/wiki?curid=23433075", "title": "Card scheme", "text": "Card scheme\n\nCard schemes are payment networks linked to payment cards, such as debit or credit cards, of which a bank or any other eligible financial institution can become a member. By becoming a member of the scheme, the member then gets the possibility to issue or acquire cards operating on the network of that card scheme.\n\nThe card schemes come in two main varieties - a three-party scheme (or closed scheme) or a four-party scheme (or open scheme).\n\nA three-party scheme consists of three main parties as described in the adjacent diagram.\n\nIn this model, the issuer (having the relationship with the cardholder) and the acquirer (having the relationship with the merchant) is the same entity. This means that there is no need for any charges between the issuer and the acquirer. Since it is a franchise setup, there is only one franchisee in each market, which is the incentive in this model. There is no competition within the brand; rather you compete with other brands.\n\nExamples of this setup are Diners Club, Discover Card, and American Express, although in recent times these schemes have also partnered with other issuers and acquirers in order to boost their circulation and acceptance, and Diners Club now operates as a four-party scheme in many regions.\n\nIn a four-party scheme, the issuer and acquirer are different entities, and this type of scheme is open for other institutions to join and issue their own cards. This signifies card schemes such as Visa, Mastercard, Verve Card, UnionPay and RuPay. There are no limitations as to who may join the scheme, as long as the requirements of the scheme are met.\n"}
{"id": "50038565", "url": "https://en.wikipedia.org/wiki?curid=50038565", "title": "Carol Christian", "text": "Carol Christian\n\nCarol Ann Christian (born 28 December 1950) is an American astronomer and science communicator, who works for the Space Telescope Science Institute (STScI; the science operations center for the Hubble Space Telescope) as a scientist on the institute's outreach program.\n\nChristian was born in Cincinnati, Ohio, and studied astronomy and physics at Boston University, from which she graduated with a PhD in 1979 with a thesis on \"Investigations of distant field stars and clusters in the galactic anticenter\". She then worked as an astronomer for University of California, Berkeley.\n\nIn August 1995, Christian was selected as the first head of STScI's new Office of Public Outreach after a national search. She has continued to act as an outreach scientist for the institute as a media spokesperson, educator and author. From 2003 to 2006, she worked as a scientific policy advisor for the State Department. In 2010, she co-authored \"A Question and Answer Guide to Astronomy\" with Pierre-Yves Bely and Jean-René Roy.\n\n"}
{"id": "20161937", "url": "https://en.wikipedia.org/wiki?curid=20161937", "title": "Collaborative engineering", "text": "Collaborative engineering\n\nCollaborative engineering is defined by the International Journal of Collaborative Engineering as a discipline that \"studies the interactive process of engineering collaboration, whereby multiple interested stakeholders resolve conflicts, bargain for individual or collective advantages, agree upon courses of action, and/or attempt to craft joint outcomes which serve their mutual interests.\"\n\nCollaborative engineering is quickly becoming a topic of great interest in recent years due to the explosion of internet technologies. This upsurge is partially due to the success of projects such as Wikipedia and Linux that have proven the efficacy of internet collaboration.\n\nK. Daniel: Collaborative Engineering, Vdm Verlag Dr. Müller, , 2007\n"}
{"id": "19989669", "url": "https://en.wikipedia.org/wiki?curid=19989669", "title": "Cypress forest", "text": "Cypress forest\n\nA Cypress forest is a western United States plant association typically dominated by one or more cypress species. Example species comprising the canopy include \"Cupressus macrocarpa\". In some cases these forests have been severely damaged by goats, cattle and other grazing animals. While cypress species are clearly dominant within a Cypress forest, other trees such as California Buckeye, \"Aesculus californica\", are found in some Cypress forests.\n\nThe Guadalupe Island Cypress Forest is situated on Guadalupe Island, offshore from Baja California. This forest was greatly destroyed by the introduction of grazing goats, but conservation biology efforts have been conducted to assist in restoring the forest.\n\nAnother example on the Pacific Coast mainland of Northern California is the Sargent's cypress Forest, located in coastal Marin County, California.\n\n\n"}
{"id": "47176237", "url": "https://en.wikipedia.org/wiki?curid=47176237", "title": "Daniel Sarokon", "text": "Daniel Sarokon\n\nDaniel Sarokon (1928 - January 1, 2006) was a NASA Launch Conductor, described as 'one of the most influential people in the history of space travel. His first launch was that of the Moon probe Surveyor 1 and in his career he supervised 30 lunar and planetary missions.\n\nThe launch of the \"New Horizons\" Pluto mission in January 2006 was dedicated in his honour.\n\n\n"}
{"id": "8651", "url": "https://en.wikipedia.org/wiki?curid=8651", "title": "Dark matter", "text": "Dark matter\n\nDark matter is a hypothetical form of matter that is thought to account for approximately 85% of the matter in the universe, and about a quarter of its total energy density. The majority of dark matter is thought to be non-baryonic in nature, possibly being composed of some as-yet undiscovered subatomic particles. Its presence is implied in a variety of astrophysical observations, including gravitational effects that cannot be explained unless more matter is present than can be seen. For this reason, most experts think dark matter to be ubiquitous in the universe and to have had a strong influence on its structure and evolution. The name dark matter refers to the fact that it does not appear to interact with observable electromagnetic radiation, such as light, and is thus invisible (or 'dark') to the entire electromagnetic spectrum, making it extremely difficult to detect using usual astronomical equipment.\n\nThe primary evidence for dark matter is that calculations show that many galaxies would fly apart instead of rotating, or would not have formed or move as they do, if they did not contain a large amount of unseen matter. Other lines of evidence include observations in gravitational lensing, from the cosmic microwave background, from astronomical observations of the observable universe's current structure, from the formation and evolution of galaxies, from mass location during galactic collisions, and from the motion of galaxies within galaxy clusters. In the standard Lambda-CDM model of cosmology, the total mass–energy of the universe contains 5% ordinary matter and energy, 27% dark matter and 68% of an unknown form of energy known as dark energy. Thus, dark matter constitutes 85% of total mass, while dark energy plus dark matter constitute 95% of total mass–energy content.\n\nBecause dark matter has not yet been observed directly, it must barely interact with ordinary baryonic matter and radiation. The primary candidate for dark matter is some new kind of elementary particle that has not yet been discovered, in particular, weakly-interacting massive particles (WIMPs), or gravitationally-interacting massive particles (GIMPs). Many experiments to directly detect and study dark matter particles are being actively undertaken, but none has yet succeeded. Dark matter is classified as cold, warm, or hot according to its velocity (more precisely, its free streaming length). Current models favor a cold dark matter scenario, in which structures emerge by gradual accumulation of particles.\n\nAlthough the existence of dark matter is generally accepted by the scientific community, some astrophysicists, intrigued by certain observations that do not fit the dark matter theory, argue for various modifications of the standard laws of general relativity, such as modified Newtonian dynamics, tensor–vector–scalar gravity, or entropic gravity. These models attempt to account for all observations without invoking supplemental non-baryonic matter.\n\nThe hypothesis of dark matter has an elaborate history. In a talk given in 1884, Lord Kelvin estimated the number of dark bodies in the Milky Way from the observed velocity dispersion of the stars orbiting around the center of the galaxy. By using these measurements, he estimated the mass of the galaxy, which he determined is different from the mass of visible stars. Lord Kelvin thus concluded that \"many of our stars, perhaps a great majority of them, may be dark bodies\". In 1906 Henri Poincaré in \"The Milky Way and Theory of Gases\" used \"dark matter\", or \"matière obscure\" in French, in discussing Kelvin's work.\n\nThe first to suggest the existence of dark matter, using stellar velocities, was Dutch astronomer Jacobus Kapteyn in 1922. Fellow Dutchman and radio astronomy pioneer Jan Oort also hypothesized the existence of dark matter in 1932. Oort was studying stellar motions in the local galactic neighborhood and found that the mass in the galactic plane must be greater than what was observed, but this measurement was later determined to be erroneous.\n\nIn 1933, Swiss astrophysicist Fritz Zwicky, who studied galaxy clusters while working at the California Institute of Technology, made a similar inference. Zwicky applied the virial theorem to the Coma Cluster and obtained evidence of unseen mass that he called \"dunkle Materie\" ('dark matter'). Zwicky estimated its mass based on the motions of galaxies near its edge and compared that to an estimate based on its brightness and number of galaxies. He estimated that the cluster had about 400 times more mass than was visually observable. The gravity effect of the visible galaxies was far too small for such fast orbits, thus mass must be hidden from view. Based on these conclusions, Zwicky inferred that some unseen matter provided the mass and associated gravitation attraction to hold the cluster together. This was the first formal inference about the existence of dark matter. Zwicky's estimates were off by more than an order of magnitude, mainly due to an obsolete value of the Hubble constant; the same calculation today shows a smaller fraction, using greater values for luminous mass. However, Zwicky did correctly infer that the bulk of the matter was dark.\n\nThe first robust indications that the mass-to-light ratio was anything other than unity came from measurements of galaxy rotation curves. In 1939, Horace W. Babcock reported the rotation curve for the Andromeda nebula (known now as the Andromeda Galaxy), which suggested that the mass-to-luminosity ratio increases radially. He attributed it to either light absorption within the galaxy or modified dynamics in the outer portions of the spiral and not to missing matter.\n\nVera Rubin and Kent Ford in the 1960s and 1970s provided further strong evidence, also using galaxy rotation curves. Rubin worked with a new spectrograph to measure the velocity curve of edge-on spiral galaxies with greater accuracy. This result was confirmed in 1978. An influential paper presented Rubin's results in 1980. Rubin found that most galaxies must contain about six times as much dark as visible mass; thus, by around 1980 the apparent need for dark matter was widely recognized as a major unsolved problem in astronomy.\n\nAt the same time that Rubin and Ford were exploring optical rotation curves, radio astronomers were making use of new radio telescopes to map the 21 cm line of atomic hydrogen in nearby galaxies. The radial distribution of interstellar atomic hydrogen (HI) often extends to much larger galactic radii than those accessible by optical studies, allowing the sampling of rotation curves—and thus of the total mass distribution—to a new dynamical regime. Early mapping of Andromeda with the 300-foot telescope at Green Bank and the 250-foot dish at Jodrell Bank already showed that the HI rotation curve did not trace the expected Keplerian decline. As more sensitive receivers became available, Morton Roberts and Robert Whitehurst were able to trace the rotational velocity of Andromeda to 30 kpc, much beyond the optical measurements. Illustrating the advantage of tracing the gas disk at large radii, Figure 16 of that paper combines the optical data (the cluster of points at radii of less than 15 kpc with a single point further out) with the HI data between 20 and 30 kpc, exhibiting the flatness of the outer galaxy rotation curve; the solid curve peaking at the center is the optical surface density, while the other curve shows the cumulative mass, still rising linearly at the outermost measurement. In parallel, the use of interferometric arrays for extragalactic HI spectroscopy was being developed. In 1972, David Rogstad and Seth Shostak published HI rotation curves of five spirals mapped with the Owens Valley interferometer; the rotation curves of all five were very flat, suggesting very large values of mass-to-light ratio in the outer parts of their extended HI disks.\n\nA stream of observations in the 1980s supported the presence of dark matter, including gravitational lensing of background objects by galaxy clusters, the temperature distribution of hot gas in galaxies and clusters, and the pattern of anisotropies in the cosmic microwave background. According to consensus among cosmologists, dark matter is composed primarily of a not yet characterized type of subatomic particle. The search for this particle, by a variety of means, is one of the major efforts in particle physics.\n\nIn standard cosmology, matter is anything whose energy density scales with the inverse cube of the scale factor, i.e., \"ρ ∝ a\". This is in contrast to radiation, which scales as the inverse fourth power of the scale factor \" ρ ∝ a \", and a cosmological constant, which is independent of \"a\". These scalings can be understood intuitively: for an ordinary particle in a cubical box, doubling the length of the sides of the box decreases the density (and hence energy density) by a factor of eight (\"2\"). For radiation, the decrease in energy density is larger because an increase in scale factor causes a proportional redshift. A cosmological constant, as an intrinsic property of space, has a constant energy density regardless of the volume under consideration.\n\nIn principle, \"dark matter\" means all components of the universe that are not visible but still obey \"ρ ∝ a\". In practice, the term \"dark matter\" is often used to mean only the non-baryonic component of dark matter, i.e., excluding \"missing baryons.\" Context will usually indicate which meaning is intended.\n\nThe arms of spiral galaxies rotate around the galactic center. The luminous mass density of a spiral galaxy decreases as one goes from the center to the outskirts. If luminous mass were all the matter, then we can model the galaxy as a point mass in the centre and test masses orbiting around it, similar to the Solar System. From Kepler's Second Law, it is expected that the rotation velocities will decrease with distance from the center, similar to the Solar System. This is not observed. Instead, the galaxy rotation curve remains flat as distance from the center increases.\n\nIf Kepler's laws are correct, then the obvious way to resolve this discrepancy is to conclude that the mass distribution in spiral galaxies is not similar to that of the Solar System. In particular, there is a lot of non-luminous matter (dark matter) in the outskirts of the galaxy.\n\nStars in bound systems must obey the virial theorem. The theorem, together with the measured velocity distribution, can be used to measure the mass distribution in a bound system, such as elliptical galaxies or globular clusters. With some exceptions, velocity dispersion estimates of elliptical galaxies do not match the predicted velocity dispersion from the observed mass distribution, even assuming complicated distributions of stellar orbits.\n\nAs with galaxy rotation curves, the obvious way to resolve the discrepancy is to postulate the existence of non-luminous matter.\n\nGalaxy clusters are particularly important for dark matter studies since their masses can be estimated in three independent ways:\n\n\nGenerally, these three methods are in reasonable agreement that dark matter outweighs visible matter by approximately 5 to 1.\n\nOne of the consequences of general relativity is that massive objects (such as a cluster of galaxies) lying between a more distant source (such as a quasar) and an observer should act as a lens to bend the light from this source. The more massive an object, the more lensing is observed.\n\nStrong lensing is the observed distortion of background galaxies into arcs when their light passes through such a gravitational lens. It has been observed around many distant clusters including Abell 1689. By measuring the distortion geometry, the mass of the intervening cluster can be obtained. In the dozens of cases where this has been done, the mass-to-light ratios obtained correspond to the dynamical dark matter measurements of clusters. Lensing can lead to multiple copies of an image. By analyzing the distribution of multiple image copies, scientists have been able to deduce and map the distribution of dark matter around the MACS J0416.1-2403 galaxy cluster.\n\nWeak gravitational lensing investigates minute distortions of galaxies, using statistical analyses from vast galaxy surveys. By examining the apparent shear deformation of the adjacent background galaxies, the mean distribution of dark matter can be characterized. The mass-to-light ratios correspond to dark matter densities predicted by other large-scale structure measurements. Dark matter does not bend light itself; mass (in this case the mass of the dark matter) bends spacetime. Light follows the curvature of spacetime, resulting in the lensing effect.\n\nAlthough both dark matter and ordinary matter are matter, they do not behave in the same way. In particular, in the early universe, ordinary matter was ionized and interacted strongly with radiation via Thomson scattering. Dark matter does not interact directly with radiation, but it does affect the CMB by its gravitational potential (mainly on large scales), and by its effects on the density and velocity of ordinary matter. Ordinary and dark matter perturbations, therefore, evolve differently with time and leave different imprints on the cosmic microwave background (CMB).\n\nThe cosmic microwave background is very close to a perfect blackbody but contains very small temperature anisotropies of a few parts in 100,000. A sky map of anisotropies can be decomposed into an angular power spectrum, which is observed to contain a series of acoustic peaks at near-equal spacing but different heights. \nThe series of peaks can be predicted for any assumed set of cosmological parameters by modern computer codes such as CMBFast and CAMB, and matching theory to data, therefore, constrains cosmological parameters. The first peak mostly shows the density of baryonic matter, while the third peak relates mostly to the density of dark matter, measuring the density of matter and the density of atoms.\n\nThe CMB anisotropy was first discovered by COBE in 1992, though this had too coarse resolution to detect the acoustic peaks. \nAfter the discovery of the first acoustic peak by the balloon-borne BOOMERanG experiment in 2000, \nthe power spectrum was precisely observed by WMAP in 2003-12, and even more precisely \nby the Planck spacecraft in 2013-15. The results support the Lambda-CDM model.\n\nThe observed CMB angular power spectrum provides powerful evidence in support of dark matter, as its precise structure is well fitted by the Lambda-CDM model, but difficult to reproduce with any competing model such as modified Newtonian dynamics (MOND).\n\nStructure formation refers to the period after the Big Bang when density perturbations collapsed to form stars, galaxies, and clusters. Prior to structure formation, the Friedmann solutions to general relativity describe a homogeneous universe. Later, small anisotropies gradually grew and condensed the homogeneous universe into stars, galaxies and larger structures. Ordinary matter is affected by radiation, which is the dominant element of the universe at very early times. As a result, its density perturbations are washed out and unable to condense into structure. If there were only ordinary matter in the universe, there would not have been enough time for density perturbations to grow into the galaxies and clusters currently seen.\n\nDark matter provides a solution to this problem because it is unaffected by radiation. Therefore, its density perturbations can grow first. The resulting gravitational potential acts as an attractive potential well for ordinary matter collapsing later, speeding up the structure formation process.\n\nIf dark matter does not exist, then the next most likely explanation is that general relativity—the prevailing theory of gravity—is incorrect. The Bullet Cluster, the result of a recent collision of two galaxy clusters, provides a challenge for modified gravity theories because its apparent center of mass is far displaced from the baryonic center of mass. Standard dark matter theory can easily explain this observation, but modified gravity has a much harder time, especially since the observational evidence is model-independent.\n\nType Ia supernovae can be used as standard candles to measure extragalactic distances, which can in turn be used to measure how fast the universe has expanded in the past. The data indicates that the universe is expanding at an accelerating rate, the cause of which is usually ascribed to dark energy. Since observations indicate the universe is almost flat, it is expected that the total energy density of everything in the universe to sum to 1 (Ω ~ 1). The measured dark energy density is Ω = ~0.690; the observed ordinary (baryonic) matter energy density is Ω = ~0.0482 and the energy density of radiation is negligible. This leaves a missing Ω = ~0.258 that nonetheless behaves like matter (see technical definition section above)—dark matter.\n\nBaryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe on large scales. These are predicted to arise in the Lambda-CDM model due to acoustic oscillations in the photon-baryon fluid of the early universe, and can be observed in the cosmic microwave background angular power spectrum. BAOs set up a preferred length scale for baryons. As the dark matter and baryons clumped together after recombination, the effect is much weaker in the galaxy distribution in the nearby universe, but is detectable as a subtle (~ 1 percent) preference for pairs of galaxies to be separated by 147 Mpc, compared to those separated by 130 or 160 Mpc. This feature was predicted theoretically in the 1990s and then discovered in 2005, in two large galaxy redshift surveys, the Sloan Digital Sky Survey and the 2dF Galaxy Redshift Survey. Combining the CMB observations with BAO measurements from galaxy redshift surveys provides a precise estimate of the Hubble constant and the average matter density in the Universe. The results support the Lambda-CDM model.\n\nLarge galaxy redshift surveys may be used to make a three-dimensional map of the galaxy distribution. These maps are slightly distorted because distances are estimated from observed redshifts; the redshift contains a contribution from the galaxy's so-called peculiar velocity in addition to the dominant Hubble expansion term. On average, superclusters are expanding but more slowly than the cosmic mean due to their gravity, while voids are expanding faster than average. In a redshift map, galaxies in front of a supercluster have excess radial velocities towards it and have redshifts slightly higher than their distance would imply, while galaxies behind the supercluster have redshifts slightly low for their distance. This effect causes superclusters to appear squashed in the radial direction, and likewise voids are stretched. Their angular positions are unaffected.\nThe effect is not detectable for any one structure since the true shape is not known, but can be measured by averaging over many structures assuming Earth is not at a special location in the Universe.\n\nThe effect was predicted quantitatively by Nick Kaiser in 1987, and first decisively measured in 2001 by the 2dF Galaxy Redshift Survey. Results are in agreement with the Lambda-CDM model.\n\nIn astronomical spectroscopy, the Lyman-alpha forest is the sum of the absorption lines arising from the Lyman-alpha transition of neutral hydrogen in the spectra of distant galaxies and quasars. Lyman-alpha forest observations can also constrain cosmological models. These constraints agree with those obtained from WMAP data.\n\nDark matter can refer to any substance that interacts predominantly via gravity with visible matter (e.g., stars and planets). Hence in principle it need not be composed of a new type of fundamental particle but could, at least in part, be made up of standard baryonic matter, such as protons or neutrons. However, for the reasons outlined below, most scientists think the dark matter is dominated by a non-baryonic component, which is likely composed of a currently unknown fundamental particle (or similar exotic state).\n\nBaryons (protons and neutrons) make up ordinary stars and planets. However, baryonic matter also encompasses less common black holes, neutron stars, faint old white dwarfs and brown dwarfs, collectively known as massive compact halo objects (MACHOs), which can be hard to detect.\n\nHowever, multiple lines of evidence suggest the majority of dark matter is not made of baryons:\n\n\nCandidates for non-baryonic dark matter are hypothetical particles such as axions, sterile neutrinos, weakly interacting massive particles (WIMPs), gravitationally-interacting massive particles (GIMPs), or supersymmetric particles. The three neutrino types already observed are indeed abundant, and dark, and matter, but because their individual masses—however uncertain they may be—are almost certainly tiny, they can only supply a small fraction of dark matter, due to limits derived from large-scale structure and high-redshift galaxies.\n\nUnlike baryonic matter, nonbaryonic matter did not contribute to the formation of the elements in the early universe (Big Bang nucleosynthesis) and so its presence is revealed only via its gravitational effects, or weak lensing. In addition, if the particles of which it is composed are supersymmetric, they can undergo annihilation interactions with themselves, possibly resulting in observable by-products such as gamma rays and neutrinos (indirect detection).\n\nIf dark matter is as common as observations suggest, an obvious question is whether it can form objects equivalent to planets, stars, or black holes. The answer has historically been that it cannot, because of two factors:\n\n\nThis question has been debated heavily during recent years. In 2016–2017 the idea of dense dark matter or dark matter being black holes, including primordial black holes, made a comeback following results of gravitation wave detection. These were again ruled out in December 2017, but research and theories based on these still continue as at 2018, including approaches to dark matter cooling, and the question is by no means settled.\n\nDark matter can be divided into \"cold\", \"warm\", and \"hot\" categories. These categories refer to velocity rather than an actual temperature, indicating how far corresponding objects moved due to random motions in the early universe, before they slowed due to cosmic expansion—this is an important distance called the free streaming length (FSL). Primordial density fluctuations smaller than this length get washed out as particles spread from overdense to underdense regions, while larger fluctuations are unaffected; therefore this length sets a minimum scale for later structure formation. The categories are set with respect to the size of a protogalaxy (an object that later evolves into a dwarf galaxy): dark matter particles are classified as cold, warm, or hot according as their FSL; much smaller (cold), similar (warm), or much larger (hot) than a protogalaxy.\n\nMixtures of the above are also possible: a theory of mixed dark matter was popular in the mid-1990s, but was rejected following the discovery of dark energy.\n\nCold dark matter leads to a bottom-up formation of structure while hot dark matter would result in a top-down formation scenario; the latter is excluded by high-redshift galaxy observations.\n\nThese categories also correspond to fluctuation spectrum effects and the interval following the Big Bang at which each type became non-relativistic. Davis \"et al.\" wrote in 1985:\n\nAnother approximate dividing line is that warm dark matter became non-relativistic when the universe was approximately 1 year old and 1 millionth of its present size and in the radiation-dominated era (photons and neutrinos), with a photon temperature 2.7 million K. Standard physical cosmology gives the particle horizon size as 2\"ct\" (speed of light multiplied by time) in the radiation-dominated era, thus 2 light-years. A region of this size would expand to 2 million light-years today (absent structure formation). The actual FSL is approximately 5 times the above length, since it continues to grow slowly as particle velocities decrease inversely with the scale factor after they become non-relativistic. In this example the FSL would correspond to 10 million light-years, or 3 megaparsecs, today, around the size containing an average large galaxy.\n\nThe 2.7 million K photon temperature gives a typical photon energy of 250 electron-volts, thereby setting a typical mass scale for warm dark matter: particles much more massive than this, such as GeV–TeV mass WIMPs, would become non-relativistic much earlier than one year after the Big Bang and thus have FSLs much smaller than a protogalaxy, making them cold. Conversely, much lighter particles, such as neutrinos with masses of only a few eV, have FSLs much larger than a protogalaxy, thus qualifying them as hot.\n\nCold dark matter offers the simplest explanation for most cosmological observations. It is dark matter composed of constituents with an FSL much smaller than a protogalaxy. This is the focus for dark matter research, as hot dark matter does not seem capable of supporting galaxy or galaxy cluster formation, and most particle candidates slowed early.\n\nThe constituents of cold dark matter are unknown. Possibilities range from large objects like MACHOs (such as black holes and Preon stars) or RAMBOs (such as clusters of brown dwarfs), to new particles such as WIMPs and axions.\n\nStudies of Big Bang nucleosynthesis and gravitational lensing convinced most cosmologists that MACHOs cannot make up more than a small fraction of dark matter. According to A. Peter: \"... the only \"really plausible\" dark-matter candidates are new particles.\"\n\nThe 1997 DAMA/NaI experiment and its successor DAMA/LIBRA in 2013, claimed to directly detect dark matter particles passing through the Earth, but many researchers remain skeptical, as negative results from similar experiments seem incompatible with the DAMA results.\n\nMany supersymmetric models offer dark matter candidates in the form of the WIMPy Lightest Supersymmetric Particle (LSP). Separately, heavy sterile neutrinos exist in non-supersymmetric extensions to the standard model that explain the small neutrino mass through the seesaw mechanism.\n\nWarm dark matter comprises particles with an FSL comparable to the size of a protogalaxy. Predictions based on warm dark matter are similar to those for cold dark matter on large scales, but with less small-scale density perturbations. This reduces the predicted abundance of dwarf galaxies and may lead to lower density of dark matter in the central parts of large galaxies. Some researchers consider this a better fit to observations. A challenge for this model is the lack of particle candidates with the required mass ~ 300 eV to 3000 eV.\n\nNo known particles can be categorized as warm dark matter. A postulated candidate is the sterile neutrino: a heavier, slower form of neutrino that does not interact through the weak force, unlike other neutrinos. Some modified gravity theories, such as scalar–tensor–vector gravity, require \"warm\" dark matter to make their equations work.\n\nHot dark matter consists of particles whose FSL is much larger than the size of a protogalaxy. The neutrino qualifies as such particle. They were discovered independently, long before the hunt for dark matter: they were postulated in 1930, and detected in 1956. Neutrinos' mass is less than 10 that of an electron. Neutrinos interact with normal matter only via gravity and the weak force, making them difficult to detect (the weak force only works over a small distance, thus a neutrino triggers a weak force event only if it hits a nucleus head-on). This makes them 'weakly interacting light particles' (WILPs), as opposed to WIMPs.\n\nThe three known flavours of neutrinos are the \"electron\", \"muon\", and \"tau\". Their masses are slightly different. Neutrinos oscillate among the flavours as they move. It is hard to determine an exact upper bound on the collective average mass of the three neutrinos (or for any of the three individually). For example, if the average neutrino mass were over 50 \"eV/c\" (less than 10 of the mass of an electron), the universe would collapse. CMB data and other methods indicate that their average mass probably does not exceed 0.3 \"eV/c\". Thus, observed neutrinos cannot explain dark matter.\n\nBecause galaxy-size density fluctuations get washed out by free-streaming, hot dark matter implies that the first objects that can form are huge supercluster-size pancakes, which then fragment into galaxies. Deep-field observations show instead that galaxies formed first, followed by clusters and superclusters as galaxies clump together.\n\nIf dark matter is made up of sub-atomic particles, then millions, possibly billions, of such particles must pass through every square centimeter of the Earth each second. Many experiments aim to test this hypothesis. Although WIMPs are popular search candidates, the Axion Dark Matter Experiment (ADMX) searches for axions. Another candidate is heavy hidden sector particles that only interact with ordinary matter via gravity.\n\nThese experiments can be divided into two classes: direct detection experiments, which search for the scattering of dark matter particles off atomic nuclei within a detector; and indirect detection, which look for the products of dark matter particle annihilations or decays.\n\nDirect detection experiments aim to observe low-energy recoils (typically a few keVs) of nuclei induced by interactions with particles of dark matter, which (in theory) are passing through the Earth. After such a recoil the nucleus will emit energy as, e.g., scintillation light or phonons, which is then detected by sensitive apparatus. To do this effectively, it is crucial to maintain a low background, and so such experiments operate deep underground to reduce the interference from cosmic rays. Examples of underground laboratories with direct detection experiments include the Stawell mine, the Soudan mine, the SNOLAB underground laboratory at Sudbury, the Gran Sasso National Laboratory, the Canfranc Underground Laboratory, the Boulby Underground Laboratory, the Deep Underground Science and Engineering Laboratory and the China Jinping Underground Laboratory.\n\nThese experiments mostly use either cryogenic or noble liquid detector technologies. Cryogenic detectors operating at temperatures below 100 mK, detect the heat produced when a particle hits an atom in a crystal absorber such as germanium. Noble liquid detectors detect scintillation produced by a particle collision in liquid xenon or argon. Cryogenic detector experiments include: CDMS, CRESST, EDELWEISS, EURECA. Noble liquid experiments include ZEPLIN, XENON, DEAP, ArDM, WARP, DarkSide, PandaX, and LUX, the Large Underground Xenon experiment. Both of these techniques focus strongly on their ability to distinguish background particles (which predominantly scatter off electrons) from dark matter particles (that scatter off nuclei). Other experiments include SIMPLE and PICASSO.\n\nCurrently there has been no well-established claim of dark matter detection from a direct detection experiment, leading instead to strong upper limits on the mass and interaction cross section with nucleons of such dark matter particles. The DAMA/NaI and more recent DAMA/LIBRA experimental collaborations have detected an annual modulation in the rate of events in their detectors, which they claim is due to dark matter. This results from the expectation that as the Earth orbits the Sun, the velocity of the detector relative to the dark matter halo will vary by a small amount. This claim is so far unconfirmed and in contradiction with negative results from other experiments such as LUX and SuperCDMS.\n\nA special case of direct detection experiments covers those with directional sensitivity. This is a search strategy based on the motion of the Solar System around the Galactic Center. A low-pressure time projection chamber makes it possible to access information on recoiling tracks and constrain WIMP-nucleus kinematics. WIMPs coming from the direction in which the Sun travels (approximately towards Cygnus) may then be separated from background, which should be isotropic. Directional dark matter experiments include DMTPC, DRIFT, Newage and MIMAC.\n\nIndirect detection experiments search for the products of the self-annihilation or decay of dark matter particles in outer space. For example, in regions of high dark matter density (e.g., the centre of our galaxy) two dark matter particles could annihilate to produce gamma rays or Standard Model particle-antiparticle pairs. Alternatively if the dark matter particle is unstable, it could decay into standard model (or other) particles. These processes could be detected indirectly through an excess of gamma rays, antiprotons or positrons emanating from high density regions in our galaxy or others. A major difficulty inherent in such searches is that various astrophysical sources can mimic the signal expected from dark matter, and so multiple signals are likely required for a conclusive discovery.\n\nA few of the dark matter particles passing through the Sun or Earth may scatter off atoms and lose energy. Thus dark matter may accumulate at the center of these bodies, increasing the chance of collision/annihilation. This could produce a distinctive signal in the form of high-energy neutrinos. Such a signal would be strong indirect proof of WIMP dark matter. High-energy neutrino telescopes such as AMANDA, IceCube and ANTARES are searching for this signal.\nThe detection by LIGO in September 2015 of gravitational waves, opens the possibility of observing dark matter in a new way, particularly if it is in the form of primordial black holes.\n\nMany experimental searches have been undertaken to look for such emission from dark matter annihilation or decay, examples of which follow.\nThe Energetic Gamma Ray Experiment Telescope observed more gamma rays in 2008 than expected from the Milky Way, but scientists concluded that this was most likely due to incorrect estimation of the telescope's sensitivity.\n\nThe Fermi Gamma-ray Space Telescope is searching for similar gamma rays. In April 2012, an analysis of previously available data from its Large Area Telescope instrument produced statistical evidence of a 130 GeV signal in the gamma radiation coming from the center of the Milky Way. WIMP annihilation was seen as the most probable explanation.\n\nAt higher energies, ground-based gamma-ray telescopes have set limits on the annihilation of dark matter in dwarf spheroidal galaxies and in clusters of galaxies.\n\nThe PAMELA experiment (launched in 2006) detected excess positrons. They could be from dark matter annihilation or from pulsars. No excess antiprotons were observed.\n\nIn 2013 results from the Alpha Magnetic Spectrometer on the International Space Station indicated excess high-energy cosmic rays that could be due to dark matter annihilation.\n\nAn alternative approach to the detection of dark matter particles in nature is to produce them in a laboratory. Experiments with the Large Hadron Collider (LHC) may be able to detect dark matter particles produced in collisions of the LHC proton beams. Because a dark matter particle should have negligible interactions with normal visible matter, it may be detected indirectly as (large amounts of) missing energy and momentum that escape the detectors, provided other (non-negligible) collision products are detected.\nConstraints on dark matter also exist from the LEP experiment using a similar principle, but probing the interaction of dark matter particles with electrons rather than quarks. It is important to note that any discovery from collider searches must be corroborated by discoveries in the indirect or direct detection sectors to prove that the particle discovered is, in fact, the dark matter of our Universe.\n\nBecause dark matter remains to be conclusively identified, many other hypotheses have emerged aiming to explain the observational phenomena that dark matter was conceived to explain. The most common method is to modify general relativity. General relativity is well-tested on solar system scales, but its validity on galactic or cosmological scales has not been well proven. A suitable modification to general relativity can conceivably eliminate the need for dark matter. The best-known theories of this class are MOND and its relativistic generalization tensor-vector-scalar gravity (TeVeS), f(R) gravity and entropic gravity. Alternative theories abound.\n\nA problem with alternative hypotheses is that the observational evidence for dark matter comes from so many independent approaches (see the \"observational evidence\" section above). Explaining any individual observation is possible but explaining all of them is very difficult. Nonetheless, there have been some scattered successes for alternative hypotheses, such as a 2016 test of gravitational lensing in entropic gravity.\n\nThe prevailing opinion among most astrophysicists is that while modifications to general relativity can conceivably explain part of the observational evidence, there is probably enough data to conclude there must be some form of dark matter.\n\nIn philosophy of science, dark matter is an example of an auxiliary hypothesis, an ad hoc postulate that is added to a theory in response to observations that falsify it. It has been argued that the dark matter hypothesis is a conventionalist hypothesis, that is, a hypothesis that adds no empirical content and hence is unfalsifiable in the sense defined by Karl Popper.\n\nMention of dark matter is made in works of fiction. In such cases, it is usually attributed extraordinary physical or magical properties. Such descriptions are often inconsistent with the hypothesized properties of dark matter in physics and cosmology.\n\n\n"}
{"id": "51331", "url": "https://en.wikipedia.org/wiki?curid=51331", "title": "Dimensionless quantity", "text": "Dimensionless quantity\n\nIn dimensional analysis, a dimensionless quantity is a quantity to which no physical dimension is assigned. It is also known as a bare number or pure number or a quantity of dimension one and the corresponding unit of measurement in the SI is one (or 1) unit and it is not explicitly shown. Dimensionless quantities are widely used in many fields, such as mathematics, physics, chemistry, engineering, and economics. Examples of quantities to which dimensions are regularly assigned are length, time, and speed, which are measured in dimensional units, such as metre, second and metre per second. This is considered to aid intuitive understanding. However, especially in mathematical physics, it is often more convenient to drop the assignment of explicit dimensions and express the quantities without dimensions, e.g., addressing the speed of light simply by the dimensionless number .\n\nQuantities having dimension 1, \"dimensionless quantities\", regularly occur in sciences, and are formally treated within the field of dimensional analysis. In the nineteenth century, French mathematician Joseph Fourier and Scottish physicist James Clerk Maxwell led significant developments in the modern concepts of dimension and unit. Later work by British physicists Osborne Reynolds and Lord Rayleigh contributed to the understanding of dimensionless numbers in physics. Building on Rayleigh's method of dimensional analysis, Edgar Buckingham proved the theorem (independent of French mathematician Joseph Bertrand's previous work) to formalize the nature of these quantities. \n\nNumerous dimensionless numbers, mostly ratios, were coined in the early 1900s, particularly in the areas of fluid mechanics and heat transfer. Measuring \"ratios\" in the (derived) unit \"dB\" (decibel) finds widespread use nowadays. \n\nIn the early 2000s, the International Committee for Weights and Measures discussed naming the unit of 1 as the \"uno\", but the idea of just introducing a new SI-name for 1 was dropped.\n\nAll pure numbers are dimensionless quantities, for example 1, , , , and . Units of number such as the dozen, gross, googol, and Avogadro's number may also be considered dimensionless.\n\nDimensionless quantities are often obtained as ratios of quantities that are not dimensionless, but whose dimensions cancel out in the mathematical operation. Examples include calculating slopes or unit conversion factors. A more complex example of such a ratio is engineering strain, a measure of physical deformation defined as a change in length divided by the initial length. Since both quantities have the dimension \"length\", their ratio is dimensionless. Another set of examples is mass fractions or mole fractions often written using parts-per notation such as ppm (= 10), ppb (= 10), and ppt (= 10), or more confusingly as ratios of two identical units (kg/kg or mol/mol). For example, alcohol by volume, which characterizes the concentration of ethanol in an alcoholic beverage, could be written as mL / 100 mL.\n\nOther common proportions are percentages % (= 0.01),  ‰ (= 0.001) and angle units such as radians, degrees (°= ) and grads(= ). In statistics the coefficient of variation is the ratio of the standard deviation to the mean and is used to measure the dispersion in the data.\n\nThe Buckingham theorem indicates that validity of the laws of physics does not depend on a specific unit system. A statement of this theorem is that any physical law can be expressed as an identity involving only dimensionless combinations (ratios or products) of the variables linked by the law (e. g., pressure and volume are linked by Boyle's Law – they are inversely proportional). If the dimensionless combinations' values changed with the systems of units, then the equation would not be an identity, and Buckingham's theorem would not hold.\n\nAnother consequence of the theorem is that the functional dependence between a certain number (say, \"n\") of variables can be reduced by the number (say, \"k\") of independent dimensions occurring in those variables to give a set of \"p\" = \"n\" − \"k\" independent, dimensionless quantities. For the purposes of the experimenter, different systems that share the same description by dimensionless quantity are equivalent.\n\nTo demonstrate the application of the theorem, consider the power consumption of a stirrer with a given shape.\nThe power, \"P\", in dimensions [M · L/T], is a function of the density, \"ρ\" [M/L], and the viscosity of the fluid to be stirred, \"μ\" [M/(L · T)], as well as the size of the stirrer given by its diameter, \"D\" [L], and the angular speed of the stirrer, \"n\" [1/T]. Therefore, we have a total of \"n\" = 5 variables representing our example. Those \"n\" = 5 variables are built up from \"k\" = 3 fundamental dimensions, the length: L (SI units: m), time: T (s), and mass: M (kg).\n\nAccording to the -theorem, the \"n\" = 5 variables can be reduced by the \"k\" = 3 dimensions to form \"p\" = \"n\" − \"k\" = 5 − 3 = 2 independent dimensionless numbers. These quantities are formula_1, commonly named the Reynolds number which describes the fluid flow regime, and formula_2, the power number, which is the dimensionless description of the stirrer.\n\nCertain universal dimensioned physical constants, such as the speed of light in a vacuum, the universal gravitational constant, Planck's constant, Coulomb's constant, and Boltzmann's constant can be normalized to 1 if appropriate units for time, length, mass, charge, and temperature are chosen. The resulting system of units is known as the natural units, specifically regarding these five constants, Planck units. However, not all physical constants can be normalized in this fashion. For example, the values of the following constants are independent of the system of units, cannot be defined, and can only be determined experimentally:\n\nPhysics often uses dimensionless quantities to simplify the characterization of systems with multiple interacting physical phenomena. These may be found by applying the Buckingham theorem or otherwise may emerge from making partial differential equations unitless by the process of nondimensionalization. Engineering, economics, and other fields often extend these ideas in design and analysis of the relevant systems.\n\n\n\n\n\n"}
{"id": "38646249", "url": "https://en.wikipedia.org/wiki?curid=38646249", "title": "Dănuț Marcu", "text": "Dănuț Marcu\n\nDănuţ Marcu (born 11 January 1952) is a Romanian mathematician and computer scientist, who received his Ph.D. from the University of Bucharest in 1981. He claimed to have authored more than 400 scientific papers.\n\nMarcu was frequently accused of plagiarism.\nThe editors of \"Studia Universitatis Babeş-Bolyai, Informatica\" decided to ban Marcu from their journal for this reason, as did the editors of \"4OR: A Quarterly Journal of Operations Research\" and the editors of \"Geombinatorics\". The editors of \"Geometriae Dedicata\" state that they suspect Marcu of plagiarism, as he submitted a manuscript which is \"more-or-less word for word the same\" as a paper by Bernt Lindström. Jerrold W. Grossman, Sanpei Kageyama, Martin R. Pettet, and anonymous reviewers have accused Marcu of plagiarism in MathSciNet reviews. According to the managing editors of \"Menemui Matematik\", Marcu's paper in that journal is a well known result in graph theory, and the paper \"should not have been published\".\n\n"}
{"id": "3885086", "url": "https://en.wikipedia.org/wiki?curid=3885086", "title": "Endemic Bird Areas of the World: Priorities for Biodiversity Conservation", "text": "Endemic Bird Areas of the World: Priorities for Biodiversity Conservation\n\nEndemic Bird Areas of the World: Priorities for Biodiversity Conservation represents an effort to document in detail the endemic biodiversity conservation importance of the world's Endemic Bird Areas.\n\nThe authors are Alison J. Stattersfield, Michael J. Crosby, Adrian J. Long, and David C. Wege, with a foreword by Queen Noor of Jordan. \"Endemic Bird Areas of the World: Priorities for Biodiversity Conservation\" contains 846 pages, and is a 1998 publication by Birdlife International, No. 7 in their Birdlife Conservation Series.\n\nThe book has six introductory sections: \n\nThese are then followed by six Regional Introductions, in which Endemic Bird Areas are grouped into six major regions:\n\nThe bulk of the book consists of accounts of each of the 218 Endemic Bird Areas. Each account contains the following information:\n\nThe book concludes with a short section giving brief details of 138 secondary areas, again grouped into the six regions.\n\n\"Endemic Bird Areas of the World: Priorities for Biodiversity Conservation\" follows on from work presented in the 1992 publication \"\".\n\n"}
{"id": "18985481", "url": "https://en.wikipedia.org/wiki?curid=18985481", "title": "Eyring Science Center", "text": "Eyring Science Center\n\nThe Carl F. Eyring Science Center (ESC) is one of the science buildings on the Brigham Young University (BYU) campus in Provo, Utah. It was built in 1950 and named after Carl F. Eyring in 1954.\n\nThe ESC houses the departments of Physics and Astronomy, Geology, and Food Science and Nutrition. The Department of Chemistry has in the past been located at the ESC but is not currently headquartered there.\n\nIn 1968 an underground physics research lab was added to the north end of the building. Research on plasma, atomic processes, lasers, high-pressure physics, nanotechnology, acoustics, and cold fusion have been conducted here. It is the home of two modern TEMs.\n\nThe Royden G. Derrick Planetarium is also in the building. This 119-seat facility with a acoustically-treated dome was built in 2005 to replace the smaller, outdated Sarah Barrett Summerhays Planetarium. In the summer of 2006 a new dome was installed on the ESC's observatory to better allow for astronomical study on campus. The building also has several acoustics labs including two anechoic chambers and two reverberation chambers for performing acoustics research.\n\nThe 5th and 6th floors of the ESC constitute the Orson Pratt Observatory.\n\nIn the early years of the ESC, James A. Jensen's dinosaur displays were often in the lobby. However, since the building of the BYU Earth Science Museum, dinosaur displays are less common.\n\nThe main lobby of the building is noted for its Foucault pendulum. It also houses a student-run restaurant, the Pendulum Court, during the fall and winter semesters.\n\nThe ESC was the first building at BYU to have an elevator.\n\n\n"}
{"id": "36380999", "url": "https://en.wikipedia.org/wiki?curid=36380999", "title": "Glossary of string theory", "text": "Glossary of string theory\n\nThis page is a glossary of terms in string theory, including related areas such as supergravity, supersymmetry, and high energy physics.\n\n \n\n\n\n"}
{"id": "9049338", "url": "https://en.wikipedia.org/wiki?curid=9049338", "title": "Gustav Herglotz", "text": "Gustav Herglotz\n\nGustav Herglotz (2 February 1881 – 22 March 1953) was a German Bohemian mathematician. He is best known for his works on the theory of relativity and seismology.\n\nHerglotz studied Mathematics and Astronomy at the University of Vienna in 1899, and attended lectures by Ludwig Boltzmann. In this time of study, he had a friendship with his colleagues Paul Ehrenfest, Hans Hahn and Heinrich Tietze. In 1900 he went to the LMU Munich and achieved his Doctorate in 1902 under Hugo von Seeliger. Afterwards, he went to the University of Göttingen, where he habilitated under Felix Klein. In 1904 he became Privatdozent for Astronomy and Mathematics there, and in 1907 Professor extraordinarius. In 1908 he became Professor extraordinarius in Vienna, and in 1909 at the University of Leipzig. From 1925 (until becoming Emeritus in 1947) he again was in Göttingen as the successor of Carl Runge on the chair of applied mathematics. One of his students was Emil Artin.\n\nHerglotz worked in the fields of seismology, number theory, celestial mechanics, theory of electrons, special relativity, general relativity, hydrodynamics, refraction theory.\n\n\n\n\n\n\n\n\n\n"}
{"id": "53495652", "url": "https://en.wikipedia.org/wiki?curid=53495652", "title": "Henryk Dziedzicki", "text": "Henryk Dziedzicki\n\nHenryk Daniel Robert Dołęga Dziedzicki (1847, Warsaw - 1921, Warsaw) was a Polish physician and entomologist.\n\nHe studied medicine at the University of Warsaw. In the years 1872-1873, he took part in an expedition to Egypt, financed and organized by Aleksander Branicki. On 28 September 1874 he received the title of doctor of Medicine of the University of Warsaw.\n\nDziedzicki was an active member of the scientific society of Warsaw and a Member of Entomological Associations in Saint Petersburg, Berlin and Vienna. His sister married the entomologist Johann Andreas Schnabl.\n\nHe especially revised the family Mycetophilidae , where detailed differences in the construction of the genitalia were used for identification. He described about 120 new species from the family Mycetophilidae and two new genera \"Heteropygium\" and \"Allophallus\".\n\nTaxa named to honour him are \n\n\n"}
{"id": "47443102", "url": "https://en.wikipedia.org/wiki?curid=47443102", "title": "Horticulture Netting or Vegetable Support Net", "text": "Horticulture Netting or Vegetable Support Net\n\nHorticulture Netting or Vegetable Support Net\n\nStarting in the 1960s, in Europe a new rigid mesh and net product made out of polypropylene started being used. The most famous factories were located in England and Italy, given the fact that these two countries where the most advanced in machine tooling and extrusion, then with the passing of time this technology found its way into other parts of the world, and today it is very easy to find who manufactures this type of base product anywhere in the world\n\nOnce the semi-finished extruded product is ready (in this phase it is when colors and UV stabilizers are added to the PP mix), the product is passed thru a stretching section, this is where the strength and other mechanical properties are given as the polymer molecules are aligned. The net is first oriented longitudinally in hot water and then transversally in a ramose. The final result is a net with a lot of tensile strength (between 50 and 70 kg per meter) that weighs between 6 and 9 grams per square meter, and a mesh size that may reach up to 30x30 cm according to the technical capacity of the manufacturer.\n\nSmaller mesh sizes (between 10 and 18 cm) are usually used for horizontal tutoring of flowers, especially carnations, mums and snaps, because in floriculture growers prefer an opening that will support vertically the flower without letting it tilt or bend because it would lose its commercial value. Larger sized meshes are preferred for vegetable support. especially cucurbits, solanaceae and legumes. The reason horticulturalists prefer a larger mesh size (which simulates the hand weaved raffia systems) is so one can work the two furrows on both sides of the walking isle, without damaging the crop or the plant during harvesting or trimming work. \n\nFlower support netting is installed in many layers over the flower bed. At the extremes of the furrow a net fastening and support system is installed so that the mesh can be stretched tightly over the growing flowers. In between the two extremities of the flower bed, intermediate supports are necessary as to insure that mesh openings lay on top of each other's symmetrically and layer after layer so that flowers may grow straight.\n\nThe great majority of horticultural applications of plant support netting are vertical, even though there are also great horizontal uses. When it is used vertically to provide support to the vegetable trellis, the netting is fastened to a line of posts or supports (metal, bamboo or wood) distanced from 1.5 to up to 8 meters (depending to the type of crop, soil type, climate etc.) where the furrow ends have posts that are larger and stronger and preferably have a tension string or twine on top that is anchored on both sides of the furrow and tensioned in between posts.\n\nOnce the posts are installed (in many cases where the farmer decides for a bit of extra support by installing a tension string on the top of the poles) one may proceed with the installation of the trellis netting, unrolling the net from post to post and fastening it both to the post. This task takes about two workers per hectare per day. It is advisable to install the netting before transplanting plants from the nursery trays or just a few days after the seed went in the ground so that plants are not damaged during installation of the support net.\n\nDepending on the type of crop and plant to be trellised one can use netting of many different heights (these vary from 50 cm to up to more than 3 meters for use in greenhouses or shade house). When choosing the height of the vegetable trellis netting one should keep in mind that the net should be installed 30–40 cm above the soil. So a net 1.5 in height will provide a trellis or espalier between 1.80 and 1.90 m (so the post sizes should be around 2.2 and 2.5 m), and this would be the ideal support system measurement for most cucumber open field varieties. The ideal size for square mesh is approx. 25x25 cm.\n\nInside a greenhouse this netting could be attached to the existing aerial structure and left hanging down to the furrows, this type of installation can call for a net that is 3 meters high, this net size is ideal for many hybrids species.\nIt is also worth mentioning the use of a large mesh tutoring net (for example the 25x25cm size) installed horizontally over tomatoes or peppers in open field, using a support system similar to the one used in flowers. In such a system, the plant grows in between the meshes and the branches loaded with fruits will lean passively over the meshes completely eliminating the need for further labor in fastening and tying operations.\n\nClimate Change is causing many great damages in agriculture. Strong out of season rains are forcing many traditionally \"on ground\" cultivation methods like the ones used for cucumbers or melons to be now grown on trellises vertically so to improve aeration and solar exposure of the foliage thus reducing the rate of mycosis attacks caused by humidity trapped between the solid and the leaf system, further avoiding direct solid contact of the fruit since this would develop spots on the vegetables thus reducing their market value.\n\nAnother important aspect to be considered when deciding the type of plant support for vegetables is the rate of mechanical transmission of viruses (for example mosaic virus) via the hands of the workers. No matter how many precautions a grower may take to keep a high standard of sanitization in the fields (clothes change, washing hands in milk or stepping the shoes on a clorox trap) all one needs to lose control over the virus is for one worker to handle a cigarette or to touch an infected plant to begin the exponential contagion rate in the whole field as if it were any aphids or trips. For this reason, contrary to the raffia tutoring system where a lot of labor is required to tie by hand the plant to the twine during many phases of growth, the use of horticulture netting diminished a great deal for the need of a worker to walk the furrows handling the plants, thus reducing the possibility of spreading the disease.\n\nCucurbitaceae naturally seek the closest support point, and the netting offers multiple support points for the trellising needs of the plant thanks to the square mesh structure. Also in the case of solanaceae (especially tomatoes or peppers) one can achieve a great reduction of hand labor by installing the net on both sides of the plant (or as a V shape) all along the furrow, creating sort of a sandwich system that holds the plant on both sides, allowing new branches to lean on the mesh without the need to spend money of labor otherwise needed to fasten and tie the plant to the structure.\n\nEach time a plant is handled, pruned or tied to the support system it will cause a few days of stress. During this re-adjustment period, instead of feeding its fruits at full efficiency, the plant will send its nutrients to the foliage so that it can reposition itself efficiently (there are mathematical models that prove the efficiency of the natural leaves alignment to the sun). During the pruning work and manual guidance of the plant, we increase the risk of contagion, especially virus, due to the handling of the workers, whom without knowing, could be transmitting a pathogen mechanically from plant to plant like any harmful insect.\n\nCultivating the type of determined bushy plants, like eggplant, tomato, chili and peppers, many growers decided to guide/tutor their plants with two \"walls\" of netting, one on each side of the furrow, therefore creating a growing system where the branches will be supported on their own without any need for manual tutoring. With this system, the lower string of the bottom mesh nearest the soil on one side of the plant is tied gently (leaving about 4–10 cm, 2-4 inches in between the two nets) with the lowest string on the other side (this job is done when the plant is about 40 cm in height, 16 inches). As the plant keeps growing, the distance in between the two walls of net also keeps increasing (30–60 cm) so that each higher branch will be supported by the nearest mesh and be able to reach full vegetative development.\nBy allowing the plant to grow at 360 degrees, and to fully express its vegetative vigor, it will allow any fumigation to reach vegetable matter and leaves far into the plant mass so that the chemical may attack the targeted pathogens effectively and therefore reduce the recurrence of diseases and further chemical uses.\n\nIn most cases trellising net made out of stretched polypropylene is re-usable, and will last many crop cycles. The ideal technique to fully maximize the investment in the agricultural implements from the first cycle, and to amortize this cost over many crops (mulching, irrigation, posts, cables, filters, netting), is to alternate a cycle of solanaceae or cucurbitaceae with one of leguminous or some other nitrogen fixer. Giving for granted that during the previous cycle there had been a good phytosanitary management, and that there are no harmful colonies of pests to the next crop, once may proceed—after the last fruits are harvested—with cutting the old plant still on the tutor net, and while the old plant dries up one may place the seed of the next crop (or wait a few days and transplant directly what comes from the nursery once the old plant matter is removed from the trellis). In a few days the old plant can be easily removed and re-incorporated into the soil.\n\nPolypropylene loses tensile and physical properties once recycled or after a prolonged solar exposure. Virgin polypropylene and a good quality netting will be noticeable from the shines and gloss, while an extruded net containing recycled PP will be opaque and not reflect light as much. Considering crop netting must support the weight of a plant and also resist to natural elements like wind and rain, it is advisable to make sure one pay attention to the product before installing a net that might cause the whole crop to fall to the ground. One more detail to keep an eye on is the color of the netting. White is advisable because it will be visible at all times of the day and prevent accidental cuts during pruning and harvesting, the same cannot be said about black or green nets that will tend to hide among the leaves, stems and fruits.\n\nGrowing on a horticulture trellis increases plant density since each plant will find expansion surface vertically on the netting mesh. As the plant grows vertically, besides the already mentioned benefits of increased aeration and sun exposure, the plant´s flowers and fruits are protected from accidental crushing due to the worker walking along the furrows. There is also a greater rate of flower pollination since the flowers are more exposed to insects as leaves will not tend to grow over flowers completely covering them. By taking care of the plant from accidental damages one extends the life span of the plant and the increasing the number of fruits harvested during a longer period.\n\nCrop support net\nTomato netting\nCucumber net\nTrellis net\nTrellising net\nTrellis netting\nFlower support net\nTutoring net\nPlant support mesh\n\nERGONOMIA EN LAS OPERACIONES DE ENTUTORADO DE CULTIVOS EN INVERNADERO \nPoda y entutorado del tomate, Zoilo Serrano, Hojas Divulgadoras, Ministerio de Agricultura\n"}
{"id": "598555", "url": "https://en.wikipedia.org/wiki?curid=598555", "title": "Index of phonetics articles", "text": "Index of phonetics articles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6577179", "url": "https://en.wikipedia.org/wiki?curid=6577179", "title": "International Arctic Research Center", "text": "International Arctic Research Center\n\nThe International Arctic Research Center, or IARC, established in 1999, is a research institution focused on integrating and coordinating study of Climate change in the Arctic. The primary partners in IARC are Japan and the United States. Participants include organizations from Canada, China, Denmark, Germany, Japan, Norway, Russia, the United Kingdom, and the United States.\n\nThe Center is located at the University of Alaska Fairbanks, in the Syun-Ichi Akasofu Building. The Keith B. Mather Library is the science library housed in the Akasofu Building, serving IARC and the Geophysical Institute of UAF. The building also houses the UAF atmospheric sciences department, the Center for Global Change and the Fairbanks forecast office of the National Weather Service.\n\nStudy projects are focused within four major themes:\n\nIARC is devoting specific effort to answering the following three questions:\n\n"}
{"id": "55868271", "url": "https://en.wikipedia.org/wiki?curid=55868271", "title": "Jacob Breyne", "text": "Jacob Breyne\n\nJacob Breyne (14 January 1637 – 25 January 1697) was a Polish merchant, naturalist, and artist, born in Danzig (Gdańsk), Royal Prussia (a fief of the Crown of Poland). He was the father of Johann Philipp Breyne.\n\nBreyne was interested in plants from a young age, and collected specimens from around Danzig. He recorded where they were found and included ecological notes on each plant. He also collected specimens and plant illustrations from elsewhere, including the famous portfolio of paintings of Cape of Good Hope plants. These artworks were purchased in 1956, by Sir Ernest Oppenheimer. \n\nIn 1661 Breyne made his first trip of many to the Netherlands. He became acquainted with prominent members of the community there who kept gardens which included some of the most beautiful and rare plants. A large number of the plants Breyne drew came from Van Beverningk, from Oud-Teilingen Sassenheim near Leiden.\n\nBreyne died in Danzig in 1697.\n\n"}
{"id": "28851892", "url": "https://en.wikipedia.org/wiki?curid=28851892", "title": "Keyite", "text": "Keyite\n\nKeyite is a mineral with the chemical formula (As) · 2O. The name comes from Charles Locke Key (born 1935), an American mineral dealer who furnished its first specimens.\nKeyite is monoclinic-prismatic, meaning its crystal form has three unequal axes, two of which have 90° angles between them and one with an angle less than 90°.\nKeyite belongs to the biaxial optical class, meaning it has more than one axis of anisotropy (optic axis), in which light travels with zero birefringence, and three indices of refraction, nα = 1.800, nβ, and nγ = 1.870. Being a very rare cadmium copper arsenate, keyite is only found in Tsumeb, Namibia in the Tsumeb mine, a world-famous copper mine known for its abundance of rare and unusual minerals.\n"}
{"id": "11391440", "url": "https://en.wikipedia.org/wiki?curid=11391440", "title": "Kobon triangle problem", "text": "Kobon triangle problem\n\nThe Kobon triangle problem is an unsolved problem in combinatorial geometry first stated by Kobon Fujimura. The problem asks for the largest number \"N\"(\"k\") of nonoverlapping triangles whose sides lie on an arrangement of \"k\" lines. Variations of the problem consider the projective plane rather than the Euclidean plane, and require that the triangles not be crossed by any other lines of the arrangement.\n\nSaburo Tamura proved that the largest integer not exceeding \"k\"(\"k\" − 2)/3 provides an upper bound on the maximal number of nonoverlapping triangles realizable by \"k\" lines. In 2007, a tighter upper bound was found by Johannes Bader and Gilles Clément, by proving that Tamura's upper bound couldn't be reached for any \"k\" congruent to 0 or 2 (mod 6). The maximum number of triangles is therefore at most one less than Tamura's bound in these cases. Perfect solutions (Kobon triangle solutions yielding the maximum number of triangles) are known for \"k\" = 3, 4, 5, 6, 7, 8, 9, 13, 15 and 17. For \"k\" = 10, 11 and 12, the best solutions known reach a number of triangles one less than the upper bound.\n\nAs proved by G. Clément and J. Bader, the solutions for \"k\" > 2 are bounded above by \n\nGiven a perfect solution with \"k\">3 lines, other Kobon triangle solution numbers can be found for all \"k\"-values where\nby using the procedure by D. Forge and J. L. Ramirez Alfonsin. For example, the solution for \"k\" = 5 leads to the maximal number of nonoverlapping triangles for \"k\" = 5,9,17,33,65...\n\n"}
{"id": "28842347", "url": "https://en.wikipedia.org/wiki?curid=28842347", "title": "List of French scientists", "text": "List of French scientists\n\nThis is a list of notable French scientists.\n\n"}
{"id": "2679930", "url": "https://en.wikipedia.org/wiki?curid=2679930", "title": "List of compounds with carbon number 10", "text": "List of compounds with carbon number 10\n\nThis is a partial list of molecules that contain 10 carbon atoms.\n\n"}
{"id": "29171632", "url": "https://en.wikipedia.org/wiki?curid=29171632", "title": "Marine habitats", "text": "Marine habitats\n\nThe marine environment supplies many kinds of habitats that support marine life. Marine life depends in some way on the saltwater that is in the sea (the term \"marine\" comes from the Latin \"mare\", meaning sea or ocean). A habitat is an ecological or environmental area inhabited by one or more living species.\n\nMarine habitats can be divided into coastal and open ocean habitats. Coastal habitats are found in the area that extends from as far as the tide comes in on the shoreline out to the edge of the continental shelf. Most marine life is found in coastal habitats, even though the shelf area occupies only seven percent of the total ocean area. Open ocean habitats are found in the deep ocean beyond the edge of the continental shelf.\n\nAlternatively, marine habitats can be divided into pelagic and demersal zones. Pelagic habitats are found near the surface or in the open water column, away from the bottom of the ocean. Demersal habitats are near or on the bottom of the ocean. An organism living in a pelagic habitat is said to be a pelagic organism, as in pelagic fish. Similarly, an organism living in a demersal habitat is said to be a demersal organism, as in demersal fish. Pelagic habitats are intrinsically shifting and ephemeral, depending on what ocean currents are doing.\n\nMarine habitats can be modified by their inhabitants. Some marine organisms, like corals, kelp, mangroves and seagrasses, are ecosystem engineers which reshape the marine environment to the point where they create further habitat for other organisms.\n\nIn contrast to terrestrial habitats, marine habitats are shifting and ephemeral. Swimming organisms find areas by the edge of a continental shelf a good habitat, but only while upwellings bring nutrient rich water to the surface. Shellfish find habitat on sandy beaches, but storms, tides and currents mean their habitat continually reinvents itself.\n\nThe presence of seawater is common to all marine habitats. Beyond that many other things determine whether a marine area makes a good habitat and the type of habitat it makes. For example:\n\nThe ocean occupies 71 percent of the world surface, averaging nearly four kilometres in depth. There are five major oceans, of which the Pacific Ocean is nearly as large as the rest put together. Coastlines fringe the land for nearly 380,000 kilometres.\n\nMarine habitats can be broadly divided into pelagic and demersal habitats. Pelagic habitats are the habitats of the open water column, away from the bottom of the ocean. Demersal habitats are the habitats that are near or on the bottom of the ocean. An organism living in a pelagic habitat is said to be a pelagic organism, as in pelagic fish. Similarly, an organism living in a demersal habitat is said to be a demersal organism, as in demersal fish. Pelagic habitats are intrinsically ephemeral, depending on what ocean currents are doing.\n\nThe land-based ecosystem depends on topsoil and fresh water, while the marine ecosystem depends on dissolved nutrients washed down from the land.\n\nOcean deoxygenation poses a threat to marine habitats, due to the growth of low oxygen zones.\nIn marine systems, ocean currents have a key role determining which areas are effective as habitats, since ocean currents transport the basic nutrients needed to support marine life. Plankton are the life forms that inhabit the ocean that are so small (less than 2 mm) that they cannot effectively propel themselves through the water, but must drift instead with the currents. If the current carries the right nutrients, and if it also flows at a suitably shallow depth where there is plenty of sunlight, then such a current itself can become a suitable habitat for photosynthesizing tiny algae called phytoplankton. These tiny plants are the primary producers in the ocean, at the start of the food chain. In turn, as the population of drifting phytoplankton grows, the water becomes a suitable habitat for zooplankton, which feed on the phytoplankton. While phytoplankton are tiny drifting plants, zooplankton are tiny drifting animals, such as the larvae of fish and marine invertebrates. If sufficient zooplankton establish themselves, the current becomes a candidate habitat for the forage fish that feed on them. And then if sufficient forage fish move to the area, it becomes a candidate habitat for larger predatory fish and other marine animals that feed on the forage fish. In this dynamic way, the current itself can, over time, become a moving habitat for multiple types of marine life.\n\nOcean currents can be generated by differences in the density of the water. How dense water is depends on how saline or warm it is. If water contains differences in salt content or temperature, then the different densities will initiate a current. Water that is saltier or cooler will be denser, and will sink in relation to the surrounding water. Conversely, warmer and less salty water will float to the surface. Atmospheric winds and pressure differences also produces surface currents, waves and seiches. Ocean currents are also generated by the gravitational pull of the sun and moon (tides), and seismic activity (tsunami).\n\nThe rotation of the Earth affects the direction ocean currents take, and explains which way the large circular ocean gyres rotate in the image above left. Suppose a current at the equator is heading north. The Earth rotates eastward, so the water possesses that rotational momentum. But the further the water moves north, the slower the earth moves eastward. If the current could get to the North Pole, the earth wouldn't be moving eastward at all. To conserve its rotational momentum, the further the current travels north the faster it must move eastward. So the effect is that the current curves to the right. This is the Coriolis effect. It is weakest at the equator and strongest at the poles. The effect is opposite south of the equator, where currents curve left.\n\nMarine (or seabed or ocean) topography refers to the shape the land has when it interfaces with the ocean. These shapes are obvious along coastlines, but they occur also in significant ways underwater. The effectiveness of marine habitats is partially defined by these shapes, including the way they interact with and shape ocean currents, and the way sunlight diminishes when these landforms occupy increasing depths. Tidal networks depend on the balance between sedimentary processes and hydrodynamics however, anthropogenic influences can impact the natural system more than any physical driver.\n\nMarine topographies include coastal and oceanic landforms ranging from coastal estuaries and shorelines to continental shelves and coral reefs. Further out in the open ocean, they include underwater and deep sea features such as ocean rises and seamounts. The submerged surface has mountainous features, including a globe-spanning mid-ocean ridge system, as well as undersea volcanoes, oceanic trenches, submarine canyons, oceanic plateaus and abyssal plains.\n\nThe mass of the oceans is approximately 1.35 metric tons, or about 1/4400 of the total mass of the Earth. The oceans cover an area of 3.618 km with a mean depth of 3,682 m, resulting in an estimated volume of 1.332 km.\n\nOne measure of the relative importance of different marine habitats is the rate at which they produce biomass.\n\nMarine coasts are dynamic environments which constantly change, like the ocean which partially shape them. The Earth's natural processes, including weather and sea level change, result in the erosion, accretion and resculpturing of coasts as well as the flooding and creation of continental shelves and drowned river valleys.\n\nThe main agents responsible for deposition and erosion along coastlines are waves, tides and currents. The formation of coasts also depends on the nature of the rocks they are made of – the harder the rocks the less likely they are to erode, so variations in rock hardness result in coastlines with different shapes.\n\nTides often determine the range over which sediment is deposited or eroded. Areas with high tidal ranges allow waves to reach farther up the shore, and areas with lower tidal ranges produce deposition at a smaller elevation interval. The tidal range is influenced by the size and shape of the coastline. Tides do not typically cause erosion by themselves; however, tidal bores can erode as the waves surge up river estuaries from the ocean.\n\nWaves erode coastline as they break on shore releasing their energy; the larger the wave the more energy it releases and the more sediment it moves. Sediment deposited by waves comes from eroded cliff faces and is moved along the coastline by the waves. Sediment deposited by rivers is the dominant influence on the amount of sediment located on a coastline.\n\nThe sedimentologist Francis Shepard classified coasts as \"primary\" or \"secondary\".\n\n\nContinental coastlines usually have a continental shelf, a shelf of relatively shallow water, less than 200 metres deep, which extends 68 km on average beyond the coast. Worldwide, continental shelves occupy a total area of about 24 million km (9 million sq mi), 8% of the ocean's total area and nearly 5% of the world's total area. Since the continental shelf is usually less than 200 metres deep, it follows that coastal habitats are generally photic, situated in the sunlit epipelagic zone. This means the conditions for photosynthetic processes so important for primary production, are available to coastal marine habitats. Because land is nearby, there are large discharges of nutrient rich land runoff into coastal waters. Further, periodic upwellings from the deep ocean can provide cool and nutrient rich currents along the edge of the continental shelf.\n\nAs a result, coastal marine life is the most abundant in the world. It is found in tidal pools, fjords and estuaries, near sandy shores and rocky coastlines, around coral reefs and on or above the continental shelf. Coastal fish include small forage fish as well as the larger predator fish that feed on them. Forage fish thrive in inshore waters where high productivity results from upwelling and shoreline run off of nutrients. Some are partial residents that spawn in streams, estuaries and bays, but most complete their life cycle in the zone. There can also be a mutualism between species that occupy adjacent marine habitats. For example, fringing reefs just below low tide level have a mutually beneficial relationship with mangrove forests at high tide level and sea grass meadows in between: the reefs protect the mangroves and seagrass from strong currents and waves that would damage them or erode the sediments in which they are rooted, while the mangroves and seagrass protect the coral from large influxes of silt, fresh water and pollutants. This additional level of variety in the environment is beneficial to many types of coral reef animals, which for example may feed in the sea grass and use the reefs for protection or breeding.\n\nCoastal habitats are the most visible marine habitats, but they are not the only important marine habitats. Coastlines run for 380,000 kilometres, and the total volume of the ocean is 1,370 million cu km. This means that for each metre of coast, there is 3.6 cu km of ocean space available somewhere for marine habitats.\n\nIntertidal zones, those areas close to shore, are constantly being exposed and covered by the ocean's tides. A huge array of life lives within this zone.\n\nShore habitats range from the upper intertidal zones to the area where land vegetation takes prominence. It can be underwater anywhere from daily to very infrequently. Many species here are scavengers, living off of sea life that is washed up on the shore. Many land animals also make much use of the shore and intertidal habitats. A subgroup of organisms in this habitat bores and grinds exposed rock through the process of bioerosion.\n\nSandy shores, also called beaches, are coastal shorelines where sand accumulates. Waves and currents shift the sand, continually building and eroding the shoreline. Longshore currents flow parallel to the beaches, making waves break obliquely on the sand. These currents transport large amounts of sand along coasts, forming spits, barrier islands and tombolos. Longshore currents also commonly create offshore bars, which give beaches some stability by reducing erosion.\n\nSandy shores are full of life, The grains of sand host diatoms, bacteria and other microscopic creatures. Some fish and turtles return to certain beaches and spawn eggs in the sand. Birds habitat beaches, like gulls, loons, sandpipers, terns and pelicans. Aquatic mammals, such sea lions, recuperate on them. Clams, periwinkles, crabs, shrimp, starfish and sea urchins are found on most beaches.\n\nSand is a sediment made from small grains or particles with diameters between about 60 µm and 2 mm. Mud (see mudflats below) is a sediment made from particles finer than sand. This small particle size means that mud particles tend to stick together, whereas sand particles do not. Mud is not easily shifted by waves and currents, and when it dries out, cakes into a solid. By contrast, sand is easily shifted by waves and currents, and when sand dries out it can be blown in the wind, accumulating into shifting sand dunes. Beyond the high tide mark, if the beach is low-lying, the wind can form rolling hills of sand dunes. Small dunes shift and reshape under the influence of the wind while larger dunes stabilise the sand with vegetation.\n\nOcean processes grade loose sediments to particle sizes other than sand, such as gravel or cobbles. Waves breaking on a beach can leave a berm, which is a raised ridge of coarser pebbles or sand, at the high tide mark. Shingle beaches are made of particles larger than sand, such as cobbles, or small stones. These beaches make poor habitats. Little life survives because the stones are churned and pounded together by waves and currents.\n\nThe relative solidity of rocky shores seems to give them a permanence compared to the shifting nature of sandy shores. This apparent stability is not real over even quite short geological time scales, but it is real enough over the short life of an organism. In contrast to sandy shores, plants and animals can anchor themselves to the rocks.\n\nCompetition can develop for the rocky spaces. For example, barnacles can compete successfully on open intertidal rock faces to the point where the rock surface is covered with them. Barnacles resist desiccation and grip well to exposed rock faces. However, in the crevices of the same rocks, the inhabitants are different. Here mussels can be the successful species, secured to the rock with their byssal threads.\n\nRocky and sandy coasts are vulnerable because humans find them attractive and want to live near them. An increasing proportion of the humans live by the coast, putting pressure on coastal habitats.\n\nMudflats are coastal wetlands that form when mud is deposited by tides or rivers. They are found in sheltered areas such as bays, bayous, lagoons, and estuaries. Mudflats may be viewed geologically as exposed layers of bay mud, resulting from deposition of estuarine silts, clays and marine animal detritus. Most of the sediment within a mudflat is within the intertidal zone, and thus the flat is submerged and exposed approximately twice daily.\n\nMudflats are typically important regions for wildlife, supporting a large population, although levels of biodiversity are not particularly high. They are of particular importance to migratory birds. In the United Kingdom mudflats have been classified as a Biodiversity Action Plan priority habitat.\n\nMangrove swamps and salt marshes form important coastal habitats in tropical and temperate areas respectively.\n\nMangroves are species of shrubs and medium size trees that grow in saline coastal sediment habitats in the tropics and subtropics – mainly between latitudes ° N and ° S. The saline conditions tolerated by various species range from brackish water, through pure seawater (30 to 40 ppt), to water concentrated by evaporation to over twice the salinity of ocean seawater (up to 90 ppt). There are many mangrove species, not all closely related. The term \"mangrove\" is used generally to cover all of these species, and it can be used narrowly to cover just mangrove trees of the genus \"Rhizophora\".\n\nMangroves form a distinct characteristic saline woodland or shrubland habitat, called a \"mangrove swamp\" or \"mangrove forest\"'. Mangrove swamps are found in depositional coastal environments, where fine sediments (often with high organic content) collect in areas protected from high-energy wave action. Mangroves dominate three quarters of tropical coastlines.\n\nAn estuary is a partly enclosed coastal body of water with one or more rivers or streams flowing into it, and with a free connection to the open sea. Estuaries form a transition zone between river environments and ocean environments and are subject to both marine influences, such as tides, waves, and the influx of saline water; and riverine influences, such as flows of fresh water and sediment. The inflow of both seawater and freshwater provide high levels of nutrients in both the water column and sediment, making estuaries among the most productive natural habitats in the world.\n\nMost estuaries were formed by the flooding of river-eroded or glacially scoured valleys when sea level began to rise about 10,000-12,000 years ago. They are amongst the most heavily populated areas throughout the world, with about 60% of the world’s population living along estuaries and the coast. As a result, estuaries are suffering degradation by many factors, including sedimentation from soil erosion from deforestation; overgrazing and other poor farming practices; overfishing; drainage and filling of wetlands; eutrophication due to excessive nutrients from sewage and animal wastes; pollutants including heavy metals, PCBs, radionuclides and hydrocarbons from sewage inputs; and diking or damming for flood control or water diversion.\n\nEstuaries provide habitats for a large number of organisms and support very high productivity. Estuaries provide habitats for salmon and sea trout nurseries, as well as migratory bird populations. Two of the main characteristics of estuarine life are the variability in salinity and sedimentation. Many species of fish and invertebrates have various methods to control or conform to the shifts in salt concentrations and are termed osmoconformers and osmoregulators. Many animals also burrow to avoid predation and to live in the more stable sedimental environment. However, large numbers of bacteria are found within the sediment which have a very high oxygen demand. This reduces the levels of oxygen within the sediment often resulting in partially anoxic conditions, which can be further exacerbated by limited water flux. Phytoplankton are key primary producers in estuaries. They move with the water bodies and can be flushed in and out with the tides. Their productivity is largely dependent on the turbidity of the water. The main phytoplankton present are diatoms and dinoflagellates which are abundant in the sediment.\n\nKelp forests are underwater areas with a high density of kelp. They form some of the most productive and dynamic ecosystems on Earth. Smaller areas of anchored kelp are called \"kelp beds\". Kelp forests occur worldwide throughout temperate and polar coastal oceans.\n\nKelp forests provide a unique three-dimensional habitat for marine organisms and are a source for understanding many ecological processes. Over the last century, they have been the focus of extensive research, particularly in trophic ecology, and continue to provoke important ideas that are relevant beyond this unique ecosystem. For example, kelp forests can influence coastal oceanographic patterns and provide many ecosystem services.\n\nHowever, humans have contributed to kelp forest degradation. Of particular concern are the effects of overfishing nearshore ecosystems, which can release herbivores from their normal population regulation and result in the over-grazing of kelp and other algae. This can rapidly result in transitions to barren landscapes where relatively few species persist.\n\nFrequently considered an ecosystem engineer, kelp provides a physical substrate and habitat for kelp forest communities. In algae (Kingdom: Protista), the body of an individual organism is known as a thallus rather than as a plant (Kingdom: Plantae). The morphological structure of a kelp thallus is defined by three basic structural units:\nIn addition, many kelp species have pneumatocysts, or gas-filled bladders, usually located at the base of fronds near the stipe. These structures provide the necessary buoyancy for kelp to maintain an upright position in the water column.\n\nThe environmental factors necessary for kelp to survive include hard substrate (usually rock), high nutrients (e.g., nitrogen, phosphorus), and light (minimum annual irradiance dose > 50 E m). Especially productive kelp forests tend to be associated with areas of significant oceanographic upwelling, a process that delivers cool nutrient-rich water from depth to the ocean’s mixed surface layer. Water flow and turbulence facilitate nutrient assimilation across kelp fronds throughout the water column. Water clarity affects the depth to which sufficient light can be transmitted. In ideal conditions, giant kelp (\"Macrocystis spp.\") can grow as much as 30-60 centimetres vertically per day. Some species such as \"Nereocystis\" are annual while others like \"Eisenia\" are perennial, living for more than 20 years. In perennial kelp forests, maximum growth rates occur during upwelling months (typically spring and summer) and die-backs correspond to reduced nutrient availability, shorter photoperiods and increased storm frequency.\n\nSeagrasses are flowering plants from one of four plant families which grow in marine environments. They are called \"seagrasses\" because the leaves are long and narrow and are very often green, and because the plants often grow in large meadows which look like grassland. Since seagrasses photosynthesize and are submerged, they must grow submerged in the photic zone, where there is enough sunlight. For this reason, most occur in shallow and sheltered coastal waters anchored in sand or mud bottoms.\n\nSeagrasses form extensive beds or meadows, which can be either monospecific (made up of one species) or multispecific (where more than one species co-exist). Seagrass beds make highly diverse and productive ecosystems. They are home to phyla such as juvenile and adult fish, epiphytic and free-living macroalgae and microalgae, mollusks, bristle worms, and nematodes. Few species were originally considered to feed directly on seagrass leaves (partly because of their low nutritional content), but scientific reviews and improved working methods have shown that seagrass herbivory is a highly important link in the food chain, with hundreds of species feeding on seagrasses worldwide, including green turtles, dugongs, manatees, fish, geese, swans, sea urchins and crabs.\n\nSeagrasses are ecosystem engineers in the sense that they partly create their own habitat. The leaves slow down water-currents increasing sedimentation, and the seagrass roots and rhizomes stabilize the seabed. Their importance to associated species is mainly due to provision of shelter (through their three-dimensional structure in the water column), and due to their extraordinarily high rate of primary production. As a result, seagrasses provide coastal zones with ecosystem services, such as fishing grounds, wave protection, oxygen production and protection against coastal erosion. Seagrass meadows account for 15% of the ocean’s total carbon storage.\n\nReefs comprise some of the densest and most diverse habitats in the world. The best-known types of reefs are tropical coral reefs which exist in most tropical waters; however, reefs can also exist in cold water. Reefs are built up by corals and other calcium-depositing animals, usually on top of a rocky outcrop on the ocean floor. Reefs can also grow on other surfaces, which has made it possible to create artificial reefs. Coral reefs also support a huge community of life, including the corals themselves, their symbiotic zooxanthellae, tropical fish and many other organisms.\n\nMuch attention in marine biology is focused on coral reefs and the El Niño weather phenomenon. In 1998, coral reefs experienced the most severe mass bleaching events on record, when vast expanses of reefs across the world died because sea surface temperatures rose well above normal. Some reefs are recovering, but scientists say that between 50% and 70% of the world's coral reefs are now endangered and predict that global warming could exacerbate this trend.\n\nThe open ocean is relatively unproductive because of a lack of nutrients, yet because it is so vast, it has more overall primary production than any other marine habitat. Only about 10 percent of marine species live in the open ocean. But among them are the largest and fastest of all marine animals, as well as the animals that dive the deepest and migrate the longest. In the depths lurk animal that, to our eyes, appear hugely alien.\n\nThe surface waters are sunlit. The waters down to about 200 metres are said to be in the epipelagic zone. Enough sunlight enters the epipelagic zone to allow photosynthesis by phytoplankton. The epipelagic zone is usually low in nutrients. This partially because the organic debris produced in the zone, such as excrement and dead animals, sink to the depths and are lost to the upper zone. Photosynthesis can happen only if both sunlight and nutrients are present.\n\nIn some places, like at the edge of continental shelves, nutrients can upwell from the ocean depth, or land runoff can be distributed by storms and ocean currents. In these areas, given that both sunlight and nutrients are now present, phytoplankton can rapidly establish itself, multiplying so fast that the water turns green from the chlorophyll, resulting in an algal bloom. These nutrient rich surface waters are among the most biologically productive in the world, supporting billions of tonnes of biomass.\n\n\"Phytoplankton are eaten by zooplankton - small animals which, like phytoplankton, drift in the ocean currents. The most abundant zooplankton species are copepods and krill: tiny crustaceans that are the most numerous animals on Earth. Other types of zooplankton include jelly fish and the larvae of fish, marine worms, starfish, and other marine organisms\". In turn, the zooplankton are eaten by filter-feeding animals, including some seabirds, small forage fish like herrings and sardines, whale sharks, manta rays, and the largest animal in the world, the blue whale. Yet again, moving up the foodchain, the small forage fish are in turn eaten by larger predators, such as tuna, marlin, sharks, large squid, seabirds, dolphins, and toothed whales.\n\nThe deep sea starts at the aphotic zone, the point where sunlight loses most of its energy in the water. Many life forms that live at these depths have the ability to create their own light a unique evolution known as bio-luminescence.\n\nIn the deep ocean, the waters extend far below the epipelagic zone, and support very different types of pelagic life forms adapted to living in these deeper zones.\n\nMuch of the aphotic zone's energy is supplied by the open ocean in the form of detritus. In deep water, marine snow is a continuous shower of mostly organic detritus falling from the upper layers of the water column. Its origin lies in activities within the productive photic zone. Marine snow includes dead or dying plankton, protists (diatoms), fecal matter, sand, soot and other inorganic dust. The \"snowflakes\" grow over time and may reach several centimetres in diameter, travelling for weeks before reaching the ocean floor. However, most organic components of marine snow are consumed by microbes, zooplankton and other filter-feeding animals within the first 1,000 metres of their journey, that is, within the epipelagic zone. In this way marine snow may be considered the foundation of deep-sea mesopelagic and benthic ecosystems: As sunlight cannot reach them, deep-sea organisms rely heavily on marine snow as an energy source.\n\nSome deep-sea pelagic groups, such as the lanternfish, ridgehead, marine hatchetfish, and lightfish families are sometimes termed \"pseudoceanic\" because, rather than having an even distribution in open water, they occur in significantly higher abundances around structural oases, notably seamounts and over continental slopes. The phenomenon is explained by the likewise abundance of prey species which are also attracted to the structures.\n\nThe fish in the different pelagic and deep water benthic zones are physically structured, and behave in ways, that differ markedly from each other. Groups of coexisting species within each zone all seem to operate in similar ways, such as the small mesopelagic vertically migrating plankton-feeders, the bathypelagic anglerfishes, and the deep water benthic rattails. \"\n\nRay finned species, with spiny fins, are rare among deep sea fishes, which suggests that deep sea fish are ancient and so well adapted to their environment that invasions by more modern fishes have been unsuccessful. The few ray fins that do exist are mainly in the Beryciformes and Lampriformes, which are also ancient forms. Most deep sea pelagic fishes belong to their own orders, suggesting a long evolution in deep sea environments. In contrast, deep water benthic species, are in orders that include many related shallow water fishes.\n\nThe umbrella mouth gulper is a deep sea eel with an enormous loosely hinged mouth. It can open its mouth wide enough to swallow a fish much larger than itself, and then expand its stomach to accommodate its catch.\n\nHydrothermal vents along the mid-ocean ridge spreading centers act as oases, as do their opposites, cold seeps. Such places support unique marine biomes and many new marine microorganisms and other lifeforms have been discovered at these locations.\n\nThe deepest recorded oceanic trenches measure to date is the Mariana Trench, near the Philippines, in the Pacific Ocean at 10,924 m (35,838 ft). At such depths, water pressure is extreme and there is no sunlight, but some life still exists. A white flatfish, a shrimp and a jellyfish were seen by the American crew of the bathyscaphe \"Trieste\" when it dove to the bottom in 1960.\n\nMarine life also flourishes around seamounts that rise from the depths, where fish and other sea life congregate to spawn and feed.\n\n\n\n"}
{"id": "57445826", "url": "https://en.wikipedia.org/wiki?curid=57445826", "title": "Markarian 335", "text": "Markarian 335\n\nMarkarian 335 is a Seyfert galaxy containing a supermassive black hole, located 324 million light-years away in the constellation of Pegasus.\n\nThe central black hole in this active galaxy nucleus is notable for its corona's spinning rate (at about 20 percent the speed of light) and its change in brightness from 2007 to 2014. The geometry of the corona has been deduced from relativistic blurring of the reflection of the accretion disc. An x-ray flare in 2013 is interpreted as an aborted jet.\n"}
{"id": "177323", "url": "https://en.wikipedia.org/wiki?curid=177323", "title": "Mars Pathfinder", "text": "Mars Pathfinder\n\nMars Pathfinder (MESUR Pathfinder) is an American robotic spacecraft that landed a base station with a roving probe on Mars in 1997. It consisted of a lander, renamed the Carl Sagan Memorial Station, and a lightweight (10.6 kg/23 lb) wheeled robotic Mars rover named \"Sojourner\", which became the first rover to operate outside the Earth–Moon system.\n\nLaunched on December 4, 1996 by NASA aboard a Delta II booster a month after the \"Mars Global Surveyor\" was launched, it landed on July 4, 1997 on Mars's Ares Vallis, in a region called Chryse Planitia in the Oxia Palus quadrangle. The lander then opened, exposing the rover which conducted many experiments on the Martian surface.\nThe mission carried a series of scientific instruments to analyze the Martian atmosphere, climate, geology and the composition of its rocks and soil. It was the second project from NASA's Discovery Program, which promotes the use of low-cost spacecraft and frequent launches under the motto \"cheaper, faster and better\" promoted by the then administrator, Daniel Goldin. The mission was directed by the Jet Propulsion Laboratory (JPL), a division of the California Institute of Technology, responsible for NASA's Mars Exploration Program. The project manager was JPL's Tony Spear.\n\nThis mission was the first of a series of missions to Mars that included rovers, and was the first successful lander since the two \"Vikings\" landed on the red planet in 1976. Although the Soviet Union successfully sent rovers to the Moon as part of the Lunokhod program in the 1970s, its attempts to use rovers in its Mars program failed.\n\nIn addition to scientific objectives, the \"Mars Pathfinder\" mission was also a \"proof-of-concept\" for various technologies, such as airbag-mediated touchdown and automated obstacle avoidance, both later exploited by the Mars Exploration Rover mission. The \"Mars Pathfinder\" was also remarkable for its extremely low cost relative to other robotic space missions to Mars. Originally, the mission was conceived as the first of the Mars Environmental Survey (MESUR) program.\n\n\nThe \"Mars Pathfinder\" conducted different investigations on the Martian soil using three scientific instruments. The lander contained a stereoscopic camera with spatial filters on an expandable pole called Imager for Mars Pathfinder (IMP), and the Atmospheric Structure Instrument/Meteorology Package (ASI/MET) which acts as a Mars meteorological station, collecting data about pressure, temperature, and winds. The MET structure included three windsocks mounted at three heights on a pole, the topmost at about one meter (yard) and generally registered winds from the West.\n\nThe \"Sojourner\" rover had an Alpha Proton X-ray Spectrometer (APXS), which was used to analyze the components of the rocks and soil. The rover also had two black-and-white cameras and a color one. These instruments could investigate the geology of the Martian surface from just a few millimeters to many hundreds of meters, the geochemistry and evolutionary history of the rocks and surface, the magnetic and mechanical properties of the land, as well as the magnetic properties of the dust, atmosphere and the rotational and orbital dynamics of the planet.\n\nThree navigation cameras were on board the rover: Two black and white 0.3-megapixel cameras were located on the front (768 horizontal pixels × 484 vertical pixels configured in 4×4+100 pixel blocks), coupled with five laser stripe projectors, which enabled stereoscopic images to be taken along with measurements for hazard detection on the rover's path. A third camera with the same resolution but taking color images was located on the back, near the APXS, and rotated by 90°. It provided images of the APXS's target area and the rover's tracks on the ground. The pixels of this colour camera were arranged in such a way, that out of the 16 pixel of a 4×4 pixel block, 12 pixel were sensitive to green, 2 pixel to red and 2 pixel were sensitive to infrared as well as blue. As all cameras had lenses made out of zinc selenide, which blocks light below a wavelength of 500 nm, no blue light actually reached these \"blue/infrared\" pixels, which therefore recorded only infrared.\n\nAll three cameras were CCDs manufactured by Eastman Kodak Company, and were controlled by the rover's CPU. They all had auto-exposure and capabilities for handling bad pixels, and the image parameters (exposure time, compression used, etc.) were included in the transmitted images as part of the image header. The rover could compress the front cameras' images using the block truncation coding (BTC) algorithm, but it could only do the same for the back camera's images if the colour information was discarded. The cameras' optical resolution was sufficient to resolve 0.6 cm details across a 0.65 m range.\n\n\n\nThe landing site was an ancient flood plain in Mars's northern hemisphere called \"Ares Vallis\" (\"the valley of Ares\", the ancient Greek equivalent of the ancient Roman deity Mars) and is among the rockiest parts of Mars. Scientists chose it because they found it to be a relatively safe surface to land on and one that contained a wide variety of rocks deposited during a catastrophic flood. After the landing, at , succeeded, the lander received the name \"The Carl Sagan Memorial Station\" in honor of the astronomer.\n\n\"Mars Pathfinder\" entered the Martian atmosphere and landed using an innovative system involving an entry capsule, a supersonic parachute, followed by solid rockets and large airbags to cushion the impact.\n\n\"Mars Pathfinder\" directly entered Mars atmosphere in a retrograde direction from a hyperbolic trajectory at 6.1 km/s using an atmospheric entry aeroshell (capsule) that was derived from the original Viking Mars lander design. The aeroshell consisted of a back shell and a specially designed ablative heatshield to slow to 370 m/s (830 mph) where a supersonic disk-gap-band parachute was inflated to slow its descent through the thin Martian atmosphere to 68 m/s (about 160 mph). The lander's on-board computer used redundant on-board accelerometers to determine the timing of the parachute inflation. Twenty seconds later the heatshield was pyrotechnically released. Another twenty seconds later the lander was separated and lowered from the backshell on a 20 m bridle (tether). When the lander reached 1.6 km above the surface, a radar was used by the on-board computer to determine altitude and descent velocity. This information was used by the computer to determine the precise timing of the landing events that followed.\nOnce the lander was 355 m above the ground, airbags were inflated in less than a second using three catalytically cooled solid rocket motors that served as gas generators. The airbags were made of 4 inter-connected multi-layer vectran bags that surrounded the tetrahedron lander. They were designed and tested to accommodate grazing angle impacts as high as 28 m/s. However, as the airbags were designed for no more than about 15 m/s vertical impacts, three solid retrorockets were mounted above the lander in the backshell. These were fired at 98 m above the ground. The lander's on-board computer estimated the best time to fire the rockets and cut the bridle so that the lander velocity would be reduced to about 0 m/s between 15 and 25 m above the ground. After 2.3 seconds, while the rockets were still firing, the lander cut the bridle loose about 21.5 m above the ground and fell to the ground. The rockets flew up and away with the backshell and parachute (they have since been sighted by orbital images). The lander impacted at 14 m/s and limited the impact to only 18 G of deceleration. The first bounce was 15.7 m high and continued bouncing for at least 15 additional bounces (accelerometer data recording did not continue through all of the bounces).\n\nThe entire entry, descent and landing (EDL) process was completed in four minutes.\n\nOnce the lander stopped rolling, the airbags deflated and retracted toward the lander using four winches mounted on the lander \"petals\". Designed to right itself from any initial orientation, the lander happened to roll right side up onto its base petal. 74 minutes after landing, the petals were deployed with \"Sojourner\" rover and the solar panels attached on the inside.\nThe lander arrived at night at 2:56:55 Mars local solar time (16:56:55 UTC) on July 4, 1997. The lander had to wait until sunrise to send its first digital signals and images to Earth. The landing site was located at 19.30° north latitude and 33.52° west longitude in Ares Vallis, only 19 kilometres southwest of the center of the 200 km wide landing site ellipse. During Sol 1, the first Martian solar day the lander spent on the planet, the lander took pictures and made some meteorologic measurements. Once the data was received, the engineers realized that one of the airbags hadn't fully deflated and could be a problem for the forthcoming traverse of \"Sojourner\"s descent ramp. To solve the problem, they sent commands to the lander to raise one of its petals and perform additional retraction to flatten the airbag. The procedure was a success and on Sol 2, \"Sojourner\" was released, stood up and backed down one of two ramps.\n\nThe \"Mars Pathfinder\" entry descent and landing system design was used (with some modification) on the Mars Exploration Rover mission. Likewise, many design aspects of the \"Sojourner\" rover (e.g. the rocker-bogie mobility architecture and the navigation algorithms) were also successfully used on the Mars Exploration Rover mission.\n\nThe \"Sojourner\" rover exit from the lander occurred on Sol 2, after its landing on July 4, 1997. As the next sols progressed it approached some rocks, which the scientists named \"Barnacle Bill\", \"Yogi\", and \"Scooby-Doo\", after famous cartoon characters. The rover made measurements of the elements found in those rocks and in the martian soil, while the lander took pictures of the \"Sojourner\" and the surrounding terrain, in addition to making climate observations.\n\nThe \"Sojourner\" is a six-wheeled 65 cm long vehicle, 48 cm wide, 30 cm tall and weighing 10.5 kg. Its maximum speed reached one centimeter per second. \"Sojourner\" travelled approximately 100 metres in total, never more than 12 m from the \"Pathfinder\" station. During its 83 sols of operation, it sent 550 photographs to Earth and analyzed the chemical properties of 16 locations near the lander. (See also Space exploration rovers)\n\nThe first analysis on a rock started on Sol 3 with Barnacle Bill. The Alpha Particle X-ray Spectrometer (APXS) was used to determine its composition, the spectrometer taking ten hours to make a full scan of the sample. It found all the elements except hydrogen, which constitutes just 0.1 percent of the rock's or soil's mass.\n\nThe APXS works by irradiating rocks and soil samples with alpha particles (helium nuclei, which consist of two protons and two neutrons). The results indicated that \"Barnacle Bill\" is much like Earth's andesites, confirming past volcanic activity. The discovery of andesites shows that some Martian rocks have been remelted and reprocessed. On Earth, Andesite forms when magma sits in pockets of rock while some of the iron and magnesium settle out. Consequently, the final rock contains less iron and magnesiums and more silica. Volcanic rocks are usually classified by comparing the relative amount of alkalis (NaO and KO) with the amount of silica (SiO). Andesite is different from the rocks found in meteorites that have come from Mars.\n\nAnalysis of the Yogi rock again using the APXS showed that it was a basaltic rock, more primitive than Barnacle Bill. Yogi's shape and texture show that it was probably deposited there by a flood.\n\nAnother rock, named Moe, was found to have certain marks on its surface, demonstrating erosion caused by the wind. Most rocks analyzed showed a high content of silicon. In another region known as Rock Garden, \"Sojourner\" encountered crescent moon-shaped dunes, which are similar to crescentic dunes on Earth.\n\nBy the time that final results of the mission were described in a series of articles in the journal \"Science\" (December 5, 1997), it was believed that the rock Yogi contained a coating of dust, but was similar to the rock Barnacle Bill. Calculations suggest that the two rocks contain mostly the minerals orthopyroxene (magnesium-iron silicate), feldspars (aluminum silicates of potassium, sodium, and calcium), quartz (silicon dioxide), with smaller amounts of magnetite, ilmenite, iron sulfide, and calcium phosphate.\nThe embedded computer on board the \"Sojourner\" rover was based around the 2 MHz Intel 80C85 CPU with 512 KB of RAM and 176 KB of flash memory solid-state storage, running a cyclic executive.\n\nThe computer of the \"Pathfinder\" lander was a Radiation Hardened IBM Risc 6000 Single Chip (Rad6000 SC) CPU with 128 MB of RAM and 6 MB of EEPROM and its operating system was VxWorks.\n\nThe mission was jeopardised by a concurrent software bug in the lander, which had been found in preflight testing but was deemed a glitch and therefore given a low priority as it only occurred in certain unanticipated heavy-load conditions, and the focus was on verifying the entry and landing code. The problem, which was reproduced and corrected from Earth using a laboratory duplicate thanks to the logging and debugging functionality enabled in the flight software, was due to computer resets caused by priority inversion. No scientific or engineering data was lost after a computer reset, but all the following operations were interrupted until the next day. Four resets occurred (on July 5, 10, 11 and 14) during the mission, before patching the software on July 21 to enable priority inheritance.\n\nThe lander sent more than 2.3 billion bits of information including 16,500 pictures and made 8.5 million measurements of the atmospheric pressure, temperature and wind speed.\n\nBy taking multiple images of the sky at different distances from the Sun, scientists were able to determine that the size of the particles in the pink haze was about one micrometre in radius. The color of some soils was similar to that of an iron oxyhydroxide phase which would support the theory of a warmer and wetter climate in the past. \"Pathfinder\" carried a series of magnets to examine the magnetic component of the dust. Eventually, all but one of the magnets developed a coating of dust. Since the weakest magnet did not attract any soil, it was concluded that the airborne dust did not contain pure magnetite or just one type of maghemite. The dust probably was an aggregate possibly cemented with ferric oxide (FeO). Using much more sophisticated instruments, Mars \"Spirit\" rover found that magnetite could explain the magnetic nature of the dust and soil on Mars. Magnetite was found in the soil and that the most magnetic part of the soil was dark. Magnetite is very dark.\n\nUsing Doppler tracking and two-way ranging, scientists added earlier measurements from the \"Viking\" landers to determine that the non-hydrostatic component of the polar moment of inertia is due to the Tharsis bulge and that the interior is not melted. The central metallic core is between 1300 km and 2000 km in radius.\n\nAlthough the mission was planned to last from a week to a month, the rover operated successfully for almost three months. Communication failed after 7 October, with a final data transmission received from Pathfinder at 10:23 UTC on September 27, 1997. Mission managers tried to restore full communications during the following five months, but the mission was terminated on March 10, 1998. During the extended operation a high-resolution stereo panorama of the surrounding terrain was being made, and the Sojourner rover was to visit a distant ridge, but the panorama was only about one-third completed and the ridge visit had not begun when communication failed.\n\nThe on-board battery—designed to operate for one month—may have failed after repeated charging and discharging. The battery was used to heat the probe's electronics to slightly above the expected nighttime temperatures on Mars. With the failure of the battery, colder-than-normal temperatures may have caused vital parts to break, leading to loss of communications. The mission had exceeded its goals in the first month.\n\nMars Reconnaissance Orbiter spotted \"Pathfinder\" lander in January 2007 (left).\n\nThe name \"Sojourner\" was chosen for the \"Mars Pathfinder\" rover after a year-long, worldwide competition in which students up to 18 years old were invited to select a heroine and submit an essay about her historical accomplishments. The students were asked to address in their essays how a planetary rover named for their heroine would translate these accomplishments to the Martian environment.\n\nInitiated in March 1994 by The Planetary Society of Pasadena, California, in cooperation with NASA's Jet Propulsion Laboratory (JPL), the contest got under way with an announcement in the January 1995 issue of the National Science Teachers Association's magazine \"Science and Children\", circulated to 20,000 teachers and schools across the nation.\n\nThe winning essay, which suggested naming the rover for Sojourner Truth, was selected from among 3,500 essays and was submitted by 12-year-old Valerie Ambroise of Bridgeport, CT. First runner-up was Deepti Rohatgi, 18, of Rockville, MD, who suggested Marie Curie. Second runner-up was Adam Sheedy, 15, of Round Rock, TX, who submitted the name of the late astronaut Judith Resnik, who perished in the 1986 \"Challenger\" space shuttle explosion. Other popular suggestions included Sacajewea and Amelia Earhart.\n\n\n\n"}
{"id": "6328794", "url": "https://en.wikipedia.org/wiki?curid=6328794", "title": "Metrologia", "text": "Metrologia\n\nMetrologia is an international journal dealing with the scientific aspects of metrology. It has been running since 1965 and has been published by the Bureau International des Poids et Mesures (BIPM) since 1991. Since 2003 the journal has been published by IOP Publishing on behalf of the BIPM. \"Metrologia\" covers the fundamentals of measurements, in particular those dealing with the 7 base units of the International System of Units (metre, kilogram, second, ampere, kelvin, candela, mole) or proposals to replace them. \n\nThe editor is Janet Miles at the BIPM, Sèvres, France.\n\nThis journal is indexed by the following databases:\n\n\n"}
{"id": "57066852", "url": "https://en.wikipedia.org/wiki?curid=57066852", "title": "Other Minds: The Octopus, the Sea, and the Deep Origins of Consciousness", "text": "Other Minds: The Octopus, the Sea, and the Deep Origins of Consciousness\n\nOther Minds is a 2016 bestseller by Peter Godfrey-Smith on the evolution and nature of consciousness. It compares the situation in cephalopods, especially the octopus and the cuttlefish, with that in mammals and birds. Complex active bodies that enable and perhaps require a measure of intelligence have evolved three times, in arthropods, cephalopods, and vertebrates. The book reflects on the nature of cephalopod intelligence in particular, constrained by their short lifespan, and embodied in large part in their partly autonomous arms which contain more nerve cells than their brains.\n\nThe book has been admired by reviewers, the \"New York Times\" for example calling it \"never dogmatic, yet startlingly incisive\".\n\nPeter Godfrey-Smith is an Australian philosopher of science, specialising in the philosophy of mind and its relationship with the philosophy of biology. He is also an experienced diver.\n\n\"Other Minds\" was published by Farrar, Strauss, and Giroux in the USA in 2016. It was first published in the United Kingdom by William Collins in 2017. It is illustrated with 17 colour plates and monochrome photographs and diagrams in the text. All the photographs of octopuses and cuttlefish were taken underwater by Godfrey-Smith. He acknowledges the influence of Daniel Dennett's philosophy.\n\nGodfrey-Smith's premise in this book is the fact that intelligence has evolved separately in two groups of animals: in cephalopods like octopuses and cuttlefish, and in vertebrates like birds and humans. He notes that studying cephalopods is \"probably the closest we will come to meeting an intelligent alien\", but that \"the minds of cephalopods are the most other of all.\" He describes many encounters with octopuses in the book, as he dives in the shallow waters off Australia, especially in a favoured place that he names \"Octopolis\" where many of the animals gather. Octopuses are, he notes, inquisitive, observant, even friendly, but the architecture of their nervous systems is entirely different from the vertebrate plan. The octopus's intelligence is distributed throughout its body: there are almost twice as many nerve cells in their eight muscular arms as in their brain.\n\nIntelligence is, Godfrey-Smith argues, predicated upon the \"complex active bodies\". Three groups of bilaterian animals with that kind of body plan evolved in the Cambrian period, some 500 million years ago: the arthropods (such as crabs and insects), the vertebrates, and within the molluscs, the cephalopods.\n\nGodfrey-Smith disagrees with an old philosophical idea that consciousness suddenly emerged from unthinking matter; it is an active relationship with the world, built up in small steps with separate capabilities for perceiving the world, taking action with muscles, remembering the simplest of events. Such capabilities, in Godfrey-Smith's view, are present in some degree even in bacteria, which detect chemicals in their environment, and in insects such as bees, which recall the locations of food sources. As for feeling, both crabs and octopuses protect a part of their body that is injured: they evidently feel pain and are sentient to this extent. Neither language nor a worldview is needed for a measure of intelligence in these \"other minds\" that share planet Earth.\n\nCarl Safina, in \"The New York Times\", calls Godfrey-Smith \"a rare philosopher\", both knowledgeable and curious, who good-naturedly explores the world for insights, \"never dogmatic, yet startlingly incisive.\"\n\nPhilip Hoare, in \"The Guardian\", quotes Samuel Taylor Coleridge's couplet \"Yea, slimy things did crawl with legs / Upon the slimy sea.\" to evoke the \"eldritch other[ness]\" of octopus intelligence, \"with its more-than-the usual complement of limbs, bulbous eyes, seeking suckers and keratinous beaks voraciously devouring anything in its slippery path.\" In his view, the book \"entirely overturns\" such preconceptions, with what Godfrey-Smith calls \"an independent experiment in the evolution of large brains and complex behaviour\" forming a \"fascinating case study\". In Hoare's view, Godfrey-Smith's empathy with the animals comes from his personal observation, scuba diving in the Pacific Ocean near his university in Sydney. He concludes that \"perhaps these animals, so incredibly sensate, learning from each other's behaviour, shifting in shape and colour, are more social than we ever suspected.\"\n\nOlivia Judson, in \"The Atlantic\", admits to a love affair with octopuses, having read Jacques-Yves Cousteau's 1973 \"Octopus and Squid: The Soft Intelligence\". She notes that Godfrey-Smith follows the neuroscientist Stanislas Dehaene in suggesting that \"there's a particular style of processing—one that we use to deal especially with time, sequences, and novelty—that brings with it conscious awareness, while a lot of other quite complex activities do not.\" She argues that the ability of octopuses to learn new skills, of the kind that may demand consciousness, indicates the possibility of \"an awareness that in some ways resembles our own.\"\n\nThe biologist Meehan Crist, in \"The Los Angeles Times\", calls the book an \"elegantly materialist telling\", describing cephalopod intelligence as \"subjective experience ... deeply embodied in physical form.\" Since most of the animals' neurons are in their partly-autonomous arms, \"'for an octopus, its arms are partly self – they can be directed and used to manipulate things. But from the central brain's perspective they are partly non-self too, partly agents of their own.' This is as alien a mind as we could hope to encounter.\" Crist notes, too, that Godfrey-Smith reflects on the short (1–2 years) lifespan of octopuses. He wonders, in what Crist calls \"a precipitous existential abyss\", why they have such a large nervous system, so costly to build and to run, to learn about the world, when they have almost no time to use the knowledge.\n\nThe ecologist Marlene Zuk, in the \"Los Angeles Review of Books\", calls Godfrey-Smith \"something of an Oliver Sacks of cephalopods\" and his subjects \"uncannily personable without being at all human.\" She notes that he meets two obstacles to seeing cephalopods as \"rubbery versions of people\": they are barely social, interacting mainly to mate; and they have such short lifespans that experience can never become very well-developed. She remarks that sexual selection is not covered, though given how important it has been to humans (she speculates that human brains may have grown under the influence of \"selection for mate attraction\"), \"it is tempting to wonder how it has influenced the octopus.\"\n\nThe neuroscientist Stephen Rose, in the \"Times Higher Education Supplement\", calls \"Other Minds\" \"a delightfully written book and a model of engaged science writing.\" He writes that Godfrey-Smith handles both biology and philosophy \"profoundly but without ever talking down to his audience.\"\n\nDrake Baer in \"New York Magazine\" compares octopuses and philosophers: \"They are both given to exploring their worlds, they both have a reputation for peculiarity, they both handle multiple subjects with ease.\" Octopus eyes, too, look and work much like those of vertebrates; but there, Baer remarks, the similarities end. Cephalopods are \"immensely foreign\", with \"a distributed sense of self\" and a \"lived reality\" quite unlike human consciousness, a feature that, he notes, Godfrey-Smith calls \"the most difficult aspect of octopus experience to imagine\".\n\nThe filmmaker Jasper Sharp in \"Interalia Magazine\" writes that once in Seoul he ate \"san-nakji\", freshly butchered raw octopus, \"its severed tentacles still twitching.\" He notes that Godfrey-Smith's suggestion that cephalopods possess both intelligence, with a nervous system of some 500 million neurons, and perhaps consciousness \"makes the recollection all the more disturbing.\" Sharp marvels at the fact, explained by Godfrey-Smith, that despite their shimmering colour displays, cephalopods lack colour receptors in their eyes: they cannot see their own patterns, and cannot therefore reflect on their own visual communications as humans reflect on their own speech: unless, indeed, the photoreceptors in their skin enable them to do this. He concludes that Godfrey-Smith \"arrives at no fixed conclusion as to whether these strange creatures actually possess a form of consciousness, nor what this word actually means in relation to non-human species, but if the book provokes more questions than it answers, this is in no way a criticism.\"\n"}
{"id": "44891133", "url": "https://en.wikipedia.org/wiki?curid=44891133", "title": "P. M. de Respour", "text": "P. M. de Respour\n\nP. M. de Respour was a Flemish metallurgist and alchemist who is known as the first person to report that he had extracted metallic zinc from zinc oxide, in 1668.\n\n"}
{"id": "9387972", "url": "https://en.wikipedia.org/wiki?curid=9387972", "title": "PH-sensitive polymers", "text": "PH-sensitive polymers\n\npH sensitive or pH responsive polymers are materials which will respond to the changes in the pH of the surrounding medium by varying their dimensions. Materials may swell, collapse, or change depending on the pH of their environment. This behavior is exhibited due to the presence of certain functional groups in the polymer chain. pH-sensitive materials can be either acidic or basic, responding to either basic or acidic pH values. These polymers can be designed with many different architectures for different applications. Key uses of pH sensitive polymers are controlled drug delivery systems, biomimetics, micromechanical systems, separation processes, and surface functionalization.\n\npH sensitive polymers can be broken into two categories: those with acidic groups (such as -COOH and -SOH) and those with basic groups (-NH). The mechanism of response is the same for both, only the stimulus varies. The general form of the polymer is a backbone with functional \"pendant groups\" that hang off of it. When these functional groups become ionized in certain pH levels, they acquire a charge (+/-). Repulsions between like charges cause the polymers to change shape.\n\nPolyacids, also known as anionic polymers, are polymers that have acidic groups. Examples of acidic functional groups include carboxylic acids (-COOH), sulfonic acids (-SOH), phosphonic acids, and boronic acids. Polyacids accept protons at low pH values. At higher pH values, they deprotonate and become negatively charged. The negative charges create a repulsion that causes the polymer to swell. This swelling behavior is observed when the pH is greater than the pKa of the polymer.\n\nPolybases are the basic equivalent of polyacids and are also known as cationic polymers. They accept protons at low pH like polyacids do, but they then become positively charged. In contrast, at higher pH values they are neutral. Swelling behavior is seen when the pH is less than the pKa of the polymer.\n\nAlthough many sources talk about synthetic pH sensitive polymers, natural polymers can also display pH-responsive behavior. Examples include chitosan, hyaluronic acid, and dextran. Chitosan, a frequently used example, is cationic. Since DNA is negatively charged, DNA could be attached to chitosan as a way to deliver genes to cells. Natural polymers have appeal because they display good biocompatibility, which makes them useful for biomedical applications. However, a disadvantage to natural polymers is that researchers can have more control over the structure of synthetic polymers and so can design those polymers for specific applications.\n\nPolymers can be designed to respond to more than one external stimulus, such as pH and temperature. Often, these polymers are structured as a copolymer where each polymer displays one type of response.\n\npH sensitive polymers have been created with linear block copolymer, star, branched, dendrimer, brush, and comb architectures. Polymers of different architectures will self-assemble into different structures. This self-assembly can occur due to the nature of the polymer and the solvent, or due to a change in pH. pH changes can also cause the larger structure to swell or deswell. For example, block copolymers often form micelles, as will star polymers and branched polymers. However, star and branched polymers can form rod or worm-shaped micelles rather than the typical spheres. Brush polymers are usually used for modifying surfaces since their structure doesn’t allow them to form a larger structure like a micelle.\n\npH sensitive polymers can be synthesized using several common polymerization methods. Functional groups may need to be protected so that they do not react depending on the type of polymerization. The masking can be remooved after polymerization so that they regain their pH-sensitive functionality. Living polymerization is often used for making pH sensitive polymers because molecular weight distribution of the final polymers can be controlled. Examples include group transfer polymerization (GTP), atom transfer radical polymerization (ATRP), and reversible addition-fragmentation chain transfer (RAFT). Graft copolymers are a popular type to synthesize because their structure is a backbone with branches. The composition of the branches can be changed to achieve different properties. Hydrogels can be produced using emulsion polymerization.\n\nSeveral methods can be used to measure the contact angle of a water drop on the surface of a polymer. The contact angle value is used to quantify wettability or hydrophobicity of the polymer.\nEqual to (swollen weight-deswelled weight)/deswelled weight *100% and determined by massing polymers before and after swelling. This indicates how much the polymer swelled upon a change in pH.\n\nThe pH at which a significant structural change in how the molecules are arranged is observed. This structural change does not involve breaking bonds, but rather a change in conformation. For example, a swelling/deswelling transition would constitute a reversible conformational change. The value of the pH critical point can be determined by examining swelling percentage as a function of pH. Researchers aim to design molecules that transition at a pH that matters for the given application.\n\nConfocal microscopy, scanning electron microscopy, Raman spectroscopy, and atomic force microscopy are all used to determine how the surface of a polymer changes in response to pH.\n\npH sensitive polymers have been considered for use in membranes. A change in pH could change the ability of the polymer to let ions through, allowing it to act as a filter.\n\npH sensitive polymers have been used to modify the surfaces of materials. For example, they can be used to change the wettability of a surface.\n\nBiomedical Use\n\npH sensitive polymers have been used for drug delivery. For example, they can be used to release of insulin.\n"}
{"id": "24462958", "url": "https://en.wikipedia.org/wiki?curid=24462958", "title": "Risk", "text": "Risk\n\nRisk is the possibility of losing something of value. Values (such as physical health, social status, emotional well-being, or financial wealth) can be gained or lost when taking risk resulting from a given action or inaction, foreseen or unforeseen (planned or not planned). Risk can also be defined as the intentional interaction with uncertainty. Uncertainty is a potential, unpredictable, and uncontrollable outcome; risk is a consequence of action taken in spite of uncertainty.\n\nRisk perception is the subjective judgment people make about the severity and probability of a risk, and may vary person to person. Any human endeavour carries some risk, but some are much riskier than others.\n\nThe Oxford English Dictionary cites the earliest use of the word in English (in the spelling of \"risque\" from its from French original, 'risque' ) as of 1621, and the spelling as \"risk\" from 1655. It defines \"risk\" as:\n\n(Exposure to) the possibility of loss, injury, or other adverse or unwelcome circumstance; a chance or situation involving such a possibility.\n\n\nThe International Organization for Standardization publication ISO 31000 (2009) / ISO Guide 73:2002 definition of risk is the 'effect of uncertainty on objectives'. In this definition, uncertainties include events (which may or may not happen) and uncertainties caused by ambiguity or a lack of information. It also includes both negative and positive impacts on objectives. Many definitions of risk exist in common usage, however this definition was developed by an international committee representing over 30 countries and is based on the input of several thousand subject matter experts.\n\nVery different approaches to risk management are taken in different fields, e.g. \"Risk is the unwanted subset of a set of uncertain outcomes\" (Cornelius Keating).\n\nRisk is ubiquitous in all areas of life and risk management is something that we all must do, whether we are managing a major organisation or simply crossing the road. When describing risk however, it is convenient to consider that risk practitioners operate in some specific practice areas.\n\nEconomic risks can be manifested in lower incomes or higher expenditures than expected. The causes can be many, for instance, the hike in the price for raw materials, the lapsing of deadlines for construction of a new operating facility, disruptions in a production process, emergence of a serious competitor on the market, the loss of key personnel, the change of a political regime, or natural disasters.\n\nRisks in personal health may be reduced by primary prevention actions that decrease early causes of illness or by secondary prevention actions after a person has clearly measured clinical signs or symptoms recognised as risk factors. Tertiary prevention reduces the negative impact of an already established disease by restoring function and reducing disease-related complications. Ethical medical practice requires careful discussion of risk factors with individual patients to obtain informed consent for secondary and tertiary prevention efforts, whereas public health efforts in primary prevention require education of the entire population at risk. In each case, careful communication about risk factors, likely outcomes and certainty must distinguish between causal events that must be decreased and associated events that may be merely consequences rather than causes.\n\nIn epidemiology, the lifetime risk of an effect is the \"cumulative incidence\", also called \"incidence proportion\" over an entire lifetime.\n\nIn terms of occupational health & safety management, the term 'risk' may be defined as the most likely consequence of a hazard, combined with the likelihood or probability of it occurring.\n\nHealth, safety, and environment (HSE) are separate practice areas; however, they are often linked. The reason for this is typically to do with organizational management structures; however, there are strong links among these disciplines. One of the strongest links between these is that a single risk event may have impacts in all three areas, albeit over differing timescales. For example, the uncontrolled release of radiation or a toxic chemical may have immediate short-term safety consequences, more protracted health impacts, and much longer-term environmental impacts. Events such as Chernobyl, for example, caused immediate deaths, and in the longer term, deaths from cancers, and left a lasting environmental impact leading to birth defects, impacts on wildlife, etc.\n\nOver time, a form of risk analysis called environmental risk analysis has developed. Environmental risk analysis is a field of study that attempts to understand events and activities that bring risk to human health or the environment.\n\nHuman health and environmental risk is the likelihood of an adverse outcome (See adverse outcome pathway). As such, risk is a function of hazard and exposure. Hazard is the intrinsic danger or harm that is posed, e.g. the toxicity of a chemical compound. Exposure is the likely contact with that hazard. Therefore, the risk of even a very hazardous substance approaches zero as the exposure nears zero, given a person's (or other organism's) biological makeup, activities and location (See exposome). Another example of health risks are when certain behaviours, such as risky sexual behaviours, increase the likelihood of contracting HIV.\n\nInformation technology risk, or IT risk, IT-related risk, is a risk related to information technology. This relatively new term was developed as a result of an increasing awareness that information security is simply one facet of a multitude of risks that are relevant to IT and the real world processes it supports.\n\nThe increasing dependencies of modern society on information and computers networks (both in private and public sectors, including military) has led to new terms like IT risk and Cyberwarfare.\nInformation security means protecting information and information systems from unauthorised access, use, disclosure, disruption, modification, perusal, inspection, recording or destruction. Information security grew out of practices and procedures of computer security.\nInformation security has grown to information assurance (IA) i.e. is the practice of managing risks related to the use, processing, storage, and transmission of information or data and the systems and processes used for those purposes. \nWhile focused dominantly on information in digital form, the full range of IA encompasses not only digital but also analogue or physical form. \nInformation assurance is interdisciplinary and draws from multiple fields, including accounting, fraud examination, forensic science, management science, systems engineering, security engineering, and criminology, in addition to computer science.\n\nSo, \"IT risk\" is narrowly focused on computer security, while \"information security\" extends to risks related to other forms of information (paper, microfilm). \"Information assurance\" risks include the ones related to the consistency of the business information stored in IT systems and the information stored by other means and the relevant business consequences.\n\nInsurance is a risk treatment option which involves risk sharing. It can be considered as a form of contingent capital and is akin to purchasing an option in which the buyer pays a small premium to be protected from a potential large loss.\n\nInsurance risk is often taken by insurance companies, who then bear a pool of risks including market risk, credit risk, operational risk, interest rate risk, mortality risk, longevity risks, etc.\n\nMeans of assessing risk vary widely between professions. Indeed, they may define these professions; for example, a doctor manages medical risk, while a civil engineer manages risk of structural failure. A professional code of ethics is usually focused on risk assessment and mitigation (by the professional on behalf of client, public, society or life in general).\n\nIn the workplace, incidental and inherent risks exist. Incidental risks are those that occur naturally in the business but are not part of the core of the business. Inherent risks have a negative effect on the operating profit of the business.\n\nThe experience of many people who rely on human services for support is that 'risk' is often used as a reason to prevent them from gaining further independence or fully accessing the community, and that these services are often unnecessarily risk averse. \"People's autonomy used to be compromised by institution walls, now it's too often our risk management practices\", according to John O'Brien. Michael Fischer and Ewan Ferlie (2013) find that contradictions between formal risk controls and the role of subjective factors in human services (such as the role of emotions and ideology) can undermine service values, so producing tensions and even intractable and 'heated' conflict.\n\nA high reliability organisation (HRO) is an organisation that has succeeded in avoiding catastrophes in an environment where normal accidents can be expected due to risk factors and complexity. Most studies of HROs involve areas such as nuclear aircraft carriers, air traffic control, aerospace and nuclear power stations. Organizations such as these share in common the ability to consistently operate safely in complex, interconnected environments where a single failure in one component could lead to catastrophe. Essentially, they are organisations which appear to operate 'in spite' of an enormous range of risks.\n\nSome of these industries manage risk in a highly quantified and enumerated way. These include the nuclear power and aircraft industries, where the possible failure of a complex series of engineered systems could result in highly undesirable outcomes. The usual measure of risk for a class of events is then: \"R\" = probability of the event × the severity of the consequence.\n\nThe total risk is then the sum of the individual class-risks; see below.\n\nIn the nuclear industry, consequence is often measured in terms of off-site radiological release, and this is often banded into five or six-decade-wide bands.\n\nThe risks are evaluated using fault tree/event tree techniques (see safety engineering). Where these risks are low, they are normally considered to be \"broadly acceptable\". A higher level of risk (typically up to 10 to 100 times what is considered broadly acceptable) has to be justified against the costs of reducing it further and the possible benefits that make it tolerable—these risks are described as \"Tolerable if ALARP\", where ALARP stands for \"as low as reasonably practicable\". Risks beyond this level are classified as \"intolerable\".\n\nThe level of risk deemed broadly acceptable has been considered by regulatory bodies in various countries—an early attempt by UK government regulator and academic F. R. Farmer used the example of hill-walking and similar activities, which have definable risks that people appear to find acceptable. This resulted in the so-called Farmer Curve of acceptable probability of an event versus its consequence.\n\nThe technique as a whole is usually referred to as probabilistic risk assessment (PRA) (or probabilistic safety assessment, PSA). See WASH-1400 for an example of this approach.\n\nIn finance, risk is the chance that the return achieved on an investment will be different from that expected, and also takes into account the size of the difference. This includes the possibility of losing some or all of the original investment. In a view advocated by Damodaran, risk includes not only \"downside risk\" but also \"upside risk\" (returns that exceed expectations). Some regard the standard deviation of the historical returns or average returns of a specific investment as providing some historical measure of risk; see modern portfolio theory. Financial risk may be market-dependent, determined by numerous market factors, or operational, resulting from fraudulent behaviour (e.g. Bernard Madoff).\n\nA fundamental idea in finance is the relationship between risk and return (see modern portfolio theory). The greater the potential return one might seek, the greater the risk that one generally assumes. A free market reflects this principle in the pricing of an instrument: strong demand for a safer instrument drives its price higher (and its return correspondingly lower) while weak demand for a riskier instrument drives its price lower (and its potential return thereby higher). For example, a US Treasury bond is considered to be one of the safest investments. In comparison to an investment or speculative grade corporate bond, US Treasury notes and bonds yield lower rates of return. The reason for this is that a corporation is more likely to default on debt than the US government. Because the risk of investing in a corporate bond is higher, investors are offered a correspondingly higher rate of return.\n\nA popular risk measure is Value-at-Risk (VaR).\n\nThere are different types of VaR: long term VaR, marginal VaR, factor VaR and shock VaR. The latter is used in measuring risk during the extreme market stress conditions.\n\nIn finance, \"risk\" has no single definition.\n\nArtzner et al. write \"we call risk the investor's future net worth\". In Novak \"risk is a possibility of an undesirable event\".\n\nIn financial markets, one may need to measure credit risk, information timing and source risk, probability model risk, operational risk and legal risk if there are regulatory or civil actions taken as a result of \"investor's regret\".\n\nWith the advent of automation in financial markets, the concept of \"real-time risk\" has gained a lot of attention. Aldridge and Krawciw define real-time risk as the probability of instantaneous or near-instantaneous loss, and can be due to flash crashes, other market crises, malicious activity by selected market participants and other events. A well-cited example of real-time risk was a US $440 million loss incurred within 30 minutes by Knight Capital Group (KCG) on 1 August 2012; the culprit was a poorly-tested runaway algorithm deployed by the firm. Regulators have taken notice of real-time risk as well. Basel III requires real-time risk management framework for bank stability.\n\nIt is not always obvious if financial instruments are \"hedging\" (purchasing/selling a financial instrument specifically to reduce or cancel out the risk in another investment) or \"speculation\" (increasing measurable risk and exposing the investor to catastrophic loss in pursuit of very high windfalls that increase expected value).\n\nSome people may be \"risk seeking\", i.e. their utility function's second derivative is positive. Such an individual willingly pays a premium to assume risk (e.g. buys a lottery ticket).\n\nThe financial audit risk model expresses the risk of an auditor providing an inappropriate opinion (or material misstatement) of a commercial entity's financial statements. It can be analytically expressed as \nwhere AR is \"audit risk\", IR is \"inherent risk\", CR is \"control risk\" and DR is \"detection risk\".\n\nNote: As defined, audit risk does not consider the impact of an auditor misstatement and so is stated as a simple probability. The impact of misstatement must be considered when determining an acceptable audit risk.\n\nSecurity risk management involves protection of assets from harm caused by deliberate acts. A more detailed definition is: \"A security risk is any event that could result in the compromise of organizational assets i.e. the unauthorized use, loss, damage, disclosure or modification of organizational assets for the profit, personal interest or political interests of individuals, groups or other entities constitutes a compromise of the asset, and includes the risk of harm to people. Compromise of organizational assets may adversely affect the enterprise, its business units and their clients. As such, consideration of security risk is a vital component of risk management.\"\n\nOne of the growing areas of focus in risk management is the field of human factors where behavioural and organizational psychology underpin our understanding of risk based decision making. This field considers questions such as \"how do we make risk based decisions?\", \"why are we irrationally more scared of sharks and terrorists than we are of motor vehicles and medications?\"\n\nIn decision theory, regret (and anticipation of regret) can play a significant part in decision-making, distinct from risk aversion(preferring the status quo in case one becomes worse off).\n\nFraming is a fundamental problem with all forms of risk assessment. In particular, because of bounded rationality (our brains get overloaded, so we take mental shortcuts), the risk of extreme events is discounted because the probability is too low to evaluate intuitively. As an example, one of the leading causes of death is road accidents caused by drunk driving – partly because any given driver frames the problem by largely or totally ignoring the risk of a serious or fatal accident.\n\nFor instance, an extremely disturbing event (an attack by hijacking, or moral hazards) may be ignored in analysis despite the fact it has occurred and has a nonzero probability. Or, an event that everyone agrees is inevitable may be ruled out of analysis due to greed or an unwillingness to admit that it is believed to be inevitable. These human tendencies for error and wishful thinking often affect even the most rigorous applications of the scientific method and are a major concern of the philosophy of science.\n\nAll decision-making under uncertainty must consider cognitive bias, cultural bias, and notational bias: No group of people assessing risk is immune to \"groupthink\": acceptance of obviously wrong answers simply because it is socially painful to disagree, where there are conflicts of interest.\n\nFraming involves other information that affects the outcome of a risky decision. The right prefrontal cortex has been shown to take a more global perspective while greater left prefrontal activity relates to local or focal processing.\n\nFrom the Theory of Leaky Modules McElroy and Seta proposed that they could predictably alter the framing effect by the selective manipulation of regional prefrontal activity with finger tapping or monaural listening. The result was as expected. Rightward tapping or listening had the effect of narrowing attention such that the frame was ignored. This is a practical way of manipulating regional cortical activation to affect risky decisions, especially because directed tapping or listening is easily done.\n\nA growing area of research has been to examine various psychological aspects of risk taking. Researchers typically run randomised experiments with a treatment and control group to ascertain the effect of different psychological factors that may be associated with risk taking. Thus, positive and negative feedback about past risk taking can affect future risk taking. In an experiment, people who were led to believe they are very competent at decision making saw more opportunities in a risky choice and took more risks, while those led to believe they were not very competent saw more threats and took fewer risks.\n\nThe concept of risk-based maintenance is an advanced form of Reliability centred maintenance. In case of chemical industries, apart from probability of failure, consequences of failure is also very important. Therefore, the selection of maintenance policies should be based on risk, instead of reliability. Risk-based maintenance methodology acts as a tool for maintenance planning and decision making to reduce the probability of failure and its consequences. In risk-based maintenance decision making, the maintenance resources can be used optimally based on the risk class (high, medium, or low) of equipment or machines, to achieve tolerable risk criteria.\n\nClosely related to information assurance and security risk, cybersecurity is the application of system security engineering in order to address the compromise of company cyber-assets required for business or mission purposes. In order to address cyber-risk, cybersecurity applies security to the supply chain, the design and production environment for a product or service, and the product itself in order to provide efficient and appropriate security commensurate with the value of the asset to the mission or business process.\n\nSince risk assessment and management is essential in security management, both are tightly related. Security assessment methodologies like CRAMM contain risk assessment modules as an important part of the first steps of the methodology. On the other hand, risk assessment methodologies like Mehari evolved to become security assessment methodologies.\nAn ISO standard on risk management (Principles and guidelines on implementation) was published under code ISO 31000 on 13 November 2009.\n\nThere are many formal methods used to \"measure\" risk.\n\nOften the probability of a negative event is estimated by using the frequency of past similar events. Probabilities for rare failures may be difficult to estimate. This makes risk assessment difficult in hazardous industries, for example nuclear energy, where the frequency of failures is rare, while harmful consequences of failure are severe.\n\nStatistical methods may also require the use of a cost function, which in turn may require the calculation of the cost of loss of a human life. This is a difficult problem. One approach is to ask what people are willing to pay to insure against death or radiological release (e.g. GBq of radio-iodine), but as the answers depend very strongly on the circumstances it is not clear that this approach is effective.\n\nRisk is often measured as the expected value of an undesirable outcome. This combines the probabilities of various possible events and some assessment of the corresponding harm into a single value. See also Expected utility. The simplest case is a binary possibility of \"Accident\" or \"No accident\". The associated formula for calculating risk is then:\n\nFor example, if performing activity \"X\" has a probability of 0.01 of suffering an accident of \"A\", with a loss of 1000, then total risk is a loss of 10, the product of 0.01 and 1000.\n\nSituations are sometimes more complex than the simple binary possibility case. In a situation with several possible accidents, total risk is the sum of the risks for each different accident, provided that the outcomes are comparable:\n\nFor example, if performing activity \"X\" has a probability of 0.01 of suffering an accident of \"A\", with a loss of 1000, and a probability of 0.000001 of suffering an accident of type \"B\", with a loss of 2,000,000, then total loss expectancy is 12, which is equal to a loss of 10 from an accident of type \"A\" and 2 from an accident of type \"B\".\n\nOne of the first major uses of this concept was for the planning of the Delta Works in 1953, a flood protection program in the Netherlands, with the aid of the mathematician David van Dantzig. The kind of risk analysis pioneered there has become common today in fields like nuclear power, aerospace and the chemical industry.\n\nIn statistical decision theory, the risk function is defined as the expected value of a given loss function as a function of the decision rule used to make decisions in the face of uncertainty.\n\nPeople may rely on their fear and hesitation to keep them out of the most profoundly unknown circumstances. Fear is a response to perceived danger. Risk could be said to be the way we collectively measure and share this \"true fear\"—a fusion of rational doubt, irrational fear, and a set of unquantified biases from our own experience.\n\nThe field of behavioural finance focuses on human risk-aversion, asymmetric regret, and other ways that human financial behaviour varies from what analysts call \"rational\". Risk in that case is the degree of uncertainty associated with a return on an asset. Recognizing and respecting the irrational influences on human decision making may do much to reduce disasters caused by naive risk assessments that presume rationality but in fact merely fuse many shared biases.\n\nAccording to one set of definitions, fear is a fleeting emotion ascribed to a particular object, while anxiety is a trait of fear (this is referring to \"trait anxiety\", as distinct from how the term \"anxiety\" is generally used) that lasts longer and is not attributed to a specific stimulus (these particular definitions are not used by all authors cited on this page). Some studies show a link between anxious behaviour and risk (the chance that an outcome will have an unfavorable result). Joseph Forgas introduced valence based research where emotions are grouped as either positive or negative (Lerner and Keltner, 2000). Positive emotions, such as happiness, are believed to have more optimistic risk assessments and negative emotions, such as anger, have pessimistic risk assessments. As an emotion with a negative valence, fear, and therefore anxiety, has long been associated with negative risk perceptions. Under the more recent appraisal tendency framework of Jennifer Lerner et al., which refutes Forgas' notion of valence and promotes the idea that specific emotions have distinctive influences on judgments, fear is still related to pessimistic expectations.\n\nPsychologists have demonstrated that increases in anxiety and increases in risk perception are related and people who are habituated to anxiety experience this awareness of risk more intensely than normal individuals. In decision-making, anxiety promotes the use of biases and quick thinking to evaluate risk. This is referred to as affect-as-information according to Clore, 1983. However, the accuracy of these risk perceptions when making choices is not known.\n\nExperimental studies show that brief surges in anxiety are correlated with surges in general risk perception. Anxiety exists when the presence of threat is perceived (Maner and Schmidt, 2006). As risk perception increases, it stays related to the particular source impacting the mood change as opposed to spreading to unrelated risk factors. This increased awareness of a threat is significantly more emphasised in people who are conditioned to anxiety. For example, anxious individuals who are predisposed to generating reasons for negative results tend to exhibit pessimism. Also, findings suggest that the perception of a lack of control and a lower inclination to participate in risky decision-making (across various behavioural circumstances) is associated with individuals experiencing relatively high levels of trait anxiety. In the previous instance, there is supporting clinical research that links emotional evaluation (of control), the anxiety that is felt and the option of risk avoidance.\n\nThere are various views presented that anxious/fearful emotions cause people to access involuntary responses and judgments when making decisions that involve risk. Joshua A. Hemmerich et al. probes deeper into anxiety and its impact on choices by exploring \"risk-as-feelings\" which are quick, automatic, and natural reactions to danger that are based on emotions. This notion is supported by an experiment that engages physicians in a simulated perilous surgical procedure. It was demonstrated that a measurable amount of the participants' anxiety about patient outcomes was related to previous (experimentally created) regret and worry and ultimately caused the physicians to be led by their feelings over any information or guidelines provided during the mock surgery. Additionally, their emotional levels, adjusted along with the simulated patient status, suggest that anxiety level and the respective decision made are correlated with the type of bad outcome that was experienced in the earlier part of the experiment. Similarly, another view of anxiety and decision-making is dispositional anxiety where emotional states, or moods, are cognitive and provide information about future pitfalls and rewards (Maner and Schmidt, 2006). When experiencing anxiety, individuals draw from personal judgments referred to as pessimistic outcome appraisals. These emotions promote biases for risk avoidance and promote risk tolerance in decision-making.\n\nIt is common for people to dread some risks but not others: They tend to be very afraid of epidemic diseases, nuclear power plant failures, and plane accidents but are relatively unconcerned about some highly frequent and deadly events, such as traffic crashes, household accidents, and medical errors. One key distinction of dreadful risks seems to be their potential for catastrophic consequences, threatening to kill a large number of people within a short period of time. For example, immediately after the 11 September attacks, many Americans were afraid to fly and took their car instead, a decision that led to a significant increase in the number of fatal crashes in the time period following the 9/11 event compared with the same time period before the attacks.\n\nDifferent hypotheses have been proposed to explain why people fear dread risks. First, the psychometric paradigm suggests that high lack of control, high catastrophic potential, and severe consequences account for the increased risk perception and anxiety associated with dread risks. Second, because people estimate the frequency of a risk by recalling instances of its occurrence from their social circle or the media, they may overvalue relatively rare but dramatic risks because of their overpresence and undervalue frequent, less dramatic risks. Third, according to the preparedness hypothesis, people are prone to fear events that have been particularly threatening to survival in human evolutionary history. Given that in most of human evolutionary history people lived in relatively small groups, rarely exceeding 100 people, a dread risk, which kills many people at once, could potentially wipe out one's whole group. Indeed, research found that people's fear peaks for risks killing around 100 people but does not increase if larger groups are killed. Fourth, fearing dread risks can be an ecologically rational strategy. Besides killing a large number of people at a single point in time, dread risks reduce the number of children and young adults who would have potentially produced offspring. Accordingly, people are more concerned about risks killing younger, and hence more fertile, groups.\n\nThe relationship between higher levels of risk perception and \"judgmental accuracy\" in anxious individuals remains unclear (Joseph I. Constans, 2001). There is a chance that \"judgmental accuracy\" is correlated with heightened anxiety. Constans conducted a study to examine how worry propensity (and current mood and trait anxiety) might influence college student's estimation of their performance on an upcoming exam, and the study found that worry propensity predicted subjective risk bias (errors in their risk assessments), even after variance attributable to current mood and trait anxiety had been removed. Another experiment suggests that trait anxiety is associated with pessimistic risk appraisals (heightened perceptions of the probability and degree of suffering associated with a negative experience), while controlling for depression.\n\nIn his seminal work \"Risk, Uncertainty, and Profit\", Frank Knight (1921) established the distinction between risk and uncertainty.\nThus, Knightian uncertainty is immeasurable, not possible to calculate, while in the Knightian sense risk is measurable.\n\nAnother distinction between risk and uncertainty is proposed by Douglas Hubbard:\n\nIn this sense, one may have uncertainty without risk but not risk without uncertainty. We can be uncertain about the winner of a contest, but unless we have some personal stake in it, we have no risk. If we bet money on the outcome of the contest, then we have a risk. In both cases there are more than one outcome. The measure of uncertainty refers only to the probabilities assigned to outcomes, while the measure of risk requires both probabilities for outcomes and losses quantified for outcomes.\n\nThe terms \"risk attitude\", \"appetite\", and \"tolerance\" are often used similarly to describe an organisation's or individual's attitude towards risk-taking. One's attitude may be described as \"risk-averse\", \"risk-neutral\", or \"risk-seeking\". Risk tolerance looks at acceptable/unacceptable deviations from what is expected. Risk appetite looks at how much risk one is willing to accept. There can still be deviations that are within a risk appetite. For example, recent research finds that insured individuals are significantly likely to divest from risky asset holdings in response to a decline in health, controlling for variables such as income, age, and out-of-pocket medical expenses.\n\nGambling is a risk-increasing investment, wherein money on hand is risked for a possible large return, but with the possibility of losing it all. Purchasing a lottery ticket is a very risky investment with a high chance of no return and a small chance of a very high return. In contrast, putting money in a bank at a defined rate of interest is a risk-averse action that gives a guaranteed return of a small gain and precludes other investments with possibly higher gain. The possibility of getting no return on an investment is also known as the rate of ruin.\n\nHubbard also argues that defining risk as the product of impact and probability presumes, unrealistically, that decision-makers are risk-neutral. A risk-neutral person's utility is proportional to the expected value of the payoff. For example, a risk-neutral person would consider 20% chance of winning $1 million exactly as desirable as getting a certain $200,000. However, most decision-makers are not actually risk-neutral and would not consider these equivalent choices. This gave rise to prospect theory and cumulative prospect theory. Hubbard proposes to instead describe risk as a vector quantity that distinguishes the probability and magnitude of a risk. Risks are simply described as a set or function of possible payoffs (gains or losses) with their associated probabilities. This array is collapsed into a scalar value according to a decision-maker's risk tolerance.\n\nThis is a list of books about risk issues.\n\n\n\n\n"}
{"id": "869878", "url": "https://en.wikipedia.org/wiki?curid=869878", "title": "Shenzhou 4", "text": "Shenzhou 4\n\nShenzhou 4 () – launched on December 29, 2002 – was the fourth unmanned launch of the Chinese Shenzhou spacecraft. Two dummy astronauts were used to test the life support systems. (A live astronaut was not used until \"Shenzhou 5\" on October 15, 2003.)\n\nThe spacecraft was equipped for a manned flight, even featuring a sleeping bag, food, and medication. The windows were constructed of a new material that was designed to stay clear even after reentry to allow an astronaut to confirm that the parachutes have deployed properly. It was said that the spacecraft flown on \"Shenzhou 4\" had no major differences to that used on \"Shenzhou 5\". It flew with the ability for manual control and emergency landing, systems needed for a manned flight. A week before the launch, astronauts trained in the spacecraft to familiarise themselves with its systems.\n\nInitially the spacecraft was in a by orbit inclined at 42.4°. This was raised to by at 23:35 UTC on December 29, 2002. On January 4 and January 5, 2003 several smaller manoeuvres were thought to have taken place. The rate of orbital decay seemed higher after January 1, suggesting that the orbital module's solar panels may have been deployed for the first time. Compared to \"Shenzhou 3\" the orbital period of \"Shenzhou 4\" was much more tightly bounded with smaller manoeuvres.\n\nThe launch of Shenzhou 4 was watched by officials including Chairman of the National People's Congress Li Peng; Vice Premier and member of the Politburo Standing Committee Wu Bangguo; Jia Qinglin, also a member of the Standing Committee; Cao Gangchuan, vice-chairman of the Central Military Commission; Song Jian, vice-chairman of the Chinese People's Political Consultative Conference; and Li Jinai, head of the General Armament Department of the People's Liberation Army.\n\nThe spacecraft carried 100 peony seeds to investigate the effect of weightlessness on plants grown from them. The 52 experiments onboard investigated areas in physics, biology, medicine, earth observation, material science, and astronomy.\n\nFour tracking ships were used for the mission — one off the coast of South Africa in the South Atlantic Ocean, one in the Indian Ocean near Western Australia, one in the North Pacific Ocean south of Japan, and one in the South Pacific Ocean west of New Zealand.\n\nThe reentry module landed safely about from Hohhot, Inner Mongolia. As with previous flights, the command for reentry to begin was given by a tracking ship off the coast of South Africa. It was thought before the flight that the Chinese would attempt a water landing to test the emergency system but this did not happen. The orbital module remained in orbit until September 9, 2003.\n\n"}
{"id": "18463689", "url": "https://en.wikipedia.org/wiki?curid=18463689", "title": "Signal Corps Radio", "text": "Signal Corps Radio\n\nSignal Corps Radios were U.S. Army military communications components that comprised \"sets\". Under the Army Nomenclature System, the abbreviation SCR initially designated \"Set, Complete Radio\", but was later misinterpreted as \"Signal Corps Radio.\"\nThe term SCR was part of a nomenclature system developed for the U.S. Signal Corps, used at least as far back as World War I. Three-letter designators beginning with \"SC\" were used to denote complete systems, while one and two-letter designators (such as \"BC\", for basic component, \"FT\" for mounting, etc.) were used for components. Only a few system designators were used:\n\nThe U.S. Signal Corps used the term \"sets\" to denote specific groupings of individual components such as transmitters, receivers, power supplies, handsets, cases, and antennas. SCR radio sets ranged from the relatively small SCR-536 \"handie talkie\" to high-powered, truck-mounted mobile communications systems like the SCR-299 and large microwave radar systems such as the SCR-584 radar.\n\nThe SCS designator was applied to groups of SCR-numbered sets comprising an extensive system, such as multiple radio sets employed in a ground-based fighter direction/control center. The SCR designator could be a single transmitting or receiving set, or a full set of both transmitting and receiving equipment.\n\nAn additional designator, \"RC\" was used for subsystems or groups of accessories. The Joint Electronics Type Designation System which came into use in 1943 absorbed or superseded the SC designations.\n\nThis is only a general list, quite a few radios crossed over between branches.\nArmor\n\nArtillery\n\nInfantry\n\ngeneral use/command\n\nCavalry\n\nCoast artillery/AAA\n\nAir Liaison (ground)\nAircraft\n\n\n\n"}
{"id": "3345917", "url": "https://en.wikipedia.org/wiki?curid=3345917", "title": "Sir David's long-beaked echidna", "text": "Sir David's long-beaked echidna\n\nSir David's long-beaked echidna (\"Zaglossus attenboroughi\"), also known as Attenborough's long-beaked echidna or the Cyclops long-beaked echidna, is one of the three species from the genus \"Zaglossus\" that occurs in New Guinea. It is named in honour of Sir David Attenborough, an eminent naturalist. It lives in the Cyclops Mountains, which are near the cities of Sentani and Jayapura in the Indonesian province of Papua.\n\nIt is the smallest member of the genus, being closer in size to the short-beaked echidna. The male is larger than the female, and can be differentiated by the spurs on its hind legs.\n\nThe echidna is not a social animal, and comes together with its own kind only once a year, in July, to mate. The female will lay the eggs after about eight days, and the babies will stay in the mother's pouch for around eight weeks or until their spines develop. The creature is nocturnal, and can roll up into a spiny ball when it feels threatened, somewhat in the manner of a hedgehog. It weighs from .\nSubsequent systematic revision of Zaglossus by Flannery & Groves (1998) identified three allopatric species and several subspecies occurring across the island, and these authors erected the new species Z. attenboroughi (Attenborough’s long-beaked echidna) to describe a single echidna specimen (Plate 1) collected in 1961 at 1,600 m near the top of Mount Rara, in the Cyclops Mountains of northern Dutch New Guinea (now the Indonesian province of Papua).\n\nThe species was described from a single damaged specimen collected in the Dutch colonial era (c. 1961), and has apparently not been collected since then. Given the ongoing anthropogenic disturbance of the Cyclops Mountain forest habitat, this has raised concern that \"Z. attenboroughi\" populations may already be endangered or even locally extirpated. However, biological surveys of Papua province are notoriously incomplete; it is possible that the animal still exists there or in related mountain ranges. The echidna is endangered by hunting and habitat loss. In fact, in the 1900s, it was thought to be extinct until some of their \"nose pokes\" were found in the mountains of New Guinea. These \"nose pokes\" are very distinct and represent the echidna's feeding techniques. The diet of this hardy-built animal consists of earthworms, termites, insect larvae and ants. This animal is so high in the endangered-species list that locals are being educated on this creature and asked to stop their tradition of hunting and killing it and sharing it with rivals as a peace offering. As reported on July 15, 2007, researchers from EDGE visiting Papua's Cyclops Mountains had recently discovered burrows and tracks thought to be those of \"Zaglossus attenboroughi\". Furthermore, communication with local people revealed that the species had perhaps been seen as recently as 2005. In 2007, Sir David's long-beaked echidna was identified as one of the top-10 \"focal species\" by the Evolutionarily Distinct and Globally Endangered (EDGE) project. The echidna is among the 25 “most wanted lost” species that are the focus of Global Wildlife Conservation's “Search for Lost Species” initiative.\n\n\n"}
{"id": "54408027", "url": "https://en.wikipedia.org/wiki?curid=54408027", "title": "Society of Catholic Social Scientists", "text": "Society of Catholic Social Scientists\n\nThe Society of Catholic Social Scientists is a non-profit organization founded in 1992 by Dr. Stephen M. Krason, Esq., of the Franciscan University of Steubenville, Ohio at the Pittsburgh Hilton Hotel and recognized as a non-profit by the United States Internal Revenue Service in 1999.. The SCSS offers a M.Th. in Catholic Social Thought at Steubenville, Ohio, as well as holding an annual meeting and conference and an academic journal, \"The Catholic Social Science Review\".\n\nThe organization's mission is to \"bring rigorous, credible scholarship to political, social and economic questions\" through a collegiality of Catholic scholars, professors, researchers, practitioners, and writers who \"approach their work in both a scholarly and evangelical spirit.\" The organization publishes the Catholic Social Science Review - an interdisciplinary, peer-reviewed journal of original articles and reviews in the social sciences and the humanities.\n\nThey are expected to strictly observe the highest scholarly and professional requirements of their disciplines as they examine their data in light of Church teaching and the Natural Law. In this way, the Society seeks to obtain objective knowledge about the social order, provide solutions to vexing social problems, and further the cause of Christ.\n"}
{"id": "3935450", "url": "https://en.wikipedia.org/wiki?curid=3935450", "title": "Soyuz T-1", "text": "Soyuz T-1\n\nSoyuz T-1 (, also called Soyuz T) was a 1979-80 unmanned Soviet space flight, a test flight of a new Soyuz craft which docked with the orbiting Salyut 6 space station.\n\n\nFour months had passed since the last Salyut 6 crew (Soyuz 32) had landed, and since the same amount of time had passed between the previous space station's long-duration crews, a December 1979 launch was considered a real possibility by observers. However, though the secretive Soviets did launch a craft that month, it was not what observers expected.\n\nSoyuz T-1 was launched 16 December, and was the fourth unmanned test flight of a modified version of the Soyuz spacecraft, the first to be given a \"Soyuz\" designation. Two days later, it approached the space station, but overshot it. A second dock attempt was made 19 December, and Soyuz T-1 successfully docked at the forward port.\n\nThe Soyuz lifted the orbit of the space station on 25 December and remained docked to it for 95 days, during which time the station remained unoccupied. It undocked on 23 March 1980, performed several days of tests, then was de-orbited 25 March. The landing date was outside a normal landing window as the craft was being flight-rated over the standard two-and-a-half months and the Soviets were planning to launch Soyuz 35 during the next launch window in April.\n\nThe mission was unusual for several reasons. Unlike other previous long unmanned missions, Soyuz T-1 was not powered down while docked to the space station. And, its recovery saw a change from the norm as well. Previous Soyuz missions saw the entire spacecraft de-orbit. But with the Soyuz T craft, the orbital module was separated prior to retro-fire, to save propellant. This allowed for more maneuvers prior to de-orbit.\n\n"}
{"id": "8485476", "url": "https://en.wikipedia.org/wiki?curid=8485476", "title": "Technopolis (Belgium)", "text": "Technopolis (Belgium)\n\nTechnopolis is a Flemish technology education centre located near Mechelen.\n\nTechnopolis is an initiative which grew out of the Flanders Technology International (FTI) Foundation, which was an initiative of the Flemish government. Since 7 October 1999, the FTI offices are located within the premises of the Technopolis centre and the name of the organization was changed to Technopolis. The goal of the organization is to stimulate biotechnology and micro-electronics, and to increase the visibility of science in Flanders.\n\nThe Technopolis science museum was founded on 26 February 2000. It has a permanent interactive (hands-on) exhibition for science and technology on display.\n\n\n"}
{"id": "5228811", "url": "https://en.wikipedia.org/wiki?curid=5228811", "title": "The Probability Broach", "text": "The Probability Broach\n\nThe Probability Broach is a 1979 novel by American science fiction writer L. Neil Smith. It is set in an alternate history, the so-called Gallatin Universe, where a libertarian society has formed on the North American continent, styled the North American Confederacy (\"NAC\"). This history was created when the Declaration of Independence has the word \"unanimous\" added to the preamble, to read that governments \"derive their just power from the \"unanimous\" consent of the governed\".\n\nEdward William \"Win\" Bear is a Ute Indian who works for the Denver Police Department in a version of the United States in an alternate history of 1987 to be controlled by an anti-business, ecofascist faction complete with a new police force created in 1984 called the Federal Security Police (FSP, or \"SecPol\" as it is more commonly known) reminiscent of the Gestapo. Henry M. Jackson is president, citizens' freedoms are very limited, and many laws and regulations have been passed. Examples include hoarding precious metals, such as silver and gold, is illegal and due to strict gun control policies, only the police and citizens with federal permits are allowed to carry guns.\n\nBear is called to investigate the unusual murder of physicist Vaughn Meiss; he eventually finds himself projected into the North American Confederacy by means of the \"Probability Broach\", an inter-dimensional conduit originally developed as a means for interstellar travel in the North American Confederacy by a bottlenose dolphin physicist, named Ooloorie Eckickeck P'Wheet, and her human compatriot, Dr. Dora Jayne Thorens.\n\nWin encounters his NAC counterpart, Edward William \"Ed\" Bear, and Ed's neighbors, most notably the \"healer\" Clarissa Olson and Lucy Kropotkin, who is later revealed to be 135 years old. Lucy's life becomes the vantage point by which Win is acclimated to life in the NAC and Laporte, the NAC equivalent to Denver. Win and Ed unravel the mystery of the Meiss murder and learn that he was killed to hide an effort by SecPol to conquer the NAC with the help of Hamiltonian forces on the NAC side, led by John Jay Madison, a.k.a. the infamous Prussian expatriate and 1918 war hero Manfred von Richthofen, known here as the Red Knight of Prussia. Win, Ed, Lucy and Clarissa lead the effort to notify the nascent NAC government of the threat. En route to the meeting of the Continental Congress, Ed and Clarissa are kidnapped, leaving Win and Lucy to reveal the plot.\n\nAfter fighting (and winning) a duel with a SecPol agent, Win and Lucy rescue their friends and track Madison and the Hamiltonians to a small town outside Laporte. Win sets off an explosion that eliminates all of the Hamiltonians.\n\nWin elects to remain in the NAC and marries Clarissa. Ed marries Lucy, who at the time of the story is awaiting a delayed \"regeneration\" because of an accident involving massive radiation exposure, and they then set out for the asteroid belt to build a new life for themselves on the NAC frontier.\n\nThe Continental Congress agrees to begin a massive propaganda campaign to force Win's United States (and the rest of the globe) toward a similar Gallatinist revolution.\n\n\"The Probability Broach\" won the 1982 Prometheus Award, which L. Neil Smith himself had created, and which is awarded by the Libertarian Futurist Society.\n\n"}
{"id": "21185695", "url": "https://en.wikipedia.org/wiki?curid=21185695", "title": "The Structure and Distribution of Coral Reefs", "text": "The Structure and Distribution of Coral Reefs\n\nThe Structure and Distribution of Coral Reefs, Being the first part of the geology of the voyage of the Beagle, under the command of Capt. Fitzroy, R.N. during the years 1832 to 1836, was published in 1842 as Charles Darwin's first monograph, and set out his theory of the formation of coral reefs and atolls. He conceived of the idea during the voyage of the \"Beagle\" while still in South America, before he had seen a coral island, and wrote it out as HMS \"Beagle\" crossed the Pacific Ocean, completing his draft by November 1835. At the time there was great scientific interest in the way that coral reefs formed, and Captain Robert FitzRoy's orders from the Admiralty included the investigation of an atoll as an important scientific aim of the voyage. FitzRoy chose to survey the Keeling Islands in the Indian Ocean. The results supported Darwin's theory that the various types of coral reefs and atolls could be explained by uplift and subsidence of vast areas of the Earth's crust under the oceans.\n\nThe book was the first volume of three Darwin wrote about the geology he had investigated during the voyage, and was widely recognised as a major scientific work that presented his deductions from all the available observations on this large subject. In 1853, Darwin was awarded the Royal Society's Royal Medal for the monograph and for his work on barnacles. Darwin's theory that coral reefs formed as the islands and surrounding areas of crust subsided has been supported by modern investigations, and is no longer disputed, while the cause of the subsidence and uplift of areas of crust has continued to be a subject of discussion.\n\nWhen the \"Beagle\" set out in 1831, the formation of coral atolls was a scientific puzzle. Advance notice of her sailing, given in the \"Athenaeum\" of 24 December, described investigation of this topic as \"the most interesting part of the \"Beagle\"'s survey\" with the prospect of \"many points for investigation of a scientific nature beyond the mere occupation of the surveyor. In 1824 and 1825, French naturalists Quoy and Gaimard had observed that the coral organisms lived at relatively shallow depths, but the islands appeared in deep oceans. In books that were taken on the \"Beagle\" as references, Henry De la Beche, Frederick William Beechey and Charles Lyell had published the opinion that the coral had grown on underwater mountains or volcanoes, with atolls taking the shape of underlying volcanic craters. The Admiralty instructions for the voyage stated:\n\nAs a student at the University of Edinburgh in 1827, Darwin learnt about marine invertebrates while assisting the investigations of the anatomist Robert Edmond Grant, and during his last year at the University of Cambridge in 1831, he had studied geology under Adam Sedgwick. So when he was unexpectedly offered a place on the \"Beagle\" expedition, as a gentleman naturalist he was well suited to FitzRoy's aim of having a companion able to examine geology on land while the ship's complement carried out its hydrographic survey. FitzRoy gave Darwin the first volume of Lyell's \"Principles of Geology\" before they left. On their first stop ashore at St Jago island in January 1832, Darwin saw geological formations which he explained using Lyell's uniformitarian concept that forces still in operation made land slowly rise or fall over immense periods of time, and thought that he could write his own book on geology. Lyell's first volume included a brief outline of the idea that atolls were based on volcanic craters, and the second volume, which was sent to Darwin during the voyage, gave more detail. Darwin received it in November 1832.\n\nWhile the \"Beagle\" surveyed the coasts of South America from February 1832 to September 1835, Darwin made several trips inland and found extensive evidence that the continent was gradually rising. After witnessing an erupting volcano from the ship, he experienced the 1835 Concepción earthquake. In the following months he speculated that as the land was uplifted, large areas of the ocean bed subsided. It struck him that this could explain the formation of atolls.\n\nDarwin's theory followed from his understanding that coral polyps thrive in the clean seas of the tropics where the water is agitated, but can only live within a limited depth of water, starting just below low tide. Where the level of the underlying land stays the same, the corals grow around the coast to form what he called fringing reefs, and can eventually grow out from the shore to become a barrier reef. Where the land is rising, fringing reefs can grow around the coast, but coral raised above sea level dies and becomes white limestone. If the land subsides slowly, the fringing reefs keep pace by growing upwards on a base of dead coral, and form a barrier reef enclosing a lagoon between the reef and the land. A barrier reef can encircle an island, and once the island sinks below sea level a roughly circular atoll of growing coral continues to keep up with the sea level, forming a central lagoon. Should the land subside too quickly or sea level rise too fast, the coral dies as it is below its habitable depth.\n\nBy the time that the \"Beagle\" set out for the Galápagos Islands on 7 September 1835, Darwin had thought out the essentials of his theory of atoll formation. While he no longer favoured the concept that atolls formed on submerged volcanos, he noted some points on these islands which supported that idea: 16 volcanic craters resembled atolls in being raised slightly more on one side, and five hills appeared roughly equal in height. He then considered a topic which was compatible with either theory, the lack of coral reefs around the Galápagos Islands. One possibility was a lack of calcareous matter around the islands, but his main proposal, which FitzRoy had suggested to him, was that the seas were too cold. As they sailed on, Darwin took note of the records of sea temperature kept in the ship's \"Weather Journal\".\n\nHe had his first glimpse of coral atolls as they passed Honden Island on 9 November and sailed on through the Low or Dangerous Archipelago (Tuamotus). Arriving at Tahiti on 15 November, Darwin saw it \"encircled by a Coral reef separated from the shore by channels & basins of still water\". He climbed the hills of Tahiti, and was strongly impressed by the sight across to the island of Eimeo, where \"The mountains abruptly rise out of a glassy lake, which is separated on all sides, by a narrow defined line of breakers, from the open sea. – Remove the central group of mountains, & there remains a Lagoon Isd.\" Rather than recording his findings about the coral reefs in his notes about the island, he wrote them up as the first full draft of his theory, an essay titled \"Coral Islands\", dated 1835. They left Tahiti on 3 December, and Darwin probably wrote his essay as they sailed towards New Zealand where they arrived on 21 December. He described the polyp species building the coral on the barrier wall, flourishing in the heavy surf of breaking waves particularly on the windward side, and speculated on reasons that corals in the calm lagoon did not grow so high. He concluded with a \"remark that the general horizontal uplifting which I have proved has & is now raising upwards the greater part of S. America & as it would appear likewise of N. America, would of necessity be compensated by an equal subsidence in some other part of the world.\"\n\nFitzRoy's instructions set detailed requirements for geological survey of a circular coral atoll to investigate how coral reefs formed, particularly if they rose from the bottom of the sea or from the summits of extinct volcanoes, and to assess the effects of tides by measurement with specially constructed gauges. FitzRoy chose the Keeling Islands in the Indian Ocean, and on arrival there on 1 April 1836, the entire crew set to work, first erecting FitzRoy's new design of a tide gauge that allowed readings to be taken from the shore. Boats were sent all around the island to carry out the survey, and despite being impeded by strong winds, they took numerous soundings to establish depths around the atoll and in the lagoon. FitzRoy noted the smooth and solid rock-like outer wall of the atoll, with most life thriving where the surf was most violent. He had great difficulty in establishing the depth reached by living coral, as pieces were hard to break off and the small anchors, hooks, grappling irons, and chains they used were all snapped off by the swell as soon as they tried to pull them up. He had more success using a sounding line with a bell-shaped lead weight armed with tallow hardened with lime; this would be indented by any shape that it struck to give an exact impression of the bottom; it would also collect any fragments of coral or grains of sand.\n\nThese soundings were taken personally by FitzRoy, and the tallow from each sounding was cut off and taken on board to be examined by Darwin. The impressions taken on the steep outside slope of the reef were marked with the shapes of living corals, and otherwise were clean down to about 10 fathoms (18 m); then at increasing depths, the tallow showed fewer such impressions and collected more grains of sand until it was evident that there were no living corals below about 20–30 fathoms (36–55 m). Darwin carefully noted the location of the different types of coral around the reef and in the lagoon. In his diary, he described, \"examining the very interesting yet simple structure & origin of these islands. The water being unusually smooth, I waded in as far as the living mounds of coral on which the swell of the open sea breaks. In some of the gullies & hollows, there were beautiful green & other colored fishes, & the forms & tints of many of the Zoophites were admirable. It is excusable to grow enthusiastic over the infinite numbers of organic beings with which the sea of the tropics, so prodigal of life, teems\", though he cautioned against the \"rather exuberant language\" used by some naturalists.\n\nAs they left the islands after eleven days, Darwin wrote out a summary of his theory in his diary: \n\nWhen the \"Beagle\" returned on 2 October 1836, Darwin was already a celebrity in scientific circles, as in December 1835 University of Cambridge Professor of Botany John Stevens Henslow had fostered his former pupil's reputation by giving selected naturalists a pamphlet of Darwin's geological letters. Charles Lyell eagerly met Darwin for the first time on 29 October, enthusiastic about the support this gave to his uniformitarianism, and in May wrote to John Herschel that he was \"very full of Darwin's new theory of Coral Islands, and have urged Whewell to make him read it at our next meeting. I must give up my volcanic crater theory for ever, though it cost me a pang at first, for it accounted for so much... the whole theory is knocked on the head, and the annular shape and central lagoon have nothing to do with volcanoes, nor even with a crateriform bottom... Coral islands are the last efforts of drowning continents to lift their heads above water. Regions of elevation and subsidence in the ocean may be traced by the state of the coral reefs.\" Darwin presented his findings and theory in a paper which he read to the Geological Society of London on 31 May 1837.\n\nDarwin's first literary project was his \"Journal and Remarks\" on the natural history of the expedition, now known as \"The Voyage of the Beagle\". In it he expanded his diary notes into a section on this theory, emphasising how the presence or absence of coral reefs and atolls can show whether the ocean bed is elevating or subsiding. At the same time he was privately speculating intensively about transmutation of species, and taking on other projects. He finished writing out his journal around the end of September, but then had the work of correcting proofs.\n\nHis tasks included finding experts to examine and report on his collections from the voyage. Darwin proposed to edit these reports, writing his own forewords and notes, and used his contacts to lobby for government sponsorship of publication of these findings as a large book. When a Treasury grant of £1,000 was allocated at the end of August 1837, Darwin stretched the project to include the geology book that he had conceived in April 1832 at the first landfall in the voyage. He selected Smith, Elder & Co. as the publisher, and gave them unrealistic commitments on the timing of providing the text and illustrations. He assured the Treasury that the work would be good value, as the publisher would only require a small commission profit, and he himself would have no profit. From October he planned what became the multi-volume \"Zoology of the Voyage of H.M.S. Beagle\" on his collections, and began writing about the geology of volcanic islands.\n\nIn January 1838, Smith, Elder & Co. advertised the first part of Darwin's geology book, \"Geological observations on volcanic islands and coral formations\", as a single octavo volume to be published that year. By the end of the month Darwin thought that his geology was \"covering so much paper, & will take so much time\" that it could be split into separate volumes (eventually \"Coral reefs\" was published first, followed by \"Volcanic islands\" in 1844, and \"South America\" in 1846). He also doubted that the treasury funds could cover all the geological writings. The first part of the zoology was published in February 1838, but Darwin found it a struggle to get the experts to produce their reports on his collections, and overwork led to illness. After a break to visit Scotland, he wrote up a major paper on the geological \"roads\" of Glen Roy. On 5 October 1838 he noted in his diary, \"Began Coral Paper: requires much reading\".\n\nIn November 1838 Darwin proposed to his cousin Emma, and they married in January 1839. As well as his other projects he continued to work on his ideas of evolution as his \"prime hobby\", but repeated delays were caused by his illness. He sporadically restarted work on \"Coral Reefs\", and on 9 May 1842 wrote to Emma, telling her he was \n\n\"The Structure and Distribution of Coral Reefs\" was published in May 1842, priced at 15 shillings, and was well received. A second edition was published in 1874, extensively revised and rewritten to take into account James Dwight Dana's 1872 publication \"Corals and Coral Islands\", and work by Joseph Jukes.\n\nThe book has a tightly logical structure, and presents a bold argument. Illustrations are used as an integral part of the argument, with numerous detailed charts and one large world map marked in colour showing all reefs known at that time. A brief introduction sets out the aims of the book.\nThe first three chapters describe the various types of coral reef, each chapter starting with a section giving a detailed description of the reef Darwin had most information about, which he presents as a typical example of the type. Subsequent sections in each chapter then describe other reefs in comparison with the typical example. In the first chapter, Darwin describes atolls and lagoon islands, taking as his typical example his own detailed findings and the \"Beagle\" survey findings on the Keeling Islands. The second chapter similarly describes a typical barrier reef then compares it to others, and the third chapter gives a similar description of what Darwin called fringing or shore reefs. Having described the principal kinds of reef in detail, his finding was that the actual surface of the reef did not differ much. An atoll differs from an encircling barrier reef only in lacking the central island, and a barrier reef differs from a fringing reef only in its distance from the land and in enclosing a lagoon.\n\nThe fourth chapter on the distribution and growth of coral reefs examines the conditions in which they flourish, their rate of growth and the depths at which the reef building polyps can live, showing that they can only flourish at a very limited depth. In the fifth chapter he sets out his theory as a unified explanation for the findings of the previous chapters, overcoming the difficulties of treating the various kinds of reef as separate and the problem of reliance on the improbable assumption that underwater mountains just happened to be at the exact depth below sea level, by showing how barrier reefs and then atolls form as the land subsides, and fringing reefs are found along with evidence that the land is being elevated. This chapter ends with a summary of his theory illustrated with two woodcuts each showing two different stages of reef formation in relation to sea level.\n\nIn the sixth chapter he examines the geographical distribution of types of reef and its geological implications, using the large coloured map of the world to show vast areas of atolls and barrier reefs where the ocean bed was subsiding with no active volcanos, and vast areas with fringing reefs and volcanic outbursts where the land was rising. This chapter ends with a recapitulation which summarises the findings of each chapter and concludes by describing the global image as \"a magnificent and harmonious picture of the movements, which the crust of the earth has within a late period undergone\". A large appendix gives a detailed and exhaustive description of all the information he had been able to obtain on the reefs of the world.\n\nThis logical structure can be seen as a prototype for the organisation of \"On the Origin of Species\", presenting the detail of various aspects of the problem, then setting out a theory explaining the phenomena, followed by a demonstration of the wider explanatory power of the theory. Unlike the \"Origin\" which was hurriedly put together as an abstract of his planned \"big book\", \"Coral Reefs\" is fully supported by citations and material gathered together in the Appendix. \"Coral Reefs\" is arguably the first volume of Darwin's huge treatise on his philosophy of nature, like his succeeding works showing how slow gradual change can account for the history of life. In presenting types of reef as an evolutionary series it demonstrated a rigorous methodology for historical sciences, interpreting patterns visible in the present as the results of history. In one passage he presents a particularly Malthusian view of a struggle for survival – \"In an old-standing reef, the corals, which are so different in kind on different parts of it, are probably all adapted to the stations they occupy, and hold their places, like other organic beings, by a struggle one with another, and with external nature; hence we may infer that their growth would generally be slow, except under peculiarly favourable circumstances.\"\n\nHaving successfully completed and published the other books on the geology and zoology of the voyage, Darwin spent eight years on a major study of barnacles. Two volumes on \"Lepadidae\" (goose barnacles) were published in 1851. While he was still working on two volumes on the remaining barnacles, Darwin learnt to his delight in 1853 that the Royal Society had awarded him the Royal Medal for Natural Science. Joseph Dalton Hooker wrote telling him that \"Pordock proposed you for the Coral Islands & Lepadidae, Bell followed seconding on the Lepadidae alone, & then followed such a shout of paeans for the Barnacles that you would have [smiled] to hear.\"\n\nA major scientific controversy over the origin of coral reefs took place in the late 19th Century, between supporters of Darwin's theory (such as the American geologist James Dwight Dana, who early in his career had seen coral reefs in Hawaii and Fiji during the 1838–42 United States Exploring Expedition), and those who supported a rival theory put forward by the Scottish oceanographer John Murray, who participated in the 1872–76 Challenger expedition. Murray's theory challenged Darwin's notion of subsidence, proposing instead that coral reefs formed when accumulating mounds of calcareous marine sediments reached the shallow depths that could support the growth of corals. Amongst Murray's supporters was the independently wealthy American scientist Alexander Agassiz, who financed and undertook several expeditions to the Caribbean, Pacific and Indian Ocean regions to examine coral reefs in search of evidence to support Murray's theory, and to discredit Darwin.\n\nA series of expeditions to test Darwin's theory by drilling on Funafuti atoll in the Ellice Islands (now part of Tuvalu) was conducted by the Royal Society of London for the purpose of investigating whether basalt or traces of shallow water organisms could be found at depth in the coral. Drilling occurred in 1896, 1897 and 1898, attaining a final depth of , still in coral. Professor Edgeworth David of the University of Sydney was a member of the 1896 expedition and leader of the 1897 expedition. At the time these results were regarded as inconclusive and it was not until the 1950s when, prior to carrying out nuclear bomb tests on Eniwetok, deep exploratory drilling through of coral to the underlying basalt finally vindicated Darwin's theory. However, the geologic history of atolls is more complex than Darwin (1842) and Davis (1920 & 1928) envisioned.\n\nDarwin's interest on the biology of reef organisms was focussed on aspects related to his geological idea of subsidence; in particular, he was looking for confirmation that the reef building organisms could only live at shallow depths. FitzRoy's soundings at the Keeling Islands gave a depth limit for live coral of about 20 fathoms (37 m), and taking into account numerous observations by others, Darwin worked with a probable limit of 30 fathoms (55 m). Modern findings suggest a limit of around 100 m, still a small fraction of the depth of the ocean floor at 3000–5000 m. Darwin recognised the importance of red algae, and he reviewed other organisms that could have helped to build the reefs. He thought they lived at similarly shallow depths, but banks formed at greater depths were found in the 1880s. Darwin reviewed the distribution of different species of coral across a reef. He thought that the seaward reefs most exposed to wind and waves were formed by massive corals and red algae; this would be the most active area of reef growth and so would cause a tendency for reefs to grow outwards once they reach sea level. He believed that higher temperatures and the calmer water of the lagoons favoured the greatest coral diversity. These ecological ideas are still current, and research on the details continues.\nIn assessing the geology of the reef, Darwin showed his remarkable ability to collect facts and find patterns to reconstruct geological history on the basis of the very limited evidence available. He gave attention to the smallest detail. Having heard that parrotfish browsed on the living coral, he dissected specimens to find finely ground coral in their intestines. He concluded that such fish, and coral eating invertebrates such as Holothuroidea, could account for the banks of fine grained mud he found at the Keeling Islands; it showed also \"that there are living checks to the growth of coral-reefs, and that the almost universal law of 'consume and be consumed,' holds good even with the polypifers forming those massive bulwarks, which are able to withstand the force of the open ocean.\"\n\nHis observations on the part played by organisms in the formation of the various features of reefs anticipated modern studies. To establish the thickness of coral barrier reefs, he relied on the old nautical rule of thumb to project the slope of the land to that below sea level, and then applied his idea that the coral reef would slope much more steeply than the underlying land. He was fortunate to guess that the maximum depth of coral would be around 5,000 ft (1,500 m), as the first test bores conducted by the United States Atomic Energy Commission on Enewetak Atoll in 1952 drilled down through 4610 ft (1,405 m) of coral before reaching the volcanic foundations. In Darwin's time no comparable thickness of fossil coral had been found on the continents, and when this was raised as a criticism of his theory neither he nor Lyell could find a satisfactory explanation. It is now thought that fossil reefs are usually broken up by tectonic movements, but at least two continental fossil reef complexes have been discovered to be about 3,000 ft (1,000 m) thick. While these findings have confirmed his argument that the islands were subsiding, his other attempts to show evidence of subsidence have been superseded by the discovery that glacial effects can cause changes in sea level.\n\nIn Darwin's global hypothesis, vast areas where the seabed was being elevated were marked by fringing reefs, sometimes around active volcanoes, and similarly huge areas where the ocean floor was subsiding were indicated by barrier reefs or atolls based on inactive volcanoes. These views received general support from deep sea drilling results in the 1980s. His idea that rising land would be balanced by subsidence in ocean areas has been superseded by modern plate tectonics, which he did not anticipate.\n\n\n \n"}
{"id": "29549602", "url": "https://en.wikipedia.org/wiki?curid=29549602", "title": "UKRC", "text": "UKRC\n\nThe UKRC (UK Resource Centre) is a UK organisation for the provision of advice, services and policy consultation regarding the under-representation of women in science, engineering, technology and the built environment (SET).\nIt is funded by the Department for Business, Innovation and Skills and was launched in 2004.\n\nThe central base is located in Bradford in the North of England but there are also centres in South East England, South Yorkshire, Scotland and Wales.\n\nThe UKRC works to promote gender equality in SET with employers and professional bodies; education institutions; women's organisations and networks; policy institutes; sector skills councils; the government and many others. This includes implementing the Athena SWAN Charter.\n\nIn 2011 it took over the leadership of the WISE Campaign and became UKRC-WISE. In late 2012 it took on the WISE name.\n\n\n"}
{"id": "26491446", "url": "https://en.wikipedia.org/wiki?curid=26491446", "title": "V-1 and V-2 Intelligence", "text": "V-1 and V-2 Intelligence\n\nMilitary intelligence on the V-1 and V-2 weapons developed by the Germans for attacks on the United Kingdom during the Second World War was important to countering them. Intelligence came from a number of sources and the Anglo-American intelligence agencies used it to assess the threat of the German V-weapons.\n\nThe activities included use of the Double Cross System for counter-intelligence and the British (code named) \"Big Ben\" project to reconstruct and evaluate German missile technology for which Denmark, Poland, Sweden, and the USSR provided assistance.\nGerman counter-intelligence ruses were used to mislead the Allies about V-1 launch sites and the Peenemünde Army Research Center which were targeted for attacks by the Allies.\n\nPR — aerial photographic reconnaissance<br>\n- exchange of early stray V2 rocket.<br>\n, — events regarding Anglo-American intelligence<br>\n, , — military operations (RAF, US, Luftwaffe)\n\nThe day after Strategic Bombing Directive No. 4 ended the strategic air war in Europe, the use of radar was discontinued in the London Civil Defence Region for detecting V-2 launches. The last launches had been on March 27 (V-2) and March 29 (V-1 flying bomb).\n\n\n"}
{"id": "48553837", "url": "https://en.wikipedia.org/wiki?curid=48553837", "title": "Vacuum cooling", "text": "Vacuum cooling\n\nVacuum cooling is known to be the most rapid cooling technique for any porous product which has free water and works on the principle of evaporative cooling. Vacuum cooling is generally used for cooling food products having a high water content and large porosities, due to its efficacy in losing water from both within and outside the products. This is the most widely used technique for rapid cooling of food product which has been proven to be one of the most efficient and economical method of cooling and storage of vegetables, fruits, flowers & more. \n\nThis cooling technology not only strongly improves the product quality, but also increases the shelf life of product and at the same time it reduces the cooling costs compared to the conventional cooling method available.\n\nThe technology is based on the phenomenon that as the vapour pressure on a liquid reduces, its boiling point reduces. The boiling point of a liquid is defined as the temperature at which the vapour pressure of the liquid is equal to the external pressure. When the pressure above a liquid is reduced, the vapour pressure needed to induce boiling is also reduced, and the boiling point of the liquid decreases. By reducing pressure we can even boil off water at lower temperatures. This rapid evaporation of moisture from the surface and within the products due to the low surrounding pressure, absorbs the necessary latent heat for phase change from the product itself. This latent heat required for evaporation is obtained mostly from the sensible heat of the product and as a consequence of this evaporation the temperature of the product falls and the product can be cooled down to its desired storage temperature.\n\nAn airtight chamber is maintained by removing air from the inside of the chamber using a vacuum pump. The products to be cooled are kept in that airtight chamber. As the pressure is reduced the boiling point of water reduces and water starts to evaporate, taking the heat from the product. As a consequence of this evaporation the product temperature begins to decrease. This cooling process of the products continues until it reaches the desired product temperature.\n\nFor maintaining steady cooling process, it is necessary to evacuate the chamber continuously.\n\nOther factors that determine the cooling process are surface area of product, available for heat transfer and also the product sensitivity during losing water.\n\nAs the product is cooled uniformly throughout the body without any temperature gradient in the body, the shelf life of the product increases.\n\nSometimes excess moisture loss during the cooling process will deteriorate the product quality and therefore there is a limit to the cooling process. This problem is to be taken care of by maintaining the required pressure, temperature and time of cooling.\n\n"}
{"id": "40868618", "url": "https://en.wikipedia.org/wiki?curid=40868618", "title": "We choose to go to the Moon", "text": "We choose to go to the Moon\n\n\"We choose to go to the Moon\" is the famous tagline of a speech about the effort to reach the Moon delivered by U.S. President John F. Kennedy to a large crowd gathered at Rice Stadium in Houston, Texas on September 12, 1962. The speech was intended to persuade the American people to support the Apollo program, the national effort to land a man on the Moon.\n\nIn his speech, Kennedy characterized space as a new frontier, invoking the pioneer spirit that dominated American folklore. He infused the speech with a sense of urgency and destiny, and emphasized the freedom enjoyed by Americans to choose their destiny rather than have it chosen for them. Although he called for competition with the Soviet Union, he also proposed making the Moon landing a joint project.\n\nThe speech resonated widely and is still remembered, although at the time there was disquiet about the cost and value of the Moon-landing effort. Kennedy's goal was posthumously realized in July 1969, with the successful Apollo 11 mission.\n\nWhen John F. Kennedy became President of the United States in January 1961, many Americans perceived that the United States was losing the Space Race with the Soviet Union, which had successfully launched the first artificial satellite, Sputnik 1, almost four years earlier. The perception increased when, on April 12, 1961, Russian cosmonaut Yuri Gagarin became the first man in space before the U.S. could launch its first Project Mercury astronaut. American prestige was further damaged by the Bay of Pigs fiasco five days later.\n\nConvinced of the political need for an achievement which would decisively demonstrate America's space superiority, Kennedy asked his Vice President, Lyndon B. Johnson, in his role as chairman of the National Aeronautics and Space Council, to identify such an achievement. He specifically asked him to investigate whether the United States could beat the Soviet Union in putting a laboratory in space, or orbiting a man around the Moon, or landing a man on the Moon, and to find out what such a project would cost. Johnson consulted with officials of the National Aeronautics and Space Administration (NASA). Its new Administrator, James E. Webb, told him that there was no chance of beating the Russians to launching a space station, and it was uncertain as to whether NASA could orbit a man around the Moon first, so the best option would be to attempt to land a man on the Moon. This would also be the most expensive option; Webb believed it would require $22 billion to achieve it by 1970. Johnson also consulted with Wernher von Braun; military leaders, including Lieutenant General Bernard Schriever; and three captains of industry: Frank Stanton from CBS, Donald C. Cook from American Electric Power, and George R. Brown from Brown & Root. \n\nKennedy stood before Congress on May 25, 1961, and proposed that the US \"should commit itself to achieving the goal, before this decade is out, of landing a man on the Moon and returning him safely to the Earth.\" Not everyone was impressed; a Gallup Poll indicated that 58 percent of Americans were opposed. \n\nKennedy's goal gave a specific mission to NASA's Apollo program. This required the expansion of NASA's Space Task Group into a Manned Spacecraft Center. Houston, Texas was chosen as the site, and the Humble Oil and Refining Company donated the land during 1961, with Rice University as an intermediary. Kennedy took a two-day visit to Houston in September 1962 to view the new facility. He was escorted by astronauts Scott Carpenter and John Glenn, and shown models of the Gemini and Apollo spacecraft; he also viewed the Mercury spacecraft, in which Glenn had made America's first orbital flight. He took advantage of the opportunity to deliver a speech to drum up support for the nation's space effort.\n\nOn September 12, 1962, a warm and sunny day, President Kennedy delivered his speech before a crowd of 40,000 people in the Rice University football stadium, many of them school children. The middle portion of the speech has been widely quoted, and reads as follows:\nThe joke referring to the Rice–Texas football rivalry was handwritten by Kennedy into the speech text, and is the part of the speech remembered by sports fans. Since then, Rice has beaten Texas in 1965 and 1994. Later in the speech Kennedy also made a joke about the heat. The jokes elicited cheers and laughter from the audience. While these side comments may have diminished the rhetorical power of the speech, and do not resonate outside Texas, they stand as a reminder of the part Texas played in the space race.\n\nKennedy's speech used three strategies: \"a characterization of space as a beckoning frontier; an articulation of time that locates the endeavor within a historical moment of urgency and plausibility; and a final, cumulative strategy that invites audience members to live up to their pioneering heritage by going to the Moon.\"\n\nWhen addressing the crowd at Rice University, he equated the desire to explore space with the pioneering spirit that had dominated American folklore since the nation’s foundation. This allowed Kennedy to reference back to his inaugural address, when he declared to the world \"Together let us explore the stars\". When he met with Nikita Khrushchev, the Premier of the Soviet Union in June 1961, Kennedy proposed making the Moon landing a joint project, but Khrushchev did not take up the offer.\n\nKennedy verbally condensed human history to fifty years, in which \"only last week did we develop penicillin and television and nuclear power, and now if America's new spacecraft succeeds in reaching Venus, we will have literally reached the stars before midnight tonight.\" With this extended metaphor, Kennedy sought to imbue a sense of urgency and change in his audience. Most prominently, the phrase \"We choose to go to the Moon\" in the Rice speech was repeated three times consecutively, followed by an explanation that climaxes in his declaration that the challenge of space is \"one that we are willing to accept, one we are unwilling to postpone, and one which we intend to win.\"\n\nConsidering the line before rhetorically asked the audience why they choose to compete in tasks that challenge them, Kennedy highlighted here the nature of the decision to go to space as being a choice, an option that the American people have elected to pursue. Rather than claim it as essential, he emphasized the benefits such an endeavor could provide – uniting the nation and the competitive aspect of it. As Kennedy told Congress earlier, \"whatever mankind must undertake, free men must fully share.\" These words emphasized the freedom enjoyed by Americans to choose their destiny rather than have it chosen for them. Combined with Kennedy's overall usage of rhetorical devices in the Rice University speech, they were particularly apt as a declaration that began the American space race.\n\nKennedy was able to describe a romantic notion of space in the Rice University speech with which all citizens of the United States, and even the world could participate, vastly increasing the number of citizens interested in space exploration. He began by talking about space as the new frontier for all of mankind, instilling the dream within the audience. He then condensed human history to show that within a very brief period of time space travel will be possible, informing the audience that their dream is achievable. Lastly, he uses the first-personal plural \"we\" to represent all the people of the world that would allegedly explore space together, but also involves the crowd.\n\nPaul Burka, the executive editor of \"Texas Monthly\" magazine, a Rice alumnus who was present in the crowd that day, recalled 50 years later that the speech \"speaks to the way Americans viewed the future in those days. It is a great speech, one that encapsulates all of recorded history and seeks to set it in the history of our own time. Unlike today’s politicians, Kennedy spoke to our best impulses as a nation, not our worst.\" Ron Sass and Robert Curl were among the many members of the Rice University faculty present. Curl was amazed by the cost of the space exploration program. They recalled that the ambitious goal did not seem so remarkable at the time, and that Kennedy's speech was not regarded as so different from one delivered by President Dwight D. Eisenhower at Rice's Autry Court in 1960; but that speech has long since been forgotten, while Kennedy's is still remembered.\n\nThe speech did not stem a rising tide of disquiet about the Moon landing effort. There were many other things that the money could be spent on. Eisenhower declared, \"To spend $40 billion to reach the Moon is just nuts.\" Senator Barry Goldwater argued that the civilian space program was pushing the more important military one aside. Senator William Proxmire feared that scientists would be diverted away from military research into space exploration. A budget cut was only narrowly averted. Kennedy gave a speech to the United Nations General Assembly on September 20, 1963, in which he again proposed a joint expedition to the Moon. Khrushchev remained cautious about participating, and responded with a statement in October 1963 in which he declared that the Soviet Union had no plans to send cosmonauts to the Moon. However, his military advisors persuaded him that the offer was a good one, as it would enable the Soviet Union to acquire American technology. Kennedy ordered reviews of the Apollo project in April, August and October 1963. The final report was received on November 29, 1963, a week after Kennedy's assassination.\nThe idea of a joint Moon mission was abandoned after Kennedy's death, but the Apollo Project became a memorial to him. His goal was fulfilled in July 1969, with the successful Apollo 11 Moon landing. This accomplishment remains an enduring legacy of Kennedy's speech, but his deadline demanded a necessarily narrow focus, and there was no indication of what should be done next once it was achieved. Apollo did not usher in an era of lunar exploration, and no further missions were sent to the Moon after Apollo 17 in 1972. Subsequent planned Apollo missions were cancelled. The Space Shuttle and International Space Station projects never captured the public imagination the way the Apollo Project did, and NASA would struggle to realize its visions with inadequate resources. Ambitious visions of space exploration were proclaimed by Presidents George H. W. Bush in 1989 and George W. Bush in 2004, but the future of the American space program remained uncertain.\n\n \n\n"}
{"id": "179581", "url": "https://en.wikipedia.org/wiki?curid=179581", "title": "Wimshurst machine", "text": "Wimshurst machine\n\nThe Wimshurst influence machine is an electrostatic generator, a machine for generating high voltages developed between 1880 and 1883 by British inventor James Wimshurst (1832–1903).\n\nIt has a distinctive appearance with two large contra-rotating discs mounted in a vertical plane, two crossed bars with metallic brushes, and a spark gap formed by two metal spheres.\n\nThese machines belong to a class of electrostatic generators called influence machines, which separate electric charges through electrostatic induction, or \"influence\", not depending on friction for their operation. Earlier machines in this class were developed by Wilhelm Holtz (1865 and 1867), August Toepler (1865), J. Robert Voss (1880), and others. The older machines are less efficient and exhibit an unpredictable tendency to switch their polarity. The Wimshurst does not have this defect.\n\nIn a Wimshurst machine, the two insulated discs and their metal sectors rotate in opposite directions passing the crossed metal neutralizer bars and their brushes. An imbalance of charges is induced, amplified, and collected by two pairs of metal combs with points placed near the surfaces of each disk. These collectors are mounted on insulating supports and connected to the output terminals. The positive feedback increases the accumulating charges exponentially until the dielectric breakdown voltage of the air is reached and an electric spark jumps across the gap.\n\nThe machine is theoretically not self-starting, meaning that if none of the sectors on the discs has any electrical charge there is nothing to induce charges on other sectors. In practice, even a small residual charge on any sector is enough to start the process going once the discs start to rotate. The machine will only work satisfactorily in a dry atmosphere. It requires mechanical power to turn the disks against the electric field, and it is this energy that the machine converts into the electric power of the spark. The steady state output of the Wimshurst machine is a direct (non-alternating) current that is proportional to the area covered by the metal sector, the rotation speed, and a complicated function of the initial charge distribution. The insulation and the size of the machine determine the maximum output voltage that can be reached. The accumulated spark energy can be increased by adding a pair of Leyden jars, an early type of capacitor suitable for high voltages, with the jars’ inner plates independently connected to each of the output terminals and the jars’ outer plates interconnected. A typical Wimshurst machine can produce sparks that are about a third of the disc's diameter in length and several tens of microamperes.\n\nThe available voltage gain can be understood by noting that the charge density on oppositely charged sectors, between the neutralizer bars, is nearly uniform across the sectors, and thus at low voltage, while the charge density on same charged sectors, approaching the collector combs, peaks near the sector edges, at a consequently high voltage relative to the opposite collector combs.\n\nWimshurst machines were used during the 19th century in physics research. They were also occasionally used to generate high voltage to power the first-generation Crookes X-ray tubes during the first two decades of the 20th century, although Holtz machines and induction coils were more commonly used. Today they are only used in science museums and education to demonstrate the principles of electrostatics.\n\nThe two contra-rotating insulating discs (usually made of glass) have a number of metal sectors stuck onto them. The machine is provided with four small earthed brushes (two on each side of the machine on conducting shafts at 90° to each other), plus a pair of charge-collection combs. The conducting shafts, that hold the brushes on a typical Wimshurst machine, would form the shape of an \"X\", if one could see through the insulating disks, as they are perpendicular to each other. The charge-collection combs are typically mounted along the horizontal and equally contact the outer edges of both front and back discs. The collection combs on each side are usually connected to respective Leyden jars.\n\nAny small charge on either of the two discs suffices to begin the charging process. Suppose, therefore, that the back disc has a small, net electrostatic charge. For concreteness, assume this charge is positive (red) and that the back disc ([A] lower chain) rotates counter-clockwise (right to left). As the charged sector (moving red square) rotates to the position of the brush ([Y] down arrow tip) next to front disc ([B] upper chain near center), it induces a polarization of charge on the conducting shaft ([Y-Y1] upper horizontal black line) holding the brush, attracting negative (green) charge to the near side ([Y] upper square becoming green), so that positive (red) charge accumulates on the far side (across the disc, 180 degrees away) ([Y1] upper square becoming red). The shaft's polarized charges attach to the nearest sectors on disc B, resulting in negative charge on B [Y] closer to the original positive charge on A, and positive charge on the opposite side of B [Y1]. After an additional 45° rotation ([Z] near lower chain middle), the positive (red) charge on A (lower chain) is repelled by a positive (red) charge on B ([Z] upper chain) approaching. The first collection comb ([Z] arrow-tipped lines to triangles) encountered allows both positive (red) charges to leave the sectors neutral (squares becoming black), and accumulate in the Leyden jar anode (red triangle) attracted to the Leyden jar cathode (green triangle). The charge completes the cycle across the discs when a spark (yellow zigzag) discharges the Leyden jar (red and green triangles).\n\nAs B rotates 90° clockwise (left to right), the charges that have been induced on it line up with the brushes next to disc A [X, X1]. The charges on B induce the opposite polarization of the A-brushes' shaft, and the shaft's polarization is transferred to its disc. Disc B keeps rotating and its charges are accumulated by the nearest charge-collection combs.\n\nDisc A rotates 90° so that its charges line up with the brush of disc B [Y, Y1], where an opposite charge-polarization is induced on the B conducting shaft and the nearest sectors of B, similar to the description two paragraphs above.\n\nThe process repeats, with each charge polarization on A inducing polarization on B, inducing polarization on A, etc. All of these induced positive and negative charges are collected by combs to charge the Leyden jars, electrical charge-storage devices similar to capacitors. The mechanical energy required to separate the opposing charges on the adjacent sectors provides the energy source for the electrical output.\n\n\n\n"}
