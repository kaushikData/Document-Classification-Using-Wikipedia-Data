{"id": "35045841", "url": "https://en.wikipedia.org/wiki?curid=35045841", "title": "Abell 133", "text": "Abell 133\n\nAbell 133 is a galaxy cluster in the Abell catalogue.\n\n"}
{"id": "52707422", "url": "https://en.wikipedia.org/wiki?curid=52707422", "title": "Alexander Killen Macbeth", "text": "Alexander Killen Macbeth\n\nAlexander Killen Macbeth CMG, DSc, FAA (1889–1957), generally called Killen or A. Killen Macbeth, was an Irish born academic and organic chemist who from 1928 until his retirement in 1954 was the Angas Professor of Chemistry at the University of Adelaide. In 1946 he was appointed a Companion of the Order of St Michael and St George, and in 1955 he was elected Fellow of the Australian Academy of Science.\n"}
{"id": "38180687", "url": "https://en.wikipedia.org/wiki?curid=38180687", "title": "Base calling", "text": "Base calling\n\nBase calling is the process of assigning bases (nucleobases) to chromatogram peaks. One computer program for accomplishing this job is Phred base-calling, which is a widely used basecalling software program by both academic and commercial DNA sequencing laboratories because of its high base calling accuracy.\n"}
{"id": "40780", "url": "https://en.wikipedia.org/wiki?curid=40780", "title": "Beam diameter", "text": "Beam diameter\n\nThe beam diameter or beam width of an electromagnetic beam is the diameter along any specified line that is perpendicular to the beam axis and intersects it. Since beams typically do not have sharp edges, the diameter can be defined in many different ways. Five definitions of the beam width are in common use: D4σ, 10/90 or 20/80 knife-edge, 1/e, FWHM, and D86. The beam width can be measured in units of length at a particular plane perpendicular to the beam axis, but it can also refer to the angular width, which is the angle subtended by the beam at the source. The angular width is also called the beam divergence.\n\nBeam diameter is usually used to characterize electromagnetic beams in the optical regime, and occasionally in the microwave regime, that is, cases in which the aperture from which the beam emerges is very large with respect to the wavelength.\n\nBeam diameter usually refers to a beam of circular cross section, but not necessarily so. A beam may, for example, have an elliptical cross section, in which case the orientation of the beam diameter must be specified, for example with respect to the major or minor axis of the elliptical cross section. The term \"beam width\" may be preferred in applications where the beam does not have circular symmetry.\n\nThe angle between the maximum peak of radiated power and the first null (no power radiated in this direction) is called the Rayleigh beamwidth.\n\nThe simplest way to define the width of a beam is to choose two diametrically opposite points at which the irradiance is a specified fraction of the beam's peak irradiance, and take the distance between them as a measure of the beam's width. An obvious choice for this fraction is ½ (−3 dB), in which case the diameter obtained is the full width of the beam at half its maximum intensity (FWHM). This is also called the \"half-power beam width\" (HPBW).\n\nThe 1/e width is equal to the distance between the two points on the marginal distribution that are 1/e = 0.135 times the maximum value. In many cases, it makes more sense to take the distance between points where the intensity falls to 1/e = 0.135 times the maximum value. If there are more than two points that are 1/e times the maximum value, then the two points closest to the maximum are chosen. The 1/e width is important in the mathematics of Gaussian beams, in which the intensity profile is described by formula_1.\n\nThe American National Standard Z136.1-2007 for Safe Use of Lasers (p. 6) defines the beam diameter as the distance between diametrically opposed points in that cross-section of a beam where the power per unit area is 1/e (0.368) times that of the peak power per unit area. This is the beam diameter definition that is used for computing the maximum permissible exposure to a laser beam. In addition, the Federal Aviation Administration also uses the 1/e definition for laser safety calculations in FAA Order JO 7400.2, Para. 29-1-5d.\n\nMeasurements of the 1/e width only depend on three points on the marginal distribution, unlike D4σ and knife-edge widths that depend on the integral of the marginal distribution. 1/e width measurements are noisier than D4σ width measurements. For multimodal marginal distributions (a beam profile with multiple peaks), the 1/e width usually does not yield a meaningful value and can grossly underestimate the inherent width of the beam. For multimodal distributions, the D4σ width is a better choice. For an ideal single-mode Gaussian beam, the D4σ, D86 and 1/e width measurements would give the same value.\n\nFor a Gaussian beam, the relationship between the 1/e width and the full width at half maximum is formula_2, where formula_3 is the full width of the beam at 1/e.\n\nThe D4σ width of a beam in the horizontal or vertical direction is 4 times σ, where σ is the standard deviation of the horizontal or vertical marginal distribution respectively. Mathematically, the D4σ beam width in the \"x\" dimension for the beam profile formula_4 is expressed as\n\nwhere\n\nis the centroid of the beam profile in the \"x\" direction.\n\nWhen a beam is measured with a laser beam profiler, the wings of the beam profile influence the D4σ value more than the center of the profile, since the wings are weighted by the square of its distance, \"x\", from the center of the beam. If the beam does not fill more than a third of the beam profiler's sensor area, then there will be a significant number of pixels at the edges of the sensor that register a small baseline value (the background value). If the baseline value is large or if it is not subtracted out of the image, then the computed D4σ value will be larger than the actual value because the baseline value near the edges of the sensor are weighted in the D4σ integral by \"x\". Therefore, baseline subtraction is necessary for accurate D4σ measurements. The baseline is easily measured by recording the average value for each pixel when the sensor is not illuminated. The D4σ width, unlike the FWHM and 1/e widths, is meaningful for multimodal marginal distributions — that is, beam profiles with multiple peaks — but requires careful subtraction of the baseline for accurate results. The D4σ is the ISO international standard definition for beam width.\n\nBefore the advent of the CCD beam profiler, the beam width was estimated using the knife-edge technique: slice a laser beam with a razor and measure the power of the clipped beam as a function of the razor position. The measured curve is the integral of the marginal distribution, and starts at the total beam power and decreases monotonically to zero power. The width of the beam is defined as the distance between the points of the measured curve that are 10% and 90% (or 20% and 80%) of the maximum value. If the baseline value is small or subtracted out, the knife-edge beam width always corresponds to 60%, in the case of 20/80, or 80%, in the case of 10/90, of the total beam power no matter what the beam profile. On the other hand, the D4σ, 1/e, and FWHM widths encompass fractions of power that are beam-shape dependent. Therefore, the 10/90 or 20/80 knife-edge width is a useful metric when the user wishes to be sure that the width encompasses a fixed fraction of total beam power. Most CCD beam profiler's software can compute the knife-edge width numerically.\n\nThe main drawback of the knife-edge technique is that the measured value is displayed only on the scanning direction, minimizing the amount of relevant beam information. To overcome this drawback, an innovative technology offered commercially allows multiple directions beam scanning to create an image like beam representation.\n\nBy mechanically moving the knife edge across the beam, the amount of energy impinging the detector area is determined by the obstruction. The profile is then measured from the knife-edge velocity and its relation to the detector's energy reading. Unlike other systems, a unique scanning technique uses several different oriented knife-edges to sweep across the beam. By using tomographic reconstruction, mathematical processes reconstruct the laser beam size in different orientations to an image similar to the one produced by CCD cameras. The main advantage of this scanning method is that it is free from pixel size limitations (as in CCD cameras) and allows beam reconstructions with wavelengths not usable with existing CCD technology. Reconstruction is possible for beams in deep UV to far IR.\n\nThe D86 width is defined as the diameter of the circle that is centered at the centroid of the beam profile and contains 86% of the beam power. The solution for D86 is found by computing the area of increasingly larger circles around the centroid until the area contains 0.86 of the total power. Unlike the previous beam width definitions, the D86 width is not derived from marginal distributions. The percentage of 86, rather than 50, 80, or 90, is chosen because a circular Gaussian beam profile integrated down to 1/e of its peak value contains 86% of its total power. The D86 width is often used in applications that are concerned with knowing exactly how much power is in a given area. For example, applications of high-energy laser weapons and lidars require precise knowledge of how much transmitted power actually illuminates the target.\n\nThe definition given before holds for stigmatic (circular symmetric) beams only. For astigmatic beams, however, a more rigorous definition of the beam width has to be used:\nand\nThis definition also incorporates information about \"x\"–\"y\" correlation formula_9, but for circular symmetric beams, both definitions are the same.\n\nSome new symbols appeared within the formulas, which are the first- and second-order moments:\nthe beam power \nand \n\nUsing this general definition, also the beam azimutal angle formula_17 can be expressed. It is the angle between the beam directions of minimal and maximal elongations, known as principal axes, and the laboratory system, being the formula_18 and formula_19 axes of the detector and given by \n\nInternational standard ISO 11146-1:2005 specifies methods for measuring beam widths (diameters), divergence angles and beam propagation ratios of laser beams (if the beam is stigmatic) and for general astigmatic beams ISO 11146-2 is applicable. The D4σ beam width is the ISO standard definition and the measurement of the M² beam quality parameter requires the measurement of the D4σ widths.\n\nThe other definitions provide complementary information to the D4σ. The D4σ and knife-edge widths are sensitive to the baseline value, whereas the 1/e and FWHM widths are not. The fraction of total beam power encompassed by the beam width depends on which definition is used.\n\nThe width of laser beams can be measured by capturing an image on a camera, or by using a laser beam profiler.\n"}
{"id": "41499988", "url": "https://en.wikipedia.org/wiki?curid=41499988", "title": "BioMA", "text": "BioMA\n\nModelling frameworks are used in modelling and simulation and can consist of a software infrastructure to develop and run mathematical models. They have provided a substantial step forward in the area of biophysical modelling with respect to monolithic implementations. The separation of algorithms from data, the reusability of I/O procedures and integration services, and the isolation of modelling solutions in discrete units has brought a solid advantage in the development of simulation systems. Modelling frameworks for agriculture have evolved over time, with different approaches and targets\n\nBioMA is a software framework developed focusing on platform-independent, re-usable components, including multi-model implementations at fine granularity.\n\nBioMA (Biophysical Model Applications) is a public domain software framework designed and implemented for developing, parameterizing and running modelling solutions based on biophysical models in the domains of agriculture and environment. It is based on discrete conceptual units codified in freely extensible software components .\n\nThe goal of this framework is to rapidly bridge from prototypes to operational applications, enabling running and comparing different modelling solutions. A key aspect of the framework is the transparency which allows for quality evaluation of outputs in the various steps of the modelling workflow. The framework is based on framework-independent components, both for the modelling solutions and the graphical user's interfaces. The goal is not only to provide a framework for model development and operational use but also, and of no lesser importance, to provide a loose collection of objects re-usable either standalone or in different frameworks. The software is developed using Microsoft C# language in the .NET framework.\n\nThe framework is a development of the work carried out under the APES task of the 6th EU Framework Program SEAMLESS project.\n\nDeployments of the platform and its tools and components have been used:\nBioMA applications and modelling solutions are the simulation tools used by the MARS unit of the European Commission to simulate agricultural production under scenarios of climate change. BioMA is also used in the EU FP7 project MODEXTREME.\n\nThe simulation system is discretized in layers, each with its own features and requirements. Such layers are the Model Layer (ModL), where fine granularity models are implemented as discrete units, the Composition Layer (CompL), where basic models are linked into more complex, aggregated models, and the Configuration Layer (ConfL), which allows providing context specific parameterization (in the software sense) for operational use. Applications can span from simple console applications to user-interacting applications based on the model-view-controller pattern, in the simplest cases linking either directly to either the ModL or the CompL, or accessing model ConfL. In all cases, the component oriented architecture allows implementing a set of functionalities which impact on the richness of functionality of the system and on its transparency. Layers implement no top-down dependency among them, hence facilitating the independent reuse of tools, utilities, and model components in different applications and frameworks.\n\nAdvanced applications can be grouped under two categories:\n\nApplications can be built based on the libraries as in the following figure. The libraries can be extended implementing new models, as shown in the software development kits, and new libraries can be added.\n\nModel components and tools can be autonomously downloaded with the SDK at the components' portal. Same for modelling solutions (starting from 2016).\n\nApplications must be requested by email, and, similarly to components, applications will be made available for free autonomous download during 2016.\n\n\nCode of core components is available under the MIT license, however, the reuse of binaries falls under the Creative Commons license as below, implying the no-commercial, share-alike clauses.\n\nApplication and tools are available under the Creative Commons license as binaries, however code can be shared under specific agreements between parties. Model component developers may make code available, however, they must make binaries available for reuse.\n"}
{"id": "14523900", "url": "https://en.wikipedia.org/wiki?curid=14523900", "title": "Budh Planitia", "text": "Budh Planitia\n\nBudh Planitia is a large basin on Mercury located at 22.0° N, 150.9° W. It lies to the east of Odin Planitia. It falls within the Tolstoj quadrangle. It is named after the Hindu word for Mercury, Budha.\n"}
{"id": "4674058", "url": "https://en.wikipedia.org/wiki?curid=4674058", "title": "Canada Foundation for Innovation", "text": "Canada Foundation for Innovation\n\nThe Canada Foundation for Innovation (CFI; , \"FCI\") is an independent not-for-profit organization that invests in research facilities and equipment in Canada's universities, colleges, research hospitals, and non-profit research institutions.\n\nThe CFI was created by the Government of Canada through the Budget Implementation Act 1997, Bill C-93, to \"help build and sustain a research landscape in Canada that will attract and retain the world's top talent, train the next generation of researchers, support private-sector innovation and create high-quality jobs that strengthen Canada's position in today's knowledge economy\".\n\nThe infrastructure funded by the CFI includes the equipment, laboratories, databases, specimens, scientific collections, computer hardware and software, communications linkages and buildings necessary to conduct research.\n\nThe CFI has established a merit-review process that relies on experts from across Canada and around the world to ensure that only the best projects receive funding. CFI funding is awarded to institutions, not individual researchers, and all funding proposals must support an institution's strategic research plan. Eligible Canadian institutions apply to the CFI through a suite of funds, and all applications are assessed using three broad criteria: quality of the research and its need for infrastructure, contribution to strengthening the capacity for innovation and potential benefits of the research to Canada.\n\nThe CFI funds up to 40 percent of a project's research infrastructure costs. This funding is then leveraged to attract the remaining investment from partners in the public, private and non-profit sectors.\n\nThe CFI was established as an independent, non-governmental organization with a Board of Directors, which meets three to four times a year. The Board of Directors reports to Members—a higher governing body similar to a company's shareholders but representing the Canadian public. Members are nominated and appointed for a five-year term. An annual public meeting is held each year.\n\nCFI has been criticized for being redundant and part of a \"convoluted\" federal funding apparatus.\n\n\n"}
{"id": "54261579", "url": "https://en.wikipedia.org/wiki?curid=54261579", "title": "Cecilie French", "text": "Cecilie French\n\nCecilie Mary French (23 October 1915 – 6 August 1962) was a British chemist and lecturer specialising in magnetochemistry.\n\nFrench was born in London on 23 October 1915 and educated at Walthamsaw Girls County School. She obtained her BSc in chemistry from University College London in 1937.\n\nFrom 1938 to 1939 French was a demonstrator in chemistry at UCL. As such, she was one of the very few women appointed to the Chemistry Department at University College London before the Second World War. She did research with Christopher Ingold and Cecil Wilson, which helped her achieve her PhD in 1940. This same year, she also she undertook a position in Imperial Chemical Industries (ICI) as a Research Chemist.\n\nIn late 1940, she accepted a new job as a Demonstrator and Assistant Lecturer in Physical and Organic Chemistry at Bedford University. However Bedford had been evacuated to Cambridge University to avoid the heavy bombing that was being faced in London.\n\nDespite being given many teaching duties, Cecilie started her research in magnetochemistry with James Spencer. As well as Bedford College being evacuated, Queen Mary's College (QMC) was also evacuated to Cambridge which began French's involvement with QMC. After the bombings in London have died down the evacuees returned to Bedford and QMC to London in 1944. At this point she was offered a position as Lecturer in Inorganic and Physical Chemistry at QMC.\n\nShe stayed with QMC for the rest of her life during which she was promoted to a Senior Lecturer in 1961. Despite heavy teaching load, French maintained a very impressive research output in magnetochemistry the electrochemistry of nonaqeuous solvents and the synthesising of novel boron compounds. She collaborated on research with Violet Trew at Bedford College until the arrival of the Gouy balance at QMC whereupon she began collaborating with D. Harrison there instead. During her time at QMC, French's tutelage and supervision of twenty-three research graduates also resulted in almost fifty publications.\n\nFrench travelled widely over her years in the Far East and in the United States, spending a year in Pennsylvania State University. When she turned forty, her health began to deteriorate, however: \"With great courage and determination, as well as cheerfulness, she continued her work as usual despite partial blindness and increasing disability. At this stage, her long-standing practice of lecturing without notes was invaluable to her. She learned Braille and characteristically gave her services to the National Institute for the Blind in advising them about the transliteration of scientific and mathematic symbols.\"\n\nFrench was invited to return to Pennsylvania State in 1962 to give the honourary Marie Curie lecture but ill health prevented her from making it. French died 6 August 1962 in Epsom, Surrey. Her obituarist, Dorothea Grove recorded that:\"Dr. French was an outstanding university teacher and will be remembered with gratitude by generations of undergraduates in whose welfare she took a keen personal interest. She proved her ability during the difficult post-war years when classes were swollen with ex-servicemen and, for a number of years, she was also responsible for teaching the first M.B. students of the London Hospital.\"\n"}
{"id": "54295008", "url": "https://en.wikipedia.org/wiki?curid=54295008", "title": "Center for Climate and Life", "text": "Center for Climate and Life\n\nThe Center for Climate and Life is a multidisciplinary climate science research initiative based at Lamont-Doherty Earth Observatory (LDEO), a research unit of Columbia University. Center research focuses on how climate change affects access to basic resources such as food, water, shelter and energy. The Center's founder and director is Peter B. de Menocal, a paleoclimatologist and Columbia University Dean of Science in the Faculty of Arts and Sciences.\n\n“The Center for Climate and Life empowers the most innovative thinkers to generate new knowledge to understand how climate change impacts life’s essential resources — food, water, and shelter — and to develop sustainable energy solutions.”\n\nColumbia University provided the Center for Climate and Life an initial budget $3.1 million for its first five years of operation.\n\nThe Center aims to engage corporate philanthropists to support its research initiatives. Through private funding and partnerships, it aims to build an endowment, which will be used to distribute annual grants to Center scientists.\n\nIn April 2016, the Center for Climate and Life partnered with the World Surf League (WSL). The philanthropic arm of the organization, WSL PURE, contributed $1.5 million to the Center to fund five research projects on the topics of ocean health and ecosystems, ocean acidification, sea-level rise, and the role the oceans play in climate change.\n\nCenter for Climate and Life initiatives focus on five areas:\n\n\nResearch by Center for Climate and Life scientists is focused on understanding how climate change will affect people and the basic resources and ecosystems that sustain them. Center activities address the widening “climate innovation gap” between the increasing need for knowledge and solutions, and declining federal support for climate and solutions research.\n\nThe Center works with public and private sector partners to help stakeholders understand how climate-related impacts on essential resources will affect their bottom line and thereby guide rational business and policy decisions.\n\nThe Center supports Climate and Life Fellows, who lead research projects on topics central to its mission. In 2016, the Center administered its first two grants to early career scientists: hydrologist Michael Puma of Columbia University's Center for Climate Systems Research and NASA’s Goddard Institute for Space Studies received a $190,000 grant to study the impact of climate change on global food systems, and bioclimatologist A. Park Williams of LDEO received a grant of $180,000 for his research on historical drought and fire cycles.\n\nPeter de Menocal, the Center director, seeks to encourage climate action by talking about climate change in terms of how it will impact human sustainability.\n\n\n"}
{"id": "21422050", "url": "https://en.wikipedia.org/wiki?curid=21422050", "title": "Chapais (crater)", "text": "Chapais (crater)\n\nChapais is a crater on Mars, named after the community of Chapais, Quebec, Canada. Chapais Crater is located at 22.6° south latitude, and 20.6° west longitude. Its diameter is .\n\nIts name was adopted in 1976 by the International Astronomical Union's Working Group for Planetary System Nomenclature (IAU/WGPSN).\n"}
{"id": "417453", "url": "https://en.wikipedia.org/wiki?curid=417453", "title": "Chicago school (sociology)", "text": "Chicago school (sociology)\n\nIn sociology and later criminology, the Chicago school (sometimes described as the ecological school) was the first major body of works emerging during the 1920s and 1930s specializing in urban sociology, and the research into the urban environment by combining theory and ethnographic fieldwork in Chicago, now applied elsewhere. While involving scholars at several Chicago area universities, the term is often used interchangeably to refer to the University of Chicago's sociology department. Following the Second World War, a \"second Chicago school\" arose whose members used symbolic interactionism combined with methods of field research (today often referred to as ethnography), to create a new body of work. \n\nThe major researchers in the first Chicago school included Nels Anderson, Ernest Burgess, Ruth Shonle Cavan, Edward Franklin Frazier, Everett Hughes, Roderick D. McKenzie, George Herbert Mead, Robert E. Park, Walter C. Reckless, Edwin Sutherland, W. I. Thomas, Frederic Thrasher, Louis Wirth, and Florian Znaniecki. The activist, social scientist, and Nobel Peace Prize winner Jane Addams also forged and maintained close ties with some of the members of the Chicago school of sociology.\n\nThe Chicago school is best known for its urban sociology and for the development of the symbolic interactionist approach, notably through the work of Herbert Blumer. It has focused on human behavior as shaped by social structures and physical environmental factors, rather than genetic and personal characteristics. Biologists and anthropologists had accepted the theory of evolution as demonstrating that animals adapt to their environments. As applied to humans who are considered responsible for their own destinies, members of the school believed that the natural environment, which the community inhabits, is a major factor in shaping human behavior, and that the city functions as a microcosm: \"In these great cities, where all the passions, all the energies of mankind are released, we are in a position to investigate the process of civilization, as it were, under a microscope.\"\n\nThe work of Frederic E. Clements (1916) was particularly influential. He proposed that a community of vegetation is a superorganism and that communities develop in a fixed pattern of successional stages from inception through to some single climax state or to a self-regulating state of equilibrium. By analogy, an individual is born, grows, matures, and dies, but the community which the individual inhabited continues to grow and exhibit properties which are greater than the sum of the properties of the parts.\n\nMembers of the school have concentrated on the city of Chicago as the object of their study, seeking evidence whether urbanization (Wirth: 1938) and increasing social mobility have been the causes of the contemporary social problems. Originally, Chicago was a clean slate, an empty physical environment. By 1860, Chicago was a small town with a population of 10,000. There was great growth after the fire of 1871. By 1910, the population exceeded two million. The rapidity of the increase was due to an influx of immigrants and it produced homelessness (Anderson: 1923), poor housing conditions, and bad working conditions based on low wages and long hours. But equally, Thomas and Znaniecki (1918) stress that the sudden freedom of immigrants released from the controls of Europe to the unrestrained competition of the new city was a dynamic for growth. See also the broken windows thesis.\n\nFor Thomas, the groups themselves had to reinscribe and reconstruct themselves to prosper. Burgess studied the history of development and concluded that the city had not grown at the edges. Although the presence of Lake Michigan prevented the complete encirclement, he postulated that all major cities would be formed by radial expansion from the center in concentric rings which he described as zones, i.e. the business area in the center, the slum area (called the zone in transition and studied by Wirth: 1928, Zorbaugh: 1929, and Suttles: 1968) around the central area, the zone of workingmen's homes farther out, the residential area beyond this zone, and then the bungalow section and the commuter's zone on the periphery. Under the influence of Albion Small, the research at the school mined the mass of official data including census reports, housing/welfare records and crime figures, and related the data spatially to different geographical areas of the city. Shaw and McKay created maps:\nThomas also developed techniques of self-reporting life histories to provide subjective balance to the analysis. Park, Burgess, and McKenzie are\ncredited with institutionalizing, if not establishing, sociology as a science. They are also criticized for their overly empiricist and idealized approach to the study of society but, in the inter-war years, their attitudes and prejudices were normative. Three broad themes characterized this dynamic period of Chicago studies: \n\nThe school is perhaps best known for the subcultural theories of Thrasher, Frazier, and Sutherland, and for applying the principles of ecology to develop the social disorganization theory which refers to consequences of the failure of:\nThomas defined social disorganization as \"the inability of a neighborhood to solve its problems together\" which suggested a level of social pathology and personal disorganization, so the term, \"differential social organization\" was preferred by many, and may have been the source of Sutherland's (1947) differential association theory. The researchers have provided a clear analysis that the city is a place where life is superficial, where people are anonymous, where relationships are transitory and friendship and family bonds are weak. They have observed the weakening of primary social relationships and relate this to a process of social disorganization (comparison with the concept of \"anomie\" and the strain theories is instructive).\n\nVasishth and Sloane argue that while it is tempting to draw analogies between organisms in nature and the human condition, the problem lies in reductionism, i.e. that the science of biology is oversimplified into rules that are then applied mechanically to explain the growth and dynamics of human communities. The most fundamental difficulties are definitional. If a community is a group of individuals who inhabit the same place, is the community merely the sum of individuals and their activities, or is it something more than an aggregation of individuals? This is critical in planning research into group interactions. Will research be effective if it focuses on the individuals composing a group, or is the community itself a proper subject of research independently of the individuals who compose it? If the former, then data on individuals will explain the community, but if the community either directly or indirectly affects the behavior of its members, then research must consider the patterns and processes of community as distinct from patterns and processes in populations of individuals. But this requires a definition and distinction between \"pattern\" and \"process\". The structures, forms, and patterns are relatively easy to observe and measure, but they are nothing more than evidence of underlying processes and functions which are the real constitutive forces in nature and society. The Chicago school wanted to develop tools by which to research and then change society by directing urban planning and social intervention agencies. It recognized that urban expansion was not haphazard but quite strongly controlled by community-level forces such as land values, zoning ordinances, landscape features, circulation corridors, and historical contingency. This was characterized as ecological because the external factors were neither chance nor intended, but rather arose from the natural forces in the environment which limit the adaptive spatial and temporal relationships between individuals. The school sought to derive patterns from a study of processes, rather than to ascribe processes to observed patterns and the patterns they saw emerge, are strongly reminiscent of Clements' ideas of community development.\n\nThe Chicago Area Project was a practical attempt by sociologists to apply their theories in a city laboratory. Subsequent research showed that the youth athletic leagues, recreation programs, and summer camp worked best along with urban planning and alternatives to incarceration as crime control policy. Such programs are non-entrepreneurial and non-self-sustaining, and they fail when local or central government does not make a sustained financial commitment to them. Although with hindsight, the school's attempts to map crime may have produced some distortions, the work was valuable in that it moved away from a study of pattern and place toward a study of function and scale. To that extent, this was work of high quality that represented the best science available to the researchers at the time.\n\nThe Social Disorganization Theory itself was a landmark and, since it focuses on the absence or breakdown of social control mechanisms, there are obvious links with social control theory. In \"Causes of Delinquency\" (1969) Travis Hirschi argued that variations in delinquent behavior among youth could be explained by variations in the dimensions of the social bond, namely attachment to others, commitments to conventional goals, acceptance of conventional moral standards or beliefs, and involvement in conventional activities. The greater the social bonds between a youth and society, the lower the odds of involvement in delinquency. When social bonds to conventional role models, values and institutions are aggregated for youth in a particular setting, they measure much the same phenomena as captured by concepts such as network ties or social integration. But the fact that these theories focus on the absence of control or the barriers to progress, means that they are ignoring the societal pressures and cultural values that drive the system Merton identified in the Strain Theory or the motivational forces Cohen proposed were generating crime and delinquency. More modern theorists like Empey (1967) argue that the system of values, norms and beliefs can be disorganized in the sense that there are conflicts among values, norms and beliefs within a widely shared, dominant culture. While condemning crime in general, law-abiding citizens may nevertheless respect and admire the criminal who takes risks and successfully engages in exciting, dangerous activities. The depiction of a society as a collection of socially differentiated groups with distinct subcultural perspectives that lead some of these groups into conflict with the law is another form of cultural disorganization, is typically called cultural conflict.\n\nModern versions of the theory sometimes use different terminology to refer to the same ecological causal processes. For example, Crutchfield, Geerken and Gove (1982: 467-482) hypothesize that the social integration of communities is inhibited by population turnover and report supporting evidence in the explanation of variation in crime rates among cities. The greater the mobility of the population in a city, the higher the crime rates. These arguments are identical to those proposed by social disorganization theorists and the evidence in support of it is as indirect as the evidence cited by social disorganization theorists. But, by referring to social integration rather than disintegration, this research has not generated the same degree of criticism as social disorganization theory.\n\nFor a comprehensive history of the Chicago school, see Martin Bulmer (1984) and Lester Kurtz (1984).\n\n"}
{"id": "30003033", "url": "https://en.wikipedia.org/wiki?curid=30003033", "title": "Continuous Progress Mathematics", "text": "Continuous Progress Mathematics\n\nThe Continuous Progress Mathematics (CPM) system of multi-age classrooms organizes students into classes based on their immediate instructional level. Implemented at Peach Plains Elementary School in Grand Haven, Michigan, in the fall of 1992, instructional groups called terminals are composed of students who have mastered the critical prerequisites of some 250 objectives within 28 areas of mathematical knowledge. Individual learner mastery of the non-linear hierarchy of mathematical objectives is carefully monitored by teachers and curriculum management software allowing all students to advance through the same curriculum, albeit at different times.\n\nStephen E. Rubin, Ph.D, of Westport, Connecticut, originally designed the hierarchy of mathematical objectives for Center School in New Canaan, Connecticut. Grand Haven Area Public Schools adapted the hierarchy and contracted Tom Orzechowski, systems engineer, to develop a networked DOS version of the management software in the summer of 1992. Kirby Chittenden provided design consultation for the graphical user interface and the software functionality. Additional design support was provided by Melinda Brink, principal of Peach Plains Elementary School, and the school’s math coordinator, Jean Sharp.\n\nCPM was designed around the developmental philosophy that learning is more a function of time rather than ability. Within this design, elementary students leave their homerooms each afternoon to participate in a multi-age instructional group called a terminal. Students flow in and out of these terminals as they are able to demonstrate mastery. \nThese instructional groups are organized according to two criteria: \n\nClasses in various mathematical strands such subtraction (S), decimals (L), statistic and probability (SP) are open and closed as student eligibility increases or decreases respectively. This fluency allows for all students to move through the hierarchy at developmentally appropriate times.\n\nTeachers often provide a focused whole group instruction based upon one of some 250 objectives available to students. In addition, students may participate in independent or small group activities using classroom manipulatives, learning center activities, cooperative learning groups, peer tutoring, and computer courseware.\nInstructors have full latitude to decide what will be used in the way of resources to bring each student to the point of mastery. Not all students achieve success on the same day and in the same way. Level of mastery, rather than rate, is emphasized.\nA criterion referenced test allows the student to demonstrate mastery of the terminal objective. The management software then assigns the student to an open eligible class the next day. The software also determines which classes need to open and close based upon student eligibility.\n\nEarly management software, designed to operate on the Apple II platform, did not take full advantage of developments in relational database management. In addition, advancements in object-oriented program language allowed access to information from various sources in a substantially more graphic form. The new management software developed by Tom Orzechowski integrated eight independent databases with the object-oriented characteristics of Borland’s Paradox (database) software development engine.\nAn integral part of Orzechowski’s development process is that of a student’s Locale, or their whereabouts in the hierarchy of mathematical objectives. The Locale (see diagram) provided data about each child’s skill set and functioned as a decision making tool and gateway for enrolling students in appropriate classes each day.\n"}
{"id": "22400988", "url": "https://en.wikipedia.org/wiki?curid=22400988", "title": "Cross-battery assessment", "text": "Cross-battery assessment\n\nCross-battery assessment refers to the process by which psychologists use information from multiple test batteries (i.e., various IQ tests) to help guide diagnostic decisions and to gain a fuller picture of an individual’s cognitive abilities than can be ascertained through the use of single-battery assessments. The cross-battery approach (XBA) was first introduced in the late 1990s by Dawn Flanagan, Samuel Ortiz and Kevin McGrew. It offers practitioners the means to make systematic, valid and up-to-date interpretations of intelligence batteries and to augment them with other tests in a way that is consistent with the empirically supported Cattell–Horn–Carroll (CHC) theory of cognitive abilities.\n\nThe XBA approach is a time efficient method to reliably measure a wider (or more in-depth but selective range) of cognitive abilities/processes than any single intelligence battery can measure. It is based on three foundational sources of information (i.e., practice, research and test development) that provide the knowledge necessary to organise theory-driven, comprehensive, reliable, and valid assessments of cognitive abilities.\n\nR. W. Woodcock conducted a joint factor analysis suggesting the necessity of cross-battery assessments to measure a broad range of cognitive abilities rather than a single intellectual battery. For instance, he found that of the major intellectual batteries utilized prior to 2000, most failed to measure three or more broad CHC abilities that were considered essential in understanding and predicting school achievement. This provided the impetus for developing the XBA approach. \nThe XBA approach also helps facilitate communication among professionals, which guards against misinterpretation. The XBA approach offers practitioners a psychometrically defensible way of identifying normative strengths and weaknesses in cognitive abilities.\n\nThe XBA approach helped to promote a greater understanding between cognitive abilities and important outcome criteria. Furthermore, improving the validity of CHC ability measures will further elucidate the relations between CHC cognitive abilities and different outcomes, such as achievement and occupational outcomes.\n\nTest authors have utilized CHC theory and XBA CHC test classifications as a blueprint for test development (WJ III, SB5, KABC-II, and DAS-II etc.). Despite the fact that cognitive abilities tests demonstrate a greater coverage of CHC broad cognitive abilities now as compared to previous years; there is still a need to use XBA approach for assessment.\n\nIt is recommended that practitioners adhere to several guiding principles in order to ensure that XBA procedures are psychometrically and theoretically sound. First, one should select an intelligence battery that best addresses referral concerns. Second, subtests and clusters or composites from a single battery should be utilized whenever possible in order to best represent the broad CHC abilities (i.e., use actual norms whenever possible). Third, it is important to construct CHC broad and narrow ability clusters through acceptable methods, such as CHC theory driven factor analyses or expert consensus content-validity studies. Fourth, when two or more qualitatively different indicators of a broad abilities of interest are not assessed or available on the core battery, then one may supplement it for broad ability indicators from another battery. Finally, when crossing batteries, select tests that were developed and normed within a few years of one another. Sixth, in order to minimize the effect of spurious differences between test scores, select tests from the smallest number of batteries.\nUnderlining the importance of the above points of consideration is the fact that the overzealous attitude of few psychologists to use this XBA approach has led to several cases of its abuse resulting in wrong and misguiding results.\n\n\nSpecific learning disability (SLD) is the largest disability identified among school-aged children. According to Flanagan, Ortiz and Alfonso, in order to receive a diagnosis of SLD the following criteria must be met following these steps: a deficit in academic functioning is determined, academic difficulties are not due to secondary exclusionary factors (e.g., neurological issues, etc.), a deficit in cognitive ability is determined, exclusionary factors are reviewed again to determine that the academic and cognitive deficits are not due to secondary factors, underachievement is established, the academic deficits are shown to have a negative effect on daily life. Flanagan, Ortiz and Alfonso suggest \"seven deadly sins\" as a metaphor for understanding the misconceptions surrounding SLD evaluation that continue to undermine its reliability and validity.\n\nOne of the most common practices in SLD evaluations is when the scores are ipsatized. Ipsatized scores are scores that have been averaged and subtracted from the overall average in order to determine the degree of deviation from the average. This suggests that when scores deviate from the mean they are clinically important indicators of either relative weaknesses (lower) or relative strengths (higher). Thus, weaknesses are thought of as evidence of SLD. This approach only focuses on the identification of discrepancies that exist within the individual. The vast majority of people do not have flat cognitive profiles and instead show significant variability in their profile of cognitive ability scores. The assumption that people who have certain scores in one domain will show similar ability in all domains is erroneous. Instead of looking for discrepancies wherever they might be found, theory should guide comparison between different sub-tests.\n\nA lower score does not automatically gain clinical significance simply because the discrepancy has been determined to be real (statistically significant). Statistical significance only means that the difference between the two scores is not due to chance (i.e., that they are different from one another), that is, it does not mean that the difference between the two scores in the comparison is clinically meaningful or indicative of impairment.\n\nThe ability-achievement discrepancy has been regarded as important to definitions and diagnostic criteria of SLD that practitioners often resort to calculating every sub-test score obtained at an evaluation. Given the high number of discrepancies available to calculate, it would be surprising if at least one significant discrepancy was not found. The significant ability-achievement discrepancy should not be synonymous with nor a necessary condition for a SLD diagnosis.\n\nThis ability-achievement discrepancy was likely fostered by the notion that IQ and other global ability composites are near-perfect predictors of an individual's academic achievement. For instance, scores of general ability, like the FSIQ, only account for about 35 to 50% of total achievement variance, which leaves about 50 to 65% of the variance unexplained. Thus, practitioners must recognize that there are other important factors that explain significant variance in achievement and global ability.\n\nIn evaluating SLD, practitioners may not always be privy to or able to implement procedures that are based on modern theory and research. Practitioners often omit contemporary psychometric theory and current research on SLD that aid in determining identification and diagnosis of SLD.\n\nDiagnostic decisions are often based on the results from either a single sub-test score or scores used to screen individuals. The reliance on these single scores may not be suitable for the purpose of diagnosis or high-stakes decision making. For instance, one of the fundamental properties of psychometrics is that a single sub-test can't be considered a reliable indicator by itself of the construct it is intended to measure. One sub-test is not sufficient to indicate the presence of an SLD or other impairment.\n\nAptitude and ability are two concepts that are often mistakenly confused. It is important to differentiate between the two given the shift in understanding SLD which is based on the difference between ability and aptitude. When evaluating SLD, looking at aptitude is important because those abilities are associated with long-term academic outcomes.\n\n This handbook for practitioners includes chapters by John D. Wasserman, Randy W. Kamphaus, Anne Pierce Winsor, Ellen W. Rowe, Sangwon Kim, John L. Horn, Nayena Blankson, W. Joel Schneider, Kevin S. McGrew, Jie-Qi Chen, Howard Gardner, Robert J. Sternberg, Jack A. Naglieri, J. P. Das, Sam Goldstein, Lisa Whipple Drozdick, Dustin Wahlstrom, Jianjun Zhu, Lawrence G. Weiss, Dustin Wahlstrom, Kristina C. Breaux, Jianjun Zhu, Lawrence G. Weiss, Gale H. Roid, Mark Pomplun, Jennie Kaufman Singer, Elizabeth O. Lichtenberger, James C. Kaufman, Alan S. Kaufman, Nadeen L. Kaufman, Fredrick A. Schrank, Barbara J. Wendling, Colin D. Elliott, R. Steve McCallum, Bruce A. Bracken, Jack A. Naglieri, Tulio M. Otero, Cecil R. Reynolds, Randy W. Kamphaus, Tara C. Raines, Robb N. Matthews, Cynthia A. Riccio, John L. Davis, Jack A. Naglieri, Tulio M. Otero, Dawn P. Flanagan, Vincent C. Alfonso, Samuel O. Ortiz, Catherine A. Fiorello, James B. Hale, Kirby L. Wycoff, Randy G. Floyd and John H. Kranzler, Samuel O. Ortiz, Salvador Hector Ochoa, Agnieszka M. Dynda, Nancy Mather, Barbara J. Wendling, Laurie Ford, Michelle L. Kozey, Juliana Negreiros, David E. McIntosh, Felicia A. Dixon, Eric E. Pierson, Vincent C. Alfonso, Jennifer T. Mascolo, Marlene Sotelo-Dynega, Laura Grofer Klinger, Sarah E. O’Kelly, Joanna L. Mussey, Sam Goldstein, Melissa DeVries, James B. Hale, Megan Yim, Andrea N. Schneider, Gabrielle Wilcox, Julie N. Henzel, Shauna G. Dixon, Scott L. Decker, Julia A. Englund, Alycia M. Roberts, Kathleen Armstrong, Jason Hangauer, Joshua Nadeau, Jeffery P. Braden, Bradley C. Niebling, Timothy Z. Keith, Matthew R. Reynolds, Daniel C. Miller, Denise E. Maricle, Denise E. Maricle, Erin Avirett, Rachel Brown-Chidsey, Kristina J. Andren, George McCloskey, James Whitaker, Ryan Murphy, Jane Rogers, and John B. Carroll. \n"}
{"id": "2690471", "url": "https://en.wikipedia.org/wiki?curid=2690471", "title": "Data farming", "text": "Data farming\n\nData farming is the process of using designed computational experiments to “grow” data, which can then be analyzed using statistical and visualization techniques to obtain insight into complex systems. These methods can be applied to any computational model.\n\nData farming differs from Data mining, as the following metaphors indicate: \n\nMiners seek valuable nuggets of ore buried in the earth, but have no control over what is out there or how hard it is to extract the nuggets from their surroundings. ... Similarly, data miners seek to uncover valuable nuggets of information buried within massive amounts of data. Data-mining techniques use statistical and graphical measures to try to identify interesting correlations or clusters in the data set.\n\nFarmers cultivate the land to maximize their yield. They manipulate the environment to their advantage using irrigation, pest control, crop rotation, fertilizer, and more. Small-scale designed experiments let them determine whether these treatments are effective. Similarly, data farmers manipulate simulation models to their advantage, using large-scale designed experimentation to grow data from their models in a manner that easily lets them extract useful information. ...the results can reveal root cause-and-effect relationships between the model input factors and the model responses, in addition to rich graphical and statistical views of these relationships.\nA NATO modeling and simulation task group has documented the data farming process in the Final Report of MSG-088.\nHere, data farming uses collaborative processes in combining rapid scenario prototyping, simulation modeling, design of experiments, high performance computing, and analysis and visualization in an iterative loop-of-loops.\n\nThe science of Design of Experiments (DOE) has been around for over a century, pioneered by R.A. Fisher for agricultural studies. Many of the classic experiment designs can be used in simulation studies. However, computational experiments have far fewer restrictions than do real-world experiments, in terms of costs, number of factors, time required, ability to replicate, ability to automate, etc. Consequently, a framework specifically oriented toward large-scale simulation experiments is warranted.\n\nPeople have been conducting computational experiments for as long as computers have been around. The term “data farming” is more recent, coined in 1998 in conjunction with the Marine Corp’s Project Albert, in which small agent-based distillation models (a type of stochastic simulation) were created to capture specific military challenges. These models were run thousands or millions of times at the Maui High Performance Computer Center and other facilities. Project Albert analysts would work with the military subject matter experts to refine the models and interpret the results.\n\nInitially, the use of brute-force full factorial (gridded) designs meant that the simulations needed to run very quickly and the studies required high-performance computing. Even so, only a small number of factors (at a limited number of levels) could be investigated, due to the curse of dimensionality.\n\nThe SEED Center for Data Farming at the Naval Postgraduate School also worked closely with Project Albert in model generation, output analysis, and the creation of new experimental designs to better leverage the computing capabilities at Maui and other facilities. Recent breakthroughs in designs specifically developed for data farming can be found in \n,\namong others.\n\nA series of international data farming workshops have been held since 1998 by the SEED Center for Data Farming. International Data Farming Workshop 1 occurred in 1991, and since then 16 more workshops have taken place. The workshops have seen a diverse array of representation from participating countries, such as Canada, Singapore, Mexico, Turkey, and the United States. \n\nThe International Data Farming Workshops operate through collaboration between various teams of experts. The most recent workshop held in 2008 saw over 100 teams participating. The teams of data farmers are assigned a specific area of study, such as robotics, homeland security, and disaster relief. Different forms of data farming are experimented with and utilized by each group, such as the Pythagoras ABM, the Logistics Battle Command model, and the agent-based sensor effector model (ABSEM).\n\n"}
{"id": "18255048", "url": "https://en.wikipedia.org/wiki?curid=18255048", "title": "Dundee Science Centre", "text": "Dundee Science Centre\n\nDundee Science Centre (formerly known as Sensation) is a science centre located in Dundee, Scotland, and a part of the Scottish Science Centres Network. It is a registered non-profit organisation that is funded by the public and donations from local corporate sponsors.\n\nThe interactive exhibits focus mainly on the life sciences, and in particular on the senses. There is also a focus on robotics, and a practical exploration of science learning. Daily shows include: Keyhole Surgery, Science on the Spot, MindBall, Looking for Life Planetarium show and Solar System show.\n\nDundee Science Centre is also a corporate venue and a HMIE-inspected resource for science learning and public engagement. The centre hosts school groups, uniformed groups and also birthday parties.\n\nThere is an in-house cafe (Infusion) and a gift shop, stocking many science and educational products.\n\nDundee Science Centre is open daily from 10am to 5pm.\n\nAs one of the Millennium Commission projects, it opened in July 2000 at the cost of around £5 million.\n\n"}
{"id": "3884618", "url": "https://en.wikipedia.org/wiki?curid=3884618", "title": "Eicosameric", "text": "Eicosameric\n\nEicosameric refers to biological polymers or multimers having exactly twenty 'monomers' (or 20 repeating components).\n\nProtein complexes having exactly 20 subunits are referred to as eicosameric (or sometimes 20-Meric).\n\nExamples of eicosameric protein complexes include;\n\n"}
{"id": "16789821", "url": "https://en.wikipedia.org/wiki?curid=16789821", "title": "El (crater)", "text": "El (crater)\n\nEl is a crater on Ganymede. It has a small \"pit\" in its center. Craters with such a \"central pit\" are common across Ganymede and are especially intriguing since they may reveal secrets about the structure of the satellite's shallow subsurface.\n"}
{"id": "9712670", "url": "https://en.wikipedia.org/wiki?curid=9712670", "title": "Eurocities", "text": "Eurocities\n\nEUROCITIES is the network of major European cities, founded in 1986 by the mayors of Barcelona, Birmingham, Frankfurt, Lyon, Milan, and Rotterdam. The 1989 Barcelona conference agreed that a key task was to put the issue of cities and their economic, political and social development onto the European agenda. Indeed, the founders suggested that major European cities could benefit from forming such an association. They intended to form a political platform to act as a channel for communicating with the European institutions and to become recognised as a significant partner in the European community and each member state.\n\nToday, EUROCITIES brings together the local governments of over 130 of Europe’s major cities from 35 different countries, representing the interests and needs of 130 million citizens.\n\nThe EUROCITIES secretariat is based in Brussels, Belgium. The Brussels office carries out policy, projects, human resources, finance, administration and communications work. \n\nEUROCITIES structures its work around five focus areas that to a large extent align with the EU’s strategic priorities:\n\nThese areas of work are described in greater detail in the EUROCITIES Strategic Framework 2014-2020: Towards an EU urban agenda for cities.\n\nMembership of Eurocities is open to any European city with a population of 250,000 or more. Cities within the European Union become full members, and other European cities become associate members. Local authorities of smaller cities, and organisations not eligible to become full or associate members, can become associated partners. Companies and businesses are allowed to become associated business partners.\n"}
{"id": "57651246", "url": "https://en.wikipedia.org/wiki?curid=57651246", "title": "Explorer 31", "text": "Explorer 31\n\nExplorer 31 (also called DME-A) was an American satellite launched as part of the Explorers program of NASA. Explorer 29 was launched on 29 November 1965 from Vandenberg Air Force Base, California, United States, with Thor Agena rocket. Explorer 31 was released along with the Canadian satellite Alouette 2.\n\nExplorer 31 was a small ionospheric observatory instrumented to make direct measurements of selected ionospheric parameters at the spacecraft. Since the spacecraft had no tape recorder, data could be observed at the spacecraft only when the spacecraft was in sight of the telemetry station and when commanded on. Experiments were operated either simultaneously or sequentially, as desired. The satellite was spin-stabilized with the spin axis perpendicular to the orbit plane. The spin rate and spin axis were controlled by an onboard magnetic torquing system. The attitude and spin rate information were observed by a sun sensor and a three-axis magnetometer.\n\nSatellite performance was satisfactory except for a partial power failure in May 1966, which reduced data acquisition time to about half the nominal amount. Some difficulties were encountered in obtaining attitude information that was necessary for the reduction of the experiment observations. On July 1, 1969, the satellite data observations were terminated with five of the seven experiments operating. Responsibility for standby monitoring of the satellite was given to the ESSA telemetry station at Boulder, Colorado, on July 8, 1969. During this standby operation, experiment data were collected only once on October 1, 1969, for 9 min from the electrostatic probe for use in studying a red arc event. On January 15, 1971, no response was received from a variety of satellite commands, and the satellite was abandoned.\n\n"}
{"id": "57288584", "url": "https://en.wikipedia.org/wiki?curid=57288584", "title": "Finlay Lorimer Kitchin", "text": "Finlay Lorimer Kitchin\n\nFinlay Lorimer Kitchin FGS, FRS (3 December 1870, Whitehaven, Cumbria, UK – 20 January 1934, London) was a British geologist and palaeontologist.\n\nKitchin was educated at St. Bees School and then at St. John’s College, Cambridge, where he received B.A. 1893, M.A. 1898, and Sc.D. 1924. At Cambridge he studied geology and palaeontology from 1890 to 1894 and then went to the University of Munich, where he studied paleontology under Karl Alfred von Zittel and received a doctoral degree (Promotion) in 1897. His doctoral dissertation is a study of Jurassic fossils discovered in the Cutch State and sent for examination by the Geological Survey of India. After returning to England, he worked unofficially for a short time in the British Museum of Natural History. He worked for the British Geological Survey from 1890 to 1905 as assistant palaeontologist and from 1905 to 1934 as palaeontologist. He was promoted in 1905 as the successor of E. T. Newton upon the latter's retirement.\n\nKitchin was elected in 1894 a Fellow of the Geographical Society and in 1929 a Fellow of the Royal Society. He was awarded the Lyell Medal shortly before his death in 1934.\n\nHe was an accomplished musician and an authority on the construction of pipe-organs, as well as an authority on developments in locomotive design.\n"}
{"id": "40749885", "url": "https://en.wikipedia.org/wiki?curid=40749885", "title": "Fuzzy cold dark matter", "text": "Fuzzy cold dark matter\n\nFuzzy cold dark matter is a hypothetical form of cold dark matter proposed to solve the cuspy halo problem. It would consist of extremely light scalar particles with masses on the order of formula_1 eV. Fuzzy cold dark matter halos in dwarf galaxies would manifest wave behavior on astrophysical scales, and the cusps would be avoided through the Heisenberg uncertainty principle.\n\nFuzzy cold dark matter is a limit of scalar field dark matter without self-interaction.\n"}
{"id": "45694613", "url": "https://en.wikipedia.org/wiki?curid=45694613", "title": "Gellner's theory of nationalism", "text": "Gellner's theory of nationalism\n\nGellner's theory of nationalism was developed by Ernest Gellner over a number of publications from around the early 1960s to his 1995 death. Gellner discussed nationalism in a number of works, starting with \"Thought and Change\" (1964), and he most notably developed it in \"Nations and Nationalism\" (1983).\n\nGellner defined nationalism as \"primarily a political principle which holds that the political and the national unit should be congruent\" and as the general imposition of a high culture on society, where previously low cultures had taken up the lives of the majority, and in some cases the totality, of the population. It means the general diffusion of a school-mediated, academy supervised idiom, codified for the requirements of a reasonably precise bureaucratic and technological communication. It is the establishment of an anonymous impersonal society, with mutually sustainable atomised individuals, held together above all by a shared culture of this kind, in place of the previous complex structure of local groups, sustained by folk cultures reproduced locally and idiosyncratically by the micro-groups themselves.\n\nGellner analyzed nationalism by a historical perspective. He saw the history of humanity culminating in the discovery of modernity, nationalism being a key functional element. Modernity, by changes in political and economical system, is tied to the popularization of education, which, in turn, is tied to the unification of language. However, as modernization spread around the world, it did so slowly, and in numerous places, cultural elites were able to resist cultural assimilation and defend their own culture and language successfully.\n\nFor Gellner, nationalism was a sociological condition and a likely but not guaranteed (he noted exceptions in multilingual states like Switzerland, Belgium and Canada) result of modernisation, the transition from agrarian to industrial society. His theory focused on the political and cultural aspects of that transition. In particular, he focused on the unifying and culturally homogenising roles of the educational systems, national labour markets and improved communication and mobility in the context of urbanisation. He thus argued that nationalism was highly compatible with industrialisation and served the purpose of replacing the ideological void left by both the disappearance of the prior agrarian society culture and the political and economical system of feudalism, which it legitimised.\n\nThomas Hylland Eriksen lists these as \"some of the central features of nationalism\" in Gellner's theory:\n\nGellner also provided a typology of \"nationalism-inducing and nationalism-thwarting situations\".\n\nGellner criticised a number of other theoretical explanations of nationalism, including the \"naturality theory\", which states that it is \"natural, self-evident and self-generating\" and a basic quality of human being, and a neutral or a positive quality; its dark version, the \"Dark Gods theory\", which sees nationalism as an inevitable expression of basic human atavistic, irrational passions; Elie Kedourie's idealist argument that it was an accidental development, an intellectual error of disseminating unhelpful ideas, and not related to industrialisation and the Marxist theory in which nations appropriated the leading role of social classes.\n\nGellner is considered one of the leading theoreticians on nationalism. Eriksen notes that \"nobody contests Ernest Gellner's central place in the research on nationalism over the last few decades\". O'Leary refers to the theory as \"the best-known modernist explanatory theory of nationalism\".\n\nGellner's theory has been subject to various criticisms:\n"}
{"id": "41529802", "url": "https://en.wikipedia.org/wiki?curid=41529802", "title": "Giant Void", "text": "Giant Void\n\nThe Giant Void (also known as the Giant Void in NGH, Canes Venatici Supervoid, and AR-Lp 36) is an extremely large region of space of underdensity of galaxies within the constellation Canes Venatici. It is the second largest confirmed void to date, with an estimated diameter of 300 to 400 Mpc (1 to 1.3 billion light years) and is approximately 1.5 billion light years away (z = 0.116). It was discovered in 1988, and was the largest void in the Northern Galactic Hemisphere, and possibly the second largest ever detected. Even the hypothesized \"Eridanus Supervoid\" corresponding to the location of the WMAP cold spot is dwarfed by this void, although the Giant Void does not correspond any significant cooling to the cosmic microwave background. Although a vast void, inside it are 17 galaxy clusters, concentrated in a spherical shaped region 50 Mpc in diameter. Studies of the motion of the clusters show that they have no interaction to each other, meaning the density of the clusters is very low resulting in weak gravitational interaction. The void's location in the sky is close to the Boötes void. \n\n"}
{"id": "28723409", "url": "https://en.wikipedia.org/wiki?curid=28723409", "title": "Global R&amp;D management", "text": "Global R&amp;D management\n\nGlobal R&D management is the discipline of designing and leading R&D processes globally, i.e. across borders, in multi-cultural and multi-lingual settings, and cutting across multiple time zones. In addition, managing R&D organizations, and ensuring smooth transfer of new know-how and technology to other groups or departments involved in innovation. Global R&D teams trade the benefits of collocation and centralization with the benefits of local responsiveness, local insight and global synergy.\n\nGlobal R&D management today benefits the automotive, aerospace, medical, software and video game industries, among many others. In order for companies to achieve their objectives\nof a timely and cost-effective product development it often requires seeking resources outside the company confines. This can range from outsourcing non-core activities to co-development and collaboration.\n\nIndustrial R&D has globalized since the 1950s, when e.g. the US company IBM started a research center in Europe, but it was not before the 1990s that global R&D reached noteworthy proportions in any firm. Access to local technology, access to local markets, and merger and acquisitions of parent companies led to a dispersion of R&D activities worldwide. Various estimates now put the average internationalization of R&D at 10% (for e.g. Japanese firms) to more than 50% (e.g. for many European and some US firms). Great variances among companies even within the same industry exist, indicating that there is no single normative strategy to determine the dispersion or globalization of R&D.\n\nGlobal R&D Management requires that engineers, scientists and managers be trained for competence in cross-cultural communication, a skill not often taught in engineering schools.\nPractical Books on the topic include as Cross-Cultural Dialogues: 74 Brief Encounters with Cultural Difference” by Craig Storti. Practical workshops on communication with India are offered by Amritt, Inc and on China by Ionis International.\n\nGlobal R&D management is concerned with the following sub-topics:\n\nLocations considered to be excellent places to find these resources are India and China because they are also the locales where many new products are targeted. Companies such as General Electric have put up vast in-house research and development centers in emerging countries such as India. Establishing and running an offshore facility for R&D take a serious commitment of management bandwidth. Hundreds of American and European companies have found that this approach gives the best combination of security and long-term financial advantages.\n\n\n\n"}
{"id": "5924712", "url": "https://en.wikipedia.org/wiki?curid=5924712", "title": "Greenhouse Item", "text": "Greenhouse Item\n\nGreenhouse Item was an American nuclear test conducted on May 25, 1951, as part of Operation Greenhouse at the Pacific Proving Ground, specifically on the island of Engebi in the Eniwetok Atoll in the Central Pacific Ocean. This test explosion was the first test of a boosted fission weapon.\n\nIn this test deuterium-tritium (D-T) gas was injected into the enriched uranium core of a nuclear fission bomb. The extreme heat of the fissioning bomb produced thermonuclear fusion reactions within the D-T gas, but not enough of them to be considered a full nuclear fusion bomb. This fusion reaction released a large number of free neutrons, which greatly increased the efficiency of the nuclear fission reaction. The explosive yield of this bomb was 45.5 kilotons, about twice the yield of the unboosted bomb.\n\nThis bomb was known as the \"Booster\" in its development stages, a name for the mechanism coined by Edward Teller in September 1947. Planning for it had begun in the late 1940s. According to the researcher Chuck Hansen, it was mentioned in official U.S. Atomic Energy Commission documents as early as 1947. The main problems in development were making modifications to the fission core to accept the gas correctly without reducing its own efficiency. The 1951 test was primarily to test the nuclear principles involved, and to gain research data, and it was not considered a design for a weaponizable device. Even as late as 1954, no boosted weapon had entered into the nuclear-weapons stockpile, and the only use for the Greenhouse Item nuclear test had been for its research results.\n\nThe \"Booster\" device was detonated at 6:17 am on May 25, 1951, from a shot tower on the island of Engebi in the Enewetok Atoll, and its fusion fuel was injected by means of a cryogenic pump at the base of the tower.\n\n\n"}
{"id": "1170053", "url": "https://en.wikipedia.org/wiki?curid=1170053", "title": "Herschel (Martian crater)", "text": "Herschel (Martian crater)\n\nHerschel is a 304 kilometer Impact Basin in the Martian southern hemisphere, at 14.5°S, 130°E, located in the Mare Tyrrhenum region of Mars and is inside Terra Cimmeria. The crater is jointly named after the seventeenth/eighteenth century father and son astronomers William Herschel and John Herschel.\nNearby named craters include the much smaller Luqa to the southeast, also further south is the slightly smaller Müller, to the north is the slightly smaller Knobel and Robert Sharp craters and northeast are the Lasswitz-Wien crater pair.\n\nMars Global Surveyor spacecraft originally photographed fields of dark sand dunes within Herschel. Images from the NASA Mars Reconnaissance Orbiter showed that sand dunes on the floor of the Herschel crater are not stationary (as previously believed), but moved over time. Images from photos taken by the Orbiter's High Resolution Imaging Science Experiment (HiRISE) on March 3, 2007 and December 1, 2010 show clear shifting of dunes and ripples. Research published in Icarus stated that the dunes in Hershel Crater moved 0.8 m in a time span of 3.7 Earth-years. Also, it was determined that dune ripple moved 1.1 m in that time period.\n\n"}
{"id": "187721", "url": "https://en.wikipedia.org/wiki?curid=187721", "title": "Imprinting (psychology)", "text": "Imprinting (psychology)\n\nIn psychology and ethology, imprinting is any kind of phase-sensitive learning (learning occurring at a particular age or a particular life stage) that is rapid and apparently independent of the consequences of behaviour. It was first used to describe situations in which an animal or person learns the characteristics of some stimulus, which is therefore said to be \"imprinted\" onto the subject. Imprinting is hypothesized to have a critical period.\n\nThe best-known form of imprinting is \"filial imprinting\", in which a young animal acquires several of its behavioural characteristics from its parent. It is most obvious in nidifugous birds, which imprint on their parents and then follow them around. It was first reported in domestic chickens, by the 19th-century amateur biologist Douglas Spalding. It was rediscovered by the early ethologist Oskar Heinroth, and studied extensively and popularized by his disciple Konrad Lorenz working with greylag geese.\n\nLorenz demonstrated how incubator-hatched geese would imprint on the first suitable moving stimulus they saw within what he called a \"critical period\" between 13–16 hours shortly after hatching. For example, the goslings would imprint on Lorenz himself (to be more specific, on his wading boots), and he is often depicted being followed by a gaggle of geese who had imprinted on him. Lorenz also found that the geese could imprint on inanimate objects. In one notable experiment, they followed a box placed on a model train in circles around the track. Filial imprinting is not restricted to non-human animals that are able to follow their parents, however.\n\nThe filial imprinting of birds was a primary technique used to create the movie \"Winged Migration\" (\"Le Peuple Migrateur\"), which contains a great deal of footage of migratory birds in flight. The birds imprinted on handlers, who wore yellow jackets and honked horns constantly. The birds were then trained to fly along with a variety of aircraft, primarily ultralights.\nThe Italian hang-glider pilot Angelo d'Arrigo extended this technique. D'Arrigo noted that the flight of a non-motorised hang-glider is very similar to the flight patterns of migratory birds; both use updrafts of hot air (thermal currents) to gain altitude that then permits soaring flight over distance. He used this fact to enable the re-introduction into the wild of threatened species of raptors.\n\nBirds that are hatched in captivity have no mentor birds to teach them traditional migratory routes. D'Arrigo had one solution to this problem. The chicks hatched under the wing of his glider and imprinted on him. Then, he taught the fledglings to fly and to hunt. The young birds followed him not only on the ground (as with Lorenz) but also in the air as he took the path of various migratory routes. He flew across the Sahara and over the Mediterranean Sea to Sicily with eagles, from Siberia to Iran (5,500 km) with a flock of Siberian cranes, and over Mount Everest with Nepalese eagles. In 2006, he worked with a condor in South America.\n\nIn a similar project, orphaned Canada geese were trained to their normal migration route by the Canadian ultralight enthusiast Bill Lishman, as shown in the fact-based movie drama \"Fly Away Home\".\n\nChicks of domestic chickens prefer to be near large groups of objects that they have imprinted on. This behaviour was used to determine that very young chicks of a few days old have rudimentary counting skills. In a series of experiments, they were made to imprint on plastic balls and could figure out which of two groups of balls hidden behind screens had the most balls.\n\nAmerican coot mothers have the ability to recognize their chicks by imprinting on cues from the first chick that hatches. This allows mothers to distinguish their chicks from parasitic chicks.\n\nThe peregrine falcon has also been known to imprint on specific structures for their breeding grounds such as cliff sides and bridges and thus will favour that location for breeding. \n\nSexual imprinting is the process by which a young animal learns the characteristics of a desirable mate. For example, male zebra finches appear to prefer mates with the appearance of the female bird that rears them, rather than that of the birth parent when they are different.\n\nSexual attraction to humans can develop in non-human mammals or birds as a result of sexual imprinting when reared from young by humans. One example is London Zoo female giant panda Chi Chi. When taken to Moscow Zoo for mating with the male giant panda An An, she refused his attempts to mate with her, but made a full sexual self-presentation to a zookeeper.\n\nIt commonly occurs in falconry birds reared from hatching by humans. Such birds are called \"imprints\" in falconry. When an imprint must be bred from, the breeder lets the male bird copulate with their head while they are wearing a special hat with pockets on to catch the male bird's semen. The breeder then courts a suitable imprint female bird (including offering food, if it is part of that species's normal courtship). At \"copulation,\" the breeder puts the flat of one hand on the female bird's back to represent the weight of a male bird, and with the other hand uses a pipette, or a hypodermic syringe without a needle, to squirt the semen into the female's cloaca.\n\nSexual imprinting on inanimate objects is a popular theory concerning the development of sexual fetishism. For example, according to this theory, imprinting on shoes or boots (as with Konrad Lorenz's geese) would be the cause of shoe fetishism.\n\n\"Reverse\" sexual imprinting is also seen in instances where two people who live in domestic proximity during the first few years in the life of either one become desensitized to later close sexual attraction. This phenomenon, known as the Westermarck effect, was first formally described by Finnish anthropologist Edvard Westermarck in his book \"The History of Human Marriage\" (1891). The Westermarck effect has since been observed in many places and cultures, including in the Israeli kibbutz system, and the Chinese shim-pua marriage customs, as well as in biological-related families.\n\nIn the case of the Israeli kibbutzim (collective farms), children were reared somewhat communally in peer groups, based on age, not biological relation. A study of the marriage patterns of these children later in life revealed that out of the nearly 3,000 marriages that occurred across the kibbutz system, only fourteen were between children from the same peer group. Of those fourteen, none had been reared together during the first six years of life. This result provides evidence not only that the Westermarck effect is demonstrable but that it operates during the period from birth to the age of six. However, Eran Shor and Dalit Simchai claimed that the case of the kibbutzim actually provides little support for the Westermarck effect.\n\nWhen proximity during this critical period does not occur—for example, where a brother and sister are brought up separately, never meeting one another—they may find one another highly sexually attractive when they meet as adults. This phenomenon is known as genetic sexual attraction. This observation supports the hypothesis that the Westermarck effect evolved because it suppressed inbreeding. This attraction may also be seen with cousin couples.\n\nSigmund Freud argued that as children, members of the same family naturally lust for one another, making it necessary for societies to create incest taboos, but Westermarck argued the reverse, that the taboos themselves arise naturally as products of innate attitudes. Steven Pinker has written that Freud's conception of an urge to incest may have derived from Freud's own erotic reaction to his mother as a boy (attested in Freud's own writings), and speculates that Freud's reaction may have been due to lack of intimacy with his mother in early childhood, as Freud was wet-nursed.\n\nIn human–computer interaction, baby duck syndrome denotes the tendency for computer users to \"imprint\" on the first system they learn, then judge other systems by their similarity to that first system. The result is that \"users generally prefer systems similar to those they learned on and dislike unfamiliar systems\". The issue may present itself relatively early in a computer user's experience, and it has been observed to impede education of students in new software systems or user interfaces.\n\n\n\n"}
{"id": "41993", "url": "https://en.wikipedia.org/wiki?curid=41993", "title": "Intensity (physics)", "text": "Intensity (physics)\n\nIn physics, intensity is the power transferred per unit area, where the area is measured on the plane perpendicular to the direction of propagation of the energy. In the SI system, it has units watts per square metre (W/m). It is used most frequently with waves (e.g. sound or light), in which case the \"average\" power transfer over one period of the wave is used. \"Intensity\" can be applied to other circumstances where energy is transferred. For example, one could calculate the intensity of the kinetic energy carried by drops of water from a garden sprinkler.\n\nThe word \"intensity\" as used here is not synonymous with \"strength\", \"amplitude\", \"magnitude\", or \"level\", as it sometimes is in colloquial speech. \n\nIntensity can be found by taking the energy density (energy per unit volume) at a point in space and multiplying it by the velocity at which the energy is moving. The resulting vector has the units of power divided by area (i.e., surface power density).\n\nIf a point source is radiating energy in all directions (producing a spherical wave), and no energy is absorbed or scattered by the medium, then the intensity decreases in proportion to the distance from the object squared. This is an example of the inverse-square law. \n\nApplying the law of conservation of energy, if the net power emanating is constant,\nwhere \"P\" is the net power radiated, I is the intensity as a function of position, and dA is a differential element of a closed surface that contains the source. \n\nIf one integrates over a surface of uniform intensity \"I\", for instance over a sphere centered around the point source, the equation becomes\nwhere \"I\" is the intensity at the surface of the sphere, and \"r\" is the radius of the sphere. (formula_3 is the expression for the surface area of a sphere). \n\nSolving for \"I\" gives\n\nIf the medium is damped, then the intensity drops off more quickly than the above equation suggests. \n\nAnything that can transmit energy can have an intensity associated with it. For a monochromatic propagating wave, such as a plane wave or a Gaussian beam, if \"E\" is the complex amplitude of the electric field, then the time-averaged energy density of the wave is given by:\nand the local intensity is obtained by multiplying this expression by the wave velocity, c/\"n\":\nwhere \"n\" is the refractive index, c is the speed of light in vacuum and formula_7 is the vacuum permittivity.\n\nFor non-monochromatic waves, the intensity contributions of different spectral components can simply be added. The treatment above does not hold for arbitrary electromagnetic fields. For example, an evanescent wave may have a finite electrical amplitude while not transferring any power. The intensity should then be defined as the magnitude of the Poynting vector.\n\nIn photometry and radiometry \"intensity\" has a different meaning: it is the luminous or radiant power \"per unit solid angle\". This can cause confusion in optics, where \"intensity\" can mean any of radiant intensity, luminous intensity or irradiance, depending on the background of the person using the term. Radiance is also sometimes called \"intensity\", especially by astronomers and astrophysicists, and in heat transfer.\n\n"}
{"id": "7856944", "url": "https://en.wikipedia.org/wiki?curid=7856944", "title": "International Society for Clinical Electrophysiology of Vision", "text": "International Society for Clinical Electrophysiology of Vision\n\nThe International Society for Clinical Electrophysiology of Vision (ISCEV) is an association that promotes research and applications of electrophysiological methods (e.g. electroretinogram, electrooculogram, and visual evoked potentials) in clinical diagnosis of ophthalmological diseases. The society was founded in 1958 as the International Society for Clinical Electroretinography (ISCERG) and holds annual meetings that take place at changing locations. The official journal is \"Documenta Ophthalmologica\". The society also establishes standards for electrophysiological diagnosis.\n\n"}
{"id": "22465870", "url": "https://en.wikipedia.org/wiki?curid=22465870", "title": "Inuvik (crater)", "text": "Inuvik (crater)\n\nInuvik is an Impact crater located outside the perennial north polar cap on Mars. The crater's diameter is 20.5 kilometres, and it lies at latitude 78.7°N, longitude 28.6°W. In 1988, the IAU's Working Group for Planetary System Nomenclature (WGPSN) named the crater after the town of Inuvik, Northwest Territories, Canada.\n"}
{"id": "48984479", "url": "https://en.wikipedia.org/wiki?curid=48984479", "title": "Invisibilia", "text": "Invisibilia\n\nInvisibilia is a radio program and podcast currently produced and hosted by Alix Spiegel and Hanna Rosin for National Public Radio. Previous seasons were also hosted by Lulu Miller. The show debuted in early 2015, and \"explores the intangible forces that shape human behavior—things like ideas, beliefs, assumptions and emotions.\" The program's title comes from Latin, meaning \"all the invisible things\". \"The Guardian\" ranked \"Invisibilia\" among \"the 10 best new podcasts of 2015.\"\n\nAlix Spiegel was a founding producer of \"This American Life\" and freelanced for NPR's Science Desk covering psychology and human behavior. At Chicago's Third Coast International Audio Festival, Spiegel met former \"Radiolab\" producer Lulu Miller and asked her to co-produce a piece she was working on. The two began collaborating on radio stories and conceived of a new long-form program that would become \"Invisibilia\". The show's first six-episode season aired from January to February 2015, with excerpts occasionally running on \"All Things Considered\", \"Morning Edition\", \"Radiolab\" and \"This American Life\". This extra exposure and Miller and Spiegel's track record helped \"Invisibilia\" debut at #1 on the iTunes podcast chart and to maintain a consistent top-ten ranking in the months following its launch. Hanna Rosin from \"The Atlantic\" joined as cohost for the second season, which premiered in June 2016 and ran for seven episodes. The third season debuted in June 2017 with Spiegel and Rosin as hosts.\n\n\n\n\n"}
{"id": "2627033", "url": "https://en.wikipedia.org/wiki?curid=2627033", "title": "John Law (sociologist)", "text": "John Law (sociologist)\n\nJohn Law (born 16 May 1946), is a sociologist currently on the Faculty of Social Sciences at the Open University and key proponent of Actor-network theory. Actor-network theory, sometimes abbreviated to ANT, is a social science approach for describing and explaining social, organisational, scientific and technological structures, processes and events. It assumes that all the components of such structures (whether these are human or otherwise) form a network of relations that can be mapped and described in the same terms or vocabulary.\n\nDeveloped by two leading French STS scholars, Michel Callon and Bruno Latour, Law himself, and others, ANT may alternatively be described as a 'material-semiotic' method. ANT strives to map relations that are simultaneously material (between things) and 'semiotic' (between concepts), for instance, the interactions in a bank involve both people and their ideas, and computers. Together these form a single network.\n\nProfessor John Law is one of the directors of the ESRC funded Centre for Research on Socio-Cultural Change.\n\n\n\n\n"}
{"id": "36193445", "url": "https://en.wikipedia.org/wiki?curid=36193445", "title": "John Veevers", "text": "John Veevers\n\nJohn James Veevers (13 October 1930 – 12 August 2018) was an Australian Professor of Geology and a fellow of the Australian Academy of Science.\n\nVeevers is the son of George Stanley Veevers and Dulcie Annie (née James) and attended Newington College (1944–1947). In 1946 he won the Wigram Allen Scholarship, endowed by Sir George Wigram Allen, for General Proficiency. At the end of 1947 Veevers was named Dux of the College and received the Schofield Scholarship and Halse Rogers Prize. He went up to the University of Sydney and graduated as a Bachelor of Science in 1952 and a Master of Science in 1954.\n\nVeevers was a cadet geologist at the Bureau of Mineral Resources, Geology and Geophysics, Canberra, (1948–1951), rising to geologist (1952–1960) and senior geologist 1961–1967. In 1968, he was appointed as a senior lecturer at Macquarie University and has since been an Associate Professor, Professor and Adjunct Professor of the Department of Earth and Planetary Sciences. He was awarded his PhD in 1956 from Imperial College London.\n\nVeevers multiple publications are held by the National Library of Australia.\n\n"}
{"id": "4045043", "url": "https://en.wikipedia.org/wiki?curid=4045043", "title": "Kristina Curry Rogers", "text": "Kristina Curry Rogers\n\nKristina A. Curry Rogers is a vertebrate paleontologist and a Professor in geology and biology at Macalester College. She holds a B.Sc. in Biology from Montana State University, and a M.Sc. Ph.D. in Anatomical Science from the State University of New York at Stony Brook. Her work focuses on questions of dinosaur biology, bone histology, growth, and evolution, especially the Titanosauria, on which she wrote her doctoral dissertation. Together with Catherine Forster, Associate Professor and her teacher at Stony Brook, she discovered and described \"Rapetosaurus\", the most complete Cretaceous sauropod and titanosaur found to date.\n\nThrough cladistic analysis of \"Rapetosaurus\" and other titanosaurs, Curry Rogers was able to revise the phylogeny of the titanosaurs (see Curry Rogers 2005).\n\n\n"}
{"id": "36422436", "url": "https://en.wikipedia.org/wiki?curid=36422436", "title": "Lewis &amp; Clark: Great Journey West", "text": "Lewis &amp; Clark: Great Journey West\n\nLewis & Clark: Great Journey West is a 40-minute documentary film released by National Geographic, produced by \"Simon and Goodman Picture Company\", recapping the Lewis and Clark Expedition. It was first released in theaters across the country on May 1, 2002 and was last shown in theaters May 28, 2007 before being released on VHS and DVD.\n\nActors recreate the experiences of Meriwether Lewis, William Clark and Sacagawea (Alex Rice) on the \"Corps of Discovery Expedition\" (1804–1806) as they make the first crossing of what would later become the United States. The film is narrated by Jeff Bridges.\n"}
{"id": "41611908", "url": "https://en.wikipedia.org/wiki?curid=41611908", "title": "List of 3D rendering software", "text": "List of 3D rendering software\n\nThis page provides a list of 3D rendering software. This is not the same as 3D modeling software, which involves the creation of 3D models, for which the software listed below can produce realistic rendered visualisations. Also not included are general-purpose packages which can have their own built-in rendering capabilities; these can be found in the List of 3D computer graphics software and List of 3D animation software. See 3D computer graphics software for more discussion about the distinctions.\n\n\n"}
{"id": "10055498", "url": "https://en.wikipedia.org/wiki?curid=10055498", "title": "List of IPv6 tunnel brokers", "text": "List of IPv6 tunnel brokers\n\nThis is a list of IPv6 tunnel brokers that conform to the principles of RFC 3053 which describes a system with which users can request creation of an IPv6 tunnel on a host called a point of presence (PoP) that provides IPv6 connectivity to the user's network.\n\n<nowiki>*</nowiki>\"Paid services\"\n\nThe columns in the table provide the following details:\n"}
{"id": "17380318", "url": "https://en.wikipedia.org/wiki?curid=17380318", "title": "List of earthquakes in Sichuan", "text": "List of earthquakes in Sichuan\n\nThe Sichuan province of China has seen many earthquakes, the most recent of which occurred in 2017.\n"}
{"id": "1141795", "url": "https://en.wikipedia.org/wiki?curid=1141795", "title": "List of typefaces included with macOS", "text": "List of typefaces included with macOS\n\nThis list of fonts contains every font shipped with Mac OS X 10.0 through macOS 10.12, including any that shipped with language-specific updates from Apple (primarily Korean and Chinese fonts). For fonts shipped only with Mac OS X 10.5, \nplease see Apple's documentation.\n\nThe following system fonts have been added with Yosemite:\n\n\nAt least the following system fonts have been added with El Capitan:\n\n\nAt least the following system fonts have been added with Sierra:\n\n\n\n\nA number of fonts have also been provided with iMovie, iLife, iDVD and other Apple applications in hidden folders, for the sole use of these applications. The reason why these fonts are hidden is unknown, with licensing issues suggested as the cause. However, they may easily be installed for use by all applications by copying them out of their Library directories and installing them as with any third-party font.\n\nNotable hidden fonts on macOS include Bank Gothic, Bodoni, Century Gothic, Century Schoolbook, Garamond, several cuts of Lucida and Monotype Twentieth Century.\n\n\n\n\n\n"}
{"id": "32898922", "url": "https://en.wikipedia.org/wiki?curid=32898922", "title": "Michigan model", "text": "Michigan model\n\nThe Michigan model is a theory of voter choice, based primarily on sociological and party identification factors. Originally proposed by political scientists in the 1950s at the University of Michigan's Survey Research Center, it looked to explain voting behavior in terms of a voter's psychological attachment to a political party, which would be built up over a period of many years.\n\nAccording to the model, this party attachment is generally stable, formulated by outside social influences, including parents, family members and others in one's sociological spectrum. However, in recent years, the model has been challenged by spatial and valence models, forcing proponents to reconsider the long term implications of party attachment. Critics claim that the Michigan model exaggerates the assumption that party identification is cemented by circumstances, but rather that party identification can change in light of a party's performance or other circumstances. The model is only applicable to American \"winner-take-all\" systems, as lack of choice contributes to small chances for Partisan ID to change. The model most famously appeared in \"The American Voter.\"\n\nThe funnel of causality:\n\nThe model relies heavily on early attachment to parties, through the funnel of causality. This shows long term effects such as: Sociological Characteristics (Race, ethnicity, gender and sexual orientation), Social Status Characteristics (Social class & Occupation), and Parental Characteristics (Values and Partisanship). These factors go on to create party Identification which is largely static within individual voters. And it is through an individuals partisan identification that short term choices, such as Candidate Evaluation and Issue Perceptions are created.\n"}
{"id": "26618100", "url": "https://en.wikipedia.org/wiki?curid=26618100", "title": "Print design", "text": "Print design\n\nPrint design, a subset of graphic design, is a form of visual communication used to convey information to an audience through intentional aesthetic design printed on a tangible surface, designed to be printed on paper, as opposed to presented on a digital platform. A design can be considered print design if its final form was created through an imprint made by the impact of a stamp, seal, or dye on the surface of the paper.\n\nThere are several methods used to create print design artworks, spanning more than five hundred years. Printing technologies available throughout history heavily influenced the style of designs created by graphic designers at the time of production, as different methods of creating print design offer varying features. Before the emergence of the design and printing technologies of the twentieth and twenty-first century such as the inkjet printer, Adobe Illustrator, Adobe Photoshop, and Adobe InDesign, print design relied on mechanical technologies such as the letterpress and lithography.\n\nThe letterpress, perfected in the mid fifteenth century by Johannes Gutenberg (1398-1468) through the combined use of the printing press, oil-based inks, and cast metal type, remained the most common and efficient method of printing until the 1960s. Used frequently with typography design and type layout, the letterpress operates through the stamping of type and photo-engraved metal blocks on paper. The metal blocks are arranged in a frame by the printer, and the text columns and etchings are separated by vertical or horizontal metal bars; it is even possible to arrange the blocks at an angle using a letterpress. With the letterpress, print design and graphics remained black and white print on paper until the late nineteenth century. The letterpress was the first technology that allowed for mass production and distribution of printed material at a large scale, and because of this, quickly replaced the slow processes of woodblock printing and hand copying of print design.\n\nLithography, introduced at the end of the nineteenth century, allowed for the use of color in prints and allowed artists to print on larger surfaces than the letterpress. Additionally, lithography enabled artists to draw their own lettering on designs, which was not possible with the letterpress. The design was drawn directly onto the stone by the artist, and then transferred onto the surface of the paper.\n\nPrint design remains prevalent in society through all forms of communicative design. The importance of printed visual design was highlighted during the first world war, as posters helped to inform and instruct the audience. A short list of print design's uses today includes:\n"}
{"id": "30598594", "url": "https://en.wikipedia.org/wiki?curid=30598594", "title": "Project 57", "text": "Project 57\n\nProject 57 was an open-air nuclear test conducted by the United States at the Nellis Air Force Range in 1957, following Operation Redwing, and preceding Operation Plumbbob. The test area, also known as Area 13, was a by block of land abutting the northeast boundary of the Nevada National Security Site.<ref name=\"DOE/NV-1046\">National Nuclear Security Administration / Nevada Site Office, \"Plutonium Dispersal Tests at the Nevada Test Site\", April 2010, DOE/NV-1046 </ref>\n\nProject 57 was a combination safety test. The high explosives of a nuclear weapon were detonated asymmetrically to simulate an accidental detonation. The purpose of the test was to verify that no yield would result as well as study the extent of plutonium contamination.\n\nThe contaminated area was initially fenced off and the contaminated equipment buried in place. In 1981, the U.S. Department of Energy decontaminated and decommissioned the site. Hundreds of thousands of cubic yards of soil and debris were removed from Area 13 and disposed of in a waste facility at the Nevada Test Site.\n\n"}
{"id": "15945948", "url": "https://en.wikipedia.org/wiki?curid=15945948", "title": "Project MATCH", "text": "Project MATCH\n\nProject MATCH began in 1989 in the United States and was sponsored by the National Institute on Alcohol Abuse and Alcoholism (NIAAA). The project was an 8-year, multi site, $27-million investigation that studied which types of alcoholics respond best to which forms of treatment. MATCH studied whether treatment should be uniform or assigned to patients based on specific needs and characteristics. The programs were administered by psychotherapists and, although twelve-step methods were incorporated into the therapy, actual Alcoholics Anonymous meetings were not included. \nThree types of treatment were investigated:\n\nThe study concluded that patient-treatment matching is not necessary in alcoholism treatment because the three techniques are equal in effectiveness.\n\nCutler and Fishbain, in a review of Project Match, stated that in out of more than 60 publications generated by Project MATCH, all have overlooked important results.\n\nDr. Stanton Peele criticized MATCH on the basis that there was no control group (a group selected specifically for non treatment) to determine whether the treatments were more effective than the natural recovery process. Therapists in MATCH were more highly trained and monitored than addiction counselors usually available to the public. Effectiveness for all treatments was measured by reduction in frequency and intensity of drinking, whereas twelve-step and abstention-based programs, he argued, should claim no improvement without full abstention. Peele states Project Match failure to confirm a matching hypothesis revealed more its oversights in their methods, as well as showing that American conceptions of alcoholism and treatment policy are fundamentally wrong. Non American research has shown that subjective matching does work and other research shows that many with alcohol abuse problems heal on their own without treatment.\n\nGeorge Vaillant argues researchers need to examine differences between alcoholics who succeed in recovering and those who fail, rather than limiting themselves to a search for contrasts among professionally run treatments.\n\nProject MATCH was poorly designed \"to say the least\" asserts psychologist G. Alan Marlatt of the University of Washington in Seattle, a pioneer in the development of behavioral treatments for alcoholism. Marlatt states: \"Everybody can now project their own views about alcoholism onto this study.\"\n\n"}
{"id": "3291080", "url": "https://en.wikipedia.org/wiki?curid=3291080", "title": "Rough fuzzy hybridization", "text": "Rough fuzzy hybridization\n\nRough fuzzy hybridization is a method of hybrid intelligent system or soft computing, where Fuzzy set theory is used for linguistic representation of patterns, leading to a \"fuzzy granulation\" of the feature space. Rough set theory is used to obtain dependency rules which model informative regions in the granulated feature space.\n\n"}
{"id": "14657447", "url": "https://en.wikipedia.org/wiki?curid=14657447", "title": "Spectrochemistry", "text": "Spectrochemistry\n\nSpectrochemistry is the application of spectroscopy in any of several fields of chemistry.\n\nIt includes the analysis of spectra in chemical terms, and the use of spectra to derive the structure of chemical compounds, and to qualitatively and quantitatively analyze their presence in samples.\n\n"}
{"id": "217858", "url": "https://en.wikipedia.org/wiki?curid=217858", "title": "Tribe (biology)", "text": "Tribe (biology)\n\nIn biology, a tribe is a taxonomic rank above genus, but below family and subfamily. It is sometimes subdivided into subtribes.\n\nIn zoology, the standard ending for the name of a zoological tribe is \"-ini\". Examples include the tribes Caprini (goat-antelopes), Hominini (hominins), Bombini (bumblebees), and Thunnini (tunas). The tribe Hominini is divided into subtribes by some scientists; subtribe Hominina then comprises \"humans\". The standard ending for the name of a zoological subtribe is \"-ina\".\n\nIn botany, the standard ending for the name of a botanical tribe is \"-eae\". Examples include the tribes Acalypheae and Hyacintheae. The tribe Hyacintheae is divided into subtribes, including the subtribe Massoniinae. The standard ending for the name of a botanical subtribe is \"-inae\".\n\nIn bacteriology, the form of tribe names is as in botany, e.g., Pseudomonadeae, based on the genus name \"Pseudomonas\".\n\n"}
{"id": "57995863", "url": "https://en.wikipedia.org/wiki?curid=57995863", "title": "Upulie Divisekera", "text": "Upulie Divisekera\n\nUpulie Pabasarie Divisekera is an Australian molecular biologist and science communicator. She is currently a doctoral student at Monash University and is the co-founder of Real Scientists, an outreach program that uses performance and writing to communicate science. She has written for \"The Sydney Morning Herald\", \"Crikey\" and \"The Guardian\".\n\nDivisekera has wanted to be a scientist since she was a child. She is of Sri Lankan descent. After finishing high school she worked for biochemist Mary-Jane Gething from 1995 through 1997. She completed her undergraduate studies at the University of Melbourne in 2001. Here she worked on molecular parasitology with Malcolm McConville. Between 2002 and 2004 she worked as a research assistant at the Walter and Eliza Hall Institute of Medical Research on apoptosis and antibody production. She joined Australian National University for her postgraduate studies, graduating in 2007. Divisekera worked on the epithelial to mesenchymal transition in fruit fly embryos in Canberra. She worked as a research assistant at the University of Melbourne in 2007. Divisekera worked as a research assistant at Peter MacCallum Cancer Centre from 2008 to 2012. During this time, she worked in developmental biology and cancer research with Mark Smyth. She studied CD73 as a potential immunotherapy for breast cancer. She is a doctoral student in the department of chemical engineering at Monash University working on nanoparticles and drug delivery.\n\nIn 2011 Divisekera participated in and won the online science communication competition, \"I'm a Scientist, Get Me Out of Here\". Divisekera spoke at TEDx Canberra in 2012 on dinosaurs, curiosity and change in science. She has written for \"The Guardian,\" \"The Sydney Morning Herald,\" \"Crikey,\" and ABC TV's panel show \"Q and A,\" while also regularly contributing to ABC Radio National. In 2013, she was one of three co-founders of the \"Real Scientists\" project, a rotating-curator Twitter account where a different scientist is responsible for a week of science communication. Real Scientists looks to democratise access to science through live diarising of a scientists' day on Twitter, as well as demonstrating the diversity in the sector. She maintains her own Twitter account, which has tens of thousands of followers and is quoted in the press. She appears regularly on the Australian Broadcasting Corporation's radio channels. Divisekera provides training for academics, postgrads, clinicians and humanities students in science communication.\n\nAlongside science communication, Divisekera is involved with arts programming, including events at the Wheeler Centre. She took part in a discussion with Cory Doctorow and Maggie Ryan Sandford about the prospect of inhabiting Mars in 2015. Since 2016 she has been a speaker at the Melbourne Writers Festival, and has spoken at The Writer's Bloc, the New South Wales Writers' Centre and the Emerging Writers' Festival.\n\nDivisekera was included in the Government of Australia Chief Scientist \"Five Scientist Pledge\". She has spoken on Australian Broadcasting Corporation about what can be done to support more women into science. She gave a keynote talk at the March for Science in Melbourne. In May 2018 Upulie took on Elon Musk in a Twitter feud after he referred to nanotechnology as \"bs\". She is a contributor to the literary magazine \"The Lifted Brow.\"\n\n"}
{"id": "6195887", "url": "https://en.wikipedia.org/wiki?curid=6195887", "title": "Vladimir Arnoldi", "text": "Vladimir Arnoldi\n\nVladimir Mitrofanovich Arnoldi () (Kozlov (Michurinsk), Russia (1871–1924)) was a Russian professor of biology. He was a Corresponding Member of Russian Academy of Sciences and scientifically listed a number of valuable plants of Malaysia.\n\nHe lived in the Russian city of Tambov for much of his life. His son Constantin Arnoldi became a prominent entomologist.\n"}
{"id": "33562411", "url": "https://en.wikipedia.org/wiki?curid=33562411", "title": "Voskhod Spacecraft \"Globus\" IMP navigation instrument", "text": "Voskhod Spacecraft \"Globus\" IMP navigation instrument\n\n\"Globus\" IMP instruments were spacecraft navigation instruments used in Soviet and Russian manned spacecraft. The IMP acronym stems from the Russian expression \"Indicator of position in flight\", but the instrument is informally referred to as the \"Globus\".\nIt displays the nadir of the spacecraft on a rotating terrestrial globe. It functions as an onboard, autonomous indicator of the spacecraft's location relative to Earth coordinates. An electro-mechanical device in the tradition of complex post-World War II clocks such as master clocks, the \"Globus\" IMP instrument incorporates hundreds of mechanical components common to horology. This instrument is a mechanical computer for navigation akin to the Norden bombsight. It mechanically computes complex functions and displays its output through mechanical displacements of the globe and other indicator components. It also modulates electric signals from other instruments.\n\nThe IMP, in successively developing versions, has been used in Soviet and Russian manned space missions ever since the world's first manned spaceflight (Yuri Gagarin, 12 April 1961) through every manned Vostok, Voskhod and Soyuz mission until 2002.\n\nThe Voskhod spacecraft was the second generation of spacecraft designed in the manned Soviet space program, essentially an adaptation of the earlier Vostok spacecraft. It flew two manned missions, Voskhod 1 (world's first multi-crewed mission, launched on 12 October 1964) and Voskhod 2 (featuring the world's first Extra-vehicular activity, or EVA, commonly called a \"spacewalk\", launched on March 18, 1965). The Voskhod spacecraft—and its \"Globus\" IMP instruments—is a close derivative of Vostok, which flew six Soviet individuals to low Earth orbit, including the world's first human in space, Yuri Gagarin, and the world's first woman in space, Valentina Tereshkova. The main difference between IMP versions 1 and 2 (Vostok spacecraft) and later versions (Voskhod and Soyuz) is the addition of the disc-shaped longitude and latitude indicators.\n\nThe design objectives for the IMP were to compute and display the geographic coordinates at the spacecraft's nadir, i.e. which point on Earth's surface it was overflying. The Globus displayed this data to the crew, and also transmitted electrical data to other systems through a variable resistance and cam-activated switching.\n\nDerivatives of Vostok's and Voskhod's IMP have been flown on every Soyuz spacecraft up to the last of the Soyuz TM mission in April 2002. The main functional addition to IMP versions designed for Soyuz was the ability to manually change the orbit inclination. On Vostok and Voskhod, the inclination to the equator had been constant at 65 degrees by virtue of the booster design limitations and the geographical location of the Baikonur Cosmodrome from which every Soviet and Russian manned mission had been launched to date, so there was no need to implement inclination modulation into versions 1 to 4 of the IMP.\n\nThe Soyuz TMA spacecraft and its successors now provide similar functions to the \"Globus\" using a computerized world map on a computer display.\n\nThe Russian early missions were mostly automated and controlled from the mission control center (the \"TsUP\"). The spacecraft essentially controlled itself, and the cosmonauts were expected to initiate maneuvers or corrections only after approval from the MCC and according to its data and parameters. Therefore, the instrumentation available to the pilot was minimal and its operational relevance was limited to contingency scenarios as much as possible. The readings from the IMP were primarily intended to help cosmonaut pilots confirm that the automated flight sequencer was operating normally. The data would also keep the crew aware of their position when they were orbiting over the nighttime part of the Earth, or when the spacecraft's viewports and \"Vzor\" periscope couldn't be pointed toward the ground.\n\nHowever, the IMP would become crucial if manual retrorocket activation became compromised by failure of either the flight sequencer or communications with mission control, as did happen on Voskhod 2. Furthermore, given the scarcity of Soviet communication stations on Earth, the cosmonauts spent most of their time out of range with ground control and needed instruments to assess their position relative to the ground.\n\nBy contrast, the US manned space programs used a similar, mechanical positional indicator only during two of its early Mercury missions before discarding it (and using its former niche in Mercury's instrument panel as an equivalent to a car's glove compartment). The American instrument was a crude, smaller equivalent to the Soviets', an all-mechanical device (hand-wound) of limited complexity. Given the more extensive radio communication network with the spacecraft afforded by NASA, all subsequent manned missions, including lunar Apollo missions and Space Shuttle missions, used onboard maps, ground telemetry and more recently, computerized maps on portable laptop computers to provide the astronauts with positional information.\n\nTwo main instrument panels were used for Vostok and Voskhod: a control panel with switches and rotary controllers, and an instrument display panel (IDS, for Instrument Display System). On Vostok and Soyuz spacecraft, the IDS Panel sits in front of the cosmonauts, above the \"Vzor\" periscope screen. As for Voskhod 1 and Voskod 2 however, mission design compromises forced upon the designers for propaganda motivations had the cosmonauts' seats rotated 90 degrees clockwise, making the reading and setting of the IDS panel less convenient.\n\nDesigned to be integrated into a niche in the instrument display panel, the IMP instrument's volume was about that of a big toaster [Width: 24.8 cm( in), H:22.2 cm ( in), D:14.6 cm( in)]. It was the prominent feature of the IDS panel. The front panel of the IMP instrument served as the structural support for all other components; it was made of thick, machined aluminum alloy with structural protrusions. The mechanical components were made of brass, steel and aluminum; the globe itself was made of aluminum covered with printed paper, a typical technique for the manufacture of terrestrial globes. The enclosing cover was made of a sheet of aluminum alloy cut and soldered to shape.\n\nLike most of early spacecrafts' cockpit instruments, the IMP \"Globus\" was designed and tested to remain operative in a complete vacuum, in case of an accidental depressurisation of the cabin.\n\n\n\nThe moving terrestrial globe was protected by a hemispherical transparent plastic dome, on which was engraved and printed a cross-shaped sight. Under normal operations, the point visible under the cross was the point on Earth which was directly under the spacecraft at any given time. A second mode of operation, activated by the cosmonauts, advanced the globe to a position where the spacecraft would land should the retrorockets be fired at that moment to effect the reentry sequence, which also lit the \"место посадки\" indicator. The latitude and longitude indicators followed these two modes of operation as well.\n\nAt least one Voskhod IMP's globe was customized with white paper bullets numbered from 1 to 8, along with unnumbered bullets. Those relate to radio communication centers linked with Mission Control in Moscow. The unnumbered bullets refer to space control-monitoring ships.\n\nPrior to launch, latitude and longitude were adjusted to the precalculated coordinates of entering orbit. Right after the launch phase, once the orbit was established, its parameters were precisely measured by radar and radio telemetry from the ground. Revised settings for the IMP were then computed on the ground and communicated to the crew, which reset the instrument's three orbital \"Correction\" parameters, the equatorial longitude and the present point in the orbit using the two \"КОРРЕКЦИЯ\" knobs, the \"Э\" knob and the \"О\" knob. After this, the cosmonauts toggled the activation switch on the left-side Control panel. This last action connected the flight sequencer system's impulses to the instrument's solenoid actuator. These impulses were then converted into a slow, regular mechanical advance which cascaded through the mechanical components, effecting the computations needed to move the globe and other indicators.\n\nMeanwhile, the instrument's variable resistor and cam-activated electric blade contacts modulated electrical signals from other electrical instruments through the spacecraft and its control systems, feeding them with an analog representation of the spacecraft's displacement relative to Earth coordinates. From a systems design perspective, it is remarkable, even in the early 1960s, that a mechanical system generated crucial, primary data to electrical and electronic control and telemetry systems through the spacecraft.\n\nDuring orbital operations, the crew periodically resynchronized the instrument's readings with ground-generated data during communication passes. On a Soyuz mission, the spacecraft has the added capability to do orbital maneuvers, and the orbital parameters on the IMP instrument had to be changed accordingly for each maneuver.\n\nWhen the critical operation of deorbit retrorockets burn was approaching in preparation for atmospheric entry, the crew monitored the automatic orientation of the spacecraft (a function of the attitude control system), then toggled the proper switch on the Control panel to \"fast-forward\" the IMP instrument in order to display the projected point of landing. The crew then stood ready to effect a manual deorbit burn if the automatic systems failed to work.\n\nThe IMP was a genuine mechanical computer. From the sole solenoid actuator's incremental motion, the horological mechanism derived irregular oscillating functions which in turn rotated the globe and varied its axis, and also moved its two cylindrical indicators for longitude and latitude. Some uncommon, intricate horological devices found in the IMP include cardioid-shaped cam discs, a cone-shaped cam cylinder with a cardioid cross-section and \"mechanical rectifiers\" which transformed an alternating motion into an analogous, but unidirectional motion (see illustrations).\n\nThe only other electric actuator used in the IMP instrument was the motor used to fast-forward the mechanism from the actual point-to-nadir to the expected landing point, some 120 degrees further east.\n\nA few dozen specimens at most of the Voskhod \"Globus\" instrument Versions 3 and 4 were built, including the two flown during the \"Voskhod 1\" and \"Voskhod 2\" missions, test articles and units flown in unmanned missions. Many of them are exhibited in various Russian aerospace museums (sometimes mislabelled as \"Vostok\" IMP instruments), while a few are known to be in individuals' space-related artifact collections. Furthermore, an unknown number of units may still belong to former Soviet space workers' and officials' estates. The Smithsonian Institution's National Air and Space Museum in Washington DC (US) exhibits several Soyuz spacecraft whose \"Globus\" are still in place, but their instrument panels are not visible to visitors. The museum also displays a TKS spacecraft with a Soyuz-version of the IMP instrument, whose panels and the \"Globus\" can be glimpsed through the porthole.\n\nSpace-related artifacts are available for purchase on online auction sites, on specialized auctions and through specialized dealers. However, items such as \"Globus\" instruments rarely come on the market.\n\nOne specimen of a Voskhod \"Globus\" IMP instrument formally the property of a Canadian collector has been exhibited in temporary exhibitions, notably at the National Watch and Clock Museum in Columbia, Pennsylvania, US, and at the Space Science Center of the Cosmodome in Laval, Quebec, Canada.\n\n\n\n"}
{"id": "44172835", "url": "https://en.wikipedia.org/wiki?curid=44172835", "title": "Walter C. Williams", "text": "Walter C. Williams\n\nWalter C. Williams (July 30, 1919 – October 7, 1995) was an American engineer, leader of National Advisory Committee for Aeronautics group at Edwards Air Force Base in the 1940s, and a NASA Deputy Associate Administrator during Project Mercury. He received a Bachelor of Science degree in Aeronautical Engineering from Louisiana State University in 1939, and prior his NASA employment, worked for Glenn L. Martin Company. He was a recipient of the NASA Distinguished Service Medal and the John J. Montgomery Award.\n"}
