{"id": "3419576", "url": "https://en.wikipedia.org/wiki?curid=3419576", "title": "Air track", "text": "Air track\n\nAn air track is a scientific device used to study motion in low friction environment. Its name comes from its structure: air is pumped through a hollow track with fine holes all along the track that allows specially fitted air track cars to glide relatively friction-free. Air tracks are usually triangular in cross-section. Carts which have a triangular base and fit neatly on to the top of the track are used to study motion in low friction environments. \n\nThe air track is also used to study collisions, both elastic and inelastic. Since there is very little energy lost through friction it is easy to demonstrate how momentum is conserved before and after a collision. The track can be used to calculate the force of gravity when placed at an angle.\nIt was invented in the mid-1960s at the California Institute of Technology by Prof Nehr and Leighton. It was first presented by them at a meeting of the American Physical Society in NYC in 1965(?) where it was viewed by Prof John Stull, Alfred University, and Frank Ferguson, the Ealing Corporation. The original track was about 1 meter long with tiny air orifices and highly compressed air. Stull returned to Alfred Univ. Where he developed a simple version using standard square aluminum tubing with large air orifices and air from the vent of a shop vacuum cleaner. Stull with Ferguson's help at Ealing designed a custom aluminum track that Ealing offered commercially in various lengths up to 10 meters. T. Walley Williams III at Ealing extended the concept to the 2-dimensional air table in 1969. \n"}
{"id": "40850116", "url": "https://en.wikipedia.org/wiki?curid=40850116", "title": "Aleksei Lotman", "text": "Aleksei Lotman\n\nAleksei Lotman (also known as Alex Lotman and Aleks Lotman; born 6 May 1960 in Leningrad) is an Estonian biologist, environmentalist and politician.\n\nFrom 2010 to 2011 Lotman served as the leader of political party Estonian Greens. In the 2011 election, the Greens lost all their parliamentary representation.\n\nFrom 2007 to 2011, Lotman was a member of Estonian parliament Riigikogu, representing the Estonian Greens. He was also leader of the parliamentary group for Tibet.\n\nSince then he has worked for the Estonian Fund for Nature.\n\nHe was awarded the Order of the National Coat of Arms, 5th Class, by the President of Estonia on 6 February 2006.\n\nAleksei Lotman graduated from the Tartu Miina Härma Secondary School No 2 in 1978, and the Tartu State University as a biologist in 1985.\n\n1991–2000, Lotman served as the vice director of the Matsalu National Park.\n\nAlex Lotman is the son of literary scholar, semiotician, and cultural historian Juri Lotman and literary scientist Zara Mints. His brothers are literature researcher and politician Mihhail Lotman and artist Grigori Lotman. Alex Lotman is married to environmentalist Kaja Lotman, they have two daughters and a son.\n"}
{"id": "7914639", "url": "https://en.wikipedia.org/wiki?curid=7914639", "title": "Analyte-specific reagent", "text": "Analyte-specific reagent\n\nAnalyte-specific reagents (ASRs) are a class of biological molecules which can be used to identify and measure the amount of an individual chemical substance in biological specimens.\n\nThe U.S. Food and Drug Administration (FDA) defines \"analyte specific reagents\" (ASRs) in \"21 CFR 864.4020\" as “antibodies, both polyclonal and monoclonal, specific receptor proteins, ligands, nucleic acid sequences, and similar reagents which, through specific binding or chemical reaction with substances in a specimen, are intended to use in a diagnostic application for identification and quantification of an individual chemical substance or ligand in biological specimens.” \n\nIn simple terms an analyte specific reagent is the active ingredient of an in-house test.\n\n"}
{"id": "21951950", "url": "https://en.wikipedia.org/wiki?curid=21951950", "title": "Arne Bjørlykke", "text": "Arne Bjørlykke\n\nArne Bjørlykke (born 3 October 1943) is a Norwegian geologist.\n\nHe took an education in mining engineering at the Norwegian Institute of Technology, graduating in 1967. He worked at the Norwegian Geological Survey from 1968 to 1984, and was then a professor at the University of Oslo from 1984 to 1994. He then returned to the Norwegian Geological Survey as managing director from 1994 to 2006 and senior researcher from 2006 to 2009. In 2009 he was hired as the new director of the Natural History Museum at the University of Oslo, succeeding Elen Roaldset. He is a fellow of the Norwegian Academy of Technological Sciences.\n"}
{"id": "463112", "url": "https://en.wikipedia.org/wiki?curid=463112", "title": "Bathymetric chart", "text": "Bathymetric chart\n\nA bathymetric chart is the submerged equivalent of an above-water topographic map. Bathymetric charts are designed to present accurate, measurable description and visual presentation of the submerged terrain. \n\nIn an ideal case, the joining of a bathymetric chart and topographic map of the same scale and projection of the same geographic area would be seamless. The only difference would be that the values begin increasing after crossing the zero at the designated sea level datum. Thus the topographic map's mountains have the greatest values while the bathymetric chart's greatest depths have the greatest values. Simply put, the bathymetric chart is intended to show the land if overlying waters were removed in exactly the same manner as the topographic map.\n\nA bathymetric chart differs from a hydrographic chart in that accurate presentation of the underwater features is the goal while safe navigation is the requirement for the hydrographic chart. A hydrographic chart will obscure the actual features to present a simplified version to aid mariners in avoiding underwater hazards.\n\nBathymetric surveys are a subset of the science of hydrography. They differ slightly from the surveys required to produce the product of hydrography in its more limited application and as conducted by the national and international agencies tasked with producing charts and publications for safe navigation. That chart product is more accurately termed a navigation or hydrographic chart with a strong bias toward the presentation of essential safety information. \n\nBathymetric surveys and charts are more closely tied to the science of oceanography, particularly marine geology, and underwater engineering or other specialized purposes.\n\nBathymetric charts can also be converted to bathymetric profiles.\n\n\n\n"}
{"id": "2526670", "url": "https://en.wikipedia.org/wiki?curid=2526670", "title": "Bully for Brontosaurus", "text": "Bully for Brontosaurus\n\nBully for Brontosaurus (1991) is the fifth volume of collected essays by the Harvard paleontologist Stephen Jay Gould. The essays were culled from his monthly column \"This View of Life\" in \"Natural History\" magazine, to which Gould contributed for 27 years. The book deals, in typically discursive fashion, with themes familiar to Gould's writing: evolution and its teaching, science biography, and probabilities.\n\nThe title essay, \"Bully for Brontosaurus\", discusses the theory and history of taxonomy by examining the debate over whether \"Brontosaurus\" should be labelled \"Apatosaurus\". In \"Justice Scalia's Misunderstanding\", Gould dissects and decisively rejects Antonin Scalia's dissent in the United States Supreme Court case \"Edwards v. Aguillard\" that overturned the last creationist statute in the country. Gould claimed his favourite essay to be \"In a Jumbled Drawer\" which discusses the debate between Nathaniel Shaler and William James over whether the improbability of our having evolved necessitates divine intervention (Gould, like James, argues no); the essay includes a letter from former President Jimmy Carter as a postscript, which discusses the issue.\n\nThe essay \"Male Nipples and Clitoral Ripples\" dealt with the issue of adaptive arguments. It derives from some work by Elisabeth Lloyd, whose subsequent 2005 book was dedicated to Gould (and her parents), and uses the case of the female orgasm to expand on the subject of adaptiveness in both depth and breadth.\n\n"}
{"id": "20833517", "url": "https://en.wikipedia.org/wiki?curid=20833517", "title": "CTA-102", "text": "CTA-102\n\nIn astronomy, CTA 102, also known by its B1950 coordinates as 2230+114 (QSR B2230+114) and its J2000 coordinates as J2232+1143 (QSO J2232+1143), is a quasar discovered in the early 1960s by a radio survey carried out by the California Institute of Technology. It has been observed by a large range of instruments since its discovery, including WMAP, EGRET, GALEX, VSOP and Parkes, and has been regularly imaged by the Very Long Baseline Array since 1995. It has also been detected in gamma rays, and a gamma-ray flare has been detected from it.\n\nIn 1963 Nikolai Kardashev proposed that the then-unidentified radio source could be evidence of a Type II or III extraterrestrial civilization on the Kardashev scale. Follow-up observations were announced in 1965 by Gennady Sholomitskii, who found that the object's radio emission was varying; a public announcement of these results on April 12, 1965, caused a worldwide sensation. The idea that the emission was caused by a civilization was rejected when the radio source was later identified as one of the many varieties of a quasar.\n\nCTA 102 is one of the two great false alarms in the history of SETI, the other being the discovery of pulsars, specifically PSR B1919+21, which are rotating neutron stars.\n\nThe American folk rock band The Byrds whimsically reflected the original view that CTA-102 was a sign of extraterrestrial intelligence in their song \"C.T.A.-102\" from their 1967 album \"Younger Than Yesterday\".\n\nIn late 2016 CTA 102, usually glowing around magnitude +17, had a bright outburst in visible light to magnitude +11 (~250 times brighter than usual). This likely was the most luminous blazar state ever observed, with an absolute magnitude in excess of -32.\n\nA new outburst began in December 2017, with increased gamma-ray and optical activity. As of 22 December 2017, it has reached magnitude +14.\n\n"}
{"id": "1385766", "url": "https://en.wikipedia.org/wiki?curid=1385766", "title": "Cognitive map", "text": "Cognitive map\n\nA cognitive map (sometimes called a mental map or mental model) is a type of mental representation which serves an individual to acquire, code, store, recall, and decode information about the relative locations and attributes of phenomena in their everyday or metaphorical spatial environment. The concept was introduced by Edward Tolman in 1948. The term was later generalized by some researchers, especially in the field of operations research, to refer to a kind of semantic network representing an individual's personal knowledge or schemas.\n\nCognitive maps have been studied in various fields, such as psychology, education, archaeology, planning, geography, cartography, architecture, landscape architecture, urban planning, management and history. As a consequence, these mental models are often referred to, variously, as cognitive maps, mental maps, scripts, schemata, and frames of reference.\n\nCognitive maps serve the construction and accumulation of spatial knowledge, allowing the \"mind's eye\" to visualize images in order to reduce cognitive load, enhance recall and learning of information. This type of spatial thinking can also be used as a metaphor for non-spatial tasks, where people performing non-spatial tasks involving memory and imaging use spatial knowledge to aid in processing the task.\n\nThe neural correlates of a cognitive map have been speculated to be the place cell system in the hippocampus and the recently discovered grid cells in the entorhinal cortex.\n\nCognitive mapping is believed to largely be a function of the hippocampus. The hippocampus is connected to the rest of the brain in such a way that it is ideal for integrating both spatial and nonspatial information. Connections from the postrhinal cortex and the medial entorhinal cortex provide spatial information to the hippocampus. Connections from the perirhinal cortex and lateral entorhinal cortex provide nonspatial information. The integration of this information in the hippocampus makes the hippocampus a practical location for cognitive mapping, which necessarily involves combining information about an object's location and its other features.\n\nO'Keefe and Nadel were the first to outline a relationship between the hippocampus and cognitive mapping. Many additional studies have shown additional evidence that supports this conclusion. Specifically, pyramidal cells (place cells, boundary cells, and grid cells) have been implicated as the neuronal basis for cognitive maps within the hippocampal system.\n\nNumerous studies by O'Keefe have implicated the involvement of place cells. Individual place cells within the hippocampus correspond to separate locations in the environment with the sum of all cells contributing to a single map of an entire environment. The strength of the connections between the cells represents the distances between them in the actual environment. The same cells can be used for constructing several environments, though individual cells' relationships to each other may differ on a map by map basis. The possible involvement of place cells in cognitive mapping has been seen in a number of mammalian species, including rats and macaque monkeys. Additionally, in a study of rats by Manns and Eichenbaum, pyramidal cells from within the hippocampus were also involved in representing object location and object identity, indicating their involvement in the creation of cognitive maps. However, there has been some dispute as to whether such studies of mammalian species indicate the presence of a cognitive map and not another, simpler method of determining one's environment.\n\nWhile not located in the hippocampus, grid cells from within the medial entorhinal cortex have also been implicated in the process of path integration, actually playing the role of the path integrator while place cells display the output of the information gained through path integration. The results of path integration are then later used by the hippocampus to generate the cognitive map. The cognitive map likely exists on a circuit involving much more than just the hippocampus, even if it is primarily based there. Other than the medial entorhinal cortex, the presubiculum and parietal cortex have also been implicated in the generation of cognitive maps.\n\nThere has been some evidence for the idea that the cognitive map is represented in the hippocampus by two separate maps. The first is the bearing map, which represents the environment through self-movement cues and gradient cues. The use of these vector-based cues creates a rough, 2D map of the environment. The second map would be the sketch map that works off of positional cues. The second map integrates specific objects, or landmarks, and their relative locations to create a 2D map of the environment. The cognitive map is thus obtained by the integration of these two separate maps.\n\nThe cognitive map is generated from a number of sources, both from the visual system and elsewhere. Much of the cognitive map is created through self-generated movement cues. Inputs from senses like vision, proprioception, olfaction, and hearing are all used to deduce a person's location within their environment as they move through it. This allows for path integration, the creation of a vector that represents one's position and direction within one's environment, specifically in comparison to an earlier reference point. This resulting vector can be passed along to the hippocampal place cells where it is interpreted to provide more information about the environment and one's location within the context of the cognitive map.\n\nDirectional cues and positional landmarks are also used to create the cognitive map. Within directional cues, both explicit cues, like markings on a compass, as well as gradients, like shading or magnetic fields, are used as inputs to create the cognitive map. Directional cues can be used both statically, when a person does not move within his environment while interpreting it, and dynamically, when movement through a gradient is used to provide information about the nature of the surrounding environment. Positional landmarks provide information about the environment by comparing the relative position of specific objects, whereas directional cues give information about the shape of the environment itself. These landmarks are processed by the hippocampus together to provide a graph of the environment through relative locations.\n\nThe idea of a cognitive map was first developed by Edward C. Tolman. Tolman, one of the early cognitive psychologists, introduced this idea when doing an experiment involving rats and mazes. In Tolman's experiment, a rat was placed in a cross shaped maze and allowed to explore it. After this initial exploration, the rat was placed at one arm of the cross and food was placed at the next arm to the immediate right. The rat was conditioned to this layout and learned to turn right at the intersection in order to get to the food. When placed at different arms of the cross maze however, the rat still went in the correct direction to obtain the food because of the initial cognitive map it had created of the maze. Rather than just deciding to turn right at the intersection no matter what, the rat was able to determine the correct way to the food no matter where in the maze it was placed.\n\nIn a review, Andrew T.D. Bennett argued that there is no clear evidence for cognitive maps in non-human animals (i.e. cognitive map according to Tolman's definition). This argument is based on analyses of studies where it has been found that simpler explanations can account for experimental results. Bennett highlights three simpler alternatives that cannot be ruled out in tests of cognitive maps in non-human animals \"These alternatives are (1) that the apparently novel short-cut is not truly novel; (2) that path integration is being used; and (3) that familiar landmarks are being recognised from a new angle, followed by movement towards them.\"\n\nA cognitive map is a spatial representation of the outside world that is kept within the mind, until an actual manifestation (usually, a drawing) of this perceived knowledge is generated, a mental map. Cognitive mapping is the implicit, mental mapping the explicit part of the same process. In most cases, a cognitive map exists independently of a mental map, an article covering just cognitive maps would remain limited to theoretical considerations.\n\nIn some uses, mental map refers to a practice done by urban theorists by having city dwellers draw a map, from memory, of their city or the place they live. This allows the theorist to get a sense of which parts of the city or dwelling are more substantial or imaginable. This, in turn, lends itself to a decisive idea of how well urban planning has been conducted.\n\n"}
{"id": "4660524", "url": "https://en.wikipedia.org/wiki?curid=4660524", "title": "Committee for International Cooperation in National Research in Demography", "text": "Committee for International Cooperation in National Research in Demography\n\nThe Committee for International Cooperation in National Research in Demography, commonly known as CICRED, is a non-governmental organization accredited with the Economic and Social Council of the United Nations. Founded in 1972, it aims at developing cooperation amongst national population research centres, and encouraging new research. It serves as a platform for interaction between research centres and international organizations, such as United Nations Population Division, United Nations Population Fund (UNFPA), World Health Organization (WHO) and Food and Agriculture Organization (FAO).\n\nCICRED was run from its creation up to 1990 by Jean Bourgeois-Pichat, followed by Léon Tabah and then Philippe Collomb. Christophe Z Guilmoto became the new director in 2005. CICRED elected Francis Gendreau as its first Council Chair in 1993 and Gavin Jones in 2001.\n\nCICRED today is an association of more than 700 research organizations dealing with population issues, from national statistical bureaux to research departments in demography. Its office is located in Paris in the premises of INED. The CICRED General Assembly consisting of the representatives of the member centres meets every four years during the IUSSP International Population Conference. The CICRED Council consists of seven members elected for four years by the members, three \"ex officio\" members from international organizations and is presided over by the chairperson. The Council decides on the administration of the Committee and on its strategic orientations.\n\nBeyond its networking activities, CICRED conducts also international research programmes. It recently organized several conferences or seminars on subjects such aids and spatial mobility, demographic dividend, population-environment-development inter-relations or female deficit in Asia. It also published books and policy papers devoted to contemporary population issues.\n\n"}
{"id": "2030477", "url": "https://en.wikipedia.org/wiki?curid=2030477", "title": "Connections (TV series)", "text": "Connections (TV series)\n\nConnections is a 10-episode documentary television series and 1978 book (\"Connections\", based on the series) created, written, and presented by science historian James Burke. The series was produced and directed by Mick Jackson of the BBC Science and Features Department and first aired in 1978 (UK) and 1979 (USA). It took an interdisciplinary approach to the history of science and invention, and demonstrated how various discoveries, scientific achievements, and historical world events were built from one another successively in an interconnected way to bring about particular aspects of modern technology. The series was noted for Burke's crisp and enthusiastic presentation (and dry humour), historical re-enactments, and intricate working models.\n\nThe popular success of the series led to the production of \"The Day the Universe Changed\" (1985), a similar program but showing a more linear history of several important scientific developments. Years later, the success in syndication led to two sequels, \"Connections\" (1994) and \"Connections\" (1997), both for TLC. In 2004, KCSM-TV produced a program called \"Re-Connections\", consisting of an interview of Burke and highlights of the original series, for the 25th anniversary of the first broadcast in the USA on PBS.\n\n\"Connections\" explores an \"Alternative View of Change\" (the subtitle of the series) that rejects the conventional linear and teleological view of historical progress. Burke contends that one cannot consider the development of any particular piece of the modern world in isolation. Rather, the entire gestalt of the modern world is the result of a web of interconnected events, each one consisting of a person or group acting for reasons of their own motivations (e.g., profit, curiosity, religion) with no concept of the final, modern result to which the actions of either them or their contemporaries would lead. The interplay of the results of these isolated events is what drives history and innovation, and is also the main focus of the series and its sequels.\n\nTo demonstrate this view, Burke begins each episode with a particular event or innovation in the past (usually ancient or medieval times) and traces the path from that event through a series of seemingly unrelated connections to a fundamental and essential aspect of the modern world. For example, the episode \"The Long Chain\" traces the invention of plastics from the development of the fluyt, a type of Dutch cargo ship.\n\nBurke also explores three corollaries to his initial thesis. The first is that, if history is driven by individuals who act only on what they know at the time, and not because of any idea as to where their actions will eventually lead, then predicting the future course of technological progress is merely conjecture. Therefore, if we are astonished by the connections Burke is able to weave among past events, then we will be equally surprised to what the events of today eventually will lead, especially events of which we were not even aware at the time.\n\nThe second and third corollaries are explored most in the introductory and concluding episodes, and they represent the downside of an interconnected history. If history progresses because of the synergistic interaction of past events and innovations, then as history does progress, the number of these events and innovations increases. This increase in possible connections causes the process of innovation to not only continue, but also to accelerate. Burke poses the question of what happens when this rate of innovation, or more importantly 'change' itself, becomes too much for the average person to handle, and what this means for individual power, liberty, and privacy.\n\nLastly, if the entire modern world is built from these interconnected innovations, all increasingly maintained and improved by specialists who required years of training to gain their expertise, what chance does the average citizen without this extensive training have in making an informed decision on practical technological issues, such as the building of nuclear power plants or the funding of controversial projects such as stem cell research? Furthermore, if the modern world is increasingly interconnected, what happens when one of those nodes collapses? Does the entire system follow suit?\n\nThe original 1978 Connections 10-episode documentary television series and had a companion book (\"Connections\", based on the series) created, written, and presented by science historian James Burke. The 1978 Connections companion book was published about the time the middle of the series was airing, so likely was written in parallel to the series and had a post-production editing release. The very popular book was re-released as a work in a 1995 edition, 1998, (relations to sections below is unknown.) and again in 2007 as both hardcover or softcover editions. Since the television series varied in content with each corresponding production run and release, it is likely the companion volumes (as is suggest by the plethora of ISBN codes) are also different works. This 1978 work's coverage deviates in some topics and details being both more in depth and a bit broader, from the lighter coverage of the episodes. It can be found in many libraries. \n\n\n\nAll three \"Connections\" documentaries have been released in their entirety as DVD box sets in the US. The ten episodes of series one were released in Europe (Region 2) on 6 February 2017.\n\nBurke also wrote a series of \"Connections\" articles in \"Scientific American\", and published a book of the same name (1995, ), all built on the same theme of exploring the history of science and ideas, going back and forth through time explaining things on the way and, generally, coming back to the starting point.\n\nBurke produced another documentary series called \"The Day the Universe Changed\" in 1985, which explored man's concept of how the universe worked in a manner similar to the original \"Connections\".\n\n\"\" is a video series launched in 2011 on Kickstarter.com that was inspired by James Burke's \"Connections\". However, it follows concepts rather than inventions through time.\n\n\"Richard Hammond's Engineering Connections\", shown on BBC2, follows a similar format.\n\n\"Connections\", a \"Myst\"-style computer game with James Burke and others providing video footage and voice acting, was released in 1995. It was a runner-up for \"Computer Gaming World\"s award for the best \"Classics/Puzzles\" game of 1995, which ultimately went to \"You Don't Know Jack\". The editors wrote of \"Connections\", \"That you enjoy yourself so much you hardly realize that you're learning is a tribute to the design.\"\n\nA clip from the episode \"Yesterday, Tomorrow and You\" appears in the 2016 video game The Witness.\n\n"}
{"id": "36660308", "url": "https://en.wikipedia.org/wiki?curid=36660308", "title": "Conroy Virtus", "text": "Conroy Virtus\n\nThe Conroy Virtus was a proposed American large transport aircraft intended to transport the Space Shuttle. Designed, beginning in 1974, by John M. Conroy of the Turbo-Three Corporation, it was to incorporate a pair of Boeing B-52 Stratofortress fuselages to form a new craft utilizing existing parts for cost-savings.\n\nWhile the project was seriously considered, it proved impractically large, and NASA chose to develop the Boeing 747-based Shuttle Carrier from surplus commercial aircraft instead.\n\nThe Space Shuttle was originally designed to utilize on-board turbofan engines for propulsion within the atmosphere, both upon re-entry and for ferry flights between landing sites, such as Edwards Air Force Base, the White Sands Missile Range, or contingency landing sites such as Easter Island, to the launch site at Kennedy Space Center at Cape Canaveral. When the air-breathing engines were deleted from the Shuttle design due to cost and weight concerns, a requirement arose for a transport aircraft capable of carrying the Shuttle from landing sites back to the Kennedy Space Center. One early design for a shuttle carrier aircraft was proposed by John M. Conroy, developer of the Pregnant Guppy and Super Guppy oversized cargo aircraft, in cooperation with the NASA Langley Research Center; named Virtus, a contract was issued for design and development work in 1974.\n\nExpected to cost US$12.5 million each, Virtus was a twin-fuselage design powered by four large jet engines; it was intended for these to be Pratt & Whitney JT9D turbofans. Conroy proposed extensive use of 'off the shelf' military parts in the design to reduce costs; this included the use of fuselages from Boeing B-52 Stratofortress strategic bombers to form the aircraft's main fuselage pods, added to the new-design wing and tail section. The Space Shuttle Orbiter would be carried under the center section of the Virtus aircraft's wing, between the fuselages; other large cargos, including the Space Shuttle external tank, the Space Shuttle Solid Rocket Boosters, or dedicated cargo pods, could be alternatively carried.\n\nThe Virtus design was tested in the NASA Langley wind tunnel; while the results of the wind tunnel tests were considered promising, the drawbacks of such a large design, including the cost of developing an entirely new aircraft, flight testing the design, and the sheer size of the aircraft requiring the development and/or expansion of infrastructure to support it, militated against further development of Virtus. The Lockheed Corporation, which had proposed a twin-fuselage version of its C-5 Galaxy airlifter to carry the Shuttle, also saw its proposal rejected for the same reasons. A more modest conversion of existing C-5s was proposed, and nearly taken up by NASA, but it was determined that having aircraft continually available was preferable to being restricted by the United States Air Force on the use of C-5s, and Boeing's proposal for a conversion of the 747 airliner was selected, becoming the Shuttle Carrier Aircraft. A proposed commercial version of the Virtus design, named Colossus, also failed to gain any further interest, and the Virtus design was abandoned.\n\n"}
{"id": "8818842", "url": "https://en.wikipedia.org/wiki?curid=8818842", "title": "Costa Rican páramo", "text": "Costa Rican páramo\n\nThe Costa Rican páramo, also known as the Talamanca páramo, is a natural region of montane grassland and shrubland of Costa Rica and western Panama.\n\nThe Costa Rican páramo includes several enclaves on the highest peaks of the Cordillera de Talamanca in eastern Costa Rica and western Panama, with a total area of 31 km². The páramo is found above 3000–3100 meters elevation on the summits of Cerro de la Muerte, Cerro Chirripó, and Cerro Kamuk in Costa Rica, Cerro Echandi on the Costa Rica-Panama border, Cerro Fábrega, and Cerro Itamut in Panama.\n\nIt is surrounded at lower elevations by the Talamancan montane forests, and the World Wildlife Fund includes the Costa Rican páramo within the montane forests ecoregion, although the páramo has a distinct flora with affinities to the páramo of the Northern Andes.\n\nThe Costa Rican páramo can be divided into three zones. The subpáramo is a dwarf forest, dominated by the dwarf bamboo \"Chusquea subtessellata\", together with short shrubs.\n\nThe páramo proper lies above the subpáramo, and is dominated by grasses, rushes, herbs, and low shrubs of the families Gramineae, Asteraceae, Cyperaceae, Rosaceae and Ericaceae. Above the páramo lies the superpáramo, a narrow zone with scarce vegetation between the grass páramo and the snow line.\n\nTapir,Jaguar toucans, countless lizards and birds\n\n\n"}
{"id": "2944700", "url": "https://en.wikipedia.org/wiki?curid=2944700", "title": "Critical international relations theory", "text": "Critical international relations theory\n\nCritical international relations theory is a diverse set of schools of thought in international relations (IR) that have criticized the theoretical, meta-theoretical and/or political status quo, both in IR theory and in international politics more broadly – from positivist as well as postpositivist positions. Positivist critiques include Marxist and neo-Marxist approaches and certain (\"conventional\") strands of social constructivism. Postpositivist critiques include poststructuralist, postcolonial, \"critical\" constructivist, critical theory (in the strict sense used by the Frankfurt School), neo-Gramscian, most feminist, and some English School approaches, as well as non-Weberian historical sociology, \"international political sociology\", \"critical geopolitics\", and the so-called \"new materialism\" (partly inspired by actor–network theory). All of these latter approaches differ from both realism and liberalism in their epistemological and ontological premises.\n\nSuch theories are now widely recognized and taught and researched in many universities, but are less common in the United States. They are taught at both undergraduate and postgraduate levels in many major universities outside the US, where a major concern is that \"a myopic discipline of IR might contribute to the continued development of a civil society in the U.S. that thinks, reflects and analyzes complex international events through a very narrow set of theoretical lenses\".\n\n"}
{"id": "16144332", "url": "https://en.wikipedia.org/wiki?curid=16144332", "title": "DENIS-P J020529.0-115925", "text": "DENIS-P J020529.0-115925\n\nDENIS-P J020529.0-115925 is a star system in the constellation of Cetus. It is located 64 light-years (19.8 parsecs) away, based on the system's parallax. It was first found in the Deep Near Infrared Survey of the Southern Sky.\n\nThis is a triple star system containing three brown dwarfs: objects that do not have enough mass to fuse hydrogen like stars. The two brightest components, designated A and B respectively, are both L-type objects. As of 2003, the two were separated 0.287° along a position angle of 246°.\n\nComponent B was observed to elongated, suggesting a third component. This third component, named C, is a T-type object. It is separated about 1.9 astronomical units (au) from B, and based on a total mass of , the two may orbit each other every 8 years.\n\n"}
{"id": "18423237", "url": "https://en.wikipedia.org/wiki?curid=18423237", "title": "Eurasian Astronomical Society", "text": "Eurasian Astronomical Society\n\nEurasian Astronomical Society (EAAS) is a scientific society, which comprises professional astronomers from the former Soviet republics, Europe, Israel and United States. It was founded in 1990. The governing bodies are placed in Moscow at the State Astronomical Institute named after P.K. Shternberg.\n\n\n"}
{"id": "23930072", "url": "https://en.wikipedia.org/wiki?curid=23930072", "title": "European Platform of Women Scientists", "text": "European Platform of Women Scientists\n\nThe European Platform of Women Scientists EPWS is an umbrella organisation bringing together networks of women scientists and organisations committed to gender equality in research in all disciplines in Europe 27 and the countries associated to the European Union’s Framework Programmes for Research and Technological Development. The Platform welcomes researchers working in any discipline and working in science in its widest sense, ranging from natural to social sciences, including, but not restricted to, science, engineering and technology. EPWS currently counts more than 100 member organisations, together working for more than 12.000 women researchers all over Europe active in academia and in industrial research.\n\nLegally established as an international non-profit organisation under Belgian law (AISBL) in November 2005 and governed by an international, multidisciplinary Board of Administration of 11 high ranking women scientists, EPWS constitutes a new strategic instrument in European research policy, complementing various initiatives taken at the European level to ensure a better participation of women scientists in research and in the research policy process as well as the inclusion of the gender dimension in research.\n\nThe Platform’s main goals are to:\n\n"}
{"id": "41174204", "url": "https://en.wikipedia.org/wiki?curid=41174204", "title": "Five Billion Years of Solitude", "text": "Five Billion Years of Solitude\n\nFive Billion Years of Solitude: The Search for Life Among the Stars is a non-fiction work by the science author Lee Billings. The text was initially published on October 3, 2013 by Current.\n\nIn this book, Billings explores the scientists and science behind the ever-expanding universe of exoplanets. Since the first detection of a planet orbiting another Sun-like star in 1995, scientists have discovered an increasing number of worlds beyond our solar system through detections by telescopes and spacecraft. Billings reveals the scientists behind these discoveries and their thoughts on not only exoplanets, but also their triumphs and frustrations in their quest to solve one of the greatest mysteries of humankind: Are we alone? Billings includes interviews with Frank Drake, Geoffrey Marcy, Greg Laughlin, James Kasting, Matt Mountain, Wesley Traub, Sara Seager, and many other prominent researchers.\n\nThe book has 10 chapters:\n\n\n\n"}
{"id": "271269", "url": "https://en.wikipedia.org/wiki?curid=271269", "title": "Gakkel Ridge", "text": "Gakkel Ridge\n\nThe Gakkel Ridge (formerly known as the Nansen Cordillera and Arctic Mid-Ocean Ridge) is a mid-oceanic ridge, a divergent tectonic plate boundary between the North American Plate and the Eurasian Plate. It is located in the Eurasian Basin of the Arctic Ocean, between Greenland and Siberia, and has a length of about 1,800 kilometers. Geologically, it connects the northern end of the Mid-Atlantic Ridge with the Laptev Sea Rift.\n\nThe existence and approximate location of the Gakkel Ridge were predicted by Soviet polar explorer Yakov Yakovlevich Gakkel, and confirmed on Soviet expeditions in the Arctic around 1950. The Ridge is named after him, and the name was recognized in April 1987 by SCUFN (under that body's old name, the Sub-Committee on Geographical Names and Nomenclature of Ocean Bottom Features).\n\nThe ridge is the slowest known spreading ridge on earth, with a rate of less than one centimeter per year. Until 1999, it was believed to be non-volcanic; that year, scientists operating from a nuclear submarine discovered active volcanoes along it. In 2001 two research icebreakers, the German \"Polarstern\" and the American \"Healy\", with several groups of scientists, cruised to the Gakkel Ridge to explore it and collect petrological samples. Among other discoveries, this expedition found evidence of hydrothermal vents. In 2007, Woods Hole Oceanographic Institution conducted the \"Arctic Gakkel Vents Expedition\" (AGAVE), which made some unanticipated discoveries, including the unconsolidated fragmented pyroclastic volcanic deposits that cover the axial valley of the ridge (whose area is greater than 10 km). These suggest volatile substances in concentrations ten times those in the magmas of normal mid-ocean ridges. Using \"free-swimming\" robotic submersibles on the Gakkel ridge, the AGAVE expedition also discovered what they called \"bizarre 'mats' of microbial communities containing a half dozen or more new species\".\n\nThe Gakkel ridge is remarkable in that is not offet by any transform faults. The ridge does have segments with variable orientation and varying degrees of volcanism: the Western Volcanic Zone From the Lena trough, 7° W, to 3° E longitude), the Sparsely Magmatic Zone (from 3° E to 29° E longitude), and the Eastern Magmatic Zone (from 29° E to 89°E) . The gaps of volcanic activity imply very cold crust and mantle, probably related to the very low spreading rate, but it is not yet know why some parts of the ridge are more magmatic than others. Some earthquakes have been detected from the mantle, below the crust, which is very unusual for a mid-ocean ridge . It confirms that the mantle and crust of Gakkel ridge, like some segments of the Southwest Indian Ridge, are very cold.\n\n\n\n\n"}
{"id": "3646668", "url": "https://en.wikipedia.org/wiki?curid=3646668", "title": "Ground substance", "text": "Ground substance\n\nGround substance is an amorphous gel-like substance in the extracellular space that contains all components of the extracellular matrix (ECM) except for fibrous materials such as collagen and elastin. Ground substance is active in the development, movement, and proliferation of tissues, as well as their metabolism. Additionally, cells use it for support, water storage, binding, and a medium for intercellular exchange (especially between blood cells and other types of cells). Ground substance provides lubrication for collagen fibers.\n\nThe components of the ground substance vary depending on the tissue. Ground substance is primarily composed of water, glycosaminoglycans (GAGs) such as hyaluronic acid, heparan sulfate, dermatan sulfate, and chondroitin sulfate, proteoglycans which GAGs are bound to, and glycoproteins. Components of the ground substance are secreted by fibroblasts. Usually it is not visible on slides, because it is lost during staining in the preparation process.\n\nLink proteins such as vinculin, spectrin and actomyosin stabilize the proteoglycans and organize elastic fibers in the ECM. Changes in the density of ground substance can allow collagen fibers to form aberrant cross-links. Loose connective tissue is characterized by few fibers and cells, and a relatively large amount of ground substance. Dense connective tissue has a smaller amount of ground substance compared to the fibrous material.\n\nThe meaning of the term has evolved over time.\n\n"}
{"id": "59218609", "url": "https://en.wikipedia.org/wiki?curid=59218609", "title": "Hanneke Jansen", "text": "Hanneke Jansen\n\nJohanna Maria \"Hanneke\" Jansen is a computational chemistry leader working at Novartis on multiple drug targets. She previously worked at Astra and at Chiron Corporation.\n\nJansen received her doctoral degree from the University of Groningen (The Netherlands) in 1995, studying computational medicinal chemistry. Her dissertation topic concerned 3D modeling of the melatonin receptor, including work on synthesizing and separating analytes to probe its chemistry. She completed a postdoctoral fellowship at Uppsala University in 1997. Her work there related to modeling receptor interactions of drug leads to judge their serotonergic or dopaminergic activities.\n\nMuch of Jansen's oeuvre uses \"in silico\" methods to study receptor biology, drug design, and drug-protein interactions. An early study (2000) while she was employed at Chiron looked at conformations of the anti-cancer treatment Taxol in nonpolar environments.\n\nIn a 2012 commentary for \"Future Medicinal Chemistry\", Jansen led a team of distinguished computational chemists in a call to action, namely that standardized data sets be used across the industry, and that sharing program code between groups at different companies and institutions should be mandated.\n\nA 2017 open-access study in \"PLoS One\" related some of Novartis' work - spearheaded by Jansen - to study the oncogenic protein RAS through inhibitors that targeted its inactive conformations.\n\nJansen has served the ACS division on Computers in Chemistry since at least 2007, holding multiple leadership positions. Outside of pharmaceutical work, she is perhaps best known as a co-Founder and Steering Committee member for the Teach-Discover-Treat initiative, which creates challenges for students and young professionals to design drugs against neglected diseases such as malaria or \"Trypanosoma\" using computational chemistry shared data sets and screens.\n\n"}
{"id": "47561162", "url": "https://en.wikipedia.org/wiki?curid=47561162", "title": "History of Sulzer diesel engines", "text": "History of Sulzer diesel engines\n\nThis article covers the History of Sulzer diesel engines from 1898 to 1997. Sulzer Brothers foundry was established in Winterthur, Switzerland, in 1834 by Johann Jakob Sulzer-Neuffert and his two sons, Johann Jakob and Salomon. Products included cast iron, firefighting pumps and textile machinery. Co-operation with Rudolf Diesel led to the construction of the first Sulzer diesel engine in 1898. In 2015, the Sulzer company lives on but it no longer manufactures diesel engines, having sold the diesel engine business to Wärtsilä in 1997. \nSulzer built diesel engines for stationary, road, rail and marine use. The engine types usually comprise a number, then some letters, then another number. For example, 6LDA28 indicates a six-cylinder engine in the \"LDA\" series with a 28cm cylinder bore.\n\nTwo cylinder engine at King Edward Mine, Camborne(http://kingedwardmine.co.uk/) original application unknown.\n\nIn 1937, Sulzer introduced an opposed piston two-stroke diesel engine for road use. This was similar to the Commer TS3 but had a piston-type blower instead of a Roots blower. It was made in two sizes: 69mm bore x 101.6mm stroke or 89mm bore by 120mm stroke. The smaller version had two cylinders, produced 35 hp, and was intended for tractors. The larger version was available with two, three or four cylinders and was intended for trucks.\n\nThese are examples, not a full list.\n\n\n\n\n\n\nThese are examples, not a full list.\n\n\n\nLicences to build diesel engines to Sulzer's design were granted to Vickers-Armstrongs in the United Kingdom, to Busch-Sulzer in the United States, to Reșița works in Romania and to H. Cegielski – Poznań in Poland.\n"}
{"id": "37521969", "url": "https://en.wikipedia.org/wiki?curid=37521969", "title": "Homoiohydry", "text": "Homoiohydry\n\nHomoiohydry is the capacity of plants to regulate, or achieve homeostasis of, cell and tissue water content. Homoiohydry evolved in land plants to a lesser or greater degree during their transition to land more than 500 million years ago, and is most highly developed in the vascular plants. It is the consequence of a suite of morphological innovations and strategies that enable plant shoots exploring aerial environments to conserve water by internalising the gas exchange surfaces, enclosing them in a waterproof membrane and providing a variable aperture control mechanism, the stomatal guard cells, which regulate the rates of water transpiration and CO exchange. In vascular plants, water is acquired from the soil by roots and transported via the xylem to aerial portions of the plant. Water evaporation from the aerial surfaces of the plant is controlled by a waterproof covering of cuticle. Gas exchange with the atmosphere is controlled by stomata, which can open and close to control water loss, and diffusion of carbon dioxide to the chloroplasts takes place in intercellular spaces between chlorenchyma cells in the stem or in the mesophyll tissue of the leaf.\n\nThe antonym of homoiohydry is poikilohydry, a condition in which plant water content is passively reduced or increased in equilibrium with environmental water status.\n"}
{"id": "27150250", "url": "https://en.wikipedia.org/wiki?curid=27150250", "title": "Information strategist", "text": "Information strategist\n\nAn information strategist analyses the information flow within an organisation and directs its information resources to better serve the organisation's strategic goals. They work with information technology to direct high quality information from a variety of sources to users, based upon their profiles and needs. In warfare, information strategists not only seek to improve information flows for their own side but also try to disrupt the information flows of the enemy in order to demoralize and deceive them.\n"}
{"id": "53670818", "url": "https://en.wikipedia.org/wiki?curid=53670818", "title": "International Physics and Culture Olympiad", "text": "International Physics and Culture Olympiad\n\nThe International Physics and Culture Olympiad (IPhCO) is a physics and culture competition for high school students. In June 2016, the IPhCO beta version was held in Curitiba, Brazil at the Federal Institute of Paraná (IFPR). The first IPhCO was held in April 2017.\n\nThe problems are based on high school physics and mathematics, but indirectly works with other areas such as: Geography, History, Arts, Biology etc. The competition is online. There are 12 problems in total and the student has 24 hours to solve each problem. Always at midnight, a new problem is released. Enrolled students receive passwords that open the portals that give access to problems.\n\nThe student can be awarded three times, according to the stages due. Each step is related to a number of problems solved successfully. The student can receive up to three postcards of Curitiba with the conquered key printed.\n\nAn important goal of the Olympiad is to value culture, arousing young people's interest in the history of Brazil and other countries. To do so, the Olympics takes students on virtual walks in places where interesting events have occurred or are remembered, such as in museums, for example. With the objective of offering more visibility to the cultural spaces of Curitiba and the state of Paraná, the Olympiad began a mapping work of those spaces that were not yet in Street View. In collaboration with Google, which provided equipment for photographing, this work began in October 2017.\nThe main venue of the Olympiad is in Curitiba, Parana, where the Olympiad began in 2017. Other cities in Brazil and the world will gradually incorporate the network of collaboration and culture.\n\n\n"}
{"id": "5487391", "url": "https://en.wikipedia.org/wiki?curid=5487391", "title": "Intraculturalism", "text": "Intraculturalism\n\nIntraculturalism is the study of behavior within one cultural group. For example, value variations among Palestinians are intracultural. This is often part of Subaltern Studies, development studies and sociology.\n"}
{"id": "27954240", "url": "https://en.wikipedia.org/wiki?curid=27954240", "title": "John Senex", "text": "John Senex\n\nJohn Senex (1678, Ludlow, Shropshire – died 1740, London) was an English cartographer, engraver and explorer.\n\nHe was also an astrologer, geologist, and geographer to Queen Anne of Great Britain, editor and seller of antique maps and most importantly creator of the pocket-size map of the world. He owned a business on Fleet Street, where he sold maps.\n\nHe was one of the principal cartographers of the 18th century. He started his apprenticeship with Robert Clavell, at the Stationers Company, in 1692. Senex is famous for his maps of the world, some of which have added elevations, and which feature minuscule detailed engravings. Many of these maps can be found in museum collections; rarely, copies are available for private sale. Some copies are held in the National Maritime Museum; many of his maps are now in the possession of Trinity College Dublin. Having worked and collaborated with Charles Price, Senex created a series of engravings for the London Almanacs and in 1714 he published together with Maxwell an English Atlas. In 1719 he published a miniature edition of \"Brittania\" by Ogilby. He became particularly interested in depicting California as an island instead of part of mainland North America, a trait which makes many of his maps appealing to collectors. In 1721 he published a new general atlas. He used the work of cartographer Guillaume de L’Isle as an influence.\n\nIn 1728 Senex was elected into the Fellowship of the Royal Society of London.\n\n\n"}
{"id": "17785091", "url": "https://en.wikipedia.org/wiki?curid=17785091", "title": "Light-cone coordinates", "text": "Light-cone coordinates\n\nIn special relativity, light-cone coordinates is a special coordinate system where two of the coordinates, x and x are null coordinates and all the other coordinates are spatial. Call them formula_1.\n\nAssume we are working with a (d,1) Lorentzian signature.\n\nInstead of the standard coordinate system (using Einstein notation)\nwith formula_3 we have\nwith formula_5, formula_6 and formula_7.\n\nBoth x and x can act as \"time\" coordinates.\n\nOne nice thing about light cone coordinates is that the causal structure is partially included into the coordinate system itself.\n\nA boost in the tx plane shows up as formula_8, formula_9, formula_10. A rotation in the ij-plane only affects formula_1. The parabolic transformations show up as formula_12, formula_13, formula_14. Another set of parabolic transformations show up as formula_15, formula_16 and formula_17.\n\nLight cone coordinates can also be generalized to curved spacetime in general relativity. Sometimes calculations simplify using light cone coordinates. See Newman–Penrose formalism.\n\nLight cone coordinates are sometimes used to describe relativistic collisions, especially if the relative velocity is very close to the speed of light. They are also used in the light cone gauge of string theory.\n\n"}
{"id": "25496530", "url": "https://en.wikipedia.org/wiki?curid=25496530", "title": "List of Superfund sites in Montana", "text": "List of Superfund sites in Montana\n\nThis is a list of Superfund sites in Montana designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). The NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of March 10, 2011, there were 16 Superfund sites on the National Priorities List in Montana. One additional site has been proposed for entry on the list. No sites have yet been removed from the list following clean up.\n\n\n"}
{"id": "14485913", "url": "https://en.wikipedia.org/wiki?curid=14485913", "title": "List of members of the National Academy of Sciences (Physiology and pharmacology)", "text": "List of members of the National Academy of Sciences (Physiology and pharmacology)\n"}
{"id": "33820413", "url": "https://en.wikipedia.org/wiki?curid=33820413", "title": "List of plasma physics articles", "text": "List of plasma physics articles\n\nThis is a list of plasma physics topics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7120024", "url": "https://en.wikipedia.org/wiki?curid=7120024", "title": "List of volcanoes in Georgia (country)", "text": "List of volcanoes in Georgia (country)\n\nThis is a list of active and extinct volcanoes in Georgia. \n"}
{"id": "7120272", "url": "https://en.wikipedia.org/wiki?curid=7120272", "title": "List of volcanoes in Tanzania", "text": "List of volcanoes in Tanzania\n\nThis is a list of active and extinct volcanoes in Tanzania.\n"}
{"id": "37375797", "url": "https://en.wikipedia.org/wiki?curid=37375797", "title": "MHW-RTG", "text": "MHW-RTG\n\nThe Multihundred-Watt radioisotope thermoelectric generators (MHW RTG) is a type of US radioisotope thermoelectric generator (RTG) developed for the Voyager spacecrafts, \"Voyager 1\" and \"Voyager 2\".\n\nEach RTG had a total weight of 37.7 kg including about 4.5 kg of Pu-238. It uses 24 pressed plutonium-238 oxide spheres and provides enough heat to generate approximately 157 watts of electrical power initially - halving every 87.7 years.\n\nEach RTG generated about 2400 Watts of thermal power.\nConversion of the decay heat of the plutonium to electrical power used 312 silicon-germanium\n(SiGe) thermoelectric couples. The initial thermoelectric couple hot junction temperature was 1273 K (1000 °C, 1832 °F) with a cold junction temperature of 573 K (300 °C, 572 °F).\n\nEach Voyager spacecraft has 3 RTGs. Collectively, the RTGs supplied each Voyager spacecraft with 470 watts at launch.\n\nMHW-RTGs were used on the Lincoln Experimental Satellites 8 and 9.\n\nSubsequent US spacecraft used the GPHS-RTG which used similar SiGe thermoelectic devices but a different packaging of the fuel.\n\nThe MMRTG is a newer RTG type, used on the Curiosity rover.\n\n"}
{"id": "86631", "url": "https://en.wikipedia.org/wiki?curid=86631", "title": "MISTRAM", "text": "MISTRAM\n\nMISTRAM (\"MISsile TRAjectory Measurement\") was a high-resolution tracking system used by the United States Air Force (and later NASA) to provide highly detailed trajectory analysis of rocket launches.\n\nA \"classic\" ranging system used since the 1960s uses radar to time a radio signal's travel to a target (in this case, the rocket) and back. This technique is accurate to approximately 1%. The accuracy of this technique is limited by the need to create a sharp \"pulse\" of radio so that the start of the signal can be accurately defined. There are both practical and theoretical limits to the sharpness of the pulse. In addition, the timing of the signals often introduced inaccuracies of its own until the introduction of high precision clocks.\n\nIn MISTRAM, this was avoided by broadcasting a continuous signal. The basic system used a ground station located down range from the launch site (at Valkaria, Florida and Eleuthera Island, Bahamas) and a transponder on the vehicle. The tracking station transmitted an X-band carrier signal which the transponder responded to by re-broadcasting it on another (shifted) frequency. By slowly changing the frequency of the carrier broadcast from the station and comparing this with the phase of the signal being returned, ground control could measure the distance to the vehicle very accurately. Even with the analog circuitry used, MISTRAM was accurate to less than 1 km at the distance of the moon.\nTo meet more stringent ballistic missile test requirements, several systems were designed, procured and added to the US Air Force Eastern Range's instrumentation in the 1950s and 1960s. The AZUSA continuous wave tracking system was added to the Cape in the mid-1950s and Grand Bahama in the early 1960s. The AN/FPS-16 radar system was introduced at the Cape, Grand Bahama, San Salvador, Ascension and East Grand Bahama Island between 1958 and 1961. In the early 1960s, the MISTRAM (Missile Trajectory Measurement) system was installed at Valkaria, Florida and Eleuthera island in the Bahamas to support Minuteman missile flights.\n\nMISTRAM is a sophisticated interferometer system consisting of a group of five receiving stations arranged in an L shape. Baselines are . and . The central stations contains a simple tracking antenna. The distance from the central station to the furthest remote station is approximately . Antennas at the central station and the four remote stations follow the flight of a missile and receive signals from its radio beacon.\n\nIn the MISTRAM system, the ground station transmits a carrier to the spacecraft and the spacecraft returns this carrier on another frequency. The ground station sweeps the uplink carrier and the phase shift of the downlink carrier is measured (counted) while it is being swept. The round trip delay time can be shown to be T=(delta-phi)/(delta-f) ; where delta-f is the frequency shift (~4000 Hz for example) and delta-phi the measured phase shift in radians. Suppose T=2 sec (~lunar distance) then delta-phi=8000 radians, i.e. (8000*180)/Pi. Assume also that the phase can be measured with an accuracy of 1 deg, i.e. means that the range can be determined with a precision of (600000*1*Pi)/(2*8000*180)=0.33 km. An additional carrier quite near the one described above that remained fixed in frequency and used as a phase reference. That carrier and the two frequencies (that the sweep changed between) were generated as multiples of the same basic oscillator frequency. In this way, all signals would have a fixed phase relationship, as was done in MISTRAM. A similar technique was used in the Soviet Luna 20 spacecraft at 183.54 MHz to survey the moon's surface.\n\nMISTRAM was a multistatic long baseline radar interferometer developed for precision measurements of missile trajectories at the US Air Force Eastern Test Range. Multistatic radar systems have a higher complexity with multiple transmitter and receiver subsystems employed in a coordinated manner at more than two sites. All of the geographically dispersed units contribute to the collective target acquisition, detection, position finding and resolution, with simultaneous reception at the receiver sites. In a simpler sense, multistatic radars are systems which have two or more receiving sites with a common spatial coverage area, and data from these coverage areas are combined and processed at a central location. These systems are considered to be multiple bistatic pairs. Multistatic radar systems have various uses, including prevention of jamming and anti-radar munitions.\n\nAlthough this method of measurement is not new, either in theory or in practice, the unique manner in which the techniques were implemented in the MISTRAM system permit measurement of vehicle flight parameters with a degree of precision and accuracy not previously obtainable in other long baseline trajectory measurement systems. To a large extent, this was accomplished by a unique method of transferring intact the phase information in the signals from outlying stations to the central station. A two-way transmission path on each baseline was used to cancel out uncertainties due to variance in ground geometry and temperature.\nThe transmitter at the master or central station generates two CW X-band frequencies, nominally 8148 MHz and 7884 to 7892 MHz. The higher frequency (the range signal) is very stable, whereas the lower frequency (the calibrated signal) is swept periodically over the indicated range. The airborne transponder receives the signals, amplifies & frequency shifts them by 68 MHz, and retransmits back to earth. The Doppler shift is used to determine velocity.\n\nThe Florida MISTRAM system had baselines (~18.9 mi.) with design performance as follows:\n\nThe Transponder receives the two phase-coherent X-band cw signals transmitted from the ground equipment. A klystron with a 68 MHz coherent frequency offset is phase locked to each of the received signals. These klystrons provide the phase coherent return transmission. There are two separate phase locked loops, continuous and calibrate.\n\n\nThe General Electric M236 computer was developed to support MISTRAM and other large military radar projects in the 1960s. (According to Dr. Neelands, certain military people involved in the project were adamant about not relying on \"computers\", therefore this \"information processor\" was developed.) This high speed 36-bit minicomputer was developed by the GE Heavy Military Electronics Department (HMED) in Syracuse, New York, eventually leading to the GE-600 series of mainframe computers. The M236 was designed for real-time processing in a radar-based missile flight measurement system and lacked some general purpose features, such as overlapped instruction processing, the floating point operations needed for Fortran, and operating system support features, such as base and bounds registers. The M-236 computer was developed for the US Air Force Cape Canaveral Missile Range, and installed it at Eleuthera (Bahamas). The 36-bit computer word length was needed for radar tracking computations and for the required exchange of data with an IBM 7094 located at the Cape. The chief architect of the M-236 was John Couleur who will become later a technical leader of the GE large computer systems.\n\nThe debate in favor or against subsequent development of an M236-derived general purpose computer took more than one year and concluded finally with the victory of the M2360 project proponents in February 1963. The GE upper management was impressed by the opportunity to save the rental fees from IBM leased equipment used internally by GE (the cost of development of the new project was estimated to be offset by only one year of rentals). The other GE departments were not very impressed and were reluctant to jettison their IBM machines.\n\nThe GE-600 series was developed by a team led by John Couleur based on work done for the MISTRAM project in 1959. MISTRAM was a missile tracking system that was used on a number of projects (including Project Apollo) and the Air Force required a data-collection computer to be installed in a tracking station downrange from Cape Canaveral. The data would eventually be shared with the 36-bit IBM 7094 machine at the Cape, so the computer would likely have to be 36-bits as well (why they didn't use an IBM 7094 is something of a mystery). GE built a machine called the M236 for the task, and as a result of the 36-bit needs, it ended up acting a lot like the 7094.\n\nThe GE Heavy Military Electronics Department in Syracuse designed and built a tracking system for the ATLAS missile system named MISTRAM that was in fact an advanced computer system. This was quite in accordance with Cordiner's directions since it would not develop a line of machines that be placed on the open market in competition with IBM. (Ralph J. Cordiner was Chairman & CEO of General Electric from 1958 to 1963.) This project also had the advantage that the up front development expenditures were to be paid by the U.S. government rather than GE, an arrangement much more satisfactory to GE's 570 \"bean counters.\" These circumstances brought about the possibility of duplicating the MISTRAM opportunity for the computer department. Much later, the result was an order for 32 computer department machines. However, the MISTRAM computer was the first in a line of developments by John Couleur that led to what may be regarded as the most successful and long-lasting machine - the GE 600 line.\n\nMISTRAM was used in the development and testing of intertial guidance system for the Minuteman ballistic missile, and subsequently was used for testing the Gemini spacecraft and the Saturn V launch system. With the decommissioning of the MISTRAM X-band interferometer at the Air Force Eastern Test Range in 1971, the flight-test community did not have a conventional ground-based range-instrumentation system better than, or comparable to, the inertial guidance systems whose performance was being assessed. This was true in the intervening years preceding GPS development and deployment.\n\nThe first Minuteman missiles (MM I) were launched in the early 1960s from the Air Force Eastern Test Range (AFETR) and were tracked with the AZUSA CW tracking system. The comparatively low quality of the AZUSA tracking data, combined with the rudimentary stage of evaluation techniques, allowed only estimation of the total error; no isolation of individual inertial measurement unit (IMU) error sources was possible.\n\nSubsequent development of improved tracking systems, UDOP and MISTRAM, at AFETR yielded much higher quality velocity tracking profiles. During the Minuteman II flight test program, significant improvements were made in the post-flight evaluation of the IMU accuracy. The most important of these improvement was the introduction of maximum likelihood error estimation using the Kalman algorithm to filter the velocity error profile. Continued improvement of the UDOP and MISTRAM tracking systems and refinement of the evaluation techniques during the Minuteman III flight test program made it possible to gain considerable insight into NS-20A1 IMU error sources.\n\nOne of the major problems in trajectory and orbital estimation is to obtain a realistic estimate of the accuracy of the trajectory and other important parameters. In the orbital case, some of the parameters which may not be solved for are geopotential constants, survey, etc. These factors will affect the total uncertainty in the orbit and, of course, ephemeris predictions. A statistical technique was developed that performs a variance-covariance propagation to obtain accuracy estimates based on random and unmodeled errors. An example of the unmodeled error propagation in the MISTRAM system was given for the Geos B satellite.\n\nDr. Lewis J. Neelands has been called an engineers' engineer by the people who worked with him when he was with the General Electric Corporation Electronics Laboratory and Heavy Military Electronics Department (HMED) in the 1950s and early 60's. His contributions to missile guidance and telemetry made him a key figure in the Altas Guidance and MISTRAM programs, two of HMED's most challenging and successful efforts.\n\nIn retrospect, Neelands said he did not get his greatest satisfaction from his work on the Atlas guidance (about which he said,\"it was successful because of a bunch of other people who put it together and made it work\"). It is MISTRAM, missile tracking and measuring system, that he remembers with greater pride. \"Nothing could match it at the time for the complexity and precision it required,\" he recalls of the real-time measuring system for precisely tracking a missile's flight. One of his colleagues remembers, \"In 1960 he solved the elusive problem of trajectory measurement -- of bringing together at one place for processing, the signals received from widely spaced receiving stations while overcoming inaccuracies due to the propagation anomalies in the medium connecting the stations. A related problem that Lew solved was how to do this using frequencies sufficiently high to develop the required angular measurement accuracy without measurement ambiguities and without requiring a large number of receiving stations to resolve these ambiguities.\" He conceived a system of unprecedented accuracy. The technical work on the Hermes A-3 rocket guidance was headed by Dr. Lewis J. Neelands and resulted in a successful system with the know-how later transferred to another ICBM guidance system known as the 8014 project and also to the highly accurate Mistram instrumentation equipment, all were based on use of a microwave interferometer. Dr. Neelands died at his home in Gainesville Florida on July 17, 2007, at the age of 91.\n\nMISTRAM was designed and developed by the Heavy Military Electronics Division, Defense Systems Department of the General Electric Company, Syracuse, New York, under the sponsorship of the U.S. Air Force Missile Test Center, Patrick Air Force Base, Florida (Contract AF08 (6060) 4891). Mistram I at Valkaria, FL was placed into operation in 1962 and Mistram II at Eleuthera, Bahamas in 1963. The original contract for $15.5M was announced on July 12, 1960.\n\nMISTRAM has been the topic of several dissertations for master of science degrees in engineering.\n"}
{"id": "3641069", "url": "https://en.wikipedia.org/wiki?curid=3641069", "title": "Magellanic Bridge", "text": "Magellanic Bridge\n\nThe Magellanic Bridge (MBR) is a stream of neutral hydrogen that links the two Magellanic Clouds, with a few known stars inside it. \nIt should not be confused with the Magellanic Stream, which links the Magellanic Clouds to the Milky Way. It was discovered in 1963 by J. V. Hindman et al.\n\nThere is a continuous stream of stars throughout the Bridge linking the Large Magellanic Cloud (LMC) with the Small Magellanic Cloud (SMC). This stellar bridge is of greater concentration in the western part. There are two major density clumps, one near the SMC, the other midway between the galaxies, referred to as the \"OGLE Island\".\n\n"}
{"id": "19892268", "url": "https://en.wikipedia.org/wiki?curid=19892268", "title": "Multiplex (assay)", "text": "Multiplex (assay)\n\nIn the biological sciences, a multiplex assay is a type of immunoassay that uses magnetic beads to simultaneously measure multiple analytes in a single experiment. A multiplex assay is a derivative of an ELISA using beads for binding the capture antibody. Multiplex assays are much more common in research than in clinical settings.\n\nIn a multiplex assay, microspheres of designated colors are coated with a specific antibodies. The results can be read by flow cytometry because the beads are distinguishable by fluorescent signature. The number of analytes measured is determined by the number of different bead colors.\n\nMultiplex assays within a given application area or class of technology can be further stratified based on how many analytes can be measured per assay, where \"multiplex\" refers to those with the highest number of analyte measurements per assay (up to millions) and \"low-plex\" or \"mid-plex\" refers to procedures that process fewer (10s to 1000s), though there are no formal guidelines for calling a procedure multi-, mid-, or low-plex based on number of analytes measured. Single-analyte assays or low-to-mid-plex procedures typically predate the rise of their multiplex versions, which often require specialized technologies or miniaturization to achieve a higher degree of parallelization.\n"}
{"id": "263324", "url": "https://en.wikipedia.org/wiki?curid=263324", "title": "Operation Crosstie", "text": "Operation Crosstie\n\nOperation Crosstie was a series of 48 nuclear tests conducted by the United States in 1967–1968 at the Nevada Test Site. These tests followed the \"Operation Latchkey\" series and preceded the \"Operation Bowline\" series.\n\nThe blast designated Gasbuggy involved an underground detonation, intended to stimulate production of natural gas by cracking the rock in the underground formation of its deposit. The test proceeded as expected, but not only did the production not increase as much as expected, but the customers also refused to buy gas contaminated with traces of radioisotopes.\n\nBuggy was a Plowshare test designed to excavate a channel. It was a simultaneous detonation of 5 devices, placed apart and below the surface that resulted in a\nchannel wide,\n900 feet long, and deep\n. Or 65 feet deep and 254 feet wide, according to Declassified U.S. film.\n\nThe USSR conducted a similar salvo-test to investigate the use of nuclear explosions in the construction of the Pechora–Kama Canal project. On March 23, 1971, three simultaneously detonated 15 kiloton underground nuclear charges were exploded in the Taiga test.\n\nThe Faultless test was a calibration test conducted in a mine cavity 3,200 feet beneath the Hot Creek Valley near Tonopah, Nevada, with a yield of around 1 megaton. This test was conducted to see if the land was fit for testing a 5 megaton thermonuclear warhead for the Spartan missile. The test failed because of the large degree of faulting that resulted in the area around the test. It was decided that the land was unfit for multi-megaton nuclear tests, so a similar calibration test was conducted at Amchitka Island, Alaska, in the fall of 1969 during Operation Mandrel.\n\nThe 7.4 foot diameter steel pipe that was used to place the bomb remains at the test site. The top of the pipe was originally flush with the surface, however, the ground sunk by nine feet following the explosion. A plaque is mounted on the exposed pipe to commemorate the event.\nThe United States's Crosstie nuclear test series was a group of 48 nuclear tests conducted in 1967-1968. These tests followed the \"Operation Latchkey\" series and preceded the \"Operation Bowline\" series.\n"}
{"id": "35952023", "url": "https://en.wikipedia.org/wiki?curid=35952023", "title": "Ortega hypothesis", "text": "Ortega hypothesis\n\nThe Ortega hypothesis holds that average or mediocre scientists contribute substantially to the advancement of science. According to this hypothesis, scientific progress occurs mainly by the accumulation of a mass of modest, narrowly specialized intellectual contributions. On this view, major breakthroughs draw heavily upon a large body of minor and little-known work, without which the major advances could not happen.\n\nThe Ortega hypothesis is widely held, but a number of systematic studies of scientific citations have favored the opposing \"Newton hypothesis\", which says that scientific progress is mostly the work of a relatively small number of great scientists (after Isaac Newton's statement that he \"stood on the shoulders of giants\"). The most important papers mostly cite other important papers by a small number of outstanding scientists, suggesting that the breakthroughs do not actually draw heavily on a large body of minor work. Rather, the pattern of citations suggests that most minor work draws heavily on a small number of outstanding papers and outstanding scientists. Even minor papers by the most eminent scientists are cited much more than papers by relatively unknown scientists; and these elite scientists are clustered mostly in a small group of elite departments and universities. The same pattern of disproportionate citation of a small number of scholars appears in fields as diverse as physics and criminology.\n\nThe matter is not settled. No research has established that citation counts reflect the real influence or worth of scientific work. So, the apparent disproof of the Ortega hypothesis may be an artifact of inappropriately chosen data. Stratification within the social networks of scientists may skew the citation statistics. Many authors cite research papers without actually reading them or being influenced by them. Experimental results in physics make heavy use of techniques and devices that have been honed by many previous inventors and researchers, but these are seldom cited in reports on those results. Theoretical papers have the broadest relevance to future research, while reports of experimental results have a narrower relevance but form the basis of the theories. This suggests that citation counts merely favor theoretical results.\n\nThe name of the hypothesis refers to José Ortega y Gasset, who wrote in \"The Revolt of the Masses\" that \"astoundingly mediocre\" men of narrow specialties do most of the work of experimental science. Ortega most likely would have disagreed with the hypothesis that has been named after him, as he held not that scientific progress is driven mainly by the accumulation of small works by mediocrities, but that scientific geniuses create a framework within which intellectually commonplace people can work successfully. For example, Ortega thought that Albert Einstein drew upon the ideas of Immanuel Kant and Ernst Mach to form his own synthesis, and that Einstein did not draw upon masses of tiny results produced systematically by mediocrities. According to Ortega, science is mostly the work of geniuses, and geniuses mostly build on each other's work, but in some fields there is a real need for systematic laboratory work that could be done by almost anyone.\n\nThe \"Ortega hypothesis\" derives only from this last element of Ortega's theory, not the main thrust of it. Ortega characterized this type of research as \"mechanical work of the mind\" that does not require special talent or even much understanding of the results, performed by people who specialize in one narrow corner of one science and hold no curiosity beyond it.\n"}
{"id": "9795767", "url": "https://en.wikipedia.org/wiki?curid=9795767", "title": "Peloton (super computer)", "text": "Peloton (super computer)\n\nAppro was awarded the contract for Peloton which includes the following machines:\n\nAll of the machines run the CHAOS variant of Red Hat Enterprise Linux and the Moab resource management system. Under the project management of John Lee, the team at Synnex, Voltaire, Supermicro and other suppliers, the scientists were able to dramatically reduce the amount of time it took to go from starting the cluster build to actually having hardware here at Livermore in production. In particular, it went from having 4 SUs on the floor on a Thursday, to bring 2 more SUs in for that final cluster and by Saturday, had all of them wired up, burned in, and running Linpack.\n\n \n"}
{"id": "33492223", "url": "https://en.wikipedia.org/wiki?curid=33492223", "title": "Richard Hart Brown", "text": "Richard Hart Brown\n\nRichard Hart Brown (1941–2005) was a founder of Interoperative Neurophysiological Monitoring and a leading expert on Amusement Ride and Roller Coaster safety.\n\nHe was a founder of ASNM | American Soc of Neurophysiologic Monitoring and a charter member of the ABNM | ABNM American Board of Neurophysiologic Monitoring he was a member of many ANSI boards relating to materials and Amusement Ride Safety.\n\n"}
{"id": "10592251", "url": "https://en.wikipedia.org/wiki?curid=10592251", "title": "Science and Technology Facilities Council", "text": "Science and Technology Facilities Council\n\nThe Science and Technology Facilities Council (STFC) is a UK government body that carries out civil research in science and engineering, and funds UK research in areas including particle physics, nuclear physics, space science and astronomy (both ground-based and space-based).\n\nSTFC was formed in April 2007 when Particle Physics and Astronomy Research Council (PPARC), the Council for the Central Laboratory of the Research Councils (CCLRC), along with the nuclear physics activities of the Engineering and Physical Sciences Research Council (EPSRC) were brought under the one umbrella organisation. From 1 November 2016 Brian Bowsher has replaced John Womersley as the CEO of STFC. John Womersley has now moved on to the European Spallation Source as the new Director General. In January 2018, Mark Thomson was announced as the new Executive Chair.\n\nReceiving its funding through the\nscience budget from the Department for Business, Innovation and Skills (BIS), STFC's mission is to \"“To maximise the impact of our knowledge, skills, facilities\nand resources for the benefit of the United Kingdom and its people.”\"\n\n· Universities: STFC supports university-based research, innovation and skills development in particle physics, nuclear physics, space science and astronomy.\n\n· Scientific Facilities: They provide access to world-leading, large-scale facilities across a range of\nphysical and life sciences, enabling research, innovation and skills.\n\n· National Campuses: Working with partners to build National Science and Innovation Campuses based around National Laboratories to promote academic and industrial collaboration and translation of research to market through direct interaction with industry.\n\n· Inspiring and Involving: STFC help create a future pipeline of skilled and enthusiastic young people by using the excitement of our sciences to encourage wider take-up of STEM subjects in school and future life (science, technology, engineering and mathematics).\n\nThe Science and Technology Facilities Council is one of Europe's largest multidisciplinary research organisations supporting scientists and engineers worldwide. Through research fellowships and grants, it is responsible for funding research in UK universities, in the fields of astronomy, particle physics, nuclear physics and space science. STFC operates its own world-class, large-scale research facilities (such as materials research, laser and space science and alternative energy exploration) and provides strategic advice to the UK government on their development. It manages international research projects in support of a broad cross-section of the UK research community and directs, coordinates and funds research, education and training. It is a partner in the UK Space Agency (formerly British National Space Centre or BNSC) providing about 40% of the UK government's expenditure in space science and technology.\n\nIt helps operate/provide access for UK and international scientists to the following large-scale facilities:\n\nSTFC’s budget is allocated annually by the Department for Business, Innovation & Skills.\n\nFor 2015-16 STFC’s budget allocation was £529 million.\n\nSTFC is active in its responsibility for knowledge exchange from government funded civil science into UKPLC. As such, many technologies are licensed to UK companies and spin-out companies created including:\n\n\nHowever knowledge exchange activities are not purely limited to commercialization of technologies, but also cover a wider range of activities which aim to transfer expertise into the wider economy.\n\n"}
{"id": "53031", "url": "https://en.wikipedia.org/wiki?curid=53031", "title": "Stefan–Boltzmann law", "text": "Stefan–Boltzmann law\n\nThe Stefan–Boltzmann law describes the power radiated from a black body in terms of its temperature. Specifically, the Stefan–Boltzmann law states that the total energy radiated per unit surface area of a black body across all wavelengths per unit time formula_1 (also known as the black-body \"radiant emittance\") is directly proportional to the fourth power of the black body's thermodynamic temperature \"T\":\n\nThe constant of proportionality \"σ\", called the Stefan–Boltzmann constant, is derived from other known physical constants. The value of the constant is\n\nwhere \"k\" is the Boltzmann constant, \"h\" is Planck's constant, and \"c\" is the speed of light in a vacuum. The radiance (watts per square metre per steradian) is given by\n\nA body that does not absorb all incident radiation (sometimes known as a grey body) emits less total energy than a black body and is characterized by an emissivity, formula_5:\n\nThe radiant emittance formula_1 has dimensions of energy flux (energy per time per area), and the SI units of measure are joules per second per square metre, or equivalently, watts per square metre. The SI unit for absolute temperature \"T\" is the kelvin. \"formula_8\" is the emissivity of the grey body; if it is a perfect blackbody, formula_9. In the still more general (and realistic) case, the emissivity depends on the wavelength, formula_10.\n\nTo find the total power radiated from an object, multiply by its surface area, formula_11:\n\nWavelength- and subwavelength-scale particles, metamaterials, and other nanostructures are not subject to ray-optical limits and may be designed to exceed the Stefan–Boltzmann law.\n\nThe law was deduced by Josef Stefan (1835–1893) in 1879 on the basis of experimental measurements made by John Tyndall and was derived from theoretical considerations, using thermodynamics, by Ludwig Boltzmann (1844–1906) in 1884. Boltzmann considered a certain ideal heat engine with light as a working matter instead of a gas. The law is highly accurate only for ideal black objects, the perfect radiators, called black bodies; it works as a good approximation for most \"grey\" bodies. Stefan published this law in the article \"Über die Beziehung zwischen der Wärmestrahlung und der Temperatur\" (\"On the relationship between thermal radiation and temperature\") in the \"Bulletins from the sessions\" of the Vienna Academy of Sciences.\n\nWith his law Stefan also determined the temperature of the Sun's surface. He inferred from the data of Jacques-Louis Soret (1827–1890) that the energy flux density from the Sun is 29 times greater than the energy flux density of a certain warmed metal lamella (a thin plate). A round lamella was placed at such a distance from the measuring device that it would be seen at the same angle as the Sun. Soret estimated the temperature of the lamella to be approximately 1900 °C to 2000 °C. Stefan surmised that ⅓ of the energy flux from the Sun is absorbed by the Earth's atmosphere, so he took for the correct Sun's energy flux a value 3/2 times greater than Soret's value, namely 29 × 3/2 = 43.5.\n\nPrecise measurements of atmospheric absorption were not made until 1888 and 1904. The temperature Stefan obtained was a median value of previous ones, 1950 °C and the absolute thermodynamic one 2200 K. As 2.57 = 43.5, it follows from the law that the temperature of the Sun is 2.57 times greater than the temperature of the lamella, so Stefan got a value of 5430 °C or 5700 K (the modern value is 5778 K). This was the first sensible value for the temperature of the Sun. Before this, values ranging from as low as 1800 °C to as high as 13,000,000 °C were claimed. The lower value of 1800 °C was determined by Claude Pouillet (1790–1868) in 1838 using the Dulong–Petit law. Pouillet also took just half the value of the Sun's correct energy flux.\n\nThe temperature of stars other than the Sun can be approximated using a similar means by treating the emitted energy as a black body radiation. So:\n\nwhere \"L\" is the luminosity, \"σ\" is the Stefan–Boltzmann constant, \"R\" is the stellar radius and \"T\" is the effective temperature. This same formula can be used to compute the approximate radius of a main sequence star relative to the sun:\n\nwhere formula_15 is the solar radius, formula_16 is the solar luminosity, and so forth.\n\nWith the Stefan–Boltzmann law, astronomers can easily infer the radii of stars. The law is also met in the thermodynamics of black holes in so-called Hawking radiation.\n\nSimilarly we can calculate the effective temperature of the Earth \"T\" by equating the energy received from the Sun and the energy radiated by the Earth, under the black-body approximation (Earth's own production of energy being small enough to be negligible). The luminosity of the Sun, \"L\", is given by:\n\nAt Earth, this energy is passing through a sphere with a radius of \"a\", the distance between the Earth and the Sun, and the irradiance (received power per unit area) is given by\n\nThe Earth has a radius of \"R\", and therefore has a cross-section of formula_19. The radiant flux (i.e. solar power) absorbed by the Earth is thus given by:\n\nBecause the Stefan–Boltzmann law uses a fourth power, it has stabilizing effect on the exchange and the flux emitted by Earth tends to be equal the flux absorbed, close to the steady state where:\n\n\"T\" can then be found:\n\nwhere \"T\" is the temperature of the Sun, \"R\" the radius of the Sun, and \"a\" is the distance between the Earth and the Sun. This gives an effective temperature of 6 °C on the surface of the Earth, assuming that it perfectly absorbs all emission falling on it and has no atmosphere.\n\nThe Earth has an albedo of 0.3, meaning that 30% of the solar radiation that hits the planet gets scattered back into space without absorption. The effect of albedo on temperature can be approximated by assuming that the energy absorbed is multiplied by 0.7, but that the planet still radiates as a black body (the latter by definition of effective temperature, which is what we are calculating). This approximation reduces the temperature by a factor of 0.7, giving 255 K (−18 °C).\n\nThe above temperature is Earth's as seen from space, not ground temperature but an average over all emitting bodies of Earth from surface to high altitude. Because of greenhouse effect, the Earth's actual average surface temperature is about 288 K (15 °C), which is higher than the 255 K effective temperature, and even higher than the 279 K temperature that a black body would have.\n\nIn the above discussion, we have assumed that the whole surface of the earth is at one temperature. Another interesting question is to ask what the temperature of a blackbody surface on the earth would be assuming that it reaches equilibrium with the sunlight falling on it. This of course depends on the angle of the sun on the surface and on how much air the sunlight has gone through. When the sun is at the zenith and the surface is horizontal, the irradiance can be as high as 1120 W/m. The Stefan–Boltzmann law then gives a temperature of\n\nor 102 °C. (Above the atmosphere, the result is even higher: 394 K.) We can think of the earth's surface as \"trying\" to reach equilibrium temperature during the day, but being cooled by the atmosphere, and \"trying\" to reach equilibrium with starlight and possibly moonlight at night, but being warmed by the atmosphere.\n\nThe fact that the energy density of the box containing radiation is proportional to formula_24 can be derived using thermodynamics. This derivation uses the relation between the radiation pressure \"p\" and the internal energy density formula_25, a relation that can be shown using the form of the electromagnetic stress–energy tensor. This relation is:\n\nNow, from the fundamental thermodynamic relation\n\nwe obtain the following expression, after dividing by formula_28 and fixing formula_29 :\n\nThe last equality comes from the following Maxwell relation:\n\nFrom the definition of energy density it follows that\n\nwhere the energy density of radiation only depends on the temperature, therefore\n\nNow, the equality\n\nafter substitution of formula_35 and formula_36 for the corresponding expressions, can be written as\n\nSince the partial derivative formula_38 can be expressed as a relationship between only formula_39 and formula_29 (if one isolates it on one side of the equality), the partial derivative can be replaced by the ordinary derivative. After separating the differentials the equality becomes\n\nwhich leads immediately to formula_42, with formula_43 as some constant of integration.\n\nThe law can be derived by considering a small flat black body surface radiating out into a half-sphere. This derivation uses spherical coordinates, with \"θ\" as the zenith angle and \"φ\" as the azimuthal angle; and the small flat blackbody surface lies on the xy-plane, where \"θ\" = /.\n\nThe intensity of the light emitted from the blackbody surface is given by Planck's law :\n\nThe quantity formula_50 is the power radiated by a surface of area A through a solid angle \"dΩ\" in the frequency range between \"ν\" and \"ν\" + \"dν\".\n\nThe Stefan–Boltzmann law gives the power emitted per unit area of the emitting body,\n\nNote that the cosine appears because black bodies are \"Lambertian\" (i.e. they obey Lambert's cosine law), meaning that the intensity observed along the sphere will be the actual intensity times the cosine of the zenith angle.\nTo derive the Stefan–Boltzmann law, we must integrate \"dΩ\" = sin(\"θ\") \"dθ dφ\" over the half-sphere and integrate \"ν\" from 0 to ∞. \n\nThen we plug in for \"I\":\n\nTo evaluate this integral, do a substitution,\n\nwhich gives:\n\nThe integral on the right is standard and goes by many names: it is a particular case of a Bose–Einstein integral, or the Riemann zeta function, formula_56, or the polylogarithm. The value of the integral is formula_57, giving the result that, for a perfect blackbody surface:\n\nFinally, this proof started out only considering a small flat surface. However, any differentiable surface can be approximated by a collection of small flat surfaces. So long as the geometry of the surface does not cause the blackbody to reabsorb its own radiation, the total energy radiated is just the sum of the energies radiated by each surface; and the total surface area is just the sum of the areas of each surface—so this law holds for all convex blackbodies, too, so long as the surface has the same temperature throughout. The law extends to radiation from non-convex bodies by using the fact that the convex hull of a black body radiates as though it were itself a black body.\n\nThe total energy density \"U\" can be similarly calculated, except the integration is over the whole sphere and there is no cosine, and the energy flux should be divided by the velocity \"c\":\nThus formula_60 is replaced by formula_61, giving an extra factor of 4.\n\nThus, in total:\n\n\n"}
{"id": "422180", "url": "https://en.wikipedia.org/wiki?curid=422180", "title": "Telescope mount", "text": "Telescope mount\n\nA telescope mount is a mechanical structure which supports a telescope. Telescope mounts are designed to support the mass of the telescope and allow for accurate pointing of the instrument. Many sorts of mounts have been developed over the years, with the majority of effort being put into systems that can track the motion of the fixed stars as the Earth rotates.\n\nFixed telescope mounts are entirely fixed in one position, such as Zenith telescopes that point only straight up and the National Radio Astronomy Observatory's Green Bank fixed radio 'horn' built to observe Cygnus X-1.\n\nFixed-altitude mounts usually have the primary optics fixed at an altitude angle while rotating horizontally (in azimuth). They can cover the whole sky but only observe objects for the short time when that object passes a specific altitude and azimuth.\n\nTransit mounts are single axis mounts fixed in azimuth while rotating in altitude, usually oriented on a north-south axis. This allows the telescope to view the whole sky, but only when the Earth's rotation allows the objects to cross (\"transit\") through that narrow north-south line (the meridian). This type of mount is used in transit telescopes, designed for precision astronomical measurement. Transit mounts are also used to save on cost or where the instruments mass makes movement on more than one axis very difficult, such as large radio telescopes.\n\nAltazimuth, altitude-azimuth, or \"alt-az mounts\" allow telescopes to be moved in altitude (up and down), or azimuth (side to side), as separate motions. This mechanically simple mount was used in early telescope designs and until the second half of the 20th century was used as a \"less sophisticated\" alternative to equatorial mounts since it did not allow tracking of the night sky. This meant until recently it was normally used with inexpensive commercial and hobby constructions. Since the invention of digital tracking systems, altazimuth mounts have come to be used in practically all modern large research telescopes. Digital tracking has also made it a popular telescope mount used in amateur astronomy.\n\nBesides the mechanical inability to easily follow celestial motion the altazimuth mount does have other limitations. The telescope's field-of-view rotates at varying speed as the telescope tracks, whilst the telescope body does not, requiring a system to counter-rotate the field of view when used for astrophotography or other types of astronomical imaging. The mount also has blind spot or \"zenith hole\", a spot near the zenith where the tracking rate in the azimuth coordinate becomes too high to accurately follow equatorial motion. These mounts also require a third axis to de-rotate the field as the telescope tracks.\n\nAlt-alt mounts, or altitude-altitude mounts, are designs similar to horizontal equatorial yoke mounts or Cardan suspension gimbals. This mount is an alternative to the altazimuth mount that has the advantage of not having a blind spot near the zenith, and for objects near the celestial equator the field rotation is minimized. It has the disadvantage of having all the mass, complexity, and engineering problems of its equatorial counterpart, so is only used in specialty applications such as satellite tracking. These mounts may include a third azimuth axis (an \"altitude-altitude-azimuth mount\") to rotate the entire mount into an orientation that allows smoother tracking.\n\nThe equatorial mount has north-south \"polar axis\" tilted to be parallel to Earth's polar axis that allows the telescope to swing in an east-west arc, with a second axis perpendicular to that to allow the telescope to swing in a north-south arc. Slewing or mechanically driving the mount's polar axis in a counter direction to the Earth's rotation allows the telescope to accurately follow the motion of the night sky. Equatorial mounts come in different shapes, include German equatorial mounts (GEM in short), equatorial fork mounts, mixed variations on yoke or cross-axis mounts, and equatorial platforms such as the Poncet Platform.\n\nTilting the polar axis adds a level of complexity to the mount. Mechanical systems have to be engineered to support one or both ends of this axis (such as in fork or yoke mounts). Designs such as German equatorial or cross axis mounts also need large counter weights to counterbalance the mass of the telescope. Larger domes and other structures are also needed to cover the increased mechanical size and range of movement of equatorial mounts. Because of this, equatorial mounts become less viable in very large telescopes and have been pretty much replaced by altazimuth mounts for those applications.\n\nInstead of the classical mounting using two axles, the mirror is supported by six extendable struts (Stewart-Gough platform). This configuration allows moving the telescope in all six spatial degrees of freedom and also provides a strong structural integrity.\n\n"}
{"id": "48900833", "url": "https://en.wikipedia.org/wiki?curid=48900833", "title": "The Penultimate Curiosity", "text": "The Penultimate Curiosity\n\nThe Penultimate Curiosity: How science swims in the slipstream of ultimate questions is a book jointly written by English author and artist Roger Wagner and English scientist Andrew Briggs, which sets out to answer one of the most important, vexed, and profound questions about the development of human thought: \"‘What lies at the root of the long entanglement between science and religion?’\"\n\nIn a prologue Wagner and Briggs begin by describing the entrances to the University Museum in Oxford and the Cavendish Laboratory in Cambridge. On the former there is a sculpture of an angel, on the latter a quotation from psalm 111: \"‘the works of the Lord are great sought out of all them that have pleasure therein’\". Their book, they suggest, is an attempt to answer the question of how this sculpture and inscription got there. Rather than directly addressing the question of whether science and religion are compatible, Wagner and Briggs examine the nature of the relationship between them.\n\nTheir first move is to consider the connection between the earliest evidences of human religion and early evidences of interest in the natural world. Drawing on recent discoveries of cave art and developments in the cognitive science of religion, they suggest that when the need to make sense of the world as a whole (‘ultimate curiosity’) began to become central to maintaining the coherence of human communities, this created a kind of slipstream in which various kinds of interest in the natural world (penultimate curiosities) were able to travel. They further suggest that particular configurations of this ‘slipstream’ (particular ways of making sense of the world) have been especially conducive to motivating an interest in the natural world.\n\nTheir second move is then to follow the way that particular religious ideas have shaped and motivated scientific thinking. They describe the way that the development in Greek religious thought of the idea of a divine arche – a source or principle – giving a rational coherence to the universe, influenced Greek scientific thinking for almost a thousand years. They then go on to describe the interaction between Greek thinking and early Jewish and Christian thought. Their focus here is on the Alexandrian Christian philosopher John Philoponus, and his argument that the heavens and the earth are made of the same materials and may be governed by the same principle.\n\nFrom there Wagner and Briggs go on to follow these ideas through Islamic and medieval Christian thought, and it is in respect of the latter that another theme begins to emerge. Their original slipstreaming metaphor suggested that religious ideas could motivate scientific thinking. However when science is made to answer religious questions or vice versa confusion can result, as when slipstreaming cyclists in the Tour de France have a clash of wheels producing a chute or pile up. Galileo’s persecution is cited as an example of this kind of chute, and when the speed of scientific advance increases, they suggest, these kinds of chutes can become more frequent.\n\nThus while describing how emerging features of religious thought, like the reformation insistence on examining the words (and also the works) of God for yourself, fed into the development of experimental science, they also describe how the weaponisation of science in the battle for intellectual credibility produced some of the modern tensions between scientific and religious ideas.\n\nIn an excursus towards the end of the book Wagner and Briggs trace the origins of a particular configuration that they call ‘the religious idea of penultimacy’ in the Biblical idea of a creator God who cannot be identified with his creation; and explore what the cuneiform texts that began to be discovered and translated in the 19th century, reveal about them.\n\nThe final section of the book describes how these ideas influenced two men: Henry Acland who was responsible for the sculpture at the Oxford University Museum, and James Clerk Maxwell who was responsible for the inscription at Cambridge.\n\nA concluding epilogue brings the story up to date, arguing that contemporary attempts to use science to discredit religion are themselves evidence of \"‘the entrenched need of human beings to make sense of the whole depth of their experience’,\" and are \"‘rooted in the cognitive capacities that…first gave rise to homo religiosus’\".\n\nThe book has so far been translated into Spanish and Portuguese. \n\nReviewing the book in the Financial Times John Cornwell remarked on its ‘generally irenic tone’ and describes it as a \"‘gripping work of history and reference that deserves to be read on both sides of the science-art divide’\".\n\nReviewing the book in the Times Higher Educational Supplement Richard Joyner wrote that \"‘to me as an atheist … Wagner and Briggs’ first premise is wrong and their second confusing’\", but nevertheless argued that \"‘their book is well worth reading’\". The reviews in more specialist publications echo this last appraisal.\n\nThe CERN Courier suggests that the book shows how science and religion \"‘can live in a mutually enriching relationship’\". Writing in the Church Times Richard Harries describes it as \"‘an exceptionally ambitious and wide-ranging book which approaches the rather stale debate of religion and science with a fresh historical perspective’\". \n\nIn a longer academic review in The Ship, St Anne’s College Journal, Howard Hotson, Professor of Early Modern Intellectual History at Oxford, wrote that in a culture in which modernity is so often defined in terms of secularization, and religion so often conceived as the archenemy of science, \"‘it seems paradoxical to find glowing testimonials on the dust jacket of The Penultimate Curiosity from the Astronomer Royal and the Director General of CERN alongside the former Chief Rabbi and the current Archbishop of Canterbury.\"’ Professor Hotson describes how the book’s thesis \"‘at its most impressively robust’\" is illustrated in the \"‘process in which the heritage of Greek natural philosophy and mathematics was reshaped by dialogue with the deepest principles of Judeo-Christian-Islamic monotheism and vice versa’\", adding that \"‘Wagner and Briggs are themselves swimming in the slipstream of a huge amount of patient scholarly work undertaken at an exponentially accelerating rate’\".\n\n"}
{"id": "2039133", "url": "https://en.wikipedia.org/wiki?curid=2039133", "title": "Theoretical astronomy", "text": "Theoretical astronomy\n\nTheoretical astronomy is the use of the analytical models of physics and chemistry to describe astronomical objects and astronomical phenomena.\n\nPtolemy's Almagest, although a brilliant treatise on theoretical astronomy combined with a practical handbook for computation, nevertheless includes many compromises to reconcile discordant observations. Theoretical astronomy is usually assumed to have begun with Johannes Kepler (1571–1630), and Kepler's laws. It is co-equal with observation. The general history of astronomy deals with the history of the descriptive and theoretical astronomy of the Solar System, from the late sixteenth century to the end of the nineteenth century. The major categories of works on the history of modern astronomy include general histories, national and institutional histories, instrumentation, descriptive astronomy, theoretical astronomy, positional astronomy, and astrophysics. Astronomy was early to adopt computational techniques to model stellar and galactic formation and celestial mechanics. From the point of view of theoretical astronomy, not only must the mathematical expression be reasonably accurate but it should preferably exist in a form which is amenable to further mathematical analysis when used in specific problems. Most of theoretical astronomy uses Newtonian theory of gravitation, considering that the effects of general relativity are weak for most celestial objects. The obvious fact is that theoretical astronomy cannot (and does not try to) predict the position, size and temperature of every star in the heavens. Theoretical astronomy by and large has concentrated upon analyzing the apparently complex but periodic motions of celestial objects.\n\n\"Contrary to the belief generally held by laboratory physicists, astronomy has contributed to the growth of our understanding of physics.\" Physics has helped in the elucidation of astronomical phenomena, and astronomy has helped in the elucidation of physical phenomena:\n\nIntegrating astronomy with physics involves\nThe aim of astronomy is to understand the physics and chemistry from the laboratory that is behind cosmic events so as to enrich our understanding of the cosmos and of these sciences as well.\n\nAstrochemistry, the overlap of the disciplines of astronomy and chemistry, is the study of the abundance and reactions of chemical elements and molecules in space, and their interaction with radiation. The formation, atomic and chemical composition, evolution and fate of molecular gas clouds, is of special interest because it is from these clouds that solar systems form.\n\nInfrared astronomy, for example, has revealed that the interstellar medium contains a suite of complex gas-phase carbon compounds called aromatic hydrocarbons, often abbreviated (PAHs or PACs). These molecules composed primarily of fused rings of carbon (either neutral or in an ionized state) are said to be the most common class of carbon compound in the galaxy. They are also the most common class of carbon molecule in meteorites and in cometary and asteroidal dust (cosmic dust). These compounds, as well as the amino acids, nucleobases, and many other compounds in meteorites, carry deuterium (H) and isotopes of carbon, nitrogen, and oxygen that are very rare on earth, attesting to their extraterrestrial origin. The PAHs are thought to form in hot circumstellar environments (around dying carbon rich red giant stars).\n\nThe sparseness of interstellar and interplanetary space results in some unusual chemistry, since symmetry-forbidden reactions cannot occur except on the longest of timescales. For this reason, molecules and molecular ions which are unstable on earth can be highly abundant in space, for example the H ion. Astrochemistry overlaps with astrophysics and nuclear physics in characterizing the nuclear reactions which occur in stars, the consequences for stellar evolution, as well as stellar 'generations'. Indeed, the nuclear reactions in stars produce every naturally occurring chemical element. As the stellar 'generations' advance, the mass of the newly formed elements increases. A first-generation star uses elemental hydrogen (H) as a fuel source and produces helium (He). Hydrogen is the most abundant element, and it is the basic building block for all other elements as its nucleus has only one proton. Gravitational pull toward the center of a star creates massive amounts of heat and pressure, which cause nuclear fusion. Through this process of merging nuclear mass, heavier elements are formed. Lithium, carbon, nitrogen and oxygen are examples of elements that form in stellar fusion. After many stellar generations, very heavy elements are formed (e.g. iron and lead).\n\nTheoretical astronomers use a wide variety of tools which include analytical models (for example, polytropes to approximate the behaviors of a star) and computational numerical simulations. Each has some advantages. Analytical models of a process are generally better for giving insight into the heart of what is going on. Numerical models can reveal the existence of phenomena and effects that would otherwise not be seen.\n\nAstronomy theorists endeavor to create theoretical models and figure out the observational consequences of those models. This helps observers look for data that can refute a model or help in choosing between several alternate or conflicting models.\n\nTheorists also try to generate or modify models to take into account new data. Consistent with the general scientific approach, in the case of an inconsistency, the general tendency is to try to make minimal modifications to the model to fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.\n\nTopics studied by theoretical astronomers include:\n\nAstrophysical relativity serves as a tool to gauge the properties of large scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (\"astro\")physics and the study of gravitational waves.\n\nSome widely accepted and studied theories and models in astronomy, now included in the Lambda-CDM model are the Big Bang, Cosmic inflation, dark matter, and fundamental theories of physics.\n\nA few examples of this process:\n\nDark matter and dark energy are the current leading topics in astronomy, as their discovery and controversy originated during the study of the galaxies.\n\nOf the topics approached with the tools of theoretical physics, particular consideration is often given to stellar photospheres, stellar atmospheres, the solar atmosphere, planetary atmospheres, gaseous nebulae, nonstationary stars, and the interstellar medium. Special attention is given to the internal structure of stars.\n\nThe observation of a neutrino burst within 3 h of the associated optical burst from Supernova 1987A in the Large Magellanic Cloud (LMC) gave theoretical astrophysicists an opportunity to test that neutrinos and photons follow the same trajectories in the gravitational field of the galaxy.\n\nA general form of the first law of thermodynamics for stationary black holes can be derived from the microcanonical functional integral for the gravitational field. The boundary data\nare the thermodynamical extensive variables, including the energy and angular momentum of the system. For the simpler case of nonrelativistic mechanics as is often observed in astrophysical phenomena associated with a black hole event horizon, the density of states can be expressed as a real-time functional integral and subsequently used to deduce Feynman's imaginary-time functional integral for the canonical partition function.\n\nReaction equations and large reaction networks are an important tool in theoretical astrochemistry, especially as applied to the gas-grain chemistry of the interstellar medium. Theoretical astrochemistry offers the prospect of being able to place constraints on the inventory of organics for exogenous delivery to the early Earth.\n\n\"An important goal for theoretical astrochemistry is to elucidate which organics are of true interstellar origin, and to identify possible interstellar precursors and reaction pathways for those molecules which are the result of aqueous alterations.\" One of the ways this goal can be achieved is through the study of carbonaceous material as found in some meteorites. Carbonaceous chondrites (such as C1 and C2) include organic compounds such as amines and amides; alcohols, aldehydes, and ketones; aliphatic and aromatic hydrocarbons; sulfonic and phosphonic acids; amino, hydroxycarboxylic, and carboxylic acids; purines and pyrimidines; and kerogen-type material. The organic inventories of primitive meteorites display large and variable enrichments in deuterium, carbon-13 (C), and nitrogen-15 (N), which is indicative of their retention of an interstellar heritage.\n\nThe chemical composition of comets should reflect both the conditions in the outer solar nebula some 4.5 × 10 ayr, and the nature of the natal interstellar cloud from which the Solar system was formed. While comets retain a strong signature of their ultimate interstellar origins, significant processing must have occurred in the protosolar nebula. Early models of coma chemistry showed that reactions can occur rapidly in the inner coma, where the most important reactions are proton transfer reactions. Such reactions can potentially cycle deuterium between the different coma molecules, altering the initial D/H ratios released from the nuclear ice, and necessitating the construction of accurate models of cometary deuterium chemistry, so that gas-phase coma observations can be safely extrapolated to give nuclear D/H ratios.\n\nWhile the lines of conceptual understanding between theoretical astrochemistry and theoretical chemical astronomy often become blurred so that the goals and tools are the same, there are subtle differences between the two sciences. Theoretical chemistry as applied to astronomy seeks to find new ways to observe chemicals in celestial objects, for example. This often leads to theoretical astrochemistry having to seek new ways to describe or explain those same observations.\n\nThe new era of chemical astronomy had to await the clear enunciation of the chemical principles of spectroscopy and the applicable theory.\n\nSupernova radioactivity dominates light curves and the chemistry of dust condensation is also dominated by radioactivity. Dust is usually either carbon or oxides depending on which is more abundant, but Compton electrons dissociate the CO molecule in about one month. The new chemical astronomy of supernova solids depends on the supernova radioactivity:\n\nLike theoretical chemical astronomy, the lines of conceptual understanding between theoretical astrophysics and theoretical physical astronomy are often blurred, but, again, there are subtle differences between these two sciences. Theoretical physics as applied to astronomy seeks to find new ways to observe physical phenomena in celestial objects and what to look for, for example. This often leads to theoretical astrophysics having to seek new ways to describe or explain those same observations, with hopefully a convergence to improve our understanding of the local environment of Earth and the physical Universe.\n\nNuclear matrix elements of relevant operators as extracted from data and from a shell-model and theoretical approximations both for the two-neutrino and neutrinoless modes of decay are used to explain the weak interaction and nuclear structure aspects of nuclear double beta decay.\n\nNew neutron-rich isotopes, Ne, Na, and Si have been produced unambiguously for the first time, and convincing evidence for the particle instability of three others, Ne, Na, and Mg has been obtained. These experimental findings compare with recent theoretical predictions.\n\nUntil recently all the time units that appear natural to us are caused by astronomical phenomena:\n\nHigh precision appears problematic:\n\nSome of these time scales are sidereal time, solar time, and universal time.\n\nFrom the Systeme Internationale (SI) comes the second as defined by the duration of 9 192 631 770 cycles of a particular hyperfine structure transition in the ground state of caesium-133 (Cs). For practical usability a device is required that attempts to produce the SI second (s) such as an atomic clock. But not all such clocks agree. The weighted mean of many clocks distributed over the whole Earth defines the Temps Atomique International; i.e., the Atomic Time TAI. From the General theory of relativity the time measured depends on the altitude on earth and the spatial velocity of the clock so that TAI refers to a location on sea level that rotates with the Earth.\n\nSince the Earth's rotation is irregular, any time scale derived from it such as Greenwich Mean Time led to recurring problems in predicting the Ephemerides for the positions of the Moon, Sun, planets and their natural satellites. In 1976 the International Astronomical Union (IAU) resolved that the theoretical basis for ephemeris time (ET) was wholly non-relativistic, and therefore, beginning in 1984 ephemeris time would be replaced by two further time scales with allowance for relativistic corrections. Their names, assigned in 1979, emphasized their dynamical nature or origin, Barycentric Dynamical Time (TDB) and Terrestrial Dynamical Time (TDT). Both were defined for continuity with ET and were based on what had become the standard SI second, which in turn had been derived from the measured second of ET.\n\nDuring the period 1991–2006, the TDB and TDT time scales were both redefined and replaced, owing to difficulties or inconsistencies in their original definitions. The current fundamental relativistic time scales are Geocentric Coordinate Time (TCG) and Barycentric Coordinate Time (TCB). Both of these have rates that are based on the SI second in respective reference frames (and hypothetically outside the relevant gravity well), but due to relativistic effects, their rates would appear slightly faster when observed at the Earth's surface, and therefore diverge from local Earth-based time scales using the SI second at the Earth's surface.\n\nThe currently defined IAU time scales also include Terrestrial Time (TT) (replacing TDT, and now defined as a re-scaling of TCG, chosen to give TT a rate that matches the SI second when observed at the Earth's surface), and a redefined Barycentric Dynamical Time (TDB), a re-scaling of TCB to give TDB a rate that matches the SI second at the Earth's surface.\n\nFor a star, the dynamical time scale is defined as the time that would be taken for a test particle released at the surface to fall under the star's potential to the centre point, if pressure forces were negligible. In other words, the dynamical time scale measures the amount of time it would take a certain star to collapse in the absence of any internal pressure. By appropriate manipulation of the equations of stellar structure this can be found to be\n\nformula_1\n\nwhere R is the radius of the star, G is the gravitational constant, M is the mass of the star and v is the escape velocity. As an example, the Sun dynamical time scale is approximately 1133 seconds. Note that the actual time it would take a star like the Sun to collapse is greater because internal pressure is present.\n\nThe 'fundamental' oscillatory mode of a star will be at approximately the dynamical time scale. Oscillations at this frequency are seen in Cepheid variables.\n\nThe basic characteristics of applied astronomical navigation are\n\nThe superiority of satellite navigation systems to astronomical navigation are currently undeniable, especially with the development and use of GPS/NAVSTAR. This global satellite system\n\nGeodetic astronomy is the application of astronomical methods into s and technical projects of geodesy for\n\nAstronomical algorithms are the algorithms used to calculate ephemerides, calendars, and positions (as in celestial navigation or satellite navigation).\n\nMany astronomical and navigational computations use the Figure of the Earth as a surface representing the earth.\n\nThe International Earth Rotation and Reference Systems Service (IERS), formerly the International Earth Rotation Service, is the body responsible for maintaining global time and reference frame standards, notably through its Earth Orientation Parameter (EOP) and International Celestial Reference System (ICRS) groups.\n\nThe Deep Space Network, or DSN, is an international of large antennas and communication facilities that supports interplanetary spacecraft missions, and radio and radar astronomy observations for the exploration of the solar system and the universe. The network also supports selected Earth-orbiting missions. DSN is part of the NASA Jet Propulsion Laboratory (JPL).\n\nAn observer becomes a deep space explorer upon escaping Earth's orbit. While the Deep Space Network maintains communication and enables data download from an exploratory vessel, any local probing performed by sensors or active systems aboard usually require astronomical navigation, since the enclosing network of satellites to ensure accurate positioning is absent.\n\n\n"}
{"id": "53526055", "url": "https://en.wikipedia.org/wiki?curid=53526055", "title": "Todd Huffman", "text": "Todd Huffman\n\nTodd Huffman is an American technology entrepreneur and prolific photographer. He is a co-founder of the biomedical imaging company, 3Scan.\n\nIn 2011, Huffman co-founded 3Scan, a firm that develops new techniques for biomedical imaging. \"Biz Journals\" called 3Scan's main technology, the Knife-edge scanning microscope, a \"robotic microscope.\" The microscope rapidly sections and scans samples, building 3d models of microscopic structures. \"Singularity Hub magazine\" quoted Huffman's description of their goal: “We’re trying to move from a world where humans are hunting and pecking through tissue looking for answers to a world where we generate large and reproducible data sets where we can use analytics to drive insights and real cures.”\n\nIn January 2015, \"Forbes magazine\" interviewed Huffman, asking him to explain the approach to technology his firm was taking.\n\nIn July 2016, \"Biz Journals\" reported that venture capital firms had invested an additional $11 million in 3Scan, reporting the total as $21 million.\n\nSharon Weinberger, author of a book about DARPA entitled \"The Imagineers of War\", described Huffman influencing DARPA decision-makers, following a chance meeting where he described how he and other volunteers had used innovative modeling techniques to aid civilians in disaster zones and warzones. Huffman was a regular visitor to Jalalabad, in Afghanistan, where he worked with other technology workers affiliated with an informal group known as the Synergy Strike Force, using technology to help improve the quality of life for Afghan civilians and training them in the use of peaceful technologies such as computers and wireless internet.\n\nHuffman is a cofounder of the BIL Conference, an \"unconference\" organized and observed by the participants as an unaffiliated counterpart to TED’s structured, ‘invite-only’ paid conference. \n\n"}
{"id": "41717452", "url": "https://en.wikipedia.org/wiki?curid=41717452", "title": "Transformational theory of imitation", "text": "Transformational theory of imitation\n\nTransformational theory of imitation is one of the two types of theories that provide alternative accounts of the psychological processes underlying imitation in animals. Associative theories (being the other theory of imitation) suggesting successful imitation matches between a behavior of the demonstrator and the model to be acquired from experiences, transformational theories are in complete contrast suggesting successful imitation matches are derived internally from a series of elaborated cognitive processes that exist void of any experiences. \n\nAccording to transformational theories, sensory input seen by the model of the demonstrator's action is transformed into a \"imaged representation\" that contains the fundamental information for the model to accurately execute the imitation of the demonstrator. In transformational theories, simply observing the behaviors of the demonstrator creates an incentive to duplicate them because we already have the codes to replicate them.\n\nIn his \"social cognitive theory\" Bandura outlines what he believed to be the four sets of constituent processes that govern imitation:\nUnlike in associative theories of imitation, Bandura's transformational theory of imitation denies that reinforcement must accompany observation of the demonstrator's behavior for the creation of a mental portrayal of the demonstrator's behavior to occur. Bandura's theory also claims reinforcement is not necessary in the maintenance of imitative behavior. Rather, it is the symbolic conception that regulates imitation, where no experience is necessary, only the conception that is created internally.\n\nAlthough there are many theories enveloped within transformational theories of imitation, as a whole, transformational theories all lack in specification. According to these theories, imitation is mediated by a sort of “symbolic conception”, but how this conception works is not specified. It is unknown, then, which behaviors can and cannot be imitated, with which species, and under what conditions because the process of this conception isn't specific. In turn, transformational theories are then impossible to test empirically.\n\nIn Bandura's theory and his sets of constituent processes governing imitation, there is essentially a three piece cognitive architecture. This architecture consists of a sensory representation where the demonstrator's behavior is first stored, a symbolic conception that is a transformation of the sensory representation, and a motor program that serves as constant reminder of that behavior's standard. What there is not, however, is specificity in regards to how this behavior moves from one piece to the next. It is unclear as to how the sensory representation moves and is transformed into a symbolic conception that serves as the behavior's standard and thus, proves to be untestable and unpredictable, a common problem that many find with transformational theories.\n"}
{"id": "18258915", "url": "https://en.wikipedia.org/wiki?curid=18258915", "title": "Tree: A Life Story", "text": "Tree: A Life Story\n\nTree: A Life Story (or Tree: A Biography in Australia) is a Canadian non-fiction book written by David Suzuki and Wayne Grady, and illustrated by Robert Bateman. The book profiles the life of a Douglas-fir tree, from seed to maturity to death. The story provides ecological context by describing interactions with other lifeforms in the forest and historical context through parallels with world events that occur during the tree's 700 years of life. Digressions from the biographical narrative, scattered throughout the book, provide background into related topics, such as the history of botany.\n\nSuzuki was inspired to write a biography of a tree when he noticed a Douglas-fir with an uncharacteristic curve in its trunk and speculated what caused it to grow into that shape. Suzuki studied the topic with the help of a research assistant and solicited Grady to help write the book. Vancouver publishers Greystone Books released the book in September 2004. In the Canadian market, it peaked at number three in the \"Maclean's\" and the \"National Post\" non-fiction best seller lists and was nominated for several awards. In February 2005 it was published in Australia by Allen & Unwin. The premise and writing were well received by critics. While several reviewers found that the authors succeeded in using accessible language, others found it too technical.\n\nInspiration for the book came from a Douglas-fir tree with a curve in its trunk. While sitting by the tree, at his home on Quadra Island, near Vancouver, David Suzuki realized that even though his family had played on it for years, he did not know how old it was or how its uncharacteristic curve had developed. Suzuki, a science writer and broadcaster, and former zoologist, speculated that the soil might have slid when the tree was young or that another plant might have blocked the sunlight. He thought that the tree must have endured much hardship throughout its life and made a connection between biographies of people and the story of this tree's life. It also reminded him of an idea he had for a children's book about interconnectivity of life, especially within plants. Along with a research assistant, he studied the topic. Suzuki started to write a draft but a busy schedule interfered so he sought a collaborator. Science writer and former \"Harrowsmith\" editor Wayne Grady agreed to participate. Suzuki provided the research, framework, and some original writing and Grady did most of the writing. Together, Grady in Ontario and Suzuki in Vancouver, went through five drafts. Wildlife artist Robert Bateman was brought into the project through social connections between the wives of Bateman and Suzuki. In creating the book, their intention was to illustrate the complexity and interconnectivity of this ecosystem by focusing on one tree's role over time.\n\nThe book consists of five chapters: \"Birth\", \"Taking Root\", \"Growth\", \"Maturity\", and \"Death\". The book opens with acknowledgments and an introduction, and closes with selected references and an index. In the introduction, Suzuki describes the tree at his home and the series of ideas and events that led to the writing of the book. Along with the narrative of the tree's life, the book includes digressions into related topics, such as the history of botany and animal life in the forest. The tree written about in the book is not any specific Douglas-fir, but rather a generic one.\n\nThe first chapter, \"Birth\", begins with lightning starting a forest fire. The heat dries the Douglas-fir cones enough for their scales to spread and release winged seeds. Rain water transports one seed to a sunlit area with well-drained soil. Rodents and insectivores, whose food stashes were destroyed in the fire, eat truffles, which survived underground, and leave feces containing nitrogen-fixing bacteria in the soil. Following one dormant winter stage, the seed begins to germinate.\n\nIn the second chapter, \"Taking Root\", the embryonic root emerges through a small opening in the seed coat and through cell division, aided by plant hormones, it grows downward. Water and nutrients enter the root by osmosis and are transported to the seedling. A symbiotic relationship develops between the roots and the truffles. The roots give its extra sugars to the truffles, which it uses for energy, and the truffles assist the roots' uptake of water and nutrients. From excess starches and nutrients gathered by the root, a stem similar to the root but surrounded with thin, grayish bark, grows upwards. As the starch reserves are exhausted, its first needles sprout and photosynthesis begins. The tree anchors itself with a deep taproot and a web of roots begin to grow laterally. Some roots develop symbiotic relationships with near-by red alders which excel at nitrogen-fixation but lack the storage capacity that the Douglas-fir can offer. In early April of every year, a new layer grows between the bark and wood. As this new layer takes over transportation of fluids throughout the tree, last year's layer of cells die and form a ring in the wood.\nAfter about 20 years, the tree begins to develop fertile cones. Buds form where auxins accumulate; these become either new needles or cones. The buds remain undifferentiated until July and continue to develop throughout the fall and winter. The next year, some buds will open in mid-May exposing a new set of needles. The cone buds on the lower end of the tree while other buds burst open in April releasing a mist of pollen. The cones at the top of the tree open their scales for wind-borne pollen to enter. Within the cone, the pollen fertilizes a seed which is released in September. The quantity and quality of seed production varies year-to-year but a particularly effective crop is produced about every 10 years. Less than 0.1% of seeds survive Douglas squirrels, dark-eyed juncos, and other seed-eating animals.\n\nOver the centuries, the tree grows thicker and taller as successive rings develop around its trunk and new buds grow on the branches. The tree becomes part of an old growth forest with a shaded and damp understory of broadleaf trees, shrubs, and ferns. In the canopy, a mat of dead needles and lichen accumulate on the wide upper branches. Exposed to light, air, and rain, the needles decompose and the mat becomes colonized by insects, fungus, and new plants.\n\nIn the opening of the final chapter, \"Death\", the tree is 550 years old and stands 80 meters (260 feet) tall. Under the weight of too much snow accumulating on the canopy mat, a branch breaks off. Stresses from a long winter with a dry summer weaken the tree's immune system. The exposed area where the branch broke becomes infected with insects and fungus. Insect larvae eat the buds and the fungus spreads into the middle of the tree and down to the roots. With its vascular tissue system compromised, the tree diverts nutrients elsewhere, resulting in needles turning orange on the abandoned branches. Death takes years to occur as successive parts are slowly starved of nutrients. As a snag, it becomes home to a succession of animals, like woodpeckers, owls, squirrels, and bats. Eventually the roots rot enough that a rainstorm blows it down. Mosses and fungi grow on the deadfall, followed by colonies of termites, ants, and mites, which all help decompose the remaining wood.\n\n\"Tree\" is a popular science book, intended to profile the life of single tree using terminology targeted at a general audience. The narrative provides ecological context, describing animals and plants that interact with the tree, as well as historical context. Parallels to the tree's age are made with historical events, like the tree taking root as empirical science was taking root in Europe during the life of 13th century philosopher Roger Bacon. The book is most commonly described, and marketed, as a \"biography\". One reviewer grouped it with the 2005 book \"The Golden Spruce\" as part of a new genre: an \"arbobiography\".\n\nThe book is written in the third person, omniscient, style. Grady's writing moderates Suzuki's characteristic rhetoric to create writing that is accessible, with a tone described as \"a breezy casualness that welcomes the reader\". According to Suzuki, making the book accessible required telling the story from a human perspective, including some anthropomorphism of biological processes.\n\nThe book was published by Greystone Books, an imprint of Douglas & McIntyre based in Vancouver that specializes in nature, travel, and sports topics. They published the hardcover version of \"Tree\" in September 2004. The book is small, measuring only 19×14 cm (7.6×5.4 inches) with 190 pages. Suzuki and Grady promoted it through media interviews and book signing events across Canada. In February 2005, Allen & Unwin published it in Australia as \"Tree: A Biography\". The Recording for the Blind & Dyslexic released the audio book in April 2006. Greystone Books published the trade paperback in February 2007.\n\nIn the Canadian market, the hardcover edition peaked at number three in the \"MacLean's\" and the \"National Post\" non-fiction best seller lists. The magazine \"Science & Spirit\" published an excerpt in the January–February 2005 edition. It was nominated for the 2004 Canadian Science Writers' Association's Science in Society Journalism Award for 'General Audience Book', the 2005 B.C. Booksellers' Choice Award and the 2006 Council on Botanical and Horticultural Libraries' Annual Litereature Award for best 'General Interest' book. The French translation by Dominique Fortier was nominated for the 2006 Governor General's Awards for best English to French translation.\n\nThe premise of a biography for a tree was well received. The writing was called engaging, lyrical, and compelling. Robert Wiersema wrote, \"\"Tree\" is science writing at its finest. It's sweeping but focused, keenly aware of both the minutiae and the big picture. ... Although some of the concepts are complex, the writing is always accessible ... Scientific matters are explained in layman's terms, and the text never bogs down or bottlenecks.\" However, some reviewers found the language too technical. In the \"Montreal Gazette\", Bronwyn Chester wrote that the scientific language \"dilut[es] our feeling and concern for this tree through too much information\". Robert Bateman's black and white illustrations, while skilled, were said to add little to the narrative.\n\n"}
{"id": "33139", "url": "https://en.wikipedia.org/wiki?curid=33139", "title": "World Wide Web", "text": "World Wide Web\n\nThe World Wide Web (WWW), also called the Web, is an information space where documents and other web resources are identified by Uniform Resource Locators (URLs), interlinked by hypertext links, and accessible via the Internet. English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser in 1990 while employed at CERN near Geneva, Switzerland. The browser was released outside CERN in 1991, first to other research institutions starting in January 1991 and to the general public on the Internet in August 1991.\n\nThe World Wide Web has been central to the development of the Information Age and is the primary tool billions of people use to interact on the Internet. Web pages are primarily text documents formatted and annotated with Hypertext Markup Language (HTML). In addition to formatted text, web pages may contain images, video, audio, and software components that are rendered in the user's web browser as coherent pages of multimedia content.\n\nEmbedded hyperlinks permit users to navigate between web pages. Multiple web pages with a common theme, a common domain name, or both, make up a website. Website content can largely be provided by the publisher, or interactively where users contribute content or the content depends upon the users or their actions. Websites may be mostly informative, primarily for entertainment, or largely for commercial, governmental, or non-governmental organisational purpose .\n\nTim Berners-Lee's vision of a global hyperlinked information system became a possibility by the second half of the 1980s. By 1985, the global Internet began to proliferate in Europe and the Domain Name System (upon which the Uniform Resource Locator is built) came into being. In 1988 the first direct IP connection between Europe and North America was made and Berners-Lee began to openly discuss the possibility of a web-like system at CERN. In March 1989 Berners-Lee issued a proposal to the management at CERN for a system called \"Mesh\" that referenced ENQUIRE, a database and software project he had built in 1980, which used the term \"web\" and described a more elaborate information management system based on links embedded in readable text: \"Imagine, then, the references in this document all being associated with the network address of the thing to which they referred, so that while reading this document you could skip to them with a click of the mouse.\" Such a system, he explained, could be referred to using one of the existing meanings of the word \"hypertext\", a term that he says was coined in the 1950s. There is no reason, the proposal continues, why such hypertext links could not encompass multimedia documents including graphics, speech and video, so that Berners-Lee goes on to use the term \"hypermedia\".\n\nWith help from his colleague and fellow hypertext enthusiast Robert Cailliau he published a more formal proposal on 12 November 1990 to build a \"Hypertext project\" called \"WorldWideWeb\" (one word) as a \"web\" of \"hypertext documents\" to be viewed by \"browsers\" using a client–server architecture. At this point HTML and HTTP had already been in development for about two months and the first Web server was about a month from completing its first successful test. This proposal estimated that a read-only web would be developed within three months and that it would take six months to achieve \"the creation of new links and new material by readers, [so that] authorship becomes universal\" as well as \"the automatic notification of a reader when new material of interest to him/her has become available.\" While the read-only goal was met, accessible authorship of web content took longer to mature, with the wiki concept, WebDAV, blogs, Web 2.0 and RSS/Atom.\n\nThe proposal was modelled after the SGML reader Dynatext by Electronic Book Technology, a spin-off from the Institute for Research in Information and Scholarship at Brown University. The Dynatext system, licensed by CERN, was a key player in the extension of SGML ISO 8879:1986 to Hypermedia within HyTime, but it was considered too expensive and had an inappropriate licensing policy for use in the general high energy physics community, namely a fee for each document and each document alteration. A NeXT Computer was used by Berners-Lee as the world's first web server and also to write the first web browser, WorldWideWeb, in 1990. By Christmas 1990, Berners-Lee had built all the tools necessary for a working Web: the first web browser (which was a web editor as well) and the first web server. The first web site, which described the project itself, was published on 20 December 1990.\n\nThe first web page may be lost, but Paul Jones of UNC-Chapel Hill in North Carolina announced in May 2013 that Berners-Lee gave him what he says is the oldest known web page during a 1991 visit to UNC. Jones stored it on a magneto-optical drive and on his NeXT computer. On 6 August 1991, Berners-Lee published a short summary of the World Wide Web project on the newsgroup \"alt.hypertext\". This date is sometimes confused with the public availability of the first web servers, which had occurred months earlier. As another example of such confusion, several news media reported that the first photo on the Web was published by Berners-Lee in 1992, an image of the CERN house band Les Horribles Cernettes taken by Silvano de Gennaro; Gennaro has disclaimed this story, writing that media were \"totally distorting our words for the sake of cheap sensationalism.\"\n\nThe first server outside Europe was installed at the Stanford Linear Accelerator Center (SLAC) in Palo Alto, California, to host the SPIRES-HEP database. Accounts differ substantially as to the date of this event. The World Wide Web Consortium's timeline says December 1992, whereas SLAC itself claims December 1991, as does a W3C document titled \"A Little History of the World Wide Web\". The underlying concept of hypertext originated in previous projects from the 1960s, such as the Hypertext Editing System (HES) at Brown University, Ted Nelson's Project Xanadu, and Douglas Engelbart's oN-Line System (NLS). Both Nelson and Engelbart were in turn inspired by Vannevar Bush's microfilm-based \"memex\", which was described in the 1945 essay \"As We May Think\".\n\nBerners-Lee's breakthrough was to marry hypertext to the Internet. In his book \"\", he explains that he had repeatedly suggested that a marriage between the two technologies was possible to members of \"both\" technical communities, but when no one took up his invitation, he finally assumed the project himself. In the process, he developed three essential technologies:\n\nThe World Wide Web had a number of differences from other hypertext systems available at the time. The Web required only unidirectional links rather than bidirectional ones, making it possible for someone to link to another resource without action by the owner of that resource. It also significantly reduced the difficulty of implementing web servers and browsers (in comparison to earlier systems), but in turn presented the chronic problem of \"link rot\". Unlike predecessors such as HyperCard, the World Wide Web was non-proprietary, making it possible to develop servers and clients independently and to add extensions without licensing restrictions. On 30 April 1993, CERN announced that the World Wide Web would be free to anyone, with no fees due. Coming two months after the announcement that the server implementation of the Gopher protocol was no longer free to use, this produced a rapid shift away from Gopher and towards the Web. An early popular web browser was ViolaWWW for Unix and the X Windowing System.\n\nScholars generally agree that a turning point for the World Wide Web began with the introduction of the Mosaic web browser in 1993, a graphical browser developed by a team at the National Center for Supercomputing Applications at the University of Illinois at Urbana–Champaign (NCSA-UIUC), led by Marc Andreessen. Funding for Mosaic came from the US High-Performance Computing and Communications Initiative and the High Performance Computing Act of 1991, one of several computing developments initiated by US Senator Al Gore. Prior to the release of Mosaic, graphics were not commonly mixed with text in web pages and the web's popularity was less than older protocols in use over the Internet, such as Gopher and Wide Area Information Servers (WAIS). Mosaic's graphical user interface allowed the Web to become, by far, the most popular Internet protocol. The World Wide Web Consortium (W3C) was founded by Tim Berners-Lee after he left the European Organization for Nuclear Research (CERN) in October 1994. It was founded at the Massachusetts Institute of Technology Laboratory for Computer Science (MIT/LCS) with support from the Defense Advanced Research Projects Agency (DARPA), which had pioneered the Internet; a year later, a second site was founded at INRIA (a French national computer research lab) with support from the European Commission DG InfSo; and in 1996, a third continental site was created in Japan at Keio University. By the end of 1994, the total number of websites was still relatively small, but many notable websites were already active that foreshadowed or inspired today's most popular services.\n\nConnected by the Internet, other websites were created around the world. This motivated international standards development for protocols and formatting. Berners-Lee continued to stay involved in guiding the development of web standards, such as the markup languages to compose web pages and he advocated his vision of a Semantic Web. The World Wide Web enabled the spread of information over the Internet through an easy-to-use and flexible format. It thus played an important role in popularising use of the Internet. Although the two terms are sometimes conflated in popular use, \"World Wide Web\" is not synonymous with \"Internet\". The Web is an information space containing hyperlinked documents and other resources, identified by their URIs. It is implemented as both client and server software using Internet protocols such as TCP/IP and HTTP. Berners-Lee was knighted in 2004 by Queen Elizabeth II for \"services to the global development of the Internet\".\n\nThe terms \"Internet\" and \"World Wide Web\" are often used without much distinction. However, the two are not the same. The Internet is a global system of interconnected computer networks. In contrast, the World Wide Web is a global collection of documents and other resources, linked by hyperlinks and URIs. Web resources are accessed using HTTP or HTTPS, which are application-level Internet protocols that use the Internet's transport protocols.\n\nViewing a web page on the World Wide Web normally begins either by typing the URL of the page into a web browser, or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after channel surfing), or 'navigating the Web'. Early studies of this new behaviour investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation.\n\nThe following example demonstrates the functioning of a web browser when accessing a page at the URL codice_1. The browser resolves the server name of the URL (codice_2) into an Internet Protocol address using the globally distributed Domain Name System (DNS). This lookup returns an IP address such as \"203.0.113.4\" or \"2001:db8:2e::7334\". The browser then requests the resource by sending an HTTP request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service, so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. The HTTP protocol normally uses port number 80. The content of the HTTP request can be as simple as two lines of text:\n\nGET /home.html HTTP/1.1\nHost: www.example.org\nThe computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the web server can fulfil the request it sends an HTTP response back to the browser indicating success:\n\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=UTF-8\nfollowed by the content of the requested page. HyperText Markup Language (HTML) for a basic web page might look like this:\n\n<html>\n</html>\nThe web browser parses the HTML and interprets the markup (<title>, <p> for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media, scripts that affect page behaviour, and Cascading Style Sheets that affect page layout. The browser makes additional HTTP requests to the web server for these other Internet media types. As it receives their content from the web server, the browser progressively renders the page onto the screen as specified by its HTML and these additional resources.\n\nMost web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:\n<a href=\"http://www.example.org/home.html\">Example.org Homepage</a>\n\nSuch a collection of useful, related resources, interconnected via hypertext links is dubbed a \"web\" of information. Publication on the Internet created what Tim Berners-Lee first called the \"WorldWideWeb\" (in its original CamelCase, which was subsequently discarded) in November 1990.\n\nThe hyperlink structure of the WWW is described by the webgraph: the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called dead links. The ephemeral nature of the Web has prompted many efforts to archive web sites. The Internet Archive, active since 1996, is the best known of such efforts.\n\nJavaScript is a scripting language that was initially developed in 1995 by Brendan Eich, then of Netscape, for use within web pages. The standardised version is ECMAScript. To make web pages more interactive, some web applications also use JavaScript techniques such as Ajax (asynchronous JavaScript and XML). Client-side script is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly poll the server to check whether new information is available.\n\nMany hostnames used for the World Wide Web begin with \"www\" because of the long-standing practice of naming Internet hosts according to the services they provide. The hostname of a web server is often \"www\", in the same way that it may be \"ftp\" for an FTP server, and \"news\" or \"nntp\" for a USENET news server. These host names appear as Domain Name System (DNS) or subdomain names, as in \"www.example.com\". The use of \"www\" is not required by any technical or policy standard and many web sites do not use it; the first web server was \"nxoc01.cern.ch\". According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of \"www\" as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page, however the DNS records were never switched, and the practice of prepending \"www\" to an institution's website domain name was subsequently copied. Many established websites still use the prefix, or they employ other subdomain names such as \"www2\", \"secure\" or \"en\" for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the \"www\" subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for load balancing incoming web traffic by creating a CNAME record that points to a cluster of web servers. Since, currently, only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root.\n\nWhen a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix \"www\" to the beginning of it and possibly \".com\", \".org\" and \".net\" at the end, depending on what might be missing. For example, entering \" may be transformed to \"<nowiki>http://www.microsoft.com/</nowiki>\" and 'openoffice' to \"<nowiki>http://www.openoffice.org</nowiki>\". This feature started appearing in early versions of Firefox, when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as Lynx. It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices.\n\nIn English, \"www\" is usually read as \"double-u double-u double-u\". Some users pronounce it \"dub-dub-dub\", particularly in New Zealand. Stephen Fry, in his \"Podgrams\" series of podcasts, pronounces it \"wuh wuh wuh\". The English writer Douglas Adams once quipped in \"The Independent on Sunday\" (1999): \"The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for\". In Mandarin Chinese, \"World Wide Web\" is commonly translated via a phono-semantic matching to \"wàn wéi wǎng\" (), which satisfies \"www\" and literally means \"myriad dimensional net\", a translation that reflects the design concept and proliferation of the World Wide Web. Tim Berners-Lee's web-space states that \"World Wide Web\" is officially spelled as three separate words, each capitalised, with no intervening hyphens. Use of the www prefix has been declining, especially when Web 2.0 web applications sought to brand their domain names and make them easily pronounceable.\nAs the mobile Web grew in popularity, services like Gmail.com, Outlook.com, Myspace.com, Facebook.com and Twitter.com are most often mentioned without adding \"www.\" (or, indeed, \".com\") to the domain.\n\nThe scheme specifiers \"<nowiki>http://</nowiki>\" and \"<nowiki>https://</nowiki>\" at the start of a web URI refer to Hypertext Transfer Protocol or HTTP Secure, respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend <nowiki>http://</nowiki> to user-entered URIs, if omitted.\n\nFor criminals, the Web has become a venue to spread malware and engage in a range of cybercrimes, including identity theft, fraud, espionage and intelligence gathering. Web-based vulnerabilities now outnumber traditional computer security concerns, and as measured by Google, about one in ten web pages may contain malicious code. Most web-based attacks take place on legitimate websites, and most, as measured by Sophos, are hosted in the United States, China and Russia. The most common of all malware threats is SQL injection attacks against websites. Through HTML and URIs, the Web was vulnerable to attacks like cross-site scripting (XSS) that came with the introduction of JavaScript and were exacerbated to some degree by Web 2.0 and Ajax web design that favours the use of scripts. Today by one estimate, 70% of all websites are open to XSS attacks on their users. Phishing is another common threat to the Web. \"SA, the Security Division of EMC, today announced the findings of its January 2013 Fraud Report, estimating the global losses from phishing at $1.5 Billion in 2012\". Two of the well-known phishing methods are Covert Redirect and Open Redirect.\n\nProposed solutions vary. Large security companies like McAfee already design governance and compliance suites to meet post-9/11 regulations, and some, like Finjan have recommended active real-time inspection of programming code and all content regardless of its source. Some have argued that for enterprises to see Web security as a business opportunity rather than a cost centre, while others call for \"ubiquitous, always-on digital rights management\" enforced in the infrastructure to replace the hundreds of companies that secure data and networks. Jonathan Zittrain has said users sharing responsibility for computing safety is far preferable to locking down the Internet.\n\nEvery time a client requests a web page, the server can identify the request's IP address and usually logs it. Also, unless set not to do so, most web browsers record requested web pages in a viewable \"history\" feature, and usually cache much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc.—web-based entities can associate current web traffic with that individual. If the website uses HTTP cookies, username and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way it is possible for a web-based organisation to develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their demographic profile. These profiles are obviously of potential interest to marketeers, advertisers and others. Depending on the website's terms and conditions and the local laws that apply information from these profiles may be sold, shared, or passed to other organisations without the user being informed. For many ordinary people, this means little more than some unexpected e-mails in their in-box or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counter terrorism, and espionage agencies can also identify, target and track individuals based on their interests or proclivities on the Web.\n\nSocial networking sites try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences. On-line bullies may make use of personal information to harass or stalk users. Modern social networking websites allow fine grained control of the privacy settings for each individual posting, but these can be complex and not easy to find or use, especially for beginners. Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an on-line profile. With modern and potential facial recognition technology, it may then be possible to relate that face with other, previously anonymous, images, events and scenarios that have been imaged elsewhere. Because of image caching, mirroring and copying, it is difficult to remove an image from the World Wide Web.\n\nMany formal standards and other technical specifications and software define the operation of different aspects of the World Wide Web, the Internet, and computer information exchange. Many of the documents are the work of the World Wide Web Consortium (W3C), headed by Berners-Lee, but some are produced by the Internet Engineering Task Force (IETF) and other organisations.\n\nUsually, when web standards are discussed, the following publications are seen as foundational:\n\nAdditional publications provide definitions of other essential technologies for the World Wide Web, including, but not limited to, the following:\n\nThere are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with disabilities. These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. The Web receives information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and equal opportunity to people with disabilities. Tim Berners-Lee once noted, \"The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect.\" Many countries regulate web accessibility as a requirement for websites. International co-operation in the W3C Web Accessibility Initiative led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using assistive technology.\n\nThe W3C Internationalisation Activity assures that web technology works in all languages, scripts, and cultures. Beginning in 2004 or 2005, Unicode gained ground and eventually in December 2007 surpassed both ASCII and Western European as the Web's most frequently used character encoding. Originally allowed resources to be identified by URI in a subset of US-ASCII. allows more characters—any character in the Universal Character Set—and now a resource can be identified by IRI in any language.\n\nBetween 2005 and 2010, the number of web users doubled, and was expected to surpass two billion in 2010. Early studies in 1998 and 1999 estimating the size of the Web using capture/recapture methods showed that much of the web was not indexed by search engines and the Web was much larger than expected. According to a 2001 study, there was a massive number, over 550 billion, of documents on the Web, mostly in the invisible Web, or Deep Web. A 2002 survey of 2,024 million web pages determined that by far the most web content was in the English language: 56.4%; next were pages in German (7.7%), French (5.6%), and Japanese (4.9%). A more recent study, which used web searches in 75 different languages to sample the Web, determined that there were over 11.5 billion web pages in the publicly indexable web as of the end of January 2005. , the indexable web contains at least 25.21 billion pages. On 25 July 2008, Google software engineers Jesse Alpert and Nissan Hajaj announced that Google Search had discovered one trillion unique URLs. , over 109.5 million domains operated. Of these, 74% were commercial or other domains operating in the generic top-level domain \"com\". Statistics measuring a website's popularity, such as the Alexa Internet rankings, are usually based either on the number of page views or on associated server \"hits\" (file requests) that it receives.\n\nA web cache is a server computer located either on the public Internet, or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a browser cache by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in online banking, or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise firewalls often cache Web resources requested by one user for the benefit of many users. Some search engines store cached content of frequently accessed websites.\n\n\n\n"}
{"id": "36556313", "url": "https://en.wikipedia.org/wiki?curid=36556313", "title": "Yajaira Sierra-Sastre", "text": "Yajaira Sierra-Sastre\n\nDr. Yajaira Sierra-Sastre (born 1977) is a Puerto Rican nanotechnology scientist, educator, mentor and aspiring astronaut. She is part of a six-person crew, and the only Hispanic, selected to participate in a four-month-long, Mars analog mission funded by NASA. Sierra-Sastre aspires to become the first Puerto Rican woman to travel to outer space.\n\nSierra-Sastre was born in Guayama, Puerto Rico, and raised in the town of Arroyo where she received her secondary education. Throughout her childhood, she dreamed of becoming an astronaut. After graduating from Carmen Bozello de Huyke High School, she attended the Mayagüez Campus of the University of Puerto Rico where she earned a Bachelor of Science degree in Chemistry. After teaching to middle and high school students for a year and a half, Sierra-Sastre participated as a post-baccalaureate in an internship at Stanford University, where she was first introduced to nanotechnology research. During that time, she was trained in the self-assembly of mono-molecular films. Upon returning to Puerto Rico, she enrolled in graduate Environmental Health courses at the School of Public Health at the University of Puerto Rico Medical Science Campus and graduate Chemistry courses at the University of Puerto Rico at Río Piedras. Sierra-Sastre then attended Cornell University, where she earned a Ph.D. in 2009 in materials chemistry and nanotechnology and specialized in the nanofabrication of semi-conductor nanowires. As part of her doctoral studies, Sierra-Sastre researched at Los Alamos National Laboratory. \n\nNASA realized that, in long duration missions, astronauts do not eat enough. They experience \"menu fatigue\" and their overall food intake declines, which puts them at risk of nutritional deficiency, loss of bone and muscle mass, and lethargy. The NASA Human Research Program is funding a project called \"HI-SEAS,\" an acronym for “Hawaii Space Exploration Analog and Simulation,” to determine the best way to keep astronauts well nourished during multiple-year missions to Mars or the moon.\n\nA planetary module was built on the Hawaiian island of Mauna Loa, which simulates the living conditions for astronauts at a future base on Mars. It is believed that the saddle area side of the island resembles a Martian environment.\nAfter receiving more than 700 applications for positions as crew members of the simulated Mars mission, NASA and researchers from Cornell University and the University of Hawaii at Mānoa chose six astronaut-like individuals. Sierra-Sastre was part of the selected crew with qualifications similar to those required by NASA for their astronaut applicants. The other five are Dr. Oleg Abramov, Simon Engler, Kate Greene, Sian Proctor and Angelo Vermeulen. Three additional individuals will make up the reserve crew.\n\nPrior to the four-month mission, which began on March 2013, the crew participated in a two-week training session. Once the mission began the crew was not to be allowed to have any communication with the outside world, except for limited e-mail. If they step outside the module they will have to wear simulated space suits. During the mission they will test new forms of food and food preparation strategies for deep-space travel.\n\nSierra-Sastre and her colleagues emerged from the space habitat on August 13, ending the four-month simulated mission. According to Sierra-Sastre and the scientists involved in the mission, the food ingredients that will be essential for future space missions on Mars or the moon will include spices, herbs and hot sauce. Also comfort foods such as Nutella, peanut butter and margarine. Plus, certain foods which contain ingredients which are rich in fiber.\n\nIn November 2011, NASA announced 15 available positions for its team of astronauts. Sierra-Sastre sent her application in January 2012. Her participation in the Mars project puts her one step closer to achieving her dream of becoming the first female Puerto Rican astronaut.\n\nSierra-Sastre lives in Ithaca, New York. She is a freelance educator working with disadvantaged school districts and communities in Puerto Rico. Sierra-Sastre is also a research scientist and program manager at the U.S. Bureau of Engraving and Printing's (BEP's) Office of Technology Development.\nSierra-Sastre also collaborates in a variety of projects with the National Nanotechnology Infrastructure Network and the Cornell Nanoscale Facility, which included writing the smallest US National Anthem ever written, \"La Borinqueña Más Pequeña\" (the Puerto Rican national anthem). Among the scientific works which she has co-authored is the 2008 article \"Vertical Growth of Ge Nanowires from Biotemplated Au Nanoparticle Catalysts,\" published in the Journal of the American Chemical Society, volume 130, issue 10488.\n"}
