{"id": "2231930", "url": "https://en.wikipedia.org/wiki?curid=2231930", "title": "Acentric factor", "text": "Acentric factor\n\nThe acentric factor formula_1 is a conceptual number introduced by Kenneth Pitzer in 1955, proven to be very useful in the description of matter. It has become a standard for the phase characterization of single & pure components. The other state description parameters are molecular weight, critical temperature, critical pressure, and critical volume (or critical compressibility).The acentric factor is said to be a measure of the non-sphericity (centricity) of molecules. As it increases, the vapor curve is \"pulled\" down, resulting in higher boiling points.\n\nIt is defined as:\n\nwhere\nformula_3 is the reduced temperature,\nformula_4 is the reduced saturation vapor pressure.\n\nFor many monatomic fluids\nis close to 0.1, therefore formula_6. In many cases, formula_7 lies above the boiling temperature of liquids at atmosphere pressure.\n\nValues of formula_1 can be determined for any fluid from accurate experimental vapor pressure data. Preferably, these data should first be regressed against a reliable vapor pressure equation such as the following:\n\nln(P) = A + B/T +C*ln(T) + D*T^6\n\n(This equation fits vapor pressure over a very wide range of temperature for most components, but is by no means the only one that should be considered.) In this regression, a careful check for erroneous vapor pressure measurements must be made, preferably using a log(P) vs. 1/T graph, and any obviously incorrect or dubious values should be discarded. The regression should then be re-run with the remaining good values until a good fit is obtained. The vapor pressure at Tr=0.7 can then be used in the defining equation, above, to estimate acentric factor.\n\nThen, using the known critical temperature, Tc, find the temperature at Tr = 0.7. At this temperature, calculate the vapor pressure, Psat, from the regressed equation.\n\nThe definition of formula_1 gives a zero-value for the noble gases argon, krypton, and xenon. formula_1 is very close to zero for other spherical molecules.\n\nBy definition, a van der Waals fluid has a critical compressibility of 3/8 and an acentric factor of about −0.302024, indicating a small ultra-spherical molecule. A Redlich-Kwong fluid has a critical compressibility of 1/3 and an acentric factor of about 0.058280, close to nitrogen; without the temperature dependence of its attractive term, its acentric factor would be only -0.293572.\n\n"}
{"id": "873947", "url": "https://en.wikipedia.org/wiki?curid=873947", "title": "Aleksandr Ivanchenkov", "text": "Aleksandr Ivanchenkov\n\nAleksandr Sergeyevich Ivanchenkov (; born 28 September 1940 ) is a retired Soviet cosmonaut who flew as Flight Engineer on Soyuz 29 and Soyuz T-6, he spent 147 days, 12 hours and 37 minutes in space.\n\nIvanchenkov is married with one child. He was selected as a cosmonaut on 27 March 1973. He retired on 3 November 1993.\n\n\n\n"}
{"id": "362207", "url": "https://en.wikipedia.org/wiki?curid=362207", "title": "Alexander Lippisch", "text": "Alexander Lippisch\n\nAlexander Martin Lippisch (November 2, 1894 – February 11, 1976) was a German aeronautical engineer, a pioneer of aerodynamics who made important contributions to the understanding of tailless aircraft, delta wings and the ground effect, and also worked in the U.S. His most famous designs are the Messerschmitt Me 163 rocket-powered interceptor and the Dornier Aerodyne.\n\nLippisch was born in Munich, Kingdom of Bavaria. He later recalled that his interest in aviation began with a demonstration conducted by Orville Wright over Tempelhof Field in Berlin in September 1909. Nonetheless, he planned to follow his father's footsteps into art school, until the outbreak of World War I intervened. During his service with the German Army, between 1915–1918, Lippisch had the chance to fly as an aerial photographer and mapper.\n\nFollowing the war, Lippisch worked with the Zeppelin Company, and it was at this time that he first became interested in tailless aircraft. In 1921, his first design to be built, by his friend Gottlob Espenlaub, was the Espenlaub E-2 glider. This was the beginning of a research programme that would result in some fifty designs throughout the 1920s and 1930s. Lippisch's growing reputation saw him appointed in 1925 as the director of the Rhön-Rossitten Gesellschaft (RRG), a glider organisation including research groups and construction facilities.\n\nLippisch also designed conventional gliders at this time, including the Wien of 1927 and its successor the Fafnir of 1930. In 1928, Lippisch's tail-first Ente (\"Duck\") became the first aircraft to fly under rocket power. From 1927, he resumed his tailless work, leading to a series of designs named Storch I – Storch IX (Stork I-IX), mostly gliders. These designs attracted little interest from the government and private industry.\n\nExperience with the Storch series led Lippisch to concentrate increasingly on delta-winged designs. The Delta I was the world's first tailless delta wing aircraft to fly (in 1931). This interest resulted in five aircraft, numbered Delta I – Delta V, which were built between 1931 and 1939. In 1933, RGG had been reorganised into the \"Deutsche Forschungsanstalt für Segelflug\" (German Institute for Sailplane Flight, \"DFS\") and the Delta IV and Delta V were designated as the DFS 39 and DFS 40 respectively.\n\nIn early 1939, the \"Reichsluftfahrtsministerium\" (RLM, Reich Aviation Ministry) transferred Lippisch and his team to work at the Messerschmitt factory, in order to design a high-speed fighter aircraft around the rocket engines then under development by Hellmuth Walter. The team quickly adapted their most recent design, the DFS 194, to rocket power, the first example successfully flying in early 1940. This successfully demonstrated the technology for what would become the Messerschmitt Me 163 Komet.\n\nAlthough technically novel, the Komet did not prove to be a successful weapon and friction between Lippisch and Messerschmitt was frequent. In 1943, Lippisch transferred to Vienna's Aeronautical Research Institute (\"Luftfahrtforschungsanstalt Wien\", \"LFW\"), to concentrate on the problems of high-speed flight. That same year, he was awarded a doctoral degree in engineering by the University of Heidelberg.\n\nWind tunnel research in 1939 had suggested that the delta wing was a good choice for supersonic flight, and Lippisch set to work designing a supersonic, ramjet-powered fighter, the Lippisch P.13a. By the time the war ended, however, the project had only advanced as far as a development glider, the DM-1.\n\nLike many German scientists, Lippisch was taken to the United States after the war under Operation Paperclip at the White Sands Missile Range.\n\nAdvances in jet engine design were making Lippisch's ideas more practical, and Convair became interested in a hybrid (mixed power) jet/rocket design which they proposed as the F-92. In order to gain experience with the delta wing handling at high speeds, they first built a test aircraft, the 7002 which, on June 9, 1948, became the first jet-powered delta-wing aircraft to fly. Although the U.S. Air Force lost interest in the F-92, the next test model 7003 was designated the XF-92A. This led Convair to proposing delta wing for most of their projects through the 1950s and into the 1960s, including the F-102 Delta Dagger, F-106 Delta Dart and B-58 Hustler.\n\nFrom 1950–1964, Lippisch worked for the Collins Radio Company in Cedar Rapids, Iowa, which had an aeronautical division. It was during this time that his interest shifted toward ground effect craft. The results were an unconventional VTOL aircraft (eventually becoming the Dornier Aerodyne) and an aerofoil boat research seaplane X-112, flown in 1963. However, Lippisch contracted cancer, and resigned from Collins.\n\nWhen he recovered in 1966, he formed his own research company, Lippisch Research Corporation, and attracted the interest of the West German government. Prototypes for both the aerodyne and the ground-effect craft RFB X-113 (1970) then RFB X-114 (1977) were built, but no further development was undertaken. The Kiekhaefer Mercury company was also interested in his ground-effect craft and successfully tested one of his designs as the Aeroskimmer, but also eventually lost interest.\n\nLippisch died in Cedar Rapids on February 11, 1976. In 1985, he was inducted into the International Air & Space Hall of Fame at the San Diego Air & Space Museum.\n\n\n\n"}
{"id": "59056569", "url": "https://en.wikipedia.org/wiki?curid=59056569", "title": "Argument technology", "text": "Argument technology\n\nArgument technology is a sub-field of artificial intelligence. A few decades ago, philosophical theories of arguments were leveraged to handle key computational challenges, such as modeling non-monotonic reasoning and designing robust coordination protocols for multi-agent systems. This soon led to several theoretical breakthroughs, which, in turn, fostered the development of argument-based applications in a variety of domains, e.g., education, healthcare, policy making, and risk management. As a result, argument technology is a thriving interdisciplinary enterprise with its own variety of sub-fields and methodologies.\n\nAn argument assistant is a software which conveniences users when writing arguments. Argument assistants can convenience users as they compose content and as they review content from one other, including in dialogical contexts. In addition to Web services, such functionalities can be provided through the plugin architectures of word processor software or those of Web browsers. Internet forums, for instance, can be greatly enhanced by such software tools and services.\n\nArguBlogging is software which allows its users to select portions of hypertext on webpages in their Web browsers and to agree or disagree with the selected content, posting their arguments to their blogs with linked argument data. It is implemented as a bookmarklet, adding functionality to Web browsers and interoperating with blogging platforms such as Blogger and Tumblr.\n\nArgument maps are visual, diagrammatic representations of arguments. Such visual diagrams facilitate diagrammatic reasoning and promote one's ability to grasp and to make sense of information rapidly and readily. Argument maps can provide structured, semi-formal frameworks for representing arguments using interactive visual language.\n\nArgument mining, or argumentation mining, is a research area within the natural language processing field. The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs.\n\nAn argument search engine is a search engine that is given a topic as a user query and returns a list of arguments for and against the topic. Such engines could be used to support informed decision-making or to help debaters prepare for debates.\n\nAn artificial debater is an artificial intelligence system which can debate with human users.\n\nThe goal of automated argumentative essay scoring systems is to assist students in improving their writing skills by measuring the quality of their argumentative content.\n\nArgument technology can enhance decision support systems and intelligent decision support systems.\n\nAn ethical decision support system is a decision support system which supports users in moral reasoning and decision-making.\n\nAn ethical decision support system is a decision support system which supports users in legal reasoning and decision-making.\n\nAn explainable or transparent artificial intelligence system is an artificial intelligence system whose actions can be easily understood by humans.\n\nAn intelligent tutoring system is a computer system that aims to provide immediate and customized instruction or feedback to learners, usually without requiring intervention from a human teacher.\n\nAn intelligent tutoring system for argumentation tutors students in critical thinking and argumentative proficiency.\n\nAn intelligent tutoring system for ethics tutors students in moral reasoning, debate and the history of ethics.\n\nAn intelligent tutoring system for law tutors students in legal reasoning, debate, legal history and philosophy.\n\nAn intelligent tutoring system for mathematics tutors students in the conceptual and procedural knowledge of mathematics and in mathematical proof.\n\nAn intelligent tutoring system for philosophy tutors students in critical thinking, the history of philosophy and debate.\n\nA legal expert system is a domain-specific expert system that uses artificial intelligence to emulate the decision-making abilities of a human expert in the field of law.\n\nMachine ethics is a part of the ethics of artificial intelligence concerned with the moral behavior of artificially intelligent beings. As humans argue with respect to morality and moral behavior, argument can be envisioned as a component of machine ethics systems and moral reasoning components.\n\nIn computer science and mathematical logic, a proof assistant or interactive theorem prover is a software tool to assist with the development of formal proofs by human-machine collaboration. This involves some sort of interactive proof editor, or other interface, with which a human can guide the search for proofs, the details of which are stored in, and some steps provided by, a computer.\n"}
{"id": "15862193", "url": "https://en.wikipedia.org/wiki?curid=15862193", "title": "BIG-NSE", "text": "BIG-NSE\n\nThe Berlin Graduate School of Natural Sciences and Engineering (BIG-NSE) is part of the Cluster of Excellence \"Unifying Concepts in Catalysis\" (UniCat) founded in November 2007 by the Technical University of Berlin and five further institutions in the Berlin area within the framework of the German government‘s Excellence Initiative.\n\nThe main research interest of the UniCat and BIG-NSE Faculty is Catalysis, in a broad sense. The research fields involved cover a broad range of topics, from natural sciences to engineering. The faculty consists of internationally renowned professors and junior researchers from 54 research groups at 6 participating institutions and active in 13 research fields, who will be intensively involved in the supervision and mentoring of the BIG-NSE students, among them the Fritz Haber Institute of the Max Planck Society, the working place of Professor Gerhard Ertl, the winner of the Nobel Prize in Chemistry 2007.\n\nVision of the research is to unify concepts in catalysis by bridging the gaps between homogeneous and heterogeneous catalysis, between elementary gas-phase reactions and complex processes in highly organised biological systems, as well as between fundamental and applied catalysis research.\n\nThe BIG-NSE offers a structured curriculum for obtaining the degree of \"Doctor\" within 3 years. The main characteristic of the BIG-NSE is a comprehensive integration and mentoring programme for its students, especially foreign students. It includes:\n\n\nThe entry requirements for the BIG-NSE are: \n\n\n"}
{"id": "37691915", "url": "https://en.wikipedia.org/wiki?curid=37691915", "title": "Biological basis of personality", "text": "Biological basis of personality\n\nThe biological basis of personality is the collection of brain systems and mechanisms that underlie human personality. Human neurobiology, especially as it relates to complex traits and behaviors, is not well understood, but research into the neuroanatomical and functional underpinnings of personality are an active field of research. Animal models of behavior, molecular biology, and brain imaging techniques have provided some insight into human personality, especially trait theories.\n\nMuch of the current understanding of personality from a neurobiological perspective places an emphasis on the biochemistry of the behavioral systems of reward, motivation, and punishment. This has led to a few biologically based personality theories such as Eysenck's three factor model of personality, Grey's reinforcement sensitivity theory (RST), and Cloninger's model of personality. The Big Five model of personality is not biologically based; yet some research in the differences in brain structures provided biological support also for this model.\n\nPersonality can be defined as a set of characteristics or traits that drive individual differences in human behavior. From a biological perspective, these traits can be traced back to brain structures and neural mechanisms. However, this definition and theory of biological basis is not universally accepted. There are many conflicting theories of personality in the fields of psychology, psychiatry, philosophy, and neuroscience. A few examples of this are the nature vs. nurture debate and how the idea of a 'soul' fits into biological theories of personality.\n\nSince the time of the ancient Greeks, humankind has attempted to explain personality through spiritual beliefs, philosophy, and psychology. Historically, studies of personality have traditionally come from the social sciences and humanities, but in the past two decades neuroscience has begun to be more influential in the understanding of human personality.\n\nHowever, the most cited and influential figures in publishing the first biology-based personality theories are Hans Eysenck and Jeffrey Alan Gray. Eysenck used both behavioral and psychophysiological methodologies to test and develop his theories.\nHe published a book in 1947 called \"Dimensions of Personality\", describing the personality dimensions of Extraversion and Neuroticism. Gray, a student of Eysenck, studied personality traits as individual differences in sensitivity to rewarding and punishing stimuli. The significance of Gray's work and theories was his use of biology to define behavior, which stimulated a lot of subsequent research.\n\nIn 1951, Hans Eysenck and Donald Prell published an experiment in which identical (monozygotic) and fraternal (dizygotic) twins, ages 11 and 12, were tested for neuroticism. It is described in detail in an article published in the \"Journal of Mental Science\". in which Eysenck and Prell concluded that, \"The factor of neuroticism is not a statistical artifact, but constitutes a biological unit which is inherited as a whole...neurotic predisposition is to a large extent hereditarily determined.\"\n\nThe idea of biology-based personality research is relatively new, but growing in interest and number of publications. In August 2004, there was a conference specifically on the topic, called \"The Biological Basis of Personality and Individual Differences\". This allowed for presenting and sharing of ideas between psychologists, psychiatrists, molecular geneticists, and neuroscientists, and eventually gave birth to the book under the same title. The book is a collection of current research (as of 2006) in the field contributed by many authors and edited by Turhan Canli. Recently, psychology professor Colin G. DeYoung has even named the idea as the field of 'Personality Neuroscience'.\n\nThere are many experimental techniques for measuring the biology of the brain, but there are five main methods used to investigate the biological basis of personality. The biological data from these methods are commonly correlated with personality traits. These personality traits are often determined by personality questionnaires. However, personality questionnaires may be biased because they are self-reported. As a result, scientists emphasize using several different measures of personality, rather than solely self-reported measures of personality. For example, another measure of personality traits is observation of behavior. Both humans and animals have been observed to measure personality traits, but animals are particularly useful for studying the long-term behavioral-biological relationship of personality.\n\nAnother interesting method that has become more sophisticated and affordable to researchers is the method of whole genome expression analysis. This method involves collecting data for a large number of genes simultaneously which provides many advantages in studying personality. In an article written by Alison M. Bell and Nadia Aubin-Horth, they describe the advantages very clearly by stating, \"For one, it is probable that the genetic basis of personality is polygenic, so it makes sense to simultaneously study many genes. In addition, gene products rarely act alone. Instead, they perform their function by interacting together in pathways and networks. As a result, the molecular changes that characterize a phenotype are frequently not based on a single marker or gene, but rather on an entire pathway. Whole genome expression profiling therefore has the potential to reveal new candidates genes and pathways.\"\n\nThe biology-based personality theories (discussed below) are based on correlating personality traits with behavioral systems related to motivation, reward, and punishment. On a broad level, this involves the autonomic nervous system, fear-processing circuits in the amygdala, the reward pathway from the ventral tegmental area (VTA) to the nucleus accumbens and prefrontal cortex. All of these circuits heavily rely on neurotransmitters and their precursors, but there has been the most research support for dopamine and serotonin pathways:\n\n\nPrevious studies show that genes account for at most 50 percent of a given trait. However, it is widely accepted that variance in gene sequence affect behavior, and genes are a significant risk factor for personality disorders. With the growing interest in using molecular genetics in tracing the biological basis of personality, there may be more gene-trait links found in the future.\n\nVarying polymorphisms and sequence repeats in the gene for dopamine receptor D4 and serotonin transporter gene 5-HTTLPR, have both been found to influence the extraversion trait in adults. Specifically, study participants with at least one copy of the 7-repeat variant of the dopamine receptor D4 gene had higher scores of self-reported extraversion. This suggests that dopamine and serotonin interact to regulate the conflicting behavioral traits of careless exploration vs. cautious inhibition.\n\nSynaptic plasticity refers to the ability of neurons to strengthen or weaken the connections between them. According to Hebbian theory, these connections are strengthened and maintained through repeated stimulation between neurons. Specifically, there is an emphasis on long-term potentiation (LTP), which is the prolonged strengthening of synaptic connections that facilitate learning from experience.\n\nOn a larger scale, there are many pathways and brain regions that are interdependent and contribute to a cohesive, stable personality. For example, the amygdala and hippocampus of the limbic system mediate emotional intensity and consolidate memory of these experiences. But the basic mechanism by which these pathways and brain regions perform these functions, is synaptic plasticity. Ultimately, it boils down to this feature of neurons that allows the brain to learn from repeated experiences, retain memories, and ultimately maintain personality. Joseph LeDoux, an award-winning neuroscientist, asserts that although humans share the same brain systems, it is the unique wiring of neurons that is different in each person and makes their personality.\n\nThere are many theories of personality that on the identification of a set of traits that encompass human personality. Few however, are biologically based. This section will describe some theories of personality that have a biological basis. Additionally, it will present biological support for a popular non-biologically based personality theory, the Five Factor Model.\n\nEysenck's three-factor model of personality was a causal theory of personality based on activation of reticular formation and limbic system. The reticular formation is a region in the brainstem that is involved in mediating arousal and consciousness. The limbic system is involved in mediating emotion, behavior, motivation, and long-term memory. \n\nGray's reinforcement sensitivity theory (RST) is based on the idea that there are three brain systems that all differently respond to rewarding and punishing stimuli. \n\nThis model of personality is based on the idea that different responses to punishing, rewarding, and novel stimuli the main characteristics of the human mind is caused by an interaction of the three dimensions below:\n\nIn one MRI study, Novelty Seeking correlated with increased grey matter volume in regions of the cingulate cortex, Harm Avoidance correlated with decreased grey matter volume in the orbitofrontal, occipital, and parietal cortex. Reward Dependence correlated with decreased grey matter volume in the caudate nucleus.\n\nThe five factor model is a widely used personality assessment that describes five core traits that a person possesses:\n\nUsing an MRI, one study found correlation between the volumes of certain brain areas with each of the five traits in the Five Factor Model. Their results found that Openness/Intellect did not have any significant correlation with the volume of any brain structures. Conscientiousness was associated with increased volume in the lateral prefrontal cortex, a region involved in planning and the voluntary control of behavior. Extraversion was associated with increased volume of medial orbitofrontal cortex, a region involved in processing reward information. Agreeableness was associated with increased volume in regions that process information about the intentions and mental states of other individuals. Neuroticism was associated with increased volume of brain regions associated with threat, punishment, and negative emotions.\n\n"}
{"id": "29333326", "url": "https://en.wikipedia.org/wiki?curid=29333326", "title": "Boydell Glacier", "text": "Boydell Glacier\n\nBoydell Glacier () is a glacier on Trinity Peninsula in northern Graham Land. It is about long, flowing southeastward from Detroit Plateau to enter Sjögren Inlet in Prince Gustav Channel north of the terminus of Sjögren Glacier and west of Mount Wild. It was mapped by the Falkland Islands Dependencies Survey from surveys (1960–61), and named by the UK Antarctic Place-Names Committee for James Boydell, English inventor of a steam traction engine, the first practical track-laying vehicle.\n\n\n"}
{"id": "22221404", "url": "https://en.wikipedia.org/wiki?curid=22221404", "title": "Charged-particle equilibrium", "text": "Charged-particle equilibrium\n\nIn radiological physics, charged-particle equilibrium (CPE) occurs when the number of charged particles leaving a volume is equal to the number entering, for each energy and type of particle. When CPE exists in an irradiated medium, the absorbed dose in the volume is equal to the collision kerma. \n\nIn order for this to occur, energy is needed. \n"}
{"id": "21701952", "url": "https://en.wikipedia.org/wiki?curid=21701952", "title": "Clientele effect", "text": "Clientele effect\n\nThe clientele effect is the idea that the set of investors attracted to a particular kind of security will affect the price of the security when policies or circumstances change.\nFor instance, some investors want a company that doesn't pay dividends but instead invests that money in growing the business, whereas other investors prefer a stock that pays a high dividend, and still others want one that balances payout and reinvestment. If a company changes its dividend policy substantially, it is said to be subject to a clientele effect as some of its investors (its established clientele) decide to sell the security due to the change. Although commonly used in reference to dividend or coupon (interest) rates, it can also be used in the context of leverage (debt levels), changes in line of business, taxes, and other aspects of the company.\n\n"}
{"id": "38292", "url": "https://en.wikipedia.org/wiki?curid=38292", "title": "Conference of European Schools for Advanced Engineering Education and Research", "text": "Conference of European Schools for Advanced Engineering Education and Research\n\nThe Conference of European Schools for Advanced Engineering Education and Research (CESAER) is a non-profit association of leading engineering universities in Europe. CESAER was set up on May 10, 1990, with headquarters in the Castle of Arenberg in Leuven, Belgium. The main objectives are to provide high quality engineering education in Europe and to improve links between association members in research, as well as postgraduate and continuing education.\n\n"}
{"id": "289702", "url": "https://en.wikipedia.org/wiki?curid=289702", "title": "DARPA Grand Challenge", "text": "DARPA Grand Challenge\n\nThe DARPA Grand Challenge is a prize competition for American autonomous vehicles, funded by the Defense Advanced Research Projects Agency, the most prominent research organization of the United States Department of Defense. Congress has authorized DARPA to award cash prizes to further DARPA's mission to sponsor revolutionary, high-payoff research that bridges the gap between fundamental discoveries and military use. The initial DARPA Grand Challenge was created to spur the development of technologies needed to create the first fully autonomous ground vehicles capable of completing a substantial off-road course within a limited time. The third event, the DARPA Urban Challenge extended the initial Challenge to autonomous operation in a mock urban environment. The most recent Challenge, the 2012 DARPA Robotics Challenge, focused on autonomous emergency-maintenance robots.\n\nFully autonomous vehicles have been an international pursuit for many years, from endeavors in Japan (starting in 1977), Germany (Ernst Dickmanns and VaMP), Italy (the ARGO Project), the European Union (EUREKA Prometheus Project), the United States of America, and other countries. DARPA funded the development of the first fully autonomous robot beginning in 1966 with the Shakey the robot project at Stanford Research Institute, now SRI International. The first autonomous ground vehicle capable of driving on and off roads was developed by DARPA as part of the Strategic Computing Initiative beginning in 1984 leading to demonstrations of autonomous navigation by the Autonomous Land Vehicle and the Navlab.\n\nThe Grand Challenge was the first long distance competition for driverless cars in the world; other research efforts in the field of driverless cars take a more traditional commercial or academic approach. The U.S. Congress authorized DARPA to offer prize money ($1 million) for the first Grand Challenge to facilitate robotic development, with the ultimate goal of making one-third of ground military forces autonomous by 2015. Following the 2004 event, Dr. Tony Tether, the director of DARPA, announced that the prize money had been increased to $2 million for the next event, which was claimed on October 9, 2005. The first, second and third places in the 2007 Urban Challenge received $2 million, $1 million, and $500,000, respectively.\n\nThe competition was open to teams and organizations from around the world, as long as there were at least one U.S. citizen on the roster. Teams have participated from high schools, universities, businesses and other organizations. More than 100 teams registered in the first year, bringing a wide variety of technological skills to the race. In the second year, 195 teams from 36 U.S. states and 4 foreign countries entered the race.\n\nThe first competition of the DARPA Grand Challenge was held on March 13, 2004 in the Mojave Desert region of the United States, along a route that follows along the path of Interstate 15 from just before Barstow, California to just past the California–Nevada border in Primm.\nNone of the robot vehicles finished the route. Carnegie Mellon University's Red Team and car Sandstorm (a converted Humvee) traveled the farthest distance, completing of the course before getting hung up on a rock after making a switchback turn. No winner was declared, and the cash prize was not given. Therefore, a second DARPA Grand Challenge event was scheduled for 2005.\n\nThe second competition of the DARPA Grand Challenge began at 6:40am on October 8, 2005. All but one of the 23 finalists in the 2005 race surpassed the distance completed by the best vehicle in the 2004 race. Five vehicles successfully completed the 212 km (132 mi) course:\n\nVehicles in the 2005 race passed through three narrow tunnels and negotiated more than 100 sharp left and right turns. The race concluded through Beer Bottle Pass, a winding mountain pass with a sheer drop-off on one side and a rock face on the other. Although the 2004 course required more elevation gain and some very sharp switchbacks (Daggett Ridge) were required near the beginning of the route, the course had far fewer curves and generally wider roads than the 2005 course.\nThe natural rivalry between the teams from Stanford and Carnegie Mellon (Sebastian Thrun, head of the Stanford team was previously a faculty member at Carnegie Mellon and colleague of Red Whittaker, head of the CMU team) was played out during the race. Mechanical problems plagued H1ghlander before it was passed by Stanley. Gray Team's entry was a miracle in itself, as the team from the suburbs of New Orleans was caught in Hurricane Katrina a few short weeks before the race. The fifth finisher, Terramax, a 30,000 pound entry from Oshkosh Truck, finished on the second day. The huge truck spent the night idling on the course, but was particularly nimble in carefully picking its way down the narrow roads of Beer Bottle Pass.\n\nThe third competition of the DARPA Grand Challenge, known as the \"Urban Challenge\", took place on November 3, 2007 at the site of the now-closed George Air Force Base (currently used as Southern California Logistics Airport), in Victorville, California (Google map). The course involved a urban area course, to be completed in less than 6 hours. Rules included obeying all traffic regulations while negotiating with other traffic and obstacles and merging into traffic.\n\nUnlike previous challenges, the 2007 Urban Challenge organizers divided competitors into two \"tracks,\" A and B. All Track A and Track B teams were part of the same competition circuit, but the teams chosen for the Track A program received US $1 million in funding. These 11 teams largely represented major universities and large corporate interests such as CMU teaming with GM as Tartan Racing, Stanford teaming with Volkswagen, Virginia Tech teaming with TORC Technologies as VictorTango, Oshkosh Truck, Honeywell, Raytheon, Caltech, Autonomous Solutions, Cornell, and MIT. One of the few independent entries in Track A was the Golem Group. DARPA has not publicly explained the rationale behind the selection of Track A teams.\n\nTeams were given maps sparsely charting the waypoints that defined the competition courses. At least one team, Tartan Racing, enhanced the maps through the insertion of additional extrapolated waypoints for improved navigation. A debriefing paper published by Team Jefferson illustrates graphically the contrast between the course map it was given by DARPA and the course map used by Tartan Racing.\n\nTartan Racing claimed the $2 million prize with their vehicle \"Boss\", a Chevy Tahoe. The second-place finisher earning the $1 million prize was the Stanford Racing Team with their entry \"Junior\", a 2006 Volkswagen Passat. Coming in third place was team VictorTango, winning the $500,000 prize with their 2005 Ford Escape hybrid, \"Odin\". MIT placed 4th, with Cornell University and University of Pennsylvania/Lehigh University also completing the course.\n\nThe six teams that successfully finished the entire course:\nWhile the 2004 and 2005 events were more physically challenging for the vehicles, the robots operated in isolation and only encountered other vehicles on the course when attempting to pass. The Urban Challenge required designers to build vehicles able to obey all traffic laws while they detect and avoid other robots on the course. This is a particular challenge for vehicle software, as vehicles must make \"intelligent\" decisions in real time based on the actions of other vehicles. Other than previous autonomous vehicle efforts that focused on structured situations such as highway driving with little interaction between the vehicles, this competition operated in a more cluttered urban environment and required the cars to perform sophisticated interactions with each other, such as maintaining precedence at a 4-way stop intersection.\n\nThe DARPA Robotics Challenge is an ongoing competition focusing on humanoid robotics. The primary goal of the program is to develop ground robotic capabilities to execute complex tasks in dangerous, degraded, human-engineered environments. It launched in October 2012, and hosted the Virtual Robotics Competition in June 2013. Two more competitions are planned: the DRC Trials in December 2013, and the DRC Finals in December 2014.\n\nUnlike prior Challenges, the construction of the \"vehicles\" will not be part of the scope of the Robotics Challenge. In August 2012 DARPA announced Boston Dynamics would act as sole source for the robots to be used in the challenge, awarding them a contract to develop and build 8 identical robots based on the PETMAN project for the software teams to use. The amount contracted was $10,882,438 cost-plus-fixed-fee contract and work is expected to be completed by Aug. 9, 2014.\n\nOn April 22, 2013, DARPA awarded a $1 million prize to \"Ground Systems\", a 3-person team with members in Ohio, Texas and California, as the winner of the Fast Adaptable Next-Generation Ground Vehicle (FANG) Mobility/Drivetrain Challenge. Team Ground Systems' final design submission received the highest score when measured against the established requirements for system performance and manufacturability. Since the beginning of the first FANG Challenge on January 14, 2013, more than 1,000 participants within more than 200 teams used the META design tools and the VehicleFORGE collaboration platform developed by Vanderbilt University in Nashville, Tennessee, to design and simulate the performance of thousands of potential mobility and drivetrain subsystems. The goal of the FANG program is to test the specially developed META design tools, model libraries and the VehicleFORGE platform, which were created to significantly compress the design-to-production time of a complex defense system.\n\nA technology paper and source code for the computer vision machine learning component of the 2005 Stanford entry has been published.\n\n2007 Urban Challenge teams employed a variety of different software and hardware combinations for interpreting sensor data, planning, and execution. Some examples:\n\n\n"}
{"id": "8487", "url": "https://en.wikipedia.org/wiki?curid=8487", "title": "David Brewster", "text": "David Brewster\n\nSir David Brewster (11 December 178110 February 1868) was a British scientist, inventor, author, and academic administrator. In science he is principally remembered for his experimental work in physical optics, mostly concerned with the study of the polarization of light and including the discovery of Brewster's angle. He studied the birefringence of crystals under compression and discovered photoelasticity, thereby creating the field of optical mineralogy. For this work, William Whewell dubbed him the \"father of modern experimental optics\" and \"the Johannes Kepler of optics.\"\n\nA pioneer in photography, Brewster invented an improved stereoscope, which he called \"lenticular stereoscope\" and which became the first portable 3D-viewing device. He also invented the binocular camera, two types of polarimeters, the polyzonal lens, the lighthouse illuminator, and the kaleidoscope.\n\nBrewster was a Presbyterian and walked arm in arm with his brother on the Disruption procession which formed the Free Church of Scotland. As a historian of science, Brewster focused on the life and work of his hero, Isaac Newton. Brewster published a detailed biography of Newton in 1831 and later became the first scientific historian to examine many of the papers in Newton's \"Nachlass\". Brewster also wrote numerous works of popular science, and was one of the founders of the British Science Association, of which he was elected President in 1849. He became the public face of higher education in Scotland, serving as Principal of the University of St Andrews (1837–59) and later of the University of Edinburgh (1859–68). Brewster also edited the 18-volume \"Edinburgh Encyclopædia\".\n\nDavid Brewster was born at the Canongate in Jedburgh, Roxburghshire, to Margaret Key (1753–1790) and James Brewster (c. 1735–1815), the rector of Jedburgh Grammar School and a teacher of high reputation. David was the third of six children, two daughters and four sons: James (1777–1847), minister at Craig, Ferryden; David; George (1784–1855), minister at Scoonie, Fife; and Patrick (1788–1859), minister at the abbey church, Paisley.\n\nAt the age of 12, David was sent to the University of Edinburgh (graduating MA in 1800), being intended for the clergy. He was licensed a minister of the Church of Scotland, and preached around Edinburgh on several occasions. He had already shown a strong inclination for natural science, and this had been fostered by his intimacy with a \"self-taught philosopher, astronomer and mathematician\", as Sir Walter Scott called him, of great local fame, James Veitch of Inchbonny, a man who was particularly skilful in making telescopes.\n\nThough Brewster duly finished his theological studies and was licensed to preach, his other interests distracted him from the duties of his profession. In 1799 fellow-student Henry Brougham persuaded him to study the diffraction of light. The results of his investigations were communicated from time to time in papers to the \"Philosophical Transactions\" of London and other scientific journals. The fact that other scientists – notably Étienne-Louis Malus and Augustin Fresnel – were pursuing the same investigations contemporaneously in France does not invalidate Brewster's claim to independent discovery, even though in one or two cases the priority must be assigned to others. A lesser-known classmate of his, Thomas Dick, also went on to become a popular astronomical writer.\n\nThe most important subjects of his inquiries can be enumerated under the following five headings:\n\nIn this line of investigation, the prime importance belongs to the discovery of\n\nThese discoveries were promptly recognised. As early as 1807 the degree of LL.D. was conferred upon Brewster by Marischal College, Aberdeen; in 1815 he was elected a Fellow of the Royal Society of London, and received the Copley Medal; in 1818 he received the Rumford Medal of the society; and in 1816 the French Institute awarded him one-half of the prize of three thousand francs for the two most important discoveries in physical science made in Europe during the two preceding years. In 1821, he was made a foreign member of the Royal Swedish Academy of Sciences, and in 1822 a Foreign Honorary Member of the American Academy of Arts and Sciences.\nAmong the non-scientific public, his fame spread more effectually by his invention in about 1815 of the kaleidoscope, for which there was a great demand in both the United Kingdom, France, and the United States. As a reflection of this fame, Brewster portrait was later printed in some cigar boxes. Brewster chose renowned achromatic lens developer Philip Carpenter as the sole manufacturer of the kaleidoscope in 1817. Although Brewster patented the kaleidoscope in 1817 (GB 4136), a copy of the prototype was shown to London opticians and copied before the patent was granted. As a consequence, the kaleidoscope became produced in large numbers, but yielded no direct financial benefits to Brewster. It proved to be a massive success with two hundred thousand kaleidoscopes sold in London and Paris in just three months.\nAn instrument of more significance, the stereoscope, which – though of much later date (1849) – along with the kaleidoscope did more than anything else to popularise his name, was not as has often been asserted the invention of Brewster. Sir Charles Wheatstone discovered its principle and applied it as early as 1838 to the construction of a cumbersome but effective instrument, in which the binocular pictures were made to combine by means of mirrors. A dogged rival of Wheatstone's, Brewster was unwilling to credit him with the invention, however, and proposed that the true author of the stereoscope was a Mr. Elliot, a \"Teacher of Mathematics\" from Edinburgh, who, according to Brewster, had conceived of the principles as early as 1823 and had constructed a lensless and mirrorless prototype in 1839, through which one could view drawn landscape transparencies, since photography had yet to be invented. Brewster's personal contribution was the suggestion to use prisms for uniting the dissimilar pictures; and accordingly the lenticular stereoscope may fairly be said to be his invention.\n\nA much more valuable and practical result of Brewster's optical researches was the improvement of the British lighthouse system. Although Fresnel, who had also the satisfaction of being the first to put it into operation, perfected the dioptric apparatus independently, Brewster was active earlier in the field than Fresnel, describing the dioptric apparatus in 1812. Brewster pressed its adoption on those in authority at least as early as 1820, two years before Fresnel suggested it, and it was finally introduced into lighthouses mainly through Brewster's persistent efforts.\n\nAlthough Brewster's own discoveries were important, they were not his only service to science. He began writing in 1799 as a regular contributor to the \"Edinburgh Magazine\", of which he acted as editor at the age of twenty. In 1807, he undertook the editorship of the newly projected \"Edinburgh Encyclopædia\", of which the first part appeared in 1808, and the last not until 1830. The work was strongest in the scientific department, and many of its most valuable articles were from the pen of the editor. At a later period he was one of the leading contributors to the \"Encyclopædia Britannica\" (seventh and eighth editions) writing, among others, the articles on electricity, hydrodynamics, magnetism, microscope, optics, stereoscope, and voltaic electricity. He was elected a member of the American Antiquarian Society in 1816.\n\nIn 1819 Brewster undertook further editorial work by establishing, in conjunction with Robert Jameson (1774–1854), the \"Edinburgh Philosophical Journal\", which took the place of the \"Edinburgh Magazine\". The first ten volumes (1819–1824) were published under the joint editorship of Brewster and Jameson, the remaining four volumes (1825–1826) being edited by Jameson alone. After parting company with Jameson, Brewster started the \"Edinburgh Journal of Science\" in 1824, 16 volumes of which appeared under his editorship during the years 1824–1832, with very many articles from his own pen.\n\nHe contributed around three hundred papers to the transactions of various learned societies, and few of his contemporaries wrote as much for the various reviews. In the \"North British Review\" alone, seventy-five articles of his appeared. A list of his larger separate works will be found below. Special mention, however, must be made of the most important of them all: his biography of Sir Isaac Newton. In 1831 he published the \"Life of Sir Isaac Newton\", a short popular account of the philosopher's life, in \"Murray's Family Library\", followed by an 1832 American edition in Harper's Family Library; but it was not until 1855 that he was able to issue the much fuller \"Memoirs of the Life, Writings and Discoveries of Sir Isaac Newton\", a work which embodied the results of more than 20 years' investigation of original manuscripts and other available sources.\n\nBrewster's position as editor brought him into frequent contact with the most eminent scientific men, and he was naturally among the first to recognise the benefit that would accrue from regular communication among those in the field of science. In a review of Charles Babbage's book \"Decline of Science in England\" in \"John Murray's Quarterly Review\", he suggested the creation of \"an association of our nobility, clergy, gentry and philosophers\". This was taken up by various \"Declinarians\" and found speedy realisation in the British Association for the Advancement of Science. Its first meeting was held at York in 1831; and Brewster, along with Babbage and Sir John Herschel, had the chief part in shaping its constitution.\n\nIn the same year in which the British Association held its first meeting, Brewster received the honour of knighthood and the decoration of the Royal Guelphic Order. In 1838, he was appointed principal of the united colleges of St Salvator and St Leonard, University of St Andrews. In 1849, he acted as president of the British Association and was elected one of the eight foreign associates of the Institute of France in succession to J. J. Berzelius; and ten years later, he accepted the office of principal of the University of Edinburgh, the duties of which he discharged until within a few months of his death. In 1855, the government of France made him an Officier de la Légion d'honneur.\n\nHe was a close friend of William Henry Fox Talbot, inventor of the calotype process, who sent Brewster early examples of his work. It was Brewster who suggested Talbot only patent his process in England, initiating the development of early photography in Scotland and eventually allowing for the formation of the first photographic society in the world, the Edinburgh Calotype Club, in 1843. Brewster was a prominent member of the club until its dissolution sometime in the mid-1850s; however, his interest in photography continued, and he was elected the first President of the Photographic Society of Scotland when it was founded in 1856.\n\nOf a high-strung and nervous temperament, Brewster was somewhat irritable in matters of controversy; but he was repeatedly subjected to serious provocation. He was a man of highly honourable and fervently religious character. In estimating his place among scientific discoverers, the chief thing to be borne in mind is that his genius was not characteristically mathematical. His method was empirical, and the laws that he established were generally the result of repeated experiment. To the ultimate explanation of the phenomena with which he dealt he contributed nothing, and it is noteworthy although he did not maintain to the end of his life the corpuscular theory he never explicitly adopted the wave theory of light. Few would dispute the verdict of James David Forbes, an editor of the eighth edition of the \"Encyclopædia Britannica\": \"His scientific glory is different in kind from that of Young and Fresnel; but the discoverer of the law of polarization of biaxial crystals, of optical mineralogy, and of double refraction by compression, will always occupy a foremost rank in the intellectual history of the age.\" In addition to the various works of Brewster already mentioned, the following may be added: \"Notes and Introduction to Carlyle's translation of Legendre's Elements of Geometry\" (1824); \"Treatise on Optics\" (1831); \" Letters on Natural Magic\", addressed to Sir Walter Scott (1832) \"The Martyrs of Science, or the Lives of Galileo, Tycho Brahe, and Kepler\" (1841); \"More Worlds than One\" (1854).\n\nIn his \"Treatise\" he demonstrated that vegetal colors were related with the absorption spectra and he described for the first time the red fluorescence of chlorophyll.\n\nAs well as his many scientific works and biographies of notable scientists, Brewster also wrote 'The History of Free Masonry, Drawn from Authentic Sources of Information; with an Account of the Grand Lodge of Scotland, from Its Institution in 1736, to the Present Time', published in 1804, when he was only 23. The work was commissioned by Alexander Lawrie, publisher to the Grand Lodge of Scotland, to whom the work has been, frequently, mis-attributed. The book has become one of the standard works on early Scottish freemasonry, although there is no evidence that Brewster was a freemason at the time he wrote the book, nor any that he became one later.\n\nBrewster's Christian beliefs stirred him to respond against the idea of the transmutation of species and the theory of evolution. His opinion was that \"science and religion must be one since each dealt with Truth, which had only one and the same Author.\" In 1845 he wrote a highly critical review of the evolutionist work \"Vestiges of the Natural History of Creation\", in the \"North British Review\". which he considered to be an insult to Christian revelation and a dangerous example of materialism.\n\nIn 1862, he responded to Darwin's \"On the Origin of Species\" and published the article \"\" in \"Good Words\". He stated that Darwin's book combined both \"interesting facts and idle fancies\" which made up a \"dangerous and degrading speculation\". He accepted adaptive changes, but he strongly opposed Darwin's statement about the \"primordial form\", which he considered an offensive idea to \"both the naturalist and the Christian.\"\n\nBrewster married twice. His first wife, Juliet Macpherson (c. 1776–1850), was a daughter of James Macpherson (1736–1796), a probable translator of Ossian poems. They married on 31 July 1810 in Edinburgh and had four sons and a daughter:\n\nBrewster married a second time in Nice, on 26 (or 27) March 1857, to Jane Kirk Purnell (b. 1827), the second daughter of Thomas Purnell of Scarborough. Lady Brewster famously fainted at the Oxford evolution debate of 30 June 1860. Brewster died in 1868, and was buried at Melrose Abbey, next to his first wife and second son. The physics building at Heriot-Watt University is named in his honour.\n\nA bust of Brewster is in the Hall of Heroes of the National Wallace Monument in Stirling.\n\nBrewster's views on the possibility of evolution of intelligence on other planets, contrasted with the opinion of William Whewell, are cited in the novel \"Barchester Towers\".\n\nHe appears as a minor antagonist in the 2015 video game \"Assassin's Creed Syndicate\" as a scientist working for the game's opposing faction. He is assassinated by one of the protagonists, Evie Frye.\n\nA street within the Kings Buildings complex (science buildings linked to Edinburgh University) was named in his memory in 2015.\n\n\n\n\n\n \n"}
{"id": "2218382", "url": "https://en.wikipedia.org/wiki?curid=2218382", "title": "De motu corporum in gyrum", "text": "De motu corporum in gyrum\n\nDe motu corporum in gyrum (\"On the motion of bodies in an orbit\") is the presumed title of a manuscript by Isaac Newton sent to Edmond Halley in November 1684. It followed a visit by Halley earlier in that year, when Halley had questioned Newton about problems then exercising the minds of Halley and his scientific circle in London, including Sir Christopher Wren and Robert Hooke.\n\nThe title of the document is only presumed because the original is now lost. Its contents are inferred from surviving documents, which are two contemporary copies and a draft. Only the draft has the title now used; both copies are without title.\n\nThis manuscript (\"De Motu\" for short, but not to be confused with several other Newtonian papers carrying titles that start with these words) gave important mathematical derivations relating to the three relations now known as \"Kepler's laws\" (before Newton's work, these had not been generally regarded as laws). Halley reported the communication from Newton to the Royal Society on 10 December 1684 (Old Style). After further encouragement from Halley, Newton went on to develop and write his book \"Philosophiæ Naturalis Principia Mathematica\" (commonly known as the \"Principia\") from a nucleus that can be seen in \"De Motu\" – of which nearly all of the content also reappears in the \"Principia\".\n\nOne of the surviving copies of \"De Motu\" was made by being entered in the Royal Society's register book, and its (Latin) text is available online.\n\nFor ease of cross-reference to the contents of \"De Motu\" that appeared again in the \"Principia\", there are online sources for the \"Principia\" in English translation, as well as in Latin.\n\n\"De motu corporum in gyrum\" is short enough to set out here the contents of its different sections. It contains 11 propositions, labelled as 'theorems' and 'problems', some with corollaries. Before reaching this core subject-matter, Newton begins with some preliminaries:\n\n\n(Newton's later first law of motion is to similar effect, Law 1 in the \"Principia\".)\n\nThen follow two more preliminary points:\n\n\nThen follows Newton's main subject-matter, labelled as theorems, problems, corollaries and scholia:\n\nTheorem 1 demonstrates that where an orbiting body is subject only to a centripetal force, it follows that a radius vector, drawn from the body to the attracting center, sweeps out equal areas in equal times (no matter how the centripetal force varies with distance). (Newton uses for this derivation – as he does in later proofs in this \"De Motu\", as well as in many parts of the later \"Principia\" – a limit argument of infinitesimal calculus in geometric form, in which the area swept out by the radius vector is divided into triangle-sectors. They are of small and decreasing size considered to tend towards zero individually, while their number increases without limit.) This theorem appears again, with expanded explanation, as Proposition 1, Theorem 1, of the \"Principia\".\n\nTheorem 2 considers a body moving uniformly in a circular orbit, and shows that for any given time-segment, the centripetal force (directed towards the center of the circle, treated here as a center of attraction) is proportional to the square of the arc-length traversed, and inversely proportional to the radius. (This subject reappears as Proposition 4, Theorem 4 in the \"Principia\", and the corollaries here reappear also.)\n\nCorollary 1 then points out that the centripetal force is proportional to V/R, where V is the orbital speed and R the circular radius.\n\nCorollary 2 shows that, putting this in another way, the centripetal force is proportional to (1/P) * R where P is the orbital period.\n\nCorollary 3 shows that if P is proportional to R, then the centripetal force would be independent of R.\n\nCorollary 4 shows that if P is proportional to R, then the centripetal force would be proportional to 1/R.\n\nCorollary 5 shows that if P is proportional to R, then the centripetal force would be proportional to 1/(R).\n\nA scholium then points out that the Corollary 5 relation (square of orbital period proportional to cube of orbital size) is observed to apply to the planets in their orbits around the Sun, and to the Galilean satellites orbiting Jupiter.\n\nTheorem 3 now evaluates the centripetal force in a non-circular orbit, using another geometrical limit argument, involving ratios of vanishingly small line-segments. The demonstration comes down to evaluating the curvature of the orbit as if it were made of infinitesimal arcs, and the centripetal force at any point is evaluated from the speed and the curvature of the local infinitesimal arc. This subject reappears in the \"Principia\" as Proposition 6 of Book 1.\n\nA corollary then points out how it is possible in this way to determine the centripetal force for any given shape of orbit and center.\n\nProblem 1 then explores the case of a circular orbit, assuming the center of attraction is on the circumference of the circle. A scholium points out that if the orbiting body were to reach such a center, it would then depart along the tangent. (Proposition 7 in the \"Principia\".)\n\nProblem 2 explores the case of an ellipse, where the center of attraction is at its center, and finds that the centripetal force to produce motion in that configuration would be directly proportional to the radius vector. (This material becomes Proposition 10, Problem 5 in the \"Principia\".)\n\nProblem 3 again explores the ellipse, but now treats the further case where the center of attraction is at one of its foci. \"A body orbits in an ellipse: there is required the law of centripetal force tending to a focus of the ellipse.\" Here Newton finds the centripetal force to produce motion in this configuration would be inversely proportional to the square of the radius vector. (Translation: 'Therefore the centripetal force is reciprocally as L X SP², that is, (reciprocally) in the doubled ratio [i.e. square] of the distance ... .') This becomes Proposition 11 in the \"Principia\".\n\nA scholium then points out that this Problem 3 proves that the planetary orbits are ellipses with the Sun at one focus. (Translation: 'The major planets orbit, therefore, in ellipses having a focus at the centre of the Sun, and with their \"radii\" (\"vectores\") drawn to the Sun describe areas proportional to the times, altogether (Latin: 'omnino') as Kepler supposed.') (This conclusion is reached after taking as initial fact the observed proportionality between square of orbital period and cube of orbital size, considered in corollary 5 to Theorem 1.) (A controversy over the cogency of the conclusion is described below.) The subject of Problem 3 becomes Proposition 11, Problem 6, in the \"Principia\".\n\nTheorem 4 shows that with a centripetal force inversely proportional to the square of the radius vector, the time of revolution of a body in an elliptical orbit with a given major axis is the same as it would be for the body in a circular orbit with the same diameter as that major axis. (Proposition 15 in the \"Principia\".)\n\nA scholium points out how this enables determining the planetary ellipses and the locations of their foci by indirect measurements.\n\nProblem 4 then explores, for the case of an inverse-square law of centripetal force, how to determine the orbital ellipse for a given starting position, speed, and direction of the orbiting body. Newton points out here, that if the speed is high enough, the orbit is no longer an ellipse, but is instead a parabola or hyperbola. He also identifies a geometrical criterion for distinguishing between the elliptical case and the others, based on the calculated size of the latus rectum, as a proportion to the distance the orbiting body at closest approach to the center. (Proposition 17 in the \"Principia\".)\n\nA scholium then remarks that a bonus of this demonstration is that it allows definition of the orbits of comets, and enables an estimation of their periods and returns where the orbits are elliptical. Some practical difficulties of implementing this are also discussed.\n\nFinally in the series of propositions based on zero resistance from any medium, Problem 5 discusses the case of a degenerate elliptical orbit, amounting to a straight-line fall towards or ejection from the attracting center. (Proposition 32 in the \"Principia\".)\n\nA scholium points out how problems 4 and 5 would apply to projectiles in the atmosphere and to the fall of heavy bodies, if the atmospheric resistance could be assumed nil.\n\nLastly, Newton attempts to extend the results to the case where there is atmospheric resistance, considering first (Problem 6) the effects of resistance on inertial motion in a straight line, and then (Problem 7) the combined effects of resistance and a uniform centripetal force on motion towards/away from the center in a homogeneous medium. Both problems are addressed geometrically using hyperbolic constructions. These last two 'Problems' reappear in Book 2 of the \"Principia\" as Propositions 2 and 3.\n\nThen a final scholium points out how problems 6 and 7 apply to the horizontal and vertical components of the motion of projectiles in the atmosphere (in this case neglecting earth curvature).\n\nAt some points in 'De Motu', Newton depends on matters proved being used in practice as a basis for regarding their converses as also proved. This has been seen as especially so in regard to 'Problem 3'. Newton's style of demonstration in all his writings was rather brief in places; he appeared to assume that certain steps would be found self-evident or obvious. In 'De Motu', as in the first edition of the \"Principia\", Newton did not specifically state a basis for extending the proofs to the converse. The proof of the converse here depends on its being apparent that there is a uniqueness relation, i.e. that in any given setup, only one orbit corresponds to one given and specified set of force/velocity/starting position. Newton added a mention of this kind into the second edition of the \"Principia\", as a Corollary to Propositions 11–13, in response to criticism of this sort made during his lifetime.\n\nA significant scholarly controversy has existed over the question whether and how far these extensions to the converse, and the associated uniqueness statements, are self-evident and obvious or not. (There is no suggestion that the converses are not true, or that they were not stated by Newton, the argument has been over whether Newton's proofs were satisfactory or not.)\n\nThe details of Edmund Halley's visit to Newton in 1684 are known to us only from reminiscences of thirty to forty years later. According to one of these reminiscences, Halley asked Newton, \"...what he thought the Curve would be that would be described by the Planets supposing the force of attraction towards the Sun to be reciprocal to the square of their distance from it.\"\n\nAnother version of the question was given by Newton himself, but also about thirty years after the event: he wrote that Halley, asking him \"if I knew what figure the Planets described in their Orbs about the Sun was very desirous to have my Demonstration\" In light of these differing reports, both produced from old memories, it is hard to know exactly what words Halley used.\n\nIt has been sometimes suggested that Newton answered a question different from the one Halley had asked, but any certainty is clearly hard to obtain on this point.\n\nNewton acknowledged in 1686 that an initial stimulus on him in 1679/80 to extend his investigations of the movements of heavenly bodies had arisen from correspondence with Robert Hooke in 1679/80.\n\nHooke had started an exchange of correspondence in November 1679 by writing to Newton, to tell Newton that Hooke had been appointed to manage the Royal Society's correspondence. Hooke therefore wanted to hear from members about their researches, or their views about the researches of others; and as if to whet Newton's interest, he asked what Newton thought about various matters, and then gave a whole list, mentioning \"compounding the celestial motions of the planetts of a direct motion by the tangent and an attractive motion towards the central body\", and \"my hypothesis of the lawes or causes of springinesse\", and then a new hypothesis from Paris about planetary motions (which Hooke described at length), and then efforts to carry out or improve national surveys, the difference of latitude between London and Cambridge, and other items. Newton replied with \"a fansy of my own\" about determining the Earth's motion, using a falling body. Hooke disagreed with Newton's idea of how the falling body would move, and a short correspondence developed.\n\nLater, in 1686, when Newton's \"Principia\" had been presented to the Royal Society, Hooke claimed from this correspondence the credit for some of Newton's content in the \"Principia\", and said Newton owed the idea of an inverse-square law of attraction to him – although at the same time, Hooke disclaimed any credit for the curves and trajectories that Newton had demonstrated on the basis of the inverse square law.\n\nNewton, who heard of this from Halley, rebutted Hooke's claim in letters to Halley, acknowledging only an occasion of reawakened interest. Newton did acknowledge some prior work of others, including Ismaël Bullialdus, who suggested (but without demonstration) that there was an attractive force from the Sun in the inverse square proportion to the distance, and Giovanni Alfonso Borelli, who suggested (again without demonstration) that there was a tendency towards the Sun like gravity or magnetism that would make the planets move in ellipses; but that the elements Hooke claimed were due either to Newton himself, or to other predecessors of them both such as Bullialdus and Borelli, but not Hooke. Wren and Halley were both sceptical of Hooke's claims, recalling an occasion when Hooke had claimed to have a derivation of planetary motions under an inverse square law, but had failed to produce it even under the incentive of a prize.\n\nThere has been scholarly controversy over exactly what if anything Newton really gained from Hooke, apart from the stimulus that Newton acknowledged.\n\nAbout thirty years after Newton's death in 1727, Alexis Clairaut, one of Newton's early and eminent successors in the field of gravitational studies, wrote after reviewing Hooke's work that it showed \"what a distance there is between a truth that is glimpsed and a truth that is demonstrated\".\n\n\n"}
{"id": "5861496", "url": "https://en.wikipedia.org/wiki?curid=5861496", "title": "Diffuson", "text": "Diffuson\n\nThe diffuson is a mathematical object, which often appears in the theory of disordered electronic systems (a part of condensed matter physics). \n\nIn a disordered system, the motion of an electron is not ballistic, but diffusive: i.e., the electron does not move along a straight line, but experiences a series of random scatterings off of impurities. This random motion (diffusion) is described by a differential equation, known as the diffusion equation. The diffuson is the Green's function of the diffusion equation.\n\nThe diffuson plays an important role in the theory of electron transport in disordered systems, especially for phase coherent effects such as universal conductance fluctuations.\n"}
{"id": "42557005", "url": "https://en.wikipedia.org/wiki?curid=42557005", "title": "Edward Pigott", "text": "Edward Pigott\n\nEdward Pigott (1753–1825) was an English astronomer.\n\nSon of the astronomer Nathaniel Pigott, Pigott's work focused on variable stars. Educated in France with a mother from Louvain, the family moved to York in 1781. Despite their significant age difference, he was a friend and collaborator of John Goodricke (his distant cousin) until the latter's untimely death at the age of 21 in 1786.\n\nIn 1784, Pigott informed the Royal Society of his discovery of a new variable star. This was Eta Aquilae which he had identified the previous year. He corresponded with leading astronomers of the day including William Herschel and Nevil Maskelyne.\n\nPigott moved to Bath in 1796. Pigott's notebooks survive at York City Archives.\n\nAsteroid 10220 Pigott is named after Edward and his father. It was discovered by R. A. Tucker at the observatory in Tucson, Arizona which bears Pigott's name and that of his friend Goodricke.\n\n"}
{"id": "3237181", "url": "https://en.wikipedia.org/wiki?curid=3237181", "title": "Energy Systems Language", "text": "Energy Systems Language\n\nThe Energy Systems Language, also referred to as Energese, Energy Circuit Language, or Generic Systems Symbols, was developed by the ecologist Howard T. Odum and colleagues in the 1950s during studies of the tropical forests funded by the United States Atomic Energy Commission. They are used to compose energy flow diagrams in the field of systems ecology.\n\nThe design intent of the energy systems language was to facilitate the generic depiction of energy flows through any scale system while encompassing the laws of physics, and in particular, the laws of thermodynamics (see energy transformation for an example).\n\nIn particular H.T. Odum aimed to produce a language which could facilitate the intellectual analysis, engineering synthesis and management of global systems such as the geobiosphere, and its many subsystems. Within this aim, H.T. Odum had a strong concern that many abstract mathematical models of such systems were not thermodynamically valid. Hence he used analog computers to make system models due to their intrinsic value; that is, the electronic circuits are of value for modelling natural systems which are assumed to obey the laws of energy flow, because, in themselves the circuits, like natural systems, also obey the known laws of energy flow, where the energy form is electrical. However Odum was interested not only in the electronic circuits themselves, but also in how they might be used as formal analogies for modeling other systems which also had energy flowing through them. As a result, Odum did not restrict his inquiry to the analysis and synthesis of any one system in isolation. The discipline that is most often associated with this kind of approach, together with the use of the energy systems language is known as systems ecology.\n\nWhen applying the electronic circuits (and schematics) to modeling ecological and economic systems, Odum believed that generic categories, or characteristic modules, could be derived. Moreover he felt that a general symbolic system, fully defined in electronic terms (including the mathematics thereof) would be useful for depicting real system characteristics, such as the general categories of production, storage, flow, transformation, and consumption. Central principles of electronics also therefore became central features of the energy systems language – Odum's generic symbolism.\n\nDepicted to the left is what the generic symbol for storage, which Odum named the Bertalanffy module, in honor of the general systems theorist Ludwig von Bertalanffy.\n\nFor Odum, in order to achieve a holistic understanding of how many apparently different systems actually affect each other, it was important to have a generic language with a massively scalable modeling capacity – to model global-to-local, ecological, physical and economic systems. The intention was, and for those who still apply it, is, to make biological, physical, ecological, economic and other system models thermodynamically, and so also energetically, valid and verifiable. As a consequence the designers of the language also aimed to include the energy metabolism of any system within the scope of inquiry.\n\nIn order to aid learning, in \"Modeling for all Scales\" Odum and Odum (2000) suggested systems might first be introduced with pictographic icons, and then later defined in the generic symbolism. Pictograms have therefore been used in software programs like ExtendSim to depict the basic categories of the Energy Systems Language. Some have argued that such an approach shares similar motivations to Otto Neurath's isotype project, Leibniz's (Characteristica Universalis) Enlightenment Project and Buckminster Fuller's works.\n"}
{"id": "9994", "url": "https://en.wikipedia.org/wiki?curid=9994", "title": "Ephemeris time", "text": "Ephemeris time\n\nThe term ephemeris time (often abbreviated ET) can in principle refer to time in connection with any astronomical ephemeris. In practice it has been used more specifically to refer to:\n\n\nMost of the following sections relate to the ephemeris time of the 1952 standard.\n\nAn impression has sometimes arisen that ephemeris time was in use from 1900: this probably arose because ET, though proposed and adopted in the period 1948–1952, was defined in detail using formulae that made retrospective use of the epoch date of 1900 January 0 and of Newcomb's Tables of the Sun.\n\nThe ephemeris time of the 1952 standard leaves a continuing legacy, through its ephemeris second which became closely duplicated in the length of the current standard SI second (see below: Redefinition of the second).\n\nEphemeris time (ET), adopted as standard in 1952, was originally designed as an approach to a uniform time scale, to be freed from the effects of irregularity in the rotation of the earth, \"for the convenience of astronomers and other scientists\", for example for use in ephemerides of the Sun (as observed from the Earth), the Moon, and the planets. It was proposed in 1948 by G M Clemence.\n\nFrom the time of John Flamsteed (1646–1719) it had been believed that the Earth's daily rotation was uniform. But in the later nineteenth and early twentieth centuries, with increasing precision of astronomical measurements, it began to be suspected, and was eventually established, that the rotation of the Earth (\"i.e.\" the length of the day) showed irregularities on short time scales, and was slowing down on longer time scales. The evidence was compiled by W de Sitter (1927) who wrote \"If we accept this hypothesis, then the 'astronomical time', given by the earth's rotation, and used in all practical astronomical computations, differs from the 'uniform' or 'Newtonian' time, which is defined as the independent variable of the equations of celestial mechanics\". De Sitter offered a correction to be applied to the mean solar time given by the Earth's rotation to get uniform time.\n\nOther astronomers of the period also made suggestions for obtaining uniform time, including A Danjon (1929), who suggested in effect that observed positions of the Moon, Sun and planets, when compared with their well-established gravitational ephemerides, could better and more uniformly define and determine time.\n\nThus the aim developed, to provide a new time scale for astronomical and scientific purposes, to avoid the unpredictable irregularities of the mean solar time scale, and to replace for these purposes Universal Time (UT) and any other time scale based on the rotation of the Earth around its axis, such as sidereal time.\n\nG M Clemence (1948) made a detailed proposal of this type based on the results of H Spencer Jones (1939). Clemence (1948) made it clear that his proposal was intended \"for the convenience of astronomers and other scientists only\" and that it was \"logical to continue the use of mean solar time for civil purposes\".\n\nDe Sitter and Clemence both referred to the proposal as 'Newtonian' or 'uniform' time. D Brouwer suggested the name 'ephemeris time'.\n\nFollowing this, an astronomical conference held in Paris in 1950 recommended \"that in all cases where the mean solar second is unsatisfactory as a unit of time by reason of its variability, the unit adopted should be the sidereal year at 1900.0, that the time reckoned in this unit be designated \"ephemeris time\"\", and gave Clemence's formula (see Definition of ephemeris time (1952)) for translating mean solar time to ephemeris time.\n\nThe International Astronomical Union approved this recommendation at its 1952 general assembly. Practical introduction took some time (see Use of ephemeris time in official almanacs and ephemerides); ephemeris time (ET) remained a standard until superseded in the 1970s by further time scales (see Revision).\nDuring the currency of ephemeris time as a standard, the details were revised a little. The unit was redefined in terms of the tropical year at 1900.0 instead of the sidereal year; and the standard second was defined first as 1/31556925.975 of the tropical year at 1900.0, and then as the slightly modified fraction 1/31556925.9747 instead, finally being redefined in 1967/8 in terms of the cesium atomic clock standard (see below).\n\nAlthough ET is no longer directly in use, it leaves a continuing legacy. Its successor time scales, such as TDT, as well as the atomic time scale IAT (TAI), were designed with a relationship that \"provides continuity with ephemeris time\". ET was used for the calibration of atomic clocks in the 1950s. Close equality between the ET second with the later SI second (as defined with reference to the cesium atomic clock) has been verified to within 1 part in 10.\n\nIn this way, decisions made by the original designers of ephemeris time influenced the length of today's standard SI second, and in turn, this has a continuing influence on the number of leap seconds which have been needed for insertion into current broadcast time scales, to keep them approximately in step with mean solar time.\n\nEphemeris time was defined in principle by the orbital motion of the Earth around the Sun, (but its practical implementation was usually achieved in another way, see below).\n\nIts detailed definition depended on Simon Newcomb's Tables of the Sun (1895), interpreted in a new way to accommodate certain observed discrepancies:\n\nIn the introduction to Newcomb's Tables of the Sun (1895) the basis of the tables (p. 9) includes a formula for the Sun's mean longitude, at a time indicated by interval T (in Julian centuries of 36525 mean solar days) reckoned from Greenwich Mean Noon on 0 January 1900:\n\nSpencer Jones' work of 1939 showed that the positions of the Sun actually observed, when compared with those obtained from Newcomb's formula, show the need for the following correction to the formula to represent the observations:\n\n(where \"the times of observation are in Universal time, not corrected to Newtonian time\", and 0.0748B represents an irregular fluctuation calculated from lunar observations).\n\nThus a conventionally corrected form of Newcomb's formula, to incorporate the corrections on the basis of mean solar time, would be the sum of the two preceding expressions:\n\nClemence's 1948 proposal did not adopt a correction of this kind in terms of mean solar time: instead, the same numbers were used as in Newcomb's original uncorrected formula (1), but now in a reverse sense, to define the time and time scale implicitly, based on the real position of the Sun:\n\nwhere the time variable, here represented as E, now represents time in ephemeris centuries of 36525 ephemeris days of 86400 ephemeris seconds. The 1961 official reference put it this way: \"The origin and rate of ephemeris time are defined to make the Sun's mean longitude agree with Newcomb's expression\"\n\nFrom the comparison of formulae (2) and (3), both of which express the same real solar motion in the same real time but on different time scales, Clemence arrived at an explicit expression, estimating the difference in seconds of time between ephemeris time and mean solar time, in the sense (ET-UT):\n\nClemence's formula, now superseded by more modern estimations, was included in the original conference decision on ephemeris time. In view of the fluctuation term, practical determination of the difference between ephemeris time and UT depended on observation. Inspection of the formulae above shows that the (ideally constant) unit of ephemeris time such as the ephemeris second has been for the whole of the twentieth century very slightly shorter than the corresponding (but not precisely constant) unit of mean solar time (which besides its irregular fluctuations tends gradually to increase), consistently also with the modern results of Morrison and Stephenson (see article ΔT).\n\nAlthough ephemeris time was defined in principle by the orbital motion of the Earth around the Sun, it was usually measured in practice by the orbital motion of the Moon around the Earth. These measurements can be considered as secondary realizations (in a metrological sense) of the primary definition of ET in terms of the solar motion, after a calibration of the mean motion of the Moon with respect to the mean motion of the Sun.\n\nReasons for the use of lunar measurements were practically based: the Moon moves against the background of stars about 13 times as fast as the Sun's corresponding rate of motion, and the accuracy of time determinations from lunar measurements is correspondingly greater.\n\nWhen ephemeris time was first adopted, time scales were still based on astronomical observation, as they always had been. The accuracy was limited by the accuracy of optical observation, and corrections of clocks and time signals were published in arrear.\n\nA few years later, with the invention of the cesium atomic clock, an alternative offered itself. Increasingly, after the calibration in 1958 of the cesium atomic clock by reference to ephemeris time, cesium atomic clocks running on the basis of ephemeris seconds began to be used and kept in step with ephemeris time. The atomic clocks offered a further secondary realization of ET, on a quasi-real time basis that soon proved to be more useful than the primary ET standard: not only more convenient, but also more precisely uniform than the primary standard itself. Such secondary realizations were used and described as 'ET', with an awareness that the time scales based on the atomic clocks were not identical to that defined by the primary ephemeris time standard, but rather, an improvement over it on account of their closer approximation to uniformity. The atomic clocks gave rise to the atomic time scale, and to what was first called Terrestrial Dynamical Time and is now Terrestrial Time, defined to provide continuity with ET.\n\nThe availability of atomic clocks, together with the increasing accuracy of astronomical observations (which meant that relativistic corrections were at least in the foreseeable future no longer going to be small enough to be neglected), led to the eventual replacement of the ephemeris time standard by more refined time scales including terrestrial time and barycentric dynamical time, to which ET can be seen as an approximation.\n\nIn 1976 the IAU resolved that the theoretical basis for its current (1952) standard of Ephemeris Time was non-relativistic, and that therefore, beginning in 1984, Ephemeris Time would be replaced by two relativistic timescales intended to constitute dynamical timescales: Terrestrial Dynamical Time (TDT) and Barycentric Dynamical Time (TDB). Difficulties were recognized, which led to these being in turn superseded in the 1990s by time scales Terrestrial Time (TT), Geocentric Coordinate Time GCT(TCG) and Barycentric Coordinate Time BCT(TCB).\n\nHigh-precision ephemerides of sun, moon and planets were developed and calculated at the Jet Propulsion Laboratory (JPL) over a long period, and the latest available were adopted for the ephemerides in the Astronomical Almanac starting in 1984. Although not an IAU standard, the ephemeris time argument T has been in use at that institution since the 1960s. The time scale represented by T has been characterized as a relativistic coordinate time that differs from Terrestrial Time only by small periodic terms with an amplitude not exceeding 2 milliseconds of time: it is linearly related to, but distinct (by an offset and constant rate which is of the order of 0.5 s/a) from the TCB time scale adopted in 1991 as a standard by the IAU. Thus for clocks on or near the geoid, T (within 2 milliseconds), but not so closely TCB, can be used as approximations to Terrestrial Time, and via the standard ephemerides T is in widespread use.\n\nPartly in acknowledgement of the widespread use of T via the JPL ephemerides, IAU resolution 3 of 2006 (re-)defined Barycentric Dynamical Time (TDB) as a current standard. As re-defined in 2006, TDB is a linear transformation of TCB. The same IAU resolution also stated (in note 4) that the \"independent time argument of the JPL ephemeris DE405, which is called T\" (here the IAU source cites), \"is for practical purposes the same as TDB defined in this Resolution\". Thus the new TDB, like T, is essentially a more refined continuation of the older ephemeris time ET and (apart from the periodic fluctuations) has the same mean rate as that established for ET in the 1950s.\n\nEphemeris time based on the standard adopted in 1952 was introduced into the Astronomical Ephemeris (UK) and the American Ephemeris and Nautical Almanac, replacing UT in the main ephemerides in the issues for 1960 and after. (But the ephemerides in the Nautical Almanac, by then a separate publication for the use of navigators, continued to be expressed in terms of UT.) The ephemerides continued on this basis through 1983 (with some changes due to adoption of improved values of astronomical constants), after which, for 1984 onwards, they adopted the JPL ephemerides.\n\nPrevious to the 1960 change, the 'Improved Lunar Ephemeris' had already been made available in terms of ephemeris time for the years 1952-1959 (computed by W J Eckert from Brown's theory with modifications recommended by Clemence (1948)).\n\nSuccessive definitions of the unit of ephemeris time are mentioned above (History). The value adopted for the 1956/1960 standard second:\n\nwas obtained from the linear time-coefficient in Newcomb's expression for the solar mean longitude (above), taken and applied with the same meaning for the time as in formula (3) above. The relation with Newcomb's coefficient can be seen from:\n\nCaesium atomic clocks became operational in 1955, and quickly confirmed the evidence that the rotation of the earth fluctuated randomly. This confirmed the unsuitability of the mean solar second of Universal Time as a measure of time interval for the most precise purposes. After three years of comparisons with lunar observations, Markowitz et al. (1958) determined that the ephemeris second corresponded to 9 192 631 770 ± 20 cycles of the chosen cesium resonance.\n\nFollowing this, in 1967/68, the General Conference on Weights and Measures (CGPM) replaced the definition of the SI second by the following:\nThe second is the duration of 9 192 631 770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom.\n\nAlthough this is an independent definition that does not refer to the older basis of ephemeris time, it uses the same quantity as the value of the ephemeris second measured by the cesium clock in 1958. This SI second referred to atomic time was later verified by Markowitz (1988) to be in agreement, within 1 part in 10, with the second of ephemeris time as determined from lunar observations.\n\nFor practical purposes the length of the ephemeris second can be taken as equal to the length of the second of Barycentric Dynamical Time (TDB) or Terrestrial Time (TT) or its predecessor TDT.\n\nThe difference between ET and UT is called ΔT; it changes irregularly, but the long-term trend is parabolic, decreasing from ancient times until the nineteenth century, and increasing since then at a rate corresponding to an increase in the solar day length of 1.7 ms per century (see leap seconds).\n\nInternational Atomic Time (TAI) was set equal to UT2 at 1 January 1958 0:00:00 . At that time, ΔT was already about 32.18 seconds. The difference between Terrestrial Time (TT) (the successor to ephemeris time) and atomic time was later defined as follows:\n\nThis difference may be assumed constant—the rates of TT and TAI are designed to be identical.\n\n"}
{"id": "10759350", "url": "https://en.wikipedia.org/wiki?curid=10759350", "title": "Erich Traub", "text": "Erich Traub\n\nErich Traub (27 June 1906 – 18 May 1985) was a German veterinarian, scientist and virologist who specialized in foot-and-mouth disease, Rinderpest and Newcastle disease. Traub was a member of the National Socialist Motor Corps (NSKK), a Nazi motorist corps, from 1938 to 1942. He worked directly for Heinrich Himmler, head of the Schutzstaffel (SS), as the lab chief of the Nazis' leading bio-weapons facility on Riems Island.\n\nTraub was rescued from the Soviet zone of Germany after World War II and taken to the United States in 1949 under the auspices of the United States government program Operation Paperclip, meant to exploit the post-war scientific knowledge in Germany, and deny it to the Soviet Union.\n\nDuring the 1930s, he studied on a fellowship at the Rockefeller Institute for Medical Research in Princeton, New Jersey mentored by Richard Shope, performing research on vaccines and viruses, including pseudorabies virus and lymphocytic choriomeningitis virus (LCM). During his stay in the United States, Traub and his wife were listed as members of the German American Bund, a pro-Nazi German-American club thirty miles west of Plum Island in Yaphank, Long Island, from 1934 to 1935.\n\nTraub worked at the University of Giessen, Germany, from 1938 to 1942. Traub was a member of the Nazi NSKK, a motorist corps, from 1938 to 1942. The NSKK was declared a condemned, not a criminal organization at the Nuremberg trials.\n\nFrom 1942 to 1948, Traub worked as lab-chief at the Reich Research Institute for Virus Diseases of Animals () on Riems Island (), a German animal virus research institute in the Baltic sea, now named the Friedrich Loeffler Institute. The institute was headed by Prof. Dr. Otto Waldmann from 1919 to 1948, while Traub was vice-president.\n\nThe Institute at Riems Island was a dual use facility during the Second World War where at least some biological warfare experiments were conducted. It had been founded in 1909-10 to study foot-and-mouth disease in animals and by World War II employed about 20 scientists and a staff of 70-120. Hanns-Christoph Nagel, a veterinarian and biological warfare expert for the German Army, conducted experiments there, as did Traub.\n\nThe institute was administered under the \"Innenministerium\" (Ministry of the Interior), which Reichsführer-SS Heinrich Himmler took over in 1943. The chain of command was Himmler, Dr. Leonardo Conti (Reich Health Leader), Kurt Blome, Waldmann, and then Traub. Traub specialized in viral and bacterial diseases. He was assisted by Anna Bürger, who was later also brought to the United States after the war, to work with the Navy's biological warfare program.\n\nOn orders from Himmler and Blome, the Deputy Reich Health Leader and head of the German biological warfare program, Traub worked on weaponizing foot-and-mouth disease virus, which has been reported to have been dispersed by aircraft onto cattle and reindeer in Russia. In 1944, Blome sent Traub to pick up a strain of Rinderpest virus in Turkey; upon his return, this strain proved inactive (nonvirulent) and therefore plans for a Rinderpest product were shelved.\n\nImmediately after the war Traub was trapped in the Soviet zone of Allied occupied Germany. He was forced to work for the Soviets from his lab on Riems Island. In July 1948, the British evacuated Erich Traub from Riems Island as a \"high priority Intelligence target\" since it was now in the Soviet Zone and they feared that Traub was assisting in their biological warfare program. Traub denied this, however, claiming that his only interest was foot-and-mouth disease in animals.\n\nTraub was brought to the United States in 1949 under the auspices of the United States government program Operation Paperclip, meant to exploit scientific knowledge in Germany, and deny it to the Soviet Union. From 1949 to 1953, he was associated with the Naval Medical Research Institute in Bethesda, Maryland.\n\nMonths into his Operation Paperclip contract, Traub was asked to meet with US scientists from Fort Detrick, the Army’s biological warfare headquarters, in Frederick, Maryland. As a noted German authority on viruses he was asked to consult on their animal disease program from a Biological Warfare perspective. Traub discussed work done at the Reich Research Institute for Virus Diseases of Animals on Riems Island during World War II for the Nazis, and work done after the war there for the Russians. Traub gave a detailed explanation of the secret operation at the Institute, and his activities there. This information provided the ground work for Fort Detrick's offshore germ warfare animal disease lab on Plum Island.\n\nHis publicly published research from his time in the United States reports disease research not directly related to weaponization. In 1951, he published a report for the Naval Medical Research Institute on Newcastle Disease virus in chicken and mammalian blood cells. Two years later, he published a paper for the Navy on the mechanisms of immunity in chickens to Newcastle and the possible role of cellular factors. Also in 1953, he published another paper for the Navy with Worth I. Capps on the foot-and-mouth disease virus and methods for rapid adaptation.\n\nTraub served as an expert on foot-and-mouth disease for the FAO of the UN in Bogota, Colombia, from 1951 to 1952, in Tehran, Iran, from 1963 to 1967, and in Ankara, Turkey, from 1969 to 1971.\n\nAfter working on biological research for the U.S. Navy from 1949 to 1953, Traub returned to Germany and founded a new branch of the Loeffler Institut in Tübingen, and headed it from 1953 to 1963. In 1960, Traub resigned as Tübingen’s director due to the scandal related to accusations of financial embezzlement. He continued with limited lab research for three more years, but then ended his career at Tübingen.\n\nIn 1964, Traub published a study for the Army Biological labs in Frederick, Maryland on Eastern Equine Encephalomyeltitis (EEE) immunity in white mice and its relationship to Lymphocytic choriomeningitis (LCM), which had long been a research interest of his.\n\nHe retired from the West German civil service in 1971. In 1972, on the occasion of the 500th anniversary of Ludwig Maximilians University of Munich Traub received an honorary doctorate degree in Veterinary Medicine for his achievements in basic and applied Virology (basic research on LCM; definition and diagnosis of type strains of FMD and their variants; development of adsorbate vaccines against fowl plague, Teschner disease of swine, and erysipelas of swine).\n\nOn May 18, 1985, Traub died in his sleep in West Germany. He was seventy-eight years old.\n\nIn theory, insects of all types, particularly the biting species, can be used as disease vectors in a biological warfare program. Germany, Japan, Britain, Russia and the U.S. all conducted experiments along these lines during the Second World War, and the Japanese used such insect-borne diseases against both soldiers and civilians in China. This was one reason that President Franklin Roosevelt and Secretary of War Henry Stimson ordered the creation of an American biological warfare program in 1942, which was headquartered at Camp Detrick, Maryland. This eventually grew to a very large facility with 245 buildings and a $60 million budget, including an Entomological Weapons Department that mass-produced flies, lice and mosquitoes as disease vectors. Although the British bio-weapon facility at Porton Down concentrated on the production of anthrax bombs, it also conducted experiments on insects as vectors.\n\nAfter the war, the Army's 406th Medical General Laboratory in Japan cooperated with former scientists from Unit 731 in experimenting with many different insect vectors, including lice, flies, mosquitoes, ticks, fleas, spiders and beetles to carry a wide variety of diseases, from cholera to meningitis. At Fort Detrick in the late 1940s, Theodore Rosebury also rated insect vectors very highly, and its entomological division had at least three insect-vectored weapons ready for use by 1950. Some of these were later tested at the Dugway Proving Grounds in Utah, and allegedly used during the Korean War as well.\n\nTraub visited the Plum Island Animal Disease Center (PIADC) in New York on at least three occasions in the 1950s. The Plum Island facility, operated by the Department of Agriculture, conducted research on foot-and-mouth disease (FMD) of cattle, one of Traub's areas of expertise. Traub was offered a leading position at Plum Island in 1958 which he officially declined. It has been alleged that the United States performed bioweapons research on Plum Island.\n\nFort Terry on Plum Island was part of the U.S. biological warfare program in 1944-46, working on veterinary testing in connection with the weaponization of brucellosis. After the war, research on biological weapons continued at Pine Bluff in Arkansas and Fort Detrick, Maryland, while officially at least Plum Island was transferred to the U.S. Department of Agriculture. From 1949, Plum Island also conducted work on biological weapons against animals and livestock, such as foot-and-mouth disease, Rinderpest, Newcastle disease, African swine fever and plague and malaria in birds. Traub's research work from the Second World War onward involved at least the first three of these (all dangerous only to non-human animal species).\n\n\n\n"}
{"id": "172394", "url": "https://en.wikipedia.org/wiki?curid=172394", "title": "Georg von Peuerbach", "text": "Georg von Peuerbach\n\nGeorg von Peuerbach (also Purbach, Peurbach, Purbachius; born May 30, 1423 – April 8, 1461) was an Austrian astronomer, mathematician and instrument maker, best known for his streamlined presentation of Ptolemaic astronomy in the \"Theoricae Novae Planetarum.\"\n\nLittle is known of Peuerbach's life before he enrolled at the University of Vienna in 1446. He received his Bachelor of Arts in 1448. His curriculum was most likely composed primarily of humanities courses, as was usual at the time. His knowledge of astronomy probably derived from independent study, as there were no professors of astronomy at the University of Vienna during Peuerbach's enrollment.\n\nFrom 1448 to 1451 Peuerbach traveled through central and southern Europe, most notably in Italy, giving lectures on astronomy. His lectures led to offers of professorships at several universities, including those at Bologna and Padua. During this time he also met Italian astronomer Giovanni Bianchini of Ferrara. He returned to the University of Vienna in 1453, earned his Masters of Arts, and began lecturing on Latin poetry.\n\nIn 1454 Peuerbach was appointed court astrologer to King Ladislas V of Bohemia and Hungary. It was in this capacity that Peuerbach first met Ladislas' cousin Frederick who was then serving as guardian to the 14-year-old king and who would later become Frederick III, Holy Roman Emperor. Ladislas resided primarily in Prague and Vienna, allowing Peuerbach to maintain his position at the University of Vienna. During this time Peuerbach met Johannes Müller von Königsberg, better known as Regiomontanus. Müller was currently a student at the university and, after he graduated in 1452 at the age of 15, began collaborating extensively with Peuerbach in his astronomical work.\n\nIn 1457, following the assassination of two notable political figures, Ladislas fled Vienna. He died of Leukemia later that year. Rather than taking of service with either of Ladislas' Successors, Peuerbach accepted an appointment as court astrologer to Frederick III.\n\nOne of Peuerbach's best known works is his \"Theoricae Novae Planetarum.\" It began as a series of lectures transcribed by Regiomontanus. The \"Theoricae Novae\" was an attempt to present Ptolemaic astronomy in a more elementary and comprehensible way. The book was very successful, replacing the older \"Theoricae Planetarum Communis\" as the standard university text on astronomy and was studied by many later-influential astronomers including Nicolaus Copernicus and Johannes Kepler.\n\nIn 1457 Peuerbach observed an eclipse and noted that it had occurred 8 minutes earlier than had been predicted by the Alphonsine Tables, the best available eclipse tables at the time. He then computed his own set of eclipse tables, the \"Tabulae Eclipsium.\" Widely read in manuscript form beginning around 1459 and formally published in 1514, these tables remained highly influential for many years.\n\nPeuerbach wrote various papers on practical mathematics, and constructed various astronomical instruments. Most notably, he computed sine tables based on techniques developed by Arabian mathematicians.\n\nIn 1460, Cardinal Johannes Bessarion, while visiting Frederick's court seeking assistance in a crusade to reclaim Constantinople from the Turks, proposed that Peuerbach and Regiomontanus create a new translation of Ptolemy's Almagest from the original Greek. Bessarion thought that a shorter and more clearly written version of the work would make a suitable teaching text. Peuerbach accepted the task and worked on it with Regiomontanus until his death in 1461, at which time 6 volumes had been completed. Regiomontanus completed the project, the final version containing 13 volumes.\n\n\n\n"}
{"id": "38499294", "url": "https://en.wikipedia.org/wiki?curid=38499294", "title": "Guild socialism", "text": "Guild socialism\n\nGuild socialism is a political movement advocating workers' control of industry through the medium of trade-related guilds \"in an implied contractual relationship with the public\". It originated in the United Kingdom and was at its most influential in the first quarter of the 20th century. It was strongly associated with G. D. H. Cole and influenced by the ideas of William Morris.\n\nGuild socialism was partly inspired by the guilds of craftsmen and other skilled workers which had existed in England in the Middle Ages. In 1906, Arthur Penty published \"Restoration of the Gild System\" in which he opposed factory production and advocated a return to an earlier period of artisanal production organised through guilds. The following year, the journal \"The New Age\" became an advocate of guild socialism, although in the context of modern industry rather than the medieval setting favoured by Penty.\n\nIn 1914, S. G. Hobson, a leading contributor to \"The New Age\", published \"National Guilds: An Inquiry into the Wage System and the Way Out\". In this work, guilds were presented as an alternative to state control of industry or conventional trade union activity. Guilds, unlike the existing trade unions, would not confine their demands to matters of wages and conditions but would seek to obtain control of industry for the workers whom they represented. Ultimately, industrial guilds would serve as the organs through which industry would be organised in a future socialist society.\n\nThe guild socialists \"stood for state ownership of industry, combined with ‘workers’ control’ through delegation of authority to national guilds organized internally on democratic lines. About the state itself they differed, some believing it would remain more or less in its existing form and others that it would be transformed into a federal body representing the workers’ guilds, consumers’ organizations, local government bodies, and other social structures.\"\n\nErnst Wigforss—a leading theorist of the Social Democratic Party of Sweden—was also inspired by and stood ideologically close to the ideas of Fabian Society and the guild socialism inspired by people like R. H. Tawney, L.T. Hobhouse and J. A. Hobson. He made contributions in his early writings about industrial democracy and workers' self-management.\n\nThe theory of guild socialism was developed and popularised by G. D. H. Cole who formed the National Guilds League in 1915 and published several books on guild socialism, including \"Self-Government in Industry\" (1917) and \"Guild Socialism Restated\" (1920). A National Building Guild was established after World War I but collapsed after funding was withdrawn in 1921.\n\nAdmiration of guild socialism led to a more \"individualistic\" form of it being suggested as a natural outcome for a united humanity in the science fiction work of Olaf Stapledon-although hundreds of years in the future.\n\nCole's ideas were also promoted by prominent anti-communist intellectuals such as the British logician Bertrand Russell, and the American liberal reformer John Dewey\n\n"}
{"id": "37160709", "url": "https://en.wikipedia.org/wiki?curid=37160709", "title": "Harald Kylin", "text": "Harald Kylin\n\nJohan Harald Kylin (5 February 1879 – 16 December 1949) was a Swedish botanist specializing in phycology and a professor at Lund University. He was also editor of the Botaniska Notiser, a Swedish scientific periodical from 1922 to 1928.\n\n"}
{"id": "13138678", "url": "https://en.wikipedia.org/wiki?curid=13138678", "title": "Hassan Farsam", "text": "Hassan Farsam\n\nHassan Farsam (September 27, 1932 in Tehran – February 5, 2016) was an Iranian pharmacist and medical chemist. He studied pharmacy at Tehran University and became a medical chemistry specialist in 1960. Farsam received post doctorate in Pharmaceutical Chemistry from Paris University (1964) and University of California (1988). He was a Professor of Medical Chemistry in Faculty of Pharmacy of Tehran University of Medical Sciences. He was also a permanent member of the Iranian Academy of Medical Sciences. He published more than 100 original research papers in international and local journals, and he also trained several generations of pharmacy students. He supervised many students and inspired them to pursue a research career in the pharmaceutical fields. \n\n\n\n\n"}
{"id": "9183439", "url": "https://en.wikipedia.org/wiki?curid=9183439", "title": "Hawking (2004 film)", "text": "Hawking (2004 film)\n\nHawking is a BBC television film about Stephen Hawking's early years as a PhD student at Cambridge University, following his search for the beginning of time, and his struggle against motor neuron disease. It stars Benedict Cumberbatch as Hawking and premiered in the UK in April 2004.\n\nThe film received acclaim, with critics particularly lauding Cumberbatch's performance as Hawking. It was nominated for Best Single Drama in the BAFTA TV Awards in 2005. Cumberbatch won the Golden Nymph for Best Performance by an Actor in a TV Film or Miniseries, and received his first nomination for a BAFTA TV Award for Best Actor.\n\nCumberbatch's portrayal of Hawking was the first portrayal of the physicist on screen not by himself.\n\nAt Stephen Hawking's 21st birthday party he meets a new friend, Jane Wilde. There is a strong attraction between the two and Jane is intrigued by Stephen's talk of stars and the universe, but realises that there is something very wrong with Stephen when he suddenly finds that he is unable to stand up. A stay in hospital results in a horrifying diagnosis. Stephen is suffering from motor neurone disease and doctors don't expect him to survive for more than two years. Stephen returns to Cambridge where the new term has started without him. But he cannot hide from the reality of his condition through work because he can't find a subject for his PhD. While his colleagues throw themselves into academic and college life, Stephen's life seems to have been put on hold. He rejects the help of his supervisor Dennis Sciama and sinks into a depression. It is only Stephen's occasional meetings with Jane and her faith in him that seem to keep him afloat. The prevailing theory in cosmology at the time is Steady State, which argues that the universe had no beginning – it has always existed, and always will – and Steady State is dominated by Professor Fred Hoyle, a plain-speaking Yorkshireman, and one of the first science TV pundits. \n\nStephen gets an early glimpse of a paper by Hoyle that is to be presented at a Royal Society lecture. He works through the calculations, identifies a mistake, and publicly confronts Hoyle after the great man has finished speaking. The row causes a stir in the department but, more importantly, it seems to give Stephen the confidence to get started on his own work. At almost the same time Stephen is introduced to a new way of thinking about his subject by another physicist, Roger Penrose. Topology is an approach that uses concepts of shape rather than equations to think about the nature of the universe, and this proves to be the perfect tool for Stephen, who is starting to find it very difficult to write. Penrose's great passion is the fate of dying stars. When a star comes to the end of its life, it begins to collapse in on itself. His calculations suggest something extraordinary. The collapse of the dying star appears to continue indefinitely, until the star is infinitely dense, forming a black hole in space. And at the heart of this black hole, Penrose shows, is something scientists call a singularity. It is this which leads Stephen to his PhD subject. He has always had a niggling scepticism about Steady State Theory, and now he can begin to see a way of explaining the revolutionary and highly controversial idea that the universe might have had a beginning. Sciama is sceptical but supportive – glad to see his student fired up and ready to work. Meanwhile, Stephen's condition continues to decline, he writes and walks with difficulty and his speech is starting to slur. But he now has a focus for his energies and, with the support of Jane, enters a new phase. He also commits to his relationship with her, asking her to marry him and in doing so exhibiting a defiant determination to survive.\n\nWith his mind fired up, Stephen begins to work away at the implications of Penrose's discovery and starts to home in on the idea of a singularity. With remarkable insight – a real Eureka moment – he asks himself: what would happen if you ran Penrose's maths backwards? Instead of something collapsing into nothingness, what if nothingness exploded into something? And what if you applied this not to a star but to the whole universe? Answer: the universe really could have originated in a big bang. At last, Stephen enters a period of feverish academic work. He applies Penrose's theorems for collapsing stars to the universe itself. Justifying Sciama's faith in him, he produces a PhD of real brilliance and profound implications. In theory, at least, the big bang could have happened. Two years after his initial diagnosis, Stephen is not only still very much alive, but has played a part in a great scientific breakthrough which revolutionises the way we think about the universe. Today, the scientific consensus is that the universe started with a big bang: billions of years ago, a cosmic explosion brought space and time into existence.\n\nA secondary, interwoven storyline follows a different but connected scientific quest. Unknown to Hawking, just as he was being diagnosed in 1963, two American scientists were embarking on their own scientific mission. Their research was to produce hard evidence to support Hawking's theoretical work. We encounter Arno Allan Penzias and Robert Woodrow Wilson in a hotel room in Stockholm in 1978. They are being interviewed about their discovery on the eve of receiving the Nobel Prize for Physics. They describe how, in the hills above New Jersey, they scanned the skies with a radio-telescope, and began to pick up a strange radio signal from space. In time, the two scientists came to realise that they had detected the left-over heat of the first, ancient explosion that had created the universe. They had found the physical proof of the big bang.\n\n\nThe program was nominated by the British Academy Television Award for Best Single Drama in 2005.\n\nBenedict Cumberbatch won the Golden Nymph for Television Film Best Performance by an Actor, and was nominated for a BAFTA TV Award for Best Actor.\n\n"}
{"id": "18053246", "url": "https://en.wikipedia.org/wiki?curid=18053246", "title": "History of condoms", "text": "History of condoms\n\nThe history of condoms goes back at least several centuries, and perhaps beyond. For most of their history, condoms have been used both as a method of birth control, and as a protective measure against sexually transmitted diseases. Condoms have been made from a variety of materials; prior to the 19th century, chemically treated linen and animal tissue (intestine or bladder) are the best documented varieties. Rubber condoms gained popularity in the mid-19th century, and in the early 20th century major advances were made in manufacturing techniques. Prior to the introduction of the combined oral contraceptive pill, condoms were the most popular birth \"control\" method in the Western world. In the second half of the 20th century, the low cost of condoms contributed to their importance in family planning programs throughout the developing world. Condoms have also become increasingly important in efforts to fight the AIDS pandemic. The oldest condoms ever excavated were found in a cesspit located in the grounds of Dudley Castle and were made from animal membrane, the condoms dated back to as early as 1642.\n\nWhether condoms were used in ancient civilizations is debated by archaeologists and historians. Societies in the ancient civilizations of Egypt, Greece, and Rome preferred small families and are known to have practiced a variety of birth control methods. However, these societies viewed birth control as a woman's responsibility, and the only well-documented contraception methods were female-controlled devices (both possibly effective, such as pessaries, and ineffective, such as amulets). The writings of these societies contain \"veiled references\" to male-controlled contraceptive methods that might have been condoms, but most historians interpret them as referring to coitus interruptus or anal intercourse.\n\nThe loincloths worn by Egyptian and Greek laborers were very spare, sometimes consisting of little more than a covering for the glans of the penis. Records of these types of loincloths being worn by men in higher classes have made some historians speculate they were worn during intercourse; others, however, are doubtful of such interpretations. Historians may also cite one legend of Minos, related by Antoninus Liberalis in 150 AD, as suggestive of condom use in ancient societies. This legend describes a curse that caused Minos' semen to contain serpents and scorpions. To protect his sexual partner from these animals, Minos used a goat's bladder as a female condom.\n\nContraceptives fell out of use in Europe after the decline of the Western Roman Empire in the 5th century; the use of contraceptive pessaries, for example, is not documented again until the 15th century. If condoms were used during the Roman Empire, knowledge of them may have been lost during its decline. In the writings of Muslims and Jews during the Middle Ages, there are some references to attempts at male-controlled contraception, including suggestions to cover the penis in tar or soak it in onion juice. Some of these writings might describe condom use, but they are \"oblique\", \"veiled\", and \"vague\".\n\nPrior to the 15th century, some use of glans condoms (devices covering only the head of the penis) is recorded in Asia. Glans condoms seem to have been used for birth control, and to have been known only by members of the upper classes. In China, glans condoms may have been made of oiled silk paper, or of lamb intestines. In Japan, they were made of tortoise shell or animal horn.\n\nThe first well-documented outbreak of what is now known as syphilis occurred in 1494 among French troops. The disease then swept across Europe. As Jared Diamond describes it, \"when syphilis was first definitely recorded in Europe in 1495, its pustules often covered the body from the head to the knees, caused flesh to fall from people's faces, and led to death within a few months.\" (The disease is less frequently fatal today.) By 1505, the disease had spread to Asia, and within a few decades had \"decimated large areas of China\".\n\nIn 16th century Italy, Gabriele Falloppio authored the earliest uncontested description of condom use. \"De Morbo Gallico\" (\"The French Disease\", referring to syphilis) was published in 1564, two years after Falloppio's death. In this tract, he recommended use of a device he claimed to have invented: linen sheaths soaked in a chemical solution and allowed to dry before use. The cloths he described were sized to cover the glans of the penis, and were held on with a ribbon. Fallopio claimed to have performed an experimental trial of the linen sheath on 1100 men, and reported that none of them had contracted the dreaded disease.\n\nAfter the publication of \"De Morbo Gallico\", use of penis coverings to protect from disease is described in a wide variety of literature throughout Europe. The first indication these devices were used for birth control, rather than disease prevention, is the 1605 theological publication \"De iustitia et iure\" (On justice and law) by Catholic theologian Leonardus Lessius: he condemned them as immoral. The first explicit description that \"un petit linge\" (a small cloth) was used to prevent pregnancy is from 1655: a French novel and play titled \"L'Escole des Filles\" (The Philosophy of Girls). In 1666, the English Birth Rate Commission attributed a recent downward fertility rate to use of \"condons\", the first documented use of that word (or any similar spelling).\n\nIn addition to linen, condoms during the Renaissance were made out of intestines and bladder. Cleaned and prepared intestine for use in glove making had been sold commercially since at least the 13th century. Condoms made from bladder and dating to the 1640s were discovered in an English privy; it is believed they were used by soldiers of King Charles I. Dutch traders introduced condoms made from \"fine leather\" to Japan. Unlike the horn condoms used previously, these leather condoms covered the entire penis.\n\nWritten references to condom use became much more common during the 18th century. Not all of the attention was positive: in 1708, John Campbell unsuccessfully asked Parliament to make the devices illegal. Noted English physician Daniel Turner condemned the condom, publishing his arguments against their use in 1717. He disliked condoms because they did not offer full protection against syphilis. He also seems to have argued that belief in the protection condoms offered encouraged men to engage in sex with unsafe partners - but then, because of the loss of sensation caused by condoms, these same men often neglected to actually use the devices. The French medical professor Jean Astruc wrote his own anti-condom treatise in 1736, citing Turner as the authority in this area. Physicians later in the 18th century also spoke against the condom, but not on medical grounds: rather, they expressed the belief that contraception was immoral.\n\nThe condom market grew rapidly, however. 18th century condoms were available in a variety of qualities and sizes, made from either linen treated with chemicals, or \"skin\" (bladder or intestine softened by treatment with sulphur and lye). They were sold at pubs, barbershops, chemist shops, open-air markets, and at the theater throughout Europe and Russia. The first recorded inspection of condom quality is found in the memoirs of Giacomo Casanova (which cover his life until 1774): to test for holes, he would often blow them up before use.\nCouples in colonial America relied on female-controlled methods of contraception, if they used contraceptives at all. The first known documents describing American condom use were written around 1800, two to three decades after the American Revolutionary War. Also around 1800, linen condoms lost popularity in the market and their production ceased: they were more expensive and were viewed as less comfortable when compared to skin condoms.\n\nUp to the 19th century, condoms were generally used only by the middle and upper classes. One reason for the lack of condom use was that the working classes tended to lack education on the dangers of sexually transmitted infections. Perhaps more importantly, condoms were unaffordable for many: for a typical prostitute, a single condom might cost several months' pay.\n\nThe early 19th century saw contraceptives promoted to the poorer classes for the first time: birth control advocates in England included Jeremy Bentham and Richard Carlile, and noted American advocates included Robert Dale Owen and Charles Knowlton. Writers on contraception tended to prefer other methods of birth control, citing both the expense of condoms and their unreliability (they were often riddled with holes, and often fell off or broke), but they discussed condoms as a good option for some, and as the only contraceptive that also protected from disease. One group of British contraceptive advocates distributed condom literature in poor neighborhoods, with instructions on how to make the devices at home; in the 1840s, similar tracts were distributed in both cities and rural areas through the United States.\n\nFrom the 1820s through the 1870s, popular women and men lecturers traveled around America teaching about physiology and sexual matters. Many of them sold birth control devices, including condoms, after their lectures. They were condemned by many moralists and medical professionals, including America's first female doctor Elizabeth Blackwell. Blackwell accused the lecturers of spreading doctrines of \"abortion and prostitution\". In the 1840s, advertisements for condoms began to appear in British newspapers, and in 1861 a condom advertisement appeared in the \"New York Times\".\n\nThe rubber vulcanization process was invented by Charles Goodyear in 1839, and patented in 1844. The first rubber condom was produced in 1855, and by the late 1850s several major rubber companies were mass-producing, among other items, rubber condoms. A main advantage of rubber condoms was their reusability, making them a more economical choice in the long term. Compared to the 19th century rubber condoms, however, skin condoms were initially cheaper and offered better sensitivity. For these reasons, skin condoms remained more popular than the rubber variety. However, by the end of the 19th century \"rubber\" had become a euphemism for condoms in countries around the world. For many decades, rubber condoms were manufactured by wrapping strips of raw rubber around penis-shaped molds, then dipping the wrapped molds in a chemical solution to cure the rubber. The earliest rubber condoms covered only the glans of the penis; a doctor had to measure each man and order the correct size. Even with the medical fittings, however, glans condoms tended to fall off during use. Rubber manufacturers quickly discovered they could sell more devices by manufacturing full-length one-size-fits-all condoms to be sold in pharmacies.\n\nDistribution of condoms in the United States was limited by passage of the Comstock laws, which included a federal act banning the mailing of contraceptive information (passed in 1873) as well as State laws that banned the manufacture and sale of condoms in thirty states. In Ireland the 1889 Indecent Advertisements Act made it illegal to advertise condoms, although their manufacture and sale remained legal. Contraceptives were illegal in 19th century Italy and Germany, but condoms were allowed for disease prevention. Despite legal obstacles, condoms continued to be readily available in both Europe and America, widely advertised under euphemisms such as \"male shield\" and \"rubber good\". In late 19th century England, condoms were known as \"a little something for the weekend\". Only in the Republic of Ireland were condoms effectively outlawed. In Ireland their sale and manufacture remained illegal until the 1970s.\n\nOpposition to condoms did not only come from moralists: by the late 19th century many feminists expressed distrust of the condom as a contraceptive, as its use was controlled and decided upon by men alone. They advocated instead for methods which were controlled by women, such as diaphragms and spermicidal douches. Despite social and legal opposition, at the end of the 19th century the condom was the Western world's most popular birth control method. Two surveys conducted in New York in 1890 and 1900 found that 45% of the women surveyed were using condoms to prevent pregnancy. A survey in Boston just prior to World War I concluded that three million condoms were sold in that city every year.\n\n1870s England saw the founding of the first major condom manufacturing company, E. Lambert and Son of Dalston. In 1882, German immigrant Julius Schmidt founded one of the largest and longest-lasting condom businesses, Julius Schmid, Inc. (he dropped the 't' from his name in an effort to appear less Jewish). This New York business initially manufactured only skin condoms (in 1890 he was arrested by Anthony Comstock for having almost seven hundred of the devices in his house). In 1912, a German named Julius Fromm developed a new, improved manufacturing technique for condoms: dipping glass molds into a raw rubber solution. Called \"cement dipping\", this method required adding gasoline or benzene to the rubber to make it liquid. In America, Schmid was the first company to use the new technique. Using the new dipping method, French condom manufacturers were the first to add textures to condoms. Fromm was the first company to sell a branded line of condoms, Fromm's Act, which remains popular in Germany today. The condom lines manufactured by Schmid, Shieks and Ramses, were sold through the late 1990s. Youngs Rubber Company, founded by Merle Youngs in late 19th century America, introduced Trojans.\n\nBeginning in the second half of the 19th century, American rates of sexually transmitted diseases skyrocketed. Causes cited by historians include effects of the American Civil War, and the ignorance of prevention methods promoted by the Comstock laws. To fight the growing epidemic, sexual education classes were introduced to public schools for the first time, teaching about venereal diseases and how they were transmitted. They generally taught that abstinence was the only way to avoid sexually transmitted diseases. Condoms were not promoted for disease prevention; the medical community and moral watchdogs considered STDs to be punishment for sexual misbehavior. The stigma on victims of these diseases was so great that many hospitals refused to treat people who had syphilis.\n\nThe German military was the first to promote condom use among its soldiers, beginning in the second half of the 19th century. Early 20th century experiments by the American military concluded that providing condoms to soldiers significantly lowered rates of sexually transmitted diseases. During World War I, the United States and (at the beginning of the war only) Britain were the only countries with soldiers in Europe who did not provide condoms and promote their use. By the end of the war, the American military had diagnosed almost 400,000 cases of syphilis and gonorrhea, a historic high.\n\nFrom just before 1900 to the beginning of World War I, almost all condoms used in Europe were imported from Germany. Germany not only exported condoms to other European countries, but was a major supplier to Australia, New Zealand, and Canada. During the war, the American companies Schmid and Youngs became the main suppliers of condoms to the European Allies. By the early 1920s, however, most of Europe's condoms were once again made in Germany.\n\nIn 1918, just before the end of the war, an American court overturned a conviction against Margaret Sanger. In this case, the judge ruled that condoms could be legally advertised and sold for the prevention of disease. There were still a few state laws against buying and selling contraceptives, and advertising condoms as birth control devices remained illegal in over thirty states. But condoms began to be publicly, legally sold to Americans for the first time in forty-five years. Through the 1920s, catchy names and slick packaging became an increasingly important marketing technique for many consumer items, including condoms and cigarettes. Quality testing became more common, involving filling each condom with air followed by one of several methods intended to detect loss of pressure. Several American companies sold their rejects under cheaper brand names rather than discarding them. Consumers were advised to perform similar tests themselves before use, although few actually did so. Worldwide, condoms sales doubled in the 1920s.\n\nStill, there were many prominent opponents of condoms. Founder of psychoanalysis Sigmund Freud opposed all methods of birth control on the grounds that their failure rates were too high. Freud was especially opposed to the condom because it cut down on sexual pleasure. Some feminists continued to oppose male-controlled contraceptives such as condoms. Many moralists and medical professionals opposed all methods of contraception. In 1920 the Church of England's Lambeth Conference condemned all \"unnatural means of conception avoidance.\" London's Bishop Arthur Winnington-Ingram complained of the \"huge\" number of condoms discarded in alleyways and parks, especially after weekends and holidays.\n\nIn the U.S., condom advertising was legally restricted to their use as disease preventatives. They could be openly marketed as birth control devices in Britain, but purchasing condoms in Britain was socially awkward compared to the U.S. They were generally requested with the euphemism \"a little something for the weekend.\" Boots, the largest pharmacy chain in Britain, stopped selling condoms altogether in the 1920s, a policy that was not reversed until the 1960s. In post-World War I France, the government was concerned about falling birth rates. In response, it outlawed all contraceptives, including condoms. Contraception was also illegal in Spain. European militaries continued to provide condoms to their members for disease protection, even in countries where they were illegal for the general population.\n\nLatex, rubber suspended in water, was invented in 1920. Youngs Rubber Company was the first to manufacture a latex condom, an improved version of their Trojan brand. Latex condoms required less labor to produce than cement-dipped rubber condoms, which had to be smoothed by rubbing and trimming. Because it used water to suspend the rubber instead of gasoline and benzene, it eliminated the fire hazard previously associated with all condom factories. Latex condoms also performed better for the consumer: they were stronger and thinner than rubber condoms, and had a shelf life of five years (compared to three months for rubber). Europe's first latex condom was an export from Youngs Rubber Company in 1929. In 1932 the London Rubber Company, which had previously served as a wholesaler for German-manufactured condoms, became Europe's first manufacturer of latex condoms, the Durex.\n\nUntil the twenties, all condoms were individually hand-dipped by semiskilled workers. Throughout the decade of the 1920s, advances in automation of condom assembly line were made. Fred Killian patented the first fully automated line in 1930 and installed it in his manufacturing plant in Akron, Ohio. Killian charged $20,000 for his conveyor system - as much as $2 million in today's dollars. Automated lines dramatically lowered the price of condoms. Major condom manufacturers bought or leased conveyor systems, and small manufacturers were driven out of business. The skin condom, now significantly more expensive than the latex variety, became restricted to a niche high-end market.\n\nIn 1927, senior medical officers in the American military began promoting condom distribution and educational programs to members of the army and navy. By 1931, condoms were standard issue to all members of the U.S. military. This coincided with a steep decline in U.S. military cases of sexually transmitted disease. The U.S. military was not the only large organization that changed its moral stance on condoms: in 1930 the Anglican Church's sanctioned the use of birth control by married couples. In 1931 the Federal Council of Churches in the U.S. issued a similar statement.\n\nThe Roman Catholic Church responded by issuing the encyclical \"Casti connubii\" affirming its opposition to all contraceptives, a stance it has never reversed. Semen analysis was first performed in the 1930s. Samples were typically collected by masturbation, another action opposed by the Catholic Church. In 1930s Spain, the first use of collection condoms was documented; holes put in the condom allowed the user to collect a sample without violating the prohibitions on contraception and masturbation.\n\nIn 1932, Margaret Sanger arranged for a shipment of diaphragms to be mailed from Japan to a sympathetic doctor in New York City. When U.S. customs confiscated the package as illegal contraceptive devices, Sanger helped file a lawsuit. In 1936, a federal appeals court ruled in United States v. One Package of Japanese Pessaries that the federal government could not interfere with doctors providing contraception to their patients. In 1938, over three hundred birth control clinics opened in America, supplying reproductive care (including condoms) to poor women all over the country. Programs led by U.S. Surgeon General Thoman Parran included heavy promotion of condoms. These programs are credited with a steep drop in the U.S. STD rate by 1940.\n\nTwo of the few places where condoms became more restricted during this period were Fascist Italy and Nazi Germany. Because of government concern about low birth rates, contraceptives were made illegal in Italy in the late 1920s. Although limited and highly controlled sales as disease preventatives were still allowed, there was a brisk black market trade in condoms as birth control. In Germany, laws passed in 1933 mandated that condoms could only be sold in plain brown wrappers, and only at pharmacies. Despite these restrictions, when World War II began Germans were using 72 million condoms every year. The elimination of moral and legal barriers, and the introduction of condom programs by the U.S. government helped condom sales. However, these factors alone are not considered to explain the Great Depression's booming condom industry. In the U.S. alone, more than 1.5 million condoms were used every day during the Depression, at a cost of over $33 million per year (not adjusted for inflation). One historian explains these statistics this way: \"Condoms were cheaper than children.\" During the Depression condom lines by Schmid gained in popularity: that company still used the cement-dipping method of manufacture. Unlike the latex variety, these condoms could be safely used with oil-based lubricants. And while less comfortable, older-style rubber condoms could be reused and so were more economical, a valued feature in hard times.\n\nMore attention was brought to quality issues in the 1930s. In 1935, a biochemist tested 2000 condoms by filling each one with air and then water: he found that 60% of them leaked. The condom industry estimated that only 25% of condoms were tested for quality before packaging. The media attention led the U.S. Food and Drug Administration to classify condoms as a drug in 1937 and mandate that every condom be tested before packaging. Youngs Rubber Company was the first to institute quality testing of every condom they made, installing automatic testing equipment designed by Arthur Youngs (the owner's brother) in 1938. The Federal Food, Drug, and Cosmetic Act authorized the FDA to seize defective products; the first month the Act took effect in 1940, the FDA seized 864,000 condoms. While these actions improved the quality of condoms in the United States, American condom manufacturers continued to export their rejects for sale in foreign markets.\n\nDuring World War II condoms were not only distributed to male U.S. military members, but enlisted men were also subject to significant contraception propaganda in the form of films, posters, and lectures. A number of slogans were coined by the military, with one film exhorting \"Don't forget — put it on before you put it in.\" African-American soldiers, who served in segregated units, were exposed to less of the condom promotion programs, had lower rates of condom usage, and much higher rates of STDs. America's female military units, the WACs and WAACs, were still engaged with abstinence programs. European and Asian militaries on both sides of the conflict also provided condoms to their troops throughout the war, even Germany which outlawed all civilian use of condoms in 1941. Despite the rubber shortages that occurred during this period, condom manufacturing was never restricted. In part because condoms were readily available, soldiers found a number of non-sexual uses for the devices, many of which continue to be utilized to this day.\n\nPost-war American troops in Germany continued to receive condoms and materials promoting their use. Nevertheless, rates of STDs in this population began to rise, reaching the highest levels since World War I. One explanation is that the success of newer penicillin treatments led soldiers to take syphilis and gonorrhea much less seriously. A similar casual attitude toward STDs appeared in the general American population; one historian states that condoms \"were almost obsolete as prophylaxis by 1960\". By 1947, the U.S. military was again promoting abstinence as the only method of disease control for its members, a policy that continued through the Vietnam War.\n\nBut condom sales continued to grow. From 1955 to 1965, 42% of Americans of reproductive age relied on condoms for birth control. In Britain from 1950 to 1960, 60% of married couples used condoms. For the more economical-minded, cement-dipped condoms continued to be available long after the war. In 1957, Durex introduced the world's first lubricated condom. Beginning in the 1960s, the Japanese used more condoms per capita than any other nation in the world. The birth control pill became the world's most popular method of birth control in the years after its 1960 debut, but condoms remained a strong second. A survey of British women between 1966 and 1970 found that the condom was the most popular birth control method with single women. New manufacturers appeared in the Soviet Union, which had never restricted condom sales. The U.S. Agency for International Development pushed condom use in developing countries to help solve the \"world population crises\": by 1970 hundreds of millions of condoms were being used each year in India alone.\n\nIn the 1960s and 1970s quality regulations tightened, and legal barriers to condom use were removed. In 1965, the U.S. Supreme Court case \"Griswold v. Connecticut\" struck down one of the remaining Comstock laws, the bans of contraception in Connecticut and Massachusetts. France repealed its anti-birth control laws in 1967. Similar laws in Italy were declared unconstitutional in 1971. Captain Beate Uhse in Germany founded a birth control business, and fought a series of legal battles continue her sales. In Ireland, legal condom sales (only to people over 18, and only in clinics and pharmacies) were allowed for the first time in 1978. (All restrictions on Irish condom sales were lifted in 1993.)\n\nAdvertising was one area that continued to have legal restrictions. In the late 1950s, the American National Association of Broadcasters banned condom advertisements from national television. This policy remained in place until 1979, when the U.S. Justice department had it overturned in court. In the U.S., advertisements for condoms were mostly limited to men's magazines such as \"Penthouse\". The first television ad, on the California station KNTV, aired in 1975: it was quickly pulled after it attracted national attention. And in over 30 states, advertising condoms as birth control devices was still illegal.\n\nThe first \"New York Times\" story on acquired immunodeficiency syndrome (AIDS) was published on July 3, 1981. In 1982 it was first suggested that the disease was sexually transmitted. In response to these findings, and to fight the spread of AIDS, the U.S. Surgeon General Dr. C. Everett Koop supported condom promotion programs. However, President Ronald Reagan preferred an approach of concentrating only on abstinence programs. Some opponents of condom programs stated that AIDS was a disease of homosexuals and illicit drug users, who were just getting what they deserved. In 1990, North Carolina senator Jesse Helms argued that the best way to fight AIDS would be to enforce state sodomy laws.\n\nNevertheless, major advertising campaigns were put in print media, promoting condoms as a way to protect against AIDS. Youngs Rubber mailed educational pamphlets to American households, although the postal service forced them to go to court to do so, citing a section of Title 39 that \"prohibits the mailing of unsolicited advertisements for contraceptives.\" In 1983 the U.S. Supreme Court held that the postal service's actions violated the free speech clause of the First Amendment. Beginning in 1985 through 1987, national condom promotion campaigns occurred in U.S. and Europe. Over the 10 years of the Swiss campaign, Swiss condom use increased by 80%. The year after the British campaign began, condom sales in the UK increased by 20%. In 1988 Britain, condoms were the most popular birth control choice for married couples, for the first time since the introduction of the pill. The first condom commercial on U.S. television aired during an episode of \"Herman's Head\" on November 17, 1991. In the U.S. in the 1990s, condoms ranked third in popularity among married couples, and were a strong second among single women.\n\nCondoms began to be sold in a wider variety of retail outlets, including in supermarkets and in discount department stores such as Wal-Mart. In this environment of more open sales, the British euphemism of \"a little something for the weekend\" fell out of use. In June 1991 America's first condom store, Condomania, opened on Bleecker Street in New York City. Condomania was the first store of its kind in North America dedicated to the sale and promotion of condoms in an upbeat, upscale and fun atmosphere. Condomania was also one of the very first retailers to offer condoms online when it launched its website in December 1995.\n\nCondom sales increased every year until 1994, when media attention to the AIDS pandemic began to decline. In response, manufacturers have changed the tone of their advertisements from scary to humorous. New developments continue to occur in the condom market, with the first polyurethane condom—branded Avanti and produced by the manufacturer of Durex—introduced in the 1990s. Durex was also the first condom brand to have a website, launched in 1997. Worldwide condom use is expected to continue to grow: one study predicted that developing nations would need 18.6 billion condoms in 2015.\n\nEtymological theories for the word \"condom\" abound. By the early 18th century, the invention and naming of the condom was attributed to an associate of England's King Charles II, and this explanation persisted for several centuries. However, the \"Dr. Condom\" or \"Earl of Condom\" described in these stories has never been proved to exist, and condoms had been used for over one hundred years before King Charles II ascended to the throne.\n\nA variety of Latin etymologies have been proposed, including \"condon\" (receptacle), \"condamina\" (house), and \"cumdum\" (scabbard or case). It has also been speculated to be from the Italian word \"guantone\", derived from \"guanto\", meaning glove. William E. Kruck wrote an article in 1981 concluding that, \"As for the word 'condom', I need state only that its origin remains completely unknown, and there ends this search for an etymology.\" Modern dictionaries may also list the etymology as \"unknown\".\nOther terms are also commonly used to describe condoms. In North America condoms are also commonly known as \"prophylactics\", or \"rubbers\". In Britain they may be called \"French letters\". Additionally, condoms may be referred to using the manufacturer's name. The insult term scumbag was originally a slang word for condom.\n\nOne analyst described the size of the condom market as something that \"boggles the mind\". Numerous small manufacturers, nonprofit groups, and government-run manufacturing plants exist around the world. Within the condom market, there are several major contributors, among them both for-profit businesses and philanthropic organizations.\n\nIn 1882, German immigrant Julius Schmidt founded one of the largest and longest-lasting condom businesses, Julius Schmid, Inc., based in New York City (he dropped the 't' from his name in an effort to appear less Jewish). The condom lines manufactured by Schmid included Sheiks and Ramses. In 1932, the London Rubber Company (which had previously been a wholesale business importing German condoms) began to produce latex condoms, under the Durex brand. In 1962 Schmid was purchased by London Rubber. In 1987, London Rubber began acquiring other condom manufacturers, and within a few years became an important international company. In the late 1990s, London Rubber (by then London International Limited) merged all the Schmid brands into its European brand, Durex. Soon after, London International was purchased by Seton Scholl Healthcare (manufacturer of Dr. Scholl's footcare products), forming Seton Scholl Limited.\n\nYoungs Rubber Company, founded by Merle Youngs in late 19th century America, introduced the Trojan line of condoms. In 1985, Youngs Rubber Company was sold to Carter-Wallace. The Trojan name switched hands yet again in 2000 when Carter-Wallace was sold to Church and Dwight.\n\nThe Australian division of Dunlop Rubber began manufacturing condoms in the 1890s. In 1905, Dunlop sold its condom-making equipment to one of its employees, Eric Ansell, who founded Ansell Rubber. In 1969, Ansell was sold back to Dunlop. In 1987, English business magnate Richard Branson contracted with Ansell to help in a campaign against HIV and AIDS. Ansell agreed to manufacture the Mates brand of condom, to be sold at little or no profit in order to encourage condom use. Branson soon sold the Mates brand to Ansell, with royalty payments made annually to the charity Virgin Unite. In addition to its Mates brand, Ansell currently manufactures Lifestyles and Lifesan for the U.S. market.\n\nIn 1934 the Kokusia Rubber Company was founded in Japan. It is now known as the Okamoto Rubber Manufacturing Company.\n\nIn 1970 Tim Black and Philip Harvey founded Population Planning Associates (now known as Adam & Eve). Population Planning Associates was a mail-order business that marketed condoms to American college students, despite U.S. laws against sending contraceptives through the mail. Black and Harvey used the profits from their company to start a non-profit organization Population Services International. By 1975, PSI was marketing condoms in Kenya and Bangladesh, and today operates programs in over sixty countries. Harvey left his position as PSI's director in the late 1970s, but in the late 1980s again founded a nonprofit company, DKT International. Named after D.K. Tyagi (a leader of family planning programs in India), DKT International annually sells millions of condoms at discounted rates in developing countries around the world. By selling the condoms instead of giving them away, DKT intends to make its customers invested in using the devices. One of DKT's more notable programs is its work in Ethiopia, where soldiers are required to carry a condom every time they leave base. The rate of HIV infection in the Ethiopian military, about 5%, is believed to be the lowest among African militaries.\n\nIn 1987, Tufts University students Davin Wedel and Adam Glickman started Global Protection Corp. in response to C. Everett Koop's statement that \"a condoms can save your life.\" Since that time, Global Protection Corp. has become known for its innovative approach to condom marketing and its support of more than 3500 non-profit organizations worldwide. The company has numerous patents and trademarks to its name, including the only FDA-approved glow-in-the-dark condom, the Pleasure Plus condom and the original condom keychain. In 2005 the company introduced its newest product, One Condoms. One represents a complete reinvention of retail condom brands, combining sleek metal packaging, innovative condom wrappers and innovative marketing programs. One is also the first condom brand to donate 5% of sales to the development of sexual health outreach and educational programs. In South Africa, some manufacturers have considered introducing an extra-large variety of condoms after several complaints from South African men claiming the condoms were too small and causing discomfort.\n"}
{"id": "15479485", "url": "https://en.wikipedia.org/wiki?curid=15479485", "title": "Husar-rover", "text": "Husar-rover\n\nThe HUSAR rover is part of the Hunveyor system. Like as Hunveyor (Hungarian UNiversity SURVEYOR) is an experimental university lander model, so the HUSAR is (Hungarian University Surface Analyser Rover) an experimental university rover model. This rover accompanies the Hunveyor as like Sojourner was accompanying the Mars Pathfinder lander. For the Hunveyor the Surveyor was the example, for the Husar-rover the Mars Pathfinder-Sojourner system was the example from the real space exploration world.\n\nThe Hunveyor-Husar system is an educational work at Hungarian Universities, Colleges and High Schools. There are smaller and larger model-cars supported by onboard computer, camera and some additional instrumentation. The building of the Husar-rover types is focusing on the basic planetary surface activities by moving vehicle. The rover has a solar panel on its board placed on a model-car chassis. In the Husar-b version the drive is on the back wheels by electric engines, movements are controlled by the camera view through a wireless connection to the computer. It can be used for roving among rocks and observing their surfaces. On a larger chassis Husar-2a version there are independent driving on all the four wheels. This fact allows directing a special movement. The rover can turn in a very narrow arc which is important to approach objects in side direction. The wheels in one axis are connected by a differential. The structural framework, the driving mechanism all have a left- and right-hand side symmetric structure. The movements are directed through servo engines, which were originally designed for analogous direction, however, even 8 of these servos can be directed by a microcontroller. This microcontroller gets the position of the servos through RS 232 port of the computer. The camera of Husar-2 transmits images with 30 frame/sec, and it has wireless connection with the computer (1200 MHz).\n\nIn a planetary simulation work we use testfields which are analog to a selected planetary surface. On the testfields (test table) the most important rock types from the Solar System are arranged around the Hunveyor and Husar. For example, desert sand morphology (according to the observations of Mars Pathfinder) can be modelled around lander and rover in order to give a real planetary environment for simulation studies. Complex studies of the main streams in the surface, wind system and rock interactions also can be modelled. (i.e. stream strength of the wind can be deciphered from the deposition of deflation style of the lee-forms). We show one of our test-fields around Hunveyor and Husar.\n\nHusar rovers were built with Hunveyor-1, -2, and -4 landers, and they were independently built by the Husar-5 and Husar-6 groups at Budapest, Pécs, Székesfehérvár, Sopron and Dorog.\n\n"}
{"id": "42840327", "url": "https://en.wikipedia.org/wiki?curid=42840327", "title": "Jean-Claude Bradley", "text": "Jean-Claude Bradley\n\nJean-Claude Bradley was a chemist who actively promoted Open Science in chemistry, including at the White House, for which he was awarded the Blue Obelisk award in 2007. He coined the term \"Open Notebook science\". He died in May 2014. A memorial symposium was held July 14, 2014 at Cambridge University, UK.\n\nOne outcome of his Open Notebook work is the collection of physicochemical properties of organic compounds he was studying. All of this data he made available as Open data under the CCZero license. For example, in 2009 Bradley et al. published their work on making solubility data of organic compounds available as Open data. Later, the melting point data set he collaborated on with Andrew Lang and Antony Williams was published with Figshare. Both data sets were also made available as books via the Lulu.com self-publishing platform.\n\nHe blogged extensively and contributed to at least 25 individual blogs. In an interview in 2008 with Bora Zivkovic titled \"Doing Science Publicly\", he spoke of his work and online presence. In 2010, he gave an extensive interview about the impact of Open Notebook science with Richard Poynder.\n\n"}
{"id": "56969413", "url": "https://en.wikipedia.org/wiki?curid=56969413", "title": "List of Polish inventors and discoverers", "text": "List of Polish inventors and discoverers\n\nThis is a list of Polish inventors and discoverers. The following list comprises people from Poland and of Polish origin, and also people of predominantly Polish heritage, in alphabetical order of the surname.\n\n\n\n"}
{"id": "36906096", "url": "https://en.wikipedia.org/wiki?curid=36906096", "title": "List of immunofluorescence findings for autoimmune bullous conditions", "text": "List of immunofluorescence findings for autoimmune bullous conditions\n\nSeveral cutaneous conditions can be diagnosed with the aid of immunofluorescence studies.\n\nCutaneous conditions with positive direct or indirect immunofluorescence when using salt-split skin include:\nFor several subtypes of pemphigus a variety of substrates are used for indirect immunofluorescence:\n\n"}
{"id": "40789317", "url": "https://en.wikipedia.org/wiki?curid=40789317", "title": "Lists of research stations", "text": "Lists of research stations\n\nThis is a list of research stations:\n\n"}
{"id": "247908", "url": "https://en.wikipedia.org/wiki?curid=247908", "title": "Macrocosm and microcosm", "text": "Macrocosm and microcosm\n\nMacrocosm and microcosm refers to a vision of cosmos where the part (microcosm) reflects the whole (macrocosm) and vice versa. It is a feature \"present in all esoteric schools of thinking\", according to scholar Pierre A. Riffard. It is closely associated with Hermeticism and underlies practices such as astrology, alchemy and sacred geometry with its premise of \"As Above, So Below\".\n\nToday, the concept of microcosm has been dominated by sociology to mean a small group of individuals whose behavior is typical of a larger social body encompassing it. A microcosm can be seen as a special kind of epitome. Conversely, a macrocosm is a social body made of smaller compounds. In physics, scale invariance describes the same phenomenon, although the universe as a whole is not physically scale invariant according to the modern understanding. However, scale invariance does appear in some physical systems, such as electrical breakdown.\n\nThis theory was initiated by Pythagoras who saw the cosmos and the body as a harmonious unity. He expressed this connection with his concept of microcosm and macrocosm.\n\n\n\n"}
{"id": "31485319", "url": "https://en.wikipedia.org/wiki?curid=31485319", "title": "Mad Scientist Toon Club", "text": "Mad Scientist Toon Club\n\nMad Scientist Toon Club (aka Mad Scientist Kids Club) is an educational children's television show produced by Saban Entertainment that aired in US syndication from September 15, 1993 to January 25, 1994. Each one-hour program (including commercials) mixed live action segments hosted by the scientist character \"Dr. Pi\" (Michael Sorich) and pre-existing Japanese animation, including Saban's Tic Tac Toons. The format of the science portions was similar to Beakman's World and Bill Nye the Science Guy (all were produced in response to the 1990 Children's Television Act). Dr. Pi stood wearing a green lab coat and a backwards baseball cap, surrounded by a colorful set, and presented experiments that children could perform at home.\n\nThis \"Mad Scientist Toon Club\" episode list was compiled from US Copyright Office listings. The episodes that aired on the same date were given a combined entry in the registry.\n\n"}
{"id": "7290120", "url": "https://en.wikipedia.org/wiki?curid=7290120", "title": "Methods of detecting exoplanets", "text": "Methods of detecting exoplanets\n\nAny planet is an extremely faint light source compared to its parent star. For example, a star like the Sun is about a billion times as bright as the reflected light from any of the planets orbiting it. In addition to the intrinsic difficulty of detecting such a faint light source, the light from the parent star causes a glare that washes it out. For those reasons, very few of the extrasolar planets reported have been observed directly, with even fewer being resolved from their host star.\n\nInstead, astronomers have generally had to resort to indirect methods to detect extrasolar planets. As of 2016, several different indirect methods have yielded success.\n\nThe following methods have at least once proved successful for discovering a new planet or detecting an already discovered planet:\n\nA star with a planet will move in its own small orbit in response to the planet's gravity. This leads to variations in the speed with which the star moves toward or away from Earth, i.e. the variations are in the radial velocity of the star with respect to Earth. The radial velocity can be deduced from the displacement in the parent star's spectral lines due to the Doppler effect. The radial-velocity method measures these variations in order to confirm the presence of the planet using the binary mass function.\n\nThe speed of the star around the system's center of mass is much smaller than that of the planet, because the radius of its orbit around the center of mass is so small. (For example, the Sun moves by about 13 m/s due to Jupiter, but only about 9 cm/s due to Earth). However, velocity variations down to 1 m/s or even somewhat less can be detected with modern spectrometers, such as the HARPS (High Accuracy Radial Velocity Planet Searcher) spectrometer at the ESO 3.6 meter telescope in La Silla Observatory, Chile, or the HIRES spectrometer at the Keck telescopes.\nAn especially simple and inexpensive method for measuring radial velocity is \"externally dispersed interferometry\".\n\nUntil around 2012, the radial-velocity method (also known as Doppler spectroscopy) was by far the most productive technique used by planet hunters. (After 2012, the transit method from the Kepler spacecraft overtook it in number.) The radial velocity signal is distance independent, but requires high signal-to-noise ratio spectra to achieve high precision, and so is generally used only for relatively nearby stars, out to about 160 light-years from Earth, to find lower-mass planets. It is also not possible to simultaneously observe many target stars at a time with a single telescope. Planets of Jovian mass can be detectable around stars up to a few thousand light years away. This method easily finds massive planets that are close to stars. Modern spectrographs can also easily detect Jupiter-mass planets orbiting 10 astronomical units away from the parent star, but detection of those planets requires many years of observation. Earth-mass planets are currently detectable only in very small orbits around low-mass stars, e.g. Proxima b.\n\nIt is easier to detect planets around low-mass stars, for two reasons: First, these stars are more affected by gravitational tug from planets. The second reason is that low-mass main-sequence stars generally rotate relatively slowly. Fast rotation makes spectral-line data less clear because half of the star quickly rotates away from observer's viewpoint while the other half approaches. Detecting planets around more massive stars is easier if the star has left the main sequence, because leaving the main sequence slows down the star's rotation.\n\nSometimes Doppler spectrography produces false signals, especially in multi-planet and multi-star systems. Magnetic fields and certain types of stellar activity can also give false signals. When the host star has multiple planets, false signals can also arise from having insufficient data, so that multiple solutions can fit the data, as stars are not generally observed continuously. Some of the false signals can be eliminated by analyzing the stability of the planetary system, conducting photometry analysis on the host star and knowing its rotation period and stellar activity cycle periods.\n\nPlanets with orbits highly inclined to the line of sight from Earth produce smaller visible wobbles, and are thus more difficult to detect. One of the advantages of the radial velocity method is that eccentricity of the planet's orbit can be measured directly. One of the main disadvantages of the radial-velocity method is that it can only estimate a planet's minimum mass (formula_1). The posterior distribution of the inclination angle \"i\" depends on the true mass distribution of the planets. However, when there are multiple planets in the system that orbit relatively close to each other and have sufficient mass, orbital stability analysis allows one to constrain the maximum mass of these planets. The radial-velocity method can be used to confirm findings made by the transit method. When both methods are used in combination, then the planet's true mass can be estimated.\n\nAlthough radial velocity of the star only gives a planet's minimum mass, if the planet's spectral lines can be distinguished from the star's spectral lines then the radial velocity of the planet itself can be found, and this gives the inclination of the planet's orbit. This enables measurement of the planet's actual mass. This also rules out false positives, and also provides data about the composition of the planet. The main issue is that such detection is possible only if the planet orbits around a relatively bright star and if the planet reflects or emits a lot of light.\n\nWhile the radial velocity method provides information about a planet's mass, the photometric method can determine the planet's radius. If a planet crosses (transits) in front of its parent star's disk, then the observed visual brightness of the star drops by a small amount, depending on the relative sizes of the star and the planet. For example, in the case of HD 209458, the star dims by 1.7%. However, most transit signals are considerably smaller; for example, an Earth-size planet transiting a Sun-like star produces a dimming of only 80 parts per million (0.008 percent).\n\nThis method has two major disadvantages. First, planetary transits are observable only when the planet's orbit happens to be perfectly aligned from the astronomers' vantage point. The probability of a planetary orbital plane being directly on the line-of-sight to a star is the ratio of the diameter of the star to the diameter of the orbit (in small stars, the radius of the planet is also an important factor). About 10% of planets with small orbits have such an alignment, and the fraction decreases for planets with larger orbits. For a planet orbiting a Sun-sized star at 1 AU, the probability of a random alignment producing a transit is 0.47%. Therefore, the method cannot guarantee that any particular star is not a host to planets. However, by scanning large areas of the sky containing thousands or even hundreds of thousands of stars at once, transit surveys can find more extrasolar planets than the radial-velocity method. Several surveys have taken that approach, such as the ground-based MEarth Project, SuperWASP, KELT, and HATNet, as well as the space-based COROT and Kepler missions. The transit method has also the advantage of detecting planets around stars that are located a few thousand light years away. The most distant planets detected by Sagittarius Window Eclipsing Extrasolar Planet Search are located near the galactic center. However, reliable follow-up observations of these stars are nearly impossible with current technology.\n\nThe second disadvantage of this method is a high rate of false detections. A 2012 study found that the rate of false positives for transits observed by the Kepler mission could be as high as 40% in single-planet systems. For this reason, a star with a single transit detection requires additional confirmation, typically from the radial-velocity method or orbital brightness modulation method. The radial velocity method is especially necessary for Jupiter-sized or larger planets, as objects of that size encompass not only planets, but also brown dwarfs and even small stars. As the false positive rate is very low in stars with two or more planet candidates, such detections often can be validated without extensive follow-up observations. Some can also be confirmed through the transit timing variation method.\n\nRed giant branch stars have another issue for detecting planets around them: while planets around these stars are much more likely to transit due to the larger star size, these transit signals are hard to separate from the main star's brightness light curve as red giants have frequent pulsations in brightness with a period of a few hours to days. This is especially notable with subgiants. In addition, these stars are much more luminous, and transiting planets block a much smaller percentage of light coming from these stars. In contrast, planets can completely occult a very small star such as a neutron star or white dwarf, an event which would be easily detectable from Earth. However, due to the small star sizes, the chance of a planet aligning with such a stellar remnant is extremely small.\nThe main advantage of the transit method is that the size of the planet can be determined from the lightcurve. When combined with the radial-velocity method (which determines the planet's mass), one can determine the density of the planet, and hence learn something about the planet's physical structure. The planets that have been studied by both methods are by far the best-characterized of all known exoplanets.\n\nThe transit method also makes it possible to study the atmosphere of the transiting planet. When the planet transits the star, light from the star passes through the upper atmosphere of the planet. By studying the high-resolution stellar spectrum carefully, one can detect elements present in the planet's atmosphere. A planetary atmosphere, and planet for that matter, could also be detected by measuring the polarization of the starlight as it passed through or is reflected off the planet's atmosphere.\n\nAdditionally, the secondary eclipse (when the planet is blocked by its star) allows direct measurement of the planet's radiation and helps to constrain the planet's orbital eccentricity without needing the presence of other planets. If the star's photometric intensity during the secondary eclipse is subtracted from its intensity before or after, only the signal caused by the planet remains. It is then possible to measure the planet's temperature and even to detect possible signs of cloud formations on it. In March 2005, two groups of scientists carried out measurements using this technique with the Spitzer Space Telescope. The two teams, from the Harvard-Smithsonian Center for Astrophysics, led by David Charbonneau, and the Goddard Space Flight Center, led by L. D. Deming, studied the planets TrES-1 and HD 209458b respectively. The measurements revealed the planets' temperatures: 1,060 K (790°C) for TrES-1 and about 1,130 K (860 °C) for HD 209458b. In addition, the hot Neptune Gliese 436 b is known to enter secondary eclipse. However, some transiting planets orbit such that they do not enter secondary eclipse relative to Earth; HD 17156 b is over 90% likely to be one of the latter.\n\nA French Space Agency mission, CoRoT, began in 2006 to search for planetary transits from orbit, where the absence of atmospheric scintillation allows improved accuracy. This mission was designed to be able to detect planets \"a few times to several times larger than Earth\" and performed \"better than expected\", with two exoplanet discoveries (both of the \"hot Jupiter\" type) as of early 2008. In June 2013, CoRoT's exoplanet count was 32 with several still to be confirmed. The satellite unexpectedly stopped transmitting data in November 2012 (after its mission had twice been extended), and was retired in June 2013.\n\nIn March 2009, NASA mission Kepler was launched to scan a large number of stars in the constellation Cygnus with a measurement precision expected to detect and characterize Earth-sized planets. The NASA Kepler Mission uses the transit method to scan a hundred thousand stars for planets. It was hoped that by the end of its mission of 3.5 years, the satellite would have collected enough data to reveal planets even smaller than Earth. By scanning a hundred thousand stars simultaneously, it was not only able to detect Earth-sized planets, it was able to collect statistics on the numbers of such planets around Sun-like stars.\n\nOn 2 February 2011, the Kepler team released a list of 1,235 extrasolar planet candidates, including 54 that may be in the habitable zone. On 5 December 2011, the Kepler team announced that they had discovered 2,326 planetary candidates, of which 207 are similar in size to Earth, 680 are super-Earth-size, 1,181 are Neptune-size, 203 are Jupiter-size and 55 are larger than Jupiter. Compared to the February 2011 figures, the number of Earth-size and super-Earth-size planets increased by 200% and 140% respectively. Moreover, 48 planet candidates were found in the habitable zones of surveyed stars, marking a decrease from the February figure; this was due to the more stringent criteria in use in the December data. By June 2013, the number of planet candidates was increased to 3,278 and some confirmed planets were smaller than Earth, some even Mars-sized (such as Kepler-62c) and one even smaller than Mercury (Kepler-37b).\n\nThe Transiting Exoplanet Survey Satellite launched in April, 2018.\n\nShort-period planets in close orbits around their stars will undergo reflected light variations because, like the Moon, they will go through phases from full to new and back again. In addition, as these planets receive a lot of starlight, it heats them, making thermal emissions potentially detectable. Since telescopes cannot resolve the planet from the star, they see only the combined light, and the brightness of the host star seems to change over each orbit in a periodic manner. Although the effect is small — the photometric precision required is about the same as to detect an Earth-sized planet in transit across a solar-type star – such Jupiter-sized planets with an orbital period of a few days are detectable by space telescopes such as the Kepler Space Observatory. Like with the transit method, it is easier to detect large planets orbiting close to their parent star than other planets as these planets catch more light from their parent star. When a planet has a high albedo and is situated around a relatively luminous star, its light variations are easier to detect in visible light while darker planets or planets around low-temperature stars are more easily detectable with infrared light with this method. In the long run, this method may find the most planets that will be discovered by that mission because the reflected light variation with orbital phase is largely independent of orbital inclination and does not require the planet to pass in front of the disk of the star. It still cannot detect planets with circular face-on orbits from Earth's viewpoint as the amount of reflected light does not change during its orbit.\n\nThe phase function of the giant planet is also a function of its thermal properties and atmosphere, if any. Therefore, the phase curve may constrain other planet properties, such as the size distribution of atmospheric particles. When a planet is found transiting and its size is known, the phase variations curve helps calculate or constrain the planet's albedo. It is more difficult with very hot planets as the glow of the planet can interfere when trying to calculate albedo. In theory, albedo can also be found in non-transiting planets when observing the light variations with multiple wavelengths. This allows scientists to find the size of the planet even if the planet is not transiting the star.\n\nThe first-ever direct detection of the spectrum of visible light reflected from an exoplanet was made in 2015 by an international team of astronomers. The astronomers studied light from 51 Pegasi b – the first exoplanet discovered orbiting a main-sequence star (a Sunlike star), using the High Accuracy Radial velocity Planet Searcher (HARPS) instrument at the European Southern Observatory's La Silla Observatory in Chile.\n\nBoth Corot and Kepler have measured the reflected light from planets. However, these planets were already known since they transit their host star. The first planets discovered by this method are Kepler-70b and Kepler-70c, found by Kepler.\n\nA separate novel method to detect exoplanets from light variations uses relativistic beaming of the observed flux from the star due to its motion. It is also known as Doppler beaming or Doppler boosting. The method was first proposed by Abraham Loeb and Scott Gaudi in 2003\n. As the planet tugs the star with its gravitation, the density of photons and therefore the apparent brightness of the star changes from observer's viewpoint. Like the radial velocity method, it can be used to determine the orbital eccentricity and the minimum mass of the planet. With this method, it is easier to detect massive planets close to their stars as these factors increase the star's motion. Unlike the radial velocity method, it does not require an accurate spectrum of a star, and therefore can be used more easily to find planets around fast-rotating stars and more distant stars.\n\nOne of the biggest disadvantages of this method is that the light variation effect is very small. A Jovian-mass planet orbiting 0.025 AU away from a Sun-like star is barely detectable even when the orbit is edge-on. This is not an ideal method for discovering new planets, as the amount of emitted and reflected starlight from the planet is usually much larger than light variations due to relativistic beaming. This method is still useful, however, as it allows for measurement of the planet's mass without the need for follow-up data collection from radial velocity observations.\n\nThe first discovery of a planet using this method (Kepler-76b) was announced in 2013.\n\nMassive planets can cause slight tidal distortions to their host stars. When a star has a slightly ellipsoidal shape, its apparent brightness varies, depending if the oblate part of the star is facing the observer's viewpoint. Like with the relativistic beaming method, it helps to determine the minimum mass of the planet, and its sensitivity depends on the planet's orbital inclination. The extent of the effect on a star's apparent brightness can be much larger than with the relativistic beaming method, but the brightness changing cycle is twice as fast. In addition, the planet distorts the shape of the star more if it has a low semi-major axis to stellar radius ratio and the density of the star is low. This makes this method suitable for finding planets around stars that have left the main sequence.\n\nA pulsar is a neutron star: the small, ultradense remnant of a star that has exploded as a supernova. Pulsars emit radio waves extremely regularly as they rotate. Because the intrinsic rotation of a pulsar is so regular, slight anomalies in the timing of its observed radio pulses can be used to track the pulsar's motion. Like an ordinary star, a pulsar will move in its own small orbit if it has a planet. Calculations based on pulse-timing observations can then reveal the parameters of that orbit.\n\nThis method was not originally designed for the detection of planets, but is so sensitive that it is capable of detecting planets far smaller than any other method can, down to less than a tenth the mass of Earth. It is also capable of detecting mutual gravitational perturbations between the various members of a planetary system, thereby revealing further information about those planets and their orbital parameters. In addition, it can easily detect planets which are relatively far away from the pulsar.\n\nThere are two main drawbacks to the pulsar timing method: pulsars are relatively rare, and special circumstances are required for a planet to form around a pulsar. Therefore, it is unlikely that a large number of planets will be found this way. Also, life \"as we know it\" could not survive on planets orbiting pulsars due to the intensity of high-energy radiation there.\n\nIn 1992, Aleksander Wolszczan and Dale Frail used this method to discover planets around the pulsar PSR 1257+12. Their discovery was quickly confirmed, making it the first confirmation of planets outside our Solar System.\n\nLike pulsars, some other types of pulsating variable stars are regular enough that radial velocity could be determined purely photometrically from the Doppler shift of the pulsation frequency, without needing spectroscopy. This method is not as sensitive as the pulsar timing variation method, due to the periodic activity being longer and less regular. The ease of detecting planets around a variable star depends on the pulsation period of the star, the regularity of pulsations, the mass of the planet, and its distance from the host star.\n\nThe first success with this method came in 2007, when V391 Pegasi b was discovered around a pulsating subdwarf star.\n\nThe transit timing variation method considers whether transits occur with strict periodicity, or if there is a variation. When multiple transiting planets are detected, they can often be confirmed with the transit timing variation method. This is useful in planetary systems far from the Sun, where radial velocity methods cannot detect them due to the low signal-to-noise ratio. If a planet has been detected by the transit method, then variations in the timing of the transit provide an extremely sensitive method of detecting additional non-transiting planets in the system with masses comparable to Earth's. It is easier to detect transit-timing variations if planets have relatively close orbits, and when at least one of the planets is more massive, causing the orbital period of a less massive planet to be more perturbed.\n\nThe main drawback of the transit timing method is that usually not much can be learned about the planet itself. Transit timing variation can help to determine the maximum mass of a planet. In most cases, it can confirm if an object has a planetary mass, but it does not put narrow constraints on its mass. There are exceptions though, as planets in the Kepler-36 and Kepler-88 systems orbit close enough to accurately determine their masses.\n\nThe first significant detection of a non-transiting planet using TTV was carried out with NASA's Kepler spacecraft. The transiting planet Kepler-19b shows TTV with an amplitude of five minutes and a period of about 300 days, indicating the presence of a second planet, Kepler-19c, which has a period which is a near-rational multiple of the period of the transiting planet.\n\nIn circumbinary planets, variations of transit timing are mainly caused by the orbital motion of the stars, instead of gravitational perturbations by other planets. These variations make it harder to detect these planets through automated methods. However, it makes these planets easy to confirm once they are detected.\n\n\"Duration variation\" refers to changes in how long the transit takes. Duration variations may be caused by an exomoon, apsidal precession for eccentric planets due to another planet in the same system, or general relativity.\n\nWhen a circumbinary planet is found through the transit method, it can be easily confirmed with the transit duration variation method. In close binary systems, the stars significantly alter the motion of the companion, meaning that any transiting planet has significant variation in transit duration. The first such confirmation came from Kepler-16b.\n\nWhen a binary star system is aligned such that – from the Earth's point of view – the stars pass in front of each other in their orbits, the system is called an \"eclipsing binary\" star system. The time of minimum light, when the star with the brighter surface is at least partially obscured by the disc of the other star, is called the primary eclipse, and approximately half an orbit later, the secondary eclipse occurs when the brighter surface area star obscures some portion of the other star. These times of minimum light, or central eclipses, constitute a time stamp on the system, much like the pulses from a pulsar (except that rather than a flash, they are a dip in brightness). If there is a planet in circumbinary orbit around the binary stars, the stars will be offset around a binary-planet center of mass. As the stars in the binary are displaced back and forth by the planet, the times of the eclipse minima will vary. The periodicity of this offset may be the most reliable way to detect extrasolar planets around close binary systems. With this method, planets are more easily detectable if they are more massive, orbit relatively closely around the system, and if the stars have low masses.\n\nThe eclipsing timing method allows the detection of planets further away from the host star than the transit method. However, signals around cataclysmic variable stars hinting for planets tend to match with unstable orbits. In 2011, Kepler-16b became the first planet to be definitely characterized via eclipsing binary timing variations.\n\nGravitational microlensing occurs when the gravitational field of a star acts like a lens, magnifying the light of a distant background star. This effect occurs only when the two stars are almost exactly aligned. Lensing events are brief, lasting for weeks or days, as the two stars and Earth are all moving relative to each other. More than a thousand such events have been observed over the past ten years.\n\nIf the foreground lensing star has a planet, then that planet's own gravitational field can make a detectable contribution to the lensing effect. Since that requires a highly improbable alignment, a very large number of distant stars must be continuously monitored in order to detect planetary microlensing contributions at a reasonable rate. This method is most fruitful for planets between Earth and the center of the galaxy, as the galactic center provides a large number of background stars.\n\nIn 1991, astronomers Shude Mao and Bohdan Paczyński proposed using gravitational microlensing to look for binary companions to stars, and their proposal was refined by Andy Gould and Abraham Loeb in 1992 as a method to detect exoplanets. Successes with the method date back to 2002, when a group of Polish astronomers (Andrzej Udalski, Marcin Kubiak and Michał Szymański from Warsaw, and Bohdan Paczyński) during project OGLE (the Optical Gravitational Lensing Experiment) developed a workable technique. During one month, they found several possible planets, though limitations in the observations prevented clear confirmation. Since then, several confirmed extrasolar planets have been detected using microlensing. This was the first method capable of detecting planets of Earth-like mass around ordinary main-sequence stars.\n\nUnlike most other methods, which have detection bias towards planets with small (or for resolved imaging, large) orbits, the microlensing method is most sensitive to detecting planets around 1-10 astronomical units away from Sun-like stars.\n\nA notable disadvantage of the method is that the lensing cannot be repeated, because the chance alignment never occurs again. Also, the detected planets will tend to be several kiloparsecs away, so follow-up observations with other methods are usually impossible. In addition, the only physical characteristic that can be determined by microlensing is the mass of the planet, within loose constraints. Orbital properties also tend to be unclear, as the only orbital characteristic that can be directly determined is its current semi-major axis from the parent star, which can be misleading if the planet follows an eccentric orbit. When the planet is far away from its star, it spends only a tiny portion of its orbit in a state where it is detectable with this method, so the orbital period of the planet cannot be easily determined. It is also easier to detect planets around low-mass stars, as the gravitational microlensing effect increases with the planet-to-star mass ratio.\n\nThe main advantages of the gravitational microlensing method are that it can detect low-mass planets (in principle down to Mars mass with future space projects such as WFIRST); it can detect planets in wide orbits comparable to Saturn and Uranus, which have orbital periods too long for the radial velocity or transit methods; and it can detect planets around very distant stars. When enough background stars can be observed with enough accuracy, then the method should eventually reveal how common Earth-like planets are in the galaxy.\n\nObservations are usually performed using networks of robotic telescopes. In addition to the European Research Council-funded OGLE, the Microlensing Observations in Astrophysics (MOA) group is working to perfect this approach.\n\nThe PLANET (Probing Lensing Anomalies NETwork)/RoboNet project is even more ambitious. It allows nearly continuous round-the-clock coverage by a world-spanning telescope network, providing the opportunity to pick up microlensing contributions from planets with masses as low as Earth's. This strategy was successful in detecting the first low-mass planet on a wide orbit, designated OGLE-2005-BLG-390Lb.\n\nPlanets are extremely faint light sources compared to stars, and what little light comes from them tends to be lost in the glare from their parent star. So in general, it is very difficult to detect and resolve them directly from their host star. Planets orbiting far enough from stars to be resolved reflect very little starlight, so planets are detected through their thermal emission instead. It is easier to obtain images when the star system is relatively near to the Sun, and when the planet is especially large (considerably larger than Jupiter), widely separated from its parent star, and hot so that it emits intense infrared radiation; images have then been made in the infrared, where the planet is brighter than it is at visible wavelengths. Coronagraphs are used to block light from the star, while leaving the planet visible. Direct imaging of an Earth-like exoplanet requires extreme optothermal stability. During the accretion phase of planetary formation, the star-planet contrast may be even better in H alpha than it is in infrared – an H alpha survey is currently underway.\n\nDirect imaging can give only loose constraints of the planet's mass, which is derived from the age of the star and the temperature of the planet. Mass can vary considerably, as planets can form several million years after the star has formed. The cooler the planet is, the less the planet's mass needs to be. In some cases it is possible to give reasonable constraints to the radius of a planet based on planet's temperature, its apparent brightness, and its distance from Earth. The spectra emitted from planets do not have to be separated from the star, which eases determining the chemical composition of planets.\n\nSometimes observations at multiple wavelengths are needed to rule out the planet being a brown dwarf. Direct imaging can be used to accurately measure the planet's orbit around the star. Unlike the majority of other methods, direct imaging works better with planets with face-on orbits rather than edge-on orbits, as a planet in a face-on orbit is observable during the entirety of the planet's orbit, while planets with edge-on orbits are most easily observable during their period of largest apparent separation from the parent star.\n\nThe planets detected through direct imaging currently fall into two categories. First, planets are found around stars more massive than the Sun which are young enough to have protoplanetary disks. The second category consists of possible sub-brown dwarfs found around very dim stars, or brown dwarfs which are at least 100 AU away from their parent stars.\n\nPlanetary-mass objects not gravitationally bound to a star are found through direct imaging as well.\n\nIn 2004, a group of astronomers used the European Southern Observatory's Very Large Telescope array in Chile to produce an image of 2M1207b, a companion to the brown dwarf 2M1207. In the following year, the planetary status of the companion was confirmed. The planet is estimated to be several times more massive than Jupiter, and to have an orbital radius greater than 40 AU.\n\nIn September 2008, an object was imaged at a separation of 330 AU from the star 1RXS J160929.1−210524, but it was not until 2010, that it was confirmed to be a companion planet to the star and not just a chance alignment.\n\nThe first multiplanet system, announced on 13 November 2008, was imaged in 2007, using telescopes at both the Keck Observatory and Gemini Observatory. Three planets were directly observed orbiting HR 8799, whose masses are approximately ten, ten, and seven times that of Jupiter. On the same day, 13 November 2008, it was announced that the Hubble Space Telescope directly observed an exoplanet orbiting Fomalhaut, with a mass no more than . Both systems are surrounded by disks not unlike the Kuiper belt.\n\nIn 2009, it was announced that analysis of images dating back to 2003, revealed a planet orbiting Beta Pictoris.\n\nIn 2012, it was announced that a \"Super-Jupiter\" planet with a mass about orbiting Kappa Andromedae was directly imaged using the Subaru Telescope in Hawaii. It orbits its parent star at a distance of about 55 AU, or nearly twice the distance of Neptune from the sun.\n\nAn additional system, GJ 758, was imaged in November 2009, by a team using the HiCIAO instrument of the Subaru Telescope, but it was a brown dwarf.\n\nOther possible exoplanets to have been directly imaged include GQ Lupi b, AB Pictoris b, and SCR 1845 b. As of March 2006, none have been confirmed as planets; instead, they might themselves be small brown dwarfs.\n\nSome projects to equip telescopes with planet-imaging-capable instruments include the ground-based telescopes Gemini Planet Imager, VLT-SPHERE, the Subaru Coronagraphic Extreme Adaptive Optics (SCExAO) instrument, Palomar Project 1640, and the space telescope WFIRST-AFTA. The New Worlds Mission proposes a large occulter in space designed to block the light of nearby stars in order to observe their orbiting planets. This could be used with existing, already planned or new, purpose-built, telescopes.\n\nIn 2010, a team from NASAs Jet Propulsion Laboratory demonstrated that a vortex coronagraph could enable small scopes to directly image planets. They did this by imaging the previously imaged HR 8799 planets, using just a 1.5 meter-wide portion of the Hale Telescope.\n\nAnother promising approach is nulling interferometry.\n\nIt has also been proposed that space-telescopes that focus light using zone plates instead of mirrors would provide higher-contrast imaging, and be cheaper to launch into space due to being able to fold up the lightweight foil zone plate.\n\nLight given off by a star is un-polarized, i.e. the direction of oscillation of the light wave is random. However, when the light is reflected off the atmosphere of a planet, the light waves interact with the molecules in the atmosphere and become polarized.\n\nBy analyzing the polarization in the combined light of the planet and star (about one part in a million), these measurements can in principle be made with very high sensitivity, as polarimetry is not limited by the stability of the Earth's atmosphere. Another main advantage is that polarimetry allows for determination of the composition of the planet's atmosphere. The main disadvantage is that it will not be able to detect planets without atmospheres. Larger planets and planets with higher albedo are easier to detect through polarimetry, as they reflect more light.\n\nAstronomical devices used for polarimetry, called polarimeters, are capable of detecting polarized light and rejecting unpolarized beams. Groups such as ZIMPOL/CHEOPS and PlanetPol are currently using polarimeters to search for extrasolar planets. The first successful detection of an extrasolar planet using this method came in 2008, when HD 189733 b, a planet discovered three years earlier, was detected using polarimetry. However, no new planets have yet been discovered using this method.\n\nThis method consists of precisely measuring a star's position in the sky, and observing how that position changes over time. Originally, this was done visually, with hand-written records. By the end of the 19th century, this method used photographic plates, greatly improving the accuracy of the measurements as well as creating a data archive. If a star has a planet, then the gravitational influence of the planet will cause the star itself to move in a tiny circular or elliptical orbit. Effectively, star and planet each orbit around their mutual centre of mass (barycenter), as explained by solutions to the two-body problem. Since the star is much more massive, its orbit will be much smaller. Frequently, the mutual centre of mass will lie within the radius of the larger body. Consequently, it is easier to find planets around low-mass stars, especially brown dwarfs.\nAstrometry is the oldest search method for extrasolar planets, and was originally popular because of its success in characterizing astrometric binary star systems. It dates back at least to statements made by William Herschel in the late 18th century. He claimed that an \"unseen companion\" was affecting the position of the star he cataloged as \"70 Ophiuchi\". The first known formal astrometric calculation for an extrasolar planet was made by William Stephen Jacob in 1855 for this star. Similar calculations were repeated by others for another half-century until finally refuted in the early 20th century.\nFor two centuries claims circulated of the discovery of \"unseen companions\" in orbit around nearby star systems that all were reportedly found using this method, culminating in the prominent 1996 announcement, of multiple planets orbiting the nearby star Lalande 21185 by George Gatewood. None of these claims survived scrutiny by other astronomers, and the technique fell into disrepute. Unfortunately, changes in stellar position are so small—and atmospheric and systematic distortions so large—that even the best ground-based telescopes cannot produce precise enough measurements. All claims of a \"planetary companion\" of less than 0.1 solar mass, as the mass of the planet, made before 1996 using this method are likely spurious. In 2002, the Hubble Space Telescope did succeed in using astrometry to characterize a previously discovered planet around the star Gliese 876.\n\nThe space-based observatory \"Gaia\", launched in 2013, is expected to find thousands of planets via astrometry, but prior to the launch of \"Gaia\", no planet detected by astrometry had been confirmed.\n\nSIM PlanetQuest was a US project (cancelled in 2010) that would have had similar exoplanet finding capabilities to Gaia.\n\nOne potential advantage of the astrometric method is that it is most sensitive to planets with large orbits. This makes it complementary to other methods that are most sensitive to planets with small orbits. However, very long observation times will be required — years, and possibly decades, as planets far enough from their star to allow detection via astrometry also take a long time to complete an orbit.\n\nPlanets orbiting around one of the stars in binary systems are more easily detectable, as they cause perturbations in the orbits of stars themselves. However, with this method, follow-up observations are needed to determine which star the planet orbits around.\n\nIn 2009, the discovery of VB 10b by astrometry was announced. This planetary object, orbiting the low mass red dwarf star VB 10, was reported to have a mass seven times that of Jupiter. If confirmed, this would be the first exoplanet discovered by astrometry, of the many that have been claimed through the years. However recent radial velocity independent studies rule out the existence of the claimed planet.\nIn 2010, six binary stars were astrometrically measured. One of the star systems, called HD 176051, was found with \"high confidence\" to have a planet.\n\nIn 2018, a study comparing observations from the Gaia spacecraft to Hipparcos data for the Beta Pictoris system was able to measure the mass of Beta Pictoris b, constraining it to Jupiter masses. This is in good agreement with previous mass estimations of roughly 13 Jupiter masses.\n\nAn optical/infrared interferometer array doesn't collect as much light as a single telescope of equivalent size, but has the resolution of a single telescope the size of the array. For bright stars, this resolving power could be used to image a star's surface during a transit event and see the shadow of the planet transiting. This could provide a direct measurement of the planet's angular radius and, via parallax, its actual radius. This is more accurate than radius estimates based on transit photometry, which are dependent on stellar radius estimates which depend on models of star characteristics. Imaging also provides more accurate determination of the inclination than photometry does.\n\nRadio emissions from magnetospheres could be detected with future radio telescopes. This could enable determination of the rotation rate of a planet, which is difficult to detect otherwise.\n\nAuroral radio emissions from giant planets with plasma sources, such as Jupiter's volcanic moon Io, could be detected with radio telescopes such as LOFAR.\n\nBy looking at the wiggles of an interferogram using a Fourier-Transform-Spectrometer, enhanced sensitivity could be obtained in order to detect faint signals from Earth-like planets.\n\nDisks of space dust (debris disks) surround many stars. The dust can be detected because it absorbs ordinary starlight and re-emits it as infrared radiation. Even if the dust particles have a total mass well less than that of Earth, they can still have a large enough total surface area that they outshine their parent star in infrared wavelengths.\n\nThe Hubble Space Telescope is capable of observing dust disks with its NICMOS (Near Infrared Camera and Multi-Object Spectrometer) instrument. Even better images have now been taken by its sister instrument, the Spitzer Space Telescope, and by the European Space Agency's Herschel Space Observatory, which can see far deeper into infrared wavelengths than the Hubble can. Dust disks have now been found around more than 15% of nearby sunlike stars.\n\nThe dust is thought to be generated by collisions among comets and asteroids. Radiation pressure from the star will push the dust particles away into interstellar space over a relatively short timescale. Therefore, the detection of dust indicates continual replenishment by new collisions, and provides strong indirect evidence of the presence of small bodies like comets and asteroids that orbit the parent star. For example, the dust disk around the star tau Ceti indicates that that star has a population of objects analogous to our own Solar System's Kuiper Belt, but at least ten times thicker.\n\nMore speculatively, features in dust disks sometimes suggest the presence of full-sized planets. Some disks have a central cavity, meaning that they are really ring-shaped. The central cavity may be caused by a planet \"clearing out\" the dust inside its orbit. Other disks contain clumps that may be caused by the gravitational influence of a planet. Both these kinds of features are present in the dust disk around epsilon Eridani, hinting at the presence of a planet with an orbital radius of around 40 AU (in addition to the inner planet detected through the radial-velocity method). These kinds of planet-disk interactions can be modeled numerically using collisional grooming techniques.\n\nSpectral analysis of white dwarfs' atmospheres often finds contamination of heavier elements like magnesium and calcium. These elements cannot originate from the stars' core, and it is probable that the contamination comes from asteroids that got too close (within the Roche limit) to these stars by gravitational interaction with larger planets and were torn apart by star's tidal forces. Up to 50% of young white dwarfs may be contaminated in this manner.\n\nAdditionally, the dust responsible for the atmospheric pollution may be detected by infrared radiation if it exists in sufficient quantity, similar to the detection of debris discs around main sequence stars. Data from the Spitzer Space Telescope suggests that 1-3% of white dwarfs possess detectable circumstellar dust.\n\nIn 2015, minor planets were discovered transiting the white dwarf WD 1145+017. This material orbits with a period of around 4.5 hours, and the shapes of the transit light curves suggest that the larger bodies are disintegrating, contributing to the contamination in the white dwarf's atmosphere.\n\nMost confirmed extrasolar planets have been found using space-based telescopes (as of 01/2015). Many of the detection methods can work more effectively with space-based telescopes that avoid atmospheric haze and turbulence. COROT (2007-2012) and Kepler were space missions dedicated to searching for extrasolar planets using transits. COROT discovered about 30 new exoplanets. \nKepler (2009-2013) and K2 (2013- ) have discovered over 2000 verified exoplanets. Hubble Space Telescope and MOST have also found or confirmed a few planets. The infrared Spitzer Space Telescope has been used to detect transits of extrasolar planets, as well as occultations of the planets by their host star and phase curves.\n\nThe Gaia mission, launched in December 2013, will use astrometry to determine the true masses of 1000 nearby exoplanets.\nCHEOPS and TESS, to be launched in 2018, and PLATO in 2024 will use the transit method.\n\n\n\n\n"}
{"id": "24057866", "url": "https://en.wikipedia.org/wiki?curid=24057866", "title": "Museum of Optical Technologies", "text": "Museum of Optical Technologies\n\nThe Museum of Optical Technologies functions as part of the St. Petersburg State University of Information Technologies, Mechanics and Optics (SPbSU ITMO). It was founded to acquaint young people with the achievements in optical technology, including both modern optical engineering and information technology.\n\nThe museum is located at 14th Birzhevaya Line and was opened on December 16, 2003, at Vasilevsky Island as an open and interactive optics museum. It was formerly known as the Laboratories of the Russian State Optical Institute with support of the Saint Petersburg City Administration.\n\nDuring 2009-2010, it planned to open an interactive hall for employment of Schoolchildren.\n\nThe exposition is located on the ground floor of the building and consists of 3 halls.\n\n(\n"}
{"id": "50652942", "url": "https://en.wikipedia.org/wiki?curid=50652942", "title": "NGC 139", "text": "NGC 139\n\nNGC 139 is a barred spiral galaxy in the constellation Pisces. It was discovered on August 29, 1864 by the German astronomer Albert Marth.\n"}
{"id": "50885756", "url": "https://en.wikipedia.org/wiki?curid=50885756", "title": "Nadia Zakamska", "text": "Nadia Zakamska\n\nNadia Zakamska is a Russian-American astronomer who is a professor at Johns Hopkins University.\n\nZakamska graduated from Moscow Institute of Physics and Technology with a master's degree in theoretical physics in 2001. Zakamska then attended Princeton University for her PhD, which she received in 2005.\n\nZakamska's research involves multi-wavelength work on Type II quasars. She also studies supermassive black holes and their role in galaxy formation. In addition, she studies extrasolar planets and extragalactic astronomy. \n\nZakamska is a Sloan Fellow. In 2014, she received the American Astronomical Society's Newton Lacy Pierce Prize, which is awarded to recognize at least five years of outstanding achievement in observational astronomical research.\n\n"}
{"id": "26021933", "url": "https://en.wikipedia.org/wiki?curid=26021933", "title": "Noel Bayliss", "text": "Noel Bayliss\n\nSir Noel Stanley Bayliss, CBE (19 December 1906 – 17 February 1996) was an eminent Australian chemist and professor of chemistry at the University of Western Australia. He was a Rhodes Scholar and graduated as dux of the academically renowned Melbourne High School. He then attended the University of Melbourne before going to Lincoln College, Oxford. The mineral baylissite KMg(CO)2•4(HO) is named for him.\n"}
{"id": "57877294", "url": "https://en.wikipedia.org/wiki?curid=57877294", "title": "Norman Garmezy", "text": "Norman Garmezy\n\nNorman Garmezy was a university professor of psychology and pioneer in research on risk and resilience in humans. Garmezy suffered from Alzheimer's in later life.\n\nIn the field of psychology he focused on risk and resilience in terms of human brain development.\n"}
{"id": "17249422", "url": "https://en.wikipedia.org/wiki?curid=17249422", "title": "Opalite", "text": "Opalite\n\nOpalite is a trade name for man-made opalescent glass and various opal simulants. Other names for this glass product include \"argenon\", \"sea opal\", \"opal moonstone\" and other similar names. It is also used to promote impure varieties of variously colored common opal.\n\nNatural Opalite (as opposed to the man made Opalite) shares the same basic chemical properties as Opal. It is made of tiny spheres of Silicon Dioxide which stack onto each other in a pyramid grid shape. This grid is what allows the cat’s eye effect to be displayed when the stone is cut into a high domed cabochon. Natural opalite is referred to as \"common opal\" to prevent confusing it with glass opalite.\n\nWhen opalite glass is placed against a dark background, it appears to have a blue color. When placed against a light background, it is milky white with an orange or pink glow. Because it is glass, it may sometimes contain air bubbles, an after effect of the forming process.\n\nOpalite is mainly used as a decorative stone, and is usually sold either tumble polished or carved into decorative objects. Some sellers will sell opalite as opal or moonstone.\n"}
{"id": "53445996", "url": "https://en.wikipedia.org/wiki?curid=53445996", "title": "Optimot", "text": "Optimot\n\nOptimot, linguistic inquiries, is a service provided by the Directorate - General of Linguistic Policy of the Catalan Government in collaboration with the Institute for Catalan Studies and the Terminology Center TERMCAT. It consists of a search engine for linguistic information that helps to clarify doubts about the Catalan language. With Optimot different sources can be checked at the same time in an integrated way. When the search options provided by Optimot do not manage to answer the linguistic question, a personalized inquiry service can be accessed.\n\nUp to 2007, the Consortium for Linguistic Normalization, the TERMCAT Terminology Centre, the Institute of Catalan Studies, and the directorate-general of Linguistics Politics dealt with linguistic inqueries that came from general population, companies, organizations, and language professionals. In order to avoid decentralization of the linguistic enquiries and to offer a unified service, Optimot was implemented. The search engine started working in October 2007 and the personalized service began in February 2008. Mainly, Optimot was to improve quality in linguistic-inquiry service by unifying criteria, and to promote linguistic autonomy.\n\nThe sources used in the searches performed by Optimot are the following:\n\n\n"}
{"id": "908764", "url": "https://en.wikipedia.org/wiki?curid=908764", "title": "Perl Data Language", "text": "Perl Data Language\n\nPerl Data Language (abbreviated PDL) is a set of free software array programming extensions to the Perl programming language. PDL extends the data structures built into Perl, to include large multidimensional arrays, and adds functionality to manipulate those arrays as vector objects. It also provides tools for image processing, computer modeling of physical systems, and graphical plotting and presentation. Simple operations are automatically vectorized across complete arrays, and higher-dimensional operations (such as matrix multiplication) are supported.\n\nPDL is a vectorized array programming language: the expression syntax is a variation on standard mathematical vector notation, so that the user can combine and operate on large arrays with simple expressions. In this respect, PDL follows in the footsteps of the APL programming language, and it has been compared to commercial languages such as MATLAB and Interactive Data Language, and to other free languages such as NumPy and Octave. Unlike MATLAB and IDL, PDL allows great flexibility in indexing and vectorization: for example, if a subroutine normally operates on a 2-D matrix array, passing it a 3-D data cube will generally cause the same operation to happen to each 2-D layer of the cube.\n\nPDL borrows from Perl at least three basic types of program structure: imperative programming, functional programming, and pipeline programming forms may be combined. Subroutines may be loaded either via a built-in autoload mechanism or via the usual Perl module mechanism. PDL-like functionality is being included in the development of Perl 6.\n\nTrue to the glue language roots of Perl, PDL borrows from several different modules for graphics and plotting support. NetPBM provides image file I/O (though FITS is supported natively). Gnuplot, PLplot, PGPLOT, and Prima modules are supported for 2-D graphics and plotting applications, and Gnuplot and OpenGL are supported for 3-D plotting and rendering.\n\nPDL provides facilities to read and write many open data formats, including JPEG, PNG, GIF, PPM, MPEG, FITS, NetCDF, GRIB, raw binary files, and delimited ASCII tables. PDL programmers can use the CPAN Perl I/O libraries to read and write data in hundreds of standard and niche file formats.\n\nAn installation of PDL usually comes with an interactive shell known as perldl, which can be used to perform simple calculations without requiring the user to create a Perl program file. A typical session of perldl would look something like the following:\n\nThe commands used in the shell are Perl statements that can be used in a program with codice_1 module included. codice_2 is an overloaded operator for matrix multiplication, and codice_3 in the last command is a shortcut for codice_4.\n\nThe core of PDL is written in C. Most of the functionality is written in PP, a PDL-specific metalanguage that handles the vectorization of simple C snippets and interfaces them with the Perl host language via Perl's XS compiler. Some modules are written in Fortran, with a C/PP interface layer. Many of the supplied functions are written in PDL itself. PP is available to the user to write C-language extensions to PDL. There is also an Inline module (Inline::Pdlpp) that allows PP function definitions to be inserted directly into a Perl script; the relevant code is low-level compiled and made available as a Perl subroutine.\n\nThe PDL API uses the basic Perl 5 object-oriented functionality: PDL defines a new type of Perl scalar object (eponymously called a \"PDL\", pronounced \"piddle\") that acts as a Perl scalar, but that contains a conventional typed array of numeric or character values. All of the standard Perl operators are overloaded so that they can be used on PDL objects transparently, and PDLs can be mixed-and-matched with normal Perl scalars. Several hundred object methods for operating on PDLs are supplied by the core modules.\n\nIn Perl 6, PDL is specified as a trait in Synopsis 9. As of January 2013, this feature is not yet implemented in Rakudo, though.\n\n\n"}
{"id": "1919367", "url": "https://en.wikipedia.org/wiki?curid=1919367", "title": "QCD matter", "text": "QCD matter\n\nQuark matter or QCD matter refers to any of a number of theorized phases of matter whose degrees of freedom include quarks and gluons. These theoretical phases would occur at extremely high temperatures and/or densities, billions of times higher than can be produced in equilibrium in laboratories. Under such extreme conditions, the familiar structure of matter, where the basic constituents are nuclei (consisting of nucleons which are bound states of quarks) and electrons, is disrupted. In quark matter it is more appropriate to treat the quarks themselves as the basic degrees of freedom.\n\nIn the standard model of particle physics, the strong force is described by the theory of quantum chromodynamics (QCD). At ordinary temperatures or densities this force just confines the quarks into composite particles (hadrons) of size around 10 m = 1 femtometer = 1 fm (corresponding to the QCD energy scale Λ ≈ 200 MeV) and its effects are not noticeable at longer distances. However, when the temperature reaches the QCD energy scale (T of order 10 kelvins) or the density rises to the point where the average inter-quark separation is less than 1 fm (quark chemical potential μ around 400 MeV), the hadrons are melted into their constituent quarks, and the strong interaction becomes the dominant feature of the physics. Such phases are called quark matter or QCD matter.\n\nThe strength of the color force makes the properties of quark matter unlike gas or plasma, instead leading to a state of matter more reminiscent of a liquid. At high densities, quark matter is a Fermi liquid, but is predicted to exhibit color superconductivity at high densities and temperatures below 10 K.\n\n\nEven though quark-gluon plasma can only occur under quite extreme conditions of temperature and/or pressure, it is being actively studied at particle colliders, such as the Large Hadron Collider LHC at CERN and the Relativistic Heavy Ion Collider RHIC at Brookhaven National Laboratory. In these collisions, the plasma only occurs for a very short time before it spontaneously disintegrates, because the extreme conditions during the collision process cannot be upheld. The plasma's physical characteristics are studied by detecting the debris emanating from the collision region with large particle detectors \n\nHeavy-ion collisions at very high energies can produce small short-lived regions of space whose energy density is comparable to that of the 20-micro-second-old universe. This has been achieved by colliding heavy nuclei such as lead nuclei at high speeds, and a first time claim of formation of quark–gluon plasma came from the SPS accelerator at CERN in February 2000. This work has been continued at more powerful accelerators, such as RHIC in the USA, and as of 2010 at the European LHC at CERN located in the border area of Switzerland and France. There is good evidence that the quark–gluon plasma has also been produced at RHIC.\n\nThe context for understanding the thermodynamics of quark matter is the standard model of particle physics, which contains six different flavors of quarks, as well as leptons like electrons and neutrinos. These interact via the strong interaction, electromagnetism, and also the weak interaction which allows one flavor of quark to turn into another. Electromagnetic interactions occur between particles that carry electrical charge; strong interactions occur between particles that carry color charge.\n\nThe correct thermodynamic treatment of quark matter depends on the physical context. For large quantities that exist for long periods of time (the \"thermodynamic limit\"), we must take into account the fact that the only conserved charges in the standard model are quark number (equivalent to baryon number), electric charge, the eight color charges, and lepton number. Each of these can have an associated chemical potential. However, large volumes of matter must be electrically and color-neutral, which determines the electric and color charge chemical potentials. This leaves a three-dimensional phase space, parameterized by quark chemical potential, lepton chemical potential, and temperature.\n\nIn compact stars quark matter would occupy cubic kilometers and exist for millions of years, so the thermodynamic limit is appropriate. However, the neutrinos escape, violating lepton number, so the phase space for quark matter in compact stars only has two dimensions, temperature (\"T\") and quark number chemical potential μ. A strangelet is not in the thermodynamic limit of large volume, so it is like an exotic nucleus: it may carry electric charge.\n\nA heavy-ion collision is in neither the thermodynamic limit of large volumes nor long times. Putting aside questions of whether it is sufficiently equilibrated for thermodynamics to be applicable, there is certainly not enough time for weak interactions to occur, so flavor is conserved, and there are independent chemical potentials for all six quark flavors. The initial conditions (the impact parameter of the collision, the number of up and down quarks in the colliding nuclei, and the fact that they contain no quarks of other flavors) determine the chemical potentials. (Reference for this section:,).\n\nThe phase diagram of quark matter is not well known, either experimentally or theoretically. A commonly conjectured form of the\nphase diagram is shown in the figure. It is applicable to matter in a compact star, where the only relevant thermodynamic potentials are quark chemical potential μ and temperature T. For guidance it also shows the typical values of μ and \"T\" in heavy-ion collisions and in the early universe. For readers who are not familiar with the concept of a chemical potential, it is helpful to think of μ as a measure of the imbalance between quarks and antiquarks in the system. Higher μ means a stronger bias favoring quarks over antiquarks. At low temperatures there are no antiquarks, and then higher μ generally means a higher density of quarks.\n\nOrdinary atomic matter as we know it is really a mixed phase, droplets of nuclear matter (nuclei) surrounded by vacuum, which exists at the low-temperature phase boundary between vacuum and nuclear matter, at μ = 310 MeV and \"T\" close to zero. If we increase the quark density (i.e. increase μ) keeping the temperature low, we move into a phase of more and more compressed nuclear matter. Following this path corresponds to burrowing more and more deeply into a neutron star. Eventually, at an unknown critical value of μ, there is a transition to quark matter. At ultra-high densities we expect to find the color-flavor-locked (CFL) phase of color-superconducting quark matter. At intermediate densities we expect some other phases (labelled \"non-CFL quark liquid\" in the figure) whose nature is presently unknown. They might be other forms of color-superconducting quark matter, or something different.\n\nNow, imagine starting at the bottom left corner of the phase diagram, in the vacuum where μ = \"T\" = 0. If we heat up the system without introducing any preference for quarks over antiquarks, this corresponds to moving vertically upwards along the \"T\" axis. At first, quarks are still confined and we create a gas of hadrons (pions, mostly). Then around \"T\" = 150 MeV there is a crossover to the quark gluon plasma: thermal fluctuations break up the pions, and we find a gas of quarks, antiquarks, and gluons, as well as lighter particles such as photons, electrons, positrons, etc. Following this path corresponds to travelling far back in time (so to say), to the state of the universe shortly after the big bang (where there was a very tiny preference for quarks over antiquarks).\n\nThe line that rises up from the nuclear/quark matter transition and then bends back towards the \"T\" axis, with its end marked by a star, is the conjectured boundary between confined and unconfined phases. Until recently it was also believed to be a boundary between phases where chiral symmetry is broken (low temperature and density) and phases where it is unbroken (high temperature and density). It is now known that the CFL phase exhibits chiral symmetry breaking, and other quark matter phases may also break chiral symmetry, so it is not clear whether this is really a chiral transition line. The line ends at the \"chiral critical point\", marked by a star in this figure, which is a special temperature and density at which striking physical phenomena, analogous to critical opalescence, are expected. (Reference for this section:,).\n\nFor a complete description of phase diagram it is required that one must have complete understanding of dense, strongly interacting hadronic matter and strongly interacting quark matter from some underlying theory e.g. quantum chromodynamics (QCD). However, because such a description requires the proper understanding of QCD in its non-perturbative regime, which is still far from being completely understood, any theoretical advance remains very challenging.\n\nThe phase structure of quark matter remains mostly conjectural because it is difficult to perform calculations predicting the properties of quark matter. The reason is that QCD, the theory describing the dominant interaction between quarks, is strongly coupled at the densities and temperatures of greatest physical interest, and hence it is very hard to obtain any predictions from it. Here are brief descriptions of some of the standard approaches.\n\nThe only first-principles calculational tool currently available is lattice QCD, i.e. brute-force computer calculations. Because of a technical obstacle known as the fermion sign problem, this method can only be used at low density and high temperature (μ < \"T\"), and it predicts that the crossover to the quark–gluon plasma will occur around \"T\" = 150 MeV However, it cannot be used to investigate the interesting color-superconducting phase structure at high density and low temperature.\n\nBecause QCD is asymptotically free it becomes weakly coupled at unrealistically high densities, and diagrammatic\nmethods can be used. Such methods show that the CFL phase occurs at very high density. At high temperatures, however, diagrammatic methods are still not under full control.\n\nTo obtain a rough idea of what phases might occur, one can use a model that has some of the same properties as QCD, but is easier to manipulate. Many physicists use Nambu-Jona-Lasinio models, which contain no gluons, and replace the strong interaction with a four-fermion interaction. Mean-field methods are commonly used to analyse the phases. Another approach is the bag model, in which the effects of confinement are simulated by an additive energy density that penalizes unconfined quark matter.\n\nMany physicists simply give up on a microscopic approach, and make informed guesses of the expected phases (perhaps based on NJL model results). For each phase, they then write down an effective theory for the low-energy excitations, in terms of a small number of parameters, and use it to make predictions that could allow those parameters to be fixed by experimental observations.\n\nThere are other methods that are sometimes used to shed light on QCD, but for various reasons have not yet yielded useful results in studying quark matter.\n\nTreat the number of colors \"N\", which is actually 3, as a large number, and expand in powers of 1/\"N\". It turns out that at high density the higher-order corrections are large, and the expansion gives misleading results.\n\nAdding scalar quarks (squarks) and fermionic gluons (gluinos) to the theory makes it more tractable, but the thermodynamics of quark matter depends crucially on the fact that only fermions can carry quark number, and on the number of degrees of freedom in general.\n\nExperimentally, it is hard to map the phase diagram of quark matter because it has been rather difficult to learn how to tune to high enough temperatures and density in the laboratory experiment using collisions of relativistic heavy ions as experimental tools. However, these collisions ultimately will provide information about the crossover from hadronic matter to QGP. It has been suggested that the observations of compact stars may also constrain the information about the high-density low-temperature region. Models of the cooling, spin-down, and precession of these stars offer information about the relevant properties of their interior. As observations become more precise, physicists hope to learn more.\n\nOne of the natural subjects for future research is the search for the exact location of the chiral critical point. Some ambitious lattice QCD calculations may have found evidence for it, and future calculations will clarify the situation. Heavy-ion collisions might be able to measure its position experimentally, but this will require scanning across a range of values of μ and T.\n\n\n\n"}
{"id": "26480", "url": "https://en.wikipedia.org/wiki?curid=26480", "title": "Radio Research Project", "text": "Radio Research Project\n\nThe Radio Research Project was a social research project funded by the Rockefeller Foundation to look into the effects of mass media on society.\n\nIn 1937, the Rockefeller Foundation started funding research to find the effects of new forms of mass media on society, especially radio. Several universities joined up and a headquarters was formed at the School of Public and International Affairs at Princeton University. \n\nAmong the subjects of the Project's first studies were soap operas, known as radio dramas at the time.\n\nThe Radio Project also conducted research on the infamous Halloween broadcast of \"The War of the Worlds\" in 1938. Of the estimated 6 million people who heard this broadcast, they found that 25% accepted the program's reports of mass destruction. The majority of these did not think they were hearing a literal invasion from Mars, but rather an attack by Germany. The researchers determined that radio broadcasts from the Munich Crisis may have lent credence to this supposition.\n\nA third research project was that of listening habits. Because of this, a new method was developed to survey an audience – this was dubbed the Little Annie Project. The official name was the Stanton-Lazarsfeld Program Analyzer. This allowed one not only to find out if a listener liked the performance, but how they felt at any individual moment, through a dial which they would turn to express their preference (positive or negative). This has since become an essential tool in focus group research.\n\nTheodor Adorno produced numerous reports on the effects of \"atomized listening\" which radio supported and of which he was highly critical. However, because of profound methodological disagreements with Lazarsfeld over the use of techniques such as listener surveys and \"Little Annie\" (Adorno thought both grossly simplified and ignored the degree to which expressed tastes were the result of commercial marketing), Adorno left the project in 1941.\n"}
{"id": "17434804", "url": "https://en.wikipedia.org/wiki?curid=17434804", "title": "Ralph Anthony Blakelock", "text": "Ralph Anthony Blakelock\n\nRalph Anthony Blakelock (1915-1963) was a British botanist. He particularly focused on the research of spermatophites.\n"}
{"id": "2205147", "url": "https://en.wikipedia.org/wiki?curid=2205147", "title": "Raunkiær plant life-form", "text": "Raunkiær plant life-form\n\nThe Raunkiær system is a system for categorizing plants using life-form categories, devised by Danish botanist Christen C. Raunkiær and later extended by various authors.\n\nIt was first proposed in a talk to the \"Danish Botanical Society\" in 1904 as can be inferred from the printed discussion of that talk, but not the talk itself, nor its title. The journal, Botanisk Tidsskrift, published brief comments on the talk by M.P. Porsild, with replies by Raunkiær. A fuller account appeared in French the following year. Raunkiær elaborated further on the system and published this in Danish in 1907.\n\nThe original note and the 1907 paper were much later translated to English and published with Raunkiær's collected works.\n\nRaunkiær's life-form scheme has subsequently been revised and modified by various authors, but the main structure has survived. Raunkiær's life-form system may be useful in researching the transformations of biotas and the genesis of some groups of phytophagous animals.\n\nThe subdivisions of the Raunkiær system are based on the place of the plant's growth-point (bud) during seasons with adverse conditions (cold seasons and dry seasons):\n\nProjecting stems into the air – normally woody perennials - with resting buds more than 25 cm above soil level, e.g. trees and shrubs, but also epiphytes, which Raunkiær separated out as a special group in later versions of the system.\n\nRaunkiær further subdivided the phanerophytes according to height as \"megaphanerophytes\", \"mesophanerophytes\", \"microphanerophytes\", and \"nanophanerophytes\". Other characters used to further subdivide were duration of leaves (evergreen or deciduous) and presence of covering bracts on buds (eight classes in all). Three further classes, phanerophytic stem succulents, phanerophytic epiphytes, and phanerophytic herbs brought the number of subclasses to 12.\n\nBuds on persistent shoots near the ground – woody plants with perennating buds borne close to the ground, no more than 25 cm above the soil surface, (e.g., bilberry and periwinkle).\n\nBuds at or near the soil surface, e.g. daisy, dandelion.\n\nBelow ground or under water - with resting buds lying either beneath the surface of the ground as a rhizome, bulb, corm, etc., or a resting bud submerged under water. Cryptophytes are divided into 3 groups:\n\nAnnual plants which complete their life-cycle rapidly under favorable conditions and survive the unfavorable cold or dry season in the form of seed. Many desert plants are by necessity therophytes.\n\nEpiphytes were originally placed in Phanerophytes (above) but then separated because of irrelevance of soil position.\n\nA later addition to the Raunkiær lifeform classification. Plant that obtains moisture and nutrients from the air and rain; usually grows on other plants but not parasitic on them. These are perennial plants whose roots atrophy; some can live on mobile sand dunes; like epiphytes and hemicryptophytes, their buds are near the surface. This group includes some but not all \"Tillandsia\" species.\n"}
{"id": "9202249", "url": "https://en.wikipedia.org/wiki?curid=9202249", "title": "Research Quality Framework", "text": "Research Quality Framework\n\nResearch Quality Framework (RQF) was a component of Backing Australia's Ability, an initiative of the Australian Government to formulate a best practice framework for assessing research quality and the impact of research, and ensure that public funding was being invested in research which would deliver real benefits to the wider community. RQF was to bring public funding of research in line with government policy for funding to be determined by outcomes achieved.\n\nIt was an assessment framework and a funding model, similar to the Research Assessment Exercise in the United Kingdom and the Performance Based Research Fund in New Zealand.\n\nOn 21 December 2007, the new Australian Government announced that it would not be proceeding with the RQF project, which was an initiative of the former Government, and began developing the Excellence in Research for Australia initiative.\n\nFocuses on quality and impact.\n\nThe quality of research includes its intrinsic merit and academic impact - that is recognition of the originality of research by peers and its impact on the development of the same or related disciple areas.\n\nThe broader impact is an assessment of value through use, i.e. the extent to which research is successfully applied - that is recognition by qualified end-users that quality research has been successfully applied.\n\n\n\n\nWill replace the RTS block funding model, and will be at least 50% of the determinations of funding. Nelson says : all of the Institutional Grants Scheme (IGS) and at least 50% of the Research Training Scheme (RTS).\n\nUK's Professor Sir Gareth Roberts was appointed as the Chair of the Expert Advisory Group, having been commissioned in June 2002 to review the future of research assessment in the UK.\n\nThe list of members was announced 21 December 2004 as:\n\nMinister Julie Bishop received the \"Research Quality Framework: Assessing the quality and impact of research in Australia – Final Advice on the Preferred RQF Model\" paper from Sir Gareth Roberts, approved its release, and announced the establishment of the RQF Development Advisory Group (RQFDAG), to be chaired by Australia's chief scientist, Dr Jim Peacock AC. \n\nThe following members of the \"Expert Advisory Group\" continued on to the DAG:\n\nThe other DAG members are:\n\n\n\n\n\n"}
{"id": "33615890", "url": "https://en.wikipedia.org/wiki?curid=33615890", "title": "Scottish Informatics and Computer Science Alliance", "text": "Scottish Informatics and Computer Science Alliance\n\nThe Scottish Informatics and Computer Science Alliance (SICSA) is a \"research pool\" funded by the Scottish Funding Council. A research pool is a collaboration of Scottish university departments whose broad objective is to create a coherent research community that will improve the quality of research carried out in Scotland in the pool-related discipline.\n\nSICSA's goals are to improve the quality of research in informatics and computer science across universities in Scotland, to promote the transfer of research results to benefit companies and the public sector in Scotland and to create a university community that represents all aspects of Scottish Informatics and Computer Science.\n\nSICSA was launched in December 2008 and is funded with an award of £14.5 million from the Scottish Funding Council with SICSA member universities providing matching funding. It is managed by the SICSA Executive which is composed of: Director of Research (who is also the SICSA Director), the Director of the SICSA Graduate Academy, the Director of Knowledge Exchange, the Director of Education, the SICSA Executive Officer and the SICSA Executive Assistant.\n\nSICSA has adopted an inclusive membership policy and all universities in Scotland that have computer science or informatics departments/schools are eligible to be members of SICSA. , the following departments/schools were SICSA members.\n\n\nSICSA’s research activities are organized around four broad research themes\nEach research theme is managed by a theme leader and organizes workshops and events for the computer science research community across Scotland.\n\nTo help achieve its goal of improving research quality, SICSA has funded the appointment of 30 academic staff across Scottish universities. Appointments have been made in Edinburgh, Glasgow, St Andrews, Stirling, Strathclyde, Abertay and Aberdeen Universities.\n\nThe SICSA Graduate Academy (SGA) is an international graduate school in informatics and computer science. It provides funding for PhD students, supports graduate training and summer schools and runs a Distinguished Visitor scheme which supports visits from distinguished computer scientists to Scotland. The SGA runs an annual conference for all PhD students working in computer science and informatics in Scotland.\n\nSICSA has made available 80 prize studentships for PhD study in Scotland to students from around the world, with the aim of attracting the research leaders of the future to work in Scotland.\n\nSICSA provides support for PhD students in Scotland to attend international summer school and sponsors up to 3 international summer schools per year for PhD students in Scotland. Nine summer schools have been sponsored in Scottish universities between June 2009 and August 2012.\n\nThe aim of the SICSA Distinguished Visitor scheme is to provide support for excellent researchers from around the world to interact with the Scottish Computer Science and Informatics research community.\n\nSICSA interacts extensively with local industry in Scotland with a view to transferring technology and informing the industrial community of informatics and computer science research in Scotland. An annual \"Demofest\" is organized that showcases Scottish University research. Specific projects with industry are concerned with smart tourism and migrating high-value software services to the cloud.\n\nIn 2013, SICSA developed a portfolio of funding mechanisms which have the specific aim of increasing exchanges between academics and industry. These mechanisms include: SICSA Postgraduate Industry Internship Program; SICSA Early Career Industry Fellowship; SICSA Distinguished Industrial Visitors Fellowship; SICSA Elevate - Incubator Program; SICSA Proof of Concept Program; SICSA Team Based Industrial Placements Program.\n\nUniquely amongst the SFC research pools, SICSA has extended its remit to include education as well as research. The aim of this move is to allow SICSA to become the single representative body for all aspects of university informatics/computer science research and education in Scotland.\n\nSICSA maintains information about undergraduate and postgraduate taught courses in Scotland for potential students and provides information for industry on opportunities for graduate recruitment.\n\n"}
{"id": "22322406", "url": "https://en.wikipedia.org/wiki?curid=22322406", "title": "Substrate reduction therapy", "text": "Substrate reduction therapy\n\nSubstrate reduction therapy offers an approach to treatment of certain metabolic disorders, especially glycogen storage diseases and lysosomal storage disorders. In a storage disorder, a critical failure in a metabolic pathway prevents cellular breakdown and disposal of some large molecule. If residual breakdown through other pathways is insufficient to prevent harmful accumulation, the molecule accumulates in the cell and eventually interferes with normal biological processes. Examples of lysosomal storage disorders include Gaucher's disease, Tay-Sachs disease, and Sandhoff disease.\n\nIn a metabolic or genetic pathway, enzymes catalyze a series of reactions. Each enzyme is regulated or mediated by one gene through its RNA and protein products. At each phase in the pathway, enzyme activity catalyzes a reaction in which a precursor molecule (the substrate) is transformed into its next intermediate state. Failure of the metabolic pathway leads to accumulation of the substrate, with possible harmful effects. Substrate reduction therapy addresses this failure by reducing the level of the substrate to a point where residual degradative activity is sufficient to prevent substrate accumulation.\n\n"}
{"id": "32526961", "url": "https://en.wikipedia.org/wiki?curid=32526961", "title": "The Mind Museum", "text": "The Mind Museum\n\nThe Mind Museum is a science museum in Taguig, Metro Manila, Philippines. It is located on a lot in the J. Y. Campos Park in Bonifacio Global City, a business district of the city.\n\nThe museum opened in March 16, 2012, although a pre-launch reception was held a year earlier on December 15 where Vice President Jejomar Binay delivered a speech in behalf of President Benigno Aquino III. The facility was developed by the Bonifacio Arts Foundation Inc (BAFI).\n\nThe museum was designed by architect Ed Calma, from Lor Calma & Partners. The design of the structure was inspired from cellular structure and growth and had a solar reflective exterior, natural wind ventilation and rainwater flow drainage.\n\nAs of 2012, the museum has five main galleries occupying a exhibit area and spanning 2 floors. The galleries each had its own theme namely, atom, life, earth, universe and technology which are linked by features called \"Nature’s Webways\". The atom, life, earth, univers galleries are located on the first floor while the technology gallery are found in the first floor of the two story museum.\n\nWith assistance from a firm based in the United States, which did the master plan of the museum, Filipino designers, scientists and fabricators did 90 percent of the museum's exhibits. The designers included designers and faculty from the College of Fine Arts of the University of the Philippines and the University of Santo Tomas.\n\nAt the 2014 THEA Awards held in April 5, at the Disneyland Resort, Anaheim, California in the United States, the museum was awarded with the THEA Award for Outstanding Achievement for the Science Museum category for the design and execution of its exhibits.The distinction is the first for a Philippine establishment and for a science museum in Asia at the THEA Awards.\n\n"}
{"id": "619632", "url": "https://en.wikipedia.org/wiki?curid=619632", "title": "Transfection", "text": "Transfection\n\nTransfection is the process of deliberately introducing naked or purified nucleic acids into eukaryotic cells. It may also refer to other methods and cell types, although other terms are often preferred: \"transformation\" is typically used to describe non-viral DNA transfer in bacteria and non-animal eukaryotic cells, including plant cells. In animal cells, transfection is the preferred term as transformation is also used to refer to progression to a cancerous state (carcinogenesis) in these cells. Transduction is often used to describe virus-mediated gene transfer into eukaryotic cells.\n\nThe word \"transfection\" is a portmanteau of \"trans-\" and \"infection\". Genetic material (such as supercoiled plasmid DNA or siRNA constructs), or even proteins such as antibodies, may be transfected.\n\nTransfection of animal cells typically involves opening transient pores or \"holes\" in the cell membrane to allow the uptake of material. Transfection can be carried out using calcium phosphate (i.e. tricalcium phosphate), by electroporation, by cell squeezing or by mixing a cationic lipid with the material to produce liposomes that fuse with the cell membrane and deposit their cargo inside.\n\nTransfection can result in unexpected morphologies and abnormalities in target cells.\n\nThe meaning of the term has evolved. The original meaning of transfection was \"infection by transformation\", i.e., introduction of genetic material, DNA or RNA, from a prokaryote-infecting virus or bacteriophage into cells, resulting in an infection. Because the term transformation had another sense in animal cell biology (a genetic change allowing long-term propagation in culture, or acquisition of properties typical of cancer cells), the term transfection acquired, for animal cells, its present meaning of a change in cell properties caused by introduction of DNA.\n\nThere are various methods of introducing foreign DNA into a eukaryotic cell: some rely on physical treatment (electroporation, cell squeezing, nanoparticles, magnetofection); others rely on chemical materials or biological particles (viruses) that are used as carriers. Gene delivery is, for example, one of the steps necessary for gene therapy and the genetic modification of crops. There are many different methods of gene delivery developed for various types of cells and tissues, from bacterial to mammalian. Generally, the methods can be divided into two categories: non-viral and viral.\n\nNon-viral methods include physical methods such as electroporation, microinjection, gene gun, impalefection, hydrostatic pressure, continuous infusion, and sonication and chemical, such as lipofection, which is a lipid-mediated DNA-transfection process utilizing liposome vectors. It can also include the use of polymeric gene carriers (polyplexes).\n\nVirus mediated gene delivery utilizes the ability of a virus to inject its DNA inside a host cell. A gene that is intended for delivery is packaged into a replication-deficient viral particle. Viruses used to date include retrovirus, lentivirus, adenovirus, adeno-associated virus and herpes simplex virus. However, there are drawbacks to using viruses to deliver genes into cells. Viruses can only deliver very small pieces of DNA into the cells, it is labor-intensive and there are risks of random insertion sites, cytopathic effects and mutagenesis.\n\nChemical-based transfection can be divided into several kinds: cyclodextrin, polymers, liposomes, or nanoparticles (with or without chemical or viral functionalization. See below).\n\n\n\n\nOther methods of transfection include nucleofection, which has proved very efficient in transfection of the THP-1 cell line, creating a viable cell line that was able to be differentiated into mature macrophages, and heat shock.\n\nDNA can also be introduced into cells using viruses as a carrier. In such cases, the technique is called viral transduction, and the cells are said to be transduced. Adenoviral vectors can be useful for viral transfection methods because they can transfer genes into a wide variety of human cells and have high transfer rates. Lentiviral vectors are also helpful due to their ability to transduce cells not currently undergoing mitosis.\n\nStable and transient transfection differ in their long term effects on a cell; a stably-transfected cell will continuously express transfected DNA and pass it on to daughter cells, while a transiently-transfected cell will express transfected DNA for a short amount of time and not pass it on to daughter cells.\n\nFor some applications of transfection, it is sufficient if the transfected genetic material is only transiently expressed. Since the DNA introduced in the transfection process is usually not integrated into the nuclear genome, the foreign DNA will be diluted through mitosis or degraded. Cell lines expressing the Epstein–Barr virus (EBV) nuclear antigen 1 (EBNA1) or the SV40 large-T antigen, allow episomal amplification of plasmids containing the viral EBV (293E) or SV40 (293T) origins of replication, greatly reducing the rate of dilution.\n\nIf it is desired that the transfected gene actually remain in the genome of the cell and its daughter cells, a stable transfection must occur. To accomplish this, a marker gene is co-transfected, which gives the cell some selectable advantage, such as resistance towards a certain toxin. Some (very few) of the transfected cells will, by chance, have integrated the foreign genetic material into their genome. If the toxin is then added to the cell culture, only those few cells with the marker gene integrated into their genomes will be able to proliferate, while other cells will die. After applying this selective stress (selection pressure) for some time, only the cells with a stable transfection remain and can be cultivated further.\n\nCommon agents for selecting stable transfection are:\n\n\nRNA can also be transfected into cells to transiently express its coded protein, or to study RNA decay kinetics. RNA transfection is often used in primary cells that do not divide.\n\nsiRNAs can also be transfected to achieve RNA silencing (i.e. loss of RNA and protein from the targeted gene). This has become a major application in research to achieve \"knock-down\" of proteins of interests (e.g. Endothelin-1) with potential applications in gene therapy. Limitation of the silencing approach are the toxicity of the transfection for cells and potential \"off-target\" effects on the expression of other genes/proteins.\n\n\n\n"}
{"id": "38106941", "url": "https://en.wikipedia.org/wiki?curid=38106941", "title": "Woozle effect", "text": "Woozle effect\n\nThe Woozle effect, also known as evidence by citation, or a woozle, occurs when frequent citation of previous publications that lack evidence misleads individuals, groups, and the public into thinking or believing there is evidence, and nonfacts become urban myths and factoids.\n\nA Woozle is an imaginary character in the A. A. Milne book \"Winnie-the-Pooh\", published in 1926. In chapter three, \"In which Pooh and Piglet Go Hunting and Nearly Catch a Woozle\", Winnie-the-Pooh and Piglet start following tracks left in snow believing they are the tracks of an imaginary animal called a \"woozle\". The tracks keep multiplying until Christopher Robin explains to them that they have been following their own tracks in circles around a tree.\n\nPrior to the introduction of the specific term \"Woozle effect\", the underlying research phenomenon (and connection to the Woozle) dates back over 60 years. Bevan (1953), writing about scientific methodology and research errors in the field of psychology, uses the term \"scientific woozle hunters\". Wohlwill (1963) refers to a \"hunt for the woozle\" in social science research, and Stevens (1971) cautions readers about woozles in the study of a misquoted letter.\n\nAccording to Richard J. Gelles, the term \"woozle effect\" was coined by Beverly Houghton in 1979. Other researchers have attributed the term to Gelles (1980) and Gelles and Murray A. Straus (1988). Gelles and Straus argue that the woozle effect describes a pattern of bias seen within social sciences and which is identified as leading to multiple errors in individual and public perception, academia, policy making and government. A woozle is also a claim made about research which is not supported by original findings. According to Dutton, a woozle effect, or a woozle, occurs when frequent citation of previous publications that lack evidence misleads individuals, groups and the public into thinking or believing there is evidence, and non-facts become urban myths and factoids. The creation of woozles is often linked to the changing of language from qualified (\"it may\", \"it might\", \"it could\") to absolute form (\"it is\") firming up language and introducing ideas and views not held by an original author or supported by evidence.\n\nDutton sees the woozle effect as an example of confirmation bias and links it to belief perseverance and groupthink. Because in the social sciences empirical evidence may be based on experiential reports rather than objective measurements, there may be a tendency for researchers to align evidence with expectation. According to Dutton it is also possible that the social sciences may be likely to align with contemporary views and ideals of social justice, leading to bias in favor of those ideals. Gambrill (2012) links the woozle effect to the processes that create pseudoscience. Gambrill and Reiman (2011) also link it with more deliberate propaganda techniques; they also identify introductory phrases like \"Every one knows ...\", \"It is clear that ...\", \"It is obvious that ...\", \"It is generally agreed that ...\" as alarm bells that what follows might be a Woozle line of reasoning.\n\nIn 1979, Houghton illustrated the Woozle effect, showing how work by Gelles (1974) based on a small sample and published in \"The Violent Home\" by Straus, who had written the foreword for Gelles's book, was presented as if it applied to a large sample. Both of these were then cited by Langley & Levy in their 1977 book, \"Wife Beating: The Silent Crisis\". In the 1998 book \"Intimate Violence\", Gelles and Straus use the Winnie-the-Pooh woozle to illustrate how poor practice in research and self-referential research causes older research to be taken as fresh evidence causing error and bias.\n\nIn a study conducted by the Vera Institute of Justice, Weiner and Hala (2008) reported some of the research-related difficulties associated with measuring human trafficking. They describe and map the unfolding of the Woozle effect in connection with prevalence estimates of human trafficking. Searching the relevant literature between 1990 and 2006, Weiner and Hala found 114 prevalence estimates in 45 publications. Only one of the publications cited original research, and several prevalence estimates appeared unsourced. The authors concluded that the sources they reviewed lacked citations, adequate operational definition, and discussion of methodology. Stransky and Finkelhor (2008/2012) criticize the general methodology involved in human trafficking research. They cite the Woozle effect (p. 3) and post a prominent warning on the first page of their report cautioning against citing any specific estimates they present, as the close inspection of the figures \"...reveals that none are based on a strong scientific foundation. (p. 1)\"\n\nGambrill and Reiman (2011) analyze scientific papers and mass-market communications about social anxiety and conclude that many of them engage in disease mongering by presenting the disease model of social anxiety as an incontrovertible fact by resorting to unchallenged repetition techniques and by leaving out of the discourse any competing theories. Gambrill and Reiman further note that even after educating their subjects about the tell-tale signs of such techniques, many of them still failed to pick up the signs in a practical test.\nJames J. Kimble gives as an example the 1994–2015 historiography of the 1943 American \"We Can Do It!\" wartime poster. After Michigan resident Geraldine Hoff Doyle said in 1994 that she was the real-life model for the poster, many sources repeated her assertion without checking the two foundational assumptions: that Doyle was the young factory worker pictured in a 1942 wartime photograph, and that the photograph had inspired commercial artist J. Howard Miller to create the poster. Though some media representations described the connection as unconfirmed, many more enthusiastically endorsed it. The weight of these multiple endorsements gave Doyle's story a \"convincing\" authority, despite the lack of authority in establishing the connection. In 2015 Kimble found the original photographic print of the factory worker, its caption identifying the young woman as Naomi Parker, working in California in March 1942, when Doyle was still in high school.\n\n"}
