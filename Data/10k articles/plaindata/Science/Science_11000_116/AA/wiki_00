{"id": "3616159", "url": "https://en.wikipedia.org/wiki?curid=3616159", "title": "1992 cageless shark-diving expedition", "text": "1992 cageless shark-diving expedition\n\nThe 1992 cage-less shark-diving expedition was the world's first recorded intentionally cage-less dive with great white sharks, contributing to a change in public opinions about the supposed ferocity of these animals.\n\nThe dive took place in January 1992, during the filming of the National Geographic documentary \"Blue Wilderness\", at Dyer Island, South Africa. After 8-10 large Great White sharks had been kept around their boat for about 6 hours using chum and sea mammal flesh, four scuba divers carried out the world's first dive amongst these animals without a safety cage, or any other protection, like chain-mail suits. The divers were Ron & Valerie Taylor, notable Australian film-makers and pioneers of underwater exploration, their friend George Askew, a South African diver and photographer, and Piet 'PJ' van der Walt, who had founded the South African cage-diving industry in 1988. \nThe Taylors and Askew, recognised shark experts and authorities, were testing their hypothesis that these animals had a much fiercer reputation than they deserved. Their hypothesis was based on many years of experiences with various types of shark, including face to face encounters underwater. In 1978, Askew had written an article entitled \"The Jaws fish - Myth or Maneater?\", published in the UK magazine \"Underwater World\", proposing that Great Whites did not deserve the horrific image and reputation that \"Jaws\" author Peter Benchley and film director Steven Spielberg had imprinted in peoples minds. Askew postulated that, as they rely on stealth and surprise when attacking, Great Whites would be unlikely to attack if you were aware of their presence. He had two more articles on the same subject published in 1983 and 1991, and then went on to prove that point with the historic dive in 1992.\n\nWhilst surface testing of the prototype \"Shark POD\" Protective Oceanic Device (now Shark Shield) for the Natal Sharks Board, the divers discovered that despite having been excited for hours previously by large amounts of blood-laden chum (mashed fish, blood and oil) and chunks of dolphin and whale meat from washed up carcasses, the sharks were actually very shy and difficult to approach, even scared of these unknown intruders. After a long 20 minute wait, the divers had several timid encounters with the very cautious sharks and were never at any time challenged, nor made to feel uneasy. This ground-breaking \"Underwater Everest\" conquest, a huge leap forward in ocean exploration, strongly challenged the idea of the Great White as a \"Mindless Monster\" eating machine, and changed the way the world viewed sharks.\n\nThe Taylors felt that the Australian sharks may have a slightly different disposition to South African ones, but as it is now known that Great Whites swim between South Africa and Australia, this is open to debate. On two occasions many years before, they had released Great Whites trapped in wire ropes from cages without being harassed, despite touching the animals.\n\nAskew had encountered Great White Sharks several times previously over the years whilst spear-fishing. The first was in 1960, when meeting one was considered to mean certain death. This encounter was with a very gravid female who had come into a small cove to drop her pup/s. She was in such an advanced stage of pregnancy that her body was distorted, with her mouth actually facing forward above her hugely distended stomach. She was what is referred to as a \"Drop-Gut\". In the animal world a mother is usually very protective and aggressive just before and just after giving birth, and yet this large Apex Predator showed no aggression towards him. Because of this and similar encounters, and those of his colleagues, he became more interested in this question.\n\nJust before the dive, Askew and Ron Taylor were kneeling on the dive platform a few centimetres above water, with their hands in the water filming. Askew stood up and stepped back, and at that moment a four-metre Great White slid onto the platform and stopped 3 inches from his foot before sliding back, but made no attempt to snap or lunge at him. It would have taken his camera and arms, and maybe pulled him in if he had not got up. Askew sees that incident as pure opportunism and not savagery.\n\nThe Prototype 'Pod\" Valerie is seen wearing during this dive was a dummy for continuity and afforded the divers no protection.\n\nThat first close encounter dive demonstrated that Great Whites are not built only to devour people but are very curious and can be quite 'friendly'. This dive is directly responsible for the upsurge in Shark Tourism – especially free-diving (i.e. Out of cage swimming) with big sharks. When existing and potential operators around the world learnt of the theory that the Great White was quite approachable and not likely to attack, it was hypothesised that the same applied to other dangerous sharks such as tiger sharks, bull sharks and oceanic whitetip sharks. This proved to be the case and shark tourism began to expand rapidly. It is now a multibillion-dollar a year industry, and has provided a lot of useful insights into sharks.\n\nSince this dive some divers have attempted cage-less dives with big sharks, even hitching rides on their dorsal fins and touching them underwater. However, such attempts are not recommended as sharks are still Apex Predators and very opportunistic. Although there have never been any serious incidents from free-swimming with Great Whites, the same cannot be said for other sharks. There have been a number of fatalities and other injuries.\n\n\n"}
{"id": "11371107", "url": "https://en.wikipedia.org/wiki?curid=11371107", "title": "Allen Kerr", "text": "Allen Kerr\n\nAllen Kerr AO, FRS, FAA (born 1926) was a Scottish-born Professor of Plant Pathology at the University of Adelaide. His most significant work was his study of crown gall - a plant cancer induced by Agrobacterium tumerfaciens.\n\nHe was born on 21 May 1926 in Edinburgh and gained a BSc degree at the University of Edinburgh.\n\nFrom 1947-1951 he was Assistant Mycologist at the North of Scotland College of Agriculture.<br> \nFrom 1951-1980 he was Lecturer, then Senior Lecturer, then Reader in Plant Pathology at the University of Adelaide.<br>\nFrom 1978-1983 he was Vice-President of the International Society for Plant Pathology.<br>\nFrom 1980-1983 he was President of the Australasian Plant Pathology Society.\n\nIn 1978 he was elected Fellow of the Australian Academy of Science.<br>\nIn 1986 he was elected Fellow of the Royal Society.<br>\nIn 1990 he received the inaugural Australia Prize for his work with plant genetics and biology.\n\nIn 1990 he became Head of the Department of Plant Pathology at the University of Adelaide, and\nin 1991 he became Head of the Department of Crop Protection at the University of Adelaide.\n\n\n"}
{"id": "22372553", "url": "https://en.wikipedia.org/wiki?curid=22372553", "title": "Association of North East Councils", "text": "Association of North East Councils\n\nThe Association of North East Councils was a partnership body made up of representatives of local authorities in North East England. It was a regional grouping of the Local Government Association.\n\nIn April 2009 it assumed the role of the regional Local Authority Leaders’ Board following the abolition of the North East Assembly.\n\nIt closed in March 2016, following the creation of a North East Combined Authority, which comprises local authorities outside of Tees Valley; with Tees Valley local authorities forming the separate Tees Valley Combined Authority.\n\n"}
{"id": "54661104", "url": "https://en.wikipedia.org/wiki?curid=54661104", "title": "Babel program", "text": "Babel program\n\nThe IARPA Babel program developed speech recognition technology for noisy telephone conversations. The main goal of the program was to improve the performance of keyword search on languages with very little transcribed data, i.e. low-resource languages. Data from 26 languages was collected with certain languages being held-out as \"surprise\" languages to test the ability of the teams to rapidly build a system for a new language.\n\nTwo industry-led teams (IBM and BBN) and two university-led teams (ICSI led by Nelson Morgan and CMU) participated.\n\nSome of the funding from Babel was used to further develop the Kaldi tookit. The speech data was later made available through the Linguistic Data Consortium.\n"}
{"id": "25177212", "url": "https://en.wikipedia.org/wiki?curid=25177212", "title": "Bacteriophage T5", "text": "Bacteriophage T5\n\nBacteriophage T5 is a caudal virus within the family \"Siphoviridae\". This bacteriophage specifically infects \"E. coli\" bacterial cells and follows a lytic life cycle.\n\nThe T5 structure includes a 90 nanometer icosahedral capsid and a 250 nanometer-long flexible, non-contractile tail. The capsid contains the phage's 121750 base pair, double-stranded DNA genome.\n\nBacteriophage T5 has been shown to infect \"E. coli\" after its receptor binding protein, pb5, binds to the host cell's outer membrane ferrichrome transporter, FhuA. The binding triggers structural changes in pb5 and eventually leads to DNA release from the phage capsid.\n\n"}
{"id": "20288128", "url": "https://en.wikipedia.org/wiki?curid=20288128", "title": "Bowden development", "text": "Bowden development\n\nThe Bowden development is an urban development in the Australian state of South Australia on a site formerly owned by the Clipsal corporation in the suburb of Bowden, within the City of Charles Sturt, in the Adelaide metropolitan area 2.5 kilometres from the city centre. \n\nThe site covers an area of and is bounded by Park Terrace to the south, the Outer Harbor railway line to the west, Drayton Street to the north and Sixth and Seventh Streets to the east. The Government also acquired the adjoining 5.9-hectare site which had been owned by Origin Energy (known as the Brompton Gasworks site). Currently the overall Bowden development site is 16.3 hectares.\n\nThe development is expected to be completed by 2020 to 2025. The development is expected to increase the population of Bowden, which was 648 in the 2006 census, to 3,500.\n\nThe Bowden development is leading the way with many groundbreaking initiatives including energy reduction through passive and active measures for buildings. Green Star is a world's best practice rating tool from the Green Building Council of Australia covering a broad range of well-tested sustainability issues. Each and every building delivered in the development must achieve a minimum 5 Star Green Star rating including commercial buildings such as Plant 4 Bowden.\n\nIn 2016, the Bowden development raised the bar and achieved its 6 Star Green Star - Communities accreditation; and with the highest concentration of Green Star homes in Australia, it is one of the most environmentally sustainable communities in the nation. Later in the same year, the Prince's Terraces Adelaide in Bowden became the first residential project to receive 6 Star Green Star design rating from the Green Building Council of Australia, encapsulating innovation and world leadership in sustainable design.\n\nThe Bowden site was occupied by Clipsal, a company manufacturing conduit and electrical accessories, in 1936. The opportunity for an urban development on the site grew out of the South Australian Government's plans for eleven transport-oriented developments in the Adelaide metropolitan area, combined with Clipsal's decision that the Bowden site is surplus to company requirements and plan to vacate. The site was originally offered for sale in early 2008 with offers to be received by July 2008. Offers in the realm of 75 to 80 million Australian dollars were expected however evidently not attained; as the 2008 economic crisis accelerated and falling property prices diminished the likelihood of a sale at expected prices, the South Australian government intervened and announced in October 2008 that it would purchase the site. It was revealed in November 2008 the government had agreed to pay A$52.5 million.\n\nIn late 2008, the State Government acquired a 10.25ha parcel of land owned by Gerard Industries (known as the Clipsal site). In early 2010, the government acquired the adjoining 5.9ha site owned by Origin Energy (known as the Brompton Gasworks site).\n\nBoth sites are located within the City of Charles Sturt and are directly adjacent to the Adelaide parklands. The aim is to transform the combined sites into an inner-city, higher intensity, mixed use urban village. The final vision developed collaboratively with the community and stakeholders states: “Bowden Urban Village is a creative and diverse community, living and working in a high density sustainable urban environment. Its character, parklands connections and integrated urban design will offer a new and distinctive place in Adelaide for residents and visitors.”\n\nThe South Australian Government anticipates that the site will be used to develop more than 2,400 residential apartments and terrace homes, in addition to retail outlets and commercial offices around a town centre. The South Australian Government is promoting the plan as an economically, socially and environmentally sustainable development. Bowden is served by Bowden railway station, North Adelaide railway station, buses on Hawker Street and Port Road, and the tram from the nearby Adelaide Entertainment Centre on Port Road, Hindmarsh.\n\nIn April 2011, the plans for the development were approved and an information and sales centre opened in March 2012.\n\nFollowing demolition, site cleanup, surveying, planning and approval, remediation and development are underway. Bowden’s first pedestrian and bicycle-friendly streets — Fifth, Sixth, Seventh and part of Gibson Streets — opened to the public in May 2013.\n\nIn 2014, the restaurant Jarmer's Kitchen opened, and the Bowden development's first residents moved in.\n\nIn 2015, His Royal Highness Prince Charles, Prince of Wales officially launched the construction of Australia's first 6 Star Green Star residential project, the Prince's Terraces Adelaide which is predicted to use 50% less energy and 50% less potable water than a typical urban townhouse, with a carbon footprint also reduced by more than 40% when compared to a standard house.\n\nThe community celebrated the completion of a major upgrade to the Adelaide Park Lands across the road from Bowden in February 2016 where the Park Terrace Community Garden currently resides.\n\nThe Bowden development achieved its sustainability milestone with a 6 Star Green Star - Communities rating in October 2016, and hosted its biggest celebration ever to mark the opening of Bowden Town Square consisting Bowden Park, the Plant 4 Bowden retail hub and Plant 3 community hub.\n\nIn early 2017, 500 new homes were sold.\n\n"}
{"id": "51863718", "url": "https://en.wikipedia.org/wiki?curid=51863718", "title": "Bregman Lagrangian", "text": "Bregman Lagrangian\n\nThe Bregman-Lagrangian framework permits a systematic understanding of the matching rates associated with higher-order gradient methods in discrete and continuous time.\n"}
{"id": "452779", "url": "https://en.wikipedia.org/wiki?curid=452779", "title": "Brilliant Light Power", "text": "Brilliant Light Power\n\nBrilliant Light Power, Inc. (BLP), formerly BlackLight Power, Inc. of Cranbury, New Jersey is a company founded by Randell L. Mills, who claims to have discovered a new energy source. The purported energy source is based on Mills' assertion that the electron in a hydrogen atom can drop below the lowest energy state known as the ground state. Mills calls these hypothetical hydrogen atoms that are in an energy state below ground level, \"hydrinos\". Mills self-published a closely related book, \"The Grand Unified Theory of Classical Physics\" and has co-authored articles on claimed hydrino-related phenomena.\n\nCritics say it lacks corroborating scientific evidence, and is a relic of cold fusion. Critical analysis of the claims have been published in the peer reviewed journals \"Physics Letters A\", \"New Journal of Physics\", \"Journal of Applied Physics\", and \"\" on the basis that Quantum Mechanics is valid, and that the proposed hydrino states are unphysical and incompatible with key equations of Quantum Mechanics.\n\nIn 2009, \"IEEE Spectrum\" magazine characterized it as a \"loser\" technology because \"most experts don't believe such lower states exist, and they say the experiments don't present convincing evidence\" and mentioned that Wolfgang Ketterle had said the claims are \"nonsense\". BLP has announced several times that it was about to deliver commercial products based on Mill's theories but has not delivered a working product.\n\nThe company, originally called HydroCatalysis Inc. was founded in 1991 by Randell Mills who claimed to have discovered a power source that \"represents a boundless form of new primary energy\" and that will \"replace all forms of fuel in the world,\" On April 25, 1991 at a press conference in Lancaster, Pennsylvania, Mills first announced his hydrino state hypothesis which rejects the idea that \"cold fusion\" was occurring in studies surrounding the Fleischmann–Pons experiment. According to Mills, no fusion was actually happening in the cells, and all the effects would be caused by shrinkage of hydrogen atoms as they fell to a state below the ground state. Experimental evidence offered by Mills was in contradiction to known chemistry and was dismissed by the scientific community.\n\nBy December 1999, BLP raised more than $25 million from about 150 investors. By January 2006, BLP funding exceeded $60 million.\n\nAmong the investors are PacifiCorp, Conectiv, retired executives from Morgan Stanley and several BLP board members like Shelby Brewer who was the top nuclear official for the Reagan Administration and Chief Executive Officer of ABB-Combustion Engineering Nuclear Power and former board member Michael H. Jordan (1936 – 2010), who was Chief Executive Officer of PepsiCo Worldwide Foods, Westinghouse Electric Corporation, CBS Corporation and Electronic Data Systems.\n\nIn 2008, Mills said that his cell stacks could provide power for long-range electric vehicles, and that this electricity would cost less than 2 cents per kilowatt-hour.\n\nIn December 2013, BLP was one of 54 applicants to receive ~$1.1M in grant funding from the New Jersey Economic Development Authority.\n\nIn 1996, NASA released a report describing experiments using a BLP electrolytic cell. Although not recreating the large heat gains reported for the cell by BLP, unexplained power gains ranging from 1.06 to 1.68 of the input power were reported, which, whilst \"...admit[ing] the existence of an unusual source of heat with the cell...falls far short of being compelling\". The authors went on to propose the recombination of hydrogen and oxygen as a possible explanation of the anomalous results.\n\nAround 2002, the NASA Institute for Advanced Concepts (NIAC) granted a Phase I grant to Anthony Marchese, a mechanical engineer at Rowan University, to study a possible rocket propulsion that would use hydrinos.\n\nIn 2002, Rowan University's Anthony Marchese said that whilst \"agnostic about the existence of hydrinos\", he was quite confident that there was no fraud involved with BLP. Although his NIAC grant was criticised by Bob Park, Marchese said \"for me to not continue with this study would be unethical to the scientific community. The only reason not to pursue this would be because of being afraid of being bullied.\"\n\nIn 1999, the Nobel prize winning physicist Philip Warren Anderson said he is \"sure that it's a fraud\", and in the same year another Nobel prize winning physicist, Steven Chu, called it \"extremely unlikely\". The following year, a 2000 patent based on its hydrino-related technology was later withdrawn by the United States Patent and Trademark Office (USPTO) due to contradictions with known physics laws and other concerns about the viability of the described processes, citing Park and others.\n\nA hydrino laser patent has not been withdrawn by the USPTO.\n\nAn April 2000 editorial column by Robert L. Park and an outside query by an unknown person prompted Group Director Esther Kepplinger of the USPTO to review this new patent herself. Kepplinger said that her \"main concern was the proposition that the applicant was claiming the electron going to a lower orbital in a fashion that I knew was contrary to the known laws of physics and chemistry\", and that the patent appeared to involve cold fusion and perpetual motion. Kepplinger contacted another Director, Robert Spar, who also expressed doubts on the patentability of the patent application. This caused the USPTO to withdraw from issue the patent application before it was granted and re-open it for review, and to withdraw four related applications, including one for a hydrino power plant.\n\nIn 2000, a law firm engaged by BLP sent letters to four prominent physicists asking them to stop making what it called \"defamatory comments\". The physicists had been quoted in the \"Village Voice\", \"Dow Jones Newswire\" and other publications as dismissing BLP's claims on the basis that they violated the laws of physics. In response, one of the physicists, Robert L. Park of the American Physical Society, said that if BLP sued, he was confident the scientific community would lend its support and that the court would side with the physicists. Park later wrote that a number of the recipients of the letter, who had \"responded honestly to questions from the media\", had since fallen silent. Scientists, Park wrote, are easy to intimidate since they are not rich enough to risk costly legal actions.\n\nIn May 2000, BLP filed suit in the US District Court of Columbia, saying that withdrawal of the application after the company had paid the fee was contrary to law. In 2002, the District Court concluded that the USPTO was acting inside the limits of its authority in withdrawing a patent over whose validity it had doubts, and later that year, the United States Court of Appeals for the Federal Circuit ratified this decision. Applications were rejected by the UK patent office for similar reasons. The European Patent Office (EPO) rejected a similar BLP patent application due to lack of clarity on how the process worked. Reexamination of this European patent is pending.\n\nRobert L. Park, emeritus professor of physics at the University of Maryland and a notable skeptic, has been particularly critical of BLP since 1991. By 2000, Park remained skeptical, stating:\n\"Unlike most schemes for free energy, the hydrino process of Randy Mills is not without ample theory. Mills has written a 1000 page tome, entitled, \"The Grand Unified Theory of Classical Quantum Mechanics\", that takes the reader all the way from hydrinos to antigravity. Fortunately, Aaron Barth [...] has taken upon himself to look through it, checking for accuracy. Barth is a post doctoral researcher at the Harvard–Smithsonian Center for Astrophysics, and holds a PhD in Astronomy, 1998, from UC Berkeley. What he found initially were mathematical blunders and unjustified assumptions. To his surprise, however, portions of the book seemed well organized. These, it now turns out, were lifted verbatim from various texts. This has been the object of a great deal of discussion from Mills' Hydrino Study Group. \"Mills seems not to understand what the fuss is all about.\" – Park\n\nBy 2008, Park continued to express his skepticism:\n\"BlackLight Power (BLP), founded 17 years ago as HydroCatalysis, announced last week that the company had successfully tested a prototype power system that would generate 50 KW of thermal power. BLP anticipates delivery of the new power system in 12 to 18 months. The BLP process, discovered by Randy Mills, is said to coax hydrogen atoms into a \"state below the ground state\", called the \"hydrino.\" There is no independent scientific confirmation of the hydrino, and BLP has a patent problem. So they have nothing to sell but bull shit. The company is therefore dependent on investors with deep pockets and shallow brains.\" – Park In 2008, Robert L. Park wrote that BLP has benefited from wealthy investors who allocate a proportion of their funds to risky ventures with a potentially huge upside, but that in the case of BLP since the science underlying the offering was \"just wrong\" investment risk was, in Park's view, \"infinite\".\n\nVarious scientists also voiced their opinions as far back as the 1990s. Steven Chu, Nobel Laureate in Physics in 1997, said \"it's extremely unlikely that this is real, and I feel sorry for the funders, the people who are backing this\". In 1999, Princeton University's physics Nobel laureate Phillip Anderson said of it, \"If you could fuck around with the hydrogen atom, you could fuck around with the energy process in the sun. You could fuck around with life itself.\" \"Everything we know about everything would be a bunch of nonsense. That's why I'm so sure that it's a fraud.\" Wolfgang Ketterle, a professor of physics at MIT, said BLP's claims are \"nonsense\" and that \"there is no state of hydrogen lower than the ground state\". Michio Kaku, a theoretical physicist based at City University of New York, adds that \"the only law that this business with Mills is proving is that a fool and his money are easily parted.\" and that \"There's a sucker born every minute.\" While Peter Zimmerman was chief arms-control scientist at the State Department, he stated that his department and the Patent Office \"have fought back with success\" against \"pseudoscientists\" and he railed against, among other things, the inventors of \"hydrinos.\" In 2009, the editors of \"IEEE Spectrum\" magazine characterized it as a \"loser\" technology because \"[m]ost experts don't believe such lower states exist, and they say the experiments don't present convincing evidence\" and mentioned that Wolfgang Ketterle had said the claims are \"nonsense\". BLP has announced several times that it was about to deliver commercial products based on Mill's theories but has not delivered a working product.\n\nIn the 2000s, several reviewed articles were published criticizing Hydrino theory for being incompatible with Quantum Mechanics.\n\nFor example, in 2005, Andreas Rathke of the European Space Agency, publishing in the \"New Journal of Physics\", wrote that Mills' description of quantum mechanics is \"inconsistent and has several serious deficiencies\", and that there is \"no theoretical support of the hydrino hypothesis\". Rathke said it would be helpful if Mills' experimental results could be independently replicated, and suggested that any evidence produced should be reconsidered in the context of a conventional physical explanation. One inconsistency of Mills' CQM with quantum mechanics regards its inability to be reconciled with the probability density function in quantum mechanics. Rathke stated, \"However, while solutions of the Schrödinger equation with n<1 indeed exist, they are not square integrable. This violates not only an axiom of quantum mechanics, but in practical terms prohibits that these solutions can in any way describe the probability density of a particle.\" In the same year, the \"Journal of Applied Physics\" published a critique by A.V. Phelps of the 2004 article, \"Water bath calorimetric study of excess heat generation in resonant transfer plasmas\" by J. Phillips, R. Mills and X. Chen. Phelps criticized both the calorimetric techniques and the underlying theory described in the Phillips/Mills/Chen article. The journal also published a response to Phelps' critique on the same day. In 2005 Šišović and others published a paper describing experimental data and analysis of Mills' claim that a resonant transfer model (RTM) explains the excessive Doppler broadening of the Hα line. Šišović concluded that: \"The detected large excessive broadening in pure hydrogen and in Ne–H2 mixture is in agreement with CM [Collision Model] and other experimental results\" and that \"these results can't be explained by RTM\". The collision model explanation for excessive broadening of the Hα line is based on established physics.\n\nIn 2006, a paper published in \"Physics Letters A\", concluded that Mills' theoretical hydrino states are unphysical. For the hydrino states, the binding strength increases as the strength of the electric potential decreases, with maximum binding strength when the potential has disappeared completely. The author Norman Dombey remarked \"We could call these anomalous states \"homeopathic\" states because the smaller the coupling, the larger the effect.\" The model also assumes that the nuclear charge distribution is a point rather than having an arbitrarily small non-zero radius. It also lacks an analogous solution in the Schrödinger equation, which governs non-relativistic systems. Dombey concluded: \"We suggest that outside of science fiction this is sufficient reason to disregard them.\" From a suggestion in Dombey's paper, further work by Antonio Di Castro has shown that states below the ground state, as described in Mills' work, are incompatible with the Schrödinger, Klein–Gordon and Dirac equations, key equations in the study of quantum systems.\n\nIn a 2007 review of cold fusion research, researcher Edmund Storms put forward the hydrino model as a possible explanation for cold fusion.\n\nIn 2008, the \"Journal of Physics D: Applied Physics\" published an article by Hans-Joachim Kunze, professor emeritus at the Institute for Experimental Physics, Ruhr University Bochum, critical of the 2003 paper authored by R. Mills and P. Ray, Extreme ultraviolet spectroscopy of helium–hydrogen. The abstract of the article is: \"It is suggested that spectral lines, on which the fiction of fractional principal quantum numbers in the hydrogen atom is based, are nothing else but artefacts.\" Kunze stated that it was impossible to detect the novel lines below 30 nm reported by Mills and Ray because the equipment they used did not have the capability to detect them as per the manufacturer and as per \"every book on vacuum-UV spectroscopy\" and \"therefore the observed lines must be artefacts\". Kunze also stated that: \"The enormous spectral widths of the novel lines point to artefacts, too.\"\n\n\n"}
{"id": "37153076", "url": "https://en.wikipedia.org/wiki?curid=37153076", "title": "Bristol/Bath to South Coast Study", "text": "Bristol/Bath to South Coast Study\n\nThe Bristol/Bath to South Coast Study is a transportation study initiated by the United Kingdom's Government Office for the South West and Bath and North East Somerset Council in southwest England. It was undertaken by WSP Group as a result of the de-trunking in 1999 of the A36/A46 trunk road network from Bath to Southampton. The final study reports were published in 2004.\n\nThe study showed that an A36-A46 link road (to the east of Bath) would significantly reduce HGV traffic in Bath. It was also shown to reduce traffic volumes in Trowbridge and Bradford on Avon. The proposed link road was costed at £40M.\n\n"}
{"id": "1761305", "url": "https://en.wikipedia.org/wiki?curid=1761305", "title": "Cognatic kinship", "text": "Cognatic kinship\n\nCognatic kinship is a mode of descent calculated from an ancestor or ancestress counted through any combination of male and female links, or a system of bilateral kinship where relations are traced through both a father and mother. Such relatives may be known as cognates.\n"}
{"id": "39833800", "url": "https://en.wikipedia.org/wiki?curid=39833800", "title": "Darcy number", "text": "Darcy number\n\nIn fluid dynamics through porous media, the Darcy number (Da) represents the relative effect of the permeability of the medium versus its cross-sectional area—commonly the diameter squared. The number is named after Henry Darcy and is found from nondimensionalizing the differential form of Darcy's Law. This number should not be confused with the Darcy friction factor which applies to pressure drop in a pipe. It is defined as\n\nwhere \n\nAlternative forms of this number do exist depending on the approach by which Darcy's Law is made dimensionless and the geometry of the system. The Darcy number is commonly used in heat transfer through porous media.\n\n"}
{"id": "8411", "url": "https://en.wikipedia.org/wiki?curid=8411", "title": "Darwinism", "text": "Darwinism\n\nDarwinism is a theory of biological evolution developed by the English naturalist Charles Darwin (1809–1882) and others, stating that all species of organisms arise and develop through the natural selection of small, inherited variations that increase the individual's ability to compete, survive, and reproduce. Also called Darwinian theory, it originally included the broad concepts of transmutation of species or of evolution which gained general scientific acceptance after Darwin published \"On the Origin of Species\" in 1859, including concepts which predated Darwin's theories. It subsequently referred to the specific concepts of natural selection, the Weismann barrier, or the central dogma of molecular biology. Though the term usually refers strictly to biological evolution, creationists have appropriated it to refer to the origin of life, and it has even been applied to concepts of cosmic evolution, both of which have no connection to Darwin's work. It is therefore considered the belief and acceptance of Darwin's and of his predecessors' work—in place of other theories, including divine design and extraterrestrial origins.\n\nEnglish biologist Thomas Henry Huxley coined the term \"Darwinism\" in April 1860. It was used to describe evolutionary concepts in general, including earlier concepts published by English philosopher Herbert Spencer. Many of the proponents of Darwinism at that time, including Huxley, had reservations about the significance of natural selection, and Darwin himself gave credence to what was later called Lamarckism. The strict neo-Darwinism of German evolutionary biologist August Weismann gained few supporters in the late 19th century. During the approximate period of the 1880s to about 1920, sometimes called \"the eclipse of Darwinism\", scientists proposed various alternative evolutionary mechanisms which eventually proved untenable. The development of the modern synthesis in the early 20th century, incorporating natural selection with population genetics and Mendelian genetics, revived Darwinism in an updated form.\n\nWhile the term \"Darwinism\" has remained in use amongst the public when referring to modern evolutionary theory, it has increasingly been argued by science writers such as Olivia Judson and Eugenie Scott that it is an inappropriate term for modern evolutionary theory. For example, Darwin was unfamiliar with the work of the Moravian scientist and Augustinian friar Gregor Mendel, and as a result had only a vague and inaccurate understanding of heredity. He naturally had no inkling of later theoretical developments and, like Mendel himself, knew nothing of genetic drift, for example. In the United States, creationists often use the term \"Darwinism\" as a pejorative term in reference to beliefs such as scientific materialism, but in the United Kingdom the term has no negative connotations, being freely used as a shorthand for the body of theory dealing with evolution, and in particular, with evolution by natural selection.\n\nWhile the term \"Darwinism\" had been used previously to refer to the work of Erasmus Darwin in the late 18th century, the term as understood today was introduced when Charles Darwin's 1859 book \"On the Origin of Species\" was reviewed by Thomas Henry Huxley in the April 1860 issue of the \"Westminster Review\". Having hailed the book as \"a veritable Whitworth gun in the armoury of liberalism\" promoting scientific naturalism over theology, and praising the usefulness of Darwin's ideas while expressing professional reservations about Darwin's gradualism and doubting if it could be proved that natural selection could form new species, Huxley compared Darwin's achievement to that of Nicolaus Copernicus in explaining planetary motion:\nThese are the basic tenets of evolution by natural selection as defined by Darwin:\n\n\nAnother important evolutionary theorist of the same period was the Russian geographer and prominent anarchist Peter Kropotkin who, in his book \"\" (1902), advocated a conception of Darwinism counter to that of Huxley. His conception was centred around what he saw as the widespread use of co-operation as a survival mechanism in human societies and animals. He used biological and sociological arguments in an attempt to show that the main factor in facilitating evolution is cooperation between individuals in free-associated societies and groups. This was in order to counteract the conception of fierce competition as the core of evolution, which provided a rationalization for the dominant political, economic and social theories of the time; and the prevalent interpretations of Darwinism, such as those by Huxley, who is targeted as an opponent by Kropotkin. Kropotkin's conception of Darwinism could be summed up by the following quote:\n\n\"Darwinism\" soon came to stand for an entire range of evolutionary (and often revolutionary) philosophies about both biology and society. One of the more prominent approaches, summed in the 1864 phrase \"survival of the fittest\" by Herbert Spencer, later became emblematic of Darwinism even though Spencer's own understanding of evolution (as expressed in 1857) was more similar to that of Jean-Baptiste Lamarck than to that of Darwin, and predated the publication of Darwin's theory in 1859. What is now called \"Social Darwinism\" was, in its day, synonymous with \"Darwinism\"—the application of Darwinian principles of \"struggle\" to society, usually in support of anti-philanthropic political agenda. Another interpretation, one notably favoured by Darwin's half-cousin Francis Galton, was that \"Darwinism\" implied that because natural selection was apparently no longer working on \"civilized\" people, it was possible for \"inferior\" strains of people (who would normally be filtered out of the gene pool) to overwhelm the \"superior\" strains, and voluntary corrective measures would be desirable—the foundation of eugenics.\nIn Darwin's day there was no rigid definition of the term \"Darwinism\", and it was used by opponents and proponents of Darwin's biological theory alike to mean whatever they wanted it to in a larger context. The ideas had international influence, and Ernst Haeckel developed what was known as \"Darwinismus\" in Germany, although, like Spencer's \"evolution\", Haeckel's \"Darwinism\" had only a rough resemblance to the theory of Charles Darwin, and was not centered on natural selection. In 1886, Alfred Russel Wallace went on a lecture tour across the United States, starting in New York and going via Boston, Washington, Kansas, Iowa and Nebraska to California, lecturing on what he called \"Darwinism\" without any problems.\n\nIn his book \"Darwinism\" (1889), Wallace had used the term \"pure-Darwinism\" which proposed a \"greater efficacy\" for natural selection. George Romanes dubbed this view as \"Wallaceism\", noting that in contrast to Darwin, this position was advocating a \"pure theory of natural selection to the exclusion of any supplementary theory.\" Taking influence from Darwin, Romanes was a proponent of both natural selection and the inheritance of acquired characteristics. The latter was denied by Wallace who was a strict selectionist. Romanes' definition of Darwinism conformed directly with Darwin's views and was contrasted with Wallace's definition of the term.\n\nThe term \"Darwinism\" is often used in the United States by promoters of creationism, notably by leading members of the intelligent design movement, as an epithet to attack evolution as though it were an ideology (an \"ism\") of philosophical naturalism, or atheism. For example, UC Berkeley law professor and author Phillip E. Johnson makes this accusation of atheism with reference to Charles Hodge's book \"What Is Darwinism?\" (1874). However, unlike Johnson, Hodge confined the term to exclude those like American botanist Asa Gray who combined Christian faith with support for Darwin's natural selection theory, before answering the question posed in the book's title by concluding: \"It is Atheism.\" Creationists use the term \"Darwinism\", often pejoratively, to imply that the theory has been held as true only by Darwin and a core group of his followers, whom they cast as dogmatic and inflexible in their belief. In the 2008 documentary film \"\", which promotes intelligent design (ID), American writer and actor Ben Stein refers to scientists as Darwinists. Reviewing the film for \"Scientific American\", John Rennie says \"The term is a curious throwback, because in modern biology almost no one relies solely on Darwin's original ideas... Yet the choice of terminology isn't random: Ben Stein wants you to stop thinking of evolution as an actual science supported by verifiable facts and logical arguments and to start thinking of it as a dogmatic, atheistic ideology akin to Marxism.\" \n\nHowever, \"Darwinism\" is also used neutrally within the scientific community to distinguish the modern evolutionary synthesis, sometimes called \"neo-Darwinism\", from those first proposed by Darwin. \"Darwinism\" also is used neutrally by historians to differentiate his theory from other evolutionary theories current around the same period. For example, \"Darwinism\" may be used to refer to Darwin's proposed mechanism of natural selection, in comparison to more recent mechanisms such as genetic drift and gene flow. It may also refer specifically to the role of Charles Darwin as opposed to others in the history of evolutionary thought—particularly contrasting Darwin's results with those of earlier theories such as Lamarckism or later ones such as the modern evolutionary synthesis.\n\nIn political discussions in the United States, the term is mostly used by its enemies. \"It's a rhetorical device to make evolution seem like a kind of faith, like 'Maoism,'\" says Harvard University biologist E. O. Wilson. He adds, \"Scientists don't call it 'Darwinism'.\" In the United Kingdom the term often retains its positive sense as a reference to natural selection, and for example British ethologist and evolutionary biologist Richard Dawkins wrote in his collection of essays \"A Devil's Chaplain\", published in 2003, that as a scientist he is a Darwinist.\n\nIn his 1995 book \"Darwinian Fairytales\", Australian philosopher David Stove used the term \"Darwinism\" in a different sense than the above examples. Describing himself as non-religious and as accepting the concept of natural selection as a well-established fact, Stove nonetheless attacked what he described as flawed concepts proposed by some \"Ultra-Darwinists.\" Stove alleged that by using weak or false \"ad hoc\" reasoning, these Ultra-Darwinists used evolutionary concepts to offer explanations that were not valid (e.g., Stove suggested that sociobiological explanation of altruism as an evolutionary feature was presented in such a way that the argument was effectively immune to any criticism). Philosopher Simon Blackburn wrote a rejoinder to Stove, though a subsequent essay by Stove's protegee James Franklin's suggested that Blackburn's response actually \"confirms Stove's central thesis that Darwinism can 'explain' anything.\"\n\n"}
{"id": "6905518", "url": "https://en.wikipedia.org/wiki?curid=6905518", "title": "Electron beam-induced current", "text": "Electron beam-induced current\n\nElectron-beam-induced current (EBIC) is a semiconductor analysis technique performed in a scanning electron microscope (SEM) or scanning transmission electron microscope (STEM). It is used to identify buried junctions or defects in semiconductors, or to examine minority carrier properties. EBIC is similar to cathodoluminescence in that it depends on the creation of electron–hole pairs in the semiconductor sample by the microscope's electron beam. This technique is used in semiconductor failure analysis and solid-state physics.\nIf the semiconductor sample contains an internal electric field, as will be present in the depletion region at a p-n junction or Schottky junction, the electron–hole pairs will be separated by drift due to the electric field. If the p- and n-sides (or semiconductor and Schottky contact, in the case of a Schottky device) are connected through a picoammeter, a current will flow.\n\nEBIC is best understood by analogy: in a solar cell, photons of light fall on the entire cell, thus delivering energy and creating electron hole pairs, and cause a current to flow. In EBIC, energetic electrons take the role of the photons, causing the EBIC current to flow. However, because the electron beam of an SEM or STEM is very small, it is scanned across the sample and variations in the induced EBIC are used to map the electronic activity of the sample.\n\nBy using the signal from the picoammeter as the imaging signal, an EBIC image is formed on the screen of the SEM or STEM. When a semiconductor device is imaged in cross-section, the depletion region will show bright EBIC contrast. The shape of the contrast can be treated mathematically to determine the minority carrier properties of the semiconductor, such as diffusion length and surface recombination velocity. In plan-view, areas with good crystal quality will show bright contrast, and areas containing defects will show dark EBIC contrast.\n\nAs such, EBIC is a semiconductor analysis technique useful for evaluating minority carrier properties and defect populations.\n\nEBIC can be used to probe subsurface hetero-junctions of nanowires and the properties of minority carriers .\n\nEBIC has also been extended to the study of local defects in insulators. For example, W.S. Lau (Lau Wai Shing) developed \"true oxide electron beam induced current\" in the 1990s. Thus, besides p-n junction or Schottky junction, EBIC can also be applied to MOS diodes. Local defects in semiconductor and local defects in the insulator could be distinguished. There exists a kind of defect which originates in the silicon substrate and extends into the insulator on top of the silicon substrate. (Please see references below.)\n\nRecently, EBIC has been applied to high-k dielectric used in advanced CMOS technology. (Please see references below.)\n\nMost EBIC images are qualitative and only show the EBIC signal as contrast image. Use of an external scan control generator on the SEM and a dedicated data acquisition system allow for sub-picoamp measurements and can give quantitative results. Some systems are commercially available that do this, and provide the ability to provide functional imaging by biasing and applying gate voltages to semiconductor devices.\n\n"}
{"id": "2312692", "url": "https://en.wikipedia.org/wiki?curid=2312692", "title": "Experimental data", "text": "Experimental data\n\nExperimental data in science and engineering is data produced by a measurement, test method, experimental design or quasi-experimental design. In clinical research any data produced are the result of a clinical trial. Experimental data may be qualitative or quantitative, each being appropriate for different investigations.\n\nGenerally speaking, qualitative data are considered more descriptive and can be subjective in comparison to having a continuous measurement scale that produces numbers. Whereas quantitative data are gathered in a manner that is normally experimentally repeatable, qualitative information is usually more closely related to phenomenal meaning and is, therefore, subject to interpretation by individual observers.\n\nExperimental data can be reproduced by a variety of different investigators and mathematical analysis may be performed on these data.\n\n\n"}
{"id": "34946963", "url": "https://en.wikipedia.org/wiki?curid=34946963", "title": "Fusion ignition", "text": "Fusion ignition\n\nFusion ignition is the point at which a nuclear fusion reaction becomes self-sustaining. This occurs when the energy being given off by the fusion reactions heats the fuel mass more rapidly than various loss mechanisms cool it. At this point, the external energy needed to heat the fuel to fusion temperatures is no longer needed. As the rate of fusion varies with temperature, the point of ignition for any given machine is typically expressed as a temperature.\n\nIgnition should not be confused with \"breakeven\", a similar concept that compares the total energy being given off to the energy being used to heat the fuel. The key difference is that breakeven ignores losses to the surroundings, which do not contribute to heating the fuel, and thus are not able to make the reaction self-sustaining. Breakeven is an important goal in the fusion energy field, but ignition is required for a practical energy producing design.\n\nIn nature, stars reach ignition at temperatures similar to that of the Sun, around 27 million degrees. Stars are so large that the fusion products will almost always interact with the plasma before it can be lost to the environment at the outside of the star. In comparison, man-made reactors are far less dense and much smaller, allowing the fusion products to easily escape the fuel. To offset this, much higher rates of fusion are required, and thus much higher temperatures; most man-made fusion reactors are designed to work at temperatures around 100 million degrees, or higher. To date, no man-made reactor has reached breakeven, let alone ignition. Ignition has however been achieved in the cores of detonating thermonuclear weapons.\n\nLawrence Livermore National Laboratory has its 1.8 MJ laser system running at full power. This laser system is designed to compress and heat a mixture of deuterium and tritium, which are both isotopes of hydrogen, in order to compress the isotopes to a fraction of their original size, and fuse them into helium atoms (releasing neutrons in the process).\n\nIn January 2012, National Ignition Facility Director Mike Dunne predicted in a Photonics West 2012 plenary talk that ignition would be achieved at NIF by October 2012. However, , NIF is operating at conditions about 1/10 to 1/3 of breakeven. Confusingly, by LLNL definitions, ignition and breakeven occur at the same point, due to specifics of their experiment.\n\nExperts believe that achieving fusion ignition is the first step towards the potentially limitless energy source that is nuclear fusion.\n\n"}
{"id": "50899567", "url": "https://en.wikipedia.org/wiki?curid=50899567", "title": "Gaspero Mazzeranghi", "text": "Gaspero Mazzeranghi\n\nGaspero Mazzeranghi (18th century) was a scientific instrument maker.\n\nA smith with a workshop in Florence in Borgo San Jacopo, Mazzeranghi worked for the Museo di Fisica e Storia Naturale throughout the last quarter of the eighteenth century, principally as an equipment and tool-maker. He also helped to build mechanical, hydraulic, and electrostatic instruments, and - most important of all - balances.\n"}
{"id": "53713645", "url": "https://en.wikipedia.org/wiki?curid=53713645", "title": "Gemma Godfrey", "text": "Gemma Godfrey\n\nGemma Godfrey is a wealth manager and entrepreneur. She founded Moola, an online investment service in the UK, and was hired as Arnold Schwarzenegger's adviser on the US edition of \"The Apprentice\" in 2016.\n\nShe was selected as one of BBC'S 100 Women in 2013.\n"}
{"id": "31847052", "url": "https://en.wikipedia.org/wiki?curid=31847052", "title": "George's Cosmic Treasure Hunt", "text": "George's Cosmic Treasure Hunt\n\nGeorge's Cosmic Treasure Hunt is a 2009 children's book written by Stephen and Lucy Hawking. George and Annie, the middle-school cosmologists, return in this sequel to the 2007 story, \"George's Secret Key to the Universe\". The book was followed by \"George and the Big Bang\" in 2011,\"George and the Unbreakable Code\" in 2014, \"George and the Blue Moon\" in 2016 and \"George and the Ship of Time\" in 2018.\n\n"}
{"id": "4104986", "url": "https://en.wikipedia.org/wiki?curid=4104986", "title": "How to Solve it by Computer", "text": "How to Solve it by Computer\n\nHow to Solve it by Computer is a computer science book by R. G. Dromey, first published by Prentice-Hall in 1982.\nIt is occasionally used as a textbook, especially in India.\n\nIt is an introduction to the \"why\"s of algorithms and data structures.\nFeatures of the book:\n\nThe very fundamental algorithms portrayed by this book are mostly presented in Pseudocode and/or Pascal notation.\n\n"}
{"id": "45210059", "url": "https://en.wikipedia.org/wiki?curid=45210059", "title": "Huggins (Martian crater)", "text": "Huggins (Martian crater)\n\nHuggins is an impact crater in the Eridania quadrangle of Mars, located at 49.4°S latitude and 204.4°W longitude, and is part of Terra Cimmeria. It is 90.0 km in diameter and was named after William Huggins, and the name was approved in 1973 by the International Astronomical Union (IAU) Working Group for Planetary System Nomenclature (WGPSN).\n\nNearby named craters include Cruls to the northeast, Rossby almost to the east, Campbell to the southeast, and the small Tycho Brahe to the west and the larger Kepler. West of Huggins is a feature known as Eridania Scopulus.\n\n"}
{"id": "18653648", "url": "https://en.wikipedia.org/wiki?curid=18653648", "title": "IC 1838", "text": "IC 1838\n\nIC 1838 (also known as MCG+03-08-002 or LEDA 10389) is a spiral galaxy of type Sc. It lies in the Taurus constellation, 52.42 million light years away from Earth. It has a diameter of 13,046.5 light years and a thickness of 1,304.7 light years.\n\n"}
{"id": "463144", "url": "https://en.wikipedia.org/wiki?curid=463144", "title": "Interactionism", "text": "Interactionism\n\nIn sociology, interactionism is a theoretical perspective that derives social processes (such as conflict, cooperation, identity formation) from human interaction. It is the study of how individuals shape society and are shaped by society through meaning that arises in interactions. Interactionist theory has grown in the latter half of the twentieth century and has become one of the dominant sociological perspectives in the world today. George Herbert Mead, as an advocate of pragmatism and the subjectivity of social reality, is considered a leader in the development of interactionism. Herbert Blumer expanded on Mead's work and coined the term \"symbolic interactionism\".\n\nInteractionism has several subdivisions:\nPhenomenology,\nVerstehen,\nSocial action,\nEthnomethodology,\nSymbolic interactionism, and\nSocial constructionism.\n\nInteractionism is micro-sociological and believes that meaning is produced through the interactions of individuals.\n\nThe social interaction is a face-to-face process consisting of actions, reactions, and mutual adaptation between two or more individuals. It also includes animal interaction such as mating. The interaction includes all language (including body language) and mannerisms. The goal of the social interaction is to communicate with others. If the interaction is in danger of ending before one intends to it, it can be conserved by conforming to the others' expectations, by ignoring certain incidents or by solving apparent problems. Erving Goffman underlines the importance of control in the interaction. One must attempt to control the others' behaviour during the interaction, in order to attain the information one is seeking and in order to control the perception of one's own image. Important concepts in the field of interactionism include the \"social role\" and Goffman's \"presentation of self.\"\n\nInteractionists are interested in how people see themselves in the broader social context. Interactionists want to understand each individual, and how they act within society. In extreme cases, they would deny class as an issue, and would say that we cannot generalize that everyone from one social class thinks in one way. Instead they believe everyone has different attitudes, values, culture and beliefs. Therefore, it is the duty of the sociologist to carry out the study within society. They set out to gather qualitative data.\n\nInteractionists reject statistical (quantitative) data, a method preferred by structuralists. These methods include; experiments, structured interviews, questionnaires, non-participant observation and secondary sources.\nThey have a few basic criticisms, namely:\n\nInteractionists prefer several methods to contrast with Structuralist methods, namely; unstructured interviews, covert participant observation, overt participant observation, and analysing historical, public and personal documents by content analysis.\n\nInteractionist methods generally reject the absolute need to provide statistics. Statistics allows cause and effect to be shown , as well as isolating variables so that relationships and trends can be distinguished over time.\nInstead, interactionists want to \"go deep\" to explain society. This draws criticisms such as:\n\nDespite these criticisms, interactionist methods do allow flexibility. The fact that there is no hypothesis means that the sociologist is not rooted in attempting to prove dogma or theory. Instead, researchers react to what they discover, not assuming anything about society. (This is not entirely true. There can be hypotheses for many studies using interactionist methods. The researcher may then be inclined to observe certain events happening while ignoring the bigger picture. This will still bias the results, if such studies are not well conducted. This is arguably why some theorists have turned to this method. \nIt also shows how human behaviour is affected and altered through interactions i.e. socialization.)\n\n\nInteractionism, or the idea that individuals have more awareness, skill and power to change their own situation, links to several other theories.\n\nNeo-Marxism is a loose term for various twentieth-century approaches that amend or extend Marxism and Marxist theory, usually by incorporating elements from other intellectual traditions, such as critical theory, psychoanalysis, or existentialism.\n\nPluralism is the idea that the \"public gets what the public wants.\" It is the notion that our lives offer choice like a representative democracy. This idea of consumer choice means that each individual has power as a consumer to change any aspect of life if he/she wishes to do so. The situation that exists is, according to the theory, a reflection of the norms, values and beliefs of the majority of people. It fits with the idea of individual power, although interactionist sociologists may not accept the idea that we are all labeled as \"consumers\".\n\n"}
{"id": "53175008", "url": "https://en.wikipedia.org/wiki?curid=53175008", "title": "Kim TallBear", "text": "Kim TallBear\n\nKim TallBear is a Sisseton Wahpeton Oyate professor at the University of Alberta, specializing in racial politics in science. TallBear was educated at the University of Massachusetts at Boston, Massachusetts Institute of Technology and University of California, Santa Cruz, where she was advised by Donna Haraway \nand Professor Emeritus James Clifford (historian).\n\nA member of the Council of the Native American and Indigenous Studies Association, in late 2016 she became the first ever Canada Research Chair in Indigenous Peoples, Technoscience and Environment. An anthropologist specialising in the intersection of science and technology with culture, TallBear is a frequent media commentator on issues of Tribal membership, genetics and identity. Her first book, \"Native American DNA: Tribal Belonging and the False Promise of Genetic Science\", was released in 2013 by the University of Minnesota Press. Described as a \"provocative and incisive work of interdisciplinary scholarship\", the book discusses the marketing of DNA testing as something capable of determining ancestry and race, and problematizes this by discussing the ways in which it shades into racial science. \n\nIn more recent work, including a keynote at the National Women's Studies Association meeting in 2016, TallBear has focused on sexuality, specifically on decolonizing the valorization of monogamy that she characterizes as emblematic of \"settler sexualities.\" This builds on work she has been doing in a blog written under an alter ego, \"The Critical Polyamorist.\" \n\nIn October 2018, she was featured in numerous media outlets critiquing Elizabeth Warren's claim to Indigenous ancestry. \n\n\n\n"}
{"id": "16020608", "url": "https://en.wikipedia.org/wiki?curid=16020608", "title": "Lean laboratory", "text": "Lean laboratory\n\nA lean laboratory is one which is focused on processes, procedures, and infrastructure that deliver results in the most efficient way in terms of cost, speed, or both. Lean laboratory is a management and organization process derived from the concept of lean manufacturing and the Toyota Production System (TPS). The goal of a lean laboratory is to reduce resource usage and costs while improving productivity, staff morale, and laboratory-driven outcomes.\n\nManufacturing companies, including medical device and pharmaceutical manufacturers, operate in highly regulated environments which often necessitate a great deal of resources, time, and money being expended in the testing, release, and quality assurance of their products. Since the early 1990s, there has been a more widespread drive to adopt more lean approaches both in the manufacturing and testing of products. The advances in lean thinking developed and refined in the automotive industry initially by Toyota (TPS) are now being used as best practices across most manufacturing sectors. The idea of lean laboratory shares its origins with lean manufacturing and uses the same tools to deliver the most efficient and least wasteful processes, tools such as Kaizen, Just In Time (JIT), Heijunka, Kanban, and Six Sigma.\n\nThe principles of lean manufacturing have been difficult at times to migrate to laboratories because they are quite different from manufacturing environments. In the hospital laboratory, for example, difficulties arise with the \"staunch adherence to traditional laboratory practices, complexity of workflow, and marked variability in sample numbers.\" In pharmaceutical and biopharmaceutical labs, \"the limiting belief\" that procedures are so different that lean won't work often slow down adoption. Compared to manufacturing environments, most analytical and microbiological laboratories have a relatively low volume of samples but a high degree of variability and complexity. Many standard lean tools are not a good fit; however, lean can still be applied to these types of labs. A generic approach is not suitable for laboratories, but careful adaptation of the techniques based on a thorough understanding of lab operations will deliver significant benefits in terms of cost, speed, or both.\n\nIt is a common occurrence for testing laboratories to suffer from long and variable lead times. Some of the problems or issues which can be attributed to conventional or “non lean” laboratories include the following issues.\n\nAnalysts and microbiologists are typically focused on test accuracy and individual test run efficiency. Very often, personnel are dedicated to specific tests and there is little or no control of the progress of individual samples through a sometimes highly variable test routing that can be dependent on product type and/or the intended market.\n\nIn many test laboratories, it is normal to find queues in front of each test where individual samples wait until enough similar samples arrive to constitute an \"efficient test run.\" This approach causes long and variable lead times and, contrary to popular belief, does not result in higher productivity.\n\nTo deal with the long lead times, \"fast track\" systems are often developed in an effort to deal with urgent samples but these often become unworkable. Frequently, the proportion of samples designated as priority becomes so large that fast tracking quickly becomes ineffective.\n\nLaboratories often maintain high levels of work in process (WIP), which inevitably results in significant (non value adding) effort being expended in controlling, tracking, and prioritizing samples and in planning analyst work. Companies often respond to this situation by investing in a laboratory information management system (LIMS) or some other IT system. However these systems do not in themselves improve performance. The underlying process by which work is organized and moves through the lab must first be re-engineered based on lean principles.\n\nFor many testing laboratories, the incoming workload is inherently volatile, with significant peaks and dips. This causes low productivity (during dips) and/or poor lead time performance (during peaks). Very often the capacity of the lab is not well understood, and there is no mechanism to level or smooth the workload.\n\nTo address the above problems and issues, a lean laboratory uses lean principles to eliminate waste or Muda. There are a number of principles that can be used, but the goal is always primarily focused on improving measurable performance and/or reducing costs.\n\nThe first step in designing any lean laboratory is to specify value. Every activity in the laboratory is identified and categorizing as \"value added,\" \"non-value added\" (from the customers perspective), and \"incidental.\" Incidental work is non value add in itself but essential to enable value add tasks to be carried out. A significant focus of any lean lab initiative will be to eliminate or reduce the non value add activities.\n\nAnother key lean step is to develop value stream maps of the overall release process. This should avoid the error of working on point solutions that only end up moving a bottleneck to another process and therefore do not deliver overall improvements. For example, there is no real value in reducing analytical laboratory lead times below the time of a release constraint test in a microbiology lab. You can however use increased velocity to help \"level the load\" or to maximize individual test run efficiency.\n\nA lean laboratory will normally have a defined sequence of tests and associated analyst roles that make good use of people and equipment. A key principle is to flow work through the laboratory so that once testing begins on a sample, it is kept moving and not allowed to queue between tests. This creates a focus and drive to reduce throughput time, which can be converted into a lead-time reduction or used to allow samples to wait in an incoming queue to facilitate level loading and /or grouping for efficiency.\n\n\"Pull\" is interpreted as testing according to customer priority. If this is not inherent in the order in which samples arrive, then the samples are taken from an incoming queue according to customer demand and thereafter processed in FIFO order with no overtaking.\n\nAt its simplest, leveling the load (overall workload) and the mix (the mix of sample types) is about putting the same amount of work into the lab on a daily basis. This is probably the most critical step and potentially the most beneficial for the majority of testing laboratories. Successfully leveling a volatile load and mix will significantly improve productivity and/or lead time. The productivity improvement can be used to provide additional capacity or converted into a cost reduction.\n\nLean laboratories continuously look to develop solutions and re-engineer processes to eliminate or reduce the non-value added and incidental tasks identified when specifying value.\n\nAn essential part of lean in the laboratory is to manage and review a lab's performance daily, ensuring that Key Performance Indicators (KPI's) are good and that the overall laboratory process is in control.\n\n"}
{"id": "38102511", "url": "https://en.wikipedia.org/wiki?curid=38102511", "title": "Life on Fire", "text": "Life on Fire\n\nLife on Fire is a six-part television documentary series, created in 2009–12, about volcanoes. It was directed by Bertrand Loyer, Jacques Bedel, and François de Riberolles and was produced by Saint Thomas Productions of Marseille between 2009 and 2012. It was narrated by Jeremy Irons. The series was first broadcast in France as \"Le Peuple des Volcans\", and had its American première on PBS on 2 January 2013.\n\nAnimals in New Britain have adapted to live with the activity of the local volcano near the city of Rabaul. Directed by Bertrand Loyer and released in France on 18 March 2010. Original title: \"Les Vagabonds des Cendres\".\nPBS first broadcast date: 30 January 2013.\n\nIn the Tongan archipelago, the sooty tern and the Alvin shrimp cope when an underwater volcano becomes an island. Directed by Bertrand Loyer and released in France on 23 October 2010. Original title: \"Naissance d'Une Ile\".\nPBS first broadcast date: 6 February 2013.\n\nVolcanic activity in Alaska 2,000 years ago disrupted the spawning journey of sockeye salmon, leading them to Surprise Lake in the caldera of Mount Aniakchak. Directed by Bertrand Loyer and released in France on 31 May 2012. Original title: \"Les Saumons Surprise\" aka \"Les Saumons du Lac Surprise\". PBS first broadcast date: 16 January 2013.\n\nNature and humans struggle to survive and thrive between eruptions of the Masaya Volcano in Nicaragua, and to rise again like the phoenix. Directed by François de Riberolles and released in France on 30 May 2012. Original title: \"Le Temple des Phénix\".\nPBS first broadcast date: 23 January 2013.\n\nVolcanologists use their research, insights, and tools to try to protect people living near volcanoes around the world. Directed by François de Riberolles and Bertrand Loyer. Released in France on 21 July 2010. Original title: \"Trappeurs de Volcans\".\nPBS first broadcast date: 9 January 2013.\n\nFuture volcanic eruptions in Iceland will affect Europe and North America. Directed by Jacques Bedel and François de Riberolles. Television première in France on 26 March 2011. Original title: \"Volcans d'Islande, et Demain?\"\nPBS first broadcast date: 2 January 2013.\n\nThe series has won more than 20 awards in international film festivals. In particular, wildlife episodes have been acclaimed for their cinematography, sound and storylines.\nWith their 5.1 surround sound and dramatic structures, the stand-alone episodes of \"Life on Fire\" contrast with those from other wildlife TV series, which are usually delivered in stereo and structured like catalogues - with a succession of sequences guided by a seasonal or geographical theme (\"Spring\", \"Abyss\", etc...).\n\nSelected awards :\n\nAsh Runners: Best Sound in UK, Wildscreen 2010 and Wildtalk Africa 2010, Best Cinematography in Russia, St Petersbourg World of Knowledge 2012, Grand Prize in Belgium, Namur Nature Film Festival\n\nPioneers of the Deep: Best Script and Special Jury Award, in France, Albert Nature Film Festival, France, Grand Prix in France, Menigoute Bird Film Festival, Silver Palm in France, Marseille International Underwater Film Festival\n\nThe Surprise salmon: Best Marine Animal Behavior, USA, CA, Monterey, Gold Palm in France, Marseille International Underwater Film Festival, Best Script and Best Sound in France, Albert Nature Film Festival\n\nPhoenix temple: Grand Prix and Best Cinematography, Albert Nature Film Festival, Grand Prix in Italy, Cogne, Gran Paradiso Nature Film Festival.\n\nCinematographers have been using innovative techniques seen in other wildlife series, such as Cineflex or Phantom cameras. But they also developed some special artisanal tools, as revealed by some behind the scenes clips on the US or French Home Video editions.\n\nThe series has not used any archive material. Most of the images have been captured in 4K and 5K formats, using RED One and Epic Digital Cameras, and down-converted to HD. By producing the series in such format, Saint Thomas Productions claims to own the largest HD stock footage library of volcanoes, displayed on a dedicated website.\n\nNo sequel of this series has been produced. However, Saint Thomas Productions has produced a 90 minutes special episode, named \"A Volcano Odyssey\". It premiered on Arte in 2012 under the name Memoires de Volcans and achieved the channel best rating in 2012 for their prime time Sunday strand.\n\n"}
{"id": "8451997", "url": "https://en.wikipedia.org/wiki?curid=8451997", "title": "List of Brazilian scientists", "text": "List of Brazilian scientists\n\nThis is a list of Brazilian scientists, those born in Brazil or who have established citizenship or residency there.\n\n<onlyinclude>\n\n\n</onlyinclude>\n\n\n"}
{"id": "4667799", "url": "https://en.wikipedia.org/wiki?curid=4667799", "title": "List of PAN dating software", "text": "List of PAN dating software\n\nPAN dating software is computer software to encourage conversation with others on a similar wireless network.\n\n\n"}
{"id": "436053", "url": "https://en.wikipedia.org/wiki?curid=436053", "title": "List of U.S. state and territory nicknames", "text": "List of U.S. state and territory nicknames\n\nThe following is a table of U.S. state and territory nicknames, including officially adopted nicknames, and other traditional nicknames for individual states and territories of the United States (and the District of Columbia).\n\nCurrent official state and territory nicknames are highlighted in bold. A state nickname is not to be confused with an official state motto.\n\n\n"}
{"id": "47232159", "url": "https://en.wikipedia.org/wiki?curid=47232159", "title": "List of fellows of the Nigerian Academy of Science", "text": "List of fellows of the Nigerian Academy of Science\n\nThe Nigerian Academy of Science is the official science academy of Nigeria . It is the apex scientific organization in Nigeria and fellow of the academy are often elected following a nomination of qualified candidate by a fellow of the academy, known as the principal nominator, who must be in the same academic field as the candidate.\n\nThis is the List of fellows of the Nigerian Academy of Science arranged in alphabetical order.\n"}
{"id": "29533390", "url": "https://en.wikipedia.org/wiki?curid=29533390", "title": "List of national parks of Paraguay", "text": "List of national parks of Paraguay\n\nThis is a list of national parks of Paraguay. \"It is incomplete.\"\n\n"}
{"id": "33206714", "url": "https://en.wikipedia.org/wiki?curid=33206714", "title": "List of rugby union playing countries", "text": "List of rugby union playing countries\n\nThis list shows each country to have a union affiliated to the International Rugby Board (IRB), the world organisation that manages the game of rugby union. As well as listing the countries, it also shows the number of registered clubs playing in each country, official referees and the number of registered players broken down by gender and age group. \n\nLatest information supplier by World Rugby (the new name for the IRB) In 2016, the total number of registered players increased from 2.82 million to 3.2 million while the total number of non-registered rugby players rose from 4.91 million to 5.3 million. South Africa has the most registered players with 651,146 and England the most players overall with 2,139,604. \n"}
{"id": "47153689", "url": "https://en.wikipedia.org/wiki?curid=47153689", "title": "Mining and Chemical Combine", "text": "Mining and Chemical Combine\n\nThe Mining and Chemical Combine was established in 1950 to produce plutonium for weapons. It is in the closed city Zheleznogorsk, Krasnoyarsk Krai. The company is currently part of the Rosatom group.\n\nThe complex has an interim storage facility. There is also a 60 t/year commercial mixed oxide (MOX) fuel fabrication facility (MFFF). The place employs 7000 people.\n\nThe MOX production line completed a 10 kg batch in September 2014.\n\n"}
{"id": "1275240", "url": "https://en.wikipedia.org/wiki?curid=1275240", "title": "Moti Lal Dhar", "text": "Moti Lal Dhar\n\nMoti Lal Dhar ( 22 October 1914 – 20 January 2002) was an eminent drug chemist, and science administrator in India. He remained Director Central Drug Research Institute, Lucknow from 1960 until his retirement in 1972 and he has been the only Kashmiri Pandit to serve as Vice- Chancellor, of Banaras Hindu University, B.H.U.\n"}
{"id": "51119912", "url": "https://en.wikipedia.org/wiki?curid=51119912", "title": "PinpointBPS", "text": "PinpointBPS\n\nPinpointBPS is a methodology for process improvement in laboratories. It is underpinned by eight principles that form the basis for decision-making in a laboratory. While its application is mainly in healthcare — particularly medical laboratories — it has also been applied in other industries. The methodology has been heralded as \"groundbreaking\" in the field of laboratory performance improvement.\n\nMedical laboratories operate in highly regulated environments that demand consistent quality of patient outputs. The external environment’s impact on the broader global healthcare industry is also dictating the need for ongoing improvements in quality and delivery while using fewer resources that leads to cost savings. Among other regulations, the Carter Report in the United Kingdom dictates that GBP£200-million in cost savings must be achieved by 2020, while the Affordable Care Act in the United States, also known as Obamacare, imposed further taxation requirements on medical laboratories, reducing cost saving ability. Dr. Jonathan Berg stated at The Royal College of Pathologists' annual meeting in 2012 that \"We need to be creative and innovative in the services we offer, and we need to get our finances under control\", indicating the critical need for innovation within pathology and a stronger focus on financial controls and performance. Within this context, PinpointBPS was founded as a methodology focusing on innovation, risk reduction and financial impact relating to laboratory performance improvement.\n\nThe PinpointBPS methodology is centered on the following process of discovery that highlights current performance, expected future performance and the means to achieve it. The methodology has also been aligned with ISO 15189 requirements.\n\nThe steps below provide a high-level overview of the methodology in practice.\n\nWhile all effort within the laboratory is put into value creation, from turnaround time improvements to quality output and cost savings / profit increases, it is important that the performance of each of these can be quantified and their relationship with the financial performance of the laboratory understood. This enables the laboratory to understand current performance needs while at the same time establishing the key performance indicators that are important and should be measured.\n\nUnderstanding the current performance of the laboratory provides the necessary context to highlight performance improvement requirements as well as areas of excellence. This should be conducted by looking at sample turnaround time and resource utilization, and should be considered on a holistic basis, creating a virtual model of the whole laboratory by mapping out all processes and activities in detail. This provides an end-to-end perspective on current performance and forms the basis for future performance improvement initiatives.\n\nThe expectation for future performance should be done through business process modeling, with current performance as the foundation. Using LIS data as well as data from workforce scheduling, and combining this data with the process model of the laboratory, creates an environment where changes to the laboratory can be simulated and the impact to performance understood in quantifiable terms. It is important to validate both the integrity of the data and the accuracy of the model. Once a desirable outcome for future performance has been found, the difference (or delta) in performance is evaluated to understand future performance requirements.\n\nThe difference in performance between current (baseline) and future (expected) performance highlights the necessary process changes and related initiatives that are required to achieve the future performance. This impact should be measured according to finance (cost or profit), quality and turnaround time. For these changes to take effect, each change initiative is prioritized based on a quantified understanding of its impact to the laboratory 'bottom line', while also improving turnaround time, quality or both.\n\nWhile the PinpointBPS methodology can support Lean (and other continuous improvement methodologies like Six Sigma) in that it ultimately aims to provide patients and other stakeholders with quality outputs, its approach differs from other methodologies like the Lean Laboratory in various ways:\n\nPinpointPBS has been based on eight practical principles that allow laboratories to take ownership of the methodology and performance improvement initiatives.\n\nAs with any organization, value creation is the ultimate goal, however purely focusing on quality output may neglect cognizance of financial value. Understanding the ultimate financial impact of any value creating activity is critical for ongoing sustainability.\n\nEnsuring that everyone in the organization (and value chain) has the same understanding of value creation and how it links to financial performance\n\nContinuous innovation requires us to draw from the same knowledge and speak the same language. We establish a universal truth that enables us to work together seamlessly.\n\nWe value a clear line of sight into our strengths, our weaknesses and the connections between every process, person, and piece of technology.\n\nWe believe in making decisions based on evidence. In our quest for bottom-line impact, every business action must be supported by real, tangible proof.\n\nWe are empowered through understanding our performance and what shapes it. This allows everyone in our organization to make decisions swiftly, with understanding and certainty.\n\nClose is not close enough. Every decision we take is based on fact, and our execution must be accomplished with the same precision\n\nWe believe in following the big opportunities that significantly impact the bottom line, rather than wasting time and resources on those that do not.\n\nWe are only as good as the collective we represent. We fundamentally believe in continually imparting knowledge and learning from each other to move the industry forward.\n\nVarious tools have been developed to enable the methodology in laboratories, including performance overviews, standardized process mapping using BPMN (business process model and notation), as well as simulation and scenario modeling.\n\nPinpointBPS professional certification can be granted to both laboratories that practice the methodology, as well as people who have successfully completed further study of the methodology. Two levels of PinpointBPS professional certifications exist. These certifications have been accredited by PACE, CPD, and the Royal College of Pathologists.\n\nA PinpointBPS Champion is a professional who is able to interpret laboratory performance improvement metrics, map out all processes within a laboratory and make improvement recommendations.\n\nA PinpointBPS Master builds on the PinpointBPS Champion certification and is a professional who is able to simulate changes to a laboratory and interpret performance outputs, to make performance improvement recommendations and manage initiatives from start to end.\n"}
{"id": "12004499", "url": "https://en.wikipedia.org/wiki?curid=12004499", "title": "Principle of Priority", "text": "Principle of Priority\n\nPriority is a fundamental principle of modern botanical nomenclature and zoological nomenclature. Essentially, it is the principle of recognising the first valid application of a name to a plant or animal. There are two aspects to this: \n\nThere are formal provisions for making exceptions to this principle. If an archaic or obscure prior name is discovered for an established taxon, the current name can be declared a \"nomen conservandum\" (botany) or \"conserved name\" (zoology), and so conserved against the prior name. Similarly, if the current name for a taxon is found to have an archaic or obscure prior homonym, the current name can be declared a \"nomen protectum\" (zoology) or the older name suppressed (\"nomen rejiciendum\", botany).\n\nThe principle of priority has not always been in place. When Carl Linnaeus laid the foundations of modern nomenclature, he offered no recognition of prior names. The botanists who followed him were just as willing to overturn Linnaeus's names. The first sign of recognition of priority came in 1813, when A. P. de Candolle laid out some principles of good nomenclatural practice. He favoured retaining prior names, but left wide scope for overturning poor prior names.\n\nDuring the 19th century, the principle gradually came to be accepted by almost all botanists, but debate continued to rage over the conditions under which the principle might be ignored. Botanists on one side of the debate argued that priority should be universal and without exception. This would have meant a one-off major disruption as countless names in current usage were overturned in favour of archaic prior names. In 1891, Otto Kuntze, one of the most vocal proponents of this position, did just that, publishing over 30000 new combinations in his \"Revisio Generum Plantarum\". He then followed with further such publications in 1893, 1898 and 1903. His efforts, however, were so disruptive that they appear to have benefited his opponents. By the 1900s, the need for a mechanism for the conservation of names was widely accepted, and details of such a mechanism were under discussion. The current system of \"modified priority\" was essentially put in place at the Cambridge Congress of 1930.\n\nThe Principle of Priority is one of the guiding principles of the \"International Code of Zoological Nomenclature\", defined by Article 23. There are exceptions: another name may be given precedence by any provision of the Code or by any ruling of the Commission. It is a fundamental guiding precept that preserves the stability of biological nomenclature. It was first formulated in 1842 by a committee appointed by the British Association to consider the rules of zoological nomenclature; the committee's report was written by Hugh Edwin Strickland.\n\n\nIn botany and horticulture, the principle of priority applies to names at the rank of family and below. When moves are made to another genus or from one species to another, the \"final epithet\" of the name is combined with the new genus name, with any adjustments necessary for Latin grammar, for example:\n\n"}
{"id": "582702", "url": "https://en.wikipedia.org/wiki?curid=582702", "title": "Quasistatic process", "text": "Quasistatic process\n\nIn thermodynamics, a quasi-static process is a thermodynamic process that happens slowly enough for the system to remain in internal equilibrium. An example of this is quasi-static compression, where the volume of a system changes at a slow rate enough to allow the pressure to remain uniform and constant throughout the system.\n\nAny reversible process is necessarily a quasi-static one. However, quasi-static processes involving entropy production are irreversible. An example of a quasi-static process that is not reversible is a compression against a system with a piston subject to friction—although the system is always in thermal equilibrium, the friction ensures the generation of dissipative entropy, which directly goes against the definition of reversible. A notable example of a process that is not even quasi-static is the slow heat exchange between two bodies at two finitely different temperatures, where the heat exchange rate is controlled by an approximately adiabatic partition between the two bodies—in this case, no matter how slowly the process takes place, the states of the two bodies are never infinitesimally close to equilibrium, since thermal equilibrium requires that the two bodies be at the same temperature.\n\nSome ambiguity exists in the literature concerning the distinction between quasi-static and reversible processes, as these are sometimes taken as synonyms. The reason is the theorem that any reversible process is also a quasi-static one, even though (as we have illustrated above) the converse is not true. In practical situations, it is essential to differentiate between the two: any engineer would remember to include friction when calculating the dissipative entropy generation, so there are no reversible processes in practice. The definition given above is closer to the intuitive understanding of the word “quasi-” (almost) “static”, and remains technically different from reversible processes.\n\n\n"}
{"id": "51138529", "url": "https://en.wikipedia.org/wiki?curid=51138529", "title": "Richard J. O'Connell", "text": "Richard J. O'Connell\n\nRichard John O'Connell (August 27, 1941 – April 2, 2015) was an American geophysicist working on the internal dynamics of the Earth and how they evolved over time and are observed at the surface. He received his B.S., M.S., and Ph.D. degrees from California Institute of Technology, and spent most of his further academic career at Harvard University. \n\nO'Connell received the Inge Lehmann Medal from American Geophysical Union in 2000, the Arthur L. Day Medal from Geological Society of America in 2001, and the Augustus Love Medal from the European Geosciences Union in 2008. He was a fellow of the American Geophysical Union, the American Association for the Advancement of Science, and American Academy of Arts and Sciences.\n\n"}
{"id": "34421644", "url": "https://en.wikipedia.org/wiki?curid=34421644", "title": "Ross–Fahroo pseudospectral method", "text": "Ross–Fahroo pseudospectral method\n\nIntroduced by I. Michael Ross and F. Fahroo, the Ross–Fahroo pseudospectral methods are a broad collection of pseudospectral methods for optimal control. Examples of the Ross–Fahroo pseudospectral methods are the pseudospectral knotting method, the flat pseudospectral method, the Legendre-Gauss-Radau pseudospectral method and pseudospectral methods for infinite-horizon optimal control.\nThe Ross–Fahroo methods are based on shifted Gaussian pseudospectral node points. The shifts are obtained by means of a linear or nonlinear transformation while the Gaussian pseudospectral points are chosen from a collection of Gauss-Lobatto or Gauss-Radau distribution arising from Legendre or Chebyshev polynomials. The Gauss-Lobatto pseudospectral points are used for finite-horizon optimal control problems while the Gauss-Radau pseudospectral points are used for infinite-horizon optimal control problems.\n\nThe Ross–Fahroo methods are founded on the Ross–Fahroo lemma; they can be applied to optimal control problems governed by differential equations, differential-algebraic equations, differential inclusions, and differentially-flat systems. They can also be applied to infinite-horizon optimal control problems by a simple domain transformation technique.\n\nThe Ross–Fahroo methods have been implemented in many practical applications and laboratories around the world. In 2006, NASA used the Ross–Fahroo method to implement the \"zero propellant maneuver\" on board the International Space Station.\nIn recognition of all these advances, the AIAA presented Ross and Fahroo, the 2010 Mechanics and Control of Flight Award, for \"... changing the landscape of flight mechanics.\" Ross was also elected AAS Fellow for \"his pioneering contributions to pseudospectral optimal control.\"\n\nA remarkable feature of the Ross–Fahroo methods is that it does away with the prior notions of \"direct\" and \"indirect\" methods. That is, through a collection of theorems put forth by Ross and Fahroo,\n\nthey showed that it was possible to design pseudospectral methods for optimal control that were equivalent in both the direct and indirect forms. This implied that one could use their methods as simply as a \"direct\" method while automatically generating accurate duals as in \"indirect\" methods. This revolutionized solving optimal control problems leading to widespread use of the Ross–Fahroo techniques.\n\nThe Ross–Fahroo methods are implemented in the MATLAB optimal control solver, DIDO.\n\n"}
{"id": "32800066", "url": "https://en.wikipedia.org/wiki?curid=32800066", "title": "Rufus Henry Gilbert", "text": "Rufus Henry Gilbert\n\nRufus Henry Gilbert (1832–1885) was an American surgeon and inventor, who worked on rapid transit in New York City.\n\nRufus Henry Gilbert was born in Guilford, New York on January 26, 1832. Gilbert was the son of William Dwight Gilbert, a county judge in Steuben County, New York.\n\nGilbert studied at the New York College of Physicians and Surgeons and graduated to become a physician and surgeon. During his time in New York City, Gilbert became concerned with the cramped and overly centralized living conditions of the working class, seeing this as a major public health hazard, and began thinking about urban rapid transit as the key to provision of more sanitary living conditions.\n\nDuring the American Civil War, he joined the 5th New York Volunteer Infantry as a surgeon, performing the first surgical procedure during the war at the Battle of Big Bethel. He eventually became Medical Director and Superintendent of the United States Army Hospitals.\n\nTowards the end of the war, Gilbert's own medical issues prevented him from a further career in this field.\n\nGilbert subsequently became Superintendent of the Central Railroad of New Jersey, where he worked on developing rapid transport in the New York City area.\n\nIn 1870 Gilbert obtained a patent for an elevated railway using the principle of pneumatics. Gilbert incorporated a company knowns as the Gilbert Elevated Railway Company but had difficulty obtaining adequate financing for the venture. Ultimately Gilbert was forced to surrender control of the company to the New York Loan and Improvement Company in order to obtain sufficient capital. The company constructed the Sixth Avenue road, known as \"Gilbert Elevated Railroad,\" which opened in 1878. Gilbert was forced out of the company by his partners soon after the road opened, however, effectively ending his career.\n\nWith his health failing, Gilbert died in New York City on July 10, 1885. He was just 53 years old at the time of his death.\n\n"}
{"id": "39155772", "url": "https://en.wikipedia.org/wiki?curid=39155772", "title": "SEEUTechPark", "text": "SEEUTechPark\n\nSEEUTechPark is a technology park located on South East European University campus in Tetovo, Macedonia. Opened on May 15, 2013 by the Board of South East European University in order to create conditions to stimulate the creation of new start-up companies, creating a synergy between the companies and encourage the growth of existing SMEs (small and medium enterprises) which in the long term provides new job opportunities.\n\nSEEUTechPark launched on February 1, 2013.\n\nSEEUTechPark\n\nSEEUTechPark\n\nSEEUTechPark\n\nTrainings not only on fundamentals technology but also technical courses for students and others that are interested to be successful during their studies and at beginning of their career.\n\nPanel discussion organized by the SEEUTechPark which comes with a meeting in a month - “On Wednesday“\n\nWednesday TALK! is designed to provide an opportunity for students and everyone who wants to hear several people knowledgeable about specific issue or topic, present information and discuss personal views. Wednesday TALK! may help the audience further clarify and evaluate their positions regarding specific issues or topics being discussed and increase their understanding of the positions of others.\n\nmulti-ethnic, multi-lingual environment (with A Abazi) US-China Journal of Education (10)\n\napproach in the Republic of Macedonia in Procedia Social and Behavioural Sciences 15, 2093-2087\n\n"}
{"id": "2909422", "url": "https://en.wikipedia.org/wiki?curid=2909422", "title": "Secund", "text": "Secund\n\nSecund (Lat. \"secundus\", \"following\") is a botanical term used of plants when similar parts are directed to one side only, as flowers on an axis.\n"}
{"id": "30262", "url": "https://en.wikipedia.org/wiki?curid=30262", "title": "Tamara E. Jernigan", "text": "Tamara E. Jernigan\n\nTamara Elizabeth \"Tammy\" Jernigan, Ph.D. (born May 7, 1959, in Chattanooga, Tennessee) is an American scientist and former NASA astronaut and a veteran of five shuttle missions.\n\nJernigan attended Santa Fe High School in Santa Fe Springs, CA. She graduated in 1977. Jernigan attended Stanford University, where she earned a B.S. degree in physics in 1981, an M.S. in engineering science in 1983. At the University of California, Berkeley, she received an M.S. in astronomy in 1985. In 1988 she was awarded a Ph.D. in space physics and astronomy from Rice University.\n\nShe entered the NASA Astronaut Corps in 1986 and retired in 2001. Her first trip to space was on June 5, 1991. She flew on five Space Shuttle program missions (three on \"Columbia\" and one each on \"Endeavour\" and \"Discovery\") and logged 1512 hours in space. In her last mission on \"Discovery\" in 1999, she performed an extra-vehicular activity for 7 hours and 55 minutes.\n\nShe currently resides in Pleasanton, CA. She is married and has a child, Jeffrey Wisoff with former astronaut Peter Wisoff.\n"}
{"id": "439891", "url": "https://en.wikipedia.org/wiki?curid=439891", "title": "Thomas Wright (geologist)", "text": "Thomas Wright (geologist)\n\nThomas Wright FRS (9 November 1809 – 17 November 1884) was a Scottish surgeon and palaeontologist.\n\nWright published a number of papers on the fossils which he had collected in the Cotswolds, including \"Lias Ammonites of the British Isles\".\n\nWright was born in Paisley and studied at the Royal college of Surgeons in Dublin. In 1846 he moved to Cheltenham, where he became medical officer of health to the urban district, and surgeon at Cheltenham General Hospital. He won the Wollaston Medal in 1878 and became a fellow of the Royal Society in 1879.\n\nAfter his death part of his fossil collection was sold to the British Museum.\n"}
{"id": "19595664", "url": "https://en.wikipedia.org/wiki?curid=19595664", "title": "Time in physics", "text": "Time in physics\n\nTime in physics is defined by its measurement: time is what a clock reads. In classical, non-relativistic physics it is a scalar quantity and, like length, mass, and charge, is usually described as a fundamental quantity. Time can be combined mathematically with other physical quantities to derive other concepts such as motion, kinetic energy and time-dependent fields. \"\" is a complex of technological and scientific issues, and part of the foundation of \"recordkeeping\".\n\nBefore there were clocks, time was measured by those physical processes which were understandable to each epoch of civilization:\n\nEventually, it became possible to characterize the passage of time with instrumentation, using operational definitions. Simultaneously, our conception of time has evolved, as shown below. \n\nIn the International System of Units (SI), the unit of time is the second (symbol: formula_1). It is a SI base unit, and it has been defined since 1967 as \"the duration of periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom\". This definition is based on the operation of a caesium atomic clock. These clocks became practical for use as primary reference standards after about 1955 and have been in use ever since.\n\nThe UTC timestamp in use worldwide is an atomic time standard. The relative accuracy of such a time standard is currently on the order of 10 (corresponding to 1 second in approximately 30 million years). The smallest time step considered theoretically observable is called the Planck time, which is approximately 5.391×10 seconds - many orders of magnitude below the resolution of current time standards.\n\nThe caesium atomic clock became practical after 1950 when advances in electronics enabled reliable measurement of the microwave frequencies it generates. As further advances occurred, atomic clock research has progressed to ever-higher frequencies, which can provide higher accuracy and higher precision. Clocks based on these techniques have been developed but are not yet in use as primary reference standards.\n\nGalileo, Newton, and most people up until the 20th century thought that time was the same for everyone everywhere. This is the basis for s, where time is a parameter. The modern conception of time is based on Einstein's theory of relativity, in which rates of time run differently depending on relative motion, and space and time are merged into spacetime, where we live on a world line rather than a timeline. In this view time is a coordinate. According to the prevailing cosmological model of the Big Bang theory time itself began as part of the entire Universe about 13.8 billion years ago.\n\nIn order to measure time, one can record the number of occurrences (events) of some periodic phenomenon. The regular recurrences of the seasons, the motions of the sun, moon and stars were noted and tabulated for millennia, before the laws of physics were formulated. The sun was the arbiter of the flow of time, but time was known only to the hour for millennia, hence, the use of the gnomon was known across most of the world, especially Eurasia, and at least as far southward as the jungles of Southeast Asia.\n\nIn particular, the astronomical observatories maintained for religious purposes became accurate enough to ascertain the regular motions of the stars, and even some of the planets.\n\nAt first, timekeeping was done by hand by priests, and then for commerce, with watchmen to note time as part of their duties.\nThe tabulation of the equinoxes, the sandglass, and the water clock became more and more accurate, and finally reliable. For ships at sea, boys were used to turn the sandglasses and to call the hours.\n\nRichard of Wallingford (1292–1336), abbot of St. Alban's abbey, famously built a mechanical clock as an astronomical orrery about 1330.\n\nBy the time of Richard of Wallingford, the use of ratchets and gears allowed the towns of Europe to create mechanisms to display the time on their respective town clocks; by the time of the scientific revolution, the clocks became miniaturized enough for families to share a personal clock, or perhaps a pocket watch. At first, only kings could afford them. Pendulum clocks were widely used in the 18th and 19th century. They have largely been replaced in general use by quartz and digital clocks. Atomic clocks can theoretically keep accurate time for millions of years. They are appropriate for standards and scientific use.\n\nIn 1583, Galileo Galilei (1564–1642) discovered that a pendulum's harmonic motion has a constant period, which he learned by timing the motion of a swaying lamp in harmonic motion at mass at the cathedral of Pisa, with his pulse.\n\nIn his \"Two New Sciences\" (1638), Galileo used a water clock to measure the time taken for a bronze ball to roll a known distance down an inclined plane; this clock was \n\nGalileo's experimental setup to measure the literal \"flow of time\", in order to describe the motion of a ball, preceded Isaac Newton's statement in his Principia:\n\nThe Galilean transformations assume that time is the same for all reference frames.\n\nIn or around 1665, when Isaac Newton (1643–1727) derived the motion of objects falling under gravity, the first clear formulation for mathematical physics of a treatment of time began: linear time, conceived as a \"universal clock\".\n\nThe water clock mechanism described by Galileo was engineered to provide laminar flow of the water during the experiments, thus providing a constant flow of water for the durations of the experiments, and embodying what Newton called \"duration\".\n\nIn this section, the relationships listed below treat time as a parameter which serves as an index to the behavior of the physical system under consideration. Because Newton's fluents treat a \"linear flow of time\" (what he called \"mathematical time\"), time could be considered to be a linearly varying parameter, an abstraction of the march of the hours on the face of a clock. Calendars and ship's logs could then be mapped to the march of the hours, days, months, years and centuries.\n\nBy 1798, Benjamin Thompson (1753–1814) had discovered that work could be transformed to heat without limit - a precursor of the conservation of energy or\nIn 1824 Sadi Carnot (1796–1832) scientifically analyzed the steam engines with his Carnot cycle, an abstract engine. Rudolf Clausius (1822–1888) noted a measure of disorder, or entropy, which affects the continually decreasing amount of free energy which is available to a Carnot engine in the:\nThus the continual march of a thermodynamic system, from lesser to greater entropy, at any given temperature, defines an arrow of time. In particular, Stephen Hawking identifies three arrows of time:\n\nEntropy is maximum in an isolated thermodynamic system, and increases. In contrast, Erwin Schrödinger (1887–1961) pointed out that life depends on a \"negative entropy flow\". Ilya Prigogine (1917–2003) stated that other thermodynamic systems which, like life, are also far from equilibrium, can also exhibit stable spatio-temporal structures. Soon afterward, the Belousov-Zhabotinsky reactions were reported, which demonstrate oscillating colors in a chemical solution. These nonequilibrium thermodynamic branches reach a \"bifurcation point\", which is unstable, and another thermodynamic branch becomes stable in its stead.\n\nIn 1864, James Clerk Maxwell (1831–1879) presented a combined theory of electricity and magnetism. He combined all the laws then known relating to those two phenomenon into four equations. These vector calculus equations which use the del operator (formula_2) are known as Maxwell's equations for electromagnetism.\n\nIn free space (that is, space not containing electric charges), the equations take the form (using SI units):\n\nwhere\n\nThese equations allow for solutions in the form of electromagnetic waves. The wave is formed by an electric field and a magnetic field oscillating together, perpendicular to each other and to the direction of propagation. These waves always propagate at the speed of light \"c\", regardless of the velocity of the electric charge that generated them.\n\nThe fact that light is predicted to always travel at speed \"c\" would be incompatible with Galilean relativity if Maxwell's equations were assumed to hold in any inertial frame (reference frame with constant velocity), because the Galilean transformations predict the speed to decrease (or increase) in the reference frame of an observer traveling parallel (or antiparallel) to the light.\n\nIt was expected that there was one absolute reference frame, that of the luminiferous aether, in which Maxwell's equations held unmodified in the known form.\n\nThe Michelson-Morley experiment failed to detect any difference in the relative speed of light due to the motion of the Earth relative to the luminiferous aether, suggesting that Maxwell's equations did, in fact, hold in all frames. In 1875, Hendrik Lorentz (1853–1928) discovered Lorentz transformations, which left Maxwell's equations unchanged, allowing Michelson and Morley's negative result to be explained. Henri Poincaré (1854–1912) noted the importance of Lorentz's transformation and popularized it. In particular, the railroad car description can be found in \"Science and Hypothesis\", which was published before Einstein's articles of 1905.\n\nThe Lorentz transformation predicted space contraction and time dilation; until 1905, the former was interpreted as a physical contraction of objects moving with respect to the aether, due to the modification of the intermolecular forces (of electric nature), while the latter was thought to be just a mathematical stipulation. \n\nAlbert Einstein's 1905 special relativity challenged the notion of absolute time, and could only formulate a definition of synchronization for clocks that mark a linear flow of time:\n\nIndeed, the Lorentz transformation (for two reference frames in relative motion, whose \"x\" axis is directed in the direction of the relative velocity)\n\ncan be said to \"mix\" space and time in a way similar to the way a Euclidean rotation around the \"z\" axis mixes \"x\" and \"y\" coordinates. Consequences of this include relativity of simultaneity. More specifically, the Lorentz transformation is a hyperbolic rotation formula_10 which is a change of coordinates in the four-dimensional Minkowski space, a dimension of which is \"ct\". (In Euclidean space an ordinary rotation formula_11 is the corresponding change of coordinates.) The speed of light \"c\" can be seen as just a conversion factor needed because we measure the dimensions of spacetime in different units; since the metre is currently defined in terms of the second, it has the \"exact\" value of . We would need a similar factor in Euclidean space if, for example, we measured width in nautical miles and depth in feet. In physics, sometimes units of measurement in which \"c\" = 1 are used to simplify equations.\n\nTime in a \"moving\" reference frame is shown to run more slowly than in a \"stationary\" one by the following relation (which can be derived by the Lorentz transformation by putting ∆\"x\"′ = 0, ∆\"τ\" = ∆\"t\"′):\nwhere:\n\nMoving objects therefore are said to \"show a slower passage of time\". This is known as time dilation.\n\nThese transformations are only valid for two frames at \"constant\" relative velocity. Naively applying them to other situations gives rise to such paradoxes as the twin paradox.\n\nThat paradox can be resolved using for instance Einstein's General theory of relativity, which uses Riemannian geometry, geometry in accelerated, noninertial reference frames. Employing the metric tensor which describes Minkowski space:\n\nEinstein developed a geometric solution to Lorentz's transformation that preserves Maxwell's equations. His field equations give an exact relationship between the measurements of space and time in a given region of spacetime and the energy density of that region.\n\nEinstein's equations predict that time should be altered by the presence of gravitational fields (see the Schwarzschild metric):\n\nWhere:\n\nOr one could use the following simpler approximation:\n\nThat is, the stronger the gravitational field (and, thus, the larger the acceleration), the more slowly time runs. The predictions of time dilation are confirmed by particle acceleration experiments and cosmic ray evidence, where moving particles decay more slowly than their less energetic counterparts. Gravitational time dilation gives rise to the phenomenon of gravitational redshift and Shapiro signal travel time delays near massive objects such as the sun. The Global Positioning System must also adjust signals to account for this effect.\n\nAccording to Einstein's general theory of relativity, a freely moving particle traces a history in spacetime that maximises its proper time. This phenomenon is also referred to as the principle of maximal aging, and was described by Taylor and Wheeler as:\n\nEinstein's theory was motivated by the assumption that every point in the universe can be treated as a 'center', and that correspondingly, physics must act the same in all reference frames. His simple and elegant theory shows that time is relative to an inertial frame. In an inertial frame, Newton's first law holds; it has its own local geometry, and therefore its \"own\" measurements of space and time; \"there is no 'universal clock\"'. An act of synchronization must be performed between two systems, at the least.\n\nThere is a time parameter in the equations of quantum mechanics. The Schrödinger equation is\nOne solution can be\nwhere formula_25\nis called the time evolution operator, and \"H\" is the Hamiltonian.\n\nBut the Schrödinger picture shown above is equivalent to the Heisenberg picture, which enjoys a similarity to the Poisson brackets of classical mechanics. The Poisson brackets are superseded by a nonzero commutator, say [H,A] for observable A, and Hamiltonian H:\n\nThis equation denotes an uncertainty relation in quantum physics. For example, with \"time\" (the observable A), the \"energy\" E (from the Hamiltonian H) gives:\n\nThe more precisely one measures the duration of a sequence of events, the less precisely one can measure the energy associated with that sequence, and vice versa. This equation is different from the standard uncertainty principle, because time is not an operator in quantum mechanics.\n\nCorresponding commutator relations also hold for momentum \"p\" and position \"q\", which are conjugate variables of each other, along with a corresponding uncertainty principle in momentum and position, similar to the energy and time relation above.\n\nQuantum mechanics explains the properties of the periodic table of the elements. Starting with Otto Stern's and Walter Gerlach's experiment with molecular beams in a magnetic field, Isidor Rabi (1898–1988), was able to modulate the magnetic resonance of the beam. In 1945 Rabi then suggested that this technique be the basis of a clock using the resonant frequency of an atomic beam.\n\nSee dynamical systems and chaos theory, dissipative structures\n\nOne could say that time is a parameterization of a dynamical system that allows the geometry of the system to be manifested and operated on. It has been asserted that \"time is an implicit consequence of chaos\" (i.e. nonlinearity/irreversibility): the characteristic time, or rate of information entropy production, of a system. Mandelbrot introduces intrinsic time in his book \"Multifractals and 1/f noise\".\n\nSignalling is one application of the electromagnetic waves described above. In general, a signal is part of communication between parties and places. One example might be a yellow ribbon tied to a tree, or the ringing of a church bell. A signal can be part of a conversation, which involves a protocol. Another signal might be the position of the hour hand on a town clock or a railway station. An interested party might wish to view that clock, to learn the time. See: Time ball, an early form of Time signal.\n\nWe as observers can still signal different parties and places as long as we live within their \"past\" light cone. But we cannot receive signals from those parties and places outside our \"past\" light cone.\n\nAlong with the formulation of the equations for the electromagnetic wave, the field of telecommunication could be founded. \nIn 19th century telegraphy, electrical circuits, some spanning continents and oceans, could transmit codes - simple dots, dashes and spaces. From this, a series of technical issues have emerged; see . But it is safe to say that our signalling systems can be only approximately synchronized, a plesiochronous condition, from which jitter need be eliminated.\n\nThat said, systems \"can\" be synchronized (at an engineering approximation), using technologies like GPS. The GPS satellites must account for the effects of gravitation and other relativistic factors in their circuitry. See: Self-clocking signal.\n\nThe primary time standard in the U.S. is currently NIST-F1, a laser-cooled Cs fountain, the latest in a series of time and frequency standards, from the ammonia-based atomic clock (1949) to the caesium-based NBS-1 (1952) to NIST-7 (1993). The respective clock uncertainty declined from 10,000 nanoseconds per day to 0.5 nanoseconds per day in 5 decades. In 2001 the clock uncertainty for NIST-F1 was 0.1 nanoseconds/day. Development of increasingly accurate frequency standards is underway.\n\nIn this time and frequency standard, a population of caesium atoms is laser-cooled to temperatures of one microkelvin. The atoms collect in a ball shaped by six lasers, two for each spatial dimension, vertical (up/down), horizontal (left/right), and back/forth. The vertical lasers push the caesium ball through a microwave cavity. As the ball is cooled, the caesium population cools to its ground state and emits light at its natural frequency, stated in the definition of \"second\" above. Eleven physical effects are accounted for in the emissions from the caesium population, which are then controlled for in the NIST-F1 clock. These results are reported to BIPM.\n\nAdditionally, a reference hydrogen maser is also reported to BIPM as a frequency standard for TAI (international atomic time).\n\nThe measurement of time is overseen by BIPM (\"Bureau International des Poids et Mesures\"), located in Sèvres, France, which ensures uniformity of measurements and their traceability to the International System of Units (SI) worldwide. BIPM operates under authority of the Metre Convention, a diplomatic treaty between fifty-one nations, the Member States of the Convention, through a series of Consultative Committees, whose members are the respective national metrology laboratories.\n\nThe equations of general relativity predict a non-static universe. However, Einstein accepted only a static universe, and modified the Einstein field equation to reflect this by adding the cosmological constant, which he later described as the biggest mistake of his life. But in 1927, Georges Lemaître (1894–1966) argued, on the basis of general relativity, that the universe originated in a primordial explosion. At the fifth Solvay conference, that year, Einstein brushed him off with \"\" (“Your math is correct, but your physics is abominable”). In 1929, Edwin Hubble (1889–1953) announced his discovery of the expanding universe. The current generally accepted cosmological model, the Lambda-CDM model, has a positive cosmological constant and thus not only an expanding universe but an accelerating expanding universe.\n\nIf the universe were expanding, then it must have been much smaller and therefore hotter and denser in the past. George Gamow (1904–1968) hypothesized that the abundance of the elements in the Periodic Table of the Elements, might be accounted for by nuclear reactions in a hot dense universe. He was disputed by Fred Hoyle (1915–2001), who invented the term 'Big Bang' to disparage it. Fermi and others noted that this process would have stopped after only the light elements were created, and thus did not account for the abundance of heavier elements.\n\nGamow's prediction was a 5–10-kelvin black-body radiation temperature for the universe, after it cooled during the expansion. This was corroborated by Penzias and Wilson in 1965. Subsequent experiments arrived at a 2.7 kelvins temperature, corresponding to an age of the universe of 13.8 billion years after the Big Bang.\n\nThis dramatic result has raised issues: what happened between the singularity of the Big Bang and the Planck time, which, after all, is the smallest observable time. When might have time separated out from the spacetime foam; there are only hints based on broken symmetries (see Spontaneous symmetry breaking, Timeline of the Big Bang, and the articles in ).\n\nGeneral relativity gave us our modern notion of the expanding universe that started in the Big Bang. Using relativity and quantum theory we have been able to roughly reconstruct the history of the universe. In our epoch, during which electromagnetic waves can propagate without being disturbed by conductors or charges, we can see the stars, at great distances from us, in the night sky. (Before this epoch, there was a time, 300,000 years after the Big Bang, during which starlight would not have been visible.)\nIlya Prigogine's reprise is \"Time precedes existence\". In contrast to the views of Newton, of Einstein, and of quantum physics, which offer a symmetric view of time (as discussed above), Prigogine points out that statistical and thermodynamic physics can explain irreversible phenomena, as well as the arrow of time and the Big Bang.\n\n\n\n \n"}
{"id": "46618447", "url": "https://en.wikipedia.org/wiki?curid=46618447", "title": "Timeline of paleontology in Michigan", "text": "Timeline of paleontology in Michigan\n\nThis timeline of paleontology in Michigan is a chronologically ordered list events in the history of paleontological research occurring within or conducted by people from the U.S. state of Michigan.\n\n1839\n\n1877\n\n1903\n\n1914\n\n1923\n\n1925\n\n1927\n\n1930\n\n1940\n\n1949\n\n1953\n\n1961\n\n1962\n\n1963\n\n1964\n\n1965\n\n2002\n"}
{"id": "803380", "url": "https://en.wikipedia.org/wiki?curid=803380", "title": "Valeri Barsukov", "text": "Valeri Barsukov\n\nValeri Leonidovich Barsukov (Валерий Леонидович Барсуков) (March 14, 1928 – July 22, 1992) was a Soviet geologist. He worked in comparative planetology and the geochemistry of space. He was director of the V. I. Vernadsky Institute of Geochemistry from 1976 to 1992. In 1987 he received the V.I. Vernadsky Gold Medal for his work. A crater on Mars was named after him.\n"}
{"id": "48795925", "url": "https://en.wikipedia.org/wiki?curid=48795925", "title": "WISEP J190648.47+401106.8", "text": "WISEP J190648.47+401106.8\n\nWISEP J190648.47+401106.8 (shortened to W1906+40) is a L-dwarf star. In 2015 it was shown to have on its surface a storm the size of Jupiter's Great Red Spot. The storm rotates around the star roughly every 9 hours and has lasted since at least 2013, when observations of the storm began.\n\nThe star is 53 light-years from Earth, has an intrinsic brightness of 0.0002 that of the sun, a radius of 0.9 Jupiters, and a surface temperature of 2,311 K. The star emits significant flares.\n\nDistance 53.3 (+1.17, -1.11) light years.\n"}
{"id": "2368279", "url": "https://en.wikipedia.org/wiki?curid=2368279", "title": "XDrawChem", "text": "XDrawChem\n\nXDrawChem is a free software program for drawing chemical structural formulas, available for Unix and macOS. It is distributed under the GNU GPL. In Microsoft Windows this program is called WinDrawChem.\n\n\n\n"}
{"id": "7576622", "url": "https://en.wikipedia.org/wiki?curid=7576622", "title": "Zénobe Gramme", "text": "Zénobe Gramme\n\nZénobe Théophile Gramme (4 April 1826 – 20 January 1901) was a Belgian electrical engineer. He was born at Jehay-Bodegnée on 4 April 1826, the sixth child of Mathieu-Joseph Gramme, and died at Bois-Colombes on 20 January 1901. He invented the Gramme machine, a type of direct current dynamo capable of generating smoother (less AC) and much higher voltages than the dynamos known to that point.\n\nGramme was poorly educated and semi-literate throughout his life. His talent was in handicraft and when he left school became a joiner. After moving to Paris he took a job as a model maker at a company that manufactured electrical equipment and there became interested in technology.\n\nHaving built an improved dynamo Gramme, in association with Hippolyte Fontaine, opened a factory to develop the device. The business, called Société des Machines Magnéto-Électriques Gramme, manufactured the Gramme dynamo, Gramme ring, Gramme armature and other devices. In 1873 a Gramme dynamo was exhibited at the Vienna exhibition.\n\nHe was made an officer of the National Order of the Legion of Honour in 1877. In 1888 he was awarded the last of the valuable Volta Prizes by the French government.\n\nIn 1873 he and Hippolyte Fontaine accidentally discovered that the device was reversible and would spin when connected to any DC power supply. The Gramme machine was the first usefully powerful electrical motor that was successful industrially. Before Gramme's inventions, electric motors attained only low power and were mainly used as toys or laboratory curiosities.\n\nIn 1875, Nikola Tesla observed a Gramme machine at the Graz University of Technology. He conceived the idea of using it for alternating current but was unable to develop the idea at this time.\n\nIn 1857 he married Hortense Nysten who was a widow and mother of a daughter, Héloïse. Hortense died in 1890.\n\nGramme died at Bois-Colombes, France, on 20 January 1901 and was buried in Père Lachaise Cemetery.\n\nIn the city of Liège there is a graduate school of engineering, l'Institut Gramme, named after him.\n\nIn 2005 he ended at the 23rd place in the election of \"Le plus grand Belge\" (The Greatest Belgian), the television show broadcast by the French-speaking RTBF and based on the BBC show 100 Greatest Britons.\n\nA958 \"Zenobe Gramme\", (1961–), a sailing ship of the Belgian Navy used for training, is named after him.\n\n\n"}
