{"id": "55862876", "url": "https://en.wikipedia.org/wiki?curid=55862876", "title": "4OR", "text": "4OR\n\n4OR - A Quarterly Journal of Operations Research is a peer-reviewed scientific journal that was \nestablished in 2003 and is published by Springer Science+Business Media. \nIt is a joint official journal of the Belgian,\nFrench, and\nItalian Operations Research Societies. \nThe journal publishes research papers and surveys on the theory and applications of Operations Research.\nThe Editors-in-chief are Yves Crama (University of Liège), \nMichel Grabisch (Pantheon-Sorbonne University), and Silvano Martello (University of Bologna).\n\nThe journal is abstracted and indexed in the following databases:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.559.\n"}
{"id": "43198295", "url": "https://en.wikipedia.org/wiki?curid=43198295", "title": "Akhouri Sinha", "text": "Akhouri Sinha\n\nAkhouri Sinha is a professor in the Department of Genetics, Cell Biology and Development at the University of Minnesota. The United States has named a mountain in Antarctica in honour of Sinha, Mt Sinha. Mr Sinha's native place is Churamanpur, a village in the state of Bihar in the eastern part of India.\n"}
{"id": "197528", "url": "https://en.wikipedia.org/wiki?curid=197528", "title": "Anatoly Filipchenko", "text": "Anatoly Filipchenko\n\nAnatoly Vasilyevich Filipchenko (; born February 26, 1928) was a Soviet cosmonaut of Ukrainian descent. He flew on the Soyuz 7 and Soyuz 16 missions. He was born in Davydovka, Voronezh Oblast, RSFSR.\n\nAfter leaving the space programme in 1982 Filipchenko became the Deputy Director of the OKB in Kharkiv.\n\nHe was awarded:\n"}
{"id": "28913386", "url": "https://en.wikipedia.org/wiki?curid=28913386", "title": "Anna Glacier", "text": "Anna Glacier\n\nAnna Glacier () is a glacier flowing southeast between Rose Peak and Rea Peak, tributary to Polonia Glacier/Polonia Ice Piedmont at the head of King George Bay, King George Island. It was named by the Polish Antarctic Expedition (PAE), 1981, after Anna Tokarska, field assistant of PAE geological party to King George Island, 1979–80, and wife of Antoni K. Tokarski.\n\n"}
{"id": "16804983", "url": "https://en.wikipedia.org/wiki?curid=16804983", "title": "Aurelia (crater)", "text": "Aurelia (crater)\n\nAurelia is a crater on Venus. It has a large dark surface up range from the crater; lobate flows emanating from crater ejecta, and very radar-bright ejecta and floor.\n"}
{"id": "3506486", "url": "https://en.wikipedia.org/wiki?curid=3506486", "title": "Boris Rohdendorf", "text": "Boris Rohdendorf\n\nBoris Borissovich Rohdendorf (12 July 1904 – 21 November 1977) was a Russian \nentomologist and curator at the Zoological Museum at the University of Moscow. He attained the position of head of the Laboratory of Arthropods, Paleontological Institute of Russian Academy of Sciences, Academy of Sciences of the USSR (now Russian Academy of Sciences) in Moscow. A student of Andrey Martynov, he was a prolific taxonomist who described numerous new taxa, including fossil Diptera, and published important syntheses on fossil insects. His work is being extensively revised by the current generation of Russian paleoentomologists.\n\n\n\n"}
{"id": "566872", "url": "https://en.wikipedia.org/wiki?curid=566872", "title": "Brian Heap", "text": "Brian Heap\n\nSir (Robert) Brian Heap, (born 27 February 1935) is a biological scientist.\n\nHe was educated at New Mills Grammar School in the Peak District, Derbyshire, and the University of Nottingham (where he earned his BSc and PhD). He also has an MA and a ScD from the University of Cambridge and Honorary DScs from Nottingham (1994), York (2001) and St Andrews (2007).\n\n\nHeap's primary research interest was in reproductive biology and the function of hormones in reproduction. His research into the control of pregnancy, birth and lactation led to important contributions in endocrine physiology and farm animal breeding. He has published on endocrine physiology, biotechnology, sustainable consumption and production, and science advice for policy makers.\n\nHe was the Master of St Edmund's College, University of Cambridge from 1996 until 2004 and has been a Special Professor in Animal Physiology at the University of Nottingham since 1988 until 2016. He was elected a fellow of the Royal Society in 1989, and held the post of Royal Society Vice President and Foreign Secretary from 1996 to 2001. He was Executive Editor of the \"Philosophical Transactions of the Royal Society, Series B\" from 2004-2007. He is a founder member of the International Society for Science and Religion and an Associate of the Faraday Institute for Science and Religion.\n\nBrian Heap was President of the Institute of Biology (now Royal Society of Biology) 1996-1998, UK Representative on the European Science Foundation Strasbourg, 1994–97, a member of the Nuffield Council on Bioethics 1996-2001, UK Representative on the NATO Science Committee 1998-2005, member of the Scientific Advisory Panel for Emergency Responses (SAPER) at the Cabinet Office, Chairman of the Cambridge Genetics Knowledge Park and Public Health Genetics, 2002-2010, and President of the , 2010-2014. In 1994 he was awarded CBE, and in 2001 knighted for services to international science.\n\nOn 8 October 2007, HRH The Duke of Edinburgh opened three new buildings at St Edmund's College, Cambridge, one of which was named the \"Brian Heap Building\".\n\n"}
{"id": "28053739", "url": "https://en.wikipedia.org/wiki?curid=28053739", "title": "Chemical Watch", "text": "Chemical Watch\n\nChemical Watch is an online publisher which gathers the latest developments on chemical legislation around the world and summarises them for its subscribers to read. Its readership includes businesses, NGOs and government agencies.\n\nChemical Watch was established in 2007 in response to the EU Regulation on the Registration, Evaluation, Authorisation and Restriction of Chemicals (REACH) and wider regulatory developments such as the Globally Harmonised System of Classification and Labelling of Chemicals (GHS).\n\nIn October 2008, the editor of Chemical Watch was invited to chair a stakeholder open day on behalf of the European Chemicals Agency. Chemical Watch has also produced a daily newsletter for delegates at ChemCon events in Kuala Lumpur in 2009 and Prague in 2010.\n\nWhile engaging with the business, technical and legal communities, Chemical Watch also provides airspace to NGOs and politicians active in the chemicals arena.\n\n"}
{"id": "5626", "url": "https://en.wikipedia.org/wiki?curid=5626", "title": "Cognitive science", "text": "Cognitive science\n\nCognitive science is the interdisciplinary, scientific study of the mind and its processes. It examines the nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, neuroscience, and anthropology. The typical analysis of cognitive science spans many levels of organization, from learning and decision to logic and planning; from neural circuitry to modular brain organization. The fundamental concept of cognitive science is that \"thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures.\"\n\nSimply put: Cognitive Science is the interdisciplinary study of cognition in humans, animals, and machines. It encompasses the traditional disciplines of psychology, computer science, neuroscience, linguistics and philosophy. The goal of cognitive science is to understand the principles of intelligence with the hope that this will lead to better comprehension of the mind and of learning and to develop intelligent devices.\nThe cognitive sciences began as an intellectual movement in the 1950s often referred to as the cognitive revolution.\n\nA central tenet of cognitive science is that a complete understanding of the mind/brain cannot be attained by studying only a single level. An example would be the problem of remembering a phone number and recalling it later. One approach to understanding this process would be to study behavior through direct observation, or naturalistic observation. A person could be presented with a phone number and be asked to recall it after some delay of time. Then, the accuracy of the response could be measured. Another approach to measure cognitive ability would be to study the firings of individual neurons while a person is trying to remember the phone number. Neither of these experiments on its own would fully explain how the process of remembering a phone number works. Even if the technology to map out every neuron in the brain in real-time were available, and it were known when each neuron was firing, it would still be impossible to know how a particular firing of neurons translates into the observed behavior. Thus, an understanding of how these two levels relate to each other is imperative. \"The Embodied Mind: Cognitive Science and Human Experience\" says, “the new sciences of the mind need to enlarge their horizon to encompass both lived human experience and the possibilities for transformation inherent in human experience.” This can be provided by a functional level account of the process. Studying a particular phenomenon from multiple levels creates a better understanding of the processes that occur in the brain to give rise to a particular behavior.\nMarr gave a famous description of three levels of analysis:\n\n\nCognitive science is an interdisciplinary field with contributors from various fields, including psychology, neuroscience, linguistics, philosophy of mind, computer science, anthropology and biology. Cognitive scientists work collectively in hope of understanding the mind and its interactions with the surrounding world much like other sciences do. The field regards itself as compatible with the physical sciences and uses the scientific method as well as simulation or modeling, often comparing the output of models with aspects of human cognition. Similarly to the field of psychology, there is some doubt whether there is a unified cognitive science, which have led some researchers to prefer 'cognitive sciences' in plural.\n\nMany, but not all, who consider themselves cognitive scientists hold a functionalist view of the mind—the view that mental states and processes should be explained by their function - what they do. According to the multiple realizability account of functionalism, even non-human systems such as robots and computers can be ascribed as having cognition.\n\nThe term \"cognitive\" in \"cognitive science\" is used for \"any kind of mental operation or structure that can be studied in precise terms\" (Lakoff and Johnson, 1999). This conceptualization is very broad, and should not be confused with how \"cognitive\" is used in some traditions of analytic philosophy, where \"cognitive\" has to do only with formal rules and truth conditional semantics.\n\nThe earliest entries for the word \"cognitive\" in the OED take it to mean roughly \"pertaining to the action or process of knowing\". The first entry, from 1586, shows the word was at one time used in the context of discussions of Platonic theories of knowledge. Most in cognitive science, however, presumably do not believe their field is the study of anything as certain as the knowledge sought by Plato.\n\nCognitive science is a large field, and covers a wide array of topics on cognition. However, it should be recognized that cognitive science has not always been equally concerned with every topic that might bear relevance to the nature and operation of minds. Among philosophers, classical cognitivists have largely de-emphasized or avoided social and cultural factors, emotion, consciousness, animal cognition, and comparative and evolutionary psychologies. However, with the decline of behaviorism, internal states such as affects and emotions, as well as awareness and covert attention became approachable again. For example, situated and embodied cognition theories take into account the current state of the environment as well as the role of the body in cognition. With the newfound emphasis on information processing, observable behavior was no longer the hallmark of psychological theory, but the modeling or recording of mental states.\n\nBelow are some of the main topics that cognitive science is concerned with. This is not an exhaustive list. See List of cognitive science topics for a list of various aspects of the field.\n\nArtificial intelligence (AI) involves the study of cognitive phenomena in machines. One of the practical goals of AI is to implement aspects of human intelligence in computers. Computers are also widely used as a tool with which to study cognitive phenomena. Computational modeling uses simulations to study how human intelligence may be structured. (See .)\n\nThere is some debate in the field as to whether the mind is best viewed as a huge array of small but individually feeble elements (i.e. neurons), or as a collection of higher-level structures such as symbols, schemes, plans, and rules. The former view uses connectionism to study the mind, whereas the latter emphasizes symbolic computations. One way to view the issue is whether it is possible to accurately simulate a human brain on a computer without accurately simulating the neurons that make up the human brain.\n\nAttention is the selection of important information. The human mind is bombarded with millions of stimuli and it must have a way of deciding which of this information to process. Attention is sometimes seen as a spotlight, meaning one can only shine the light on a particular set of information. Experiments that support this metaphor include the dichotic listening task (Cherry, 1957) and studies of inattentional blindness (Mack and Rock, 1998). In the dichotic listening task, subjects are bombarded with two different messages, one in each ear, and told to focus on only one of the messages. At the end of the experiment, when asked about the content of the unattended message, subjects cannot report it.\n\nThe ability to learn and understand language is an extremely complex process. Language is acquired within the first few years of life, and all humans under normal circumstances are able to acquire language proficiently. A major driving force in the theoretical linguistic field is discovering the nature that language must have in the abstract in order to be learned in such a fashion. Some of the driving research questions in studying how the brain itself processes language include: (1) To what extent is linguistic knowledge innate or learned?, (2) Why is it more difficult for adults to acquire a second-language than it is for infants to acquire their first-language?, and (3) How are humans able to understand novel sentences?\n\nThe study of language processing ranges from the investigation of the sound patterns of speech to the meaning of words and whole sentences. Linguistics often divides language processing into orthography, phonetics, phonology, morphology, syntax, semantics, and pragmatics. Many aspects of language can be studied from each of these components and from their interaction.\n\nThe study of language processing in \"cognitive science\" is closely tied to the field of linguistics. Linguistics was traditionally studied as a part of the humanities, including studies of history, art and literature. In the last fifty years or so, more and more researchers have studied knowledge and use of language as a cognitive phenomenon, the main problems being how knowledge of language can be acquired and used, and what precisely it consists of. Linguists have found that, while humans form sentences in ways apparently governed by very complex systems, they are remarkably unaware of the rules that govern their own speech. Thus linguists must resort to indirect methods to determine what those rules might be, if indeed rules as such exist. In any event, if speech is indeed governed by rules, they appear to be opaque to any conscious consideration.\n\nLearning and development are the processes by which we acquire knowledge and information over time. Infants are born with little or no knowledge (depending on how knowledge is defined), yet they rapidly acquire the ability to use language, walk, and recognize people and objects. Research in learning and development aims to explain the mechanisms by which these processes might take place.\n\nA major question in the study of cognitive development is the extent to which certain abilities are innate or learned. This is often framed in terms of the nature and nurture debate. The nativist view emphasizes that certain features are innate to an organism and are determined by its genetic endowment. The empiricist view, on the other hand, emphasizes that certain abilities are learned from the environment. Although clearly both genetic and environmental input is needed for a child to develop normally, considerable debate remains about \"how\" genetic information might guide cognitive development. In the area of language acquisition, for example, some (such as Steven Pinker) have argued that specific information containing universal grammatical rules must be contained in the genes, whereas others (such as Jeffrey Elman and colleagues in Rethinking Innateness) have argued that Pinker's claims are biologically unrealistic. They argue that genes determine the architecture of a learning system, but that specific \"facts\" about how grammar works can only be learned as a result of experience.\n\nMemory allows us to store information for later retrieval. Memory is often thought of as consisting of both a long-term and short-term store. Long-term memory allows us to store information over prolonged periods (days, weeks, years). We do not yet know the practical limit of long-term memory capacity. Short-term memory allows us to store information over short time scales (seconds or minutes).\n\nMemory is also often grouped into declarative and procedural forms. Declarative memory—grouped into subsets of semantic and episodic forms of memory—refers to our memory for facts and specific knowledge, specific meanings, and specific experiences (e.g. \"Who was the first president of the U.S.A.?\", or \"What did I eat for breakfast four days ago?\"). Procedural memory allows us to remember actions and motor sequences (e.g. how to ride a bicycle) and is often dubbed implicit knowledge or memory .\n\nCognitive scientists study memory just as psychologists do, but tend to focus in more on how memory bears on cognitive processes, and the interrelationship between cognition and memory. One example of this could be, what mental processes does a person go through to retrieve a long-lost memory? Or, what differentiates between the cognitive process of recognition (seeing hints of something before remembering it, or memory in context) and recall (retrieving a memory, as in \"fill-in-the-blank\")?\n\nPerception is the ability to take in information via the senses, and process it in some way. Vision and hearing are two dominant senses that allow us to perceive the environment. Some questions in the study of visual perception, for example, include: (1) How are we able to recognize objects?, (2) Why do we perceive a continuous visual environment, even though we only see small bits of it at any one time? One tool for studying visual perception is by looking at how people process optical illusions. The image on the right of a Necker cube is an example of a bistable percept, that is, the cube can be interpreted as being oriented in two different directions.\n\nThe study of haptic (tactile), olfactory, and gustatory stimuli also fall into the domain of perception.\n\nAction is taken to refer to the output of a system. In humans, this is accomplished through motor responses. Spatial planning and movement, speech production, and complex motor movements are all aspects of action.\n\nConsciousness is the awareness whether something is an external object or something within oneself. \nThis helps the mind having the ability to experience or to feel a sense of self.\n\nMany different methodologies are used to study cognitive science. As the field is highly interdisciplinary, research often cuts across multiple areas of study, drawing on research methods from psychology, neuroscience, computer science and systems theory.\n\nIn order to have a description of what constitutes intelligent behavior, one must study behavior itself. This type of research is closely tied to that in cognitive psychology and psychophysics. By measuring behavioral responses to different stimuli, one can understand something about how those stimuli are processed. Lewandowski and Strohmetz (2009) review a collection of innovative uses of behavioral measurement in psychology including behavioral traces, behavioral observations, and behavioral choice. Behavioral traces are pieces of evidence that indicate behavior occurred, but the actor is not present (e.g., litter in a parking lot or readings on an electric meter). Behavioral observations involve the direct witnessing of the actor engaging in the behavior (e.g., watching how close a person sits next to another person). Behavioral choices are when a person selects between two or more options (e.g., voting behavior, choice of a punishment for another participant).\n\nBrain imaging involves analyzing activity within the brain while performing various tasks. This allows us to link behavior and brain function to help understand how information is processed. Different types of imaging techniques vary in their temporal (time-based) and spatial (location-based) resolution. Brain imaging is often used in cognitive neuroscience.\n\nComputational models require a mathematically and logically formal representation of a problem. Computer models are used in the simulation and experimental verification of different specific and general properties of intelligence. Computational modeling can help us understand the functional organization of a particular cognitive phenomenon.\nThere are two basic approaches to cognitive modeling. The first is focused on abstract mental functions of an intelligent mind and operates using symbols, and the second, which follows the neural and associative properties of the human brain, is called subsymbolic.\n\nAll the above approaches tend to be generalized to the form of integrated computational models of a synthetic/abstract intelligence, in order to be applied to the explanation and improvement of individual and social/organizational decision-making and reasoning.\n\nResearch methods borrowed directly from neuroscience and neuropsychology can also help us to understand aspects of intelligence. These methods allow us to understand how intelligent behavior is implemented in a physical system.\n\nCognitive science has given rise to models of human cognitive bias and risk perception, and has been influential in the development of behavioral finance, part of economics. It has also given rise to a new theory of the philosophy of mathematics, and many theories of artificial intelligence, persuasion and coercion. It has made its presence known in the philosophy of language and epistemology as well as constituting a substantial wing of modern linguistics. Fields of cognitive science have been influential in understanding the brain's particular functional systems (and functional deficits) ranging from speech production to auditory processing and visual perception. It has made progress in understanding how damage to particular areas of the brain affect cognition, and it has helped to uncover the root causes and results of specific dysfunction, such as dyslexia, anopia, and hemispatial neglect.\n\nThe cognitive sciences began as an intellectual movement in the 1950s, called the cognitive revolution. Cognitive science has a prehistory traceable back to ancient Greek philosophical texts (see Plato's Meno and Aristotle's De Anima); and includes writers such as Descartes, David Hume, Immanuel Kant, Benedict de Spinoza, Nicolas Malebranche, Pierre Cabanis, Leibniz and John Locke. However, although these early writers contributed greatly to the philosophical discovery of mind and this would ultimately lead to the development of psychology, they were working with an entirely different set of tools and core concepts than those of the cognitive scientist.\n\nThe modern culture of cognitive science can be traced back to the early cyberneticists in the 1930s and 1940s, such as Warren McCulloch and Walter Pitts, who sought to understand the organizing principles of the mind. McCulloch and Pitts developed the first variants of what are now known as artificial neural networks, models of computation inspired by the structure of biological neural networks.\n\nAnother precursor was the early development of the theory of computation and the digital computer in the 1940s and 1950s. Kurt Gödel, Alonzo Church, Alan Turing, and John von Neumann were instrumental in these developments. The modern computer, or Von Neumann machine, would play a central role in cognitive science, both as a metaphor for the mind, and as a tool for investigation.\n\nThe first instance of cognitive science experiments being done at an academic institution took place at MIT Sloan School of Management, established by J.C.R. Licklider working within the psychology department and conducting experiments using computer memory as models for human cognition.\n\nIn 1959, Noam Chomsky published a scathing review of B. F. Skinner's book \"Verbal Behavior\". At the time, Skinner's behaviorist paradigm dominated the field of psychology within the United States. Most psychologists focused on functional relations between stimulus and response, without positing internal representations. Chomsky argued that in order to explain language, we needed a theory like generative grammar, which not only attributed internal representations but characterized their underlying order.\n\nThe term \"cognitive science\" was coined by Christopher Longuet-Higgins in his 1973 commentary on the Lighthill report, which concerned the then-current state of Artificial Intelligence research. In the same decade, the journal \"Cognitive Science\" and the Cognitive Science Society were founded. The founding meeting of the Cognitive Science Society was held at the University of California, San Diego in 1979, which resulted in cognitive science becoming an internationally visible enterprise. In 1972, Hampshire College started the first undergraduate education program in Cognitive Science, led by Neil Stillings. In 1982, with assistance from Professor Stillings, Vassar College became the first institution in the world to grant an undergraduate degree in Cognitive Science. In 1986, the first Cognitive Science Department in the world was founded at the University of California, San Diego.\n\nIn the 1970s and early 1980s, as access to computers increased, artificial intelligence research expanded. Researchers such as Marvin Minsky would write computer programs in languages such as LISP to attempt to formally characterize the steps that human beings went through, for instance, in making decisions and solving problems, in the hope of better understanding human thought, and also in the hope of creating artificial minds. This approach is known as \"symbolic AI\".\n\nEventually the limits of the symbolic AI research program became apparent. For instance, it seemed to be unrealistic to comprehensively list human knowledge in a form usable by a symbolic computer program. The late 80s and 90s saw the rise of neural networks and connectionism as a research paradigm. Under this point of view, often attributed to James McClelland and David Rumelhart, the mind could be characterized as a set of complex associations, represented as a layered network. Critics argue that there are some phenomena which are better captured by symbolic models, and that connectionist models are often so complex as to have little explanatory power. Recently symbolic and connectionist models have been combined, making it possible to take advantage of both forms of explanation. While both connectionism and symbolic approaches have proven useful for testing various hypotheses and exploring approaches to understanding aspects of cognition and lower level brain functions, neither are biologically realistic and therefore, both suffer from a lack of neuroscientific plausibility. Connectionism has proven useful for exploring computationally how cognition emerges in development and occurs in the human brain, and has provided alternatives to strictly domain-specific / domain general approaches. For example, scientists such as Jeff Elman, Liz Bates, and Annette Karmiloff-Smith have posited that networks in the brain emerge from the dynamic interaction between them and environmental input.\n\nSome of the more recognized names in cognitive science are usually either the most controversial or the most cited. Within philosophy, some familiar names include Daniel Dennett, who writes from a computational systems perspective, John Searle, known for his controversial Chinese room argument, and Jerry Fodor, who advocates functionalism.\n\nOthers include David Chalmers, who advocates Dualism and is also known for articulating the hard problem of consciousness, and Douglas Hofstadter, famous for writing \"Gödel, Escher, Bach\", which questions the nature of words and thought.\n\nIn the realm of linguistics, Noam Chomsky and George Lakoff have been influential (both have also become notable as political commentators). In artificial intelligence, Marvin Minsky, Herbert A. Simon, and Allen Newell are prominent.\n\nPopular names in the discipline of psychology include George A. Miller, James McClelland, Philip Johnson-Laird, and Steven Pinker. Anthropologists Dan Sperber, Edwin Hutchins, and Scott Atran, have been involved in collaborative projects with cognitive and social psychologists, political scientists and evolutionary biologists in attempts to develop general theories of culture formation, religion, and political association.\n\nComputational theories (with models and simulations) have also been developed, by the likes of David Rumelhart, James McClelland, Philip Johnson-Laird, and so on.\n\nOther contributions have been made by Marvin Minsky and Noam Chomsky.\n\n\n\n\n"}
{"id": "1077155", "url": "https://en.wikipedia.org/wiki?curid=1077155", "title": "Compact Linear Collider", "text": "Compact Linear Collider\n\nThe Compact Linear Collider (CLIC) is a concept for a future linear particle accelerator that aims to explore the next energy frontier. CLIC would collide electrons with positrons and is currently the only mature option for a multi-TeV linear collider. The accelerator would be between long, more than ten times longer than the existing Stanford Linear Accelerator (SLAC) in California, USA. CLIC is proposed to be built at CERN, across the border between France and Switzerland near Geneva, with ﬁrst beams starting by the time the Large Hadron Collider (LHC) has ﬁnished operations around 2035.\n\nThe CLIC accelerator would use a novel two-beam acceleration technique at an acceleration gradient of 100 MeV/m, and its staged construction would provide collisions at three centre-of-mass energies up to 3 TeV for optimal physics reach. Cutting-edge research and development (R&D) are being carried out in the study to achieve the high precision physics goals under challenging beam and background conditions.\n\nCLIC aims to discover new physics beyond the Standard Model of particle physics, through precision measurement\"s\" of Standard Model properties as well as direct detection of new particles. The collider would offer superior sensitivity to electroweak states, exceeding the predicted precision of the full LHC programme. The current CLIC design includes the possibility for electron beam polarisation, further constraining the underlying physics.\n\nThe CLIC study produced a Conceptual Design Report (CDR) in 2012 and is working to present the case for the CLIC concept for the next Update of the European Strategy for Particle Physics in 2019-2020.\n\nThere are two main types of particle colliders, which differ in the types of particles they collide: lepton colliders and hadron colliders. Each type of collider can produce different ﬁnal states of particles and can study different physics phenomena. Examples of hadron colliders are ISR at CERN, SPS at CERN, Tevatron in United States, and the LHC at CERN. Examples of lepton colliders are SuperKEKB in Japan, BEPC II in China, DAFNE in Italy, VEPP in Russia, SLAC in United States, and Large Electron-Positron Collider at CERN. Some of these lepton colliders are still running.\n\nHadrons are compound objects, which leads to more complicated collision events and limits the achievable precision of physics measurements. Lepton colliders collide fundamental particles, therefore the initial state of each event is known and higher precision measurements can be achieved.\n\nCLIC is foreseen to be built and operated in three stages with different centre-of-mass energies: 380 GeV, 1.5 TeV, and 3 TeV. The luminosities at each stage are expected to be 500 fb, 1.5 ab, and 3 ab respectively, providing a broad physics programme over a 22-year period. These centre-of-mass energies have been motivated by current LHC data and physics potential studies carried out by the CLIC study.\n\nAlready at 380 GeV CLIC has good coverage of Standard Model physics; the energy stages beyond this allow for the discovery of new physics as well as increased precision of Standard Model processes. Additionally, CLIC will operate at the top quark pair-production threshold around 350 GeV with the aim of precisely measuring the properties of the top quark.\n\nThe CLIC linear collider would allow the exploration of new energy frontiers, provide possible solutions to unanswered problems, and enable the discovery of phenomena beyond our current understanding.\nThe current LHC data suggest that the particle found in 2012 is the Higgs boson as predicted by the Standard Model of particle physics. However, the LHC can only partially answer questions about the true nature of this particle, such as its composite/fundamental nature, coupling strengths, and possible role in an extended electroweak sector. CLIC could examine these questions in more depth by measuring the Higgs couplings to a precision not achieved before.<br> The 380 GeV stage of CLIC allows, for example, accurate model-independent measurements of Higgs boson couplings to fermions and bosons through the Higgsstrahlung and WW-fusion production processes. The second and third stages give access to phenomena such as the top-Yukawa coupling, rare Higgs decays and the Higgs self-coupling.\n\nThe top quark, the heaviest of all known fundamental particles, has currently never been studied in electron-positron collisions. The CLIC linear collider plans to have an extensive top quark physics programme. A major aim of this programme would be a threshold scan around the top quark pair-production threshold (~350 GeV) to precisely determine the mass and other signiﬁcant properties of the top quark. For this scan, CLIC currently plans to devote 15% of the running time of the ﬁrst stage, collecting 100 fb. This study would allow the top quark mass to be ascertained in a theoretically well-deﬁned manner and at a higher precision than possible with hadron colliders. CLIC would also aim to measure the top quark electroweak couplings to the Z boson and the photon, as deviations of these values from those predicted by the Standard Model could be evidence of new physics phenomena, such as extra dimensions. Further observation of top quark decays with ﬂavour-changing neutral currents at CLIC would be an indirect indication of new physics, as these should not be seen by CLIC under current Standard Model predictions.\n\nCLIC could discover new physics phenomena either through indirect measurements or by direct observation. Large deviations in precision measurements of particle properties from the Standard Model prediction would indirectly signal the presence of new physics. Such indirect methods give access to energy scales far beyond the available collision energy, reaching sensitivities of up to tens of TeV. Examples of indirect measurements CLIC would be capable of at 3 TeV are: the detection of a Z′ boson (reach up to ~30 TeV) indicating a simple gauge extension beyond the Standard Model; the Higgs composite scale Λ (reach up to ~70 TeV) to determine the elementary or composite nature of the Higgs boson; and vector boson scattering (sensitivity of the Effective Field Theory operators α4, α5 <0.001) giving insight into the mechanism of electroweak symmetry breaking.\n\nDirect pair production of particles up to a mass of 1.5 TeV, and single particle production up to a mass of 3 TeV is possible at CLIC. Due to the clean environment of electron-positron colliders, CLIC would be able to measure the properties of these potential new particles to a very high precision. Examples of particles CLIC could directly observe at 3 TeV are some of those proposed by the supersymmetry theory: charginos, neutralinos (both ~≤ 1.5 TeV), and sleptons (≤ 1.5 TeV).\n\nTo reach the desired 3 TeV beam energy, while keeping the length of the accelerator compact, CLIC targets an accelerating gradient up to 100 MV/m. CLIC is based on normal-conducting acceleration cavities operated at room temperature, as they allow for higher acceleration gradients than superconducting cavities. With this technology, the main limitation is the high-voltage breakdown rate (BDR), which follows the empirical law formula_1, where formula_2 is the accelerating gradient and formula_3 is the RF pulse length. The high accelerating gradient and the target BDR value (3 × 107 pulsem) drive most of the beam parameter\"s\" and machine design.\n\nIn order to reach these high accelerating gradients while keeping the power consumption affordable, CLIC makes use of a novel two-beam-acceleration scheme: a so-called Drive Beam runs parallel to the colliding Main Beam. The Drive Beam is decelerated in special devices called Power Extraction and Transfer Structures (PETS) that extract energy from the Drive Beam in the form of powerful Radio Frequency (RF) waves, which is then used to accelerate the Main Beam. Up to 90% of the energy of the Drive Beam is extracted and efﬁciently transferred to the Main Beam.\nThe electrons needed for the main beam are produced by illuminating a GaAs-type cathode with a Q-switched polarised laser, and are longitudinally polarised at the level of 80%. The positron\"s\" for the main beam are produced by sending a 5 GeV electron beam on a tungsten target. After an initial acceleration up to 2.86 GeV, both electrons and positrons enter damping rings for emittance reduction by radiation damping. Both beams are then further accelerated to 9 GeV in a common booster linac. Long transfer lines transport the two beams to the beginning of the main linacs where they are accelerated up to 1.5 TeV before going into the Beam Delivery System (BSD), which squeezes and brings the beams into collision. The two beams collide at the IP with 20 mrad crossing angle in the horizontal plane.\n\nEach Drive Beam complex is composed of a 2.5 km-long linac, followed by a Drive Beam Recombination Complex: a system of delay lines and combiner rings where the incoming beam pulses are interleaved to ultimately form a 12 GHz sequence and a local beam current as high as 100A. Each 2.5 km-long Drive Beam linac is powered by 1 GHz klystron\"s\". This produces a 148 µs-long beam (for the 1.5 TeV energy stage scenario) with a bunching frequency of 0.5 GHz. Every 244 ns the bunching phase is switched by 180 degrees, i.e. odd and even buckets at 1 GHz are ﬁlled alternately. This phase-coding allows the ﬁrst factor two recombination: the odd bunches are delayed in a Delay Loop (DL), while the even bunches bypass it. The time of flight of the DL is about 244 ns and tuned at the picosecond level such that the two trains of bunches can merge, forming several 244 ns-long trains with bunching frequency at 1 GHz, separated by 244 ns of empty space. This new time-structure allows for further factor 3 and factor 4 recombination in the following combiner rings with a similar mechanism as in the DL. The ﬁnal time structure of the beam is made of several (up to 24) 244 ns-long trains of bunches at 12 GHz, spaced by gaps of about 5.5 µs. The recombination is timed such that each combined train arrives in its own decelerator sector, synchronized with the arrival of the Main Beam. The use of low-frequency (1 GHz), long-pulse-length (148 µs) klystrons for accelerating the Drive Beam and the beam recombination makes it more convenient than using klystrons to directly accelerate the Main Beam.\n\nThe main technology challenges of the CLIC accelerator design have been successfully addressed in various test facilities. The Drive Beam production and recombination, and the two-beam acceleration concept were demonstrated at the CLIC Test Facility 3 (CTF3). X-band high-power klystron-based RF sources were built in stages at the high-gradient X-band test facility (XBOX), CERN, ). These facilities provide the RF power and infrastructure required for the conditioning and veriﬁcation of the performance of CLIC accelerating structures, and other X-band based projects. Additional X-band high-gradient tests are being carried out at the NEXTEF facility at KEK and at SLAC, a new test stand is being commissioned at Tsinghua University and further test stands are being constructed at INFN Frascati and SINAP in Shanghai.\n\nA state-of-the-art detector is essential to proﬁt from the complete physics potential of CLIC. The current detector design, named CLICdet, has been optimised via full simulation studies and R&D activities. The detector follows the standard design of grand particle detectors at high energy colliders: a cylindrical detector volume with a layered conﬁguration, surrounding the beam axis. CLICdet would have dimensions of ~13 x 12 m (height x length) and weigh ~8000 tonnes.\n\nCLICdet consists of four main layers of increasing radius: vertex and tracking system, calorimeters, solenoid magnet, muon detector.\n\nThe vertex and tracking system is located at the innermost region of CLICdet and aims to detect the position and momenta of particles with minimum adverse impact on their energy and trajectory. The vertex detector is cylindrical with three double layers of detector materials at increasing radii and has three segmented disks at each end in a spiral conﬁguration to aid air ﬂow cooling. These are assumed to be made of 25x25 µm2 silicon pixels of thickness 50 µm, and the aim is to have a single point resolution of 3 µm. The tracking system is made of silicon sensor modules expected to be 300 µm thick.\n\nThe calorimeters surround the vertex and tracking system and aim to measure the energy of particles via absorption. The electromagnetic calorimeter (ECAL) consists ~40 layers of silicon tungsten in a sandwich structure; the hadronic calorimeter (HCAL) has 60 steel absorber plates with scintillating material inserted in between.\n\nThese inner CLICdet layers are enclosed in a superconducting solenoid magnet with a ﬁeld strength of 4 T. This magnetic ﬁeld bends charged particles, allowing for momentum and charge measurements. The magnet is then surrounded by an iron yoke which would contain large area detectors for muon identiﬁcation.\n\nThe detector also has a luminosity calorimeter (LumiCal) to measure the products of Bhabha scattering events; a beam calorimeter to complete the ECAL coverage down to 10 mrads polar angle; and an intratrain feedback system to counteract luminosity loss due to correct for relative beam-beam offsets.\nStrict requirements on the material budget for the vertex and tracking system does not allow the use of conventional liquid cooling systems for CLICdet. Therefore, it is proposed that a dry gas cooling system will be used for this inner region. Air gaps have been factored into the design of the detector to allow the ﬂow of the gas, which will be air or Nitrogen. To allow for effective air cooling, the average power consumption of the Silicon sensors in the vertex detector needs to be lowered. Therefore, these sensors will operate via a current-based power pulsing scheme: switching the sensors from a high to low power consumption state whenever possible, corresponding to the 50 Hz bunch train crossing rate \n\nAs of 2017, approximately two percent of the CERN annual budget is invested in the development of CLIC technologies. The ﬁrst stage of CLIC with a length of around is currently estimated at a cost of six to seven billion CHF. CLIC is a global project of more than 70 institutes in more than 30 countries. It consists of two collaborations: the CLIC detector and physics collaboration (CLICdp), and the CLIC accelerator study. CLIC is currently in the development stage, conducting performance studies for accelerator parts and systems, detector technology and optimisation studies, and physics analysis. In parallel, the collaborations are working with the theory community to evaluate the physics potential of CLIC. The CLIC project has produced a Conceptual Design Report (CDR) in 2012 and is working to present the case for the CLIC concept for the next Update of the European Strategy for Particle Physics in 2019-2020.\n\n\n"}
{"id": "12324407", "url": "https://en.wikipedia.org/wiki?curid=12324407", "title": "Daiwa Adrian Prize", "text": "Daiwa Adrian Prize\n\nThis Daiwa Adrian Prize is an award given by The Daiwa Anglo-Japanese Foundation, a UK charity, to scientists who have made significant achievements in science through Anglo-Japanese collaborative research. Prizes are awarded every third year and applications are handled by the foundation with an assessment conducted by a panel of Fellows of The Royal Society. \n\nThe prize was initiated 1992 by Lord Adrian (2nd Baron Adrian), a former Trustee of the Foundation. The physiologist Richard Adrian was Master of Pembroke College, Vice-Chancellor of the University of Cambridge and the only son of the Nobel laureate Edgar Adrian (1st Baron Adrian).\n\nThe ceremony was held at the Royal Society on 26 November 2013 and was attended by Trustees of the Foundation including the Chairman, Sir Peter Williams, who is former Vice President of the Royal Society. The Prizes were presented by Lord Adrian's wife Lady Adrian.\n\nChemonostics: Using chemical receptors in the development of simple diagnostic devices for age-related diseases.\n\nCircadian regulation of photosynthesis: discovering mechanisms that connect the circadian clock with photosynthesis in chloroplasts in order to understand how circadian and environmental signals optimise photosynthesis and plant productivity.\n\nExploration of active functionality in abundant oxide materials utilising unique nanostructure: discovering novel properties in traditional materials and addressing the limited availability of technologically important elements through curiosity-driven research.\n\nExtension of terrestrial radiocarbon age calibration curve using annually laminated sediment core from Lake Suigetsu, Japan – establishing a reliable calibration for radiocarbon dates thus considerably improving the accuracy of the age determination.\n\nThe ceremony was held at the Royal Society on 2 December 2010 and was attended by Trustees of the Foundation including the then Chairman, Sir John Whitehead, and Sir Peter Williams. The Prizes were presented by Lord Adrian's wife Lady Adrian.\n\n"}
{"id": "56275884", "url": "https://en.wikipedia.org/wiki?curid=56275884", "title": "Data-driven control system", "text": "Data-driven control system\n\nData-driven control systems are a broad family of control systems, in which the identification of the process model and/or the design of the controller are based entirely on \"experimental data\" collected from the plant .\n\nIn many control applications, trying to write a mathematical model of the plant is considered a hard task, requiring efforts and time to the process and control engineers. This problem is overcome by \"data-driven\" methods, which allow to fit a system model to the experimental data collected, choosing it in a specific models class. The control engineer can then exploit this model to design a proper controller for the system. However, it is still difficult to find a simple yet reliable model for a physical system, that includes only those dynamics of the system that are of interest for the control specifications. The \"direct\" data-driven methods allow to tune a controller, belonging to a given class, without the need of an identified model of the system. In this way, one can also simply weight process dynamics of interest inside the control cost function, and exclude those dynamics that are out of interest.\n\nThe \"standard\" approach to control systems design is organized in two-steps: \nTypical objectives of system identification are to have formula_9 as close as possible to formula_7, and to have formula_6 as small as possible. However, from an identification for control perspective, what really matters is the performance achieved by the controller, not the intrinsic quality of the model.\n\nOne way to deal with uncertainty is to design a controller that has an acceptable performance with all models in formula_6, including formula_7. This is the main idea behind robust control design procedure, that aims at building frequency domain uncertainty descriptions of the process. However, being based on worst-case assumptions rather than on the idea of averaging out the noise, this approach typically leads to \"conservative\" uncertainty sets. Rather, data-driven techniques deal with uncertainty by working on experimental data, and avoiding excessive conservativism.\n\nIn the following, the main classifications of data-driven control systems are presented.\n\nThere are many methods available to control the systems. \nThe fundamental distinction is between indirect and direct controller design methods. The former group of techniques is still retaining the standard two-step approach, \"i.e.\" first a model is identified, then a controller is tuned based on such model. The main issue in doing so is that the controller is computed from the estimated model formula_9 (according to the certainty equivalence principle), but in practice formula_16. To overcome this problem, the idea behind the latter group of techniques is to map the experimental data \"directly\" onto the controller, without any model to be identified in between.\n\nAnother important distinction is between iterative and noniterative (or one-shot) methods. In the former group, repeated iterations are needed to estimate the controller parameters, during which the optimization problem is performed based on the results of the previous iteration, and the estimation is expected to become more and more accurate at each iteration. This approach is also prone to on-line implementations (see below). In the latter group, the (optimal) controller parametrization is provided with a single optimization problem. This is particularly important for those systems in which iterations or repetitions of data collection experiments are limited or even not allowed (for example, due to economic aspects). In such cases, one should select a design technique capable of delivering a controller on a single data set. This approach is often implemented off-line (see below).\n\nSince, on practical industrial applications, open-loop or closed-loop data are often available continuously, on-line data-driven techniques use those data to improve the quality of the identified model and/or the performance of the controller each time new information is collected on the plant. Instead, off-line approaches work on batch of data, which may be collected only once, or multiple times at a regular (but rather long) interval of time.\n\nThe iterative feedback tuning (IFT) method was introduced in 1994 , starting from the observation that, in identification for control, each iteration is based on the (wrong) certainty equivalence principle.\n\nIFT is a model-free technique for the direct iterative optimization of the parameters of a fixed-order controller; such parameters can be successively updated using information coming from standard (closed-loop) system operation.\n\nLet formula_17 be a desired output to the reference signal formula_18; the error between the achieved and desired response is formula_19. The control design objective can be formulated as the minimization of the objective function:\n\nGiven the objective function to minimize, the \"quasi-Newton method\" can be applied, i.e. a gradient-based minimization using a gradient search of the type:\n\nThe value formula_22 is the step size, formula_23 is an appropriate positive definite matrix and formula_24 is an approximation of the gradient; the true value of the gradient is given by the following:\n\nThe value of formula_26 is obtained through the following three-step methodology:\n\n\nA crucial factor for the convergence speed of the algorithm is the choice of formula_23; when formula_38 is small, a good choice is the approximation given by the Gauss–Newton direction:\n\nNoniterative correlation-based tuning (nCbT) is a noniterative method for data-driven tuning of a fixed-structure controller. It provides a one-shot method to directly synthesize a controller based on a single dataset.\n\nSuppose that formula_4 denotes an unknown LTI stable SISO plant, formula_41 a user-defined reference model and formula_42 a user-defined weighting function. An LTI fixed-order controller is indicated as formula_43, where formula_44, and formula_45 is a vector of LTI basis functions. Finally, formula_46 is an ideal LTI controller of any structure, guaranteeing a closed-loop function formula_41 when applied to formula_4.\n\nThe goal is to minimize the following objective function:\n\nformula_50 is a convex approximation of the objective function obtained from a model reference problem, supposing that formula_51.\n\nWhen formula_4 is stable and minimum-phase, the approximated model reference problem is equivalent to the minimization of the norm of formula_53 in the scheme in figure.\n\nThe input signal formula_54 is supposed to be a persistently exciting input signal and formula_55 to be generated by a stable data-generation mechanism. The two signals are thus uncorrelated in an open-loop experiment; hence, the ideal error formula_56 is uncorrelated with formula_54. The control objective thus consists in finding formula_58 such that formula_54 and formula_56 are uncorrelated.\n\nThe vector of \"instrumental variables\" formula_61 is defined as:\n\nwhere formula_63 is large enough and formula_64, where formula_65 is an appropriate filter.\n\nThe correlation function is:\n\nand the optimization problem becomes:\n\nDenoting with formula_68 the spectrum of formula_54, it can be demonstrated that, under some assumptions, if formula_65 is selected as:\n\nthen, the following holds:\n\nThere is no guarantee that the controller formula_73 that minimizes formula_74 is stable. Instability may occur in the following cases:\n\n\nConsider a stabilizing controller formula_81 and the closed loop transfer function formula_82.\nDefine:\n\n\nCondition 1. is enforced when:\n\n\nThe model reference design with stability constraint becomes:\n\nA convex data-driven estimation of formula_94 can be obtained through the discrete Fourier transform. \n\nDefine the following:\n\nFor stable minimum phase plants, the following convex data-driven oprimization problem is given:\n\nVirtual Reference Feedback Tuning (VRFT) is a noniterative method for data-driven tuning of a fixed-structure controller. It provides a one-shot method to directly synthesize a controller based on a single dataset.\n\nVRFT was first proposed as formula_97 and then fixed and extended for LTI and LPV systems.\n\nThe main idea is to define a desired closed loop model formula_41 and to use its inverse dynamics to obtain a virtual reference formula_99 from the measured output signal formula_100.\n\nThe virtual signals are formula_101 and formula_102\n\nThe optimal controller is obtained from noiseless data by solving the following optimization problem:\n\nwhere the optimization function is given as follows:\n"}
{"id": "172333", "url": "https://en.wikipedia.org/wiki?curid=172333", "title": "Dispersion (optics)", "text": "Dispersion (optics)\n\nIn optics, dispersion is the phenomenon in which the phase velocity of a wave depends on its frequency.\nMedia having this common property may be termed \"dispersive media\". Sometimes the term \"chromatic\" dispersion is used for specificity.\nAlthough the term is used in the field of optics to describe light and other electromagnetic waves, dispersion in the same sense can apply to any sort of wave motion such as acoustic dispersion in the case of sound and seismic waves, in gravity waves (ocean waves), and for telecommunication signals along transmission lines (such as coaxial cable) or optical fiber.\n\nIn optics, one important and familiar consequence of dispersion is the change in the angle of refraction of different colors of light, as seen in the spectrum produced by a dispersive prism and in chromatic aberration of lenses. Design of compound achromatic lenses, in which chromatic aberration is largely cancelled, uses a quantification of a glass's dispersion given by its Abbe number \"V\", where \"lower\" Abbe numbers correspond to \"greater\" dispersion over the visible spectrum. In some applications such as telecommunications, the absolute phase of a wave is often not important but only the propagation of wave packets or \"pulses\"; in that case one is interested only in variations of group velocity with frequency, so-called group-velocity dispersion\n\nThe most familiar example of dispersion is probably a rainbow, in which dispersion causes the spatial separation of a white light into components of different wavelengths (different colors). However, dispersion also has an effect in many other circumstances: for example, group velocity dispersion (GVD) causes pulses to spread in optical fibers, degrading signals over long distances; also, a cancellation between group-velocity dispersion and nonlinear effects leads to soliton waves.\n\nMost often, chromatic dispersion refers to bulk material dispersion, that is, the change in refractive index with optical frequency. However, in a waveguide there is also the phenomenon of \"waveguide dispersion\", in which case a wave's phase velocity in a structure depends on its frequency simply due to the structure's geometry. More generally, \"waveguide\" dispersion can occur for waves propagating through any inhomogeneous structure (e.g., a photonic crystal), whether or not the waves are confined to some region. In a waveguide, \"both\" types of dispersion will generally be present, although they are not strictly additive. For example, in fiber optics the material and waveguide dispersion can effectively cancel each other out to produce a zero-dispersion wavelength, important for fast fiber-optic communication.\n\nMaterial dispersion can be a desirable or undesirable effect in optical applications. The dispersion of light by glass prisms is used to construct spectrometers and spectroradiometers. Holographic gratings are also used, as they allow more accurate discrimination of wavelengths. However, in lenses, dispersion causes chromatic aberration, an undesired effect that may degrade images in microscopes, telescopes, and photographic objectives.\n\nThe \"phase velocity\", \"v\", of a wave in a given uniform medium is given by\nwhere \"c\" is the speed of light in a vacuum and \"n\" is the refractive index of the medium.\n\nIn general, the refractive index is some function of the frequency \"f\" of the light, thus \"n\" = \"n\"(\"f\"), or alternatively, with respect to the wave's wavelength \"n\" = \"n\"(\"λ\"). The wavelength dependence of a material's refractive index is usually quantified by its Abbe number or its coefficients in an empirical formula such as the Cauchy or Sellmeier equations.\n\nBecause of the Kramers–Kronig relations, the wavelength dependence of the real part of the refractive index is related to the material absorption, described by the imaginary part of the refractive index (also called the extinction coefficient). In particular, for non-magnetic materials (\"μ\" = \"μ\"), the susceptibility \"χ\" that appears in the Kramers–Kronig relations is the electric susceptibility \"χ\" = \"n\" − 1.\n\nThe most commonly seen consequence of dispersion in optics is the separation of white light into a color spectrum by a prism. From Snell's law it can be seen that the angle of refraction of light in a prism depends on the refractive index of the prism material. Since that refractive index varies with wavelength, it follows that the angle that the light is refracted by will also vary with wavelength, causing an angular separation of the colors known as \"angular dispersion\".\n\nFor visible light, refraction indices \"n\" of most transparent materials (e.g., air, glasses) decrease with increasing wavelength \"λ\":\n\nor alternatively:\n\nIn this case, the medium is said to have \"normal dispersion\". Whereas, if the index increases with increasing wavelength (which is typically the case in the ultraviolet), the medium is said to have \"anomalous dispersion\".\n\nAt the interface of such a material with air or vacuum (index of ~1), Snell's law predicts that light incident at an angle \"θ\" to the normal will be refracted at an angle arcsin(). Thus, blue light, with a higher refractive index, will be bent more strongly than red light, resulting in the well-known rainbow pattern.\n\nAnother consequence of dispersion manifests itself as a temporal effect. The formula \"v\" = calculates the \"phase velocity\" of a wave; this is the velocity at which the \"phase\" of any one frequency component of the wave will propagate. This is not the same as the \"group velocity\" of the wave, that is the rate at which changes in amplitude (known as the \"envelope\" of the wave) will propagate. For a homogeneous medium, the group velocity \"v\" is related to the phase velocity \"v\" by (here \"λ\" is the wavelength in vacuum, not in the medium):\n\nThe group velocity \"v\" is often thought of as the velocity at which energy or information is conveyed along the wave. In most cases this is true, and the group velocity can be thought of as the \"signal velocity\" of the waveform. In some unusual circumstances, called cases of anomalous dispersion, the rate of change of the index of refraction with respect to the wavelength changes sign (becoming negative), in which case it is possible for the group velocity to exceed the speed of light (\"v\" > \"c\"). Anomalous dispersion occurs, for instance, where the wavelength of the light is close to an absorption resonance of the medium. When the dispersion is anomalous, however, group velocity is no longer an indicator of signal velocity. Instead, a signal travels at the speed of the wavefront, which is \"c\" irrespective of the index of refraction. Recently, it has become possible to create gases in which the group velocity is not only larger than the speed of light, but even negative. In these cases, a pulse can appear to exit a medium before it enters. Even in these cases, however, a signal travels at, or less than, the speed of light, as demonstrated by Stenner, et al.\n\nThe group velocity itself is usually a function of the wave's frequency. This results in group velocity dispersion (GVD), which causes a short pulse of light to spread in time as a result of different frequency components of the pulse travelling at different velocities. GVD is often quantified as the \"group delay dispersion parameter\" (again, this formula is for a uniform medium only):\n\nIf \"D\" is greater than zero, the medium is said to have \"positive dispersion\" (anomalous dispersion). If \"D\" is less than zero, the medium has \"negative dispersion\" (normal dispersion). If a light pulse is propagated through a normally dispersive medium, the result is the shorter wavelength components travel slower than the longer wavelength components. The pulse therefore becomes \"positively chirped\", or \"up-chirped\", increasing in frequency with time. Conversely, if a pulse travels through an anomalously dispersive medium, high frequency components travel faster than the lower ones, and the pulse becomes \"negatively chirped\", or \"down-chirped\", decreasing in frequency with time.\n\nThe result of GVD, whether negative or positive, is ultimately temporal spreading of the pulse. This makes dispersion management extremely important in optical communications systems based on optical fiber, since if dispersion is too high, a group of pulses representing a bit-stream will spread in time and merge, rendering the bit-stream unintelligible. This limits the length of fiber that a signal can be sent down without regeneration. One possible answer to this problem is to send signals down the optical fibre at a wavelength where the GVD is zero (e.g., around 1.3–1.5 μm in silica fibres), so pulses at this wavelength suffer minimal spreading from dispersion. In practice, however, this approach causes more problems than it solves because zero GVD unacceptably amplifies other nonlinear effects (such as four wave mixing). Another possible option is to use soliton pulses in the regime of negative dispersion, a form of optical pulse which uses a nonlinear optical effect to self-maintain its shape. Solitons have the practical problem, however, that they require a certain power level to be maintained in the pulse for the nonlinear effect to be of the correct strength. Instead, the solution that is currently used in practice is to perform dispersion compensation, typically by matching the fiber with another fiber of opposite-sign dispersion so that the dispersion effects cancel; such compensation is ultimately limited by nonlinear effects such as self-phase modulation, which interact with dispersion to make it very difficult to undo.\n\nDispersion control is also important in lasers that produce short pulses. The overall dispersion of the optical resonator is a major factor in determining the duration of the pulses emitted by the laser. A pair of prisms can be arranged to produce net negative dispersion, which can be used to balance the usually positive dispersion of the laser medium. Diffraction gratings can also be used to produce dispersive effects; these are often used in high-power laser amplifier systems. Recently, an alternative to prisms and gratings has been developed: chirped mirrors. These dielectric mirrors are coated so that different wavelengths have different penetration lengths, and therefore different group delays. The coating layers can be tailored to achieve a net negative dispersion.\n\nWaveguides are highly dispersive due to their geometry (rather than just to their material composition). Optical fibers are a sort of waveguide for optical frequencies (light) widely used in modern telecommunications systems. The rate at which data can be transported on a single fiber is limited by pulse broadening due to chromatic dispersion among other phenomena.\n\nIn general, for a waveguide mode with an angular frequency \"ω\"(\"β\") at a propagation constant \"β\" (so that the electromagnetic fields in the propagation direction \"z\" oscillate proportional to \"e\"), the group-velocity dispersion parameter \"D\" is defined as:\n\nwhere \"λ\" =  is the vacuum wavelength and \"v\" =  is the group velocity. This formula generalizes the one in the previous section for homogeneous media, and includes both waveguide dispersion and material dispersion. The reason for defining the dispersion in this way is that |\"D\"| is the (asymptotic) temporal pulse spreading Δ\"t\" per unit bandwidth\nΔ\"λ\" per unit distance travelled, commonly reported in ps/nm/km for optical fibers.\n\nIn the case of multi-mode optical fibers, so-called modal dispersion will also lead to pulse broadening. Even in single-mode fibers, pulse broadening can occur as a result of polarization mode dispersion (since there are still two polarization modes). These are \"not\" examples of chromatic dispersion as they are not dependent on the wavelength or bandwidth of the pulses propagated.\n\nWhen a broad range of frequencies (a broad bandwidth) is present in a single wavepacket, such as in an ultrashort pulse or a chirped pulse or other forms of spread spectrum transmission, it may not be accurate to approximate the dispersion by a constant over the entire bandwidth, and more complex calculations are required to compute effects such as pulse spreading.\n\nIn particular, the dispersion parameter \"D\" defined above is obtained from only one derivative of the group velocity. Higher derivatives are known as \"higher-order dispersion\". These terms are simply a Taylor series expansion of the dispersion relation \"β\"(\"ω\") of the medium or waveguide around some particular frequency. Their effects can be computed via numerical evaluation of Fourier transforms of the waveform, via integration of higher-order slowly varying envelope approximations, by a split-step method (which can use the exact dispersion relation rather than a Taylor series), or by direct simulation of the full Maxwell's equations rather than an approximate envelope equation.\n\nIn the technical terminology of gemology, \"dispersion\" is the difference in the refractive index of a material at the B and G (686.7 nm and 430.8 nm) or C and F (656.3 nm and 486.1 nm) Fraunhofer wavelengths, and is meant to express the degree to which a prism cut from the gemstone demonstrates \"fire\". Fire is a colloquial term used by gemologists to describe a gemstone's dispersive nature or lack thereof. Dispersion is a material property. The amount of fire demonstrated by a given gemstone is a function of the gemstone's facet angles, the polish quality, the lighting environment, the material's refractive index, the saturation of color, and the orientation of the viewer relative to the gemstone.\n\nIn photographic and microscopic lenses, dispersion causes chromatic aberration, which causes the different colors in the image not to overlap properly. Various techniques have been developed to counteract this, such as the use of achromats, multielement lenses with glasses of different dispersion. They are constructed in such a way that the chromatic aberrations of the different parts cancel out.\n\nPulsars are spinning neutron stars that emit pulses at very regular intervals ranging from milliseconds to seconds. Astronomers believe that the pulses are emitted simultaneously over a wide range of frequencies. However, as observed on Earth, the components of each pulse emitted at higher radio frequencies arrive before those emitted at lower frequencies. This dispersion occurs because of the ionized component of the interstellar medium, mainly the free electrons, which make the group velocity frequency dependent. The extra delay added at a frequency is\n\nwhere the dispersion constant \"k\" is given by\n\nand the dispersion measure (DM) is the column density of free electrons (total electron content) — i.e. the number density of electrons \"n\" (electrons/cm) integrated along the path traveled by the photon from the pulsar to the Earth — and is given by\n\nwith units of parsecs per cubic centimetre (1 pc/cm = 30.857 × 10 m).\n\nTypically for astronomical observations, this delay cannot be measured directly, since the emission time is unknown. What \"can\" be measured is the difference in arrival times at two different frequencies. The delay Δ\"t\" between a high frequency and a low frequency component of a pulse will be\n\nRewriting the above equation in terms of Δ\"t\" allows one to determine the DM by measuring pulse arrival times at multiple frequencies. This in turn can be used to study the interstellar medium, as well as allow for observations of pulsars at different frequencies to be combined.\n\n"}
{"id": "44759283", "url": "https://en.wikipedia.org/wiki?curid=44759283", "title": "Domesticated animal nomenclature chart", "text": "Domesticated animal nomenclature chart\n\nDomesticated animal nomenclature chart\n"}
{"id": "19379318", "url": "https://en.wikipedia.org/wiki?curid=19379318", "title": "Dry thunderstorm", "text": "Dry thunderstorm\n\nA dry thunderstorm or heat storm is a thunderstorm that produces thunder and lightning, but most or all of its precipitation evaporates before reaching the ground. Dry lightning refers to lightning strikes occurring in this situation. Both are so common in the American West that they are sometimes used interchangeably. The latter term is a technical misnomer since lightning itself is neither wet nor dry.\n\nDry thunderstorms occur essentially in dry conditions, and their lightning is a major cause of wildfires. Because of that, the National Weather Service, and other agencies around the world, issue forecasts for its likelihood over large areas.\n\nDry thunderstorms generally occur in deserts or places where the lower layers of the atmosphere usually contain little water vapor. Any precipitation that falls from elevated thunderstorms can be entirely evaporated as it falls through the lower dry layers. They are common during the summer months across much of western North America and other arid areas. The shaft of precipitation that can be seen falling from a cloud without reaching the ground is called \"virga\".\n\nA thunderstorm does not have to be completely dry to be considered dry; in many areas is the threshold between a \"wet\" and \"dry\" thunderstorm.\n\nDry thunderstorms are notable for two reasons: they are the most common natural origin of wildland fires, and they can produce strong gusty surface winds that can fan flames.\n\nStrong winds often develop around dry thunderstorms as the evaporating precipitation causes excessive cooling of the air beneath the storm, which increases its density and thereby its weight relative to the surrounding air. This cool air then descends rapidly and fans out upon impacting the ground, an event often described as a dry microburst. As the gusty winds expand outward from the storm, dry soil and sand are often picked up by the strong winds, creating dust and sand storms known as haboobs.\n\nIn areas where trees or other vegetation are present, there is little to no rain that can prevent the lightning from causing them to catch fire. Storm winds also fan the fire and firestorm, causing it to spread more quickly.\n\nPyrocumulonimbus are cumuliform clouds that can form over a large fire and that are particularly dry. When the higher levels of the atmosphere are cooler, and the surface is thus warmed to extreme temperatures due to a wildfire, volcano, or other event, convection will occur, and produce clouds and lightning. They are similar to any cumulus cloud but ingest extra particulates from the fire. This increases the voltage difference between the base and the top of the cloud, helping to produce lightning. \n"}
{"id": "5792578", "url": "https://en.wikipedia.org/wiki?curid=5792578", "title": "Dublin Accord", "text": "Dublin Accord\n\nThe Dublin Accord is an agreement for the international recognition of \"Engineering Technician\" qualifications.\nIn May 2002, the national engineering organisations of Ireland, the United Kingdom, South Africa and Canada signed an agreement mutually recognising the qualifications which underpin the granting of Engineering Technician titles in the four countries. Operation of the Dublin Accord is similar as for the Washington Accord and Sydney Accord.\n\nEach signatory has full rights of participation in the Accord.\n\n\n"}
{"id": "49528672", "url": "https://en.wikipedia.org/wiki?curid=49528672", "title": "Duplex sequencing", "text": "Duplex sequencing\n\nDuplex sequencing is a library preparation and analysis method for next-generation sequencing (NGS) platforms that employs random tagging of double stranded DNA to detect mutations with higher accuracy and lower error rate. This method uses degenerate molecular tags in addition to sequencing adapters to recognize reads originating from each strand of DNA. The generated sequencing reads then will be analyzed using two methods: single strand consensus sequences (SSCSs) and Duplex consensus sequences (DCSs) assembly. Duplex sequencing theoretically can detect mutations with frequencies as low as 5 x 10 that is more than 10,000 fold higher in accuracy compared to the conventional next-generation sequencing methods.\n\nThe estimated error rate of standard next-generation sequencing platforms is 10 - 10 per base call. With this error rate billions of base calls that are produced by NGS will results in millions of the errors. The errors are introduced during sample preparation and sequencing such as polymerase , sequencing and image analysis errors. While the NGS platforms error rate is admissible to some applications such as detection of clonal variants, it is a major limit for applications that require higher accuracy for detection of low frequency variants such as detection of intra-organismal mosaicism, subclonal variants in genetically heterogeneous cancers or circulating tumor DNA.\n\nSeveral library preparation strategies have been developed that increase accuracy of NGS platforms such as molecular barcoding and circular consensus sequencing method. The data generated by these methods the same as NGS platforms originate from single strand of DNA and therefore the errors that are introduced during PCR amplification, tissue processing, DNA extraction, hybridization-capture (where used) or DNA sequencing itself can still be distinguished as a true variant. The duplex sequencing method addresses this problem by taking advantage of complementary nature of two strands of DNA and confirming only variants that are present in both strands of DNA. Because the probability of two complementary errors arising at the same exact same location in both strands is exceedingly low, duplex sequencing increases the accuracy of sequencing significantly.\n\nDuplex sequencing tagged adapters can be used in combination with majority of NGS adapters. In the figures and workflow section of this article Illumina sequencing adapters are used as an example in accordance to the original published protocol.\nTwo oligonucleotides are used for this step (Figure 1: Adapter oligos). One of the oligonucleotides contains a 12 nucleotide single stranded random tag sequence followed by a fixed 5' nucleotide sequence (Black sequence in figure 1). In this step oligonucleotides are annealed in a complementary region by incubation at the required temporal condition.\n\nThe adapters that annealed successfully are extended and synthesized by a DNA polymerase to complete a double stranded adapter containing complementary tags (Figure 1).\n\nThe extended double stranded adapters are cleaved by HpyCH4III at a specific located at 3’ side of the tag sequence and will results in a 3’-dT overhang that will be ligated to the 3’-dA overhang on DNA libraries in adapter ligation step (Figure 1).\n\nDouble stranded DNA is sheared using one of the methods: Sonication, enzymatic digestion or nebulization. Fragments are size selected using Ampure XP beads. Gel-based size selection is not recommended for this method since it can cause melting of DNA double strands and DNA damage as the results of UV exposure. The size selected fragments of DNA are subjected to 3’-end-dA-tailing.\n\nIn this step two tagged adapters are ligated from 3’-dT-tails to 3’-dA-tails on both sides of double stranded DNA library fragments. This process results in double stranded library fragments that contain two random tags (α and β) in each side that are reverse complement of each other (Figure 1 and 2). The \"DNA:adapter\" ratio is crucial in determining the success of ligation.\n\nIn the last step of duplex sequencing library preparation, Illumina sequencing adapters are added to the tagged double stranded libraries by PCR amplification using primers containing sequencing adapters. During PCR amplification both complementary strands of DNA are amplified and generate two types of PCR products. Product 1 derive from strand 1 which have a unique tag sequence (called α in the figure 2) next to the Illumina adapter 1 and product 2 that have a unique tag (called β in the figure 2) next to illumina adapter 1. (Please note that in each strand, tag α is the reverse complement of tag β and vice versa). The libraries containing duplex tags and Illumina adapters are sequenced using Illumina TruSeq system. Reads that are originating from each single strand of DNA form a group of reads (tag families) that are sharing the same tag. The detected families of reads will be used in next step for analyzing sequencing data.\n\nAdapter ligation efficiency is very important in a successful duplex sequencing. Extra amount of libraries or adapters can affect the DNA:adapter balance and therefore result in inefficient ligation and excess amount of primer dimers, respectively. Therefore, it is important to keep the molar concentration of DNA:adapter to the optimal ratio that is 0.05.\n\nEfficiency of duplex sequencing depends on final number of DCSs which is directly related to number of reads in each family (family size). If the family size is too small then the DCS can not be assembled and if too many reads are sharing the same tag the data yield will be low. Family size is determined by the amount of DNA template for PCR amplification and dedicated sequencing lane fraction. The optimal tag family size is between 6 and 12 members. To obtain the optimal family size the amount of DNA template and dedicated sequencing lane fraction needs to be adjusted. The following formula takes into account the most important variables that can affect depth of coverage (N=40DG÷R) where \"N\" is number of reads, \"D\" is desired depth of coverage, \"G\" is size of DNA target in basepair and \"R\" is final read length.\n\nEach duplex sequencing read contains a fixed 5-nucleotide sequence (shown in figures in Black color) located upstream of the 12-nucleotide tag sequence. The reads are filtered out if they do not have the expected 5-nucleotide sequence or have more than nine identical or ambiguous bases within each tag. The two 12-nucleotide tags at each end of reads are combined and moved to the read header. Two families of reads are formed that originate from the two strands of DNA. One family contains reads with αβ header originating from strand 1 and the second family contains reads with βα header originating from strand 2 (Figure 2). Then the reads are trimmed by removing the fixed 5 bp sequence and 4 error prone nucleotides located at the sites of ligation and end repair. The remaining reads are assembled to consensus sequences using single strand consensus sequences (SSCSs) assembly and duplex consensus sequences (DCSs) assembly.\n\nTrimmed sequences from the previous step are aligned to the reference genome using Burrows-Wheeler aligner (BWA) and the unmapped reads are removed. The aligned reads that have the same 24 bp tag sequence and genomic region are detected and grouped together (Family αβ and βα in the figure 2). Each group represents a “tag family”. Tag families with lower than three members are removed from the analysis. To remove errors arise during PCR amplification or sequencing, mutations that are supported by less than 70% of the members (reads) are filtered out from the analysis. Then a consensus sequence is generated for each family using the identical sequences in each position of the remaining reads. The consensus sequence is called single strand consensus sequence (SSCS). The SSCS method increases the NGS accuracy to about 20 fold higher, however this method relies on the sequencing information from single strands of DNA and therefore is sensitive to the errors induced at the first round or before PCR amplification.\n\nThe reads from last step are realigned to the reference genome. In this method SSCS family pairs that have complementary tags will be grouped together (Family αβ and βα in figure 2). These reads originate from two complementary strands of DNA. High confidence sequences are selected based on the perfectly matched base calls of each family. The final sequence is called duplex consensus sequence (DCS). True mutations are those that match perfectly between complementary SSCSs. This step filter out remaining errors that raised during first round of PCR amplification or during sample preparation.\n\nHigh error rate (0.01-0.001) of standard NGS platforms that introduced during sample preparation or sequencing is a major limitation for detection of variants present in small fraction of cells. Due to the duplex tagging system and use of information in both strands of DNA, duplex sequencing has significantly decreased the error rate of sequencing about 10 million fold using both SSCS and DCS method.\n\nIt is challenging to identify rare variants accurately using standard NGS methods with the mutations rate of (10 - 10). Errors that happen early during sample preparation can be detected as rare variants. An example of such errors is C>A/G>T transversion that is detected in low frequencies using deep sequencing or targeted capture data and arise as the result of DNA oxidation during sample preparation. These types of false positive variants are filter out by duplex sequencing method since mutations need to be accurately matched in both strands of DNA to be validated as true mutations. Duplex sequencing can theoretically detect mutations with frequencies as low as 10 compare to 10 rate of standard NGS methods.\n\nAnother advantage of duplex sequencing is that it can be used in combination with majority of NGS platforms without making significant changes to the standard protocols.\n\nBecause duplex sequencing provide a significantly higher sequencing accuracy and uses information in both strands of DNA, this method needs a much higher sequencing depth and therefore is a costly approach. The high cost of duplex sequencing limits its application to targeted and amplicon sequencing at present time and will not be applicable for whole genome sequencing approaches. However, with decreasing cost of NGS, the application of duplex sequencing for larger DNA targets will be more feasible.\n\nDuplex sequencing is a new method and its efficiency was studied in limited applications such as detecting point mutations using targeted capture sequencing. More studies need to be performed to expand application and feasibility of duplex sequencing to more complex samples with large number of mutations, indels and copy number variations.\n\nDuplex sequencing and the significant increase of sequencing accuracy has important impacts on applications such as detection of rare human genetic variants, detection of subclonal mutations involve in mechanisms of resistance to therapy in genetically heterogeneous cancers, screening variants in circulating tumor DNA as a non-invasive biomarker and prenatal screening for detection of genetic abnormalities in fetus.\n\nAnother suggested application for duplex sequencing is detection of DNA/RNA copy numbers by estimating the relative frequency of variants. A method for counting PCR template molecules with application to next-generation sequencing.\n\nA list of required tools and packages for SSCS and DCS analysis can be found in software package.\n\n"}
{"id": "171414", "url": "https://en.wikipedia.org/wiki?curid=171414", "title": "Engineering drawing", "text": "Engineering drawing\n\nAn engineering drawing, a type of technical drawing, is used to fully and clearly define requirements for engineered items.\n\nEngineering drawing (the activity) produces engineering drawings (the documents). More than merely the drawing of pictures, it is also a language—a graphical language that communicates ideas and information from one mind to another.\n\nEngineering drawing and artistic types of drawing, and either may be called simply \"drawing\" when the context is implicit. Engineering drawing shares some traits with artistic drawing in that both create pictures. But whereas the purpose of artistic drawing is to convey emotion or artistic sensitivity in some way (subjective impressions), the purpose of engineering drawing is to convey information (objective facts). One of the corollaries that follow from this fact is that, whereas anyone can appreciate artistic drawing (even if each viewer has his own unique appreciation), engineering drawing requires some training to understand (like any language); but there is also a high degree of objective commonality in the interpretation (also like other languages). In fact, engineering drawing has evolved into a language that is more precise and unambiguous than natural languages; in this sense it is closer to a programming language in its communication ability. Engineering drawing uses an extensive set of conventions to convey information very precisely, with very little ambiguity.\n\nThe process of producing engineering drawings, and the skill of producing those, is often referred to as technical drawing or drafting (draughting) although technical drawings are also required for disciplines that would not ordinarily be thought of as parts of engineering (such as architecture, landscaping, cabinet making, and garment-making).\n\nPersons employed in the trade of producing engineering drawings were called draftsmen (or draughtsmen) in the past. Although these terms are still in use, the not\n-gender-specific terms draftsperson and drafter are now more common.\n\nThe various fields share many common conventions of drawing, while also having some field-specific conventions. For example, even within metalworking, there are some process-specific conventions to be learned—casting, machining, fabricating, and assembly all have some special drawing conventions, and within fabrication there is further division, including welding, riveting, pipefitting, and erecting. Each of these trades has some details that only specialists will have memorized.\n\nAn engineering drawing is a legal document (that is, a legal instrument), because it communicates all the needed information about \"what is wanted\" to the people who will expend resources turning the idea into a reality. It is thus a part of a contract; the purchase order and the drawing together, as well as any ancillary documents (engineering change orders [ECOs], called-out specs), constitute the contract. Thus, if the resulting product is wrong, the worker or manufacturer are protected from liability as long as they have faithfully executed the instructions conveyed by the drawing. If those instructions were wrong, it is the fault of the engineer. Because manufacturing and construction are typically very expensive processes (involving large amounts of capital and payroll), the question of liability for errors has great legal implications as each party tries to blame the other and assign the wasted cost to the other's responsibility. This is the biggest reason why the conventions of engineering drawing have evolved over the decades toward a very precise, unambiguous state.\n\nEngineering drawings specify requirements of a component or assembly which can be complicated. Standards provide rules for their specification and interpretation. Standardization also aids internationalization, because people from different countries who speak different languages can read the same engineering drawing, and interpret it the same way. \n\nOne major set of engineering drawing standards is ASME Y14.5 and Y14.5M (most recently revised in 2009). These apply widely in the United States, although ISO 8015 (Geometrical product specifications (GPS) — Fundamentals — Concepts, principles and rules) is now also important. \n\nIn 2011, a new revision of ISO 8015 (Geometrical product specifications (GPS) — Fundamentals — Concepts, principles and rules) was published containing the Invocation Principle. This states that, \"Once a portion of the ISO geometric product specification (GPS) system is invoked in a mechanical engineering product documentation, the entire ISO GPS system is invoked.\" It also goes on to state that marking a drawing \"Tolerancing ISO 8015\" is optional. The implication of this is that any drawing using ISO symbols can only be interpreted to ISO GPS rules. The only way not to invoke the ISO GPS system is to invoke a national or other standard. \n\nIn Britain, BS 8888 (Technical Product Specification) has undergone important updates in the 2010s.\n\nFor centuries, until the post-World War II era, all engineering drawing was done manually by using pencil and pen on paper or other substrate (e.g., vellum, mylar). Since the advent of computer-aided design (CAD), engineering drawing has been done more and more in the electronic medium with each passing decade. Today most engineering drawing is done with CAD, but pencil and paper have not entirely disappeared.\n\nSome of the tools of manual drafting include pencils, pens and their ink, straightedges, T-squares, French curves, triangles, rulers, protractors, dividers, compasses, scales, erasers, and tacks or push pins. (Slide rules used to number among the supplies, too, but nowadays even manual drafting, when it occurs, benefits from a pocket calculator or its onscreen equivalent.) And of course the tools also include drawing boards (drafting boards) or tables. The English idiom \"to go back to the drawing board\", which is a figurative phrase meaning to rethink something altogether, was inspired by the literal act of discovering design errors during production and returning to a drawing board to revise the engineering drawing. Drafting machines are devices that aid manual drafting by combining drawing boards, straightedges, pantographs, and other tools into one integrated drawing environment. CAD provides their virtual equivalents.\n\nProducing drawings usually involves creating an original that is then reproduced, generating multiple copies to be distributed to the shop floor, vendors, company archives, and so on. The classic reproduction methods involved blue and white appearances (whether white-on-blue or blue-on-white), which is why engineering drawings were long called, and even today are still often called, \"blueprints\" or \"bluelines\", even though those terms are anachronistic from a literal perspective, since most copies of engineering drawings today are made by more modern methods (often inkjet or laser printing) that yield black or multicolour lines on white paper. The more generic term \"print\" is now in common usage in the U.S. to mean any paper copy of an engineering drawing. In the case of CAD drawings, the original is the CAD file, and the printouts of that file are the \"prints\".\n\nFor centuries, engineering drawing was the sole method of transferring information from design into manufacture. In recent decades another method has arisen, called model-based definition (MBD) or digital product definition (DPD). In MBD, the information captured by the CAD software app is fed automatically into a CAM app (computer-aided manufacturing), which (with or without postprocessing apps) creates code in other languages such as G-code to be executed by a CNC machine tool (computer numerical control), 3D printer, or (increasingly) a hybrid machine tool that uses both. Thus today it is often the case that the information travels from the mind of the designer into the manufactured component without having ever been codified by an engineering drawing. In MBD, the dataset, not a drawing, is the legal instrument. The term \"technical data package\" (TDP) is now used to refer to the \"complete package of information\" (in one medium or another) that communicates information from design to production (such as 3D-model datasets, engineering drawings, engineering change orders (ECOs), spec revisions and addenda, and so on). However, even in the MBD era, where theoretically production could happen without any drawings or humans at all, it is still the case that drawings and humans are involved. It still takes CAD/CAM programmers, CNC setup workers, and CNC operators to do manufacturing, as well as other people such as quality assurance staff (inspectors) and logistics staff (for materials handling, shipping-and-receiving, and front office functions). These workers often use drawings in the course of their work that have been produced by rendering and plotting (printing) from the MBD dataset. \nWhen proper procedures are being followed, a clear chain of precedence is always documented, such that when a person looks at a drawing, s/he is told by a note thereon that this drawing is not the governing instrument (because the MBD dataset is). In these cases, the drawing is still a useful document, although legally it is classified as \"for reference only\", meaning that if any controversies or discrepancies arise, it is the MBD dataset, not the drawing, that governs.\n\nAlmost all engineering drawings (except perhaps reference-only views or initial sketches) communicate not only geometry (shape and location) but also dimensions and tolerances for those characteristics. Several systems of dimensioning and tolerancing have evolved. The simplest dimensioning system just specifies distances between points (such as an object's length or width, or hole center locations). Since the advent of well-developed interchangeable manufacture, these distances have been accompanied by tolerances of the plus-or-minus or min-and-max-limit types. \"Coordinate dimensioning\" involves defining all points, lines, planes, and profiles in terms of Cartesian coordinates, with a common origin. Coordinate dimensioning was the sole best option until the post-World War II era saw the development of geometric dimensioning and tolerancing (GD&T), which departs from the limitations of coordinate dimensioning (e.g., rectangular-only tolerance zones, tolerance stacking) to allow the most logical tolerancing of both geometry and dimensions (that is, both form [shapes/locations] and sizes).\n\nDrawings convey the following critical information:\n\n\nA variety of line styles graphically represent physical objects. Types of \"lines\" include the following:\n\nLines can also be classified by a letter classification in which each line is given a letter.\n\nIn most cases, a single view is not sufficient to show all necessary features, and several views are used. Types of \"views\" include the following:\n\nA \"multiview projection\" is a type of orthographic projection that shows the object as it looks from the front, right, left, top, bottom, or back (e.g. the \"primary views\"), and is typically positioned relative to each other according to the rules of either first-angle or third-angle projection. The origin and vector direction of the projectors (also called projection lines) differs, as explained below. \n\nUntil the late 19th century, first-angle projection was the norm in North America as well as Europe; but circa the 1890s, third-angle projection spread throughout the North American engineering and manufacturing communities to the point of becoming a widely followed convention, and it was an ASA standard by the 1950s. Circa World War I, British practice was frequently mixing the use of both projection methods.\n\nAs shown above, the determination of what surface constitutes the front, back, top, and bottom varies depending on the projection method used.\n\nNot all views are necessarily used. Generally only as many views are used as are necessary to convey all needed information clearly and economically. The front, top, and right-side views are commonly considered the core group of views included by default, but any combination of views may be used depending on the needs of the particular design. In addition to the six principal views (front, back, top, bottom, right side, left side), any auxiliary views or sections may be included as serve the purposes of part definition and its communication. View lines or section lines (lines with arrows marked \"A-A\", \"B-B\", etc.) define the direction and location of viewing or sectioning. Sometimes a note tells the reader in which zone(s) of the drawing to find the view or section.\n\nAn \"auxiliary view\" is an orthographic view that is projected into any plane other than one of the six \"primary views\". These views are typically used when an object contains some sort of inclined plane. Using the auxiliary view allows for that inclined plane (and any other significant features) to be projected in their true size and shape. The true size and shape of any feature in an engineering drawing can only be known when the Line of Sight (LOS) is perpendicular to the plane being referenced.\nIt is shown like a three-dimensional object. Auxiliary views tend to make use of axonometric projection. When existing all by themselves, auxiliary views are sometimes known as \"pictorials\".\n\nAn isometric projection shows the object from angles in which the scales along each axis of the object are equal. Isometric projection corresponds to rotation of the object by ± 45° about the vertical axis, followed by rotation of approximately ± 35.264° [= arcsin(tan(30°))] about the horizontal axis starting from an orthographic projection view. \"Isometric\" comes from the Greek for \"same measure\". One of the things that makes isometric drawings so attractive is the ease with which 60° angles can be constructed with only a compass and straightedge.\n\nIsometric projection is a type of axonometric projection. The other two types of axonometric projection are:\n\nAn oblique projection is a simple type of graphical projection used for producing pictorial, two-dimensional images of three-dimensional objects: \nIn both oblique projection and orthographic projection, parallel lines of the source object produce parallel lines in the projected image.\n\nPerspective is an approximate representation on a flat surface, of an image as it is perceived by the eye. The two most characteristic features of perspective are that objects are drawn:\n\nProjected views (either Auxiliary or Multiview) which show a cross section of the source object along the specified cut plane. These views are commonly used to show internal features with more clarity than may be available using regular projections or hidden lines. In assembly drawings, hardware components (e.g. nuts, screws, washers) are typically not sectioned.\n\nPlans are usually \"scale drawings\", meaning that the plans are drawn at specific ratio relative to the actual size of the place or object. Various scales may be used for different drawings in a set. For example, a floor plan may be drawn at 1:50 (1:48 or ″ = 1′ 0″) whereas a detailed view may be drawn at 1:25 (1:24 or ″ = 1′ 0″). Site plans are often drawn at 1:200 or 1:100.\n\nScale is a nuanced subject in the use of engineering drawings. On one hand, it is a general principle of engineering drawings that they are projected using standardized, mathematically certain projection methods and rules. Thus, great effort is put into having an engineering drawing accurately depict size, shape, form, aspect ratios between features, and so on. And yet, on the other hand, there is another general principle of engineering drawing that nearly diametrically opposes all this effort and intent—that is, the principle that \"users are not to scale the drawing to infer a dimension not labeled.\" This stern admonition is often repeated on drawings, via a boilerplate note in the title block telling the user, \"DO NOT SCALE DRAWING.\"\n\nThe explanation for why these two nearly opposite principles can coexist is as follows. The first principle—that drawings will be made so carefully and accurately—serves the prime goal of why engineering drawing even exists, which is successfully communicating part definition and acceptance criteria—including \"what the part should look like if you've made it correctly.\" The service of this goal is what creates a drawing that one even \"could\" scale and get an accurate dimension thereby. And thus the great temptation to do so, when a dimension is wanted but was not labeled. The second principle—that even though scaling the drawing \"will\" usually work, one should nevertheless \"never\" do it—serves several goals, such as enforcing total clarity regarding who has authority to discern design intent, and preventing erroneous scaling of a drawing that was never drawn to scale to begin with (which is typically labeled \"drawing not to scale\" or \"scale: NTS\"). When a user is forbidden from scaling the drawing, s/he must turn instead to the engineer (for the answers that the scaling would seek), and s/he will never erroneously scale something that is inherently unable to be accurately scaled.\n\nBut in some ways, the advent of the CAD and MBD era challenges these assumptions that were formed many decades ago. When part definition is defined mathematically via a solid model, the assertion that one cannot interrogate the model—the direct analog of \"scaling the drawing\"—becomes ridiculous; because when part definition is defined this way, it is not \"possible\" for a drawing or model to be \"not to scale\". A 2D pencil drawing can be inaccurately foreshortened and skewed (and thus not to scale), yet still be a completely valid part definition as long as the labeled dimensions are the only dimensions used, and no scaling of the drawing by the user occurs. This is because what the drawing and labels convey is in reality a \"symbol\" of what is wanted, rather than a true \"replica\" of it. (For example, a sketch of a hole that is clearly not round still accurately defines the part as having a true round hole, as long as the label says \"10mm DIA\", because the \"DIA\" implicitly but objectively tells the user that the skewed drawn circle is a symbol \"representing\" a perfect circle.) But if a mathematical model—essentially, a vector graphic—is declared to be the official definition of the part, then any amount of \"scaling the drawing\" can make sense; there may still be an error in the model, in the sense that what was \"intended\" is not \"depicted\" (modeled); but there can be no error of the \"not to scale\" type—because the mathematical vectors and curves are replicas, not symbols, of the part features.\n\nEven in dealing with 2D drawings, the manufacturing world has changed since the days when people paid attention to the scale ratio claimed on the print, or counted on its accuracy. In the past, prints were plotted on a plotter to exact scale ratios, and the user could know that a line on the drawing 15mm long corresponded to a 30mm part dimension because the drawing said \"1:2\" in the \"scale\" box of the title block. Today, in the era of ubiquitous desktop printing, where original drawings or scaled prints are often scanned on a scanner and saved as a PDF file, which is then printed at any percent magnification that the user deems handy (such as \"fit to paper size\"), users have pretty much given up caring what scale ratio is claimed in the \"scale\" box of the title block. Which, under the rule of \"do not scale drawing\", never really did that much for them anyway.\n\nSizes of drawings typically comply with either of two different standards, ISO (World Standard) or ANSI/ASME Y14.1 (American).\n\nThe metric drawing sizes correspond to international paper sizes. These developed further refinements in the second half of the twentieth century, when photocopying became cheap. Engineering drawings could be readily doubled (or halved) in size and put on the next larger (or, respectively, smaller) size of paper with no waste of space. And the metric technical pens were chosen in sizes so that one could add detail or drafting changes with a pen width changing by approximately a factor of the square root of 2. A full set of pens would have the following nib sizes: 0.13, 0.18, 0.25, 0.35, 0.5, 0.7, 1.0, 1.5, and 2.0 mm. However, the International Organization for Standardization (ISO) called for four pen widths and set a colour code for each: 0.25 (white), 0.35 (yellow), 0.5 (brown), 0.7 (blue); these nibs produced lines that related to various text character heights and the ISO paper sizes.\n\nAll ISO paper sizes have the same aspect ratio, one to the square root of 2, meaning that a document designed for any given size can be enlarged or reduced to any other size and will fit perfectly. Given this ease of changing sizes, it is of course common to copy or print a given document on different sizes of paper, especially within a series, e.g. a drawing on A3 may be enlarged to A2 or reduced to A4.\n\nThe U.S. customary \"A-size\" corresponds to \"letter\" size, and \"B-size\" corresponds to \"ledger\" or \"tabloid\" size. There were also once British paper sizes, which went by names rather than alphanumeric designations.\n\nAmerican Society of Mechanical Engineers (ASME) ANSI/ASME Y14.1, Y14.2, Y14.3, and Y14.5 are commonly referenced standards in the U.S.\n\nTechnical lettering is the process of forming letters, numerals, and other characters in technical drawing. It is used to describe, or provide detailed specifications for an object. With the goals of legibility and uniformity, styles are standardized and lettering ability has little relationship to normal writing ability. Engineering drawings use a Gothic sans-serif script, formed by a series of short strokes. Lower case letters are rare in most drawings of machines. ISO Lettering templates, designed for use with technical pens and pencils, and to suit ISO paper sizes, produce lettering characters to an international standard. The stroke thickness is related to the character height (for example, 2.5mm high characters would have a stroke thickness - pen nib size - of 0.25mm, 3.5 would use a 0.35mm pen and so forth). The ISO character set (font) has a seriffed one, a barred seven, an open four, six, and nine, and a round topped three, that improves legibility when, for example, an A0 drawing has been reduced to A1 or even A3 (and perhaps enlarged back or reproduced/faxed/ microfilmed &c). When CAD drawings became more popular, especially using US American software, such as AutoCAD, the nearest font to this ISO standard font was Romantic Simplex (RomanS) - a proprietary shx font) with a manually adjusted width factor (over ride) to make it look as near to the ISO lettering for the drawing board. However, with the closed four, and arced six and nine, romans.shx typeface could be difficult to read in reductions. In more recent revisions of software packages, the TrueType font ISOCPEUR reliably reproduces the original drawing board lettering stencil style, however, many drawings have switched to the ubiquitous Arial.ttf.\n\nThe title block (T/B, TB) is an area of the drawing that conveys header-type information about the drawing, such as: \n\nTraditional locations for the title block are the bottom right (most commonly) or the top right or center.\n\nThe revisions block (rev block) is a tabulated list of the revisions (versions) of the drawing, documenting the revision control.\n\nTraditional locations for the revisions block are the top right (most commonly) or adjoining the title block in some way.\n\nThe next assembly block, often also referred to as \"where used\" or sometimes \"effectivity block\", is a list of higher assemblies where the product on the current drawing is used. This block is commonly found adjacent to the title block.\n\nThe notes list provides notes to the user of the drawing, conveying any information that the callouts within the field of the drawing did not. It may include general notes, flagnotes, or a mixture of both.\n\nTraditional locations for the notes list are anywhere along the edges of the field of the drawing.\n\nGeneral notes (G/N, GN) apply generally to the contents of the drawing, as opposed to applying only to certain part numbers or certain surfaces or features.\n\nFlagnotes or flag notes (FL, F/N) are notes that apply only where a flagged callout points, such as to particular surfaces, features, or part numbers. Typically the callout includes a flag icon. Some companies call such notes \"delta notes\", and the note number is enclosed inside a triangular symbol (similar to capital letter delta, Δ). \"FL5\" (flagnote 5) and \"D5\" (delta note 5) are typical ways to abbreviate in ASCII-only contexts.\n\nThe field of the drawing (F/D, FD) is the main body or main area of the drawing, excluding the title block, rev block, P/L and so on\n\nThe list of materials (L/M, LM, LoM), bill of materials (B/M, BM, BoM), or parts list (P/L, PL) is a (usually tabular) list of the materials used to make a part, and/or the parts used to make an assembly. It may contain instructions for heat treatment, finishing, and other processes, for each part number. Sometimes such LoMs or PLs are separate documents from the drawing itself.\n\nTraditional locations for the LoM/BoM are above the title block, or in a separate document.\n\nSome drawings call out dimensions with parameter names (that is, variables, such a \"A\", \"B\", \"C\"), then tabulate rows of parameter values for each part number.\n\nTraditional locations for parameter tables, when such tables are used, are floating near the edges of the field of the drawing, either near the title block or elsewhere along the edges of the field.\n\nEach view or section is a separate set of projections, occupying a contiguous portion of the field of the drawing. Usually views and sections are called out with cross-references to specific zones of the field.\n\nOften a drawing is divided into zones by a grid, with zone labels along the margins, such as A,B,C,D up the sides and 1,2,3,4,5,6 along the top and bottom. Names of zones are thus, for example, A5, D2, or B1. This feature greatly eases discussion of, and reference to, particular areas of the drawing.\n\nAs in many technical fields, a wide array of abbreviations and symbols have been developed in engineering drawing during the 20th and 21st centuries. For example, cold rolled steel is often abbreviated as CRS, and diameter is often abbreviated as DIA, D, or ⌀.\n\nWith the advent of computer generated drawings for manufacturing and machining, many symbols have fallen out of common use. This poses a problem when attempting to interpret an older hand-drawn document that contains obscure elements that cannot be readily referenced in standard teaching text or control documents such as AMSE and ANSI standards. For example, AMSE Y14.5M 1994 excludes a few elements that convey critical information as contained in older US Navy drawings and aircraft manufacturing drawings of World War 2 vintage. Researching the intent and meaning of some symbols can prove difficult.\n\nHere is an example of an engineering drawing (an isometric view of the same object is shown above). The different line types are colored for clarity.\n\n\nSectional views are indicated by the direction of arrows,as in the example right side.\n\nTechnical drawing has existed since ancient times, and formidable technical drawings were done in renaissance times, such as the drawings of Leonardo da Vinci, but modern engineering drawing, with its precise conventions of orthographic projection and scale, arose in France at a time when the Industrial Revolution was in its infancy. L. T. C. Rolt's biography of Isambard Kingdom Brunel says of his father, Marc Isambard Brunel, that \"It seems fairly certain that Marc's drawings of his block-making machinery [in 1799] made a contribution to British engineering technique much greater than the machines they represented. For it is safe to assume that he had mastered the art of presenting three-dimensional objects in a two-dimensional plane which we now call mechanical drawing. It had been evolved by Gaspard Monge of Mezieres in 1765 but had remained a military secret until 1794 and was therefore unknown in England.\"\n\n\n\n"}
{"id": "1815506", "url": "https://en.wikipedia.org/wiki?curid=1815506", "title": "Gelifluction", "text": "Gelifluction\n\nGelifluction, very similar to solifluction, is the seasonal freeze-thaw action upon waterlogging topsoils which induces downslope movement. Gelifluction is prominent in periglacial regions where snow falls during six to eight months of the year. In spring, the snow and ice melt, and the landscape is effectively inundated with half a year's worth of rainfall in the space of a couple of days. The top soil becomes waterlogged, and flows like a liquid. Because the soil now a fluid, gelifluction is a form of mass movement that can occur on slopes with a slope angle of less than half a degree.\n\nThe most distinctive landforms created by gelifluction include gelifluction lobes and gelifluction benches. The former refer to tongue-shaped deposits of geliflucted material orientated downslope that tend to form on slopes of between 10° and 20°, whereas the latter refer to terrace-like deposits forming on gentler slopes with a long axis running parallel to the slope contour. Gelifluction lobes can be further subdivided into either stone-banked or turf-banked lobes depending on vegetation cover.\n\nA lobe is usually measured in terms of its front (riser) and its length upslope (tread); gelifluction lobes typically have risers of up to and treads of up to .\n"}
{"id": "2162538", "url": "https://en.wikipedia.org/wiki?curid=2162538", "title": "Genetic predisposition", "text": "Genetic predisposition\n\nA genetic predisposition is a genetic characteristic which influences the possible phenotypic development of an individual organism within a species or population under the influence of environmental conditions. In medicine, genetic susceptibility to a disease refers to a genetic predisposition to a health problem, which may eventually be triggered by particular environmental or lifestyle factors, such as tobacco smoking or diet. Genetic testing is able to identify individuals who are genetically predisposed to certain diseases.\n\nPredisposition is the capacity we are born with to learn things such as language and concept of self. Negative environmental influences may block the predisposition (ability) we have to do some things. Behaviors displayed by animals can be influenced by genetic predispositions. Genetic predisposition towards certain human behaviors is scientifically investigated by attempts to identify patterns of human behavior that seem to be invariant over long periods of time and in very different cultures.\n\nFor example, philosopher Daniel Dennett has proposed that humans are genetically predisposed to have a theory of mind because there has been evolutionary selection for the human ability to adopt the intentional stance. The \"intentional stance\" is a useful behavioral strategy by which humans assume that others have minds like their own. This assumption allows you to predict the behavior of others based on personal knowledge of what you would do.\n\nIn 1951, Hans Eysenck and Donald Prell published an experiment in which identical (monozygotic) and fraternal (dizygotic) twins, ages 11 and 12, were tested for neuroticism. It is described in detail in an article published in the \"Journal of Mental Science\". in which Eysenck and Prell concluded that, \"The factor of neuroticism is not a statistical artifact, but constitutes a biological unit which is inherited as a whole...neurotic Genetic predisposition is to a large extent hereditarily determined.\"\n\nE. O. Wilson's and his book Consilience discuss the idea of genetic predisposition to behaviors\n\nThe field of evolutionary psychology explores the idea that certain behaviors have been selected for during the course of evolution.\n\nThe Genetic Information Nondiscrimination Act, which was signed into law by President Bush on May 21, 2008, prohibits discrimination in employment and health insurance based on genetic information.\n\n\n"}
{"id": "41036105", "url": "https://en.wikipedia.org/wiki?curid=41036105", "title": "Great Annihilator", "text": "Great Annihilator\n\n1E1740.7-2942, or the Great Annihilator, is a large black hole thought to be located in the core region of the Milky Way, near the supermassive black hole Sgr A* at the Galactic Center.\nIt is one of the brightest gamma ray sources found in the Milky Way, producing massive amounts of photon pairs at 511 keV, which usually indicates the annihilation of an electron-positron pair. The Great Annihilator also has a radio source counterpart that emits jets approximately three light-years long. These jets are probably synchrotron emissions from positron-electron pairs streaming out at high velocities from the source of antimatter.\n\n"}
{"id": "1471966", "url": "https://en.wikipedia.org/wiki?curid=1471966", "title": "Hubble Origins Probe", "text": "Hubble Origins Probe\n\nThe Hubble Origins Probe (HOP) was a proposal for an orbital telescope made in 2005 in response to the first cancellation of the fourth Hubble Space Telescope (HST) servicing mission. It would have used an Atlas V rocket or similar launch vehicle to launch a much lighter, unaberrated mirror and optical telescope assembly, using the instruments that had already been built for SM4, along with a new wide-field imager. It would have cost between $700 million and $1 billion.\n\nFunding for the mission was never allocated; in February 2005, Sean O'Keefe, the NASA administrator who had cancelled SM4, resigned. Michael D. Griffin, NASA administrator after O'Keefe, reinstated the servicing missions, making HOP redundant.\n\n"}
{"id": "12461038", "url": "https://en.wikipedia.org/wiki?curid=12461038", "title": "Johann Angelo Ferrari", "text": "Johann Angelo Ferrari\n\nJohann Angelo Ferrari (1806-18 May 1876, Vienna was an Austrian entomologist born in Italy who specialised in Coleoptera especially Scolytidae\nHe is not to be confused with Pietro Mansueto Ferrari also an entomologist.\nHe wrote \"Die Forst- und Baumzuchtschädlichen Borkenkäfer (Tomicides Lac.) aus der Familie der Holzverderber (Scolytides Lac.), mit besonderer Berücksichtigung vorzüglich der europäischen Formen, und der Sammlung der k. k. zoologischen Kabinettes in Wien\". Gerolds Sohn, Wien\n"}
{"id": "3007856", "url": "https://en.wikipedia.org/wiki?curid=3007856", "title": "Kálmán Kittenberger", "text": "Kálmán Kittenberger\n\nKálmán Kittenberger (Léva, October 10, 1881 - Nagymaros, January 4, 1958) was a Hungarian traveller, natural historian, biologist and collector. He was born in Léva, now in Slovakia (\"Levice\").\n\nHe made six travels to Africa, the first time in 1902, where he was sent by the Hungarian Royal Society in Budapest. He spent altogether ten and a half years in Africa. During his journeys he faced financial difficulties as he received no sponsorship, but he was still able to grant 60,000 items to the biological collections of the Hungarian National Museum, including 300 new animal species. (Almost 40 of them were named after Kittenberger, including \"Pachyonomastus kittenbergeri\") Part of that collection was annihilated by a fire in 1956.\n\n"}
{"id": "44084491", "url": "https://en.wikipedia.org/wiki?curid=44084491", "title": "List of Groupe Bull products", "text": "List of Groupe Bull products\n\nThe following is a list of products from the French-owned computer hardware and software company Groupe Bull.\n\nIn October 2013 Groupe Bull introduced the Hoox line of cellular phones with enhanced encryption and biometric authentication targeting security-conscious users.\n\nModels:\n\nAs of June 2012 Bull has 16 machines on the TOP500 supercomputer list\n\n"}
{"id": "24331628", "url": "https://en.wikipedia.org/wiki?curid=24331628", "title": "List of Space Shuttle landing sites", "text": "List of Space Shuttle landing sites\n\nThree locations in the United States were used as landing sites for the Space Shuttle system. Each site included runways of sufficient length to provide adequate distance for the slowing-down of a returning spacecraft. The prime landing site was the Shuttle Landing Facility at the Kennedy Space Center in Florida, a purpose-built landing strip. Landings also occurred at Edwards Air Force Base in California, and one took place at White Sands Space Harbor in New Mexico. No space shuttle landed on a dry lakebed runway after 1991. \n\nThe first international site was Cartago,Valle in Colombia (CTA)\n\nVarious international landing sites were also available in the event of a Transoceanic Abort Landing (TAL) scenario, as well as other sites in the United States and Canada in case of an East Coast Abort Landing (ECAL) situation. Space shuttle landings were intended to regularly take place at Vandenberg Air Force Base in California for Department of Defense missions launched from the site, but none occurred due to the cancellation of all launches from Vandenberg.\n\nThe Shuttle Landing Facility at the Kennedy Space Center in Florida has a single concrete runway, 15/33. It is designated Runway 15 or 33, depending on the direction of use. The first landing at the SLF was for mission STS-41B in 1984; landings were suspended at the site following brake damage and a blown tire during the STS-51D landing in 1985, and resumed in 1990. 78 space shuttle missions landed at Kennedy Space Center.\n\nEdwards Air Force Base in California was the site of the first space shuttle landing, and became a back-up site to the prime landing location, the Shuttle Landing Facility at the Kennedy Space Center. Several runways are arrayed on the dry lakebed at Rogers Dry Lake, and there are also concrete runways. Space shuttle landings on the lake bed took place on Runways 05/23, 15/33 and 17/35. Of the concrete strips, the main Runway 04/22 was utilized. During the renovation of 04/22, a temporary runway (with the same designation) was constructed parallel to it and used for one landing (STS-126). 54 space shuttle missions, as well as all five free flights of Space Shuttle Enterprise, landed on Edwards Air Force Base runways.\n\nWhite Sands Space Harbor at White Sands Test Facility in New Mexico was an emergency landing site for the space shuttle and was used as a backup when the runways at Edwards Air Force Base and the Kennedy Space Center were unavailable. Two runways and a runway were available for landings on the dry lake bed. One mission, STS-3, used Runway 17 for a landing due to flooding at its originally planned landing site, Edwards Air Force Base.\n\nIn the event of an abort during launch, NASA had several international locations designated as Transoceanic Abort Landing (TAL) sites. The sites included Lajes Air Base in Terceira island, Azores, Portugal, Zaragoza Air Base in Spain, Morón Air Base in Spain, and Istres Air Base in France. All sites have runways of sufficient length to support the landing of a space shuttle, and included personnel from NASA as well as equipment to aid a space shuttle landing. Zaragoza Air Base features Runway 30L with a length of ; Morón Air Base features an runway; and Istres Air Base features Runway 33 with a length of . Former TAL sites include Diego Garcia in the British Indian Ocean Territory; Cologne Bonn Airport in Germany; Ben Guerir Air Base, Morocco (1988–2002); Casablanca, Morocco (up to 1986); Banjul International Airport, The Gambia (1987–2002); Dakar, Senegal; Rota, Spain; and Kano, Nigeria. Had a TAL situation arisen during a launch from Vandenberg Air Force Base, Hao and Easter Islands in the Pacific Ocean would have been the TAL sites.\n\nRAF Fairford was the only Transoceanic Abort Landing site for NASA's Space Shuttle in the UK. As well as having a sufficiently long runway for a shuttle landing (the runway is 3 km long), Fairford also had NASA-trained fire and medical crews stationed on the base.\n\nIn certain launch abort situations where the mission profile supports a trajectory for such a landing, runways on the East Coast of the United States and Canada could have been used for an East Coast Abort Landing (ECAL) situation. The following sites could have been used for an ECAL: Miami International Airport, Miami, Florida; Plattsburgh International Airport, Plattsburgh, New York, Francis S. Gabreski Airport, Westhampton Beach, New York; Atlantic City International Airport, New Jersey; Myrtle Beach International Airport, Myrtle Beach, South Carolina; Wilmington International Airport, North Carolina Marine Corps Air Station Cherry Point, North Carolina; Naval Air Station Oceana, Virginia; Dover Air Force Base, Delaware; Bangor International Airport, Maine; Westover Air Reserve Base, Massachusetts; Bradley International Airport, Connecticut; Otis Air National Guard Base, Massachusetts; Pease Air National Guard Base, Portsmouth, New Hampshire; Halifax Stanfield International Airport, Enfield, Nova Scotia; Stephenville International Airport, Stephenville, Newfoundland; CFB Goose Bay, Labrador; Gander International Airport, Gander, Newfoundland, St. John's International Airport, St. Johns, Newfoundland;\nLoring Air Force Base, Limestone, Maine. Griffiss International Airport Rome, NY U.S.A.\n\nSpace shuttle missions to be launched from Vandenberg Air Force Base in California were planned to conclude with a landing at Runway 12/30 at the site. The runway was lengthened to support shuttle landings. The first landing at Vandenberg was planned for mission STS-62-A, which was scheduled for launch in July 1986, but canceled in the wake of the STS-51-L accident. No space shuttle operations or landings ever occurred at the site.\n\nThe joint use civilian/military Lincoln Airport/Lincoln Air National Guard Base in Lincoln, Nebraska, USA was designated as an alternate landing site for its unusually and extremely long main runway (12,900 ft. (3,932 m) runway with 1000 ft. over-runs on each end, totaling almost 15,000 ft. length) and low air traffic, both commercial and military. No space shuttle landing ever occurred there. The Amílcar Cabral International Airport on the island of Sal, Cape Verde, was another designated emergency landing site. Runway 01/19 at Amílcar Cabral International Airport is 10,735 ft long and is paved. No space shuttle landing occurred here either.\nAlso Gran Canaria Airport was used as a back-up site.\n\n\n"}
{"id": "913560", "url": "https://en.wikipedia.org/wiki?curid=913560", "title": "List of U.S. state dinosaurs", "text": "List of U.S. state dinosaurs\n\nThis is a list of U.S. state dinosaurs in the United States, including the District of Columbia. A large number of states also have dinosaurs as state fossils, but this list only includes those that have been officially designated as \"state dinosaurs\". All U.S. states also have a state bird, the direct evolutionary descendants of dinosaurs.\n\n\n"}
{"id": "49458571", "url": "https://en.wikipedia.org/wiki?curid=49458571", "title": "List of cluster management software", "text": "List of cluster management software\n\nList of software for cluster management.\n\n\n\n"}
{"id": "3001081", "url": "https://en.wikipedia.org/wiki?curid=3001081", "title": "List of countries by exports", "text": "List of countries by exports\n\nThis is a list of countries by merchandise exports, based on \"The World Factbook\" of the CIA.\n\nWorld Bank's World Integrated Trade Solution now provides list of countries and their share Top exporting countries\n\n"}
{"id": "2701625", "url": "https://en.wikipedia.org/wiki?curid=2701625", "title": "List of countries by life expectancy", "text": "List of countries by life expectancy\n\nThis is a collection of lists of countries by average life expectancy at birth.\n\nLife expectancy equals the average number of years a person born in a given country is expected to live if mortality rates at each age were to remain steady in the future. The life expectancy is shown separately for males and females, as well as a combined figure. Several non-sovereign entities are also included in this list.\n\nThe figures reflect the quality of healthcare in the countries listed as well as other factors including ongoing wars, obesity, and HIV infections.\n\nWorldwide, the average life expectancy at birth was 71.5 years (68 years and 4 months for males and 72 years and 8 months for females) over the period 2010–2015 according to United Nations World Population Prospects 2015 Revision, or 69 years (67 years for males and 71.1 years for females) for 2016 according to \"The World Factbook\". According to the 2015 World Health Organization (WHO) data, women on average live longer than men in all major regions and in all individual countries except for Mali and Eswatini (Swaziland).\n\nThe countries with the lowest overall life expectancies per the WHO are Sierra Leone, the Central African Republic, the Democratic Republic of the Congo, Guinea-Bissau, Lesotho, Somalia, Eswatini, Angola, Chad, Mali, Burundi, Cameroon, and Mozambique. Of those countries, only Lesotho, Eswatini, and Mozambique in 2011 were suffering from an HIV prevalence rate of greater than 12 percent in the 15–49 age group.\n\nComparing life expectancies from birth across countries can be problematic. There are differing definitions of live birth vs stillbirth even among more developed countries and less developed countries often have poor reporting.\n\n2015 data published in May 2016.\n\nHALE: Health-adjusted life expectancy\n\nOn July 2015, the Population Division of the United Nations Department of Economic and Social Affairs (UN DESA), released \"World Population Prospects, The 2015 Revision\". The following table shows the life expectancy at birth for the period 2010 to 2015.\n\nThe Global Burden of Disease 2010 study published updated figures in 2012, including recalculations of life expectancies which differ substantially in places from the UN estimates for 2010 (reasons for this are discussed in the freely available appendix to the paper, pages 25–27, currently not available). Although no estimate is given for the sexes combined, for the first time life expectancy estimates have included uncertainty intervals.\n\nThe US CIA published the following life expectancy data in its World Factbook.\n\nFigures are from the CIA World Factbook 2009 and from the 2010 revision of the United Nations World Population Prospects report, for 2005–2010, (data viewable at http://esa.un.org/wpp/Sorting-Tables/tab-sorting_mortality.htm, with equivalent spreadsheets here, here, and here).\n\nOnly countries/territories with a population of 100,000 or more in 2010 are included in the United Nations list. \nWHO database 2013 http://www.who.int/gho/publications/world_health_statistics/EN_WHS2013_Full.pdf\n"}
{"id": "38117998", "url": "https://en.wikipedia.org/wiki?curid=38117998", "title": "List of medical schools in Bahrain", "text": "List of medical schools in Bahrain\n\nThere are only four Medical schools in Bahrain, two of which are part of public universities and another two of which are private universities.\n"}
{"id": "48172", "url": "https://en.wikipedia.org/wiki?curid=48172", "title": "List of mental disorders", "text": "List of mental disorders\n\nThe following is a list of mental disorders as defined by the DSM and ICD.\n\nThe \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM) is the American Psychiatric Association's standard reference for psychiatry which includes over 450 different definitions of mental disorders. The International Classification of Diseases (ICD) published by the World Health Organization (WHO) is the international standard system for classifying all medical diseases. It also includes a section on .\n\nThe diagnostic criteria and information in the DSM and ICD are revised and updated with each new version. This list contains conditions which are currently recognised as mental disorders as defined by these two systems. There is disagreement in various fields of mental health care, including the field of psychiatry, over the definitions and criteria used to delineate mental disorders. Of concern to some professionals is whether certain mental disorders should be classified as 'mental illnesses' or whether they may be better described as neurological disorders, or in other ways.\n\n"}
{"id": "25240364", "url": "https://en.wikipedia.org/wiki?curid=25240364", "title": "List of national parks in Macedonia", "text": "List of national parks in Macedonia\n\nThere are three National parks in Republic of Macedonia: Pelister, Mavrovo and Galičica.\n"}
{"id": "52901746", "url": "https://en.wikipedia.org/wiki?curid=52901746", "title": "Lists of borders", "text": "Lists of borders\n\nLists of borders on Wikipedia include:\n\n\n"}
{"id": "18371194", "url": "https://en.wikipedia.org/wiki?curid=18371194", "title": "Mahala Andrews", "text": "Mahala Andrews\n\nMahala Andrews was a palaeontologist who worked for the National Museum of Scotland. During her career, Andrews wrote many studies on the prehistoric lobe-finned fish \"Onychodus\". She was a Christian and died on the Scottish island of Iona\n"}
{"id": "1827851", "url": "https://en.wikipedia.org/wiki?curid=1827851", "title": "Marine debris", "text": "Marine debris\n\nMarine debris, also known as marine litter, is human-created waste that has deliberately or accidentally been released in a lake, sea, ocean, or waterway. Floating oceanic debris tends to accumulate at the center of gyres and on coastlines, frequently washing aground, when it is known as \"beach litter\" or tidewrack. Deliberate disposal of wastes at sea is called \"ocean dumping\". Naturally occurring debris, such as driftwood, are also present.\n\nWith the increasing use of plastic, human influence has become an issue as many types of plastics do not biodegrade. Waterborne plastic poses a serious threat to fish, seabirds, marine reptiles, and marine mammals, as well as to boats and coasts. Dumping, container spillages, litter washed into storm drains and waterways and wind-blown landfill waste all contribute to this problem.\n\nIn efforts to prevent and mediate marine debris and pollutants, laws and policies have been adopted internationally. Depending on relevance to the issues and various levels of contribution, some countries have introduced more specified protection policies.\n\nResearchers classify debris as either land- or ocean-based; in 1991, the United Nations Joint Group of Experts on the Scientific Aspects of Marine Pollution estimated that up to 80% of the pollution was land-based, with the remaining 20% originating from catastrophic events or maritime sources. More recent studies have found that more than half of plastic debris found on Korean shores is ocean-based.\n\nA wide variety of man-made objects can become marine debris; plastic bags, balloons, buoys, rope, medical waste, glass and plastic bottles, cigarette stubs, cigarette lighters, beverage cans, polystyrene, lost fishing line and nets, and various wastes from cruise ships and oil rigs are among the items commonly found to have washed ashore. Six pack rings, in particular, are considered emblematic of the problem.\n\nThe US military used ocean dumping for unused weapons and bombs, including ordinary bombs, UXO, landmines and chemical weapons from at least 1919 until 1970. Millions of pounds of ordnance were disposed of in the Gulf of Mexico and off the coasts of at least 16 states, from New Jersey to Hawaii (although these, of course, do not wash up onshore, and the US is not the only country who has practiced this).\n\nEighty percent of marine debris is plastic. Plastics accumulate because they typically do not biodegrade as many other substances do. They photodegrade on exposure to sunlight, although they do so only under dry conditions, as water inhibits photolysis. In a 2014 study using computer models, scientists from the group 5 Gyres, estimated 5.25 trillion pieces of plastic weighing 269,000 tons were dispersed in oceans in similar amount in the Northern and Southern Hemispheres, and one-hundredth of them are particles the scale of a sand.\n\nFishing nets left or lost in the ocean by fishermen – ghost nets – can entangle fish, dolphins, sea turtles, sharks, dugongs, crocodiles, seabirds, crabs, and other creatures. These nets restrict movement, causing starvation, laceration and infection, and, in animals that breathe air, suffocation.\n\n8.8 million metric tons of plastic waste are dumped in the world's oceans each year. Asia was the leading source of mismanaged plastic waste, with China alone accounting for 2.4 million metric tons.\n\nPlastic waste has reached all the world's oceans. This plastic pollution harms an estimated 100,000 sea turtles and marine mammals and 1,000,000 sea creatures each year. Larger plastics (called \"macroplastics\") such as plastic shopping bags can clog the digestive tracts of larger animals when consumed by them and can cause starvation through restricting the movement of food, or by filling the stomach and tricking the animal into thinking it is full. Microplastics on the other hand harm smaller marine life. For example, pelagic plastic pieces in the center of our ocean’s gyres outnumber live marine plankton, and are passed up the food chain to reach all marine life. A 1994 study of the seabed using trawl nets in the North-Western Mediterranean around the coasts of Spain, France, and Italy reported mean concentrations of debris of 1,935 items per square kilometre. Plastic debris accounted for 77%, of which 93% was plastic bags.\n\nNurdles, also known as \"mermaids' tears\", are plastic pellets, typically under five millimetres in diameter, that are a major component of marine debris. They are a raw material in plastics manufacturing, and enter the natural environment when spilled. Weathering produces ever smaller pieces. Nurdles strongly resemble fish eggs.\n\nLitter, made from diverse materials that are denser than surface water (such as glasses, metals and some plastics), have been found to spread over the floor of seas and open oceans, where it can become entangled in corals and interfere with other sea-floor life, or even become buried under sediment, making clean-up extremely difficult, especially due to the wide area of its dispersal compared to shipwrecks. Research performed by MBARI found items including plastic bags below 2000m depth off the west coast of North America and around Hawaii.\n\nAn estimated 10,000 containers at sea each year are lost by container ships, usually during storms. One spillage occurred in the Pacific Ocean in 1992, when thousands of rubber ducks and other toys (now known as the \"Friendly Floatees\") went overboard during a storm. The toys have since been found all over the world, providing a better understanding of ocean currents. Similar incidents have happened before, such as when \"Hansa Carrier\" dropped 21 containers (with one notably containing buoyant Nike shoes). In 2007, MSC Napoli beached in the English Channel, dropping hundreds of containers, most of which washed up on the Jurassic Coast, a World Heritage Site.\n\nIn Halifax Harbour, Nova Scotia, 52% of items were generated by recreational use of an urban park, 14% from sewage disposal and only 7% from shipping and fishing activities. Around four fifths of oceanic debris is from rubbish blown onto the water from landfills, and urban runoff.\n\nSome studies show that marine debris may be dominant in particular locations. For example, a 2016 study of Aruba found that debris found the windward side of the island was predominantly marine debris from distant sources. In 2013, debris from six beaches in Korea was collected and analyzed: 56% was found to be \"ocean-based\" and 44% \"land-based\". \n\nIn the 1987 Syringe Tide, medical waste washed ashore in New Jersey after having been blown from Fresh Kills Landfill. On the remote sub-Antarctic island of South Georgia, fishing-related debris, approximately 80% plastics, are responsible for the entanglement of large numbers of Antarctic fur seals.\n\nMarine litter is even found on the floor of the Arctic ocean.\n\nOnce waterborne, debris becomes mobile. Flotsam can be blown by the wind, or follow the flow of ocean currents, often ending up in the middle of oceanic gyres where currents are weakest. The Great Pacific Garbage Patch is one such example of this, comprising a vast region of the North Pacific Ocean rich with anthropogenic wastes. Estimated to be double the size of Texas, the area contains more than 3 million tons of plastic.\nPatches may be large enough to be viewed by satellite. For example, when the Malaysian Flight MH370, disappeared in 2014, satellites were scanning the oceans surface for any sign of it, and instead of finding debris from the plane they came across floating garbage. The gyre contains approximately six pounds of plastic for every pound of plankton.\n\nThe oceans may contain as much as one hundred million tons of plastic. It is estimated that each garbage patch in the ocean have up to one million tons of trash swirling around in them, sometimes extending down to around one hundred feet below the surface. Some items that have been extracted from these garbage patches are: a drum of hazardous chemicals, plastic hangers, tires, cable cords, a ton of tangled netting etc.\n\nOver 40% of oceans are classified as subtropical gyres, a fourth of the planets surface area has become an accumulator of floating plastic debris.\n\nIslands situated within gyres frequently have coastlines flooded by waste that washes ashore; prime examples are Midway and Hawaii. Clean-up teams around the world patrol beaches to attack this environmental threat.\n\nMore than 37 million pieces of plastic debris have accumulated on Henderson Island, a remote Pitcairn island in the South Pacific, reported to be the highest density of debris reported anywhere in the world, yet the trash accounts for only 1.98 seconds’ worth of the annual global production of plastic.\n\nMany animals that live on or in the sea consume flotsam by mistake, as it often looks similar to their natural prey. Bulky plastic debris may become permanently lodged in the digestive tracts of these animals, blocking the passage of food and causing death through starvation or infection. Tiny floating plastic particles also resemble zooplankton, which can lead filter feeders to consume them and cause them to enter the ocean food chain. In samples taken from the North Pacific Gyre in 1999 by the Algalita Marine Research Foundation, the mass of plastic exceeded that of zooplankton by a factor of six.\n\nToxic additives used in plastic manufacturing can leach into their surroundings when exposed to water. Waterborne hydrophobic pollutants collect and magnify on the surface of plastic debris, thus making plastic more deadly in the ocean than it would be on land. Hydrophobic contaminants bioaccumulate in fatty tissues, biomagnifying up the food chain and pressuring apex predators and humans. Some plastic additives disrupt the endocrine system when consumed; others can suppress the immune system or decrease reproductive rates.\n\nThe hydrophobic nature of plastic surfaces stimulates rapid formation of biofilms, which support a wide range of metabolic activities, and drive succession of other micro- and macro-organisms.\n\nConcern among experts has grown since the 2000s that some organisms have adapted to live on floating plastic debris, allowing them to disperse with ocean currents and thus potentially become invasive species in distant ecosystems. Research in 2014 in the waters around Australia confirmed a wealth of such colonists, even on tiny flakes, and also found thriving ocean bacteria eating into the plastic to form pits and grooves. These researchers showed that \"plastic biodegradation is occurring at the sea surface\" through the action of bacteria, and noted that this is congruent with a new body of research on such bacteria. Their finding is also congruent with the other major research undertaken in 2014, which sought to answer the riddle of the overall lack of build up of floating plastic in the oceans, despite ongoing high levels of dumping. Plastics were found as microfibres in core samples drilled from sediments at the bottom of the deep ocean. The cause of such widespread deep sea deposition has yet to be determined.\n\nNot all anthropogenic artifacts placed in the oceans are harmful. Iron and concrete structures typically do little damage to the environment because they generally sink to the bottom and become immobile, and at shallow depths they can even provide scaffolding for artificial reefs. Ships and subway cars have been deliberately sunk for that purpose.\n\nAdditionally, hermit crabs have been known to use pieces of beach litter as a shell when they cannot find an actual seashell of the size they need.\n\nTechniques for collecting and removing marine (or riverine) debris include the use of debris skimmer boats \"(pictured)\". Devices such as these can be used where floating debris presents a danger to navigation. For example, the US Army Corps of Engineers removes 90 tons of \"drifting material\" from San Francisco Bay every month. The Corps has been doing this work since 1942, when a seaplane carrying Admiral Chester W. Nimitz collided with a piece of floating debris and sank, costing the life of its pilot. Once debris becomes \"beach litter\", collection by hand and specialized beach-cleaning machines are used to gather the debris.\n\nElsewhere, \"trash traps\" are installed on small rivers to capture waterborne debris before it reaches the sea. For example, South Australia's Adelaide operates a number of such traps, known as \"trash racks\" or \"gross pollutant traps\" on the Torrens River, which flows (during the wet season) into Gulf St Vincent.\n\nIn lakes or near the coast, manual removal can also be used. Project AWARE for example promotes the idea of letting dive clubs clean up litter, for example as a diving exercise.\n\nOn the sea, the removal of artificial debris (i.e. plastics) is still in its infancy. However some projects have been started which used ships with nets (Kaisei and New Horizon) to catch some plastics, primarily for research purposes. Another method to gather artificial litter has been proposed by Boyan Slat. He suggested using platforms with arms to gather the debris, situated inside the current of gyres.\n\nAnother issue is that removing marine debris from our oceans can potentially cause more harm than good. Cleaning up micro-plastics could also accidentally take out plankton, which are the main lower level food group for the marine food chain and over half of the photosynthesis on earth. One of the most efficient and cost effective ways to help reduce the amount of plastic entering our oceans is to not participate in using single use plastics, avoid plastic bottled drinks such as water bottles, use reusable shopping bags, and to buy products with reusable packaging.\n\nOnce a year there is a diving marine debris removal operation in Scapa Flow in the Orkneys, run by Ghost Fishing UK, funded by World Animal Protection and Fat Face Foundation.\n\nThe ocean is a global common, so negative externalities of marine debris are not usually experienced by the producer. In the 1950s, the importance of government intervention with marine pollution protocol was recognized at the First Conference on the Law of the Sea.\n\nOcean dumping is controlled by international law, including:\n\nOne of the earliest anti-dumping laws was Australia's \"Beaches, Fishing Grounds and Sea Routes Protection Act 1932\", which prohibited the discharge of \"garbage, rubbish, ashes or organic refuse\" from \"any vessel in Australian waters\" without prior written permission from the federal government. It also required permission for scuttling. The act was passed in response to large amounts of garbage washing up on the beaches of Sydney and Newcastle from vessels outside the reach of local governments and the New South Wales government. It was repealed and replaced by the \"Environment Protection (Sea Dumping) Act 1981\", which gave effect to the London Convention.\n\nIn 1972 and 1974, conventions were held in Oslo and Paris respectively, and resulted in the passing of the OSPAR Convention, an international treaty controlling marine pollution in the north-east Atlantic Ocean. The Barcelona Convention protects the Mediterranean Sea. The Water Framework Directive of 2000 is a European Union directive committing EU member states to free inland and coastal waters from human influence. In the United Kingdom, the Marine and Coastal Access Act 2009 is designed to \"ensure clean healthy, safe, productive and biologically diverse oceans and seas, by putting in place better systems for delivering sustainable development of marine and coastal environment\".\n\nIn the waters of the United States, there have been many observed consequences of pollution including: hypoxic zones, harmful agal blooms, and threatened species. In 1972, the United States Congress passed the Ocean Dumping Act, giving the Environmental Protection Agency power to monitor and regulate the dumping of sewage sludge, industrial waste, radioactive waste and biohazardous materials into the nation's territorial waters. The Act was amended sixteen years later to include medical wastes. It is illegal to dispose of any plastic in US waters.\n\nProperty law, admiralty law and the law of the sea may be of relevance when lost, mislaid, and abandoned property is found at sea. Salvage law rewards salvors for risking life and property to rescue the property of another from peril. On land the distinction between deliberate and accidental loss led to the concept of a \"treasure trove\". In the United Kingdom, shipwrecked goods should be reported to a Receiver of Wreck, and if identifiable, they should be returned to their rightful owner.\n\nA large number of groups and individuals are active in preventing or educating about marine debris. For example, 5 Gyres is an organization aimed at reducing plastics pollution in the oceans, and was one of two organizations that recently researched the Great Pacific Garbage Patch. Heal the Bay is another organization, focusing on protecting California's Santa Monica Bay, by sponsoring beach cleanup programs along with other activities. Marina DeBris is an artist focusing most of her recent work on educating people about beach trash.\nInteractive sites like Adrift demonstrate where marine plastic is carried, over time, on the worlds ocean currents.\n\nOn 11 April 2013 in order to create awareness, artist Maria Cristina Finucci founded The Garbage patch state at UNESCO –Paris in front of Director General Irina Bokova. First of a series of events under the patronage of UNESCO and of Italian Ministry of the Environment.\n\nForty-eight plastics manufacturers from 25 countries, are members of the Global Plastic Associations for solutions on Marine Litter, have made the pledge to help prevent marine debris and to encourage recycling.\n\nMarine debris is a problem created by all of us, not only those in coastal regions. Ocean debris can come from as far away as Nebraska. The places that see the most damage are often not the places that produce the pollution. For ocean pollution, much of the trash may come from inland states, where people may never see the ocean and thus may never put any thought into protecting it. The problem continues to grow in tandem with plastics usage and disposal. Steps can be taken to prevent the movement of inland plastics into the oceans.\n\nPlastic debris from inland states come from two main sources: ordinary litter and materials from open dumps and landfills that blow or wash away to inland waterways and wastewater outflows. The refuse finds its way from inland waterways, rivers, streams and lakes to the ocean. Though ocean and coastal area cleanups are important, it is crucial to address plastic waste that originates from inland and landlocked states.\n\nAt the systems level, there are various ways to reduce the amount of debris entering our waterways:\nAs consumers, there are things we can do to help reduce the amount of plastic entering our waterways: \n\nThough the awareness of inland ocean conservation debris mitigation seems to be small compared to coastal states, some organizations in the United States are already working to improve this. The Colorado Ocean Coalition was formed in 2011 with the goal of impressing upon inland citizens that they don’t need to see the ocean to care about its health. It has since grown beyond one state and now forms the Inland Ocean Coalition, with the mission of promoting knowledge and awareness of how inland states contribute to pollution of the ocean, aiming to shatter the ‘out of sight, out of mind’ mentality that often applies in this region.\n\n“Those who live among mountains, rivers and inland cities have a direct impact on the cycle of life in the ocean,” reads the IOCO website. \"The changes we need to make to address the largest threats facing our seas—lowering carbon emissions, reducing trash and pollution, eating sustainable seafood, safeguarding watersheds, promoting marine protected areas (MPAs)—can happen from anywhere in the world.”\n\nThis organization has chapters in many inland US states and promotes programs like watershed cleanups, youth-centered education, and decreasing the use of plastic.\n\nThe Clean Oceans Project (TCOP) promotes conversion of the plastic waste into valuable liquid fuels, including gasoline, diesel and kerosene, using plastic-to-fuel conversion technology developed by Blest Co. Ltd., a Japanese environmental engineering company. TCOP plans to educate local communities and create a financial incentive for them to recycle plastic, keep their shorelines clean, and minimize plastic waste.\n\n\n"}
{"id": "14139126", "url": "https://en.wikipedia.org/wiki?curid=14139126", "title": "Materials physics", "text": "Materials physics\n\nMaterial physics is the use of physics to describe the physical properties of materials. It is a synthesis of physical sciences such as chemistry, solid mechanics, solid state physics, and materials science. Materials physics is considered a subset of condensed matter physics and applies fundamental condensed matter concepts to complex multiphase media, including materials of technological interest.\n\nCurrent fields that materials physicists work in include electronic, optical, and magnetic materials, novel materials and structures, quantum phenomena in materials, nonequilibrium physics, and soft condensed matter physics. New experimental and computational tools are constantly improving how materials systems are modeled and studied and are also fields when materials physicists work in.\n\n"}
{"id": "58069887", "url": "https://en.wikipedia.org/wiki?curid=58069887", "title": "Michele Bannister", "text": "Michele Bannister\n\nMichele Bannister (born 1986) is an astrophysicist and science communicator at the Queen's University Belfast. Asteroid 10463 Bannister is named after her.\n\nBannister is from Waitara, New Zealand. She attended Waitara High School, where she won the Korean War Essay Competition. She studied astronomy and geology at the University of Canterbury, graduating in 2007 with first class honours. She spent nine weeks working in the McMurdo Dry Valleys. Before starting her PhD she completed a summer school in Castel Gandolfo. She earned her PhD in 2014, working on trans Neptunian objects at the Australian National University. She searched for new dwarf planets at the Uppsala Southern Schmidt Telescope. The telescope survived the Warrumbungles fire which destroyed twelve properties in Coonabarabran. Whilst at Australian National University she played in the Flying Disc team.\n\nIn 2014 she was co-investigator on the COLours for the Outer Solar System Origins Survey (OSSOS). She was appointed a postdoctoral fellow at the University of Victoria and the National Research Council (Canada) in 2013. Whilst at the University of Victoria she discovered a new dwarf planet (RR245) with the Canada–France–Hawaii Telescope. RR245 is near the Kuiper belt. She played for a local Ultimate team, and published poetry.\n\nIn August 2016 she joined Queen's University Belfast. She is on the Science Team of the Maunakea Spectroscopic Explorer. She was involved with the observation of ʻOumuamua, an interstellar object from another solar system that passed through our own in 2017. She studied the brightness of ʻOumuamua and presented the colour composite image. 10463 Bannister was named after her in 2017.\n\nBannister is a popular science communicator, and has spoken at the Royal Society, The Planetary Society, SETI Institute, Irish Astronomical Society and European Astrofest. In 2013 she was a curator on the RealScientists channel.She reported on the images coming in from pluto during the spacecraft flyby on Radio New Zealand and Nature in 2015.\n\nShe discussed astronomy on Canadian radio station CFAX between 2015 and 2016. She appeared on the BBC Sky at Night in 2107 and 2018. She has written for The Conversation and The Planetary Society magazine, as well as contributing to Scientific American, Newsweek, National Geographic New Scientist, Slate and The Guardian.\n\n"}
{"id": "29444672", "url": "https://en.wikipedia.org/wiki?curid=29444672", "title": "Old Weather", "text": "Old Weather\n\nOld Weather is an online weather data project that currently invites members of the public to assist in digitising weather observations recorded in US log books dating from the mid-19th century onwards. It is an example of citizen science that enlists members of the public to help in scientific research. It contributes to the Atmospheric Circulation Reconstructions over the Earth initiative. Data collected by Old Weather has been used by at least five different climate reanalysis projects, including HURDAT, SODA and ECMWF. In February 2013, the project was awarded the Royal Meteorological Society IBM Award for Meteorological Innovation that Matters.\n\nOld Weather is a Zooniverse project and is a collaboration between researchers at many institutions, including the University of Oxford, Oxford Martin School, ACRE (International Atmospheric Circulation Reconstructions over the Earth), Naval-History.Net of Penarth, Jisc which encourages UK colleges and universities in the innovative use of digital technologies, the National Maritime Museum at Maritime Greenwich, London, and the UK National Archives, Kew, London.\n\nIn the past, computer programs have proved unable to read handwriting reliably.\nHowever, it may be worth exploring the current status\nof automatic and computer-assisted transcription\nand probabilistic indexing\ntechnologies for handwritten text images—also called Handwriting Text Recognition (HTR), Computer Assisted Transcription of Text Images (CATTI) and Keyword Spotting (KWS), respectively).\n\nIn any case, the task is much better performed by the human brain and the results transferred to a digital form. \nIn the site's tutorial, would-be volunteers are shown how to digitise a weather record. Further instructions on how to transcribe the logs are available on the associated Old Weather forum.\nIt is intended that the pages of the logs are digitised by at least three people. The results will be used to make climate model projections and an improved database of weather extremes.\n\nCurrently, the log books of 2 US vessels are available, each of which have been scanned page by page, and the logs of another 21 vessels have been completed. \nMore log books will be added at intervals.\nThe transcriber notes the following from the log books: date, location (or voyage) and weather records, usually consisting of wind direction and strength, weather conditions, cloud type and/or amount of clear sky, barometric pressure and temperature readings. Other log entries, such as refueling figures and sightings of sea-ice, ships, people, landmarks or animals may also be recorded, as well as interesting events.\n\nPhase I was launched in October 2010 and all the available Royal Navy logs from that phase and from Phase II have now been completed. By July 23, 2012,\nofficially, 16,400 volunteers had transcribed the weather data from 1,090,745 pages of the log books of 302 ships. These phases of the project have generated 1.6 million weather observations.\n\nPhase III, consisting of logs from US ships voyaging in the Arctic and worldwide from the mid-1800s onwards, was launched in October 2012 and is now in progress.\n\nCurrently, the scope of the project is being extended to include Arctic voyages and expeditions. Satellite imagery of this region goes back only to the 1950s, but it was explored for 100 years before that (for example the Franklin Expedition).\n\nInitial results of Phase I will be published after data collection is complete and conclusions can be made.\n\nIndeed, the readings are still being assessed at a very broad level. But the distribution of temperature by latitude and wind force by latitude have been plotted for 120,000 results for which three readings have been taken.\n\nBecause climate change is a very political issue, interested parties could try to corrupt the data by, say, entering temperature figures that are too high or too low. Because three sets of records for each data point will be entered, any set from a digitiser showing a marked deviation from the other records should be easily checkable and eliminated. Large-scale fraud is unlikely because the data is entered one log page at a time, and so is immune to a spam type of attack. Collaborative projects such as Linux and Wikipedia have for the most part been able to rely on the transparent honesty of those taking part.\n\nAccidental errors, such as reading '4's for '7's are possible, but often context will sort this out. A temperature of 40 °F is unlikely to be correct for a latitude in the tropics and may safely be assumed to be 70 °F.\n\nZooniverse projects:\n\n\n"}
{"id": "42546394", "url": "https://en.wikipedia.org/wiki?curid=42546394", "title": "Payson Center for International Development", "text": "Payson Center for International Development\n\nThe Payson Center for International Development at Tulane University in New Orleans, Louisiana, United States, is an interdisciplinary center for the study of international development.\n\nThe Payson Center confers the following major and degrees:\n\nThe Payson Center was founded in 1998 by William E. Bertrand and Eamon Kelly as the Payson School for International Development and Technology Transfer, with a focus on information and communication technology in international development. Up until 2007 Payson maintained an office in Arlington, VA.\n\nIn 2008 it was reorganized as part of the Tulane Law School, which at the time was a unique affiliation. In 2014, the Center underwent another restructuring and no longer exists as a functioning entity."}
{"id": "2888489", "url": "https://en.wikipedia.org/wiki?curid=2888489", "title": "Pioneer P-1", "text": "Pioneer P-1\n\nPioneer P-1 was a failed mission in the Pioneer program. The spacecraft was a 1-meter diameter sphere, with a propulsion module. It was launched on September 24, 1959 on an Atlas C-Able launcher. It was to carry a TV camera and a magnetic field sensor. It was to be spin-stabilized, and was known as a 'paddlewheel' spacecraft. The Atlas-Able launch vehicle was destroyed in an explosion on the launch pad at Cape Canaveral during a pre-launch static test. The payload of P-1 spacecraft and Able IV space engine was not present on the launch vehicle when it exploded, and the payload was later used in the P-3 mission.\n\n"}
{"id": "61334", "url": "https://en.wikipedia.org/wiki?curid=61334", "title": "Pygmy hippopotamus", "text": "Pygmy hippopotamus\n\nThe pygmy hippopotamus (\"Choeropsis liberiensis\" or \"Hexaprotodon liberiensis\") is a small hippopotamid which is native to the forests and swamps of West Africa, primarily in Liberia, with small populations in Sierra Leone, Guinea, and Ivory Coast.\n\nThe pygmy hippo is reclusive and nocturnal. It is one of only two extant species in the family Hippopotamidae, the other being its much larger relative, the common hippopotamus (\"Hippopotamus amphibius\") or Nile hippopotamus. The pygmy hippopotamus displays many terrestrial adaptations, but like the hippo, it is semiaquatic and relies on water to keep its skin moist and its body temperature cool. Behaviors such as mating and giving birth may occur in water or on land. The pygmy hippo is herbivorous, feeding on ferns, broad-leaved plants, grasses, and fruits it finds in the forests.\n\nA rare nocturnal forest creature, the pygmy hippopotamus is a difficult animal to study in the wild.\nPygmy hippos were unknown outside West Africa until the 19th century. Introduced to zoos in the early 20th century, they breed well in captivity and the vast majority of research is derived from zoo specimens. The survival of the species in captivity is more assured than in the wild; the World Conservation Union estimates that fewer than 3,000 pygmy hippos remain in the wild.\n\nPygmy hippos are primarily threatened by loss of habitat, as forests are logged and converted to farm land, and are also vulnerable to poaching, hunting for bushmeat, natural predators, and war. Pygmy hippos are among the species illegally hunted for food in Liberia.\n\nNomenclature of the pygmy hippopotamus reflects that of the hippopotamus. The plural form is pygmy hippopotami (\"hippopotamuses\" is also accepted as a plural form by the OED, or \"pygmy hippos\" for short). A male pygmy hippopotamus is known as a \"bull\", a female as a \"cow\", and a baby as a \"calf\". A group of hippopotami is known as a \"herd\" or a \"bloat\".\nThe pygmy hippopotamus is a member of the family Hippopotamidae where it is classified as a member of either the genus \"Choeropsis\" (\"resembling a hog\") or, the genus \"Hexaprotodon\" (\"six front teeth\"). Members of Hippopotamidae are sometimes known as hippopotamids. Sometimes the sub-family Hippopotaminae is used. Further, some taxonomists group hippopotami and anthracotheres in the superfamily \"Anthracotheroidea\" or \"Hippopotamoidea\".\n\nA sister species of the pygmy hippopotamus may have been the little-studied Malagasy pygmy hippopotamus (\"Hexaprotodon madagascariensis\" or \"Hippopotamus madagascariensis\"), one of three recently extinct species from Madagascar. \"C. madagascariensis\" was the same size as \"C. liberiensis\" and shared its terrestrial behavior, inhabiting the forested highlands of Madagascar, rather than open rivers. It is believed to have gone extinct within the last 500 years.\n\nThe taxonomy of the genus of the pygmy hippopotamus has changed as understanding of the animal has developed. Samuel G. Morton initially classified the animal as \"Hippopotamus minor\", but later determined it was distinct enough to warrant its own genus, and labeled it \"Choeropsis\". In 1977, Shirley C. Coryndon proposed that the pygmy hippopotamus was closely related to \"Hexaprotodon\", a genus that consisted of prehistoric hippos mostly native to Asia.\n\nThis assertion was widely accepted, until Boisserie asserted in 2005 that the pygmy hippopotamus was not a member of \"Hexaprotodon\", after a thorough examination of the phylogeny of Hippopotamidae. He suggested instead that the pygmy hippopotamus was a distinct genus, and returned the animal to \"Choeropsis\". All agree that the modern pygmy hippopotamus, be it \"H. liberiensis\" or \"C. liberiensis\", is the only extant member of its genus.\n\nA distinct subspecies of pygmy hippopotamus existed in Nigeria until at least the 20th century, though the validity of this has been questioned. The existence of the subspecies, makes \"Choeropsis liberiensis liberiensis\" (or \"Hexaprotodon liberiensis liberiensis\" under the old classification) the full trinomial nomenclature for the Liberian pygmy hippopotamus. The Nigerian pygmy hippopotamus was never studied in the wild and never captured. All research and all zoo specimens are the Liberian subspecies. The Nigerian subspecies is classified as \"C. liberiensis heslopi\".\n\nThe Nigerian pygmy hippopotamus ranged in the Niger River Delta, especially near Port Harcourt, but no reliable reports exist after the collection of the museum specimens secured by I. R. P. Heslop, a British colonial officer, in the early 1940s. It is probably extinct. The subspecies is separated by over and the Dahomey Gap, a region of savanna that divides the forest regions of West Africa. The subspecies is named after I. R. P. Heslop, who claimed in 1945 to have shot a pygmy hippo in the Niger Delta region and collected several skulls. He estimated that perhaps no more than 30 pygmy hippos remained in the region.\n\nHeslop reportedly sent four pygmy hippopotamus skulls he collected to the British Museum of Natural History in London. These specimens were not subjected to taxonomic evaluation, however, until 1969 when G. B. Corbet classified the skulls as belonging to a separate subspecies based on consistent variations in the proportions of the skulls. The Nigerian pygmy hippos were seen or shot in Rivers State, Imo State and Bayelsa State, Nigeria. While some local populations are aware that the species once existed, its history in the region is poorly documented.\n\nThe evolution of the pygmy hippopotamus is most often studied in the context of its larger cousin. Both species were long believed to be most closely related to the family Suidae (pigs and hogs) or Tayassuidae (peccaries), but research within the last 10 years has determined that pygmy hippos and hippos are most closely related to cetaceans (whales and dolphins). Hippos and whales shared a common semi-aquatic ancestor that branched off from other artiodactyls around .\n\nThis hypothesized ancestor likely split into two branches about six million years later. One branch would evolve into cetaceans, the other branch became the anthracotheres, a large family of four-legged beasts, whose earliest member, from the Late Eocene, would have resembled narrow hippopotami with comparatively small and thin heads.\n\nHippopotamids are deeply nested within the family Anthracotheriidae. The oldest known hippopotamid is the genus \"Kenyapotamus\", which lived in Africa from . \"Kenyapotamus\" is known only through fragmentary fossils, but was similar in size to \"C. liberiensis\". The Hippopotamidae are believed to have evolved in Africa, and while at one point the species spread across Asia and Europe, no hippopotami have ever been discovered in the Americas. Starting the \"Archaeopotamus\", likely ancestors to the genus \"Hippopotamus\" and \"Hexaprotodon\", lived in Africa and the Middle East.\n\nWhile the fossil record of hippos is still poorly understood, the lineages of the two modern genera, \"Hippopotamus\" and \"Choeropsis\", may have diverged as far back as . The ancestral form of the pygmy hippopotamus may be the genus \"Saotherium\". \"Saotherium\" and \"Choeropsis\" are significantly more basal than \"Hippopotamus\" and \"Hexaprotodon\", and thus more closely resemble the ancestral species of hippos.\n\nSeveral species of small hippopotamids have also become extinct in the Mediterranean in the late Pleistocene or early Holocene. Though these species are sometimes known as \"pygmy hippopotami\" they are not believed to be closely related to \"C. liberiensis\". These include the Cretan dwarf hippopotamus (\"Hippopotamus creutzburgi\"), the Sicilian hippopotamus (\"Hippopotamus pentlandi\"), the Maltese hippopotamus (\"Hippopotamus melitensis\") and the Cyprus dwarf hippopotamus (\"Hippopotamus minor\").\n\nThese species, though comparable in size to the pygmy hippopotamus, are considered dwarf hippopotamuses, rather than pygmies. They are likely descended from a full-sized species of European hippopotamus, and reached their small size through the evolutionary process of insular dwarfism which is common on islands; the ancestors of pygmy hippopotami were also small and thus there was never a dwarfing process. There were also several species of pygmy hippo on the island of Madagascar (see Malagasy hippopotamus).\n\nPygmy hippos share the same general form as a hippopotamus. They have a graviportal skeleton, with four stubby legs and four toes on each foot, supporting a portly frame. The pygmy hippo, however, is only half as tall as the hippopotamus and weighs less than 1/4 as much as its larger cousin. Adult pygmy hippos stand about high at the shoulder, are in length and weigh . Their lifespan in captivity ranges from 30 to 55 years, though it is unlikely that they live this long in the wild.\n\nThe skin is greenish-black or brown, shading to a creamy gray on the lower body. Their skin is very similar to the common hippo's, with a thin epidermis over a dermis that is several centimeters thick. Pygmy hippos have the same unusual secretion as common hippos, that gives a pinkish tinge to their bodies, and is sometimes described as \"blood sweat\" though the secretion is neither sweat nor blood. This substance, hipposudoric acid, is believed to have antiseptic and sunscreening properties. The skin of hippos dries out quickly and cracks, which is why both species spend so much time in water.\n\nThe skeleton of \"C. liberiensis\" is more gracile than that of the common hippopotamus, meaning their bones are proportionally thinner. The common hippo's spine is parallel with the ground; the pygmy hippo's back slopes forward, a likely adaptation to pass more easily through dense forest vegetation. Proportionally, the pygmy hippo's legs and neck are longer and its head smaller.\nThe orbits and nostrils of a pygmy hippo are much less pronounced, an adaptation from spending less time in deep water (where pronounced orbits and nostrils help the common hippo breathe and see). The feet of pygmy hippos are narrower, but the toes are more spread out and have less webbing, to assist in walking on the forest floor.\n\nDespite adaptations to a more terrestrial life than the common hippopotamus, pygmy hippos are still more aquatic than all other even-toed ungulates. The ears and nostrils of pygmy hippos have strong muscular valves to aid submerging underwater, and the skin physiology is dependent on the availability of water.\n\nThe behavior of the pygmy hippo differs from the common hippo in many ways. Much of its behavior is more similar to that of a tapir, though this is an effect of convergent evolution. While the common hippopotamus is gregarious, pygmy hippos live either alone or in small groups, typically a mated pair or a mother and calf. Pygmy hippos tend to ignore each other rather than fight when they meet. Field studies have estimated that male pygmy hippos range over , while the range of a female is .\n\nPygmy hippos spend most of the day hidden in rivers. They will rest in the same spot for several days in a row, before moving to a new spot. At least some pygmy hippos make use of dens or burrows that form in river banks. It is unknown if the pygmy hippos help create these dens, or how common it is to use them. Though a pygmy hippo has never been observed burrowing, other artiodactyls, such as warthogs, are burrowers.\n\nLike the common hippopotamus, the pygmy hippo emerges from the water at dusk to feed. It relies on game trails to travel through dense forest vegetation. It marks trails by vigorously waving its tail while defecating to further spread its feces. The pygmy hippo spends about six hours a day foraging for food.\nPygmy hippos are herbivorous. They do not eat aquatic vegetation to a significant extent and rarely eat grass because it is uncommon in the thick forests they inhabit. The bulk of a pygmy hippo's diet consists of ferns, broad-leaved plants and fruits that have fallen to the forest floor. The wide variety of plants pygmy hippos have been observed eating suggests that they will eat any plants available. This diet is of higher quality than that of the common hippopotamus.\n\nA study of breeding behavior in the wild has never been conducted; the artificial conditions of captivity may cause the observed behavior of pygmy hippos in zoos to differ from natural conditions. Sexual maturity for the pygmy hippopotamus occurs between three and five years of age. The youngest reported age for giving birth is a pygmy hippo in the Zoo Basel, Switzerland which bore a calf at three years and three months. The oestrus cycle of a female pygmy hippo lasts an average of 35.5 days, with the oestrus itself lasting between 24–48 hours.\n\nPygmy hippos consort for mating, but the duration of the relationship is unknown. In zoos they breed as monogamous pairs. Copulation can take place on land or in the water, and a pair will mate one to four times during an oestrus period. In captivity, pygmy hippos have been conceived and born in all months of the year. The gestation period ranges from 190–210 days, and usually a single young is born, though twins are known to occur.\n\nThe common hippopotamus gives birth and mates only in the water, but pygmy hippos mate and give birth on both land and water. Young pygmy hippos can swim almost immediately. At birth, pygmy hippos weigh 4.5–6.2 kg (9.9–13.7 lb) with males weighing about 0.25 kg (0.55 lb) more than females. Pygmy hippos are fully weaned between six and eight months of age; before weaning they do not accompany their mother when she leaves the water to forage, but instead hide in the water by themselves. The mother returns to the hiding spot about three times a day and calls out for the calf to suckle. Suckling occurs with the mother lying on her side.\n\nThe greatest threat to the remaining pygmy hippopotamus population in the wild is loss of habitat. The forests in which pygmy hippos live have been subject to logging, settling and conversion to agriculture, with little efforts made to make logging sustainable. As forests shrink, the populations become more fragmented, leading to less genetic diversity in the potential mating pool.\n\nPygmy hippos are among the species illegally hunted for food in Liberia. Their meat is said to be of excellent quality, like that of a wild boar; unlike those of the common hippo, the pygmy hippo's teeth have no value. The effects of West Africa's civil strife on the pygmy hippopotamus are unknown, but unlikely to be positive. The pygmy hippopotamus can be killed by leopards, pythons and crocodiles. How often this occurs is unknown.\n\n\"C. liberiensis\" was identified as one of the top-10 \"focal species\" in 2007 by the Evolutionarily Distinct and Globally Endangered (EDGE) project. Some populations inhabit protected areas, such as the Gola Forest Reserve in Sierra Leone.\n\nBasel Zoo in Switzerland holds the international studbook and coordinates the entire captive pygmy hippo population that freely breeds in zoos around the world. Between 1970 and 1991 the population of pygmy hippos born in captivity more than doubled. The survival of the species in zoos is more certain than the survival of the species in the wild. In captivity, the pygmy hippo lives from 42 to 55 years, longer than in the wild. Since 1919, only 41 percent of pygmy hippos born in zoos have been male.\n\nWhile the common hippopotamus was known to Europeans since classical antiquity, the pygmy hippopotamus was unknown outside its range in West Africa until the 19th century. Due to their nocturnal, forested existence, they were poorly known within their range as well. In Liberia the animal was traditionally known as a \"water cow\".\n\nEarly field reports of the animal misidentified it as a wild hog. Several skulls of the species were sent to the American natural scientist Samuel G. Morton, during his residency in Monrovia, Liberia. Morton first described the species in 1843. The first complete specimens were collected as part of a comprehensive investigation of Liberian fauna in the 1870s and 1880s by Dr. Johann Büttikofer. The specimens were taken to the Natural History Museum in Leiden, The Netherlands.\n\nThe first pygmy hippo was brought to Europe in 1873 after being captured in Sierra Leone by a member of the British Colonial Service but died shortly after arrival. Pygmy hippos were successfully introduced to Europe in 1911. They were first shipped to Germany and then to the Bronx Zoo in New York City where they also thrived.\n\nIn 1927, Harvey Firestone of Firestone Tires presented Billy the pygmy hippo to U.S. President Calvin Coolidge. Coolidge donated Billy to the National Zoo in Washington, D.C. According to the zoo, Billy is a common ancestor to most pygmy hippos in U.S. zoos today.\n\nSeveral folktales have been collected about the pygmy hippopotamus. One tale says that pygmy hippos carry a shining diamond in their mouths to help travel through thick forests at night; by day the pygmy hippo has a secret hiding place for the diamond, but if a hunter catches a pygmy hippo at night the diamond can be taken. Villagers sometimes believed that baby pygmy hippos do not nurse but rather lick secretions off the skin of the mother.\n\n"}
{"id": "3932412", "url": "https://en.wikipedia.org/wiki?curid=3932412", "title": "Richard of Wallingford", "text": "Richard of Wallingford\n\nRichard of Wallingford (1292–1336) was an English mathematician, astronomer, horologist, and cleric who made major contributions to astronomy and horology while serving as abbot of St Albans Abbey in Hertfordshire.\n\nRichard was born, the son of a blacksmith, at Wallingford in Berkshire (now Oxfordshire) in England in 1292. When he was orphaned he was taken to William de Kirkeby the Prior of Holy Trinity Priory. Richard subsequently spent 6 years studying at Oxford University before becoming a monk at St Albans. He later studied for 9 more years at Oxford. In 1327 he became abbot of St Albans.\n\nRichard of Wallingford is best known for the astronomical clock he designed, while he was abbot, which is described in the \"Tractatus Horologii Astronomici\" (1327). The clock was completed about 20 years after Richard's death by William of Walsham but was apparently destroyed during Henry VIII's reformation and the dissolution of St Albans Abbey in 1539. His clock almost certainly was the most complex clock mechanism in existence at the time in the British isles, and one of the most sophisticated ones anywhere.The only other clocklike mechanism of comparable complexity that is documented in the 14th century is the astrarium by Giovanni de Dondi. It gave not just the hours and minutes of the day, but also the tide's ebb and flow as well as the motions of the sun and moon. Based on the 14th-century literary evidence still surviving in the 20th century, several scholars of horological history have tried to build recreations of Richard of Wallingford's clock. The best known of these was built by Haward Horological and for many years was displayed at the Time Museum (now defunct) in Rockford, Illinois; one was built by Eric Watson and is now in the Wallingford Museum, and one built in 1988 is located at St Albans Cathedral.\n\nRichard also designed and constructed calculation devices: a torquetum, the \"Rectangulus\", and an equatorium, which he called \"Albion\". The Albion could be used for astronomical calculations such as lunar, solar and planetary longitudes and could predict eclipses, and was capable of doing this without relying on a set of tables that had to be copied out. This is described in the \"Tractatus Albionis\". He published other works on trigonometry, celestial coordinates, astrology, and various religious works.\n\nRichard suffered from what was then thought to be leprosy (although it might have been scrofula or tuberculosis) which he apparently contracted when he went to have his position as abbot of St Albans Abbey confirmed by the Pope at Avignon. He died at St Albans in 1336.\n\n\n\n"}
{"id": "205859", "url": "https://en.wikipedia.org/wiki?curid=205859", "title": "Steven Weinberg", "text": "Steven Weinberg\n\nSteven Weinberg (; born May 3, 1933) is an American theoretical physicist and Nobel laureate in Physics for his contributions with Abdus Salam and Sheldon Glashow to the unification of the weak force and electromagnetic interaction between elementary particles.\n\nHe holds the Josey Regental Chair in Science at the University of Texas at Austin, where he is a member of the Physics and Astronomy Departments. His research on elementary particles and physical cosmology has been honored with numerous prizes and awards, including in 1979 the Nobel Prize in Physics and in 1991 the National Medal of Science. In 2004 he received the Benjamin Franklin Medal of the American Philosophical Society, with a citation that said he is \"considered by many to be the preeminent theoretical physicist alive in the world today.\" He has been elected to the US National Academy of Sciences and Britain's Royal Society, as well as to the American Philosophical Society and the American Academy of Arts and Sciences.\n\nWeinberg's articles on various subjects occasionally appear in \"The New York Review of Books\" and other periodicals. He has served as consultant at the U. S. Arms Control and Disarmament Agency, President of the Philosophical Society of Texas, and member of the Board of Editors of \"Daedalus\" magazine, the Council of Scholars of the Library of Congress, the JASON group of defense consultants, and many other boards and committees.\n\nSteven Weinberg was born in 1933 in New York City. His parents were Jewish immigrants. He graduated from Bronx High School of Science in 1950. He was in the same graduating class as Sheldon Glashow, whose own research, independent of Weinberg's, would result in their (and Abdus Salam) sharing the 1979 Nobel in Physics (see below).\n\nWeinberg received his bachelor's degree from Cornell University in 1954. He then went to the Niels Bohr Institute in Copenhagen where he started his graduate studies and research. After one year, Weinberg moved to Princeton University where he earned his PhD degree in physics in 1957, for research supervised by Sam Treiman.\n\nAfter completing his PhD, Weinberg worked as a postdoctoral researcher at Columbia University (1957–1959) and University of California, Berkeley (1959) and then he was promoted to faculty at Berkeley (1960–1966). He did research in a variety of topics of particle physics, such as the high energy behavior of quantum field theory, symmetry breaking, pion scattering, infrared photons and quantum gravity. It was also during this time that he developed the approach to quantum field theory that is described in the first chapters of his book \"The Quantum Theory of Fields\" and started to write his textbook \"Gravitation and Cosmology\". Both textbooks are among the most influential texts in the scientific community in their subjects.\n\nIn 1966, Weinberg left Berkeley and accepted a lecturer position at Harvard. In 1967 he was a visiting professor at MIT. It was in that year at MIT that Weinberg proposed his model of unification of electromagnetism and of nuclear weak forces (such as those involved in beta-decay and kaon-decay), with the masses of the force-carriers of the weak part of the interaction being explained by spontaneous symmetry breaking. One of its fundamental aspects was the prediction of the existence of the Higgs boson. Weinberg's model, now known as the electroweak unification theory, had the same symmetry structure as that proposed by Glashow in 1961: hence both models included the then-unknown weak interaction mechanism between leptons, known as neutral current and mediated by the Z boson. The 1973 experimental discovery of weak neutral currents (mediated by this Z boson) was one verification of the electroweak unification.\nThe paper by Weinberg in which he presented this theory is one of the most cited works ever in high energy physics.\n\nAfter his 1967 seminal work on the unification of weak and electromagnetic interactions, Steven Weinberg continued his work in many aspects of particle physics, quantum field theory, gravity, supersymmetry, superstrings and cosmology, as well as a theory called Technicolor.\n\nIn the years after 1967, the full Standard Model of elementary particle theory was developed through the work of many contributors. In it, the weak and electromagnetic interactions already unified by the work of Weinberg, Abdus Salam and Sheldon Glashow, are made consistent with a theory of the strong interactions between quarks, in one overarching theory. In 1973 Weinberg proposed a modification of the Standard Model which did not contain that model's fundamental Higgs boson.\n\nWeinberg became Eugene Higgins Professor of Physics at Harvard University in 1973.\n\nIn 1979 he pioneered the modern view on the renormalization aspect of quantum field theory that considers all quantum field theories as effective field theories and changed the viewpoint of previous work (including his own in his 1967 paper) that a sensible quantum field theory must be renormalizable. This approach allowed the development of effective theory of quantum gravity, low energy QCD, heavy quark effective field theory and other developments, and it is a topic of considerable interest in current research.\n\nIn 1979, some six years after the experimental discovery of the neutral currents – i.e. the discovery of the inferred existence of the Z boson – but following the 1978 experimental discovery of the theory's predicted amount of parity violation due to Z bosons' mixing with electromagnetic interactions, Weinberg was awarded the Nobel Prize in Physics, together with Sheldon Glashow, and Abdus Salam who had independently proposed a theory of electroweak unification based on spontaneous symmetry breaking.\n\nIn 1982 Weinberg moved to the University of Texas at Austin as the Jack S. Josey-Welch Foundation Regents Chair in Science and founded the \"Theory Group\" of the Physics Department.\n\nThere is current (2008) interest in Weinberg's 1976 proposal of the existence of new strong interactions – a proposal dubbed \"Technicolor\" by Leonard Susskind – because of its chance of being observed in the LHC as an explanation of the hierarchy problem.\n\nSteven Weinberg is frequently among the top scientists with highest research effect indices, such as the h-index and the creativity index.\n\nBesides his scientific research, Steven Weinberg has been a prominent public spokesman for science, testifying before Congress in support of the Superconducting Super Collider, writing articles for the \"New York Review of Books\", and giving various lectures on the larger meaning of science. His books on science written for the public combine the typical scientific popularization with what is traditionally considered history and philosophy of science and atheism.\n\nWeinberg was a major participant in what is known as the Science Wars, standing with Paul R. Gross, Norman Levitt, Alan Sokal, Lewis Wolpert, and Richard Dawkins, on the side arguing for the hard realism of science and scientific knowledge and against the constructionism proposed by such social scientists as Stanley Aronowitz, Barry Barnes, David Bloor, David Edge, Harry Collins, Steve Fuller, and Bruno Latour.\n\nAlthough still teaching physics, he has, in recent years, turned his hand to the history of science, efforts that culminated in \"To Explain the World: The Discovery of Modern Science\" (2015). A hostile review in the Wall Street Journal by Steven Shapin attracted a number of commentaries, a response by Weinberg, and an exchange of views between Weinberg and Arthur Silverstein in the \"NYRB\" in February 2016.\n\nIn 2016, he became a default figurehead for faculty and students opposed to a new law that allowed the carrying of concealed guns in UT classrooms. Weinberg announced that he would be prohibiting guns from his classes, and said he would stand by his decision to violate university regulations in this matter even if faced with a lawsuit.\n\nWeinberg married Louise Weinberg in 1954 and has one daughter, Elizabeth.\n\nWeinberg is also known for his support of Israel. He wrote an essay titled \"Zionism and Its Cultural Adversaries\" to explain his views on the issue.\n\nWeinberg has canceled trips to universities in the United Kingdom because of British boycotts directed towards Israel. He has explained:\n\nWeinberg is an atheist. Weinberg stated his views on religion in 1999:\n\nFrederick Douglass told in his Narrative how his condition as a slave became worse when his master underwent a religious conversion that allowed him to justify slavery as the punishment of the children of Ham. Mark Twain described his mother as a genuinely good person, whose soft heart pitied even Satan, but who had no doubt about the legitimacy of slavery, because in years of living in antebellum Missouri she had never heard any sermon opposing slavery, but only countless sermons preaching that slavery was God's will. With or without religion, good people can behave well and bad people can do evil; but for good people to do evil—that takes religion.\n\nThe honors and awards that Professor Weinberg received include:\n\n\nA list of Weinberg's publications can be found on the arXiv and Scopus.\n\n\n\n\n"}
{"id": "31110202", "url": "https://en.wikipedia.org/wiki?curid=31110202", "title": "TUN (product standard)", "text": "TUN (product standard)\n\nTUN is a Danish product standard numbering system identifying building materials, managed by Danish Timber & Building Merchants' Trade Organization (Trælasthandlerunionen). Currently more than 30,000 products are identified. TUN numbers are assigned to suppliers identifying products and therefore a product with several suppliers can have more than one TUN number. TUN numbers are currently being mapped against UNSPSC.\n\n"}
{"id": "31296", "url": "https://en.wikipedia.org/wiki?curid=31296", "title": "Tachyon", "text": "Tachyon\n\nA tachyon () or tachyonic particle is a hypothetical particle that always travels faster than light. Most physicists believe that faster-than-light particles cannot exist because they are not consistent with the known laws of physics. If such particles did exist, they could be used to build a tachyonic antitelephone and send signals faster than light, which (according to special relativity) would lead to violations of causality. No experimental evidence for the existence of such particles has been found.\n\nThe possibility of particles moving faster-than-light was first proposed by O. M. P. Bilaniuk, V. K. Deshpande, and E. C. G. Sudarshan in 1962, although the term they used for it was \"meta-particle\". In the 1967 paper that coined the term, Gerald Feinberg proposed that tachyonic particles could be quanta of a quantum field with imaginary mass. However, it was soon realized that excitations of such imaginary mass fields do \"not\" under any circumstances propagate faster than light, and instead the imaginary mass gives rise to an instability known as tachyon condensation. Nevertheless, in modern physics the term \"tachyon\" often refers to imaginary mass fields rather than to faster-than-light particles. Such fields have come to play a significant role in modern physics.\n\nThe term comes from the , \"tachy\", meaning \"rapid\". The complementary particle types are called luxons (which always move at the speed of light) and bradyons (which always move slower than light); both of these particle types are known to exist.\n\nIn special relativity, a faster-than-light particle would have space-like four-momentum, in contrast to ordinary particles that have time-like four-momentum. Although in some theories the mass of tachyons is regarded as imaginary, in some modern formulations the mass is considered real, the formulas for the momentum and energy being redefined to this end. Moreover, since tachyons are constrained to the spacelike portion of the energy–momentum graph, they could not slow down to subluminal speeds.\n\nIn a Lorentz invariant theory, the same formulas that apply to ordinary slower-than-light particles (sometimes called \"bradyons\" in discussions of tachyons) must also apply to tachyons. In particular the energy–momentum relation:\n(where p is the relativistic momentum of the bradyon and m is its rest mass) should still apply, along with the formula for the total energy of a particle:\nThis equation shows that the total energy of a particle (bradyon or tachyon) contains a contribution from its rest mass (the \"rest mass–energy\") and a contribution from its motion, the kinetic energy.\nWhen \"v\" is larger than \"c\", the denominator in the equation for the energy is \"imaginary\", as the value under the radical is negative. Because the total energy must be real, the numerator must \"also\" be imaginary: i.e. the rest mass m must be imaginary, as a pure imaginary number divided by another pure imaginary number is a real number.\n\nIn some modern formulations of the theory, the mass of tachyons is regarded as real.\n\nOne curious effect is that, unlike ordinary particles, the speed of a tachyon \"increases\" as its energy decreases. In particular, formula_3 approaches zero when formula_4 approaches infinity. (For ordinary bradyonic matter, \"E\" increases with increasing speed, becoming arbitrarily large as \"v\" approaches \"c\", the speed of light). Therefore, just as bradyons are forbidden to break the light-speed barrier, so too are tachyons forbidden from slowing down to below \"c\", because infinite energy is required to reach the barrier from either above or below\n\nAs noted by Einstein, Tolman, and others, special relativity implies that faster-than-light particles, if they existed, could be used to communicate backwards in time.\n\nIn 1985, Chodos proposed that neutrinos can have a tachyonic nature. The possibility of standard model particles moving at superluminal speeds can be modeled using Lorentz invariance violating terms, for example in the Standard-Model Extension. In this framework, neutrinos experience Lorentz-violating oscillations and can travel faster than light at high energies. This proposal was strongly criticized.\n\nA tachyon with an electric charge would lose energy as Cherenkov radiation—just as ordinary charged particles do when they exceed the local speed of light in a medium (other than a hard vacuum). A charged tachyon traveling in a vacuum, therefore, undergoes a constant proper time acceleration and, by necessity, its worldline forms a hyperbola in space-time. However reducing a tachyon's energy \"increases\" its speed, so that the single hyperbola formed is of \"two\" oppositely charged tachyons with opposite momenta (same magnitude, opposite sign) which annihilate each other when they simultaneously reach infinite speed at the same place in space. (At infinite speed, the two tachyons have no energy each and finite momentum of opposite direction, so no conservation laws are violated in their mutual annihilation. The time of annihilation is frame dependent.)\n\nEven an electrically neutral tachyon would be expected to lose energy via gravitational Cherenkov radiation, because it has a gravitational mass, and therefore increase in speed as it travels, as described above. If the tachyon interacts with any other particles, it can also radiate Cherenkov energy into those particles. Neutrinos interact with the other particles of the Standard Model, and Andrew Cohen and Sheldon Glashow recently used this to argue that the faster-than-light neutrino anomaly cannot be explained by making neutrinos propagate faster than light, and must instead be due to an error in the experiment.\n\nCausality is a fundamental principle of physics. If tachyons can transmit information faster than light, then according to relativity they violate causality, leading to logical paradoxes of the \"kill your own grandfather\" type. This is often illustrated with thought experiments such as the \"tachyon telephone paradox\" or \"logically pernicious self-inhibitor.\"\n\nThe problem can be understood in terms of the relativity of simultaneity in special relativity, which says that different inertial reference frames will disagree on whether two events at different locations happened \"at the same time\" or not, and they can also disagree on the order of the two events (technically, these disagreements occur when the spacetime interval between the events is 'space-like', meaning that neither event lies in the future light cone of the other).\n\nIf one of the two events represents the sending of a signal from one location and the second event represents the reception of the same signal at another location, then as long as the signal is moving at the speed of light or slower, the mathematics of simultaneity ensures that all reference frames agree that the transmission-event happened before the reception-event. However, in the case of a hypothetical signal moving faster than light, there would always be some frames in which the signal was received before it was sent so that the signal could be said to have moved backward in time. Because one of the two fundamental postulates of special relativity says that the laws of physics should work the same way in every inertial frame, if it is possible for signals to move backward in time in any one frame, it must be possible in all frames. This means that if observer A sends a signal to observer B which moves faster than light in A's frame but backwards in time in B's frame, and then B sends a reply which moves faster than light in B's frame but backwards in time in A's frame, it could work out that A receives the reply before sending the original signal, challenging causality in \"every\" frame and opening the door to severe logical paradoxes. Mathematical details can be found in the tachyonic antitelephone article, and an illustration of such a scenario using spacetime diagrams can be found in \"Baker, R. (2003)\"\n\nThe reinterpretation principle asserts that a tachyon sent \"back\" in time can always be \"reinterpreted\" as a tachyon traveling \"forward\" in time, because observers cannot distinguish between the emission and absorption of tachyons. The attempt to \"detect\" a tachyon \"from\" the future (and violate causality) would actually \"create\" the same tachyon and send it \"forward\" in time (which is causal).\n\nHowever, this principle is not widely accepted as resolving the paradoxes. Instead, what would be required to avoid paradoxes is that unlike any known particle, tachyons do not interact in any way and can never be detected or observed, because otherwise a tachyon beam could be modulated and used to create an anti-telephone or a \"logically pernicious self-inhibitor\". All forms of energy are believed to interact at least gravitationally, and many authors state that superluminal propagation in Lorentz invariant theories always leads to causal paradoxes.\n\nIn modern physics, all fundamental particles are regarded as excitations of quantum fields. There are several distinct ways in which tachyonic particles could be embedded into a field theory.\n\nIn the paper that coined the term \"tachyon\", Gerald Feinberg studied Lorentz invariant quantum fields with imaginary mass. Because the group velocity for such a field is superluminal, naively it appears that its excitations propagate faster than light. However, it was quickly understood that the superluminal group velocity does not correspond to the speed of propagation of any localized excitation (like a particle). Instead, the negative mass represents an instability to tachyon condensation, and all excitations of the field propagate subluminally and are consistent with causality. Despite having no faster-than-light propagation, such fields are referred to simply as \"tachyons\" in many sources.\n\nTachyonic fields play an important role in modern physics. Perhaps the most famous is the Higgs boson of the Standard Model of particle physics, which has an imaginary mass in its uncondensed phase. In general, the phenomenon of spontaneous symmetry breaking, which is closely related to tachyon condensation, plays an important role in many aspects of theoretical physics, including the Ginzburg–Landau and BCS theories of superconductivity. Another example of a tachyonic field is the tachyon of bosonic string theory.\n\nTachyons are predicted by bosonic string theory and also the Neveu-Schwarz (NS) and NS-NS sectors, which are respectively the open bosonic sector and closed bosonic sector, of RNS Superstring theory prior to the GSO projection. However such tachyons are not possible due to the Sen conjecture, also known as tachyon condensation. This resulted in the necessity for the GSO projection.\n\nIn theories that do not respect Lorentz invariance, the speed of light is not (necessarily) a barrier, and particles can travel faster than the speed of light without infinite energy or causal paradoxes. A class of field theories of that type is the so-called Standard Model extensions. However, the experimental evidence for Lorentz invariance is extremely good, so such theories are very tightly constrained.\n\nBy modifying the kinetic energy of the field, it is possible to produce Lorentz invariant field theories with excitations that propagate superluminally. However, such theories, in general, do not have a well-defined Cauchy problem (for reasons related to the issues of causality discussed above), and are probably inconsistent quantum mechanically.\n\nThe term \"tachyon\" was coined by Gerald Feinberg in a 1967 paper titled \"Possibility of Faster-Than-Light Particles\". He had been inspired by the science-fiction story \"Beep\" by James Blish. Feinberg studied the kinematics of such particles according to special relativity. In his paper he also introduced fields with imaginary mass (now also referred to as \"tachyons\") in an attempt to understand the microphysical origin such particles might have.\n\nThe first hypothesis regarding faster-than-light particles is sometimes attributed to German physicist Arnold Sommerfeld in 1904, and more recent discussions happened in 1962 and 1969.\n\nIn September 2011, it was reported that a tau neutrino had traveled faster than the speed of light in a major release by CERN; however, later updates from CERN on the OPERA project indicate that the faster-than-light readings were resultant from \"a faulty element of the experiment's fibre optic timing system\".\n\nTachyons have appeared in many works of fiction. They have been used as a standby mechanism upon which many science fiction authors rely to establish faster-than-light communication, with or without reference to causality issues. The word \"tachyon\" has become widely recognized to such an extent that it can impart a science-fictional connotation even if the subject in question has no particular relation to superluminal travel (a form of technobabble, akin to \"positronic brain\").\n\n\n"}
{"id": "10222635", "url": "https://en.wikipedia.org/wiki?curid=10222635", "title": "VA (Public &amp; Science)", "text": "VA (Public &amp; Science)\n\nVA (Public & Science) (Swedish: \"\") is a Swedish non-profit civil society organisation that works to promote dialogue and openness between researchers and the public.\n\nThe organisation was founded in 2002 and is based in Stockholm, Sweden.\n\nVA's members consist of some 80 organisations, authorities, universities, companies and associations. In addition, it has a number of individual members. The organisation is funded through membership fees, project grants and a grant from the Swedish Ministry of Education and Research.\n\nVA's main aims:\n\nVA carries out surveys and studies with the aim of increasing knowledge about the relationships between science and society at large. This includes an annual barometer into the Swedish public's general attitudes towards science and researchers, as well as more specific studies on how opinion-forming groups in society view and use research, how researchers interact with society, and methods for dialogue on science and research.\n\nVA also arranges many events and activities aimed at stimulating dialogue between researchers and the public in new ways and in novel arenas. VA is the Swedish national co-ordinator for the science festival European Researchers’ Night. The first Researchers' Night was run in Sweden in September 2005 and in 2017 events were run in 29 cities in Sweden. \nVA also organises the Researchers' Grand Prix , a science communication competition for researchers and mass experiments in schools, a citizen science initiative that engages pupils in real research. Other activities include Science Cafés. VA also participates in societal debate via traditional as well as social media.\n\nVA is currently a partner in three EU-funded Horizon 2020 projects SciShops: \"ORION, Open Responsible research and Innovation to further Outstanding Knowledge\", is aimed at fostering RRI and open science in research performing and research funding organisations. \"SciShops\" will expand the ecosystem of Science Shops in Europe and \"BLOOM\" is aimed at raising public awareness and interest in the bioeconomy through dialogue and co-creation activities.\n\nVA is a member of EUSEA (European Science Events Association), ECSA (European Citizen Science Association) and the Living Knowledge network.\n\nSecretary Generals\n\nChairs\n\n"}
{"id": "47800230", "url": "https://en.wikipedia.org/wiki?curid=47800230", "title": "Valdemar Poulsen Gold Medal", "text": "Valdemar Poulsen Gold Medal\n\nThe Valdemar Poulsen Gold Medal, named after radio pioneer Valdemar Poulsen, was awarded each year for outstanding research in the field of radio techniques and related fields by the . The award was presented on November 23, the anniversary of Poulsen's birth. The award was discontinued in 1993.\n\n\n"}
{"id": "38464926", "url": "https://en.wikipedia.org/wiki?curid=38464926", "title": "Zuni ethnobotany", "text": "Zuni ethnobotany\n\nThis is a list of plants and how they are used in Zuni culture.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
