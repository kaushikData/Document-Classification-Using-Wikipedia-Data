{"id": "53850430", "url": "https://en.wikipedia.org/wiki?curid=53850430", "title": "Acholeplasma virus MV-L51", "text": "Acholeplasma virus MV-L51\n\nAcholeplasma phage MV-L51 (L51) is a virus species. It is the type species of the plectrovirus genus.\n"}
{"id": "47560448", "url": "https://en.wikipedia.org/wiki?curid=47560448", "title": "Agassiz (crater)", "text": "Agassiz (crater)\n\nAgassiz is an impact crater on Mars, named in honor of geologist Louis Agassiz(1807–1873). The crater is in diameter and is located at 271.1°E 69.8°N. The name was approved in 1973 by the International Astronomical Union (IAU) Working Group for Planetary System Nomenclature (WGPSN).\n\nNearly a crater diameter to the east is the large Schmidt and somewhat 50 km west (half a crater diameter) is Heaviside, also further northwest in the Thaumasia quadrangle is the crater Bianchini. North is Argentea Planum (Argentean Plain) and south is Parva Planum.\n\n"}
{"id": "966870", "url": "https://en.wikipedia.org/wiki?curid=966870", "title": "Baird's tapir", "text": "Baird's tapir\n\nBaird's tapir (\"Tapirus bairdii\"), also known as the Central American tapir, is a species of tapir native to Mexico, Central America and northwestern South America. It is one of four Latin American species of tapir.\n\nBaird's tapir is named for the American naturalist Spencer Fullerton Baird, who traveled to Mexico in 1843 and observed the animals. However, the species was first documented by another American naturalist, W. T. White. The tapir is the largest land mammal in Central America.\n\nLike the other Latin American tapirs (the mountain tapir, the South American tapir, and the little black tapir), the Baird's tapir is commonly called \"danta\" by people in all areas. In the regions around Oaxaca and Veracruz, it is referred to as the \"anteburro\". Panamanians, and Colombians call it \"macho de monte\", and in Belize, where the Baird's tapir is the national animal, it is known as the mountain cow.\n\nIn Mexico, it is called \"tzemen\" in Tzeltal; in Lacandon, it is called \"cash-i-tzimin\", meaning \"jungle horse\" and in Tojolab'al it is called \"niguanchan\", meaning \"big animal\". In Panama, the Kunas people call the Baird's tapir \"moli\" in their colloquial language (Tule kaya), \"oloalikinyalilele\", \"oloswikinyaliler,\" or \"oloalikinyappi\" in their political language (Sakla kaya), and \"ekwirmakka\" or \"ekwilamakkatola\" in their spiritual language (Suar mimmi kaya).\n\nBaird's tapir has a distinctive cream-colored marking on its face and throat and a dark spot on each cheek, behind and below the eye. The rest of its hair is dark brown or grayish brown. This tapir is the largest of the four American species and the largest native land mammal in both Central and South America. Baird's tapirs average in length but can range between , not counting a stubby, vestigal tail of , and in height. Body mass in adults can range from . Like the other species of tapirs, they have small, stubby tails and long, flexible proboscises. They have four toes on each front foot and three toes on each back foot.\n\nThe gestation period is about 400 days, after which one offspring is born. Multiple births are extremely rare. The babies, as with all species of tapir, have reddish-brown hair with white spots and stripes, a camouflage which affords them excellent protection in the dappled light of the forest. This pattern eventually fades into the adult coloration.\n\nFor the first week of their lives, infant Baird's tapirs are hidden in secluded locations while their mothers forage for food and return periodically to nurse them. Later, the young follow their mothers on feeding expeditions. At three weeks of age, the young are able to swim. Weaning occurs after one year, and sexual maturity is usually reached six to 12 months later. Baird's tapirs can live for over 30 years.\n\nBaird's tapir may be active at all hours, but is primarily nocturnal. It forages for leaves and fallen fruit, using well-worn tapir paths which zig-zag through the thick undergrowth of the forest. The animal usually stays close to water and enjoys swimming and wading – on especially hot days, individuals will rest in a watering hole for hours with only their heads above water.\n\nIt generally leads a solitary life, though feeding groups are not uncommon and individuals, especially those of different ages (young with their mothers, juveniles with adults) are often observed together. The animals communicate with one another through shrill whistles and squeaks.\n\nAdults can be potentially dangerous to humans and should not be approached if spotted in the wild. The animal is most likely to follow or chase a human for a bit, though they have been known to charge and gore humans on rare occasions.\n\nAccording to the IUCN, Baird's tapir is in danger of extinction, and in 1996 it was officially classified as \"Vulnerable\". There are two main contributing factors in the decline of the species; poaching and habitat loss. Though in many areas the animal is only hunted by a few humans, any loss of life is a serious blow to the tapir population, especially because their reproductive rate is so slow.\n\nIn Mexico, Belize, Guatemala, Costa Rica, and Panama, hunting of the Baird's tapirs is illegal, but the laws protecting them are often unenforced. Furthermore, restrictions against hunting do not address the problem of deforestation. Therefore, many conservationists focus on environmental education and sustainable forestry to try to save the Baird's tapir and other rainforest species from extinction.\n\nAttacks on humans are rare and normally in self-defense. In 2006, Carlos Manuel Rodríguez Echandi, the former Costa Rican Minister of Environment and Energy, was attacked and injured by a tapir after he followed it off the trail.\n\nAn adult Baird's tapir, being such a massive mammal, has very few natural predators. Only large adult American crocodiles ( or more) and adult jaguars are capable of preying on tapirs, although even in these cases the outcomes are unpredictable and often in the tapir's favor (as is evident on multiple tapirs documented in Corcovado National Park with large claw marks covering their hides).\n\n"}
{"id": "3139737", "url": "https://en.wikipedia.org/wiki?curid=3139737", "title": "Batchelor vortex", "text": "Batchelor vortex\n\nIn fluid dynamics, Batchelor vortices, first described by George Batchelor in a 1964 article, have been found useful in analyses of airplane vortex wake hazard problems.\n\nThe Batchelor vortex is an approximate solution to the Navier-Stokes equations obtained using a boundary layer approximation. The physical reasoning behind this approximation is the assumption that the axial gradient of the flow field of interest is of much smaller magnitude than the radial gradient. \n<br>\nThe axial, radial and azimuthal velocity components of the vortex are denoted formula_1,formula_2 and formula_3 respectively and can be represented in cylindrical coordinates formula_4 as follows:<br>\nThe parameters in the above equations are\n<br>\nNote that the radial component of the velocity is zero and that the axial and azimuthal components depend only on formula_13. \n<br>\nWe now write the system above in dimensionless form by scaling time by a factor formula_14. Using the same symbols for the dimensionless variables, the Batchelor vortex can be expressed in terms of the dimensionless variables as\n<br>\nwhere formula_16 denotes the free stream axial velocity and formula_17 is the Reynolds number.\n\nIf one lets formula_18 and considers an infinitely large swirl number then the Batchelor vortex simplifies to the Lamb–Oseen vortex for the azimuthal velocity:\n\nwhere formula_20 is the circulation.\n\n"}
{"id": "49771479", "url": "https://en.wikipedia.org/wiki?curid=49771479", "title": "CGTG-102", "text": "CGTG-102\n\nCGTG-102 (Ad5/3-D24-GMCSF), (developed by Oncos Therapeutics) is an oncolytic adenovirus currently in orphan drug status for soft tissue sarcomas.\n\nIt is modified to selectively replicate in p16/Rb-defective cells, which include most human cancer cells. In addition, CGTG-102 codes for the granulocyte–macrophage colony-stimulating factor (GM-CSF), a potent immunostimulatory molecule.\n\nThe therapeutic potential of oncolytic virotherapy is not a simple consequence of the cytopathic effect but strongly relies on the induction of an endogenous immune response against transformed cells. Superior anticancer effects have been observed when oncolytic viruses are engineered to express (or be co-administered with) immunostimulatory molecules such as GM-CSF.\n\nWhile the CGTG-102 oncolytic adenovirus has shown efficacy as a single agent against several soft tissue sarcomas, it would also be appealing to use in combination with other regimes, as oncolytic viruses have demonstrated very little overlap in side effects with traditional therapies such as chemotherapy and radiation. CGTG-102 has recently been studied in combination with doxorubicin, and a synergistic effect was observed. At least part of doxorubicin’s mechanism of action is as an inducer of immunogenic cell death, and it has been suggested that immune response contributes to its overall anti-tumor activity. Doxorubicin has been shown to increase adenoviral replication in soft tissue sarcoma cells as well, potentially contributing to the observed synergistic effect in the virus/doxorubicin combination.\n\nWhile in phase I was already used to treat 200 advanced cancer patients in the company's Advanced Therapy Access Program.\n"}
{"id": "8899765", "url": "https://en.wikipedia.org/wiki?curid=8899765", "title": "Carbuncle (gemstone)", "text": "Carbuncle (gemstone)\n\nA carbuncle is any red gemstone, most often a red garnet. The word occurs in four places in many English translations of the Bible. The English King James translation is a rendering of the Hebrew Masoretic text (Old Testament) and Greek Textus Receptus (New Testament). \"Carbunculus\" is a word used in Latin for a small coal (or charcoal), and also for any of a number of precious or semi-precious stones, especially those of a red color.; Jerome apparently chose the term because of its similarity in meaning to the Septuagint's ἄνθραξ (\"anthrax\" meaning \"coal\"), which was in turn used by the Greek to translate the Hebrew נֹפֶךְ (\"nōphek\") in two of its four occurrences in the Old Testament. The etymology of the Hebrew term is uncertain, though Koehler-Baumgartner suggests a connection to פּוּךְ (\"phook\"), used in the Old Testament as a term for eye makeup, and probably implying a colored powder most likely made from a crushed mineral. For נֹפֶךְ (\"nōphek\") itself they suggest the gloss \"semi-precious stone\" (of uncertain color).\n\n"}
{"id": "275473", "url": "https://en.wikipedia.org/wiki?curid=275473", "title": "Control system", "text": "Control system\n\nA control system manages, commands, directs, or regulates the behavior of other devices or systems using control loops. It can range from a single home heating controller using a thermostat controlling a domestic boiler to large Industrial control systems which are used for controlling processes or machines.\n\nFor continuously modulated control, a feedback controller is used to automatically control a process or operation. The control system compares the value or status of the process variable (PV) being controlled with the desired value or setpoint (SP), and applies the difference as a control signal to bring the process variable output of the plant to the same value as the setpoint.\n\nFor sequential and combinational logic, software logic, such as in a programmable logic controller, is used.\n\nThere are two common classes of control action: open loop and closed loop. In an open-loop control system, the control action from the controller is independent of the process variable. An example of this is a central heating boiler controlled only by a timer. The control action is the switching on or off of the boiler. The process variable is the building temperature.This controller operates the heating system for a constant time regardless of the temperature of the building.\n\nIn a closed-loop control system, the control action from the controller is dependent on the desired and actual process variable. In the case of the boiler analogy, this would utilise a thermostat to monitor the building temperature, and feed back a signal to ensure the controller output maintains the building temperature close to that set on the thermostat. A closed loop controller has a feedback loop which ensures the controller exerts a control action to control a process variable at the same value as the setpoint. For this reason, closed-loop controllers are also called feedback controllers.\n\nIn the case of linear feedback systems, a control loop including sensors, control algorithms, and actuators is arranged in an attempt to regulate a variable at a setpoint (SP). An everyday example is the cruise control on a road vehicle; where external influences such as gradients would cause speed changes, and the driver has the ability to alter the desired set speed. The PID algorithm in the controller restores the actual speed to the desired speed in the optimum way, with minimal delay or overshoot, by controlling the power output of the vehicle's engine.\n\nControl systems that include some sensing of the results they are trying to achieve are making use of feedback and can adapt to varying circumstances to some extent. Open-loop control systems do not make use of feedback, and run only in pre-arranged ways.\n\nLogic control systems for industrial and commercial machinery were historically implemented by interconnected electrical relays and cam timers using ladder logic. Today, most such systems are constructed with microcontrollers or more specialized programmable logic controllers (PLCs). The notation of ladder logic is still in use as a programming method for PLCs.\n\nLogic controllers may respond to switches and sensors, and can cause the machinery to start and stop various operations through the use of actuators. Logic controllers are used to sequence mechanical operations in many applications. Examples include elevators, washing machines and other systems with interrelated operations. An automatic sequential control system may trigger a series of mechanical actuators in the correct sequence to perform a task. For example, various electric and pneumatic transducers may fold and glue a cardboard box, fill it with product and then seal it in an automatic packaging machine.\n\nPLC software can be written in many different ways – ladder diagrams, SFC (sequential function charts) or statement lists.\n\nA thermostat can be described as a bang-bang controller. When the temperature, PV, goes below a SP, the heater is switched on. Another example could be a pressure switch on an air compressor. When the pressure, PV, drops below the setpoint, SP, the pump is powered. Refrigerators and vacuum pumps contain similar mechanisms.\n\nSimple on–off control systems like these are cheap and effective.\n\nLinear control systems use linear negative feedback to produce a control signal to maintain the controlled process variable (PV) at the desired setpoint (SP).\n\nProportional control is a type of linear feedback control system in which a correction is applied to the controlled variable which is proportional to the difference between the desired value (setpoint – SP) and the measured value (process value – PV). Two classic mechanical examples are the toilet bowl float proportioning valve and the fly-ball governor.\n\nThe proportional control system is more complex than an on–off control system like a bi-metallic domestic thermostat, but simpler than a proportional-integral-derivative (PID) control system used in something like an automobile cruise control. On–off control will work quite well eventually, over a long time compared to the overall system response time, but is not effective for rapid and timely corrections and responses. Proportional control overcomes this by modulating the output to the controlling device, such as a control valve at a level which avoids instability, but applies correction as fast as practicable by applying the optimum quantity of proportional correction.\n\nA drawback of proportional control is that it cannot eliminate the residual SP–PV error, as it requires an error to generate a proportional output. To overcome this the PI controller was devised, which uses a proportional term (P) to remove the gross error, and an integral term (I) to eliminate the residual offset error by integrating the error over time to produce an \"I\" component within the controller output.\n\nIn some systems there are practical limits to the range of the manipulated variable (MV). For example, a heater can be off or fully on, or a valve can be closed or fully open. Adjustments to the gain simultaneously alter the range of error values over which the MV is between these limits. The width of this range, in units of the error variable and therefore of the PV, is called the \"proportional band\" (PB) which is the inverse of the proportional gain. While the gain is useful in mathematical treatments, the proportional band is often referred to in practical situations.\n\nWhen controlling the temperature of an industrial furnace, it is usually better to control the opening of the fuel valve \"in proportion to\" the current needs of the furnace. This helps avoid thermal shocks and applies heat more effectively.\n\nAt low gains, only a small corrective action is applied when errors are detected. The system may be safe and stable, but may be sluggish in response to changing conditions. Errors will remain uncorrected for relatively long periods of time and the system is overdamped. If the proportional gain is increased, such systems become more responsive and errors are dealt with more quickly. There is an optimal value for the gain setting when the overall system is said to be critically damped. Increases in loop gain beyond this point lead to oscillations in the PV and such a system is underdamped.\n\nIn the furnace example, suppose the temperature is increasing towards a setpoint at which, say, 50% of the available power will be required for steady-state. At low temperatures, 100% of available power is applied. When the process value (PV) is within, say 10° of the SP the heat input begins to be reduced by the proportional controller. This implies a 20° proportional band (PB) from full to no power input, evenly spread around the setpoint value. At the setpoint the controller will be applying 50% power as required, but stray stored heat within the heater sub-system and in the walls of the furnace will keep the measured temperature rising beyond what is required. At 10° above SP, we reach the top of the proportional band (PB) and no power is applied, but the temperature may continue to rise even further before beginning to fall back. Eventually as the PV falls back into the PB, heat is applied again, but now the heater and the furnace walls are too cool and the temperature falls too low before its fall is arrested, so that the oscillations continue.\n\nThe temperature oscillations that an underdamped furnace control system produces are unacceptable for many reasons, including the waste of fuel and time (each oscillation cycle may take many minutes), as well as the likelihood of seriously overheating both the furnace and its contents.\n\nSuppose that the gain of the control system is reduced drastically and it is restarted. As the temperature approaches, say 30° below SP (A 60° proportional band (PB) this time), the heat input begins to be reduced, the rate of heating of the furnace has time to slow and, as the heat is still further reduced, it eventually is brought up to setpoint, just as 50% power input is reached and the furnace is operating as required. There was some wasted time while the furnace crept to its final temperature using only 52% then 51% of available power, but at least no harm was done. By carefully increasing the gain (i.e. reducing the width of the PB) this overdamped and sluggish behavior can be improved until the system is critically damped for this SP temperature. Doing this is known as 'tuning' the control system. A well-tuned proportional furnace temperature control system will usually be more effective than on-off control, but will still respond more slowly than the furnace could under skillful manual control.\n\nApart from sluggish performance to avoid oscillations, another problem with proportional-only control is that power application is always in direct proportion to the error. In the example above we assumed that the set temperature could be maintained with 50% power. What happens if the furnace is required in a different application where a higher set temperature will require 80% power to maintain it? If the gain was finally set to a 50° PB, then 80% power will not be applied unless the furnace is 15° below setpoint, so for this other application the operators will have to remember always to set the setpoint temperature 15° higher than actually needed. This 15° figure is not completely constant either: it will depend on the surrounding ambient temperature, as well as other factors that affect heat loss from or absorption within the furnace.\n\nTo resolve these two problems, many feedback control schemes include mathematical extensions to improve performance. The most common extensions lead to proportional-integral-derivative control, or PID control.\n\nThe derivative part is concerned with the rate-of-change of the error with time: If the measured variable approaches the setpoint rapidly, then the actuator is backed off early to allow it to coast to the required level; conversely if the measured value begins to move rapidly away from the setpoint, extra effort is applied—in proportion to that rapidity—to try to maintain it.\n\nDerivative action makes a control system behave much more intelligently. On control systems like the tuning of the temperature of a furnace, or perhaps the motion-control of a heavy item like a gun or camera on a moving vehicle, the derivative action of a well-tuned PID controller can allow it to reach and maintain a setpoint better than most skilled human operators could.\n\nIf derivative action is over-applied, it can lead to oscillations too. An example would be a PV that increased rapidly towards SP, then halted early and seemed to \"shy away\" from the setpoint before rising towards it again.\n\nThe integral term magnifies the effect of long-term steady-state errors, applying ever-increasing effort until they reduce to zero. In the example of the furnace above working at various temperatures, if the heat being applied does not bring the furnace up to setpoint, for whatever reason, integral action increasingly \"moves\" the proportional band relative to the setpoint until the PV error is reduced to zero and the setpoint is achieved.\n\nSome controllers include the option to limit the \"ramp up % per minute\". This option can be very helpful in stabilizing small boilers (3 MBTUH), especially during the summer, during light loads.\nA utility boiler \"unit may be required to change load at a rate of as much as 5% per minute (IEA Coal Online - 2, 2007)\".\n\nIt is possible to filter the PV or error signal. Doing so can reduce the response of the system to undesirable frequencies, to help reduce instability or oscillations. Some feedback systems will oscillate at just one frequency. By filtering out that frequency, more \"stiff\" feedback can be applied, making the system more responsive without shaking itself apart.\n\nFeedback systems can be combined. In cascade control, one control loop applies control algorithms to a measured variable against a setpoint, but then provides a varying setpoint to another control loop rather than affecting process variables directly. If a system has several different measured variables to be controlled, separate control systems will be present for each of them.\n\nControl engineering in many applications produces control systems that are more complex than PID control. Examples of such fields include fly-by-wire aircraft control systems, chemical plants, and oil refineries. Model predictive control systems are designed using specialized computer-aided-design software and empirical mathematical models of the system to be controlled.\n\nHybrid systems of PID and logic control are widely used. The output from a linear controller may be interlocked by logic for instance.\n\nFuzzy logic is an attempt to apply the easy design of logic controllers to the control of complex continuously varying systems. Basically, a measurement in a fuzzy logic system can be partly true, that is if yes is 1 and no is 0, a fuzzy measurement can be between 0 and 1.\n\nThe rules of the system are written in natural language and translated into fuzzy logic. For example, the design for a furnace would start with: \"If the temperature is too high, reduce the fuel to the furnace. If the temperature is too low, increase the fuel to the furnace.\"\n\nMeasurements from the real world (such as the temperature of a furnace) are converted to values between 0 and 1 by seeing where they fall on a triangle. Usually, the tip of the triangle is the maximum possible value which translates to 1.\n\nFuzzy logic, then, modifies Boolean logic to be arithmetical. Usually the \"not\" operation is \"output = 1 - input,\" the \"and\" operation is \"output = input.1 multiplied by input.2,\" and \"or\" is \"output = 1 - ((1 - input.1) multiplied by (1 - input.2))\". This reduces to Boolean arithmetic if values are restricted to 0 and 1, instead of allowed to range in the unit interval [0,1].\n\nThe last step is to \"defuzzify\" an output. Basically, the fuzzy calculations make a value between zero and one. That number is used to select a value on a line whose slope and height converts the fuzzy value to a real-world output number. The number then controls real machinery.\n\nIf the triangles are defined correctly and rules are right the result can be a good control system.\n\nWhen a robust fuzzy design is reduced into a single, quick calculation, it begins to resemble a conventional feedback loop solution and it might appear that the fuzzy design was unnecessary. However, the fuzzy logic paradigm may provide scalability for large control systems where conventional methods become unwieldy or costly to derive.\n\nFuzzy electronics is an electronic technology that uses fuzzy logic instead of the two-value logic more commonly used in digital electronics.\n\nThe range of implementation is from compact controllers often with dedicated software for a particular machine or device, to distributed control systems for industrial process control.\n\nLogic systems and feedback controllers are usually implemented with programmable logic controllers.\n\n"}
{"id": "34248737", "url": "https://en.wikipedia.org/wiki?curid=34248737", "title": "Courtenay Adrian Ilbert", "text": "Courtenay Adrian Ilbert\n\nCourtenay Adrian Ilbert (1888–1956), a professional civil engineer interested in horology, was a notable private collector of watches. He brought together the most important collection of watches ever achieved by a private collector. In 1958, after his death, his collection was acquired by the British Museum. Initially, the collection had been put up for auction, but was saved for the public by a private donation to the British Museum for this purpose and the auction was subsequently cancelled. The collection, now known as the Ilbert collection, includes the Earnshaw 509 chronometer one of only two surviving from the voyage of the Beagle.\n"}
{"id": "13757128", "url": "https://en.wikipedia.org/wiki?curid=13757128", "title": "Cuprate superconductor", "text": "Cuprate superconductor\n\nCuprate superconductors are high temperature superconductors made of cuprates. They are layered materials, consisting of superconducting CuO layers separated by spacer layers.\n\nInterest in cuprates sharply increased in 1986 with the discovery of high-temperature superconductivity in the Non-stoichiometric cuprate lanthanum barium copper oxide LaBaCuO. The T for this material was 35 K, well above the previous record of 23K. Thousands of publications examine the superconductivity in cuprates between 1986 and 2001, and Bednorz and Müller were awarded the Nobel Prize in Physics only a year after their discovery.\n\nFrom 1986 to 2008, many cuprate superconductors were identified, the most famous being yttrium barium copper oxide (YBaCuO, \"YBCO\" or \"1-2-3\"). Another example is bismuth strontium calcium copper oxide (BSCCO or BiSrCaCuO) with T = 95–107 K depending on the \"n\" value. Thallium barium calcium copper oxide (TBCCO, TlBaCaCuO) was the next class of high-T cuprate superconductors with T = 127 K observed in TlBaCaCuO (TBCCO-2223) in 1988. The highest confirmed, ambient-pressure, T is 135 K, achieved in 1993 with the layered cuprate HgBaCaCuO. Few months later, another team measured superconductivity above 150K in the same compound under applied pressure (153 K at 150 kbar).\n\nCuprate superconductors usually feature copper oxides in both the oxidation state 3+ as well as 2+. For example, YBaCuO is described as Y(Ba)(Cu)(Cu)(O). All superconducting cuprates are layered materials having a complex structure described as a superlattice of superconducting CuO layers separated by spacer layers where the misfit strain between different layers and dopants in the spacers induce a complex heterogeneity that in the superstripes scenario is intrinsic for high temperature superconductivity.\n\nBSCCO superconductors already have large-scale applications. For example, tens of kilometers of BSCCO-2223 superconductive tape are being used (at 77 K) in the current leads of the Large Hadron Collider, (but the main field coils are using low temperature superconductors).\n"}
{"id": "39140", "url": "https://en.wikipedia.org/wiki?curid=39140", "title": "Dyson's eternal intelligence", "text": "Dyson's eternal intelligence\n\nDyson's eternal intelligence concept (the Dyson Scenario), proposed by Freeman Dyson in 1979, \nproposes a means by which an immortal society of intelligent beings in an open universe\nmay escape the prospect of heat death by extending subjective time to infinity even though expending only a finite amount of energy.\n\nBremermann's limit can be invoked to deduce that the amount of time to perform a computation on 1 bit is inversely proportional to the change in energy in the system. As a result, the amount of computations that can be performed grows logarithmically over time. Therefore, any arbitrary amount of computation can be performed in a finite, albeit exponentially growing, time span. \n\nThe intelligent beings would begin by storing a finite amount of energy. They then use half (or any fraction) of this energy to power their thought. When the energy gradient created by unleashing this fraction of the stored fuel was exhausted, the beings would enter a state of zero-energy-consumption until the universe cooled. Once the universe had cooled sufficiently, half of the remaining half (one quarter of the original energy) of the intelligent beings' fuel reserves would once again be released, powering a brief period of thought once more. This would continue, with smaller and smaller amounts of energy being released. As the universe cooled, the thoughts would be slower and slower, but there would still be an infinite number of them. \n\nIn 1998 it was discovered that the expansion of the universe appears to be accelerating rather than decelerating due to a positive cosmological constant, implying that any two regions of the universe will eventually become permanently separated from one another.\n\nFrank J. Tipler has cited Dyson's writings, and specifically his writings on the eternal intelligence, as a major influence on his own highly controversial Omega Point theory.\nTipler's theory differs from Dyson's theory on several key points, most notable of which is that Dyson's eternal intelligence presupposes an open universe while Tipler's Omega Point presupposes a closed/ contracting universe. Both theories will be invalidated if the observed universal expansion continues to accelerate.\n\n"}
{"id": "27868007", "url": "https://en.wikipedia.org/wiki?curid=27868007", "title": "Dzhalindite", "text": "Dzhalindite\n\nDzhalindite is a rare indium hydroxide mineral discovered in Siberia. Its chemical formula is In(OH). \n\nIt was first described in 1963 for an occurrence in the Dzhalinda tin deposit, Malyi Khingan Range, Khabarovskiy Kray, Far-Eastern Region, Russia.\n\nIt has also been reported from Mount Pleasant, New Brunswick, Canada; the Flambeau mine, Ladysmith, Rusk County, Wisconsin, US; in the Mangabeira tin deposit, Goiás, Brazil; Attica, mines of the Lavrion District, Greece; Erzgebirge, Saxony, Germany; the Krušné Hory Mountains of Bohemia, Czech Republic; the Chubu Region, Honshu Island, Japan; and the Arashan Massif of Tashkent, Uzbekistan.\n\n"}
{"id": "15911846", "url": "https://en.wikipedia.org/wiki?curid=15911846", "title": "Eureka TV", "text": "Eureka TV\n\nEureka TV is a British children's television series about science that ran from 2001 to 2005 on the children's TV channel CBBC.\n\n\nHigh Tech Eureka\nThe latest technology. \n\nMicro Eureka\n\nShowed an everyday object, magnified hundreds of times.\nLittle Eureka\nWild Eureka\nBig Eureka\nPaper Eureka\nPresenters\n\n\nThe first series aired on 24 September 2001 and ended on 17 December 2001. The series was presented by Kate Heavenor and Fearne Cotton. This series had all of the features above except for Paper and Micro Eureka.\n\nThe series was the second which aired midway through 2003, it was put off an extra few months than intended because of the new science show X-perimental which was presented by Ortis Deley and Holly Willoughby. In the series there was the addition of Paper and Micro Eureka.\n\nThis was the final series, which was presented by Mohini Sule and Kate Heavoner. Eureka Mondays was added, on BBC One and BBC Two, presented by Monhini Sule and Sophie McDonnell in the morning and Kate Heavoner along with Angellica Bell and Andrew Hayden Smith in the afternoons. All the same features remained in this series as in the series before.\n\n"}
{"id": "1205478", "url": "https://en.wikipedia.org/wiki?curid=1205478", "title": "Explorer 11", "text": "Explorer 11\n\nExplorer 11 (also known as S15) was an American Earth-orbital satellite that carried the first space-borne gamma-ray telescope. This was the earliest beginning of space gamma-ray astronomy. Launched on April 27, 1961 by a Juno II rocket the satellite returned data until November 17, when power supply problems ended the science mission. During the spacecraft's seven-month lifespan it detected twenty-two events from gamma-rays and approximately 22,000 events from cosmic radiation.\n\nThe Explorer 11 telescope, developed at MIT under the direction of William L. Kraushaar, used a combination of a sandwich scintillator detector along with a Cherenkov counter to measure the arrival directions and energies of high-energy gamma rays. Since the telescope could not be aimed, the spacecraft was set in a slow spin to scan the celestial sphere. Due to a higher than planned orbit that carried the spacecraft into the detector-jamming radiation of the Van Allen belt, and an early failure of the on-board tape recorder, only 141 hours of useful observing time could be culled from about 7 months during which the instrument operated. During this time thirty-one \"gamma-ray signature\" events were recorded when the telescope was pointing in directions well away from the Earth's atmosphere, which is a relatively bright source of gamma rays produced in interactions of ordinary cosmic ray protons with air atoms. The celestial distribution of the thirty-one arrival directions showed no statistically significant correlation with the direction of any potential cosmic source. Lacking such a correlation, the identification of the cause of the thirty-one events as gamma rays of cosmic origin could not be established. The results of the experiment were therefore reported as upper limits that were significantly lower than the limits obtained from previous balloon-borne experiments.\n\nAn improved gamma-ray telescope, also developed at MIT under the direction of W. L. Kraushaar, was flown on the Third Solar Observatory (OSO-3), which was launched in 1967. It achieved the first definitive observation of high-energy cosmic gamma rays from both galactic and extragalactic sources. Later experiments, both in orbit and on the ground have identified numerous discrete sources of cosmic gamma rays in our Galaxy and beyond.\n\nKraushaar, W. L., G. W. Clark, G. Garmire, H. Helmken, P. Higbie, and M. Agogino 1965. Explorer XI experiment on cosmic gamma rays. Ap. J. 141:845-863. (http://adsabs.harvard.edu/abs/1965ApJ...141..845K)\n\n"}
{"id": "9704008", "url": "https://en.wikipedia.org/wiki?curid=9704008", "title": "Fimbria (bacteriology)", "text": "Fimbria (bacteriology)\n\nIn bacteriology, a fimbria (Latin for 'fringe', plural fimbriae), also referred to as an \"attachment pilus\" by some scientists, is an appendage that can be found on many Gram-negative and some Gram-positive bacteria that is thinner and shorter than a flagellum. This appendage ranges from 3-10 nanometers in diameter and can be up to several micrometers long. Fimbriae are used by bacteria to adhere to one another and to adhere to animal cells and some inanimate objects. A bacterium can have as many as 1,000 fimbriae. Fimbriae are only visible with the use of an electron microscope. They may be straight or flexible.\n\nFimbriae carry adhesins which attach them to the substratum so that the bacteria can withstand shear forces and obtain nutrients. For example, \"E. coli\" uses them to attach to mannose receptors. \n\nSome aerobic bacteria form a very thin layer at the surface of a broth culture. This layer, called a pellicle, consists of many aerobic bacteria that adhere to the surface by their fimbriae. Thus, fimbriae allow the aerobic bacteria to remain on the broth, from which they take nutrients, while they congregate near the air.\n\n\"Gram-negative bacteria assemble functional amyloid surface fibers called curli.\" Curli are a type of fimbriae; another type are called type I fimbriae. Curli are composed of proteins called curlins. Some of the genes involved are \"CsgA\", \"CsgB\", \"CsgC\", \"CsgD\", \"CsgE\", \"CsgF\", and \"CsgG\".\nFimbriae are one of the primary mechanisms of virulence for \"E. coli\", \"Bordetella pertussis\", \"Staphylococcus\" and \"Streptococcus\" bacteria. Their presence greatly enhances the bacteria's ability to attach to the host and cause disease.\n"}
{"id": "421703", "url": "https://en.wikipedia.org/wiki?curid=421703", "title": "Flags of the U.S. states and territories", "text": "Flags of the U.S. states and territories\n\nThe flags of the U.S. states, territories, and federal district exhibit a variety of regional influences and local histories, as well as different styles and design principles. Nonetheless, the majority of the states' flags share the same design pattern consisting of the state seal superimposed on a monochrome background, commonly every different shade of blue.\n\nThe most recent current state flag is that of Utah (February 16, 2011), while the most recent current territorial flag is that of the Northern Mariana Islands (July 1, 1985).\n\nModern U.S. state flags date from the 1890s, when states wanted to have distinctive symbols at the 1893 World's Columbian Exposition in Chicago, Illinois. Most U.S. state flags were designed and adopted between 1893 and World War I.\n\nAccording to a 2001 survey by the North American Vexillological Association, New Mexico has the best-designed flag of any U.S. state, U.S. territory, or Canadian province, while Georgia's state flag was rated the worst design. (Georgia adopted a new flag in 2003; Nebraska's state flag, whose design was rated second worst, remains in use to date.)\n\nDates in parentheses denote when the current flag was adopted by the state's legislature.\n\nThese are the current flags of the federal district and territories of the United States. Dates in parenthesis denote when the district or territory's current flag was adopted by its respective political body.\n\nThe U.S. national flag is the official flag for all islands, atolls, and reefs composing the United States Minor Outlying Islands. However, unofficial flags are in use on five of these nine insular areas:\n\nBaker Island, Howland Island and Jarvis Island do not have flags.\n\nMaine and Massachusetts have ensigns for use at sea.\n\n\n"}
{"id": "22002887", "url": "https://en.wikipedia.org/wiki?curid=22002887", "title": "Gastre Fault", "text": "Gastre Fault\n\nThe Gastre Fault Zone (GFZ) is a NW-SE striking dextral Jurassic Gastre Fault System (cf. Rapela & Pankhurst, 1992) in Central Patagonia, Argentina.\n\nFrom a tentative correlation of the fault zone with the similarly NW-SE trend, it was termed ‘Gastre Fault Zone’ or ‘Gastre-Purén Fault Zone’ to the Lanalhue Fault Zone in Chile by early works. However, in later works it is shown that this correlation is incorrect. Since the lake ‘Lago Lanalhue’, is located on the fault trace and shows a NW-SE-elongated shape, ‘Lanalhue Fault Zone (LFZ)’ stands as appropriate name for the here discussed fault zone.\nThe Mocha-Villarrica Fault Zone is the NW-SE trending fault responsible for the alignment of Villarrica, Quetrupillán and Lanín volcanoes.\n"}
{"id": "12720295", "url": "https://en.wikipedia.org/wiki?curid=12720295", "title": "Gert Korthof", "text": "Gert Korthof\n\nGert Korthof is a Dutch biologist who was trained at Utrecht University. He has reviewed various books of evolution, creationism, and intelligent design, including Michael Behe's \"The Edge of Evolution\". He contributed to \"Why Intelligent Design Fails: A Scientific Critique of the New Creationism\".\n\n"}
{"id": "12529812", "url": "https://en.wikipedia.org/wiki?curid=12529812", "title": "Gowdy solution", "text": "Gowdy solution\n\nGowdy universes or, alternatively, Gowdy solutions of Einstein's equations are simple model spacetimes in general relativity which represent an expanding universe filled with a regular pattern of gravitational waves.\n\n"}
{"id": "19121488", "url": "https://en.wikipedia.org/wiki?curid=19121488", "title": "Hodoscope", "text": "Hodoscope\n\nA hodoscope (from the Greek \"hodos\" for way or path, and \"skopos\" an observer) is an instrument used in particle detectors to detect passing charged particles and determine their trajectories. Hodoscopes are characterized by being made up of many segments; the combination of which segments record a detection is then used to infer where the particle passed through hodoscope.\n\nThe typical detector segment is a piece of scintillating material, which emits light when a particle passes through it. The scintillation light can be converted to an electrical signal either by a photomultiplier tube (PMT) or a PIN diode. If a segment measures some significant amount of light, the experimenter can infer that a particle passed through that segment. In addition to coordinate information, for some systems the strength of the light can be proportional to the deposited energy. By doing necessary calibrations, the deposited energy can be determined, which then can be used to infer information about the original particle's energy.\n\nAs an example: a simple hodoscope might be used to determine where a particle crossed a plane or a wall. In this case, the experimenter could use two segments shaped like strips, arranged in two layers. One layer of strips could be arranged horizontally, while a second layer could be arranged vertically. A particle passing through the wall would hit a strip in each layer; the vertical strip would reveal the particle's horizontal position when it crossed the wall, while the horizontal strip would indicate the particle's vertical position.\n\nHodoscopes are some of the simplest detectors for tracking charged particles. However, their spatial resolution is limited by the segment size. In applications where the spatial resolution is very important, hodoscopes have been superseded by other detectors such as drift chambers and time projection chambers.\n\n"}
{"id": "38365867", "url": "https://en.wikipedia.org/wiki?curid=38365867", "title": "How to Create a Mind", "text": "How to Create a Mind\n\nHow to Create a Mind: The Secret of Human Thought Revealed is a non-fiction book about brains, both human and artificial, by the inventor and futurist Ray Kurzweil. First published in hardcover on November 13, 2012 by Viking Press it became a New York Times Best Seller. It has received attention from \"The Washington Post\", \"The New York Times\" and \"The New Yorker\".\n\nKurzweil describes a series of thought experiments which suggest to him that the brain contains a hierarchy of pattern recognizers. Based on this he introduces his Pattern Recognition Theory of Mind (PRTM). He says the neocortex contains 300 million very general pattern recognition circuits and argues that they are responsible for most aspects of human thought. He also suggests that the brain is a \"recursive probabilistic fractal\" whose line of code is represented within the 30-100 million bytes of compressed code in the genome.\n\nKurzweil then explains that a computer version of this design could be used to create an artificial intelligence more capable than the human brain. It would employ techniques such as hidden Markov models and genetic algorithms, strategies Kurzweil used successfully in his years as a commercial developer of speech recognition software. Artificial brains will require massive computational power, so Kurzweil reviews his law of accelerating returns which explains how the compounding effects of exponential growth will deliver the necessary hardware in only a few decades.\n\nCritics felt the subtitle of the book, \"The Secret of Human Thought Revealed\", overpromises. Some protested that pattern recognition does not explain the \"depth and nuance\" of mind including elements like emotion and imagination. Others felt Kurzweil's ideas might be right, but they are not original, pointing to existing work as far back as the 1980s. Yet critics admire Kurzweil's \"impressive track record\" and say that his writing is \"refreshingly clear\", containing \"lucid discussions\" of computing history.\n\nKurzweil has written several futurology books including \"The Age of Intelligent Machines\" (1990), \"The Age of Spiritual Machines\" (1999) and \"The Singularity is Near\" (2005). In his books he develops the law of accelerating returns. The law is similar to Moore's Law, the persistent doubling in capacity of computer chips, but extended to all \"human technological advancement, the billions of years of terrestrial evolution\" and even \"the entire history of the universe\".\n\nDue to the exponential growth in computing technologies predicted by the law, Kurzweil says that by \"the end of the 2020s\" computers will have \"intelligence indistinguishable to biological humans\". As computational power continues to grow, machine intelligence will represent an ever-larger percentage of total intelligence on the planet. Ultimately it will lead to the Singularity, a merger between biology and technology, which Kurzweil predicts will occur in 2045. He says \"There will be no distinction, post-Singularity, between human and machine...\".\n\nKurzweil himself plans to \"stick around\" for the Singularity. He has written two health and nutrition books aimed at living longer, the subtitle of one is \"Live Long Enough to Live Forever\". One month after \"How to Create a Mind\" was published, Google announced that it had hired Kurzweil to work as Director of Engineering \"on new projects involving machine learning and language processing\". Kurzweil said his goal at Google is to \"create a truly useful AI [artificial intelligence] that will make all of us smarter\".\n\nKurzweil opens the book by reminding us of the importance of thought experiments in the development of major theories, including evolution and relativity. It's worth noting that Kurzweil sees Darwin as \"a good contender\" for the leading scientist of the 19th century. He suggests his own thought experiments related to how the brain thinks and remembers things. For example, he asks the reader to recite the alphabet, but then to recite the alphabet backwards. The difficulty in going backwards suggests \"our memories are sequential and in order\". Later he asks the reader to visualize someone he has met only once or twice, the difficulty here suggests \"there are no images, videos, or sound recordings stored in the brain\" only sequences of patterns. Eventually he concludes the brain uses a hierarchy of pattern recognizers.\n\nKurzweil states that the neocortex contains about 300 million very general pattern recognizers, arranged in a hierarchy. For example, to recognize a written word there might be several pattern recognizers for each different letter stroke: diagonal, horizontal, vertical or curved. The output of these recognizers would feed into higher level pattern recognizers, which look for the pattern of strokes which form a letter. Finally a word-level recognizer uses the output of the letter recognizers. All the while signals feed both \"forward\" and \"backward\". For example, if a letter is obscured, but the remaining letters strongly indicate a certain word, the word-level recognizer might suggest to the letter-recognizer which letter to look for, and the letter-level would suggest which strokes to look for. Kurzweil also discusses how listening to speech requires similar hierarchical pattern recognizers.\n\nKurzweil's main thesis is that these hierarchical pattern recognizers are used not just for sensing the world, but for nearly all aspects of thought. For example, Kurzweil says memory recall is based on the same patterns that were used when sensing the world in the first place. Kurzweil says that learning is critical to human intelligence. A computer version of the neocortex would initially be like a new born baby, unable to do much. Only through repeated exposure to patterns would it eventually self-organize and become functional.\n\nKurzweil writes extensively about neuroanatomy, of both the neocortex and \"the old brain\". He cites recent evidence that interconnections in the neocortex form a grid structure, which suggests to him a common algorithm across \"all neocortical functions\".\n\nKurzweil next writes about creating a digital brain inspired by the biological brain he has been describing. One existing effort he points to is Henry Markram's Blue Brain Project, which is attempting to create a full brain simulation by 2023. Kurzweil says the full molecular modeling they are attempting will be too slow, and that they will have to swap in simplified models to speed up initial self-organization.\n\nKurzweil believes these large scale simulations are valuable, but says a more explicit \"functional algorithmic model\" will be required to achieve human levels of intelligence. Kurzweil is unimpressed with neural networks and their potential while he's very bullish on vector quantization, hidden Markov models and genetic algorithms since he used all three successfully in his speech recognition work. Kurzweil equates pattern recognizers in the neocortex with statements in the LISP programming language, which is also hierarchical. He also says his approach is similar to Jeff Hawkins' hierarchical temporal memory, although he feels the hierarchical hidden Markov models have an advantage in pattern detection.\n\nKurzweil touches on some modern applications of advanced AI including Google's self-driving cars, IBM's Watson which beat the best human players at the game Jeopardy!, the Siri personal assistant in the Apple iPhone or its competitor Google Voice Search. He contrasts the hand-coded knowledge of the Douglas Lenat's Cyc project with the automated learning of systems like Google Translate and suggests the best approach is to use a combination of both, which is how IBM's Watson was so effective. Kurzweil says that John Searle's has leveled his \"Chinese Room\" objection at Watson, arguing that Watson only manipulates symbols without meaning. Kurzweil thinks the human brain is \"just\" doing hierarchical statistical analysis as well.\n\nIn a section entitled \"A Strategy for Creating a Mind\" Kurzweil summarizes how he would put together a digital mind. He would start with a pattern recognizer and arrange for a hierarchy to self-organize using a hierarchical hidden Markov model. All parameters of the system would be optimized using genetic algorithms. He would add in a \"critical thinking module\" to scan existing patterns in the background for incompatibilities, to avoid holding inconsistent ideas. Kurzweil says the brain should have access to \"open questions in every discipline\" and have the ability to \"master vast databases\", something traditional computers are good at. He feels the final digital brain would be \"as capable as biological ones of effecting changes in the world\".\n\nA digital brain with human-level intelligence raises many philosophical questions, the first of which is whether it is conscious. Kurzweil feels that consciousness is \"an emergent property of a complex physical system\", such that a computer emulating a brain would have the same emergent consciousness as the real brain. This is in contrast to people like John Searle, Stuart Hameroff and Roger Penrose who believe there is something special about the physical brain that a computer version could not duplicate.\n\nAnother issue is that of free will, the degree to which people are responsible for their own choices. Free will relates to determinism, if everything is strictly determined by prior state, then some would say that no one can have free will. Kurzweil holds a pragmatic belief in free will because he feels society needs it to function. He also suggests that quantum mechanics may provide \"a continual source of uncertainty at the most basic level of reality\" such that determinism does not exist.\n\nFinally Kurzweil addresses identity with futuristic scenarios involving cloning a nonbiological version of someone, or gradually turning that same person into a nonbiological entity one surgery at a time. In the first case it is tempting to say the clone is not the original person, because the original person still exists. Kurzweil instead concludes both versions are equally the same person. He explains that an advantage of nonbiological systems is \"the ability to be copied, backed up, and re-created\" and this is just something people will have to get used to. Kurzweil believes identity \"is preserved through continuity of the pattern of information that makes us\" and that humans are not bound to a specific \"substrate\" like biology.\n\nThe law of accelerating returns is the basis for all of these speculations about creating a digital brain. It explains why computational capacity will continue to increase unabated even after Moore's Law expires, which Kurzweil predicts will happen around 2020. Integrated circuits, the current method of creating computer chips, will fade from the limelight, while some new more advanced technology will pick up the slack. It is this new technology that will get us to the massive levels of computation needed to create an artificial brain.\n\nAs exponential progress continues into and beyond the Singularity, Kurzweil says \"we will merge with the intelligent technology we are creating\". From there intelligence will expand outward rapidly. Kurzweil even wonders whether the speed of light is really a firm limit to civilization's ability to colonize the universe.\n\nSimson Garfinkel, an entrepreneur and professor of computer science at the Naval Postgraduate School, says Kurzweil's pattern recognition theory of mind (PRTM) is misnamed because of the word \"theory\", he feels it is not a theory since it cannot be tested. Garfinkel rejects Kurzweil's one-algorithm approach instead saying \"the brain is likely to have many more secrets and algorithms than the one Kurzweil describes\". Garfinkel caricatures Kurzweil's plan for artificial intelligence as \"build something that can learn, then give it stuff to learn\", which he thinks is hardly the \"secret of human thought\" promised by the subtitle of the book.\n\nGary Marcus, a research psychologist and professor at New York University, says only the name PRTM is new. He says the basic theory behind PRTM is \"in the spirit of\" a model of vision known as the neocognitron, introduced in 1980. He also says PRTM even more strongly resembles Hierarchical Temporal Memory promoted by Jeff Hawkins in recent years. Marcus feels any theory like this needs to be proven with an actual working computer model. And to that end he says that \"a whole slew\" of machines have been programmed with an approach similar to PRTM, and they have often performed poorly.\n\nColin McGinn, a philosophy professor at the University of Miami, asserted in \"The New York Review of Books\" that \"pattern recognition pertains to perception specifically, not to all mental activity\". While Kurzweil does say \"memories are stored as sequences of patterns\" McGinn asks about \"emotion, imagination, reasoning, willing, intending, calculating, silently talking to oneself, feeling pain and pleasure, itches, and mood\" insisting these have nothing to do with pattern recognition. McGinn is also critical of the \"homunculus language\" Kurzweil uses, the anthropomorphization of anatomical parts like neurons. Kurzweil will write that a neuron \"shouts\" when it \"sees\" a pattern, where McGinn would prefer he say a neuron \"fires\" when it receives certain stimuli. In McGinn's mind only conscious entities can \"recognize\" anything, a bundle of neurons cannot. Finally he takes objection with Kurzweil's \"law\" of accelerating change, insisting it is not a law, but just a \"fortunate historical fact about the twentieth century\".\n\nIn 2015, Kurzweil's theory was extended to a Pattern Activation/Recognition Theory of Mind with a stochastic model of self-describing neural circuits.\n\nGarfinkel says Kurzweil is at his best with the thought experiments early in the book, but says the \"warmth and humanitarianism\" evident in Kurzweil's talks is missing. Marcus applauds Kurzweil for \"lucid discussion\" of Alan Turing and John von Neumann and was impressed by his descriptions of computer algorithms and the detailed histories of Kurzweil's own companies.\n\nMatthew Feeney, assistant editor for Reason, was disappointed in how briefly Kurzweil dealt with the philosophical aspects of the mind-body problem, and the ethical implications of machines which appear to be conscious. He does say Kurzweil's \"optimism about an AI-assisted future is contagious.\" While Drew DeSilver, business reporter at the Seattle Times, says the first half of the book \"has all the pizazz and drive of an engineering manual\" but says Kurzweil's description of how the Jeopardy! computer champion Watson worked \"is eye-opening and refreshingly clear\".\n\nMcGinn says the book is \"interesting in places, fairly readable, moderately informative, but wildly overstated.\" He mocks the book's subtitle by writing \"All is revealed!\" after paraphrasing Kurzweil's pattern recognition theory of mind. Speaking as a philosopher, McGinn feels that Kurzweil is \"way of out of his depth\" when discussing Wittgenstein.\n\nMatt Ridley, journalist and author, wrote in \"The Wall Street Journal\" that Kurzweil \"has a more impressive track record of predicting technological progress than most\" and therefore he feels \"it would be foolish, not wise, to bet against the emulation of the human brain in silicon within a couple of decades\".\n\n\n\n"}
{"id": "295248", "url": "https://en.wikipedia.org/wiki?curid=295248", "title": "Hypodermic needle model", "text": "Hypodermic needle model\n\nThe hypodermic needle model (known as the hypodermic-syringe model, transmission-belt model, or magic bullet theory) is a model of communication suggesting that an intended message is directly received and wholly accepted by the receiver. The model was originally rooted in 1930s behaviorism and largely considered obsolete for a long time, but big data analytics-based mass customization has led to a modern revival of the basic idea.\n\nThe \"magic bullet\" or \"hypodermic needle theory\" of direct influence effects was based on early observations of the effect of mass media, as used by Nazi propaganda and the effects of Hollywood in the 1930s and 1940s. People were assumed to be \"uniformly controlled by their biologically based 'instincts' and that they react more or less uniformly to whatever 'stimuli' came along\". The \"Magic Bullet\" theory graphically assumes that the media's message is a bullet fired from the \"media gun\" into the viewer's \"head\". Similarly, the \"Hypodermic Needle Model\" uses the same idea of the \"shooting\" paradigm. It suggests that the media injects its messages straight into the passive audience. This passive audience is immediately affected by these messages. The public essentially cannot escape from the media's influence, and is therefore considered a \"sitting duck\". Both models suggest that the public is vulnerable to the messages shot at them because of the limited communication tools and the studies of the media's effects on the masses at the time. It means the media explores information in such a way that it injects in the mind of audiences as bullets.\n\nThe \"magic bullet\" and \"hypodermic needle\" models originate from Harold Lasswell's 1927 book, \"Propaganda Technique in the World War\". Recent work in the history of communication studies have documented how the two models may have served as strawman theory or fallacy or even a \"myth\". Others have documented the possible medical origins of the metaphor of the magic bullet model.\n\nThe phrasing \"hypodermic needle\" is meant to give a mental image of the direct, strategic, and planned infusion of a message into an individual. But as research methodology became more highly developed, it became apparent that the media had selective influences on people.\n\nThe most famous incident often cited as an example for the hypodermic needle model was the 1938 broadcast of \"The War of the Worlds\" and the subsequent reaction of widespread panic among its American mass audience. However, this incident actually sparked the research movement, led by Paul Lazarsfeld and Herta Herzog, that would disprove the magic bullet or hypodermic needle theory, as Hadley Cantril managed to show that reactions to the broadcast were, in fact, diverse, and were largely determined by situational and attitudinal attributes of the listeners.\n\nIn the 1940s, Lazarsfeld disproved the \"magic bullet\" theory and \"hypodermic needle model theory\" through elections studies in \"The People's Choice\". Lazarsfeld and colleagues executed the study by gathering research during the election of Franklin D. Roosevelt in 1940. The study was conducted to determine voting patterns and the relationship between the media and political power. Lazarsfeld discovered that the majority of the public remained unfazed by propaganda surrounding Roosevelt's campaign. Instead, interpersonal outlets proved more influential than the media. Therefore, Lazarsfeld concluded that the effects of the campaign were not all powerful to the point where they completely persuaded \"helpless audiences\", a claim that the Magic Bullet, Hypodermic Needle Model, and Lasswell asserted. These new findings also suggested that the public can select which messages affect and don't affect them.\n\nLazarsfeld's debunking of these models of communication provided the way for new ideas regarding the media's effects on the public. Lazarsfeld introduced the idea of the two-step flow of communication in 1944. Elihu Katz contributed to the model in 1955 through studies and publications. The model of the two-step flow of communication assumes that ideas flow from the mass media to opinion leaders and then to the greater public. They believed the message of the media to be transferred to the masses via this opinion leadership. Opinion leaders are categorized as individuals with the best understanding of media content and the most accessibility to the media as well. These leaders essentially take in the media's information, and explain and spread the media's messages to others.\n\nThus, the two step flow model and other communication theories suggest that the media does not directly have an influence on viewers anymore. Instead, interpersonal connections and even selective exposure play a larger role in influencing the public in the modern age.\n\nMore recently, the use of big data analytics to identify user preferences and to send tailor-made messages to individuals led back to the idea of a \"one-step flow of communication\", which is in principle similar to the hypodermic needle model. The difference is that today's massive databases allow for the mass customization of messages. So it is not one generic mass media message, but many individualized messages, coordinated by a massive algorithm. For example, empirical studies have found that in Twitter networks, traditional mass media outlets receive 80–90% of their Twitter mentions directly through a direct one-step flow from average Twitter users. However, these same studies also argue that there is a multitude of step-flow models at work in today's digital communication landscape.\n\n\n"}
{"id": "10103051", "url": "https://en.wikipedia.org/wiki?curid=10103051", "title": "Hypotheticals", "text": "Hypotheticals\n\nHypotheticals could be possible situations, statements or questions about something imaginary rather than something real. Hypotheticals might deal with the concept of \"what if?\"'. Grammatically, the term could be a noun formed from an adjective, and the word might be pluralized because it refers to the \"members\" of a \"class\" of hypothetical things.\n\nHypotheticals can be important because they provide a means for understanding what we would do if the world was different. Although this may assist our understanding of risk, and help us plan and create a new and better future, hypotheticals also help us understand the past, and why things happened or how things work. For example, in seeking to understand why a war started we could ask: \"What if the parties had talked more first? Would they have worked out a better way of solving their problems? Could war have been averted?\" Hypotheticals about the past are challenging to consider, as it is not possible to enter the past to change things according to our hypotheticals and determine what then may have occurred.\n\nThe philosopher David Lewis suggested in his book \"Counterfactuals\" (Blackwell Publishers, 1973) that when we use hypotheticals (\"counter-to-fact-uals\"), what we mean is: \"In an imaginary world, exactly like ours, except in the one difference we are talking about ...\". This idea is called \"possible worlds\" and some people believe they actually exist, only we can't get to them, because the whole point is that they are \"different\" to our world.\n\n"}
{"id": "220425", "url": "https://en.wikipedia.org/wiki?curid=220425", "title": "James Gregory (mathematician)", "text": "James Gregory (mathematician)\n\nJames Gregory FRS (November 1638 – October 1675) was a Scottish mathematician and astronomer. His surname is sometimes spelled as Gregorie, the original Scottish spelling. He described an early practical design for the reflecting telescope – the Gregorian telescope – and made advances in trigonometry, discovering infinite series representations for several trigonometric functions.\n\nIn his book \"Geometriae Pars Universalis\" (1668) Gregory gave both the first published statement and proof of the fundamental theorem of the calculus (stated from a geometric point of view, and only for a special class of the curves considered by later versions of the theorem), for which he was acknowledged by Isaac Barrow.\n\nThe youngest of the 3 children of John Gregory, an Episcopalian Church of Scotland minister, James was born in the manse at Drumoak, Aberdeenshire, and was initially educated at home by his mother, Janet Anderson (~1600–1668). It was his mother who endowed Gregory with his appetite for geometry, her uncle – Alexander Anderson (1582–1619) – having been a pupil and editor of French mathematician Viète. After his father's death in 1651 his elder brother David took over responsibility for his education. He attended Aberdeen Grammar School, and then Marischal College from 1653–1657, graduating AM in 1657.\n\nIn 1663 he went to London, meeting John Collins and fellow Scot Robert Moray, one of the founders of the Royal Society. In 1664 he departed for the University of Padua, in the Venetian Republic, passing through Flanders, Paris and Rome on his way. At Padua he lived in the house of his countryman James Caddenhead, the professor of philosophy, and he was taught by Stefano Angeli.\n\nUpon his return to London in 1668 he was elected a Fellow of the Royal Society, before travelling to St Andrews in late 1668 to take up his post as the first Regius Professor of Mathematics, a position created for him by Charles II, probably upon the request of Robert Moray.\n\nHe was successively professor at the University of St Andrews and the University of Edinburgh.\n\nHe had married Mary, daughter of George Jameson, painter, and widow of John Burnet of Elrick, Aberdeen; their son James was Professor of Physics at King's College, Aberdeen. He was the grandfather of John Gregory (FRS 1756); uncle of David Gregorie (FRS 1692) and brother of David Gregory (1627–1720), a physician and inventor.\n\nAbout a year after assuming the Chair of Mathematics at Edinburgh, James Gregory suffered a stroke while viewing the moons of Jupiter with his students. He died a few days later at the age of 36.\n\nIn the \"Optica Promota\", published in 1663, Gregory described his design for a reflecting telescope, the \"Gregorian telescope\". He also described the method for using the transit of Venus to measure the distance of the Earth from the Sun, which was later advocated by Edmund Halley and adopted as the basis of the first effective measurement of the Astronomical Unit.\n\nBefore he left Padua, Gregory published \"Vera Circuli et Hyperbolae Quadratura\" (1667) in which he approximated the areas of the circle and hyperbola with convergent series:\n\n\"The first proof of the fundamental theorem of calculus and the discovery of the Taylor series can both be attributed to him.\"\n\nThe book also contains series expansions of sin(\"x\"), cos(\"x\"), arcsin(\"x\") and arccos(\"x\"). Gregory was probably unaware that the earliest enunciations of these expansions were made by Madhava in India in the 14th century. The book was reprinted in 1668 with an appendix, \"Geometriae Pars\", in which Gregory explained how the volumes of solids of revolution could be determined.\n\nIn his 1663 \"Optica Promota\", James Gregory described his reflecting telescope which has come to be known by his name, the Gregorian telescope. Gregory pointed out that a reflecting telescope with a parabolic mirror would correct spherical aberration as well as the chromatic aberration seen in refracting telescopes. In his design he also placed a concave secondary mirror with an elliptical surface past the focal point of the parabolic primary mirror, reflecting the image back through a hole in the primary mirror where it could be conveniently viewed. According to his own confession, Gregory had no practical skill and he could find no optician capable of actually constructing one.\n\nThe telescope design attracted the attention of several people in the scientific establishment such as Robert Hooke, the Oxford physicist who eventually built the telescope 10 years later, and Sir Robert Moray, polymath and founding member of the Royal Society.\n\nThe Gregorian telescope design is rarely used today, as other types of reflecting telescopes are known to be more efficient for standard applications. Gregorian optics are also used in radio telescopes such as Arecibo, which features a \"Gregorian dome\".\n\nThe following excerpt is from the \"Pantologia. A new (cabinet) cyclopædi\" (1813)\n\nMr. James Gregory was a man of a very acute and penetrating genius. ...The most brilliant part of his character was that of his mathematical genius as an inventor, which was of the first order; as will appear by... his inventions and discoveries [which include] quadrature of the circle and hyperbola, by an infinite converging series; his method for the transformation of curves; a geometrical demonstration of Lord Brouncker's series for squaring the hyperbola—his demonstration that the meridian line is analogous to a scale of logarithmic tangents of the half complements of the latitude; he also invented and demonstrated geometrically, by help of the hyperbola, a very simple converging series for making the logarithms; he sent to Mr. Collins the solution of the famous Keplerian problem by an infinite series; he discovered a method of drawing Tangents to curves geometrically, without any previous calculations; a rule for the direct and inverse method of tangents, which stands upon the same principle (of exhaustions) with that of fluxions, and differs not much from it in the manner of application; a series for the length of the arc of a circle from the tangent, and vice versa; as also for the secant and logarithmic tangent and secant, and vice versa. These, with others, for measuring the length of the elliptic and hyperbolic curves, were sent to Mr. Collins, in return for some received from him of Newton's, in which he followed the elegant example of this author, in delivering his series in simple terms, independent of each other.\nIn 1671, or perhaps earlier, he established the theorem that\n\nthe result being true only if θ lies between −(1/4)π and (1/4)π.\nThis formula was later used to calculate digits of π, although more efficient formulas were later discovered.\n\nJames Gregory discovered the diffraction grating by passing sunlight through a bird feather and observing the diffraction pattern produced. In particular he observed the splitting of sunlight into its component colours – this occurred a year after Newton had done the same with a prism and the phenomenon was still highly controversial.\n\nA round wheel is unsuitable for irregular surfaces, and Gregory devised an appropriate \"adaptable wheel\" using a \"Gregory transformation\". \n\nGregory, an enthusiastic supporter of Newton, later had much friendly correspondence with him and incorporated his ideas into his own teaching, ideas which at that time were controversial and considered quite revolutionary.\n\nThe crater Gregory on the Moon is named after him. He was the uncle of mathematician David Gregory.\n\n\n\n\n"}
{"id": "1905691", "url": "https://en.wikipedia.org/wiki?curid=1905691", "title": "Joel Brind", "text": "Joel Brind\n\nJoel Lewis Brind is a professor of human biology and endocrinology at Baruch College, City University of New York since 1986, a research biochemist since 1981, and CEO of Natural Food Science, a maker of glycine supplement products founded in 2010. Brind is a leading advocate of the abortion-breast cancer hypothesis, which posits that abortion increases the risk of breast cancer. This idea is rejected by mainstream medical professional organizations and there is overwhelming evidence in the peer-reviewed medical literature debunking it. Brind is openly contemptuous of mainstream medical professional organizations and journals, accusing them of conducting a deliberate cover-up with the goal of \"protecting the abortion industry.\" Brind asserts that the National Cancer Institute \"is just another corrupt federal agency like the IRS and the NSA.\"\n\nBrind grew up in Laurelton, Queens, where he decided he wanted to become a biochemist at the age of 10 after reading an issue of \"Life\" magazine where the cover story described the discoveries scientists had recently made about the inner workings of the cell, using electron microscopy. He has a bachelor's degree from Yale (1971) and a Ph.D. from New York University in biochemistry, immunology and physiology. Four years after receiving his PhD in 1981, Brind had a spiritual awakening, after which he converted to Christianity, and decided to try to use science to pursue what he saw as a \"noble task\" of discouraging women from having abortions.\n\nWorking on studies concerning amino acid metabolism and aging, Brind concluded that most diets are deficient in the amino acid glycine, and that this deficiency is responsible for illnesses caused by chronic inflammation, including arthritis, diabetes, cardiovascular disease and cancer. These conclusions are not supported by mainstream medicine. He also attributes what are thought to be normal responses such as pain and stiffness following strenuous exercise and injury, to glycine deficiency. In 2010, Brind founded Natural Food Science, LLC, through which glycine supplement products Proglyta and Sweetamine are manufactured and sold.\n\nFollowing his conversion to Christianity, Brind began working as a consultant and expert witness for the anti-abortion cause. \"Discover\" magazine reported in 2003 that since 1997, \"Brind has spent about 90 percent of his time outside the classroom investigating and publicizing\" his claimed abortion-breast cancer link, testifying \"in courthouses and statehouses in Arizona, Florida, Massachusetts, Ohio, North Dakota, New Hampshire, and Alaska.\"\n\nHe fought against the legalization of RU-486, testifying at a federal hearing that \"thousands upon thousands\" of women would develop breast cancer as a result of using the drug. Brind was an invitee to the National Cancer Institute's conference on the abortion-breast-cancer issue where he was the only member to file a dissenting opinion. In a meeting between Colorado Right To Life and the Denver affiliate of Susan G. Komen for the Cure regarding Komen grants to Planned Parenthood, Brind urged the breast cancer group to re-consider the idea that abortion is linked to breast cancer.\n\nIn 1999, Brind co-founded the Breast Cancer Prevention Institute, a non-profit group which promotes a link between abortion and breast cancer. The group is identified in the academic literature as an anti-abortion activist group that promotes \"the notion that a link to cancer has been both irrefutably proved and deliberately concealed by the medical establishment.\"\n\nBrind \"et al.\" (1996) conducted a meta-analysis of 23 independent epidemiologic studies. It calculated that there was on average a relative risk of 1.3 (1.2 - 1.4) increased risk of breast cancer. The meta-analysis was criticized in the \"Journal of the National Cancer Institute\" for ignoring the role of response bias and for a \"blurring of association with causation.\" It was also criticized for selection bias by using studies with widely varying results, using different types of studies and not working with the raw data from several studies, and including studies that have methodological weaknesses. The statistician who collaborated with Brind later stated of their findings: \"I have some doubts. I don't think the issue has been resolved. When we were talking about the conclusions, he [Brind] wanted to make the strongest statements. I tried to temper them a little bit, but Dr. Brind is very adamant about his opinion.\"\n\nExperts believe Brind overlooks the methodological weaknesses of some studies he uses as evidence for an abortion-breast cancer link. Furthermore, medical researchers note Brind overstates his findings since his own research shows a \"barely statistically significant\" increase in breast cancer rates. In reaction to the criticism an editor of the journal that published Brind's study noted with concern:\n\nHowever, in the light of recent unease about appropriate but open communication of risks associated with oral contraceptive pills, it will surely be agreed that open discussion of risks\nis vital and must include the people – in this case the women – concerned. I believe that if you take a view (as I do), which is often called 'pro-choice', you need at the same time to have a view which might be called 'pro-information' without excessive paternalistic censorship (or interpretation) of the data.\n\n\n"}
{"id": "19859246", "url": "https://en.wikipedia.org/wiki?curid=19859246", "title": "John Spurgeon Henkel", "text": "John Spurgeon Henkel\n\nJohannas Elias Spurgeon Henkel aka John Spurgeon Henkel (1871 in Peddie, Eastern Cape – 5 April 1962 in Pietermaritzburg), was a South African botanist and forester.\n\nHe was the son of soldier and botanist Caesar Carl Hans Henkel (1839-1913). He joined the Cape Forest Department in 1888, working in the Eastern Cape for several years. Chosen to attend the Royal Indian Engineering College at Cooper's Hill, he received the College Diploma.\n\nWhen the Anglo-Boer War broke out he served as captain, receiving the Queen's Medal. After the war he was appointed Assistant Conservator of Forests in the Eastern Conservancy in 1905, and Conservator of Forests for Natal and Zululand in 1912. In 1918, he became Chief of the Rhodesian Forest Service.\n\nHe was a founder member of the SA Association for the Advancement of Science, a founder member of the Royal Society of South Africa, and was awarded an honorary DSc by the University of South Africa. His father, Cäsar Carl Henkel, is commemorated in \"Podocarpus henkelii\".\n\n"}
{"id": "56753715", "url": "https://en.wikipedia.org/wiki?curid=56753715", "title": "Journal of Bioscience and Bioengineering", "text": "Journal of Bioscience and Bioengineering\n\nThe Journal of Bioscience and Bioengineering is a monthly peer-reviewed scientific journal. The editor-in-chief is Junichi Kato (Hiroshima University). It is published by The Society for Biotechnology, Japan and distributed outside Japan by Elsevier. It was founded in 1923 as a Japanese-language journal and took its current title in 1999.\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 2.0.15.\n\n"}
{"id": "898354", "url": "https://en.wikipedia.org/wiki?curid=898354", "title": "Landau's constants", "text": "Landau's constants\n\nIn complex analysis, a branch of mathematics, Landau's constants are certain mathematical constants that describe the behaviour of holomorphic functions defined on the unit disk. Consider the set \"F\" of all those holomorphic functions \"f\" on the unit disk for which \n\nWe define \"L\" to be the radius of the largest disk contained in the image of \"f\", and \"B\" to be the radius of the largest disk that is the biholomorphic image of a subset of a unit disk.\n\nLandau's constants are then defined as the infimum of \"L\" or \"B\", where \"f\" is any holomorphic function or any injective holomorphic function on the unit disk with \n\nThe three resulting constants are abbreviated \"L\", \"B\", and \"A\" (for injective functions), respectively.\n\nThe exact values of \"L\", \"B\", and \"A\" are not known, but it is known that\n\nB is the Bloch's constant.\n\n"}
{"id": "467962", "url": "https://en.wikipedia.org/wiki?curid=467962", "title": "List of ethnic enclaves in North American cities", "text": "List of ethnic enclaves in North American cities\n\nThis is a list of ethnic enclaves in various countries of different ethnic and cultural backgrounds to the native population. An ethnic enclave in this context denotes an area primarily populated by a population with similar ethnic or racial background. This list also includes concentrations rather than enclaves, and historic examples which may no longer be an ethnic enclave.\n\nList of African-American neighborhoods - Thousands of African-American neighborhoods exist today. However, many of these communities are now less populated by African Americans than they were during the earlier, sometimes mid and late parts of the 20th century.\n\n\n\n\n\nCanada-\nMexico-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12 Largest Bulgarian-American communities.\n\n\n\n\n\n\n\n\n\n\n\nCanada -\nMexico -\nUSA -\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlso persons of mainly English American ancestry. See White Anglo-Saxon Protestants (WASPs).\n- Outside USA -\n\n\n\n\n\n\n\n\n\nAn estimated 50,000 Chechens live in the USA, here are the top communities (#1 is Washington, D.C. area).\n\n\n\n\n\n\n\n\n\nExcept Puerto Ricans (4th as a nationality, 2nd in ancestry among Hispanics and Latinos), Salvadorans are the 2nd largest Hispanic/Latino ethnicity in the USA, close to Dominicans who are 3rd. Large Salvadoran communities developed in the late 20th/early 21st century period as a result of civil war, economic conditions, political turmoil and gang violence in the country, among the smallest in size in the Western Hemisphere. The largest Salvadoran population is in Central parts of Los Angeles and throughout California (i.e. the Coachella Valley/Palm Springs) along with Central American groups like Guatemalans, Hondurans and Nicaraguans. Recent census data shows that for the first time, there are more Salvadorans living on Long Island than Puerto Ricans, with Salvadorans now numbering nearly 100,000, representing nearly a quarter of all Hispanics in the region, making them largest Latino group in Long Island (New York State). They tend to live in suburban towns like Hempstead and Uniondale. \n\n\n\n\n\n\n\n\n\n\nAll are largest Belizean, Cuban and Puerto Rican communities in California, respectively, all in Los Angeles area.\n\n\nEthnic enclaves of Americans\n\n- Mexico - (one to 3.5 million Americans, expatriates and by descent)\n\nAjijic and the Lake Chapala region, Mexico; Cancun; Guerrero Negro and Los Cabos, Baja California Sur; Puerto Vallarta; San Felipe, Baja California; Tijuana, and many others (see Americans in Mexico).\n\n- Canada - (500,000 to a million Americans, expatriates and by descent)\n\nAlberta, esp. cities of Calgary and Edmonton; the Windsor, Ontario and Niagara Falls, Ontario areas facing the border; and Vancouver, British Columbia. In the 1960s-70s, about 50,000 Vietnam war draft dodgers from the USA settled in Canada, esp. Toronto.\n\n\nSeasonal residents known as \"Snowbirds\" are in Florida (esp. Hollywood near Miami in the South Florida metro area), the Carolinas, Gulf Coast of the United States, South Texas, Arizona (the Phoenix metro area), Las Vegas, Nevada; and Southern California (the largest concentrations in the Coachella Valley).\n\n- Canada (they form a majority of population in Quebec) -\n\n- USA -\n\n\nSouth Florida, California , Oregon and Colorado have large Jewish communities, many are seasonal or retirement.\n\n\nThe highest concentration of Urban Indians is believed to be in Anchorage, Alaska where over 10 percent of the population identify themselves in the census as having some Native ancestry, with 7.3 percent identifying that as their only ancestry.\n\nSome moderate-sized cities and suburbs lie adjacent to Indian Reservations, examples being Asheville, North Carolina; Auburn, New York (nearby Iroquois Nation); Bartlesville, Oklahoma (Osage Indian Reservation); Billings, Montana; Bismarck, North Dakota; Claremore, Oklahoma (historically Cherokee Nation); Grand Coulee, Washington; Muskogee, Oklahoma; Oneida Indian Reservation, New York; Pierre, South Dakota; Rapid City, South Dakota; St. Ignace, Michigan; Sedona, Arizona; Tacoma, Washington; Utica, New York; Wenatchee, Washington; Yankton, South Dakota; and Zebulon, North Carolina. A mostly non-Indian community of Salamanca, New York within the Allegany Indian Reservation located in Upstate New York.\n\n\nSee also Samoans and Guamanians.\n\n\nExcept for Paterson, New Jersey, all communities in California.\nIn Canada, Vancouver, British Columbia is largest Sikh/South Asian community in North America.\n\n\n\n\n"}
{"id": "33418058", "url": "https://en.wikipedia.org/wiki?curid=33418058", "title": "List of opioids", "text": "List of opioids\n\nThis is a list of opioids, opioid antagonists and inverse agonists.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "758690", "url": "https://en.wikipedia.org/wiki?curid=758690", "title": "List of physical quantities", "text": "List of physical quantities\n\nThis is a list of physical quantities.\n\nThe first table lists the base quantities used in the International System of Units to define the physical dimension of physical quantities for dimensional analysis. The second table lists the derived physical quantities. Derived quantities can be mentioned in terms of the base quantities.\n\nNote that neither the names nor the symbols used for the physical quantities are international standards. Some quantities are known as several different names such as the magnetic \"B-field\" which known as the \"magnetic flux density\", the \"magnetic induction\" or simply as the \"magnetic field\" depending on the context. Similarly, surface tension can be denoted by either \"σ\", \"γ\" or \"T\". The table usually lists only one name and symbol.\n\nThe final column lists some special properties that some of the quantities have, such as their scaling behavior (i.e. whether the quantity is intensive or extensive), their transformation properties (i.e. whether the quantity is a scalar, vector or tensor) or whether the quantity is conserved.\n\n"}
{"id": "183965", "url": "https://en.wikipedia.org/wiki?curid=183965", "title": "Lysocline", "text": "Lysocline\n\nThe lysocline is the depth in the ocean below which the rate of dissolution of calcite increases dramatically.\n\nShallow marine waters are generally supersaturated in calcite, CaCO, so as marine organisms (which often have shells made of calcite or its polymorph, aragonite) die, they will tend to fall downwards without dissolving. As depth and pressure increases within the water column, the corresponding calcite saturation of seawater decreases and the shells start to dissolve. The reaction involved, though more complex, can be thought of as: CaCO(s) + HO + CO → Ca(aq) + 2HCO(aq). At the lysocline, the rate of dissolution increases dramatically, and the concentration of calcite decreases by up to 90%. Below this, there exists a depth known as the \"carbonate compensation depth\" (CCD) below which the rate of supply of calcite equals the rate of dissolution, such that no calcite is deposited. This depth is the equivalent of a marine snow-line, and averages about 4,500 meters below sea level. Hence, the lysocline and CCD are not equivalent. The lysocline and compensation depth occur at greater depths in the Atlantic (5000–6000 m) than in the Pacific (4000 – 5000 m), and at greater depths in equatorial regions than in polar regions.\n\nThe depth of the CCD varies as a function of the chemical composition of the seawater and its temperature. Specifically, it is the deep waters that are undersaturated with calcium carbonate primarily because its solubility increases strongly with increasing pressure and salinity and decreasing temperature. Furthermore, it is not constant over time, having been globally much shallower in the Cretaceous through to Eocene. If the atmospheric concentration of carbon dioxide continues to increase, the CCD can be expected to rise, along with the ocean's acidity.\n\n\n"}
{"id": "47092471", "url": "https://en.wikipedia.org/wiki?curid=47092471", "title": "Manuel Alfonseca", "text": "Manuel Alfonseca\n\nManuel Alfonseca (born 1946, Madrid) is a Spanish writer and university professor. He is the son of the painter and sculptor Manuel Alfonseca Santana.\n\nHe is a doctor on communications engineering and graduated on Computer Science. He worked 22 years in IBM (1972–1994), where he was Senior Technical Staff Member. He has been a professor in several universities: Complutense de Madrid, Politécnica de Madrid and (now) Autónoma de Madrid, where he was a full professor (currently an honorary professor) and director of the Escuela Politécnica Superior (2001–2004).\n\nHe has published about two hundred technical papers in Spanish and English, as well as many articles on popular science in high difussion newspapers and web blogs.\n\nHe is the author of over fifty books in the fields of computer science, popular science, historic novel, science fiction and young adult literature.\n\nHe was granted the Lazarillo Award (1988) and La Brújula Award for Children and Young Adult Narrative (2012), besides having been finalist and included in honor lists several times. He was also given three Outstanding Technical Achievements Awards (1981, 1983, 1985) and one Technical Paper Award (1989) by IBM.\n\n\na) Fantasy\n\nb) Historical\n\nc) Science Fiction\n\nd) Mystery and intrigue\ne) Miscellaneous\n\n\n\n\n\n"}
{"id": "34421851", "url": "https://en.wikipedia.org/wiki?curid=34421851", "title": "Maurice Holland Award", "text": "Maurice Holland Award\n\nThe Maurice Holland Award is presented each year by the Industrial Research Institute (IRI) to honor the most outstanding paper published in the previous year’s volume of IRI’s journal, Research-Technology Management (RTM). Named for IRI’s founder, the Holland Award is a bronze replica of a “Jenny”, the model of airplane Maurice Holland flew during his service in World War I. The award was first presented in 1982 by Maurice Holland's son.\n\nThe Holland Award is presented to the winning paper’s authors at IRI’s Member Summit, held each year in the fall. The recipient is determined by RTM's Board of Editors, who select the winning paper “according to the criteria of significance to the field of R&D, technology, and innovation management; originality of new management concepts; and excellence in clarity of presentation.” IRI has collated the winning articles into a reprint collection titled “Winning Concepts & Practices for Managing Industrial R&D.” \n\n\n\n"}
{"id": "52252646", "url": "https://en.wikipedia.org/wiki?curid=52252646", "title": "Max Weber Foundation", "text": "Max Weber Foundation\n\nThe Max Weber Foundation (Ger. Max Weber Stiftung) is a German humanities research organisation based in Bonn and funded by the German Federal Government. It is composed of ten independent institutes:\n\n"}
{"id": "674489", "url": "https://en.wikipedia.org/wiki?curid=674489", "title": "No-till farming", "text": "No-till farming\n\nNo-till farming (also called zero tillage or direct drilling) is a way of growing crops or pasture from year to year without disturbing the soil through tillage. No-till is an agricultural technique which increases the amount of water that infiltrates into the soil, the soil's retention of organic matter and its cycling of nutrients. In many agricultural regions, it can reduce or eliminate soil erosion. It increases the amount and variety of life in and on the soil, including disease-causing organisms and disease organisms. The most powerful benefit of no-tillage is improvement in soil biological fertility, making soils more resilient. Farm operations are made much more efficient, particularly improved time of sowing and better trafficability of farm operations.\n\nTillage remains relevant in agriculture today, but the success of no-till methods in many contexts keeps farmers aware that multiple options exist. In some cases low-till methods combine aspects of till and no-till methods. For example, some approaches may use a limited amount of shallow disc harrowing but no plowing.\n\nTillage is the agricultural preparation of soil by mechanical agitation, typically killing the plants earlier in place. Tilling can create a flat seed bed or one that has formed areas, such as rows or raised beds, to enhance the growth of desired plants. It is an ancient technique with clear evidence of its use since at least 3000 B.C.E.\n\nThe effects of tillage can include soil compaction; loss of organic matter; degradation of soil aggregates; death or disruption of soil microbes and other organisms including mycorrhizae, arthropods, and earthworms; and soil erosion where topsoil is washed or blown away.\n\nThe idea of modern no-till farming started in the 1940s with Edward H. Faulkner, author of \"Plowman's Folly\", but it wasn't until the development of several chemicals after WWII that various researchers and farmers started to try out the idea. The first adopters of no-till include Klingman (North Carolina), Edward Faulkner, L.A. Porter (New Zealand), Harry and Lawrence Young (Herndon, Kentucky), the Instituto de Pesquisas Agropecuarias Meridional (1971 in Brazil) with Herbert Bartz.\n\nNo-till farming is widely used in the United States and the number of acres managed in this way continues to grow. This growth is supported by a decrease in costs related to tillage; no-till management results in fewer passes with equipment for approximately equal harvests, and the crop residue prevents evaporation of rainfall and increases water infiltration into the soil.\n\nStudies have found that no-till farming can be more profitable if performed correctly.\n\nIt reduces labour, fuel, irrigation and machinery costs. No-till can increase yield because of higher water infiltration and storage capacity, and less erosion. Another benefit of no-till is that because of the higher water content, instead of leaving a field fallow it can make economic sense to plant another crop instead.\n\nAs sustainable agriculture becomes more popular, monetary grants and awards are becoming readily available to farmers who practice conservation tillage. Some large energy corporations which are among the greatest generators of fossil-fuel-related pollution may purchase carbon credits, which can encourage farmers to engage in conservation tillage. Under such schemes, the farmers' land is legally redefined as a carbon sink for the power generators' emissions. This helps the farmer in several ways , and it helps the energy companies meet regulatory demands for reduction of pollution, specifically carbon emissions.\n\nNo-till farming can increase organic (carbon based) matter in the soil, which is a form of carbon sequestration. However, there is debate over whether this increased sequestration detected in scientific studies of no-till agriculture is actually occurring, or is due to flawed testing methods or other factors. Regardless of this debate, a case can still be made for no-till, in the form of reduction in fossil fuel use, less erosion and better soil quality.\n\nNo-till farming has carbon sequestration potential through storage of soil organic matter in the soil of crop fields. Tilling inverts soil layers, mixes in air, and greatly increases microbial activity. Organic matter breaks down much faster, releasing its carbon into the atmosphere. Also, farm tractors emit carbon dioxide.\n\nCropland soils are ideal as a carbon sink, as they have been depleted of carbon in most areas. Tillage and conventional farming have released an estimated 78 billion metric tonnes of carbon, by removing crop residues such as left over corn stalks and adding chemical fertilizers. Without tillage, residues decompose where they lie, and growing of winter cover crops can slow and reverse carbon loss.\n\nHowever, there is evidence that no-till systems still lose carbon over time. A 2014 study led by Ken Olson of University of Illinois concluded that this differing result occurs in part because tested soil samples need to include the full depth of rooting, 1–2 meters. He said, “That no-till subsurface layer is often losing more soil organic carbon stock over time than is gained in the surface layer.”. Also, there has not been a uniform definition of soil organic carbon sequestration among researchers. The study concludes, \"Additional investments in soil organic carbon (SOC) research is needed to better understand the agricultural management practices that are most likely to sequester SOC or at least retain more net SOC stocks.\"\n\nBesides reducing carbon emissions, no-till farming reduces nitrous oxide (NO) emissions by 40-70%, depending on rotation. Nitrous oxide is a potent greenhouse gas, 300 times stronger than CO2, and stays in the atmosphere for 120 years. Fertilizing farmlands with (excessive) nitrogen increases the release of nitrous oxide.\n\nNo-till farming improves soil quality (soil function), carbon, organic matter, aggregates, protecting from erosion, evaporation of water, and structural breakdown. Reducing of tillage reduces compaction of soil. This can help reduce soil erosion almost to soil production rates.\n\nRecently, researchers at the Agricultural Research Service of the United States Department of Agriculture found that no-till farming makes soil much more stable than plowed soil. Their conclusions draw from over 19 years of collaborated tillage studies. No-till stores more carbon in the soil and carbon in the form of organic matter is a key factor in holding soil particles together. The first inch of no-till soil is two to seven times less vulnerable than that of plowed soil. The practice of no-till farming is especially beneficial to Great Plains farmers because of its avoidance of erosion.\n\nCrop residues left intact help both natural precipitation and irrigation water to infiltrate the soil. Residue also limits evaporation, conserving water for plant growth. Evaporation from tilling reduces the amount of water by around 1/3 to 3/4 inches (0.85 to 1.9 cm) per pass. By reducing soil compaction and no tillage-pan, the soil absorbs more water, and roots grow deeper, reaching more water.\n\nNo-till farming leaves soil intact and crop residue on the field. Soil layers and soil biota, remain in their natural state. No-tilled fields often have more beneficial insects and annelids, more organic material and microbial content, and variety of wildlife. This is the result of improved cover, reduced traffic and the reduced chance of destroying ground nesting birds and animals (plowing destroys all of them).\n\nNo ploughing also means less airborne dust.\n\nTillage lowers the albedo of croplands. The potential for global cooling as a result of decreased Albedo in no till croplands is similar in magnitude to the biogeochemical (carbon sequestration) potential.\n\nTilling regularly damages ancient structures under the soil such as long barrows. In the UK, half of the long barrows in Gloucestershire and almost all the burial mounds in Essex have been damaged. According to English Heritage modern tillage techniques have done as much damage in the last six decades as traditional tilling did in the previous six centuries. By using no-till methods these structures can be preserved and can be properly investigated instead.\n\nNo-till farming requires specialized seeding equipment such as seed drills, to plant seeds into undisturbed crop residues and soil. The cost can be offset by selling plows and tractors, but farmers often keep their old equipment while trying out no-till farming. This would result in more money being invested into equipment in the short term (until old equipment is sold off).\n\nIf a soil has poor drainage, it may need drainage tiles or other devices to remove excess water under no-till. Water infiltration improves after 5-8 years of no-till farming, so farmers may want to wait before investing in such an expensive system.\n\nGullies can be a problem in the long-term. While much less soil is displaced by no-till farming, any drainage gulleys that do form deepen each year since they are not smoothed out by plowing. This may necessitate either sod drainways, waterways, permanent drainways, cover crops, etc. Gully formation can be avoided entirely with proper water management practices, including the creation of swales on contour.\n\nOne of the purposes of tilling is to remove weeds. No-till farming does change weed composition drastically. Faster growing weeds may no longer be a problem in the face of increased competition, but shrubs and trees may begin to grow eventually.\n\nSome farmers attack this problem with a “burn-down” herbicide such as glyphosate in lieu of tillage for seedbed preparation and because of this, no-till is often associated with increased chemical use in comparison to traditional tillage based methods of crop production. However, there are many agroecological alternatives to increased chemical use, such as winter cover crops and the mulch cover they provide, soil solarization or burning.\n\nNo-till farming requires some different skills than conventional farming. As with any production system, if done incorrectly, yields can drop. A combination of technique, equipment, pesticides, crop rotation, fertilization, and irrigation have to be used for local conditions.\n\nIn no-till occasionally uses cover crops to help control weeds and increase nutrients in the soil (by using legumes) or by using plants with long roots to pull mobile nutrients to the surface from lower layers of the soil. Cover crops then need to be killed so that the newly planted crops can get enough light, water, nutrients, etc. This can be done by rollers, crimper, choppers and other ways.\n\nWith no-till farming, residue from the previous years crops lie on the surface of the field, cooling it and increasing the moisture. This can increase or decrease disease or cause it to vary compared to tillage farming. To reduce weeds, pests and disease, crop rotation is used. Planting different crops year after year denies a pest or pathogen population a supply of whatever food it is adapted to consume.\n\nSome farmers who practice organic management often place ordinary, non-dyed corrugated cardboard on seed-beds and vegetable areas. Used correctly, cardboard placed on a specific area can \n\nPlant residues (left over plant matter originating from cover crops, grass clippings, original plant life etc.) rots while underneath the cardboard so long as it remains moist enough. This rotting attracts worms and other beneficial microorganisms to the site of decomposition, and over a few seasons (usually Spring to Fall or Fall to Spring) and up to a few years, creates a layer of rich topsoil. Plants can then be seeded into the soil in spring, or holes can be cut into the cardboard to allow transplanting. Using this method in conjunction with other sustainable practices such as composting/vermicompost, cover crops and rotations are often considered beneficial to both land and those who take from it.\n\nOn fields too large to manually apply a residue with a high carbon-to-nitrogen ratio, a cover crop may be used to produce a similar effect. The cover crop may be killed by mowing or by crimping the stalk, as with a roller/crimper. The residue is then planted through, and left as a mulch to retard weed growth and slowly release the nutrients contained therein. Cover crops typically must be crimped when they enter the flowering/pollination stage.\n\nNo-till farming dramatically reduces erosion in a field. While much less soil is displaced, any gullies that form get deeper each year instead of being smoothed out by regular plowing. This may be handled by creating sod drainways, waterways, permanent drainways, cover crops, etc.\n\nA problem in some fields is water saturation in soils. Switching to no-till farming corrects drainage because of the qualities of soil under continuous no-till include a higher water infiltration rate.\n\nIt is very important to have planting equipment that can properly penetrate through the residue, into the soil and prepare a good seedbed. No-till farming requires much less maximum power from farm tractors, so equipment (a tractor) can be smaller than under tilling. Using a smaller, lighter tractor has the added benefit of reducing compaction.\n\nAnother problem of no-till farming is that in spring, the soil both warms and dries more slowly, which may delay planting. The slower warming is due to crop residue being a lighter color than the soil which would be exposed in conventional tillage, which then absorbs less solar energy. This can be managed by using row cleaners on a planter. Since the soil can be cooler, harvest can occur a few days later than a conventionally tilled field. Note: A cooler soil is also a benefit because water doesn't evaporate as fast.\n\nOn some crops, like continuous no-till corn, the thickness of the residue on the surface of the field can become a problem without proper preparation and/or equipment.\n\nOne of the most common yield reducers is nitrogen being immobilized in the crop residue, which can take a few months to several years to decompose, depending on the crop's C to N ratio and the local environment. Fertilizer needs to be applied at a higher rate during the transition period while the soil rebuilds its organic matter. The nutrients in the organic matter are eventually released into the soil, so this is only a concern during the transition time frame (4–5 years for Kansas, USA). An innovative solution to this problem is to integrate animal husbandry in various ways to aid in decomposition.\n\nAlthough no-till farming often causes a slight increase in soil bulk density, periodic tilling is not needed to “fluff” the soil back up. No-till farming mimics the natural conditions under which most soils formed more closely than any other method of farming, in that the soil is left undisturbed except to place seeds in a position to germinate.\n\nNo-till farming is not equivalent to conservation tillage or strip tillage. Conservation tillage is a group of practices that reduce the amount of tillage needed. No-till and strip tillage are both forms of conservation tillage. No-till is the practice of never tilling a field. Tilling every other year is called rotational tillage.\n\n\n\n"}
{"id": "228053", "url": "https://en.wikipedia.org/wiki?curid=228053", "title": "Organizational communication", "text": "Organizational communication\n\nIn communication studies, organizational communication is the study of communication within organizations. The flow of communication could be either formal or informal. \n\nThe field traces its lineage through business information, business communication, and early mass communication studies published in the 1930s through the 1950s. Until then, organizational communication as a discipline consisted of a few professors within speech departments who had a particular interest in speaking and writing in business settings. The current field is well established with its own theories and empirical concerns distinct from other fields.\n\nSeveral seminal publications stand out as works broadening the scope and recognizing the importance of communication in the organizing process, and in using the term \"organizational communication\". Nobel Laureate Herbert A. Simon wrote in 1947 about \"organization communications systems\", saying communication is \"absolutely essential to organizations\". W. Charles Redding played a prominent role in the establishment of organizational communication as a discipline.\n\nIn the 1950s, organizational communication focused largely on the role of communication in improving organizational life and organizational output. In the 1980s, the field turned away from a business-oriented approach to communication and became concerned more with the constitutive role of communication in organizing. In the 1990s, critical theory influence on the field was felt as organizational communication scholars focused more on communication's possibilities to oppress and liberate organizational members.\n\nSome of the main assumptions underlying much of the early organizational communication research were:\n\n\nHerbert A. Simon introduced the concept of \"bounded rationality\" which challenged assumptions about the perfect rationality of communication participants. He maintained that people making decisions in organizations seldom had complete information, and that even if more information was available, they tended to pick the first acceptable option, rather than exploring further to pick the optimal solution.\n\nIn the early 1990s Peter Senge developed new theories on organizational communication. These theories were learning organization and systems thinking. These have been well received and are now a mainstay in current beliefs toward organizational communications.\n\nNetworks are another aspect of direction and flow of communication. Bavelas has shown that communication patterns, or networks, influence groups in several important ways. Communication networks may affect the group's completion of the assigned task on time, the position of the de facto leader in the group, or they may affect the group members' satisfaction from occupying certain positions in the network. Although these findings are based on laboratory experiments, they have important implications for the dynamics of communication in formal organizations.\n\nThere are several patterns of communication, such as \"chain\", \"wheel\", \"star\", \"all-channel\" network, and \"circle\".\n\nAbbreviations are used to indicate the two-way flow of information or other transactions, e.g. \"B2B\" is \"business to business\". Duplex point-to-point communication systems, computer networks, non-electronic telecommunications, and meetings in person are all possible with the use of these terms. Examples:\n\nOne-to-one or \"interpersonal communication\" between individuals may take several forms- messages may be verbal (that is, expressed in words), or non-verbal such as gestures, facial expressions, and postures (\"body language\"). Nonverbal messages may even stem from silence.\n\nManagers do not need answers to operate a successful business; they need questions. Answers can come from anyone, anytime, anywhere in the world thanks to the benefits of all the electronic communication tools at our disposal. This has turned the real job of management into determining what it is the business needs to know, along with the who/what/where/when and how of\nlearning it. To effectively solve problems, seize opportunities, and achieve objectives, questions need to be asked by managers—these are the people responsible for the operation of the enterprise as a whole.\n\nIdeally, the meanings sent are the meanings received. This is most often the case when the messages concern something that can be verified objectively. For example, \"This piece of pipe fits the threads on the coupling.\" In this case, the receiver of the message can check the sender's words by actual trial, if necessary. However, when the sender's words describe a feeling or an opinion about something that cannot be checked objectively, meanings can be very unclear. \"This work is too hard\" or \"Watergate was politically justified\" are examples of opinions or feelings that cannot be verified. Thus they are subject to interpretation and hence to distorted meanings. The receiver's background of experience and learning may differ enough from that of the sender to cause significantly different perceptions and evaluations of the topic under discussion. As we shall see later, such differences form a basic barrier to communication.\n\nNonverbal content always accompanies the verbal content of messages. When speaking about nonverbal communication, Birdwhistell says \"it is complementary to (meaning \"adds to\") rather than redundant with (or repeating of) the verbal behavior\". For example, if someone is talking about the length of an object, they may hold out their hands to give a visual estimate of it. This is reasonably clear in the case of face-to-face communication. As Virginia Satir has pointed out, people cannot help but communicate symbolically (for example, through their clothing or possessions) or through some form of body language. In messages that are conveyed by the telephone, a messenger, or a letter, the situation or context in which the message is sent becomes part of its non-verbal content. For example, if the company has been losing money, and in a letter to the production division, the front office orders a reorganization of the shipping and receiving departments, this could be construed to mean that some people were going to lose their jobs — unless it were made explicitly clear that this would not occur.\n\nA number of variables influence the effectiveness of communication. Some are found in the environment in which communication takes place, some in the personalities of the sender and the receiver, and some in the relationship that exists between sender and receiver. These different variables suggest some of the difficulties of communicating with understanding between two people. The sender wants to formulate an idea and communicate it to the receiver. This desire to communicate may arise from his thoughts or feelings or it may have been triggered by something in the environment. The communication may also be influenced by the relationship between the sender and the receiver, such as status differences, a staff-line relationship, or a learner-teacher relationship.\n\nWhatever its origin, information travels through a series of filters, both in the sender and in the receiver, and is affected by different channels, before the idea can be transmitted and re-created in the receiver's mind. Physical capacities to see, hear, smell, taste, and touch vary between people, so that the image of reality may be distorted even before the mind goes to work. In addition to physical or sense filters, cognitive filters, or the way in which an individual's mind interprets the world around him, will influence his assumptions and feelings. These filters will determine what the sender of a message says, how he says it, and with what purpose. Filters are present also in the receiver, creating a double complexity that once led Robert Louis Stevenson to say that human communication is \"doubly relative\". It takes one person to say something and another to decide what he said.\n\nPhysical and cognitive, including semantic filters (which decide the meaning of words) combine to form a part of our memory system that helps us respond to reality. In this sense, March and Simon compare a person to a data processing system. Behavior results from an interaction between a person's internal state and environmental stimuli. What we have learned through past experience becomes an inventory, or data bank, consisting of values or goals, sets of expectations and preconceptions about the consequences of acting one way or another, and a variety of possible ways of responding to the situation. This memory system determines what things we will notice and respond to in the environment. At the same time, stimuli in the environment help to determine what parts of the memory system will be activated. Hence, the memory and the environment form an interactive system that causes our behavior. As this interactive system responds to new experiences, new learnings occur which feed back into memory and gradually change its content. This process is how people adapt to a changing world.\n\nInformal and formal communication are used in an organization. Communication flowing through formal channels are downward, horizontal and upward whereas communication through informal channels are generally termed as grapevine.\n\nInformal communication, generally associated with interpersonal, horizontal communication, was primarily seen as a potential hindrance to effective organizational performance. This is no longer the case. Informal communication has become more important to ensuring the effective conduct of work in modern organizations.\n\nGrapevine is a random, unofficial means of informal communication. It spreads through an organization with access to individual interpretation as gossip, rumors, and single-strand messages. Grapevine communication is quick and usually more direct than formal communication. An employee who receives most of the grapevine information but does not pass it onto others is known as a \"dead-ender\". An employee that receives less than half of the grapevine information is an \"isolate\". Grapevine can include destructive miscommunication, but it can also be beneficial from allowing feelings to be expressed, and increased productivity of employees. \n\nTop-down approach: This is also known as downward communication. This approach is used by the top level management to communicate to the lower levels. This is used to implement policies, guidelines, etc. In this type of organizational communication, distortion of the actual information occurs. This could be made effective by feedbacks.\n\nAdditionally, McPhee and Zaug (1995) take a more nuanced view of communication as constitutive of organizations (also referred to as CCO). They identify four constitutive flows of communication, formal and informal, which become interrelated in order to constitute organizing and an organization:\n\nShockley-Zalabak identified the following two perspectives, essentially as ways of understanding the organizational communication process as a whole.\n\nAccording to Shockley-Zalabak, the functional tradition is \"a way of understanding organizational communication by describing what messages do and how they move through organizations.\" There are different functions within this process that all work together to contribute to the overall success of the organization, and these functions occur during the repetition of communication patterns in which the members of the organization engage in. The first types of functions are message functions which are \"What communication does or how it contributes to the overall functioning of the organization\", and we describe message functions in three different categories which are organizational functions, relationship functions, and change functions. Organizing functions as Shockley-Zalabak states, are \"Messages that establish the rules and regulations of a particular environment\". These messages can include items such as newsletters or handbooks for a specific organization, that individuals can read to learn the policies and expectations for a certain company. Relationship functions are \"Communication that helps individuals define their roles and assess the compatibility of individual and organizational goals\". These relationship functions are a key aspect to how individuals identify with a company and it helps them develop their sense of belonging which can greatly influence their quality of work. The third and final subcategory within message functions are change functions, which are defined as \"messages that help organizations adapt what they do and how they do it\". Change messages occur in various choice making decisions, and they are essential to meet the employee's needs as well as have success with continual adaptations within the organization.\n\nAccording to Shockley-Zalabak, the meaning centered approach is \"a way of understanding organizational communication by discovering how organizational reality is generated through human interaction\". This approach is more concerned with what communication is instead of why and how it works, and message functions as well as message movement are not focused on as thoroughly in this perspective.\n\nHistorically, organizational communication was driven primarily by quantitative research methodologies. Included in functional organizational communication research are statistical analyses (such as surveys, text indexing, network mapping and behavior modeling). In the early 1980s, the interpretive revolution took place in organizational communication. In Putnam and Pacanowsky's 1983 text \"Communication and Organizations: An Interpretive Approach\". they argued for opening up methodological space for qualitative approaches such as narrative analyses, participant-observation, interviewing, rhetoric and textual approaches readings) and philosophic inquiries.\n\nIn addition to qualitative and quantitative research methodologies, there is also a third research approach called mixed methods. \"Mixed methods is a type of procedural approach for conducting research that involves collecting, analyzing, and mixing quantitative and qualitative data within a single program of study. Its rationale postulates that the use of both qualitative and quantitative research provides a better and richer understanding of a research problem than either traditional research approach alone provides.\" Complex contextual situations are easier to understand when using a mixed methods research approach, compared to using a qualitative or quantitative research approach. There are more than fifteen mixed method design typologies that have been identified. Because these typologies share many characteristics and criteria, they have been classified into six different types. Three of these types are sequential, meaning that one type of data collection and analysis happens before the other. The other three designs are concurrent, meaning both qualitative and quantitative data are collected at the same time.\n\nTo achieve results from a sequential explanatory design, researchers would take the results from analyzing quantitative data and get a better explanation through a qualitative follow up. They then interpret how the qualitative data explains the quantitative data.\n\nAlthough sequential exploratory design may resemble sequential explanatory design, the order in which data is collect is revered. Researchers begin with collecting qualitative data and analyzing it, then follow up by building on it through a quantitative research method. They use the results from qualitative data to form variables, instruments and interventions for quantitative surveys and questionnaires.\n\nResearchers in line with sequential transformative design are led by a \"theoretical perspective such as a conceptual framework, a specific ideology, or an advocacy position and employs what will best serve the researcher's theoretical or ideological perspective\". Therefore, with this research design, it could be either the qualitative or quantitative method that is used first and priority depends on circumstance and resource availability, but can be given to either.\n\nWith a concurrent triangulation design, although data is collected through both quantitative and qualitative methods at the same time, they are collected separately with equal priority during one phase. Later, during the analysis phase, the mixing of the two methods takes place.\n\nIn a concurrent embedded design, again, both qualitative and quantitative data is collected, although here one method supports the other. Then, one of the two methods (either qualitative or quantitative) transforms into a support for the dominant method.\n\nThe concurrent transformative design allows the researcher to be guided by their theoretical perspective, so their qualitative and quantitative data may have equal or unequal priority. Again, they are both collected during one phase.\n\nMixed methods capitalizes on maximizing the strengths of qualitative and quantitative research and minimizing their individual weaknesses by combining both data. Quantitative research is criticized for not considering contexts, having a lack of depth and not giving participants a voice. On the other hand, qualitative research is criticized for smaller sample sizes, possible researcher bias and a lack of generalizability.\n\nDuring the 1980s and 1990s critical organizational scholarship began to gain prominence with a focus on issues of gender, race, class, and power/knowledge. In its current state, the study of organizational communication is open methodologically, with research from post-positive, interpretive, critical, postmodern, and discursive paradigms being published regularly.\n\nOrganizational communication scholarship appears in a number of communication journals including but not limited to \"Management Communication Quarterly\", \"Journal of Applied Communication Research\", \"Communication Monographs\", \"Academy of Management Journal\", \"Communication Studies\", and \"Southern Communication Journal\".\n\nIn some circles, the field of organizational communication has moved from acceptance of mechanistic models (e.g., information moving from a sender to a receiver) to a study of the persistent, hegemonic and taken-for-granted ways in which we not only use communication to accomplish certain tasks within organizational settings (e.g., public speaking) but also how the organizations in which we participate affect us.\n\nThese approaches include \"postmodern\", \"critical\", \"participatory\", \"feminist\", \"power/political\", \"organic\", etc. and adds to disciplines as wide-ranging as sociology, philosophy, theology, psychology, business, business administration, institutional management, medicine (health communication), neurology (neural nets), semiotics, anthropology, international relations, and music.\n\nCurrently, some topics of research and theory in the field are:\n\nConstitution, e.g.,\n\nNarrative, e.g.,\n\nIdentity, e.g.,\n\nInterrelatedness of organizational experiences, e.g.,\n\nPower e.g.,\n\n\n\n\n\n"}
{"id": "14988379", "url": "https://en.wikipedia.org/wiki?curid=14988379", "title": "Orto Botanico dell'Università di Modena e Reggio Emilia", "text": "Orto Botanico dell'Università di Modena e Reggio Emilia\n\nThe Orto Botanico dell'Università di Modena e Reggio Emilia, also known as the Orto Botanico di Modena or formerly Hortus Botanicus Mutinensis, is a botanical garden operated by the University of Modena and Reggio Emilia. It is located next to the Garden Ducale (Public Gardens), at viale Caduti in Guerra 127, Modena, Italy, and open weekdays during the warmer months except August. Admission is free.\n\nThe garden was established in 1758 by Duke Francesco III d'Este for medicinal plants, becoming part of the university in 1772. The garden is an irregular shape, almost 1 hectare in size, with several greenhouses (total area 300 m²) running in a line across the garden's center. It currently contains about 1,400 species plus a major herbarium. The principal outdoor areas are:\n\n\nGreenhouses are as follows:\n\n\n\n"}
{"id": "2475751", "url": "https://en.wikipedia.org/wiki?curid=2475751", "title": "Otto Vasilievich Bremer", "text": "Otto Vasilievich Bremer\n\nOtto Vasilievich Bremer (died 11 November 1873) was a Russian naturalist and entomologist.\n\nHe wrote:\n\nHe described many insects, including the large skipper butterfly. Bremer's collection is in the Zoological Museum of the Russian Academy of Science in Saint Petersburg where he lived.\n\n\n"}
{"id": "44531850", "url": "https://en.wikipedia.org/wiki?curid=44531850", "title": "Permanent cell", "text": "Permanent cell\n\nPermanent cells are cells that are incapable of regeneration. These cells are considered to be terminally differentiated and non proliferative in postnatal life. This includes brain cells, neurons, heart cells, skeletal muscle cells, and red blood cells. Disease and virology studies can use permanent cells to maintain cell count and accurately quantify the effects of vaccines. Some embryology studies also use permanent cells to avoid harvesting embryonic cells from pregnant animals; since the cells are permanent, they may be harvested at a later age when an animal is fully developed.\n"}
{"id": "49331144", "url": "https://en.wikipedia.org/wiki?curid=49331144", "title": "Pressure-driven flow", "text": "Pressure-driven flow\n\nPressure driven flow is a method to displace liquids in a capillary or microfluidic channel with pressure. The pressure is typically generated pneumatically by compressed air or other gases (Nitrogen, Carbon dioxide, etc) or by electrical and magnetical fields or gravitation.\n\nIt is known from thermodynamics that conjugated quantities scale in a different manner. Two classes can be distinguished: intensive quantities as temperature \"T\", pressure \"P\" and amount of substance \"N\" or extensive quantities as entropy \"S\", volume \"V\" and chemical potential \"μ\". Extensive quantities scale with system size, whereas the intensive ones do not. The quantity pressure, for example, is defined as the (differential) quotient of two extensive variables: \"p\"=d\"E\"/d\"V\" (Energy \"E\" und Volume \"V\") and therefore scale independent as the same scaling factors appearing in the nominator as well as the denominator cancel. In microsystems the problem rises that the extremely small volumes are difficult to be controlled. The reason is the predominance of surface effects as surface charges, van-der-Waals forces and entropic effects (e.g. dewetting due to rough surfaces: the restriction in degrees of freedom of molecules penetrating such a surface is entropically more expensive than staying in bulk). Furthermore, the microsystem has to be controlled from a macroscopic human scale.\n"}
{"id": "17887712", "url": "https://en.wikipedia.org/wiki?curid=17887712", "title": "Research program", "text": "Research program\n\nA research program (UK: research programme) is a professional network of scientists conducting basic research. The term was used by philosopher of science Imre Lakatos to blend and revise the normative model of science offered by Karl Popper's \"falsificationism\" and the descriptive model of science offered by Thomas Kuhn's \"normal science\". Lakatos found falsificationism impractical and often not practiced, and found normal science—where a \"paradigm of science\", mimicking an \"exemplar\", extinguishes differing perspectives—more monopolistic than actual.\n\nLakatos found that many research programmes coexisted. Each had a \"hard core\" of theories immune to revision, surrounded by a \"protective belt\" of malleable theories. A research programme vies against others to be most \"progressive\". Extending the research programme's theories into new domains is \"theoretical progress\", and experimentally corroborating such is \"empirical progress\", always refusing falsification of the research programme's hard core. A research programme might \"degenerate\"—lose progressiveness—but later return to progressiveness.\n\n"}
{"id": "1380178", "url": "https://en.wikipedia.org/wiki?curid=1380178", "title": "Tad Murty", "text": "Tad Murty\n\nTad S. Murty (or\" Murthy\") is an Indian-Canadian oceanographer and expert on tsunamis. He is the former president of the Tsunami Society. He is an adjunct professor in the departments of Civil Engineering and Earth Sciences at the University of Ottawa. Murty has a PhD degree in oceanography and meteorology from the University of Chicago.\nHe is co-editor of the journal Natural Hazards with Tom Beer of CSIRO and Vladimir Schenk of the Czech Republic.\n\nHe has taken part in a review of the 2007 Intergovernmental Panel on Climate Change.\n\nMurty characterizes himself as a global warming skeptic. In an August 17, 2006 interview, he stated that \"I started with a firm belief about global warming, until I started working on it myself...I switched to the other side in the early 1990s when Fisheries and Oceans Canada asked me to prepare a position paper and I started to look into the problem seriously.\". Murty has also stated that global warming is \"the biggest scientific hoax being perpetrated on humanity. There is no global warming due to human anthropogenic activities.\" Murty was among the sixty scientists from climate research and related disciplines who authored a 2006 open letter to Canadian Prime Minister Stephen Harper criticizing the Kyoto Protocol and the scientific basis of anthropogenic global warming.\n\n"}
{"id": "19869578", "url": "https://en.wikipedia.org/wiki?curid=19869578", "title": "The Energy Construct", "text": "The Energy Construct\n\nThe Energy Construct is a 2007 non-fiction book by author Ben Cipiti that examines the challenge of achieving a clean, independent, and economical energy future for the United States. The book examines alternative transportation fuels, renewable energy, nuclear energy, and clean fossil energy to develop a path forward.\n\nThe continued reliance on fossil fuels in the United States is having drastic consequences to the environment, and continued reliance on foreign oil threatens national security and economic well-being. Alternative energy technologies have a number of strengths and weaknesses. This book examines the various technologies from the standpoints of economics, environmental impact, domestic resource potential, public acceptability, and reliability to find a path forward.\n\nThe transportation alternatives to oil that are examined in the book include biofuels, hydrogen, and electric vehicles. The author concludes that only electric vehicles will achieve drastically reduced emissions in an efficient manner, and plug-in hybrids will be the first step that will lead to economical competitiveness.\n\nIncreased reliance on electricity will make it that much more important to develop clean power plants. Renewable energy will be part of the solution, but high costs are poor reliability continue to make renewable sources challenging to develop cost-effectively. The author concludes that wind energy has the greatest potential for near-term expansion. However, smaller-scale use of solar, geothermal, biomass, and ocean sources along with current hydroelectric sources could add up to make a significant contribution. The advantages and disadvantages of each renewable energy option are examined.\n\nNuclear energy is also supported by the author as the only way to produce large amounts of emission-free power. The author pays close attention to the safety of nuclear energy and provides a unique comparison of the safety of energy generation from a number of sources.\n\nFossil energy is then examined with a focus on the development of carbon sequestration technologies. Coal power plants will full carbon sequestration are shown to be competitive with existing natural gas plants due to the currently high natural gas prices. With abundant coal resources in the United States, the author concludes that coal will continue to be an important part of U.S. energy generation for many decades.\n\n"}
{"id": "40602098", "url": "https://en.wikipedia.org/wiki?curid=40602098", "title": "The Road to Infinity", "text": "The Road to Infinity\n\nThe Road to Infinity is a collection of seventeen scientific essays by Isaac Asimov. It was the fourteenth of a series of books collecting Asimov's science essays from \"The Magazine of Fantasy and Science Fiction\". It also included a list of all of Asimov's essays in that magazine up to 1979. It was first published by Doubleday & Company in 1979.\n\n\n"}
{"id": "33452725", "url": "https://en.wikipedia.org/wiki?curid=33452725", "title": "Unscented transform", "text": "Unscented transform\n\nThe unscented transform (UT) is a mathematical function used to estimate the result of applying a given nonlinear transformation to a probability distribution that is characterized only in terms of a finite set of statistics. The most common use of the unscented transform is in the nonlinear projection of mean and covariance estimates in the context of nonlinear extensions of the Kalman filter. Its creator Jeffrey Uhlmann explained that \"unscented\" was an arbitrary name that he adopted to avoid it being referred to as the “Uhlmann filter.”\n\nMany filtering and control methods represent estimates of the state of a system in the form of a mean vector and an associated error covariance matrix. As an example, the estimated 2-dimensional position of an object of interest might be represented by a mean position vector, formula_1, with an uncertainty given in the form of a 2x2 covariance matrix giving the variance in formula_2, the variance in formula_3, and the cross covariance between the two. A covariance that is zero implies that there is no uncertainty or error and that the position of the object is exactly what is specified by the mean vector.\n\nThe mean and covariance representation only gives the first two moments of an underlying, but otherwise unknown, probability distribution. In the case of a moving object, the unknown probability distribution might represent the uncertainty of the object's position at a given time. The mean and covariance representation of uncertainty is mathematically convenient because any linear transformation formula_4 can be applied to a mean vector formula_5 and covariance matrix formula_6 as formula_7 and formula_8. This linearity property does not hold for moments beyond the first raw moment (the mean) and the second central moment (the covariance), so it is not generally possible to determine the mean and covariance resulting from a nonlinear transformation because the result depends on all the moments, and only the first two are given.\n\nAlthough the covariance matrix is often treated as being the expected squared error associated with the mean, in practice the matrix is maintained as an upper bound on the actual squared error. Specifically, a mean and covariance estimate formula_9 is conservatively maintained so that the covariance matrix formula_6 is greater than or equal to the actual squared error associated with formula_5. Mathematically this means that the result of subtracting the expected squared error (which is not usually known) from formula_6 is a semi-definite or positive-definite matrix. The reason for maintaining a conservative covariance estimate is that most filtering and control algorithms will tend to diverge (fail) if the covariance is underestimated. This is because a spuriously small covariance implies less uncertainty and leads the filter to place more weight (confidence) than is justified in the accuracy of the mean.\n\nReturning to the example above, when the covariance is zero it is trivial to determine the location of the object after it moves according to an arbitrary nonlinear function formula_13: just apply the function to the mean vector. When the covariance is not zero the transformed mean will \"not\" generally be equal to formula_13 and it is not even possible to determine the mean of the transformed probability distribution from only its prior mean and covariance. Given this indeterminacy, the nonlinearly transformed mean and covariance can only be approximated. The earliest approximation was to linearize the nonlinear function and apply the resulting Jacobian matrix to the given mean and covariance. This is the basis of the extended Kalman Filter (EKF), and although it was known to yield poor results in many circumstances, there was no practical alternative for many decades.\n\nIn 1994 Jeffrey Uhlmann noted that the EKF takes a nonlinear function and partial distribution information (in the form of a mean and covariance estimate) of the state of a system but applies an approximation to the known function rather than to the imprecisely-known probability distribution. He suggested that a better approach would be to use the exact nonlinear function applied to an approximating probability distribution. The motivation for this approach is given in his doctoral dissertation, where the term \"unscented transform\" was first defined:\n\nConsider the following intuition: \"With a fixed number of parameters it should be easier to approximate a given distribution than it is to approximate an arbitrary nonlinear function/transformation\". Following this intuition, the goal is to find a parameterization that captures the mean and covariance information while at the same time permitting the direct propagation of the information through an arbitrary set of nonlinear equations. This can be accomplished by generating a discrete distribution having the same first and second (and possibly higher) moments, where each point in the discrete approximation can be directly transformed. The mean and covariance of the transformed ensemble can then be computed as the estimate of the nonlinear transformation of the original distribution. More generally, the application of a given nonlinear transformation to a discrete distribution of points, computed so as to capture a set of known statistics of an unknown distribution, is referred to as an \"unscented transformation\".\n\nIn other words, the given mean and covariance information can be exactly encoded in a set of points, referred to as \"sigma points\", which if treated as elements of a discrete probability distribution has mean and covariance equal to the given mean and covariance. This distribution can be propagated \"exactly\" by applying the nonlinear function to each point. The mean and covariance of the transformed set of points then represents the desired transformed estimate. The principal advantage of the approach is that the nonlinear function is fully exploited, as opposed to the EKF which replaces it with a linear one. Eliminating the need for linearization also provides advantages independent of any improvement in estimation quality. One immediate advantage is that the UT can be applied with any given function whereas linearization may not be possible for functions that are not differentiable. A practical advantage is that the UT can be easier to implement because it avoids the need to derive and implement a linearizing Jacobian matrix.\n\nTo compute the unscented transform, one first has to choose a set of sigma points. Since the seminal work of Uhlmann, many different sets of sigma points have been proposed in the literature. A thoroughgoing review of these variants can be found in the work of Menegaz et. al. In general, formula_15 sigma points are necessary and sufficient to define a discrete distribution having a given mean and covariance in formula_16 dimensions. \n\nA canonical set of sigma points is the symmetric set originally proposed by Uhlmann. Consider the following simplex of points in two dimensions:\n\nIt can be verified that the above set of points has mean formula_18 and covariance formula_19 (the identity matrix). Given any 2-dimensional mean and covariance, formula_20, the desired sigma points can be obtained by multiplying each point by the matrix square root of formula_21 and adding formula_2. A similar canonical set of sigma points can be generated in any number of dimensions formula_16 by taking the zero vector and the points comprising the rows of the identity matrix, computing the mean of the set of points, subtracting the mean from each point so that the resulting set has a mean of zero, then computing the covariance of the zero-mean set of points and applying its inverse to each point so that the covariance of the set will be equal to the identity.\n\nUhlmann showed that it is possible to conveniently generate a symmetric set of formula_24 sigma points from the columns of formula_25, where formula_21 is the given covariance matrix, without having to compute a matrix inverse. It is computationally efficient and, because the points form a symmetric distribution, captures the third central moment (the skew) whenever the underlying distribution of the state estimate is known or can be assumed to be symmetric. He also showed that weights, including negative weights, can be used to affect the statistics of the set. Julier also developed and examined techniques for generating sigma points to capture the third moment (the skew) of an arbitrary distribution and the fourth moment (the kurtosis) of a symmetric distribution.\n\nThe unscented transform is defined for the application of a given function to any partial characterization of an otherwise unknown distribution, but its most common use is for the case in which only the mean and covariance is given. A common example is the conversion from one coordinate system to another, such as from a Cartesian coordinate frame to polar coordinates.\n\nSuppose a 2-dimensional mean and covariance estimate, formula_27, is given in Cartesian coordinates with:\n\nand the transformation function to polar coordinates, formula_29, is:\n\nMultiplying each of the canonical simplex sigma points (given above) by formula_31 and adding the mean, formula_5, gives:\n\nApplying the transformation function formula_34 to each of the above points gives:\n\nThe mean of these three transformed points, formula_36, is the UT estimate of the mean in polar coordinates:\nThe UT estimate of the covariance is:\nwhere each squared term in the sum is a vector outer product. This gives:\n\nThis can be compared to the linearized mean and covariance:\n\nThe absolute difference between the UT and linearized estimates in this case is relatively small, but in filtering applications the cumulative effect of small errors can lead to unrecoverable divergence of the estimate. The effect of the errors are exacerbated when the covariance is underestimated because this causes the filter to be overconfident in the accuracy of the mean. In the above example it can be seen that the linearized covariance estimate is smaller than that of the UT estimate, suggesting that linearization has likely produced an underestimate of the actual error in its mean.\n\nIn this example there is no way to determine the absolute accuracy of the UT and linearized estimates without ground truth in the form of the actual probability distribution associated with the original estimate and the mean and covariance of that distribution after application of the nonlinear transformation (e.g., as determined analytically or through numerical integration). Such analyses have been performed for coordinate transformations under the assumption of Gaussianity for the underlying distributions, and the UT estimates tend to be significantly more accurate than those obtained from linearization.\n\nEmpirical analysis has shown that the use of the minimal simplex set of formula_15 sigma points is significantly less accurate than the use of the symmetric set of formula_24 points when the underlying distribution is Gaussian. This suggests that the use of the simplex set in the above example would not be the best choice if the underlying distribution associated with formula_9 is symmetric. Even if the underlying distribution is not symmetric, the simplex set is still likely to be less accurate than the symmetric set because the asymmetry of the simplex set is not matched to the asymmetry of the actual distribution.\n\nReturning to the example, the minimal symmetric set of sigma points can be obtained from the covariance matrix formula_44 simply as the mean vector, formula_45 plus and minus the columns of formula_46:\n\nThis construction guarantees that the mean and covariance of the above four sigma points is formula_9, which is directly verifiable. Applying the nonlinear function formula_34 to each of the sigma points gives:\n\nThe mean of these four transformed sigma points, formula_51, is the UT estimate of the mean in polar coordinates:\nThe UT estimate of the covariance is:\nwhere the each squared term in the sum is a vector outer product. This gives:\nThe difference between the UT and linearized mean estimates gives a measure of the effect of the nonlinearity of the transformation. When the transformation is linear, for instance, the UT and linearized estimates will be identical. This motivates the use of the square of this difference to be added to the UT covariance to guard against underestimating of the actual error in the mean. This approach does not improve the accuracy of the mean but can significantly improve the accuracy of a filter over time by reducing the likelihood that the covariance is underestimated.\n\nUhlmann noted that given only the mean and covariance of an otherwise unknown probability distribution, the transformation problem is ill-defined because there is an infinite number of possible underlying distributions with the same first two moments. Without any a priori information or assumptions about the characteristics of the underlying distribution, any choice of distribution used to compute the transformed mean and covariance is as reasonable as any other. In other words, there is no choice of distribution with a given mean and covariance that is superior to that provided by the set of sigma points, therefore the unscented transform is trivially optimal.\n\nThis general statement of optimality is of course useless for making any quantitative statements about the performance of the UT, e.g., compared to linearization; consequently he, Julier and others have performed analyses under various assumptions about the characteristics of the distribution and/or the form of the nonlinear transformation function. For example, if the function is differentiable, which is essential for linearization, these analyses validate the expected and empirically-corroborated superiority of the unscented transform.\n\nThe unscented transform can be used to develop a non-linear generalization of the Kalman filter, known as the Unscented Kalman Filter (UKF). This filter has largely replaced the EKF in many nonlinear filtering and control applications, including for underwater, ground and air navigation, and spacecraft. The unscented transform has also been used as a computational framework for Riemann-Stieltjes optimal control. This computational approach is known as unscented optimal control.\n\nUhlmann and Simon Julier published several papers showing that the use of the unscented transformation in a Kalman filter, which is referred to as the unscented Kalman filter (UKF), provides significant performance improvements over the EKF in a variety of applications.\nIt should be noted that Julier and Uhlmann published papers using a particular parameterized form of the unscented transform in the context of the UKF which used negative weights to capture assumed distribution information. That form of the UT is susceptible to a variety of numerical errors that the original formulations (the symmetric set originally proposed by Uhlmann) do not suffer. Julier has subsequently described parameterized forms which do not use negative weights and also are not subject to those issues.\n\n"}
{"id": "4844150", "url": "https://en.wikipedia.org/wiki?curid=4844150", "title": "Value-added theory", "text": "Value-added theory\n\nValue-added theory (also known as social strain theory) was first proposed by Neil Smelser and is based on the assumption that certain conditions are needed for the development of a social movement. Smelser saw social movements as side-effects of rapid social change.\n\nSmelser argued that six things were necessary and sufficient for collective behavior to emerge, and that social movement evolves through those relevant stages:\n\nThe concept of value added was used earlier in economics, where it refers to the increasing value of product in progressing stages of production.\n\nCritics of this theory note that it is too focused on the structural-functional approach and views all strains on society as disruptive.\n\n\n\n"}
{"id": "393911", "url": "https://en.wikipedia.org/wiki?curid=393911", "title": "Vostok (rocket family)", "text": "Vostok (rocket family)\n\nVostok (Russian: Восток, translated as \"East\") was a family of rockets derived from the Soviet R-7 Semyorka ICBM and was designed for the human spaceflight programme. This family of rockets launched the first artificial satellite (Sputnik 1) and the first manned spacecraft (Vostok) in human history. It was a subset of the R-7 family of rockets.\n\nOn March 18, 1980 a Vostok-2M rocket exploded on its launch pad at Plesetsk during a fueling operation, killing 48 people. An investigation into a similarbut avoidedaccident revealed that the substitution of lead-based for tin-based solder in hydrogen peroxide filters allowed the breakdown of the HO, thus causing the resultant explosion.\n\nThe major versions of the rocket were:\n\n\n\n"}
{"id": "10328125", "url": "https://en.wikipedia.org/wiki?curid=10328125", "title": "Wan Chun Cheng", "text": "Wan Chun Cheng\n\nZheng Wanjun or Wan Chun Cheng (, 1908–1987) was one of the most eminent Chinese botanists of the 20th century. Initially one of the Chinese plant collectors who followed in the wake of the Europeans after 1920, he became one of the world's leading authorities on the taxonomy of gymnosperms. Working at the National Central University in Nanjing, he was instrumental in the identification in 1944 of the Dawn Redwood, \"Metasequoia glyptostroboides\" previously known only from fossils. The plant \"Juniperus chengii\" is named in his honour.\n"}
