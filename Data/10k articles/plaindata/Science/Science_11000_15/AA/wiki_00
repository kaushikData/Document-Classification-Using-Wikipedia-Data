{"id": "57752176", "url": "https://en.wikipedia.org/wiki?curid=57752176", "title": "21-Crown-7", "text": "21-Crown-7\n\n21-Crown-7 is an organic compound with the formula [CHO] and the IUPAC name of 1,4,7,10,13,16,19-heptaoxacycloheneicosane. Like other crown ethers, 21-crown-7 functions as a ligand for some metal cations with a particular affinity for caesium cations. The point group of 21-crown-7 is S. The dipole moment of 21-crown-7 varies in different solvents and under different temperatures.\n\n"}
{"id": "20926478", "url": "https://en.wikipedia.org/wiki?curid=20926478", "title": "4th meridian east", "text": "4th meridian east\n\nThe meridian 4° east of Greenwich is a line of longitude that extends from the North Pole across the Arctic Ocean, the Atlantic Ocean, Europe, Africa, the Southern Ocean, and Antarctica to the South Pole.\n\nThe 4th meridian east forms a great circle with the 176th meridian west.\n\nStarting at the North Pole and heading south to the South Pole, the 4th meridian east passes through:\n"}
{"id": "17524713", "url": "https://en.wikipedia.org/wiki?curid=17524713", "title": "A*STAR Talent Search", "text": "A*STAR Talent Search\n\nThe A*STAR Talent Search (ATS) is a research-based science competition in Singapore for high school students between 15–21 years of age. It was formerly known as National Science Talent Search. The ATS is an annual competition which acknowledges and rewards students who have a strong aptitude for science & technology. This competition provides students the opportunity to showcase their stellar projects and encourage them to further explore science and technology.\n\nThe ATS is administered by the Agency for Science, Technology and Research (A*STAR) and Science Centre Singapore (SCS) from 2006. Participants are required to compete in the Singapore Science and Engineering Fair (SSEF) and winners from the fair will then proceed to the short-listing round of ATS. The panel of judges consists of distinguished scientists from local and international universities, as well as A*STAR research institutes and a Nobel Laureate as the Chief Judge. ATS winners need to display resourcefulness, mastery of scientific concepts, as well as passion for scientific research.\n\nThe First Prize winner will be given S$5000, inclusive of a sponsored overseas conference.\n\nWinners and finalists (top 8 students) of the ATS have gone on to top universities worldwide, such as Harvard University, Princeton University, Yale University, Stanford University, Massachusetts Institute of Technology and California Institute of Technology in the United States, and University of Cambridge, University of Oxford and Imperial College London in the United Kingdom.\n\n"}
{"id": "8219401", "url": "https://en.wikipedia.org/wiki?curid=8219401", "title": "A4119 road", "text": "A4119 road\n\nThe A4119 links Tonypandy with Cardiff in South Wales.\n\nSettlements served by the route include Tonypandy, Penygraig, Williamstown, Tonyrefail, Ynysmaerdy, Talbot Green, Llantrisant, Groes-faen, Creigiau, Llandaff, Canton, Riverside, Grangetown, Butetown, Cardiff Bay.\n\nThe A4119 starts outside the Wales Millennium Centre at Cardiff Bay and proceeds through Butetown, Grangetown, Riverside and Cathedral Road in Canton until meeting the A48 road (Western Avenue) at Llandaff.\nFrom here is proceeds through Llandaff passing the BBC Wales studios. After leaving Llandaff the road takes on a more rural setting with many bends crossing the M4 Motorway near Capel Llanitern. From here the road snakes passed the settlement of Creigiau.\n\nThe road enters the County Borough of Rhondda Cynon Taf (formerly Mid Glamorgan) at Groes-faen. The road continues through the Village until it comes to a T-Junction at The Castell Mynach Public House. To the left is a Spur of the A4119 that links to Junction 34 of the M4 Motorway. Off the roundabout at the Junction is the Bosch Electronics Plant.\n\nThe road continues after the t-junction past Miskin and Mwyndy.\n\nThe A4119 through Talbot Green was first built Circa. 1977 as a single carriageway passing to the side of the village straight on to Ely Valley Road, easing pressure off the original small lanes that lead from Talbot Green to the area known as Mwyndy, south of the village. The road here was built at the same time as an extension of the original Talbot - Cardiff road, leading to Junction 34 of the M4 Motorway, and creating better access to the lanes leading to the Vale of Glamorgan.\nAt Mwyndy Cross, you can still see the original lane that the road used, signposted \"Arthur Llewelyn Jenkins\", the lane is accessible to the furniture store and as far as Cefn-y-Parc cemetery but is blocked at the bypass.\nThere is a roundabout with the Talbot Green Bypass after passing Mwyndy Cross and the original roads - the A473 Pontypridd to Bridgend Road. This roundabout was added when the bypass was built in 1991 and was improved during mid 2016 to mid 2017 to add extra dedicated lanes and a traffic lighting system. Another roundabout, was located at the base of Llantrisant, where the A473 originally ran. When the bypass was built, it cut off the original main road from Cowbridge via Pontyclun and therefore traffic is forced to use the new Bypass.\n\nThe decision was made by Mid Glamorgan County Council in the early 1990s to dual the section of A4119 between Ynysmaerdy and Mwyndy. This would be the last major road project undertaken by Mid Glamorgan before its abolishment in 1996.\n\nThe completed \"Talbot Green to Ynysmaerdy Dual Carriageway\" officially opened on 28 July 1995. The scheme replaced the old Roundabout near Llantrisant with a four way traffic-lit junction, and completely replaced the original Ely Valley Road at Talbot Green apart from the first portion: a t-junction from the centre Talbot Green's high street that leads to a row of houses and a Golf Club, this is the only remaining section Ely Valley Road that is un-altered, still bearing the original road surface and signage, although it is now a Cul-de-sac.\nA department store called Homeworld opened in the 1980s prior to the construction of the new Dual Carriageway at Talbot Green until the store was closed in 1999 and demolished in 2000. This, alongside a Tesco Store marked the rush of new developments in the area, the original Tesco store which opened in the 1980s was closed and demolished in 2003, after it relocated to larger premises on the former Homeworld site. Talbot Green Shopping Park was built on the former Tesco site and opened in 2004.\n\nAt the next roundabout the road takes the shape of a bow passed the Ely Meadows which is now the site of Magden Park, a development of Offices, Hotel, Pub and private Health facility. Bypassing the Royal Glamorgan Hospital, Avionics department of British Airways and the Headquarters of the Welsh Blood Service.\n\nOn the south-bound carriageway is a long stretch of original road, although now a parking area where refreshment vans and sleepy truckers are a common sight, it provides access to some houses and a rural lane leading to Llantrisant.\n\nThe Road then comes to a roundabout with a turn off for the Llantrisant Industrial Park which is home to the Royal Mint.\n\nAnother smaller roundabout at the village of Ynysmaerdy, where the new Headquarters of the South Wales Fire and Rescue Service is located, marks the end of the Dual Carriageway, the road becomes a single carriageway again at this point.\n\nPast Ynysmaerdy the duel carriage way merged into a single lane to climb a hill, which is known locally as \"Stink Pot Hill\" because of the sewerage works that is on the side of the road. \n\nAt the next roundabout the road splits, this is where a bypass separates from the original course of the A4119. The old road through the village of Coed-Ely is to the right. The bypass, completed in 1987, takes the route of the old Ely Valley Railway, and the former site of the Coed-Ely Colliery is on the left. From here the road is still single carriageway with numerous rest areas. The road continues on another bypass past Tonyrefail.\nShortly after Tonyrefail the road once again forks, with a junction for the A4233 which links with Trebanog and Porth. The A4119 from here follows the floor of the Valley through woodland and past a small industrial estate with the Village of Williamstown. At this roundabout the A4119 meets momentarily with its original course before continuing along the old Railway line. The route then bypasses the village of Penygraig and south of Tonypandy where is progresses down a steep incline. There is a spur with Clydach Vale before the road, still following an old railway line, progresses down to the floor of the valley where the A4119 ends with a roundabout of the A4058.\nOriginally the A4119 continued along Colliers Way and Llwynypia Road, where it again followed its original course, to a Terminus at Llwynypia Hospital with the A4058, but following the completion of the Porth Relief Road, the roads were re-numbered. Therefore, Colliers Way and the Tonypandy North Bypass are now part of the A4058 Pontypridd to Treorchy route.\n"}
{"id": "54271259", "url": "https://en.wikipedia.org/wiki?curid=54271259", "title": "Abell 2152", "text": "Abell 2152\n\nAbell 2152 is a bimodal galaxy cluster and one of three clusters comprising the Hercules Supercluster. It contains 3 BCGs; the S0 lenticular UGC 10204, the pair UGC 10187, and the SA0 unbarred lenticular CGCG 108-083. In total there are 41 galaxies which are confirmed to be members of the cluster. The cluster is classified as a Bautz-Morgan type III and Rood-Sastry class F cluster, indicating morphological irregularity and perhaps dynamical youth. It is receding from the Milky Way galaxy with a velocity of 12385 km/s.\n\nAbell 2152 is the nearest cluster in which significant gravitational lensing of a background source has been observed. The arc-like background galaxy, known as J160529.52+162633.9, lies at a redshift z=0.1423 and has been magnified by a factor ~1.9 due to the lensing effect.\n"}
{"id": "53575775", "url": "https://en.wikipedia.org/wiki?curid=53575775", "title": "Aggregate (geology)", "text": "Aggregate (geology)\n\nIn the Earth sciences, aggregrate has three possible meanings. \n\nIn mineralogy and petrology, an aggregate is a mass of mineral crystals, mineraloid particles or rock particles. Examples are dolomite rock, which is an aggregate of crystals of the mineral dolomite, and \"rock gypsum\", an aggregate of crystals of the mineral gypsum. Lapis lazuli is a type of rock composed of an aggregate of crystals of many minerals including lazurite, pyrite, phlogopite, calcite, potassium feldspar, wollastonite and some sodalite group minerals. \n\nIn the construction industry, an aggregate (often referred to as a construction aggregate) is sand, gravel or crushed rock that has been mined or quarried for use as a building material.\n\nIn pedology, an aggregate is a mass of soil particles. If the aggregate has formed naturally, it can be called a ped; if formed artificially, it can be called a clod.\n\n\nAggregates are used extensively in the construction industry Often in making concrete, a construction aggregate is used, \nwith about 6 billion tons of concrete produced per year.\n\n\n"}
{"id": "23511391", "url": "https://en.wikipedia.org/wiki?curid=23511391", "title": "Arika Kimura", "text": "Arika Kimura\n\nArika Kimura (in Japanese : 木村有香), (1 March 1900 – 9 August 1996), was a Japanese botanist. He was a professor of botany at the University of Tokyo. A species of spider, \"Heptathela kimurai\", was named in his honour.\n"}
{"id": "57954250", "url": "https://en.wikipedia.org/wiki?curid=57954250", "title": "Atomic trap trace analysis", "text": "Atomic trap trace analysis\n\nAtom Trap Trace Analysis (ATTA) is an extremely sensitive trace analysis method developed by Argonne National Lab (ANL). ATTA is used on long-lived, stable radioisotopes such as , , and . By using a laser that is locked to an atomic transition, a CCD or PMT will detect the laser induced fluorescence to allow highly selective, parts-per-trillion to parts-per-quadrillion concentration measurement with single atom detection. This method is useful for atomic transport processes, such as in the atmosphere, geological dating, as well as noble gas purification.\n\nATTA measurements are possible only if the atoms are excited to a metastable state prior to detection. The main difficulty to accomplishing this is the large energy gap (10-20 eV) between the ground and excited state. The current solution is to use an RF discharge, which is a brute force technique that is inefficient and leads to complications such contamination of the walls from ion sputtering and high gas density. A new scheme for generating a metastable beam which can achieve a cleaner, slower, and preferably more intense source would provide a substantial advance to ATTA technology. All-optical techniques have been considered, but have not yet been able to compete with the discharge source. A new technique for generation of metastable krypton involves the use of a two photon transition driven by a pulsed, far-UV laser to populate the excited state which decays to the metastable state with high probability.\n\n"}
{"id": "2704639", "url": "https://en.wikipedia.org/wiki?curid=2704639", "title": "Awn (botany)", "text": "Awn (botany)\n\nIn botany, an awn is either a hair- or bristle-like appendage on a larger structure, or in the case of the Asteraceae, a stiff needle-like element of the pappus.\n\nAwns are characteristic of various plant families, including Geraniaceae and many grasses (Poaceae). \n\nIn grasses awns typically extend from the lemmas of the florets. This often makes the hairy appearance of the grass synfloresce. Awns may be long (several centimeters) or short, straight or curved, single or multiple per floret. Some genera are named after their awns, such as the three-awns (\"Aristida\").\n\nIn some species, the awns can contribute significantly to photosynthesis, as, for example, in barley.\n\nThe awns of wild emmer wheat spikelets effectively self-cultivate by propelling themselves mechanically into soils. During a period of increased humidity during the night, the awns of the spikelet become erect and draw together, and in the process push the grain into the soil. During the daytime the humidity drops and the awns slacken back again; however, fine silica hairs on the awns act as ratchet hooks in the soil and prevent the spikelets from reversing back out again. During the course of alternating stages of daytime and nighttime humidity, the awns' pumping movements, which resemble a swimming frog kick, drill the spikelet as much as an inch into the soil.\n\nWhen awns occur in the Geraniaceae, they form the distal (rostral) points of the five carpels, lying parallel in the style above the ovary. Depending on the species, such awns have various seed dispersal functions, either dispersing the seed by flinging it out (seed ejection); flinging away the entire carpel so that it snaps off (carpel projection); entangling the awn or bristles on passing animals (zoochory); or possibly burying the seed by twisting as it lies on soft soil.\n"}
{"id": "5325536", "url": "https://en.wikipedia.org/wiki?curid=5325536", "title": "Birth dearth", "text": "Birth dearth\n\nBirth dearth is a neologism referring to falling fertility rates. In the late 1980s, the term was used in the context of American and European society. The use of the term has since been expanded to include many other industrialized nations. It is often cited as a response to overpopulation, but is not incompatible with it. The term was coined by Ben Wattenberg in his 1987 book by that same name.\nCountries and geographic regions that are currently experiencing falling population include Russia, Europe, Japan, and populations of people of these descents in other countries such as in the United States.\n\nRussia is often mentioned in articles concerning birth dearth because of its rapidly declining population, and the proposal by Vladimir Putin to offer women additional benefits for having more children. It is predicted that Russia's population will be an estimated 111 million in 2050, instead to 147 million in 2000 if current trends continue, according to the UN World Population Prospects report (2004 Revision, medium variant).\n\nEurope is one of the few major geographic regions in the World that is expected to \"decline\" in population in the coming years. Europe's population is forecast to decline by nearly 70 million people by 2050, as the total fertility rate has remained perpetually below the replacement rate. (Further information: Sub-replacement fertility and Population decline)\n\n\n"}
{"id": "33893766", "url": "https://en.wikipedia.org/wiki?curid=33893766", "title": "Bulletin of Science, Technology &amp; Society", "text": "Bulletin of Science, Technology &amp; Society\n\nBulletin of Science, Technology & Society is a bimonthly peer-reviewed academic journal that publishes papers in the field of science education. The editor-in-chief is Willem H. Vanderburg (University of Toronto). It was established in 1981 and is currently published by SAGE Publications.\n\n\"Bulletin of Science, Technology & Society\" is abstracted and indexed in:\n"}
{"id": "875176", "url": "https://en.wikipedia.org/wiki?curid=875176", "title": "Colemanite", "text": "Colemanite\n\nColemanite (CaBO·5HO) or (CaBO(OH)·HO) is a found in evaporite deposits of alkaline lacustrine environments. Colemanite is a secondary mineral that forms by alteration of borax and ulexite.\n\nIt was first described in 1884 for an occurrence near Furnace Creek in Death Valley and was named after William Tell Coleman (1824–1893), owner of the mine \"Harmony Borax Works\" where it was first found. At the time, Coleman had alternatively proposed the name \"smithite\" instead after his business associate Francis Marion Smith.\n\nColemanite is an important ore of boron, and was the most important boron ore until the discovery of kernite in 1926. It has many industrial uses, like the manufacturing of heat resistant glass.\n\n\n"}
{"id": "863161", "url": "https://en.wikipedia.org/wiki?curid=863161", "title": "Da Vinci Project", "text": "Da Vinci Project\n\nThe Da Vinci Project was a privately funded, volunteer-staffed attempt to launch a reusable manned suborbital spacecraft. It was formed in 1996 specifically to be a contender for the Ansari X PRIZE for the first non-governmental reusable manned spacecraft. The project was based in Toronto, Ontario, Canada and led by Brian Feeney.\n\nThe original da Vinci Project is no longer operating. A documentary was filmed throughout much of the project's life from 2000 through post-XPRIZE roundup footage in 2008. The documentary accumulated some 1000 hours or so of footage. It was a private undertaking by Michel Jones of Riverstone Productions, Toronto, and as of early 2009 was still in a preliminary stage of editing and completion.\n\nThe project last participated in the X PRIZE Cup 2005, displaying a mock-up of its Wild Fire MK Vl spacecraft.\n\nThe project's design was a rocket-powered spacecraft to be air-launched from a helium balloon at an altitude of about 21 km (65,000 ft). The project scope included design and construction of both the spacecraft and the launching balloon. The chosen design can be described as a manned rockoon.\n\nThe project was established in 1996. It is named after Leonardo da Vinci, who, among innumerable other inventions, was the first recorded person to design an aircraft. The project was staffed entirely by volunteers.\n\nThe project unveiled a mockup of their spacecraft, Wild Fire, on August 5, 2004 at a hangar at Downsview Airport in Toronto. At this point, it was considered a contender for the Ansari X PRIZE, and Tier One had just given notice of their planned competitive flights. When announcing the unveiling, the da Vinci Project also appealed for funds to fly Wild Fire. An agreement was reached with GoldenPalace.com, and the project subsequently gave the required 60-day notice that they would make Ansari X PRIZE competitive flights. GoldenPallace.com, known for its marketing gimicks, will place a soccer ball kicked out of the stadium by David Beckham during the 2004 Euro World Cup inside the space craft.\n\nThe da Vinci Project initially announced that it would fly first on October 2, 2004, launching from Kindersley, Saskatchewan. This was only three days after the first expected X PRIZE flight, by Scaled Composites, on September 29, 2004. However, on September 23, 2004 the da Vinci project announced that they would not be ready. Scaled Composites won the X PRIZE on October 4, 2004.\n\nThe rocket and support equipment was mostly COTS components with a hybrid propulsion system using nitrous oxide and a spin cast paraffin fuel engine in a re-loadable and expendable cardboard cartridge. The most notable development problem was finding a practical low cost solution to the thermal contraction of the liquid paraffin fuel when it cooled and solidified inside the cartridge inner casing.\nThe capsule used two automotive racing seats and aviation BRS parachute systems and was designed and modeled with finite element software. The nozzle was carbon fiber exterior with a tough, thermally insulating inner coating. The combustion chamber was metallic, although a wound carbon fiber exterior was planned but never completed. The planned tracking system used a four car team with networked laptop computers using a hybrid cellular and shortwave radio with the capability of automatically predicting the landing spot so a support team could converge on the landing spot hand prior to landing. The highest forces were predicted to be at re-entry, peaking at approximately at up to 6G. Development, construction and testing continued in earnest until the second flight of the X Prize on October 4th, 2004.\n\nThe project had a small group of core area leaders and relied heavily on volunteer efforts. It followed a variety of business models including share ownership partners, technology partnerships, employee style volunteering, integrator as well as technology/IP aggregator. Many of the expensive components were donated by businesses in exchange for recognition on the website homepage, since removed.\n\n"}
{"id": "2458875", "url": "https://en.wikipedia.org/wiki?curid=2458875", "title": "Data assimilation", "text": "Data assimilation\n\nData assimilation is a mathematical discipline that seeks to optimally combine theory (usually in the form of a numerical model) with observations. There may be a number of different goals sought, for example—to determine the optimal state estimate of a system, to determine initial conditions for a numerical forecast model, to interpolate sparse observation data using (e.g. physical) knowledge of the system being observed, to train numerical model parameters based on observed data. Depending on the goal, different solution methods may be used. Data assimilation is distinguished from other forms of machine learning, image analysis, and statistical methods in that it utilizes a dynamical model of the system being analyzed.\n\nData assimilation initially developed in the field of numerical weather prediction. Numerical weather prediction models are equations describing the dynamical behavior of the atmosphere, typically coded into a computer program. In order to use these models to make forecasts, initial conditions are needed for the model that closely resemble the current state of the atmosphere. Simply inserting point-wise measurements into the numerical models did not provide a satisfactory solution. Real world measurements contain errors both due to the quality of the instrument and how accurately the position of the measurement is known. These errors can cause instabilities in the models that eliminate any level of skill in a forecast. Thus, more sophisticated methods were needed in order to initialize a model using all available data while making sure to maintain stability in the numerical model. Such data typically includes the measurements as well as a previous forecast valid at the same time the measurements are made. If applied iteratively, this process begins to accumulate information from past observations into all subsequent forecasts.\n\nBecause data assimilation developed out of the field of numerical weather prediction, it initially gained popularity amongst the geosciences. In fact, one of the most cited publication in all of the geosciences is an application of data assimilation to reconstruct the observed history of the atmosphere.\n\nClassically, data assimilation has been applied to chaotic dynamical systems that are too difficult to predict using simple extrapolation methods. The cause of this difficulty is that small changes in initial conditions can lead to large changes in prediction accuracy. This is sometimes known as the butterfly effect - the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n\nAt any update time, data assimilation usually takes a forecast (also known as the first guess, or background information) and applies a correction to the forecast based on a set of observed data and estimated errors that are present in both the observations and the forecast itself. The difference between the forecast and the observations at that time is called the departure or the innovation (as it provides new information to the data assimilation process). A weighting factor is applied to the innovation to determine how much of a correction should be made to the forecast based on the new information from the observations. The best estimate of the state of the system based on the correction to the forecast determined by a weighting factor times the innovation is called the analysis. In one dimension, computing the analysis could be as simple as forming a weighted average of a forecasted and observed value. In multiple dimensions the problem becomes more difficult. Much of the work in data assimilation is focused on adequately estimating the appropriate weighting factor based on intricate knowledge of the errors in the system.\n\nThe measurements are usually made of a real-world system, rather than of the model's incomplete representation of that system, and so a special function called the observation operator (usually depicted by \"h()\" for a nonlinear operator or H for its linearization) is needed to map the modeled variable to a form that can be directly compared with the observation.\n\nOne of the common mathematical philosophical perspectives is to view data assimilation as a Bayesian estimation problem. From this perspective, the analysis step is an application of Bayes' theorem and the overall assimilation procedure is an example of recursive Bayesian estimation. However, the probabilistic analysis is usually simplified to a computationally feasible form. Advancing the probability distribution in time would be done exactly in the general case by the Fokker-Planck equation, but that is not feasible for high-dimensional systems, so various approximations operating on simplified representations of the probability distributions are used instead. Often the probability distributions are assumed Gaussian so that they can be represented by their mean and covariance, which gives rise to the Kalman filter.\n\nMany methods represent the probability distributions only by the mean and input some pre-calculated covariance. An example of a direct (or sequential) method to compute this is called optimal statistical interpolation, or simply optimal interpolation (OI). An alternative approach is to iteratively solve a cost function that solves an identical problem. These are called variational methods, such as 3D-Var and 4D-Var. Typical minimization algorithms are the Conjugate gradient method or the Generalized minimal residual method. The Ensemble Kalman filter is sequential method that uses a Monte Carlo approach to estimate both the mean and the covariance of a Gaussian probability distribution by an ensemble of simulations. More recently, hybrid combinations of ensemble approaches and variational methods have become more popular (e.g. they are used for operational forecasts both at the European Centre for Medium-Range Weather Forecasts (ECMWF) and at the NOAA National Centers for Environmental Prediction (NCEP)).\n\nIn numerical weather prediction applications, data assimilation is most widely known as a method for combining observations of meteorological variables such as temperature and atmospheric pressure with prior forecasts in order to initialize numerical forecast models.\n\nThe atmosphere is a fluid. The idea of numerical weather prediction is to sample the state of the fluid at a given time and use the equations of fluid dynamics and thermodynamics to estimate the state of the fluid at some time in the future. The process of entering observation data into the model to generate initial conditions is called \"initialization\". On land, terrain maps available at resolutions down to globally are used to help model atmospheric circulations within regions of rugged topography, in order to better depict features such as downslope winds, mountain waves and related cloudiness that affects incoming solar radiation. The main inputs from country-based weather services are observations from devices (called radiosondes) in weather balloons that measure various atmospheric parameters and transmits them to a fixed receiver, as well as from weather satellites. The World Meteorological Organization acts to standardize the instrumentation, observing practices and timing of these observations worldwide. Stations either report hourly in METAR reports, or every six hours in SYNOP reports. These observations are irregularly spaced, so they are processed by data assimilation and objective analysis methods, which perform quality control and obtain values at locations usable by the model's mathematical algorithms. Some global models use finite differences, in which the world is represented as discrete points on a regularly spaced grid of latitude and longitude; other models use spectral methods that solve for a range of wavelengths. The data are then used in the model as the starting point for a forecast.\nA variety of methods are used to gather observational data for use in numerical models. Sites launch radiosondes in weather balloons which rise through the troposphere and well into the stratosphere. Information from weather satellites is used where traditional data sources are not available. Commerce provides pilot reports along aircraft routes and ship reports along shipping routes. Research projects use reconnaissance aircraft to fly in and around weather systems of interest, such as tropical cyclones. Reconnaissance aircraft are also flown over the open oceans during the cold season into systems which cause significant uncertainty in forecast guidance, or are expected to be of high impact from three to seven days into the future over the downstream continent. Sea ice began to be initialized in forecast models in 1971. Efforts to involve sea surface temperature in model initialization began in 1972 due to its role in modulating weather in higher latitudes of the Pacific.\n\nIn 1922, Lewis Fry Richardson published the first attempt at forecasting the weather numerically. Using a hydrostatic variation of Bjerknes's primitive equations, Richardson produced by hand a 6-hour forecast for the state of the atmosphere over two points in central Europe, taking at least six weeks to do so. His forecast calculated that the change in surface pressure would be , an unrealistic value incorrect by two orders of magnitude. The large error was caused by an imbalance in the pressure and wind velocity fields used as the initial conditions in his analysis, indicating the need for a data assimilation scheme.\n\nOriginally \"subjective analysis\" had been used in which NWP forecasts had been adjusted by meteorologists using their operational expertise. Then \"objective analysis\" (e.g. Cressman algorithm) was introduced for automated data assimilation. These objective methods used simple interpolation approaches, and thus were 3DDA methods.\n\nLater, 4DDA methods, called \"nudging\", were developed, such as in the MM5 model. They are based on the simple idea of Newtonian relaxation (the 2nd axiom of Newton). They introduce into the right part of dynamical equations of the model a term that is proportional to the difference of the calculated meteorological variable and the observed value. This term that has a negative sign keeps the calculated state vector closer to the observations. Nudging can be interpreted as a variant of the Kalman-Bucy filter (a continuous time version of the Kalman filter) with the gain matrix prescribed rather than obtained from covariances.\n\nA major development was achieved by L. Gandin (1963) who introduced the \"statistical interpolation\" (or \"optimal interpolation\") method, which developed earlier ideas of Kolmogorov. This is a 3DDA method and is a type of regression analysis which utilizes information about the spatial distributions of covariance functions of the errors of the \"first guess\" field (previous forecast) and \"true field\". These functions are never known. However, the different approximations were assumed.\n\nThe optimal interpolation algorithm is the reduced version of the Kalman filtering (KF) algorithm and in which the covariance matrices are not calculated from the dynamical equations but are pre-determined in advance.\nAttempts to introduce the KF algorithms as a 4DDA tool for NWP models came later. However, this was (and remains) a difficult task because the full version requires solution of the enormous number of additional equations (~N*N~10**12, where N=Nx*Ny*Nz is the size of the state vector, Nx~100, Ny~100, Nz~100 - the dimensions of the computational grid). To overcome this difficulty, approximate or suboptimal Kalman filters were developed. These include the Ensemble Kalman filter and the Reduced-Rank Kalman filters (RRSQRT).\n\nAnother significant advance in the development of the 4DDA methods was utilizing the optimal control theory (variational approach) in the works of Le Dimet and Talagrand (1986), based on the previous works of G. Marchuk, who was the first to apply that theory in the environmental modeling. The significant advantage of the variational approaches is that the meteorological fields satisfy the dynamical equations of the NWP model and at the same time they minimize the functional, characterizing their difference from observations. Thus, the problem of constrained minimization is solved. The 3DDA variational methods were developed for the first time by Sasaki (1958).\n\nAs was shown by Lorenc (1986), all the above-mentioned 4DDA methods are in some limit equivalent, i.e. under some assumptions they minimize the same cost function. However, in practical applications these assumptions are never fulfilled, the different methods perform differently and generally it is not clear what approach (Kalman filtering or variational) is better. The fundamental questions also arise in application of the advanced DA techniques such as convergence of the computational method to the global minimum of the functional to be minimised. For instance, cost function or the set in which the solution is sought can be not convex. The 4DDA method which is currently most successful is hybrid incremental 4D-Var, where an ensemble is used to augment the climatological background error covariances at the start of the data assimilation time window, but the background error covariances are evolved during the time window by a simplified version of the NWP forecast model. This data assimilation method is used operationally at forecast centres such as the Met Office.\n\nThe process of creating the analysis in data assimilation often involves minimization of a cost function. A typical cost function would be the sum of the squared deviations of the analysis values from the observations weighted by the accuracy of the observations, plus the sum of the squared deviations of the forecast fields and the analyzed fields weighted by the accuracy of the forecast. This has the effect of making sure that the analysis does not drift too far away from observations and forecasts that are known to usually be reliable.\n\nformula_1\n\nwhere formula_2 denotes the background error covariance, formula_3 the observational error covariance.\n\nformula_4\n\nformula_5\n\nprovided that formula_6 is a linear operator (matrix).\n\nFactors driving the rapid development of data assimilation methods for NWP models include:\n\n\nData assimilation has been used, in the 1980s and 1990s, in several HAPEX (Hydrologic and Atmospheric Pilot Experiment) project for monitoring energy transfers between the soil, vegetation and atmosphere. For instance:\nHAPEX-MobilHy, HAPEX-Sahel, the \"Alpilles-ReSeDA\" (Remote Sensing Data Assimilation) experiment, which took place in the Alpilles region, South-East of France (1996–97).\n\nThe Flow chart diagram (right) shows how to infer variables of interest such as canopy state, fluxes, environmental budget, production in quantity and quality from remote sensing data and ancillary information. The small green arrows indicate the direct way the models actually run.\n\nData assimilation methods are currently also used in other environmental forecasting problems, e.g. in hydrological forecasting. Basically, the same types of data assimilation methods as those described above are in use there. An example of chemical data assimilation using Autochem can be found at CDACentral.\n\nGiven the abundance of spacecraft data for other planets in the solar system, data assimilation is now also applied beyond the Earth to obtain re-analyses of the atmospheric state of extraterrestrial planets. Mars is the only extraterrestrial planet to which data assimilation has been applied so far. Available spacecraft data include, in particular, retrievals of temperature and dust/water ice optical ticknesses from the Thermal Emission Spectrometer onboard NASA's Mars Global Surveyor and the Mars Climate Sounder onboard NASA's Mars Reconnaissance Orbiter. Two methods of data assimilation have been applied to these datasets: an Analysis Correction scheme and two Ensemble Kalman Filter schemes, both using a global circulation model of the martian atmosphere as forward model. The Mars Analysis Correction Data Assimilation (MACDA) dataset is publicly available from the British Atmospheric Data Centre.\n\nData assimilation is a part of the challenge for every forecasting problem.\n\nDealing with biased data is a serious challenge in data assimilation. Further development of methods to deal with biases will be of particular use. If there are several instruments observing the same variable then intercomparing them using probability distribution functions can be instructive.\n\nThe numerical forecast models are becoming of higher resolution due to the increase of computational power, with operational atmospheric models now running with horizontal resolutions of order of 1 km (e.g. at the German National Meteorological Service, the Deutscher Wetterdienst (DWD) and Met Office in the UK). This increase in horizontal resolutions is starting to allow us to resolve more chaotic features of our non-linear models, e.g. resolve convection on the grid scale, clouds, in the atmospheric models. This increasing non-linearity in the models and observation operators poses a new problem in the data assimilation. The existing data assimilation methods such as many variants of ensemble Kalman filters and variational methods, well established with linear or near-linear models, are being assessed on non-linear models, as well as many new methods are being developed e.g. particle filters for high-dimensional problems, hybrids data assimilation methods.\n\nOther uses include trajectory estimation for the Apollo program, GPS, and atmospheric chemistry.\n\n\nExamples of how variational assimilation is implemented weather forecasting at:\n\nOther examples of assimilation:\n"}
{"id": "47616558", "url": "https://en.wikipedia.org/wiki?curid=47616558", "title": "Department of Chemistry, University of Oxford", "text": "Department of Chemistry, University of Oxford\n\nThe Department of Chemistry is the chemistry department of the University of Oxford, England, which is part of the university's Mathematical, Physical and Life Sciences Division \n\nThe department has several laboratories in the Science Area at Oxford:\n\n\n\n\n\nChemistry has a long history at Oxford University. The early pioneer of chemistry Robert Boyle and his assistant Robert Hooke began working in Oxford in the mid-seventeenth century. A chemistry laboratory was built in the basement of the Old Ashmolean Building in 1683, which was used until 1860. Chemical research was also conducted in laboratories set up in individual colleges – Christ Church (1767), Magdalen (Daubeny Laboratory, 1848), Balliol (1853, later joined with Trinity to become the Balliol-Trinity Laboratories), Queen's (1900), and Jesus (1907).\n\nChemistry was first recognized as a separate discipline at Oxford with the building of a laboratory attached to the Oxford University Museum of Natural History, opening in 1860. The laboratory is a small octagonal structure to the right of the museum, built in stone in the Victorian Gothic style. The design was based on the Abbot's Kitchen at Glastonbury and it adopted the same name despite being a laboratory. The building was one of the first ever purpose-built chemical laboratories anywhere and was extended in 1878.\n\n\n"}
{"id": "53727277", "url": "https://en.wikipedia.org/wiki?curid=53727277", "title": "Douglas Vakoch", "text": "Douglas Vakoch\n\nDouglas Vakoch (born June 16, 1961) is an American search for extraterrestrial intelligence (SETI) researcher, psychologist, and president of METI International (Messaging Extraterrestrial Intelligence), a nonprofit research and educational organization devoted to transmitting intentional signals to extraterrestrial civilizations. Vakoch led METI's participation in Sónar Calling GJ 273b, which transmitted a series of interstellar messages to Luyten's Star, located 12.4 light years from Earth. Vakoch advocates ongoing transmission projects, arguing that this does not increase risks of an alien invasion as suggested by British cosmologist Stephen Hawking. He has participated in several SETI observation programs, and after sixteen years at the SETI Institute, where he was director of Interstellar Message Composition, Vakoch founded METI International. He has edited over a dozen books in SETI, astrobiology, the psychology of space exploration, and ecocriticism. He is general editor of two book series in ecocriticism and in the intersection of space and society. Vakoch has appeared widely on television and radio as a commentator on SETI and astrobiology. He is an emeritus professor of clinical psychology at the California Institute of Integral Studies (CIIS).\n\nDouglas Vakoch grew up in rural Minnesota. He created his first interstellar message as a high school student—a series of two-dimensional pictures that built upon a message transmitted from Arecibo Observatory in 1974. \"The issue that really hit me early on, and that has stayed with me, is just the challenge of creating a message that would be understandable,\" he told \"The New York Times Magazine\". Vakoch earned a bachelor's degree in comparative religion from Carleton College, a master's degree in history and philosophy of science from the University of Notre Dame, and a PhD in psychology from Stony Brook University. He completed a postdoctoral fellowship at Vanderbilt University before he accepted a position at the SETI Institute in Mountain View, California.\n\nVakoch argues that in order to make contact, humankind may need to take the initiative in transmitting, a project called active SETI. He has been called \"a prominent voice in favor of active SETI,\" \"the most prominent METI [messaging to extraterrestrial intelligence] proponent,\" and \"the man who speaks for Earth.\" In \"Discover\"'s ranking of scientists either in favor of or opposed to transmitting, Vakoch was cited as \"super pro,\" at the extreme of those advocating messaging. After sixteen years at the SETI Institute, where he was director of Interstellar Message Composition, Vakoch founded METI (Messaging Extraterrestrial Intelligence), a nonprofit research and educational organization.\n\nVakoch led METI's participation in Sónar Calling GJ 273b, which transmitted a series of interstellar messages to Luyten's Star, located 12.4 light years from Earth. The project was initiated by the Sónar Festival to celebrate its 25th anniversary, and METI was invited to collaborate along with the Institute of Space Studies of Catalonia. METI created a scientific and mathematical tutorial that was transmitted at two radio frequencies on October 16, 17, and 18, 2017, from the 32-meter radio antenna in Tromsø, Norway, operated by the European Incoherent Scatter Scientific Association (EISCAT). This tutorial was transmitted at 125 bits per second, using EISCAT's 930 MHz transmitter. The duration of METI's tutorial message was 11 minutes on each of the three transmission dates in October 2017, for a total of 33 minutes. Within each 11-minute transmission, the tutorial was sent three times. The only element of the tutorial that changed with each repetition was the time indicated on a \"cosmic clock.\" A novel feature of this clock is that it explains the passage of time by making reference to the duration of radio pulses themselves, meant to increase its intelligibility. Luyten's Star was chosen as the target of the transmission because it is the nearest star visible from the EISCAT facility that is known to be orbited by an exoplanet in the star's habitable zone. The transmission from Tromsø was announced on November 16, 2017, on the 43rd anniversary of the transmission of the Arecibo message.\n\nVakoch said that METI's tutorial is \"distinctive because it's designed with extraterrestrial SETI scientists in mind. We sent the sort of signal we'd want to receive here on Earth.\" The repetition of the tutorial three times each day, which was repeated across three days, provides extraterrestrial scientists with an opportunity to conduct follow-up observations to confirm the signal. \"This sort of confirmation is essential to having a credible SETI signal,\" Vakoch told \"CNET\". \"The last thing we want to do is send the aliens a Wow! Signal that's seen only once, but never replicated.\"\n\nVakoch noted that many past interstellar messages attempted to be encyclopedic, with the risk that by attempting to explain everything, the message may be unintelligible. As an alternative, METI's message emphasized a few key scientific and mathematical concepts, starting with basic arithmetic, introducing triangles, and from there describing sine waves, allowing them to make reference to the radio signals used to transmit the message. If extraterrestrials can detect METI's message, that means they have a radio receiver, which in turn requires a knowledge of some fundamental mathematics. \"You’re not going to be a very good engineer, on Earth or GJ 273b, if you don’t know that one plus one equals two,\" Vakoch told \"Forbes\".\n\nGiven the distance between Earth and GJ 273b, a response could be received in less than 25 years. \"I think that’s an unlikely outcome,\" Vakoch told \"New Scientist\", \"but it would be a welcome outcome.\" If a response comes, however, that means \"the galaxy is chock-full of alien life,\" he told \"Forbes\". Instead Vakoch suggested the process would need likely need to be replicated with many additional targets to receive a response. \"It is a prototype for what I think we would most likely need to do 100 times, or 1,000 times, or 1 million times,\" he told \"Space.com\". \"To me, the big success of the project will come if, 25 years from now, there's someone who remembers to look [for a response]. If we could accomplish that, that would be a radical shift of perspective.\" METI plans to repeat this process with other stars.\n\nVakoch says that intentional transmissions to GJ 273b do not increase the risk of an alien invasion, contrary to the concerns of Stephen Hawking, telling \"Space.com\" that \"[i]t's really hard to imagine a scenario in which a civilization around Luyten's star could have the capacity to come to Earth and threaten us, and yet they're not able to pick up our leakage radiation.\" He added that evidence of microbial life on Earth has been detectable even longer: \"Any civilization that is capable of an alien invasion is already privy to our existence. Earth's atmosphere has been giving off evidence of the existence of life for two and a half billion years, by virtue of the oxygen in our atmosphere, so any paranoid aliens have had plenty of time to do us harm. There's no sign they've been here.\" \n\nWhen asked whether he thought the announcement of Sónar Calling GJ 273b would upset some within the SETI community, Vakoch told \"Newsweek\" that \"[e]veryone engaged in SETI is already endorsing transmissions to extraterrestrials through their actions. If we detect a signal from aliens through a SETI program, there’s no way to prevent a cacophony of responses from Earth... Once the news gets out that we’ve detected extraterrestrials, anyone with a transmitter can say whatever they want.\" Vakoch says that some in the SETI community have criticized Active SETI as a viable strategy precisely because of the challenges of undertaking long-term projects. Vakoch noted that efforts like Sónar Calling GJ 273b are not replacements for traditional passive SETI projects, but rather provide a complementary strategy.\n\nVakoch has participated in public discussions about interstellar communication, arguing in favor of initiating transmission projects. This included a debate hosted by the American Association for the Advancement of Science (AAAS) at its annual convention, held in San Jose, California on February 13, 2015 . \"[W]e should expand our strategies, so we are not only passively listening, but also transmitting intentional, information-rich signals,\" Vakoch said at the AAAS meeting, adding that \"[w]ith recent detections of Earth-like planets in the habitable zones of other stars, we have natural targets for such transmission projects.\" Vakoch argued that transmitting intentional signals does not increase the risk of an alien invasion, contrary to concerns raised by British cosmologist Stephen Hawking, because \"[a]ny civilization that has the ability to travel between the stars can already pick up our accidental radio and TV leakage.” At the same AAAS meeting, astrophysicist and science fiction writer David Brin argued against this \"barn door excuse\" and Brin contended that there should be no transmissions without international discussion. Vakoch and Brin had a second debate at the annual ideacity conference in 2016, held in Toronto. Vakoch also questions the logic of extraterrestrials traversing interstellar space to secure resources from Earth. Vakoch told the \"Associated Press\" that active SETI is an “attempt to join the galactic club,” and he argues that \"it’s a reflection of the natural growth that you see in the science\" and \"a reflection of SETI growing up as a discipline.\"\n\nVakoch has argued for international consultation about transmission, and he suggests avoiding “either-or” thinking by continuing international discussions even after beginning to transmit. In an October 2015 letter in \"Nature Physics\", he advocated the use of scientific peer review to determine whether to schedule time for transmission projects at publicly supported observatories, a process that American astrobiologist Dirk Schulz-Makuch and \"Centauri Dreams\"'s Paul Gilster argue is inadequate.\n\n“It may be that signaling of our intention to make contact is what’s really required to trigger a response,” Vakoch told \"Business Insider\", adding that \"the most critical reason to add Active SETI to our search strategy is that this may be the strategy that lets us make contact.\" He suggests that this initiative from humankind may be a prerequisite for making contact. \"We've always assumed that if the other civilization has the ability to communicate, they'll take on the burden of contacting us,\" he said to \"Monitor on Psychology\", adding \"[t]hat's not at all obvious to me.\" Instead, he suggests that advanced extraterrestrials may be akin to “hyperintelligent cats—they know we’re here, but they just don’t care,” and the goal of sending messages is to intrigue extraterrestrials enough to respond. Vakoch suggests that sending intentional signals can test the Zoo Hypothesis, which assumes that extraterrestrial intelligence may be monitoring Earth, but they are waiting for a clear indication that humans wish to communicate. He notes that if extraterrestrial civilizations are far from Earth, an exchange of messages could be a communication across generations. Vakoch says that the financing of interstellar transmissions requires supporters who take a long-term perspective.\n\nVakoch advocates initial active SETI projects that make use of Arecibo Observatory in Puerto Rico, during gaps in the schedule for planetary radar studies of recently discovered asteroids. He proposes targeting stars located within 25 parsecs of Earth, and he advocates transmitting repeatedly to \"a set of nearby stars over the course of several months or years.” Vakoch participated in radar studies of the near-Earth asteroid 2015 HM10 using the Green Bank Telescope in West Virginia in conjunction with the NASA Deep Space Network.\n\nWhen astronomers announced in August 2016 the discovery of a potentially Earth-like planet in the habitable zone of the star nearest our solar system, Proxima Centauri, Vakoch emphasized that if inhabited, the star's close proximity to Earth would allow a round trip exchange of messages with extraterrestrials in less than nine years, telling \"CNET\" \"[w]e could have several back-and-forth exchanges with any civilizations there over the course of a human lifetime.\" “[W]e could finally have something like a real conversation with an alien, with the usual give-and-take that happens when we meet a stranger,” Vakoch told \"WIRED\", adding “In less than a decade, we could send a message and receive a reply from curious Centaurians.” Vakoch responded to earlier theories that planets orbiting red dwarf stars like Proxima Centauri would not be suitable to sustain life because they would be tidally locked, with one side of the planet always facing the star and the other side facing away, meaning that \"[o]ne side of the planet would be scorched, while the other side would be perpetually frozen.\" Vakoch argued that \"new models of exoplanet atmospheres and oceans suggest that heat might be distributed around a tidally locked planet, leaving the door open for habitability. Even if the newly discovered exoplanet around Proxima Centauri is tidally locked, it could still be prime real estate for life.\" Nevertheless, Vakoch noted challenges to the habitability of this planet, saying \"[t]he biggest downside of Proxima Centauri for alien hunters is that it's a flare star, dramatically and unpredictably varying in brightness over the course of a few minutes due to its magnetic activity.\"\n\nVakoch made similar comments about the potential habitability of exoplanets orbiting another red dwarf star, TRAPPIST-1, noting \"[b]ecause these planets have an orbit so close to the TRAPPIST-1 star, they are thought to be subject to the phenomenon of synchronized rotation.\" He added that heat might be transferred on tidally locked planets, drawing a parallel to Earth: \"[w]hen the Sun sets at night on the Golden Gate Bridge, I do not worry that San Francisco Bay will freeze.\"\n\nVakoch's edited book \"Communication with Extraterrestrial Intelligence (CETI)\", based on papers presented at sessions he chaired at the 2010 NASA. Astrobiology Science Conference (AbSciCon), includes a section devoted to Active SETI.\n\nIn 2010, Vakoch was one of the leaders of Project Dorothy, a multinational effort launched by Japanese astronomer Shin-ya Narusawa to observe several stars for signals from other civilizations to commemorate the fiftieth anniversary of Project Ozma, the first modern-day search for extraterrestrial intelligence (SETI). Telling \"The Washington Post\" about the Project Dorothy observations, Vakoch said \"[w]hat this weekend really does is begin the process of making it possible to track a possible SETI signal around the globe,\" and he added \"[i]f a signal is detected, it has to be confirmed and followed, and now we're setting up a network to do that.\"\n\nVakoch participated in the earliest SETI observations of the anomalous star KIC 8452852, also known as Tabby's Star, which some astronomers have hypothesized may be orbited by an alien megastructure. The observations were conducted at optical frequencies at METI's Boquete Optical SETI Observatory in Panama, and also at radio frequencies at the SETI Institute’s Allen Telescope Array in the United States. No indications of artificial signals were detected at either facility: \"The hypothesis of an alien megastructure around KIC 8462852 is rapidly crumbling apart,\" he said, adding \"[w]e found no evidence of an advanced civilization beaming intentional laser signals toward Earth.” Vakoch argued that the name Tabby's Star is sexist because stars named after male astronomers have not used their first names. Instead, Vakoch suggests that KIC 8452852 should be referred to as Boyajian's Star, in recognition of Tabetha S. Boyajian, who led the team discovering the anomalous dimming.\n\nWhen a candidate SETI signal from the direction of the star HD 164595 was first announced by Russian astronomers, Vakoch told \"CNN\" that \"[t]he signal from HD 164595 is intriguing, because it comes from the vicinity of a sun-like star, and if it's artificial, its strength is great enough that it was clearly made by a civilization with capabilities beyond those of humankind.\" Given the strength of the signal detected from Russia, Vakoch said we could determine how advanced the transmutting civilization is if the signal is artificial, using the Kardashev Scale that hypothesizes Type I, Type II, and Type III civilizations: \"If the signal from HD 164595 is from a civilization that is radiating out electromagnetic signals in all directions, that takes tremendous energy -- the energy of an entire star, represented by a Type II civilization,\" Vakoch told \"CNET\". “If it’s focused at Earth, then the civilization doesn’t need to have quite that great of a capability. It could be a Type I,” Vakoch said. \"Humanity is currently somewhere between Type 0 and Type I,\" he explained. Vakoch said that \"[i]n the past, plans for SETI follow-up observations have focused on confirmation of the original signal, seeking a repeat signal at the same frequency. That’s a critical step for confirmation – and we don’t yet have evidence that this sort of follow-up has happened for HD 164595.\" Vakoch said that METI's optical SETI observatory in Panama would be able observe HD 164595 \"about an hour shortly after sunset each night,” weather permitting, telling \"CNN\" that they would be \"searching for any brief laser pulses that might be sent as a beacon from advanced extraterrestrials.\" Because the Panama observatory is designed to avoid false positives, “[i]f we get a signal there, that’s a really strong sign we’ve really discovered extraterrestrial life,” Vakoch said, adding “[n]ow, we don’t expect to find that, but we’re going to do our due diligence. This is the sort of thing that, internally, we do all the time.” While the original signal from the direction of HD 164595 was detected at radio frequencies, Vakoch argued for expanding the search to other frequencies: \"if this were really a signal from extraterrestrials, we'd want to survey the target star across as much of the electromagnetic spectrum as we could.\"\n\nVakoch was not optimistic about detecting a signal from HD 164595 but he said that follow-up observations help prepare for detecting a real signal: “I think the likely outcome of this is that there’s no indication that it’s ET, but it provides a critical preparation for a day we may really discover intelligence out there,” he told \"New Scientist\". Vakoch argues that replication of a putative SETI signal is essential for confirmation, and the lack of such replication means that past signals such as the Wow! signal have little credibility. \"Without corroboration from an independent observatory, a putative signal from extraterrestrials doesn't have a lot of credibility,\" he told \"CNN\". He urged that HD 164595 should not be treated as a plausible candidate without further confirmation: “Assuming we don’t find any evidence of a transmitting civilization as we conduct follow-up observations, the worst outcome would be to turn HD 164595 into another Wow signal – seen once, never confirmed, but lurking in the imagination as perhaps really a message from another world. Unless we can observe another similar signal from the vicinity of this star, we need to dismiss the May 2015 signal as a spurious result, and not wishfully hope it was really from ET.”\n\nWhen two Canadian astronomers argued that they potentially discovered 234 extraterrestrial civilizations through analysis of the Sloan Digital Sky Survey database, Vakoch doubted their explanation for their findings, noting that it would be unusual for all of these stars to pulse at exactly the same frequency unless they were part a coordinated network: “If you take a step back,” he said, “that would mean you have 234 independent stars that all decided to transmit the same way.” “The general mindset in SETI is that, before you say it’s an extraterrestrial intelligence, you have to think really creatively about what the natural explanations might be—and I think it’s way too early in the game to jump to the conclusion that it’s extraterrestrial intelligence,” Vakoch said.\n\nWhen astronomers announced in January 2017 that the fast radio burst FRB 121102 comes from a dwarf galaxy almost three billion light years from Earth, Vakoch said that METI was using the Boquete Optical SETI Observatory in Panama to search for brief laser pulses from the same target. \"I'm not holding my breath, and I'm not expecting to find any evidence of ET, but the follow-up is straightforward, so we'll make the observations out of due diligence,\" he told \"CNET\", adding \"[s]o far we've found nothing that looks like the telltale sign of extraterrestrial technology.\"\n\nAfter the star Wolf 1061 was announced to be orbited by three \"super-Earths,\" including Wolf 1061c that lies in the star's habitable zone, Vakoch reported that METI had on four separate occasions observed the star for brief laser pulses from its optical SETI observatory in Panama. \"[T]he fact that there’s a roughly Earth-like planet in the habitable zone of a star so close to our own solar system is a good omen as we continue our search for life on other planets,” he told \"Gizmodo\". \"So far, we've found no indications of advanced technologies in this promising exoplanet just 14 light-years from Earth,\" he said.\n\nWhen unusual radio signals were reported in the vicinity of the red dwarf Ross 128, Vakoch noted that this star had already been searched for brief laser pulses five times in the past from METI's optical SETI observatory in Panama, with no evidence of extraterrestrial technology detected. He observed that the signals detected from Ross 128 did not fit the expected profile of a radio signal from extraterrestrials, and the signals had not repeated, which is further reason not to treat these signals as credible evidence of extraterrestrial intelligence.\n\nWhen China's Five-hundred-meter Aperture Spherical Telescope (FAST) went online, Vakoch told \"CNN\" that \"China's latest telescope will be able to look faster and further than past searches for extraterrestrial intelligence.\" He told \"The Telegraph\" that FAST is a “game-changer in the search for life in the universe\". \"FAST's innovative design and huge collecting area give it unsurpassed speed and sensitivity, making it vital to the search for extraterrestrial intelligence in the coming decades,\" Vakoch told \"Xinhua\", adding \"[w]e can expect China to become a world leader in the search for extraterrestrial intelligence because of its demonstrated commitment in building FAST.\" The telescope will study the distribution of hydrogen to understand the origin of the universe, and Vakoch says that \"[b]ecause of FAST's incredible sensitivity, it will be able to chart the hydrogen distribution even in far flung galaxies.\" He expected that FAST would lead to \"a dramatic increase in the number and variety of pulsars discovered.\" Vakoch also noted the limitations of the telescope, saying \"FAST may help explain the origin of the universe and the structure of the cosmos, but it won't provide warning of Earth-bound asteroids that could destroy human civilization.\"\n\nWhen plans were announced to build the Qitai Radio Telescope (QTT), which would be the largest steerable radio telescope in the world, Vakoch noted its ability to pinpoint stars that are not accessible to FAST. He noted the QTT could support the search for extraterrestrial life by scanning interstellar space for complex organic molecules, as well as searching for narrowband techno-signatures indicative of extraterrestrial intelligence. Because FAST and the QTT can both search within the \"water hole\" that SETI scientists have long preferred, Vakoch told \"CNET\" that each observatory could confirm interesting signals detected by the other observatory within this frequency range.\n\nVakoch suggests that the detection of extraterrestrials in a standard SETI scenario may be less clear-cut than usually assumed: \"I think the assumption that one day someone is going to announce that we’ve discovered extraterrestrial intelligence, and now the world knows, is a fallacy, because there’s going to be much more ambiguity in the process.\" “Unlike Hollywood movies, where you get a quick 'yes or no' about a possible signal from aliens,” Vakoch told \"Universe Today\", “the real SETI confirmation process takes some time. It’s easy to think that all we need to do is get on the phone with an astronomer at another location, and we’re all set. But even when colleagues at other facilities are willing to observe, they may face technical limitations.” He also assumes it may take some time to decode any message: \"I don't think we're going to understand immediately what they have to say,\" he told \"ABC News.\" “There’s going to be a lot of guesswork in trying to interpret another civilization,\" he told \"Science Friday\", adding that \"[i]n some ways, any message we get from an extraterrestrial will be like a cosmic Rorschach ink blot test.”\n\nVakoch contends that it is essential to expand an understanding of SETI beyond the technology needed to search by also re-examining assumptions about the nature of intelligence, which was the motivation for the METI workshop, “The Intelligence of SETI: Cognition and Communication in Extraterrestrial Intelligence,” held in San Juan, Puerto Rico on May 18, 2016. \"By studying the variety of intelligence found on Earth,\" Vakoch said, \"we can gain new insights into sending messages to life on other planets.\" He called for \"rethinking what SETI means,\" saying \"what we haven't caught up with is the real understanding of intelligence.\" Vakoch told the International Business Times that \"[i]n this new approach, we're putting the intelligence back into SETI.\" He argues that the fact that extraterrestrial intelligence may rely on different senses than humans adds to the complexity of interspecies communication.\n\nVakoch \"leads an international group of scientists, artists and scholars from the humanities, as they ponder how we could communicate what it’s like to be human across the vast distances of interstellar space.\" He advocates creating interstellar messages that begin with concepts shared by humans and extraterrestrials, such as basic mathematics and science and building on these shared concepts to express content that may be distinctly human. He argues that while mathematics and science provide the best starting point for interstellar messages, it is possible that extraterrestrial mathematics and science may vary significantly from human mathematics and science. He notes that on Earth both Euclidean and non-Euclidean geometries provide internally consistent frameworks for understanding the world, but they vary in their foundational assumptions. Vakoch provided an overview of interstellar message design at the 2012 TEDx Nashville.\n\nIn contrast to the images included on the Voyager Golden Record that emphasized the positive aspects of life on Earth, Vakoch proposes that we should be honest about human frailties. He suggests that the most informative things that humankind can convey to an advanced civilization are the struggles humankind is going through as an adolescent technological civilization. Vakoch argues that if we contact other civilizations, they will likely be thousands or millions of years older than humanity's civilization, meaning the extraterrestrial civilization would have greater stability. \"Perhaps it is not the beauty of our symphonies that will set us apart from extraterrestrials, nor our moral perfection – living true to our ideals of altruism. If we wish to convey what it is about us that is distinctive, it may be our weakness, our fears, our unknowing – and yet a willingness to forge ahead to attempt contact in spite of this,\" Vakoch told \"The Psychologist\".\n\nVakoch calls for increasing the range of people participating in interstellar message design, and he led a workshop in Paris in 2002 on the interface of art and science in interstellar messages. Speaking to \"Reuters\" on the day of the meeting, he said \"Today the focus has been on whether we can explain something about our aesthetic sensibilities. Is there something about art that is either universal or that can be taught, step by step, to another intelligence?\"\n\nHe chaired a follow-up meeting in Paris in 2003 called \"Encoding Altruism,\" focusing on communicating altruism in interstellar messages. Vakoch stressed that the goal of the workshop was not to present an image of humanity as unambiguously altruistic. \"Previous attempts at communication, such as on the Voyager space probe, presented the positive side of human beings,\" Vakoch told \"Nature\", adding \"There's been no poverty, or war, or the nuclear mushroom cloud. We're trying to start a tough dialogue about how we describe the breadth of human experience.\"\n\nVakoch also led meetings attended by anthropologists and sociologists, and he advocates interstellar messages that capture the diversity of human cultures.\n\nIn 2009 Vakoch launched an internet-based project called \"Earth Speaks\" to collect messages from people around the world that they would want to send to extraterrestrial intelligence. \"One of the strongest themes we see in ‘Earth Speaks’ is a concern with our current environmental crisis,\" Vakoch said. Among the messages gathered in the \"Earth Speaks\" project, \"the loudest message is people asking for help,\" Vakoch told \"The New York Times\". \"We gathered messages from people in over 80 countries around the world, and really sort of got the pulse of what people think we should say to other civilizations. Sometimes it maps on to what scientists are saying, suggesting we should start with math and science, and a lot of the greetings are just basic greetings of 'Hello, from the people of planet Earth,'\" Vakoch told the \"Observer\", adding that \"[o]ne of the most interesting findings are that sometimes people have said if we find another civilization beyond Earth, it will diminish the differences we see between cultures and ethnicities here on Earth, because they’ll be so minuscule compared to the differences between ourselves and extraterrestrials. The exercise of preparing for a reply seems to be furthering that process of seeing our commonalities.\" Vakoch said that \"Earth Speaks\" messages differed from greetings in the Voyager Golden Record insofar as the \"Earth Speaks\" messages included negative depictions of life on Earth. The \"Earth Speaks\" project was also expanded to Spanish-speaking respondents as \"La Tierra Habla\".\n\nVakoch suggests that even if we never make contact with extraterrestrial intelligence, the process of creating interstellar messages is valuable, for example, by encouraging people to reflect on what they most care about.,\n\nHe judges the proposal to send extraterrestrials a \"digital data dump\" of the full contents of the internet as \"ugly,\" though he says there is no harm in doing this unless interstellar communication is a form of commerce, in which case we may shortchange future generations of humans by giving away everything from the outset of an interstellar exchange. In contrast, Vakoch advocates sending interstellar messages that convey aesthetic concepts. His own interstellar messages have been exhibited at the Chabot Space and Science Center and at the Maison Européenne de la Photographie, the latter in his @rt outsiders 2003 exhibit on interstellar messages about altruism titled \"Le sacrifice de soi.\" When asked by \"Science\" magazine what message he would want to send to extraterrestrials, Vakoch responded, \"I would want to ask an extraterrestrial: What do you care about? What makes you happy?\"\n\nVakoch argues for the importance of identifying analogues for making contact with extraterrestrials, because humans do not have direct access to extraterrestrial civilizations in advance of contact. He suggests that the challenges of decoding the Rosetta Stone may provide insights into the challenges facing SETI scientists if they detect an information-rich signal from another civilization.\n\nVakoch and Yuh-shiow Lee conducted a survey to assess people's reactions to receiving a message from extraterrestrials, including their judgments about likelihood that extraterrestrials would be malevolent. \"Chinese participants were able to imagine contact would lead to both risks and benefits,” Vakoch told \"The Washington Post\", while Americans imagined the discovery of extraterrestrials would be \"all good or all bad, but not both.\" \"People who view the world as a hostile place are more likely to think extraterrestrials will be hostile,\" Vakoch told \"USA Today\".\n\nVakoch says the majority of people already believe in the existence of extraterrestrial intelligence. Vakoch argues that the discovery of extraterrestrial intelligence would not diminish the uniqueness of humankind. Vakoch suggests that learning about an extraterrestrial civilization would increase people's self-understanding. He argues that it is not likely that the discovery of extraterrestrial life will impact religious beliefs, and he doubts that humans would be inclined to adopt extraterrestrial religions, telling \"ABC News\" \"I think religion meets very human needs, and unless extraterrestrials can provide a replacement for it, I don't think religion is going to go away,\" and he added, \"[i]f there are incredibly advanced civilizations with a belief in God, I don't think Richard Dawkins will start believing.\" Citing a survey by theologian Ted Peters that showed people were likely to think the discovery of extraterrestrial life would challenge the religious beliefs of other people, but their own beliefs would remain intact, Vakoch told \"The Washington Post\" that \"[i]t looks like we don’t need to be worried about others not being able to handle an announcement of extraterrestrial life... They’ll do just fine.\"\n\nVakoch has edited a number of books that examine topics in SETI and astrobiology from multiple disciplinary perspectives, and these books have been reviewed in a number of journals. The review journal \"Choice\" described Vakoch's \"Extraterrestrial Altruism: Evolution and Ethics in the Cosmos\" as \"a fascinating speculation into the human condition and what makes us unique or perhaps not unique among all the (hypothetical) intelligences in the universe\" and noted that the \"book gives a broad perspective on the human condition and will be enjoyable to readers with a wide range of interests,\" highly recommending it for \"[a]ll academic, general, and professional readers.\" The same journal said about Vakoch and Matthew F. Dowd's \"The Drake Equation: Estimating the Prevalence of Life Through the Ages\" that \"[m]ost chapters are accessible to general readers, while some are at the advanced undergraduate or graduate level. Useful as a novel approach to a popular subject\" and recommended the book for \"[a]ll library collections.\" Vakoch and Albert A. Harrison's \"Civilizations Beyond Earth: Extraterrestrial Life and Society\" was recommended for \"[a]ll readers\" by \"Choice\". Vakoch's \"Archaeology, Anthropology, and Interstellar Communication\", published by NASA, was selected by \"Library Journal\" as a \"notable government document\" of 2014, saying \"[w]hile this is serious scholarship, general readers will appreciate the accessible writing.\" \"DttP, Documents to the People\" wrote that \"[t]hese authors have tackled a somewhat controversial subject in a very serious and non-condescending manner, which will be much appreciated by the reader. \"Archaeology, Anthropology, and Interstellar Communication\" is highly recommended for anyone interested in learning about the human quest to communicate with alien civilizations.\" The same book was cited as an example of budgetary waste by Senator Tom Coburn. \"Archaeology, Anthropology, and Interstellar Communication\" included a passage that several news stories took out of context to suggest that NASA had found evidence of extraterrestrial visitation in rock art.\n\n\"Are we alone in the universe? If not, then what might that mean?\" begins the \"Journal for the History of Astronomy\"'s review of Vakoch's \"Astrobiology, History, and Society\" and continues by saying \"[t]his fascinating volume offers a history of what Western cultures have thought about these questions, a sampling of current work by scientists in astrobiology, and a group of probing essays on how human societies might respond if/when first contact with extraterrestrial life (ETL) or intelligence (ETI) would occur. It is useful to have all this in a single volume - a useful source for scientists, historians, anthropologists, and many other disciplines that concern themselves with these two large questions.\"\n\nVakoch serves as the general editor of the Space and Society series, a scholarly book series published by Springer.\n\nVakoch has contributed to the study of space exploration, most notably through books examining psychological dimensions of space travel. \"The\" \"Journal of Military History\" noted about Vakoch's edited book \"Psychology of Space Exploration: Contemporary Research in Historical Perspective\" that \"[f]or those interested in an overview and synthesis of some of the key issues in the psychology of space exploration, this book provides a great introduction,\" adding that \"[m]ost interestingly to those whose primary interest lies in history, many of the chapters engage the \"history of the psychology\" of space exploration quite well, most notably in the book’s first two chapters.\" \"Aviation, Space, and Environmental Medicine\" observed that \"[t]his book is unique in that it places much of the ongoing research and interest in the topic of human behavior and performance in space in historical perspective\" and concluded that \"[i]t is certainly worthwhile reading for those directly involved in the next phase of human exploration of space as well as those who will witness this phase from the confines of Earth.\" \"Isis\" wrote that \"[t]his diverse and thought-provoking collection represents an important departure for the NASA History Series, a turn from works focused on machines, missions, and management structures to a concern with the smaller group of space sciences interested in human subjects, like space medicine and human factors engineering\" and noted that the book \"represents an important step in bringing the human-focused space sciences to the attention of a wider audience.\"\n\n\"The Journal of Mind and Behavior\" noted that Vakoch's follow-up book \"On Orbit and Beyond: Psychological Perspectives on Human Spaceflight\" includes several chapters that address the implications of the increased autonomy that astronauts would have on missions to Mars and Saturn, as compared to orbiting Earth or travelling to the Moon. In an interview Vakoch explained the implications of this increased autonomy: \"On missions to Mars, where greater autonomy will be expected of astronauts because of the greater distances, ground personnel should expect that their own roles will change over the course of the mission. Unless ground personnel are prepared, they may feel like they are no longer playing as central a role as they did shortly after liftoff because astronauts are now making more decisions.\"\n\nVakoch explored the positive impact that space exploration can have on astronauts through the Overview Effect: \"At its core, the Overview Effect is about gaining a new perspective about life on Earth, by viewing Earth from a distance. Having survived the flight into Earth orbit, contained in the artificial environment of a space capsule, astronauts look back to their home world with a sense of awe and wonder, appreciative of the fragility of their own planet.\" He also drew parallels between the Overview Effect and the impacting of discovering extraterrestrial intelligence: \"It has often been said that if we ever detect a signal from an extraterrestrial intelligence, letting us know that we are not the only intelligent species in the galaxy, then the differences across terrestrial cultures that seem so crucial today will seem much less important. The differences between Us and Them across cultures here on Earth will seem trivial, when we compare our human commonalities with a new alien Them that evolved independently on another world. The Overview Effect provides a similar sense of perspective for those astronauts who have been able to see Earth from a distance, where the boundaries that we draw on maps are invisible, seeming more like human-made constructions than inevitabilities.\" \"From space, those lines that separate nations are invisible,\" Vakoch told \"Forbes\", speaking of a photograph taken of Earth by astronaut William Anders during the Apollo 8 mission, adding that \"the Earthrise photo implies unity, one planet without boundaries.\"\n\nVakoch told \"WIRED\" that today's astronauts need capacities that differ from the earliest astronauts: \"Historically someone with the 'right stuff' was a tough, individualistic person who could explore an unknown frontier with great courage and certainty…. Now, you not only need to be a self-sufficient individual, you need to be able to work with astronauts from other cultures on the International Space Station... If you’re an American astronaut, very often you’ll be working with people who don’t put as high an emphasis on individualism as the United States does. So, beyond the need for autonomy and independence, there is a greater need for interpersonal and intercultural sensitivity among astronauts.\" \"In the early days of space exploration, it really was important to just have the 'right stuff.' If you don’t get along very well with people, you can suck it up for a couple weeks,\" he told \"Inverse\", adding \"[b]ut when you’re talking about a mission that’s going to last for a year, and you don’t have a safe way to vent, that’s going to be a big problem.\" \"The expectation that astronauts 'have the right stuff' is a big barrier,\" Vakoch told \"The Verge\", adding that \"[t]hey don't want to admit faults, nor do they want to lose flight status.\" He said that astronauts on long-duration missions beyond Earth's orbit will face new challenges, telling \"NOW.SPACE\" that \"there’s going to be a lot of boredom on a trip to Mars.\" \"Boredom and feeling separation from home will peak during the travel between Earth and Mars and back,\" he told \"Nightsky\". Vakoch told \"OMNI\" that despite the challenges of boredom, \"[t]he greatest advantage of having humans onboard a deep space mission is that we excel at dealing with the unpredictable.\"\n\nThe Mars One plan to send settlers on a one-way mission adds additional stresses for astronauts, Vakoch told \"Discovery Channel Magazine\": \"Mars One is revolutionary because it overcomes one of the greatest challenges faced by past missions planned for the Red Planet: getting back home safely,\" adding that \"[a]stronauts usually have the comfort of knowing they can return to a safe, familiar environment. However difficult the mission is, there’s an end in sight. Not so for Mars One. By launching off to Mars, they’ll be making a commitment to an unknown way of life that they can never 'unchoose.'\" He said there are analogues of people making one-way trips, though \"[m]ost people can’t conceive of going on Mars One, because they can’t imagine leaving behind everything they love on Earth,\" adding that \"[i]n reality, this is similar to the loss that all immigrants felt throughout the history of human migrations, whether they were forced to leave home or whether they left voluntarily.\"\n\nAt the 2008 annual convention of the American Psychological Association, Vakoch chaired the symposium \"To the Moon and Mars: Psychology of Long-Duration Space Exploration,\" which was identified as a \"highlight\" of the convention.\n\nVakoch has also examined unmanned space exploration. Commenting on plans to send miniature spacecraft to a nearby star, he told the \"International Business Times\" that \"[b]y sending hundreds or thousands of space probes the size of postage stamps, Breakthrough Starshot gets around the hazards of spaceflight that could easily end a mission relying on a single spacecraft. Only one nanocraft needs to make its way to Alpha Centauri and send back a signal for the mission to be successful. When that happens, Starshot will make history.\" Focusing within the solar system, he commented on NASA's plans to send a lander to Jupiter's moon Europa, telling \"Gizmodo\" that \"[t]he top priority of this lander mission will be to search for evidence of life on Europa,\" adding that \"even if that main goal isn’t met, we will learn a great deal about the potential habitability of this icy moon, which will be essential for future, even more ambitious missions.\"\n\nVakoch has collaborated on several empirical studies of human cognition. His research in psycholinguistics with Lee Wurm explores the perception of speech and emotion from an evolutionary framework, with their findings indicating that \"speech perception and the affective lexicon\" are \"closely tied together.\" Vakoch's experimental work with Yuh-Shiow Lee suggests that complex rules are learned better through implicit learning, while simple rules are learned better though explicit learning, with their research suggesting that \"implicit learning can be more efficient than explicit learning.\" Vakoch and the late psychotherapy researcher Hans Herrman Strupp suggested that the expert understanding of experienced psychotherapists is not adequately captured by manualized psychotherapy, and they argued that manualized training can impede \"the development of clinical judgment and complex reasoning.\"\n\nVakoch's cross-cultural research on the perception of tonal languages with Yuh-Shiow Lee and Lee Wurm suggests that native speakers of tonal languages are better able to discriminate tonal differences than speakers of non-tonal languages, finding \"that English listeners had difficulty discriminating Cantonese and Mandarin tones; these authors also found that speakers of the two tone languages were better at discriminating tone contrasts of their own language than of the other language (although they were still better than the English listeners).\"\n\nVakoch's edited volume \"Altruism in Cross-Cultural Perspective\" has been called \"a high-quality tool for cross-cultural studies of altruism and beyond” and \"PsycCRITIQUES\" notes that \"[t]his book should be of interest to both students and professionals concerned with gaining a broader understanding of altruism in cross-cultural and cross-disciplinary perspectives” and judges that the epilogue \"is worth the price of the book alone.\" \"PsycCRITIQUES\" also identifies limitations of this book: \"Although a number of authors address the role of altruism in the context of Eastern religions (Taosim and Hinduism), traditional rituals, and spiritualism, a weakness of the book is that the role of altruism in the Judeo-Christian tradition is barely examined\" and \"[a]nother weakness is that the chapters are not pulled together very successfully. It would have been helpful to have a final overview chapter to pull the various strands together.\"\n\nVakoch is a professor of clinical psychology at the California Institute of Integral Studies (CIIS).\n\nIn conjunction with the annual convention of the American Psychological Association in 2008, Vakoch assessed people's responses to environmental problems. \"I think most people recognize we face a severe environmental crisis, but it's hard to deal with that head-on because most people feel helpless to do anything about it,\" he told \"USA Today\", adding \"[i]f we look at the nature of the problem, it is so big it's hard to know what any individual can do in their own life to make a difference.\" Vakoch called for additional research into the psychology of environmental issues, saying \"[w]e are recognizing that environmental problems have a tremendous impact on many aspects of our lives, but we need a lot more work.\" He cautioned that \"[w]e can't afford to let this increased environmental concern become just another fashionable trend.\"\n\n\"Psychologists are increasingly becoming involved in helping alleviate environmental problems,\" Vakoch told \"CQ Researcher\", adding that \"[p]sychologists are very experienced in dealing with denial and in helping to frame messages in ways that people can hear the bad news without being paralyzed by it.\" \"The most important lesson . . . is that there is no 'one size fits all' solution to environmental problems,\" Vakoch said, adding \"[t]o create effective public policies, leaders need to recognize that different people are willing to adopt more environmentally sound behaviors for different reasons. What's compelling for one person will fail for another.\"\n\n\"PsycCRITIQUES\"'s review of Vakoch and Fernando Castrillón's \"Ecopsychology, Phenomenology, and the Environment\" observes that \"[a]lthough ecopsychological phenomenology’s lack of shrillness is refreshing and laudable for the intrinsic human values it promotes, the book’s strengths contain its vulnerability. We \"have\" reached some critical tipping points, and we cannot continue with business as usual. The authors in \"Ecopsychology, Phenomenology, and the Environment\" are well aware of this. Rather than making dire warnings, they ask the big questions.\" This reviewer noted that \"[w]hat I liked least about \"Ecopsychology, Phenomenology, and the Environment\" is that all of its authors are North Americans or people living in North America\" and \"[w]hat I liked most about it, apart from some really beautiful writing, is its mature approach to suffering and the wildness of our nature, as part of the great chain of being. There is a cogent argument that we must address our sense of separateness from the world that holds us. I believe that readers will come away with an expanded sense of identity, and with a sense of calmness about what can be done and how one might go about contributing.\"\n\nVakoch's books in ecofeminism have been widely reviewed in scholarly journals. The review journal \"Choice\" recommended Vakoch's \"Feminist Ecocriticism\" for “[u]pper-division undergraduates through faculty,” and \"Environmental Philosophy\" called it \"an excellent contribution of Post-structuralist Ecofeminist thought on the contemporary liberatory alternatives debate.\" \"English Studies\" noted that \"[u]nfortunately, \"Feminist Ecocriticism\" presents a slightly anachronistic view of ecofeminism, and fails to take into account the many exciting developments taking place in the field since 2000, such as material feminism\" and \"[t]he collection's main flaw is that the essays rarely, if at all, reference scholarly work published since the turn of the century,\" while \"the biggest contribution it makes, which is certainly not unimportant, is to the study of individual works, most notably Carson's sea books (Sullivan) and \"Silent Spring\" (Magee).\"\n\nThe reviewer of Vakoch's \"Ecofeminism and Rhetoric\" for the journal \"Women & Language\" noted that \"[t]he material is challenging—in terms of its challenge to popular thinking and conventional scholarship—and, yes, even troubling. However, the authors' reflection on their own work and the frames offered at the beginning and the end of the book provide useful ways of processing the theories and examples, making this is a book to which I will return. \"Ecofeminism and Rhetoric\" provides a strong overview of the field, news ways of operationalizing ecofeminist theory, and an awareness of possible problems with ecofeminism and answers to those critiques.\" The same reviewer wrote that \"[b]ecause \"Ecofeminism and Rhetoric\" provides a strong overview of the field, proffering both classic and innovative ecofeminist critiques, it is appropriate for a variety of audiences ranging from the seasoned ecofeminist to one who has just discover ecofeminism. Additionally, reading the epilogue first will help the selective reader choose how to prioritize the chapters. I will use this book again in projects outside of ecofeminism because the lens is well cultivated and the meta-conversation insightful.\"\n\n\"The\" \"Journal of Ecocriticism\" said of the same book \"[i]n many ways, \"Ecofeminism and Rhetoric\" continues the shift toward an interdisciplinary approach in academic publishing. Relevant to many academics, its clear prose, recognizable ecologies, and varied topics attract readers from English departments to the Biological sciences. Undoubtedly, versatility and applicability of Vakoch’s essay collection should not be overlooked.\" \"\" commended \"the collection on its ambitious scope\" and suggested that \"research students and academics will enjoy reading it, and undergraduate students may benefit from the overview chapters that frame the theoretical approaches therein.\" \"Anthropological Forum\" said \"\"Ecofeminism and Rhetoric\" is a worthwhile contribution which will be valuable for undergraduates and researchers both within and outside the academic field of ecofeminism. It has much to commend it as an introductory text in the area.\" \"Rhetoric Review\" noted that \"\"Ecofeminism and Rhetoric\" gives us a small but potentially very valuable window on one avenue of developing the field in this postapocalyptic era of global warming, species reduction, desertification, ecoracism, environmental injustice, slumming as a growth industry in rapid urbanization, and one energy crisis after another.\" The same journal wrote that \"[a] book on ecofeminist rhetoric assumes a difficult task. Not only does it venture into relatively uncharted territory, it also proposes to bring together three fields that all have distinct identities and a large specialized literature: feminism, environmental studies, and rhetoric. The temptation in any analysis of this kind is to focus on one of the three areas at the expense of the others rather than to achieve an effective synthesizing vision. While this collection of essays does an admirable job of opening the topic of ecofeminist rhetoric to further inquiry, it does not achieve the synthesizing vision except in rare moments. What suffers most in the mix is rhetorical scholarship.\"\n\nVakoch's edited volumes in ecofeminism have been criticized for not including adequate multicultural representation among the contributors and perspectives. One reviewer questioned a volume on ecofeminism that includes so many male contributors.\n\nVakoch serves as the general editor of the Ecocritical Theory and Practice series, published by Lexington Books.\n\n\nVakoch, D. A. (2011). \"Communication with extraterrestrial intelligence\". Albany, State University of New York Press.\n\nVakoch, D. A. (2011). \"Ecofeminism and rhetoric: critical perspectives on sex, technology, and discourse\". New York, Berghahn Books.\n\nVakoch, D. A. (2011). \"Psychology of space exploration: contemporary research in historical perspective\". Washington, DC, National Aeronautics and Space Administration, Office of Communications, History Program Office.\n\nVakoch, D. A. (2012). \"Feminist ecocriticism: environment, women, and literature\". Lanham, MD, Lexington Books.\n\nVakoch, D. A. (2013). \"Altruism in cross-cultural perspective\". New York, Springer.\n\nVakoch, D. A. (2013). \"Astrobiology, history and society: life beyond earth and the impact of discovery\". Heidelberg, Springer Verlag.\n\nVakoch, D. A. (2013). \"On orbit and beyond: psychological perspectives on human spaceflight\". Heidelberg, Germany, Springer-Verlag.\n\nVakoch, D. A. (2014). \"Archaeology, anthropology, and interstellar communication\". Washington, DC, National Aeronautics and Space Administration, Office of Communications, Public Outreach Division, History Program Office.\n\nVakoch, D. A. (2014). \"Extraterrestrial altruism: evolution and ethics in the cosmos\". Heidelberg [u.a.], Springer.\n\nVakoch, D. A., & Castrillon, F. (2014). \"Ecopsychology, phenomenology, and the environment: the experience of nature\". New York, Springer.\n\nVakoch, D. A., & Dowd, M. F. (2015). \"The Drake equation: estimating the prevalence of extraterrestrial life through the ages\". Cambridge, Cambridge University Press.\n\nVakoch, D. A., & Harrison, A. A. (2011). \"Civilizations beyond earth: extraterrestrial life and society\". New York, Berghahn Books.\n\nVakoch, D. A., & Mickey, S. (2018). \"Ecofeminism in dialogue\". Lanham, MD, Lexington Books.\n\nVakoch, D. A., & Mickey, S. (2018). \"Literature and ecofeminism: intersectional and international voices\". London, Routledge.\n\nVakoch, D. A., & Mickey, S. (2018). \"Women and nature?: beyond dualism in gender, body, and environment\". London, Routledge.\n\n"}
{"id": "43145843", "url": "https://en.wikipedia.org/wiki?curid=43145843", "title": "Edmund Copeland", "text": "Edmund Copeland\n\nEdmund \"Ed\" Copeland () is a theoretical physicist, cosmologist, and professor of physics working in the Faculty of Science at the University of Nottingham, United Kingdom. Copeland won the 2013 Rayleigh Medal and Prize awarded by the Institute of Physics for his work on particle/string cosmology.\n\nCopeland is well known for his appearances on the physics-popularizing YouTube channel Sixty Symbols, as well as the mathematics-popularizing channel Numberphile.\n"}
{"id": "49643772", "url": "https://en.wikipedia.org/wiki?curid=49643772", "title": "Ekplexite", "text": "Ekplexite\n\nEkplexite is a unique sulfide-hydroxide niobium-rich mineral with the formula (Nb,Mo)S•(MgAl)(OH). It is unique because niobium is usually found in oxide or, eventually, silicate minerals. Ekplexite is a case in which chalcophile behaviour of niobium is shown, which means niobium present in a sulfide mineral. The unique combination of elements in ekplexite has to do with its name, which comes from a Greek world on \"surprise\". The other example of chalcophile behaviour of niobium is edgarite, FeNbS, and both minerals were found in the same environment, which is a fenitic rock of Mt. Kaskasnyunchorr, Khibiny Massif, Kola Peninsula, Russia. Analysis of the same rock has revealed the presence of two analogues of ekplexite, kaskasite (molybdenum-analogue) and manganokaskasite (molybdenum- and manganese-analogue). All three minerals belong to the valleriite group, and crystallize in the trigonal system with similar possible space groups.\n\nBeside niobium, molybdenum, sulfur, magnesium and aluminium ekplexite contains also relatively small amounts of tungsten, vanadium and iron.\n\nThe rock in which contains ekplexite is classified as fenite. In this rock ekplexite associates with fluorophlogopite, nepheline, orthoclase-anorthoclasee (silicates), alabandine, edgarite, pyrite, molybdenite, tungstenite (sulfides), corundum, graphite and monazite-(Ce).\n\nCrystal structure of ekplexite is described as non-commensurate. It is composed of two modules:\n"}
{"id": "13343193", "url": "https://en.wikipedia.org/wiki?curid=13343193", "title": "Evan Kohlmann", "text": "Evan Kohlmann\n\nEvan F. Kohlmann (born 1979) is an American terrorism consultant who has worked for the FBI and other governmental organizations.\n\nHe is a contributor to the \"Counterterrorism Blog\", a senior investigator with The Nine Eleven Finding Answers Foundation, and a terrorism analyst for \"NBC News\".\n\nIn his manifesto Anders Behring Breivik copied 25 pages verbatim from an ideological text by Evan Kohlmann and published by an institute led by Magnus Ranstorp.\n\nIn the profile for the \"Penn Law Journal\", Kohlmann said he spent summers in France while growing up, because his father studied there. Kohlmann graduated from Pine Crest School in Fort Lauderdale, Florida.\n\nHe attended the Georgetown University Edmund A. Walsh School of Foreign Service, where he studied under Mamoun Fandy. Fandy's mentorship sparked his interest in Middle East politics. \"When [Fandy] lived in Egypt, he passed by the number two guy in al-Qaeda there every day. He really knew his subject.\"\n\nKohlmann entered the University of Pennsylvania Law School in the fall of 2001, a few weeks before al-Qaeda's attacks on the U.S. on September 11, 2001.\n\nKohlmann worked as an intern at The Investigative Project, a Washington, DC, counter-terrorism think-tank.\n\nHe wrote \"Al- Qaida’s Jihad in Europe: The Afghan-Bosnian Network\".\n\nHe is a Senior Terrorism Consultant for The NEFA Foundation. He is also a contributor to the \"Counterterrorism Blog\", and a terrorism analyst for \"NBC News\".\n\nHe has called Anwar al-Awlaki \"one of the principal \"jihadi\" luminaries for would-be homegrown terrorists. His fluency with English, his unabashed advocacy of \"jihad\" and \"mujahideen\" organizations, and his Web-savvy approach are a powerful combination.\" He calls al-Awlaki's lecture \"Constants on the Path of Jihad\", which he says was based on a similar document written by al-Qaeda in Saudi Arabia's founder, the \"virtual bible for lone-wolf Muslim extremists.\"\n\nHe produced \"The Al Qaida Plan\", a 90-minute movie, to serve as evidence and stress relief during the Guantanamo Military Commissions, \nwhich was sponsored by the Office of Military Commissions. According to Carol Rosenberg of the \"Miami Herald\": \"He modeled the video after The Nazi Plan, an instructional movie shown at the late 1940s Nuremberg tribunals for the most senior Nazi leadership.\"\n\nInitially Captain Keith Allred, the President of Salim Ahmed Hamdan's Military Commission ruled that the film would be prejudicial, but he reversed this decision.\n\nKohlmann has served frequently as an expert witness for the prosecution in terrorism trials. \"There haven’t been that many cases yet, so sometimes the prosecutors are doing their first ones. I know how the courts work, so I am pretty valuable right now.” Despite being considered a terrorism expert, Kohlmann cannot read, write or speak Arabic.\n\nHis expertise and neutrality have been disputed by defense attorneys and other experts, while his book ″Al-Qaida’s Jihad in Europe: The Afghan-Bosnian Network″ was declined by University of Pennsylvania Press.\n\nHe testified as an expert witness in the following cases:\n\n\n\n"}
{"id": "8793206", "url": "https://en.wikipedia.org/wiki?curid=8793206", "title": "External flow", "text": "External flow\n\nIn fluid mechanics, external flow is such a flow that boundary layers develop freely, without constraints imposed by adjacent surfaces. Accordingly, there will always exist a region of the flow outside the boundary layer in which velocity, temperature, and/or concentration gradients are negligible. It can be defined as the flow of a fluid around a body that is completely submerged in it.\n\nAn example includes fluid motion over a flat plate (inclined or parallel to the free stream velocity) and flow over curved surfaces such as a sphere, cylinder, airfoil, or turbine blade, air flowing around an airplane and water flowing around the submarines.\n"}
{"id": "39180591", "url": "https://en.wikipedia.org/wiki?curid=39180591", "title": "Field theory (sociology)", "text": "Field theory (sociology)\n\nIn sociology, field theory examines how individuals construct social fields, and how they are affected by such fields. Social fields are environments in which competition between individuals and between groups takes place, such as markets, academic disciplines, musical genres, etc.\n\nUnstable fields are defined by rapid change and frequently by destructive forms of competition, such as pure competition over prices that drives profit margins to untenably low levels. Fields thus need to be stabilized with rules which make sure that competition takes non-destructive forms. Stable fields rarely emerge on their own, but must be constructed by skilled entrepreneurs. The government frequently plays a role in this process as well.\n\nFields feature different positions which social actor can occupy. The dominant players in the field are called the \"incumbents\". They are generally invested in maintaining the field in its current form, as changes to the rules of competition risk destabilizing their dominant position. Fields may also feature \"insurgents\" who instead aim to alter the field so they can successfully compete with the incumbents. Dramatic change in previously stable fields can come from either successful incumbents or intrusion from other fields, or from government-imposed rule change.\n\nIn general, different field positions create different incentives. Field position is experienced by individuals in the form of motivation.\n"}
{"id": "44853890", "url": "https://en.wikipedia.org/wiki?curid=44853890", "title": "Frances Northcutt", "text": "Frances Northcutt\n\nFrances 'Poppy' Northcutt (August 10, 1943-) is a Texas attorney who notably began her career as a 'computress' and then an engineer for the technical staff on NASA's Apollo Program during the space race. She was the first female engineer to work in NASA's Mission Control during Apollo 8. Lunar crater Poppy in her honor for her work and pioneering in the Apollo Program.\n\nLater in her career, Northcutt became an attorney specializing in women's rights. In the early 1970s, she served on the national Board of Directors of the National Organization for Women.\n\nFrances \"Poppy\" Northcutt was born in Manny, Louisiana on August 10, 1943. She grew up in Luling, Texas and then moved to Dayton, Texas. Northcutt attended high school at Dayton High School in Liberty County and then went on to study mathematics at the University of Texas. \n\nAfter graduating in three and a half years, Northcutt was hired in 1965 by TRW, an aerospace contractor with NASA, as a computer for the new Apollo program. She was soon promoted to engineer stationed in the Mission Control's Mission Planning and Analysis room. She was the first female engineer to work as part of NASA's Mission Control. \n\nNorthcutt and her team designed the return-to-Earth trajectory that the Apollo 8 crew took from the moon back to Earth. She made key insights that identified mistakes in the plan before they were executed. If there was any delay in swinging around the moon resulting in using up more fuel than planned it was up to Northcutt and her team to make on-the-spot calculations and changes to the plan to ensure the astronauts' safe return. Apollo 8 was the second crewed spacecraft for the United States and became the first crewed mission to ever leave orbit. It successfully reached the Earth's moon, orbited it and then returned to Earth safely on December 27,1968. \nNorthcutt continued working with TRW and NASA for several more years working as a critical component of some of NASA's most historic missions such as Apollo 13. After learning about the exploded oxygen tank on the Apollo 13 mission, Northcutt and the other engineers who developed the computer program for Apollo 13 to get home immediately came in to find a way to get the astronauts home safely. Her program that she worked on was used to compute the maneuvers to come home. Northcutt and the Mission Operations Team were later awarded the Presidential Medal of Freedom Team Award for finding a way to safely return Apollo 13. Apollo 17, the last mission to the moon, landed next to \"Crater Poppy\", a crater named in Northcutt's honor for her work as one of the early pioneers in the Apollo program.\n\nAs one of few women working in engineering, Northcutt became increasing involved in the Women's Liberation movement. She helped put on demonstrations, strikes, speeches, press releases and whatever she could to help the cause with the National Organization for Women. She spoke at City Council many times and in 1974, the mayor of Houston, TX named her the first Women's Advocate for the City. In this position she passed a great deal of legislation passed improving the status of women. She negotiated an agreement with the Police Chief enabling women to become cops. She got the Fire Department to agree to let women in. She led a big equal pay study going through the entire payroll. She was so dedicated to improving equality that she counted every women's versus men's bathroom in all of Houston helping bring even this number to parity. \n\nNorthcutt helped drastically increase the number of women that were on appointed boards and commissions. She helped pass a law that no longer allowed hospitals to charge women who came in for a rape kit. Later on, Northcutt would become President of both the Houston and entire Texas chapter for the National Organization of Women. \n\nDuring this time, Northcutt was still employed by TRW, receiving a partial salary as she was on loan. When her loan expired she went back to TRW for a while. However, she believed \"if you were doing your job, you should do yourself out of a job\" and thus went to Merrill Lynch, a stockbroker firm for a year. Northcutt then switched into the TRW Controls division and during this time attended law school at night. In 1984, Northcutt graduated summa cum laude from the University of Houston Law Center becoming a criminal defense lawyer. Northcutt practiced law for the rest of her life with special emphasis and dedication to her fight for civil rights. \n"}
{"id": "19161243", "url": "https://en.wikipedia.org/wiki?curid=19161243", "title": "Global Challenge Award", "text": "Global Challenge Award\n\nThe Global Challenge Award is an online science and engineering design program for pre-college school students (e.g. middle school through high school) from all over the world. It is an initiative that started with a partnership with the University of Vermont in collaboration with the National Science Foundation, currently funded by the MacArthur Foundation Digital Media and Learning program as well as other foundations and corporations, wherein students have the opportunity to form teams with international counterparts and work towards a solution to mitigate global warming and help envision the future of renewable energy. The program is an online educational environment that uses game based learning, simulation and Web-based science resources in a global competition. It relies on the personal initiative and creativity of students working in diverse teams. The access to the project via the Web makes it possible for students, parents, homeschooling families, teachers and interested global community members to get involved to help young people with their creative ideas for innovation in new forms of energy, conservation and increased productivity.\n\nFounded in 2005 by Craig Deluca and David Rocchio of the Arno Group and Biddle Duke, the publisher of the Stowe Reporter newspaper company in Stowe, Vt., working in close partnership with Domenico Grasso of The University of Vermont (see) College of Engineering and Mathematical Sciences, the program gives international student teams the opportunity to experience the excitement of scientific understanding and engineering design while working on significant human and societal issues – bringing science to life in innovative new applications. The program mission is to \"give students the tools and confidence to solve global problems together.\"\n\nThe overarching model for the learning experiences offered worldwide to any student was influenced by The George Lucas Foundation's Big ideas For Better Schools, the Partnership for 21st Century Schools and game based learning. The Global Challenge was funded in part by a National Science Foundation award from the Innovative Technology Experiences for Students (ITEST) program, validating the project's design for engaging youth in science, technology, engineering and mathematics learning.\n\nSince its founding in 2005, The Global Challenge has reached over 100,000 people worldwide and engaged over 4,000 students from 60 countries in forming teams to solve the challenge. About $200,000 in scholarships, travel, summer study have been provided to over 200 students from 10 countries.\n\nThe Global Challenge Award is responsible for identification of high school students who represent the United States in the International Earth Science Olympiad or IESO. Students and teachers travel to South Korea in 2007 and the Philippines in 2008. Plans are now underway to form a US-IESO selection process with the support of the American Geological Institute.\n\nIn addition, the design of the program builds international student teams. Students from over 79 countries participate each year. Top countries by participation with over 100 students each year have been the United States, India, China, and South Korea.\n\nThere are several project areas in the Challenge. Some are designed specifically for teams, others students can work on alone. Students can mix and match projects based on their interest level and time. They can form a team to compete in one competition and, at the same time, work on individual points.\n\n\nEach Challenge earns certain points, and in the end, teams with the highest scores win and earn scholarships, travel awards to the Governor's Institute on Engineering in Vermont, cash prizes, and tuition scholarships.\n\nThe program was the lead story \"Save the World\" in Learning & Leading with Technology November 2007. In the Burlington Free Press in July 2008, and has led to a number of youth-authored articles on Cogito.org, for example: Using Nanotechnology for Cost Effective Converters as well as Educating Myself, International Style.\n\nThe Challenge's founders envision creating a World Game space for STEM learning.\n\n\n"}
{"id": "977799", "url": "https://en.wikipedia.org/wiki?curid=977799", "title": "HCG 87", "text": "HCG 87\n\nHCG 87 is a compact group of galaxies listed in the Hickson Compact Group Catalogue. This group is about 400 million light-years away in the constellation Capricornus. \nThe group distinguishes itself as one of the most compact groups of galaxies, hosting two active galactic nuclei and a starburst among its three members, all of which show signs of interaction. This interaction, which astronomers have called visually, and scientifically, intriguing is being examined to understand the influence of active nuclei on star formation histories.\n\n"}
{"id": "42868360", "url": "https://en.wikipedia.org/wiki?curid=42868360", "title": "Heliothrix oregonensis", "text": "Heliothrix oregonensis\n\nHeliothrix oregonensis is a phototrophic filamentous, gliding bacterium containing bacteriochlorophyll a\nthat is aerotolerant and photoheterotrophic.\n\n\n"}
{"id": "14684866", "url": "https://en.wikipedia.org/wiki?curid=14684866", "title": "Illinois Soil Nitrogen Test", "text": "Illinois Soil Nitrogen Test\n\nThe Illinois Soil Nitrogen Test (\"ISNT\") is a method for measuring the amount of Nitrogen in soil that is available for use by plants as a nutrient. The test predicts whether the addition of nitrogen fertilizer to agricultural land will result in increased crop yields.\n\nNitrogen is essential for plant development. Indeed, for crops that are destined to be food for farm animal or human consumption, incorporation of nitrogen into the crop is an important goal, since this forms the basis for protein in the human diet.\n\nNitrogen is commonly present in soils in many forms, and there are many ways to measure this nitrogen. None of these are completely satisfactory as a measure of the nitrogen that is available for use by crops. The ISNT is a new (2007) method for measuring nitrogen available for plant uptake.\n\nISNT estimates the amount of nitrogen present in the soil as amino sugar nitrogen. With respect to corn and soybeans, the optimal range for plant growth appears to be around 225 to 240 mg/Kg. Some form of nitrogen fertilizer is needed if levels are below this range. On the other hand, if levels are above this range, addition of nitrogen fertilizer will not increase crop yield.\n\nIn the corn belt, since about 1975, the predominant method of estimating the amount of nitrogen needed for corn has been the \"yield-based\" method. A farmer first estimates the yield of corn he intends to produce. He then applies 1.1 to 1.4 lbs of nitrogen per bushel of expected yield. ISNT represents an alternative approach to managing nitrogen application. However, ISNT does not offer a simple answer as to the amount of nitrogen fertilizer that is needed, or as to the optimal form of that fertilizer. \n\nIn field trials in Illinois, some fields have been found to be under-fertilized when managed according to the \"yield-based\" method, as judged by the ISNT. In the majority of trials, however, the yield-based method calls for the addition of nitrogen far in excess of the levels needed for optimal crop production. This nitrogen, which is applied by farmers at great cost, does not find its way into the crop, but is lost to the atmosphere or leaches into waterways.\n\nWithin the corn belt, stalks and other crop residues are left in the field with the intention of enhancing the amount of organic material in the soil. Excessive nitrogen application, however, appears to promote the rapid decomposition of organic matter in the soil, resulting in release of carbon dioxide. As a result, the amount of organic material in soils managed according to the yield-based method in the corn belt appears to be decreasing in spite of the large amounts of crop residues left in the fields.\n\n"}
{"id": "4933087", "url": "https://en.wikipedia.org/wiki?curid=4933087", "title": "Industrial deconcentration", "text": "Industrial deconcentration\n\nIndustrial deconcentration is the movement of industrial zones (factories) away from the center of the city, and further away from each other. It is similar to suburbanization, a residential trend in which a large number of the population move away from the metropolis as the inner city becomes overcrowded.\n\nIndustrial deconcentration occurs when a previously established industrial district becomes unable to provide efficiently for its own populace due to overcrowding. In a market economy the massive competition and overcrowding of the metropolitan area forces people and businesses to move out to less-industrial areas with less traffic congestion. Modernization in the social, economic, and technological fields of a country is a factor in accelerating industrial deconcentration.\n\nThis phenomenon is more apparent in nations that have been industrialized for a longer time. Most countries experiencing industrial deconcentration at the beginning of the 21st century are the states that began industrializing after the end of World War II.\n\nIndustrial deconcentration is a conscious goal in a comprehensive decentralization policy: to deconcentrate population without excessive commuting, jobs need to be created outside the cities.\n\n"}
{"id": "44075293", "url": "https://en.wikipedia.org/wiki?curid=44075293", "title": "Interploidy hybridization", "text": "Interploidy hybridization\n\nInterploidy hybridization is a term to describe a hybridization (or manual cross) between two different individuals of different ploidy levels. Individuals resulting from this type of hybridization are called interploidy hybrids. This phenomenon is often observed in plants. Interploidy hybridizations in angiosperms often cause abnormal seed development, leading to reduced seed size or seed abortion. This reproductive bottle neck leads to a phenomenon called triploid block. In agriculture, development of new plant cultivars, utilizing interploidy hybrids, is usually preceded by interspecific cross between two closely related species with different ploidy levels.\n"}
{"id": "45619048", "url": "https://en.wikipedia.org/wiki?curid=45619048", "title": "Ladies Who Code", "text": "Ladies Who Code\n\nLadies of Code is an international non-profit organization dedicated to supporting professional women software developers. The organization is best known for its meet-ups, conferences, hack nights, career development workshops, study groups, and speaker series featuring influential information technology industry experts.\n\nLadies Who Code was founded in April 2011 by Angie Maguire and Shoshi Robert with support from Mint Digital. The first US event took place in New York on June 29th followed by the first European event in London on September 7th.\n\nIn 2013, Ladies Who Code hosted their first UK conference bringing together the European membership. In 2018, Ladies of Code will relaunch their UK conference after a three year hiatus.\n\nIn 2013, Ladies Who Code launched a series of scholarships focussed on providing free professional training and access to industry events.\n\nIn 2015, Ladies Who Code became Ladies of Code \n\nIn 2018, on International Women's Day, Ladies Who Code launched a monthly interview series.. \n\nLadies of Code has active chapters in London, Manchester, Newcastle, Leeds, Birmingham, Glasgow and Paris. \n\n\n\n"}
{"id": "13617924", "url": "https://en.wikipedia.org/wiki?curid=13617924", "title": "List of Schedule V drugs (US)", "text": "List of Schedule V drugs (US)\n\nThis is the list of Schedule V drugs as defined by the United States Controlled Substances Act.\nThe following findings are required for drugs to be placed in this schedule:\n\nThe complete list of Schedule V drugs follows. The Administrative Controlled Substances Code Number for each drug is included.\n"}
{"id": "55294041", "url": "https://en.wikipedia.org/wiki?curid=55294041", "title": "List of the Mesozoic life of Alabama", "text": "List of the Mesozoic life of Alabama\n\nThis list of the Mesozoic life of Alabama contains the various prehistoric life-forms whose fossilized remains have been reported from within the US state of Alabama and are between 252.17 and 66 million years of age.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "11486229", "url": "https://en.wikipedia.org/wiki?curid=11486229", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: V", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: V\n\n"}
{"id": "22374191", "url": "https://en.wikipedia.org/wiki?curid=22374191", "title": "Local Government Yorkshire and Humber", "text": "Local Government Yorkshire and Humber\n\nLocal Government Yorkshire and Humber (LGYH) was the partnership of local authorities, including fire, police and national park authorities, across Yorkshire and Humber. It had links to the Local Government Association at national level and incorporated the regional employers organisation for Yorkshire and Humber.\n\nIt brought local authorities together on key issues, supports the improvement of service delivery, influenced Government on the future of local government, promotes good employment practices, and worked with local authorities to improve the public perception of local government.\n\nLocal Government Yorkshire and Humber closed in March 2015. \n\n"}
{"id": "2606518", "url": "https://en.wikipedia.org/wiki?curid=2606518", "title": "Macromolecular docking", "text": "Macromolecular docking\n\nMacromolecular docking is the computational modelling of the quaternary structure of complexes formed by two or more interacting biological macromolecules. Protein–protein complexes are the most commonly attempted targets of such modelling, followed by protein–nucleic acid complexes.\n\nThe ultimate goal of docking is the prediction of the three-dimensional structure of the macromolecular complex of interest as it would occur in a living organism. Docking itself only produces plausible candidate structures. These candidates must be ranked using methods such as scoring functions to identify structures that are most likely to occur in nature.\n\nThe term \"docking\" originated in the late 1970s, with a more restricted meaning; then, \"docking\" meant refining a model of a complex structure by optimizing the separation between the interactors but keeping their relative orientations fixed. Later, the relative orientations of the interacting partners in the modelling was allowed to vary, but the internal geometry of each of the partners was held fixed. This type of modelling is sometimes referred to as \"rigid docking\". With further increases in computational power, it became possible to model changes in internal geometry of the interacting partners that may occur when a complex is formed. This type of modelling is referred to as \"flexible docking\".\n\nThe biological roles of most proteins, as characterized by which other macromolecules they interact with, are known at best incompletely. Even those proteins that participate in a well-studied biological process (e.g., the Krebs cycle) may have unexpected interaction partners or functions which are unrelated to that process.\n\nIn cases of known protein–protein interactions, other questions arise. Genetic diseases (e.g., cystic fibrosis) are known to be caused by misfolded or mutated proteins, and there is a desire to understand what, if any, anomalous protein–protein interactions a given mutation can cause. In the distant future, proteins may be designed to perform biological functions, and a determination of the potential interactions of such proteins will be essential.\n\nFor any given set of proteins, the following questions may be of interest, from the point of view of technology or natural history:\n\nIf they do bind,\n\nIf they do not bind,\n\nProtein–protein docking is ultimately envisaged to address all these issues. Furthermore, since docking methods can be based on purely physical principles, even proteins of unknown function (or which have been studied relatively little) may be docked. The only prerequisite is that their molecular structure has been either determined experimentally, or can be estimated by a protein structure prediction technique.\n\nProtein–nucleic acid interactions feature prominently in the living cell. Transcription factors, which regulate gene expression, and polymerases, which catalyse replication, are composed of proteins, and the genetic material they interact with is composed of nucleic acids. Modeling protein–nucleic acid complexes presents some unique challenges, as described below.\n\nIn the 1970s, complex modelling revolved around manually identifying features on the surfaces of the interactors, and interpreting the consequences for binding, function and activity; any computer programmes were typically used at the end of the modelling process, to discriminate between the relatively few configurations which remained after all the heuristic constraints had been imposed. The first use of computers was in a study on hemoglobin interaction in sickle-cell fibres. This was followed in 1978 by work on the trypsin-BPTI complex. Computers discriminated between good and bad models using a scoring function which rewarded large interface area, and pairs of molecules in contact but not occupying the same space. The computer used a simplified representation of the interacting proteins, with one interaction centre for each residue. Favorable electrostatic interactions, including hydrogen bonds, were identified by hand.\n\nIn the early 1990s, more structures of complexes were determined, and available computational power had increased substantially. With the emergence of bioinformatics, the focus moved towards developing generalized techniques which could be applied to an arbitrary set of complexes at acceptable computational cost. The new methods were envisaged to apply even in the absence of phylogenetic or experimental clues; any specific prior knowledge could still be introduced at the stage of choosing between the highest ranking output models, or be framed as input if the algorithm catered for it.\n1992 saw the publication of the correlation method, an algorithm which used the fast Fourier transform to give a vastly improved scalability for evaluating coarse shape complementarity on rigid-body models. This was extended in 1997 to cover coarse electrostatics.\n\nIn 1996 the results of the first blind trial were published, in which six research groups attempted to predict the complexed structure of TEM-1 Beta-lactamase with Beta-lactamase inhibitor protein (BLIP). The exercise brought into focus the necessity of accommodating conformational change and the difficulty of discriminating between conformers. It also served as the prototype for the CAPRI assessment series, which debuted in 2001.\n\nIf the bond angles, bond lengths and torsion angles of the components are not modified at any stage of complex generation, it is known as \"rigid body docking\". A subject of speculation is whether or not rigid-body docking is sufficiently good for most docking. When substantial conformational change occurs within the components at the time of complex formation, rigid-body docking is inadequate. However, scoring all possible conformational changes is prohibitively expensive in computer time. Docking procedures which permit conformational change, or \"flexible docking\" procedures, must intelligently select small subset of possible conformational changes for consideration.\n\nSuccessful docking requires two criteria:\n\nFor many interactions, the binding site is known on one or more of the proteins to be docked. This is the case for antibodies and for competitive inhibitors. In other cases, a binding site may be strongly suggested by mutagenic or phylogenetic evidence. Configurations where the proteins interpenetrate severely may also be ruled out \"a priori\".\n\nAfter making exclusions based on prior knowledge or stereochemical clash, the remaining space of possible complexed structures must be sampled exhaustively, evenly and with a sufficient coverage to guarantee a near hit. Each configuration must be scored with a measure that is capable of ranking a nearly correct structure above at least 100,000 alternatives. This is a computationally intensive task, and a variety of strategies have been developed.\n\nEach of the proteins may be represented as a simple cubic lattice. Then, for the class of scores which are discrete convolutions, configurations related to each other by translation of one protein by an exact lattice vector can all be scored almost simultaneously by applying the convolution theorem. It is possible to construct reasonable, if approximate, convolution-like scoring functions representing both stereochemical and electrostatic fitness.\n\nReciprocal space methods have been used extensively for their ability to evaluate enormous numbers of configurations. They lose their speed advantage if torsional changes are introduced. Another drawback is that it is impossible to make efficient use of prior knowledge. The question also remains whether convolutions are too limited a class of scoring function to identify the best complex reliably.\n\nIn Monte Carlo, an initial configuration is refined by taking random steps which are accepted or rejected based on their induced improvement in score (see the Metropolis criterion), until a certain number of steps have been tried. The assumption is that convergence to the best structure should occur from a large class of initial configurations, only one of which needs to be considered. Initial configurations may be sampled coarsely, and much computation time can be saved. Because of the difficulty of finding a scoring function which is both highly discriminating for the correct configuration and also converges to the correct configuration from a distance, the use of two levels of refinement, with different scoring functions, has been proposed. Torsion can be introduced naturally to Monte Carlo as an additional property of each random move.\n\nMonte Carlo methods are not guaranteed to search exhaustively, so that the best configuration may be missed even using a scoring function which would in theory identify it. How severe a problem this is for docking has not been firmly established.\n\nTo find a score which forms a consistent basis for selecting the best configuration, studies are carried out on a standard benchmark (see below) of protein–protein interaction cases. Scoring functions are assessed on the rank they assign to the best structure (ideally the best structure should be ranked 1), and on their coverage (the proportion of the benchmark cases for which they achieve an acceptable result).\nTypes of scores studied include:\n\nIt is usual to create hybrid scores by combining one or more categories above in a weighted sum whose weights are optimized on cases from the benchmark. To avoid bias, the benchmark cases used to optimize the weights must not overlap with the cases used to make the final test of the score.\n\nThe ultimate goal in protein–protein docking is to select the ideal ranking solution according to a scoring scheme that would also give an insight into the affinity of the complex. Such a development would drive \"in silico\" protein engineering, computer-aided drug design and/or high-throughput annotation of which proteins bind or not (annotation of interactome). Several scoring functions have been proposed for binding affinity / free energy prediction. However the correlation between experimentally determined binding affinities and the predictions of nine commonly used scoring functions have been found to be nearly orthogonal (R ~ 0). It was also observed that some components of the scoring algorithms may display better correlation to the experimental binding energies than the full score, suggesting that a significantly better performance might be obtained by combining the appropriate contributions from different scoring algorithms. Experimental methods for the determination of binding affinities are: surface plasmon resonance (SPR), Förster resonance energy transfer, radioligand-based techniques, isothermal titration calorimetry (ITC), microscale thermophoresis (MST) or spectroscopic measurements and other fluorescence techniques. Textual information from scientific articles can provide useful cues for scoring.\n\nA benchmark of 84 protein–protein interactions with known complexed structures has been developed for testing docking methods. The set is chosen to cover a wide range of interaction types, and to avoid repeated features, such as the profile of interactors' structural families according to the SCOP database. Benchmark elements are classified into three levels of difficulty (the most difficult containing the largest change in backbone conformation). The protein–protein docking benchmark contains examples of enzyme-inhibitor, antigen-antibody and homomultimeric complexes.\n\nThe latest version of protein-protein docking benchmark consists of 230 complexes. A protein-DNA docking benchmark consists of 47 test cases. A protein-RNA docking benchmark was curated as a dataset of 45 non-redundant test cases with complexes solved by X-ray crystallography only as well as an extended dataset of 71 test cases with structures derived from homology modelling as well. The protein-RNA benchmark has been updated to include more structures solved by X-ray crystallography and now it consists of 126 test cases. The benchmarks have a combined dataset of 209 complexes.\n\nA binding affinity benchmark has been based on the protein–protein docking benchmark. 81 protein–protein complexes with known experimental affinities are included; these complexes span over 11 orders of magnitude in terms of affinity. Each entry of the benchmark includes several biochemical parameters associated with the experimental data, along with the method used to determine the affinity. This benchmark was used to assess the extent to which scoring functions could also predict affinities of macromolecular complexes.\n\nThis Benchmark was post-peer reviewed and significantly expanded. The new set is diverse in terms of the biological functions it represents, with complexes that involve G-proteins and receptor extracellular domains, as well as antigen/antibody, enzyme/inhibitor, and enzyme/substrate complexes. It is also diverse in terms of the partners' affinity for each other, with K ranging between 10 and 10 M. Nine pairs of entries represent closely related complexes that have a similar structure, but a very different affinity, each pair comprising a cognate and a noncognate assembly. The unbound structures of the component proteins being available, conformation changes can be assessed. They are significant in most of the complexes, and large movements or disorder-to-order transitions are frequently observed. The set may be used to benchmark biophysical models aiming to relate affinity to structure in protein–protein interactions, taking into account the reactants and the conformation changes that accompany the association reaction, instead of just the final product.\n\nThe Critical Assessment of PRediction of Interactions is an ongoing series of events in which researchers throughout the community try to dock the same proteins, as provided by the assessors. Rounds take place approximately every 6 months. Each round contains between one and six target protein–protein complexes whose structures have been recently determined experimentally. The coordinates and are held privately by the assessors, with the cooperation of the structural biologists who determined them. The assessment of submissions is double blind.\n\nCAPRI attracts a high level of participation (37 groups participated worldwide in round seven) and a high level of interest from the biological community in general. Although CAPRI results are of little statistical significance owing to the small number of targets in each round, the role of CAPRI in stimulating discourse is significant. (The CASP assessment is a similar exercise in the field of protein structure prediction).\n\n"}
{"id": "24771127", "url": "https://en.wikipedia.org/wiki?curid=24771127", "title": "Madras Institute of Development Studies", "text": "Madras Institute of Development Studies\n\nThe Madras Institute of Development Studies (MIDS) is a research institute based in Chennai. It is a joint undertaking of the Governments of India and Tamil Nadu for conducting research on development problems in Tamil Nadu and the rest of India.\n\nMIDS was established by Malcolm Adiseshiah and his wife Elizabeth Adiseshiah in January 1971. Malcolm Adiseshiah served as its first Director. In March 1977, MIDS was reconstituted as a National Research Institute under the sponsorship of Government of India through the Indian Council of Social Science Research and the Government of Tamil Nadu. The Adiseshiah's donated the land, buildings, furniture, equipment and made cash endowments to the newly constituted institute. Malcolm Adiseshiah resigned as MIDS' Director in 1978 and became the Chairman and Honorary fellow. Professor C.T. Kurien became the next Director of MIDS. In 1985, the Reserve Bank of India established a Chair for applied research in regional economics, which has since been converted to a fully autonomous unit in 2002.\n\nMIDS is governed by a Governing Council and an Academic Council. The Governing Council consists of the Chairperson, the Institute's Director as member-secretary, representatives of faculty, the Indian Council of Social Science Research, the Government of Tamil Nadu, and from the universities of the four South Indian states, trustees of the Institute, and co-opted social scientists, as members. It ordinarily meets once a year, while its Finance Committee and Executive Council meet twice a year. The Academic Council consists of the Director as chairperson, and both external and internal members. All the professors of the Institute are its members; other faculty members and the Ph.D. scholars serve on it in rotation. Five reputed social scientists from other universities and institutes are members of the Academic Council. Each of them serve for three years. The Council meets twice a year to review, and provide guidelines on, the academic activities of the Institute.\n\nMIDS' research focuses on subjects like development and planning, centre-state relationship, poverty, inequality, agrarian issues, social movements, caste and communal politics. It offers a full-time doctoral program under three classes - Indian Council of Social Science Research fellowships, RBI fellowships and Malcolm & Elizabeth Adiseshiah Ph.D. Merit Scholarship. It also offers an annual award called \"The Malcolm Adiseshiah award\" for mid-career academicians in the field of developmental studies. Prominent faculty members of MIDS (both past and present) include Kaushik Basu, A R Venkatachalapathy, Padmini Swaminathan and K. Nagaraj. Its publication cell publishes books and academic papers (sometimes in collaboration with Kalachuvadu Pathippagam, a Tamil publishing house). It also brings out a bi-annual bulletin called \"Review of Development and Change\".\n"}
{"id": "29801958", "url": "https://en.wikipedia.org/wiki?curid=29801958", "title": "Matthiessen's ratio", "text": "Matthiessen's ratio\n\nIn optics, Matthiessen's ratio is the ratio between the distance from the centre of the lens to the retina, versus the lens radius.\n\nThis is of particular importance in fish, where the value may decrease from as high as 3.6 to 2.3, decreasing the focal ratio of the lens. A higher focal ratio is thought to compensate for the relatively high Matthiessen’s ratio brought about by constraints of small eye size during early development. This provides a means for larval fish to focus images from different distances, before the ability to accommodate is gained.\n"}
{"id": "52160778", "url": "https://en.wikipedia.org/wiki?curid=52160778", "title": "Normal contact stiffness", "text": "Normal contact stiffness\n\nNormal contact stiffness is a physical quantity related to the generalized force displacement behavior of rough surfaces in contact with a rigid body or a second similar rough surface. As two solid bodies of the same material approach one another, they transition from conditions of non-contact to homogeneous bulk type behaviour. The varying values of stiffness and true contact area that is exhibited at an interface during this transition is dependent on conditions of applied pressure and is of notable importance for the study of systems involving the physical interactions of multiple bodies including granular matter, electrode contacts, and thermal contacts, where the interface-localized structures govern overall system performance.\n\nThe role of surface structure in normal contact mechanics, in terms of stiffness and true contact area is a frequently studied topic. Parameters of roughness, fractal dimension and asperity geometry are often discussed with reference to their significance on contact mechanics of surfaces.\n"}
{"id": "52969818", "url": "https://en.wikipedia.org/wiki?curid=52969818", "title": "Oluf Winge", "text": "Oluf Winge\n\nGustav Oluf Bang Winge (May 14, 1855 Copenhagen – February 16, 1889) was a Danish zoologist.\n\nThe older brother of the zoologist Herluf Winge, Oluf also had since childhood a great interest in animals. He took in 1881 the title of \"Magisterkonferens\" in Natural History and in 1883 became assistant at the Zoologisk Museum. He worked mainly with ornithology, such as the birds recovered from the lighthouses in Denmark and his most important work, \"Fugle fra Knoglehuler i Brasilien\" (1888), on the fossil birds discovered in the caves in of the state of Minas Gerais in Brazil by his compatriot paleontologist Peter Wilhelm Lund.\n\nHe died in 1889, 33 years old, from an \"incurable breast suffering\".\n\nThe Pleistocene condor genus \"Wingegyps\" is named after him.\n"}
{"id": "19912212", "url": "https://en.wikipedia.org/wiki?curid=19912212", "title": "Poole–Frenkel effect", "text": "Poole–Frenkel effect\n\nIn solid-state physics, the Poole–Frenkel effect (also known as Frenkel-Poole emission) is a means by which an electrical insulator can conduct electricity. It is named after Yakov Frenkel, who published on it in 1938, and also after H. H. Poole (Horace Hewitt Poole, 1886-1962), Ireland.\n\nElectrons can move (slowly) through an insulator by the following method. The electrons are generally trapped in localized states (loosely speaking, they are \"stuck\" to a single atom, and not free to move around the crystal). Occasionally, random thermal fluctuations will give that electron enough energy to get out of its localized state, and move to the conduction band. Once there, the electron can move through the crystal, for a brief amount of time, before relaxing into another localized state (in other words, \"sticking\" to a different atom). The Poole–Frenkel effect describes how, in a large electric field, the electron doesn't need as much thermal energy to get into the conduction band (because part of this energy comes from being pulled by the electric field), so it does not need as large a thermal fluctuation and will be able to move more frequently.\n\nTaking everything into account (both the frequency with which electrons get excited into the conduction band, and their motion once they're there), the standard quantitative expression for the Poole–Frenkel effect is:\nwhere:\n\n"}
{"id": "40206213", "url": "https://en.wikipedia.org/wiki?curid=40206213", "title": "Principle of similitude", "text": "Principle of similitude\n\nThe principle of similitude is a supplement to the scientific method advocated by Lord Rayleigh (1842–1919) that requires that any suggested scientific law be examined for its relationship to similar laws.\n\nThe principle of similitude is very often used in scientific studies. For example, the study of blood flow known as hemodynamics. This study sometimes require the modeling of blood vessels in vitro. Here, the principle of similitude plays a huge role in ensuring the conditions of the model reproduces all aspects of behavior as the blood vessel in study. \n\nVarious techniques are implemented when applying the principle of similitude. A well known technique is dimensional analysis.\n"}
{"id": "3735409", "url": "https://en.wikipedia.org/wiki?curid=3735409", "title": "Psychological resilience", "text": "Psychological resilience\n\nPsychological resilience is the ability to successfully cope with a crisis and to return to pre-crisis status quickly. Resilience exists when the person uses \"mental processes and behaviors in promoting personal assets and protecting self from the potential negative effects of stressors\". In simpler terms, psychological resilience exists in people who develop psychological and behavioral capabilities that allow them to remain calm during crises/chaos and to move on from the incident without long-term negative consequences. Psychological resilience is an evolutionary advantage that most people have and use to manage normal stressors.\n\nResilience is generally thought of as a \"positive adaptation\" after a stressful or adverse situation. When a person is \"bombarded by daily stress, it disrupts their internal and external sense of balance, presenting challenges as well as opportunities\". Resilience is the integrated adaptation of physical, mental and spiritual aspects in a set of \"good or bad\" circumstances, a coherent sense of self that is able to maintain normative developmental tasks that occur at various stages of life.\nThe Children's Institute of the University of Rochester explains that \"resilience research is focused on studying those who engage in life with hope and humor despite devastating losses\".\nIt is important to note that resilience is not only about overcoming a deeply stressful situation, but also coming out of the said situation with \"competent functioning\". Resiliency allows a person to rebound from adversity as a strengthened and more resourceful person.\n\nThe first research on resilience was published in 1973. The study used epidemiology, which is the study of disease prevalence, to uncover the risks and the protective factors that now help define resilience. A year later, the same group of researchers created tools to look at systems that support development of resilience.\n\nEmmy Werner was one of the early scientists to use the term \"resilience\" in the 1970s. She studied a cohort of children from Kauai, Hawaii. Kauai was quite poor and many of the children in the study grew up with alcoholic or mentally ill parents. Many of the parents were also out of work. Werner noted that of the children who grew up in these detrimental situations, two-thirds exhibited destructive behaviors in their later teen years, such as chronic unemployment, substance abuse, and out-of-wedlock births (in case of teenage girls). However, one-third of these youngsters did not exhibit destructive behaviours. Werner called the latter group 'resilient'. Thus, resilient children and their families were those who, by definition, demonstrated traits that allowed them to be more successful than non-resilient children and families.\n\nResilience also emerged as a major theoretical and research topic from the studies of children with mothers diagnosed with schizophrenia in the 1980s. In a 1989 study, the results showed that children with a schizophrenic parent may not obtain an appropriate level of comforting caregiving—compared to children with healthy parents—and that such situations often had a detrimental impact on children's development. On the other hand, some children of ill parents thrived well and were competent in academic achievement, and therefore led researchers to make efforts to understand such responses to adversity.\n\nSince the onset of the research on resilience, researchers have been devoted to discovering the protective factors that explain people's adaptation to adverse conditions, such as maltreatment, catastrophic life events, or urban poverty. The focus of empirical work then has been shifted to understand the underlying protective processes. Researchers endeavor to uncover how some factors (e.g. connection to family) may contribute to positive outcomes.\n\nIn all these instances, resilience is best understood as a process. It is often mistakenly assumed to be a trait of the individual, an idea more typically referred to as \"resiliency\". Most research now shows that resilience is the result of individuals being able to interact with their environments and the processes that either promote well-being or protect them against the overwhelming influence of risk factors. It is essential to understand the process or this cycle of resiliency. When people are faced with an adverse condition, there are three ways in which they may approach the situation.\n\n\nOnly the third approach promotes well-being. It is employed by resilient people, who become upset about the disruptive state and thus change their current pattern to cope with the issue. The first and second approaches lead people to adopt the victim role by blaming others and rejecting any coping methods even after the crisis is over. These people prefer to instinctively react, rather than respond to the situation. Those who respond to the adverse conditions by adapting themselves tend to cope, spring back, and halt the crisis. Negative emotions involve fear, anger, anxiety, distress, helplessness, and hopelessness which decrease a person's ability to solve the problems they face and weaken a person's resiliency. Constant fears and worries weaken people's immune system and increase their vulnerability to illnesses.\n\nThese processes include individual coping strategies, or may be helped by a protective environment like good families, schools, communities, and social policies that make resilience more likely to occur. In this sense \"resilience\" occurs when there are cumulative \"protective factors\". These factors are likely to play a more important role, the greater the individual's exposure to cumulative risk factors.\n\nThree notable bases for resilience—self-confidence, self-esteem and self-concept—all have roots in three different nervous systems—respectively, the somatic nervous system, the autonomic nervous system and the central nervous system.\n\nAn emerging field in the study of resilience is the neurobiological basis of resilience to stress. For example, neuropeptide Y (NPY) and 5-Dehydroepiandrosterone (5-DHEA) are thought to limit the stress response by reducing sympathetic nervous system activation and protecting the brain from the potentially harmful effects of chronically elevated cortisol levels respectively. In addition, the relationship between social support and stress resilience is thought to be mediated by the oxytocin system's impact on the hypothalamic-pituitary-adrenal axis. \"Resilience, conceptualized as a positive bio-psychological adaptation, has proven to be a useful theoretical context for understanding variables for predicting long-term health and well-being\".\n\nThere is some limited research that, like trauma, resilience is epigenetic—that is, it may be inherited—but the science behind this finding is preliminary.\n\nStudies show that there are several factors which develop and sustain a person's resilience:\n\n\nResilience is negatively correlated with personality traits of neuroticism and negative emotionality, which represents tendencies to see and react to the world as threatening, problematic, and distressing, and to view oneself as vulnerable. Positive correlations stands with personality traits of openness and positive emotionality, that represents tendencies to engage and confront the world with confidence in success and a fair value to self-directedness.\n\nThere is significant research found in scientific literature on the relationship between positive emotions and resilience. Studies show that maintaining positive emotions whilst facing adversity promote flexibility in thinking and problem solving. Positive emotions serve an important function in their ability to help an individual recover from stressful experiences and encounters. That being said, maintaining a positive emotionality aids in counteracting the physiological effects of negative emotions. It also facilitates adaptive coping, builds enduring social resources, and increases personal well-being.\n\nFormation of conscious perception and monitoring one's own socioemotional factors is considered as a stability aspect of positive emotions. This is not to say that positive emotions are merely a by-product of resilience, but rather that feeling positive emotions during stressful experiences may have adaptive benefits in the coping process of the individual. Empirical evidence for this prediction arises from research on resilient individuals who have a propensity for coping strategies that concretely elicit positive emotions, such as benefit-finding and cognitive reappraisal, humor, optimism, and goal-directed problem-focused coping. Individuals who tend to approach problems with these methods of coping may strengthen their resistance to stress by allocating more access to these positive emotional resources. Social support from caring adults encouraged resilience among participants by providing them with access to conventional activities.\n\nPositive emotions not only have physical outcomes but also physiological ones. Some physiological outcomes caused by humor include improvements in immune system functioning and increases in levels of salivary immunoglobulin A, a vital system antibody, which serves as the body's first line of defense in respiratory illnesses. Moreover, other health outcomes include faster injury recovery rate and lower readmission rates to hospitals for the elderly, and reductions in a patient's stay in the hospital, among many other benefits. A study was done on positive emotions in trait-resilient individuals and the cardiovascular recovery rate following negative emotions felt by those individuals. The results of the study showed that trait-resilient individuals experiencing positive emotions had an acceleration in the speed in rebounding from cardiovascular activation initially generated by negative emotional arousal, i.e. heart rate and the like.\n\nGrit refers to the perseverance and passion for long-term goals. This is characterized as working persistently towards challenges, maintained effort and interest over years despite negative feedback, adversity, plateaus in progress, or failure. High grit people view accomplishments as a marathon rather than an immediate goal. High grit individuals display a sustained and focused application of self in problematic situations than less gritty individuals.\n\nGrit affects the effort a person contributes by acting on the importance pathway. When people value a goal as more valuable, meaningful, or relevant to their self-concept they are willing to expend more effort on it when necessary. The influence of individual differences in grit results in different levels of effort-related cardiac activity when gritty and less gritty individuals performed the same task. Grit is associated with differences in potential motivation, one pathway in motivational intensity theory. Grit may also influence an individual's perception of task difficulty.\n\nGrit was highly correlated with the Big Five conscientiousness trait. Although grit and conscientiousness highly overlap in their achievement aspects, they differ in their emphasis. Grit emphasizes long-term stamina, whereas conscientiousness focuses on short-term intensity.\n\nGrit varies with level of education and age. More educated adults tend to be higher in grit than less educated individuals of the same age. Post college graduates report higher grit levels than most other education level groups. Grit increases with age when education level is controlled for.\n\nIn life achievements, grit may be as important as talent. College students at an elite university who scored high in grit also earned higher GPAs than their classmates, despite having lower SAT scores. In a study at the West Point military academy it was found that grit was a more reliable predictor of first summer retention than self-control or a summary measure of cadet quality. Gritty competitors at the Scripps National Spelling Bee outranked other competitors who scored lower in grit, at least partially due to accumulated practice.\n\nGrit may also serve as a protective factor against suicide. A study at Stanford University found that grit was predictive of psychological health and well-being in medical residents. Gritty individuals possess self-control and regular commitment to goals that allows them to resist impulses, such as to engage in self-harm. Individuals high in grit also focus on future goals, which may stop them from attempting suicide. It is believed that because grit encourages individuals to create and sustain life goals, these goals provide meaning and purpose in life. Grit alone does not seem to be sufficient, however. Only individuals with high gratitude and grit have decreased suicidal ideation over long periods of time. Gratitude and grit work together to enhance meaning in life, offering protection against death and suicidal thoughts or plans.\n\nA study was conducted among high achieving professionals who seek challenging situations that require resilience. Research has examined 13 high achievers from various professions, all of whom had experienced challenges in the workplace and negative life events over the course of their careers but who had also been recognized for their great achievements in their respective fields. Participants were interviewed about everyday life in the workplace as well as their experiences with resilience and thriving. The study found six main predictors of resilience: positive and proactive personality, experience and learning, sense of control, flexibility and adaptability, balance and perspective, and perceived social support. High achievers were also found to engage in many activities unrelated to their work such as engaging in hobbies, exercising, and organizing meetups with friends and loved ones.\n\nSeveral factors are found to modify the negative effects of adverse life situations. Many studies show that the primary factor for the development of resilience is social support. While many competing definitions of social support exist, most can be thought of as the degree of access to, and use of, strong ties to other individuals who are similar to one’s self. Social support requires not only that you have relationships with others, but that these relationships involve the presence of solidarity and trust, intimate communication, and mutual obligation both within and outside the family. Additional factors are also associated with resilience, like the capacity to make realistic plans, having self-confidence and a positive self image, developing communications skills, and the capacity to manage strong feelings and impulses.\n\nTemperamental and constitutional disposition is considered as a major factor in resilience. It is one of the necessary precursors of resilience along with warmth in family cohesion and accessibility of prosocial support systems. There are three kinds of temperamental systems that play part in resilience, they are the appetitive system, defensive system and attentional system.\n\nAnother protective factor is related to moderating the negative effects of environmental hazards or a stressful situation in order to direct vulnerable individuals to optimistic paths, such as external social support. More specifically a 1995 study distinguished three contexts for protective factors:\n\nFurthermore, a study of the elderly in Zurich, Switzerland, illuminated the role humor plays as a coping mechanism to maintain a state of happiness in the face of age-related adversity.\n\nBesides the above distinction on resilience, research has also been devoted to discovering the individual differences in resilience. Self-esteem, ego-control, and ego-resiliency are related to behavioral adaptation. For example, maltreated children who feel good about themselves may process risk situations differently by attributing different reasons to the environments they experience and, thereby, avoid producing negative internalized self-perceptions. Ego-control is \"the threshold or operating characteristics of an individual with regard to the expression or containment\" of their impulses, feelings, and desires. Ego-resilience refers to \"dynamic capacity, to modify his or her model level of ego-control, in either direction, as a function of the demand characteristics of the environmental context\"\n\nMaltreated children who experienced some risk factors (e.g., single parenting, limited maternal education, or family unemployment), showed lower ego-resilience and intelligence than nonmaltreated children. Furthermore, maltreated children are more likely than nonmaltreated children to demonstrate disruptive-aggressive, withdraw, and internalized behavior problems. Finally, ego-resiliency, and positive self-esteem were predictors of competent adaptation in the maltreated children.\n\nDemographic information (e.g., gender) and resources (e.g., social support) are also used to predict resilience. Examining people's adaptation after disaster showed women were associated with less likelihood of resilience than men. Also, individuals who were less involved in affinity groups and organisations showed less resilience.\n\nCertain aspects of religions and spirituality may, hypothetically, promote or hinder certain psychological virtues that increase resilience. Research has not established connection between spirituality and resilience. According to the 4th edition of \"Psychology of Religion\" by Hood, et al., the \"study of positive psychology is a relatively new development...there has not yet been much direct empirical research looking specifically at the association of religion and ordinary strengths and virtues\". In a review of the literature on the relationship between religiosity/spirituality and PTSD, amongst the significant findings, about half of the studies showed a positive relationship and half showed a negative relationship between measures of religiosity/spirituality and resilience. The United States Army has received criticism for promoting spirituality in its new [Comprehensive Soldier Fitness] program as a way to prevent PTSD, due to the lack of conclusive supporting data.\n\nIn military studies it has been found that resilience is also dependent on group support: unit cohesion and morale is the best predictor of combat resiliency within a unit or organization. Resilience is highly correlated to peer support and group cohesion. Units with high cohesion tend to experience a lower rate of psychological breakdowns than units with low cohesion and morale. High cohesion and morale enhance adaptive stress reactions.\n\nIn cognitive behavioral therapy, building resilience is a matter of mindfully changing basic behaviors and thought patterns. The first step is to change the nature of self-talk. Self-talk is the internal monologue people have that reinforce beliefs about the person's self-efficacy and self-value. To build resilience, the person needs to eliminate negative self-talk, such as \"I can't do this\" and \"I can't handle this\", and to replace it with positive self-talk, such as \"I can do this\" and \"I can handle this\". This small change in thought patterns helps to reduce psychological stress when a person is faced with a difficult challenge. The second step a person can take to build resilience is to be prepared for challenges, crises, and emergencies. In business, preparedness is created by creating emergency response plans, business continuity plans, and contingency plans. For personal preparedness, the individual can create a financial cushion to help with economic crises, he/she can develop social networks to help him/her through trying personal crises, and he/she can develop emergency response plans for his/her household.\n\nResilience is also enhanced by developing effective coping skills for stress. Coping skills help the individual to reduce stress levels, so they remain functional. Coping skills include using meditation, exercise, socialization, and self-care practices to maintain a healthy level of stress, but there are many other lists associated with psychological resilience.\n\nThe American Psychological Association suggests \"10 Ways to Build Resilience\", which are:\n\n\nThe Besht model of natural resilience building in an ideal family with positive access and support from family and friends, through parenting illustrates four key markers. They are:\n\n\nIn this model, self-efficacy is the belief in one's ability to organize and execute the courses of action required to achieve necessary and desired goals and hardiness is a composite of interrelated attitudes of commitment, control, and challenge.\n\nA number of self-help approaches to resilience-building have been developed, drawing mainly on the theory and practice of cognitive behavioral therapy (CBT) and rational emotive behavior therapy (REBT). For example, a group cognitive-behavioral intervention, called the Penn Resiliency Program (PRP), has been shown to foster various aspects of resilience. A meta-analysis of 17 PRP studies showed that the intervention significantly reduces depressive symptoms over time.\n\nThe idea of 'resilience building' is debatably at odds with the concept of resilience as a process, since it is used to imply that it is a developable characteristic of oneself. Those who view resilience as a description of doing well despite adversity, view efforts of 'resilience building' as method to encourage resilience. Bibliotherapy, positive tracking of events, and enhancing psychosocial protective factors with positive psychological resources are other methods for resilience building. In this way, increasing an individual's resources to cope with or otherwise address the negative aspects of risk or adversity is promoted, or builds, resilience.\n\nContrasting research finds that strategies to regulate and control emotions, in order to enhance resilience, allows for better outcomes in the event of mental illness. While initial studies of resilience originated with developmental scientists studying children in high-risk environments, a study on 230 adults diagnosed with depression and anxiety that emphasized emotional regulation, showed that it contributed to resilience in patients. These strategies focused on planning, positively reappraising events, and reducing rumination helped in maintaining a healthy continuity. Patients with improved resilience were found to yield better treatment outcomes than patients with non-resilience focused treatment plans, providing potential information for supporting evidence based psychotherapeutic interventions that may better handle mental disorders by focusing on the aspect of psychological resilience.\n\nThe Head Start program was shown to promote resilience. So was the Big Brothers Big Sisters Programme, the Abecedarian Early Intervention Project, and social programs for youth with emotional or behavioral difficulties.\n\nTuesday's Children, a family service organization that made a long-term commitment to the individuals that have lost loved ones to 9/11 and terrorism around the world, works to build psychological resilience through programs such as Mentoring and Project COMMON BOND, an 8-day peace-building and leadership initiative for teens, ages 15–20, from around the world who have been directly impacted by terrorism.\n\nMilitary organizations test personnel for the ability to function under stressful circumstances by deliberately subjecting them to stress during training. Those students who do not exhibit the necessary resilience can be screened out of the training. Those who remain can be given stress inoculation training. The process is repeated as personnel apply for increasingly demanding positions, such as special forces.\n\nResilience in children refers to individuals who are doing better than expected, given a history that includes risk or adverse experience. Once again, it is not a trait or something that some children simply possess. There is no such thing as an 'invulnerable child' that can overcome any obstacle or adversity that he or she encounters in life—and in fact, the trait is quite common. Resilience is the product of a number of developmental processes over time, that has allowed children experience small exposures to adversity or some sort of age appropriate challenges to develop mastery and continue to develop competently. This gives children a sense of personal pride and self-worth.\n\nResearch on 'protective factors', which are characteristics of children or situations that particularly help children in the context of risk has helped developmental scientists to understand what matters most for resilient children. Two of these that have emerged repeatedly in studies of resilient children are good cognitive functioning (like cognitive self-regulation and IQ) and positive relationships (especially with competent adults, like parents). Children who have protective factors in their lives tend to do better in some risky contexts when compared to children without protective factors in the same contexts. However, this is not a justification to expose any child to risk. Children do better when not exposed to high levels of risk or adversity.\n\nResilient children within classroom environments have been described as working and playing well and holding high expectations, have often been characterized using constructs such as locus of control, self-esteem, self-efficacy, and autonomy. All of these things work together to prevent the debilitating behaviors that are associated with learned helplessness.\n\nCommunities play a huge role in fostering resilience. The clearest sign of a cohesive and supportive community is the presence of social organizations that provide healthy human development. Services are unlikely to be used unless there is good communication concerning them. Children who are repeatedly relocated do not benefit from these resources, as their opportunities for resilience-building, meaningful community participation are removed with every relocation.\n\nFostering resilience in children requires family environments that are caring and stable, hold high expectations for children's behavior and encourage participation in the life of the family. Most resilient children have a strong relationship with at least one adult, not always a parent, and this relationship helps to diminish risk associated with family discord. The definition of parental resilience, as the capacity of parents to deliver a competent and quality level of parenting to children, despite the presence of risk factors, has proven to be a very important role in children's resilience. Understanding the characteristics of quality parenting is critical to the idea of parental resilience. Even if divorce produces stress, the availability of social support from family and community can reduce this stress and yield positive outcomes. Any family that emphasizes the value of assigned chores, caring for brothers or sisters, and the contribution of part-time work in supporting the family helps to foster resilience. Resilience research has traditionally focused on the well being of children, with limited academic attention paid to factors that may contribute to the resilience of parents.\n\nNumerous studies have shown that some practices that poor parents utilize help promote resilience within families. These include frequent displays of warmth, affection, emotional support; reasonable expectations for children combined with straightforward, not overly harsh discipline; family routines and celebrations; and the maintenance of common values regarding money and leisure. According to sociologist Christopher B. Doob, \"Poor children growing up in resilient families have received significant support for doing well as they enter the social world—starting in daycare programs and then in schooling.\"\n\nBeyond preventing bullying, it is also important to consider how interventions based on emotional intelligence (EI) are important in the case that bullying does occur. Increasing EI may be an important step in trying to foster resilience among victims. When a person faces stress and adversity, especially of a repetitive nature, their ability to adapt is an important factor in whether they have a more positive or negative outcome.\n\nA 2013 study examined adolescents who illustrated resilience to bullying and found some interesting gendered differences, with higher behavioral resilience found among girls and higher emotional resilience found among boys. Despite these differences, they still implicated internal resources and negative emotionality in either encouraging or being negatively associated with resilience to bullying respectively and urged for the targeting of psychosocial skills as a form of intervention. Emotional intelligence has been illustrated to promote resilience to stress and as mentioned previously, the ability to manage stress and other negative emotions can be preventative of a victim going on to perpetuate aggression. One factor that is important in resilience is the regulation of one's own emotions. Schneider et al. (2013) found that emotional perception was significant in facilitating lower negative emotionality during stress and Emotional Understanding facilitated resilience and has a positive correlation with positive affect.\n\nTransgender youth experience a wide range of abuse and lack of understanding from the people in their environment and are better off with a high resilience to deal with their lives. A study was done looking at 55 transgender youths studying their sense of personal mastery, perceived social support, emotion-oriented coping and self-esteem. It was seen that around 50% of the variation in the resilience aspects accounted for the problematic issues of the teens. This means that transgender youths with lower resilience were more prone to mental health issues, including depression and trauma symptoms. Emotion-oriented coping was a strong aspect of resilience in determining how depressed the individuals were.\n\nPregnancies among adolescents are considered as a complication, as they favour education interruption, poor present and future health, higher rates of poverty, problems for present and future children, among other negative outcomes.\n\nInvestigators from the Ecuadorian Catholic University (Universidad Católica de Santiago de Guayaquil) (Guayaquil) and the Spanish University of Zaragoza (Zaragoza), performed a comparative study at the Enrique C. Sotomayor Obstetric and Gynecology Hospital (Guayaquil) assessing resilience differences between pregnant adolescents and adults.\n\nA 56.6% of gravids presented total CESD-10 scores 10 or more indicating depressed mood. Despite this, total CESD-10 scores and depressed mood rate did not differ among studied groups. Adolescents did, however, display lower resilience reflected by lower total resilience scores and a higher rate of scores below the calculated median (P < 0.05). Logistic regression analysis could not establish any risk factor for depressed mood among studied subjects; however, having an adolescent partner and a preterm delivery related to a higher risk for lower resilience.\n\nOftentimes divorce is viewed as detrimental to one's emotional health, but studies have shown that cultivating resilience may be beneficial to all parties involved. The level of resilience a child will experience after their parents have split is dependent on both internal and external variables. Some of these variables include their psychological and physical state and the level of support they receive from their schools, friends, and family friends. The ability to deal with these situations also stems from the child's age, gender, and temperament. Children will experience divorce differently and thus their ability to cope with divorce will differ too. About 20–25% of children will \"demonstrate severe emotional and behavioral problems\" when going through a divorce. This percentage is notably higher than the 10% of children exhibiting similar problems in married families. Despite having divorces parents of approximately 75–80% of these children will \"develop into well-adjusted adults with no lasting psychological or behavioral problems\". This comes to show that most children have the tools necessary to allow them to exhibit the resilience needed to overcome their parents' divorce.\n\nThe effects of the divorce extend past the separation of both parents. The remaining conflict between parents, financial problems, and the re-partnering or remarriage of parents can cause lasting stress. Studies conducted by Booth and Amato (2001) have shown that there is no correlation between post-divorce conflict and the child's ability to adjust to their life circumstance. On the other hand, Hetherington (1999) completed research on this same topic and did find adverse effects in children. In regards to the financial standing of a family, divorce does have the potential to reduce the children's style of living. Child support is often given to help cover basic needs such as schooling. If the parents' finances are already scarce then their children may not be able to participate in extracurricular activities such as sports and music lessons, which can be detrimental to their social lives.\n\nRepartnering or remarrying can bring in additional levels of conflict and anger into their home environment. One of the reasons that re-partnering causes additional stress is because of the lack of clarity in roles and relationships; the child may not know how to react and behave with this new \"parent\" figure in their life. In most cases, bringing in a new partner/spouse will be the most stressful when done shortly after the divorce. In the past, divorce had been viewed as a \"single event\", but now research shows that divorce encompasses multiple changes and challenges. It is not only internal factors that allow for resiliency, but the external factors in the environment are critical for responding to the situation and adapting. Certain programs such as the 14-week Children's Support Group and the Children of Divorce Intervention Program may help a child cope with the changes that occur from a divorce.\n\nResilience after a natural disaster can be gauged in a number of different ways. It can be gauged on an individual level, a community level, and on a physical level. The first level, the individual level, can be defined as each independent person in the community. The second level, the community level, can be defined as all those inhabiting the locality affected. Lastly, the physical level can be defined as the infrastructure of the locality affected.\n\nUNESCAP funded research on how communities show resiliency in the wake of natural disasters. They found that, physically, communities were more resilient if they banded together and made resiliency an effort of the whole community. Social support is key in resilient behavior, and especially the ability to pool resources. In pooling social, natural, and economic resources, they found that communities were more resilient and able to over come disasters much faster than communities with an individualistic mindset.\n\nThe World Economic Forum met in 2014 to discuss resiliency after natural disasters. They conclude that countries that are more economically sound, and have more individuals with the ability to diversify their livelihoods, will show higher levels of resiliency. This has not been studied in depth yet, but the ideas brought about through this forum appear to be fairly consistent with already existing research.\n\nLittle research has been done on the topic of family resilience in the wake of the death of a family member. Traditionally, clinical attention to bereavement has focused on the individual mourning process rather than on those of the family unit as a whole. Resiliency is distinguished from recovery as the \"ability to maintain a stable equilibrium\" which is conducive to balance, harmony, and recovery. Families must learn to manage familial distortions caused by the death of the family member, which can be done by reorganizing relationships and changing patterns of functioning to adapt to their new situation. Exhibiting resilience in the wake of trauma can successfully traverse the bereavement process without long-term negative consequences.\n\nOne of the healthiest behaviors displayed by resilient families in the wake of a death is honest and open communication. This facilitates an understanding of the crisis. Sharing the experience of the death can promote immediate and long-term adaptation to the recent loss of a loved one. Empathy is a crucial component in resilience because it allows mourners to understand other positions, tolerate conflict, and be ready to grapple with differences that may arise. Another crucial component to resilience is the maintenance of a routine that helps to bind the family together through regular contact and order. The continuation of education and a connection with peers and teachers at school is an important support for children struggling with the death of a family member.\n\nBrad Evans and Julian Reid criticize resilience discourse and its rising popularity in their book, \"Resilient Life\". The authors assert that policies of resilience can put the onus of disaster response on individuals rather than publicly coordinated efforts. Tied to the emergence of neoliberalism, climate change theory, third-world development, and other discourses, Evans and Reid argue that promoting resilience draws attention away from governmental responsibility and towards self-responsibility and healthy psychological affects such as \"posttraumatic growth\".\n\nAnother criticism regarding resilience is its definition. Like other psychological phenomena, by defining specific psychological and affective states in certain ways, controversy over meaning will always ensue. How the term resilience is defined affects research focuses; different or insufficient definitions of resilience will lead to inconsistent research about the same concepts. Research on resilience has become more heterogeneous in its outcomes and measures, convincing some researchers to abandon the term altogether due to it being attributed to all outcomes of research where results were more positive than expected.\n\nThere is also controversy about the indicators of good psychological and social development when resilience is studied across different cultures and contexts. The American Psychological Association's Task Force on Resilience and Strength in Black Children and Adolescents, for example, notes that there may be special skills that these young people and families have that help them cope, including the ability to resist racial prejudice. Researchers of indigenous health have shown the impact of culture, history, community values, and geographical settings on resilience in indigenous communities. People who cope may also show \"hidden resilience\" when they don't conform with society's expectations for how someone is supposed to behave (in some contexts, aggression may be required to cope, or less emotional engagement may be protective in situations of abuse). Recently there has also been evidence that resilience can indicate a capacity to resist a sharp decline in other harm even though a person temporarily appears to get worse.<ref name=\"doi10.1177/0044118X03257030\"></ref>\n\n\n"}
{"id": "58045840", "url": "https://en.wikipedia.org/wiki?curid=58045840", "title": "Rebecca Goss (chemist)", "text": "Rebecca Goss (chemist)\n\nRebecca Jane Miriam Goss is a professor of organic chemistry at the University of St. Andrews who won the 2006 Royal Society of Chemistry Meldola Medal. She is known for combining synthetic biology and chemistry for medicinal purposes.\n\nGoss completed her undergraduate studies in Chemistry at the University of Durham in 1997. Under the supervision of Prof. David O'Hagan, Goss completed her Ph.D. studying the biosynthesis of various natural products including the stereochemistry of enzymatic fluorination in fluoroacetate biosynthesis.\n\nGoss specialises in the biosynthesis of natural products at the chemical and genetic level.\n\nGoss joined the University of Cambridge in 2000 to study the chemistry and molecular biology of polyketide biosynthesis in the research group of Professors Jim Staunton (FRS) and Peter Leadlay (FRS). She held a one year teaching fellowship at the School of Chemistry, University of Nottingham between 2002 to 2003 before obtaining a lectureship at the School of Biological and Chemical Science, University of Exeter in 2003. Between 2005 and 2010 Goss held a lectureship at the University of East Anglia before being promoted to senior lecturer in 2010 and then reader in organic chemistry in 2012. Goss moved to the University of St. Andrews in 2012 to become a reader in biomolecular and organic chemistry. In 2018 she became the first woman to be appointed professor of organic chemistry in St Andrews' 600 year history.\n\nShe was awarded the Royal Society of Chemistry Meldola prize for her work to understand the interface of organic chemistry and molecular biology. In 2013 she was awarded the Royal Society of Chemistry Natural Product Report Emerging Researcher Lectureship for her pioneering approach to 'Genochemetics', which combines synthetic biology and chemistry for medicinal purposes. In 2014 she was awarded an ERC consolidator grant.\n\nGoss is on the advisory board for the peer-reviewed journals Chemical Communications and Natural Product Reports.\n\n"}
{"id": "33783147", "url": "https://en.wikipedia.org/wiki?curid=33783147", "title": "Robert T. Lackey", "text": "Robert T. Lackey\n\nRobert T. Lackey (born 1944) is a Canadian born fisheries scientist and political scientist living in the United States. He is best known for his work involving the interplay between science and policy, natural resource management, and assessments of the future of salmon runs. Lackey is a professor of fisheries and wildlife and adjunct professor of political science at Oregon State University. From 1981-2008, he held senior leadership posts at the United States Environmental Protection Agency research laboratory in Corvallis, Oregon.\n\nRobert Thomas Lackey received a B.S. (fisheries) from Humboldt State University (California) in 1967. He then entered the Zoology graduate program at the University of Maine to study “Seasonal abundance and availability of forage fishes and their utilization by landlocked Atlantic salmon and brook trout in Echo Lake, Mount Desert Island, Maine“ under the advisement of Professor W. Harry Everhart. After obtaining an M.S. (Zoology) in 1968, Lackey enrolled in the graduate program at Colorado State University to pursue a doctorate. His research on the “Effects of artificial destratification on a lake ecosystem“ was also supervised by Professor Everhart who had recently left the University of Maine to become head of the fisheries program at Colorado State University. In 1971, Lackey was awarded a PhD (Fisheries and Wildlife) and was hired immediately by Virginia Tech (Blacksburg, Virginia) as an assistant professor of fisheries. In 1973, he was promoted to Associate Professor at Virginia Tech. In 1976-77, he spent a sabbatical year in Washington, D.C., working with the United States Fish and Wildlife Service Environment Program as the national program coordinator. In 1977, he returned to Virginia Tech and resumed teaching and research. He left Virginia Tech in 1979 to assume leadership of the U.S. Fish and Wildlife Service’s National Water Resources Analysis Group located in Leetown, West Virginia. In 1981, he accepted a job as senior biologist with the U.S. Environmental Protection Agency research laboratory in Corvallis, Oregon. In 1982, he became Courtesy Professor in the Department of Fisheries and Wildlife at Oregon State University, a position that he continues to hold. Lackey’s career with the EPA laboratory in Corvallis included a variety of senior leadership posts, including Deputy Director, a position he held from 1989 to 2000. In 1999, Lackey was awarded a senior Fulbright Fellowship and spend his tenure at the University of Northern British Columbia in Prince George. Lackey retired from the Environmental Protection Agency in 2008 to work at Oregon State University.\n\nLackey career evolved from his early focus on solving practical fisheries problems (improving fish yields using various novel habitat enhancement techniques) to his more recent efforts to better define the appropriate role of scientific information and scientists in natural resource policy. Lackey’s leadership in developing policy options to restore wild salmon to the west coast of the United States is well known and controversial. It has been criticized for being overly pessimistic, but is regarded by others as a blunt assessor of reality. In 2005, he completed a 4-year collaborative effort, the Salmon 2100 Project. This project mobilized three dozen senior scientists and policy exports to develop practical policy options that, if implemented, would restore wild salmon runs in California, Oregon, Washington, Idaho, and southern British Columbia. The book of the same title was published in 2006. He lectures widely advocating the view that scientists should be vigilant about keeping their personal policy preferences out of their scientific activities. He is also a proponent of the view that the pervasive use of normative science is undermining the credibility of the scientific enterprise. His current work focuses on education, especially developing online graduate courses in ecological and natural resource policy.\n\nBooks\n\nLackey, Robert T. 1974. \"Introductory Fisheries Science\". Sea Grant, Virginia Polytechnic Institute and State University, Blacksburg, Virginia, 280 pp. \nhttp://blogs.oregonstate.edu/lackey/files/2017/08/Intro-Fisheries-Science.pdf\n\nLackey, Robert T., and Wayne A. Hubert. Editors. 1978. \"Analysis of Exploited Fish Populations\". Sea Grant, Virginia Polytechnic Institute and State University, Blacksburg, Virginia, 97 pp. \nhttp://blogs.oregonstate.edu/lackey/files/2017/07/Analysis-of-Exploited-Fish-Populations.pdf\n\nLackey, Robert T., and Larry A. Nielsen. Editors. 1980. \"Fisheries Management\". John Wiley & Sons, New York, New York, 422 pp. \nhttp://blogs.oregonstate.edu/lackey/files/2017/07/14.-Fisheries-Management.pdf\n\nMazaika, Rosemary, Robert T. Lackey, and Stephen L. Friant. Editors. 1995. \"Ecological Risk Assessment: Use, Abuse, and Alternatives\". Amherst Scientific Publishers, Amherst, Massachusetts. \nhttp://blogs.oregonstate.edu/lackey/files/2017/07/13.-Ecological-Risk-Assessment-Use-Abuse-and-Alternatives.pdf\n\nLackey, Robert T., Denise H. Lach, and Sally L. Duncan. Editors. 2006. \"Salmon 2100: The Future of Wild Pacific Salmon\". American Fisheries Society, Bethesda, Maryland, 629 pp. \nhttp://blogs.oregonstate.edu/lackey/files/2017/07/The-Future-of-Wild-Pacific-Salmon.pdf\n\nJournal Publications\n\nLackey, Robert T. 1998. Seven pillars of ecosystem management. \"Landscape and Urban Planning\". 40(1-3): 21-30. \nhttp://blogs.oregonstate.edu/lackey/files/2017/07/37.-Seven-Pillars-of-Ecosystem-Management.pdf\n\nLackey, Robert T. 2003. Pacific Northwest salmon: forecasting their status in 2100. \"Reviews in Fisheries Science\". 11(1): 35-88. \nhttp://blogs.oregonstate.edu/lackey/files/2017/08/Salmon-Forecasting-to-2100.pdf\n\nLackey, Robert T. 2006. Axioms of ecological policy. \"Fisheries\". 31(6): 286-290. \nhttp://blogs.oregonstate.edu/lackey/files/2017/07/2006f-Axioms-of-Ecological-Policy-Reprint-Lackey.pdf\n\nLackey, Robert T. 2007. Science, scientists, and policy advocacy. \"Conservation Biology\". 21(1): 12-17. \nhttp://blogs.oregonstate.edu/lackey/files/2017/07/2007a-Science-Scientists-and-Policy-Advocacy-Reprint-Lackey-1.pdf\n\nLackey, Robert T. 2009. Challenges to sustaining diadromous fishes through 2100: lessons learned from western North America. pp. 609–617. In: Haro, A., et al., editors. \"Challenges for Diadromous Fishes in a Dynamic Global Environment\", American Fisheries Society. \nhttp://blogs.oregonstate.edu/lackey/files/2017/07/Challenges-to-Sustaining-Diadromous-Fishes-Through-2100.-.pdf\n\nOregon State University (2017)\nhttp://blogs.oregonstate.edu/lackey/\n\nEPA (2011)\nhttp://www.epa.gov/wed/pages/staff/lackey/\n\nHumboldt State University (2008)\nhttp://www.humboldt.edu/green/people/people/lackey.php\n\nEPA (1999)\nhttp://yosemite.epa.gov/opa/admpress.nsf/b1ab9f485b098972852562e7004dc686/8053a4a97e8c3bd4852567a1006e7b71?OpenDocument\n\nGoogle Scholar\nhttps://scholar.google.co.nz/scholar?hl=en&num=100&q=author%3A%22Robert+T+Lackey%22&btnG=Search&as_sdt=0%2C38&as_ylo=&as_vis=0\n\nOregonian Newspaper (2009)\nhttp://www.oregonlive.com/opinion/index.ssf/2009/09/facing_the_facts_on_the_future.html\n\nMartin (2011)\nhttp://articles.sfgate.com/2011-07-25/bay-area/29811577_1_wild-salmon-salmon-numbers-salmon-producing\n\nMSNBC (2005)\nhttp://www.msnbc.msn.com/id/9239431/ns/technology_and_science-science/t/population-numbers-may-doom-salmon/\n\nAmerican Fisheries Society (2011)\nhttp://fisheries.org/shop/x55050xm\n\nDunbar (2008)\nhttp://www.modbee.com/latest-news/article3103148.html\n\nOregon State University (2013)\nhttp://oregonstate.edu/terra/2013/01/normative-science/\n\n"}
{"id": "44592731", "url": "https://en.wikipedia.org/wiki?curid=44592731", "title": "Scanning transmission X-ray microscopy", "text": "Scanning transmission X-ray microscopy\n\nScanning transmission X-ray microscopy (STXM) is a type of X-ray microscopy in which a zone plate focuses an X-ray beam onto a small spot, a sample is scanned in the focal plane of the zone plate and the transmitted X-ray intensity is recorded as a function of the sample position. A stroboscopic scheme is used where the excitation is the pump and the synchrotron X-ray flashes are the probe. X-ray microscopes work by exposing a film or charged coupled device detector to detect X-rays that pass through the specimen. The image formed is of a thin section of specimen. Newer X-ray microscopes use X-ray absorption spectroscopy to heterogeneous materials at high spatial resolution. The essence of the technique is a combination of spectromicroscopy, imaging with spectral sensitivity, and microspectroscopy, recording spectra from very small spots.\n\nElectron energy loss spectroscopy (EELS) in combination with transmission electron microscopy has modest spectral resolution and is rather damaging to the sample material. STXM with variable X-ray energy gives high spectral resolution. Radiation damage effects are typically two orders of magnitude lower than for EELS. Radiation concerns are also relevant with organic materials.\n\nUnlike other methods such as electron microscopy, the spectra samples with water and carbon can be obtained. STXM run at atmospheric pressure allows for convenient sample installation and fewer restrictions on sample preparation. Cells have even been built which can examine hydrated precipitates and solutions.\n\nIn order to obtain spectromicroscopy data the following operating procedure is followed. The desired monochromator grating is selected along with photon energy in the middle of NEXAFS range. Refocus mirrors are adjusted to put the beam into the microscope and steered to maximize the flux passing through the zone plate. A pinhole is placed in the photon beam upstream in a transverse position to maximize transmission. Pinhole size is determined by demagnification to the size of the diffraction limit of the zone plate lens. An undersized pinhole is often used to reduce intensity which controls radiation damage. The order sorting aperture is positioned to eliminate transmission of unfocused zero order light, which would blur the image. Then an x/y line scan is defined across an intensity variation in the image. The x/y line scans are repeated with varying focus conditions. Adsorption spectra can also be obtained with a stationary photon spot.\n\nSTXM has been used to study reinforce filler particles used in molded compressed polyurethane foams in the automotive and fishing industries to achieve higher load bearing capability. Two types of polymers, copolymer styrene and acrylonitrile (SAN) and aromatic-carbamate rich poly-isocyanate poly-addition (PIPA), are chemically indistinguishable by transmission electron spectroscopy. With NEXAFS, spectra of SAN and PIPA absorb strongly at 285.0 eV associated with the phenyl groups of the aromatic filler particles and thus show the same electron spectroscopy image. Only SAN has a strong absorption at 286.7 eV due to the acrylonitrile component. NEXAFS can be a quick and reliable means to differentiate chemical species at a sub-micron spatial scale.\n\nSTXM which uses near-edge X-ray absorption spectroscopy is able to be applied to fully hydrated biological molecules due to the ability of X-rays to penetrate water. Soft X-rays also provide spatial resolution better than 50 nm which is suitable for bacterial and bacterial microfilms. With this, quantitative chemical mapping at a spatial scale below 50 nm may be achieved. Soft X-rays also interact with almost all elements and allow mapping of chemical species based on bonding structure. STXM allows for study of a variety of questions regarding the nature, distribution, and role of protein, carbohydrate, lipid, and nucleic acid in biofilms, especially in the extracellular matrix. The study of these biofilms is useful for environmental remediation applications.\n"}
{"id": "4218673", "url": "https://en.wikipedia.org/wiki?curid=4218673", "title": "Spatial relation", "text": "Spatial relation\n\nA spatial relation, specifies how some object is located in space in relation to some reference object. When the reference object is much bigger than the object to locate, the latter is often represented by a point. The reference object is often represented by a bounding box.\n\nIn Anatomy it might be the case that a spatial relation is not fully applicable. Thus, the degree of applicability is defined which specifies from 0 till 100% how strongly a spatial relation holds. Often researchers concentrate on defining the applicability function for various spatial relations.\n\nIn spatial databases and Geospatial topology the \"spatial relations\" are used for \"spatial analysis\" and constraint specifications.\n\nIn cognitive development for walk and for catch objects, or for understand objects-behaviour; in robotic Natural Features Navigation; and many other areas, \"spatial relations\" plays a central role.\n\nCommonly used types of \"spatial relations\" are: \"topological\", \"directional\" and \"distance\" relations.\n\nThe DE-9IM model expresses important \"space relations\" which are invariant to rotation, translation and scaling transformations.\n\nFor any two spatial objects \"a\" and \"b\", that can be points, lines and/or polygonal areas, there are 9 relations derived from \"DE-9IM\":\nDirectional relations can again be differentiated into external directional relations and internal directional relations. An internal directional relation specifies where an object is located inside the reference object while an external relations specifies where the object is located outside of the reference objects.\n\n\nDistance relations specify how far is the object away from the reference object.\n\nReference objects represented by a bounding box or another kind of \"spatial envelope\" that encloses its borders, can be denoted with the maximum number of dimensions of this envelope: 0 for punctual objects, 1 for linear objects, 2 for planar objects, 3 for volumetric objects. So, any object, in a 2D modeling, can by classified as \"point\", \"line\" or \"area\" according to its delimitation. Then, a \"type of spatial relation\" can be expressed by the class of the objects that participate in the relation:\n\n\nMore \"complex\" modeling schemas can represent an object as a composition of \"simple sub-objects\". Examples: represent in a astronomical map a star by a \"point\" and a binary star by \"two points\"; represent in geographical map a river with a \"line\", for its source stream, and with an strip-\"area\", for the rest of the river. These schemas can use the above classes, uniform composition classes (\"multi-point\", \"multi-line\" and \"multi-area\") and heterogeneous composition (\"points\"+\"lines\" as \"object of dimension 1\", \"points\"+\"lines\"+\"areas\" as \"object of dimension 2\").\n\nTwo internal components of a \"complex object\" can express (the above) binary relations between them, and ternary relations, using the whole object as a frame of reference. Some relations can be expressed by an abstract component, such the center of mass of the binary star, or a center line of the river.\n\nFor human thinking, spatial relations include qualities like size, distance, volume, order, and, also, time:\n\nStockdale and Possin discusses the many ways in which people with difficulty establishing spatial and temporal relationships can face problems in ordinary situations.\n\n"}
{"id": "32754632", "url": "https://en.wikipedia.org/wiki?curid=32754632", "title": "Sputnik 41", "text": "Sputnik 41\n\nSputnik 41 (, ), also known as Sputnik Jr 2 and Radio Sputnik 18 (RS-18), was a Franco-Russian amateur radio satellite which was launched in 1998 to commemorate the hundredth anniversary of the Aéro-Club de France, and the forty-first anniversary of the launch of Sputnik 1, the world's first artificial satellite. A one-third scale model of Sputnik 1, Sputnik 41 was deployed from the Mir space station on 10 November 1998.\n\nSputnik 41 was launched aboard Progress M-40 at 04:14 UTC on 25 October 1998, along with supplies for Mir and the Znamya-2.5 reflector experiment. A Soyuz-U carrier rocket placed the spacecraft into orbit, flying from Site 1/5 at the Baikonur Cosmodrome in Kazakhstan: the same launch pad used by Sputnik 1. Progress M-40 docked to Mir on 27 October, and the satellite was transferred to the space station. At about 19:30 UTC on 10 November, during an extra-vehicular activity, Sputnik 41 was deployed by cosmonauts Gennady Padalka and Sergei Avdeyev.\n\nOn 24 November, a fortnight after deployment, Sputnik 41 was in a low Earth orbit with a perigee of , an apogee of , an inclination of 51.6 degrees, and a period of 91.44 minutes. The satellite was given the International Designator 1998-062C, and was catalogued by the United States Space Command as 25533. Having ceased operations on 11 December 1998 after its batteries expired, Sputnik 41 decayed from orbit on 11 January 1999.\n\nSputnik 41 was originally intended to be built aboard Mir, based on a satellite launched in October 1997 as a backup for Sputnik 40. That spacecraft had been stored aboard the space station for a year after the successful deployment of Sputnik 40, and it was intended that it would be fitted with upgraded electronics and deployed. By the time of launch, the project had grown to involve a complete satellite, and the Sputnik 40 backup was never deployed.\n"}
{"id": "32444086", "url": "https://en.wikipedia.org/wiki?curid=32444086", "title": "Stephen Blackmore", "text": "Stephen Blackmore\n\nStephen Blackmore CBE FRSE FRSB FLS (born 30 July 1952) is a British botanist, who was educated at St. George's School, Hong Kong and the University of Reading where he completed his PhD in 1976. He then worked at the Royal Society of London’s Research Station on Aldabra Atoll in the Indian Ocean before being appointed Lecturer in Biology and Head of the National Herbarium and Botanic Garden at the University of Malawi. In 1980, he was appointed Head of Palynology at Natural History Museum in London and from 1990 to 1999 served there as Keeper of Botany. He was the 15th Regius Keeper of the Royal Botanic Garden Edinburgh from 1999 until 20 December 2013, and was appointed Her Majesty's Botanist in Scotland in 2010.\n\nBlackmore has received three awards from the Linnean Society of London: the Trail-Crisp Medal Award in 1987; the Bicentenary Medal of the Linnean Society in 1992; and the Linnean Medal in 2012. The Royal Caledonian Horticultural Society awarded him the Scottish Horticultural Medal in 2008, and the Royal Horticultural Society presented him the Victoria Medal of Honour in 2012. In the 2011 New Year Honours list he was appointed a CBE for \"services to plant conservation\".\n\nIn 2013 Blackmore was appointed Chairman of the Darwin Expert Committee of the Darwin Initiative and, since 2014, he has also been Chairman of Botanic Gardens Conservation International. He serves on the board of the Seychelles Islands Foundation.\n"}
{"id": "28521958", "url": "https://en.wikipedia.org/wiki?curid=28521958", "title": "Streetlight effect", "text": "Streetlight effect\n\nThe streetlight effect, or the drunkard's search principle, is a type of observational bias that occurs when people only search for something where it is easiest to look. Both names refer to a well-known joke:\n\nThe anecdote goes back at least to the 1920s,\nand has been used metaphorically in the social sciences since at least 1964, when Abraham Kaplan referred to it as \"the principle of the drunkard's search\". The anecdote has also been attributed to Nasreddin. According to Idries Shah, this tale is used by many Sufis, commenting upon people who seek exotic sources for enlightenment.\n\n\n"}
{"id": "31429", "url": "https://en.wikipedia.org/wiki?curid=31429", "title": "Twin paradox", "text": "Twin paradox\n\nIn physics, the twin paradox is a thought experiment in special relativity involving identical twins, one of whom makes a journey into space in a high-speed rocket and returns home to find that the twin who remained on Earth has aged more. This result appears puzzling because each twin sees the other twin as moving, and so, according to an incorrect and naive application of time dilation and the principle of relativity, each should paradoxically find the other to have aged less. However, this scenario can be resolved within the standard framework of special relativity: the travelling twin's trajectory involves two different inertial frames, one for the outbound journey and one for the inbound journey, and so there is no symmetry between the spacetime paths of the twins. Therefore, the twin paradox is not a paradox in the sense of a logical contradiction. \n\nStarting with Paul Langevin in 1911, there have been various explanations of this paradox. These explanations \"can be grouped into those that focus on the effect of different standards of simultaneity in different frames, and those that designate the acceleration [experienced by the travelling twin] as the main reason\". Max von Laue argued in 1913 that since the traveling twin must be in two separate inertial frames, one on the way out and another on the way back, this frame switch is the reason for the aging difference, not the acceleration \"per se\". Explanations put forth by Albert Einstein and Max Born invoked gravitational time dilation to explain the aging as a direct effect of acceleration. General relativity is not necessary to explain the twin paradox; special relativity alone can explain the phenomenon.\n\nTime dilation has been verified experimentally by precise measurements of atomic clocks flown in aircraft and satellites. For example, gravitational time dilation and special relativity together have been used to explain the Hafele–Keating experiment. It was also confirmed in particle accelerators by measuring the time dilation of circulating particle beams.\n\nIn his famous paper on special relativity in 1905, Albert Einstein deduced that when two clocks were brought together and synchronized, and then one was moved away and brought back, the clock which had undergone the traveling would be found to be lagging behind the clock which had stayed put. Einstein considered this to be a natural consequence of special relativity, not a paradox as some suggested, and in 1911, he restated and elaborated on this result as follows (with physicist Robert Resnick's comments following Einstein's):\n\nIn 1911, Paul Langevin gave a \"striking example\" by describing the story of a traveler making a trip at a Lorentz factor of (99.995% the speed of light). The traveler remains in a projectile for one year of his time, and then reverses direction. Upon return, the traveler will find that he has aged two years, while 200 years have passed on Earth. During the trip, both the traveler and Earth keep sending signals to each other at a constant rate, which places Langevin's story among the Doppler shift versions of the twin paradox. The relativistic effects upon the signal rates are used to account for the different aging rates. The asymmetry that occurred because only the traveler underwent acceleration, is used to explain why there is any difference at all, because \"any change of velocity, or any acceleration has an absolute meaning\".\n\nMax von Laue (1911, 1913) elaborated on Langevin's explanation. Using Hermann Minkowski's spacetime formalism, Laue went on to demonstrate that the world lines of the inertially moving bodies maximize the proper time elapsed between two events. He also wrote that the asymmetric aging is completely accounted for by the fact that the astronaut twin travels in two separate frames, while the Earth twin remains in one frame, and the time of acceleration can be made arbitrarily small compared with the time of inertial motion. Eventually, Lord Halsbury and others removed any acceleration by introducing the \"three-brother\" approach. The traveling twin transfers his clock reading to a third one, traveling in the opposite direction. Another way of avoiding acceleration effects is the use of the relativistic Doppler effect (see below).\n\nNeither Einstein nor Langevin considered such results to be problematic: Einstein only called it \"peculiar\" while Langevin presented it as a consequence of absolute acceleration. Both men argued that, from the time differential illustrated by the story of the twins, no self-contradiction could be constructed. In other words, neither Einstein nor Langevin saw the story of the twins as constituting a challenge to the self-consistency of relativistic physics.\n\nConsider a space ship traveling from Earth to the nearest star system: a distance years away, at a speed (i.e., 80 percent of the speed of light).\n\nTo make the numbers easy, the ship is assumed to attain full speed in a negligible time upon departure (even though it would actually take close to a year accelerating at 1 \"g\" to get up to speed). Similarly, at the end of the outgoing trip, the change in direction needed to start the return trip is assumed to occur in a negligible time. \n\nThe parties will observe the situation as follows:\n\nThe Earth-based mission control reasons about the journey this way: the round trip will take in Earth time (\"i.e.\" everybody on Earth will be 10 years older when the ship returns). The amount of time as measured on the ship's clocks and the aging of the travelers during their trip will be reduced by the factor formula_1, the reciprocal of the Lorentz factor (time dilation). In this case and the travelers will have aged only when they return.\n\nThe ship's crew members also calculate the particulars of their trip from their perspective. They know that the distant star system and the Earth are moving relative to the ship at speed \"v\" during the trip. In their rest frame the distance between the Earth and the star system is years (length contraction), for both the outward and return journeys. Each half of the journey takes , and the round trip takes twice as long (6 years). Their calculations show that they will arrive home having aged 6 years. The travelers' final calculation about their aging is in complete agreement with the calculations of those on Earth, though they experience the trip quite differently from those who stay at home.\n\nNo matter what method they use to predict the clock readings, everybody will agree about them. If twins are born on the day the ship leaves, and one goes on the journey while the other stays on Earth, they will meet again when the traveler is 6 years old and the stay-at-home twin is 10 years old.\n\nThe paradoxical aspect of the twins' situation arises from the fact that at any given moment the travelling twin's clock is running slow in the earthbound twin's inertial frame, but based on the relativity principle one could equally argue that the earthbound twin's clock is running slow in the travelling twin's inertial frame. One proposed resolution is based on the fact that the earthbound twin is at rest in the same inertial frame throughout the journey, while the travelling twin is not: in the simplest version of the thought-experiment, the travelling twin switches at the midpoint of the trip from being at rest in an inertial frame which moves in one direction (away from the Earth) to being at rest in an inertial frame which moves in the opposite direction (towards the Earth). In this approach, determining which observer switches frames and which does not is crucial. Although both twins can legitimately claim that they are at rest in their own frame, only the traveling twin experiences acceleration when the spaceship engines are turned on. This acceleration, measurable with an accelerometer, makes his rest frame temporarily non-inertial. This reveals a crucial asymmetry between the twins's perspectives: although we can predict the aging difference from both perspectives, we need to use different methods to obtain correct results.\n\nAlthough some solutions attribute a crucial role to the acceleration of the travelling twin at the time of the turnaround, others note that the effect also arises if one imagines separate outward-going and inward-coming travellers, who pass each other and synchronize their clocks at the point corresponding to \"turnaround\" of a single traveller. In this version, physical acceleration of the travelling clock plays no direct role; \"the issue is how long the world-lines are, not how bent\". The length referred to here is the Lorentz-invariant length or \"proper time interval\" of a trajectory which corresponds to the elapsed time measured by a clock following that trajectory (see Section Difference in elapsed time as a result of differences in twins' spacetime paths below). In Minkowski spacetime, the travelling twin must feel a different history of accelerations from the earthbound twin, even if this just means accelerations of the same size separated by different amounts of time, however \"even this role for acceleration can be eliminated in formulations of the twin paradox in curved spacetime, where the twins can fall freely along space-time geodesics between meetings\".\n\nFor a moment-by-moment understanding of how the time difference between the twins unfolds, one must understand that in special relativity there is no concept of \"absolute present\". For different inertial frames there are different sets of events that are simultaneous in that frame. This relativity of simultaneity means that switching from one inertial frame to another requires an adjustment in what slice through spacetime counts as the \"present\". In the spacetime diagram on the right, drawn for the reference frame of the Earth-based twin, that twin's world line coincides with the vertical axis (his position is constant in space, moving only in time). On the first leg of the trip, the second twin moves to the right (black sloped line); and on the second leg, back to the left. Blue lines show the planes of simultaneity for the traveling twin during the first leg of the journey; red lines, during the second leg. Just before turnaround, the traveling twin calculates the age of the Earth-based twin by measuring the interval along the vertical axis from the origin to the upper blue line. Just after turnaround, if he recalculates, he will measure the interval from the origin to the lower red line. In a sense, during the U-turn the plane of simultaneity jumps from blue to red and very quickly sweeps over a large segment of the world line of the Earth-based twin. When one transfers from the outgoing inertial frame to the incoming inertial frame there is a jump discontinuity in the age of the Earth-based twin (6.4 years in the example above).\n\nAs mentioned above, an \"out and back\" twin paradox adventure may incorporate the transfer of clock reading from an \"outgoing\" astronaut to an \"incoming\" astronaut, thus entirely eliminating the effect of acceleration. Also, according to the so-called \"clock postulate\", physical acceleration of clocks doesn't contribute to the kinematical effects of special relativity. Rather, the time differential between two reunited clocks is produced purely by uniform inertial motion, as discussed in Einstein's original 1905 relativity paper, as well as in all subsequent kinematical derivations of the Lorentz transformations.\n\nBecause spacetime diagrams incorporate Einstein's clock synchronization (with its lattice of clocks methodology), there will be a requisite jump in the reading of the Earth clock time made by a \"suddenly returning astronaut\" who inherits a \"new meaning of simultaneity\" in keeping with a new clock synchronization dictated by the transfer to a different inertial frame, as explained in Spacetime Physics by John A. Wheeler.\n\nIf, instead of incorporating Einstein's clock synchronization (lattice of clocks), the astronaut (outgoing and incoming) and the Earth-based party regularly update each other on the status of their clocks by way of sending radio signals (which travel at light speed), then all parties will note an incremental buildup of asymmetry in time-keeping, beginning at the \"turn around\" point. Prior to the \"turn around\", each party regards the other party's clock to be recording time differently from his own, but the noted difference is symmetrical between the two parties. After the \"turn around\", the noted differences are not symmetrical, and the asymmetry grows incrementally until the two parties are reunited. Upon finally reuniting, this asymmetry can be seen in the actual difference showing on the two reunited clocks.\n\nAll processes—chemical, biological, measuring apparatus functioning, human perception involving the eye and brain, the communication of force—are constrained by the speed of light. There is clock functioning at every level, dependent on light speed and the inherent delay at even the atomic level. Biological aging, therefore, is in no way different from clock time-keeping. This means that biological aging would be slowed in the same manner as a clock.\n\nIn view of the frame-dependence of simultaneity for events at different locations in space, some treatments prefer a more phenomenological approach, describing what the twins would observe if each sent out a series of regular radio pulses, equally spaced in time according to the emitter's clock. This is equivalent to asking, if each twin sent a video feed of themselves to each other, what do they see in their screens? Or, if each twin always carried a clock indicating his age, what time would each see in the image of their distant twin and his clock?\n\nShortly after departure, the traveling twin sees the stay-at-home twin with no time delay. At arrival, the image in the ship screen shows the staying twin as he was 1 year after launch, because radio emitted from Earth 1 year after launch gets to the other star 4 years afterwards and meets the ship there. During this leg of the trip, the traveling twin sees his own clock advance 3 years and the clock in the screen advance 1 year, so it seems to advance at the normal rate, just 20 image seconds per ship minute. This combines the effects of time dilation due to motion (by factor ε=0.6, five years on earth are 3 years on ship) and the effect of increasing light-time-delay (which grows from 0 to 4 years).\n\nOf course, the observed frequency of the transmission is also the frequency of the transmitter (a reduction in frequency; \"red-shifted\"). This is called the relativistic Doppler effect. The frequency of clock-ticks (or of wavefronts) which one sees from a source with rest frequency \"f\" is\nwhen the source is moving directly away. This is \"f\" = \"f\" for \"v\"/\"c\" = 0.8.\n\nAs for the stay-at-home twin, he gets a slowed signal from the ship for 9 years, at a frequency the transmitter frequency. During these 9 years, the clock of the traveling twin in the screen seems to advance 3 years, so both twins see the image of their sibling aging at a rate only their own rate. Expressed in other way, they would both see the other's clock run at their own clock speed. If they factor out of the calculation the fact that the light-time delay of the transmission is increasing at a rate of 0.8 seconds per second, \"both\" can work out that the other twin is aging slower, at 60% rate.\n\nThen the ship turns back toward home. The clock of the staying twin shows \"1 year after launch\" in the screen of the ship, and during the 3 years of the trip back it increases up to \"10 years after launch\", so the clock in the screen seems to be advancing 3 times faster than usual.\n\nWhen the source is moving towards the observer, the observed frequency is higher (\"blue-shifted\") and given by\nThis is \"f\" = 3\"f\" for \"v\"/\"c\" = 0.8.\n\nAs for the screen on Earth, it shows that trip back beginning 9 years after launch, and the traveling clock in the screen shows that 3 years have passed on the ship. One year later, the ship is back home and the clock shows 6 years. So, during the trip back, \"both\" twins see their sibling's clock going 3 times faster than their own. Factoring out the fact that the light-time-delay is decreasing by 0.8 seconds every second, each twin calculates that the other twin is aging at 60% his own aging speed.\nThe \"x\"–\"t\" (space–time) diagrams at left show the paths of light signals traveling between Earth and ship (1st diagram) and between ship and Earth (2nd diagram). These signals carry the images of each twin and his age-clock to the other twin. The vertical black line is the Earth's path through spacetime and the other two sides of the triangle show the ship's path through spacetime (as in the Minkowski diagram above). As far as the sender is concerned, he transmits these at equal intervals (say, once an hour) according to his own clock; but according to the clock of the twin receiving these signals, they are not being received at equal intervals.\n\nAfter the ship has reached its cruising speed of 0.8\"c\", each twin would see 1 second pass in the received image of the other twin for every 3 seconds of his own time. That is, each would see the image of the other's clock going slow, not just slow by the \"ε\" factor 0.6, but even slower because light-time-delay is increasing 0.8 seconds per second. This is shown in the figures by red light paths. At some point, the images received by each twin change so that each would see 3 seconds pass in the image for every second of his own time. That is, the received signal has been increased in frequency by the Doppler shift. These high frequency images are shown in the figures by blue light paths.\n\nThe asymmetry between the Earth and the space ship is manifested in this diagram by the fact that more blue-shifted (fast aging) images are received by the ship. Put another way, the space ship sees the image change from a red-shift (slower aging of the image) to a blue-shift (faster aging of the image) at the midpoint of its trip (at the turnaround, 5 years after departure); the Earth sees the image of the ship change from red-shift to blue shift after 9 years (almost at the end of the period that the ship is absent). In the next section, one will see another asymmetry in the images: the Earth twin sees the ship twin age by the same amount in the red and blue shifted images; the ship twin sees the Earth twin age by different amounts in the red and blue shifted images.\n\nThe twin on the ship sees low frequency (red) images for 3 years. During that time, he would see the Earth twin in the image grow older by . He then sees high frequency (blue) images during the back trip of 3 years. During that time, he would see the Earth twin in the image grow older by When the journey is finished, the image of the Earth twin has aged by \n\nThe Earth twin sees 9 years of slow (red) images of the ship twin, during which the ship twin ages (in the image) by He then sees fast (blue) images for the remaining 1 year until the ship returns. In the fast images, the ship twin ages by The total aging of the ship twin in the images received by Earth is , so the ship twin returns younger (6 years as opposed to 10 years on Earth).\n\nTo avoid confusion, note the distinction between what each twin sees and what each would calculate. Each sees an image of his twin which he knows originated at a previous time and which he knows is Doppler shifted. He does not take the elapsed time in the image as the age of his twin now.\nwhen the image was emitted. A similar calculation reveals that his twin was aging at the same reduced rate of \"εf\" in all low frequency images.\n\nIt may be difficult to see where simultaneity came into the Doppler shift calculation, and indeed the calculation is often preferred because one does not have to worry about simultaneity. As seen above, the ship twin can convert his received Doppler-shifted rate to a slower rate of the clock of the distant clock for both red and blue images. If he ignores simultaneity, he might say his twin was aging at the reduced rate throughout the journey and therefore should be younger than he is. He is now back to square one, and has to take into account the change in his notion of simultaneity at the turnaround. The rate he can calculate for the image (corrected for Doppler effect) is the rate of the Earth twin's clock at the moment it was sent, not at the moment it was received. Since he receives an unequal number of red and blue shifted images, he should realize that the red and blue shifted emissions were not emitted over equal time periods for the Earth twin, and therefore he must account for simultaneity at a distance.\n\nDuring the turnaround, the traveling twin is in an accelerated reference frame. According to the equivalence principle, the traveling twin may analyze the turnaround phase as if the stay-at-home twin were freely falling in a gravitational field and as if the traveling twin were stationary. A 1918 paper by Einstein presents a conceptual sketch of the idea. From the viewpoint of the traveler, a calculation for each separate leg, ignoring the turnaround, leads to a result in which the Earth clocks age less than the traveler. For example, if the Earth clocks age 1 day less on each leg, the amount that the Earth clocks will lag behind amounts to 2 days. The physical description of what happens at turnaround has to produce a contrary effect of double that amount: 4 days' advancing of the Earth clocks. Then the traveler's clock will end up with a net 2-day delay on the Earth clocks, in agreement with calculations done in the frame of the stay-at-home twin.\n\nThe mechanism for the advancing of the stay-at-home twin's clock is gravitational time dilation. When an observer finds that inertially moving objects are being accelerated with respect to themselves, those objects are in a gravitational field insofar as relativity is concerned. For the traveling twin at turnaround, this gravitational field fills the universe. In a weak field approximation, clocks tick at a rate of where \"Φ\" is the difference in gravitational potential. In this case, where \"g\" is the acceleration of the traveling observer during turnaround and \"h\" is the distance to the stay-at-home twin. The rocket is firing towards the stay-at-home twin, thereby placing that twin at a higher gravitational potential. Due to the large distance between the twins, the stay-at-home twin's clocks will appear to be sped up enough to account for the difference in proper times experienced by the twins. It is no accident that this speed-up is enough to account for the simultaneity shift described above. The general relativity solution for a static homogeneous gravitational field and the special relativity solution for finite acceleration produce identical results.\n\nOther calculations have been done for the traveling twin (or for any observer who sometimes accelerates), which do not involve the equivalence principle, and which do not involve any gravitational fields. Such calculations are based only on the special theory, not the general theory, of relativity. One approach calculates surfaces of simultaneity by considering light pulses, in accordance with Hermann Bondi's idea of the k-calculus. A second approach calculates a straightforward but technically complicated integral to determine how the traveling twin measures the elapsed time on the stay-at-home clock. An outline of this second approach is given in a .\n\nThe following paragraph shows several things:\n\nLet clock \"K\" be associated with the \"stay at home twin\".\nLet clock K' be associated with the rocket that makes the trip.\nAt the departure event both clocks are set to 0.\n\nKnowing that the clock \"K\" remains inertial (stationary), the total accumulated proper time Δ\"τ\" of clock K' will be given by the integral function of coordinate time Δ\"t\"\nwhere \"v\"(\"t\") is the \"coordinate velocity\" of clock K' as a function of \"t\" according to clock \"K\", and, e.g. during phase 1, given by\n\nThis integral can be calculated for the 6 phases:\n\nwhere \"a\" is the proper acceleration, felt by clock K' during the acceleration phase(s) and where the following relations hold between \"V\", \"a\" and \"T\":\n\nSo the traveling clock K' will show an elapsed time of\nwhich can be expressed as\n\nwhereas the stationary clock \"K\" shows an elapsed time of\nwhich is, for every possible value of \"a\", \"T\", \"T\" and \"V\", larger than the reading of clock K':\n\nIn the standard proper time formula\n\nΔ\"τ\" represents the time of the non-inertial (travelling) observer K' as a function of the elapsed time Δ\"t\" of the inertial (stay-at-home) observer \"K\" for whom observer K' has velocity \"v\"(\"t\") at time \"t\".\n\nTo calculate the elapsed time Δ\"t\" of the inertial observer \"K\" as a function of the elapsed time Δ\"τ\" of the non-inertial observer K', where only quantities measured by K' are accessible, the following formula can be used:\nwhere \"a(τ)\" is the proper acceleration of the non-inertial observer K' as measured by himself (for instance with an accelerometer) during the whole round-trip. The Cauchy–Schwarz inequality can be used to show that the inequality follows from the previous expression:\n\nUsing the Dirac delta function to model the infinite acceleration phase in the standard case of the traveller having constant speed \"v\" during the outbound and the inbound trip, the formula produces the known result:\n\nIn the case where the accelerated observer K' departs from \"K\" with zero initial velocity, the general equation reduces to the simpler form:\nwhich, in the \"smooth\" version of the twin paradox where the traveller has constant proper acceleration phases, successively given by \"a\", −\"a\", −\"a\", \"a\", results in\nwhere the convention \"c\" = 1 is used, in accordance with the above expression with acceleration phases and inertial (coasting) phases \n\nTwins Bob and Alice inhabit a space station in circular orbit around a massive body in space. Bob suits up and exits the station. While Alice remains inside the station, continuing to orbit with it as before, Bob uses a rocket propulsion system to cease orbiting and hover where he was. When the station completes an orbit and returns to Bob, he rejoins Alice. Alice is now younger than Bob. In addition to rotational acceleration, Bob must decelerate to become stationary and then accelerate again to match the orbital speed of the space station.\n\nEinstein's conclusion of an actual difference in registered clock times (or aging) between reunited parties caused Paul Langevin to posit an actual, albeit experimentally undetectable, absolute frame of reference: \n\nIn 1911, Langevin wrote: \"A uniform translation in the aether has no experimental sense. But because of this it should not be concluded, as has sometimes happened prematurely, that the concept of aether must be abandoned, that the aether is non-existent and inaccessible to experiment. Only a uniform velocity relative to it cannot be detected, but any change of velocity .. has an absolute sense.\"\n\nIn 1913, Henri Poincaré posthumous \"Last Essays\" were published and there he had restated his position: \"Today some physicists want to adopt a new convention. It is not that they are constrained to do so; they consider this new convention more convenient; that is all. And those who are not of this opinion can legitimately retain the old one.\".\"\n\nIn the relativity of Poincaré and Hendrik Lorentz, which assumes an absolute (though experimentally indiscernable) frame of reference, no twin paradox arises due to the fact that clock slowing (along with length contraction and velocity) is regarded as an actuality, hence the actual time differential between the reunited clocks. \n\nThat interpretation of relativity, which John A. Wheeler calls \"ether theory B (length contraction plus time contraction)\", did not gain as much traction as Einstein's, which simply disregarded any deeper reality behind the symmetrical measurements across inertial frames. There is no physical test which distinguishes one interpretation from the other.\n\nMore recently (in 2005), Robert B. Laughlin (Physics Nobel Laureate, Stanford University), wrote about the nature of space: \n\n\"It is ironic that Einstein's most creative work, the general theory of relativity, should boil down to conceptualizing space as a medium when his original premise [in special relativity] was that no such medium existed . . . The word 'ether' has extremely negative connotations in theoretical physics because of its past association with opposition to relativity. This is unfortunate because, stripped of these connotations, it rather nicely captures the way most physicists actually think about the vacuum. . . . Relativity actually says nothing about the existence or nonexistence of matter pervading the universe, only that any such matter must have relativistic symmetry.\" (i.e., as measured).\"\n\nA. P. French writes, in \"Special Relativity\": \n\n\"Note, though, that we are appealing to the reality of A's acceleration, and to the observability of the inertial forces associated with it. Would such effects as the twin paradox exist if the framework of fixed stars and distant galaxies were not there? Most physicists would say no. Our ultimate definition of an inertial frame may indeed be that it is a frame having zero acceleration with respect to the matter of the universe at large.\").\"\n\n\nThe \"ideal clock\" is a clock whose action depends only on its instantaneous velocity, and is independent of any acceleration of the clock. \n\n\n"}
{"id": "31574789", "url": "https://en.wikipedia.org/wiki?curid=31574789", "title": "Unit cohesion", "text": "Unit cohesion\n\nUnit cohesion is a military concept, defined by one former United States Chief of staff in the early 1980s as \"the bonding together of soldiers in such a way as to sustain their will and commitment to each other, the unit, and mission accomplishment, despite combat or mission stress\". However the concept lacks a consensus definition among military analysts, sociologists and psychologists.\n\nUnit cohesion is a military concept dating back to at least Carl von Clausewitz, if not to antiquity.\n\nSeveral scholars have cited the influence of Sigmund Freud's thinking on theories of unit cohesion. A number of them noted that Freud wrote of cohesion breakdown among soldiers, asserting that it leads to panic, insubordination, self-interested rather than cooperative reactions to threats, and \"a gigantic and senseless dread\".\n\nThe later development of the concept is strongly informed by the work of Morris Janowitz, who, with Edward Shils, began writing on the topic in the late 1940s. Janowitiz continued to work in this area in his sociological work, as the disruptive policy of frequently rotating individual soldiers and officers during the Vietnam War came under scrutiny as a large factor behind low morale.\n\nFollowing the studies of several World War II armies, sociologists concluded that comradely ties between small combat units is a decisive factor in providing good morale, cohesion, and organization framework.\n\nThe defeat of the Western forces by the poorly equipped Chinese People's Liberation Army in the Korean War in 1950 further generated interest on the role of \"human elements\" on modern battlefields. Although Western armies traditionally created ties between soldiers through informal means such as teamwork or shared hardships instilled by discipline, the Chinese army relied on formal methods to assimilate recruits into their units. The assimilation process involved features such as coercive persuasion, surveillance, and political control, while military ranks and physical punishments were abolished to allow closer relations between officers and soldiers. The stringent assimilation methods allowed the Chinese to create high morale and cohesion compared to the Western forces. However, high casualty rates and the lack of modern equipment later resulted in a significant erosion of morale and cohesion as the Korean War dragged on. One of the worst cases of this erosion was the partial disintegration of the Chinese army during the spring offensive in May 1951.\n\nIn the late 1980s, one researcher stated that, regardless of whether unit cohesion was an actual motivator or merely a stabilizer, what mattered was that unit cohesion \"enhanced fighting power\", because it reduced \"combat inhibitors (stress, fear, isolation)\" and promoted \"esprit de corps, morale and teamwork\". Other research has, however, concluded that there is value in distinguishing the components of social cohesion and \"[t]ask cohesion ... the commitment to working together on a shared goal\", since some studies conclude that unit effectiveness correlates strongly with task cohesion, not with social cohesion. This debate about the relative importance, or even need for, the concepts of social cohesion and task cohesion is exemplified by an exchange between Anthony King and Guy Siebold in the journal \"Armed Forces & Society\" in 2006–2007.\n\nOne U.S. military researcher has drawn a distinction between teamwork and unit cohesion—claiming teamwork as being merely \"collaboration\", while unit cohesion involves a bond that can sustain mutual commitment, not just to the mission, but to each other, and to the group as a whole. This added bond, he argued, enabled teamwork under conditions under which an organization might otherwise break down.\n\nThe concept of cohesion was originally used primarily to examine combat behavior. However, more recently models of cohesion have been applied to other phenomena characterized by stress, uncertainty, and the strategic interaction of groups. Uzi Ben-Shalom et al. looked at cohesion during Israeli Defense Force operations in the Occupied Territories during the al-Aqsa Intifada, while Paul Bartone and Amy Adler examined cohesion in a multi-national peacekeeping operation. Terence Lee used a broad concept of cohesion to explain military behavior during events in China in 1989 and Indonesia in 1998 and, in another article, the Philippines in 1986 and Indonesia in 1998.\n\nLucan Way and Steven Levitsky also used a broad concept of cohesion in order to explain regime maintenance in the former Soviet Union. Jesse Lehrke developed a multi-level model to facilitate the use of both social and task cohesion for examining military behavior during revolutions. Less elaborate versions of this approach can also be seen in work by Dale Herspring and earlier work by Jesse Lehrke.\n\n\n\n"}
