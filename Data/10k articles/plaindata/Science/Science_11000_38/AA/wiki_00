{"id": "351889", "url": "https://en.wikipedia.org/wiki?curid=351889", "title": "Aleksei Gubarev", "text": "Aleksei Gubarev\n\nAleksei Aleksandrovich Gubarev (; 29 March 1931 – 21 February 2015) was a Soviet cosmonaut who flew on two space flights: Soyuz 17 and Soyuz 28.\n\nGubarev graduated from the Soviet Naval Aviation School in 1952 and went on to serve with the Soviet Air Force. He undertook further studies at the Gagarin Air Force Academy before being accepted into the space programme.\n\nHe was originally trained for the Soviet lunar programme and for military Soyuz flights before training for Salyut missions. His next mission, in 1978, was Soyuz 28, the first Interkosmos flight, where he was accompanied by Vladimír Remek from Czechoslovakia.\n\nHe resigned as a cosmonaut in 1981 and took up an administrative position at the Gagarin Cosmonaut Training Centre.\n\nIn the 1980s he worked at the 30th Central Scientific Research Institute, Ministry of Defence (Russia).\n\nHis awards includes the Gagarin Gold Medal, which was bestowed upon him twice. He was an honorary citizen of Kaluga, Arkalyk, Tselinograd, and Prague.\n\nGubarev published a book, \"The Attraction of Weightlessness\", in 1982.\n\nGubarev died at the age of 83 on 21 February 2015.\n\n\nForeign awards:\n"}
{"id": "1466225", "url": "https://en.wikipedia.org/wiki?curid=1466225", "title": "Allee effect", "text": "Allee effect\n\nThe Allee effect is a phenomenon in biology characterized by a correlation between population size or density and the mean individual fitness (often measured as \"per capita\" population growth rate) of a population or species.\n\nAlthough the concept of Allee effect had no title at the time, it was first described in the 1930s by its namesake, Warder Clyde Allee. Through experimental studies, Allee was able to demonstrate that goldfish grow more rapidly when there are more individuals within the tank. This led him to conclude that aggregation can improve the survival rate of individuals, and that cooperation may be crucial in the overall evolution of social structure. The term \"Allee principle\" was introduced in the 1950s, a time when the field of ecology was heavily focused on the role of competition among and within species. The classical view of population dynamics stated that due to competition for resources, a population will experience a reduced overall growth rate at higher density and increased growth rate at lower density. In other words, individuals in a population would be better off when there are fewer individuals around due to a limited amount of resources (see ). However, the concept of the Allee effect introduced the idea that the reverse holds true when the population density is low. Individuals within a species often require the assistance of another individual for more than simple reproductive reasons in order to persist. The most obvious example of this is observed in animals that hunt for prey or defend against predators as a group.\n\nThe generally accepted definition of Allee effect is positive density dependence, or the positive correlation between population density and individual fitness. It is sometimes referred to as \"undercrowding\" and it is analogous (or even considered synonymous by some) to \"depensation\" in the field of fishery sciences. Listed below are a few significant subcategories of the Allee effect used in the ecology literature.\n\nThe component Allee effect is the positive relationship between any measurable component of individual fitness and population density. The demographic Allee effect is the positive relationship between the overall individual fitness and population density.\n\nThe distinction between the two terms lies on the scale of the Allee effect: the presence of a demographic Allee effect suggests the presence of at least one component Allee effect, while the presence of a component Allee effect does not necessarily result in a demographic Allee effect. For example, cooperative hunting and the ability to more easily find mates, both influenced by population density, are component Allee effects, as they influence individual fitness of the population. At low population density, these component Allee effects would add up to produce an overall demographic Allee effect (increased fitness with higher population density). When population density reaches a high number, negative density dependence often offsets the component Allee effects through resource competition, thus erasing the demographic Allee effect. It is important to note that Allee effects might occur even at high population density for some species.\n\nThe strong Allee effect is a demographic Allee effect with a critical population size or density. The weak Allee effect is a demographic Allee effect without a critical population size or density.\n\nThe distinction between the two terms is based on whether or not the population in question exhibits a critical population size or density. A population exhibiting a weak Allee effect will possess a reduced per capita growth rate (directly related to individual fitness of the population) at lower population density or size. However, even at this low population size or density, the population will always exhibit a positive per capita growth rate. Meanwhile, a population exhibiting a strong Allee effect will have a critical population size or density under which the population growth rate becomes negative. Therefore, when the population density or size hits a number below this threshold, the population will be destined for extinction without any further aid. A strong Allee effect is often easier to demonstrate empirically using time series data, as one can pinpoint the population size or density at which per capita growth rate becomes negative.\n\nDue to its definition as the positive correlation between population density and average fitness, the mechanisms for which an Allee effect arises are therefore inherently tied to survival and reproduction. In general, these Allee effect mechanisms arise from cooperation or facilitation among individuals in the species. Examples of such cooperative behaviors include better mate finding, environmental conditioning, and group defense against predators. As these mechanisms are more easily observable in the field, they tend to be more commonly associated with the Allee effect concept. Nevertheless, mechanisms of Allee effect that are less conspicuous such as inbreeding depression and sex ratio bias should be considered as well.\n\nAlthough numerous ecological mechanisms for Allee effects exist, the list of most commonly cited facilitative behaviors that contribute to Allee effects in the literature include: mate limitation, cooperative defense, cooperative feeding, and environmental conditioning. While these behaviors are classified in separate categories, note that they can overlap and tend to be context dependent (will operate only under certain conditions – for example, cooperative defense will only be useful when there are predators or competitors present).\n\n\n\n\n\nClassic economic theory predicts that human exploitation of a population is unlikely to result in species extinction because the escalating costs to find the last few individuals will exceed the fixed price one achieves by selling the individuals on the market. However, when rare species are more desirable than common species, prices for rare species can exceed high harvest costs. This phenomenon can create an \"anthropogenic\" Allee effect where rare species go extinct but common species are sustainably harvested. The anthropogenic Allee effect has become a standard approach for conceptualizing the threat of economic markets on endangered species. However, the original theory was posited using a one dimensional analysis of a two dimensional model. It turns out that a two dimensional analysis yields an Allee curve in human exploiter and biological population space and that this curve separating species destined to extinction vs persistence can be complicated. Even very high population sizes can potentially pass through the originally proposed Allee thresholds on predestined paths to extinction.\n\nDeclines in population size can result in a loss of genetic diversity, and owing to genetic variation's role in the evolutionary potential of a species, this could in turn result in an observable Allee effect. As a species' population becomes smaller, its gene pool will be reduced in size as well. One possible outcome from this genetic bottleneck is a reduction in fitness of the species through the process of genetic drift, as well as inbreeding depression. This overall fitness decrease of a species is caused by an accumulation of deleterious mutations throughout the population. Genetic variation within a species could range from beneficial to detrimental. Nevertheless, in a smaller sized gene pool, there is a higher chance of a stochastic event in which deleterious alleles become fixed (genetic drift). While evolutionary theory states that expressed deleterious alleles should be purged through natural selection, purging would be most efficient only at eliminating alleles that are highly detrimental or harmful. Mildly deleterious alleles such as those that act later in life would be less likely to be removed by natural selection, and conversely, newly acquired beneficial mutations are more likely to be lost by random chance in smaller genetic pools than larger ones.\n\nAlthough the long-term population persistence of several species with low genetic variation has recently prompted debate on the generality of inbreeding depression, there are various empirical evidences for genetic Allee effects. One such case was observed in the endangered Florida panther (\"Puma concolor coryi\"). The Florida panther experienced a genetic bottleneck in the early 1990s where the population was reduced to ~ 25 adult individuals. This reduction in genetic diversity was correlated with defects that include lower sperm quality, abnormal testosterone levels, cowlicks, and kinked tails. In response, a genetic rescue plan was put in motion and several female pumas from Texas were introduced into the Florida population. This action quickly led to the reduction in the prevalence of the defects previously associated with inbreeding depression. Although the timescale for this inbreeding depression is larger than of those more immediate Allee effects, it has significant implications on the long-term persistence of a species.\n\nDemographic stochasticity refers to variability in population growth arising from sampling random births and deaths in a population of finite size. In small populations, demographic stochasticity will decrease the population growth rate, causing an effect similar to the Allee effect, which will increase the risk of population extinction. Whether or not demographic stochasticity can be considered a part of Allee effect is somewhat contentious however. The most current definition of Allee effect considers the correlation between population density and mean individual fitness. Therefore, random variation resulting from birth and death events would not be considered part of Allee effect as the increased risk of extinction is not a consequence of the changing fates of individuals within the population.\nMeanwhile, when demographic stochasticity results in fluctuations of sex ratios, it arguably reduces the mean individual fitness as population declines. For example, a fluctuation in small population that causes a scarcity in one sex would in turn limit the access of mates for the opposite sex, decreasing the fitness of the individuals within the population. This type of Allee effect will likely be more prevalent in monogamous species than polygynous species.\n\nDemographic and mathematical studies demonstrate that the existence of an Allee effect can reduce the speed of range expansion of a population and can even prevent biological invasions.\n\nRecent results based on spatio-temporal models show that the Allee effect can also promote genetic diversity in expanding populations. These results counteract commonly held notions that the Allee effect possesses net adverse consequences. Reducing the growth rate of the individuals ahead of the colonization front simultaneously reduces the speed of colonization and enables a diversity of genes coming from the core of the population to remain on the front. The Allee effect also affects the spatial distribution of diversity. Whereas spatio-temporal models which do not include an Allee effect lead to a vertical pattern of genetic diversity (i.e., a strongly structured spatial distribution of genetic fractions), those including an Allee effect lead to a \"horizontal pattern\" of genetic diversity (i.e., an absence of genetic differentiation in space).\n\nA simple mathematical example of an Allee effect is given by the cubic growth model\nwhere the population has a negative growth rate for formula_2, and \na positive growth rate for formula_3 (assuming formula_4).\nThis is a departure from the logistic growth equation\nwhere\n\nAfter dividing both sides of the equation by the population size N, in the logistic growth the left hand side of the equation represents the per capita population growth rate, which is dependent on the population size N, and decreases with increasing N throughout the entire range of population sizes. In contrast, when there is an Allee effect the per-capita growth rate increases with increasing N over some range of population sizes [0, N].\n\nSpatio-temporal models can take Allee effect into account as well. A simple example is given by the reaction-diffusion model\nwhere\n\nWhen a population is made up of small sub-populations additional factors to the Allee effect arise.\n\nIf the sub-populations are subject to different environmental variations (i.e. separated enough that a disaster could occur at one sub-population site without affecting the other sub-populations) but still allow individuals to travel between sub-populations, then the individual sub-populations are more likely to go extinct than the total population. In the case of a catastrophic event decreasing numbers at a sub-population, individuals from another sub-population site may be able to repopulate the area.\n\nIf all sub-populations are subject to the same environmental variations (i.e. if a disaster affected one, it would affect them all) then fragmentation of the population is detrimental to the population and increases extinction risk for the total population. In this case, the species receives none of the benefits of a small sub-population (loss of the sub-population is not catastrophic to the species as a whole) and all of the disadvantages (inbreeding depression, loss of genetic diversity and increased vulnerability to environmental instability) and the population would survive better unfragmented.\n\nClumping results due to individuals aggregating in response to: 1) local habitat or landscape differences, 2) daily and seasonal weather changes, 3) reproductive processes, or 4) as the result of social attractions.\n\n\n"}
{"id": "881699", "url": "https://en.wikipedia.org/wiki?curid=881699", "title": "Almucantar", "text": "Almucantar\n\nAn almucantar (also spelled almucantarat or almacantara) is a circle on the celestial sphere parallel to the horizon. Two stars that lie on the same almucantar have the same altitude. \nThe term was introduced into European astronomy by monastic astronomer Hermann Contractus of Reichenau, Latinized from the Arabic word ' (\"the almucantar, sundial\", plural: '), derived from \"\" (\"arch, bridge\")\n\nAn almucantar staff is an instrument chiefly used to determine the time of sunrise and sunset, in order to find the amplitude and consequently the variations of the compass. Usually made of pear tree or boxwood, with an arch of 15° to 30°, it is an example of a backstaff.\n\nThe sun casts that shadow of a vane (B in the adjacent image) on a \"horizon vane\" (A). The horizon vane has a slit or hole to allow the observer to see the horizon in the distance. The observer aligns the horizon and shadow so they show at the same point on the horizon vane and sets the \"sighting vane\" (C) to align his line of sight with the horizon. The altitude of the sun is the angle between the shadow vane and the sighting vane.\n\nThe almucantar plane that contains the Sun is used to characterize multiple scattering of aerosols. Measurements are carried out rapidly at several angle at both sides of the Sun using a spectroradiometer or a photometer. There are several models to obtain aerosol properties from the solar almucantar. The most relevant were developed by Oleg Dubovik and used in the NASA AERONET network and by Teruyuki Nakajima (named SkyRad.pack).\n\n\n"}
{"id": "1401635", "url": "https://en.wikipedia.org/wiki?curid=1401635", "title": "Apollo 1 Hills", "text": "Apollo 1 Hills\n\nThe Apollo 1 Hills are three vastly separated hills located in Gusev Crater, on Mars. They were photographed from a great distance by the \"Spirit\" Rover. They are named in memory of the three astronauts who died on the launchpad of Apollo 1. \n\nThe International Astronomical Union has yet to officially designate the hills with the names of the astronauts.\n\n\n"}
{"id": "29332872", "url": "https://en.wikipedia.org/wiki?curid=29332872", "title": "Bondeson Glacier", "text": "Bondeson Glacier\n\nBondeson Glacier () is a glacier about long, flowing north along the east side of Benson Ridge into the lower portion of Robb Glacier. It was mapped by the United States Geological Survey from tellurometer surveys (1961–62) and from Navy air photos (1960), and named by the Advisory Committee on Antarctic Names for W. Bondeson, Master of the USNS \"Private John R. Towle\" during U.S. Navy Operation Deepfreeze 1964 and 1965.\n"}
{"id": "1159136", "url": "https://en.wikipedia.org/wiki?curid=1159136", "title": "Bridgman–Stockbarger technique", "text": "Bridgman–Stockbarger technique\n\nThe Bridgman–Stockbarger technique is named after Harvard physicist Percy Williams Bridgman (1882-1961) and MIT physicist Donald C. Stockbarger (1895–1952). The technique includes two similar but distinct methods primarily used for growing boules (single crystal ingots), but which can be used for solidifying polycrystalline ingots as well.\n\nThe methods involve heating polycrystalline material above its melting point and slowly cooling it from one end of its container, where a seed crystal is located. A single crystal of the same crystallographic orientation as the seed material is grown on the seed and is progressively formed along the length of the container. The process can be carried out in a horizontal or vertical orientation, and usually involves a rotating crucible/ampoule to stir the melt.\n\nThe Bridgman method is a popular way of producing certain semiconductor crystals such as gallium arsenide, for which the Czochralski process is more difficult. The process can reliably produce single crystal ingots, but does not necessarily result in uniform properties through the crystal.\n\nThe difference between the Bridgman technique and Stockbarger technique is subtle: While both methods utilize a temperature gradient and a moving crucible, the Bridgman technique utilizes the relatively uncontrolled gradient produced at the exit of the furnace; the Stockbarger technique introduces a baffle, or shelf, separating two coupled furnaces with temperatures above and below the freezing point. Stockbarger's modification of the Bridgman technique allows for better control over the temperature gradient at the melt/crystal interface.\n\nWhen seed crystals are not employed as described above, polycrystalline ingots can be produced from a feedstock consisting of rods, chunks, or any irregularly shaped pieces once they are melted and allowed to re-solidify. The resultant microstructure of the ingots so obtained are characteristic of directionally solidified metals and alloys with their aligned grains.\n\nA variant of the technique known as the horizontal directional solidification method or HDSM developed by Khachik Bagdasarov starting in the 1960s in the Soviet Union uses a flat-bottomed crucible with short sidewalls rather than an enclosed ampoule, and has been used to grow various large oxide crystals including Yb:YAG (a laser host crystal), and sapphire crystals 45 cm wide and over 1 meter long.\n\n"}
{"id": "5776607", "url": "https://en.wikipedia.org/wiki?curid=5776607", "title": "Bulk purchasing", "text": "Bulk purchasing\n\nBulk purchasing is the purchase of much larger quantities than the usual, for a unit price that is lower than the usual.\n\nWholesaling is selling goods in large quantities at a low unit price to retail merchants. The wholesaler will accept a slightly lower sales price for each unit, if the retailer will agree to purchase a much greater quantity of units, so the wholesaler can maximize profit. A wholesaler usually represents a factory where goods are produced. The factory owners can use economy of scale to increase profit as quantities sold soars.\n\nRetailing is buying goods in a wholesale market to sell them in small quantities at higher prices to consumers. Part of this profit is justified by logistics, the useful distribution function of the retailer, who delivers the goods to consumers and divides those large quantities of goods into many smaller units suitable for many transactions with many small parties of consumers. Retailers can also benefit from economy of scale to increase profit, just like a wholesaler does.\n\nBulk purchasing is when a consumer captures part of the benefits of economy of scale by doing with the retailer what the retailer does with the wholesaler: paying a lower price per unit in exchange for purchasing much larger quantities. This allows the consumer to satisfy more of his or her demands at a lower total cost by acquiring more use value per dollar spent.\n\nConsumer demand for savings by bulk purchase has led to the success of big-box stores. Although effected by marginal cost, the total cost does not increase.\n\nIn the \"Saturday Night Live\" sketch \"The Coneheads\", the Coneheads engage in bulk purchasing and then \"consume mass quantities\".\n\n"}
{"id": "18582351", "url": "https://en.wikipedia.org/wiki?curid=18582351", "title": "Cabalzarite", "text": "Cabalzarite\n\nCabalzarite is a rare arsenate mineral with formula: (Ca(Mg,Al,Fe)[AsO]·2(HO,OH). It is a member of the tsumcorite group. It crystallizes in the monoclinic system and typically occurs as clusters of crystals or granular aggregates.\n\nIt was first described for samples from an abandoned manganese mine in Falotta, Graubünden, Switzerland and named for Swiss amateur mineralogist Walter Cabalzar. It was approved as a new mineral by the IMA in 1997. It has also been reported from the Aghbar mine in Ouarzazate Province, Morocco.\n"}
{"id": "1258362", "url": "https://en.wikipedia.org/wiki?curid=1258362", "title": "Cervical fracture", "text": "Cervical fracture\n\nA cervical fracture, commonly called a broken neck, is a catastrophic fracture of any of the seven cervical vertebrae in the neck. Examples of common causes in humans are traffic collisions and diving into shallow water. Abnormal movement of neck bones or pieces of bone can cause a spinal cord injury resulting in loss of sensation, paralysis, or usually instant death.\n\nConsiderable force is needed to cause a cervical fracture. Vehicle collisions and falls are common causes. A severe, sudden twist to the neck or a severe blow to the head or neck area can cause a cervical fracture.\n\nSports that involve violent physical contact carry a risk of cervical fracture, including American football, association football (especially the goalkeeper), ice hockey, rugby, and wrestling. Spearing an opponent in football or rugby, for instance, can cause a broken neck. Cervical fractures may also be seen in some non-contact sports, such as gymnastics, skiing, diving, surfing, powerlifting, equestrianism, mountain biking, and motor racing.\n\nCertain penetrating neck injuries can also cause cervical fracture which can also cause internal bleeding among other complications.\n\nHanging also causes a cervical fracture which kills the victim.\n\nSevere pain will usually be present at the point of injury. Pressure on a nerve may also cause pain from the neck down the shoulders and/or arms. Bruising and swelling may be present at the back of the neck. A neurological exam will be performed to assess for spinal cord injury. X-rays will be ordered to determine the severity and location of the fracture. CT (computed tomography) scans may be ordered to assess for gross abnormalities not visible by regular X-ray. MRI (magnetic resonance imaging) tests may be ordered to provide high resolution images of soft tissue and determine whether there has been damage to the spinal cord, although such damage is usually obvious in the conscious patient because of the immediate functional consequences of numbness and paralysis in much of the body.\n\nIt is also common for imaging (either a plain film X-ray or CT scan) to be completed when assessing a cervical injury. This is the most common way to diagnose the location and severity of the fracture. To decrease the use C-spine scans yielding negative findings for fracture, thus unnecessarily exposing people to radiation and increase time in the hospital and cost of the visit, multiple clinical decision support rules have been developed to help clinicians weigh the option to scan a patient with a neck injury. Among these are the Canadian C-spine rule and the NEXUS criteria for C-Spine imaging, which both help make these decisions from easily obtained information. Both rules are widely used in emergency departments and by paramedics.\n\nThe indication to surgically stabilize a cervical fracture can be estimated from the \"Subaxial Injury Classification\" (SLIC). In this system, a score of 3 or less indicates that conservative management is appropriate, a score of 5 or more indicates that surgery is needed, and a score of 4 is equivocal. The score is the sum from 3 different categories: morphology, discs and ligaments, and neurology:\n\nComplete immobilization of the head and neck should be done as early as possible and before moving the patient. Immobilization should remain in place until movement of the head and neck is proven safe. \"In the presence of severe head trauma, cervical fracture must be presumed until ruled out.\" Immobilization is imperative to minimize or prevent further spinal cord injury. The only exceptions are when there is imminent danger from an external cause, such as becoming trapped in a burning building.\n\nNon-steroidal anti-inflammatory medications (NSAIDs), such as aspirin or ibuprofen, are contraindicated because they interfere with bone healing. Tylenol (acetaminophen) is a better option. Patients with cervical fractures will likely be prescribed medication for pain control.\n\nIn the long term, physical therapy will be given to build strength in the muscles of the neck to increase stability and better protect the cervical spine.\n\nCollars, traction and surgery can be used to immobilize and stabilize the neck after a cervical fracture.\n\nMinor fractures can be immobilized with a cervical collar without need for traction or surgery. A soft collar is fairly flexible and is the least limiting but can carry a high risk of further neck damage in patients with osteoporosis. It can be used for minor injuries or after healing has allowed the neck to become more stable.\n\nA range of manufactured rigid collars are also used, usually comprising a firm plastic bi-valved shell secured with Velcro straps and removable padded liners. The most frequently prescribed are the Aspen, Malibu, Miami J, and Philadelphia collars. All these can be used with additional chest and head extension pieces to increase stability.\n\nRigid braces that support the head and chest are also prescribed. Examples include the Sterno-Occipital Mandibular Immobilization Device (SOMI), Lerman Minerva and Yale types. Special patients, such as very young children or non-cooperative adults, are sometimes still immobilized in medical plaster of paris casts, such as the Minerva cast.\n\nTraction can be applied by free weights on a pulley or a Halo type brace. The Halo brace is the most rigid cervical brace, used when limiting motion to the minimum that is essential, especially with unstable cervical fractures. It can provide stability and support during the time (typically 8–12 weeks) needed for the cervical bones to heal.\n\nSurgery may be needed to stabilize the neck and relieve pressure on the spinal cord. A variety of surgeries are available depending on the injury. Surgery to remove a damaged intervertebral disc may be done to relieve pressure on the spinal cord. The discs are cushions between the vertebrae. After the disc is removed, the vertebrae may be fused together to provide stability. Metal plates, screws, or wires may be needed to hold vertebrae or pieces in place.\n\nArab physician and surgeon Ibn al-Quff (d. 1286 CE) described a treatment of cervical fractures through the oral route in his book \"Kitab al-ʿUmda fı Ṣinaʿa al-Jiraḥa\" (Book of Basics in the Art of Surgery).\n\n\n"}
{"id": "7587223", "url": "https://en.wikipedia.org/wiki?curid=7587223", "title": "Chiral shift reagent", "text": "Chiral shift reagent\n\nA chiral shift reagent is a reagent used in analytical chemistry for determining the optical purity of a sample. Some analytical techniques such as HPLC and NMR, in their most commons forms, cannot distinguish enantiomers within a sample, but can distinguish diastereomers. Therefore, converting a mixture of enantiomers to a corresponding mixture of diastereomers can allow analysis.\n\nOne method involves the reaction of a chiral derivatizing agent (CDA) with a mixture of enantiomers to produce diasteromers via covalent attachment. One of the most common CDA is Mosher's acid.\n\nAnother method involves non-covalent interactions. NMR shift reagents such as EuFOD and TRISPHAT take advantage of the formation of diastereomeric complexes between the shift reagent and the analytical sample.\n"}
{"id": "40676777", "url": "https://en.wikipedia.org/wiki?curid=40676777", "title": "Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey", "text": "Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey\n\nCANDELS, the Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey, is the largest project in the history of the Hubble Space Telescope, with 902 assigned orbits (about two months) of observing time. It was carried out between 2010 and 2013 with two cameras on board Hubble – WFC3 and ACS – and aims to explore galactic evolution in the early Universe, and the very first seeds of cosmic structure at less than one billion years after the Big Bang.\n\nThe Cosmic Assembly Near-IR Deep Extragalactic Legacy Survey is designed to document the first third of galactic evolution – on the redshifts from 8 to 1.5 – via deep imaging of more than 250,000 galaxies.Another goal is to find the first Type Ia SNe beyond z > 1.5 and establish their accuracy as standard candles for cosmology.Plus some \"daytime\" UVIS exposures in GOODS-N to exploit the CVZ opportunity in that field.\nCANDELS' main instrument is the Wide Field Camera 3, a near-infrared camera, installed on Hubble in May 2009. WFC3 works in pair with the visible-light Advanced Camera for Surveys, which together gives unprecedented panchromatic coverage of galaxies from optical wavelengths to the near-IR.\n\n"}
{"id": "46470", "url": "https://en.wikipedia.org/wiki?curid=46470", "title": "Crop rotation", "text": "Crop rotation\n\nCrop rotation is the practice of growing a series of dissimilar or different types of crops in the same area in sequenced seasons. It is done so that the soil of farms is not used for only one set of nutrients.\nIt helps in reducing soil erosion and increases soil fertility and crop yield.\n\nGrowing the same crop in the same place for many years in a row (monocropping) disproportionately depletes the soil of certain nutrients. With rotation, a crop that leaches the soil of one kind of nutrient is followed during the next growing season by a dissimilar crop that returns that nutrient to the soil or draws a different ratio of nutrients. In addition, crop rotation mitigates the buildup of pathogens and pests that often occurs when one species is continuously cropped, and can also improve soil structure and fertility by increasing biomass from varied root structures.\n\nCrop cycle is used in both conventional and organic farming systems.\n\nAgriculturalists have long recognized that suitable rotations—such as planting spring crops for livestock in place of grains for human consumption—make it possible to restore or to maintain a productive soil. Middle Eastern farmers practiced crop rotation in 6000 BC without understanding the chemistry, alternately planting legumes and cereals. In the Bible, chapter 25 of the Book of Leviticus instructs the Israelites to observe a \"Sabbath of the Land\". Every seventh year they would not till, prune or even control insects.\n\nUnder a two-field rotation, half the land was planted in a year, while the other half lay fallow. Then, in the next year, the two fields were reversed. From the times of Charlemagne (died 814), farmers in Europe transitioned from a two-field crop rotation to a three-field crop rotation.\n\nFrom the end of the Middle Ages until the 20th century, Europe's farmers practised three-field rotation, dividing available lands into three parts. One section was planted in the autumn with rye or winter wheat, followed by spring oats or barley; the second section grew crops such as peas, lentils, or beans; and the third field was left fallow. The three fields were rotated in this manner so that every three years, a field would rest and be fallow. Under the two-field system, if one has a total of of fertile land, one would only plant 300 acres. Under the new three-field rotation system, one would plant (and therefore harvest) 400 acres. But the additional crops had a more significant effect than mere quantitative productivity. Since the spring crops were mostly legumes, they increased the overall nutrition of the people of Northern Europe.\n\nFarmers in the region of Waasland (in present-day northern Belgium) pioneered a four-field rotation in the early 16th century, and the British agriculturist Charles Townshend (1674–1738) popularised this system in the 18th century. The sequence of four crops (wheat, turnips, barley and clover), included a fodder crop and a grazing crop, allowing livestock to be bred year-round. The four-field crop rotation became a key development in the British Agricultural Revolution. The rotation between arable and ley is sometimes called ley farming.\n\nGeorge Washington Carver (1860s–1943) studied crop-rotation methods in the United States, teaching southern farmers to rotate soil-depleting crops like cotton with soil-enriching crops like peanuts and peas.\n\nIn the Green Revolution of the mid-20th century the traditional practice of crop rotation gave way in some parts of the world to the practice of supplementing the chemical inputs to the soil through topdressing with fertilizers, adding (for example) ammonium nitrate or urea and restoring soil pH with lime. Such practices aimed to increase yields, to prepare soil for specialist crops, and to reduce waste and inefficiency by simplifying planting and harvesting.\n\nA preliminary assessment of crop interrelationships can be found in how each crop: (1) contributes to soil organic matter (SOM) content, (2) provides for pest management, (3) manages deficient or excess nutrients, and (4) how it contributes to or controls for soil erosion.\n\nCrop choice is often related to the goal the farmer is looking to achieve with the rotation, which could be weed management, increasing available nitrogen in the soil, controlling for erosion, or increasing soil structure and biomass, to name a few. When discussing crop rotations, crops are classified in different ways depending on what quality is being assessed: by family, by nutrient needs/benefits, and/or by profitability (i.e. cash crop versus cover crop). For example, giving adequate attention to plant family is essential to mitigating pests and pathogens. However, many farmers have success managing rotations by planning sequencing and cover crops around desirable cash crops. The following is a simplified classification based on crop quality and purpose.\n\nMany crops which are critical for the market, like vegetables, are row crops (that is, grown in tight rows). While often the most profitable for farmers, these crops are more taxing on the soil. Row crops typically have low biomass and shallow roots: this means the plant contributes low residue to the surrounding soil and has limited effects on structure. With much of the soil around the plant exposed to disruption by rainfall and traffic, fields with row crops experience faster break down of organic matter by microbes, leaving fewer nutrients for future plants.\n\nIn short, while these crops may be profitable for the farm, they are nutrient depleting. Crop rotation practices exist to strike a balance between short-term profitability and long-term productivity.\n\nA great advantage of crop rotation comes from the interrelationship of nitrogen fixing-crops with nitrogen demanding crops. Legumes, like alfalfa and clover, collect available nitrogen from the soil in nodules on their root structure. When the plant is harvested, the biomass of uncollected roots breaks down, making the stored nitrogen available to future crops. Legumes are also a valued green manure: a crop that collects nutrients and fixes them at soil depths accessible to future crops.\n\nIn addition, legumes have heavy tap roots that burrow deep into the ground, lifting soil for better tilth and absorption of water.\n\nCereal and grasses are frequent cover crops because of the many advantages they supply to soil quality and structure. The dense and far-reaching root systems give ample structure to surrounding soil and provide significant biomass for soil organic matter.\n\nGrasses and cereals are key in weed management as they compete with undesired plants for soil space and nutrients.\n\nGreen manure is a crop that is mixed into the soil. Both nitrogen-fixing legumes and nutrient scavengers, like grasses, can be used as green manure. Green manure of legumes is an excellent source of nitrogen, especially for organic systems, however, legume biomass doesn't contribute to lasting soil organic matter like grasses do.\n\nThere are numerous factors that must be taken into consideration when planning a crop rotation. Planning an effective rotation requires weighing fixed and fluctuating production circumstances: market, farm size, labor supply, climate, soil type, growing practices, etc. Moreover, a crop rotation must consider in what condition one crop will leave the soil for the succeeding crop and how one crop can be seeded with another crop. For example, a nitrogen-fixing crop, like a legume, should always precede a nitrogen depleting one; similarly, a low residue crop (i.e. a crop with low biomass) should be offset with a high biomass cover crop, like a mixture of grasses and legumes.\n\nThere is no limit to the number of crops that can be used in a rotation, or the amount of time a rotation takes to complete. Decisions about rotations are made years prior, seasons prior, or even at the very last minute when an opportunity to increase profits or soil quality presents itself. In short, there is no singular formula for rotation, but many considerations to take into account.\n\nCrop rotation systems may be enriched by the influences of other practices such as the addition of livestock and manure, intercropping or multiple cropping, and organic management low in pesticides and synthetic fertilizers.\n\nIntroducing livestock makes the most efficient use of critical sod and cover crops; livestock (through manure) are able to distribute the nutrients in these crops throughout the soil rather than removing nutrients from the farm through the sale of hay.\n\nIn Sub-Saharan Africa, as animal husbandry becomes less of a nomadic practice many herders have begun integrating crop production into their practice. This is known as mixed farming, or the practice of crop cultivation with the incorporation of raising cattle, sheep and/or goats by the same economic entity, is increasingly common. This interaction between the animal, the land and the crops are being done on a small scale all across this region. Crop residues provide animal feed, while the animals provide manure for replenishing crop nutrients and draft power. Both processes are extremely important in this region of the world as it is expensive and logistically unfeasible to transport in synthetic fertilizers and large-scale machinery. As an additional benefit, the cattle, sheep and/or goat provide milk and can act as a cash crop in the times of economic hardship.\n\nCrop rotation is a required practice in order for a farm to receive organic certification in the United States. The “Crop Rotation Practice Standard” for the National Organic Program under the U.S. Code of Federal Regulations, section §205.205, states that:\n\nFarmers are required to implement a crop rotation that maintains or builds soil organic matter, works to control pests, manages and conserves nutrients, and protects against erosion. Producers of perennial crops that aren’t rotated may utilize other practices, such as cover crops, to maintain soil health.\n\nIn addition to lowering the need for inputs by controlling for pests and weeds and increasing available nutrients, crop rotation helps organic growers increase the amount of biodiversity on their farms. Biodiversity is also a requirement of organic certification, however, there are no rules in place to regulate or reinforce this standard. Increasing the biodiversity of crops has beneficial effects on the surrounding ecosystem and can host a greater diversity of fauna, insects, and beneficial microorganism in the soil.< Some studies point to increased nutrient availability from crop rotation under organic systems compared to conventional practices as organic practices are less likely to inhibit of beneficial microbes in soil organic matter.\n\nWhile multiple cropping and intercropping benefit from many of the same principals as crop rotation, they do not satisfy the requirement under the NOP.\n\nMultiple cropping systems, such as intercropping or companion planting, offer more diversity and complexity within the same season or rotation, for example the three sisters. An example of companion planting is the inter-planting of corn with pole beans and vining squash or pumpkins. In this system, the beans provide nitrogen; the corn provides support for the beans and a \"screen\" against squash vine borer; the vining squash provides a weed suppressive canopy and a discouragement for corn-hungry raccoons.\n\nDouble-cropping is common where two crops, typically of different species, are grown sequentially in the same growing season, or where one crop (e.g. vegetable) is grown continuously with a cover crop (e.g. wheat). This is advantageous for small farms, who often cannot afford to leave cover crops to replenish the soil for extended periods of time, as larger farms can. When multiple cropping is implemented on small farms, these systems can maximize benefits of crop rotation on available land resources.\n\nAgronomists describe the benefits to yield in rotated crops as \"The Rotation Effect\". There are many found benefits of rotation systems: however, there is no specific scientific basis for the sometimes 10-25% yield increase in a crop grown in rotation versus monoculture. The factors related to the increase are simply described as alleviation of the negative factors of monoculture cropping systems. Explanations due to improved nutrition; pest, pathogen, and weed stress reduction; and improved soil structure have been found in some cases to be correlated, but causation has not been determined for the majority of cropping systems.\n\nOther benefits of rotation cropping systems include production cost advantages. Overall financial risks are more widely distributed over more diverse production of crops and/or livestock. Less reliance is placed on purchased inputs and over time crops can maintain production goals with fewer inputs. This in tandem with greater short and long term yields makes rotation a powerful tool for improving agricultural systems.\n\nThe use of different species in rotation allows for increased soil organic matter (SOM), greater soil structure, and improvement of the chemical and biological soil environment for crops. With more SOM, water infiltration and retention improves, providing increased drought tolerance and decreased erosion.\n\nSoil organic matter is a mix of decaying material from biomass with active microorganisms. Crop rotation, by nature, increases exposure to biomass from sod, green manure, and a various other plant debris. The reduced need for intensive tillage under crop rotation allows biomass aggregation to lead to greater nutrient retention and utilization, decreasing the need for added nutrients. With tillage, disruption and oxidation of soil creates a less conducive environment for diversity and proliferation of microorganisms in the soil. These microorganisms are what make nutrients available to plants. So, where \"active\" soil organic matter is a key to productive soil, soil with low microbial activity provides significantly fewer nutrients to plants; this is true even though the quantity of biomass left in the soil may be the same.\n\nSoil microorganisms also decrease pathogen and pest activity through competition. In addition, plants produce root exudates and other chemicals which manipulate their soil environment as well as their weed environment. Thus rotation allows increased yields from nutrient availability but also alleviation of allelopathy and competitive weed environments.\n\nStudies have shown that crop rotations greatly increase soil organic carbon (SOC) content, the main constituent of soil organic matter. Carbon, along with hydrogen and oxygen, is a macronutrient for plants. Highly diverse rotations spanning long periods of time have shown to be even more effective in increasing SOC, while soil disturbances (e.g. from tillage) are responsible for exponential decline in SOC levels. In Brazil, conversion to no-till methods combined with intensive crop rotations has been shown an SOC sequestration rate of 0.41 tonnes per hectare per year.\n\nIn addition to enhancing crop productivity, sequestration of atmospheric carbon has great implications in reducing rates of climate change by removing carbon dioxide from the air.\n\nRotating crops adds nutrients to the soil. Legumes, plants of the family Fabaceae, for instance, have nodules on their roots which contain nitrogen-fixing bacteria called rhizobia. During a process called nodulation, the rhizobia bacteria use nutrients and water provided by the plant to convert atmospheric nitrogen into ammonia, which is then converted into an organic compound that the plant can use as its nitrogen source. It therefore makes good sense agriculturally to alternate them with cereals (family Poaceae) and other plants that require nitrates. How much nitrogen made available to the plants depends on factors such as the kind of legume, the effectiveness of rhizobia bacteria, soil conditions, and the availability of elements necessary for plant food.\n\nCrop rotation is also used to control pests and diseases that can become established in the soil over time. The changing of crops in a sequence decreases the population level of pests by (1) interrupting pest life cycles and (2) interrupting pest habitat. Plants within the same taxonomic family tend to have similar pests and pathogens. By regularly changing crops and keeping the soil occupied by cover crops instead of lying fallow, pest cycles can be broken or limited, especially cycles that benefit from overwintering in residue. For example, root-knot nematode is a serious problem for some plants in warm climates and sandy soils, where it slowly builds up to high levels in the soil, and can severely damage plant productivity by cutting off circulation from the plant roots. Growing a crop that is not a host for root-knot nematode for one season greatly reduces the level of the nematode in the soil, thus making it possible to grow a susceptible crop the following season without needing soil fumigation.\n\nThis principle is of particular use in organic farming, where pest control must be achieved without synthetic pesticides.\n\nIntegrating certain crops, especially cover crops, into crop rotations is of particular value to weed management. These crops crowd out weed through competition. In addition, the sod and compost from cover crops and green manure slows the growth of what weeds are still able to make it through the soil, giving the crops further competitive advantage. By removing slowing the growth and proliferation of weeds while cover crops are cultivated, farmers greatly reduce the presence of weeds for future crops, including shallow rooted and row crops, which are less resistant to weeds. Cover crops are, therefore, considered conservation crops because they protect otherwise fallow land from becoming overrun with weeds.\n\nThis system has advantages over other common practices for weeds management, such as tillage. Tillage is meant to inhibit growth of weeds by overturning the soil; however, this has a countering effect of exposing weed seeds that may have gotten buried and burying valuable crop seeds. Under crop rotation, the number of viable seeds in the soil is reduced through the reduction of the weed population.\n\nIn addition to their negative impact on crop quality and yield, weeds can slow down the harvesting process. Weeds make farmers less efficient when harvesting, because weeds like bindweeds, and knotgrass, can become tangled in the equipment, resulting in a stop-and-go type of harvest.\n\nCrop rotation can significantly reduce the amount of soil lost from erosion by water. In areas that are highly susceptible to erosion, farm management practices such as zero and reduced tillage can be supplemented with specific crop rotation methods to reduce raindrop impact, sediment detachment, sediment transport, surface runoff, and soil loss.\n\nProtection against soil loss is maximized with rotation methods that leave the greatest mass of crop stubble (plant residue left after harvest) on top of the soil. Stubble cover in contact with the soil minimizes erosion from water by reducing overland flow velocity, stream power, and thus the ability of the water to detach and transport sediment. Soil Erosion and Cill prevent the disruption and detachment of soil aggregates that cause macropores to block, infiltration to decline, and runoff to increase. This significantly improves the resilience of soils when subjected to periods of erosion and stress.\n\nWhen a forage crop breaks down, binding products are formed that act like an adhesive on the soil, which makes particles stick together, and form aggregates. The formation of soil aggregates is important for erosion control, as they are better able to resist raindrop impact, and water erosion. Soil aggregates also reduce wind erosion, because they are larger particles, and are more resistant to abrasion through tillage practices.\n\nThe effect of crop rotation on erosion control varies by climate. In regions under relatively consistent climate conditions, where annual rainfall and temperature levels are assumed, rigid crop rotations can produce sufficient plant growth and soil cover. In regions where climate conditions are less predictable, and unexpected periods of rain and drought may occur, a more flexible approach for soil cover by crop rotation is necessary. An opportunity cropping system promotes adequate soil cover under these erratic climate conditions. In an opportunity cropping system, crops are grown when soil water is adequate and there is a reliable sowing window. This form of cropping system is likely to produce better soil cover than a rigid crop rotation because crops are only sown under optimal conditions, whereas rigid systems are not necessarily sown in the best conditions available.\n\nCrop rotations also affect the timing and length of when a field is subject to fallow. This is very important because depending on a particular region's climate, a field could be the most vulnerable to erosion when it is under fallow. Efficient fallow management is an essential part of reducing erosion in a crop rotation system. Zero tillage is a fundamental management practice that promotes crop stubble retention under longer unplanned fallows when crops cannot be planted. Such management practices that succeed in retaining suitable soil cover in areas under fallow will ultimately reduce soil loss. In a recent study that lasted a decade, it was found that a common winter cover crop after potato harvest such as fall rye can reduce soil run-off by as much as 43%, and this is typically the most nutritional soil.\n\nIncreasing the biodiversity of crops has beneficial effects on the surrounding ecosystem and can host a greater diversity of fauna, insects, and beneficial microorganisms in the soil. Some studies point to increased nutrient availability from crop rotation under organic systems compared to conventional practices as organic practices are less likely to inhibit of beneficial microbes in soil organic matter, such as arbuscular mycorrhizae, which increase nutrient uptake in plants. Increasing biodiversity also increases the resilience of agro-ecological systems.\n\nCrop rotation contributes to increased yields through improved soil nutrition. By requiring planting and harvesting of different crops at different times, more land can be farmed with the same amount of machinery and labour.\n\nDifferent crops in the rotation can reduce the risks of adverse weather for the individual farmer.\n\nWhile crop rotation requires a great deal of planning, crop choice must respond to a number of fixed conditions (soil type, topography, climate, and irrigation) in addition to conditions that may change dramatically from year to the next (weather, market, labor supply). In this way, it is unwise to plan crops years in advance. Improper implementation of a crop rotation plan may lead to imbalances in the soil nutrient composition or a buildup of pathogens affecting a critical crop. The consequences of faulty rotation may take years to become apparent even to experienced soil scientists and can take just as long to correct.\n\nMany challenges exist within the practices associated with crop rotation. For example, green manure from legumes can lead to an invasion of snails or slugs and the decay from green manure can occasionally suppress the growth of other crops.\n\n\n"}
{"id": "153783", "url": "https://en.wikipedia.org/wiki?curid=153783", "title": "Crystal optics", "text": "Crystal optics\n\nCrystal optics is the branch of optics that describes the behaviour of light in \"anisotropic media\", that is, media (such as crystals) in which light behaves differently depending on which direction the light is propagating. The index of refraction depends on both composition and crystal structure and can be calculated using the Gladstone–Dale relation. Crystals are often naturally anisotropic, and in some media (such as liquid crystals) it is possible to induce anisotropy by applying an external electric field.\n\nTypical transparent media such as glasses are \"isotropic\", which means that light behaves the same way no matter which direction it is travelling in the medium. In terms of Maxwell's equations in a dielectric, this gives a relationship between the electric displacement field D and the electric field E:\n\nwhere ε is the permittivity of free space and P is the electric polarization (the vector field corresponding to electric dipole moments present in the medium). Physically, the polarization field can be regarded as the response of the medium to the electric field of the light.\n\nIn an isotropic and linear medium, this polarization field P is proportional and parallel to the electric field E:\n\nwhere χ is the \"electric susceptibility\" of the medium. The relation between D and E is thus:\n\nwhere\n\nis the dielectric constant of the medium. The value 1+χ is called the \"relative permittivity\" of the medium, and is related to the refractive index \"n\", for non-magnetic media, by\n\nIn an anisotropic medium, such as a crystal, the polarisation field P is not necessarily aligned with the electric field of the light E. In a physical picture, this can be thought of as the dipoles induced in the medium by the electric field having certain preferred directions, related to the physical structure of the crystal. This can be written as:\n\nHere χ is not a number as before but a tensor of rank 2, the \"electric susceptibility tensor\". In terms of components in 3 dimensions:\n\nformula_7\n\nor using the summation convention:\n\nSince χ is a tensor, P is not necessarily colinear with E.\n\nIn nonmagnetic and transparent materials, χ = χ, i.e. the χ tensor is real and symmetric. In accordance with the spectral theorem, it is thus possible to diagonalise the tensor by choosing the appropriate set of coordinate axes, zeroing all components of the tensor except χ, χ and χ. This gives the set of relations:\n\nThe directions x, y and z are in this case known as the \"principal axes\" of the medium. Note that these axes will be orthogonal if all entries in the χ tensor are real, corresponding to a case in which the refractive index is real in all directions.\n\nIt follows that D and E are also related by a tensor:\n\nHere ε is known as the \"relative permittivity tensor\" or \"dielectric tensor\". Consequently, the refractive index of the medium must also be a tensor. Consider a light wave propagating along the z principal axis polarised such the electric field of the wave is parallel to the x-axis. The wave experiences a susceptibility χ and a permittivity ε. The refractive index is thus:\n\nFor a wave polarised in the y direction:\n\nThus these waves will see two different refractive indices and travel at different speeds. This phenomenon is known as \"birefringence\" and occurs in some common crystals such as calcite and quartz. \n\nIf χ = χ ≠ χ, the crystal is known as uniaxial. (See Optic axis of a crystal.) If χ ≠ χ and χ ≠ χ the crystal is called biaxial. A uniaxial crystal exhibits two refractive indices, an \"ordinary\" index (\"n\") for light polarised in the x or y directions, and an \"extraordinary\" index (\"n\") for polarisation in the z direction. A uniaxial crystal is \"positive\" if n > n and \"negative\" if n < n. Light polarised at some angle to the axes will experience a different phase velocity for different polarization components, and cannot be described by a single index of refraction. This is often depicted as an index ellipsoid.\n\nCertain nonlinear optical phenomena such as the electro-optic effect cause a variation of a medium's permittivity tensor when an external electric field is applied, proportional (to lowest order) to the strength of the field. This causes a rotation of the principal axes of the medium and alters the behaviour of light travelling through it; the effect can be used to produce light modulators.\n\nIn response to a magnetic field, some materials can have a dielectric tensor that is complex-Hermitian; this is called a gyro-magnetic or magneto-optic effect. In this case, the principal axes are complex-valued vectors, corresponding to elliptically polarized light, and time-reversal symmetry can be broken. This can be used to design optical isolators, for example.\n\nA dielectric tensor that is not Hermitian gives rise to complex eigenvalues, which corresponds to a material with gain or absorption at a particular frequency.\n\n"}
{"id": "30872162", "url": "https://en.wikipedia.org/wiki?curid=30872162", "title": "DNA barcoding", "text": "DNA barcoding\n\nDNA barcoding is a taxonomic method that uses a short genetic marker in an organism's DNA to identify it as belonging to a particular species. It differs from molecular phylogeny in that the main goal is not to determine patterns of relationship but to identify an unknown sample in terms of a preexisting classification. Although barcodes are sometimes used in an effort to identify unknown species or assess whether species should be combined or separated, the utility of DNA barcoding for these purposes is subject to debate.\nThe most commonly used barcode region for animals and some protists is a segment of approximately 600 base pairs of the mitochondrial gene cytochrome oxidase I (COI or COX1). This differs in the case of fungi, where part of Internal Transcribed Spacer 2 (ITS2) between rRNA genes is used, and again in plants, where multiple regions are used.\n\nApplications include, for example, identifying plant leaves even when flowers or fruit are not available, identifying pollen collected on the bodies of pollinating animals, identifying insect larvae (which may have fewer diagnostic characters than adults and are frequently less well-known), identifying the diet of an animal, based on its stomach contents or faeces and identifying products in commerce (for example, herbal supplements, wood, or skins and other animal parts).\n\nAlthough the use of nucleotide sequence variations to investigate evolutionary relationships is not a new concept (Carl Woese used sequence differences in ribosomal RNA (rRNA) to discover archaea, which in turn led to the redrawing of the evolutionary tree, and molecular markers (e.g., allozymes, rDNA, and mtDNA sequences) have been successfully used in molecular systematics for decades), the modern concept of DNA barcoding as a proposed standardized method for identifying species, as well as potentially allocating unknown sequences to higher taxa such as orders and phyla, originates from a 2003 paper by Paul D.N. Hebert and co-workers from the University of Guelph, Ontario, Canada. Hebert and his colleagues demonstrated the utility of a 648 base pair region of the cytochrome \"c\" oxidase I (COI) gene, first utilized by O. Folmer and co-workers at Rutgers University in 1994 as a tool for phylogenetic analyses at the species and higher taxonomic levels, as a suitable discriminatory tool between metazoan animal species. The study authors created a COI \"profile\" for eight of the most diverse orders of insects, based on a single representative from each of 100 different families, and showed that this profile assigned each of 50 newly analysed taxa to its correct order; they then created a COI profile for 200 closely allied species of the insect order Lepidoptera, and employed the method to successfully assign 150 newly analysed individuals to species.\n\nCalling the profiles \"barcodes\", Hebert \"et al.\" envisaged the development of a COI database that could serve as the basis for a \"global bioidentification system\", and wrote: \"When fully developed, a COI identification system will provide a reliable, cost-effective and accessible solution to the current problem of species identification. Its assembly will also generate important new insights into the diversification of life and the rules of molecular evolution.\" In a follow-up paper, Hebert and different co-authors tested COI differences in congeneric species pairs (2,238 species) from 11 phyla of animals plus the four dominant orders of insects (Coleoptera, Diptera, Lepidoptera and Hymenoptera) as well as \"other insects\" and concluded that species level discrimination was satisfactory using the proposed COI gene region in all the groups studied with the exception of Cnidaria, which they ascribed to the exceptionally low rates of mitochondrial evolution in the latter group.\n\nSince that time, DNA barcoding has been widely adopted in numerous studies of animals in particular, using the initially proposed \"Folmer region\" of the COI gene, based on its patterns of variation at the DNA level, the relative ease of retrieving the sequence, and its characteristic of being sufficiently conserved within species, yet sufficiently variable between species to enable reliable identification of each taxon. Global DNA barcoding was initially regarded as a \"big science\" programme and even as the renaissance of taxonomy. However, the COI sequence, which has been developed as a universal barcode in animals, does not discriminate most plants or fungi because of a much slower mutation rate in those groups; barcoding of protists also presents some challenges, as documented by Pawlowski \"et al.\", 2012.\n\nCoordination of global activities in DNA Barcoding is now managed via the Consortium for the Barcode of Life (CBOL). Vouchered DNA sequences are deposited in the publicly accessible Barcode of Life Data Systems (BOLD) database which, as of June 2017, contained nearly 5,500,000 barcode sequences from over 265,000 species of animals, plants, and fungi.\n\nA desirable locus for DNA barcoding should be standardized (so that large databases of sequences for that locus can be developed), present in most of the taxa of interest and sequenceable without species-specific PCR primers, short enough to be easily sequenced with current technology, and provide a large variation between species yet a relatively small amount of variation within a species.\n\nAlthough several loci have been suggested, a common set of standardized regions were selected by the respective committees of COBOL:\n\n\nFor protists, a final recommendation has not yet been made; a 2012 Working Group report suggests that a 2-stage approach will most likely be required, using a \"pre-barcode\" based on 18S rDNA followed by a yet to be defined second test according to the result of the first (additional details given below).\n\nDNA barcoding of animals is based on a relatively simple concept. All eukaryote cells contain mitochondria, and animal mitochondrial DNA (mtDNA) has a relatively fast mutation rate, resulting in the generation of diversity within and between populations over relatively short evolutionary timescales (thousands of generations). Typically, in animals, a single mtDNA genome is transmitted to offspring by each breeding female, and the genetic effective population size is proportional to the number of breeding females. This contrasts with the nuclear genome, which is around 100 000 times larger, where males and females each contribute two full genomes to the gene pool and effective size is therefore proportional to twice the total population size. This reduction in effective population size leads to more rapid sorting of mtDNA gene lineages within and among populations through time, due to variance in fecundity among individuals (the principle of coalescence). The combined effect of higher mutation rates and more rapid sorting of variation usually results in divergence of mtDNA sequences among species and a comparatively small variance within species.\n\nExceptions, where mtDNA fails as a test of species identity, can occur through occasional recombination (direct evidence for recombination in mtDNA is available in some bivalves such as \"Mytilus\" but it is suspected that it may be more widespread) and through occurrences of hybridization. Male-killing microorganisms, cytoplasmic incompatibility-inducing symbionts (e.g., \"Wolbachia\"), as well as heteroplasmy, where an individual carries two or more mtDNA sequences, may affect patterns of mtDNA diversity within species, although these do not necessarily result in bar-coding failure. Occasional horizontal gene transfer (such as via cellular symbionts), or other \"reticulate\" evolutionary phenomena in a lineage can lead to misleading results (i.e., it is possible for two different species to share mtDNA). In particular, mtDNA seems to be particularly prone to interspecific introgression probably due to difference between sexes in mate-choice and dispersal. Additionally, some species may carry divergent mtDNA lineages segregating within populations, often due to historical geographic structure, where these divergent lineages do not reflect species boundaries.\n\nA 2017 study by Rach \"et al.\" in the dragonflies and the damselflies (Odonata), a basal group of insects, found that the \"standard\" (Folmer) region of the COI gene was sub-optimal for species resolution in that group, and that a different portion of the same gene, which they termed COIB, showed higher success in discriminating sister taxa at different taxonomic levels. These authors therefore suggested that a layered barcode approach, i.e. adding a second, a third or even more additional markers to enhance the discrimination potential, may be desirable, particularly in metabarcoding studies where the taxonomic composition within the samples may not be known in advance.\n\nIn Cnidaria, where the COI gene has been found to be unsuitable on account of its slow rate of evolution in that group, more success has been reported using a combination of COI plus a short, adjacent intergenic region (igr1) plus a fragment of the octocoral‐specific mitochondrial protein‐coding gene, msh1 in octocorals, and the 16S mitochondrial ribosomal RNA gene in pelagic forms. In sponges, the other major non-Bilaterian animal group, congeneric species are difficult to amplify or separate with the standard COI barcoding fragment, and data compilation and study is presently focussed on the ribosomal RNA 28S C-Region.\n\nThe use of the COI sequence is not appropriate in plants because of slower rate of cytochrome c oxidase I gene evolution in higher plants than in animals. A series of experiments was conducted to find a more suitable region of the genome for use in the DNA barcoding of flowering plants (or the larger group of land plants). Nuclear internal transcribed spacer region and the plastid trnH-psbA intergenic spacer; other researchers advocated other regions such as matK.\n\nTwo chloroplast genes, the combination of rbcL and matK have been proposed as a barcode for plants. Adding the nuclear internal transcribed spacer ITS2 region was proposed to provide better resolution between species. The chloroplast region \"ycf1\" may be a more suitable gene.\n\nAs noted above, the current, officially approved barcoding locus for fungi is the ITS region, chosen from a group of six candidates (SSU, LSU, ITS, RPB1, RPB2, MCM7) as the most broadly applicable across major fungal lineages. However, the ITS region has been noted as not working well in some highly speciose genera such as \"Aspergillus\", \"Cladosporium\", \"Fusarium\", \"Penicillium\" and \"Trichoderma\", since these taxa have narrow or no barcode gaps in their ITS regions; it may therefore be necessary to sequence one or more single-copy protein-coding genes as a secondary barcode marker for certain fungal genera and/or lineages in order to obtain the most precise identifications at the species level. Stielow \"et al.\" (2015) also discuss the applicability of a number of potential secondary fungal DNA barcodes including TEF1α, TOPI, PGK and LNS2 in particular groups.\n\nThe Protist Working Group (ProWG) of the Consortium for the Barcode of Life (CBOL) reported that for protists—a \"convenience\" group of mainly single-celled eukaryotes representing many diverse lineages presently characterized as a range of \"supergroups\"—a 2-stage strategy is recommended: first, a preliminary identification using a universal eukaryotic barcode, called the pre-barcode, proposed to be the ∼500 base pair variable V4 region of 18S rDNA, followed by a second, group-specific barcode yet to be fully defined, for which stated possibilities include 28S rDNA, ITS rDNA, 18S rDNA, COI, rbcL, SL RNA and perhaps more.\n\nDNA sequence databases like GenBank contain many sequences that are not tied to vouchered specimens (for example, herbarium specimens, cultured cell lines, or sometimes images). This is problematic in the face of taxonomic issues such as whether several species should be split or combined, or whether past identifications were sound. Therefore, best practice for DNA barcoding is to sequence vouchered specimens.\n\nIn an effort to find a relationship between traditional species boundaries established by taxonomy and those inferred by DNA barcoding, Hebert and co-workers sequenced DNA barcodes of 260 of the 667 bird species that breed in North America (Hebert \"et al.\" 2004a). They found that every single one of the 260 species had a different COI sequence. 130 species were represented by two or more specimens; in all of these species, COI sequences were either identical or were most similar to sequences of the same species. COI variations between species averaged 7.93%, whereas variation within species averaged 0.43%. In four cases there were deep intraspecific divergences, indicating possible new species. Three out of these four polytypic species are already split into two by some taxonomists. Hebert \"et al.\"'s (2004a) results reinforce these views and strengthen the case for DNA barcoding. Hebert \"et al.\" also proposed a standard sequence threshold to define new species, this threshold, the so-called \"barcoding gap\", was defined as 10 times the mean intraspecific variation for the group under study.\n\nThe Fish Barcode of Life Initiative (FISH-BOL), is a global effort to coordinate an assembly of a standardised DNA barcode library for all fish species, one that is derived from voucher specimens with authoritative taxonomic identifications. The benefits of barcoding fishes include facilitating species identification for all potential users, including taxonomists; highlighting specimens that represent a range expansion of known species; flagging previously unrecognized species; and perhaps most importantly, enabling identifications where traditional methods are not applicable. An example is the possible identification of groupers causing Ciguatera fish poisoning from meal remnants.\n\nSince its inception in 2005 FISH-BOL has been creating a valuable public resource in the form of an electronic database containing DNA barcodes for almost 10000 species, images, and geospatial coordinates of examined specimens. The database contains linkages to voucher specimens, information on species distributions, nomenclature, authoritative taxonomic information, collateral natural history information and literature citations. FISH-BOL thus complements and enhances existing information resources, including the Catalog of Fishes, FishBase and various genomics databases .\n\nThe next major study into the efficacy of DNA barcoding was focused on the neotropical skipper butterfly, \"Astraptes fulgerator\" at the Area de Conservación de Guanacaste (ACG) in north-western Costa Rica. This species was already known as a cryptic species complex, due to subtle morphological differences, as well as an unusually large variety of caterpillar food plants. However, several years would have been required for taxonomists to completely delimit species. Hebert \"et al.\" (2004b) sequenced the COI gene of 484 specimens from the ACG. This sample included \"at least 20 individuals reared from each species of food plant, extremes and intermediates of adult and caterpillar color variation, and representatives\" from the three major ecosystems where \"Astraptes fulgerator\" is found. Hebert \"et al.\" (2004b) concluded that \"Astraptes fulgerator\" consists of 10 different species in north-western Costa Rica. These results, however, were subsequently challenged by Brower (2006), who pointed out numerous serious flaws in the analysis, and concluded that the original data could support no more than the possibility of three to seven cryptic taxa rather than ten cryptic species. This highlights that the results of DNA barcoding analyses can be dependent upon the choice of analytical methods used by the investigators, so the process of delimiting cryptic species using DNA barcodes can be as subjective as any other form of taxonomy.\n\nA more recent example used DNA barcoding for the identification of cryptic species included in the ongoing long-term database of tropical caterpillar life generated by Dan Janzen and Winnie Hallwachs in Costa Rica at the ACG. In 2006 Smith \"et al.\" examined whether a COI DNA barcode could function as a tool for identification and discovery for the 20 morphospecies of \"Belvosia\" parasitoid flies (Tachinidae) that have been reared from caterpillars in ACG. Barcoding not only discriminated among all 17 highly host-specific morphospecies of ACG \"Belvosia\", but it also suggested that the species count could be as high as 32 by indicating that each of the three generalist species might actually be arrays of highly host-specific cryptic species.\n\nIn 2007 Smith \"et al.\" expanded on these results by barcoding 2,134 flies belonging to what appeared to be the 16 most generalist of the ACG tachinid morphospecies. They encountered 73 mitochondrial lineages separated by an average of 4% sequence divergence and, as these lineages are supported by collateral ecological information, and, where tested, by independent nuclear markers (28S and ITS1), the authors therefore viewed these lineages as provisional species. Each of the 16 initially apparent generalist species were categorized into one of four patterns: (i) a single generalist species, (ii) a pair of morphologically cryptic generalist species, (iii) a complex of specialist species plus a generalist, or (iv) a complex of specialists with no remaining generalist. In sum, there remained 9 generalist species classified among the 73 mitochondrial lineages analyzed.\n\nHowever, also in 2007, Whitworth \"et al.\" reported that flies in the related family Calliphoridae could not be discriminated by barcoding. They investigated the performance of barcoding in the fly genus \"Protocalliphora\", known to be infected with the endosymbiotic bacteria \"Wolbachia\". Assignment of unknown individuals to species was impossible for 60% of the species, and if the technique had been applied, as in the previous study, to identify new species, it would have underestimated the species number in the genus by 75%. They attributed the failure of barcoding to the non-monophyly of many of the species at the mitochondrial level; in one case, individuals from four different species had identical barcodes. The authors went on to state: \n\nMwabvu \"et al.\" (2013) observed a high level of divergence (19.09% for CO1, 520 base pairs) between two morphologically indistinguishable populations of \"Bicoxidens flavicollis\" millipedes in Zimbabwe, and suggested the presence of cryptic species in \"Bicoxidens flavicollis\".\n\nMarine biologists have also considered the value of the technique in identifying cryptic and polymorphic species and have suggested that the technique may be helpful when associations with voucher specimens are maintained, though cases of \"shared barcodes\" (e.g., non-unique) have been documented in cichlid fishes and cowries\n\nLambert \"et al.\" (2005) examined the possibility of using DNA barcoding to assess the past diversity of the Earth's biota. The COI gene of a group of extinct ratite birds, the moa, were sequenced using 26 subfossil moa bones. As with Hebert's results, each species sequenced had a unique barcode and intraspecific COI sequence variance ranged from 0 to 1.24%. To determine new species, a standard sequence threshold of 2.7% COI sequence difference was set. This value is 10 times the average intraspecies difference of North American birds, which is inconsistent with Hebert's recommendation that the threshold value be based on the group under study. Using this value, the group detected six moa species. In addition, a further standard sequence threshold of 1.24% was also used. This value resulted in 10 moa species which corresponded with the previously known species with one exception. This exception suggested a possible complex of species which was previously unidentified. Given the slow rate of growth and reproduction of moa, it is probable that the interspecies variation is rather low. On the other hand, there is no set value of molecular difference at which populations can be assumed to have irrevocably started to undergo speciation. It is safe to say, however, that the 2.7% COI sequence difference initially used was far too high.\n\nThe Moorea Biocode Project is a barcoding initiative to create the first comprehensive inventory of all non-microbial life in a complex tropical ecosystem, the island of Moorea in Tahiti. Supported by a grant from the Gordon and Betty Moore Foundation, the Moorea Biocode Project is a 3-year project that brings together researchers from the Smithsonian Institution, UC Berkeley, France’s National Center for Scientific Research (CNRS), and other partners. The outcome of the project is a library of genetic markers and physical identifiers for every species of plant, animal and fungi on the island that will be provided as a publicly available database resource for ecologists and evolutionary biologists around the world.\n\nThe software back-end to the Moore Biocode Project is Geneious Pro and two custom-developed plugins from the New Zealand-based company, Biomatters. The Biocode LIMS and Genbank Submission plugins have been made freely available to the public and users of the free Geneious Basic software will be able to access and view the Biocode database upon completion of the project, while a commercial copy of Geneious Pro is required for researchers involved in data creation and analysis.\n\nIn the initial years following its proposal, DNA barcoding met with spirited reaction from scientists, especially systematists, ranging from enthusiastic endorsement to vociferous opposition. For example, some stressed the fact that DNA barcoding does not provide reliable information above the species level, while others opined that it was inapplicable at the species level, but may still have merit for higher-level groups. Others resented what they saw as a gross oversimplification of the science of taxonomy. And, more practically, some suggested that recently diverged species might not be distinguishable on the basis of their COI sequences. In an early study, Funk & Omland (2003) found that some 23% of animal species were polyphyletic if their mtDNA data were accurate, indicating that using an mtDNA barcode to assign a species name to an animal would be ambiguous or erroneous in those cases (see also Meyer & Paulay, 2005). Some studies with insects suggested an equal or even greater error rate, due to the frequent lack of correlation between the mitochondrial genome and the nuclear genome or the lack of a barcoding gap (e.g., Hurst and Jiggins, 2005, Whitworth \"et al.\", 2007, Wiemers & Fiedler, 2007).\n\nMoritz and Cicero (2004) questioned the efficacy of DNA barcoding by suggesting that other avian data is inconsistent with Hebert \"et al.\"'s interpretation, namely, Johnson and Cicero's (2004) finding that 74% of sister species comparisons fall below the 2.7% threshold suggested by Hebert \"et al.\" These criticisms are somewhat misleading considering that, of the 39 species comparisons reported by Johnson and Cicero, only 8 actually use COI data to arrive at their conclusions. Johnson and Cicero (2004) have also claimed to have detected bird species with identical DNA barcodes, however, these 'barcodes' refer to an unpublished 723-bp sequence of ND6 which has never been suggested as a likely candidate for DNA barcoding.\n\nThe criticisms given above date from the first few years following Hebert's initial (2003) papers in which the method was proposed. Writing in 2016, with 13 years elapsed since their initial proposal, Hebert and co-workers wrote:\n[In animals,] DNA barcodes typically discriminate about 95% of known species; cases of compromised resolution involve sister taxa, often species that hybridize. In the many taxa where geographical variation in barcode sequences is small, a few records per species are sufficient to create an effective identification system. However, the analysis of more specimens is advantageous because it often reveals discordances that indicate misidentifications or cryptic taxa, and it also provides insights into the extent of geographical variation in barcode sequences. There are two animal phyla in which COI often fails to deliver species-level resolution, sponges and some benthic cnidarians, apparently because of their slowed rates of mitochondrial evolution. Barcoding also fails to distinguish a small fraction of species in other groups, typically sister taxa or those whose status is uncertain.\n\nIn a more recent (2018) review, M. Stoeckle and D. Thaler write:\nThe current field of COI barcodes is no longer fragile but neither is it complete. As of late 2016 there were close to five million COI barcodes between the GenBank and BOLD databases. Objections can now be seen in the cumulative light of these data and more than a decade’s experience. There is no longer any doubt that DNA barcodes are useful and practical. The agreement with specialists encompasses most cases in several important animal domains. Many cases where DNA barcodes and domain specialists do not agree reflect geographic splits within species or hybridization between species. Others upon further investigation been attributed to mislabeling or sequence error. Some may represent \"bona fide\" exceptions to the rule that mitochondrial sequence clusters coincide with species defined by other means. In the great majority of cases COI barcodes yield a close approximation of what specialists come up with after a lot of study. Birds are one of the best characterized of all animal groups and COI barcode clusters have been tabulated as agreeing with expert taxonomy for 94% of species.\n\nAs noted above, the current status of barcoding for vascular plants is presently both less settled and less effective than for animals. In a recent study covering most (96%) of the 5108 vascular plant species known from Canada, the three barcode markers tested (matK, ITS2 and rbcL) were all effective at discriminating genera (98%, 97% and 91%, respectively); at species level, matK delivered the highest discrimination (81%) followed by ITS2 (72%) and rbcL (44%), however the effectiveness of matK was also variable by biogeographic region, varying from 69%-87% according to the geographic origin of the plants concerned. Resolution also varied by family, with the poorest species discrimination within Canadian species of Salicaceae, Asteraceae and Fabaceae. The authors of this study did not report on the combined efficacy of either any two, or all three markers, in part due to sampling limitations, but commented that although ITS2 showed slightly lower performance, it had two important advantages (its short length making it suitable for high-throughput sequencing (HTS)-based applications, and it is readily recovered from diverse taxa, including vascular plants and fungi), and looked forward to the development of more comprehensive reference libraries of both matK and ITS2 to further assist in the identification of unknown samples.\n\nSoftware for DNA barcoding requires integration of a field information management system (FIMS), laboratory information management system (LIMS), sequence analysis tools, workflow tracking to connect field data and laboratory data, database submission tools and pipeline automation for scaling up to eco-system scale projects. Geneious Pro can be used for the sequence analysis components, and the two plugins made freely available through the Moorea Biocode Project, the Biocode LIMS and Genbank Submission plugins handle integration with the FIMS, the LIMS, workflow tracking and database submission.\n\nThe Barcode of Life Data Systems (BOLD) is a web based workbench and database supporting the acquisition, storage, analysis, and publication of DNA barcode records. By assembling molecular, morphological, and distributional data, it bridges a traditional bioinformatics chasm. BOLD is the most prominently used barcoding software and is freely available to any researcher with interests in DNA barcoding. By providing specialized services, it aids the assembly of records that meet the standards needed to gain BARCODE designation in the global sequence databases. Because of its web-based delivery and flexible data security model, it is also well positioned to support projects that involve broad research alliances.\n\n"}
{"id": "47431960", "url": "https://en.wikipedia.org/wiki?curid=47431960", "title": "Die Knoff-Hoff-Show", "text": "Die Knoff-Hoff-Show\n\nDie Knoff-Hoff-Show was a comedy science TV show on the German public broadcaster ZDF. The original series was broadcast between 1986 and 1999; it returned as \"Die große Knoff-Hoff-Show\" in 2002–04. The name is a joke German pronunciation of the English expression know-how.\n\nThe concept of the show was developed in the mid-1980s by , a TV presenter trained as a physicist. He was a presenter in all episodes. His co-presenters were , , Monica Lierhaus and .\n\nThe show explained scientific concepts by means of simple experiments that anyone could replicate. In addition, hobbyists were given the opportunity to present their inventions; these included a pretzel-cutting machine and a foam-throwing machine. Each week, the show included some \"crazy\" experiments by Knoff-Hoff Professor Charlie (played by Egon Keresztes); these were so absurd, they frequently went wrong.\n\nThe \"Veterinary Street Jazz Band\" played the theme tune, an interpretation of the 1927 song \"Ain’t She Sweet\", first recorded by , at the start and end of the program, and also brief musical interludes between segments.\n\n\"Die Knoff-Hoff-Show\" rapidly became one of the most successful science shows on German television. It was dubbed in nine languages and shown on other continents.\n\nThe show aired for the first time on February 16, 1986, and ended with Episode 79 on March 21, 1999. Beginning in 2002 ZDF broadcast a second series, titled \"Die große Knoff-Hoff-Show\" (The Big Knoff-Hoff Show); this ended in December 2004. According to Bublath, it was canceled because it proved impossible to keep up the pace of experiments (\"a new experiment every minute\").\n\nThere were also two special broadcasts in summer 2005 under the title \"Der Sommer mit Knoff-Hoff\" (Summer with Knoff-Hoff).\n\n"}
{"id": "51050351", "url": "https://en.wikipedia.org/wiki?curid=51050351", "title": "Dominique Bergmann", "text": "Dominique Bergmann\n\nDominique C. Bergmann is a plant scientist working on cell differentiation, stem cell renewal, and cell polarity with a focus on guard cell development as a model for all three cellular behaviors. She is a professor at Stanford University. \n\nHer lab has a website with more information about her research program. For the last several years she has been a Gordon and Betty Moore HHMI funded researcher.\n\nBergmann won the American Society of Plant Biologists' Charles Schull Award in 2010. She was elected to the National Academy of Sciences in 2017.\n"}
{"id": "14615313", "url": "https://en.wikipedia.org/wiki?curid=14615313", "title": "Earth Expeditions", "text": "Earth Expeditions\n\nEarth Expeditions is a global education and conservation program offering graduate courses worldwide. The program was created by Project \"Dragonfly\" at Miami University in Oxford, Ohio. Earth Expeditions courses also count toward a master's degree in global leadership, education, and environmental stewardship through the Global Field Program (GFP).\n\nThe \"Earth Expeditions\" program began in 2003 as a global education and conservation initiative from Miami University. Earth Expeditions graduate courses can be used toward a Master of Arts in Teaching in the Biological Sciences or a Master of Arts in Biology from Miami. Example Earth Expeditions graduate courses:\n\n\nThe mission of Earth Expeditions is to build an alliance of people with direct knowledge of inquiry-driven, community-based learning for the benefit of ecological communities, student achievement, and global understanding.\nThe image of people who merely transmit and receive knowledge from books is of limited usefulness in an era of rapid social, environmental and technological change. This program envisions each person as an ambassador who creates and transmits knowledge and who promotes authentic dialogue at all levels of society, inspiring others to do the same. With the adoption of participatory models of education, schools become centers of investigation, students engage more deeply in their studies, and communities achieve higher levels of self-determination.\n\nSince its inception nearly 20 years ago, Project \"Dragonfly\" has reached millions of children, parents and educators through science learning media, exhibits, and graduate programs. The project began in 1994 with the creation of the award-winning \"Dragonfly\" magazine, the first national magazine to feature the investigations and discoveries of children. Created in 1994 at Miami University, \"Dragonfly\" magazine was funded by NSF and published by the National Science Teachers Association. Through mid-2000, the magazine published young investigators alongside such adult researchers as Dr. Jane Goodall.\n\nProject \"Dragonfly\" pioneered the \"Real Kids, Real Science\" approach to learning and continues to work for inquiry-driven reform to increase public involvement in science and global understanding. In addition to Earth Expeditions, Project \"Dragonfly\" worked with TPT Public Television to launch the Emmy-Award winning PBS children's television series \"DragonflyTV\", which led to the 2010 launch of the spinoff series \"SciGirls\", both produced by TPT Public Television. Project \"Dragonfly\" also oversees two national exhibit projects: Wild Research and iSaveSpecies, which are creating public research stations at zoos and aquariums nationwide to engage families in science inquiry and conservation action. The National Science Foundation (NSF) has called \"Dragonfly\" \"A true innovation and a model of what active learning should be.\"\n\nCincinnati Zoo and Botanical Garden opened in 1875 and is the second oldest zoo in the nation. This national historical landmark hosts more than 500 animal species and 3,000 plant species. More than 1.2 million people visit the Cincinnati Zoo annually. A non-profit entity, the zoo is internationally known for its success in the protection and propagation of plants and animals in danger and engages in research and conservation projects around the world.\n\n\n Myers, C., Myers, L.B., & Hudson, R. (2009) Science is not a spectator sport: Three principles from 15 years of Project \"Dragonfly\". In R. Yager (Ed.), \"Inquiry: The key to exemplary science\" (pp. 29–40. Arlington, Virginia: NSTA Press.\n\n"}
{"id": "17780837", "url": "https://en.wikipedia.org/wiki?curid=17780837", "title": "Empirical statistical laws", "text": "Empirical statistical laws\n\nAn empirical statistical law or (in popular terminology) a law of statistics represents a type of behaviour that has been found across a number of datasets and, indeed, across a range of types of data sets. Many of these observances have been formulated and proved as statistical or probabilistic theorems and the term \"law\" has been carried over to these theorems. There are other statistical and probabilistic theorems that also have \"law\" as a part of their names that have not obviously derived from empirical observations. However, both types of \"law\" may be considered instances of a scientific law in the field of statistics. What distinguishes an empirical statistical law from a formal statistical theorem is the way these patterns simply appear in natural distributions, without a prior theoretical reasoning about the data. \n\nThere are several such popular \"laws of statistics\". \n\nThe Pareto principle is a popular example of such a \"law\". It states that roughly 80% of the effects come from 20% of the causes, and is thusly also known as the 80/20 rule. In business, the 80/20 rule says that 80% of your business comes from just 20% of your customers. In software engineering, it's often said that 80% of the errors are caused by just 20% of the bugs. 20% of the world creates roughly 80% of worldwide GDP. 80% of healthcare expenses in the US are caused by 20% of the population.\n\nZipf's law, described as an \"empirical statistical law\" of linguistics, is another example. According to the \"law\", given some dataset of text, the frequency of a word is inversely proportional to its frequency rank. In other words, the second most common word should appear about half as often as the most common word, and the fifth most common world would appear about once every five times the most common word appears. However, what sets Zipf's law as an \"empirical statistical law\" rather than just a theorem of linguistics is that it applies to phenomena outside of its field, too. For example, a ranked list of US metropolitan populations also follow Zipf's law, and even forgetting follows Zipf's law. This act of summarizing several natural data patterns with simple rules is a defining characteristic of these \"empirical statistical laws\". \n\nExamples of empirically inspired statistical laws that have a firm theoretical basis include:\n\nExamples of \"laws\" with a weaker foundation include: \n\nExamples of \"laws\" which are more general observations than having a theoretical background:\n\nExamples of supposed \"laws\" which are incorrect include:\n\n"}
{"id": "213665", "url": "https://en.wikipedia.org/wiki?curid=213665", "title": "Enthalpy of neutralization", "text": "Enthalpy of neutralization\n\nThe enthalpy of neutralization (Δ\"H\") is the change in enthalpy that occurs when one equivalent of an acid and one equivalent of a base undergo a neutralization reaction to form water and a salt. It is a special case of the enthalpy of reaction. It is defined as the energy released with the formation of 1 mole of water.\n\nWhen a reaction is carried out under standard conditions at the temperature of 298 K (25 degrees Celsius) and 1 atm of pressure and one mole of water is formed it is called the \"standard enthalpy of neutralization\" (Δ\"H\").\n\nThe heat (\"Q\") released during a reaction is\n\nwhere \"m\" is the mass of the solution, \"c\" is the specific heat capacity of the solution, and ∆\"T\" is the temperature change observed during the reaction. From this, the standard enthalpy change (∆H) is obtained by division with the amount of substance (in moles) involved.\n\nWhen a strong acid, HA, reacts with a strong base, BOH, the reaction that occurs is\nas the acid and the base are fully dissociated and neither the cation B nor the anion A are involved in the neutralization reaction. The enthalpy change for this reaction is -57.62 kJ/mol at 25°C.\n\nFor weak acids or bases, the heat of neutralization is pH dependent. In the absence of any added mineral acid or alkali some heat is required for complete dissociation. The total heat evolved during neutralization will be smaller.\n\nThe heat of ionization for this reaction is equal to (–12 + 57.3) = 45.3 kJ/mol at 25°C.\n"}
{"id": "50969944", "url": "https://en.wikipedia.org/wiki?curid=50969944", "title": "Glossary of civil engineering", "text": "Glossary of civil engineering\n\n\"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\"\nThis glossary of civil engineering terms pertains specifically to civil engineering and its sub-disciplines. Please see glossary of engineering for a broad overview of the major concepts of engineering.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "14623681", "url": "https://en.wikipedia.org/wiki?curid=14623681", "title": "Grimke (crater)", "text": "Grimke (crater)\n\nGrimke is a crater on Venus at latitude 17.2, longitude 215.3. It is 34.8 km in diameter and is named after Sarah Grimké.\n"}
{"id": "153961", "url": "https://en.wikipedia.org/wiki?curid=153961", "title": "Heliacal rising", "text": "Heliacal rising\n\nThe heliacal rising (, ) or star rise of a star, star cluster, or galaxy occurs annually when it becomes visible above the eastern horizon for a moment before sunrise, after a period of less than a year when it had not been visible. Historically, the most important such rising is that of Sirius, which was an important feature of the Egyptian calendar and astronomical development. The rising of the Pleiades heralded the start of the Ancient Greek sailing season, using celestial navigation.\n\nRelative to the other stars, the sun appears to drift eastward (about of Earth's orbit—hence almost one degree—per solar day) along a path called the ecliptic (specifically appearing in front of 13 constellations considered the zodiac constellations from a total of 88 modern constellations) which is, by definition, the plane of the earth's orbit. While the sun appears in front of (or south or north of) a relatively small group of stars they can no longer be seen either before dawn, during daytime or after sunset — their appearance coincides with that of the sun above the horizon. Depending on the observer's latitude many stars are subject to annual heliacal risings and settings. Rising means the local latitude of the earth has moved along its orbit such that the star, star cluster or galaxy emerges for part of the year to within months be visible for the whole night and then for the early portion of the night. Thus the star's first emergence, an annual rise, is immediately before dawn. The risen status of each star is easiest considered day-on-day in the tropics where the time of dawn varies less. The rising of a star which has had its annual rising (\"heliacal rising\"), typically over months, rises earlier at night and so at dawn figures more toward its annual highest point (meridian) and then in later dawns more toward the west all by about th of its arc (about of the circle) per day, until, observing the western sky after sunset, it has already disappeared. This is called the \"cosmical setting\". The same star will reappear in the eastern sky at dawn approximately one year after its previous rising. For zodiac and near-zodiac constellations (near the ecliptic, the apparent daily path of the sun), Earth's precession means the date of their rising decreases gradually, completing one cycle in about 26,000 years (for example at the March Equinox, the position of the Sun relative the stars — at which Right Ascension is calibrated as zero, the First Point of Aries, is in the preceding constellation of Pisces.)\n\nSome stars, when viewed from latitudes outside of the tropics on Earth, do not rise or set. These are circumpolar stars, which are either always in the sky or never. For example, the North Star (Polaris) is not visible in Australia and the Southern Cross is not seen in Europe, because they always stay below the respective horizons. \n\nThe term \"circumpolar\" is somewhat localised as between the Tropic of Cancer and the Equator, the Southern polar constellations have a brief spell of annual visibility (thus \"heliacal\" rising and \"cosmic\" setting) and the same applies as to the other polar constellations in respect of the reverse tropic.\n\nConstellations containing stars that rise and set were incorporated into early calendars or zodiacs. The Sumerians, Babylonians, Egyptians, and Greeks all used the heliacal risings of various stars for the timing of agricultural activities.\n\nBecause of its position about 40° off the ecliptic, the heliacal risings of the bright star Sirius occur over a \"Sothic year\" almost exactly synchronized with the solar year. Since the development of civilization, this has occurred at Cairo on July 19 on the Julian calendar. Its returns also roughly corresponded to the onset of the annual flooding of the Nile before it was ended by the Aswan Low and High Dams. The ancient Egyptians appear to have constructed their 365-day civil calendar at a time when Wep Renpet, its New Year, corresponded with Sirius's return to the night sky. Although this calendar's lack of leap years caused the event to shift one day every four years or so, astronomical records of this displacement led to the discovery of the Sothic cycle and, later, the establishment of the more accurate Julian and Alexandrian calendars.\n\nThe Egyptians also devised a method of telling the time at night based on the heliacal risings of 36 decan stars, one for each 10° segment of the 360° circle of the zodiac and corresponding to the ten-day \"weeks\" of their civil calendar.\n\nTo the Māori of New Zealand, the Pleiades are called Matariki, and their heliacal rising signifies the beginning of the new year (around June). The Mapuche of South America called the Pleiades \"Ngauponi\" which in the vicinity of the \"we tripantu\" (Mapuche new year) will disappear by the west, \"lafkenmapu\" or \"ngulumapu\", appearing at dawn to the East, a few days before the birth of new life in nature. Heliacal rising of Ngauponi, i.e. appearance of the Pleiades by the horizon over an hour before the Sun approximately 12 days before the winter solstice, announced \"we tripantu\".\n\nWhen a planet has a heliacal rising, there is a conjunction with the sun beforehand. Depending on the type of conjunction, there may be a syzygy, eclipse, transit, or occultation of the sun.\n\nThe rising of a planet above the eastern horizon at sunset is called its \"acronychal rising\", which for a superior planet signifies an opposition, another type of syzygy. \n\nWhen the Moon has an acronychal rising, it will occur near full moon and thus, in a small number of cases, a noticeable lunar eclipse.\n"}
{"id": "57316443", "url": "https://en.wikipedia.org/wiki?curid=57316443", "title": "Herbert G. Baker", "text": "Herbert G. Baker\n\nHerbert George Baker (February 23, 1920 – July 2, 2001) was a British-American botanist and evolutionary ecologist who was an authority on pollination biology and breeding systems of angiosperms. He originated Baker's law, the idea that the ability to self-fertilize should be common among species which successfully established populations through long-distance dispersal.\n\n"}
{"id": "43963036", "url": "https://en.wikipedia.org/wiki?curid=43963036", "title": "Hierarchy theory", "text": "Hierarchy theory\n\nHierarchy theory is a means of studying ecological systems in which the relationship between all of the components is of great complexity. Hierarchy theory focuses on levels of organization and issues of scale, with a specific focus on the role of the observer in the definition of the system. Complexity in this context does not refer to an intrinsic property of the system but to the possibility of representing the systems in a plurality of non-equivalent ways depending on the pre-analytical choices of the observer. Instead of analyzing the whole structure, hierarchy theory refers to the analysis of hierarchical levels, and the interactions between them.\n\n"}
{"id": "4527733", "url": "https://en.wikipedia.org/wiki?curid=4527733", "title": "High-resolution transmission electron microscopy", "text": "High-resolution transmission electron microscopy\n\nHigh-resolution transmission electron microscopy (HRTEM) (or HREM) is an imaging mode of specialized transmission electron microscopes (TEMs) that allows for direct imaging of the atomic structure of the sample. HRTEM is a powerful tool to study properties of materials on the atomic scale, such as semiconductors, metals, nanoparticles and sp-bonded carbon (e.g., graphene, C nanotubes). While HRTEM is often also used to refer to high resolution scanning TEM (STEM, mostly in high angle annular dark field mode), this article describes mainly the imaging of an object by recording the 2D spatial wave amplitude distribution in the image plane, in analogy to a \"classic\" light microscope. For disambiguation, the technique is also often referred to as phase contrast TEM. At present, the highest point resolution realised in phase contrast TEM is around . At these small scales, individual atoms of a crystal and its defects can be resolved. For 3-dimensional crystals, it may be necessary to combine several views, taken from different angles, into a 3D map. This technique is called electron crystallography.\n\nOne of the difficulties with HRTEM is that image formation relies on phase contrast. In phase-contrast imaging, contrast is not necessarily intuitively interpretable, as the image is influenced by aberrations of the imaging lenses in the microscope. The largest contributions for uncorrected instruments typically come from defocus and astigmatism. The latter can be estimated from the so-called Thon ring pattern appearing in the Fourier transform modulus of an image of a thin amorphous film.\n\nThe contrast of a HRTEM image arises from the interference in the image plane of the electron wave with itself. Due to our inability to record the phase of an electron wave, only the amplitude in the image plane is recorded. However, a large part of the structure information of the sample is contained in the phase of the electron wave. In order to detect it, the aberrations of the microscope (like defocus) have to be tuned in a way that converts the phase of the wave at the specimen exit plane into amplitudes in the image plane.\n\nThe interaction of the electron wave with the crystallographic structure of the sample is complex, but a qualitative idea of the interaction can readily be obtained. Each imaging electron interacts independently with the sample. Above the sample, the wave of an electron can be approximated as a plane wave incident on the sample surface. As it penetrates the sample, it is attracted by the positive atomic potentials of the atom cores, and channels along the atom columns of the crystallographic lattice (s-state model). At the same time, the interaction between the electron wave in different atom columns leads to Bragg diffraction. The exact description of dynamical scattering of electrons in a sample not satisfying the weak phase object approximation (WPOA), which is almost all real samples, still remains the holy grail of electron microscopy. However, the physics of electron scattering and electron microscope image formation are sufficiently well known to allow accurate simulation of electron microscope images.\n\nAs a result of the interaction with a crystalline sample, the electron exit wave right below the sample \"φ(x,u)\" as a function of the spatial coordinate x is a superposition of a plane wave and a multitude of diffracted beams with different in plane spatial frequencies u (spatial frequencies correspond to scattering angles, or distances of rays from the optical axis in a diffraction plane). The phase change \"φ(x,u)\" relative to the incident wave peaks at the location of the atom columns. The exit wave now passes through the imaging system of the microscope where it undergoes further phase change and interferes as the image wave in the imaging plane (mostly a digital pixel detector like a CCD camera). It is important to realize, that the recorded image is NOT a direct representation of the samples crystallographic structure. For instance, high intensity might or might not indicate the presence of an atom column in that precise location (see simulation). The relationship between the exit wave and the image wave is a highly nonlinear one and is a function of the aberrations of the microscope. It is described by the \"contrast transfer function\".\n\nThe phase contrast transfer function (CTF) is a function of limiting apertures and aberrations in the imaging lenses of a microscope. It describes their effect on the phase of the exit wave \"φ(x,u)\" and propagates it to the image wave. Following \"Williams and Carter\", if we assume the WPOA holds (thin sample) the CTF becomes\n\nwhere \"A(u)\" is the \"aperture function\", \"E(u)\" describes the attenuation of the wave for higher spatial frequency \"u\", also called \"envelope function\". \"χ(u)\" is a function of the aberrations of the electron optical system.\n\nThe last, sinusoidal term of the CTF will determine the sign with which components of frequency u will enter contrast in the final image. If one takes into account only spherical aberration to third order and defocus, χ is rotationally symmetric about the optical axis of the microscope and thus only depends on the modulus \"u\" = |u|, given by\n\nwhere \"C\" is the spherical aberration coefficient, \"λ\" is the electron wavelength, and Δ\"f\" is the defocus. In TEM, defocus can easily be controlled and measured to high precision. Thus one can easily alter the shape of the CTF by defocusing the sample. Contrary to optical applications, defocusing can actually increase the precision and interpretability of the micrographs.\n\nThe \"aperture function\" cuts off beams scattered above a certain critical angle (given by the objective pole piece for ex), thus effectively limiting the attainable resolution. However it is the \"envelope function\" \"E(u)\" which usually dampens the signal of beams scattered at high angles, and imposes a maximum to the transmitted spatial frequency. This maximum determines the highest resolution attainable with a microscope and is known as the information limit. \"E(u)\" can be described as a product of single envelopes:\n\ndue to\n\nSpecimen drift and vibration can be minimized in a stable environment. It is usually the spherical aberration \"C\" that limits spatial coherency and defines \"E(u)\" and the chromatic aberration \"C\", together with current and voltage instabilities that define the temporal coherency in \"E(u)\". These two envelopes determine the information limit by damping the signal transfer in Fourier space with increasing spatial frequency \"u\"\n\nwhere α is the semiangle of the pencil of rays illuminating the sample. Clearly, if the wave aberration ('here represented by \"C\" and Δ\"f\") vanished, this envelope function would be a constant one. In case of an uncorrected TEM with fixed \"C\", the damping due to this envelope function can be minimized by optimizing the defocus at which the image is recorded (Lichte defocus).\n\nThe temporal envelope function can be expressed as\n\nHere, δ is the focal spread with the chromatic aberration \"C\" as the parameter:\n\nThe terms formula_7 and formula_8 represent instabilities in of the total current in the magnetic lenses and the acceleration voltage. formula_9 is the energy spread of electrons emitted by the source.\n\nThe information limit of current state-of-the-art TEMs is well below 1 Å. The TEAM project at Lawrence Berkeley National Laboratory resultet in the first TEM to reach an information limit of <0.5 Å in 2009 by the use of a highly stable mechanical and electrical environment, an ultra-bright, monochromated electron source and double-hexapole aberration correctors.\n\nChoosing the optimum defocus is crucial to fully exploit the capabilities of an electron microscope in HRTEM mode. However, there is no simple answer as to which one is the best.\n\nIn Gaussian focus one sets the defocus to zero, the sample is in focus. As a consequence contrast in the image plane gets its image components from the minimal area of the sample, the contrast is \"localized\" (no blurring and information overlap from other parts of the sample). The CTF now becomes a function that oscillates quickly with \"Cu\". What this means is that for certain diffracted beams with a given spatial frequency \"u\" the contribution to contrast in the recorded image will be reversed, thus making interpretation of the image difficult.\n\nIn Scherzer defocus, one aims to counter the term in \"u\" with the parabolic term Δ\"fu\" of \"χ\"(\"u\"). Thus by choosing the right defocus value \"Δf\" one flattens \"χ\"(\"u\") and creates a wide band where low spatial frequencies \"u\" are transferred into image intensity with a similar phase. In 1949, Scherzer found that the optimum defocus depends on microscope properties like the spherical aberration \"C\" and the accelerating voltage (through \"λ\") in the following way:\n\nwhere the factor 1.2 defines the extended Scherzer defocus. For the CM300 at NCEM, \"C\" = 0.6mm and an accelerating voltage of 300keV (\"λ\" = 1.97 pm) (Wavelength calculation) result in \"Δf = -41.25 nm\".\n\nThe point resolution of a microscope is defined as the spatial frequency \"u\" where the CTF crosses the abscissa for the first time. At Scherzer defocus this value is maximized:\n\nwhich corresponds to 6.1 nm on the CM300. Contributions with a spatial frequency higher than the point resolution can be filtered out with an appropriate aperture leading to easily interpretable images at the cost of a lot of information lost.\n\nGabor defocus is used in electron holography where both amplitude and phase of the image wave are recorded. One thus wants to minimize crosstalk between the two. The Gabor defocus can be expressed as a function of the Scherzer defocus as\n\nTo exploit all beams transmitted through the microscope up to the information limit, one relies on a complex method called exit wave reconstruction which consists in mathematically reversing the effect of the CTF to recover the original exit wave \"φ(x,u)\". To maximize the information throughput, Hannes Lichte proposed in 1991 a defocus of a fundamentally different nature than the Scherzer defocus: because the dampening of the envelope function scales with the first derivative of \"χ(u)\", Lichte proposed a focus minimizing the modulus of d\"χ\"(\"u\")/d\"u\"\n\nformula_13\n\nwhere \"u\" is the maximum transmitted spatial frequency. For the CM300 with an information limit of 0.8 Å Lichte defocus lies at −272 nm.\n\nTo calculate back to \"φ(x,u)\" the wave in the image plane is back propagated numerically to the sample. If all properties of the microscope are well known, it is possible to recover the real exit wave with very high accuracy.\n\nFirst however, both phase and amplitude of the electron wave in the image plane must be measured. As our instruments only record amplitudes, an alternative method to recover the phase has to be used. There are two methods in use today:\n\nBoth methods extend the point resolution of the microscope past the information limit, which is the highest possible resolution achievable on a given machine. The ideal defocus value for this type of imaging is known as Lichte defocus and is usually several hundred nanometers negative.\n\n"}
{"id": "344933", "url": "https://en.wikipedia.org/wiki?curid=344933", "title": "Index of optics articles", "text": "Index of optics articles\n\nOptics is the branch of physics which involves the behavior and properties of light, including its interactions with matter and the construction of instruments that use or detect it. Optics usually describes the behavior of visible, ultraviolet, and infrared light. Because light is an electromagnetic wave, other forms of electromagnetic radiation such as X-rays, microwaves, and radio waves exhibit similar properties.\n"}
{"id": "38448389", "url": "https://en.wikipedia.org/wiki?curid=38448389", "title": "Index of physics articles (O)", "text": "Index of physics articles (O)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "51456706", "url": "https://en.wikipedia.org/wiki?curid=51456706", "title": "Isotropic beacon", "text": "Isotropic beacon\n\nAn Isotropic beacon is a hypothetical type of transmission beacon that emits a uniform EM signal in all directions for the purposes of communication with extraterrestrial intelligence.\n\nAn isotropic beacon can be any transmitter that emits a uniform electromagnetic field. However, the term is most commonly used to describe a transmitter used by a civilization to call attention to itself over interstellar distances. Project Cyclops is and was one of the first looks at the theoretical framework of what it would take to create such a device.\n"}
{"id": "545909", "url": "https://en.wikipedia.org/wiki?curid=545909", "title": "Kinesiology", "text": "Kinesiology\n\nKinesiology is the scientific study of human or non-human body movement. Kinesiology addresses physiological, biomechanical, and psychological dynamic principles and mechanisms of movement. Applications of kinesiology to human health (i.e., human kinesiology) include biomechanics and orthopedics; strength and conditioning; sport psychology; methods of rehabilitation, such as physical and occupational therapy; and sport and exercise. Studies of human and animal motion include measures from motion tracking systems, electrophysiology of muscle and brain activity, various methods for monitoring physiological function, and other behavioral and cognitive research techniques.\n\nThe word comes from the Greek \"kínēsis\", \"movement\" (itself from κινεῖν \"kineîn\", \"to move\"), and -λογία \"-logia\", \"study\".\n\nKinesiology is the study of human and nonhuman animal-body movements, performance, and function by applying the sciences of biomechanics, anatomy, physiology, psychology, and neuroscience. Applications of kinesiology in human-health include physical education teacher, rehabilitation, health and safety, health promotion, workplaces, sport and exercise industries. A bachelor's degree in kinesiology can provide strong preparation for graduate study in biomedical research, as well as in professional programs, such as medicine.\n\nWhereas the term \"kinesiologist\" is neither a licensed nor professional designation in the United States nor most countries (with the exception of Canada), individuals with training in this area can teach physical education, provide consulting services, conduct research and develop policies related to rehabilitation, human motor performance, ergonomics, and occupational health and safety. In North America, kinesiologists may study to earn a Bachelor of Science, Master of Science, or Doctorate of Philosophy degree in Kinesiology or a Bachelor of Kinesiology degree, while in Australia or New Zealand, they are often conferred an Applied Science (Human Movement) degree (or higher). Many doctoral level faculty in North American kinesiology programs received their doctoral training in related disciplines, such as neuroscience, mechanical engineering, psychology, and physiology.\n\nThe world's first kinesiology department was launched in 1967 at the University of Waterloo, Canada.\n\nAdaptation through exercise is a key principle of kinesiology that relates to improved fitness in athletes as well as health and wellness in clinical populations. Exercise is a simple and established intervention for many movement disorders and musculoskeletal conditions due to the neuroplasticity of the brain and the adaptability of the musculoskeletal system. Therapeutic exercise has been shown to improve neuromotor control and motor capabilities in both normal and pathological populations.\n\nThere are many different types of exercise interventions that can be applied in kinesiology to athletic, normal, and clinical populations. Aerobic exercise interventions help to improve cardiovascular endurance. Anaerobic strength training programs can increase muscular strength, power, and lean body mass. Decreased risk of falls and increased neuromuscular control can be attributed to balance intervention programs. Flexibility programs can increase functional range of motion and reduce the risk of injury.\n\nAs a whole, exercise programs can reduce symptoms of depression and risk of cardiovascular and metabolic diseases. Additionally, they can help to improve quality of life, sleeping habits, immune system function, and body composition.\n\nThe study of the physiological responses to physical exercise and their therapeutic applications is known as exercise physiology, which is an important area of research within kinesiology.\n\nNeuroplasticity is also a key scientific principle used in kinesiology to describe how movement and changes in the brain are related. The human brain adapts and acquires new motor skills based on this principle, which includes both adaptive and maladaptive brain changes.\n\nAdaptive plasticity\n\nRecent empirical evidence indicates the significant impact of physical activity on brain function; for example, greater amounts of physical activity are associated with enhanced cognitive function in older adults. The effects of physical activity can be distributed throughout the whole brain, such as higher gray matter density and white matter integrity after exercise training, and/or on specific brain areas, such as greater activation in prefrontal cortex and hippocampus. Neuroplasticity is also the underlying mechanism of skill acquisition. For example, after long-term training, pianists showed greater gray matter density in sensorimotor cortex and white matter integrity in the internal capsule compared to non-musicians.\n\nMaladaptive plasticity\n\nMaladaptive plasticity is defined as neuroplasticity with negative effects or detrimental consequences in behavior. Movement abnormalities may occur among individuals with and without brain injuries due to abnormal remodeling in central nervous system. Learned non-use is an example commonly seen among patients with brain damage, such as stroke. Patients with stroke learned to suppress paretic limb movement after unsuccessful experience in paretic hand use; this may cause decreased neuronal activation at adjacent areas of the infarcted motor cortex.\n\nThere are many types of therapies that are designed to overcome maladaptive plasticity in clinic and research, such as constraint-induced movement therapy (CIMT), body weight support treadmill training (BWSTT) and virtual reality therapy. These interventions are shown to enhance motor function in paretic limbs and stimulate cortical reorganization in patients with brain damage.\n\nMotor redundancy is a widely used concept in kinesiology and motor control which states that, for any task the human body can perform, there are effectively an unlimited number of ways the nervous system could achieve that task. This redundancy appears at multiple levels in the chain of motor execution:\n\n\nThe concept of motor redundancy is explored in numerous studies, usually with the goal of describing the relative contribution of a set of motor elements (e.g. muscles) in various human movements, and how these contributions can be predicted from a comprehensive theory. Two distinct (but not incompatible) theories have emerged for how the nervous system coordinates redundant elements: simplification and optimization. In the simplification theory, complex movements and muscle actions are constructed from simpler ones, often known as primitives or synergies, resulting in a simpler system for the brain to control. In the optimization theory, motor actions arise from the minimization of a control parameter, such as the energetic cost of movement or errors in movement performance.\n\nIn Canada, kinesiology is a professional designation as well as an area of study. In the province of Ontario the scope has been officially defined as, \"the assessment of human movement and performance and its rehabilitation and management to maintain, rehabilitate or enhance movement and performance\" \n\nKinesiologists work in a variety of roles as health professionals. They work as rehabilitation providers in hospitals, clinics and private settings working with populations needing care for musculoskeletal, cardiac and neurological conditions. They provide rehabilitation to persons injured at work and in vehicular accidents. Kinesiologists also work as functional assessment specialists, exercise therapists, ergonomists, return to work specialists, case managers and medical legal evaluators. They can be found in hospital, long term care, clinic, work, and community settings. Additionally, kinesiology is applied in areas of health and fitness for all levels of athletes, but more often found with training of elite athletes.\n\nIn Canada, Kinesiology has been designated a regulated health profession in Ontario. Kinesiology was granted the right to regulate in the province of Ontario in the summer of 2007 and similar proposals have been made for other provinces. The College of Kinesiologists of Ontario achieved proclamation on April 1, 2013, at which time the professional title \"Kinesiologist\" became protected by law. In Ontario only members of the college may call themselves a Registered Kinesiologist.\nIndividuals who have earned degrees in kinesiology can work in research, the fitness industry, clinical settings, and in industrial environments. They also work in cardiac rehabilitation, health and safety, hospital and long-term care facilities and community health centers just to name a few.\n\n\nRoyal Central Institute of Gymnastics () G.C.I. was founded 1813 in Stockholm, Sweden by Pehr Henrik Ling. It was the first Physiotherapy School in the world, training hundreds of medical gymnasts who spread the Swedish physical therapy around the entire world. \nIn 1887, Sweden was the first country in the world to give a national state licence to physiotherapists/physical therapists.\n\nThe Swedish medical gymnast and kinesiologist Carl August Georgii (), Professor at the Royal Gymnastic Central Institute GCI in Stockholm, was the one who created and coined the new international word Kinesiology in 1854.<br>\nThe term \"Kinesiology\" is a literal translation to Greek+English from the original Swedish word \"Rörelselära\", meaning \"Movement Science\". It was the foundation of the Medical Gymnastics, the original Physiotherapy and Physical Therapy, developed during 100 years in Sweden (starting 1813).\nThe new medical therapy created in Sweden was originally called Rörelselära (), and later in 1854 translated to the new and invented international word \"Kinesiology\". \nThe Kinesiology consisted of nearly 2,000 physical movements and 50 different types of massage therapy techniques. \nThey were all used to affect various dysfunctions and even illnesses, not only in the movement apparatus, but also into the internal physiology of man. \nThus, the original classical and Traditional Kinesiology was not only a system of rehabilitation for the body, or biomechanics like in modern Academic Kinesiology, but also a new therapy for relieving and curing diseases, by affecting the autonomic nervous system, organs and glands in the body.,\n\nIn 1886, the Swedish Medical Gymnast Nils Posse (1862-1895) introduced the term kinesiology in the U.S. Nils Posse was a graduate of the Royal Gymnastic Central Institute in Stockholm, Sweden and founder of the Posse Gymnasium in Boston, MA. He was teaching at Boston Normal School of Gymnastics BNSG.\n\"The Special Kinesiology Of Educational Gymnastics\" was the first book ever written in the world with the word \"Kinesiology\" in the title of the book. It was written by Nils Posse and published in Boston, 1894-1895.\n\nMotion capture technology has application in measuring human movement, and thus kinesiology. Historically, motion capture labs have recorded high fidelity data. While accurate and credible, these systems can come at high capital and operational costs. Modern-day systems have increased accessibility to mocap technology.\n"}
{"id": "18006982", "url": "https://en.wikipedia.org/wiki?curid=18006982", "title": "Knowledge spillover", "text": "Knowledge spillover\n\nKnowledge spillover is an exchange of ideas among individuals. In knowledge management economics, knowledge spillovers are non-rival knowledge market costs incurred by a party not agreeing to assume the costs that has a spillover effect of stimulating technological improvements in a neighbor through one's own innovation. Such innovations often come from specialization within an industry.\n\nA recent, general example of a knowledge spillover could be the collective growth associated with the research and development of online social networking tools like Facebook, YouTube, and Twitter. Such tools have not only created a positive feedback loop, and a host of originally unintended benefits for their users, but have also created an explosion of new software, programming platforms, and conceptual breakthroughs that have perpetuated the development of the industry as a whole. The advent of online marketplaces, the utilization of user profiles, the widespread democratization of information, and the interconnectivity between tools within the industry have all been products of each tool’s individual developments. These developments have since spread outside the industry into the mainstream media as news and entertainment firms have developed their own market feedback applications within the tools themselves, and their own versions of online networking tools (e.g. CNN’s iReport).\n\nThere are two kinds of knowledge spillovers: internal and external. Internal knowledge spillover occurs if there is a positive impact of knowledge between individuals within an organization that produces goods and/or services. An external knowledge spillover occurs when the positive impact of knowledge is between individuals without or outside of a production organization. Marshall-Arrow-Romer (MAR) spillovers, Porter spillovers and Jacobs spillovers are three types of spillovers.\n\nMAR spillover has its origins in 1890, where the English economist Alfred Marshall developed a theory of knowledge spillovers. Knowledge spillovers later were extended by economists Kenneth Arrow (1962) and Paul Romer (1986). In 1992, Edward Glaeser, Hedi Kallal, José Scheinkman, and Andrei Shleifer pulled together the Marshall-Arrow-Romer views on knowledge spillovers and accordingly named the view MAR spillover in 1992.\n\nUnder the Marshall-Arrow-Romer (MAR) spillover view, the proximity of firms within a common industry often affects how well knowledge travels among firms to facilitate innovation and growth. The closer the firms are to one another, the greater the MAR spillover. The exchange of ideas is largely from employee to employee, in that employees from different firms in an industry exchange ideas about new products and new ways to produce goods. The opportunity to exchange ideas that lead to innovations key to new products and improved production methods.\n\nBusiness parks are a good example of concentrated businesses that may benefit from MAR spillover. Many semiconductor firms intentionally located their research and development facilities in Silicon Valley to take advantage of MAR spillover. In addition, the film industry in Los Angeles, California and elsewhere relies on a geographic concentration of specialists (directors, producers, scriptwriters, and set designers) to bring together narrow aspects of movie-making into a final product.\n\nHowever, research on the Cambridge IT Cluster (UK) suggests that technological knowledge spillovers might only happen rarely and are less important than other cluster benefits such as labour market pooling.\n\nPorter (1990), like MAR, argues that knowledge spillovers in specialized, geographically concentrated industries stimulate growth. He insists, however, that local competition, as opposed to local monopoly, fosters the pursuit and rapid adoption of innovation. He gives examples of Italian ceramics and gold jewellery industries, in which hundreds of firms are located together and fiercely compete to innovate since the alternative to innovation is demise. Porter's externalities are maximized in cities with geographically specialized, competitive industries.\n\nUnder the Jacobs spillover view, the proximity of firms from different industries affect how well knowledge travels among firms to facilitate innovation and growth. This is in contrast to MAR spillovers, which focus on firms in a common industry. The diverse proximity of a Jacobs spillover brings together ideas among individuals with different perspectives to encourage an exchange of ideas and foster innovation in an industrially diverse environment.\n\nDeveloped in 1969 by urbanist Jane Jacobs and John Jackson the concept that Detroit’s shipbuilding industry from the 1830s was the critical antecedent leading to the 1890s development of the auto industry in Detroit since the gasoline engine firms easily transitioned from building gasoline engines for ships to building them for automobiles.\n\nAs information is largely non-rival in nature, certain measures must be taken to ensure that, for the originator, the information remains a private asset. As the market cannot do this efficiently, public regulations have been implemented to facilitate a more appropriate equilibrium.\n\nAs a result, the concept of intellectual property rights have developed and ensure the ability of entrepreneurs to temporarily hold on to the profitability of their ideas through patents, copyrights, and other governmental safeguards. Conversely, such barriers to entry prevent the exploitation of informational developments by rival firms within an industry.\n\nOn the other hand, when the research and development of a private firm results in a social benefit, unaccounted for within the market price, often greater than the private return of the firm’s research, then a subsidy to offset the underproduction of that benefit might be offered to the firm in return for its continued output of that benefit. Government subsidies are often controversial, and while they might often result in a more appropriate social equilibrium, they could also lead to undesirable political repercussions as such a subsidy must come from taxpayers, some of whom may not directly benefit from the researching firm’s subsidized knowledge spillover. The concept of knowledge spillover is also used to justify subsidies to foreign direct investment, as foreign investors help diffuse technology among local firms.\n"}
{"id": "42421874", "url": "https://en.wikipedia.org/wiki?curid=42421874", "title": "Laser ignition", "text": "Laser ignition\n\nLaser ignition is an alternative method for igniting mixtures of fuel and oxidiser. The phase of the mixture can be gaseous or liquid. The method is based on laser ignition devices that produce short but powerful flashes regardless of the pressure in the combustion chamber. Usually, high voltage spark plugs are good enough for automotive use, as the typical compression ratio of an otto cycle internal combustion engine is around 10:1 and in some rare cases reach 14:1. However, fuels such as natural gas or methanol can withstand high compression without self ignition. This allows higher compression ratios, because it is economically reasonable, as the fuel efficiency of such engines is high. Using high compression ratio and high pressure requires special spark plugs that are expensive and their electrodes still wear out. Thus, even expensive laser ignition systems could be economical, because they would last longer.\n\nLaser ignition is considered as a potential ignition system for non-hypergolic liquid rocket engines and reaction control systems which need an ignition system. Conventional ignition technologies like torch igniters are more complex in sequencing and need additional components like propellant feed lines and valves. Therefore, they are heavy compared to a laser ignition system. Pyrotechnical devices allow only one ignition per unit and imply increased launch pad precautions as they are made of explosives.\n"}
{"id": "53843927", "url": "https://en.wikipedia.org/wiki?curid=53843927", "title": "Lester Fuess Eastman", "text": "Lester Fuess Eastman\n\nLester Fuess Eastman (May 21, 1928 – August 9, 2013) was a physicist, engineer and educator\n\nEastman worked primarily with the development of high frequency semiconductor device engineering and science technologies from the early 1960s through to his retirement. While at Cornell University, he was awarded the status of Fellow in the American Physical Society, after he was nominated by their Forum on Industrial and Applied Physics in 2001, for \"pioneering contributions to the concepts of ballistic transport and piezoelectric doping in ultra-small III-V heterojunction transistors for applications in high-speed and microwave power devices and circuits and for leadership in transitioning electric.\"\n"}
{"id": "50325121", "url": "https://en.wikipedia.org/wiki?curid=50325121", "title": "List of foreign satellites launched by India", "text": "List of foreign satellites launched by India\n\nIndia has launched 239 satellites for 28 different countries as of October, 2018. Commercial launches for foreign nations are negotiated through Antrix, the commercial arm of the Indian Space Research Organization (ISRO). All satellites were launched using the ISRO's Polar Satellite Launch Vehicle (PSLV) expendable launch system. Between 2013 and 2015, India launched 28 foreign satellites for 13 different countries earning a total revenue of US$101 million.\n\nISRO successfully launched 104 satellites on 15 February 2017, of which 3 satellites are Indian satellites while the remaining are foreign commercial satellites. Ninety-six satellites are from the United States, while the others come from Israel, the UAE, Kazakhstan, the Netherlands, Belgium and Germany. It is the largest number of satellites launched on a single flight by any space agency. The previous record was held by Russia's Dnepr launcher which launched 37 in June 2014.\n\nTotal Foreign Satellites launched by ISRO : 269 (as of 29/11/2018)\n----<div style=\"float: left; padding-left: 7px;\">\n\n\n"}
{"id": "5971822", "url": "https://en.wikipedia.org/wiki?curid=5971822", "title": "List of mathematicians (P)", "text": "List of mathematicians (P)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "14485655", "url": "https://en.wikipedia.org/wiki?curid=14485655", "title": "List of members of the National Academy of Sciences (computer and information sciences)", "text": "List of members of the National Academy of Sciences (computer and information sciences)\n"}
{"id": "8673754", "url": "https://en.wikipedia.org/wiki?curid=8673754", "title": "List of second moments of area", "text": "List of second moments of area\n\nThe following is a list of second moments of area of some shapes. The second moment of area, also known as area moment of inertia, is a geometrical property of an area which reflects how its points are distributed with regard to an arbitrary axis. The unit of dimension of the second moment of area is length to fourth power, L, and should not be confused with the mass moment of inertia. If the piece is thin, however, the mass moment of inertia equals the area density times the area moment of inertia.\n\nThe parallel axis theorem can be used to determine the second moment of area of a rigid body about any axis, given the body's moment of inertia about a parallel axis through the object's center of mass and the perpendicular distance (d) between the axes.\n\nformula_1\n\n"}
{"id": "12067755", "url": "https://en.wikipedia.org/wiki?curid=12067755", "title": "List of the brightest Kuiper belt objects", "text": "List of the brightest Kuiper belt objects\n\nSince the year 2000, a number of Kuiper belt objects (KBOs) with diameters of between 500 and 1500 km (more than half that of Pluto) have been discovered. 50000 Quaoar, a classical KBO discovered in 2002, is over 1200 km across. and , both announced on 29 July 2005, are larger still. Other objects, such as 28978 Ixion (discovered in 2001) and 20000 Varuna (discovered in 2000) measure roughly 500 km across. This has led gradually to the acceptance of Pluto as the largest member of the Kuiper belt.\n\nThe brightest known dwarf planets and other KBOs (with absolute magnitudes < 4.0), are:\n\n"}
{"id": "1384073", "url": "https://en.wikipedia.org/wiki?curid=1384073", "title": "Martin-de-Viviès", "text": "Martin-de-Viviès\n\nMartin-de-Viviès, or La Roche Godon, formerly Camp Heurtin, is a research station and the only settlement on the Île Amsterdam and Île Saint-Paul islands of the French Southern and Antarctic Lands in the southern Indian Ocean. It lies on the north coast of Amsterdam Island and houses about thirty people.\n\nIt was named after Paul de Martin de Viviès who, with ten others, spent the winter of 1949 on the island.\n\nThe station was originally named Camp Heurtin and has been in operation since 1 January 1981, superseding the first station, La Roche Godon.\n\nThe Global Atmosphere Watch is one of the programs that the station participates in.\n\n"}
{"id": "6122598", "url": "https://en.wikipedia.org/wiki?curid=6122598", "title": "Matt Lebofsky", "text": "Matt Lebofsky\n\nMatt Lebofsky is an Oakland, California-based multi-instrumentalist and composer. Growing up in New York he studied piano/composition with Arthur Cunningham from 1978-1988. As a performer/composer he is currently active in several bands such as miRthkon, MoeTar, Secret Chiefs 3, Bodies Floating Ashore, The Fuxedos, Three Piece Combo, Research & Development, Midline Errors, Fuzzy Cousins and JOB. He is also a long-time prolific member of the Immersion Composition Society Origin Lodge. He toured nationally in 2006 as a member of Faun Fables, and throughout 2000-2001 as a member of Species Being, and released three albums and toured internationally with Mumble & Peg from 1995-2002.\n\nMatt is also a computer programmer, webmaster, and database/systems administrator at the Berkeley SETI Research Center, working with Breakthrough Listen since 2015, and as a core member of the small staff developing/maintaining the world's largest distributed computing project SETI@home (since its inception at the University of California at Berkeley's Space Sciences Laboratory in 1997). He also works on the open-source general distributed computing engine BOINC, and designed levels for the iPhone video game Tap Tap Revenge.\n"}
{"id": "17836562", "url": "https://en.wikipedia.org/wiki?curid=17836562", "title": "Michał Horodecki", "text": "Michał Horodecki\n\nMichał Horodecki is a physicist at the University of Gdańsk working in the field of quantum information theory, notable for his work on entanglement theory. \n\nTogether with Jonathan Oppenheim and Andreas Winter, he discovered quantum state-merging and used this primitive to show that quantum information could be negative. He also co-discovered the Peres-Horodecki criterion for testing whether a state is entangled, and used it to find bound entanglement together with his brother Paweł Horodecki and father Ryszard Horodecki. He co-discovered with Jonathan Oppenheim, Paweł Horodecki and Karol Horodecki that secret key can be drawn from some bound entangled states. Together with Fernando Brandao he proved that every one-dimensional quantum state with a finite correlation length obeys an area law for entanglement entropy. \n\n"}
{"id": "2769113", "url": "https://en.wikipedia.org/wiki?curid=2769113", "title": "Mycoremediation", "text": "Mycoremediation\n\nMycoremediation (from ancient Greek \"μύκης (mukēs)\", meaning \"fungus\" and the suffix \"-remedium\", in Latin meaning 'restoring balance') is a form of bioremediation in which fungi-based technology is used to decontaminate the environment. Fungi have been proven to be a very cheap, effective and environmentally sound way for helping to remove a wide array of toxins from damaged environments or wastewater. The toxins include heavy metals, persistent organic pollutants, textile dyes, leather tanning industry chemicals and wastewater, petroleum fuels, polycyclic aromatic hydrocarbon, pharmaceuticals and personal care products, pesticides and herbicide, in land, fresh water and marine environments. \nThe byproducts of the remediation can be valuable materials themselves, such as enzymes (like laccase), edible or medicinal mushrooms, making the remediation process even profitable.\n\nFungi, thanks to their non-specific enzymes, are able to break down many kinds of substances. They are used for pharmaceuticals and fragrances that normally are recalcitant to bacteria degradation, such as paracetamol, the breakdown products of which are toxic in traditional water treatment, using \"Mucor hiemalis\", but also the phenols and pigments of wine distillery wastewater, X-ray contrast agents and ingredients of personal care products.\n\nMycoremediation is one of the cheaper solutions to remediation, and it doesn't usually require expensive equipment. For this reasons it is often used also in small scale applications, such as mycofiltration of domestic wastewater, and to help with the decomposition process of a compost toilet.\n\nPollution from metals is very common, as they are used in many industrial processes such as electroplating, paint and leather. The wastewater from these industries is often used for agricultural purposes, so besides the immediate damage to the ecosystem it is spilled into, the metals can enter far away creatures and humans through the food chain. Mycoremediation is one of the cheapest, most effective and environmental-friendly solutions to this problem.\nMany fungi are hyperaccumulators, that means they are able to concentrate toxins in their fruiting bodies for later removal. This is usually true for populations that have been exposed to contaminants for long time, and have developed a high tolerance, and happens via biosorption on the cellular surface, which means that the metals enter the mycelium in a passive way with very little intracellular uptake.\nA variety of fungi, such as \"Pleurotus\", \"Aspergillus\", \"Trichoderma\" has proven to be effective in the removal of lead, cadmium, nickel, chromium, mercury, arsenic, copper, boron, iron and zinc in marine environment, wastewater and on land.\n\nNot all the individuals of a species are effective in the same way in the accumulation of toxins. The single individuals are usually selected from an old-time polluted environment, such as sludge or wastewater, where they had time to adapt to the circumstances, and the selection is carried on in the laboratory. A dilution of the water can drastically improve the ability of biosorption of the fungi.\n\nThe capacity of certain fungi to extract metals from the ground also can be useful for bioindicator purposes, and can be a problem when the mushroom is an edible one. For example, the shaggy ink cap (\"Coprinus comatus\"), a common edible north-hemisphere mushroom, can be a very good bioindicator of mercury, and accumulate it in its body, which can also be toxic to the consumer.\n\nThe capacity of metals uptake of mushroom has also been used to recover precious metals from medium. VTT Technical Research Centre of Finland reported an 80% recovery of gold from electronic waste using mycofiltration techniques.\n\nFungi are amongst the primary saprotrophic organisms in an ecosystem, as they are efficient in the decomposition of matter.\nWood-decay fungi, especially white rot, secretes extracellular enzymes and acids that break down lignin and cellulose, the two main building blocks of plant fiber. These are long-chain organic (carbon-based) compounds, structurally similar to many organic pollutants. \nThey do so using a wide array of enzymes. In the case of polycyclic aromatic hydrocarbons (PAHs), complex organic compounds with fused, highly stable, polycyclic aromatic rings, fungi are very effective also in marine environments. The enzymes involved in this degradation are ligninolytic and include lignin peroxidase, versatile peroxidase, manganese peroxidase, general lipase, laccase and sometimes intracellular enzymes, especially the cytochrome P450.\n\nOther toxins fungi are able to degrade into harmless compounds include petroleum fuels, phenols in wastewater, polychlorinated biphenyl (PCB) in contaminated soils using \"Pleurotus ostreatus\", polyurethane in aerobic and anaerobic conditions such as found at the bottom of landfills using two species of the Ecuadorian fungus \"Pestalotiopsis\", and more.\n\nThe mechanisms of degradation are not always clear, as the mushroom may be a precursor to subsequent microbial activity rather than individually effective in the removal of pollutants.\n\nPesticide contamination can be long-term and have a significant impact on decomposition processes and thus nutrient cycling and their degradation can be expensive and difficult.\nThe most used fungi for helping in the degradation of such substances are white rot ones which, thanks to their extracellular ligninolytic enzymes like laccase and manganese peroxidase, are able to degrade high quantity of such components. Examples includes the insecticide endosulfan, imazalil, thiophanate methyl, ortho-phenylphenol, diphenylamine, chlorpyrifos in wastewater, and atrazine in clay-loamy soils.\n\nDyes are used in many industries, like paper printing or textile. They are often recalcitant to degradation and in some cases, like some azo dyes, cancerogenic or otherwise toxic.\n\nThe mechanism the fungi degrade this dyes is their lignolytic enzymes, especially laccase, so white rot mushrooms are the most commonly used.\n\nMycoremediation has proven to be a cheap and effective remediation technology for dyes such as malachite green, nigrosin and basic fuchsin with \"Aspergillus niger\" and \"Phanerochaete chrysosporium\" and Congo red, a carcinogenic dye recalcitrant to biodegradative processes, direct blue 14 (using \"Pleurotus\").\n\nPhytoremediation is the use of plant-based technologies to decontamination an area. Most of the plants can form a symbiosis with fungi, from which both the organisms get an advantage. This relationship is called mycorrhiza.\n\nMycorrhizal fungi, especially arbuscular mycorrhizal fungi (AMF), can greatly improve the phytoremediation capacity of some plants. This is mostly because the stress the plants suffer because of the pollutants is greatly reduced in presence of AMF, so they can grow more and produce more biomass. The fungi also provide more nutrition, especially phosphorus, and promotes the overall health of the plant. The mycelium quick expansion also can greatly extend the rhizosphere influenze zone (hyphosphere), providing the plant with access to more nutrients and contaminants. Increasing the rhizosphere overall health also means a rise in the bacteria population, which can also contribute to the bioremediation process.\n\nThis relationship has been proven useful with many pollutants, such as \"Rhizophagus intraradices\" and \"Robinia pseudoacacia\" in lead contaminated soil, \"Rhizophagus intraradices \"with \"Glomus versiforme \"incoulated into vetiver grass for lead removal, AMF and \"Calendula officinalis\" in cadmium and lead contaminated soil, and in general was effective in increasing the plant bioremediation capacity for metals, petroleum fuels, and PAHs. In wetlands AMF greatly promote the biodegradation of organic pollutants like benzene-, methyl tert-butyl ether- and ammonia from groundwater when inoculated into \"Phragmites australis\".\n\n"}
{"id": "4648672", "url": "https://en.wikipedia.org/wiki?curid=4648672", "title": "Robert Blinc", "text": "Robert Blinc\n\nRobert Blinc (October 31, 1933 – September 26, 2011) was a prominent Slovene physicist.\n\nHe completed his undergraduate studies in 1958 at the Faculty of Natural Sciences in Ljubljana and received a PhD a year later. He then started post-doc study at the Massachusetts Institute of Technology. When he returned to Slovenia, he continued his work at the Jožef Stefan Institute. He became a professor at the University of Ljubljana in 1970. He was the first Dean of Jožef Stefan International Postgraduate School in Ljubljana since 2004.\n\nProfessor Blinc was one of the founders of uses of nuclear magnetic resonance for investigations of phase transitions and liquid crystals. He established NMR/EPR spectroscopy in Slovenia (his first paper on the subject was published in early 1958) and was the head of the Condensed Matter Physics Department at the Jožef Stefan Institute. He was the president of AMPERE Groupement for two mandates during 1990–1996. He was a member of the Slovenian Academy of Sciences and Arts and served as its vice president from October 2, 1980 to May 6, 1999. He was also a member of the European Academy of Sciences and Arts and an honorary member of the Society of Mathematicians, Physicists and Astronomers of Slovenia. He died in Ljubljana, Slovenia.\n\n"}
{"id": "21578153", "url": "https://en.wikipedia.org/wiki?curid=21578153", "title": "Social sustainability", "text": "Social sustainability\n\nSocial life is the least defined and least understood of the different ways of approaching sustainability and sustainable development. Social sustainability has had considerably less attention in public dialogue than economic and environmental sustainability.\n\nThere are several approaches to sustainability. The first, which posits a triad of environmental sustainability, economic sustainability, and social sustainability, is the most widely accepted as a model for addressing sustainability. The concept of \"social sustainability\" in this approach encompasses such topics as: social equity, livability, health equity, community development, social capital, social support, human rights, labour rights, placemaking, social responsibility, social justice, cultural competence, community resilience, and human adaptation.\n\nA second, more recent, approach suggests that all of the domains of sustainability are social: including ecological, economic, political and cultural sustainability. These domains of social sustainability are all dependent upon the relationship between the social and the natural, with the \"ecological domain\" defined as human embeddedness in the environment. In these terms, social sustainability encompasses all human activities. It is not just relevant to the focussed intersection of economics, the environment and the social. (See the Venn diagram and the Circles of Sustainability diagram).\n\nAccording to the Western Australia Council of Social Services (WACOSS): \"Social sustainability occurs when the formal and informal processes; systems; structures; and relationships actively support the capacity of current and future generations to create healthy and liveable communities. Socially sustainable communities are equitable, diverse, connected and democratic and provide a good quality of life.\"\n\nAnother definition has been developed by Social Life, a UK-based social enterprise specialising in place-based innovation. They define social sustainability as \"\"a process for creating sustainable, successful places that promote wellbeing, by understanding what people need from the places they live and work. Social sustainability combines design of the physical realm with design of the social world – infrastructure to support social and cultural life, social amenities, systems for citizen engagement and space for people and places to evolve\".\"\n\nSocial Life have developed a framework for social sustainability which has four dimensions: amenities and infrastructure, social and cultural life, voice and influence, and space to grow.\n\nNobel Laureate Amartya Sen gives the following dimensions for social sustainability:\n\nAlso we can speak of Sustainable Human Development that can be seen as development that promotes the capabilities of present people without compromising capabilities of future generations. In the human development paradigm, environment and natural resources should constitute a means of achieving better standards of living just as income represents a means of increasing social expenditure and, in the end, well-being.\n\nThe different aspects of social sustainability are often considered in socially responsible investing (SRI). Social sustainability criteria that are commonly used by SRI funds and indexes to rate publicly traded companies include: community, diversity, employee relations, human rights, product safety, reporting, and governance structure.\n\nThe UN Guiding Principles on Business and Human Rights state that countries have the obligation to “respect, protect, and fulfill human rights and fundamental freedoms” and that business enterprises are required to comply with all applicable laws and respect human rights. Both production and procurement of goods and services should be documented to verify satisfaction of these international principles and laws.\n\nThe UN Guiding Principles also include a reporting framework, which teaches companies how to report their interaction with human rights issues. In addition resources like Free2Work, the Global Reporting Initiative, and Business and Human Rights Resource Centre all provide information on organizational disclosures and performance in social sustainability. Certifications from internationally recognized and accredited organizations are available to aid in verifying the social sustainability of products and services. The Forest Stewardship Council (paper and forest products), and Kimberly Process (diamonds) are examples of such organizations and initiatives.\n\n\n"}
{"id": "45272929", "url": "https://en.wikipedia.org/wiki?curid=45272929", "title": "Tech Gate Vienna", "text": "Tech Gate Vienna\n\nTech Gate Vienna is a science and technology park in the City of Vienna, Austria, situated in the 22nd district Donaustadt. It was built between 1999 and 2005 following the plans from architects Wilhelm Holzbauer and Sepp Frank in an area called Donau City.\n\nTech Gate Vienna consists of two buildings. The first, 26 meter high building was completed in 2001 and has 7 floors, with a total of 36,000 square meters of room. The second building was built from 2004 to 2005 and is 75 meters high with 18,000 square meters of room on 19 floors.\n\nAside many companies and start-ups, several technology labs are situated in Tech Gate Vienna buildings, such as the Austrian Institute of Technology, the Telecommunications Research Center Vienna (FTW), and the VRVis Research Center. Four attractive stages are utilized for a variety of technology affine events.\n\n"}
{"id": "47590706", "url": "https://en.wikipedia.org/wiki?curid=47590706", "title": "The Dorito Effect", "text": "The Dorito Effect\n\nThe Dorito Effect: The Surprising New Truth about Food and Flavor, by Mark Schatzker, is a book about two detrimental trends in modern food culture. The pursuit of maximum monetary gain by means of increased yield and pest resistance has involved the neglect of flavor in the breeding process. On the other hand, to provide the flavor missing from industrialized food, food processors add flavors which fool the body into believing that it is receiving the necessary nutrients. The book was published in 2015 by Simon & Schuster.\n\n"}
{"id": "2243574", "url": "https://en.wikipedia.org/wiki?curid=2243574", "title": "Thermal Hall effect", "text": "Thermal Hall effect\n\nThe thermal Hall effect is the thermal analog of the Hall effect. Here, a thermal gradient is produced across a solid instead of an electric field. When a magnetic field is applied, an orthogonal temperature gradient develops.\n\nFor conductors, a significant portion of the thermal current is carried by the electrons. In particular, the Righi–Leduc effect describes the heat flow resulting from a perpendicular temperature gradient and vice versa, and the Maggi–Righi–Leduc effect describes changes in thermal conductivity when placing a conductor in a magnetic field.\n\nA thermal Hall effect has also been measured in a paramagnetic \"insulator\" and called the \"phonon Hall effect\". In this case, there are no charged currents in the solid, so the magnetic field cannot exert a Lorentz force. An analogous thermal Hall effect for neutral particles exists in polyatomic gases (known as the Senftleben–Beenakker effect).\n\nMeasurements of the thermal Hall conductivity are used to distinguish between the electronic and lattice contributions to thermal conductivity. These measurements are especially useful when studying superconductors.\n\n"}
{"id": "914025", "url": "https://en.wikipedia.org/wiki?curid=914025", "title": "Triune brain", "text": "Triune brain\n\nThe triune brain is a model of the evolution of the vertebrate forebrain and behavior, proposed by the American physician and neuroscientist Paul D. MacLean. MacLean originally formulated his model in the 1960s and propounded it at length in his 1990 book \"The Triune Brain in Evolution\". The triune brain consists of the reptilian complex, the paleomammalian complex (limbic system), and the neomammalian complex (neocortex), viewed as structures sequentially added to the forebrain in the course of evolution. However, this hypothesis is no longer espoused by the majority of comparative neuroscientists in the post-2000 era. \nThe triune brain hypothesis became familiar to a broad popular audience through Carl Sagan's Pulitzer prize winning 1977 book \"The Dragons of Eden\". The theory has been embraced by some psychiatrists and at least one leading affective neuroscience researcher.\n\nThe reptilian complex, also known as the R-complex or \"reptilian brain\" was the name MacLean gave to the basal ganglia, structures derived from the floor of the forebrain during development. The term derives from the idea that comparative neuroanatomists once believed that the forebrains of reptiles and birds were dominated by these structures. MacLean proposed that the reptilian complex was responsible for species-typical instinctual behaviours involved in aggression, dominance, territoriality, and ritual displays. \n\nThe paleomammalian brain consists of the septum, amygdalae, hypothalamus, hippocampal complex, and cingulate cortex. MacLean first introduced the term \"limbic system\" to refer to this set of interconnected brain structures in a paper in 1952. MacLean's recognition of the limbic system as a major functional system in the brain was widely accepted among neuroscientists, and is generally regarded as his most important contribution to the field. MacLean maintained that the structures of the limbic system arose early in mammalian evolution (hence \"paleomammalian\") and were responsible for the motivation and emotion involved in feeding, reproductive behaviour, and parental behaviour. \n\nThe neomammalian complex consists of the cerebral neocortex, a structure found uniquely in higher mammals, and especially humans. MacLean regarded its addition as the most recent step in the evolution of the mammalian brain, conferring the ability for language, abstraction, planning, and perception. \n\nMacLean originally formulated the triune brain hypothesis in the 1960s, drawing on comparative neuroanatomical work done by Ludwig Edinger, Elizabeth C. Crosby and Charles Judson Herrick early in the twentieth century. The 1980s saw a rebirth of interest in comparative neuroanatomy, motivated in part by the availability of a variety of new neuroanatomical techniques for charting the circuitry of animal brains. Subsequent findings have refined the traditional neuroanatomical ideas upon which MacLean based his hypothesis.\n\nFor example, the basal ganglia (structures derived from the floor of the forebrain and making up MacLean's reptilian complex) were shown to take up a much smaller portion of the forebrains of reptiles and birds (together called sauropsids) than previously supposed, and to exist in amphibians and fish as well as mammals and sauropsids. Because the basal ganglia are found in the forebrains of all modern vertebrates, they most likely date to the common evolutionary ancestor of the vertebrates, more than 500 million years ago, rather than to the origin of reptiles.\n\nSome recent behavioral studies do not support the traditional view of sauropsid behavior as stereotyped and ritualistic (as in MacLean's reptilian complex). Birds have been shown to possess highly sophisticated cognitive abilities, such as the toolmaking of the New Caledonian crow and the language-like categorization abilities of the grey parrot. Structures of the limbic system, which MacLean proposed arose in early mammals, have now been shown to exist across a range of modern vertebrates. The \"paleomammalian\" trait of parental care of offspring is widespread in birds and occurs in some fishes as well. Thus, like the basal ganglia, the evolution of these systems presumably dates to a common vertebrate ancestor.\n\nFinally, recent studies based on paleontological data or comparative anatomical evidence strongly suggest that the neocortex was already present in the earliest emerging mammals. In addition, although non-mammals do not have a neocortex in the true sense (that is, a structure comprising part of the forebrain roof, or pallium, consisting of six characteristic layers of neurons), they possess pallial regions, and some parts of the pallium are considered homologous to the mammalian neocortex. While these areas lack the characteristic six neocortical layers, birds and reptiles generally possess three layers in the dorsal pallium (the homolog of the mammalian neocortex). The telencephalon of birds and mammals makes neuroanatomical connections with other telecencephalic structures like those made by neocortex. It mediates similar functions such as perception, learning and memory, decision making, motor control, conceptual thinking.\n\nThe triune model of the mammalian brain is seen as an oversimplified organizing theme by some in the field of comparative neuroscience. It continues to hold public interest because of its simplicity. While technically inaccurate in many respects as an explanation for brain activity, it remains one of very few approximations of the truth we have to work with: the \"neocortex\" represents that cluster of brain structures involved in advanced cognition, including planning, modeling and simulation; the \"limbic brain\" refers to those brain structures, wherever located, associated with social and nurturing behaviors, mutual reciprocity, and other behaviors and affects that arose during the age of the mammals; and the \"reptilian brain\" refers to those brain structures related to territoriality, ritual behavior and other \"reptile\" behaviors. The broad explanatory value makes this approximation very engaging and is a useful level of complexity for high school students to begin engaging with brain research.\nHoward Bloom, in his book \"The Lucifer Principle\", references the concept of the triune brain in his explanations of certain aspects of human behavior. Arthur Koestler made MacLean's concept of the triune brain the centerpiece of much of his later work, notably \"The Ghost in the Machine\". English novelist Julian Barnes quotes MacLean on the triune brain in the foreword to his 1982 novel \"Before She Met Me\". Peter A. Levine uses the triune brain concept in his book \"Waking the Tiger\" to explain his somatic experiencing approach to healing trauma.\n\nGlynda-Lee Hoffmann, in her book, \"The Secret Dowry of Eve, Women's Role in the Development of Consciousness,\" references the triune theory explored by MacLean, and she goes one step further. Her theory about human behavior and the problems we create with that behavior, distinguishes the prefrontal cortex as uniquely different from the rest of the neocortex. The prefrontal cortex, with its agenda of integration, is the part of the brain that can get the other parts to work together for the good of the individual. In many humans the reptilian cortex (agenda: territory and reproduction [in humans that translates to power and sex] is out of control and the amygdala stokes the fear that leads to more bad behavior. The prefrontal cortex is the key to our future if we can harness its power.\n\n"}
{"id": "40845893", "url": "https://en.wikipedia.org/wiki?curid=40845893", "title": "Who's Afraid of Peer Review?", "text": "Who's Afraid of Peer Review?\n\n\"Who's Afraid of Peer Review?\" is an article written by \"Science\" correspondent John Bohannon that describes his investigation of peer review among fee-charging open-access journals. Between January and August 2013, Bohannon submitted fake scientific papers to 304 journals owned by as many fee-charging open access publishers. The papers, writes Bohannon, \"were designed with such grave and obvious scientific flaws that they should have been rejected immediately by editors and peer reviewers\", but 60% of the journals accepted them. The article and associated data were published in the 4 October 2013 issue of \"Science\" as open access.\n\nThe first fee-charging open access scientific journals began appearing in 2000 with the creation of BioMed Central and then the Public Library of Science. Rather than deriving at least some of their revenue from subscription fees, fee-charging open access journals only charge the authors (or their funders) a publication fee. The published papers are then freely available on the internet. This business model, gold open access, is one of several solutions devised to make open access publishing sustainable. The number of articles published open access, or made freely available after some time behind a paywall (delayed open access), has grown rapidly. In 2013 more than half of the scientific papers published in 2011 were available for free.\n\nIn part because of the low barrier to entry into this market, as well as the fast and potentially large return on investment, many so-called \"predatory publishers\" have created low-quality journals that provide little to no peer review or editorial control, essentially publishing every submitted article as long as the publication fee is paid. Some of these publishers additionally deceive authors about publication fees, use the names of scientists as editors and reviewers without their knowledge, and/or obfuscate the true location and identity of the publishers. The prevalence of these deceptive publishers, and what the scientific community should do about them, has been hotly debated.\n\nBohannon used Python to create a \"scientific version of Mad Libs\". The paper's template is \"Molecule X from lichen species Y inhibits the growth of cancer cell Z\". He created a database of molecules, lichens, and cancer cells to substitute for X, Y, and Z. The data and conclusions were identical in every paper. The authors and their affiliations were also unique, and fake. The papers all described the discovery of a new cancer drug extracted from a lichen, but the data did not support that conclusion and the papers had intentionally obvious flaws.\n\nTo build a comprehensive list of fee-charging open access publishers, Bohannon relied on two sources: Beall's List of predatory publishers and the Directory of Open Access Journals (DOAJ). After filtering both lists for open access journals published in English, that charge authors a publication fee, and that have at least one medical, biological, or chemical journal, the list of targets included 304 publishers: 167 from the DOAJ, 121 from Beall's list, and 16 that were listed by both. The investigation focused entirely on fee-charging open access journals. Bohannon did not include other types of open access journals or subscription journals for comparison because the turnaround time for reviews in traditional journals is too long. The study consequently makes no claim about the relative quality of the different types of journals.\n\nIn total, 157 of the journals accepted the paper and 98 rejected it, with the other 49 not having completed their evaluation by the time Bohannon wrote his article. Of the 255 papers that underwent the entire peer review process to acceptance or rejection, about 60% of the final decisions occurred with no sign of actual peer review. For rejections, that may possibly have reflected filtering at the editorial level, but for acceptance can only reflect a flawed process. Only 36 submissions generated review comments recognizing any of the paper's scientific problems. 16 of those 36 papers were nonetheless accepted, in spite of poor to damning reviews. Many of the journals that accepted the paper are published by prestigious institutions and publishing companies, including Elsevier, Sage, Wolters Kluwer (through its subsidiary Medknow), and several universities.\n\nAmong those that rejected the paper are journals published by PLOS, BioMed Central, and Hindawi. The peer review provided by \"PLOS ONE\" was reported to be the most rigorous of all, and it was the only journal that identified the paper's ethical problems, for example the lack of documentation of how animals were treated in the creation of the cancer cell lines.\n\nAmong the publishers on Beall's list that completed the review process, 82% accepted the paper. Bohannon stated \"the results show that Beall is good at spotting publishers with poor quality control\". According to Jeffrey Beall, who created the list, this supports his claim to be identifying \"predatory\" publishers. However, the remaining 18% of publishers identified by Beall as predatory rejected the fake paper, leading science communicator Phil Davis to state \"That means that Beall is falsely accusing nearly one in five\".\n\nAmong the DOAJ publishers that completed the review process, 45% accepted the paper. According to a statement published on the DOAJ website, new criteria for inclusion in the DOAJ are being implemented.\n\nAlong with the report, \"Science\" published a map that shows the location of publishers, editors, and their bank accounts, color-coded by acceptance or rejection of the paper. The locations were derived from IP address traces within the raw headers of e-mails, WHOIS registrations, and bank invoices for publication fees. India emerged as the world's largest base for fee-charging open-access publishing, with 64 accepting the fatally flawed papers and only 15 rejecting it. The United States is the next largest base, with 29 publishers accepting the paper and 26 rejecting it. In Africa, Nigeria has the largest number, of which 100% accepted the paper.\n\nSince the story was released, publishers of three journals have stated that they are shutting them down. The DOAJ is reviewing its list and instituting tighter criteria for inclusion. The Open Access Scholarly Publishers Association (OASPA) formed a committee to investigate the circumstances that led to the acceptance of the fake paper by three of its members. On 11 November 2013, OASPA terminated the membership of two publishers (Dove Medical Press and Hikari Ltd.) who accepted the fake paper. Sage Publications, which also accepted a fake paper, was put \"under review\" for 6 months. Sage announced in a statement that it was reviewing the journal that accepted the fake paper, but that it would not shut it down. Sage's membership was reinstated at the end of the review period following changes to the journal's editorial processes.\n\nWithin hours of its publication, the \"Science\" investigation came under intense criticism by some supporters of the open-access movement.\n\nThe first substantial critique was posted by PLOS cofounder Michael Eisen on his blog. \"To suggest – as \"Science\" (though not Bohannon) are trying to do – that the problem with scientific publishing is that open access enables internet scamming is like saying that the problem with the international finance system is that it enables Nigerian wire transfer scams. There are deep problems with science publishing. But the way to fix this is not to curtail open-access publishing. It is to fix peer review.\" Eisen pointed out the irony of a subscription-based journal like \"Science\" publishing this report when its own peer review has failed so badly before, as in the 2010 publication of the arsenic DNA paper.\n\nIn an exchange between Eisen and Bohannon in a discussion hosted by Peter Suber, director of the Harvard Open Access Project, Eisen criticized the investigation for the bad publicity it generated for the open-access movement. \"Your study exclusively targeted open access journals – [which] strongly suggested, whether you meant to suggest this or not, that open access journals are more likely to engage in shoddy peer review and therefore more deserving of scrutiny.\" Bohannon responded that this critique was equivalent to \"shooting the messenger\".\n\nThere have also been many statements of support for the investigation, and statements of concern about the publishing fraud that it revealed.\nThe Committee on Publication Ethics has responded that \"There is no doubt that this 'sting' raises a number of issues ... though I'd argue they are not necessarily the ones that \"Science\" thinks are top priorities.\"\n\nSome scientists have discussed a number of options for making peer review more transparent. Doing so would make it harder to maintain a predatory journal that does no peer review, because the record of peer review would be lacking or would need to be faked. Another option is to more rigorously vet journals, for example by further empowering DOAJ or OASPA. DOAJ has recently tightened up its inclusion criteria, with the purpose of serving as a whitelist, very much like Beall's has been a blacklist.\n\n\n"}
{"id": "10605287", "url": "https://en.wikipedia.org/wiki?curid=10605287", "title": "William Gurley", "text": "William Gurley\n\nWilliam Gurley (March 16, 1821 – January 11, 1887) co-founded what is now known as Gurley Precision Instruments with his brother, and served as vice president and, from 1886 to 1887, acting president of Rensselaer Polytechnic Institute.\n\nGurley was born in Troy, New York, March 16, 1821, the son of Ephraim and Clarissa (Sharp) Gurley. He received the best education afforded by the schools in this vicinity, and choosing the profession of a civil engineer, attended the Rensselaer Polytechnic Institute, from which he graduated in 1839. He worked as a surveyor for a few years and then joined the firm of Oscar Hanks, who was a well-known maker of surveying instruments and church bells in Troy. He worked for Hanks for five years.\n\nIn 1845, Gurley established his own scientific instrument manufacturing business in partnership with James Phelps. Phelps had had his own shop since 1838. They called the business Phelps & Gurley. In 1844, William Gurley's brother Lewis Ephraim Gurley joined Phelps' shop as an apprentice and he then attended Union College from 1847 to 1851. Lewis Gurley rejoined the firm in 1851, at which point it was renamed Phelps and Gurleys. In 1852, Phelps sold out his interest and the business was renamed W.& L.E. Gurley. The firm still exists today as Gurley Precision Instruments. William Gurley was involved in several local organizations. For many years he was connected with the YMCA, and in 1851 was elected its president. He served as alderman from 1860 to 1864, and as fire commissioner he helped to improve fire prevention systems in major cities. He was a member of the New York State Assembly (Rensselaer Co., 1st D.) in 1867. In 1868, he was appointed by the Secretary of the Treasury to serve on a commission with Prof. Joseph Henry and other scientific experts to examine the best meter devised for determining the products of distillation, to be subsequently adopted by the department.\n\nGurley was a long-time trustee of Rensselaer, having been elected to that position in 1855. He was secretary of the institution from 1861 to 1872, vice-president from 1872 to 1887 and acting president from 1886 to 1887. He died on January 11, 1887 in Troy.\n\nThe W. & L. E. Gurley Building in Troy was designated a National Historic Landmark in 1983.\n\n"}
{"id": "1799104", "url": "https://en.wikipedia.org/wiki?curid=1799104", "title": "World Psychiatric Association", "text": "World Psychiatric Association\n\nThe World Psychiatric Association is an international umbrella organisation of psychiatric societies.\n\nOriginally created to produce world psychiatric congresses, it has evolved to hold regional meetings, to promote professional education and to set ethical, scientific and treatment standards for psychiatry.\n\nJean Delay was the first president of the Association for the Organization of World Congresses of Psychiatry when it was started in 1950. Donald Ewen Cameron became president of the World Psychiatric Association at its formal founding in 1961. \n\nIn February 1983, the Soviet All-Union Society of Neurologists and Psychiatrists resigned from the World Psychiatric Association. This resignation occurred as a preemptive action amid a movement to expel the Soviet body from the global organization due to political abuse of psychiatry in the Soviet Union. The Soviet body was conditionally readmitted into the World Psychiatric Association in 1989, following some improvements in human rights conditions, and an intensive debate among the association's delegates, in which the acting secretary of the Soviet delegation issued a statement conceding that \"previous political conditions in the U.S.S.R. created an environment in which psychiatric abuse occurred, including for nonmedical reasons.\"\n\n the institutional members of the World Psychiatric Association are 138 national psychiatric societies in 118 countries representing more than 200,000 psychiatrists worldwide. The societies are clustered into 18 zones and four regions: the Americas, Europe, Africa & Middle East, and Asia & Australasia. Representatives of the societies constitute the World Psychiatric Association General Assembly, the governing body of the organization. The association also has individual members and there are provisions for affiliation of other associations (e.g., those dealing with a particular topic in psychiatry). There are 72 scientific sections.\n\nThe official publication of the association is \"World Psychiatry.\" \"World Psychiatry\" and the association's official books are published by Wiley-Blackwell. WPA also self-publishes a quarterly newsletter on its website.\n\nSeveral WPA scientific sections have their own official journals and newsletters:\n\nThe association has helped establish a code of professional ethics for psychiatrists. The association has also looked into charges regarding China's treatment of the Falun Gong.\n\n"}
