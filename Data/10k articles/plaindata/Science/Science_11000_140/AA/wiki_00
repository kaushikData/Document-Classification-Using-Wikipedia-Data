{"id": "56897087", "url": "https://en.wikipedia.org/wiki?curid=56897087", "title": "Agglomerated food powder", "text": "Agglomerated food powder\n\nAgglomeration is a unit operation during which native particles are assembled to form bigger agglomerates, in which the original particle can still be distinguished. Agglomeration can be achieved through processes that use liquid as a binder (wet methods) or methods that do not involve any binder (dry methods). The liquid used in wet methods can be added directly to the product or via a humid environment. Using a fluidized bed dryer and multiple step spray drying are two examples of wet methods while roller compacting and extrusion are two examples of dry methods. \n\nAdvantages of agglomeration for food include:\n\n\nDisadvantages of food agglomeration:\n\n\nParticle size distribution is an important parameter to monitor in agglomerated food products. In both wet and dry agglomeration, particles of undesired sizes must be removed to ensure the best possible finished product performance. High-powered cyclones are the most common way to separate undesired fine particles (or \"fines\") from larger agglomerates (or \"overs\"). Cyclones utilize the combination of wind power and the different densities of the two products to pull the fines out of the mix. The fines can then be reworked through the agglomeration process to reduce yield loss. In contrast, shaker screens are often used to separate out the overs from the rest of the product. The overs can be reworked into the process by first being broken into smaller particles.\n\nWet agglomeration is a process that introduces a liquid binder to develop adhesion forces between the dry particles to be agglomerated. Mixing disperses the liquid over the particles evenly and promotes growth of the aggregate to the desired size. A final drying step is required to stabilize the agglomerates. \n\nIn all wet agglomeration methods, the first step is wetting the particles. This initiates adhesion forces between the particles. The next step, nucleation, is the process by which the native particles come together and are held with liquid bridges and capillary forces. Then, through coalescence or the growth phase, these small groups of particles come together to create larger particles until the particles are the desired size. Consolidation occurs as the agglomerates increase in density and strength through drying and collisions with other particles. Mixing as the powder dries also causes some particles to break and erode, creating smaller particles and fines. To achieve the correct particle size, erosion and growth must be balanced. The last step in wet agglomeration is the final stabilization through drying. The agglomerated particles are dried to less than 5% water content, and cooled to below their glass transition temperature. \n\nWet agglomeration falls into to categories based on method of agitation: Mechanical mixing and pneumatic mixing.\n\n\n\nDry agglomeration is agglomeration performed without water or binding liquids, instead using compression only. \n\nRoller compaction is a process in which powders are forced between two rolls, which compress the powders into dense sticks or sheets. These sticks or sheets are then ground into granules. Material properties will affect the mechanical properties of the resulting granules. Food particles with crystalline structures will deform plastically under pressure, and amorphous materials will deform viscoelastically. Roller compaction is more commonly used on individual ingredients of a finished powdered food product, than on a blend of ingredients producing a granulated finished product.\n\nSome advantages of roller compaction are \n\nDisadvantages: \n\nExamples of agglomerated food powders: Sucrose, sodium chloride, monosodium glutamate and fibers. \nExtrusion is executed by mixing the powder with liquid, additives, or dispersants and then compressing the mixture and forcing it through a die. The product is then dried and broken down to the desired particle size. . Extruded powders are dense. Extrusion is typically used for ingredients such as minerals and highly-hygroscopic products which benefit from reduced surface area, as well as products that are subject to oxidation. Extrusion for agglomeration should not be confused with the more common food extrusion process that involves creating a dough that is cooked and expands as it passes through the die.\n\n\n"}
{"id": "31010416", "url": "https://en.wikipedia.org/wiki?curid=31010416", "title": "Air-tractor sledge", "text": "Air-tractor sledge\n\nSir Douglas Mawson's air-tractor sledge was a converted fixed-wing aircraft taken on the 1911–14 Australasian Antarctic Expedition, the first plane to be taken to the Antarctic. Expedition leader Douglas Mawson had planned to use the Vickers R.E.P. Type Monoplane as a reconnaissance and search and rescue tool, and to assist in publicity, but the aircraft crashed heavily during a test flight in Adelaide, Australia, only two months before Mawson's scheduled departure date. The plane was nevertheless sent south with the expedition, after having been stripped of its wings and metal sheathing from the fuselage. Engineer Frank Bickerton spent most of the 1912 winter working to convert it to a sledge, fashioning brakes from a pair of geological drills and a steering system from the plane's landing gear. It was first tested on 15 November 1912 and subsequently assisted in laying depots for the summer sledging parties, but its use during the expedition was minimal.\n\nTowing a train of four sledges, the air-tractor accompanied a party led by Bickerton to explore the area to the west of the expedition's base at Cape Denison. The freezing conditions resulted in the jamming of the engine's pistons after just , and the air-tractor was left behind. Some time later it was dragged back to Cape Denison, and its frame was left on the ice when the expedition returned home in 1913. In 2008 a team from the Mawson's Huts Foundation began searching for the remains of the air-tractor sledge; a seat was found in 2009, and fragments of the tail assembly a year later.\n\nThe Mawson's Huts Foundation has undertaken extensive investigation using sophisticated equipment in 2009 and 2010. Results indicate that the air tractor, or parts of it, is still buried under 3m of ice where it was abandoned at Cape Denison.\n\nDouglas Mawson had accompanied Ernest Shackleton's 1907–09 British Antarctic Expedition. Along with Edgeworth David and Alistair Mackay, he had been part of a man-hauled sledging expedition, the first to reach the area of the South Magnetic Pole. Upon his return from Antarctica, he recommenced to his post as geology lecturer at the University of Adelaide. Despite an offer from Robert Falcon Scott to join his Terra Nova Expedition to reach the Geographic South Pole, Mawson began planning his own Antarctic expedition. Mawson's plan, which led to the Australasian Antarctic Expedition, envisaged three bases on the Antarctic continent, collectively surveying much of the coast directly south of Australia. He approached Shackleton, who not only approved of his plan but was prepared to lead the expedition himself. Although Shackleton withdrew from the expedition in December 1910, he continued to assist Mawson with publicity and fund-raising.\n\nMawson travelled to Britain in early 1911 to raise funds, hire crew, and purchase equipment. He considered taking a plane to the Antarctic, which could work as a reconnaissance tool, transport cargo, and assist with search and rescue. Crucially, as no plane had yet been taken to the continent, it could also be used to generate publicity. Unsure of the type of plane he should take, but considering a Blériot, Mawson mentioned his plans to Scott's wife Kathleen Scott, an aircraft enthusiast. She recommended he take a monoplane, and conveyed his interest to Lieutenant Hugh Evelyn Watkins of the Essex Regiment. Watkins had connections with the ship and aircraft manufacturer Vickers Limited, which had recently entered into a licence agreement to build and sell aircraft in Britain designed by the Frenchman Robert Esnault-Pelterie. In a letter to Mawson on 18 May, Kathleen wrote:\n\nOn Kathleen Scott's advice, Mawson purchased a Vickers R.E.P. Type Monoplane, one of only eight built. It was fitted with a five-cylinder R.E.P. engine developing , and had a maximum range of at a cruising speed of . Its wingspan was , and its length . The pilot used a joystick for pitch and roll, with lateral control by wing warping. Mawson opted for a two-seater version, in a tandem arrangement, with a spare ski undercarriage. The total bill, dated 17 August 1911, came to £955 4s 8d. Mawson hired Watkins to fly the plane, and Frank Bickerton to accompany as engineer. After Vickers tested the aircraft at Dartford and Brooklands, P&O shipped the plane to Adelaide aboard the steamship \"Macedonia\", at half the usual rate of freight.\n\nA series of public demonstrations were planned in Australia to assist in fund-raising, the first of which was scheduled for 5 October 1911 at the Cheltenham Racecourse in Adelaide. During a test flight the day before, excessive pressure in the fuel tank caused it to rupture, almost blinding Watkins. That problem resolved, Watkins took Frank Wild, whom Mawson had hired to command a support base during the expedition, on another test flight the morning of the demonstration. In Watkins' account, which he addressed to Vickers' Aviation Department, he wrote: \"[we were] about 200 ft. up. I got into a fierce tremor, and then into an air pocket, and was brought down about 100 ft., got straight, and dropped into another, almost a vacuum. That finished it. We hit the ground with an awful crash, both wings damaged, one cylinder broken, and the Nose bent up, the tail in half, etc.\"\n\nAlthough the two men were only slightly injured, the plane was damaged beyond repair. Mawson decided to salvage the plane by converting it into a motorised sledge. He fitted the skis, and removed the wings and most of the sheathing to save weight. In his official account of the expedition, \"The Home of the Blizzard\", Mawson wrote that the advantages of this \"air-tractor sledge\" were expected to be \"speed, steering control, and comparative safety from crevasses owing to the great length of the runners\". No longer needing a pilot, and believing him to be responsible for the crash, Mawson dismissed Watkins.\n\nThe air-tractor sledge was taken to Hobart, where the expedition ship SY \"Aurora\" was being loaded. It was secured on board in a crate lined with tin, which weighed far more than the sledge itself, on top of the ship's forecastle and two boat-skids. To fuel the sledge, along with the motor launch and the wireless equipment, the \"Aurora\" also carried of benzine and of kerosene. Fully loaded, the ship left Hobart on 2 December 1911.\n\nThe \"Aurora\" reached the Antarctic mainland on 8 January 1912, after a two-week stop on Macquarie Island to establish a wireless relay station and research base. The expedition's main base was established in Adélie Land, at Cape Denison in Commonwealth Bay. While the \"Aurora\" was unloading, a violent whirlwind lifted the lid off the air-tractor's crate, throwing it . The main hut was erected immediately, but the strong winds meant that work on the air-tractor's hangar was delayed until March. When the winds abated, a by hangar was constructed next to the main hut, from empty packing cases.\n\nBickerton began work on the air-tractor sledge on 14 April 1912. His first job was to repair the sledge, which had been damaged in transit when a violent storm hit the \"Aurora\". A giant wave had slammed into the crate containing the sledge, driving the fuselage through its side. With the repair completed, Bickerton began the serious work of converting the plane into a sledge. He constructed brakes from a pair of geological drills, and a steering system from the landing gear. Bickerton painted the engine and fuel tank black to absorb heat better and protect them from freezing. By June he had the engine running properly, and during a lull in the winds in early September he fitted the skis. Finally, he raised the fuselage off the ground to allow the propeller free movement.\n\nOn 27 October 1912, Mawson outlined the summer sledging program. Seven sledging parties would depart from Cape Denison, surveying the coast and interior of Adélie Land and neighbouring King George V Land. They were required to return to the base by 15 January, when the \"Aurora\" was due to depart; any later, it was feared, and she would be trapped by ice. Bickerton was to lead one of the parties, which would use the air-tractor to haul four sledges and explore the coast to the west of the hut. Most of the parties left in early November, but Bickerton's Western party delayed until December, in the hope of avoiding the ferocious winter winds. Work on the air-tractor sledge was delayed by the fierce winds, and the first trial took place on 15 November, between the main base and Aladdin's Cave—a depot which had been established on the plateau above Cape Denison. The air-tractor reached a speed of , covering the , expedition member Charles Laseron recorded, \"in great style\". Soon, the sledge began hauling cargo up the slope, laying depots for the summer sledging parties.\n\nThe Western party left Cape Denison on 3 December 1912. Accompanying Bickerton and the air-tractor were cartographer Alfred Hodgeman and surgeon Leslie Whetter. The air-tractor made slow progress hauling its train of sledges, and about out from the base its engine began experiencing difficulty. Bickerton shut it down and the three set up camp. At 4 am the next morning the party set off again, but the engine continued to struggle; oil ejected from an idle cylinder and the cylinder's lack of compression led Bickerton to suspect broken piston rings to be the root of the problem. This would take only a matter of hours to fix. As he later recorded, \"These thoughts were brought to a sudden close by the engine, without any warning, pulling up with such a jerk that the propeller was smashed. On moving the latter, something fell into the oil in the crank-case and fizzled, while the propeller could only be swung through an angle of about 30 [degrees].\"\n\nThe party continued without the air-tractor, man-hauling the sledges to a point west of Cape Denison, and returned to base on 18 January 1913. Mawson's Far Eastern Party failed to return, and six men, including Bickerton, remained for an extra winter. On 8 February, just hours after \"Aurora\" left Commonwealth Bay after waiting for three weeks, Mawson staggered alone into base, his colleagues Belgrave Edward Sutton Ninnis and Xavier Mertz dead. As Mawson was being nursed back to health, Bickerton dragged the air-tractor sledge back to base to diagnose the reason for its failure. He found that the freezing conditions had caused the engine oil to congeal, jamming the pistons. He abandoned the sledge at Boat Harbour, next to the base. When \"Aurora\" returned to Cape Denison for the final time on 13 December 1913, only the engine and propeller were taken back to Australia.\n\nThe bill for the plane remained unpaid. In 1914 Vickers reminded Mawson, who had apparently forgotten the outstanding debt. Mawson wrote to Vickers director Sir Trevor Dawson in November 1916, requesting the company write off the bill as a donation. His company buoyed by armaments contracts, Dawson agreed. The next expedition to take a plane to the Antarctic was Shackleton's 1921–22 \"Quest\" Expedition, but the Avro Baby remained grounded owing to missing parts. Not until 16 November 1928—when Hubert Wilkins and Carl Ben Eielson flew for 20 minutes around Deception Island, just over a year before Admiral Richard Evelyn Byrd's first flight over the South Pole—was a plane airborne in the Antarctic.\n\nThe frame of the air-tractor sledge remained on the ice at Boat Harbour where Bickerton had left it. The last expedition to Cape Denison to see the frame was in 1976; the next expedition, in 1981, could find no trace of it. The ice in that location does not move, and the implication is that the frame sank through the ice. It is therefore possible the frame is still there.\n\nIn 2007-8 a team from the Mawson's Huts Foundation began to search for the remnants of the plane. Using photographs from 1913, 1931 and 1976 it was possible to derive transits between the frame and distant objects which located the frame to a small area of ice about 50 m from the hut. Comparison with a 1931 photograph by Frank Hurley confirmed this location.\n\nThe following summer (2008–9), the team extensively surveyed the area where they believed the air-tractor to be, using ground-penetrating radar. A 3 metre deep trench was dug in a promising area, but nothing was found except fragments of seaweed indicating the overlying ice must have melted sometime in the past. Temperature records from the nearby Dumont d'Urville Station showed that there had been extended periods (each of about six weeks) of above average temperatures in 1976 and 1981, suggesting the ice around the harbour could have melted. Dr Chris Henderson, the leader of the team, believes \"the frame sank in situ to the rock surface, three metres below the present ice surface\".\n\nNext year (the 2009–10 season) further search was undertaken using differential GPS, bathymetry equipment, ice augers, a magnetometer and a metal detector (whose sensor was placed down the ice auger holes after drilling). The ice showed signs of having extensively melted in the past, was about 3 metres thick and covering smooth rock which extended Northwards to become the harbour bottom. Visual examination of the harbour bottom during the bathymetry survey did not reveal any fragments of the frame in the first 30 metres of the harbour.\n\nThe most significant findings from the ice survey were a positive reading from the metal detector, coupled with a significant echo from the Ground Penetrating Radar, both from the small area where the frame is assumed to have sunk.\n\nParts of the Air Tractor are already known to exist: The Australian Antarctic Division has one wheel from the frame, and its ice-rudder – both of which were found in the harbour. In January 2009 the remains of a seat from the air-tractor were found in rocks near the hut, about from where the team believes the frame to be buried.<ref name=\"ABC 21/1/2009\"></ref> On 1 January 2010, a day of unusually low tide, 4 small capping pieces from the end section of the tail were found by the edge of the harbour. The tail and a section of fuselage had been removed from the rest of the air-tractor before it was abandoned in 1913, therefore this discovery did not shed much light on the location of the rest of the frame, but it suggests that \"the frame, or parts of it, can survive for nearly 100 years in this environment\".\n\nThe team returned to Cape Denison over the 2010–11 summer, but the crash of a French helicopter near Dumont d'Urville Station in October 2010 forced deployment of a much reduced team with no resources to continue the search.<ref name=\"ABC 5/1/2011\"></ref>\n\nThe findings to date (2011) suggest that metal object(s) exist at a depth of 3 metres, on rock, in the location where the frame was last known to have been seen in 1976. This is likely to be the remains of Mawson's Air Tractor, but confirmation awaits a future opportunity.\n\n"}
{"id": "30220865", "url": "https://en.wikipedia.org/wiki?curid=30220865", "title": "Alexander Bryson", "text": "Alexander Bryson\n\nAlexander Bryson (12 October 1816 – 7 December 1866) was a Scottish biologist, geologist and horologist who served as president of the Royal Scottish Society of Arts (1860–61) and as president of the Royal Physical Society of Edinburgh (1863).\n\nHe was born on 12 October 1816 in Edinburgh, the son of Robert Bryson FRSE (1778-1852), a watchmaker, and Janet Gillespie (1788-1858). \n\nHe attended the High School in Edinburgh then trained as a watchmaker and entered the family business, then renamed Robert Bryson & Son.\n\nHis first wife, Elizabeth Waterstone Gillespie (possibly a cousin) bore him two children who died in infancy, and a daughter and son (William Alexander Bryson) and died 10 April 1855 aged 44.\n\nHis second wife, Catherine McDonald Cuthbertson, also died young in September 1859, aged only 32, after bearing him another son.\n\nHis third wife, Jane Thomson, bore him another son, Leonard Horner Bryson, survived him and remarried.\n\nHe was President of the Royal Scottish Society of Antiquarians 1860-1861. He was President of the Royal Physical Society of Edinburgh in 1863. He was also a member of the Botanical Society of Edinburgh and the Edinburgh Geological Society.\n\nIn 1858 he was elected a Fellow of the Royal Society of Edinburgh. He was President of the Royal Scottish Society of the Arts 1860-61.\n\nHe died on 7 December 1866 at Hawkhill House, a country villa between Leith and Edinburgh. He is buried in New Calton Cemetery with his two wives next to his parents.\n\n"}
{"id": "321017", "url": "https://en.wikipedia.org/wiki?curid=321017", "title": "Allen Brain Atlas", "text": "Allen Brain Atlas\n\nThe Allen Mouse and Human Brain Atlases are projects within the Allen Institute for Brain Science which seek to combine genomics with neuroanatomy by creating gene expression maps for the mouse and human brain. They were initiated in September 2003 with a $100 million donation from Paul G. Allen and the first atlas went public in September 2006. \n, seven brain atlases have been published: Mouse Brain Atlas, Human Brain Atlas, Developing Mouse Brain Atlas, Developing Human Brain Atlas, Mouse Connectivity Atlas, Non-Human Primate Atlas, and Mouse Spinal Cord Atlas. There are also three related projects with data banks: Glioblastoma, Mouse Diversity, and Sleep. It is the hope of the Allen Institute that their findings will help advance various fields of science, especially those surrounding the understanding of neurobiological diseases. The atlases are free and available for public use online.\n\nIn 2001, Paul Allen gathered a group of scientists, including James Watson and Steven Pinker, to discuss the future of neuroscience and what could be done to enhance neuroscience research (Jones 2009). During these meetings David Anderson from the California Institute of Technology proposed the idea that a three-dimensional atlas of gene expression in the mouse brain would be of great use to the neuroscience community. The project was set in motion in 2003 with a 100 million dollar donation by Allen through the Allen Institute for Brain Science. \nThe project used a technique for mapping gene expression developed by Gregor Eichele and colleagues at the Max Planck Institute for Biophysical Chemistry in Goettingen, Germany. The technique uses colorimetric in situ hybridization to map gene expression. The project set a 3-year goal of finishing the project and making it available to the public.\n\nAn initial release of the first atlas, the mouse brain atlas, occurred in December 2004. Subsequently, more data for this atlas was released in stages. The final genome-wide data set was released in September 2006. However, the final release of the atlas was not the end of the project; the Atlas is still being improved upon. Also, other projects including the human brain atlas, developing mouse brain, developing human brain, mouse connectivity, non-human primate atlas, and the mouse spinal cord atlas are being developed through the Allen Institute for Brain Science in conjunction with the Allen Mouse Brain Atlas.\n\nThe overarching goal and motto for all Allen Institute projects is \"fueling discovery\". The project strives to fulfill this goal and advance science in a few ways. First, they create brain atlases to better understand the connections between genes and brain functioning. They aim to advance the research and knowledge about neurobiological conditions such as Parkinson's, Alzheimer's, and Autism with their mapping of gene expression throughout the brain. \nThe Brain Atlas projects also follow the \"Allen Institute\" motto with their open release of data and findings. This policy is also related to another goal of the Institute: collaborative and multidisciplinary research. Thus, any scientist from any discipline is able to look at the findings and take them into account while designing their own experiments. Also available to the public is the Brain Explorer application.\n\nThe Allen Institute for Brain Science uses a project-based philosophy for their research. Each brain atlas focuses on its own project, made up of its own team of researchers. To complete an atlas, each research team collects and synthesizes brain scans, medical data, genetic information and psychological data. With this information, they are able to construct the 3-D biochemical architecture of the brain and figure out which proteins are expressed in certain parts of the brain. To gather the needed data, scientists at the Allen Institute use various techniques. One technique involves the use of postmortem brains and brain scanning technology to discover where in the brain genes are turned on and off. Another technique, called in situ hybridization, or ISH, is used to view gene expression patterns as in situ hybridization images.\n\nWithin the Brain Atlases, these 3-D ISH digital images and graphs reveal, in color, the regions where a given gene is expressed. In the Brain Explorer, any gene can be searched for and selected resulting in the in situ image appearing as an easily manipulated and explored fashion. Part of the creation of this anatomy-centred database of gene expression, includes aligning ISH data for each gene with a three-dimensional coordinate space through registration with a reference atlas created for the project.\n\nThe different types of cells in the central nervous system originate from varying gene expression. A map of gene expression in the brain allows researchers to correlate forms and functions. The Allen Brain Atlas lets researchers view the areas of differing expression in the brain which enables the viewing of neural connections throughout the brain. Viewing these pathways through differing gene expression as well as functional imaging techniques permits researchers to correlate between gene expression, cell types, and pathway function in relation to behaviors or phenotypes.\n\nEven though the majority of research has been done in mice, 90% of genes in mice have a counterpart in humans. This makes the Atlas particularly useful for modeling neurological diseases. The gene expression patterns in normal individuals provide a standard for comparing and understanding altered phenotypes. Extending information learned from mouse diseases will help better the understanding of human neurological disorders. The atlas can show which genes and particular areas are effected in neurological disorders; the action of a gene in a disease can be evaluated in conjunction with general expression patterns and this data could shed light on the role of the particular gene in the disorder.\n\nThe Allen Brain Atlas website contains a downloadable 3-D interactive Brain explorer. The explorer is essentially a search engine for locations of gene expression; this is particularly useful in finding regions that express similar genes. Users can delineate networks and pathways using this application by connecting regions that co-express a certain gene. The explorer uses a multicolor scale and contains multiple planes of the brain that let viewers see differences in density and expression level. The images are a composite of many averaged samples so it is useful when comparing to individuals with abnormally low gene expression.\n\nThe Allen Mouse Brain Atlas is a comprehensive genome-wide map of the adult mouse brain that reveals where each gene is expressed. The mouse brain atlas was the original project of the Allen Brain Atlas and was finished in 2006. The purpose of the atlas is to aid in the development of neuroscience research. The hope of the project is that it will allow scientists to gain a better understanding of brain diseases and disorders such as autism and depression.\n\nThe Allen Human Brain Atlas was made public in May 2010. It was the first anatomically and genomically comprehensive three-dimensional human brain map. The atlas was created to enhance research in many neuroscience research fields including neuropharmacology, human brain imaging, human genetics, neuroanatomy, genomics and more. The atlas is also geared toward furthering research into mental health disorders and brain injuries such as Alzheimer's disease, autism, schizophrenia and drug addiction.\n\nThe Allen Developing Mouse Brain Atlas is an atlas which tracks gene expression throughout the development of a C57BL/6 mouse brain. The project began in 2008 and is currently ongoing. The atlas is based on magnetic resonance imaging (MRI). It traces the growth, white matter, connectivity, and development of the C57BL/6 mouse brain from embryonic day 12 to postnatal day 80.\nThis atlas enhances the ability of neuroscientists to study how pollutants and genetic mutations effect the development of the brain. Thus, the atlas may be used to determine what toxins pose special threats to children and pregnant mothers.\n\nThe Allen Mouse Brain Connectivity Atlas was launched in November 2011. Unlike other atlases from the Allen Institute, this atlas focuses on the identification of neural circuitry that govern behavior and brain function. This neural circuitry is responsible for functions like behavior and perception. This map will allow scientists to further understand how the brain works and what causes brain diseases and disorders, such as Parkinson's disease and depression.\n\nUnveiled in July 2008, the Allen Mouse Spinal Cord Atlas was the first genome-wide map of the mouse spinal cord ever constructed. The spinal cord atlas is a map of genome wide gene expression in the spinal cord of adult and juvenile C57 black mice. The initial unveiling included data for 2,000 genes and an anatomical reference section. A plan for the future includes expanding the amount of data to about 20,000 genes spanning the full length of the spinal cord.\n\nThe aim of the spinal cord atlas is to enhance research in the treatment of spinal cord injury, diseases, and disorders such as Lou Gehrig's diseases and spinal muscular atrophy. The project was funded by an array of donors including the Allen Research Institute, Paralyzed Veterans of America Research Foundation, the ALS Association, Wyeth Research, PEMCO Insurance, National Multiple Sclerosis Society, International Spinal Research Trust, and many other organizations, foundations, corporate and private donors.\n\n\n\n"}
{"id": "14436015", "url": "https://en.wikipedia.org/wiki?curid=14436015", "title": "Amiga support and maintenance software", "text": "Amiga support and maintenance software\n\nAmiga support and maintenance software performs service functions such as formatting media for a specific filesystem, diagnosing failures that occur on formatted media, data recovery after media failure, and installation of new software for the Amiga family of personal computers—as opposed to application software, which performs business, education, and recreation functions.\n\nThe Amiga came with some embedded utility programs, but many more were added over time, often by third-party developers and companies.\n\nCommodore included utility programs with the operating system. Many of these were original features, which were adopted into other systems:\n\n\nNone of these update systems was widely used by the Amiga community.\n\nAmiga places system utilities in two standard directories:\n\n\nAmigaOS features a standard centralized utility to partition and format hard disks, called \"HDToolBox\".\n\nMorphOS uses an updated version of the \"SCSIConfig\" utility (since MorphOS version 2, \"HDConfig\") implemented by third party vendor Phase5. In spite of the name, \"SCSIConfig\" possessed a unique feature at the time, which was providing a consistent mechanism to manage all types of disk interfaces, including IDE, irrespective of which interface the disk(s) in question used.\n\nAmigaOS diagnostic tools are usually programs which display the current state of Exec and AmigaDOS activities.\n\n\nPromoter and ForceMonitor are utilities that allow the user to control the resolution of Intuition screens for Amiga programs.\n\nWHDLoad is a utility to install legacy Amiga games on a hard disk and load them from Workbench desktop instead of floppies, on which they were often delivered.\n\njst is an older utility which the developer abandoned in order to concentrate efforts on WHDLoad. Old jstloaders can be read with WHDLoad, and jst itself has some early level of WHDLoad compatibility.\n\nThe original Amiga CLI (Command Line Interface) had some basic editing capabilities, command templates, and other features such as ANSI compatibility and color selection. In AmigaOS 1.3, the program evolved into a complete text-based shell called AmigaShell, with command history and enhanced editing capabilities.\n\nThird-party developers created improved shells because the console-handler standard command line device driver (or \"handler\" in Amiga technical language) is independent of the command-line interpreter. This program controlled text-based interfaces into Amiga. Console-handler replacements include KingCON, ViNCEd, and Conman.\n\nSome well-known shells from other platforms were ported to Amiga. These included bash (Bourne Again SHell), CSH (C Shell), and ZSH (Z shell). The shells taken from Unix and Linux were adapted into Amiga and improved with its peculiar capabilities and functions.\n\nThe MorphOS Shell is an example of Z shell mixed with the KingCON console handler. It originated as a Unix-like shell and is provided with all the features expected from such a component: AmigaDOS commands (more than 100 commands, most of which are Unix-like), local and global variables, command substitution, command redirection, named and unnamed pipes, history, programmable menus, multiple shells in a window, ANSI compatibility, color selection, and so on. It also includes all the necessary commands for scripting.\n\nStarting from the original Amiga WIMP standard desktop, Workbench, Amiga interfaces were enhanced by third-party developers. Amiga users are free to replace the original Workbench interface with Scalos and Directory Opus. The standard GUI toolkit, called Intuition, was enhanced in OS2.x with the introduction of GadTools; and third parties created their own toolkits such as Magic User Interface (MUI) (the standard on MorphOS systems) and ClassAct, which evolved into ReAction GUI (the standard GUI on AmigaOS 4.0).\n\nMany users have added advanced graphics drivers to their Amiga. This lets the AmigaOS handle high resolution graphics, enhanced with millions of colors. Standard GUI interfaces with this capability are CyberGraphX, EGS, and Picasso96.\n\nGraphical libraries available on the Amiga include:\n\nAll Amiga systems can also support the SDL (Simple DirectMedia Layer) cross-platform, multimedia, and free software libraries written in C which creates an abstraction over various platforms' graphics, sound, and input APIs, allowing a developer to write a computer game or other multimedia application once and run it on many operating systems.\n\nAmiga supports PostScript through Ghostscript and SaxonScript (included with Saxon Publisher). Ghostview is the foremost used graphical GUI for GhostScript on the Amiga.\n\nSince AmigaOS 2.1, in the Prefs (Preferences) system directory, there is a printer preferences program called \"PrinterPS\", which allows the use of PostScript printers on the Amiga.\n\nOriginal Amiga outline fonts (also called vector fonts) were Agfa Compugraphic fonts available since AmigaOS 2.0 with the standard utility Fountain (later called IntelliFont) from Commodore. Third-party developers added support for TrueType fonts using various libraries, such as TrueType Library I and II, and LibFreeType library.\n\nThe standard diskfont.library also supported bitmap multicolour fonts (ColorFonts), such as the commercial Kara Fonts, or even animated fonts also originally created by Kara Computer Graphics.\n\nCommodore provided a bitmap font editor called FED. Personal Fonts Maker was the most widely used Amiga software to create bitmap fonts, while TypeSmith v.2.5b was the \"de facto\" standard utility to create outline fonts.\n\nIn the first Amiga OS releases, Commodore included a standard floppy disk recovery utility called DiskDoctor. Its purpose was to recover files from mangled floppy disks. Unfortunately, this utility worked only with AmigaDOS standard disks. A major fault was that it did not save the recovered data on different disks, rather it saved the info on the original and performed its operations directly on the original. It wrote on original disks and destroyed non-AmigaDOS disks (mainly autobooting games) by overwriting their bootblock. DiskDoctor renamed recovered disks to \"Lazarus\" (after the resurrected man in the New Testament).\n\nThese features were undocumented and led to an Amiga urban legend that there was a computer virus nicknamed the Lazarus Virus, whose final purpose was to make disks unreadable and renaming it with that name. Third-party developers released data recovery programs such as DiskSalv, which was more often used to validate Amiga filesystems on hard disk partitions.\n\nOther Amiga disk repair and backup tools included:\n\nDuring the 8 bit and 16/32 bit era, copying software was not considered illegal in many countries, and piracy was not perceived as being a crime by the users of home computers (usually young people). Commodore 64 and Sinclair ZX Spectrum software was copied using audio cassettes, while IBM PC, Atari, and Amiga software was copied using special programs called disk copiers which were engineered to copy any floppy disk surface byte by byte, often using special, efficient, and advanced techniques of programming and \"Disk Track driving\" to maintain Floppy Disk read/write head alignment.\n\nIn the early days of the Amiga platform, about 16 disk copiers were created in a short amount of time (1985–1989) that enabled copying Amiga floppy disks, including Nibbler, QuickNibble, ZCopier, XCopy/Cachet, FastCopier, Disk Avenger, Tetra Copy (which enabled the user to play Tetris while copying disks), Cyclone, Maverick, D-Copy, Safe II, PowerCopier, Quick Copier, Marauder II (styled as \"Marauder //\"), Rattle Copy, and BurstNibble.\n\nMany were legal in many countries until years later. These programs (for example, Marauder, X-Copy, and Nibbler) were then sold in packages complete with instructions, warranty, and EULA like other productivity software. Some floppy drives included LED track indicators to show if the disks were hacked by the original programmers to support up to track 82 of the disk. There were also copying solutions that included both hardware and software, like Super Card Ami II and Syncro Express I/II/III.\n\nDFC5 could only copy standard AmigaOS formatted disks for backup purposes; however, it multitasked inside of the Amiga Workbench GUI.\n\nX-COPY III, and later the final version, X-COPY Pro, were the most popular Amiga copy programs. They were capable of bit-by-bit copying, also called \"nibbling\". Although incapable of true multitasking, the programs were capable of taking advantage of Amiga configurations with multiple floppy drives; for instance, on Amiga systems with four floppy drives, X-COPY was capable of simultaneously copying from a source drive to three others. Coupled with excellent bit-by-bit replication capabilities, these features made X-COPY the \"de facto\" standard for copying floppy disks on the Amiga.\n\nAnother popular copying program was D-COPY, by a Swedish group \"D-Mob\", which, in spite of some innovative features and better/faster copying routines, failed to gain dominance.\n\nThe most popular archivers were LhA and LZX. Programs to archive ZIP, Gzip, Bzip2, and RAR files were available but seldom used, and many have an Amiga counterpart, such as 7-Zip. Utilities were available for reading and writing archive formats such as ARC, ARJ (unarchive only), the CAB files common in Windows installation, StuffIt SIT archives from Macintosh, Uuencode (used for encoding binary attachments of e-mail messages), TAR (common on UNIX and GNU/Linux), RPM (from Red Hat), and more.\n\nAmiga supported \"packed\" or \"crunched\" (meaning compressed) executables, which were common in the age of floppy disks, when disk space and memory conservation was critical. These executable binary files had a decompress routine attached to them that would automatically unpack or decrunch (decompress) the executable upon loading into memory.\n\nThe Amiga also included \"level depacking\", implemented by \"Titanics Cruncher\", which enabled a binary executable to be decrunched as it was being loaded, requiring a very small amount of memory to do so. In general, packing and crunching was taken from the Commodore 64 cracking scene. Some crunchers, such as Time Cruncher, were \"ported\" from Commodore 64, displaying the same visual effects during decrunching. The CPU in the Amiga was completely different from the one in the Commodore 64, requiring a complete rewrite.\n\nNoteworthy were TurboImploder and PowerPacker, as they were easy to use, with graphical interfaces. Other popular crunchers were DefjamPacker, TetraPack, DoubleAction, Relokit, StoneCracker, Titanics and CrunchMania. The ability to compress and decompress single files and directories on the fly has been present on the AmigaOS since at least 1994.\n\nA similar feature was implemented relatively recently as a property in the ZFS filesystem.\n\nThe AmigaOS packers and cruncher libraries are centralized by using the XPK system. The XPK system consists of a master library and several (de)packer sublibraries. Programs use only the master library directly, while sublibraries (akin to plug-ins) implement the actual (de)compression. When unpacking/decrunching, the applications do not need to know which library was used to pack or crunch the data. XPK is a wrapper for crunchers; to decrunch non-XPK packed formats requires XFD.\n\nAnother important invention on the Amiga platform was the ADF format for creating images of Amiga floppy disks, either standard AmigaDOS floppies or non-DOS (\"NDOS\") ones, for use in Amiga emulators, such as WinUAE. Amiga emulators and AmigaOS (with third-party software) can use these files as if they were virtual floppy disks. Unlimited virtual floppies could be created on modern Amigas, although WinUAE on a real PC can handle only four at a time, the maximum number of floppy drives that the Amiga hardware could have connected at any one time.\n\nAll the popular Amiga compression implementations and archive files are now centralized and implemented by a single system library called XAD, which has a front-end GUI named Voodoo-X. It is included in AmigaOS 3.9 and up with UnArc. This library is modular and can handle more than 80 compression formats.\n\nAmiga can use various filesystems. The historical standards are the original Amiga filesystem, called the Old File System. This was good for floppy disks but wasted space on hard disks and is considered obsolete.\n\nThe Fast File System (FFS) can handle file names up to 30 characters, has international settings (it can optionally recognise upper- and lower-case accented letters as equivalent) and could also be cached, if the users chose to format the partition with the cache option. The FFS filesystem evolved into FFS2.\n\nModern journaling file systems for Amiga are the Smart File System (SFS) and Professional File System (PFS).\n\nThe MultiUser File System (MuFS) supports multiple users. Using MuFS the owner of the system could grant various privileges on files by creating privileges for groups and users. It was first available with the Ariadne Ethernet card, and later standalone. The Professional File System suite has a utility to let PFS to be patched to support MuFS and MuFS features. The latest version is 1.8 and was released in 2001.\n\nCrossDOS is a utility to read MS-DOS formatted floppy disks in FAT12 and FAT16 filesystem, either 720 KiB double-density format or high-density (1440 KiB) (on connected floppy drives that can read 1440 MS-DOS disks). It is a commercial product, and a slightly cut-down version was included with AmigaOS beginning with version 2.1.\n\nThe FAT95 library recognizes partitions of various filesystems common in other systems such as FAT16 and FAT32. It also reads DOS floppies and USB pen drives formatted with FAT16 or FAT32.\n\nFilesystems like ext2 for Linux, NTFS from Microsoft, and more are supported by third-party developers.\n\nMorphOS natively supports SFS, FFS/FFS2, PFS, MacOS HFS, HFS+, Linux Ext2, FAT16, FAT32, and NTFS filesystems.\n\nThe Datatype system of AmigaOS is a centralized, expandable, modular system describing any kind of file (text, music, image, videos). Each has a standard load/save module.\n\nAny experienced programmer, using the Amiga Datatype programming guidelines, could create new standard datatype modules. The module could be left visible to the whole Amiga system (thus to all Amiga programs) by copying the datatype into the system directory \"SYS:Classes/DataTypes/\", and the descriptor (used to identify files) into \"DEVS:DataTypes/\".\n\nThis allows programs to load and save any files for which the corresponding datatypes exist. File descriptors did not need to be embedded in the executable code. An independent system of loaders was not needed for new productivity software. Amiga productivity software tools therefore have a smaller size and a more clean design than similar programs running in other operating systems.\n\nSupported Amiga datatypes include:\n\"MultiView\" is the Amiga universal viewer. It can load and display any file for which a corresponding datatype exists.\n\nModern Amiga-like operating systems such as AmigaOS 4.0 and MorphOS can handle also MIME types. Any kind of file, due to its peculiar characteristics (thanks to filename extensions), or data embedded into the file itself (for example into file header) can be associated with a program that handle it, and this feature improves and completes the capabilities of Amiga to recognize and deal with any kind of file.\n\nThe only known historical USB stack for the Amiga was created for the MacroSystem DraCo Amiga clone. It supported only USB 1.0 and ceased with the demise of that platform.\n\nModern USB support drivers for Amiga are:\n\nThe only known historical Amiga support for FireWire was built for the DraCo Amiga clone by Macrosystem.\n\nOnly one FireWire interface exists for Amiga. It is named Fireworks, and it was created for the MorphOS system by programmer Pavel Fedin. It is still in an early stage of development and is freely downloadable.\n\nThe print manager program TurboPrint, by German firm IrseeSoft, is the \"de facto\" standard for advanced printing on the Amiga. It is a modular program with many drivers which support many modern printers. PrintStudio Professional I and II are another well known printer driver system for the Amiga.\n\nPrintManager v39 by Stephan Rupprecht, available at the Aminet repository, is a print spooler for AmigaOS 3.x and 4.0.\n\nVideo digitizing includes DigiView; the FrameMachine Zorro II expansion card for A2000, 3000, 4000; the Impact Vision IV24 from GVP; the VidiAmiga real time digitizer; and the Paloma module for the Picasso IV graphics card.\n\nIn the 1980s, SummaGraphics tablets were common. Summagraphics directly supported Amiga with its drivers.\n\nIn 1994, GTDriver (Graphic Tablet Driver) was the most common driver for serial port tablets, like Summagraphics MM, Summagraphics Bitpadone, CalComp 2000, Cherry, TekTronix 4967, and WACOM. It could also be used as a mouse driver.\n\nGraphics tablets now are mainly USB devices and are automatically recognized by Amiga USB stacks. The most widely used driver for graphic tablets is FormAldiHyd. FormAldiHyd can be used with Aiptek, Aldi, Tevion, and WACOM IV (Graphire, ArtPad, A3, A4, A5, and PenPartner) graphic tablets.\n\nThe Poseidon USB driver, written by the same author as FormAldiHyd, Chris Hodges, directly supports USB graphics tablets, including ones more modern than FormAldiHyd. \n\nAmiga programs often have scanner drivers embedded in their interface and are limited to some ancient scanner models. One example is Art Department Professional (ADPro).\n\nIn recent times, scanner management is managed by the Amiga Poseidon USB stack. Poseidon detects scanners from their signature, and loads the corresponding HIDD scanner module. The graphical interface is managed by programs like ScanTrax and ScanQuix.\n\nThe Amiga has special circuitry to support a genlock signal and chromakey. Genlock software vendors included GVP (Great Valley Products) (an American hardware manufacturer) and Hama, Electronic Design, and Sirius genlocks from Germany.\n\nThe IRCom class is a driver that supports the IRCom standard and is available for the USB Poseidon Stack.\n\nPegasos computers have an internal IrDA port connector for connecting infrared devices, but MorphOS offers no support for it. The internal IrDA port can be used by installing Linux.\n\nThe Amiga can use WiFi external routers connected physically through Ethernet cable and talk with remote WiFi devices. Drivers are available for Prism2 internal PCI and PCMCIA WiFi expansion cards, but there are no drivers for Bluetooth standard devices like mobile phones, Bluetooth handsets, keyboards, or mice.\n\nA USB class exists for the Poseidon stack to use the \"Wireless PC Lock\" USB device by Sitecom Europe BV and engage its security functions. It is called Wireless PC Lock.\n\nIn the past, drivers and hardware cards were available to drive the Polaroid Freeze Frame Digital Camera System Polaroid Digital Palette CI-3000 and Digital Palette CI 5000, with Polaroid software.\n\nDrivers for single-frame video recorders allow users to save on tape the 3D animations created on the Amiga using Ampex and Betacam devices. Also available are time-base correctors (TBCs), a family of devices correcting timing errors; one was the Personal TBC series.\n\nThe Amiga helped to create and launch digital recorders coupled with an internal hard disk and a DVD drive for file transfer. One was Broadcaster Elite, one of the first digital video recorders, based on a SCSI system and a Zorro II Amiga expansion card.\n\nExpansion cards could transform an Amiga into a waveform monitor or vectorscope.\n\nThe Phonepak card from GVP transformed the Amiga into a telephone switchboard, fax system, and SOHO (small office/home office) answering machine.\n\nThe Amiga was used as a video titler system in the experimental era of high-definition television. A battery of three Amigas was used as a video titler on analog HDTV experiments on HDTV NTSC 1125 lines standard, by channels like ESPN, ABC, and NBC.\n\n"}
{"id": "4814673", "url": "https://en.wikipedia.org/wiki?curid=4814673", "title": "Anastasio Alfaro", "text": "Anastasio Alfaro\n\nAnastasio Alfaro (February 16, 1865 – January 20, 1951) was a Costa Rican zoologist, geologist and explorer.\n\nAlfaro was director of the National Museum of Costa Rica, and whilst holding this position arranged the Costa Rican display at the Historical American Exposition in Madrid. Limon worm salamander \"Oedipina alfaroi\" is named after him.\n\n"}
{"id": "28914283", "url": "https://en.wikipedia.org/wiki?curid=28914283", "title": "Bartlett Glacier", "text": "Bartlett Glacier\n\nBartlett Glacier () is a tributary glacier, about long and wide at its terminus, flowing northeast from Nilsen Plateau and joining Scott Glacier close north of Mount Gardiner. It was discovered in December 1934 by the Byrd Antarctic Expedition geological party under Quin Blackburn, and named by Richard E. Byrd for Captain Robert A. Bartlett of Brigus, Newfoundland, a noted Arctic navigator and explorer who recommended that the expedition acquire the \"Bear\", an ice-ship which was purchased and rechristened by Byrd as the \"Bear of Oakland\".\n\n"}
{"id": "46601456", "url": "https://en.wikipedia.org/wiki?curid=46601456", "title": "Chequered retreat", "text": "Chequered retreat\n\nA Chequered retreat, (\"retraite en échiquier\", Fr.) is so called from the several component parts of a pre-mechanised line or battalion, which alternately retreat and face about in the presence of an enemy, exhibiting the figure of the chequered squares upon a chess board.\n\n"}
{"id": "12176442", "url": "https://en.wikipedia.org/wiki?curid=12176442", "title": "Chiapan climbing rat", "text": "Chiapan climbing rat\n\nThe Chiapan climbing rat (Tylomys bullaris) is a species of rodent in the family Cricetidae.\nIt is found only in Mexico. The species is known from only one location in Tuxtla Gutierrez, Chiapas. The habitat in the region is being converted to agricultural and urban use, which is likely causing critical declines in numbers of \"T. bullaris\".\n\n"}
{"id": "50326", "url": "https://en.wikipedia.org/wiki?curid=50326", "title": "Cognitive neuroscience", "text": "Cognitive neuroscience\n\nThe term cognitive neuroscience was coined by George Armitage Miller and Michael Gazzaniga in year 1976. Cognitive neuroscience is the scientific field that is concerned with the study of the biological processes and aspects that underlie cognition, with a specific focus on the neural connections in the brain which are involved in mental processes. It addresses the questions of how cognitive activities are affected or controlled by neural circuits in the brain. Cognitive neuroscience is a branch of both neuroscience and psychology, overlapping with disciplines such as behavioral neuroscience, cognitive psychology, physiological psychology and affective neuroscience. Cognitive neuroscience relies upon theories in cognitive science coupled with evidence from neurobiology, and computational modeling.\n\nParts of the brain play an important role in this field. Neurons play the most vital role, since the main point is to establish an understanding of cognition from a neural perspective, along with the different lobes of the cerebral cortex.\n\nMethods employed in cognitive neuroscience include experimental procedures from psychophysics and cognitive psychology, functional neuroimaging, electrophysiology, cognitive genomics, and behavioral genetics.\n\nStudies of patients with cognitive deficits due to brain lesions constitute an important aspect of cognitive neuroscience. The damages in lesioned brains provide a comparable basis with regards to healthy and fully functioning brains. \nThese damages change the neural circuits in the brain and cause it to malfunction during basic cognitive processes, such as memory or learning. With the damage, we can compare how the healthy neural circuits are functioning, and possibly draw conclusions about the basis of the affected cognitive processes.\n\nAlso, cognitive abilities based on brain development are studied and examined under the subfield of developmental cognitive neuroscience. This shows brain development over time, analyzing differences and concocting possible reasons for those differences.\n\nTheoretical approaches include computational neuroscience and cognitive psychology.\n\nCognitive neuroscience is an interdisciplinary area of study that has emerged from neuroscience and psychology. There were several stages in these disciplines that changed the way researchers approached their investigations and that led to the field becoming fully established.\n\nAlthough the task of cognitive neuroscience is to describe how the brain creates the mind, historically it has progressed by investigating how a certain area of the brain supports a given mental faculty. However, early efforts to subdivide the brain proved to be problematic. The phrenologist movement failed to supply a scientific basis for its theories and has since been rejected. The aggregate field view, meaning that all areas of the brain participated in all behavior, was also rejected as a result of brain mapping, which began with Hitzig and Fritsch’s experiments<ref name=\"G. Fritsch, E. Hitzig, Electric excitability of the cerebrum (Über die elektrische Erregbarkeit des Grosshirns), Epilepsy & Behavior, Volume 15, Issue 2, June 2009, Pages 123-130, ISSN 1525-5050, 10.1016/j.yebeh.2009.03.001.\">G. Fritsch, E. Hitzig, Electric excitability of the cerebrum (Über die elektrische Erregbarkeit des Grosshirns), Epilepsy & Behavior, Volume 15, Issue 2, June 2009, Pages 123-130, ISSN 1525-5050, 10.1016/j.yebeh.2009.03.001.</ref> and eventually developed through methods such as positron emission tomography (PET) and functional magnetic resonance imaging (fMRI). Gestalt theory, neuropsychology, and the cognitive revolution were major turning points in the creation of cognitive neuroscience as a field, bringing together ideas and techniques that enabled researchers to make more links between behavior and its neural substrates.\n\nPhilosophers have always been interested in the mind: \"the idea that explaining a phenomenon involves understanding the mechanism responsible for it has deep roots in the History of Philosophy from atomic theories in 5th century B.C. to its rebirth in the 17th and 18th century in the works of Galileo, Descartes, and Boyle. Among others, it’s Descartes’ idea that machines humans build could work as models of scientific explanation.\"\nFor example, Aristotle thought the brain was the body’s cooling system and the capacity for intelligence was located in the heart. It has been suggested that the first person to believe otherwise was the Roman physician Galen in the second century AD, who declared that the brain was the source of mental activity, although this has also been accredited to Alcmaeon. However, Galen believed that personality and emotion were not generated by the brain, but rather by other organs. Andreas Vesalius, an anatomist and physician, was the first to believe that the brain and the nervous system are the center of the mind and emotion. Psychology, a major contributing field to cognitive neuroscience, emerged from philosophical reasoning about the mind.\n\nOne of the predecessors to cognitive neuroscience was phrenology, a pseudoscientific approach that claimed that behavior could be determined by the shape of the scalp. In the early 19th century, Franz Joseph Gall and J. G. Spurzheim believed that the human brain was localized into approximately 35 different sections. In his book, The Anatomy and Physiology of the Nervous System in General, and of the Brain in Particular, Gall claimed that a larger bump in one of these areas meant that that area of the brain was used more frequently by that person. This theory gained significant public attention, leading to the publication of phrenology journals and the creation of phrenometers, which measured the bumps on a human subject's head. While phrenology remained a fixture at fairs and carnivals, it did not enjoy wide acceptance within the scientific community. The major criticism of phrenology is that researchers were not able to test theories empirically.\n\nThe localizationist view was concerned with mental abilities being localized to specific areas of the brain rather than on what the characteristics of the abilities were and how to measure them. Studies performed in Europe, such as those of John Hughlings Jackson, supported this view. Jackson studied patients with brain damage, particularly those with epilepsy. He discovered that the epileptic patients often made the same clonic and tonic movements of muscle during their seizures, leading Jackson to believe that they must be occurring in the same place every time. Jackson proposed that specific functions were localized to specific areas of the brain, which was critical to future understanding of the brain lobes.\n\nAccording to the aggregate field view, all areas of the brain participate in every mental function.\n\nPierre Flourens, a French experimental psychologist, challenged the localizationist view by using animal experiments. He discovered that removing the cerebellum in rabbits and pigeons affected their sense of muscular coordination, and that all cognitive functions were disrupted in pigeons when the cerebral hemispheres were removed. From this he concluded that the cerebral cortex, cerebellum, and brainstem functioned together as a whole. His approach has been criticised on the basis that the tests were not sensitive enough to notice selective deficits had they been present.\n\nPerhaps the first serious attempts to localize mental functions to specific locations in the brain was by Broca and Wernicke. This was mostly achieved by studying the effects of injuries to different parts of the brain on psychological functions. In 1861, French neurologist Paul Broca came across a man who was able to understand language but unable to speak. The man could only produce the sound \"tan\". It was later discovered that the man had damage to an area of his left frontal lobe now known as Broca's area. Carl Wernicke, a German neurologist, found a patient who could speak fluently but non-sensibly. The patient had been the victim of a stroke, and could not understand spoken or written language. This patient had a lesion in the area where the left parietal and temporal lobes meet, now known as Wernicke's area. These cases, which suggested that lesions caused specific behavioral changes, strongly supported the localizationist view.\n\nIn 1870, German physicians Eduard Hitzig and Gustav Fritsch published their findings about the behavior of animals. Hitzig and Fritsch ran an electric current through the cerebral cortex of a dog, causing different muscles to contract depending on which areas of the brain were electrically stimulated. This led to the proposition that individual functions are localized to specific areas of the brain rather than the cerebrum as a whole, as the aggregate field view suggests. Brodmann was also an important figure in brain mapping; his experiments based on Franz Nissl’s tissue staining techniques divided the brain into fifty-two areas.\n\nAt the start of the 20th century, attitudes in America were characterised by pragmatism, which led to a preference for behaviorism as the primary approach in psychology. J.B. Watson was a key figure with his stimulus-response approach. By conducting experiments on animals he was aiming to be able to predict and control behaviour. Behaviourism eventually failed because it could not provide realistic psychology of human action and thought – it focused primarily on stimulus-response associations at the expense of explaining phenomena like thought and imagination. This led to what is often termed as the \"cognitive revolution\".\n\nIn the early 20th century, Santiago Ramón y Cajal and Camillo Golgi began working on the structure of the neuron. Golgi developed a silver staining method that could entirely stain several cells in a particular area, leading him to believe that neurons were directly connected with each other in one cytoplasm. Cajal challenged this view after staining areas of the brain that had less myelin and discovering that neurons were discrete cells. Cajal also discovered that cells transmit electrical signals down the neuron in one direction only. Both Golgi and Cajal were awarded a Nobel Prize in Physiology or Medicine in 1906 for this work on the neuron doctrine.\n\nSeveral findings in the 20th century continued to advance the field, such as the discovery of ocular dominance columns, recording of single nerve cells in animals, and coordination of eye and head movements. Experimental psychology was also significant in the foundation of cognitive neuroscience. Some particularly important results were the demonstration that some tasks are accomplished via discrete processing stages, the study of attention, and the notion that behavioural data do not provide enough information by themselves to explain mental processes. As a result, some experimental psychologists began to investigate neural bases of behaviour. \nWilder Penfield created maps of primary sensory and motor areas of the brain by stimulating cortices of patients during surgery. Sperry and Gazzaniga’s work on split brain patients in the 1950s was also instrumental in the progress of the field.\n\nNew brain mapping technology, particularly fMRI and PET, allowed researchers to investigate experimental strategies of cognitive psychology by observing brain function. Although this is often thought of as a new method (most of the technology is relatively recent), the underlying principle goes back as far as 1878 when blood flow was first associated with brain function. Angelo Mosso, an Italian psychologist of the 19th century, had monitored the pulsations of the adult brain through neurosurgically created bony defects in the skulls of patients. He noted that when the subjects engaged in tasks such as mathematical calculations the pulsations of the brain increased locally. Such observations led Mosso to conclude that blood flow of the brain followed function.\n\nOn September 11, 1956, a large-scale meeting of cognitivists took place at the Massachusetts Institute of Technology. George A. Miller presented his \"The Magical Number Seven, Plus or Minus Two\" paper while Noam Chomsky and Newell & Simon presented their findings on computer science. Ulric Neisser commented on many of the findings at this meeting in his 1967 book \"Cognitive Psychology\". The term \"psychology\" had been waning in the 1950s and 1960s, causing the field to be referred to as \"cognitive science\". Behaviorists such as Miller began to focus on the representation of language rather than general behavior. David Marr concluded that one should understand any cognitive process at three levels of analysis. These levels include computational, algorithmic/representational, and physical levels of analysis.\n\nBefore the 1980s, interaction between neuroscience and cognitive science was scarce. Cognitive neuroscience began to integrate the newly laid theoretical ground in cognitive science, that emerged between the 1950s and 1960s, with approaches in experimental psychology, neuropsychology and neuroscience. (Neuroscience was not established as a unified discipline until 1971). In the very late 20th century new technologies evolved that are now the mainstay of the methodology of cognitive neuroscience, including TMS (1985) and fMRI (1991). Earlier methods used in cognitive neuroscience include EEG (human EEG 1920) and MEG (1968). Occasionally cognitive neuroscientists utilize other brain imaging methods such as PET and SPECT. An upcoming technique in neuroscience is NIRS which uses light absorption to calculate changes in oxy- and deoxyhemoglobin in cortical areas. In some animals Single-unit recording can be used. Other methods include microneurography, facial EMG, and eye tracking. Integrative neuroscience attempts to consolidate data in databases, and form unified descriptive models from various fields and scales: biology, psychology, anatomy, and clinical practice. In 2014, Stanislas Dehaene, Giacomo Rizzolatti and Trevor Robbins, were awarded the Brain Prize \"for their pioneering research on higher brain mechanisms underpinning such complex human functions as literacy, numeracy, motivated behaviour and social cognition, and for their efforts to understand cognitive and behavioural disorders\". Brenda Milner, Marcus Raichle and John O'Keefe received the Kavli Prize in Neuroscience “for the discovery of specialized brain networks for memory and cognition\" and O'Keefe shared the Nobel Prize in Physiology or Medicine in the same year with May-Britt Moser and Edvard Moser \"for their discoveries of cells that constitute a positioning system in the brain\". In 2017, Wolfram Schultz, Peter Dayan and Ray Dolan were awarded the Brain Prize \"for their multidisciplinary analysis of brain mechanisms that link learning to reward, which has far-reaching implications for the understanding of human behaviour, including disorders of decision-making in conditions such as gambling, drug addiction, compulsive behaviour and schizophrenia\".,\n\nDavid H. Hubel and Torsten Wiesel, both neurophysiologists, studied the visual system in cats to better understand sensory processing. They performed experiments which demonstrated the specificity of the responding of neurons. Their experiments showed that neurons fired rapidly at some angles, and not so much at others. A difference was also found in light and dark settings. Their studies gave rise to the idea of complex visual representations being formed from relatively simple stimuli.\n\nThey also discovered the simple cell and complex cell. These exist in the primary visual cortex and respond differentially to differently oriented presentations of light.\n\nRecently the foci of research have expanded from the localization of brain area(s) for specific functions in the adult brain using a single technology, studies have been diverging in several different directions: exploring the interactions between different brain areas, using multiple technologies and approaches to understand brain functions, and using computational approaches. Advances in non-invasive functional neuroimaging and associated data analysis methods have also made it possible to use highly naturalistic stimuli and tasks such as feature films depicting social interactions in cognitive neuroscience studies.\n\n\nExperimental methods of specific psychology fields include:\n\n\n\n"}
{"id": "1502499", "url": "https://en.wikipedia.org/wiki?curid=1502499", "title": "Cultural capital", "text": "Cultural capital\n\nIn sociology, cultural capital consists of the social assets of a person (education, intellect, style of speech and dress, etc.) that promote social mobility in a stratified society. Cultural capital functions as a social-relation within an economy of practices (system of exchange), and comprises all of the material and symbolic goods, without distinction, that society considers rare and worth seeking. As a social relation within a system of exchange, cultural capital includes the accumulated cultural knowledge that confers social status and power.\n\nIn \"Cultural Reproduction and Social Reproduction\" (1977), Pierre Bourdieu and Jean-Claude Passeron presented \"cultural capital\" to conceptually explain the differences among the levels of performance and academic achievement of children within the educational system of France in the 1960s; and further developed the concept in the essay \"The Forms of Capital\" (1985) and in the book \"The State Nobility: Élite Schools in the Field of Power\" (1996).\n\nIn the sociological essay, \"The Forms of Capital\" (1985), Pierre Bourdieu identifies three categories of capital:\n\n\nThere are three types of cultural capital: (i) Embodied capital; (ii) Objectified capital; and (iii) Institutionalised capital:\n\n\nThe cultural capital of a person is linked to his or her habitus (embodied disposition and tendencies) and field (social positions), which are configured as a social-relation structure. The field is the place of social position that is constituted by the conflicts that occur when social groups endeavour to establish and define what is cultural capital, within a given social space; therefore, depending upon the social field, one type of cultural capital can simultaneously be legitimate and illegitimate. In that way, the legitimization (societal recognition) of a type of cultural capital can be arbitrary and derived from symbolic capital.\n\nThe habitus of a person is composed of the intellectual dispositions inculcated to him or her by family and the familial environment, and are manifested according to the nature of the person. As such, the social formation of a person's habitus is influenced by family, by objective changes in social class, and by social interactions with other people in daily life; moreover, the habitus of a person also changes when he or she changes social positions within the field.\n\nThe concept of cultural capital has received widespread attention all around the world, from theorists and researchers alike. It is mostly employed in relation to the education system, but on the odd occasion has been used or developed in other discourses. Use of Bourdieu's cultural capital can be broken up into a number of basic categories. First, are those who explore the theory as a possible means of explanation or employ it as the framework for their research. Second, are those who build on or expand Bourdieu's theory. Finally, there are those who attempt to disprove Bourdieu's findings or to discount them in favour of an alternative theory. The majority of these works deal with Bourdieu's theory in relation to education, only a small number apply his theory to other instances of inequality in society.\n\nThose researchers and theorists who explore or employ Bourdieu's theory use it in a similar way as it was articulated by Bourdieu. They usually apply it uncritically, and depending on the measurable indicators of cultural capital and the fields within which they measure it, Bourdieu's theory either works to support their argument totally, or in a qualified way.. These works to help portray the usefulness of Bourdieu's concept in analysing (mainly educational) inequality but they do not add anything to the theory.\n\nOne work which does employ Bourdieu's work in an enlightening way is that of Emirbayer & Williams (2005) who use Bourdieu's notion of fields and capital to examine the power relations in the field of social services, particularly homeless shelters. The authors talk of the two separate fields that operate in the same geographic location (the shelter) and the types of capital that are legitimate and valued in each. Specifically they show how homeless people can possess \"staff-sanctioned capital\" or \"client-sanctioned capital\" (2005:92) and show how in the shelter, they are both at the same time, desirable and undesirable, valued and disparaged, depending on which of the two fields they are operating in. Although the authors do not clearly define staff-sanctioned and client-sanctioned capital as cultural capital, and state that usually the resources that form these two capitals are gathered from a person's life as opposed to their family, it can be seen how Bourdieu's theory of cultural capital can be a valuable theory in analyzing inequality in any social setting.\n\nA number of works expand Bourdieu's theory of cultural capital in a beneficial manner, without deviating from Bourdieu's framework of the different forms of capital. In fact, these authors can be seen to explore unarticulated areas of Bourdieu's theory as opposed to constructing a new theory. For instance, Stanton-Salazar & Dornbusch (1995:121) examine how those people with the desired types of cultural (and linguistic) capital in a school transform this capital into \"instrumental relations\" or social capital with institutional agents who can transmit valuable resources to the person, furthering their success in the school. They state that this is simply an elaboration of Bourdieu's theory. Similarly, Dumais (2002) introduces the variable of gender to determine the ability of cultural capital to increase educational achievement. The author shows how gender and social class interact to produce different benefits from cultural capital. In fact in Distinction (1984:107), Bourdieu states \"sexual properties are as inseparable from class properties as the yellowness of lemons is inseparable from its acidity\". He simply did not articulate the differences attributable to gender in his general theory of reproduction in the education system.\n\nOn the other hand, two authors have introduced new variables into Bourdieu's concept of cultural capital. Emmison & Frow's (1998) work centers on an exploration of the ability of Information Technology to be considered a form of cultural capital. The authors state that \"a familiarity with, and a positive disposition towards the use of bourgeoisie technologies of the information age can be seen as an additional form of cultural capital bestowing advantage on those families that possess them\". Specifically computers are \"machines\" (Bourdieu, 1986:47) that form a type of objectified cultural capital, and the ability to use them is an embodied type of cultural capital. This work is useful because it shows the ways in which Bourdieu's concept of cultural capital can be expanded and updated to include cultural goods and practices which are progressively more important in determining achievement both in the school and without.\n\nHage uses Bourdieu's theory of cultural capital to explore multiculturalism and racism in Australia. His discussion around race is distinct from Bourdieu's treatment of migrants and their amount of linguistic capital and habitus. Hage actually conceives of \"whiteness\" (in Dolby, 2000:49) as being a form of cultural capital. 'White' is not a stable, biologically determined trait, but a \"shifting set of social practices\" (Dolby, 2000:49). He conceptualizes the nation as a circular field, with the hierarchy moving from the powerful center (composed of 'white' Australians) to the less powerful periphery (composed of the 'others'). The 'others' however are not simply dominated, but are forced to compete with each other for a place closer to the centre. This use of Bourdieu's notion of capital and fields is extremely illuminating to understand how people of non-Anglo ethnicities may try and exchange the cultural capital of their ethnic background with that of 'whiteness' to gain a higher position in the hierarchy. It is especially useful to see it in these terms as it exposes the arbitrary nature of what is \"Australian\", and how it is determined by those in the dominant position (mainly 'white' Australians). In a path-breaking study, Bauder (2006) uses the notions of habitus and cultural capital to explain the situation of migrants in the labor market and society.\n\nIn the article \"Against School\" (2003), the retired teacher John Taylor Gatto addresses education in modern schooling. The relation of cultural capital can be linked to \"Principles of Secondary Education\" (1918), by Alexander Inglis, which indicates how American schooling is what like Prussian schooling in the 1820s. The objective was to divide children into sections, by distributing them by subject, by age, and by test score. Inglis introduces six basic functions for modern schooling; functions three, four, and five are related to cultural capital, and describe the manner in which schooling enforces the cultural capital of each child, from a young age. Functions three, four, and five are: 3. Diagnosis and direction:\nSchool is meant to determine the proper social role of each student, by logging mathematic and anecdotal evidence into cumulative records. 4. Differentiation: Once the social role of a student is determined, the children are sorted by role and trained only as merited for his or her social destination. 5. Selection: This refers to Darwin's theory of natural selection applied to \"the favored races\".\n\nThe idea is to help American society, by consciously attempting to improve the breeding stock. Schools are meant to tag the socially unfit with poor grades, remedial-schooling placement, and other notable social punishments that their peers will then view and accept them as intellectually inferior, and effectively bar them from the reproductive (sexual, economic, and cultural) sweepstakes of life. That was the purpose of petty humiliation in school: \"It was the dirt down the drain.\" The three functions are directly related to cultural capital, because through schooling children are discriminated by social class and cognitively placed into the destination that will make them fit to sustain that social role. That is the path leading to their determined social class; and, during the fifth function, they will be socially undesirable to the privileged children, and so kept in a low social stratum.\n\nPaul DiMaggio expands on Bourdieu's view on cultural capital and its influence on education saying: \"Following Bourdieu, I measure high school students' cultural capital using self-reports of involvement in art, music, and literature.\" In his journal article titled Cultural Capital and School Success: The Impact of Status Culture Participation on the Grades of U.S. High School Students in the \"American Sociological Review\".\n\nIn the US, Richard A. Peterson and A Simkus (1992) extended the cultural capital theory, exclusively on (secondary) analysis of survey data on Americans, in 'How musical tastes mark occupational status groups', with the term \"cultural omnivores\" as a particular higher status section in the US that has broader cultural engagements and tastes spanning an eclectic range from highbrow arts to popular culture. Originally, it was Peterson (1992) who coined the term 'cultural omnivore' to address an anomaly observed in the evidence revealed by his work with Simkus (Peterson and Simkus, 1992) which showed that people of higher social status, contrary to elite-mass models of cultural taste developed by French scholars with French data, were not averse to participation in activities associated with popular culture. The work rejected the universal adaptation of the cultural capital theory, especially in the 20th century in advanced post-industrialist societies like the United States.\n\nIn the UK, Louise Archer and colleagues (2015) developed the concept of science capital. The concept of science capital draws from the work of Bourdieu, in particular his studies focusing on the reproduction of social inequalities in society. Science capital is made up of science related cultural capital and social capital as well as habitus. It encapsulates the various influences that a young person's life experiences can have on their science identity and participation in science-related activities. The empirical work on science capital builds from a growing body of data into students' aspirations and attitudes to science, including ASPIRES and Enterprising Science. The concept of science capital was developed as a way to understand why these science-related resources, attitudes and aspirations led some children to pursue science, while others did not. The concept provides policy makers and practitioners with a useful framework to help understand what shapes young people's engagement with (and potential resistance to) science.\n\nBourdieu's theory has been expanded to reflect modern forms of cultural capital, such as internet memes. Studies conducted by Asaf Nissenbaum and Limor Shifman on the topic of internet memes; utilised the website 4chan to analyse how these memes can be seen as forms of cultural capital. Discourse demonstrates the different forums and mediums that memes can be expressed through, such as different 'boards' on 4chan. \n\nCriticisms of Bourdieu's concept have been made on many grounds, including a lack of conceptual clarity. Perhaps due to this lack of clarity, researchers have operationalised the concept in diverse ways, and have varied in their conclusions. While some researchers may be criticised for using measures of cultural capital which focus only on certain aspects of 'highbrow' culture, this is a criticism which could also be leveled at Bourdieu's own work. Several studies have attempted to refine the measurement of cultural capital, in order to examine which aspects of middle-class culture actually have value in the education system.\n\nIt has been observed that Bourdieu's theory, and in particular his notion of habitus, is entirely deterministic, leaving no place for individual agency or even individual consciousness. Although Bourdieu claimed to have transcended the dichotomy of structure and agency, this is not necessarily convincing. For example, the Oxford academic John Goldthorpe has long argued that:\n\nBourdieu has also been criticised for his lack of consideration of gender. Kanter (in Robinson & Garnier, 1986) point out the lack of interest in gender inequalities in the labour market in Bourdieu's work. \nHowever, Bourdieu addressed the topic of gender head-on in his 2001 book \"Masculine Domination\". Bourdieu stated on the first page of the prelude in this book that he considered masculine domination to be a prime example of symbolic violence.\n\n\n"}
{"id": "33949679", "url": "https://en.wikipedia.org/wiki?curid=33949679", "title": "Digital South Asia Library", "text": "Digital South Asia Library\n\nThe Digital South Asia Library (DSAL) is a global collaboration to provide universal access to materials for reference and research on South Asian topics, utilizing digital technologies, to scholars, public officials, business leaders, and other users. \n\nParticipants in the Digital South Asia Library include leading U.S. universities, led by the University of Chicago, the Center for Research Libraries, the South Asia Microform Project, the Committee on South Asian Libraries and Documentation, the Association for Asian Studies, the Library of Congress, the Asia Society, American Institute of Indian Studies, the British Library, the University of Oxford, the University of Cambridge, MOZHI in India (till 2008), the Sundarayya Vignana Kendram in India, Madan Puraskar Pustakalaya in Nepal, and other institutions in South Asia.\n\nThis project builds upon a two-year pilot project funded by the Association of Research Libraries' Global Resources Program with support from The Andrew W. Mellon Foundation. The United States Department of Education has provided support through four grants under the Technological Innovation and Cooperation for Foreign Information Access (TICFIA) program and two grants under the International Research and Studies (IRS) program. A grant from the Ford Foundation made possible the formation of the South Asia Union Catalogue, a subordinate program under the Digital South Asia Library.\n\n\n"}
{"id": "1331039", "url": "https://en.wikipedia.org/wiki?curid=1331039", "title": "Dirac large numbers hypothesis", "text": "Dirac large numbers hypothesis\n\nThe Dirac large numbers hypothesis (LNH) is an observation made by Paul Dirac in 1937 relating ratios of size scales in the Universe to that of force scales. The ratios constitute very large, dimensionless numbers: some 40 orders of magnitude in the present cosmological epoch. According to Dirac's hypothesis, the apparent similarity of these ratios might not be a mere coincidence but instead could imply a cosmology with these unusual features:\nNeither of these two features has gained wide acceptance in mainstream physics.\n\nLNH was Dirac's personal response to a set of large number 'coincidences' that had intrigued other theorists of his time. The 'coincidences' began with Hermann Weyl (1919), who speculated that the observed radius of the universe, \"R\", might also be the hypothetical radius of a particle whose rest energy is equal to the gravitational self-energy of the electron:\nwhere \"r\" is the classical electron radius, \"m\" is the mass of the electron, \"m\" denotes the mass of the hypothetical particle, and \"r\" is its electrostatic radius.\n\nThe coincidence was further developed by Arthur Eddington (1931) who related the above ratios to N, the estimated number of charged particles in the universe:\n\nIn addition to the examples of Weyl and Eddington, Dirac was also influenced by the primeval-atom hypothesis of Georges Lemaître, who lectured on the topic in Cambridge in 1933. The notion of a varying-G cosmology first appears in the work of Edward Arthur Milne a few years before Dirac formulated LNH. Milne was inspired not by large number coincidences but by a dislike of Einstein's general theory of relativity. For Milne, space was not a structured object but simply a system of reference in which relations such as this could accommodate Einstein's conclusions:\nwhere \"M\" is the mass of the universe and \"t\" is the age of the universe in seconds. According to this relation, \"G\" increases over time.\n\nThe Weyl and Eddington ratios above can be rephrased in a variety of ways, as for instance in the context of time:\n\nwhere \"t\" is the age of the universe, formula_10 is the speed of light and \"r\" is the classical electron radius. Hence, in units where \"c\"=1 and \"r\" = 1, the age of the universe is about 10 units of time. This is the same order of magnitude as the ratio of the electrical to the gravitational forces between a proton and an electron:\n\nHence, interpreting the charge formula_12 of the electron, the mass formula_13/formula_14 of the proton/electron, and the permittivity factor formula_15 in atomic units (equal to 1), the value of the gravitational constant is approximately 10. Dirac interpreted this to mean that formula_16 varies with time as formula_17. Although George Gamow noted that such a temporal variation does not necessarily follow from Dirac's assumptions, a corresponding change of G has not been found.\nAccording to general relativity, however, G is constant, otherwise the law of conserved energy is violated. Dirac met this difficulty by introducing into the Einstein field equations a gauge function that describes the structure of spacetime in terms of a ratio of gravitational and electromagnetic units. He also provided alternative scenarios for the continuous creation of matter, one of the other significant issues in LNH:\n\nDirac's theory has inspired and continues to inspire a significant body of scientific literature in a variety of disciplines. In the context of geophysics, for instance, Edward Teller seemed to raise a serious objection to LNH in 1948 when he argued that variations in the strength of gravity are not consistent with paleontological data. However, George Gamow demonstrated in 1962 how a simple revision of the parameters (in this case, the age of the Solar System) can invalidate Teller's conclusions. The debate is further complicated by the choice of LNH cosmologies: In 1978, G. Blake argued that paleontological data is consistent with the 'multiplicative' scenario but not the 'additive' scenario. Arguments both for and against LNH are also made from astrophysical considerations. For example, D. Falik argued that LNH is inconsistent with experimental results for microwave background radiation whereas Canuto and Hsieh argued that it \"is\" consistent. One argument that has created significant controversy was put forward by Robert Dicke in 1961. Known as the anthropic coincidence or fine-tuned universe, it simply states that the large numbers in LNH are a necessary coincidence for intelligent beings since they parametrize fusion of hydrogen in stars and hence carbon-based life would not arise otherwise.\n\nVarious authors have introduced new sets of numbers into the original 'coincidence' considered by Dirac and his contemporaries, thus broadening or even departing from Dirac's own conclusions. Jordan (1947) noted that the mass ratio for a typical star and an electron approximates to 10, an interesting variation on the 10 and 10 that are typically associated with Dirac and Eddington respectively.\n\nSeveral authors have recently identified and pondered the significance of yet another large number, approximately 120 orders of magnitude. This is for example the ratio of the theoretical and observational estimates of the energy density of the vacuum, which Nottale (1993) and Matthews (1997) associated in an LNH context with a scaling law for the cosmological constant. Carl Friedrich von Weizsäcker identified 10 with the ratio of the universe's volume to the volume of a typical nucleon bounded by its Compton wavelength, and he identified this ratio with the sum of elementary events or bits of information in the universe.\n\n\n\n"}
{"id": "50634536", "url": "https://en.wikipedia.org/wiki?curid=50634536", "title": "Early Harvest Scheme", "text": "Early Harvest Scheme\n\nEarly harvest scheme is a precursor to a free trade agreement (FTA) between two trading partners. This is to help the two trading countries to identify certain products for tariff liberalisation pending the conclusion of FTA negotiation. It is primarily a confidence building measure.\n"}
{"id": "1274285", "url": "https://en.wikipedia.org/wiki?curid=1274285", "title": "Earth orbit rendezvous", "text": "Earth orbit rendezvous\n\nEarth orbit rendezvous (EOR) is a potential methodology for conducting round trip human flights to the Moon, involving the use of space rendezvous to assemble, and possibly fuel, components of a translunar vehicle in low Earth orbit. It was considered and ultimately rejected in favor of lunar orbit rendezvous (LOR) for NASA's Apollo Program of the 1960s and 1970s. Three decades later, it was planned to be used for Project Constellation, until that program’s cancellation in October 2010.\n\nThe Agena target vehicle (ATV) was used for testing Earth orbit rendezvous in the NASA Gemini Program. Gemini 6 and Gemini 7 rendezvoused in orbit in 1965, but without Agena. Next, Gemini 8 successfully docked with the Agena on March 16, 1966. The Agena-Gemini rendezvous also achieved other objectives in later Gemini launches, including docked orbital maneuvering (Gemini 10 and Gemini 11), inspection of the abandoned Gemini 8 ATV (Gemini 10) and space walks (Gemini 12).\n\nThe EOR proposal for Apollo consisted of using a series of small rockets half the size of a Saturn V to put different components of a spacecraft to go to the Moon in orbit around the Earth, then assemble them in orbit. Experiments of Project Gemini involving docking with the Agena target vehicle were designed partly to test the feasibility of this program.\n\nIn the end, NASA employed the Lunar Orbit Rendezvous for the Apollo Program: a Saturn V would simultaneously lift both the Apollo Command and Lunar Modules into low Earth orbit, and then the Saturn V third stage would fire again (Trans-lunar injection) to send both spacecraft to the Moon.\n\nThis mode had been revived for Project Constellation as the Earth Departure Stage (EDS) and Altair (LSAM), which would be launched into low Earth orbit on the Ares V rocket. The EDS and Altair would be met by the separately launched Orion (CEV). Once joined in low Earth orbit, the three would then travel out to the Moon and the Orion/Altair combination would fly a lunar orbit rendezvous flight pattern.\n"}
{"id": "7071096", "url": "https://en.wikipedia.org/wiki?curid=7071096", "title": "Engineering design process", "text": "Engineering design process\n\nThe engineering design process is a methodical series of steps that engineers use in creating functional products and processes. The process is highly iterative - parts of the process often need to be repeated many times before another can be entered - though the part(s) that get iterated and the number of such cycles in any given project may vary.\n\nThe steps tend to get articulated, subdivided, and/or illustrated in different ways, but they generally reflect certain core principles regarding the underlying concepts and their respective sequence and interrelationship.\n\nOne framing of the engineering design process delineates the following stages: \"research, conceptualization, feasibility assessment, establishing design requirements, preliminary design, detailed design, production planning and tool design, and production\". Others, noting that \"different authors (in both research literature and in textbooks) define different phases of the design process with varying activities occurring within them,\" have suggested more simplified/generalized models - such as \"problem definition, conceptual design, preliminary design, detailed design, and design communication\". A standard summary of the process in European engineering design literature is that of \"clarification of the task, conceptual design, embodiment design, detail design\". In these examples, other key aspects - such as concept evaluation and prototyping - are subsets and/or extensions of one or more of the listed steps. It's also important to understand that in these as well as other articulations of the process, different terminology employed may have varying degrees of overlap, which affects what steps get stated explicitly or deemed \"high level\" versus subordinate in any given model.\n\nVarious stages of the design process (and even earlier) can involve a significant amount of time spent on locating information and research. Consideration should be given to the existing applicable literature, problems and successes associated with existing solutions, costs, and marketplace needs.\n\nThe source of information should be relevant, including existing solutions. Reverse engineering can be an effective technique if other solutions are available on the market. Other sources of information include the Internet, local libraries, available government documents, personal organizations, trade journals, vendor catalogs and individual experts available.\n\nEstablishing design requirements and conducting requirement analysis, sometimes termed problem definition (or deemed a related activity), is one of the most important elements in the design process, and this task is often performed at the same time as a feasibility analysis. The design requirements control the design of the product or process being developed, throughout the engineering design process. These include basic things like the functions, attributes, and specifications - determined after assessing user needs. Some design requirements include hardware and software parameters, maintainability, availability, and testability.\n\nIn some cases, a feasibility study is carried out after which schedules, resource plans and estimates for the next phase are developed. The feasibility study is an evaluation and analysis of the potential of a proposed project to support the process of decision making. It outlines and analyses alternatives or methods of achieving the desired outcome. The feasibility study helps to narrow the scope of the project to identify the best scenario.\nA feasibility report is generated following which Post Feasibility Review is performed.\n\nThe purpose of a feasibility assessment is to determine whether the engineer's project can proceed into the design phase. This is based on two criteria: the project needs to be based on an achievable idea, and it needs to be within cost constraints. It is important to have engineers with experience and good judgment to be involved in this portion of the feasibility study.\n\nA concept study (conceptualization, conceptual design) is often a phase of project planning that includes producing ideas and taking into account the pros and cons of implementing those ideas. This stage of a project is done to minimize the likelihood of error, manage costs, assess risks, and evaluate the potential success of the intended project. In any event, once an engineering issue or problem is defined, potential solutions must be identified. These solutions can be found by using ideation, the mental process by which ideas are generated. In fact, this step is often termed Ideation or \"Concept Generation.\" The following are widely used techniques:\n\n\nVarious generated ideas must then undergo a concept evaluation step, which utilizes various tools to compare and contrast the relative strengths and weakness of possible alternatives.\n\nThe preliminary design, or high-level design includes (also called FEED), often bridges a gap between design conception and detailed design, particularly in cases where the level of conceptualization achieved during ideation is not sufficient for full evaluation. So in this task, the overall system configuration is defined, and schematics, diagrams, and layouts of the project may provide early project configuration. (This notably varies a lot by field, industry, and product.) During detailed design and optimization, the parameters of the part being created will change, but the preliminary design focuses on creating the general framework to build the project on.\n\nS. Blanchard and J. Fabrycky describe it as:\n“The ‘whats’ initiating conceptual design produce ‘hows’ from the conceptual design evaluation effort applied to feasible conceptual design concepts. Next, the ‘hows’ are taken into preliminary design through the means of allocated requirements. There they become ‘whats’ and drive preliminary design to address ‘hows’ at this lower level.”\n\nFollowing FEED is the Detailed Design (Detailed Engineering) phase, which may consist of procurement of materials as well.\nThis phase further elaborates each aspect of the project/product by complete description through solid modeling, drawings as well as specifications.\n\nDesign for manufacturability (DFM) is the general engineering art of designing products in such a way that they are easy to manufacture.\n\n\nComputer-aided design (CAD) programs have made detailed design phase more efficient. For example, a CAD program can provide optimization to reduce volume without hindering a part's quality. It can also calculate stress and displacement using the finite element method to determine stresses throughout the part. \n\nThe production planning and tool design consists of planning how to mass-produce the product and which tools should be used in the manufacturing process. Tasks to complete in this step include selecting materials, selection of the production processes, determination of the sequence of operations, and selection of tools such as jigs, fixtures, metal cutting and metal or plastics forming tools. This task also involves additional prototype testing iterations to ensure the mass-produced version meets qualification testing standards.\n\nEngineering is formulating a problem that can be solved through design. Science is formulating a question that can be solved through investigation. \nThe engineering design process bears some similarity to the scientific method. Both processes begin with existing knowledge, and gradually become more specific in the search for \"knowledge\" (in the case of \"pure\" or basic science) or a \"solution\" (in the case of \"applied\" science, such as engineering). The key difference between the engineering process and the scientific process is that the engineering process focuses on design, creativity and innovation while the scientific process emphasizes Discovery (observation).\n\n\n"}
{"id": "58545176", "url": "https://en.wikipedia.org/wiki?curid=58545176", "title": "Enzootic nasal tumor virus", "text": "Enzootic nasal tumor virus\n\nThe enzootic nasal tumor virus is a carcinogenic retrovirus that causes Enzootic nasal adenocarcinoma in sheep and goats.\n"}
{"id": "16926318", "url": "https://en.wikipedia.org/wiki?curid=16926318", "title": "Equatorial ring", "text": "Equatorial ring\n\nAn equatorial ring was an astronomical instrument used in the Hellenistic world to determine the exact moment of the spring and autumn equinoxes. Equatorial rings were placed before the temples in Alexandria, in Rhodes, and perhaps in other places, for calendar purposes. \n\nThe easiest way to understand the use of an equatorial ring is to imagine a ring placed vertically in the east-west plane at the Earth's equator. At the time of the equinoxes, the Sun will rise precisely in the east, move across the zenith, and set precisely in the west. Throughout the day, the bottom half of the ring will be in the shadow cast by the top half of the ring. On other days of the year, the Sun passes to the north or south of the ring, and will illuminate the bottom half. For latitudes away from the equator, the ring merely needs to be placed at the correct angle in the equatorial plane. At the Earth's poles, the ring would be horizontal.\n\nThe equatorial ring was about one to two cubits (45cm-90cm) in diameter. Because the Sun is not a point source of light, the width of the shadow on the bottom half of the ring is slightly less than the width of the ring. By waiting until the shadow was centered on the ring, the time of the equinox could be fixed to within an hour or so. If the equinox happened at night, or if the sky was cloudy, an interpolation could be made between two days' measurements.\n\nThe main disadvantage with the equatorial ring is that it needed to be aligned very precisely or false measurements could occur. Ptolemy mentions in the \"Almagest\" that one of the equatorial rings in use in Alexandria had shifted slightly, which meant that the instrument showed the equinox occurring twice on the same day. False readings can also be produced by atmospheric refraction of the Sun when it is close to the horizon.\n\nEquatorial rings can also be found on armillary spheres and equatorial sundials.\n\n"}
{"id": "21858352", "url": "https://en.wikipedia.org/wiki?curid=21858352", "title": "Experimetrics", "text": "Experimetrics\n\nExperimetrics refers to the application of econometrics to economics experiments. Experimetrics refers to formal procedures used in designed investigations of economic hypotheses.\n\nOne branch of experimetrics uses experiments to evaluate the performance of econometric estimators \n\nIn short, experimetrics is the field of study that lies at the intersection of experimental economics and econometrics. It refers to a broad swath of the economics literature, and encompasses both the theoretical and statistical basis of econometrics, as well as the methodology of the experimental method.\n\n"}
{"id": "3131424", "url": "https://en.wikipedia.org/wiki?curid=3131424", "title": "Flat adverb", "text": "Flat adverb\n\nIn English grammar, a flat adverb or bare adverb is an adverb that has the same form as a related adjective. It does not end in \"-ly\", e.g. \"drive \"slow\"\", \"drive \"fast\"\". A flat adverb is sometimes also called \"simple adverb\". Flat adverbs were once quite common but have been largely replaced by their \"-ly\" counterparts. In the 18th century, grammarians believed flat adverbs to be adjectives, and insisted that adverbs needed to end in \"-ly\". According to the Merriam-Webster Dictionary, \"It's these grammarians we have to thank for ... the sad lack of flat adverbs today\". There are now only a few flat adverbs, and some are widely thought of as incorrect. Despite bare adverbs being grammatically correct and widely used by respected authors, they are often incorrectly stigmatized. There have been public campaigns against street signs with the traditional text \"go slow\" and the innovative text \"drive friendly\".\n\nFor most bare adverbs, an alternative form exists ending in \"-ly\" (\"slowly\"). \"S\"ometimes the -ly form has a different meaning (\"hardly\", \"nearly\", \"cleanly\", \"rightly\", \"closely\", \"lowly\"), and sometimes the -ly form is not used for certain meanings (\"sit tight\", \"sleep tight\"). \n\nSome bare adverbs don't alternate; e.g. \"fast\", \"soon\", \"straight\", \"tough\", \"far\", \"low.\" In addition, the ending \"-ly\" is also found on some words that are both adverbs and adjectives (e.g. \"friendly\") and some words that are only adjectives (e.g. \"lonely\").\n\nA survey carried out in the 1960s studied peoples attitudes towards usage problems in English. The examples \"you'd better go slow\" (rather than \"slowly\") and \"he did it quicker than he'd ever done it before\" (rather than \"more quickly\") contained flat adverbs – and the latter was found to be acceptable by just 42% of respondents. However, in a follow-up by the Leiden University Centre for Linguistics, using the same examples from the 1960s survey and others containing flat adverbs, they found that acceptance of flat adverbs has become much more widespread in recent years. \"Quicker\" was found to have an acceptance rate of 75%, while \"you'd better go slow\" was universally accepted.\n"}
{"id": "49622455", "url": "https://en.wikipedia.org/wiki?curid=49622455", "title": "Fontarnauite", "text": "Fontarnauite\n\nFontarnauite is a relatively recently described, rare sulfate, borate mineral with the formula (Na,K)(Sr,Ca)(SO)[BO(OH)]·2HO. It is found in an evaporite boron deposit. It coexists with other evaporite boron minerals, especially probertite. It is monoclinic, crystallizing in the space group \"P\"2/\"c\".\n\nIt was named for Ramon Fontarnau i Griera, a materials scientist of the University of Barcelona.\n"}
{"id": "8599355", "url": "https://en.wikipedia.org/wiki?curid=8599355", "title": "Generalized Pareto distribution", "text": "Generalized Pareto distribution\n\nIn statistics, the generalized Pareto distribution (GPD) is a family of continuous probability distributions. It is often used to model the tails of another distribution. It is specified by three parameters: location formula_5, scale formula_6, and shape formula_7. Sometimes it is specified by only scale and shape and sometimes only by its shape parameter. Some references give the shape parameter as formula_8.\n\nThe standard cumulative distribution function (cdf) of the GPD is defined by\n\nwhere the support is formula_10 for formula_11 and formula_12 for formula_13.\n\nThe related location-scale family of distributions is obtained by replacing the argument \"z\" by formula_15 and adjusting the support accordingly: The cumulative distribution function is\n\nfor formula_17 when formula_18, and formula_19 when formula_13, where formula_21, formula_22, and formula_23.\n\nThe probability density function (pdf) is\n\nor equivalently\n\nagain, for formula_17 when formula_27, and formula_19 when formula_13.\n\nThe pdf is a solution of the following differential equation:\n\n\nIf \"U\" is uniformly distributed on\n(0, 1<nowiki>]</nowiki>, then\n\nand\n\nBoth formulas are obtained by inversion of the cdf.\n\nIn Matlab Statistics Toolbox, you can easily use \"gprnd\" command to generate generalized Pareto random numbers.\n\nA GPD random variable can also be expressed as an exponential random variable, with a Gamma distributed rate parameter. \n\nand\nthen\n\nNotice however, that since the parameters for the Gamma distribution must be greater than zero, we obtain the additional restrictions that:formula_7 must be positive.\n\n\n\n"}
{"id": "53743634", "url": "https://en.wikipedia.org/wiki?curid=53743634", "title": "Hapter", "text": "Hapter\n\nHapters are short, peg-like attachments of the lower surface of lichen to the substrate on which they grow. They are distinct from rhizines, which are more hair-like.\n"}
{"id": "41170351", "url": "https://en.wikipedia.org/wiki?curid=41170351", "title": "Heather Dewey-Hagborg", "text": "Heather Dewey-Hagborg\n\nHeather Dewey-Hagborg (born June 4, 1982, Philadelphia, Pennsylvania) is an information artist and bio-hacker. She is most noted for her project \"Stranger Visions\": a series of portraits created from DNA she recovered from discarded items, such as hair, cigarettes and chewing gum while living in Brooklyn, New York. She sequenced the DNA at the Brooklyn open biotechnology laboratory, Genspace. From the extracted DNA, she determined gender, ethnicity and other factors and then used face-generating software and a 3D printer to create a speculative, algorithmically determined 3D portrait. While critical of technology and surveillance, her work has been found disturbing by some critics.\n\nDewey-Hagborg is an information and bio artist whose works explore the intersection between art and science. As a student in the Information Arts program at Bennington College, she participated in computer science classes, which laid the groundwork for the science-based artwork she would later envision using algorithms, electronics, and computer programming. She earned a Bachelor of Arts (B.A.) degree in 2003.\n\nDewey-Hagborg continued refining her work as an artist and computer programmer, studying artificial intelligence, while obtaining a Master of Professional Studies (M.P.S.) in Interactive Telecommunications from New York University (NYU) in 2007. It was here she curated a robotic performance art show called \"Robots on the March!\" in March 2005, and exhibited a piece called \"Lighter than Air: an experiment in constructing an autonomous flying robot\".\n\nAs a final project at NYU, Dewey-Hagborg explored the question \"Can computers be creative?\" in an exhibit she called Spurious Memories. She developed an autonomous face categorizing and generating software program which recognized facial components, made comparisons and adjustments, and produced unique representations of the human face through mass exposure to facial images.\n\nAfter graduating from NYU, she became a doctoral candidate in electronics arts at Rensselaer Polytechnic Institute and is expected to graduate in 2016.\n\nDewey-Hagborg has worked as a teaching assistant at Rensselaer Polytechnic Institute, an adjunct professor at NYU's Interactive Telecommunications Program, and an adjunct professor at NYU's Courant Institute of Mathematical Sciences.\n\nAs of August 2014, Dewey-Hagborg resides in Chicago, Illinois, teaching art and technology studies at the School of the Art Institute of Chicago. As an educator her areas of interest include art and technology, multimedia, digital photography, research-based art and programming, and computer science.\n\nDewey-Hagborg's \"Totem\" (2010) was a site-specific multimedia sculpture characterizing her earlier work. \"Totem\", an idol, was designed to explore the implications of language and artificial intelligence using machine learning technology. Exploiting audio surveillance techniques to eavesdrop on and record conversations at the installation site, Dewey-Hagborg wrote algorithms to then isolate word sequences and grammatical structures into commonly used units. Influenced by Hebbian theory, she programmed the sculpture's computer to generate speech based on the most frequently occurring language structures in any given recording period. Over time, the least frequently elicited words or units would fade or be dropped from the sculpture's spoken vocabulary. The remaining units, stored in the sculpture's memory, were then spoken at random intervals.\n\nThe piece also reflected the writings of psychologist Julian Jaynes. According to Dewey-Hagborg, Jaynes theorized that an early form of human consciousness evolved as a result of hallucinated speech experienced by the Mari civilization of the Euphrates. The ancient people interpreted water trickling through a goddess statue as whispers or vocalizations from the goddess herself. Dewey-Hagborg saw a parallel between the \"mystical and authoritative power of language in the ancient world and the force of power of technology today\" and, with this piece, sought to explore how technology is used to shape public opinion and behavior.\n\nMartha Schwendener, of \"The New York Times\", wrote that \"Totem\" showed promise, but, because of audio difficulties and its fragmented, randomly generated speech, the piece \"failed to connect human speech, meaning, and technology in a profound fashion.\"\n\n\"Stranger Visions\" (2012–2014) is a science-based, artistic exploration using DNA as a starting point for Dewey-Hagborg's lifelike, computer generated 3-D portraits.\n\nShe began this project questioning how much information could be understood about a person using genetic detritus left behind by strangers in New York City. \"I was really struck by this idea that the very things that make us human – hair, skin, saliva, and fingernails – become a real liability for us as we constantly shed them in public. Anyone could come along and mine them for information.\" She hoped, by producing realistic sculptures of anonymous people using clues from their DNA, to spark a debate about the use or the potential misuse of DNA profiling, privacy, and genetic surveillance.\n\nAs part of her research for \"Stranger Visions\", Dewey-Hagborg took a three-week crash-course in biotechnology at Genspace, a non-profit, community-based biotechnology laboratory in New York. She was astonished to learn how much an amateur biologist could learn about someone.\n\nShe began the process of extracting DNA from the samples she collected. The extraction involves treating a hair sample, for example, with a gel that dissolves the hair, and a primer specifically developed to help locate characteristics like eye color or gender along the genome. Dewey-Hagborg might repeat this process up to 40 times, looking for genetic variants influencing traits like eye color, hair color, and racial ancestry, in order to complete a portrait.\n\nOnce the DNA strands are extracted from the samples, Dewey-Hagborg then amplifies, or copies, specific regions of the genome, using a technique called Polymerase Chain Reaction, or PCR, a process advanced by Kary Mullis, a winner of the Nobel Prize in Chemistry (1993). These amplified regions of the genome make it possible to identifying single nucleotide polymorphisms, or SNPs (pronounced \"snips\"), which contain variables in the base pairs that give clues to a person's individual genetic make up (e.g., whether or not a person's eyes might be blue, brown or green). These results are then sent for analysis to a company for sequencing. Dewey-Hagborg used 23andMe, a DNA analysis service, for \"Stranger Visions\". \n\nThe genetic blueprint she receives in return is a text file full of coded information identifying the unique positioning of the 4 nucleobases adenine, thymine, cytosine, and guanine, or ATC and G, that make up the sections of the genome she is interested in. This data is then entered into a customized computer program she wrote. The program interprets the code and provides her with a list of traits, including propensity for obesity, eye color, hair color, hair curl, skin tone, freckles, and gender. She then takes these traits, as many as 50, and enters them into a face-generating program to configure the 3-D portraits. Her previous experience with facial recognition algorithms gave her the ability to repurpose an existing facial recognition program, from Basel, Switzerland. She reworked the program to generate faces instead of just recognizing facial features. The resulting model changes facial dimensions (e.g., width of the nose and mouth) and characteristics with the genetic information it receives. Before making the final 3-D print, Dewey-Hagborg generates several different versions of the face, finally choosing the one she finds most aesthetically pleasing.\n\nCritics of Dewey-Hagborg's \"Stranger Visions\" question whether or not the work crosses ethical and legal boundaries. They make a distinction between an artist's right to express societal concerns through artwork and the act of collecting personal, genetic information without informed consent. The fact that DNA samples are regularly \"left behind\" or abandoned does not mean those people have relinquished their interest in how that information is used.\n\nSome laws, like that of the Human Tissue Act of 2004 in the United Kingdom, prohibit private individuals from collecting biological samples for DNA analysis. What laws that exist to regulate the collection and use of DNA samples in the United States are not consistent among the states and rarely address the private sector. Only some states, like New York, outlaw most DNA testing without written consent. Others worry about the misuse of the information, fearing discrimination based on existing medical or mental health issues or a predisposition for disease-related illnesses or \"unreasonable\" searches of DNA evidence by law enforcement. One scientist and one gallery, according to Dewey-Hagborg, turned down her proposal fearing the project would \"cause a fright\" among people.\n\nOther critics focus on the growing do-it-yourself or biohacking movement. Supporters like Genspace's Ellen Jorgensen claim projects like \"Stranger Visions\" engage the public and make the new technology more accessible. Detractors fear unintended or unexpected consequences from unregulated experiments conducted by D.I.Y. amateur biologists developed in non-traditional laboratory settings.\n\nStill others, including Daniel MacArthur, an assistant professor at Harvard Medical School, John D. Hawks, an anthropologist at the University of Wisconsin-Madison, Michelle N. Meyer, an academic fellow at the Petrie-Flom Center for Health Law Policy, Biotechnology & Bioethics at Harvard Medical School, and Arthur Caplan, PhD, Director of the Division of Medical Ethics, N.Y.U., report that the technological capability to construct an accurate likeness of a human face based on DNA evidence is not currently available.\n\nAlthough it is possible to identify certain genetic markers linked to facial structures, scientists have yet to isolate all the genes and their variations needed to produce an accurate likeness with a computer simulation. Meyer, who analyzed the data from Dewey-Hagborg's website concludes:\n\nSo far as I can tell, she's working with sex; ancestral groups that are usually very broad, and in any event only reflects half of the individual's DNA (from which she presumably guesses hair color and texture and bone structure); and a decent guess at eye color. There are hundreds of thousands (at least) of people who would fit these descriptions even if each of her phenotype predictions were accurate, and in many cases, one or more of the predictions are probably going to be wrong.\n\nThe environment, the probabilistic nature of interpreting the DNA data collected, and limitations of computer technology all influence the outcome. She likens her work to that of a sketch artist. At most, her portraits bear only a vague, family resemblance to the people whose genetic information was used as a foundation for the portraits.\n\nIn the summer of 2017 Dewey-Hagborg's collaborative exhibition with transgender activist Chelsea E. Manning opened at Fridman Gallery. The exhibition was an iteration of the artist's successful and controversial \"Stranger Visions\" project. In the case of \"A Becoming Resemblance\", Dewey-Hagborg created a series titled \"Radical Love\", 30 3-D printed portraits of Manning, based on cheek swabs and hair clippings that Manning sent her while incarcerated for leaking classified information to WikiLeaks. While all 30 portraits were based on Manning's DNA, their variances in skin color and features presents the malleability of DNA data. The installation demonstrated how much the human genome is up for interpretation once condensed and subjectively interpreted. \" \"A Becoming Resemblance\" has since traveled to numerous institutions for exhibition, including The Transmedial in Berlin\n,Fridman Gallery, MU Art Space in the Netherlands, and the 2018 Seattle Art Fair.\n\nIn 2013, Dewey-Hagborg was contacted by an assistant medical examiner in Delaware, as a result of her work with \"Stranger Visions\". The project involved developing a portrait of an unidentified woman whose case has remained unsolved for 20 years. She agreed to be an adviser to assist with the case. Though the resulting portrait based on the unidentified woman's DNA could only be as accurate as existing technology allowed, leaving room for speculation, Dewey-Hagborg viewed working on the case as the only potential use for this type of face-generating technology. \"If you can add anything at all to her description, if you can increase the possibility her loved ones may find her even one little bit I think it's worth it.\" Critics of Dewey-Hagborg's involvement in the Delaware case express concern for what they call \"D.I.Y. forensic science\" and question the role of civilians in state investigations.\n\nDewey-Hagborg's work with \"Stranger Visions\" and interest in issues surrounding genetic surveillance lead to the development of two products whose purpose is to eliminate DNA traces. The first, \"Erase\", is a bleaching spray that cleans surfaces (e.g., cups, silverware) of DNA evidence. The second, \"Replace\", is a spray consisting of a blend of genes designed to introduce foreign DNA evidence to the surface, therefore masking any of the original DNA remaining in that area. Dewey-Hagborg views these as a \"citizens' defense against the looming DNA surveillance state.\"\n\nDewey-Hagborg's work has been exhibited at The Monitor Digital Festival in Guadalajara, Mexico, PS1 MoMA, Long Island City, New York, the New York Public Library in New York City, the Science Gallery at Trinity College Dublin, Ireland, the UTS Gallery in Sydney, Australia, the Jaaga Art and Technology Center in Bangalore, India, the Museum Boijmans in Rotterdam, Netherlands, and the Ars Electronica Center in Linz, Austria.\n\nDewey-Hagborg has also produced the following selected works:\n\n\n"}
{"id": "4367300", "url": "https://en.wikipedia.org/wiki?curid=4367300", "title": "Helen Verran", "text": "Helen Verran\n\nHelen Verran is an Australian historian and empirical philosopher of science, primarily working in the Social Studies of Science and Technology (STS), and currently Adjunct Professor at Charles Darwin University.\n\nVerran is from New South Wales, Australia (born c1945/6?). She trained as a scientist and teacher in the 1960s (BSc, DipEd, University of New England and has a PhD in metabolic biochemistry (UNE, 1972). She then spent eight years lecturing in science education at Obafemi Awolowo University in Ile-Ife, southwestern Nigeria. In the 1980s she became lecturer and later Associate Professor at the University of Melbourne, working in a unit dedicated to the study of history and philosophy of science. She retired in 2012. On retiring she became Adjunct Professor at the Northern Institute, Charles Darwin University in Darwin, NT, Australia where she still teaches.\n\nVerran is best known for her book, \"Science and an African Logic\" (University of Chicago Press, 2001), for which she received the Ludwik Fleck Prize in 2003. It analyses counting, and its relation to the ontology of numbers based on her lengthy field observations as a mathematics lecturer and teacher in Nigeria. The book draws on her sudden realisation of the radically different nature of Yoruba counting and discusses how this realisation grounded her post-relativist theorising.\n\nShe contributed to actor-network theory, working with British sociologist John Law. Specifically, she is credited for contributing with postcolonial studies to nuancing STS.\nHer work is also seen as part of ANT's ontological turn.\n\nHer work on Yolngu Aboriginal Australians understandings of the world, their use of technology, and their knowledge systems ranges from the 1990s to current engagement.\n\nStarting with work on alternative modes of knowing nature management through fire,\nVerran's recent work contributed to social studies of ecosystem services.\n\nShe was awarded US Ludwig Fleck Prize of the US Society for Social Studies of Science in 2003 for \"Science and an African Logic\". (\"UniNews\", Vol. 12, No. 2 24 February - 10 March 2003).\n\n\n \n"}
{"id": "1816593", "url": "https://en.wikipedia.org/wiki?curid=1816593", "title": "Heureka (science center)", "text": "Heureka (science center)\n\nHeureka is a Finnish science center in Vantaa, Finland, north of Helsinki, designed by Heikkinen – Komonen Architects. The aim of the science centre, which opened its doors to the public in 1989, is to popularize scientific information and to develop the methods used to teach science and scientific concepts. The name ‘Heureka’ (eureka in English) refers to the Greek exclamation, presumably uttered by Archimedes, to mean “I’ve found it!” (made a discovery). The Science Centre Heureka features both indoor and outdoor interactive exhibitions with exhibits that enable visitors to independently test different concepts and ideas. There is also a digital planetarium with 135 seats.\n\nThe Heureka Science Centre is a non-profit organization run by the Finnish Science Centre Foundation. The Finnish Science Centre Foundation is a broadly based co-operation organization that includes the Finnish scientific community, education sector, trade and industry, and national and local government. The ten background organisations of the Foundation support, develop and actively participate in the activities of Heureka. The foundation’s highest body is the Board of Trustees, whose decisions are implemented by the Governing Board. Everyday activities are the responsibility of Heureka’s Director assisted by a management team and other staff.\n\nThe roots of the Finnish Science Centre Heureka can be traced back to the University of Helsinki and scientists, who had become acquainted with different science centres located around the world. The initial spark was lit by Adjunct Professors Tapio Markkanen, Hannu I. Miettinen and Heikki Oja. It all began with the Physics 82 exhibition held at the House of the Estates in Helsinki on 20–26 May 1982. During autumn of that same year, the science centre project was launched with the initial support of the Academy of Finland, the Ministry of Education, and various foundations. The project led to the establishment of the Finnish Science Centre Foundation during 1983-1984. The original founding members of the foundation included the University of Helsinki, the Helsinki University of Technology, the Federation of Finnish Learned Societies, and the Confederation of Industries.\n\nIn 1984, the City of Vantaa offered to be the host city and partial financier for the Science Centre, and also designated a property lot located in the southern end of Tikkurila as the future site of the centre. An architectural competition, held in 1985, turned out two first prizes from which the winning design was selected; namely the “Heureka” design submitted by Mikko Heikkinen, Markku Komonen and Lauri Anttila. That's how the Finnish Science Centre Heureka got its apt name!\nBefore the building was completed, a number of test exhibitions were set up at other sites. The interior plan for the Science Centre was completed in 1986. The foundation for the building was laid in October 1987, and the construction work was completed one year later. The overall area of the building is 8,200 m2, of which 2,800 m2 is exhibition space. The Finnish Science Centre Heureka opened its doors to the public on 28 April 1989.\n\nThe main exhibition hall houses about 200 exhibits related to different fields of science. The main exhibition was renewed completely in 1999, but there are small changes taking place in the main exhibition hall each year as well. The topics include, for example, digestion and the functions of the intestines, the production of money and traffic. The exhibition ‘The Wind in the Bowels’ has been designed in co-operation with the Finnish Medical Society Duodecim. The exhibition ‘About a Coin’ was implemented through collaboration with the Mint of Finland to mark the company’s 150th anniversary.\n\nAs an extension of the main exhibition, the Heureka Classics exhibition was opened in 2009 in honour of Heureka’s 20th anniversary. From the beginning of August 2009, Heureka has also had the Science on a Sphere exhibit on display. \n\nIn addition to the main exhibition, Heureka generally also houses two temporary exhibitions. The topics of past temporary exhibitions have included, for example, dinosaurs, humans, sports, forests, the art of film, flying and ancient cultures. Since Heureka’s opening, the most successful exhibitions have been the dinosaur exhibitions. The 2001 exhibition about the family life of dinosaurs, for example, attracted 406,000 visitors. Many of the exhibitions independently produced by Heureka have made guest appearances in numerous science centres all over the world. Heureka also features exhibitions imported from abroad.\n\nHeureka’s outdoor exhibition area, Science Park Galilei, opened in 2002. This area of the centre can be visited annually during the summer season. Galilei is a sort of “scientific playground”. The 7,500 m2 area holds dozens of exhibits, many of which feature water as the primary element. The exhibits are based on mathematical, physical and musical phenomena. The outdoor park also contains moving works of art, such as the sand plotter created by well-known Finnish artist Osmo Valtonen. Galilei also features an arboretum with species of conifers from the northern hemisphere.\n\nThe area in front of Heureka features a permanent bedrock exhibition, which contains both common and rare types of rocks found in Finland’s bedrock. The rocks are situated to reflect their distribution throughout different geographical provinces of Finland. Leading up to the front entrance, visitors are also greeted by perennial gardens that were planted in accordance with the historical classification system designed by Carolus Linnaeus. The front of the entrance is tiled in Penrose tiling.\n\nThe hemispheric-shaped planetarium primarily presents films dealing with astronomy. Until 2007, the theatre was called the Verne Theatre, and it ran super films and multimedia programmes made with special slide projectors that took advantage of the entire 500 m2 surface of the hemispheric screen. At the end of 2007, the theatre was entirely renovated, and reopened on 26 December 2007 as one of Europe’s most modern digital planetariums. There are altogether 135 seats in Heureka’s planetarium.\n\nIn addition to the exhibitions and planetarium films, Heureka also offers the opportunity to view daily science theatre shows, to participate in supervised programmes and to watch Rat Basketball games. Furthermore, a number of other individual events, such as Science Days, science holidays and science camps in the summer are organised at Heureka. Public lectures with different themes are also regularly held in Heureka’s auditorium. Public lectures are given in the planetarium as well. Other services at Heureka include a science shop and a restaurant, as well as conference facilities and a 220-seat auditorium for meetings.\n\nFrom 1989 to 2011, an average of about 280,000 people has visited Heureka each year. The total number of visitors exceeded six million in May 2010. Altogether more than 22 million people have viewed Heureka’s exhibitions on display both in Finland and abroad.\nOf the average 280,000 people who visit Heureka each year, more than half represent families, one fourth school students, about 10% are corporate visits, and the rest are individual visitors. About 6-10% of the visitors arrive from abroad, with the highest percentage coming from Russia and Estonia. The number of visitors is affected by, for example, the general economic situation, the weather and the excursion funds available to school groups.\n\nHeureka is run by the Finnish Science Centre Foundation, whose original members include the University of Helsinki, the Helsinki University of Technology (nowadays Aalto University), the Federation of Finnish Learned Societies, and the Confederation of Industries (nowadays Confederation of Finnish Industries, EK), the City of Vantaa, the Ministry of Education (nowadays Ministry of Education and Culture), the Ministry of Trade and Industry (nowadays Ministry of Employment and the Economy), the Ministry of Finance, the Central Organisation of Finnish Trade Unions (SAK), and the Trade Union of Education in Finland (OAJ).\n\nHeureka’s funding is provided through subsidies from the City of Vantaa and the Ministry of Education and Culture, as well as through its own operational revenue: admission and rental fees, fundraising and exhibition export. Heureka’s overall funding is approximately nine million euro, of which the revenue from own operations was 48% in 2011. The share of the funding provided by the City of Vantaa and the Ministry of Education and Culture was altogether 52% in 2011. The public support is notably less than for many other cultural institutions. Part of the funding also comes through corporate co-operation. The temporary exhibitions are often sponsored by main partners and other partners. Heureka also has two companies owned entirely by the Foundation, namely the Science Shop Magneetti Oy, which runs the Heureka Shop at Heureka and at the Kamppi Shopping Centre in central Helsinki, and Heureka Overseas Productions Oy Ltd, which manages the export activities of Heureka.\n\nAt the end of 2014, the Foundation had a staff of 68 full-time salaried employees and 32 part-time or fixed-term employees. The total number of person-years was 77. Additionally 7 206 hours were carried out by volunteers. There have been volunteers in Heureka since 1998, and there are currently about 60-70 volunteers in the service of Heureka.\n\nThe center is a member of three associations of science centers:\n\n\n"}
{"id": "1756165", "url": "https://en.wikipedia.org/wiki?curid=1756165", "title": "Index Kewensis", "text": "Index Kewensis\n\nThe Index Kewensis (IK), maintained by the Royal Botanic Gardens, Kew, is a publication that aims to register all botanical names for seed plants at the rank of species and genera. It later came to include names of taxonomic families and ranks below that of species.\n\nThe \"Index\" is currently maintained as part of the International Plant Names Index in combination with the Gray Herbarium and Australian Plant Name indexes. This database is anticipated to complete the task of creating a complete list of plant names, although it does not determine which are accepted species names.\n\nThe preparation for this venture was made by Benjamin Daydon Jackson of the Linnaean Society, directed by Joseph Dalton Hooker at Kew. Charles Darwin provided the funding for the indexing project. When he died in 1882 his will stipulated that provision be made for £250 per annum over a 5-year period. \nIn providing citations of plant names, the starting point was taken from 1753 onward; the year of publication for the \"Systema Naturae\" of Linnaeus. \nDarwin had found difficulties in applying these to the plants he studied, and Hooker's directive was to ‘the compilation of an Index to the Names and Authorities of all known flowering plants and their countries’. \nWhile the \"Index\" has never fulfilled this original charter, it was the most comprehensive for over 100 years.\n\nPrevious attempts at a comprehensive index had relied on secondary sources, this was the first attempt to provide the original publication details of the names. \nA note on the country of origin was also included. \nThe publications of De Candolle, Pfeiffer, and Bentham provided models for the acceptance of names. \nHowever, the editor admitted that not all earlier sources were included; this sometimes led to subsequent errors in botanical nomenclature.\n\nThe scope of the project was also changed in early editions, the editor noting that to include a full synonymy was too ambitious. The work originally indicated acceptance of a name, acting as a nomenclator rather than an index, but by 1913 it avoided making taxonomic judgement in its citations. \nThe integrity of the document was liable to criticism as only representing the 'Kew view' on nomenclatural validity, the objective task of indexing gave the work itself greater international acceptance.\n\nA description of Hooker's systematic works by F O Bower notes the \"scheme originated in the difficulty he had found in the accurate naming of plants\", and anticipates the importance of this work,\n\"Surely no greater technical benefit was ever conferred upon a future generation by the veterans of science than this Index. It smooths the way for every systematist who comes after. It stands as a monument to an intimate friendship. It bears witness to the munificence of Darwin, and the ungrudging personal care of Hooker.\"\n\nA hard copy was reprinted in 1996, providing access to the original publication details of plant names; these were also made available in microfiche format as the Cumulated Index Kewensis. The publication titled Kew Index was issued from 1986 until 1989.\n\nThe first index contained the scientific names of 400,000 species, regular supplements were then issued on newly published names. The supplements were issued every five years, each one adding around 6000 names to the index, eventually forming a compilation of over 1,000,000 entries. \nThe sixteenth supplement began to include bibliographic details at the rank of family and below, the later annual supplements included ferns and their allies. A digitalized version of the index was issued on a Compact Disc. A digital version was incorporporated with other indexes as the \"International Plant Names Index\" (IPNI), and may fulfill the original intention - a complete index of plant names. Entries at IPNI are designated with the abbreviation \"(IK)\".\n\n"}
{"id": "42997120", "url": "https://en.wikipedia.org/wiki?curid=42997120", "title": "Isentropic nozzle flow", "text": "Isentropic nozzle flow\n\nIsentropic nozzle flow describes the movement of a gas or fluid through a narrowing opening without an increase or decrease in entropy.\n\nWhenever a gas is forced through a tube, the gaseous molecules are deflected by the tube’s walls. If the speed of the gas is much less than the speed of sound, the gas density will remain constant and the velocity of the flow will increase. However, as the speed of the flow approximates the speed of sound, compressibility effects on the gas are to be considered. The density of the gas becomes position dependent. While considering flow through a tube, if the flow is very gradually compressed (i.e. area decreases) and then gradually expanded (i.e. area increases), the flow conditions are restored (i.e. return to its initial position). So, such a process is a reversible process. According to the Second Law of Thermodynamics, whenever there is a reversible and adiabatic flow, constant value of entropy is maintained. Engineers classify this type of flow as an \"isentropic\" flow of fluids. Isentropic is the combination of the Greek word \"iso\" (which means - same) and entropy.\nWhen the change in flow variables is small and gradual, isentropic flows occur. The generation of sound waves is an isentropic process. A supersonic flow that is turned while there is an increase in flow area is also isentropic. Since there is an increase in area, therefore we call this an \"isentropic expansion\". If a supersonic flow is turned abruptly and the flow area decreases, the flow is irreversible due to the generation of shock waves. The isentropic relations are no longer valid and the flow is governed by the oblique or normal shock relations.\n\nIn fluid dynamics, a stagnation point is a point in a flow field where the local velocity of the fluid is zero. The isentropic stagnation state is the state a flowing fluid would attain if it underwent a reversible adiabatic deceleration to zero velocity. There are both \"actual\" and the \"isentropic\" stagnation states for a typical gas or vapor. Sometimes it is advantageous to make a distinction between the actual and the isentropic stagnation states. The actual stagnation state is the state achieved after an actual deceleration to zero velocity (as at the nose of a body placed in a fluid stream), and there may be irreversibility associated with the deceleration process. Therefore, the term \"stagnation property\" is sometimes reserved for the properties associated with the actual state, and the term total property is used for the isentropic stagnation state. The enthalpy is the same for both the actual and isentropic stagnation states (assuming that the actual process is adiabatic). Therefore, for an ideal gas, the actual stagnation temperature is the same as the isentropic stagnation temperature. However, the actual stagnation pressure may be less than the isentropic stagnation pressure. For this reason the term \"total pressure\" (meaning isentropic stagnation pressure) has particular meaning compared to the actual stagnation pressure.\n\nThe isentropic efficiency is h1-h2a/h1-h2.\nThe variation of fluid density for compressible flows requires attention to density and other fluid property relationships. The fluid equation of state, often unimportant for incompressible flows, is vital in the analysis of compressible flows. Also, temperature variations for compressible flows are usually significant and thus the energy equation is important. Curious phenomena can occur with compressible flows.\n\nThere are numerous applications where a steady, uniform, isentropic flow is a good approximation to the flow in conduits. These include the flow through a jet engine, through the nozzle of a rocket, from a broken gas line, and past the blades of a turbine.\n\nEnergy equation for the steady flow:\n\nTo model such situations, consider the control volume in the changing area of the conduit of Fig. The continuity equation between two sections an infinitesimal distance dx apart is\nIf only the first-order terms in a differential quantity are retained, continuity takes the form \nThe energy equation is:\n\nThis simplifies to, neglecting higher-order terms,:\nAssuming an isentropic flow, the energy equation becomes:\nSubstitute from the continuity equation to obtain\nor, in terms of the Mach number:\n\nThis equation applies to a steady, uniform, isentropic flow.\nThere are several observations that can be made from an analysis of Eq. (9.26).\nThey are:\n\nA nozzle for a supersonic flow must increase in area in the flow direction, and a diffuser must decrease in area, opposite to a nozzle and diffuser for a subsonic flow. So, for a supersonic flow to develop from a reservoir where the velocity is zero, the subsonic flow must first accelerate through a converging area to a throat, followed by continued acceleration through an enlarging area.\n\nThe nozzles on a rocket designed to place satellites in orbit are constructed using such converging-diverging geometry. The energy and continuity equations can take on particularly helpful forms for the steady, uniform, isentropic flow through the nozzle. Apply the energy equation with Q_ W_S 0 between the reservoir and some location in the nozzle to obtain\n\nAny quantity with a zero subscript refers to a stagnation point where the velocity is zero, such as in the reservoir. Using several thermodynamic relations equations can be put in the forms:\n\nIf the above equations are applied at the throat (the critical area signified by an\nAsterisk (*) superscript, where M =1), the energy equation takes the forms\n\nThe critical area is often referenced even though a throat does not exist. For air with k =1.4, the equations above provide\n\nThe mass flux through the nozzle is of interest and is given by:\n\nWith the use of Eq. (9.28), the mass flux, after applying some algebra, can be\nExpressed as\nIf the critical area is selected where M =1, this takes the form\n\nwhich, when combined with previous it provides:\n\nConsider a converging nozzle connecting a reservoir with a receiver. If the reservoir pressure is held constant and the receiver pressure reduced, the Mach number at the exit of the nozzle will increase until Me=1 is reached, indicated by the left curve in figure 2. After Me =1 is reached at the nozzle exit for formula_21, the condition of choked flow occurs and the velocity throughout the nozzle cannot change with further decreases informula_22. This is due to the fact that pressure changes downstream of the exit cannot travel upstream to cause changes in the flow conditions.\nThe right curve of Fig2. represents the case when the reservoir pressure is increased and the receiver pressure is held constant. When formula_23, the condition of choked flow also occurs; but Eq indicates that the mass flux will continue to increase as formula_24 is increased. This is the case when a gas line ruptures.\n\nIt is interesting that the exit pressure formula_25 is able to be greater than the receiver pressure formula_26. Nature allows this by providing the streamlines of a gas the ability to make a sudden change of direction at the exit and expand to a much greater area resulting in a reduction of the pressure from formula_25 to formula_26. The case of a converging-diverging nozzle allows a supersonic flow to occur, providing the receiver pressure is sufficiently low. This is shown in Fig. 9.6 assuming a constant reservoir pressure with a decreasing receiver pressure. If the receiver pressure is equal to the reservoir pressure, no flow occurs, represented by curve formula_29. If pr is slightly less than p_0, the flow is subsonic throughout, with a minimum pressure at the throat, represented by curve B. As the pressure is reduced still further, a pressure is reached that result in M =1 at the throat with subsonic flow throughout the remainder of the nozzle.\nThere is another receiver pressure substantially below that of curve C that also results in isentropic flow throughout the nozzle, represented by curve D; after the throat the flow is supersonic. Pressures in the receiver in between those of curve C and curve D result in non-isentropic flow (a shock wave occurs in the flow) and will be considered in the next section. If pr is below that of curve D, the exit pressure pe is greater than pr. Once again, for receiver pressures below that of curve C, the mass flux remains constant since the conditions at the throat remain unchanged. It may appear that the supersonic flow will tend to separate from the nozzle, but just the opposite is true. A supersonic flow can turn very sharp angles, since nature provides expansion fans that do not exist in subsonic flows. To avoid separation in subsonic nozzles, the expansion angle should not exceed 10°. For larger angles, vanes are used so that the angle between the vanes does not exceed 10°.\n\n\n"}
{"id": "510791", "url": "https://en.wikipedia.org/wiki?curid=510791", "title": "List of cognitive biases", "text": "List of cognitive biases\n\nCognitive biases are systematic patterns of deviation from norm or rationality in judgment, and are often studied in psychology and behavioral economics.\n\nAlthough the reality of these biases is confirmed by replicable research, there are often controversies about how to classify these biases or how to explain them. Some are effects of information-processing rules (i.e., mental shortcuts), called \"heuristics\", that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (\"cold\") bias, such as mental noise, or motivational (\"hot\") bias, such as when beliefs are distorted by wishful thinking. Both effects can be present at the same time.\n\nThere are also controversies over some of these biases as to whether they count as useless or irrational, or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill: a way to establish a connection with the other person.\n\nAlthough this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, hyperbolic discounting has been observed in rats, pigeons, and monkeys.\n\nMany of these biases affect belief formation, business and economic decisions, and human behavior in general.\n\nMost of these biases are labeled as attributional biases.\n\nIn psychology \"and\" cognitive science, a memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory. There are many types of memory bias, including:\n\n\nA 2012 \"Psychological Bulletin\" article suggested that at least eight seemingly unrelated biases can be produced by the same information-theoretic generative mechanism that assumes noisy information processing during storage and retrieval of information in human memory.\n\nPeople do appear to have stable individual differences in their susceptibility to decision biases such as overconfidence, temporal discounting, and bias blind spot. That said, these stable levels of bias within individuals are possible to change. Participants in experiments who watched training videos and played debiasing games showed medium to large reductions both immediately and up to three months later in the extent to which they exhibited susceptibility to six cognitive biases: anchoring, bias blind spot, confirmation bias, fundamental attribution error, projection bias, and representativeness.\n\nDebiasing is the reduction of biases in judgment and decision making through incentives, nudges, and training. Cognitive bias mitigation and cognitive bias modification are forms of debiasing specifically applicable to cognitive biases and their effects. Reference class forecasting is a method for systematically debiasing estimates and decisions, based on what Daniel Kahneman has dubbed the outside view.\n\n\n"}
{"id": "188479", "url": "https://en.wikipedia.org/wiki?curid=188479", "title": "List of file formats", "text": "List of file formats\n\nThis is a list of file formats used by computers, organized by type. Filename extensions are usually noted in parentheses if they differ from the file format name or abbreviation. Many operating systems do not limit filenames to one extension shorter than 4 characters, as was common with some operating systems that supported the File Allocation Table (FAT) file system. Examples of operating systems that do not impose this limit include Unix-like systems, and Microsoft Windows NT, 95, 98, and Me which have no three character limit on extensions for 32-bit or 64-bit applications on file systems other than pre-Windows 95 and Windows NT 3.5 versions of the FAT file system. Some filenames are given extensions longer than three characters.\n\nSome file formats may be listed twice or more. An example is the .txt .\n\n\n\n\n(MPEG-1 is found in a .DAT file on a video CD.)\n\n\nFile formats for software, databases, and websites used by potters and ceramic artists to manage glaze recipes, glaze chemistry, etc.\n\n\nComputer-aided is a prefix for several categories of tools (e.g., design, manufacture, engineering) which assist professionals in their respective fields (e.g., machining, architecture, schematics).\n\nComputer-aided design (CAD) software assists engineers, architects and other design professionals in project design.\n\nElectronic design automation (EDA), or electronic computer-aided design (ECAD), is specific to the field of electrical engineering.\n\nFiles output from Automatic Test Equipment or post-processed from such.\n\n\n\nThese files store formatted text and plain text.\n\n\n\n\n\n\n\n\n\nRaster or bitmap files store images as a group of pixels.\n\nVector graphics use geometric primitives such as points, lines, curves, and polygons to represent images.\n\n3D graphics are 3D models that allow building models in real-time or non real-time 3D rendering.\n\n\n\n\n\n\n\n\nFormats of files used for bibliographic information (citation) management.\n\n\n\n\n\n\n\n\n\n\n\nAuthentication and general encryption formats are listed here.\n\n\n\n\n____________________________________________________________________________________\n\nSource code for computer programs\n\n\"(see also: Script)\"\n\n\n\n\n\nList of common file formats of data for video games on systems that support filesystems, most commonly PC games.\n\nList of the most common filename extensions used when a game's ROM image or storage medium is copied from an original read-only memory (ROM) device to an external memory such as hard disk for back up purposes or for making the game playable with an emulator. In the case of cartridge-based software, if the platform specific extension is not used then filename extensions \".rom\" or \".bin\" are usually used to clarify that the file contains a copy of a content of a ROM. ROM, disk or tape images usually do not consist of one file or ROM, rather an entire file or ROM structure contained within one file on the backup medium.\n\n\n\n\n\n\n\n\n\n\nThese file formats are fairly well defined by long-term use or a general standard, but the content of each file is often highly specific to particular software or has been extended by further standards for specific uses.\n\n\nThese are filename extensions and broad types reused frequently with differing formats or no specific format by different programs.\n\n\n\n\n\n\n"}
{"id": "1613357", "url": "https://en.wikipedia.org/wiki?curid=1613357", "title": "List of inorganic compounds", "text": "List of inorganic compounds\n\nAlthough most compounds are referred to by their IUPAC systematic names (following IUPAC nomenclature), \"traditional\" names have also been kept where they are in wide use or of significant historical interests.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "32734960", "url": "https://en.wikipedia.org/wiki?curid=32734960", "title": "List of largest giant sequoias", "text": "List of largest giant sequoias\n\nThe giant sequoia (\"Sequoiadendron giganteum\") is the world's most massive tree, and arguably the largest living organism on Earth. It is neither the tallest extant species of tree (that distinction belongs to the coast redwood), nor is it the widest (that distinction belongs to the baobab tree or Montezuma Cypresses), nor is it the longest-lived (that distinction belongs to the Great Basin bristlecone pine). However, with a height of or more, a circumference of or more, an estimated bole volume of up to , and an estimated life span of 1800–2700 years, the giant sequoia is among the tallest, widest and longest-lived of all organisms on Earth.\n\nGiant sequoias grow in well-defined groves in California mixed evergreen forests, along with other old-growth species such as California incense-cedar (\"Calocedrus decurrens\"). Because most of the neighboring trees are also quite large, it can be difficult to appreciate the size of an individual giant sequoia. The largest giant sequoias are as tall as a 26-story building, and the width of their bases can exceed that of a city street. They grow at such a rate as to produce roughly of wood each year, approximately equal to the volume of a 50-foot-tall tree one foot in diameter. This makes them among the fastest growing organisms on Earth, in terms of annual increase in mass.\n\nGiant sequoias occur naturally in only one place on Earth—the western slope of the Sierra Nevada mountain range in California, on moist, unglaciated ridges and valleys at an altitude of above mean sea level. There are 65–75 groves of giant sequoias in the Sierra Nevada, depending upon the criteria used to define a grove. The northernmost of these groves is Placer County Grove in the Tahoe National Forest, Placer County, California, while the southernmost grove is Deer Creek Grove in the Giant Sequoia National Monument, Tulare County, California. The combined total area of all groves of giant sequoias is approximately .\n\nGiant sequoias are in many ways adapted to forest fires. Their bark is unusually fire resistant, and their cones will normally open immediately after a fire. However, fire is also the most serious damaging agent of giant sequoias. Seedlings and saplings are highly susceptible to death or serious injury by fire. Larger giant sequoias are more resistant to fire damage, due to their thick protective layer of nonresinous bark and elevated crowns. However, repeated fires over many centuries may penetrate the bark and destroy the vascular cambium. Nearly all of the larger trees have fire scars, many of which cover a large area of the base of the tree. Older trees are rarely killed by fire alone, but the resulting structural damage may predispose a tree to collapse and fire scars also provide entry for fungi which may cause root disease and heart rot. The resulting decayed wood is then more easily consumed by subsequent fires. The result of this cycle is further structural weakening of the tree, which may eventually lead to its collapse.\n\nFire scars are thought to be the main cause of dead tops. Although lightning strikes rarely kill mature trees, lightning sometimes knocks out large portions of crowns or ignites dead tops. The most common cause of death in mature giant sequoias is toppling, due to weakening of the roots and lower trunk by fire and decay. The extreme weight of the trees coupled with their shallow root systems contributes to this weakening. Other causative factors include wind, water-softened soils, undercutting by streams, and heavy snow loads in the crowns\nThe Washington tree, located in the Giant Forest Grove in Sequoia National Park provides a good example of the aforementioned phenomenon. This tree was the second largest tree in the world (only the General Sherman tree was larger) until just a few years ago. In September 2003, the tree lost a portion of its crown as a result of a fire caused by a lightning strike. This reduced its height from nearly to about . The structurally weakened tree partially collapsed in January 2005, as the result of a heavy snow load in the remaining portion of its crown; it is now approximately tall.\n\nAs with other trees, measurement of giant sequoias is conducted using established dendrometric techniques. The most frequent measurements acquired in the field include the height of the tree, the horizontal dimension of its canopy, and its diameter at breast height (DBH). These measurements are then subjected to tree allometry, which employs certain mathematical and statistical principles to estimate the amount of timber volume in a tree.\n\nCalculating the volume of a standing tree is the practical equivalent of calculating the volume of an irregular cone, and is subject to error for various reasons. This is partly due to technical difficulties in measurement, and variations in the shape of trees and their trunks. Measurements of trunk circumference are taken at only a few predetermined heights up the trunk, and assume that the trunk is circular in cross-section, and that taper between measurement points is even. Also, only the volume of the trunk (including the restored volume of basal fire scars) is taken into account, and not the volume of wood in the branches or roots. The volume measurements also do not take cavities into account. For example, while studying sequoia tree canopies in 1999, researchers discovered that the Washington tree in Giant Forest Grove was largely hollow.\n\nThe following table is a list of the largest giant sequoias, all of which are located in California. The table is sorted by trunk volume, ignoring wood in the branches of the tree. Many sequoias cut down in the past were probably far larger, such as the Mother of the Forest.\n\n\n\n"}
{"id": "2877750", "url": "https://en.wikipedia.org/wiki?curid=2877750", "title": "List of palaces", "text": "List of palaces\n\n\n\n\n\nIn Azerbaijani Turkic language have different means of the word \"house\" and \"palace\". Usually, church-houses were custom during 2nd century BC – 7th century AD. \"Mulk\" is a foreign word which came from Arabia during Caliphate Era. The word \"Saray\" is a castle, or government building which was considered to have particular administrative importance in various parts of the former Safavid Empire. Imarat or Igamatgah are big house which belong to rich people, khans, shahs. Same type buildings were popular in Midia, Afshar Empire, Karabakh Khanate, Baku Khanate, Shaddadids etc. Now, the term \"Villa\" is very popular and modern in Azerbaijan since the 1990s for a capitalist system.\n\nOld Era (BC 100-799):\n\nShirvanshahs Era (799-1539):\n\nKhanates of the Caucasus:\n\n19th-21st centuries:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidences of provincial Lieutenant-Governors:\n\n\nThe English word \"palace\" is used to translated the Chinese word 宮 (pronounced \"gōng\" in Mandarin). This character represents two rooms connected (呂), under a roof (宀). Originally the character applied to any residence or mansion, but starting with the Qin Dynasty (3rd century BC) it was used only for the residence of the emperor and members of the imperial family. Chinese palaces are different from post-Renaissance European palaces in the sense that they are not made up of one building only (however big and convoluted the building may be), but are in fact huge spaces surrounded by a wall and containing large separated halls (殿 diàn) for ceremonies and official business, as well as smaller buildings, galleries, courtyards, gardens, and outbuildings, more like the Roman or Carolingian \"palatium.\"\n\n\nApart from the main imperial palace, Chinese dynasties also had several other imperial palaces in the capital city where the empress, crown prince, or other members of the imperial family dwelled. There also existed palaces outside of the capital city called \"away palaces\" (離宮) where the emperors resided when traveling. The habit also developed of building garden estates in the countryside surrounding the capital city, where the emperors retired at times to get away from the rigid etiquette of the imperial palace, or simply to escape from the summer heat inside their capital. This practice reached a zenith with the Qing Dynasty, whose emperors built the fabulous Imperial Gardens (御園), now known in China as the Gardens of Perfect Brightness (圓明園), and better known in English as the Old Summer Palace. The emperors of the Qing Dynasty resided and worked in the Imperial Gardens, 8 km/5 miles outside of the walls of Beijing, the Forbidden City inside Beijing being used only for formal ceremonies.\n\nThese gardens were made up of three gardens: the Garden of Perfect Brightness proper, the Garden of Eternal Spring (長春園), and the Elegant Spring Garden (綺春園); they covered a huge area of 3.5 km² (865 acres), almost 5 times the size of the Forbidden City, and 8 times the size of the Vatican City. comprising hundreds of halls, pavilions, temples, galleries, gardens, lakes, etc. Several famous landscapes of southern China had been reproduced in the Imperial Gardens, hundreds of invaluable Chinese art masterpieces and antiquities were stored in the halls, making the Imperial Gardens one of the largest museum in the world. Some unique copies of literary work and compilations were also stored inside the Imperial Gardens. In 1860, during the Second Opium War, the British and French expeditionary forces looted the Old Summer Palace. Then on October 18, 1860, in order to \"punish\" the imperial court, which had refused to allow Western embassies inside Beijing, the British general Lord Elgin – with protestations from the French – purposely ordered to set fire to the huge complex which burned to the ground. It took 3500 British troops to set the entire place ablaze and took three whole days to burn. The burning of the Gardens of Perfect Brightness is still a very sensitive issue in China today.\n\nFollowing this cultural catastrophe, the imperial court was forced to relocate to the old and austere Forbidden City where it stayed until 1924, when the Last Emperor was expelled by a republican army. Empress dowager Cixi (慈禧太后) built the Summer Palace (頤和園 – \"The Garden of Nurtured Harmony\") near the Old Summer Palace, but on a much smaller scale than the Old Summer Palace. There are currently some projects in China to rebuild the Imperial Gardens, but this appears as a colossal undertaking, and no rebuilding has started yet.\n\nSome other palaces include:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGerman has two contrasting words for what may be considered a palace: \"Schloss\" which connotes a seat that is \"enclosed\" by walls, a fastness or keep, and \"Palast\" (or mostly \"Palais\"), a more conscious borrowing, with the usual connotations of splendour. In practice, the \"Schloss\" is more likely to be a royal or ducal palace or a noble manor house. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaghadan Palace, Amman. Royal Residence of the Hussein Family\n\n\n\n\n\n\n\n\n\nKUALA LUMPUR\nIstana Hinggap can be divided into two types. First, they are the city-palaces located in Kuala Lumpur. They function as the royal residence when the Sultan, Raja or Yang Dipertuan Besar come to Kuala Lumpur. There are nine Istana Hinggap built respectively for the nine Kings of Malaysia. Second, they are the temporary/leisure palace when each Sultan, Raja or Yang Dipertuan Besar goes to visit their territory inside/outside their own state. Some of them even have Istana Hinggap outside the country.\n\nISTANA HINGGAP IN KUALA LUMPUR\n\nPERLIS\n\nKEDAH\n\n\n\nPULAU PINANG\n\nPERAK\n\nSELANGOR\n\nNEGERI SEMBILAN\n\nMELAKA\n\nJOHOR\n\nPAHANG\nTERENGGANU\n\nKELANTAN\n\nSABAH\n\nSARAWAK\n\n\n\n\n\n\n\n\n\nApart from the large complex at Turangawaewae Marae located in the town of Ngaruawahia, the previous Māori Monarch Te Atairangikaahu had a home at Waahi Marae in Huntly where she lived for most of her 40-year reign with her consort Whatumoana Paki. The Māori King or Queen are required to attend 33 Poukai annually conducted at Marae loyal to the Kingitangi movement. Many of these Marae maintain residences for the Māori King or Queen for them to use during such visits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe province of Scania in southernmost Sweden is well known for its many castles.\n\n\n\n\n\n\n\n\nIn Turkish, a palace is a Saray.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome large impressive buildings which were not meant to be residences, but are nonetheless called palaces, include:\n\nNote, too, the French use of the word \"palais\" in such constructions as \"palais des congrès\" (convention centre) and \"palais de justice\" (courthouse).\n\n"}
{"id": "11664498", "url": "https://en.wikipedia.org/wiki?curid=11664498", "title": "List of sequenced bacterial genomes", "text": "List of sequenced bacterial genomes\n\nThis list of sequenced eubacterial genomes contains all the eubacteria known to have publicly available complete genome sequences. Most of these sequences have been placed in the International Nucleotide Sequence Database Collaboration, a public database which can be searched on the web. A few of the listed genomes may not be in the INSDC database, but in other public databases.\n\nGenomes listed as \"Unpublished\" are in a database, but not in the peer-reviewed scientific literature.\n\nFor the genomes of archaea see list of sequenced archaeal genomes.\n\n\n"}
{"id": "33362511", "url": "https://en.wikipedia.org/wiki?curid=33362511", "title": "List of sequenced plant genomes", "text": "List of sequenced plant genomes\n\nThis list of sequenced plant genomes contains plant species known to have publicly available complete genome sequences that have been assembled, annotated and published. Unassembled genomes are not included, nor are organelle only sequences. For all kingdoms, see the list of sequenced genomes.\n\nUnicellular photosynthetic eukaryotes. For a more complete list, see the List of Sequenced Algae genomes\n\nthe genome from Galdieria sulphuraria has finally been published Genome size is 13.7 MB, and 6623 protein-coding genes were annotated.\nNakamura et al. published the genome sequence for Pyropia yezoensis.\nBhattacharya et al. published the genome of Porphyridium purpureum.\n\nNot meeting criteria of the first paragraph of this article in being nearly full sequences with high quality, published, assembled and publicly available. This list includes species where sequences are announced in press releases or websites, but not in a data-rich publication in a refereed Journal with doi.\n\n\n"}
{"id": "11418170", "url": "https://en.wikipedia.org/wiki?curid=11418170", "title": "List of worker cooperatives", "text": "List of worker cooperatives\n\nThis is a list of worker cooperatives by country.\n\n\n\n\n\n\n\n\n\n"}
{"id": "38033909", "url": "https://en.wikipedia.org/wiki?curid=38033909", "title": "Lénárt sphere", "text": "Lénárt sphere\n\nA Lénárt sphere is a teaching and educational research model for spherical geometry. The Lénárt sphere is a modern replacement of a \"spherical blackboard\".\nIt can be used for visualizing spherical polygons (especially triangles) showing the relationships between the sides and the angles.\n\nThe Lenart Sphere Basic Set includes:\n\nOther products for studying spherical geometry include visualization software programs such as The Geometer's Sketchpad, GeoGebra and Spherical Easel (see External links for Spherical Easel information, and visit List of interactive geometry software for non-Euclid, and many other interactive Projective geometry apps and programs).\nWhere these products only work on a flat plane, the Lénárt sphere is giving hands-on experience of spherical geometry.\n\nThe Lénárt sphere was invented by István Lénárt in Hungary in the early 1990s and its use is described in his 2003 book comparing planar and spherical geometry.\n\nSpherical trigonometry used to be an important mathematics topic from antiquity through the end of World War II, and has been replaced in modern education and (in navigation) with more algorithmic methods as well as GPS, including the Haversine formula, linear algebraic matrix multiplication, and Napier's pentagon. The Lénárt sphere is still widely used throughout Europe in non-Euclidean geometry as well as GIS courses.\n\nThe Lénárt sphere is useful in modeling and demonstrating spherical tesselation techniques, especially as they apply to finite analysis problems. Using 3D graphics programs or Python code (see external reference link 8 for open source Python code examples vs. NURBS), greater and greater numbers of polygons can be projected to and from the sphere both for analysis of finite elements, and synthesis of objects and features on the sphere, such as the asteroid ridden planet in the example. In this case, the Lénárt sphere is useful for tesselation (tiling) as a simplification or approximation shortcut to the extremely complex differential math of finite analysis and construction (technically: modeling), especially of animated objects.\n\n"}
{"id": "47359227", "url": "https://en.wikipedia.org/wiki?curid=47359227", "title": "M59-UCD3", "text": "M59-UCD3\n\nM59-UCD3 is an ultra-compact dwarf galaxy located near the Messier 59 galaxy. , it is the second-densest galaxy currently observed, second to M85-HCC1.\n\n"}
{"id": "30797587", "url": "https://en.wikipedia.org/wiki?curid=30797587", "title": "Morphs collaboration", "text": "Morphs collaboration\n\nThe Morphs collaboration was a coordinated study to determine the morphologies of galaxies in distant clusters and to investigate the evolution of galaxies as a function of environment and epoch. Eleven clusters were examined and a detailed ground-based and space-based study was carried out. \n\nThe project was begun in 1997 based upon the earlier observations by two groups using data from images derived from the pre-refurbished Hubble Space Telescope. It was a collaboration of Alan Dressler and Augustus Oemler, Jr., at Observatory of the Carnegie Institute of Washington, Warrick J. Couch at the University of New South Wales, Richard Ellis at Caltech, Bianca M. Poggianti at the University of Padua, Amy Barger at the University of Hawaii's Institute for Astronomy, Harvey Butcher at ASTRON, and Ray M. Sharples and Ian Smail at Durham University. Results were published through 2000.\n\nThe collaboration sought answers to the differences in the origins of the various galaxy types — elliptical, lenticular, and spiral. The studies found that elliptical galaxies were the oldest and formed from the violent merger of other galaxies about two to three billion years after the Big Bang. Star formation in elliptical galaxies ceased about that time. On the other hand, new stars are still forming in the spiral arms of spiral galaxies. Lenticular galaxies (SO) are intermediate between the first two. They contain structures similar to spiral arms, but devoid of the gas and new stars of the spiral galaxies. Lenticular galaxies are the prevalent form in rich galaxy clusters, which suggests that spirals may be transformed into lenticular galaxies as time progresses. The exact process may be related to high galactic density, or to the total mass in a rich cluster’s central core. The Morphs collaboration found that one of the principal mechanisms of this transformation involves the interaction among spiral galaxies, as they fall toward the core of the cluster.\n\nThe Inamori Magellan Areal Camera and Spectrograph (IMACS) Cluster Building Survey is the follow-on project to the Morphs collaboration.\n\n"}
{"id": "53843976", "url": "https://en.wikipedia.org/wiki?curid=53843976", "title": "Mukunda Prasad Das", "text": "Mukunda Prasad Das\n\nMukunda Prasad Das from the Australian National University, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Forum on International Physics in 2003, for \"notable theoretical investigations in condensed matter physics, namely: mesoscopic transport and noise, high temperature superconductivity and density functional theory; and for significant leadership in promoting international meetings and collaborations.\"\n"}
{"id": "10747528", "url": "https://en.wikipedia.org/wiki?curid=10747528", "title": "Olof Immanuel von Fåhraeus", "text": "Olof Immanuel von Fåhraeus\n\nOlof Immanuel Fåhræus (23 March 1796 in Slite – 28 May 1884 in Gotland or Stockholm), was a Swedish politician and entomologist mainly interested in Coleoptera. \n\nHis collection is shared between the natural history museums of Stockholm and Gothenburg. He wrote, with Carl Henrik Boheman \"Insecta Caffrariae annis 1838-1845 a J.A. Wahlberg collecta. Coleoptera\". Holmiae : Fritze & Norstedt Vol. 1 pp. 299–625 published in 1851. \n\nHe was elected a member of the Royal Swedish Academy of Sciences in 1840.\n"}
{"id": "1051609", "url": "https://en.wikipedia.org/wiki?curid=1051609", "title": "Pott's fracture", "text": "Pott's fracture\n\nPott's fracture, also known as Pott's syndrome I and Dupuytren fracture, is an archaic term loosely applied to a variety of bimalleolar ankle fractures. The injury is caused by a combined abduction external rotation from an eversion force. This action strains the sturdy medial (deltoid) ligament of the ankle, often tearing off the medial malleolus due to its strong attachment. The talus then moves laterally, shearing off the lateral malleolus or, more commonly, breaking the fibula superior to the tibiofibular syndesmosis. If the tibia is carried anteriorly, the posterior margin of the distal end of the tibia is also sheared off by the talus. A fractured fibula in addition to detaching the medial malleolus will tear the tibiofibular syndesmosis. The combined fracture of the medial malleolus, lateral malleolus, and the posterior margin of the distal end of the tibia is known as a \"trimalleolar fracture\".\n\nAn example of Pott's fracture would be in a sports tackling injury. The player receives a blow to the outside of the ankle, causing the ankle to roll inwards (so that the sole of the foot faces laterally). This damages the ligaments on the inside of the ankle and fractures the fibula at the point of contact (usually just above the tibiofibular syndesmosis). A better way to visualize this is the two hands of a clock, with one hand facing 12 and the other facing 6. The vertical line they form represents the fibula of the person's right leg. The lateral force approaches from 3 o'clock, sending the lower hand snapping outwards to point at 5 o'clock.\n\nBimalleolar fractures are less likely to result in arthritis than trimalleolar fractures.\n\nEnglish physician Percivall Pott experienced this injury in 1765 and described his clinical findings in a paper published in 1769.\n\nThe term \"Dupuytren fracture\" refers to the same mechanism, and it is named for Guillaume Dupuytren. Pott did not describe disruption of the tibio-fibular ligament, whereas Dupuytren did.\n\n"}
{"id": "243349", "url": "https://en.wikipedia.org/wiki?curid=243349", "title": "Rayleigh–Jeans law", "text": "Rayleigh–Jeans law\n\nIn physics, the Rayleigh–Jeans Law is an approximation to the spectral radiance of electromagnetic radiation as a function of wavelength from a black body at a given temperature through classical arguments. For wavelength formula_1, it is:\n\nwhere formula_3 is the spectral radiance; the power emitted per unit emitting area, per steradian, per unit wavelength, formula_4 is the speed of light, formula_5 is the Boltzmann constant and formula_6 is the temperature in kelvins. For frequency formula_7, the expression is instead\n\nThe Rayleigh–Jeans law agrees with experimental results at large wavelengths (low frequencies) but strongly disagrees at short wavelengths (high frequencies). This inconsistency between observations and the predictions of classical physics is commonly known as the ultraviolet catastrophe. Its resolution in 1900 with the derivation by Max Planck of Planck's law, which gives the correct radiation at all frequencies, was a foundational aspect of the development of quantum mechanics in the early 20th century.\n\nIn 1900, the British physicist Lord Rayleigh derived the \"λ\" dependence of the Rayleigh–Jeans law based on classical physical arguments and empirical facts. A more complete derivation, which included the proportionality constant, was presented by Rayleigh and Sir James Jeans in 1905. The Rayleigh–Jeans law revealed an important error in physics theory of the time. The law predicted an energy output that diverges towards infinity as wavelength approaches zero (as frequency tends to infinity). Measurements of the spectral emission of actual black bodies revealed that the emission agreed with the Rayleigh–Jeans law at low frequencies but diverged at high frequencies; reaching a maximum and then falling with frequency, so the total energy emitted is finite.\n\nIn 1900 Max Planck empirically obtained an expression for black-body radiation expressed in terms of wavelength (Planck's law):\n\nwhere \"h\" is the Planck constant and the Boltzmann constant. The Planck's law does not suffer from an ultraviolet catastrophe, and agrees well with the experimental data, but its full significance (which ultimately led to quantum theory) was only appreciated several years later. Since,\n\nthen in the limit of high temperatures or long wavelengths, the term in the exponential becomes small, and the exponential is well approximated with the Taylor polynomial's first-order term,\n\nSo,\n\nThis results in Planck's blackbody formula reducing to\nwhich is identical to the classically derived Rayleigh–Jeans expression.\n\nThe same argument can be applied to the blackbody radiation expressed in terms of frequency . In the limit of small frequencies, that is formula_14,\n\nThis last expression is the Rayleigh–Jeans law in the limit of small frequencies.\n\nWhen comparing the frequency and wavelength dependent expressions of the Rayleigh–Jeans law it is important to remember that\nTherefore,\neven after substituting the value formula_19, because formula_20 has units of energy emitted per unit time per unit area of emitting surface, per unit solid angle, per unit wavelength, whereas formula_21 has units of energy emitted per unit time per unit area of emitting surface, per unit solid angle, per unit frequency. To be consistent, we must use the equality\nwhere both sides now have units of power (energy emitted per unit time) per unit area of emitting surface, per unit solid angle.\n\nStarting with the Rayleigh–Jeans law in terms of wavelength we get\nwhere\nThis leads us to find:\n\nDepending on the application, the Planck function can be expressed in 3 different forms. The first involves energy emitted per unit time per unit area of emitting surface, per unit solid angle, per spectral unit. In this form, the Planck function and associated Rayleigh–Jeans limits are given by\nor\n\nAlternatively, Planck's law can be written as an expression formula_28 for emitted power integrated over all solid angles. In this form, the Planck function and associated Rayleigh–Jeans limits are given by\nor\n\nIn other cases, Planck's law is written as formula_31 for energy per unit volume (energy density). In this form, the Planck function and associated Rayleigh–Jeans limits are given by\nor\n\n\n4. Beiser, Concepts of modern physics, Mcgraw Hill Education.\n\n"}
{"id": "35398475", "url": "https://en.wikipedia.org/wiki?curid=35398475", "title": "Resonating valence bond theory", "text": "Resonating valence bond theory\n\nIn condensed matter physics, the resonating valence bond theory (RVB) is a theoretical model that attempts to describe high temperature superconductivity, and in particular the superconductivity in cuprate compounds. It was first proposed by an American physicist P. W. Anderson and Indian theoretical physicist Ganapathy Baskaran in 1987. The theory states that in copper oxide lattices, electrons from neighboring copper atoms interact to form a valence bond, which locks them in place. However, with doping, these electrons can act as mobile Cooper pairs and are able to superconduct. Anderson observed in his 1987 paper that the origins of superconductivity in doped cuprates was in the Mott insulator nature of crystalline copper oxide. RVB builds on the Hubbard and t-J models used in the study of strongly correlated materials.\n\nIn 2014, evidence showing that fractional particles can happen in quasi two-dimensional magnetic materials, was found by EPFL scientists lending support for Anderson's theory of high-temperature superconductivity.\n\nThe physics of Mott insulators is described by the repulsive Hubbard model Hamiltonian:\n\nIn 1971, Anderson first suggested that this Hamiltonian can have a non-degenerate ground state that is composed of disordered spin states. Shortly after the high-temperature superconductors were discovered, Anderson and Kivelson et al. proposed a \"resonating valence bond\" ground state for these materials, written as\n\nWhere formula_3 represented a covering of a lattice by nearest neighbor dimers. Each such covering is weighted equally. In a mean field approximation, the RVB state can be written in terms of a Gutzwiller projection, and displays a superconducting phase transition per the Kosterlitz-Thouless mechanism. However, a rigorous proof for the existence of a superconducting ground state in either the Hubbard or the t-J Hamiltonian is not yet known. Further the stability of the RVB ground state has not yet been confirmed.\n"}
{"id": "53513295", "url": "https://en.wikipedia.org/wiki?curid=53513295", "title": "Science and technology in Pacific Island countries", "text": "Science and technology in Pacific Island countries\n\nPacific Island economies are mostly dependent on natural resources, with a tiny manufacturing sector and no heavy industry. In Fiji and Papua New Guinea, for instance, there is a need to adopt automated machinery and design in forestry and to improve training, in order to add value to exports.\n\nPapua New Guinea experienced the strongest economic growth between 2005 and 2013 (60%), during the commodities boom. Even during the global financial and economic crisis of 2008-2009, its economy grew by 13%. Vanuatu saw the next strongest growth (35%) over this period, including 10% growth in 2008-2009. Growth was more pedestrian in the Marshall Islands (a cumulative 19%), Tuvalu (16%), Samoa (15%), Kiribati (13%), Fiji (12%) and Tongo (8%). The economies of the Federated States of Micronesia and Palau actually shrank over this nine-year period. Samoa, the Marshall Islands and Fiji all experienced recession in 2008 and 2009.\n\nThe trade balance is more skewed towards imports than exports, with the exception of Papua New Guinea, which has a mining industry. There is growing evidence that Fiji is becoming a re-export hub in the Pacific; between 2009 and 2013, its re-exports grew threefold, accounting for more than half of all exports by Pacific Island states. Samoa can also expect to become more integrated in global markets from now on, having joined the World Trade Organization in 2012. Fiji, Papua New Guinea and the Solomon Islands are also members of the World Trade Organization.\n\nPacific Island states make up a very small share of the South Pacific's high-tech exports. These exports receded between 2008 and 2013 by 46% for Fiji and by 41% for Samoa, according to the United Nations' Comtrade database. Fiji's high-tech exports were down from US$5.0 million to US$2.7 million and Samoa's from US$0.3 to US$0.2 million.\n\nIn 2013, the majority of Fijian high-tech exports were pharmaceutical products (84%), whereas Samoa exports mainly scientific instruments (86%) and Kiribati non-electrical machinery (79%). Armaments make up 92% of high-exports from the Solomon Islands.\n\nBy 2013, one in three inhabitants of Fiji, Tonga and Tuvalu had Internet access. Growth in internet access since 2010 has levelled out the disparity between countries to some extent, although connectivity remained extremely low in Vanuatu (11%), the Solomon Islands (8%) and Papua New Guinea (7%) in 2013.\n\nAdvances in mobile phone technology have clearly been a factor in the provision of internet access to remote areas. The flow of knowledge and information through internet is likely to play an important role in the more effective dissemination and application of knowledge across the vast Pacific Island nations.\n\nThe lack of national and regional policy frameworks has been a major stumbling block for developing integrated national agendas. Since 2010, Pacific Island states have moved forward in this regard by establishing a number of regional bodies to address technological issues for sectorial development. None of these agencies has a specific mandate for science and technology policy however. Examples are the:\n\nThe establishment of the Pacific–Europe Network for Science, Technology and Innovation (PACE-Net Plus) goes some way towards filling the void in science policy, at least temporarily. Funded by the European Commission within its Seventh Framework Programme for Research and Innovation (2007–2013), this project has spanned the period 2013–2016 and thus overlaps with the European Union’s Horizon 2020 programme.\n\nThe objectives of PACE–Net Plus are to reinforce the dialogue between the Pacific region and Europe in science, technology and innovation; to support biregional research and innovation through calls for research proposals; and to promote scientific excellence and industrial and economic competition. Ten of its 16 members come from the Pacific region and the remainder from Europe.\n\nThe Pacific partners are the Australian National University, Montroix Pty Ltd (Australia), University of the South Pacific, Institut Malardé in French Caledonia, National Centre for Technological Research into Nickel and its Environment in New Caledonia, South Pacific Community, Landcare Research Ltd in New Zealand, University of Papua New Guinea, Samoa National University and the Vanuatu Cultural Centre.\n\nThe other six partners are: the Association of Commonwealth Universities, the Institut de recherche pour le développement in France, the Technical Centre for Agricultural and Rural Cooperation, a joint international institution of the African, Caribbean and Pacific Group of States and the European Union, the Sociedade Portuguesa de Inovação, United Nations Industrial Development Organization and Leibniz Centre for Tropical Marine Ecology in Germany.\n\nPACE-Net Plus focuses on three societal challenges:\nPACE–Net Plus has organized a series of high-level policy dialogue platforms alternately in the Pacific region and in Brussels, the headquarters of the European Commission. These platforms bring together key government and institutional stakeholders in both regions, around STI issues.\n\nA conference held in Suva (Fiji) in 2012 under the umbrella of PACE–Net Plus produced recommendations for a strategic plan for research, innovation and development in the Pacific. The conference report published in 2013 identified R&D needs in the Pacific in seven areas:\n\nNoting the general absence of regional and national policies and plans for science, technology and innovation in the Pacific, the PACE–Net Plus conference established the Pacific Islands University Research Network to support intra- and inter- regional knowledge creation and sharing and to prepare succinct recommendations for the development of a regional policy framework for science, technology and innovation.\n\nThis formal research network will complement the Fiji-based University of the South Pacific, which has campuses in other Pacific Island countries.\n\nIt was intended for the policy role of the Pacific Islands University Research Network to be informed by evidence gleaned from measuring capability in science, technology and innovation but the absence of data presents a formidable barrier. As of 2015, only Fiji had recent data on expenditure on research and development (R&D) and there were no recent data on researchers and technicians for any of the developing Pacific island countries.\n\nWithout relevant data, it will be difficult for developing Pacific Island states to monitor their progress towards Sustainable Development target 9.5, namely: \"Enhance scientific research, upgrade the technological capabilities of industrial sectors in all countries, in particular developing countries, including, by 2030, encouraging innovation and substantially increasing the number of research and development workers per 1 million people and public and private research and development spending\". The two indicators chosen by the United Nations to measure progress are research and development expenditure as a proportion of GDP (9.5.1) and Researchers (in full-time equivalent) per million inhabitants (9.5.2).\n\nFiji, Papua New Guinea and Samoa all consider education to be one of the key policy tools for driving science, technology and innovation, as well as modernization. Fiji, in particular, has made a supreme effort to re-visit existing policies, rules and regulations in this sector. The Fijian government allocates a larger portion of its national budget to education than any other Pacific Island country (4.2% of GDP in 2011), although this is down from 6% of GDP in 2000. The proportion of the education budget allocated to higher education (0.5% of GDP) amounts to 13% of the public education budget. Scholarship schemes like National Toppers, introduced in 2014, and the availability of student loans have made higher education attractive and rewarding in Fiji.\n\nAccording to an internal investigation into the choice of disciplines in school-leaving examinations (year 13), Fijian students have shown a greater interest in science since 2011. A similar trend can be observed in enrolment figures at all three Fijian universities.\n\nMany Pacific Island countries take Fiji as a benchmark for education. The country draws education leaders from other Pacific Island countries for training and, according to the Ministry of Education, teachers from Fiji are in great demand in these countries.\n\nOne important initiative has been the creation of the Higher Education Commission (FHEC) in 2010, the regulatory body in charge of tertiary education in Fiji. FHEC has embarked on registration and accreditation processes for tertiary-level education providers to improve the quality of higher education in Fiji. In 2014, FHEC allocated research grants to universities with a view to enhancing the research culture among faculty.\n\nFiji is the only developing Pacific Island country with recent data for gross domestic expenditure on research and development (GERD). The national Bureau of Statistics cites a GERD/GDP ratio of 0.15% in 2012. Private-sector research and development (R&D) is negligible. Between 2007 and 2012, government investment in R&D tended to favour agriculture. Scientists publish much more in geosciences and medical sciences than in agricultural sciences, however.\n\nFood security has been given high priority in the \"Fiji 2020 Agriculture Sector Policy\", as part of a shift from subsistence to commercial agriculture and agro-processing. Strategies outlined in Fiji 2020 include:\nFiji has taken the initiative of shifting away from subsistence agriculture towards commercial agriculture and agro-processing of root crops, tropical fruits, vegetables, spices, horticulture and livestock. In 2013, the Ministry of Agriculture revived Fiji’s \"Agricultural Journal\" in 2013, which had been dormant for 17 years.\n\nIn 2007, agriculture and primary production accounted for just under half of government expenditure on R&D, according to the Fijian National Bureau of Statistics. By 2012, this had risen to almost 60%. Scientists publish much more in the field of geosciences than in agriculture, though. Between 2008 and 2014, agriculture accounted for only 11 out of Fiji's 460 articles catalogued in Thomson Reuters' Web of Science (Science Citation Index Expanded), compared to 85 articles in geosciences.\n\nThe rise in government spending on agricultural research has come to the detriment of research in education, which dropped to 35% of total research spending between 2007 and 2012. Government expenditure on health has remained fairly constant, at about 5% of the total for research, according to the Fijian National Bureau of Statistics.\n\nOver the six years to 2012, government expenditure on health remained fairly constant but low in Fiji, at about 5% of the total for research, according to the Fijian National Bureau of Statistics. This may explain why medical sciences accounted for only 72 out of Fiji's 460 articles catalogued in Thomson Reuters' Web of Science (Science Citation Index Expanded) between 2008 and 2014.\n\nThe Fijian Ministry of Health is seeking to develop endogenous research capacity through the \"Fiji Journal of Public Health\", which it launched in 2012. A new set of guidelines are now in place to help build endogenous capacity in health research through training and access to new technology. The new policy guidelines require that all research projects initiated in Fiji with external bodies demonstrate how the project will contribute to local capacity-building in health research.\n\nThe desire to ensure that fisheries remain sustainable is fuelling the drive to use science and technology to make the transition to value-added production. The fisheries sector in Fiji is currently dominated by the catch of tuna for the Japanese market. The Fijian government plans to diversify this sector through aquaculture, inshore fisheries and offshore fish products such as sunfish and deep-water snapper. Accordingly, many incentives and concessions are being offered to encourage the private sector to invest in these areas.\n\nFiji has shown substantial growth in access to Internet and mobile phone services. This trend has been supported by its geographical location, service culture, pro-business policies, English-speaking population and well-connected e-society. Relative to many other South Pacific Islands, Fiji has a fairly reliable and efficient telecommunications system with access to the Southern Cross submarine cable linking New Zealand, Australia and North America. A recent move to establish the University of the South Pacific Stathan ICT Park, the Kalabo ICT economic zone and the ATH technology park in Fiji should boost the ICT support service sector in the Pacific region.\n\nIn its \"Higher Education Plan III 2014–2023\", Papua New Guinea sets out a strategy for transforming tertiary education and R&D through the introduction of a quality assurance system and a programme to overcome its limited R&D capacity.\n\nThe \"National Vision 2050\" was adopted in 2009. It has led to the establishment of the Research, Science and Technology Council. At its gathering in November 2014, the Council re-emphasized the need to focus on sustainable development through science and technology.\n\n\"Vision 2050\"’s medium-term priorities are:\n\nBetween 2008 and 2014, 82% of scientific articles from Papua New Guinea concerned the biological and medical sciences. Less than 10% of the country's 517 articles catalogued in Thomson Reuters' Web of Science (Science Citation Index Expanded) focused on geosciences.\n\nCountries around the Pacific Rim are seeking ways to link their national knowledge base to regional and global advances in science. One motivation for this greater interconnectedness is the region’s vulnerability to geohazards such as earthquakes and tsunamis – the Pacific Rim is not known as the Ring of Fire for nothing. In 2009, Samoa suffered a submarine earthquake of a magnitude of 8.1 on the Richter Scale, the strongest earthquake recorded that year. The subsequent tsunami caused substantial damage and loss of life in Samoa, American Samoa, and Tonga.\n\nThe need for greater disaster resilience is inciting countries to develop collaboration in the geosciences.\n\nClimate change is a parallel concern, as the Pacific Rim is also one of the most vulnerable regions to rising sea levels and increasingly capricious weather patterns. In March 2015, for instance, much of Vanuatu was flattened by Cyclone Pam.\n\nClimate change seems to be the most pressing environmental issue for Pacific Island countries, as it is already affecting almost all socio-economic sectors. The consequences of climate change can be seen in agriculture, food security, forestry and even in the spread of communicable diseases. Climate change mostly concerns marine issues, such as the growing frequency and severity of storms, rising sea levels and the increased salinity of soils and groundwater.\n\nThe Secretariat of the Pacific Community has initiated several activities to tackle problems associated with climate change. These cover a great variety of areas, including fisheries, freshwater, agriculture, coastal zone management, disaster management, energy, traditional knowledge, education, forestry, communication, tourism, culture, health, weather, gender implications and biodiversity. Almost all Pacific Island countries are involved in one or more of these activities.\n\nThe first major scheme focusing on adaptation to climate change and climate variability dates back to 2009. Pacific Adaptation to Climate Change involves 13 Pacific Island nations, with international funding from the Global Environment Facility, as well as from the US and Australian governments.\n\nSeveral projects related to climate change are also being co-ordinated by the United Nations Environment Programme, within the Secretariat of the Pacific Region Environmental Programme (SPREP). The aim of SPREP is to help all members improve their ‘capacity to respond to climate change through policy improvement, implementation of practical adaptation measures, enhancing ecosystem resilience to the impacts of climate change and implementing initiatives aimed at achieving low-carbon development’.\n\nThe blueprint for the subregion’s sustainable development over the coming decade is the \"Samoa Pathway\", the action plan adopted by countries at the third United Nations Conference on Small Island Developing States in Apia (Samoa) in September 2014. The \"Samoa Pathway\" focuses on, inter alia, sustainable consumption and production; sustainable energy, tourism and transportation; climate change; disaster risk reduction; forests; water and sanitation, food security and nutrition; chemical and waste management; oceans and seas; biodiversity; desertification, land degradation and drought; and health and non-communicable diseases.[https://sustainabledevelopment.un.org/samoapathway.html <nowiki>[1]</nowiki>]\n\nForestry is an important economic resource for Fiji and Papua New Guinea. However, forestry in both countries uses low and semi-intensive technological inputs. As a result, product ranges are limited to sawed timber, veneer, plywood, block board, moulding, poles and posts and wood chips. Only a few limited finished products are exported. Lack of automated machinery, coupled with inadequately trained local technical personnel, are some of the obstacles to introducing automated machinery and design. Policy-makers need to turn their attention to eliminating these barriers, in order for forestry to make a more efficient and sustainable contribution to national economic development.\n\nOn average, 10% of the GDP of Pacific Island countries funds imports of petroleum products but in some cases this figure can exceed 30%. In addition to high fuel transport costs, this reliance on fossil fuels leaves Pacific economies vulnerable to volatile global fuel prices and potential spills by oil tankers.\n\nConsequently, many Pacific Island countries are convinced that renewable energy will play a role in their socio-economic development. In Fiji, Papua New Guinea, Samoa and Vanuatu, renewable energy sources already represent significant shares of the total electricity supply: 60%, 66%, 37% and 15% respectively. Tokelau has even become the first country in the world to generate 100% of its electricity using renewable sources.\n\nAccording to the Secretariat of the Pacific Community, renewable energy still represented less than 10% of total energy use in the 22 Pacific Island countries and territories in 2015. The Secretariat of the Pacific Community observed that, 'while Fiji, Papua New Guinea and Samoa are leading the way with large-scale hydropower projects, there is enormous potential to expand the deployment of other renewable energy options such as solar, wind, geothermal and ocean-based energy sources'.\n\nInternational development partners are participating in several projects to develop renewable energy in the Pacific island states.\n\nIn the Cook Islands, for instance, the Asian Development Bank plans to supply electricity from renewable energy to all inhabited islands by 2020, within the \"Cook Islands Renewable Energy Chart Implementation Plan for 2012–2020\". New solar photovoltaic power plants with lithium-ion batteries were being built on up to six islands of the Southern Group in 2014.\n\nIn April 2014, Pacific Ministers for Energy and Transport agreed to establish the Pacific Centre for Renewable Energy and Energy Efficiency, 'a first for the Pacific'. The centre will become part of the United Nations Industrial Development Organization's network of regional Sustainable Energy for All Centres of Excellence, along with centres for the Caribbean Community, Economic Community of West African States, the Southern African Development Community and the East African Community.\n\nEfforts are under way to improve countries’ capacity to produce, conserve and use renewable energy. For example, the European Union has funded the Renewable Energy in Pacific Island Countries Developing Skills and Capacity programme (EPIC). Since its inception in 2013, EPIC has developed two master’s programmes in renewable energy management and helped to establish two Centres of Renewable Energy, one at the University of Papua New Guinea and the other at the University of Fiji. Both centres became operational in 2014 and aim to create a regional knowledge hub for the development of renewable energy.\n\nIn February 2014, the European Union and the Pacific Islands Forum Secretariat signed an agreement for a programme on Adapting to Climate Change and Sustainable Energy worth €37.26 million which will benefit 15 Pacific Island states. These are the Cook Islands, Fiji, Kiribati, Marshall Islands, Federated States of Micronesia, Nauru, Niue, Palau, Papua New Guinea, Samoa, Solomon Islands, Timor-Leste, Tonga, Tuvalu and Vanuatu.\n\nLimited freedom of expression and, in some cases, religious conservatism discourage research in certain areas but the experience of Pacific Island countries shows that sustainable development and a green economy can benefit from the inclusion of traditional knowledge in formal science and technology, as underlined by the \"Sustainable\" \"Development Brief\" prepared by the Secretariat of the Pacific Community in 2013.\n\nMany Pacific Island countries established new targets for renewable energy between 2010 and 2012.\n\nNational Energy targets for Pacific Island countries\n\nSource: \"UNESCO Science Report: towards 2030\", Table 27.3, based on Secretariat of the Pacific Community's Sustainable Development Brief (2013)\n\nAccording to the Web of Science, Papua New Guinea had the largest number of publications (110) among Pacific Island states in 2014, followed by Fiji (106). Fijian research was concentrated in a handful of scientific disciplines, such as medical sciences, geosciences and biology. Nine out of ten scientific publications from Papua New Guinea focused on immunology, genetics, biotechnology and microbiology.\n\nThis pattern contrasts with the trend observed in the French territories of New Caledonia and French Polynesia, where there was a strong emphasis on geosciences: six to eight times the world average for this field.\n\nMore than three-quarters of articles published by scientists from Pacific Island nations between 2008 and 2014 were signed by international collaborators, according to Thomson Reuters' Web of Science, Science Citation Index Expanded. International co-authorship was higher for Papua New Guinea and Fiji (90% and 83% respectively) than for New Caledonia and French Polynesia (63% and 56% respectively).\n\nAll countries counted North American partners among their top five partners. Fijian research collaboration with North American partners even exceeded that with India, even though a large proportion of Fijians are of Indian origin.\n\nResearch partnerships also involved Australia and countries in Europe. Surprisingly, there was little co-authorship with authors based in France, with the notable exception of Vanuatu. Some Pacific Island states counted their neighbours among their closest scientific collaborators, as in the case of the Solomon Islands and Vanuatu.\n\nMany of the smaller Pacific Island states have a near-100% rate of co-authorsip This extremely high rate can be a double-edged sword. According to the Fijian Ministry of Health, research collaboration often results in an article being published in a reputed journal but gives very little back in terms of strengthening health in Fiji. A new set of guidelines are now in place in Fiji to help build endogenous capacity in health research through training and access to new technology. The new policy guidelines require that all research projects initiated in Fiji with external bodies demonstrate how the project will contribute to local capacity-building in health research.\n\nTop five foreign collaborators for South Pacific scientists, 2008-2014\n\nSource: \"UNESCO Science Report: towards 2030\" (2015), Figure 27.8. Data from Thomson Reuters' Web of Science, Science Citation Index Expanded, data treatment by Science Metrix\n\nCountries are struggling to steer their scientific efforts toward sustainable development, at a time when the United Nations’ Sustainable Development Goals have taken over from the Millennium Development Goals in 2016. It has been suggested that countries could begin by encouraging their scientists to focus more on attaining local goals for sustainable development, rather than on publishing in high-profile international journals on topics that may be of lesser local relevance. The difficulty with this course of action is that the key metrics for recognizing scientific quality are publications and citation data. The answer to this dilemma most likely lies in the need to recognize the global nature of many local development problems. 'We are dealing with problems without boundaries and we underestimate the scale and nature of their consequences at our collective peril. As global citizens, the research and policy communities have an obligation to collaborate and deliver, so arguing for national priorities seems irrelevant'.\n\nIn 2012, the Fijian Ministry of Health launched the \"Fiji Journal of Public Health\", in an attempt to develop endogenous research capacity. In parallel, the Ministry of Agriculture revived Fiji’s \"Agricultural Journal\" in 2013, which had been dormant for 17 years.\n\nIn addition, two regional journals were launched in 2009 as a focus for Pacific scientific research, the \"Samoan Medical Journal\" and the \"Papua New Guinea Journal of Research, Science and Technology\".\n"}
{"id": "51336603", "url": "https://en.wikipedia.org/wiki?curid=51336603", "title": "Stodtmeister cell", "text": "Stodtmeister cell\n\nStodtmeister cells are a sub-classification of neutrophils exhibiting a Pelger-Huet anomaly with a non-lobed nucleus that may appear round or oval shaped.\n"}
{"id": "1426688", "url": "https://en.wikipedia.org/wiki?curid=1426688", "title": "Test Stand VII", "text": "Test Stand VII\n\nTest Stand VII (, P-7) was the principal V-2 rocket testing facility at Peenemünde Airfield and was capable of static firing of rocket motors up to 200 tons thrust. Notable events at the site include the first successful V-2 launch on 3 October 1942, visits by German military leaders, and Allied reconnaissance overflights and bombing.\n\nTwo distinguishing features of P-7 were the 670-yard-long elliptical high-sloped sand wall and the wide concrete-lined trench (flame pit) with a large symmetrical water-cooled flame deflector of molybdenum-steel pipes. The concrete trench, nearly wide with concrete walls, sloped gradually away from each side of the flame deflector to a depth of , rising again symmetrically toward the side of the arena. Beside the flame pit was a long underground room where diameter delivery pipes were housed to route cooling water at 120 gallon per second from three huge pumps in the pumphouse to the flame deflector in the pit.\n\nWhile the elliptical sand wall was for blocking high sea winds and blown sand, concrete structures were integrated into the wall and under the ground to protect equipment and personnel from rocket explosions and enemy bombing (a sand-filled dummy warhead, called \"the elephant\", was normally used). A large gap in the wall allowed easy entry by vehicles (particularly railcars with propellants), and an open tunnel through the ellipse wall at the narrower southern end also allowed entry. Integrated into the ellipse wall next to the tunnel was a massive observation and measuring blockhouse containing the control center. The control center had a double door with a bulletproof glass window from which an observer maintained telephone communication with the Telemetering Building at a remote location from P-7. A receiver in a lighthouse near Koserow provided telemetry from rockets with the Wolman System for Doppler tracking. For rockets that used radio control for V-2 engine cutoff, the Brennschluss equipment included a transmitter on the bank of the Peene about from P-7 and the Doppler radar at Lubmin (a motorized Würzburg radar, the \"rhinoceros\").\n\nThe control room also had switchboards, a row of four periscopes, manometers, frequency gauges, voltmeters and ammeters, green/red/white signal lamps, and switches at the propulsion console and guidance panel to dynamically display approximately 15 measurement points within the rocket. Additionally, the control room had a big \"X-time\" countdown clock that display the time until launch, which was announced via loudspeakers as \"X minus four minutes\", etc. In addition to the control room, the blockhouse also contained offices, a conference room, a small dormitory with double bunks and an adjoining shower, a wash room, and a workshop. A long underground corridor led from the measurement blockhouse to a room in the concrete foundation by the flame pit, and multiple rows of measurement cables covered the walls of the tunnel. A different gradually rising tunnel led from the long flame pit room to the exterior of the arena near the pumphouse (). Near the pumphouse were high wooden towers to cool the water, and high tanks for the recooling water were integrated into the ellipse wall.\n\nThe prominent tower within the arena was a mobile test frame/crane (\"Fahrbare Kranbühne\") which could be moved over the flame pit to position the rocket nozzle 25 feet above the deflector, and which allowed an entire missile to be gimbaled in two directions up to five degrees from vertical. The tower included an elevator and a German-made Toledo scale for thrust measurements. Actual launches were from a steel table-like structure (firing stand, \"Brennstand\") across the railway from the flame pit on the test stand's large concrete foundation. Under the concrete foundation were the recorder room, a small shop, an office, compressed nitrogen storage cylinders, and catch tanks. The arena also included an engine cold-calibration pad for conducting flow test measurements by pumping water (instead of Liquid oxygen) and alcohol (which was recovered afterward) via the turbopump through the combustion chamber. Since the V-2 motor had no controller for the turbopump, cold-calibration allowed the determination of \"freak cases\" of equipment.\n\nOutside of the arena was the 150x185x100h foot assembly and preparation hall/hangar (), which had been designed to be able to handle a larger A9/A10 multi-stage rocket that was planned, but never built. The roof of the hangar had camera stations for filming events.\n\nOn 15 May 1942 after photographing German destroyers berthed at the port of Kiel, Spitfire pilot Flight Lieutenant D. W. Stevenson photographed 'heavy construction work' near the Peenemünde aerodrome. Later in the month Constance Babington Smith decided \"the scale was too small ... then something unusual caught my eye ... some extraordinary circular embankments ... I then dismissed the whole thing from my mind.\" Then a year later on 22 April 1943, Bill White and Ron Prescott in RAF de Havilland Mosquito DZ473 were sent from Leuchars to photograph damage from Allied bombing at the Stettin railyards: \"On leaving Stettin, we left our cameras running all down the north coast of Germany, and when the film was developed, it was found to contain pictures of Peenemünde.\" The Medmenham interpreters studied the elliptical earthworks (originally photographed in May 1942) and noticed an \"object\" long projecting from what was thought to be a service building, although it had mysteriously disappeared on the next frame.\n\nOn 22 April 1943 a large cloud of steam was photographed near the embankments, which was later identified as coming from a rocket engine being test fired. Duncan Sandys' first photographic reconnaissance report on Peenemünde was circulated on 29 April 1943, which identified that the lack of power-station activity (Germany had installed electrostatic dust and smoke removers on the power station near Kölpin) indicates that \"\"The circular and elliptical constructions are probably for the testing of explosives and projectiles. ... In view of the above, it is clear that a heavy long-range rocket is not an immediate threat.\" Then on 14 May, an \"unusually high level of activity\"\" was visible at \"the Ellipse\" on photos from two sorties on 14 May, which was the date the Reich Director of Manpower (Gauleiter Fritz Sauckel) was a distinguished visitor at a launch.\nThe first solid evidence of the existence of a rocket came with a sortie (N/853) on 12 June, when a Spitfire flown by Sqn Ldr Gordon Hughes photographed Peenemünde: one photograph included an object on a railway truck. Reginald Victor Jones identified the object on 18 June as \"\"a whiteish [sic] cylinder about 35 feet long and 5 or so feet in diameter with a blueish nose and fins at the other end...I had found the rocket.\"\nAfter Operation Hydra bombed other areas of Peenemünde in 1943, the P-7 blockhouse roof was reinforced, and in a 1944 raid, the blockhouse occupants suffered one injury when a periscope fell. (Hermann Weidner's Test Stand 8 was lost in the 1944 July and August raids).\n\nThe last V-2 launch at Peenemünde was in February 1945, and on 5 May 1945, the 2nd Belorussian Front under General Konstantin Rokossovsky captured the port of Swinemünde and the Usedom island. Russian infantry under Major Anatole Vavilov stormed Peenemünde and found it \"75 per cent wreckage\" (the research buildings and test stands had been demolished.) A former adjutant at Peenemünde, Oberstleutnant Richard Rumschöttel, and his wife were killed during the attack, and Vavilov had orders to destroy the facility.\n"}
{"id": "43762481", "url": "https://en.wikipedia.org/wiki?curid=43762481", "title": "Timber trees of Gauteng", "text": "Timber trees of Gauteng\n\nThis is an alphabetical list of useful timber trees, indigenous (cultivated and natural) and exotic, growing in the Gauteng area of South Africa. These trees range in size up to some 1.5m DBH, such as \"Cedrus deodara\", the Himalayan Cedar. Hobbyists will seek out even small pieces of highly valued timber, such as \"Buxus macowanii\", the South African counterpart of \"Buxus sempervirens\", for turnery or the making of boxes and small items. Despite the wealth of useful woods available in Gauteng, most of the trees, felled or fallen, are dumped or cut into short lengths for fuel. Trees grown in urban or suburban environments are rarely pruned and are consequently often knotty. Timber frequently holds nails, wire and spikes, attesting to a variety of abuse during the lifetime of a tree, and requiring the use of a metal detector by the sawmiller. Garden cuttings are piled below trees and burnt, leaving charred scars and inclusions. \n\n \n"}
{"id": "33549502", "url": "https://en.wikipedia.org/wiki?curid=33549502", "title": "Unidentified infrared emission", "text": "Unidentified infrared emission\n\nThe unidentified infrared emission (UIR or UIE) bands are infrared discrete emissions from circumstellar regions, interstellar media, star-forming regions and extragalactic objects for which the identity of the emitting materials is unknown. The main infrared features occur around 3.3, 6.2, 7.7, 8.6, 11.2, and 12.7 μm, although there are many other weak emission features within the ~ 5–18 μm spectral range. In the 1980s, astronomers discovered that the origin of the UIR emission bands is inherent in compounds made of aromatic C–H and C=C chemical bonds, and hypothesized that the materials responsible should be polycyclic aromatic hydrocarbon (PAH) molecules. Nevertheless, data recorded with the ESA's Infrared Space Observatory and NASA's Spitzer Space Telescope have suggested that the UIR emission bands arise from compounds that are far more complex in composition and structure than PAH molecules. Moreover, the UIR bands follow a clear evolutionary spectral trend that is linked to the lifespan of the astronomical source; from the time the UIR bands first appear around evolved stars in the protoplanetary nebula stage to evolved stages such as the planetary nebula phase.\n\nThe UIR emission phenomenon has been studied for approximately 30 years.\n\n"}
{"id": "50257654", "url": "https://en.wikipedia.org/wiki?curid=50257654", "title": "WISE J1147-2040", "text": "WISE J1147-2040\n\nWISEA 1147 is a brown dwarf in the TW Hydrae association, a nearby group of very young stars and brown dwarfs. The object is notable because its estimate mass, 6±1 times the mass of Jupiter, places it in the mass range for planets. Nevertheless, it is a free-floating object, unassociated with any star system. \n\nThe object was discovered using information from NASA's WISE (Wide-field Infrared Survey Explorer) and the 2MASS (Two Micron All-Sky Survey). Researchers inferred the young age for WISEA 1147 because it is a member of a group of stars that is only 10 million years old, and they estimated its mass using evolutionary models for brown dwarf cooling.\n"}
