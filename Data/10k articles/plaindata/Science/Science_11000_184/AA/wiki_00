{"id": "1369507", "url": "https://en.wikipedia.org/wiki?curid=1369507", "title": "Active duty", "text": "Active duty\n\nActive duty is a full-time occupation as part of a military force, as opposed to reserve duty. In the United Kingdom and the Commonwealth of Nations the equivalent term is active service.\n\nThe Indian Armed Forces are considered to be one of the largest active service forces in the world, with almost 1.42 million Active Standing Army. An additional 2.20 million reserve forces can be activated in a few weeks as per the situation under the order of the President of India who is the Commander and Chief of the Armed Forces of India. This does not include the additional 1.40 million troops of the Paramilitary who too are an active force whose full-time responsibility is to guard the sovereignty of the nation from internal and external threats.\n\nIn the Israel Defense Forces, there are two types of active duty: regular service (, \"Sherut Sadir\"), and active reserve duty ( \"Sherut Milu'im Pa'il\", abbr. \"Shamap\"). Regular service refers to either mandatory service (, \"Sherut Hova\"), according to the laws of Israel, or standing army service (, \"Sherut Keva\"), which consists of paid NCOs and officers.\n\nActive reserve service refers to the actual time in which reservists are called up. This varies from once every few years to a month every year. During active reserve duty, military law can be applied to reservists, similarly to regular soldiers.\n\nThe Pakistan Armed Forces are one of the largest active service forces in the world, with almost 617,000 full-time personnel, due to the complex and volatile nature of Pakistan's relationship with India and the Kashmir region, and its porous border with Afghanistan. An additional 513,000 part-time reservists (including armed civilians of FATA) can be activated in a few weeks as per the situation under the order of the President of Pakistan who is the Commander and Chief of the Armed Forces of Pakistan. This does not include the additional 427,627 troops of the Paramilitary who too are an active force whose full-time responsibility is to guard the sovereignty of the nation from internal and external threats.\n\nIn the United States military, active duty refers to military members who are currently serving on full-time status in their military capacity. Full-time status is not limited to members of the active components of the military services; members of any of the three components (active, reserve, and the National Guard) may be placed into active status. All personnel in the active components are in active status. Reservists may be placed into active status as units or individuals. Units may be mobilized in support of operations, such as the reserve units that have been deployed in support of the Global War on Terror or those called up within the United States to provide support to civil authorities. Individuals may be placed in active status as part of the Active Guard Reserve program, as augmentees to active or reserve component units, or to attend full-time military training.\n"}
{"id": "57218855", "url": "https://en.wikipedia.org/wiki?curid=57218855", "title": "African women in engineering", "text": "African women in engineering\n\nGlobally women are often under-represented in the STEM-related fields such as the medical field and engineering. The under-representation gets even worse in the case of African women compared to the rest of the world. This has created gender gaps in the engineering field. These gender gaps are detrimental as they are associated with the loss of potential talent. A number of organizations within and out of Africa are working towards closing these gaps.\n\nStereotype threat may contribute to the under-representation of women in engineering. Because engineering is a traditionally male-dominated field, women may be less confident about their abilities, even when performing equally. At a young age, girls do not express the same level of interest in engineering as boys, possibly due in part to gender stereotypes. There is also significant evidence of the remaining presence of implicit bias against female engineers, due to the belief that men are mathematically superior and better suited to engineering jobs. Women who persist are able to overcome these difficulties, enabling them to find fulfilling and rewarding experiences in the engineering profession.\nDue to this gender bias, women’s choice in entering an engineering field for college is also highly correlated to the background and exposure they have had with mathematics and other science courses during high school. Most women that do choose to study engineering have significant experience with regarding themselves better at these types of courses and as a result, think they are capable of studying in a male-dominated field. \n\nWomen’s self-efficacy is also a contributor to the gender stereotype that plays a role in the underrepresentation of women in engineering. Women’s ability to think critically that they can be successful and perform accomplishments is correlated to the choices they have when choosing a college career. Women that show high self-efficacy personalities are more prone to choose to study in the engineering field. Self-efficacy is also correlated to gender roles because men often present higher self-efficacy than women, which can also be a cause to why when choosing a major, most women opt to not choose the engineering major.\n\nWomen are under-represented in engineering education programs as in the workforce (see Statistics). Enrollment and graduation rates of women in post-secondary engineering programs are very important determinants of how many women go on to become engineers. Because undergraduate degrees are acknowledged as the \"latest point of standard entry into scientific fields\", the under-representation of women in undergraduate programs contributes directly to under-representation in scientific fields. Additionally, in the United States, women who hold degrees in science, technology, and engineering fields are less likely than their male counterparts to have jobs in those fields.\n\nThis degree disparity varies across engineering disciplines. Women tend to be more interested in the engineering disciplines that have societal and humane developments, such as agricultural and environmental engineering. They are therefore well-represented in environmental and biomedical engineering degree programs, receiving 40-50% of awarded degrees in the U.S. (2014–15), women are far less likely to receive degrees in fields like mechanical, electrical and computer engineering.\n\nA study made by the Harvard Business Review discussed the reasons why the rates of women representation in the engineering field are still low. The study discovered that rates of female students in engineering programs are continuous because of the collaboration aspects in the field. The results of the study chiefly determined how women are treated differently in group works in which there are more male than female members and how male members “excluded women from the real engineering work”. Aside from this, women in this study also described how professors treated female students differently “just because they were women”.\n\nDespite the fact that fewer women enroll in engineering programs across the nation, the representation of women in STEM-based careers can potentially increase when college and university administrators work on implementing mentoring programs and work-life policies for women. \nResearch shows that these rates have a hard time increasing since women are judged as less competent than men to perform supposedly “masculine jobs”. \n\nAutodidact computer chip designer and inventor, Jeri Ellsworth, at the Bay Area \"Maker Faire\" in 2009.|alt=Jeri Ellsworth]]Another possible reason for lower female participation in engineering fields is the prevalence of values \"associated with the male gender role\" in workplace culture. For example, some women in engineering have found it difficult to re-enter the workforce after a period of absence. Because men are less likely to take time off to raise a family, this disproportionately affects women.\n\nMales are also associated with taking leadership roles in the workplace. By holding a position of power over the women, they may create an uncomfortable environment for them. For example, lower pay, more responsibilities, less appreciation as compared to men.\n\nCommunication is also a contributing factor to the divide between men and women in the workplace. A male to male communication is said to be more direct,\nbut when a man explains a task to a woman, they tend to talk down, or “dumb down” terms. This comes from the stereotype that men are more qualified than women for engineering, causing men to treat women as inferiors instead of equals.\n\nPart of the male dominance in the engineering field is explained by their perception towards engineering itself. A study in 1964 found that both women and men believed that engineering was in fact masculine.\n\nThe masculinity dominating engineering majors and fields proves the issues that men themselves believe that they “naturally” excel in fields related to mathematics and sciences while women “naturally” excel in linguistics and liberal arts. In the past few decades, women’s representation in the workforce in STEM fields, specifically engineering, has significantly improved. In 1960 women made up around 1% of all the engineers and by the year 2000 women have made up 11% of all engineers.\n\nSeveral colleges and universities nationwide want to decrease the gender gap between men and women in the engineering field by recruiting more women into their programs. The strategies used for recruiting more female undergraduate students are: increasing women’s exposure to stem-courses during high school, planting the idea of positivism relating gender from the engineering culture, producing a more female-friendly environment inside and outside the classroom. These strategies have helped institutions encourage more women to enroll in engineering programs as well as other STEM-based majors. \nFor universities to encourage women to enroll in their graduate programs, institutions have to emphasize the importance of recruiting women, emphasize the importance of STEM education in the undergraduate level, offer financial aid, and develop more efficient methods for recruiting women to their programs.\n"}
{"id": "6487479", "url": "https://en.wikipedia.org/wiki?curid=6487479", "title": "Alvey", "text": "Alvey\n\nThe Alvey Programme was a British government sponsored research program in information technology that ran from 1983 to 1987. The program was a reaction to the Japanese Fifth generation computer project.\n\nThe project was named after John Alvey, a technology director at BT who led the enquiry that proposed the creation of the program. John Alvey was not involved in the program itself \n\nFocus areas for the Alvey Programme included:\n\n\n"}
{"id": "45629075", "url": "https://en.wikipedia.org/wiki?curid=45629075", "title": "Amaechi Moshe", "text": "Amaechi Moshe\n\nAmaechi Moshe is a Director in Platform Petroleum Limited, an Oil and Gas Exploration and Production Company wholly owned by Nigerians; operator of the Egbaoma field (formerly Asuokpu/Umutu) located in Delta state, Nigeria.\n\nAmaechi Moshe is a highly skilled oil and gas specialist with over 37 years of professional experience. He graduated with a bachelor's degree in Geology from the University of Nigeria, Nsukka (UNN) in 1972. He holds a certificate in Geophysics, 1974 and a Master’s of Science degree in Geology, 1979.\n\nHis successful activist role was recorded in December 2011, when the Geology guru filed a suit to challenge the non-payment of the reviewed pension arrears for Nigerian National Petroleum Corporation (NNPC) pensioners (of which he was one). Consequently, in March 2013, the National Industrial Court in Nigeria ordered the Nigerian National Petroleum Pension Fund Limited to pay 500 million Naira to the NNPC pensioners who retired prior to 2004 as approved by the NNPC Board in 2009.\n\nAmaechi is a member of the American Association of Petroleum Geologists (AMAPG), Nigeria Mining and Geoscience Society (NMGS), as well as the Nigerian Association of Petroleum Explorationists (NAPE). He is also a fellow of the Geological Society of London.\n\nHis many years experience revolve around the oil and gas industry. Moshe Amaechi served as Manager of Exploration FES, NAPIMS from 1993 to 1995. He also served as Head Exploration of Shell J.V. OPS, NNPC from 1990 to 1991 and Head of Geophysics, J.V. Operation of NNPC from 1986 to 1990. He served 11 Years as Consultant Geophysicist in 1995. He has 24 years continuous oil and gas industry experience; 3 Years as Field Geophysicist with SSL, CGG & Geological Surveys and 21 Years with NNPC.\n\n"}
{"id": "51128933", "url": "https://en.wikipedia.org/wiki?curid=51128933", "title": "Amye Everard Ball", "text": "Amye Everard Ball\n\nAmye Everard Ball was the first woman in England to be granted a patent. Her patent for tincture of saffron was registered in 1637, merely 76 years after Elizabeth I had awarded the first patent. The original patent registration is held at the British Library.\n"}
{"id": "18498287", "url": "https://en.wikipedia.org/wiki?curid=18498287", "title": "An Introduction to Sustainable Development", "text": "An Introduction to Sustainable Development\n\nAn Introduction to Sustainable Development is a 2007 Earthscan book which presents sustainable development as a process that \"meets the needs of the present generation without compromising the ability of future generations to meet their own needs\". This textbook examines the environmental, economic, and social dimensions of sustainable development by exploring changing patterns of consumption, production, and distribution of resources. Case studies include coastal wetlands; community-based water supply and sanitation systems; and sustainable energy, forest, and industrial development.\n\nAuthor Peter P. Rogers is a Professor of Environmental Engineering at Harvard University, USA. Co-authors Kazi F. Jalal and John A. Boyd are lecturers at Harvard’s Extension School.\n"}
{"id": "16438307", "url": "https://en.wikipedia.org/wiki?curid=16438307", "title": "Argonaut Glacier", "text": "Argonaut Glacier\n\nArgonaut Glacier () is a tributary glacier about long in the Mountaineer Range of Victoria Land, Antarctica. It flows east to enter Mariner Glacier just north of Engberg Bluff. It was named by the New Zealand Geological Survey Antarctic Expedition, 1962–63, in association with Aeronaut, Cosmonaut and Cosmonette Glaciers.\n\n"}
{"id": "14703618", "url": "https://en.wikipedia.org/wiki?curid=14703618", "title": "Arp 87", "text": "Arp 87\n\nArp 87 (also known as NGC 3808) is a pair of two interacting galaxies, NGC 3808A and NGC 3808B. They are situated in the Leo constellation. NGC 3808A, the brighter, is a peculiar spiral galaxy, while NGC 3808B is an irregular galaxy.\n\nThe two galaxies were discovered on 10 April 1785 by William Herschel. The two are located about 330 million light-years (100 megaparsecs) away from the Earth. Arp 87 was observed by the Hubble Telescope in 2007, which revealed massive clouds of gas and dust flowing from one galaxy to another. Additionally, both galaxies appear to have been distorted.\n\n"}
{"id": "35186422", "url": "https://en.wikipedia.org/wiki?curid=35186422", "title": "Austrian Economics Newsletter", "text": "Austrian Economics Newsletter\n\nAustrian Economics Newsletter is a newsletter that was published quarterly by the Ludwig von Mises Institute until Winter 2003. It was established in the Fall of 1977 and published by the Center for Libertarian Studies, but moved to the Mises Institute in 1984. The newsletter covers economics from an Austrian perspective: \"Each issue spotlights the writings and research of a scholar or financial journalist who works within the tradition of the Austrian School.\"\n\n\n"}
{"id": "3402144", "url": "https://en.wikipedia.org/wiki?curid=3402144", "title": "Biomedical scientist", "text": "Biomedical scientist\n\nA biomedical scientist is a scientist trained in biology, particularly in the context of medicine. These scientists work to gain knowledge on the main principles of how the human body works and to find new ways to cure or treat disease by developing advanced diagnostic tools or new therapeutic strategies. The research of biomedical scientists is referred to as biomedical research.\n\nThe specific activities of the biomedical scientist can differ in various parts of the world and vary with the level of education. Generally speaking, biomedical scientists conduct research in a laboratory setting, using living organisms as models to conduct experiments. These can include cultured human or animal cells grown outside of the whole organism, small animals such as flies, worms, fish, mice, and rats, or, rarely, larger animals and primates. Biomedical scientists may also work directly with human tissue specimens to perform experiments as well as participate in clinical research.\n\nBiomedical scientists employ a variety of techniques in order to carry out laboratory experiments. These include:\n\nBiomedical scientists typically obtain a bachelor of science degree, and usually take postgraduate studies leading to a diploma, master or doctorate. This degree is necessary for faculty positions at academic institutions, as well as senior scientist positions at most companies. Some biomedical scientists also possess a medical degree (MD, DO, PharmD, Doctor of Medical Laboratory Sciences[MLSD], MBBS, etc.) in addition to an academic degree.\n\nThis category includes tenured faculty positions at universities, colleges, non-profit research institutes, and sometimes hospitals. These positions usually afford more intellectual freedom and give the researcher more latitude in the direction and content of the research. Scientists in academic settings, in addition to conducting experiments, will also attend scientific conferences, compete for research grant funding, publish scientific papers, and teach classes.\n\nIndustry jobs refer to private sector jobs at for-profit corporations. In the case of biomedical scientists, employment is usually at large pharmaceutical companies or biotechnology companies. Positions in industry tend to pay higher salaries than those at academic institutions, but job security compared to tenured academic faculty is significantly less. Researchers in industry tend to have less intellectual freedom in their research than those in the academic sector, owing to the ultimate goal of producing marketable products that benefit the company.\n\nIn recent years, more biomedical scientists have pursued careers where advanced education and experience in biomedical research is needed outside of traditional laboratory research. These areas include patent law, consulting, public policy, and science journalism. The primary reason for growth in these areas is that in recent years fewer positions are available in traditional academic research relative to the number of seekers; approximately 15-20% of PhD life scientists will obtain a tenure-track position or lab-head position in industry.\n\n\"Biomedical scientist\" is the protected title used by professionals qualified to work unsupervised within the pathology department of a hospital. The biomedical sciences are made up of the following disciplines; biochemistry, haematology, immunology, microbiology, histology, cytology, and transfusion services. These professions are regulated within the United Kingdom by the Health and Care Professions Council. Anyone who falsely claims to be a biomedical scientist commits an offence and could be fined up to £5000.\n\nEach department specialises in aiding the diagnosis and treatment of disease. Entry to the profession requires an Institute of Biomedical Science (IBMS) accredited BSc honours degree followed by a minimum of 12 months laboratory training in one of the pathology disciplines, however the actual time spent training can be considerably longer. Trainees are also required to complete a certificate of competence training portfolio, this requires gathering extensive amounts of evidence to demonstrate professional competence. At the end of this period the trainees portfolio and overall competence are assessed; if successful, a certificate of competence is awarded, which can be used to apply for registration with the HCPC. State registration indicates that the applicant has reached a required standard of education and will follow the guidelines and codes of practice created by the Health and Care Professions Council. The NHS, the largest employer of Biomedical Scientist, now run the 'Practitioners Training Program' in conjunction with several Universities which includes a years experienced as a part of a 3-year degree. This is known as BSc Healthcare Science (Life Science) \nBiomedical Scientists are the second largest profession registered by the Health and Care Professions Council and make up a vital component of the health care team. Many of the decisions doctors make are based on the test results generated by Biomedical Scientists. Despite this, much of the general public are unaware of Biomedical Scientists and the important role they play.\n\nBiomedical Scientists are not confined to NHS laboratories. Biomedical Scientists along with scientists in other inter-related medical disciplines seek out to understand human anatomy, genetics, immunology, physiology and behaviour at all levels. This is sometimes achieved through the use of model systems that are homologous to various aspects of human biology. The research that is carried out either in Universities or Pharmaceutical companies by Biomedical Scientists has led to the development of new treatments for a wide range of degenerative and genetic disorders. Stem cell biology, cloning, genetic screening/therapies and other areas of biomedical science have all been generated by the work of Biomedical Scientists from around the world.\n\nBiomedical science graduate programs are maintained at academic institutions and medical schools around the world, and some biomedical graduate programs are administered jointly by an academic institution and a business, hospital, or independent research institute. While graduate students historically committed to a particular research specialty, such as molecular biology, biochemistry, genetics, or developmental biology, the recent trend (particularly in the United States) is to offer interdisciplinary programs that do not specialize and instead aim to incorporate a broad education in multiple biological disciplines.\n\nInitially, graduate students usually rotate through the laboratories of several faculty researchers, after which the student commits to joining a particular laboratory for the remainder of his or her education. The remaining time is spent conducting original research under the direction of the principal investigator to complete and publish a dissertation. Unlike undergraduate and professional schools, there is no set time period for graduate education. Students graduate once a thesis project of significant scope to justify the writing of their dissertation has been completed, a point that is determined by the student's principal investigator as well as his or her faculty advisory committee. The average time to graduation can vary between institutions, but most programs average around 5–6 years.\n\nBiomedical scientists typically study in undergraduate majors that are focused on biological sciences, such as biochemistry, microbiology, zoology, biophysics, etc.\n\nEducation programmes have traditionally encompassed an initial bachelor's degree, which is presupposed for two years of further studies eventually earning the students a \"medicine master's examina\". Many students choose to study on (for as much as) another 4 years to earn a PhD degree, at this time the students specialize in a certain areas such as nephrology, neurology, oncology or virology.\n\nIn the UK specifically, prospective undergraduate students wishing to undertake a BSc in biomedical sciences are required to apply via the UCAS application system (usually during the final year of college or sixth form secondary school). A PhD in Biomedicine is however required for most higher research and teaching positions, which most colleges and universities offer. These graduate degree programs may include classroom and fieldwork, research at a laboratory, and a dissertation. Although a degree in a medicine or biology (biochemistry, microbiology, zoology, biophysics) is common, recent research projects also need graduates in statistics, bioinformatics, physics and chemistry. Abilities preferred for entry in this field include: technical, scientific, numerical, written, and oral skills.\n\nUniversity departments offering degree programmes and/or research in biomedical sciences are represented by the Heads of University Centres of Biomedical Sciences (HUCBMS). HUCBMS has an international membership.\n\nBiomedical scientists can focus on several areas of specialty, including:\n\nHowever, recent trends in biomedical graduate education (particularly in the United States) are for biomedical scientists to remain interdisciplinary and to not specialize. This approach emphasizes focus on a particular body or disease process as a whole and drawing upon the techniques of multiple specialties. (\"See also: Systems biology\")\n\nIn the United Kingdom, the salaries for biomedical scientists range from £21,692 to £67,805 plus high cost area supplements and out of hours payments, depending on experience, education, and position. Job growth for the profession has been forecasted as follows:\nAccording to the US Bureau of Labor Statistics (BLS), the 2010-2011 occupational outlook report suggests that biomedical scientist employment is expected \"to increase 40 percent over the 2008-18 decade, much faster than the average for all occupations.\"\n\nAccording to the 2010 BLS report, the median salaries for biomedical scientists in the United States in particular employment areas are:\n\nThese figures include the salaries of post-doctoral fellows, which are paid significantly less than employees in more permanent positions.\n\n\n"}
{"id": "41615741", "url": "https://en.wikipedia.org/wiki?curid=41615741", "title": "Borromean nucleus", "text": "Borromean nucleus\n\nA Borromean nucleus is an atomic nucleus that has a nuclear halo containing two neutrons. Such a nucleus breaks into three components and never two when disrupted, in analogy to Borromean rings. One example is the nucleus of carbon-22 C.\n"}
{"id": "1871873", "url": "https://en.wikipedia.org/wiki?curid=1871873", "title": "Botanic Garden Meise", "text": "Botanic Garden Meise\n\nThe Botanic Garden Meise (, ; until 2014 called the National Botanic Garden of Belgium (, )) is located in the grounds of Bouchout Castle in the town of Meise, just north of Brussels in the province of Flemish Brabant. It is one of the largest botanical gardens in the world with an extensive collection of living plants in addition to a herbarium of over 3 million specimens. The current garden was established in 1958 after it moved from the centre of Brussels; the former site is now the Botanical Garden of Brussels. Researchers at the garden conduct research particularly on Belgian and African plants.\n\nThe Botanic Garden contains about 18,000 plant species—about 6% of all known plant species of the world. Half are in greenhouses, the other half, including cultivated and indigenous plants, are outdoors. The gardens are grouped around the castle and lake of the Bouchout domain.\n\nThe mission statement of the Botanic Garden Meise specifies the increasing and spreading \"the knowledge of plants\" and contributions to \"the conservation of biodiversity.\"\n\nThe Botanic Garden was property of the Belgian federal government, but after several years of negotiations it was eventually transferred to the Flemish Community (Flanders) effective 1 January 2014. The French Community still has its own employees and representation in the board of directors. The plants, library, etc. remain property of the federal State but given as commodate to the Flemish Community.\n\nThe first botanic garden in Brussels belonged to the du Département de la Dyle that was created during the French rule of Belgium at the end of the 18th century. Due to their costs, those French schools were soon dropped and some municipalities, including the City of Brussels, took over the garden that was about to be abandoned. In 1815, Belgium became part of the United Kingdom of the Netherlands. Around the same period, the maintenance costs of the garden were regarded as too high by the city administration. A group of local bourgeois decided to create a new kind of botanical garden in Brussels. At the time the bourgeoisie was the new leading-class and since companies were popular financing method the garden was created as a company. The creators thought it would be their contribution to the city's reputation. Although it was rooted on a private enterprise, it was also supposed to be a national institution dedicated to science.\n\nBoth the City and the Home Office supported it financially. But, the Independence of Belgium (1830-1831) was detrimental to the Dutch-born institution: it was regarded as orangist, as a mere playground for the local elites, and as not useful for the country's agriculture, among other critiques. From then on, the garden would have to battle to survive. The state and the city did not want to support it anymore unless it proved useful to the whole country, so the Garden was obliged to develop its commercial activities. It sold plants by the thousands, and created several money-consuming attractions and events for the local élite, like aquaria, a dance room, fairs, a fish nursery, concerts etc. In the 1860s, the aging buildings required renovation. The board of the Society of Horticulture tried to raise the money, but the costs were just too high for the company. In 1870, the Belgian Government took over the company. The National Botanic Garden was created in the very same year. Barthélemy Dumortier (1797-1878), a Belgian politician and botanist, had played a major role in this process. He wanted a “Belgian Kew” to be created in the capital of Belgium, that is to say a botanical garden dedicated to taxonomy. That is why, some months before the garden was bought by the state, the Belgian Government had purchased the famous von Martius Herbarium that was held in Munich. So, in 1870, Belgium had a great herbarium and an appropriate building. This was the dawn of a new era for Belgian botany.\n\nIn 1927, just after the death of empress Charlotte, it was proposed to set up the National Botanical Garden at the Bouchout Domain. It took until 1937 before the final decision was made and major constructions were started. To the South-East of Bouchout Castle, the \"Palace of the Plants\" was built, which consists of a number of greenhouses. More to the South-West of the castle, the Victoria Balet greenhouse was placed. This greenhouse was designed in 1853 by Alfonse Balat and transported from its original location at the Botanical Garden of Brussels.\n\nDuring the second world war, Bouchout Castle was occupied by the German forces and the domain altered into a fortress. Next to the Palace of Plants, six baracks were placed and concrete defenses were erected. The court of honour of Bouchout Castle was used to store ammunition, while artillery defenses were placed at the borders of the domain. The last German soldiers left Bouchout domain at 3 September 1944. Just a few days later, the Allied Forces arrived who used it as a training location, while stationing about 200 vehicles at the domain. At 29 November 1944, a bomb exploded at the Western part of the parc destroying the windows of Bouchout Castle. A second bomb exploded at 2 December nearby the adjacent Hoogvorst Castle, causing its complete destruction.\n\n\n"}
{"id": "47523070", "url": "https://en.wikipedia.org/wiki?curid=47523070", "title": "Cirque stairway", "text": "Cirque stairway\n\nA cirque stairway or sequence of cirque steps is a stepped succession of glacially eroded rock basins. Their individual formation is that of a cirque.\n\nThese steps are arranged one above and behind the other at different heights in the terrain and caused by the same geomorphodynamic processes, albeit resulting in different landform shapes depending on the type of rock and the depositional circumstances involved. The lower step often lacks the steep headwalls typical of cirques.\n\nA well-known example is the Zastler Loch below the summit of the Feldberg, the highest mountain of the Black Forest in Germany.\n"}
{"id": "52551941", "url": "https://en.wikipedia.org/wiki?curid=52551941", "title": "David A. Lewis", "text": "David A. Lewis\n\nDavid A. Lewis is an American psychiatrist and neuroscientist, currently a Distinguished Professor of Psychiatry and Neuroscience, Thomas Detre Professor of Academic Psychiatry and also Director of Conte Center for Translational Mental Health Research at University of Pittsburgh.\n"}
{"id": "27345600", "url": "https://en.wikipedia.org/wiki?curid=27345600", "title": "EXMARaLDA", "text": "EXMARaLDA\n\nEXMARaLDA (Extensible Markup Language for Discourse Annotation) is a set of free software tools for creating, managing and analyzing spoken language corpora. It consists of a transcription tool (comparable to tools like Praat or Transcriber), a tool for administering corpus meta data and a tool for doing queries (KWIC searches) on spoken language corpora. EXMARaLDA is used for doing conversation and discourse analysis, dialectology, phonology and research into first and second language acquisition in children and adults. EXMARaLDA is based on the open standards XML and Unicode and programmed in Java.\n\n\n"}
{"id": "516133", "url": "https://en.wikipedia.org/wiki?curid=516133", "title": "Equipartition theorem", "text": "Equipartition theorem\n\nIn classical statistical mechanics, the equipartition theorem relates the temperature of a system to its average energies. The equipartition theorem is also known as the law of equipartition, equipartition of energy, or simply equipartition. The original idea of equipartition was that, in thermal equilibrium, energy is shared equally among all of its various forms; for example, the average kinetic energy per degree of freedom in translational motion of a molecule should equal that in rotational motion.\n\nThe equipartition theorem makes quantitative predictions. Like the virial theorem, it gives the total average kinetic and potential energies for a system at a given temperature, from which the system's heat capacity can be computed. However, equipartition also gives the average values of individual components of the energy, such as the kinetic energy of a particular particle or the potential energy of a single spring. For example, it predicts that every atom in a monatomic ideal gas has an average kinetic energy of (3/2)\"k\"\"T\" in thermal equilibrium, where \"k\" is the Boltzmann constant and \"T\" is the (thermodynamic) temperature. More generally, equipartition can be applied to any classical system in thermal equilibrium, no matter how complicated. It can be used to derive the ideal gas law, and the Dulong–Petit law for the specific heat capacities of solids. The equipartition theorem can also be used to predict the properties of stars, even white dwarfs and neutron stars, since it holds even when relativistic effects are considered.\n\nAlthough the equipartition theorem makes accurate predictions in certain conditions, it is inaccurate when quantum effects are significant, such as at low temperatures. When the thermal energy \"k\"\"T\" is smaller than the quantum energy spacing in a particular degree of freedom, the average energy and heat capacity of this degree of freedom are less than the values predicted by equipartition. Such a degree of freedom is said to be \"frozen out\" when the thermal energy is much smaller than this spacing. For example, the heat capacity of a solid decreases at low temperatures as various types of motion become frozen out, rather than remaining constant as predicted by equipartition. Such decreases in heat capacity were among the first signs to physicists of the 19th century that classical physics was incorrect and that a new, more subtle, scientific model was required. Along with other evidence, equipartition's failure to model black-body radiation—also known as the ultraviolet catastrophe—led Max Planck to suggest that energy in the oscillators in an object, which emit light, were quantized, a revolutionary hypothesis that spurred the development of quantum mechanics and quantum field theory.\n\nThe name \"equipartition\" means \"equal division,\" as derived from the Latin \"equi\" from the antecedent, æquus (\"equal or even\"), and partition from the noun, \"partitio\" (\"division, portion\"). The original concept of equipartition was that the total kinetic energy of a system is shared equally among all of its independent parts, \"on the average\", once the system has reached thermal equilibrium. Equipartition also makes quantitative predictions for these energies. For example, it predicts that every atom of an inert noble gas, in thermal equilibrium at temperature \"T\", has an average translational kinetic energy of (3/2)\"k\"\"T\", where \"k\" is the Boltzmann constant. As a consequence, since kinetic energy is equal to 1/2(mass)(velocity), the heavier atoms of xenon have a lower average speed than do the lighter atoms of helium at the same temperature. Figure 2 shows the Maxwell–Boltzmann distribution for the speeds of the atoms in four noble gases.\n\nIn this example, the key point is that the kinetic energy is quadratic in the velocity. The equipartition theorem shows that in thermal equilibrium, any degree of freedom (such as a component of the position or velocity of a particle) which appears only quadratically in the energy has an average energy of \"k\"\"T\" and therefore contributes \"k\" to the system's heat capacity. This has many applications.\n\nThe (Newtonian) kinetic energy of a particle of mass \"m\", velocity v is given by\n\nwhere \"v\", \"v\" and \"v\" are the Cartesian components of the velocity v. Here, \"H\" is short for Hamiltonian, and used henceforth as a symbol for energy because the Hamiltonian formalism plays a central role in the most general form of the equipartition theorem.\n\nSince the kinetic energy is quadratic in the components of the velocity, by equipartition these three components each contribute \"k\"\"T\" to the average kinetic energy in thermal equilibrium. Thus the average kinetic energy of the particle is (3/2)\"k\"\"T\", as in the example of noble gases above.\n\nMore generally, in an ideal gas, the total energy consists purely of (translational) kinetic energy: by assumption, the particles have no internal degrees of freedom and move independently of one another. Equipartition therefore predicts that the average total energy of an ideal gas of \"N\" particles is (3/2) \"N k\" \"T\".\n\nIt follows that the heat capacity of the gas is (3/2) \"N k\" and hence, in particular, the heat capacity of a mole of such gas particles is (3/2)\"N\"\"k\" = (3/2)\"R\", where \"N\" is the Avogadro constant and \"R\" is the gas constant. Since \"R\" ≈ 2 cal/(mol·K), equipartition predicts that the molar heat capacity of an ideal gas is roughly 3 cal/(mol·K). This prediction is confirmed by experiment.\n\nThe mean kinetic energy also allows the root mean square speed \"v\" of the gas particles to be calculated:\n\nwhere \"M\" = \"N\"\"m\" is the mass of a mole of gas particles. This result is useful for many applications such as Graham's law of effusion, which provides a method for enriching uranium.\n\nA similar example is provided by a rotating molecule with principal moments of inertia \"I\", \"I\" and \"I\". The rotational energy of such a molecule is given by\nwhere \"ω\", \"ω\", and \"ω\" are the principal components of the angular velocity. By exactly the same reasoning as in the translational case, equipartition implies that in thermal equilibrium the average rotational energy of each particle is (3/2)\"k\"\"T\". Similarly, the equipartition theorem allows the average (more precisely, the root mean square) angular speed of the molecules to be calculated.\n\nThe tumbling of rigid molecules—that is, the random rotations of molecules in solution—plays a key role in the relaxations observed by nuclear magnetic resonance, particularly protein NMR and residual dipolar couplings. Rotational diffusion can also be observed by other biophysical probes such as fluorescence anisotropy, flow birefringence and dielectric spectroscopy.\n\nEquipartition applies to potential energies as well as kinetic energies: important examples include harmonic oscillators such as a spring, which has a quadratic potential energy\n\nwhere the constant \"a\" describes the stiffness of the spring and \"q\" is the deviation from equilibrium. If such a one-dimensional system has mass \"m\", then its kinetic energy \"H\" is\n\nwhere \"v\" and \"p\" = \"mv\" denote the velocity and momentum of the oscillator. Combining these terms yields the total energy\n\nEquipartition therefore implies that in thermal equilibrium, the oscillator has average energy\n\nwhere the angular brackets formula_8 denote the average of the enclosed quantity,\n\nThis result is valid for any type of harmonic oscillator, such as a pendulum, a vibrating molecule or a passive electronic oscillator. Systems of such oscillators arise in many situations; by equipartition, each such oscillator receives an average total energy \"k\"\"T\" and hence contributes \"k\" to the system's heat capacity. This can be used to derive the formula for Johnson–Nyquist noise and the Dulong–Petit law of solid heat capacities. The latter application was particularly significant in the history of equipartition.\n\nAn important application of the equipartition theorem is to the specific heat capacity of a crystalline solid. Each atom in such a solid can oscillate in three independent directions, so the solid can be viewed as a system of 3\"N\" independent simple harmonic oscillators, where \"N\" denotes the number of atoms in the lattice. Since each harmonic oscillator has average energy \"k\"\"T\", the average total energy of the solid is 3\"Nk\"\"T\", and its heat capacity is 3\"Nk\".\n\nBy taking \"N\" to be the Avogadro constant \"N\", and using the relation \"R\" = \"N\"\"k\" between the gas constant \"R\" and the Boltzmann constant \"k\", this provides an explanation for the Dulong–Petit law of specific heat capacities of solids, which stated that the specific heat capacity (per unit mass) of a solid element is inversely proportional to its atomic weight. A modern version is that the molar heat capacity of a solid is \"3R\" ≈ 6 cal/(mol·K).\n\nHowever, this law is inaccurate at lower temperatures, due to quantum effects; it is also inconsistent with the experimentally derived third law of thermodynamics, according to which the molar heat capacity of any substance must go to zero as the temperature goes to absolute zero. A more accurate theory, incorporating quantum effects, was developed by Albert Einstein (1907) and Peter Debye (1911).\n\nMany other physical systems can be modeled as sets of coupled oscillators. The motions of such oscillators can be decomposed into normal modes, like the vibrational modes of a piano string or the resonances of an organ pipe. On the other hand, equipartition often breaks down for such systems, because there is no exchange of energy between the normal modes. In an extreme situation, the modes are independent and so their energies are independently conserved. This shows that some sort of mixing of energies, formally called \"ergodicity\", is important for the law of equipartition to hold.\n\nPotential energies are not always quadratic in the position. However, the equipartition theorem also shows that if a degree of freedom \"x\" contributes only a multiple of \"x\" (for a fixed real number \"s\") to the energy, then in thermal equilibrium the average energy of that part is \"k\"\"T\"/\"s\".\n\nThere is a simple application of this extension to the sedimentation of particles under gravity. For example, the haze sometimes seen in beer can be caused by clumps of proteins that scatter light. Over time, these clumps settle downwards under the influence of gravity, causing more haze near the bottom of a bottle than near its top. However, in a process working in the opposite direction, the particles also diffuse back up towards the top of the bottle. Once equilibrium has been reached, the equipartition theorem may be used to determine the average position of a particular clump of buoyant mass \"m\". For an infinitely tall bottle of beer, the gravitational potential energy is given by\n\nwhere \"z\" is the height of the protein clump in the bottle and \"g\" is the acceleration due to gravity. Since \"s\" = 1, the average potential energy of a protein clump equals \"k\"\"T\". Hence, a protein clump with a buoyant mass of 10 MDa (roughly the size of a virus) would produce a haze with an average height of about 2 cm at equilibrium. The process of such sedimentation to equilibrium is described by the Mason–Weaver equation.\n\nThe equipartition of kinetic energy was proposed initially in 1843, and more correctly in 1845, by John James Waterston. In 1859, James Clerk Maxwell argued that the kinetic heat energy of a gas is equally divided between linear and rotational energy. In 1876, Ludwig Boltzmann expanded on this principle by showing that the average energy was divided equally among all the independent components of motion in a system. Boltzmann applied the equipartition theorem to provide a theoretical explanation of the Dulong–Petit law for the specific heat capacities of solids.\n\nThe history of the equipartition theorem is intertwined with that of specific heat capacity, both of which were studied in the 19th century. In 1819, the French physicists Pierre Louis Dulong and Alexis Thérèse Petit discovered that the specific heat capacities of solid elements at room temperature were inversely proportional to the atomic weight of the element. Their law was used for many years as a technique for measuring atomic weights. However, subsequent studies by James Dewar and Heinrich Friedrich Weber showed that this Dulong–Petit law holds only at high temperatures; at lower temperatures, or for exceptionally hard solids such as diamond, the specific heat capacity was lower.\n\nExperimental observations of the specific heat capacities of gases also raised concerns about the validity of the equipartition theorem. The theorem predicts that the molar heat capacity of simple monatomic gases should be roughly 3 cal/(mol·K), whereas that of diatomic gases should be roughly 7 cal/(mol·K). Experiments confirmed the former prediction, but found that molar heat capacities of diatomic gases were typically about 5 cal/(mol·K), and fell to about 3 cal/(mol·K) at very low temperatures. Maxwell noted in 1875 that the disagreement between experiment and the equipartition theorem was much worse than even these numbers suggest; since atoms have internal parts, heat energy should go into the motion of these internal parts, making the predicted specific heats of monatomic and diatomic gases much higher than 3 cal/(mol·K) and 7 cal/(mol·K), respectively.\n\nA third discrepancy concerned the specific heat of metals. According to the classical Drude model, metallic electrons act as a nearly ideal gas, and so they should contribute (3/2) \"N\"\"k\" to the heat capacity by the equipartition theorem, where \"N\" is the number of electrons. Experimentally, however, electrons contribute little to the heat capacity: the molar heat capacities of many conductors and insulators are nearly the same.\n\nSeveral explanations of equipartition's failure to account for molar heat capacities were proposed. Boltzmann defended the derivation of his equipartition theorem as correct, but suggested that gases might not be in thermal equilibrium because of their interactions with the aether. Lord Kelvin suggested that the derivation of the equipartition theorem must be incorrect, since it disagreed with experiment, but was unable to show how. In 1900 Lord Rayleigh instead put forward a more radical view that the equipartition theorem and the experimental assumption of thermal equilibrium were \"both\" correct; to reconcile them, he noted the need for a new principle that would provide an \"escape from the destructive simplicity\" of the equipartition theorem. Albert Einstein provided that escape, by showing in 1906 that these anomalies in the specific heat were due to quantum effects, specifically the quantization of energy in the elastic modes of the solid. Einstein used the failure of equipartition to argue for the need of a new quantum theory of matter. Nernst's 1910 measurements of specific heats at low temperatures supported Einstein's theory, and led to the widespread acceptance of quantum theory among physicists.\n\nThe most general form of the equipartition theorem states that under suitable assumptions (discussed below), for a physical system with Hamiltonian energy function \"H\" and degrees of freedom \"x\", the following equipartition formula holds in thermal equilibrium for all indices \"m\" and \"n\":\n\nHere \"δ\" is the Kronecker delta, which is equal to one if \"m\" = \"n\" and is zero otherwise. The averaging brackets formula_8 is assumed to be an ensemble average over phase space or, under an assumption of ergodicity, a time average of a single system.\n\nThe general equipartition theorem holds in both the microcanonical ensemble, when the total energy of the system is constant, and also in the canonical ensemble, when the system is coupled to a heat bath with which it can exchange energy. Derivations of the general formula are given later in the article.\n\nThe general formula is equivalent to the following two:\n\nIf a degree of freedom \"x\" appears only as a quadratic term \"ax\" in the Hamiltonian \"H\", then the first of these formulae implies that\nwhich is twice the contribution that this degree of freedom makes to the average energy formula_15. Thus the equipartition theorem for systems with quadratic energies follows easily from the general formula. A similar argument, with 2 replaced by \"s\", applies to energies of the form \"ax\".\n\nThe degrees of freedom \"x are coordinates on the phase space of the system and are therefore commonly subdivided into generalized position coordinates \"q\" and generalized momentum coordinates \"p\", where \"p\" is the conjugate momentum to \"q\". In this situation, formula 1 means that for all \"k\",\n\nUsing the equations of Hamiltonian mechanics, these formulae may also be written\n\nSimilarly, one can show using formula 2 that\n\nand\n\nThe general equipartition theorem is an extension of the virial theorem (proposed in 1870), which states that\n\nwhere \"t\" denotes time. Two key differences are that the virial theorem relates \"summed\" rather than \"individual\" averages to each other, and it does not connect them to the temperature \"T\". Another difference is that traditional derivations of the virial theorem use averages over time, whereas those of the equipartition theorem use averages over phase space.\n\nIdeal gases provide an important application of the equipartition theorem. As well as providing the formula\nfor the average kinetic energy per particle, the equipartition theorem can be used to derive the ideal gas law from classical mechanics. If q = (\"q\", \"q\", \"q\") and p = (\"p\", \"p\", \"p\") denote the position vector and momentum of a particle in the gas, and\nF is the net force on that particle, then\nwhere the first equality is Newton's second law, and the second line uses Hamilton's equations and the equipartition formula. Summing over a system of \"N\" particles yields\n\nBy Newton's third law and the ideal gas assumption, the net force on the system is the force applied by the walls of their container, and this force is given by the pressure \"P\" of the gas. Hence\n\nwhere d\"S\" is the infinitesimal area element along the walls of the container. Since the divergence of the position vector q is\n\nthe divergence theorem implies that\n\nwhere d\"V\" is an infinitesimal volume within the container and \"V\" is the total volume of the container.\n\nPutting these equalities together yields\n\nwhich immediately implies the ideal gas law for \"N\" particles:\n\nwhere \"n\" = \"N\"/\"N\" is the number of moles of gas and \"R\" = \"N\"\"k\" is the gas constant. Although equipartition provides a simple derivation of the ideal-gas law and the internal energy, the same results can be obtained by an alternative method using the partition function.\n\nA diatomic gas can be modelled as two masses, \"m\" and \"m\", joined by a spring of stiffness \"a\", which is called the \"rigid rotor-harmonic oscillator approximation\". The classical energy of this system is\n\nwhere p and p are the momenta of the two atoms, and \"q\" is the deviation of the inter-atomic separation from its equilibrium value. Every degree of freedom in the energy is quadratic and, thus, should contribute \"k\"\"T\" to the total average energy, and \"k\" to the heat capacity. Therefore, the heat capacity of a gas of \"N\" diatomic molecules is predicted to be 7\"N\"·\"k\": the momenta p and p contribute three degrees of freedom each, and the extension \"q\" contributes the seventh. It follows that the heat capacity of a mole of diatomic molecules with no other degrees of freedom should be (7/2)\"N\"\"k\" = (7/2)\"R\" and, thus, the predicted molar heat capacity should be roughly 7 cal/(mol·K). However, the experimental values for molar heat capacities of diatomic gases are typically about 5 cal/(mol·K) and fall to 3 cal/(mol·K) at very low temperatures. This disagreement between the equipartition prediction and the experimental value of the molar heat capacity cannot be explained by using a more complex model of the molecule, since adding more degrees of freedom can only \"increase\" the predicted specific heat, not decrease it. This discrepancy was a key piece of evidence showing the need for a quantum theory of matter.\n\nEquipartition was used above to derive the classical ideal gas law from Newtonian mechanics. However, relativistic effects become dominant in some systems, such as white dwarfs and neutron stars, and the ideal gas equations must be modified. The equipartition theorem provides a convenient way to derive the corresponding laws for an extreme relativistic ideal gas. In such cases, the kinetic energy of a single particle is given by the formula\n\nTaking the derivative of \"H\" with respect to the \"p\" momentum component gives the formula\n\nand similarly for the \"p\" and \"p\" components. Adding the three components together gives\n\nwhere the last equality follows from the equipartition formula. Thus, the average total energy of an extreme relativistic gas is twice that of the non-relativistic case: for \"N\" particles, it is 3 \"Nk\"\"T\".\n\nIn an ideal gas the particles are assumed to interact only through collisions. The equipartition theorem may also be used to derive the energy and pressure of \"non-ideal gases\" in which the particles also interact with one another through conservative forces whose potential \"U\"(\"r\") depends only on the distance \"r\" between the particles. This situation can be described by first restricting attention to a single gas particle, and approximating the rest of the gas by a spherically symmetric distribution. It is then customary to introduce a radial distribution function \"g\"(\"r\") such that the probability density of finding another particle at a distance \"r\" from the given particle is equal to 4π\"r\"\"ρg\"(\"r\"), where \"ρ\" = \"N\"/\"V\" is the mean density of the gas. It follows that the mean potential energy associated to the interaction of the given particle with the rest of the gas is\n\nThe total mean potential energy of the gas is therefore formula_34, where \"N\" is the number of particles in the gas, and the factor is needed because summation over all the particles counts each interaction twice.\nAdding kinetic and potential energies, then applying equipartition, yields the \"energy equation\"\n\nA similar argument, can be used to derive the \"pressure equation\"\n\nAn anharmonic oscillator (in contrast to a simple harmonic oscillator) is one in which the potential energy is not quadratic in the extension \"q\" (the generalized position which measures the deviation of the system from equilibrium). Such oscillators provide a complementary point of view on the equipartition theorem. Simple examples are provided by potential energy functions of the form\n\nwhere \"C\" and \"s\" are arbitrary real constants. In these cases, the law of equipartition predicts that\n\nThus, the average potential energy equals \"k\"\"T\"/\"s\", not \"k\"\"T\"/2 as for the quadratic harmonic oscillator (where \"s\" = 2).\n\nMore generally, a typical energy function of a one-dimensional system has a Taylor expansion in the extension \"q\":\n\nfor non-negative integers \"n\". There is no \"n\" = 1 term, because at the equilibrium point, there is no net force and so the first derivative of the energy is zero. The \"n\" = 0 term need not be included, since the energy at the equilibrium position may be set to zero by convention. In this case, the law of equipartition predicts that\n\nIn contrast to the other examples cited here, the equipartition formula\ndoes \"not\" allow the average potential energy to be written in terms of known constants.\n\nThe equipartition theorem can be used to derive the Brownian motion of a particle from the Langevin equation. According to that equation, the motion of a particle of mass \"m\" with velocity v is governed by Newton's second law\n\nwhere F is a random force representing the random collisions of the particle and the surrounding molecules, and where the time constant τ reflects the drag force that opposes the particle's motion through the solution. The drag force is often written F = −γv; therefore, the time constant τ equals \"m\"/γ.\n\nThe dot product of this equation with the position vector r, after averaging, yields the equation\n\nfor Brownian motion (since the random force F is uncorrelated with the position r). Using the mathematical identities\n\nand\n\nthe basic equation for Brownian motion can be transformed into\n\nwhere the last equality follows from the equipartition theorem for translational kinetic energy:\n\nThe above differential equation for formula_48 (with suitable initial conditions) may be solved exactly:\n\nOn small time scales, with \"t\" « \"τ\", the particle acts as a freely moving particle: by the Taylor series of the exponential function, the squared distance grows approximately \"quadratically\":\n\nHowever, on long time scales, with \"t\" » \"τ\", the exponential and constant terms are negligible, and the squared distance grows only \"linearly\":\n\nThis describes the diffusion of the particle over time. An analogous equation for the rotational diffusion of a rigid molecule can be derived in a similar way.\n\nThe equipartition theorem and the related virial theorem have long been used as a tool in astrophysics. As examples, the virial theorem may be used to estimate stellar temperatures or the Chandrasekhar limit on the mass of white dwarf stars.\n\nThe average temperature of a star can be estimated from the equipartition theorem. Since most stars are spherically symmetric, the total gravitational potential energy can be estimated by integration\n\nwhere \"M\"(\"r\") is the mass within a radius \"r\" and \"ρ\"(\"r\") is the stellar density at radius \"r\"; \"G\" represents the gravitational constant and \"R\" the total radius of the star. Assuming a constant density throughout the star, this integration yields the formula\n\nwhere \"M\" is the star's total mass. Hence, the average potential energy of a single particle is\n\nwhere \"N\" is the number of particles in the star. Since most stars are composed mainly of ionized hydrogen, \"N\" equals roughly \"M\"/\"m\", where \"m\" is the mass of one proton. Application of the equipartition theorem gives an estimate of the star's temperature\n\nSubstitution of the mass and radius of the Sun yields an estimated solar temperature of \"T\" = 14 million kelvins, very close to its core temperature of 15 million kelvins. However, the Sun is much more complex than assumed by this model—both its temperature and density vary strongly with radius—and such excellent agreement (≈7% relative error) is partly fortuitous.\n\nThe same formulae may be applied to determining the conditions for star formation in giant molecular clouds. A local fluctuation in the density of such a cloud can lead to a runaway condition in which the cloud collapses inwards under its own gravity. Such a collapse occurs when the equipartition theorem—or, equivalently, the virial theorem—is no longer valid, i.e., when the gravitational potential energy exceeds twice the kinetic energy\n\nAssuming a constant density ρ for the cloud\n\nyields a minimum mass for stellar contraction, the Jeans mass \"M\"\n\nSubstituting the values typically observed in such clouds (\"T\" = 150 K, ρ = 2 g/cm) gives an estimated minimum mass of 17 solar masses, which is consistent with observed star formation. This effect is also known as the Jeans instability, after the British physicist James Hopwood Jeans who published it in 1902.\n\nThe original formulation of the equipartition theorem states that, in any physical system in thermal equilibrium, every particle has exactly the same average kinetic energy, (3/2)\"k\"\"T\". This may be shown using the Maxwell–Boltzmann distribution (see Figure 2), which is the probability distribution\n\nfor the speed of a particle of mass \"m\" in the system, where the speed \"v\" is the magnitude formula_60 of the velocity vector formula_61\n\nThe Maxwell–Boltzmann distribution applies to any system composed of atoms, and assumes only a canonical ensemble, specifically, that the kinetic energies are distributed according to their Boltzmann factor at a temperature \"T\". The average kinetic energy for a particle of mass \"m\" is then given by the integral formula\n\nas stated by the equipartition theorem. The same result can also be obtained by averaging the particle energy using the probability of finding the particle in certain quantum energy state.\n\nMore generally, the equipartition theorem states that any degree of freedom \"x\" which appears in the total energy \"H\" only as a simple quadratic term \"Ax\", where \"A\" is a constant, has an average energy of ½\"k\"\"T\" in thermal equilibrium. In this case the equipartition theorem may be derived from the partition function \"Z\"(\"β\"), where \"β\" = 1/(\"k\"\"T\") is the canonical inverse temperature. Integration over the variable \"x\" yields a factor\n\nin the formula for \"Z\". The mean energy associated with this factor is given by\n\nas stated by the equipartition theorem.\n\nGeneral derivations of the equipartition theorem can be found in many statistical mechanics textbooks, both for the microcanonical ensemble and for the canonical ensemble.\nThey involve taking averages over the phase space of the system, which is a symplectic manifold.\n\nTo explain these derivations, the following notation is introduced. First, the phase space is described in terms of generalized position coordinates \"q\" together with their conjugate momenta \"p\". The quantities \"q\" completely describe the configuration of the system, while the quantities (\"q\",\"p\") together completely describe its state.\n\nSecondly, the infinitesimal volume\n\nof the phase space is introduced and used to define the volume Σ(\"E\", Δ\"E\") of the portion of phase space where the energy \"H\" of the system lies between two limits, \"E\" and \"E\" + Δ\"E\":\n\nIn this expression, Δ\"E\" is assumed to be very small, Δ\"E\" « \"E\". Similarly, Ω(\"E\") is defined to be the total volume of phase space where the energy is less than \"E\":\n\nSince Δ\"E\" is very small, the following integrations are equivalent\n\nwhere the ellipses represent the integrand. From this, it follows that Γ is proportional to Δ\"E\"\n\nwhere \"ρ\"(\"E\") is the density of states. By the usual definitions of statistical mechanics, the entropy \"S\" equals \"k\" log \"Ω\"(\"E\"), and the temperature \"T\" is defined by\n\nIn the canonical ensemble, the system is in thermal equilibrium with an infinite heat bath at temperature \"T\" (in kelvins). The probability of each state in phase space is given by its Boltzmann factor times a normalization factor formula_71, which is chosen so that the probabilities sum to one\n\nwhere \"β\" = 1/\"k\"\"T\". Integration by parts for a phase-space variable \"x\" (which could be either \"q\" or \"p\") between two limits \"a\" and \"b\" yields the equation\n\nwhere d\"Γ\" = d\"Γ\"/d\"x\", i.e., the first integration is not carried out over \"x\". The first term is usually zero, either because \"x\" is zero at the limits, or because the energy goes to infinity at those limits. In that case, the equipartition theorem for the canonical ensemble follows immediately\n\nHere, the averaging symbolized by formula_75 is the ensemble average taken over the canonical ensemble.\n\nIn the microcanonical ensemble, the system is isolated from the rest of the world, or at least very weakly coupled to it. Hence, its total energy is effectively constant; to be definite, we say that the total energy \"H\" is confined between \"E\" and \"E\"+d\"E\". For a given energy \"E\" and spread d\"E\", there is a region of phase space Σ in which the system has that energy, and the probability of each state in that region of phase space is equal, by the definition of the microcanonical ensemble. Given these definitions, the equipartition average of phase-space variables \"x\" (which could be either \"q\"or \"p\") and \"x\" is given by\n\nwhere the last equality follows because \"E\" is a constant that does not depend on \"x\". Integrating by parts yields the relation\n\nsince the first term on the right hand side of the first line is zero (it can be rewritten as an integral of \"H\" − \"E\" on the hypersurface where \"H\" = \"E\").\n\nSubstitution of this result into the previous equation yields\n\nSince formula_79 the equipartition theorem follows:\n\nThus, we have derived the general formulation of the equipartition theorem\n\nwhich was so useful in the applications described above.\n\nThe law of equipartition holds only for ergodic systems in thermal equilibrium, which implies that all states with the same energy must be equally likely to be populated. Consequently, it must be possible to exchange energy among all its various forms within the system, or with an external heat bath in the canonical ensemble. The number of physical systems that have been rigorously proven to be ergodic is small; a famous example is the hard-sphere system of Yakov Sinai. The requirements for isolated systems to ensure ergodicity—and, thus equipartition—have been studied, and provided motivation for the modern chaos theory of dynamical systems. A chaotic Hamiltonian system need not be ergodic, although that is usually a good assumption.\n\nA commonly cited counter-example where energy is \"not\" shared among its various forms and where equipartition does \"not\" hold in the microcanonical ensemble is a system of coupled harmonic oscillators. If the system is isolated from the rest of the world, the energy in each normal mode is constant; energy is not transferred from one mode to another. Hence, equipartition does not hold for such a system; the amount of energy in each normal mode is fixed at its initial value. If sufficiently strong nonlinear terms are present in the energy function, energy may be transferred between the normal modes, leading to ergodicity and rendering the law of equipartition valid. However, the Kolmogorov–Arnold–Moser theorem states that energy will not be exchanged unless the nonlinear perturbations are strong enough; if they are too small, the energy will remain trapped in at least some of the modes.\n\nAnother way ergodicity can be broken is by the existence of nonlinear soliton symmetries. In 1953, Fermi, Pasta, Ulam and Tsingou conducted computer simulations of a vibrating string that included a non-linear term (quadratic in one test, cubic in another, and a piecewise linear approximation to a cubic in a third). They found that the behavior of the system was quite different from what intuition based on equipartition would have led them to expect. Instead of the energies in the modes becoming equally shared, the system exhibited a very complicated quasi-periodic behavior. This puzzling result was eventually explained by Kruskal and Zabusky in 1965 in a paper which, by connecting the simulated system to the Korteweg–de Vries equation led to the development of soliton mathematics.\n\nThe law of equipartition breaks down when the thermal energy \"kT\" is significantly smaller than the spacing between energy levels. Equipartition no longer holds because it is a poor approximation to assume that the energy levels form a smooth continuum, which is required in the derivations of the equipartition theorem above. Historically, the failures of the classical equipartition theorem to explain specific heats and blackbody radiation were critical in showing the need for a new theory of matter and radiation, namely, quantum mechanics and quantum field theory.\n\nTo illustrate the breakdown of equipartition, consider the average energy in a single (quantum) harmonic oscillator, which was discussed above for the classical case. Neglecting the irrelevant zero-point energy term, its quantum energy levels are given by \"E = nhν\", where \"h\" is the Planck constant, \"ν\" is the fundamental frequency of the oscillator, and \"n\" is an integer. The probability of a given energy level being populated in the canonical ensemble is given by its Boltzmann factor\n\nwhere \"β\" = 1/\"k\"\"T\" and the denominator \"Z\" is the partition function, here a geometric series\n\nIts average energy is given by\n\nSubstituting the formula for \"Z\" gives the final result\n\nAt high temperatures, when the thermal energy \"k\"\"T\" is much greater than the spacing \"hν\" between energy levels, the exponential argument \"βhν\" is much less than one and the average energy becomes \"k\"\"T\", in agreement with the equipartition theorem (Figure 10). However, at low temperatures, when \"hν\" » \"k\"\"T\", the average energy goes to zero—the higher-frequency energy levels are \"frozen out\" (Figure 10). As another example, the internal excited electronic states of a hydrogen atom do not contribute to its specific heat as a gas at room temperature, since the thermal energy \"k\"\"T\" (roughly 0.025 eV) is much smaller than the spacing between the lowest and next higher electronic energy levels (roughly 10 eV).\n\nSimilar considerations apply whenever the energy level spacing is much larger than the thermal energy. This reasoning was used by Max Planck and Albert Einstein, among others, to resolve the ultraviolet catastrophe of blackbody radiation. The paradox arises because there are an infinite number of independent modes of the electromagnetic field in a closed container, each of which may be treated as a harmonic oscillator. If each electromagnetic mode were to have an average energy \"k\"\"T\", there would be an infinite amount of energy in the container. However, by the reasoning above, the average energy in the higher-frequency modes goes to zero as \"ν\" goes to infinity; moreover, Planck's law of black body radiation, which describes the experimental distribution of energy in the modes, follows from the same reasoning.\n\nOther, more subtle quantum effects can lead to corrections to equipartition, such as identical particles and continuous symmetries. The effects of identical particles can be dominant at very high densities and low temperatures. For example, the valence electrons in a metal can have a mean kinetic energy of a few electronvolts, which would normally correspond to a temperature of tens of thousands of kelvins. Such a state, in which the density is high enough that the Pauli exclusion principle invalidates the classical approach, is called a degenerate fermion gas. Such gases are important for the structure of white dwarf and neutron stars. At low temperatures, a fermionic analogue of the Bose–Einstein condensate (in which a large number of identical particles occupy the lowest-energy state) can form; such superfluid electrons are responsible for superconductivity.\n\n\n\n"}
{"id": "838256", "url": "https://en.wikipedia.org/wiki?curid=838256", "title": "Figurational sociology", "text": "Figurational sociology\n\nFigurational sociology is a research tradition in which figurations of humans—evolving networks of interdependent humans—are the unit of investigation. Although more a methodological stance than a determinate school of practice, the tradition has one essential feature:\n\n\nPractitioners may be said to be inspired by the ideal that the usual humanities barrier between micro (e.g. psychological) and macro (e.g. state organization) is removed, and their causal links opened to examination. As a consequence, much of the work done in the name of this approach has examined the connection between changes in psychology and personhood, on the one hand, and changes in macro social structures on the other. \n\nNorbert Elias is usually acknowledged as an early or primary practitioner, as a consequence of his ground-breaking 1939 work, \"The Civilizing Process\".\n\n"}
{"id": "9267447", "url": "https://en.wikipedia.org/wiki?curid=9267447", "title": "Gittins index", "text": "Gittins index\n\nThe Gittins index is a measure of the reward that can be achieved through a given stochastic process with certain properties, namely: the process has an ultimate termination state and evolves with an option, at each intermediate state, of terminating. Upon terminating at a given state, the reward achieved is the sum of the probabilistic expected rewards associated with every state from the actual terminating state to the ultimate terminal state, inclusive. The index is a real scalar.\n\nTo illustrate the theory we can take two examples from a developing sector, such as from electricity generating technologies: wind power and wave power. If we are presented with the two technologies when they are both proposed as ideas we cannot say which will be better in the long run as we have no data, as yet, to base our judgments on. It would be easy to say that wave power would be too problematic to develop as it seems easier to put up lots of wind turbines than to make the long floating generators, tow them out to sea and lay the cables necessary.\n\nIf we were to make a judgment call at that early time in development we could be condemning one technology to being put on the shelf and the other would be developed and put into operation. If we develop both technologies we would be able to make a judgment call on each by comparing the progress of each technology at a set time interval such as every three months. The decisions we make about investment in the next stage would be based on those results.\n\nIn a paper in 1979 called \"Bandit Processes and Dynamic Allocation Indices\" John C. Gittins suggests a solution for problems such as this. He takes the two basic functions of a \"Scheduling Problem\" and a \"Multi-armed bandit\" problem and shows how these problems can be solved using \"Dynamic allocation indices\". He first takes the \"Scheduling Problem\" and reduces it to a machine which has to perform jobs and has a set time period, every hour or day for example, to finish each job in. The machine is given a reward value, based on finishing or not within the time period, and a probability value of whether it will finish or not for each job is calculated. The problem is \"to decide which job to process next at each stage so as to maximize the total expected reward.\" He then moves on to the \"Multi–armed bandit problem\" where each pull on a \"one armed bandit\" lever is allocated a reward function for a successful pull, and a zero reward for an unsuccessful pull. The sequence of successes forms a Bernoulli process and has an unknown probability of success. There are multiple \"bandits\" and the distribution of successful pulls is calculated and different for each machine. Gittins states that the problem here is \"to decide which arm to pull next at each stage so as to maximize the total expected reward from an infinite sequence of pulls.\"\n\nGittins says that \"Both the problems described above involve a sequence of decisions, each of which is based on more information than its predecessors, and these both problems may be tackled by dynamic allocation indices.\"\n\nIn applied mathematics, the \"Gittins index\" is a real scalar value associated to the state of a stochastic process with a reward function and with a probability of termination. It is a measure of the reward that can be achieved by the process evolving from that state on, under the probability that it will be terminated in future. The \"index policy\" induced by the Gittins index, consisting of choosing at any time the stochastic process with the currently highest Gittins index, is the solution of some \"stopping problems\" such as the one of dynamic allocation, where a decision-maker has to maximize the total reward by distributing a limited amount of effort to a number of competing projects, each returning a stochastic reward. If the projects are independent from each other and only one project at a time may evolve, the problem is called multi-armed bandit (one type of Stochastic scheduling problems) and the Gittins index policy is optimal. If multiple projects can evolve, the problem is called \"Restless bandit\" and the Gittins index policy is a known good heuristic but no optimal solution exists in general. In fact, in general this problem is NP-complete and it is generally accepted that no feasible solution can be found.\n\nQuestions about the optimal stopping policies in the context of clinical trials have been open from the 1940s and in the 1960s a few authors analyzed simple models leading to optimal index policies, but it was only in the 1970s that Gittins and his collaborators demonstrated in a markovian framework that the optimal solution of the general case is an index policy whose \"dynamic allocation index\" is computable in principle for every state of each project as a function of the single project's dynamics.\n\nSoon after the seminal paper of Gittins, Peter Whittle\ndemonstrated that the index emerges as a lagrangian multiplier from a dynamic programming formulation of the problem called \"retirement process\" and conjectured that the same index would be a good heuristic in a more general setup named \"Restless bandit\". The question of how to actually calculate the index for Markov chains was first addressed by Varaiya and his collaborators with an algorithm that computes the indexes\nfrom the largest first down to the smallest and by Chen and Katehakis who showed that standard LP could be used to calculate the index of a state without requiring its calculation for all states with higher index values.\nLCM Kallenberg provided a parametric LP implementation to compute the indices for all states of a Markov chain. Further, Katehakis and Veinott demonstrated that the index is the expected reward of a Markov decision process constructed over the Markov chain and known as \"Restart in State\" and can be calculated exactly by solving that problem with the \"policy iteration\" algorithm, or approximately with the \"value iteration\" algorithm. \nThis approach also has the advantage of calculating the index for one specific state without having to calculate all the greater indexes and it is valid under more general state space conditions. A faster algorithm for the calculation of all indices has been obtained in 2004 by Sonin as a consequence of his \"elimination algorithm\" for the optimal stopping of a Markov chain. In this algorithm the termination probability of the process may depend on the current state rather than being a fixed factor. A faster algorithm has been proposed in 2007 by Niño-Mora by exploiting the structure of a parametric simplex to reduce the computational effort of the pivot steps and thereby achieving the same complexity as the Gaussian elimination algorithm.\nCowan, W. and Katehakis (2014), provide a solution to the problem, with potentially non-Markovian, uncountable state space reward processes, under frameworks in which, either the discount factors may be non-uniform and vary over time, or the periods of activation of each bandit may be not be fixed or uniform, subject instead to a possibly stochastic duration of activation before a change to a different bandit is allowed. The solution is based on generalized restart-in-state indices.\n\nThe classical definition by Gittins et al. is:\n\nwhere formula_2 is a stochastic process, formula_3 is the\nutility (also called reward) associated to the discrete state formula_4,\nformula_5 is the probability that the stochastic process does not\nterminate, and formula_6 is the conditional expectation\noperator given \"c\":\n\nwith formula_8 being the range of \"X\".\n\nThe dynamic programming formulation in terms of retirement process, given by Whittle, is:\n\nwhere formula_10 is the \"value function\"\n\nwith the same notation as above. It holds that\n\nIf formula_2 is a Markov chain with rewards, the interpretation of Katehakis and Veinott (1987) associates to every state the action of restarting from one arbitrary state formula_4, thereby constructing a Markov decision process formula_15.\n\nThe Gittins Index of that state formula_4 is the highest total reward which can be achieved on formula_15 if one can always choose to continue or restart from that state formula_4.\n\nwhere formula_20 indicates a policy over formula_15. It holds that\n\nIf the probability of survival formula_23 depends on the state formula_4, a generalization introduced by Sonin (2008) defines the Gittins index formula_25 as the maximum discounted total reward per chance of termination.\n\nwhere\n\nIf formula_29 is replaced by formula_30 in the definitions of formula_31, formula_32 and formula_33, then it holds that\nthis observation leads Sonin to conclude that formula_25 and not formula_31 is the \"true meaning\" of the Gittins index.\n\nIn queueing theory, Gittins index is used to determine the optimal scheduling of jobs, e.g., in an M/G/1 queue. The mean completion time of jobs under a Gittins index schedule can be determined using the SOAP approach. Note that the dynamics of the queue are intrinsically Markovian, and stochasticity is due to the arrival and service processes. This is in contrast to most of the works in the learning literature, where stochasticity is explicitly accounted through a noise term.\n\n"}
{"id": "2017677", "url": "https://en.wikipedia.org/wiki?curid=2017677", "title": "Heinrich Wankel", "text": "Heinrich Wankel\n\nHeinrich Wankel (Czech: Jindřich Wankel; July 15, 1821, Prague – April 5, 1897, Olomouc) was a Bohemian palaeontologist and archaeologist.\n\nWankel was born to Damian Wankel, a clerk, and his wife Magdalena, née Schwarz in a bilingual environment. He was attending German schools in Prague and later studied Medicine at the University of Prague as a student of Josef Hyrtl.\n\nHe came to work into area of Moravský kras (\"Moravian Karst\", today Czech Republic) in 1847 and since 1849 lived in Blansko as a medical doctor. He started with geological exploration of the area and later with palaeontology, archaeology and anthropology.\n\nIn 1850, in Blansko, he set up first ever laboratory to research fossil bones from Cenozoic Era where he assembled complete skeleton of cave bear (until then such bones were used for spodium in nearby sugar refinery ). His most famous discovery (1872) was burial site of a nobleman from Bronze Age at \"Býčí skála\", with skeletons of 40 ritually killed young women (, ).\n\nHis grandson Karel Absolon was also famous archaeologist and worked in the same area.\n\n\n"}
{"id": "2072213", "url": "https://en.wikipedia.org/wiki?curid=2072213", "title": "Human Genetic Diversity: Lewontin's Fallacy", "text": "Human Genetic Diversity: Lewontin's Fallacy\n\n\"Human Genetic Diversity: Lewontin's Fallacy\" is a 2003 paper by A. W. F. Edwards. He criticises an argument first made by Richard Lewontin in his 1972 article \"The Apportionment of Human Diversity\", which argued that division of humanity into races is taxonomically invalid. Edwards' critique is discussed in a number of academic and popular science books, with varying degrees of support.\n\nIn the 1972 study \"The Apportionment of Human Diversity\", Richard Lewontin performed a fixation index (\"F\") statistical analysis using 17 markers, including blood group proteins, from individuals across classically defined \"races\" (Caucasian, African, Mongoloid, South Asian Aborigines, Amerinds, Oceanians, and Australian Aborigines). He found that the majority of the total genetic variation between humans (i.e., of the 0.1% of DNA that varies between individuals), 85.4%, is found within populations, 8.3% of the variation is found between populations within a \"race\", and only 6.3% was found to account for the racial classification. Numerous later studies have confirmed his findings. Based on this analysis, Lewontin concluded, \"Since such racial classification is now seen to be of virtually no genetic or taxonomic significance either, no justification can be offered for its continuance.\"\n\nThis argument has been cited as evidence that racial categories are biologically meaningless, and that behavioral differences between groups cannot have any genetic underpinnings. One example is the \"Statement on 'Race'\" published by the American Anthropological Association in 1998, which rejected the existence of races as unambiguous, clearly demarcated, biologically distinct groups.\n\nEdwards argued that while Lewontin's statements on variability are correct when examining the frequency of different alleles (variants of a particular gene) at an individual locus (the location of a particular gene) between individuals, it is nonetheless possible to classify individuals into different racial groups with an accuracy that approaches 100 percent when one takes into account the frequency of the alleles at several loci at the same time. This happens because differences in the frequency of alleles at different loci are correlated across populations—the alleles that are more frequent in a population at two or more loci are correlated when we consider the two populations simultaneously. Or in other words, the frequency of the alleles tends to cluster differently for different populations.\n\nIn Edwards's words, \"most of the information that distinguishes populations is hidden in the correlation structure of the data.\" These relationships can be extracted using commonly used ordination and cluster analysis techniques. Edwards argued that, even if the probability of misclassifying an individual based on the frequency of alleles at a single locus is as high as 30 percent (as Lewontin reported in 1972), the misclassification probability becomes close to zero if enough loci are studied.\n\nEdwards's paper stated that the underlying logic was discussed in the early years of the 20th century. Edwards wrote that he and Luigi Luca Cavalli-Sforza had presented a contrasting analysis to Lewontin's, using very similar data, already at the 1963 International Congress of Genetics. Lewontin participated in the conference but did not refer to this in his later paper. Edwards argued that Lewontin used his analysis to attack human classification in science for social reasons.\n\nEvolutionary biologist Richard Dawkins discusses genetic variation across human races in his book \"The Ancestor's Tale\". In the chapter \"The Grasshopper's Tale\", he characterizes the genetic variation between races as a very small fraction of the total human genetic variation. He goes on to disagree with Lewontin's conclusions about taxonomy, writing, \"However small the racial partition of the total variation may be, if such racial characteristics as there are highly correlate with other racial characteristics, they are by definition informative, and therefore of taxonomic significance.\" Neven Sesardic has argued that, unbeknownst to Edwards, Jeffry B. Mitton already made the same argument about Lewontin's claim in two articles published in \"The American Naturalist\" in the late 1970s.\nSimilarly, biological anthropologist Jonathan Marks agrees with Edwards that correlations between geographical areas and genetics obviously exist in human populations, but goes on to note that \"What is unclear is what this has to do with 'race' as that term has been used through much in the twentieth century—the mere fact that we can find groups to be different and can reliably allot people to them is trivial. Again, the point of the theory of race was to discover large clusters of people that are principally homogeneous within and heterogeneous between, contrasting groups. Lewontin's analysis shows that such groups do not exist in the human species, and Edwards' critique does not contradict that interpretation.\"\n\nThe view that, while geographic clustering of biological traits does exist, this does not lend biological validity to racial groups, was proposed by several evolutionary anthropologists and geneticists prior to the publication of Edwards critique of Lewontin.\n\nIn the 2007 paper \"Genetic Similarities Within and Between Human Populations\", Witherspoon \"et al.\" attempt to answer the question, \"How often is a pair of individuals from one population genetically more dissimilar than two individuals chosen from two different populations?\". The answer depends on the number of polymorphisms used to define that dissimilarity, and the populations being compared. When they analysed three geographically distinct populations (European, African and East Asian) and measured genetic similarity over many thousands of loci, the answer to their question was \"never\". However, measuring similarity using smaller numbers of loci yielded substantial overlap between these populations. Rates of between-population similarity also increased when geographically intermediate and admixed populations were included in the analysis.\n\n"}
{"id": "52893218", "url": "https://en.wikipedia.org/wiki?curid=52893218", "title": "Industrial microbiology", "text": "Industrial microbiology\n\nIndustrial microbiology is a branch of biotechnology that applies microbial sciences to create industrial products in mass quantities. There are multiple ways to manipulate a microorganism in order to increase maximum product yields. Introduction of mutations into an organism may be accomplished by introducing them to mutagens. Another way to increase production is by gene amplification, this is done by the use of plasmids, and vectors. The plasmids and/ or vectors are used to incorporate multiple copies of a specific gene that would allow more enzymes to be produced that eventually cause more product yield. The manipulation of organisms in order to yield a specific product has many applications to the real world like the production of some antibiotics, vitamins, enzymes, amino acids, solvents, alcohol and daily products. They can also be used in an agricultural application and use them as a biopesticede instead of using dangerous chemicals or as inoculants and help plant proliferation.\n\nThe medical application to industrial microbiology is the production of new drugs synthesized in a specific organism for medical purposes. Production of antibiotics is necessary for the treatment of many bacterial infections.Some natural occurring antibiotics and precursors, are produced through a process called fermentation. The microorganisms grow in a liquid media where the population size is controlled in order to yield the greatest amount of product. In this environment nutrient, pH, temperature, and oxygen are controlled also in order to maximize the amount of cells and cause them not to die before the production of the antibiotic of interest. Once the antibiotic is produced it must be extracted in order to yield an income.\n\nVitamins also get produced in massive quantities either by fermentation or biotransformation. Vitamin B 2 (riboflavin) for example is produced both ways. Biotransformation is mostly used for the production of riboflavin, and the carbon source starting material for this reaction is glucose. There are a few strains of microorganisms that were engineered to increase the yield of riboflavin produced. The most common organism used for this reaction is \"Ashbya gossypii\". The fermentation process is another common way to produce riboflavin. The most common organism used for production of riboflavin through fermentation is \"Eremothecium ashbyii\". Once riboflavin is produced it must be extracted from the broth, this is done by heating the cells for a certain amount of time, and then the cells can be filtered out of solution. Riboflavin is later purified and released as final product.\n\nEnzymes can be produced through fermentation either by submerged fermentation and/ or by solid state fermentation. Submerged fermentation is referred to when the microorganisms are in contact with media. In this process the contact with oxygen is essential. The bioreactors/fermentors that are used to do these mass production of product can store up to 500 cubic meters in volume. Solid state fermentation is less common than submerged fermentation, but has many benefits. There is less need for the environment to be sterile since there is less water, there is a higher stability and concentration for the end product. Insulin synthesis is done through the fermentation process and the use of recombinant \"E.coli\" or yeast in order to make human insulin also called Humulin\".\"\n\n_Fermentation is a reaction where sugar can be converted into a gas, alcohols or acids. _Microorganisms like yeast and bacteria are used to massively produce the many things. Drinking alcohol also known as ethanol is produced by yeast and bacteria.\n_Ethanol can also be used as a fuel source. _The drinking alcohol is produced from natural sugars like glucose. \n_Carbon dioxide is produced as a side product in this reaction and can be used to make bread, and can also be used to carbonate beverages.\nFermentationWine :\n_Alcoholic beverages like beer and wine are fermented by microorganisms when there is no oxygen present.\n\n_Most yeast can tolerate between 10 and 15 percent alcohol, but there are some strains that can tolerate up to 21 percent alcohol.\n_Dairy products like cheese and yogurt can also be made through fermentation using microbes. \nCheese was produced as a way to preserve the nutrients obtained from milk, through\n\nfermentation thus elongating the shelf-life of the product. \n_Microbes are used to convert the lactose sugars into lactic acid through fermentation. _The bacteria used for such fermentation are usually from \"Lactococci\", \"Lactobacilli\", or \"Streptococci\" families.\n_Sometimes these microbes are added before or after the acidification step needed for cheese production. \n_Also these microbes are responsible for the different flavors of cheese, since they have enzymes that breakdown milk sugars and fats into multiple building blocks. \n\n_The production of yogurt starts from the pasteurization of milk, where undesired microbes are reduced or eliminated. \n_Once the milk is pasteurized the milk is ready to be processed to reduce fat and liquid content, so what remains is mostly solid content. \n_This can be done by drying the milk so that the liquid evaporates or by adding concentrated milk. \n_Increasing the solid content of the milk also increases the nutritional value since the nutrients are more concentrated.\n_After this step is accomplished, the milk is ready for fermentation where the milk gets inoculated with bacteria in hygienic stainless steel containers and then gets carefully monitored for lactic acid production, temperature and pH.\n\nBiopesticide is a pesticide derivatized from a living organism or natural occurring substances. Biochemical pesticides can also be produced from naturally occurring substances that can control pest populations in a non-toxic matter. An example of a biochemical pesticide is garlic and pepper based insecticides, these work by repelling insects from the desired location. Microbial pesticides, usually a virus, bacterium, or fungus are used to control pest populations in a more specific manner. The most commonly used microbe for the production of microbial bio-pesticides is \"Bacillus thuringiensis\", also known as Bt. This spore forming bacterium produces a delta-endotoxins in which it causes the insect or pest to stop feeding on the crop or plant because the endotoxin destroys the lining of the digestive system. Another mechanism that is used to reduce plant pathogens is by introducing other microbes that are non-pathogenic but compete for the rhizosphere, and succeed by producing anti fungal chemicals yielding plant growth.\n\nMicrobial inoculants are addition of microbes into a plant that would essentially help the plant grow by introducing nutrients, and stimulating plant growth. The preparation in mass quantities of any inoculum is performed by a process called fermentation. The first step to making it is by selecting a microbial strain, and letting it grow and increase in bacterial concentration. The greater the bacterial concentration the greater the fermentation yield would be (fermentation yield is ratio of bacterial concentration to mass of substrate) The bacterial concentration can be measured by monitoring the turbidity, wet or dry weight, or residual nutrient concentration. After the inoculant is ready then it gets transferred in a fermentor where oxygen and temperature most be highly monitored for the survival of the microbes and it can vary depending on the microbe that is being used. After enough time has passed for the microbes to be properly incubated the product is ready to be extracted purified and packaged.\nSynthesis of amino acids and organic solvents can also be made using microbes. The synthesis of essential amino acids such as are L-Methionine, L-Lysine, L-Tryptophan and the non-essential amino acid L-Glutamic acid are used today mainly for feed, food, and pharmaceutical industries. The production of these amino acids is due to \"Corynebacterium glutamicum\" and fermentation. \"C.glutamicum\" was engineered to be able to produce L-lysine and L-Glutamic acid is large quantities. L-Glutamic acid had a high demand for production because this amino acid is used to produce Monosodium glutamate (MSG) a food flavoring agent. In 2012 the total production of L-Glutamic acid was 2.2 million tons and is produced using a submerged fermentation technique inoculated with \"C.glutamicum.\" L-Lysine was originally produced from diaminopimelic acid (DAP) by \"E.coli\", but once the \"C.glutamicum\" was discovered for the production of L-Glutamic acid. This organism and other autotrophs were later modified to yield other amino acids such as lysine, aspartate, methionine, isoleucine and threonine. L-Lysine is used for the feeding of pigs and chicken, as well as to treat nutrient deficiency, increase energy in a patient, and sometimes used to treat viral infections. L-Tryptophan is also produced through fermentation and by \"Corynebacterium\" and \"E.coli,\" though the production is not as large as the rest of the amino acids it is still produced for pharmaceutical purposes since it can be converted and used to produce neurotransmitters.\n\nThe production of organic solvents like acetone, butanol, and isopropanol through fermentation was one of the very first things to be produced by using bacteria, since achieving the necessary chirality of the products is easily achieved by using living systems. Solvent fermentation uses a series of \"Clostridia\" bacterial species. Solvent fermentation at first was not as productive as it is used today. The amount of bacteria required to yield a product was high, and the actual yield of product was low. Later technological advances were discovered that allowed scientist to genetically alter these strains to achieve a higher yield for these solvents. These Clostridial strains were transformed to have extra gene copies of enzymes necessary for solvent production, as well as being more tolerant to higher concentrations of the solvent being produced, since these bacteria have a range of product in which they can survive in before the environment becomes toxic. Yielding more strains that can use other subtrates was also another way to increase the productivity of these bacteria.\n"}
{"id": "56864579", "url": "https://en.wikipedia.org/wiki?curid=56864579", "title": "Katie Hinde", "text": "Katie Hinde\n\nKatherine (Katie) Hinde is a Professor of Evolutionary Biology and Senior Sustainability Scientist at Arizona State University, where she researches lactation. She is also a science writer and science communicator.\n\nHinde attended Seattle Central College and was part of the Running Start and College Transfer programs. She earned a bachelor's of arts in anthropology from the University of Washington in 1999. She joined University of California, Los Angeles for her doctoral studies, where she was awarded the Chancellor’s Dissertation Fellowship in 2007. She completed her PhD at UCLA in 2008.\n\nHinde served as a postdoctoral scholar in Neuroscience in the Brain, Mind, and Behavior Unit of California National Primate Research Center at UC Davis until 2009. She joined Harvard University as an Assistant Professor in 2011, where she remained until 2015.\n\nHinde is now the Director of the Comparative Lactation Lab at Arizona State University. Here she investigates the hormones, food and medicine contained within mother's milk. She argues that we know twice as much about erectile dysfunction than we do about breast milk. Hinde identified that the combination of fat, protein, mineral, sugar, bacteria and hormones contained within mother's milk are equivalent to fingerprints and influence infant outcomes from postnatal life to adulthood. Human breast milke contains oligosaccharides, of which there are more than 200 varieties. These cannot be digested by babies, but instead provide the right community of microbes to prevent pathogens from establishing. Hinde identified that the milk of young monkey mothers contained fewer calories but more of the stress hormone cortisol than that of their older counterparts. She found that more cortisol contributes to infants that are more active and playful, as well as infants who are better at coping in stressful situations.\n\nShe is recognised as a young researcher who has made outstanding, original scientific contributions to the study of human milk. Hinde is a member of the Executive Council of the International Society for Research in Human Milk and Lactation. Her research has been featured in \"National Geographic, Slate (magazine), Science News, The Washington Times\" and \"The New York Times\". She speaks regularly at international conferences.\n\nIn 2011 Hinde began the popular science blog \"Mammals Suck ... Milk!\", which has since had over one million views. She is associate editor of Splash! Milk Science Update. She created Mammal March Madness in 2013, a month of science outreach events used in classrooms across America. In 2014 she wrote Building Babies. She appeared on the Center for Academic Research and Training in Anthropogeny YouTube channel, discussing Childrearing in Human Evolution. She was a guest on the comedy show \"You're the Expert\" with Wyatt Cenac. In 2016 Hinde was named the Milk Maven in GRIST 50, an annual list of innovators who are working toward a more sustainable future. She part of the SAFE13 study, which looks to expose how widespread sexual harassment and assault are in scientific fieldwork. In 2017 Hinde delivered a TED talk \"What we don't know about mother's milk\".\n\n2016 – Ehrlich-Koldovsky Early Career Award, International Society for Research in Human Milk & Lactation\n\n2016 – Sustainability Innovators, Organizers, & Visionaries #Grist50 Grist Magazine\n\n2014 – Early Career Achievement Award, American Society of Primatologists\n\n2014 – Distinguished Alumni Award, Seattle Central College\n\n2013 – Most Valuable Presentation Award 10th Annual Milk Genomics and Human Health Meeting\n"}
{"id": "58710832", "url": "https://en.wikipedia.org/wiki?curid=58710832", "title": "Lionel Naccache", "text": "Lionel Naccache\n\nLionel Naccache (born 27 March 1969 in Sarcelles) is a French neurologist and specialist in cognitive neuroscience.\n"}
{"id": "356369", "url": "https://en.wikipedia.org/wiki?curid=356369", "title": "List of DOS commands", "text": "List of DOS commands\n\nThis article presents a list of commands used by DOS operating systems, especially as used on x86-based IBM PC compatibles (PCs). Other DOS operating systems are not part of the scope of this list.\n\nIn DOS, many standard system commands were provided for common tasks such as listing files on a disk or moving files. Some commands were built into the command interpreter, others existed as external commands on disk. Over the several generations of DOS, commands were added for the additional functions of the operating system. In the current Microsoft Windows operating system, a text-mode command prompt window, cmd.exe, can still be used.\n\nThe command interpreter for DOS runs when no application programs are running. When an application exits, if the transient portion of the command interpreter in memory was overwritten, DOS will reload it from disk. Some commands are internal — built into COMMAND.COM; others are external commands stored on disk. When the user types a line of text at the operating system command prompt, COMMAND.COM will parse the line and attempt to match a command name to a built-in command or to the name of an executable program file or batch file on disk. If no match is found, an error message is printed, and the command prompt is refreshed.\n\nExternal commands were too large to keep in the command processor, or were less frequently used. Such utility programs would be stored on disk and loaded just like regular application programs but were distributed with the operating system. Copies of these utility command programs had to be on an accessible disk, either on the current drive or on the command path set in the command interpreter.\n\nIn the list below, commands that can accept more than one file name, or a filename including wildcards (* and ?), are said to accept a \"filespec\" (file specification) parameter. Commands that can accept only a single file name are said to accept a \"filename\" parameter. Additionally, command line switches, or other parameter strings, can be supplied on the command line. Spaces and symbols such as a \"/\" or a \"-\" may be used to allow the command processor to parse the command line into filenames, file specifications, and other options.\n\nThe command interpreter preserves the case of whatever parameters are passed to commands, but the command names themselves and file names are case-insensitive.\n\nMany commands are the same across many DOS systems, but some differ in command syntax or name.\n\nA partial list of the most common commands for MS-DOS follows below.\n\nSets the path to be searched for data files or displays the current search path. \nThe APPEND command is similar to the PATH command that tells DOS where to search for program files (files with a .COM, . EXE, or .BAT file name extension).\n\nThe command redirects requests for disk operations on one drive to a different drive. It can also display drive assignments or reset all drive letters to their original assignments. The command is available in MS-DOS 5.00.\n\nAttrib changes or views the attributes of one or more files. It defaults to display the attributes of all files in the current directory. The file attributes available include read-only, archive, system, and hidden attributes. The command has the capability to process whole folders and subfolders of files,\n\nThese are commands to backup and restore files from an external disk. These appeared in version 2, and continued to PC DOS 5 and MS-DOS 6 (PC DOS 7 had a deversioned check). In DOS 6, these were replaced by commercial programs (CPBACKUP, MSBACKUP), which allowed files to be restored to different locations.\n\nAn implementation of the BASIC programming language for PCs. Implementing BASIC in this way was very common in operating systems on 8- and 16-bit machines made in the 1980s.\n\nIBM computers had BASIC 1.1 in ROM, and IBM's versions of BASIC used code in this ROM-BASIC, which allowed for extra memory in the code area. BASICA last appeared in IBM PC DOS 5.02, and in OS/2 (2.0 and later), the version had ROM-BASIC moved into the program code.\n\nMicrosoft released GW-BASIC for machines with no ROM-BASIC. Some OEM releases had basic.com and basica.com as loaders for GW-BASIC.EXE.\n\nBASIC was dropped after MS-DOS 4, and PC DOS 5.02. OS/2 (which uses PC DOS 5), has it, while MS-DOS 5 does not.\n\nStarts a batch file from within another batch file and returns when that one ends.\n\nThe CHDIR (or the alternative name CD) command either displays or changes the current working directory.\n\nThe command either displays or changes the active code page used to display character glyphs in a console window.\n\nCHKDSK verifies a storage volume (for example, a hard disk, disk partition or floppy disk) for file system integrity. The command has the ability to fix errors on a volume and recover information from defective disk sectors of a volume.\n\nThe CHOICE command is used in batch files to prompt the user to select one item from a set of single-character \"choices\". Choice was introduced as an external command with MS-DOS 6.0; Novell DOS 7 and PC DOS 7.0. Earlier versions of DR DOS supported this function with the built-in \"switch\" command (for numeric choices) or by beginning a command with a question mark. This command was formerly called ync (yes-no-cancel).\n\nThe CLS or CLRSCR command clears the terminal screen.\n\nShow differences between any two files, or any two sets of files.\n\nCopies files from one location to another. It is used to make copies of existing files. This command can be used to combine multiple files into target files. The destination defaults to the current directory. If multiple source files are indicated, the destination must be a directory, or an error will result. COPY has the ability to concatenate files. The command can copy in text mode or binary mode; in text mode, codice_1 will stop when it reaches the EOF character; in binary mode, the files will be concatenated in their entirety, ignoring EOF characters.\n\nFiles may be copied to devices. For example, codice_2 outputs \"file\" to the screen console. Devices themselves may be copied to a destination file, for example, codice_3 takes the text typed into the console and puts it into \"FILE\", stopping when EOF (Ctrl+Z) is typed.\n\nDefines the terminal device (for example, COM1) to use for input and output.\n\nDisplays the system date and prompts the user to enter a new date. Complements the TIME command.\n\nA very primitive assembler and disassambler.\n\nThe command has the ability to analyze the file fragmentation on a disk drive or to defragment a drive. This command is called DEFRAG in MS-DOS/PC DOS and diskopt in DR-DOS.\n\nDEL (or the alternative form ERASE) is used to delete one or more files.\n\nDeletes a directory along with all of the files and subdirectories that it contains. Normally, it will ask for confirmation of the potentially dangerous action. We know that the RD(RMDIR) command can not delete a directory if the directory is not empty. DELTREE command can be used to delete the whole directory if the directory is not empty.\n\nThe codice_4 command is included in certain versions of Microsoft Windows and Microsoft DOS Operating Systems. It is specifically available only in versions of MS-DOS 6.0 and higher, and in Microsoft Windows 9x. In Windows NT, the functionality provided exists but is handled by the command \"rd\" or \"rmdir\" which has slightly different syntax. This command has been deprecated for Windows 7.\n\nThe DIR command displays the contents of a directory. The contents comprise the disk's volume label and serial number; one directory or filename per line, including the filename extension, the file size in bytes, and the date and time the file was last modified; and the total number of files listed, their cumulative size, and the free space (in bytes) remaining on the disk. The command is one of the few commands that exist from the first versions of DOS. The command can display files in subdirectories. The resulting directory listing can be sorted by various criteria and filenames can be displayed in a chosen format.\n\nThe ECHO command prints its own arguments back out to the DOS equivalent of the standard output stream. (Hence the name, ECHO) Usually, this means directly to the screen, but the output of \"echo\" can be redirected, like any other command, to files or devices. Often used in batch files to print text out to the user.\n\nAnother important use of the echo command is to toggle echoing of commands on and off in batch files. Traditionally batch files begin with the codice_5 statement. This says to the interpreter that echoing of commands should be off during the whole execution of the batch file, thus resulting in a \"tidier\" output (the codice_6 symbol declares that this particular command (echo off) should also be executed without echo.)\n\nEDIT is a full-screen text editor, included with MS-DOS 5 and 6, OS/2 and Windows NT to 4.0 The corresponding program in Windows 95 and later, and W2k and later is Edit v2.0. PC DOS 6 and later use the DOS \"E\" Editor and DR-DOS used \"editor\" up to version 7.\n\nDOS line-editor. It can be used with a script file, like debug, this makes it of some use even today. The absence of a console editor in MS-DOS/PC DOS 1-4 created an after-market for third-party editors.\n\nIn DOS 5, an extra command \"?\" was added to give the user much-needed help.\n\nDOS 6 was the last version to contain EDLIN; for MS-DOS 6, it's on the supplemental disks, while PC DOS 6 had it in the base install. Windows NT 32-bit, and OS/2 have Edlin.\n\nConverts an executable (.exe) file into a binary file with the extension .com, which is a memory image of the program.\n\nThe size of the resident code and data sections combined in the input .exe file must be less than 64 KB. The file must also have no stack segment.\n\nExits the current command processor. If the exit is used at the primary command, it has no effect unless in a DOS window under Microsoft Windows, in which case the window is closed and the user returns to the desktop.\n\nShow differences between any two files, or any two sets of files.\n\nThe FDISK command manipulates hard disk partition tables. The name derives from IBM's habit of calling hard drives \"fixed disks\". FDISK has the ability to display information about, create, and delete DOS partitions or logical DOS drive. It can also install a standard master boot record on the hard drive.\n\nThe FIND command is a filter to find lines in the input data stream that contain or don't contain a specified string and send these to the output data stream. It may also be used as a pipe.\nThe FOR loop can be used to parse a file or the output of a command.\n\nDeletes the FAT entries and the root directory of the drive/partition, and reformats it for MS-DOS. In most cases, this should only be used on floppy drives or other removable media. This command can potentially erase everything on a computer's drive.\n\nA TSR program to enable the sending of graphical screen dump to printer by pressing <Print Screen>.\n\nGives help about DOS commands.\n\nEvaluate the condition, and only if it is true, then it execute the remainder of the command line\nOtherwise, it skips the remainder of the line and continues with next command line. \n\nUsed in Batch files.\n\nIn MS-DOS; filelink in DR-DOS.\n\nNetwork PCs using a null modem cable or LapLink cable. The server-side version of InterLnk, it also immobilizes the machine it's running on as it is an active app (As opposed to a TSR app) which must be running for any transfer to take place. DR-DOS' filelink is executed on both the client and server.\n\nNew in PC DOS 5.02, MS-DOS 6.0\n\nThe JOIN command attaches a drive letter to a specified directory on another drive. The opposite can be achieved via the SUBST command.\n\nChanges the label on a logical drive, such as a hard disk partition or a floppy disk.\n\nLoads a program above the first 64K of memory, and runs the program. The command is included only in MS-DOS/PC DOS. DR-DOS used memmax, which opened or closed lower, upper, and video memory access, to block the lower 64K of memory.\n\n\"hiload\" in DR-DOS.\n\nMakes a new directory. The parent of the directory specified will be created if it does not already exist.\n\nDisplays memory usage. It is capable of displaying program size and status, memory in use, and internal drivers.It is internal command.\n\nStarting with version 6, MS-DOS included the external program MemMaker which was used to free system memory (especially Conventional memory) by automatically reconfiguring the AUTOEXEC.BAT and CONFIG.SYS files. This was usually done by moving TSR programs and device drivers to the upper memory. The whole process required two system restarts. Before the first restart the user was asked whether he/she wanted to enable EMS Memory, since use of expanded memory required a reserved 64KiB region in upper memory. The first restart inserted the SIZER.EXE program which gauged the memory needed by each TSR or Driver. MemMaker would then calculate the optimal Driver and TSR placement in upper memory and modify the AUTOEXEC.BAT and CONFIG.SYS accordingly, and reboot the second time.\n\nMEMMAKER.EXE and SIZER.EXE were developed for Microsoft by Helix Software Company and were eliminated starting in MS DOS 7 / Windows 95. PC DOS uses another program RamBoost to optimize memory, working either with PC DOS's HIMEM/EMM386 or a third-party memory manager. RamBoost was licensed to IBM by Central Point Software.\n\nConfigures system devices. Changes graphics modes, adjusts keyboard settings, prepares code pages, and sets up port redirection.\n\nThe MORE command paginates text, so that one can view files containing more than one screen of text. \"More\" may also be used as a filter. While viewing MORE text, the return key displays the next line, the space bar displays the next page.\n\nMoves files or renames directories. DR-DOS used a separate command for renaming directories, codice_7.\n\nMSCDEX is a driver executable which allows DOS programs to recognize, read, and control CD-ROMs.\n\nThe MSD command provides detailed technical information about the computer's hardware and software. MSD was new in MS-DOS 6; the PC DOS version of this command is QCONFIG. The command appeared first in Word2, and then in Windows 3.10.\n\nDisplays or sets a search path for executable files.\n\nSuspends processing of a batch program and displays the message 'Press any key to continue. . .', if not given other text to display.\n\nThe PRINT command adds or removes files in the print queue. This command was introduced in MS-DOS version 2. Before that there was no built-in support for background printing files. The user would usually use the copy command to copy files to LPT1.\n\nRemove a directory (delete a directory); by default the directories must be empty of files for the command to succeed. The deltree command in some versions of MS-DOS and all versions of Windows 9x removes non-empty directories.\n\nRemark (comment) command, normally used within a batch file, and for DR-DOS, PC/MS-DOS 6 and above, in CONFIG.SYS. This command is processed by the command processor. Thus, its output can be redirected to create a zero-byte file. REM is useful in logged sessions or screen-captures. One might add comments by way of labels, usually starting with double-colon (::). These are not processed by the command processor.\n\nThe REN command renames a file. Unlike the codice_8 command, this command cannot be used to rename subdirectories, or rename files across drives. Mass renames can be accomplished by the use of the wildcards characters asterisk (*) and question mark (?).\n\nDisk diagnostic utility. Scandisk was a replacement for the codice_9 utility, starting with later versions of MS-DOS. Its primary advantages over codice_9 is that it is more reliable and has the ability to run a surface scan which finds and marks bad clusters on the disk. It also provided mouse point-and-click TUI, allowing for interactive session to complement command-line batch run.\ncodice_9 had surface scan and bad cluster detection functionality included, and was used again on Windows NT based operating systems.\n\nSets environment variables. cmd.exe in Windows NT 2000, 4DOS, 4OS2, 4NT, and a number of third-party solutions allow direct entry of environment variables from the command prompt. From at least Windows 2000, the codice_12 command allows for the evaluation of strings into variables, thus providing \"inter alia\" a means of performing integer arithmetic.\n\nSetVer is a TSR program designed to return a different value to the version of DOS that is running. This allows programs that look for a specific version of DOS to run under a different DOS.\n\nSetver appeared in version 4, and has been in every version of DOS, OS/2 and Windows NT since.\n\nInstalls support for file sharing and locking capabilities.\n\nA filter to sort lines in the input data stream and send them to the output data stream. Similar to the Unix command codice_13. Handles files up to 64k. This sort is always case insensitive.\n\nA utility to map a subdirectory to a drive letter. The opposite can be achieved via the JOIN command.\ncommands the drive letter to main.\n\nA utility to make a volume bootable. Sys rewrites the Volume Boot Code (the first sector of the partition that SYS is acting on) so that the code, when executed, will look for IO.SYS. SYS also copies the core DOS system files, IO.SYS, MSDOS.SYS, and COMMAND.COM, to the volume. SYS does \"not\" rewrite the Master Boot Record, contrary to widely held belief.\n\nDisplay the system time and waits for the user to enter a new time. Complements the DATE command.\n\nEnables a user to change the title of their MS-DOS window.\n\nIt is an external command, graphically displays the path of each directory and sub-directories on the specified drive.\n\nThe TRUENAME command will expand the name of a file, directory, or drive, and display the result. It will expand an abbreviated form which the command processor can recognise into its full form. It can see through SUBST and JOIN to find the actual directory.\n\nMS-DOS can find files and directories given their names, without full path information, if the search object is on a path specified by the environment variable PATH. For example, if PATH includes C:\\PROGRAMS, and file MYPROG.EXE is on this directory, then if codice_14 is typed at the command prompt, the command processor will execute codice_15. In this case,\n\nwould display\n\nThis command displays the UNC pathnames of mapped network or local CD drives. This command is an undocumented DOS command. The help switch \"/?\" defines it as a \"Reserved command name\". It is available in MS-DOS 5.00.0. This command is similar to the Unix which command, which, given an executable found in $PATH, would give a full path and name. The C library function codice_16 performs this function. The Microsoft Windows command processors do not support this command.\n<real code.st>\n\nDisplays a file. The more command is frequently used in conjunction with this command, e.g. type \"long-text-file\" | more. TYPE can be used to concatenate files (type file1 file2 > file3); however this won't work for large files—use copy command instead.\n\nRestores file previously deleted with del. By default all recoverable files in the working directory are restored; options are used to change this behavior. If the MS-DOS mirror TSR program is used, then deletion tracking files are created and can be used by undelete.\n\nAn internal DOS command, that reports the DOS version presently running, and since MS-DOS 5, whether DOS is loaded high. The corresponding command to report the Windows version is codice_17. Values returned:\n\nEnables or disables the feature to determine if files have been correctly written to disk. If no parameter is provided, the command will display the current setting.\n\nCopy entire directory trees. Xcopy is a version of the copy command that can move files and directories from one location to another.\n\n\n\nThere are several guides to DOS commands available that are licensed under the GNU Free Documentation License:\n"}
{"id": "40432339", "url": "https://en.wikipedia.org/wiki?curid=40432339", "title": "List of Nintendo 3DS colors and styles", "text": "List of Nintendo 3DS colors and styles\n\nThis following is a list of case colors and styles that have been produced for the Nintendo 3DS family of handheld game consoles produced by Nintendo. Although the system initially launched with only two colors, its lineup has subsequently expanded to incorporate many more color varieties and limited editions, some of which are exclusive to various regions. As such, the list includes all color variations of regular Nintendo 3DS and New Nintendo 3DS, as well as their larger models, the Nintendo 3DS XL and New Nintendo 3DS XL along with the Chinese version, the iQue 3DS XL, and the entry-level versions, the Nintendo 2DS and New Nintendo 2DS XL.\n\n\n\n\n\n"}
{"id": "14485955", "url": "https://en.wikipedia.org/wiki?curid=14485955", "title": "List of members of the National Academy of Sciences (Medical physiology and metabolism)", "text": "List of members of the National Academy of Sciences (Medical physiology and metabolism)\n"}
{"id": "35283370", "url": "https://en.wikipedia.org/wiki?curid=35283370", "title": "Lists of occupations", "text": "Lists of occupations\n\nThe following are lists of occupations grouped by category.\n\n\n\n\n\n\n"}
{"id": "653782", "url": "https://en.wikipedia.org/wiki?curid=653782", "title": "Little Joe 2", "text": "Little Joe 2\n\nThe Little Joe 2 was a test of the Mercury space capsule, carrying the rhesus monkey Sam (\"Macaca mulatta\") close to the edge of space. He was sent to test the space equipment and the adverse effects of space on humans.\nThe flight was launched December 4, 1959, at 11:15 a.m. ET from Wallops Island, Virginia, United States. Little Joe 2 flew to an altitude of 55 miles (88 km) . It was recovered, with the monkey intact, in the Atlantic Ocean by USS \"Borie\". Sam was one of a series of monkeys in space. Sam, from the School of Aviation Medicine in San Antonio, Texas, received his name as an acronym of the facility. The flight time was 11 minutes, 6 seconds, with a payload of 1,007 kg.\n\nThe boilerplate Mercury spacecraft used in the Little Joe 2 mission is currently displayed at Airpower Park and Museum, Hampton, Virginia.\n\n"}
{"id": "51056252", "url": "https://en.wikipedia.org/wiki?curid=51056252", "title": "Mathematics mastery", "text": "Mathematics mastery\n\nMathematics mastery is an approach to mathematics education which is based on mastery learning in which most students are expected to achieve a high level of competence before progressing. This technique is used in countries such as China and Singapore where good results have been achieved and so the approach is now being promoted in the UK by people such as schools minister Nick Gibb. Chinese teachers were brought to the UK to demonstrate the Shanghai mastery approach in 2015. A trial was made in the UK with about 10,000 students of ages 5–6 and 11–12. In one year, test scores indicated that the students were about a month ahead of students in schools using other approaches. This result was considered small but significant.\n\nThe National Association of Mathematics Advisers has highlighted five issues in understanding this approach.\n\n"}
{"id": "4125128", "url": "https://en.wikipedia.org/wiki?curid=4125128", "title": "Nikolay Demyanov", "text": "Nikolay Demyanov\n\nNikolay Yakovlevich Demyanov (; , Tver – March 19, 1938, Moscow), also known as Demjanov and Demjanow, was a Russian organic chemist and a member of the USSR Academy of Sciences (1929). He is internationally known for the Demjanov rearrangement organic reaction and other discoveries.\n\nHe was a recipient of the Lenin Prize in 1930.\n\n"}
{"id": "1674351", "url": "https://en.wikipedia.org/wiki?curid=1674351", "title": "Optical instrument", "text": "Optical instrument\n\nAn optical instrument either processes light waves to enhance an image for viewing, or analyzes light waves (or photons) to determine one of a number of characteristic properties.\n\nThe first optical instruments were telescopes used for magnification of distant images, and microscopes used for magnifying very tiny images. Since the days of Galileo and Van Leeuwenhoek, these instruments have been greatly improved and extended into other portions of the electromagnetic spectrum. The binocular device is a generally compact instrument for both eyes designed for mobile use. A camera could be considered a type of optical instrument, with the pinhole camera and \"camera obscura\" being very simple examples of such devices.\n\nAnother class of optical instrument is used to analyze the properties of light or optical materials. They include:\n\n\nDNA sequencers can be considered optical instruments as they analyse the \"color\" and intensity of the light emitted by a fluorochrome attached to a specific nucleotide of a DNA strand.\n\nSurface plasmon resonance-based instruments use refractometry to measure and analyze biomolecular interactions.\n\n\n"}
{"id": "15644495", "url": "https://en.wikipedia.org/wiki?curid=15644495", "title": "Perepiteia", "text": "Perepiteia\n\nPerepiteia is claimed to be a new generator developed by the Canadian inventor Thane Heins. The device is named after the Greek word for peripety, a dramatic reversal of circumstances or turning point in a story. The device was quickly attributed the term \"perpetual motion machine\" by several media outlets. Due to the long history of hoaxes and failures of perpetual motion machines and the incompatibility of such a device with accepted principles of physics, Heins' claims about Perepiteia have been treated with considerable skepticism.\n\nIn 2003, Heins filed a patent application in Canada but no patent was granted. Heins also founded Potential Difference Inc, the website of which contains a series of videos of the inventor demonstrating the machine. US patent #9,230,730 issued in 2016 pertaining to another of Thane's inventions, a bi-toroidal topology transformer.\n\nHeins has recently stated that he is unsure whether or not the machine really produces energy, but in communications with science writer David Bradley of ScienceBase, Heins made claims of up to 7000% efficiency for the bi-toroidal transformer. Heins, who reportedly works 8–12 hours a day on the Perepiteia, insists that it is viable and that \"This technology should be mainstream.\"\n\nMechanically, the device appears to be an induction motor with a magnetic material placed inside the rotor core. Heins believes that the device's potential may rest in its atypical manipulation of the back electromotive force (back EMF). A more detailed description of the device may be found in the patent application, minus supporting figures.\n\nThe apparent unique quality of the Perepiteia machine is that instead of maintaining a certain state of motion, it appears to generate acceleration. According to Heins, the Perepiteia produces magnetic friction which somehow gets turned into a magnetic boost. Using an electric motor, the drive shaft is attached to a steel rotor with small round magnets lining its outer edges. In this set-up of a simple generator, the rotor spins so that the magnets pass by a wire coil just in front of them, generating electrical energy.\n\nPerepiteia's process begins by overloading the generator to get a current, which typically causes the wire coil to build up a large electromagnetic field. Usually, this kind of electromagnetic field creates an effect called the back electromotive force (back EMF) due to Lenz's law. The effect should repel the spinning magnets on the rotor, and slow them down until the motor stops completely, in accordance with the law of conservation. However, instead of stopping, the rotor accelerates - i.e. the magnetic friction did not repel the magnets and wire coil. Heins states that the steel rotor and driveshaft had conducted the magnetic resistance away from the coil and back into the electric motor. In effect, the back EMF was boosting the magnetic fields used by the motor to generate electrical energy and cause acceleration. The faster the motor accelerated, the stronger the electromagnetic field it would create on the wire coil, which in turn would make the motor go even faster. Heins seemed to have created a positive feedback loop. To confirm the theory, Heins replaced part of the driveshaft with plastic pipe that wouldn't conduct the magnetic field. There was no acceleration.\n\nIn early 2008, Heins was given access to equipment to demonstrate it by professor Riadh Habash of the University of Ottawa, who says of it, \"It accelerates, but when it comes to an explanation, there is no backing theory for it. That's why we're consulting MIT. But at this time we can't support any claim.\"\n\nAfter examining the machine and witnessing a demonstration, Massachusetts Institute of Technology (MIT) professor Markus Zahn admitted that he could not fully explain its operation. Although he refused to call it perpetual motion, he stated that it might be an extremely efficient motor. Regarding the device, Zahn stated that \"It's an unusual phenomena I wouldn't have predicted in advance. But I saw it. It's real. Now I'm just trying to figure it out...To my mind this is unexpected and new, and it's worth exploring all the possible advantages once you're convinced it's a real effect.\" However, even if Perepiteia does not produce perpetual motion, Zahn still believes that the device could have considerable practical applications, noting that \"There are an infinite number of induction machines in people's homes and everywhere around the world. If you could make them more efficient, cumulatively, it could make a big difference.\"\n\nHowever, Zahn later stated in an interview that \"I can't understand how [Heins] can even breathe the words 'perpetual motion.' He plugs it into the wall.\" In a subsequent e-mail to Heins, Zahn wrote that: \"Any talk of perpetual motion, over unity efficiency, etc. discredits you, now me, and your ideas.\" Zahn further stated that he would not endorse Heins' device until \"the foolishness is stopped of hinting that your motor violates fundamental laws of physics.\"\n\nCritics of the system have pointed out that the system described by Heins simply demonstrates a change in the motor's hysteresis drag, increasing the speed of the rotor but not producing any energy. In other words, when the rotor exhibits acceleration following a specific electrical short-out, the device is merely more efficiently converting the input electricity to mechanical energy than in the other test configurations.\n\nOn February 29, 2008, six members of Ottawa Skeptics, met at the Colonel By building at the University of Ottawa to witness a demonstration of Perepiteia. Heins, who conducted the demonstration, later met with the members to discuss his device and answer questions. In a subsequent report released in May, Ottawa Skeptics expressed severe doubts about Heins' claims regarding Perepiteia. They noted that Perepiteia produces either observed acceleration or a slight increase in generator electrical output but that this alone does not automatically mean that \"free energy\" or perpetual motion is being produced or that there is a \"real and measurable effect.\" While acknowledging that the speed-up behaviour of the generator cannot be fully explained, they stated that there is no evidence that Perepiteia \"represents any challenge to currently known laws of physics.\"\n\nOn May 21, 2009, a skeptic writing under the name Natan Weissman wrote an explanation of Perepiteia in relation to its motor, a Ryobi bench grinder. The author states that the acceleration behavior of the machine is due to the consumption of torque from the induction motor, rather than any unconventional manipulation of Electromagnetic fields or Counter-electromotive force.\n\nOn June 3, 2013, posting in response to questions \"Pure Energy Blog\", Heins provided an explanation of his claims, stating that: \"A generator that requires a 1 Watt increase in mechanical drive shaft power to deliver 1 Watt of electrical power to a load would be 100% efficient. A generator that delivers 0.95 Watts with a 1 Watt increase in mechanical drive shaft power from no-load to on-load would be 95% efficient.\"\n\n"}
{"id": "31987233", "url": "https://en.wikipedia.org/wiki?curid=31987233", "title": "Peter J. Schwendinger", "text": "Peter J. Schwendinger\n\nPeter J. Schwendinger (born 27 April 1959 in Dornbirn, Austria) is an Austrian arachnologist.\n\nHe graduated from Innsbruck University in 1985, and in 1990 with a PhD, where he studied with Konrad Thaler. He was a lecturer at Innsbruck University, from 1989 to 1999. He taught at Chiang Mai University from 1996 to 1997.\nHe is a curator at the Natural History Museum of Geneva.\nHe is an editor of the journal \"Zootaxa\".\n\n\n"}
{"id": "494745", "url": "https://en.wikipedia.org/wiki?curid=494745", "title": "Petrus Apianus", "text": "Petrus Apianus\n\nPetrus Apianus (April 16, 1495 – April 21, 1552), also known as Peter Apian, Peter Bennewitz, and Peter Bienewitz was a German humanist, known for his works in mathematics, astronomy and cartography. His work on \"cosmography\", the field that dealt with the earth and its position in the universe and presented in his most famous works, \"Astronomicum Caesareum\" (1540) and \"Cosmographicus liber\" (1524) which were extremely influential in his time with the numerous editions in multiple languages being published until 1609. The lunar crater \"Apianus\" and asteroid 19139 Apian are named in his honour.\n\nApianus was born as Peter Bienewitz (or Bennewitz) in Leisnig in Saxony; his father, Martin, was a shoemaker. The family was relatively well off, belonging to the middle-class citizenry of Leisnig. Apianus was educated at the Latin school in Rochlitz. From 1516 to 1519 he studied at the University of Leipzig; during this time, he Latinized his name to Apianus (lat. \"apis\" means \"bee\"; \"Biene\" is the German word for bee).\n\nIn 1519, Apianus moved to Vienna and continued his studies at the University of Vienna, which was considered one of the leading universities in geography and mathematics at the time and where Georg Tannstetter taught. When the plague broke out in Vienna in 1521, he completed his studies with a B.A. and moved to Regensburg and then to Landshut. At Landshut, he produced his \"Cosmographicus liber\" (1524), a highly respected work on astronomy and navigation which was to see at least 30 reprints in 14 languages and that remained popular until the end of the 16th century. Later editions were produced by Gemma Frisius. \n\nIn 1527, Peter Apian was called to the University of Ingolstadt as a mathematician and printer. His print shop started small. Among the first books he printed were the writings of Johann Eck, Martin Luther's antagonist. This print shop was active between 1543 and 1540 and became well known for its high-quality editions of geographic and cartographic works. It is thought that he used stereotype printing techniques on woodblocks. The printer's logo included the motto \"Industria superat vires\" in Greek, Hebrew, and Latin around the figure of a boy.\nThrough his work, Apian became a favourite of emperor Charles V. Charles had praised his work (the \"Cosmographicus liber\") at the Imperial Diet of 1530 and granted him a printing monopoly in 1532 and 1534. In 1535, the emperor made Apian an armiger, i.e. granted him the right to display a coat of arms. In 1540, Apian printed the \"Astronomicum Caesareum\", dedicated to Charles V. Charles promised him a truly royal sum (3,000 golden guilders), appointed him his court mathematician, and made him a \"Reichsritter\" (a Free Imperial Knight) and in 1544 even an Imperial Count Palatine. All this furthered Apian's reputation as an eminent scientist. \"Astronomicum Caesareum\" is noted for its visual appeal. Printed and bound decoratively, with about a 100 known copies, it included several volvelles that allowed users to calculate dates, the positions of constellations and so on. Apian noted that it took a month to produce some of the plates. Thirty five octagonal paper cut instruments were included with woodcuts that are thought to have been made by Hans Brosamer (c 1495-1555) who may have trained under Lucas Cranach, Sr. in Wittemberg. It also incorporated star and constellation names from the work of the Arab astronomer \"Azophi\" (Abu al-Husain al-Sufi AD 903-986). Apian is also remembered for publishing the only known depiction of the Bedouin constellations in 1533. On this map Ursa Minor is an old woman and three maidens, Draco is four camels and Cepheus was illustrated as a shepherd with sheep and dog. \nDespite many calls from other universities, including Leipzig, Padua, Tübingen, and Vienna, Apian remained in Ingolstadt until his death. Although he neglected his teaching duties, the university evidently was proud to host such an esteemed scientist. Apian's work included in mathematics—in 1527 he published a variation of Pascal's triangle, and in 1534 a table of sines— as well as astronomy. In 1531, he observed Halley's comet and noted that a comet's tail always point away from the sun. Girolamo Fracastoro also detected this in 1531, but Apian's publication was the first to also include graphics. He designed sundials, published manuals for astronomical instruments and crafted volvelles (\"Apian wheels\"), measuring instruments useful for calculating time and distance for astronomical and astrological applications.\n\nApianus married the daughter of a councilman of Landshut, Katharina Mosner, in 1526. They would have 14 children together, five girls and nine sons, one of whom was Philipp Apian (1531–1589), who, in addition to his own research, preserved the legacy of his father.\n\n\n Whether Apian ever received the promised money is uncertain; in any case he wrote a letter to the emperor in 1549 asking him to finally pay the promised sum.\n\n\n"}
{"id": "8431600", "url": "https://en.wikipedia.org/wiki?curid=8431600", "title": "Progress Station", "text": "Progress Station\n\nProgress () is a Russian (formerly Soviet) research station in Antarctica. It is located at the Larsemann Hills antarctic oasis on the shore of Prydz Bay.\n\nThe station was established by the 33rd Soviet Antarctic Expedition on April 1, 1988 and was moved to another place on February 26, 1989 In 2000, work was temporarily halted but it reopened in 2003.\n\nA landing field is located close to the station for air connection with other stations. From 1998-2001 works were performed to transfer transportation operations to Progress from the Mirny Station and make it the main support base for Vostok station.\n\nIn 2004, work began on a year-round facility at the station. On October 4, 2008, a fire broke out at the construction site resulting in the death of a construction worker and two serious injuries. The fire resulted in the complete loss of the new structure, as well as damage to the station's communications and scientific equipment.\n\nIn 2013, the construction of a new wintering complex was completed. It is a residential unit with a sauna and gym, rooms for meteorologists and radio operators, medical unit, equipped with no less modern regional hospital, and its own galley.\n\n\n"}
{"id": "28722165", "url": "https://en.wikipedia.org/wiki?curid=28722165", "title": "R&amp;D management", "text": "R&amp;D management\n\nR&D management is the discipline of designing and leading R&D processes, managing R&D organizations, and ensuring smooth transfer of new know-how and technology to other groups or departments involved in innovation.\n\nR&D management can be defined as where the tasks of innovation management (i.e., creating and commercializing inventions) meet the tasks of technology management (i.e., external and internal creation and retention of technological know-how). It covers activities such as basic research, fundamental research, technology development, advanced development, concept development, new product development, process development, prototyping, R&D portfolio management, technology transfer, etc., but generally is not considered to include technology licensing, innovation management, IP management, corporate venturing, incubation, etc. as those are sufficiently independent activities that can be carried out without the presence of a R&D function in a firm.\n\nFew dedicated management models for R&D exist. Among the more popularized ones are Arthur D. Little's Third generation R&D management, the Development funnel, the Phase–gate model All these models are concerned with improving R&D performance and result productivity, managing R&D as a process, and providing the R&D function with an environment in which the inherent technological and market uncertainties can be managed.\n\nThe Path to Developing Successful New Products a joint research by MIT & McKinsey & Co. points out three key practices that can play critical role in R&D Management: Talk to the customer, Nurture a project culture, Keep it focused.\n\n\nJournals:\n\nAssociations and communities\n\n"}
{"id": "42914330", "url": "https://en.wikipedia.org/wiki?curid=42914330", "title": "Rent-gap theory", "text": "Rent-gap theory\n\nThe rent-gap theory or Mietlückentheorie was developed in 1979 by the geographer Neil Smith as an economic explanation for the process of gentrification. It describes the disparity between the current rental income of a property and the potentially achievable rental income. Only from this difference arises the interest of investors, a particular object (to entire neighborhoods) to renovate, resulting in an increase in rents and also the value of the property.\n\nInvestment in the property market will therefore only be made if a rent gap exists. Thus, it is contrary to other explanations for gentrification related to cultural and consumption preferences and housing preferences. The rent-gap theory is a purely economic approach. The processes described with the rent-gap theory can be observed especially in North America.\n\n"}
{"id": "18493018", "url": "https://en.wikipedia.org/wiki?curid=18493018", "title": "Sarphatipark", "text": "Sarphatipark\n\nThe Sarphatipark is a public urban park located in the \"stadsdeel\" Amsterdam Oud-Zuid in Amsterdam, Netherlands. The park is named after Samuel Sarphati.\n\nIn 1942, the park was renamed \"Bollandpark\" after G.J.P.J. Bolland, because Samuel Sarphati was a Jew. The old name was restored after the war in 1945.\n\nThe Dutch painter Mommie Schwarz and his wife Else Berg lived adjacent to the park from 1927 until their deportation to, and execution at, the Auschwitz concentration camp in 1942. Some of their last works were landscape paintings of the park.\n"}
{"id": "26762483", "url": "https://en.wikipedia.org/wiki?curid=26762483", "title": "Science 2.0", "text": "Science 2.0\n\nScience 2.0 is a suggested new approach to science that uses information-sharing and collaboration made possible by network technologies. It is similar to the open research and open science movements and is inspired by Web 2.0 technologies. Science 2.0 stresses the benefits of increased collaboration between scientists. Science 2.0 uses collaborative tools like wikis, blogs and video journals to share findings, raw data and \"nascent theories\" online. Science 2.0 benefits from openness and sharing, regarding papers and research ideas and partial solutions.\n\nA general view is that Science 2.0 is gaining traction with websites beginning to proliferate, yet at the same time there is considerable resistance within the scientific community about aspects of the transition as well as discussion about what, exactly, the term means. There are several views that there is a \"sea change\" happening in the status quo of scientific publishing, and substantive change regarding how scientists share research data. There is considerable discussion in the scientific community about whether scientists should embrace the model and exactly how Science 2.0 might work, as well as several reports that many scientists are slow to embrace collaborative methods and are somewhat \"inhibited and slow to adopt a lot of online tools.\"\n\nThe term has many meanings and continues to evolve in scientific parlance. It not only describes what is currently happening in science, but describes a direction in which proponents believe science should move towards, as well as a growing number of websites which promote free scientific collaboration.\nThe term \"Science 2.0\" suggests a contrast between traditional ways of doing science, often denoted \"Science 1.0\", with more collaborative approaches, and suggests that the new forms of science will work with Web 2.0 technologies. One description from \"Science\" is that Science 2.0 uses the \"networking power of the internet to tackle problems with multiple interacting variables - the problems, in other words, of everyday life.\" A different and somewhat controversial view is that of Ben Shneiderman, who suggested that Science 2.0 combines hypothesis-based inquiries with social science methods, partially for the purpose of improving those new networks.\n\nWhile the term describes websites for sharing scientific knowledge, it also includes efforts by existing science publishers to embrace new digital tools, such as offering areas for discussions following published online articles. Sometimes it denotes open access which, according to one view, means that the author continues to hold the copyright but that others can read it and use it for reasonable purposes, provided that the attribution is maintained. Most online scientific literature is behind paywalls, meaning that a person can find the title of an article on Google but they can not read the actual article. People who can access these articles are generally affiliated with a university or secondary school or library or other educational institution, or who pay on a per-article or subscription basis.\n\nOne view is that \"Science 2.0\" should include an effort by scientists to offer papers in non-technical language, as a way of reaching out to non-scientists. For others, it includes building vast databases of case histories. There is a sense in which \"Science 2.0\" indicates a general direction for scientific collaboration, although there is little clarity about how exactly this might happen. One aim is to \"make scientific collaboration as easy as sharing videos of trips home from the dentist,\" according to one view.\n\nClosely related terms are \"cyberscience\" focussing on scientists communicating in the cyberspace and \"cyberscience 2.0\" expanding the notion to the emerging trend of academics using Web 2.0 tools.\n\nThe rise of the Internet has transformed many activities such as retailing and information searching. In journalism, Internet technologies such as blogging, tagging and social networking have caused many existing media sources such as newspapers to \"adopt whole new ways of thinking and operating,\" according to a report in \"Scientific American\" in 2008. The idea is that while the Internet has transformed many aspects of life, it has not changed scientific research as much as it could. While firms such as eBay, Amazon and Netflix have changed consumer retailing, and online patient-centered medical data has enabled better health care, Science 2.0 advocate Ben Shneiderman said:\n\nAccording to one view, a similar web-inspired transformation that has happened to other areas is now happening to science. The general view is that science has been slower than other areas to embrace the web technology, but that it is beginning to catch up.\nBefore the Internet, scientific publishing has been described as a \"highly integrated and controlled process.\" Research was done in private. Next, it was submitted to scientific publications and reviewed by editors and gatekeepers and other scientists. Last, it was published. This has been the traditional pathway of scientific advancement, sometimes dubbed \"Science 1.0\".\n\nEstablished journals provided a \"critical service\", according to one view. Publications such as \"Science\" and \"Nature\" have large editorial staffs to manage the peer-review process as well as have hired fact-checkers and screeners to look over submissions. These publications get revenue from subscriptions, including online ones, as well as advertising revenue and fees paid by authors. According to advocates of Science 2.0, however, this process of paper-submission and review was rather long. Detractors complained that the system is \"hidebound, expensive and elitist\", sometimes \"reductionist\", as well as being slow and \"prohibitively costly\". Only a select group of gatekeepers—those in charge of the traditional publications—limited the flow of information. Proponents of open science claimed that scientists could learn more and learn faster if there is a \"friction-free collaboration over the Internet.\"\n\nYet there is considerable resistance within the scientific community to a change of approach. The act of publishing a new finding in a major journal has been at the \"heart of the career of scientists,\" according to one view, which elaborated that many scientists would be reluctant to sacrifice the \"emotional reward\" of having their discoveries published in the usual, traditional way. Established scientists are often loath to switch to an open-source model, according to one view.\n\nTimo Hannay explained that the traditional publish-a-paper model, sometimes described as \"Science 1.0\", was a workable one but there need to be other ways for scientists to make contributions and get credit for their work:\n\nIn 2008, a scientist at the University of Maryland named Ben Shneiderman wrote an editorial entitled \"Science 2.0\". Shneiderman argued that Science 2.0 was about studying social interactions in the \"real world\" with study of e-commerce, online communities and so forth. A writer in \"Wired Magazine\" criticized Shneiderman's view, suggesting that Shneiderman's call for more collaboration, more real-world tests, and more progress should not be called \"Science 2.0\" or \"Science 1.0\" but simply science.\n\nThere are reports that established journals are moving towards greater openness. Some help readers network online; others enable commenters to post links to websites; others make papers accessible after a certain period of time has elapsed. But it remains a \"hotly debated question\", according to one view, whether the business of scientific research can move away from the model of \"peer-vetted, high-quality content without requiring payment for access.\" The topic has been discussed in a lecture series at the California Institute of Technology. Proponent Adam Bly thinks that the key elements needed to transform Science 2.0 are \"vision\" and \"infrastructure\":\n\nThere are numerous examples of more websites offering opportunities for scientific collaboration.\n\n\n\n"}
{"id": "21880366", "url": "https://en.wikipedia.org/wiki?curid=21880366", "title": "Semantic Sensor Web", "text": "Semantic Sensor Web\n\nThe Semantic Sensor Web (SSW) is a marriage of sensor and Semantic Web technologies. The encoding of sensor descriptions and\nsensor observation data with Semantic Web languages enables more expressive representation, advanced access, and formal analysis of\nsensor resources. The SSW annotates sensor data with spatial, temporal, and thematic semantic metadata. This technique builds on current standardization efforts within the Open Geospatial Consortium's Sensor Web Enablement (SWE) and extends them with Semantic Web technologies to provide enhanced descriptions and access to sensor data.\n\nOntologies and other semantic technologies can be key enabling technologies for sensor networks because they will improve semantic interoperability and integration, as well as facilitate reasoning, classification and other types of assurance and automation not included in the Open Geospatial Consortium (OGC) standards. A semantic sensor network will allow the network, its sensors and the resulting data to be organised, installed and managed, queried, understood and controlled through high-level specifications. Ontologies for sensors provide a framework for describing sensors. These ontologies allow classification and reasoning on the capabilities and measurements of sensors, provenance of measurements and may allow reasoning about individual sensors as well as reasoning about the connection of a number of sensors as a macroinstrument. The sensor ontologies, to some degree, reflect the OGC standards and, given ontologies that can encode sensor descriptions, understanding how to map between the ontologies and OGC models is an important consideration. Semantic annotation of sensor descriptions and services that support sensor data exchange and sensor network management will serve a similar purpose as that espoused by semantic annotation of Web services. This research is conducted through the W3C Semantic Sensor Network Incubator Group (SSN-XG) activity.\n\nThe World Wide Web Consortium (W3C) initiated the Semantic Sensor Networks Incubator Group (SSN-XG) to develop the Semantic Sensor Network (SSN) ontology, intended to model sensor devices, systems, processes, and observations. The Incubator Group later transitioned into the Semantic Sensor Networks Community Group. It was then picked up in the joint OGC and W3C Spatial Data on the Web Working Group and published as a W3C Recommendation . \n\nThe Semantic Sensor Network (SSN) ontology enables expressive representation of sensor observations, sampling, and actuation. The SSN ontology is encoded in the Web Ontology Language (OWL2). A number of projects have used it for improved management of sensor data on the Web, involving annotation, integration, publishing, and search.\n\nSensors around the globe currently collect avalanches of data about the world. The rapid development and deployment of sensor technology is intensifying the existing problem of too much data and not enough knowledge . With a view to alleviating this glut, sensor data can be annotated with semantic metadata to increase interoperability between heterogeneous sensor networks, as well as to provide contextual information essential for situation awareness. Semantic web techniques can greatly help with the problem of data integration and discovery as it helps map between different metadata schema in a structured way.\n\nSemantic Sensor Web (SSW) technologies are utilized in fields such as agriculture, disaster management, building management and laboratory management.\n\nMonitoring various environmental attributes is critical to the growth of plants. Environmental attributes that are critical for growers are mainly temperature, moisture, pH, electric conductivity (EC), and more. Real-time monitoring in addition to setting alerts for the mentioned sensors was never possible. With the creation of SSW, growers can now track their plant growing conditions in real-time.\n\nAn example of such advancement in agriculture through utilization of SSW is the research conducted in 2008 on Australian farms where temperature, humidity, barometric pressure, wind speed, wind direction and rainfall were monitored using SSW methodology. The architecture of this research project consists of personal integration needs, Semantic web, and more in addition to semantic data integration, i.e. where data is centralized to make sensor semantic web technologies meaningful and useful.\n\nManaging buildings can be quite sophisticated, as the cost of fixing damages is significantly higher than having proper monitoring tools in place to prevent damages from happening. SSW allows for getting notified of water leaks, controlling apartment temperature via smartphone, and more.\n\nManaging laboratory tests can be quite challenging, especially if tests take place over a long time, across multiple locations, or in infrastructures where many tests occur. Such tests include creep tests for a material, reaction tests of a certain chemical or wireless transmission tests of a circuit. Advancements in SSW allow for real-time monitoring of laboratory variables via sensors. Such sensors can take more than one factor into consideration before alerting. For example, an alert can go off when pressure and temperature both exceed a certain limit, or an alert can go off when pressure in one building drops, yet pressure in another building remains the same.\n\nStandardization is a lengthy and difficult process, as players in a field that have existing solutions would see any standardization as an additional cost to their activities. Open Geospatial Consortium (OGC), an international voluntary consensus standards organization that was founded in 1994, is making efforts to enhance and accelerate the growth of the SSW community and standardize sensor information across web. Most OGC standards depend on generalized architecture that is collectively captured in set of documents. The goal of OGC is to provide enhancements in description and meaning of sensor data. Also, OGC had enabled Sensor Web communication. OGC is in charge of creating open geospatial standards. Moreover, OCG is supported by industry, government, and academic partners to allow for easy creation of geo-processing technologies known as “plug and play”.\n\nCurrent challenges in the SSW field include a lack of standardization, which slows down the growth rate of sensors created to measure things. For the semantic sensor web to be meaningful, the languages, tags, and labels across various applications, developed by various developers, must be the same. Unfortunately, due to scattered development of various architectures, such standardization is not possible. This problem is called vastness.\n\nThere is also the problem of inconsistency, such that when changing the architecture of an existing solution, the system logic will no longer hold. In order to resolve this problem, there is a need for an extensive amount of resources (depending on the size and complexity of system). For example, many existing systems use twelve bits to transfer temperature data to a local computer. However, in a SSW 16 bits of data is acceptable. This inconsistency results in higher data traffic with no additional accuracy improvement. In order for the old system to improve, there is a need of allocating extra bits and changing the buffer requirements, which is costly. Assuming the resources required to make the tag requirement are available, there is still the existence of unnecessary data that requires additional storage space in addition to creating confusion for other SSW members. The only solution remaining is changing the hardware requirements, which requires a lot of resources.\n\n\n\n"}
{"id": "53166752", "url": "https://en.wikipedia.org/wiki?curid=53166752", "title": "Siamese buffalo", "text": "Siamese buffalo\n\nThe Krabue buffalo ( (\"krabue\") being the Thai word for \"water buffalo\") also known as the Siamese buffalo, Thai water buffalo or Thai swamp buffalo is a large breed of water buffalo indigenous to Thailand.\n"}
{"id": "3022885", "url": "https://en.wikipedia.org/wiki?curid=3022885", "title": "Stealth conservative", "text": "Stealth conservative\n\nStealth conservative individuals or organizations are those whom some perceive to present themselves as progressive, liberal, or moderate while using that status to forward a so-called \"conservative agenda\". According to journalist Laura Flanders:\nThis is the new face of the radical right. The attractive right-wingers who talk about 'choice' and 'civil rights' pose a threat to progressives who support the federal government’s role as a protector of individual citizens.\n\nChristian Coalition leader Ralph Reed has openly advocated a policy of stealth politics by the right. In a March 1992 interview with the Los Angeles Times, Reed stated \"It's like guerrilla warfare. If you reveal your location, all it does is allow your opponent to improve his artillery bearings. It's better to move quietly, with stealth, under the cover of night.\"\n\n"}
{"id": "5201544", "url": "https://en.wikipedia.org/wiki?curid=5201544", "title": "THEMIS", "text": "THEMIS\n\nThe Time History of Events and Macroscale Interactions during Substorms (THEMIS) mission began in February 2007 as a constellation of five NASA satellites (THEMIS A through THEMIS E) to study energy releases from Earth's magnetosphere known as substorms, magnetic phenomena that intensify auroras near Earth's poles. The name of the mission is an acronym alluding to the Titan, Themis.\n\nThree of the satellites orbit the Earth within the magnetosphere, while two have been moved into orbit around the Moon. Those two were renamed ARTEMIS for Acceleration, Reconnection, Turbulence and Electrodynamics of the Moon’s Interaction with the Sun. THEMIS B became ARTEMIS P1 and THEMIS C became ARTEMIS P2.\n\nThe THEMIS satellites were launched February 17, 2007 from Cape Canaveral Air Force Station Space Launch Complex 17 aboard a Delta II rocket. Each satellite carries identical instrumentation, including a fluxgate magnetometer (FGM), an electrostatic analyzer (ESA), a solid state telescope (SST), a search-coil magnetometer (SCM) and an electric field instrument (EFI). Each probe has a mass of 126 kg, including 49 kg of hydrazine fuel.\n\nTHEMIS data can be accessed using the SPEDAS software.\n\nTHEMIS was originally scheduled to launch on October 19, 2006. Owing to delays caused by workmanship problems with Delta II second stages—an issue that also affected the previous mission, STEREO—the THEMIS launch was delayed to Thursday, February 15, 2007. Due to weather conditions occurring on Tuesday, February 13, fueling of the second stage was delayed, and the launch pushed back 24 hours. On February 16, the launch was scrubbed in a hold at the T-4 minute point in the countdown due to the final weather balloon reporting a red, or nogo condition for upper level winds. A 24-hour turnaround procedure was initiated, targeting a new launch window between 23:01 and 23:17 UTC on February 17.\n\nFavorable weather conditions were observed on February 17, and the countdown proceeded smoothly. THEMIS successfully launched at 6:01 p.m. EST. The spacecraft separated from the launch vehicle approximately 73 minutes after liftoff. By 8:07 p.m. EST, mission operators at the Space Sciences Laboratory (SSL) of the University of California, Berkeley, commanded and received signals from all five spacecraft, confirming nominal separation status.\n\nThe launch service was provided by the United Launch Alliance through NASA Launch Services Program (LSP).\n\nFrom February 15, 2007 until September 15, 2007 the five THEMIS satellites coasted in a string-of-pearls orbital configuration. From September 15, 2007 until December 4, 2007 the satellites were moved to more distant orbits in preparation for data collection in the magnetotail. This phase of the mission was called the \"Dawn Phase\" because the satellites' orbits were in apogee on the dawn side of the magnetosphere. On December 4, 2007 the first tail science phase of the mission began. In this segment of the mission scientists will collect data from the magnetotail of the Earth's magnetosphere. During this phase the satellites' orbits are in apogee inside the magnetotail. The scientists hope to observe substorms and magnetic reconnection events. During these events charged particles stored in the Earth's magnetosphere are discharged to form the aurora borealis. Tail science is performed in the winter of the northern hemisphere because the ground magnetometers that Themis scientists correlate the satellite data with have relatively longer periods of night. During the night, observations are not interrupted by charged particles from the Sun.\n\nIn 2007, THEMIS \"found evidence of magnetic ropes connecting Earth's upper atmosphere directly to the Sun,\" reconfirming the theory of solar-terrestrial electrical interaction (via \"Birkeland currents\" or \"field-aligned currents\") proposed by Kristian Birkeland circa 1908. NASA also likened the interaction to a \"30 kiloVolt battery in space,\" noting the \"flux rope pumps 650,000 Amp current into the Arctic!\"\n\nOn 26 February 2008, THEMIS probes were able to determine, for the first time, the triggering event for the onset of magnetospheric substorms. Two of the five probes, positioned approximately one third the distance to the Moon, measured events suggesting a magnetic reconnection event 96 seconds prior to Auroral intensification. Dr. Vassilis Angelopoulos of the University of California, Los Angeles, who is the principal investigator for the THEMIS mission, claimed, \"Our data show clearly and for the first time that magnetic reconnection is the trigger.\"\n\nOn May 19, 2008 the Space Sciences Laboratory (SSL) at Berkeley announced NASA had extended the THEMIS mission to the year 2012. NASA officially approved the movement of THEMIS B and THEMIS C into lunar orbit under the mission name ARTEMIS (Acceleration, Reconnection, Turbulence and Electrodynamics of the Moon's Interaction with the Sun). In February 2017, THEMIS celebrated ten years of science operations. As of August 2017, the three THEMIS inner probes continue to collect valuable data on the Sun's interaction with the Earth's magnetosphere.\n\nAs of spring 2010, ARTEMIS P1 (THEMIS B) had performed two Lunar flybys and one Earth flyby, and was approaching insertion into a Lissajous orbit around a Lunar Lagrange point. Lunar orbit insertion was targeted for April 2011. ARTEMIS P2 (THEMIS C) completed a Lunar flyby and was on the inbound leg of the first of three deep space excursions on its way to a Lissajous orbit and was targeted for Lunar orbit in April 2011.\n\nOn June 22, 2011, ARTEMIS P1 began firing its thrusters to move out of its kidney-shaped \"libration\" orbit on one side of the Moon, where it had been since January. As of July 2, 2011 12:30 p.m. EDT, ARTEMIS P1 achieved lunar orbit. The second spacecraft, ARTEMIS P2, moved into lunar orbit as of July 17, 2011. Along the way, the two spacecraft were the first to ever achieve orbit around the Moon's Lagrangian points.\n\nAs of August 2017, both lunar probes are in stable orbits, and the health of all instruments and the spacecraft remains very good.\n\n\nAs the satellites monitor the magnetosphere from orbit, twenty ground stations in North America simultaneously monitor auroras. Ground station mission and science operations are being managed by the University of California Space Sciences Laboratory.\n\n\nSwales Aerospace, now part of Orbital ATK, (Beltsville, Maryland) manufactured all five probes for this mission. Each was built-up and tested at the Beltsville facility, before being delivered to University of California, Berkeley for instrument integration. Swales was responsible for integrating the BAU, IRU, Solar Arrays, Antenna, Battery, and other components necessary for functionality. This was the second major satellite built by Swales, the first being the EO-1 spacecraft, which continues to orbit Earth. Swales was also responsible for designing and building the Electrical Ground Support Equipment (EGSE) used for monitoring the probes during all phases of pre-launch activities, including use at the launch site.\n\nAfter the installation of instruments at SSL, Berkeley, pre-launch testing including thermal-vac, vibration and acoustic tests, was conducted at NASA's Jet Propulsion Laboratory in Pasadena, California.\nThe Fast Auroral Snapshot Explorer (FAST) mission supported THEMIS in 2008 and 2009 before being retired. FAST was a Small Explorer program (SMEX) mission launched in 1996.\n\n\n\n"}
{"id": "34747393", "url": "https://en.wikipedia.org/wiki?curid=34747393", "title": "Tax choice", "text": "Tax choice\n\nIn public choice theory, tax choice (sometimes called taxpayer sovereignty or earmarking) is the belief that individual taxpayers should have direct control over how their taxes are spent. Its proponents apply the theory of consumer choice to public finance. They claim taxpayers react positively when they are allowed to allocate portions of their taxes to specific spending.\n\nDaniel J. Brown examines tax-target plans in educational programs.\n\nAlan Peacock, in his 1961 book \"The Welfare Society\", advocates greater diversity in public services (education, housing, hospitals).\n\nAccording to Vincent and Elinor Ostrom, it is possible that government may oversupply, and a market arrangement may undersupply, those public goods for which exclusion is not feasible.\n\nVoting with your feet and voting with your taxes are two methods that allow taxpayers to reveal their preferences for public policies. Foot voting refers to where people move to areas that offer a more attractive bundle of public policies. In theory foot voting would force local governments to compete for taxpayers. Tax choice, on the other hand, would allow taxpayers to indicate their preferences with their individual taxes.\n\nFour bills involving tax choice have been introduced by the United States Congress since 1971. The Presidential Election Campaign Fund, enacted in 1971, allows taxpayers to allocate $3 of their taxes to presidential election campaigns. The 2000 Taxpayers’ Choice Debt Reduction Act would have allowed taxpayers to designate money toward reduction of the national debt. The 2007 Opt Out of Iraq War Act would have allowed taxpayers to designate money toward certain social programs. The 2011 Put Your Money Where Your Mouth Is Act would have allowed taxpayers to make voluntary contributions (not tax payments) to the government. These later bills died in committee.\n\n\n"}
{"id": "812525", "url": "https://en.wikipedia.org/wiki?curid=812525", "title": "Transformers technology", "text": "Transformers technology\n\nTransformers technology refers to various technologies in Transformers series of comic books, films, animated series, and other media. The term refers to unusual processes, such as the departure from the standard Transformer ability to change between two different forms, as well as technology utilized by the Transformers in-series.\n\nThe term \"Combiner\" refers to a sub-group of Transformers able to combine their bodies and minds into a singular, larger, more powerful super-robot (the process is referred to in Computron's Marvel tech spec as \"combinatics\"). The term \"Combiner\" comes from the sub-group's ability to \"combine\" into a larger robot.\n\nCombiner technology has its downsides as well, the primary one being that the combined robot can only do what all of its components agree upon. For beings such as Devastator, that is typically wanton destruction and nothing else. There is a flipside to this: Computron's main problem is that every problem must be worked out to every conceivable solution; even with Computron's enhanced computers, this is a time-consuming task, which for example led to his quick defeat by Abominus (another Decepticon Combiner) around the time of the Hate Plague.\n\nThe term \"gestalt\" was adopted by small minority of fans as the catch-all name for combining Transformers. Although not officially employed on any merchandise by Hasbro, it appears that the term is recognized by them for its use in the fandom, although it is generally passed over in favor of the more widely used term Combiners in fandom. It may also be noted that the US Marvel \"Generation 2\" comic issue #9 uses the term \"fusilateral-quintrocombiners\" for 5-member teams such as the Combaticons, (and presumably other \"Scramble City\" style-combiners).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the American animated series, and various comic incarnations, many Transformers have the ability to change size during transformation, while a glowing outline occasionally appears around their bodies as they do so. Notable examples include Megatron, Soundwave and his operatives Ravage, Rumble, Frenzy, Laserbeak, the transporter Astrotrain and the Autobot Blaster. Long since regarded as a source of confusion, such technology first received a full-scale explanation in the final issue of the \"Generation One\" version of Dreamwave Productions' series, which focused on many different aspects of Transformers technology and other information. In IDW Publishing's \"Transformers: Escalation\" it is also mentioned that it takes considerable power to accomplish this feat. During Optimus Prime and Prowl's conversation, it is hinted at that the technology is old, but has not been in use for some time. Megatron and Soundwave are the first Transformers in this continuity to display mass displacement. Although used in the animated series quite often, mass displacement was not utilized by any of the Autobots or Decepticons in the 2007 Transformers film, as the producers claimed that they considered it a form of \"cheating\". This claim is contradicted however, when the film depicts the Allspark displacing mass when it reduces itself from its Hoover Dam filling size to one that Sam is capable of carrying around.\nIn Transformers Prime, Predaking, a Predacon gets bigger when transforming into his alt mode ( read: dragon ), as well as fellow Predacons Darksteel and Skylynx, from the series finale movie Predacons Rising.\n\nIn the American comic and cartoon, the Master process was acquired from Nebulos. It allows humans or Nebulons to become components of Transformers using cybernetic exosuits in a process referred to as Binary Bonding (although not all Master processes involve bonding with humans). Still experimental, the results of this are unpredictable. Known Master technologies are:\n\n\nIn the extended Japanese continuity, the master process can use either humanoids or robots to form the binary bonding component. The original Headmasters, for example, were originally small Cybertronians, taken to the planet \"Master\" by Fortress, where they began to experiment with how to transform. They created large, lifeless Transformer bodies named \"Transtectors\" to which they connected as heads, forming the basis for Master technology. After returning to Cybertron, Headmaster technology (called the Masterforce) was modified to allow humans to become Headmasters; the first six were a group of teenagers known as the \"Headmaster Juniors\".\n\nWhile the Japanese concept for Headmasters featured only one mind in the process, Targetmasters did indeed consist of a bonding of two beings, as in America - in this case, a group of refugees from Master who were fused to the arms of several larger Transformers in a plasma energy explosion.\n\nIn Japan, Powermasters are known as \"Godmasters\", and, like Headmasters, consist of a human being bonded to a Transtector. The Godmasters were created as part of a plan by the super-energy being, Devil Z, to create the ultimate super robot lifeform, and possess control over the energies of the Earth, the heavens and man.\n\nAlthough made from a living metal, various Cybertronians have found sufficient cause or reason to incorporate purely organic material into their forms.\n\nPretenders, introduced in 1988, followed the concept of Transformers using organic shells as an extra disguise or as armor for defense. The concept was first seen in Marvel Comics with Thunderwing and Bludgeon as notable Decepticons. Dreamwave Productions detailed the process and its transition from simple power-upgrading armor to a whole host of potential future applications in their series. In IDW Publishing's rebooted comic, Pretender technology had the benefit of turning a Transformer into a supremely powerful being (for example, Thunderwing destroyed much of Cybertron) — but an improper grafting process could destroy the Transformer's mind.\n\nIn many series, it is shown that an organic shell could protect a Transformer's body from intense environmental conditions such as energon radiation. Dreamwave' \"More Than Meets The Eye\" series noted this as the logical progression for Pretenders technology, explaining that it had branched out to include specially designed suits for dealing with hazardous materials, etc. Similarly, in the IDW comic, Thunderwing had developed his Pretender technology, called \"Bio-Cybernetic Grafting\" as a way for Transformers to survive Cybertron's declining conditions. In \"Beast Wars\", The Maximals and Predacons on earth adopted organic shells and animal forms because of the high amounts of natural energon on the planet. Similarly, in \"\", The Dinobots assumed dinosaur forms with organic shells to protect themselves from energon radiation.\n\nTechno-Organic Hybrids are transformers who possess or have obtained organic halves (having been infused with an organic being's DNA) and can be considered a fusion of both technological & organic lifeforms at cellular level. Transformers of this type are featured prominently in the \"Beast Wars\" era and even more so in \"Beast Machines\" (with their existence and techno-organic nature being an important plot element). Their outward appearance often varies from slightly robotic, completely organic, or a combination of the two.\n\nIn \"\", transformers with organic halves are rare due to limited cybertronian interaction with organic life (planets with organic life are considered mostly off limits by the Autobots). Due to this unfamiliarity with organic life many Transformers have difficulty relating to it and some (like Sentinel Prime) have developed a \"phobia of organics\", with others seeing it is seen as a form of contamination. As a result, techno-organic transformers are likely to be viewed by other \"pure\" transformers with contempt (being regarded as freaks/abomination). These Transformers have their original abilities augmented by the genetic makeup of the organic they can transform into.\n\nIn this series, the Decepticon Blackarachnia is techno-organic reformat of Elita-1 (a female Autobot Elite Guard trainee), after she tried to \"download\" the abilities of an \"organic species of alien spider\" to neutralize the venom injected into her. But the attempt unintentually transformed into a techno-organic hybrid (and is considered by both herself and others a freak). Though she repeatedly tries to purge herself of her organic-half, she has failed every time (and it appears she is likely to remain as such). Her recent attempt involved duplicating the process using a transwarp generator, using it on Wasp and resulting in his reformating in Waspinator.\n\n<br>\nAnother character in the series, a girl named Sari Sumdac, was originally a cybertronian protoform, that mysteriously appeared in the lab of human inventor, Professor Isaac Sumdac. While investigating the mysterious \"\"liquid metal being\", he accidentally touched the protoform (which rendered him briefly unconscious), unintentually infusing the young protoform with his DNA, resulting in the birth of a redheaded \"techno-organic female clone\" of Sumdac (a \"techno-organic human/transformer hybrid\" in the form of a human girl). Professor Sumdac decides to unofficially adopt her as his daughter, Sari (Prof. Sumdac decided to keep the truth of her origins a secret from her, due in part to him being unaware of what had actually occurred). Unlike Blackarachnia and Waspinator, Sari was not actually a Transformer to begin with was and started off more organic at first, both in biology (possessing normal human biological functions) and in outward appearance.\n\nHer techno-organic nature (except for a few hints) is not fully revealed until the very end of the 2nd season, when her elbow is injured, tearing the skin and revealing a robotic joint underneath. After the truth is revealed, Sari's techno-organic nature becomes more apparent, being able to transform into a \"slightly robotic\" version of herself. Her techno-organic nature is further enhanced when she uses her \"All-Spark enhanced Key\" to reformat herself into a teenaged form, with her new body functioning like a \"techno-organic powersuit\"\" (equipped which energy-based roller blades and weapons).\n\nThe life essence of a Transformer is called a Spark, an incandescent sphere of light that is, in essence, both the heart and soul of the Transformer. From a scientific point of view it is said (By Rattrap in Beast Machines) to be composed of or at least containing positrons. Following the construction of the Transformer body, Sparks can be implanted in a variety of ways, depending on the continuity.\n\n\nOther methods of creation include:\n\n\nIn a few notable cases, Transformers have displayed the ability to physically age. Most Transformers' ages are clear from their personalities, defined by their life experiences - for example, Hot Shot would be a young adult and Hot Rod is the equivalent of a teenager while Kup, Jazz and Ironhide are old warhorses. Optimus Prime, although clearly a mature, older father figure to most Autobots is still considerably younger than the old veterans and even his arch-foe Megatron. In Kup's case, he physically appears older through simple wear to his body over time. However, at the opposite end of the specutrum, there is the most ancient Autobot, Alpha Trion, who has appeared in three distinct stages of life, (one million years old, three million years old, and twelve million years old), with a suitably different physical construction each time (including the growth of shaped metal that resembles facial \"hair\"). Transformers facial structures have even changed over time with brow crevices, angled cheek lines and discolored tooled metal appearing on their chins that show aging on their living metal skin. Notable examples of this are Prowl, Rodimus Prime, Galvatron, Blurr, Cyclonus, Scourge, Ultra Magnus and even the ancient destroyer Unicron. In the comics, an aged and mentally ravaged Rodimus Prime discusses \"Aspects of Evil\" with a soon to become turncoat autobot.\n\nAlthough aging takes its toll on performance and appearance, there are no known incidences of transformers dying from \"old age\". Presumably, with regular maintenance and Energon, a Transformer could live forever.\n\nStasis lock is an operational state meant to protect the spark of a Transformer following severe trauma; it is similar to a coma in humans. Stasis lock is known to be able to maintain the spark of a Transformer for millions of years if necessary. Once in stasis lock, outside intervention is required to reactivate the Transformer.\n\nIn the event of critical damage, outside life support mechanisms can support the body of a Transformer while the spark is transplanted. This is a rare occurrence as usually the spark is terminated quickly after the damage is inflicted. A notable occurrence of this happening is Optimus Primal taking the Autobot Matrix of Leadership, and Optimus Prime's spark, during the Beast Wars on prehistoric Earth while Teletraan I and other Maximals were repairing the incredible damage to his body.\n\nA CR Chamber (Critical Recovery Chamber) can reverse serious damage to Transformers. It was introduced as Maximal technology during \"Beast Wars\". The process takes time, but is not as lengthy as manual repair. A CR Chamber is used when a Transformer's internal repair processes cannot repair damage taken in a battle. It restores the bot to perfect physical condition. Problems with programming and data, such as viruses and core conscience damage, cannot be remedied and must be resolved manually. Similar technologies include the Predacon R Tank. The CR Chmamber/R Tank may also have protective qualities, as it possibly prevented Waspinator from becoming a Transmetal during a quantum surge after he fell into one, Rhinox and Dinobot may have also been protected by CR Chambers in the same manner. However, Tigertron, Airazor, Blackarachnia and Inferno also did not become Transmetals, despite not being protected, so it is more likely that the quantum surge simply did not affect all Transformers. Also some protoforms in Stasis Pods were mutated by the surge, such as Rampage's and the one used to replace Optimus Primal's destroyed body, while others, such as Fuzor Silverbolt's were unaffected. The Fuzor Quickstrike was noted to have Transmetal-like plating for his Scorpion half, but not his Cobra half and he lacked a vehicle mode.\n\nDeath of a Transformer can follow irreversible (mortal) stasis lock or be caused by a sudden traumatic injury (such as a close-proximity nuclear explosion, or spark excision). A few weapons, such as a high powered fusion cannon, are known to be powerful enough to cause severe enough damage to immediately terminate a Transformer. Also, while the utter destruction of a body can and usually does cause death, a Transformer can often survive total dismemberment. Notable examples include Optimus Prime (during the Generation 1 series), Ultra Magnus (during the movie), Starscream and Waspinator (repeatedly).\n\nTransformers who die from stasis lock usually turn a neutral gray, as the color leaves their living metal bodies. For example, Optimus Prime, various Insecticons and Seekers, Blaster, Soundwave, and Starscream (debatable, as his body was incinerated) have all turned gray upon death. Ultra Magnus did not turn gray, however, this may be because his actual body was encased in armor at the time. There is debate on whether the armor was constructed of \"living metal\" or not. Also, in , Iguanus' body turned a uniform purple upon his death.\n\nTransformers who die from mortal injury (as seen during the movie) have shown signature flashes in their optics, fire or smoke that discharges from their mouths or open wounds and bodily function failure depending on the type of energy that penetrates their outer shells and disables their inner circuits. For example, Brawn, Prowl, Ironhide and Ratchet were shot to death with explosive plasma blasts. Because they didn't die an immediate death or suffered a slow death from their wounds, their inherit colors seemed to remain (temporarily) intact. Transformers also show the ability to feel pain from more damaging wounds that crack, bends and even breaks parts of their living metal skin.\n\nIn the \"\" series debut (2007–2009), Optimus Prime was killed by an energy shockwave from the which was meant to drive Starscream away after they had fought each other. However, 's new AllSpark-upgraded key card instantly restored Optimus to life. When Optimus died, his body also turned gray, though he was not in stasis lock at the time; when he was restored, he changed back to his normal coloring.\n\nIt is known that some or all of a Transformer's essence moves to a separate spatial realm accessible by the Matrix or into the Matrix itself upon spark termination.\n\nOn rare occasions, a destroyed Transformer can be brought back to life using the proper knowledge. Quintessons have been known to do this. The Transformer creator Primus can also accomplish resurrections, but it is beyond the scope of modern Transformers technology. The method of a spark returning to the body has yet to be pinpointed, however it is believed an outside force, seen or unseen, may guide the spark back to the body, perhaps utilizing Zone Energy. Incidents of this occurring include the Quintesson revival of Optimus Prime and Rhinox's recovery of Optimus Primal after a transwarp explosion in space destroyed Primal's original body (and created a transwarp rift through which an ion trail could be created. If this feat is impossible with normal transformer technology, then the alien energy released by the detonation of the \"Vox\" device must be seen as responsible for access to \"the other side of the Matrix\" as well as producing part alien transmetal forms.)\n\nStarscream was also given a new physical body after death by Unicron. However, unlike normal sparks that disappear from this world, Starscream's spark was able to continue existing in the physical world as a ghost and had the power to possess and control over Transformers, such as Cyclonus and Scourge. In Beast Wars it is stated that Starscream's spark could also travel through time as well as space, explaining his possession of Waspinator, and is believed by Transformer scientists to be a rare mutant spark that is indestructible/immortal and therefore not subject to the normal laws governing death.\n\nIn \"Transformers: Revenge of the Fallen\", Megatron is also resurrected from the bottom of the ocean (where he was left in the first movie) to find The Fallen.\n\nThe Space Bridge is a technology that enables Transformers to traverse great distances-usually interstellar as indicated by the name-without the use of a spacecraft or alternate mode.\n\nThe Space Bridge was introduced as a Decepticon technology, typically enabling them to move back and forth between Earth and their homeworld of Cybertron while the Autobots were typically confined to the former. However, on several occasions the Autobots commandeered the Space Bridge for their own uses, usually to reach Cybertron to stop a Decepticon plot. Its use was largely discontinued after the Decepticons fled Cybertorn during the events of \"The Transformers Movie\" and began relying upon their alternate forms for travel.\n\nIn this series the term Space Bridge referred to the Autobots' means of traversing the planet unseen, a hidden road network that extends to most parts of the planet. Created by the Build Team, this Space Bridge is more limited than other versions, as it is a physical road rather than a portal or teleportation device.\n\nIn \"\", the Autobots and Decepticons utilize a similar technology to travel, opening portals through which they are able to travel vast distances. \"Transformers: Cybertron\" saw this means of transit replaced by the warp abilities of Vector Prime and other Cybertronians.\n\nIn this series the Autobots control a vast network of Space Bridges, which was one of the key forces behind their victory over the Decepticons in the Great War. The Space Bridge network is enabled by numerous relay stations across the galaxy, which individual Transformers and vessels can access in order to travel from one point to another. Omega Supreme was also equipped with his own Space Bridge drive, enabling him to teleport without accessing a Space Bridge.\n\nBoth Autobots and Decepticons utilize Space Bridge technology in the Aligned continuity, which also introduced a variant of the technology known as the Ground Bridge. This version is a less powerful variant of the Space Bridge, with its name referring to its more limited range, as it is only capable of transport to other points on the same planet or in nearby space.\n\nIn the 2007 Michael Bay-directed live-action film, Optimus Prime revealed to Sam Witwicky that the surviving Cybertronians managed to learn Earth's languages through the World Wide Web (with the auction site eBay being cited throughout the film as the primary source of information regarding the eyeglasses of Sam's great-grandfather, Captain Archibald Witwicky), hinting that the Transformers, being highly evolved sentient electronically composed beings, possess the capability to both interface with communications networks and protocols and glean information from Internet-accessible documents, files and applications. However, the capability of the Transformers to learn both written and spoken languages through the World Wide Web and translate between the Cybertronian language and Earth-human languages was not fully explained in the film.\n"}
{"id": "4161087", "url": "https://en.wikipedia.org/wiki?curid=4161087", "title": "Tudor Bompa", "text": "Tudor Bompa\n\nProf. Tudor Bompa PhD., one of the world's top specialists who revolutionized the training methods of athletes around the world, is an honorary guest of the Politehnica University Timișoara, who awarded him with the title of Doctor Honoris Causa during the Politehnica Week. The famous professor, who is behind the success of the athlete Ben Johnson, met on November 8 in Timisoara with athletes, coaches, and mass media representatives, who learned some of the secrets that underlie the top athletes' training. Tudor Bompa is known as the man who revolutionized Western training methods. After more than 40 years of work in the international sports arena, he is considered one of the world's top specialists in periodization, planning, performance and strength training.\n\nCurrently, Bompa is a Professor Emeritus at York University in Toronto, Ontario, Canada. He is married to Tamara Bompa who is an associate lecturer at York University.\n\nProfessor TUDOR BOMPA was born on 23 December 1932 in the town of Nasaud, in Northern Transylvania. He attended his first school classes in his hometown, and in 1949 he moved to the Sports School in Cluj-Napoca, under the patronage of the local “Victor Babeş” University. During his junior years, he was part of the national athletics team, and won several silver and bronze medals at the National Championships, in the pentathlon, javelin and discus competitions.\n\nIn his teens, Bompa played competitive soccer and competed in sprint and pentathlon. After an ankle injury, he swapped to rowing, which he found difficult as it is an endurance sport rather than a strength sport that he had been used to through his track and field career. At the 1958 European Rowing Championships, he won a silver medal in the coxed four event.\n\nHe attended the courses of the Institute of Physical Culture and Sports, a prestigious sports academy in the Romanian capital city, Bucharest, which he graduated at the age of 24, in 1956. In Bucharest, he activated within the Central Army House Club, with which he won six national titles at rowing and a silver medal in 1958 at the European Championship. As a student, but mainly after graduation, between 1960 and 1970, he served as Director of the Central Army House Sports Club, as university assistant professor at the Polytechnic Institute of Timisoara and the University of Bucharest, and as Athletic Director at Politehnica Timișoara Sports Club. Starting in 1960, he laid the foundations of new training methods for performance athletes, which were first published in the journal “Studies and Research” Politehnica Timişoara, T. BOMPA being part of the editorial board.\n\nAs a coach, Bompa trained 11 medalists in various Olympics (2 gold medals) and World championships in two sport disciplines: track and field and rowing. He revolutionized the training concepts in cross country skiing.\n\nThe system and training methods of the distinguished specialist are proven by the performance of “ Bompațs champions “, who won 11 medals (4 gold, 2 silver and 5 bronze) in the Olympic Games and World Championships; 3 medals (2 gold) in the Pan American Games; 22 medals in National Championships (in Canada and Romania).\n\nTwo outstanding results have crowned his permanent, uninterrupted concerns in the development and application of his methods in order to achieve the highest level of performance:\n\nMihaela Peneş, Romanian junior, javelin, Olympic champion in Tokyo in 1964. TUDOR BOMPA managed the physical training of young Mihaela Peneş, for a year and a half in the years 1963-1964. Thus, the 17-year-old unknown, who was not taken into account after a 51-meter qualification, became the Olympic champion in Tokyo with a 60.54-meter throw in the first attempt. She was the “bomb” of the Olympics, “the wonder of Tokyo,” who shocked the world.\n\nBen Johnson, Canadian Sprinter, Olympic Champion at the 1988 Olympics in Seoul, 100 meters. T. BOMPA’s best known “product” is the Canadian Sprinter Ben Johnson. In 1988, at the Seoul Olympics, in the “race of the century”, while competing with American sprinter Carl Lewis, Johnson arrived first in the 100-meter competition, with a new world record of 9.79 seconds. This was the peak of the confrontation between the two world speed titans.\n\nWith 15 books published, translated into 18 languages, sold in over one million copies and used in 180 countries, Prof. Tudor Bompa Ph.D. has received so far only in Romania the title of honorary citizen of his native town, Năsăud. Moreover, when awarding the title of Doctor Honoris Causa of the Politehnica University Timișoara, Prof. Tudor Bompa Ph.D. said, visibly touched, that \"in my trophy room, it will occupy the most important place.\" \n\n"}
{"id": "50239422", "url": "https://en.wikipedia.org/wiki?curid=50239422", "title": "Valeriy Makrushin", "text": "Valeriy Makrushin\n\nValeriy Grigoryevich Makrushin (Макрушин, Валерий Григорьевич) (born 14 January 1940, died 2003) was a cosmonaut for the Soviet Union.\n\nMakrushin joined the Chelomey Design Bureau after graduating from the Leningrad Institute of Aviation Instrumentation in 1963. He was recruited to a cosmonaut team on March 22, 1972 and was one of the first cosmonauts selected from this design bureau. He became the head of the Chelomey OKB-52 Mashinostroyeniya cosmonaut team until it was disbanded on April 8, 1987. Makrushin then worked on the Almaz military program with the design bureau until his retirement.\n"}
{"id": "206018", "url": "https://en.wikipedia.org/wiki?curid=206018", "title": "Weather station", "text": "Weather station\n\nA weather station is a facility, either on land or sea, with instruments and equipment for measuring atmospheric conditions to provide information for weather forecasts and to study the weather and climate. The measurements taken include temperature, atmospheric pressure, humidity, wind speed, wind direction, and precipitation amounts. Wind measurements are taken with as few other obstructions as possible, while temperature and humidity measurements are kept free from direct solar radiation, or insolation. Manual observations are taken at least once daily, while automated measurements are taken at least once an hour. Weather conditions out at sea are taken by ships and buoys, which measure slightly different meteorological quantities such as sea surface temperature (SST), wave height, and wave period. Drifting weather buoys outnumber their moored versions by a significant amount.\n\nTypical weather stations have the following instruments:\n\nIn addition, at certain automated airport weather stations, additional instruments may be employed, including:\n\nMore sophisticated stations may also measure the ultraviolet index, leaf wetness, soil moisture, soil temperature, water temperature in ponds, lakes, creeks, or rivers, and occasionally other data.\n\nExcept for those instruments requiring direct exposure to the elements (anemometer, rain gauge), the instruments should be sheltered in a vented box, usually a Stevenson screen, to keep direct sunlight off the thermometer and wind off the hygrometer. The instrumentation may be specialized to allow for periodic recording otherwise significant manual labour is required for record keeping. Automatic transmission of data, in a format such as METAR, is also desirable as many weather station's data is required for weather forecasting.\n\nA personal weather station is a set of weather measuring instruments operated by a private individual, club, association, or business (where obtaining and distributing weather data is not a part of the entity's business operation). Personal weather stations have become more advanced and can include many different sensors to measure weather conditions. These sensors can vary between models but most measure wind speed, wind direction, outdoor and indoor temperatures, outdoor and indoor humidity, barometric pressure, rainfall, and UV or solar radiation. Other available sensors can measure soil moisture, soil temperature, and leaf wetness. The quality, number of instruments, and placement of personal weather stations can vary widely, making the determination of which stations collect accurate, meaningful, and comparable data difficult. There are a comprehensive number of retail weather stations available.\n\nPersonal weather stations typically involve a digital console that provides readouts of the data being collected. These consoles may interface to a personal computer where data can be displayed, stored, and uploaded to websites or data ingestion/distribution systems. Open-source weather stations are available that are designed to be fully customizable by users.\n\nPersonal weather stations may be operated solely for the enjoyment and education of the owner, while some owners share their results with others. They do this by either by manually compiling data and distributing it, distributing data over the internet, or sharing data via amateur radio. The Citizen Weather Observer Program (CWOP) is a service which facilitates the sharing of information from personal weather stations. This data is submitted through use of software, a personal computer, and internet connection (or amateur radio) and are utilized by groups such as the National Weather Service (NWS) when generating forecast models. Each weather station submitting data to CWOP will also have an individual Web page that depicts the data submitted by that station. The Weather Underground Internet site is another popular destination for the submittal and sharing of data with others around the world. As with CWOP, each station submitting data to Weather Underground has a unique Web page displaying their submitted data. The UK Met Office's Weather Observations Website (WOW) also allows such data to be shared and displayed.\n\nHome weather stations include hygrometers, thermometers, barographs, and barometers. Commonly wall mounted and made by manufacturers such as Airguide, Taylor, Springfield, Sputnik and Stormoguide.\n\nA weather ship was a ship stationed in the ocean as a platform for surface and upper air meteorological measurements for use in weather forecasting. It was also meant to aid in search and rescue operations and to support transatlantic flights. The establishment of weather ships proved to be so useful during World War II that the International Civil Aviation Organization (ICAO) established a global network of 13 weather ships in 1948. Of the 12 left in operation in 1996, nine were located in the northern Atlantic ocean while three were located in the northern Pacific ocean. The agreement of the weather ships ended in 1990. Weather ship observations proved to be helpful in wind and wave studies, as they did not avoid weather systems like merchant ships tended to and were considered a valuable resource. The last weather ship was , known as weather station M (\"jilindras\") at 66°N, 02°E, run by the Norwegian Meteorological Institute. MS \"Polarfront\" was removed from service January 1, 2010. Since the 1960s this role has been largely superseded by satellites, long range aircraft and weather buoys. Weather observations from ships continue from thousands of voluntary merchant vessels in routine commercial operation; the Old Weather crowdsourcing project transcribes naval logs from before the era of dedicated ships.\n\nWeather buoys are instruments which collect weather and oceanography data within the world's oceans and lakes. Moored buoys have been in use since 1951, while drifting buoys have been used since the late 1970s. Moored buoys are connected with the seabed using either chains, nylon, or buoyant polypropylene. With the decline of the weather ship, they have taken a more primary role in measuring conditions over the open seas since the 1970s. During the 1980s and 1990s, a network of buoys in the central and eastern tropical Pacific ocean helped study the El Niño-Southern Oscillation. Moored weather buoys range from in diameter, while drifting buoys are smaller, with diameters of . Drifting buoys are the dominant form of weather buoy in sheer number, with 1250 located worldwide. Wind data from buoys has smaller error than that from ships. There are differences in the values of sea surface temperature measurements between the two platforms as well, relating to the depth of the measurement and whether or not the water is heated by the ship which measures the quantity.\n\nSynoptic weather stations are instruments which collect meteorological information at synoptic time 00h00, 06h00, 12h00, 18h00 (UTC) and at intermediate synoptic hours 03h00, 09h00, 15h00, 21h00 (UTC).\n\nThe common instruments of measure are anemometer, wind vane, pressure sensor, thermometer, hygrometer, and rain gauge.\n\nThe weather measures are formatted in special format and transmit to WMO to help the weather forecast model.\n\nA variety of land-based weather station networks have been set up globally. Some of these are basic to analyzing weather fronts and pressure systems, such as the synoptic observation network, while others are more regional in nature, known as mesonets.\n\n\n\n\n\n"}
{"id": "1304805", "url": "https://en.wikipedia.org/wiki?curid=1304805", "title": "Zhang Jiaxiang", "text": "Zhang Jiaxiang\n\nZhang Jiaxiang (; born in Oct. 1932), also known as Chia-Hsiang Chang, is a Chinese astronomer affiliated with Purple Mountain Observatory, Chinese Academy of Sciences, and president of the Minor Planet Foundation at that observatory.\n\nHe discovered 5384 Changjiangcun, an inner main-belt asteroid of the Hungaria family at the Purple Mountain Observatory in 1957 named after the village near Zhangjiagang, China. It is his only discovery credited by the Minor Planet Center under the name \"C.-H. Chang\".\n\nZhang Jiaxiang, astronomer, was born in Oct. 1932 in Nanjing, Jiangsu province. In 1951, he first joined the Purple Mountain Observatory as a technician, supervised by director Zhang Yuzhe.\n\nIn 1957, Zhang Yuzhe and Zhang Jiaxiang worked together and published a paper discussing the orbit of artificial satellites. From 1965 to 1972, Prof. Zhang led a research group to accomplish the project of orbit determination of the first Chinese artificial satellite and thereafter the systematic studies of the orbit of Chinese synchronous satellite. In total, they have discovered more than 150 internationally numbered new minor planets and four comets. In the 1990s, he accurately predicted a series of collision times between 19 comet nuclei and Jupiter, based on his self-established numerical model of the solar system dynamics. In the most recent 10 years, he was named as the chief scientist, leading a project of “Construction of Near Earth Object Telescope，” which has been successfully completed.\n\nTo recognize his contribution in his field, Harvard Smithsonian Observatory in Boston named the asteroid 4760 Jia-xiang after him.\n\n"}
