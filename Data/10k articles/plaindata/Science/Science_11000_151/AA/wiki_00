{"id": "13953606", "url": "https://en.wikipedia.org/wiki?curid=13953606", "title": "A588 road", "text": "A588 road\n\nThe A588 is a road in England, which runs from Poulton-le-Fylde to Lancaster in Lancashire. It is the main route serving the Over Wyre areas of the Fylde.\n\nThe road runs a total distance of , in a roughly north-easterly direction, and is largely rural in character. It begins at a junction with the A586 in Poulton-le-Fylde and runs north-east, as Breck Road, to meet the A585 eastbound at the River Wyre roundabout. After sharing with the A585 as Mains Lane for , it turns north (left) away from the A585, crossing the River Wyre at Shard Bridge, the last crossing of the river before its estuary. From here the road winds through the Over-Wyre villages of Hambleton, Stalmine and Pilling and across the marshy land that abuts the Cockerham Sands portion of Morecambe Bay. At Cockerham, almost from Shard Bridge, the road turns to the left, and, as Lancaster Road, runs a further north into Lancaster, eventually terminating as it joins the A6 by the Royal Lancaster Infirmary \n"}
{"id": "1027276", "url": "https://en.wikipedia.org/wiki?curid=1027276", "title": "ASCI Blue Pacific", "text": "ASCI Blue Pacific\n\nASCI Blue Pacific was a supercomputer installed at the Lawrence Livermore National Laboratory in Livermore, CA at the end of 1998. It was a collaboration between IBM Corporation and Lawerence Livermore Lab.\n\nIt was an IBM RS6000 SP \"massively parallel processing\" system. It contained 5,856 PowerPC 604e microprocessors. Its theoretical top performance was 3.9 teraflops. \n\nIt was built as a stage of the Accelerated Strategic Computing Initiative (ASCI) started by the U.S. Department of Energy and the National Nuclear Security Administration to build a simulator to replace live nuclear weapon testing following the moratorium on testing started by President George H. W. Bush in 1992 and extended by Bill Clinton in 1993.\n\n"}
{"id": "21770271", "url": "https://en.wikipedia.org/wiki?curid=21770271", "title": "A Wealth of Fable", "text": "A Wealth of Fable\n\nA Wealth of Fable by Harry Warner, Jr., is a Hugo Award-winning history of science fiction fandom of the 1950s, an essential reference work in the field. It is a followup to Warner's \"All Our Yesterdays\" (), which covered the 1940s, and helped to earn Warner a Hugo Award in 1969.\n\nAccording to science fiction fan and author Mike Resnick, \"It's not even a sequel, but rather a continuation, of \"All Our Yesterdays,\" heavily illustrated, obviously written by the same hand, chock full of the anecdotes that almost instantly become fannish legend.\"\n\nIt was originally published by Joe Siclari in a three-volume, mimeographed Fanhistorica Press edition in 1977. SCIFI Press brought out an expanded hardcover edition () in 1992. The members of the World Science Fiction Society voted that version the Hugo Award for Best Related Book.\n\nWarner also wrote a related series of historical columns called \"All Our Yesterdays.\"\n\n"}
{"id": "11847504", "url": "https://en.wikipedia.org/wiki?curid=11847504", "title": "Anita Conti", "text": "Anita Conti\n\nAnita Conti (\"née\" Caracotchian) (17 May 1899 – 25 December 1997) was a French explorer and photographer, and the first French female oceanographer.\n\nAnita Caracotchian was born in Ermont in Seine-et-Oise to a wealthy Armenian family. She spent her childhood being educated at home by different tutors and travelling with her family, gradually developing a passion for books and the sea.\n\nAfter moving to Paris, she concentrated on writing poems and the art of book binding. Her work got the attention of celebrities and she won different awards and prizes for her creativity in London, Paris, New York and Brussels.\n\nIn 1927, she married a diplomat, Marcel Conti, and started traveling around the world, exploring the seas, documenting and reporting what she saw and experimented. Spending time on the fishing boats for days and even months on certain occasions gave her a deeper understanding of the problematic faced by the fishermen. In between the two world war, she developed the technique of fishing maps apart from the already used navigational charts. For two years, from one vessel to another, she observed the French fishermen along the coast and Saharan Africa discovering fish species unknown in France. She published many scientific reports on the negative effects of industrial fishing and the different problems related to fishing practices.\n\nFrom 1943 and approximately for 10 years, she studied in the Mauritian islands, Senegal, Guinea and Ivory Coast, the nature of the seabed, different fish species and their nutritional values in regards of protein deficiency for the local populations. Gradually, she developed better preservation techniques, fishing methods and installed artificial dens for further studies. She even founded an experimental fishery for sharks. She became more and more conscientious of the misuse of natural resources by the fishing industry and the major waste that could be prevented.\n\nIn 1971 she published \"L’Ocean, Les Betes et L’Homme\", to denounce the disaster that men create and its effects on the oceans. Through many conferences and forums and for the rest of her life, she advocated for the betterment of the marine world.\n\nShe died on 25 December 1997 in Douarnenez.\n\n\n\n"}
{"id": "13951278", "url": "https://en.wikipedia.org/wiki?curid=13951278", "title": "Baby Glacier (Wyoming)", "text": "Baby Glacier (Wyoming)\n\nBaby Glacier is in the Bridger Wilderness of Bridger-Teton National Forest, in the U.S. state of Wyoming. The glacier is immediately west of the much larger Mammoth Glacier, both of which are on the west side of the Continental Divide in the northern Wind River Range. The glacier occupies a north facing cirque and flows northward from the slopes of Mount Whitecap. Baby Glacier is in the Bridger Wilderness and is part of the largest grouping of glaciers in the American Rocky Mountains.\n\n"}
{"id": "50900356", "url": "https://en.wikipedia.org/wiki?curid=50900356", "title": "Bortolo d'Alvise", "text": "Bortolo d'Alvise\n\nBortolo d'Alvise was a 16th-century Italian scientific instrument maker.\n\nHe was a Venetian glassmaker who, thanks to the negotiations by Grand Duke Cosimo I de' Medici (1519–1574) with the Venetian Republic, was called to Florence as a crystal-maker. He was documented in Florence as early as September 1569, and along with Jacomo and Alvise Della Luna was one of the finest Venetian glassmakers to settle in Florence. He remained there for about fourteen years, where he introduced new techniques. The inventory of his workshop records \"reticello\" (filigree) glass, engraved glass, ice glass, a gilded tray, and weave-pattern vases with handles.\n"}
{"id": "57119083", "url": "https://en.wikipedia.org/wiki?curid=57119083", "title": "Chavonda Jacobs-Young", "text": "Chavonda Jacobs-Young\n\nDr. Chavonda Jacobs-Young serves as the Administrator of the U.S. Department of Agriculture's Agricultural Research Service. The Agricultural Research Service is the USDA's chief, in-house scientific research agency. Dr. Jacobs-Young has been the administrator since February 2014. In 1998 Jacobs-Young became the first African-American to earn a Ph.D in paper science.\n\nDr. Jacobs-Young is a native of Augusta, Georgia. She graduated from Hephzibah High School in 1985. During high school and her time at North Carolina State University, she participated in the high jump event and was a three time Atlantic Coast Conference champion. Dr. Jacobs-Young earned a B.S. in paper science and engineering (1989, NC State) and an M.S. in wood and paper science (1992, NC State). Then in 1998, she earned her Ph.D in paper science from North Carolina State University. In 2008 Dr. Jacobs-Young received an Executive Leadership Certificate in Public Policy Implementation from American University in Washington, D.C.\n\nAfter completion of her Ph.D, she worked as an Assistant Professor of Paper Science and Engineering at the University of Washington from 1995 until 2002. In 2002, Dr. Jacobs-Young was approached about joining the government. She saw an opportunity to learn about federal service and took a job as a National Program Leader in the Cooperative State Research, Education, and Extension Service. Eventually she served as the senior policy analyst for agriculture in the White House Office of Science and Technology Policy. In this capacity she supported the President's science advisor and others within the Executive Office of the President on a variety of agricultural science activities. She worked across the Federal Government to improve interagency cooperation and collaboration on high-priority scientific issues. When she returned full time to USDA, Dr. Jacobs-Young served as the Director of the newly formed USDA Office of the Chief Scientist. There, she facilitated the coordination of scientific leadership across the Department and ensured the highest standards of intellectual rigor and scientific integrity for the research being disseminated from the department. Dr. Jacobs-Young then served as the acting director for the USDA's National Institute of Food and Agriculture before joining the Agricultural Research Service in 2012 as Associate Administrator for Research Programs. She is also a member of the United States Senior Executive Service.\n"}
{"id": "3176466", "url": "https://en.wikipedia.org/wiki?curid=3176466", "title": "Development anthropology", "text": "Development anthropology\n\nDevelopment anthropology refers to the application of anthropological perspectives to the multidisciplinary branch of development studies. It takes international development and international aid as primary objects. In this branch of anthropology, the term \"development\" refers to the social action made by different agents (e.g. institutions, businesses, states, or independent volunteers) who are trying to modify the economic, technical, political, or/and social life of a given place in the world, especially in impoverished, formerly colonized regions.\n\nDevelopment anthropologists share a commitment to simultaneously critique and contribute to projects and institutions that create and administer Western projects that seek to improve the economic well-being of the most marginalized, and to eliminate poverty. While some theorists distinguish between the anthropology of development (in which development is the object of study) and development anthropology (as an applied practice), this distinction is increasingly thought of as obsolete. With researches on the field, the anthropologist can describe, analyze, and understand the different actions of development that took and take place in a given place. The various impacts on the local population, environment, society, and economy are to be examined.\n\nIn 1971, Glynn Cochrane proposed development anthropology as a new field for practitioners interested in a career outside academia. Given the growing complexity of development assistance, Cochrane suggested that graduates needed to prepare themselves to work in interdisciplinary settings. In 1973, Cochrane was invited by the World Bank to make recommendations for the use of anthropology, and his report (which stressed the need for the systematic treatment of social issues) laid a foundation for future use of the discipline in the World Bank Group. Around ninety anthropologists are now employed by the World Bank Group in various roles.\n\nIn 1974, Bob Berg—of the United States Agency for International Development (USAID)—and Cochrane worked together, and, as a result, USAID introduced \"social soundness analysis\" as a project preparation requirement. This innovation led to the employment of more than seventy anthropologists. Social soundness analysis has now been in USAID use for over forty years. USAID ran an in-house development studies course in the 1970s, through which several hundred field personnel eventually passed. In addition to anthropology, the course covered development economics, regional and national planning, and institution building.\n\nIn the late 1970s, Thayer Scudder, Michael Horowitz, and David Brokensha established an Institute for Development Anthropology at the State University of New York at Binghamton. This institute has played an influential role in the continuing expansion of this branch of the discipline.\n\nBy the 1980s and 1990s, development anthropology began to be more widely used in the private sector. Corporate social responsibility and issues ranging from resettlement and human rights to micro-enterprise are now routinely addressed by systematic social assessment as an integral part of investment appraisal.\n\nCriticism of Western development became an important goal in the late 1980s, after the wake of severe economic crisis brought disease, poverty, and starvation to countries and sectors that were the focus of large Western structural adjustment development projects throughout Latin America, Africa, and other parts of the former colonial world. Despite the failure of many of these development projects, and some 40 years of post World War II funding from the US and Europe, scholars know that development has been the key way that Western post-industrialized countries intervene in non-Western society. Development criticism seeks to discover why, given the funds and best intentions of volunteers and policy makers, do the majority of development projects continue to fail to (1) redistribute economic power and resources in a way that helps the poorest sectors of society, and (2) to create economic growth that is sustainable in the country.\n\nAnthropologists who study development projects themselves have criticized the fundamental structure of Western development projects coming from such institutions as USAID and bilateral lenders such as the World Bank. Because they are often working from the perspective of the objects of development in the non-Western world, rather than from within the aid institutions, anthropologists encountering such projects have a unique perspective from which to see the problems. Anthropologists write with concern about the ways that non-Western objects of aid have been left out of the widespread drive to develop after World War II, especially in the ways that such projects limit solutions to poverty in the form of narrow Western capitalist models that promote exploitation and the destruction of household farms, or, more suspiciously, naturalise inequality between Western post-industrialized countries and former colonial subjects.\n\nSome describe the anthropological critique of development as one that pits modernization and an eradication of the indigenous culture, but this is too reductive and not the case with the majority of scholarly work. In fact, most anthropologists who work in impoverished areas desire the same economic relief for the people they study as policymakers; however, they are wary about the assumptions and models on which development interventions are based. Anthropologists and others who critique development projects instead view Western development itself as a product of Western culture that must be refined in order to better help those it claims to aid. The problem therefore is not that of markets driving out culture, but of the fundamental blind-spots of Western developmental culture itself. Criticism often focuses therefore on the cultural bias and blind-spots of Western development institutions, or modernization models that systematically represent non-Western societies as more deficient than the West; erroneously assume that Western modes of production and historical processes are repeatable in all contexts; or that do not take into account hundreds of years of colonial exploitation by the West that has tended to destroy the resources of former colonial society. Most critically, anthropologists argue that sustainable development requires at the very least more involvement of the people who the project aims to target in the project's creation, its management, and its decision-making processes.\n\nA major critique of development from anthropologists came from Arturo Escobar's seminal book \"Encountering Development\", which argued that Western development largely exploited non-Western peoples and enacted an Orientalism (see Edward Said). Escobar even sees international development as a means for the Occident to keep control over the resources of former colonies. Escobar shows that, between 1945 and 1960, the former colonies were going through the decolonization era, and the development plan helped to maintain the third world's dependency on the old metropole. Development projects themselves flourished in the wake of World War II, and during the Cold War, when they were developed to (1) stop the spread of communism with the spread of capitalist markets, and (2) create more prosperity for the West and its products by creating a global consumer demand for finished Western products abroad. Some scholars blame the different agents for having only considered a small aspect of the local people's lives without analyzing broader consequences, while others like dependency theory or Escobar argue that development projects are doomed to failure for the fundamental ways they privilege Western industry and corporations. Escobar's argument echos the earlier work of dependency theory and follows a larger critique more recently posed by Michel Foucault and other post-structuralists.\n\nMore recent studies like James Ferguson's \"The Anti-Politics Machine\" argue that the ideas and institutional structure that support Western development projects are fundamentally flawed because of the way the West continues to represent the former colonial world. International development uses an \"anti-politics\" that ultimately produces failure, despite the best intentions. Finally, studies also point out how development efforts often attempt to de-politicize change by a focus on instrumental assistance (like a school building) but not on the objective conditions that led to the development failure (e.g., the state's neglect of rural children at the expense of urban elite), nor the content of what the school might or might not teach. In this sense, the critique of international development focuses on the insidious effects of projects that at the least offer band-aids that address symptoms but not causes, and that at the worst promote projects that systematically redirect economic resources and profit to the West.\n\nWhile anthropological studies critique the Western assumptions and political context of development projects, anthropologists also consult on and work within aid institutions in the creation and implementation of development projects. While economists look at aggregate measures like gross national product and per capita income, as well as measures of income distribution and economic inequality in a society, anthropologists can provide a more fine-grained analysis of the qualitative information behind these numbers, such as the nature of the social groups involved and the social significance of the composition of income. Thus, development anthropologists often deal with assessing the important qualitative aspects of development sometimes ignored by an economic approach.\n\n\n"}
{"id": "13433538", "url": "https://en.wikipedia.org/wiki?curid=13433538", "title": "District Regionalism", "text": "District Regionalism\n\nDistrict Regionalism is a finance-related urban planning method. The practice creates a neighborhood stock system in which community shares are sold. The term \"District Regionalism\" was initially used by the administration of the city of Guthrie, Kentucky as the distinctive traits of each neighborhood emerged during the trial study. The University of Kentucky's College of Design became instrumental in devising the study concept, parameters and goals. The practice concerns organizing cities into smaller communities by allowing creating and distinctive regional and cultural references emerge within neighborhoods by allowing the public to transform what is typically considered as government owned areas and blighted properties. The practice has been proven to alter larger sprawling co-dependant cities into smaller self-reliant communities.\n\nCommunity shares are sold in District Regionalist environments. As a community is defined by boundaries and image, the collective population of that community issues shares with monetary value. Initially, stock ownership is given to the community in order to promote the process. An increased number of shares allows a larger amount of voting privileges within that community, therefore allowing some margin within city ordinances and zoning. As more community shares are sold, the pool increases for that community allowing them greater ability to project that money towards community projects. In a District Regionalist community, the city never loses over 51% of shares, allowing the city government to retain rights to city property and have majority rule voting privileges. As improvements are made to community properties, land values increase and improve the value of private properties within those districts. Cities can also use the community district funding pool to match grants at the discretion of a district's approval. The share system operates on the premise that communities will care for what they own, as opposed to caring for properties owned by cities.\n"}
{"id": "5160010", "url": "https://en.wikipedia.org/wiki?curid=5160010", "title": "Emotion in animals", "text": "Emotion in animals\n\nThe existence and nature of emotions in animals are believed to be correlated with those of humans and to have evolved from the same mechanisms. Charles Darwin was one of the first scientists to write about the subject, and his observational (and sometimes anecdotal) approach has since developed into a more robust, hypothesis-driven, scientific approach. Cognitive bias tests and learned helplessness models, have shown feelings of optimism and pessimism in a wide range of species including rats, dogs, cats, rhesus macaques, sheep, chicks, starlings, pigs, and honeybees.\n\nSome behaviourists, such as John B. Watson, claim that stimulus–response models provide a sufficient explanation for animal behaviours that have been described as emotional, and that all behaviour, no matter how complex, can be reduced to a simple stimulus-response association. Watson described that the purpose of psychology was \"to predict, given the stimulus, what reaction will take place; or given the reaction, state what the situation or stimulus is that has caused the reaction\".\n\nThe word \"emotion\" dates back to 1579, when it was adapted from the French word \"émouvoir\", which means \"to stir up\". However, the earliest precursors of the word likely date back to the very origins of language.\n\nEmotions have been described as discrete and consistent responses to internal or external events which have a particular significance for the organism. Emotions are brief in duration and consist of a coordinated set of responses, which may include physiological, behavioural, and neural mechanisms. Emotions have also been described as the result of evolution because they provided good solutions to ancient and recurring problems that faced ancestors.\n\nIt has been proposed that negative, withdrawal-associated emotions are processed predominantly by the right hemisphere, whereas the left hemisphere is largely responsible for processing positive, approach-related emotions. This has been called the \"laterality-valence hypothesis\".\n\nIn humans, a distinction is sometimes made between \"basic\" and \"complex\" emotions. Six emotions have been classified as basic: anger, disgust, fear, happiness, sadness and surprise. Complex emotions would include contempt, jealousy and sympathy. However, this distinction is difficult to maintain, and animals are often said to express even the complex emotions.\n\nPrior to the development of animal sciences such as comparative psychology and ethology, interpretation of animal behaviour tended to favour a minimalistic approach known as behaviourism. This approach refuses to ascribe to an animal a capability beyond the least demanding that would explain a behaviour; anything more than this is seen as unwarranted anthropomorphism. The behaviourist argument is, why should humans postulate consciousness and all its near-human implications in animals to explain some behaviour, if mere stimulus-response is a sufficient explanation to produce the same effects?\n\nThe cautious wording of Dixon exemplifies this viewpoint:\nMoussaieff Masson and McCarthy describe a similar view (with which they disagree):\nBecause of the philosophical questions of consciousness and mind that are involved, many scientists have stayed away from examining animal and human emotion, and have instead studied measurable brain functions through neuroscience.\n\nIn 1903, C. Lloyd Morgan published Morgan's Canon, a specialised form of Occam's razor used in ethology, in which he stated:\nCharles Darwin initially planned to include a chapter on emotion in \"The Descent of Man\" but as his ideas progressed they expanded into a book, \"The Expression of the Emotions in Man and Animals\". Darwin proposed that emotions are adaptive and serve a communicative and motivational function, and he stated three principles that are useful in understanding emotional expression: First, \"The Principle of Serviceable Habits\" takes a Lamarckian stance by suggesting that emotional expressions that are useful will be passed on to the offspring. Second, \"The Principle of Antithesis\" suggests that some expressions exist merely because they oppose an expression that is useful. Third, \"The Principle of the Direct Action of the Excited Nervous System on the Body\" suggests that emotional expression occurs when nervous energy has passed a threshold and needs to be released.\n\nDarwin saw emotional expression as an outward communication of an inner state, and the form of that expression often carries beyond its original adaptive use. For example, Darwin remarks that humans often present their canine teeth when sneering in rage, and he suggests that this means that a human ancestor probably utilized their teeth in aggressive action. A domestic dog's simple tail wag may be used in subtly different ways to convey many meanings as illustrated in Darwin's \"The Expression of the Emotions in Man and Animals\" published in 1872.\n\nEvidence for emotions in animals has been primarily anecdotal, from individuals who interact with pets or captive animals on a regular basis. However, critics of animals having emotions often suggest that anthropomorphism is a motivating factor in the interpretation of the observed behaviours. Much of the debate is caused by the difficulty in defining emotions and the cognitive requirements thought necessary for animals to experience emotions in a similar way to humans. The problem is made more problematic by the difficulties in testing for emotions in animals. What is known about human emotion is almost all related or in relation to human communication.\n\nIn recent years, the scientific community has become increasingly supportive of the idea of emotions in animals. Scientific research has provided insight into similarities of physiological changes between humans and animals when experiencing emotion.\n\nMuch support for animal emotion and its expression results from the notion that feeling emotions doesn't require significant cognitive processes, rather, they could be motivated by the processes to act in an adaptive way, as suggested by Darwin. Recent attempts in studying emotions in animals have led to new constructions in experimental and information gathering. Professor Marian Dawkins suggested that emotions could be studied on a functional or a mechanistic basis. Dawkins suggests that merely mechanistic or functional research will provide the answer on its own, but suggests that a mixture of the two would yield the most significant results.\n\nFunctional approaches rely on understanding what roles emotions play in humans and examining that role in animals. A widely used framework for viewing emotions in a functional context is that described by Oatley and Jenkins who see emotions as having three stages: (i) appraisal in which there is a conscious or unconscious evaluation of an event as relevant to a particular goal. An emotion is positive when that goal is advanced and negative when it is impeded (ii) action readiness where the emotion gives priority to one or a few kinds of action and may give urgency to one so that it can interrupt or compete with others and (iii) physiological changes, facial expression and then behavioural action. The structure, however, may be too broad and could be used to include all the animal kingdom as well as some plants.\n\nThe second approach, mechanistic, requires an examination of the mechanisms that drive emotions and search for similarities in animals.\n\nThe mechanistic approach is utilized extensively by Paul, Harding and Mendl. Recognizing the difficulty in studying emotion in non-verbal animals, Paul et al. demonstrate possible ways to better examine this. Observing the mechanisms that function in human emotion expression, Paul et al. suggest that concentration on similar mechanisms in animals can provide clear insights into the animal experience. They noted that in humans, cognitive biases vary according to emotional state and suggested this as a possible starting point to examine animal emotion. They propose that researchers may be able to use controlled stimuli which have a particular meaning to trained animals to induce particular emotions in these animals and assess which types of basic emotions animals can experience.\n\nA cognitive bias is a pattern of deviation in judgment, whereby inferences about other animals and situations may be drawn in an illogical fashion. Individuals create their own \"subjective social reality\" from their perception of the input. It refers to the question \"Is the glass half empty or half full?\", used as an indicator of optimism or pessimism.\nTo test this in animals, an individual is trained to anticipate that stimulus A, e.g. a 20 Hz tone, precedes a positive event, e.g. highly desired food is delivered when a lever is pressed by the animal. The same individual is trained to anticipate that stimulus B, e.g. a 10 Hz tone, precedes a negative event, e.g. bland food is delivered when the animal presses a lever. The animal is then tested by being played an intermediate stimulus C, e.g. a 15 Hz tone, and observing whether the animal presses the lever associated with the positive or negative reward, thereby indicating whether the animal is in a positive or negative mood. This might be influenced by, for example, the type of housing the animal is kept in.\n\nUsing this approach, it has been found that rats which are subjected to either handling or playful, experimenter-administered manual stimulation (tickling) showed different responses to the intermediate stimulus: rats exposed to tickling were more optimistic. The authors stated that they had demonstrated \"...for the first time a link between the directly measured positive affective state and decision making under uncertainty in an animal model.\"\n\nCognitive biases have been shown in a wide range of species including rats, dogs, rhesus macaques, sheep, chicks, starlings and honeybees.\n\nHumans can suffer from a range of emotional or mood disorders such as depression, anxiety, fear and panic. To treat these disorders, scientists have developed a range of psychoactive drugs such as anxiolytics. Many of these drugs are developed and tested by using a range of laboratory species. It is inconsistent to argue that these drugs are effective in treating human emotions whilst denying the experience of these emotions in the laboratory animals on which they have been developed and tested.\n\nStandard laboratory cages prevent mice from performing several natural behaviours for which they are highly motivated. As a consequence, laboratory mice sometimes develop abnormal behaviours indicative of emotional disorders such as depression and anxiety. To improve welfare, these cages are sometimes enriched with items such as nesting material, shelters and running wheels. Sherwin and Ollson tested whether such enrichment influenced the consumption of Midazolam, a drug widely used to treat anxiety in humans. Mice in standard cages, standard cages but with unpredictable husbandry, or enriched cages, were given a choice of drinking either non-drugged water or a solution of the Midazolam. Mice in the standard and unpredictable cages drank a greater proportion of the anxiolytic solution than mice from enriched cages, indicating that mice from the standard and unpredictable laboratory caging may have been experiencing greater anxiety than mice from the enriched cages.\n\nSpindle neurons are specialised cells found in three very restricted regions of the human brain – the anterior cingulate cortex, the frontoinsular cortex and the dorsolateral prefrontal cortex. The first two of these areas regulate emotional functions such as empathy, speech, intuition, rapid \"gut reactions\" and social organization in humans. Spindle neurons are also found in the brains of humpback whales, fin whales, killer whales, sperm whales, bottlenose dolphin, Risso's dolphin, beluga whales, and the African and Asian elephants.\n\nWhales have spindle cells in greater numbers and are maintained for twice as long as humans. The exact function of spindle cells in whale brains is still not understood, but Hof and Van Der Gucht believe that they act as some sort of \"high-speed connections that fast-track information to and from other parts of the cortex\". They compared them to express trains that bypass unnecessary connections, enabling organisms to instantly process and act on emotional cues during complex social interactions. However, Hof and Van Der Gucht clarify that they do not know the nature of such feelings in these animals and that we cannot just apply what we see in great apes or ourselves to whales. They believe that more work is needed to know whether emotions are the same for humans and whales.\n\nFrom at least the time of Darwin, it has been known that chimpanzees and other great apes also perform a laugh-like vocalization.\n\nResearch with rats has revealed that under particular conditions, they emit 50-kHz ultrasonic vocalisations (USV) which have been postulated to reflect a positive affective state (emotion) analogous to primitive human joy; these calls have been termed \"laughter\". The 50 kHz USVs in rats are uniquely elevated by hedonic stimuli—such as tickling, rewarding electrical brain stimulation, amphetamine injections, mating, play, and aggression—and are suppressed by aversive stimuli. Of all manipulations that elicit 50 kHz chirps in rats, tickling by humans elicits the highest rate of these calls.\n\nSome vocalizations of domestic cats, such as purring, are well-known to be produced in situations of positive valence, such as mother kitten interactions, contacts with familiar partner, or during tactile stimulation with inanimate objects as when rolling and rubbing. Therefore, purring can be generally considered as an indicator of \"pleasure\" in cats.\n\nLow pitched bleating in sheep has been associated with some positive-valence situations, as they are produced by males as an estrus female is approaching or by lactating mothers while licking and nursing their lambs.\n\nThe argument that animals experience emotions is sometimes rejected due to a lack of evidence, and those who don't believe in the idea of animal intelligence, often argue that anthropomorphism plays a role in individuals' perspectives. Those who reject that animals have the capacity to experience emotion do so mainly by referring to inconsistencies in studies that have endorsed the belief emotions exist. Having no linguistic means to communicate emotion beyond behavioral response interpretation, the difficulty of providing an account of emotion in animals relies heavily on interpretive experimentation, that relies on results from human subjects.\n\nSome people oppose the concept of animal emotions and suggest that emotions aren't universal, including in humans. If emotions are not universal, this indicates that there is not a phylogenetic relationship between human and non-human emotion. The relationship drawn by proponents of animal emotion, then, would be merely a suggestion of mechanistic features that promote adaptivity, but lack the complexity of human emotional constructs. Thus, a social life-style may play a role in the process of basic emotions developing into more complex emotions.\n\nDarwin concluded, through a survey, that humans share universal emotive expressions and suggested that animals likely share in these to some degree. Social constructionists disregard the concept that emotions are universal. Others hold an intermediate stance, suggesting that basic emotional expressions and emotion are universal but the intricacies are developed culturally. A study by Elfenbein and Ambady indicated that individuals within a particular culture are better at recognising other cultural members' emotions.\n\nPrimates, in particular great apes, are candidates for being able to experience empathy and theory of mind. Great apes have complex social systems; young apes and their mothers have strong bonds of attachment and when a baby chimpanzee or gorilla dies, the mother will not uncommonly carry the body around for several days. Jane Goodall has described chimpanzees as exhibiting mournful behavior. Koko, a gorilla trained to use sign language, was reported to have expressed vocalisations indicating sadness after the death of her pet cat, All Ball.\n\nBeyond such anecdotal evidence, support for empathetic reactions has come from experimental studies of rhesus macaques. Macaques refused to pull a chain that delivered food to themselves if doing so also caused a companion to receive an electric shock. This inhibition of hurting another conspecific was more pronounced between familiar than unfamiliar macaques, a finding similar to that of empathy in humans.\n\nFurthermore, there has been research on consolation behavior in chimpanzees. De Waal and Aureli found that third-party contacts attempt to relieve the distress of contact participants by consoling (e.g. making contact, embracing, grooming) recipients of aggression, especially those that have experienced more intense aggression. Researchers were unable to replicate these results using the same observation protocol in studies of monkeys, demonstrating a possible difference in empathy between monkeys and apes.\n\nOther studies have examined emotional processing in the great apes. Specifically, chimpanzees were shown video clips of emotionally charged scenes, such as a detested veterinary procedure or a favorite food, and then were required to match these scenes with one of two species-specific facial expressions: \"happy\" (a play-face) or \"sad\" (a teeth-baring expression seen in frustration or after defeat). The chimpanzees correctly matched the clips to the facial expressions that shared their meaning, demonstrating that they understand the emotional significance of their facial expressions. Measures of peripheral skin temperature also indicated that the video clips emotionally affected the chimpanzees.\n\nIn 1998, Jaak Panksepp proposed that all mammalian species are equipped with brains capable of generating emotional experiences. Subsequent work examined studies on rodents to provide foundational support for this claim. One of these studies examined whether rats would work to alleviate the distress of a conspecific. Rats were trained to press a lever to avoid the delivery of an electric shock, signaled by a visual cue, to a conspecific. They were then tested in a situation in which either a conspecific or a Styrofoam block was hoisted into the air and could be lowered by pressing a lever. Rats that had previous experience with conspecific distress demonstrated greater than ten-fold more responses to lower a distressed conspecific compared to rats in the control group, while those who had never experienced conspecific distress expressed greater than three-fold more responses to lower a distressed conspecific relative to the control group. This suggests that rats will actively work to reduce the distress of a conspecific, a phenomenon related to empathy. Comparable results have also been found in similar experiments designed for monkeys.\n\nLangford et al. examined empathy in rodents using an approach based in neuroscience. They reported that (1) if two mice experienced pain together, they expressed greater levels of pain-related behavior than if pain was experienced individually, (2) if experiencing different levels of pain together, the behavior of each mouse was modulated by the level of pain experienced by its social partner, and (3) sensitivity to a noxious stimulus was experienced to the same degree by the mouse observing a conspecific in pain as it was by the mouse directly experiencing the painful stimulus. The authors suggest this responsiveness to the pain of others demonstrated by mice is indicative of emotional contagion, a phenomenon associated with empathy, which has also been reported in pigs. One behaviour associated with fear in rats is freezing. If female rats experience electric shocks to the feet and then witness another rat experiencing similar footshocks, they freeze more than females without any experience of the shocks. This suggests empathy in experienced rats witnessing another individual being shocked. Furthermore, the demonstrator's behaviour was changed by the behaviour of the witness; demonstrators froze more following footshocks if their witness froze more creating an empathy loop.\n\nSeveral studies have also shown rodents can respond to a conditioned stimulus that has been associated with the distress of a conspecific, as if it were paired with the direct experience of an unconditioned stimulus. These studies suggest that rodents are capable of shared affect, a concept critical to empathy.\n\nAlthough not direct evidence that horses experience emotions, a 2016 study showed that domestic horses react differently to seeing photographs of positive (happy) or negative (angry) human facial expressions. When viewing angry faces, horses look more with their left eye which is associated with perceiving negative stimuli. Their heart rate also increases more quickly and they show more stress-related behaviours. One rider wrote, 'Experienced riders and trainers can learn to read the subtle moods of individual horses according to wisdom passed down from one horseman to the next, but also from years of trial-and-error. I suffered many bruised toes and nipped fingers before I could detect a curious swivel of the ears, irritated flick of the tail, or concerned crinkle above a long-lashed eye.' This suggests that horses have emotions and display them physically but it isn't concrete evidence.\n\nMarc Bekoff reported accounts of animal behaviour which he believed was evidence of animals being able to experience emotions in his book \"The Emotional Lives of Animals\". The following is an excerpt from his book:\nBystander affiliation is believed to represent an expression of empathy in which the bystander tries to console a conflict victim and alleviate their distress. There is evidence for bystander affiliation in ravens (e.g. contact sitting, preening, or beak-to-beak or beak-to-body touching) and also for solicited bystander affiliation, in which there is post-conflict affiliation from the victim to the bystander. This indicates that ravens may be sensitive to the emotions of others, however, relationship value plays an important role in the prevalence and function of these post-conflict interactions.\n\nThe capacity of domestic hens to experience empathy has been studied. Mother hens show one of the essential underpinning attributes of empathy: the ability to be affected by, and share, the emotional state of their distressed chicks. However, evidence for empathy between familiar adult hens has not yet been found.\n\nSome research indicates that domestic dogs may experience negative emotions in a similar manner to humans, including the equivalent of certain chronic and acute psychological conditions. Much of this is from studies by Martin Seligman on the theory of learned helplessness as an extension of his interest in depression:\nA further series of experiments showed that, similar to humans, under conditions of long-term intense psychological stress, around one third of dogs do not develop learned helplessness or long term depression. Instead these animals somehow managed to find a way to handle the unpleasant situation in spite of their past experience. The corresponding characteristic in humans has been found to correlate highly with an explanatory style and optimistic attitude that views the situation as \"other than\" personal, pervasive, or permanent.\n\nSince these studies, symptoms analogous to clinical depression, neurosis, and other psychological conditions have also been accepted as being within the scope of emotion in domestic dogs. The postures of dogs may indicate their emotional state. In some instances, the recognition of specific postures and behaviors can be learned.\n\nPsychology research has shown that when humans gaze at the face of another human, the gaze is not symmetrical; the gaze instinctively moves to the right side of the face to obtain information about their emotions and state. Research at the University of Lincoln shows that dogs share this instinct when meeting a human, and only when meeting a human (i.e. not other animals or other dogs). They are the only non-primate species known to share this instinct.\n\nThe existence and nature of personality traits in dogs have been studied (15,329 dogs of 164 different breeds). Five consistent and stable \"narrow traits\" were identified, described as playfulness, curiosity/fearlessness, chase-proneness, sociability and aggressiveness. A further higher order axis for shyness–boldness was also identified.\n\nDogs presented with images of either human or dog faces with different emotional states (happy/playful or angry/aggressive) paired with a single vocalization (voices or barks) from the same individual with either a positive or negative emotional state or brown noise. Dogs look longer at the face whose expression is congruent to the emotional state of the vocalization, for both other dogs and humans. This is an ability previously known only in humans. The behavior of a dog can not always be an indication its friendliness. This is because when a dog wags its tail, most people interpret this as the dog expressing happiness and friendliness. Though indeed tail wagging can express these positive emotions, tail wagging is also an indication of fear, insecurity, challenging of dominance, establishing social relationships or a warning that the dog may bite.\n\nSome researchers are beginning to investigate the question of whether dogs have emotions with the help of magnetic resonance imaging.\n\nIt has been postulated that domestic cats can learn to manipulate their owners through vocalizations that are similar to the cries of human babies. Some cats learn to add a purr to the vocalization, which makes it less harmonious and more dissonant to humans, and therefore harder to ignore. Individual cats learn to make these vocalizations through trial-and-error; when a particular vocalization elicits a positive response from a human, the probability increases that the cat will use that vocalization in the future.\n\nGrowling can be an expression of annoyance or fear, similar to humans. When annoyed or angry, a cat wriggles and thumps its tail much more vigorously than when in a contented state. In larger felids such as lions, what appears to be irritating to them varies between individuals. A male lion may let his cubs play with his mane or tail, or he may hiss and hit them with his paws. Domestic male cats also have variable attitudes towards their family members, for example, older male siblings tend not to go near younger or new siblings and may even show hostility toward them.\n\nHoneybees (\"Apis mellifera carnica\") were trained to extend their proboscis to a two-component odour mixture (CS+) predicting a reward (e.g., 1.00 or 2.00 M sucrose) and to withhold their proboscis from another mixture (CS−) predicting either punishment or a less valuable reward (e.g., 0.01 M quinine solution or 0.3 M sucrose). Immediately after training, half of the honeybees were subjected to vigorous shaking for 60 s to simulate the state produced by a predatory attack on a concealed colony. This shaking reduced levels of octopamine, dopamine, and serotonin in the hemolymph of a separate group of honeybees at a time point corresponding to when the cognitive bias tests were performed. In honeybees, octopamine is the local neurotransmitter that functions during reward learning, whereas dopamine mediates the ability to learn to associate odours with quinine punishment. If flies are fed serotonin, they are more aggressive; flies depleted of serotonin still exhibit aggression, but they do so much less frequently.\n\nWithin 5 minutes of the shaking, all the trained bees began a sequence of unreinforced test trials with five odour stimuli presented in a random order for each bee: the CS+, the CS−, and three novel odours composed of ratios intermediate between the two learned mixtures. Shaken honeybees were more likely to withhold their mouthparts from the CS− and from the most similar novel odour. Therefore, agitated honeybees display an increased expectation of bad outcomes similar to a vertebrate-like emotional state. The researchers of the study stated that, \"Although our results do not allow us to make any claims about the presence of negative subjective feelings in honeybees, they call into question how we identify emotions in any non-human animal. It is logically inconsistent to claim that the presence of pessimistic cognitive biases should be taken as confirmation that dogs or rats are anxious but to deny the same conclusion in the case of honeybees.\"\n\nCrayfish naturally explore new environments but display a general preference for dark places. A 2014 study on the freshwater crayfish \"Procambarus clarkii\" tested their responses in a fear paradigm, the elevated plus maze in which animals choose to walk on an elevated cross which offers both aversive and preferable conditions (in this case, two arms were lit and two were dark). Crayfish which experienced an electric shock displayed enhanced fearfulness or anxiety as demonstrated by their preference for the dark arms more than the light. Furthermore, shocked crayfish had relatively higher brain serotonin concentrations coupled with elevated blood glucose, which indicates a stress response. Moreover, the crayfish calmed down when they were injected with the benzodiazepine anxiolytic, chlordiazepoxide, used to treat anxiety in humans, and they entered the dark as normal. The authors of the study concluded \"...stress-induced avoidance behavior in crayfish exhibits striking homologies with vertebrate anxiety.\"\n\nA follow-up study using the same species confirmed the anxiolytic effect of chlordiazepoxide, but moreover, the intensity of the anxiety-like behaviour was dependent on the intensity of the electric shock until reaching a plateau. Such a quantitative relationship between stress and anxiety is also a very common feature of human and vertebrate anxiety.\n\n"}
{"id": "23661142", "url": "https://en.wikipedia.org/wiki?curid=23661142", "title": "Energetically Autonomous Tactical Robot", "text": "Energetically Autonomous Tactical Robot\n\nThe Energetically Autonomous Tactical Robot (EATR) was a project by Robotic Technology Inc. (RTI) and Cyclone Power Technologies Inc. to develop a robotic vehicle that could forage for plant biomass to fuel itself, theoretically operating indefinitely. It was being developed as a concept as part of the DARPA military projects for the United States military.\n\nThe project elicited some internet and media rumors after news circulated that the robot would (or at least could) ingest human remains. Cyclone Power Technologies stated that animal or human biomass was not intended to be used in the \"waste heat\" combustion engine of the robot, and that sensors would be able to distinguish foraged materials, although the project overview from RTI listed other sources including chicken fat. \n\nThe robot was powered by a steam engine built by the Cyclone Power Technologies company called the Cyclone Waste Heat Engine, which produced power through external combustion of biomass (i.e. combustion outside of the engine), an example of a Rankine cycle engine. The engine would power the vehicle's movement as well as being used to recharge the batteries that run the sensors, arms and ancillary devices.\n\nThe EATR was programmed to consume certain types of vegetation as biomass to convert into fuel, and only those types of vegetation. EATR could also use other fuels such as gasoline, kerosene, cooking oil, or solar energy.\n\nThe company also included \"chicken fat\" as one of its proposed fuel sources in the project overview.\n\nThe system was quoted as delivering an expected 100 miles (~161 km) of driving on 150 lbs (~68 kg) of vegetation.\n\n"}
{"id": "57734130", "url": "https://en.wikipedia.org/wiki?curid=57734130", "title": "Explorer 55", "text": "Explorer 55\n\nExplorer 55, also called as AE-E (Atmospheric Explorer E), was a NASA scientific satellite belonging to series Atmosphere Explorer, being launched on November 20, 1975 from Cape Canaveral AFS board a Delta 2910 rocket.\n\nThe purpose of the Explorer 55 mission was to investigate the chemical processes and energy transfer mechanisms that control the structure and behavior of the earth's atmosphere and ionosphere in the region of high absorption of solar energy at low and equatorial latitudes. The simultaneous sampling at higher latitudes was carried out by the Explorer 54 spacecraft until its failure on January 29, 1976, and then by Explorer 51, until it reentered on December 12, 1978. The same type of spacecraft as Explorer 51 was used, and the payload consisted of the same types of instruments except that the low-energy electron and UV nitric oxide experiments were deleted and a backscatter UV spectrometer was added to monitor the ozone content of the atmosphere. \n\nThe 2 experiments that were deleted were more appropriate for the high-latitude regions. The perigee swept through more than 6 full latitude cycles and two local time cycles during the first year after launch when the orbit was elliptical and the perigee height was varied between and . The circularization of the orbit around was made on November 20, 1976 and the spacecraft was raised to this height whenever it would decay to about . AE-E reentered on June 10, 1981.\n"}
{"id": "28222069", "url": "https://en.wikipedia.org/wiki?curid=28222069", "title": "Garden square", "text": "Garden square\n\nA garden square is a type of communal garden in an urban area wholly or substantially surrounded by buildings and, commonly, continues to be applied to public and private parks formed after such a garden becomes accessible to the public at large. The archetypal garden square is surrounded by tall terraced houses and other types of townhouse. It is subtly distinguished from a public-access version throughout the existence of the square – the town square. Due to its inherent private history it may have a pattern of dedicated footpaths and tends to have considerably more plants than hard surfaces and/or large monuments. \n\nAt their conception in the early 17th century each such garden was a private communal amenity for the residents of the overlooking houses akin to a garden courtyard within a palace or community. Such community courtyards date back to at least Ur in 2000 BC where two-storey houses were built of fired brick around an open square. Kitchen, working, and public spaces were located on the ground floor, with private rooms located upstairs. \n\nThe conversion of many during the 20th century into public parks renders those garden squares a subset of town squares, that is those with a garden square heritage. Some remain private — they may open intermittently or regularly — but many today are open to the public at least during part of every day, serving as small parks.\n\nLondon is famous for them; they are described as one of the glories of the capital. Many were built or rebuilt during the late eighteenth and early nineteenth centuries, at the height of Georgian architecture, and are surrounded by elegant townhouses. Large projects, such as the Bedford Estate, included garden squares in their development. The Notting Hill and Bloomsbury neighbourhoods both have many garden squares, with the former mostly still restricted to residents, and the latter open to all. Other UK cities prominent in the Georgian era such as Edinburgh, Bath, Bristol and Leeds have several garden squares.\n\nHouseholders with access to a private garden square are commonly required to pay a maintenance levy. Normally the charge is set annually by a garden committee. \n\nSometimes private garden squares are opened to the public, such as during Open Garden Squares Weekend.\n\nPrivately owned squares which survived the decades after the French Revolution and 19th century Haussmann's renovation of Paris include the Place des Vosges and Square des Épinettes in Paris. It was a fashionable and expensive square to live in during the 17th and 18th centuries, and one of the central reasons that Le Marais district became so fashionable for French nobility. It was inaugurated in 1612 with a grand \"carrousel\" to celebrate the engagement of Louis XIII to Anne of Austria and is a prototype of the residential squares of European cities that were to come. What was new about the \"Place Royale\" as it was known in 1612 was that the house fronts were all built to the same design, probably by Baptiste du Cerceau.\n\nIn town squares, similarly green but publicly accessible from the outset, is the Square René Viviani. Gardens substantially cover a few of the famous \"Places\" in the capital; instead the majority are paved and replete with profoundly hard materials such as Place de la Concorde. Inspired by ecological interests and a 21st century focus on pollution mitigation, an increasing number of the Places in Paris today many have a focal tree, or surrounding raised flower beds/and or rows of trees such as the Place de la République. \n\nThe enclosed garden terraces (\"French: jardins en terrain\") and courtyards (\"French: cours\") of some French former palaces have resulted in redevelopments into spaces equivalent to garden squares. The same former single-owner scenario applies to at least one garden square in London (Coleridge Square).\n\nGrandiose instances of garden-use town squares are a part of many French cities, others opt for solid material town squares. \n\nThe Square de Meeûs and Square Orban are notable examples in Brussels. \n\nDublin has several Georgian examples, including Merrion Square.\n\nRittenhouse Square in the Center City, Philadelphia encases a public garden, one of the five original open-space parks planned by William Penn and his surveyor Thomas Holme during the late 17th century. It was first named Southwest Square.\n\nNearby Fitler Square is a similar garden square named for late 19th century Philadelphia mayor Edwin Henry Fitler shortly after his death in 1896. The Square, cared for through a public private partnership between the Department of Parks and Recreation and the Fitler Square Improvement Association.\n\nIn Boston tens of squares exist, some having a mainly residential use.\n\n"}
{"id": "32000713", "url": "https://en.wikipedia.org/wiki?curid=32000713", "title": "Gemmatimonas aurantiaca", "text": "Gemmatimonas aurantiaca\n\nGemmatimonas aurantiaca is a Gram-negative, aerobic, polyphosphate-accumulating micro-organism. It is a Gram-negative, rod-shaped aerobe, with type strain T-27 (=JCM 11422 =DSM 14586). It replicates by budding.\n\n\n"}
{"id": "20722644", "url": "https://en.wikipedia.org/wiki?curid=20722644", "title": "Genopolitics", "text": "Genopolitics\n\nGenopolitics is the study of the genetic basis of political behavior and attitudes. It combines behavior genetics, psychology, and political science and it is closely related to the emerging fields of neuropolitics (the study of the neural basis of political behavior and attitudes) and political physiology (the study of biophysical correlates of political attitudes and behavior).\n\nIn 2008, \"The Chronicle of Higher Education\" reported on the increase in academicians' recognition of and engagement in genopolitics as a discrete field of study, and \"New York Times Magazine\" included genopolitics in its \"Eighth Annual Year in Ideas\" for the same year, noting that the term was originally coined by James Fowler. Critics of genopolitics have argued that it is \"a fundamentally misguided undertaking\", and that it is inconsistent with evidence in the fields of genetics, neuroscience, and evolutionary psychology.\n\nPsychologists and behavior geneticists began using twin studies in the 1980s to study variation in social attitudes, and these studies suggested that both genes and environment played a role. In particular, Nick Martin and his colleagues published an influential twin study of social attitudes in \"Proceedings of the National Academy of Sciences\" in 1986.\n\nHowever, this early work did not specifically analyze whether or not political orientations were heritable, and political scientists remained mostly unaware of the heritability of social attitudes until 2005. In that year, the \"American Political Science Review\" published a reanalysis of political questions on Martin's social attitude survey of twins in that the suggested liberal and conservative ideology is heritable. The article sparked considerable debate between critics, the authors and their defenders.\n\nInitial twin studies suggested that predispositions toward espousal of certain political ideas are heritable, but they said little about political behavior (patterns of voting and/or activism) or predispositions toward it. A 2008 article published in the \"American Political Science Review\" matched publicly available voter registration records to a twin registry in Los Angeles, analyzed self-reported voter turnout in the National Longitudinal Study of Adolescent Health (Add Health), and studied other forms of political participation. In all three cases, both genes and environment contributed significantly to variation in political behavior.\n\nAdditional studies showed that genes did not play a direct role in the choice of a political party, supporting a core finding in the study of American politics that the choice to be a Democrat or a Republican is largely shaped by parental socialization. However, other studies showed that the decision to affiliate with any political party and the strength of this attachment are significantly influenced by genes.\n\nScholars therefore recently turned their attention to specific genes that might be associated with political behaviors and attitudes. In the first-ever research to link specific genes to political phenotypes, a direct association was established between voter turnout and monoamine oxidase A (MAO-A) and a gene–environment interaction between turnout and the serotonin transporter (5HTT) gene among those who frequently participated in religious activities. In other research scholars have also found an association between voter turnout and a dopamine receptor (DRD2) gene that is mediated by a significant association between that gene and the tendency to affiliate with a political party. More recent studies show an interaction between friendships and the dopamine receptor (DRD4) gene that is associated with political ideology. Although this work is preliminary and needs replication, it suggests that neurotransmitter function has an important effect on political behavior.\n\nThe candidate genes approach to genopolitics received substantial criticism in a 2012 article, published in the \"American Political Science Review\", which argued that many of the candidate genes identified in the above research are associated with innumerable traits and behaviors. The degree to which these genes are associated with so many outcomes thus undermines the apparent important of evidence linking a gene to any particular outcome.\n\nEmploying a more general approach, researchers used genome-wide linkage analysis to identify chromosomal regions associated with political attitudes assessed using scores on a liberalism-conservativism scale. Their analysis identified several significant linkage peaks and the associated chromosomal regions implicate a possible role for NMDA and glutamate related receptors in forming political attitudes. However, this role is speculative as linkage analysis cannot identify the effect of individual genes.\n\nAssociations between genetic markers and political behavior are often assumed to predict a causal connection between the two. Scholars have little incentive to be skeptical of this presumed causal link. Yet it is possible that a confounding factor exists which makes the genetic relationship with politics purely correlative. For instance work on Irish parties, which shows some evidence of a genetic basis for the otherwise inexplicable distinction between the historically two main parties there, is also and more easily explained by socialization.\n\n\n"}
{"id": "12591990", "url": "https://en.wikipedia.org/wiki?curid=12591990", "title": "George Merryweather", "text": "George Merryweather\n\nGeorge Merryweather was a doctor and inventor born on 10 April 1794 in Burley in Wharfedale, Yorkshire, England. He married Jane Anderson Loy in 1826. They had four children: Mary, Jane, John and Emily. He then married Hannah Baker in 1844, after the death of his first wife in 1832. Merryweather died on 4 November 1870 in Whitby, Yorkshire, England. He was interred in Whitby's Larpool Cemetery. Despite this, he was seen in 1871 at 10 Abbey Terrace West, although his death was officially registered a year before.\n\nIn 1832 George wrote his first essay \"The means of maintaining uniform temperature and supporting fire without the agency of wood or coal\". He invented the so-called \"Platina Lamp\", which was described to \"keep burning for a fortnight on an economical mixture of pure alcohol and whisky, at a cost of one penny for eight hours\".\n\nBut his most remarkable invention was the \"Tempest Prognosticator\"—a weather predicting device also called \"The Leech Barometer\". It had great success and caused a sensation when it was put on show at the Great Exhibition, so in 1850-1 Merryweather wrote \"An essay explanatory of the tempest prognosticator in the Great Exhibition 1851\". At this time he was an honorary curator of Whitby Philosophical Society.\nAfter the success of the \"Tempest Prognosticator\" at the Great Exhibition, Merryweather tried to persuade the British government to install his device at ports around the British coast. However, the government reacted coolly to the proposal and the Meteorological Department insisted on using barometers and weather charts instead.\n\nModern science considers Merryweather's methods underlying the \"Tempest Prognosticator\" to be unproven.\n\nMerryweather referred to the leeches as his \"jury of philosophical councilors\" and explains that the twelve bottles were placed in a circle in order that his \"little comrades\" might see one another and \"not endure the affliction of solitary confinement\".\n\nIn 1835 Merryweather finished studying in MD University of Edinburgh. In 1840 he worked as a family doctor in Whitby, and by 1849 he was a surgeon.\n\n"}
{"id": "8402579", "url": "https://en.wikipedia.org/wiki?curid=8402579", "title": "Handbook of Automated Reasoning", "text": "Handbook of Automated Reasoning\n\nThe Handbook of Automated Reasoning (, 2128 pages) is a collection of survey articles on the field of automated reasoning. Published on June 2001 by MIT Press, it is edited by John Alan Robinson and Andrei Voronkov. Volume 1 describes methods for classical logic, first-order logic with equality and other theories, and induction. Volume 2 covers higher-order, non-classical and other kinds of logic.\n\n\n\n\n\n\n\n"}
{"id": "53924985", "url": "https://en.wikipedia.org/wiki?curid=53924985", "title": "Heather Paxson", "text": "Heather Paxson\n\nHeather Paxson is an American cultural anthropologist and science and technology studies scholar.\nShe is an expert on the anthropology of reproduction, and on the anthropology of food, including in particular cheese and commonplace family food practices. She is the William R. Kenan, Jr. Professor of Anthropology at the Massachusetts Institute of Technology.\nPaxson is a graduate of Haverford College, and obtained her Ph.D. from Stanford University.\n\nShe is an editor of the \"Oxford Companion to Cheese\". Her other books include:\n\nShe is married to fellow cultural anthropologist, STS scholar and professor, Stefan Helmreich.\n"}
{"id": "1462135", "url": "https://en.wikipedia.org/wiki?curid=1462135", "title": "Hotelling's law", "text": "Hotelling's law\n\nHotelling's law is an observation in economics that in many markets it is rational for producers to make their products as similar as possible. This is also referred to as the principle of minimum differentiation as well as Hotelling's linear city model. The observation was made by Harold Hotelling (1895–1973) in the article \"Stability in Competition\" in \"Economic Journal\" in 1929.\n\nThe opposing phenomenon is product differentiation, which is usually considered to be a business advantage if executed properly.\n\nSuppose there are two competing shops located along the length of a street running north and south, with customers spread equally along the street. Each shop owner wants to locate his shop such that he maximises his own market share by drawing the largest number of customers. In this example, the shop itself is the 'product' considered and both products are equal in quality and price. There is no difference in product to the customers. Therefore, each customer will always choose the nearest shop because there is no difference in product or price.\n\nFor a single shop, the optimal location is anywhere along the length of the street. The shop owner is completely indifferent about the location of the shop since it will draw all customers to it, by default. However, from the point of view of a social welfare function that tries to minimize the distance that people need to walk, the optimal point is halfway along the length of the street.\n\nHotelling's law predicts that a street with two shops will also find both shops right next to each other at the same halfway point. Each shop will serve half the market; one will draw customers from the north, the other all customers from the south.\n\nAnother example of the law in action or practice is to think of two food pushcarts at a beach. Assume one starts at the south end of the beach and one starts at the north. Again assuming a rational consumer and equal distribution along the beach, each cart will get 50% of the customers, divided along an invisible line equidistant from the carts. But, each cart owner will be tempted to push his cart slightly towards the other, in order to move the invisible line so that it encompasses more than 50% of the beach. Eventually, the pushcart operators end up next to each other in the center of the beach.\n\nObviously, it would be more socially beneficial if the shops separated themselves and moved to one quarter of the way along the street from each end — each would still draw half of the customers (the northern or southern half) and the customers would enjoy a shorter travel distance. However, neither shop would be willing to do this independently, as it would then allow the other shop to relocate and capture more than half the market.\n\nWhen not all people along the street, or along the range of possible different product positions, consume a minimum number of goods, companies can position their products to sections where consumers exist to maximize profit; this will often mean that companies will position themselves in different sections of the street, occupying niche markets. When prices are not fixed, companies can modify their prices to compete for customers; in those cases it is in the company's best interest to differentiate themselves as far away from each other as possible so they face less competition from each other.\n\nThe street is a metaphor for product differentiation; in the specific case of a street, the stores differentiate themselves from each other by location. The example can be generalized to all other types of horizontal product differentiation in almost any product characteristic, such as sweetness, colour, or size. The above case where the two stores are side by side would translate into products that are identical to each other. This phenomenon is present in many markets, particularly in those considered to be primarily commodities, and results in less variety for the consumer.\n\nAn extension of the principle into other environments of rational choice such as election \"markets\" can explain the common complaint that, for instance, the presidential candidates of the two American political parties are \"practically the same\". Once each candidate is confirmed during primaries, they are usually established within their own partisan camps. The remaining undecided electorate resides in the middle of the political spectrum, and there is a tendency for the candidates to \"rush for the middle\" in order to appeal to this crucial bloc. Like the paradigmatic example, the assumption is that people will choose the least distant option, (in this case, the distance is ideological) and that the most votes can be had by being directly in the center.\n\nThis phenomenon can be observed in real life, not just in commodity businesses like bars, restaurants, and gas stations, but even in large, branded chains:\n\n\n"}
{"id": "33878168", "url": "https://en.wikipedia.org/wiki?curid=33878168", "title": "Iñaki Antigüedad", "text": "Iñaki Antigüedad\n\nIñaki Antigüedad Auzmendi (born 1955, Bilbao) is a Basque geologist and politician. He is professor of hydrogeology at the University of the Basque Country, and leader of Amaiur.\n\nFollowing the strong performance of Amaiur in the 2011 Spanish general election, Antigüedad is leading a campaign to hold a referendum on Basque independence.\n\n\n"}
{"id": "26615704", "url": "https://en.wikipedia.org/wiki?curid=26615704", "title": "John Browne (anatomist)", "text": "John Browne (anatomist)\n\nJohn Browne (1642–1702) was an English anatomist, surgeon and author. He published the first description of cirrhosis of the liver in 1685 and the first description of necrotising pancreatitis in 1684. He was also known for publishing the work of others under his name.\n\nBrowne was brought up in Norwich, in a surgical family, being related to William Crop, a surgeon in Norfolk, but not closely related to Sir Thomas Browne whom he knew. He studied at St. Thomas's Hospital, London, under Thomas Hollyer, served as a naval surgeon, and then practised in Norwich.\n\nIn 1678 Browne moved to London, and about the same time was made surgeon in ordinary to King Charles II. With the king's recommendation he became surgeon at St. Thomas's Hospital, on 21 June 1683, chosen over Edward Rice who had taken charge of the hospital during the Great Plague of 1665, when other surgeons deserted their posts.\n\nIn 1691 complaints arose that the surgeons did not obey the regulations of the hospital, and claimed that being appointed by royal mandamus they were not responsible to the governors. The Whig Sir Robert Clayton was then President, the governors were determined to maintain their authority, and on 7 July 1691 they dismissed the whole of their surgical staff, including Browne, and appointed others. Browne appealed to the lords commissioners of the great seal, and the governors were called upon to defend their proceedings.\n\nBrowne was also surgeon to William III. In 1698 he petitioned the governors to be reinstated, though without success.\n\nBrowne wrote:\n\n\n\n\n"}
{"id": "47967448", "url": "https://en.wikipedia.org/wiki?curid=47967448", "title": "Josephson voltage standard", "text": "Josephson voltage standard\n\nA Josephson voltage standard is a complex system that uses a superconductive integrated circuit chip operating at 4 K to generate stable voltages that depend only on an applied frequency and fundamental constants. It is an intrinsic standard in the sense that it does not depend on any physical artifact. It is the most accurate method to generate or measure voltage and, by international agreement in 1990, is the basis for voltage standards around the world.\n\nIn 1962, Brian Josephson, a graduate student at Cambridge University, derived equations for the current and voltage across a junction consisting of a thin insulating barrier separating two superconductors – now generally known as a Josephson junction. His equations predicted that if a junction is driven at frequency formula_1, then its current-voltage (I-V) curve will develop regions of constant voltage at the values formula_2 , where formula_3 is an integer and formula_4 is the ratio of the Planck constant formula_5 to the elementary charge formula_6. This prediction was verified experimentally by Shapiro in 1963 and has become known as the AC Josephson effect. This effect found immediate application in metrology because it relates the volt to the second through a proportionality involving only fundamental constants. Initially, this led to an improved value of the ratio formula_7. Today it is the basis for all primary voltage standards. Josephson's equation for the supercurrent through a superconductive tunnel junction is given by\n\nwhere formula_9 is the junction current, formula_10 is the critical current, formula_11 is the junction voltage. formula_12 is a function of the junction geometry, the temperature, and any residual magnetic field inside the magnetic shields that are used with voltage standard devices. When a dc voltage is applied across the junction, Eq. (1) shows that the current will oscillate at a frequency formula_13 , where formula_14 is approximately equal to 484 GHz/mV. The very high frequency and low level of this oscillation make it difficult to observe directly. However, if an AC current at frequency formula_1 is applied to the junction, the junction oscillation formula_16 tends to phase lock to the applied frequency. Under this phase lock, the average voltage across the junction equals formula_17. This effect, known as the AC Josephson effect, is observed as a constant voltage step at formula_18 in the voltage-current (I-V) curve of the junction. It is also possible for the junction to phase lock to harmonics of formula_1. This results in a series of steps at voltages formula_20, where formula_3 is an integer, as shown in Fig. 1a.\n\nThe Josephson effect was initially used to improve the measurement of the constant formula_14 based on voltage values derived from the SI volt realization as maintained by Weston cells. The uncertainty of these measurements was limited by the uncertainty of the SI volt realization and the stability of the Weston cells. The stability of the Josephson volt depends only on the stability of formula_1 (which can easily be a part in 10), and is at least four orders of magnitude better than the stability of Weston cells. Thus, in the early 1970s, many national standards laboratories adopted a value for the Josephson constant formula_24 and began using the AC Josephson effect as the practical standard of voltage. Owing to small differences in existing national standards, different values of formula_25 were adopted by various countries. This inconsistency was corrected in 1990 when, by international agreement, the constant formula_26 was assigned the value 483597.9 GHz/V and adopted by all standards laboratories. The assigned value is based on a weighted average of volt realization measurements made prior to 1990 at many national measurement institutions. The uncertainty in formula_26 is 0.4 ppm. Standards such as the Josephson volt that depend on fundamental constants rather than physical artifacts are known as intrinsic standards. Although the Josephson voltage standard (JVS) does not realize the SI definition of the volt, it provides a very stable reference voltage that can be reproduced anywhere without the need to transfer artifacts such as Weston cells. The accuracy of the Josephson voltage-frequency relation formula_28, and its independence from experimental conditions, such as bias current, temperature, and junction materials, have been subjected to many tests. No significant deviation from this relation has ever been found. In the most precise of these experiments, two Josephson devices are driven by the same frequency source, biased on the same step, and connected in a series opposition loop across a small inductor. Since this loop is entirely superconductive, any voltage difference leads to a changing magnetic field in the inductor. This field is detected with a SQUID magnetometer and its constancy has set an upper limit on the voltage difference of less than 3 parts in 10. Figure 2 is a semilog plot that illustrates how typical differences in dc voltage measurements among National Measurement Institutes (NMIs) have decreased over the last 70 years. The two major improvements coincide with the introduction of single-junction Josephson standards in the early 1970s and the introduction of series-array Josephson standards beginning in 1984.\n\nAlthough the ac Josephson effect provides a much more stable voltage reference than Weston cells, the first single-junction Josephson standards \nwere difficult to use because they generated very small voltages (1-10 mV). Several attempts were made to raise the voltage by connecting two or more junctions in series. The most ambitious of these\nused 20 junctions in series to realize a voltage of 100 mV with an uncertainty of a few parts in 10. Ensuring that every junction was on a constant voltage step required individually adjusting the bias current to each of the 20 junctions. The difficulty of this procedure makes arrays of significantly more than 20 junctions impractical.\n\nIn 1977, Levinson et al.\nmade a suggestion that would ultimately lead to a solution to the multiple-bias problem. Levinson pointed out the importance of the parameter formula_29 in determining the characteristics of rf-induced Josephson steps. formula_30 is a measure of the damping of Josephson oscillations by the junction shunting resistanceformula_31. In particular, he showed that junctions with a large capacitance formula_32 and a large formula_33 could generate an I-V curve with hysteretic constant-voltage steps like those shown in Fig. 1b. These steps have become known as zero-crossing steps because they cross the zero-current axis of the I-V curve. The lack of stable regions between the first few steps means that for small dc bias currents, the junction voltage must be quantized. With a common bias current at or near zero, the voltage across a large array of these junctions must also be quantized. The possibility of obtaining constant-voltage steps at zero current over a wide range of junction and operating parameters suggested the possibility of building a voltage standard using large arrays of junctions.\n\nAfter several preliminary experiments,\na joint effort in 1984 between the National Bureau of Standards in the U. S. and the Physikalisch-Technische Bundes-Anstalt in Germany resolved the problems of junction stability and microwave distribution and created the first large Josephson array based on Levinson's idea. Further design improvements and system development produced the first practical 1 V Josephson standards in 1985.\nAdvances in superconductive integrated circuit technology, largely driven by the quest for a Josephson junction computer,\nsoon made possible much larger arrays. In 1987, the design was extended to a chip with 14484 junctions that generated about 150 000 quantized voltages spanning the range from -10 V to +10 V.\nNumerous further refinements were made as 10 V Josephson standards were implemented in many national standards laboratories.\nBy 1989, all of the hardware and software for a complete voltage metrology system was commercially available. Today, there are Josephson array voltage standards in more than 70 national, industrial, and military standards laboratories around the world. A program of international comparisons carried out by the Bureau International des Poids et Mesures (BIPM) has measured differences between a traveling Josephson standard and those of NMI’s that are typically less than 1 part in 10.\n\nFigure 3 illustrates the basic structure of one junction in a large series array. The junction is an overlap between two superconductive thin films that are separated by a thin oxide barrier. The junction sits above a ground plane and is separated from it by a few micrometers of insulation. A dc current formula_34 and a microwave current formula_35 are driven through the junction. The design parameters for the junction are its length formula_36, width formula_37, critical current density formula_38 (critical current per unit area), and the microwave drive frequency formula_1. The practical realization of an array voltage standard requires a thorough understanding of how these parameters affect the stability of the quantized voltage levels shown in Fig. 1b. Stable operation requires that four conditions be satisfied:\nIf any of these conditions is violated, the junction voltage is likely to switch randomly among several steps, making measurements impossible. A rigorous derivation of these conditions is the subject of several papers by Kautz.\n\nFigure 4 illustrates the region of stable behavior in the three-dimensional space of formula_36, formula_37, and formula_38. The margin of stable operation, represented by the shaded volume in Fig. 4, increases with formula_1 and is ultimately set by a trade-off between stability and the economics of providing a very high frequency microwave source. While stable arrays have been demonstrated at frequencies as low as 24 GHz,\nmost practical standards operate in the range 70–96 GHz. Table 1 lists a typical set of junction parameters for a commonly used design.\n\nThe I-V curve shown in Fig. 1b shows steps covering the range from about -1 mV to +1 mV and is for a junction driven by a nearly optimum level of microwave current. At lower microwave current the steps cover a smaller range of voltage and at higher microwave current the steps become smaller and begin to move off the zero current axis. In a large array, every junction must generate a large zero crossing step and thus the microwave power must be adjusted to a value low enough to accommodate the one junction receiving the largest microwave drive. Thus, in order to obtain the largest voltage from the smallest number of junctions, an array standard requires a circuit design that can deliver nearly uniform microwave power to many thousands of junctions, all of which are connected in series. The solution to this problem is a simple extension of Fig. 3 to a series of junctions in a line over a ground plane as shown in Fig. 5a. This results in a microwave stripline that can propagate microwave power with relatively low loss.\nThe capacitive impedance of the junctions is so small (approximately 1 m Ω ) relative to the strip line impedance (approx. 3 Ω) that each junction has a very minor effect on the propagation of microwave power in the strip line. Typically, each junction will absorb about 0.02% to 0.04% of the power propagating through it. It is thus possible to connect several thousand junctions in series and still achieve a power uniformity of about ±1.5 dB. With careful design, striplines with as many as 4800 junctions have been used.\n\nBecause 10 V Josephson standards require about 20 000 junctions, it is necessary to adopt a series/parallel circuit similar to that shown in Fig. 5b. Here, a network of low- and high-pass filters allow the microwave power to be split into four parallel paths while maintaining a dc path in which all junctions are connected in series.\n\nA typical integrated circuit layout for an array of 20 208 junctions is shown in Fig. 6. The microwave drive power is collected from a waveguide by a finline antenna, split 16 ways, and injected into 16 junction strip lines of 1263 junctions each. The junction striplines are separated from a superconductive ground plane by about 2 micrometers of SiO2 dielectric. Symmetry in the splitting network ensures that the same power is delivered to each subarray. Several precautions are required to avoid reflections that would lead to standing waves and the consequent nonuniform power distribution within the subarrays: (1) Each stripline is terminated by a matched load that consists of several wavelengths of resistive strip line. The use of resistive stripline rather than a discrete resistor guarantees a near perfect match over a wide range of fabrication parameters. (2) The dimensions of capacitors in the low- and high-pass filters are chosen to avoid resonances near the drive frequency. (3) The microwave bend radius has a minimum value of three times the stripline width. Sharper bends result in unacceptable reflections. In order to meet the bend requirement while still packing the array strips close together, \"curly\" bends that turn 215° and then back 45° are used. (4) The junction spacing along the line must be close enough to avoid a resonance between adjacent junctions. Microwave power is applied by inserting the finline end of the chip into a slot parallel to the E-field in a WR-12 waveguide. The dc output appears across superconducting pads at the edge of the chip.\n\nVoltage standard chips are typically fabricated on silicon or glass substrates. The integrated circuit has eight levels: (1) a 300 nm thick Nb ground plane, (2) a 2 μm layer of SiO2 that forms the microstripline dielectric, (3) a 200 nm Nb film that forms the lower electrode of the Josephson junctions, (4) a 3 nm metal oxide layer that forms the Josephson tunneling barrier, (5) a 100 nm Nb junction counter electrode (6) a 300 nm SiO film with windows for contacts to the counter electrode, (7) a 400 nm film of Nb that connects the junction counter electrodes, and (8) a 100 nm resistive film that forms the stripline terminations.\n\nA block diagram of a modern Josephson voltage standard system is shown in Fig. 7. The Josephson array chip is mounted inside a high-permeability magnetic shield at the end of a cryoprobe that makes the transition between a liquid helium Dewar and the room temperature environment. Some systems use a cryocooler to cool the chip and eliminate the need for liquid helium. Three pairs of copper wires are connected to the array. One pair supplies bias current, a second monitors the array voltage with an oscilloscope, and the third pair delivers the array voltage to the calibration system. All of the wires pass through multiple levels of RFI filtering in a box at the top of the Dewar. The box, the filters, and the Dewar itself form a shield that protects the Josephson array from electromagnetic interference that could cause step transitions. Microwave power is delivered through a waveguide consisting of a 12 mm diameter tube with WR-12 launching horns on each end. Tubes of solid German silver or stainless steel plated internally with silver or gold are commonly used. This waveguide simultaneously achieves low thermal loss (<0.5 L liquid He per day) and low microwave loss (as low as 0.7 dB at 75 GHz).\nA phase-locked oscillator (PLO) operating at a frequency near 75 GHz provides the microwave power to the chip. The primary requirements for the 75 GHz source are: (1) its frequency must be known with high accuracy ( 1 part in 10 ) and (2) it should produce a stable output power of at least 50 mW. It is useful, although not essential, to be able to tune the source over a range of frequencies. The PLO may be constructed using a commercial microwave counter with feedback capability or it may be a custom built phase-locked loop. More recently microwave frequency synthesizers that are more reliable and offer a wider tuning range and resolution have become the preferred microwave source. The frequency reference for the system is usually a 10 MHz sine wave derived from a GPS receiver or an atomic clock.\n\nThe zero crossing steps of Fig. 1b allow a single bias current passing through the entire junction array while insuring that every junction in the array is on a constant voltage step. This leads to significant complication in setting the array to a particular desired step. Figure 8a illustrates a simplified diagram of the bias circuit. In this circuit a computer sets the bias voltage formula_53 with one digital-to-analog converter (DAC) and uses a second DAC to control the bias impedance formula_54 via optically modulated resistors. Figure 8b shows a graphical solution for the stable operating points of the array and illustrates how control of both the bias voltage and the bias impedance is used to select a particular quantum voltage step.\nThe load line plots the range of voltage and current that are defined by the bias supply. The intersections of this load line with the I-V curve of the array (vertical lines) are possible stable bias points. Changes to formula_53 shift the load line left and right, whereas changes to formula_54 change its slope. To select a step at a given voltage formula_57, the source voltage is set to formula_57 and the source impedance is set to about formula_59, where formula_60 is the step height. This makes the load line steep enough to intersect only one or two steps and forces the array to a step at or very near formula_57. Applying a damped oscillation to formula_57 helps move the array to the step closest to formula_57. After a step is selected, the source impedance is smoothly increased on all four bias connections (load line becomes horizontal) until the array current goes to zero and the array is effectively disconnected from the bias source. This open-bias condition is the most stable state for the array and eliminates the possibility of any errors resulting from a small series resistance in the array – a common array defect. Computer control of this three-step process enables the system to find and stabilize the array voltage on a particular step within a few seconds. High quality Josephson arrays will remain on a selected step for many hours.\n\nNumerous algorithms have been developed to compare a Josephson standard with a secondary standard or another Josephson standard. These algorithms differ in the amount of averaging used, the type and placement of reversing switches, and the statistical methods used to reduce the data and compute uncertainty. The selection of an algorithm depends on the type of comparison, the desired level of uncertainty, and the time available. One commonly used algorithm that is appropriate for calibrations of Zener reference standards is described here.\n\nThe voltage of an unknown reference formula_64 relative to the Josephson array voltage is determined using the circuit shown in Fig. 9 (a subset of Fig. 7) in which the unknown and the Josephson array are connected in series opposition across a null meter. A reversing switch is used to eliminate the effect of thermal and other offset voltages. The step number formula_65 and sometimes the frequency formula_66 are adjusted to make the null voltage as small as possible. The circuit equation can then be written:\n\nHere, formula_68 is the Josephson array voltage, \"V\" is a combination of thermal offset voltages and any offset voltage in the nullmeter, mt represents a linear drift component of the offset voltage, formula_69 is the polarity of the reversing switch, formula_70 is the differential null voltage, and formula_71 represents noise in the unknown, the null meter, and any other sources of random noise. Now define a parameter formula_72 , where formula_70 is a measurement at time formula_74 and formula_65 is determined from formula_70 using\n\nwhere formula_78 is an initial direct measurement of formula_64 by the system voltmeter and the \"Round\" function means rounded to the nearest integer. The direct measurement of formula_64 is obtained by setting the array to the formula_81 step, which can be seen from Fig. 7 to connect the voltmeter directly to the Zener reference.\n\nBased on measurements of formula_82 and formula_83, a set of values formula_84 and formula_85 is acquired for formula_86. Three successive values of formula_87 are examined for consistency within 2 μV before the data are accepted. This eliminates data that may be corrupted by the transient that occurs when there is a spontaneous transition between quantum voltage steps. Since formula_57 and formula_70 change by equal amounts during a step transition, formula_84 remains constant thus making the data collection process relatively immune to step transitions. Data are collected efficiently even for a Josephson array chip that may be making as many as five transitions per minute. The scatter in the data that results from noise in the unknown and in the null meter can generally be modeled by a Gaussian process with one standard deviation on the order of 20 to 100 nV. There are, however, occasional noise spikes that do not fit this process and generate glitches in the formula_84 data that may lie 1 μV to 10 μV away from the well behaved data. An outlier test is used to detect and eliminate such data.\n\nAfter the collection of the first data set, the polarity of the unknown is reversed (formula_92), the bias is readjusted to select a step that minimizes formula_70, and a second set of data is acquired. Two more reversals generate third and fourth data sets. Best estimates for formula_94, and formula_95 are obtained from a least-squares recursion analysis that minimizes the root-sum-square (RSS) error of the set formula_96 for all formula_97 in the four data sets. In typical measurements of Zener standards, the noise of the standard often dominates the computed value of formula_95. The type A uncertainty for formula_99 is the standard deviation of the mean for the set of formula_84. Typically, this entire calibration algorithm is controlled by a computer and is completed in a few minutes. Except in the case of data with nonuniform delays between the reversals, a simple average of the absolute values of the full set of formula_84 is an equally good estimate of formula_102.\nSystems like that shown in Fig. 7 are used to calibrate secondary standards, such as Weston cells, Zener references, and precise digital voltmeters. These calibrations are greatly simplified by the fact that the Josephson array voltage can be set to any value formula_103, where the integer formula_65 can have any value in the range of about -75 000 to +75 000. The typical uncertainty in measurements of 10 V Zener standards is limited by noise in the Zener to about 0.01 ppm. The ability to set the Josephson array to a wide range of discrete voltages also makes it the most accurate tool for measuring the linearity of high-accuracy digital voltmeters.\n\nWhile the voltage appearing across the terminals of a Josephson device is, in principle, given exactly by formula_103, in any real measurement there are a variety of potential sources of error and uncertainty as listed in Table 2. In the case of a known error, such as a reference frequency offset or a known leakage resistance, a correction can be made. It is then the metrologist’s task to assign realistic numbers to all uncertainties including the uncertainty in the corrections. One method of doing this notes that only items 1 and 2 in Table 2 depend on the voltage across the Josephson array. All of the other components are about the same regardless of the array voltage. Therefore, the combined effect of items 3-8 can be quantitatively evaluated by making a set of measurements of a short circuit using exactly the same algorithm that is used for any other measurement. The standard error resulting from items 3-8 is just the root mean square (RMS) value of the set of short circuit measurements.\nAdditional experiments must be performed to estimate frequency and leakage uncertainty. Internationally accepted procedures for combining uncertainty and establishing confidence intervals is the subject of the BIPM’s Guide to the Evaluation of Uncertainty in Measurement.\nTypically, the total uncertainty contribution of a Josephson system in a measurement averaging time of a few minutes is a few nanovolts. Since the most common use of these systems is the calibration of Zener standards with a noise level of 50-100 nV, the contribution of the Josephson system is negligible.\n\nTable 2. Potential sources of error and uncertainty for a Josephson standard\n\nA Congressional act in 1904 established the U.S. Legal Volt to be a quantity defined by the National Bureau of Standards, now the National Institute of Standards and Technology (NIST). With the 1990 international agreement on the Josephson representation of the volt, NIST defined the U.S. Legal Volt to be the same as the international volt representation. Since the success of the first Josephson array voltage standards in 1984, their use has proliferated to more than 70 national measurement institutes (NMIs), military, and commercial laboratories around the world. This has resulted in some confusion about the traceability of non-NMI's that are in possession of a JVS that is, in principle, as good as the national standard. Some guidance on this question is provided in International Standards Organization (ISO) documents that state the general principle that intrinsic standards like the JVS, that have participated in a comparison with an NMI, can claim traceability.\n"}
{"id": "239981", "url": "https://en.wikipedia.org/wiki?curid=239981", "title": "La Géométrie", "text": "La Géométrie\n\nLa Géométrie was published in 1637 as an appendix to \"Discours de la méthode\" (\"Discourse on the Method\"), written by René Descartes. In the \"Discourse\", he presents his method for obtaining clarity on any subject. \"La Géométrie\" and two other appendices, also by Descartes, \"La Dioptrique\" (\"Optics\") and \"Les Météores\" (\"Meteorology\"), were published with the \"Discourse\" to give examples of the kinds of successes he had achieved following his method (as well as, perhaps, considering the contemporary European social climate of intellectual competitiveness, to show off a bit to a wider audience).\n\nThe work was the first to propose the idea of uniting algebra and geometry into a single subject and invented an algebraic geometry called analytic geometry, which involves reducing geometry to a form of arithmetic and algebra and translating geometric shapes into algebraic equations. For its time this was ground-breaking. It also contributed to the mathematical ideas of Leibniz and Newton and was thus important in the development of calculus.\n\nThis appendix is divided into three \"books\". \n\nBook I is titled \"Problems Which Can Be Constructed by Means of Circles and Straight Lines Only.\" In this book he introduces algebraic notation that is still in use today. The letters at the end of the alphabet, viz., , , , etc. are to denote unknown variables, while those at the start of the alphabet, , , , etc. denote constants. He introduces modern exponential notation for powers (except for squares, where he kept the older tradition of writing repeated letters, such as, ). He also breaks with the Greek tradition of associating powers with geometric referents, with an area, with a volume and so on, and treats them all as possible lengths of line segments. These notational devices permit him to describe an association of numbers to lengths of line segments that could be constructed with straightedge and compass. The bulk of the remainder of this book is occupied by Descartes's solution to \"the locus problems of Pappus.\" According to Pappus, given three or four lines in a plane, the problem is to find the locus of a point that moves so that the product of the distances from two of the fixed lines (along specified directions) is proportional to the square of the distance to the third line (in the three line case) or proportional to the product of the distances to the other two lines (in the four line case). In solving these problems and their generalizations, Descartes takes two line segments as unknown and designates them and . Known line segments are designated , , , etc. The germinal idea of a Cartesian coordinate system can be traced back to this work.\n\nIn the second book, called \"On the Nature of Curved Lines\", Descartes described two kinds of curves, called by him \"geometrical\" and \"mechanical\". Geometrical curves are those which are now described by algebraic equations in two variables, however, Descartes described them kinematically and an essential feature was that \"all\" of their points could be obtained by construction from lower order curves. This represented an expansion beyond what was permitted by straightedge and compass constructions. Other curves like the quadratrix and spiral, where only some of whose points could be constructed, were termed mechanical and were not considered suitable for mathematical study. Descartes also devised an algebraic method for finding the normal at any point of a curve whose equation is known. The construction of the tangents to the curve then easily follows and Descartes applied this algebraic procedure for finding tangents to several curves. \n\nThe third book, \"On the Construction of Solid and Supersolid Problems\", is more properly algebraic than geometric and concerns the nature of equations and how they may be solved. He recommends that all terms of an equation be placed on one side and set equal to 0 to facilitate solution. He points out the factor theorem for polynomials and gives an intuitive proof that a polynomial of degree has roots. He systematically discussed negative and imaginary roots of equations and explicitly used what is now known as Descartes' rule of signs.\n\nDescartes wrote \"La Géométrie\" in French rather than the language used for most scholarly publication at the time, namely Latin. His exposition style was far from clear, the material was not arranged in a systematic manner and he generally only gave indications of proofs, leaving many of the details to the reader. His attitude toward writing is indicated by statements such as \"I did not undertake to say everything,\" or \"It already wearies me to write so much about it,\" that occur frequently. In conclusion, Descartes justifies his omissions and obscurities with the remark that much was deliberately omitted \"in order to give others the pleasure of discovering [it] for themselves.\"\n\nDescartes is often credited with inventing the coordinate plane because he had the relevant concepts in his book, however, nowhere in \"La Géométrie\" does the modern rectangular coordinate system appear. This and other improvements were added by mathematicians who took it upon themselves to clarify and explain Descartes' work.\n\nThis enhancement of Descartes' work was primarily carried out by Frans van Schooten, a professor of mathematics at Leiden and his students. van Schooten published a Latin version of \"La Géométrie\" in 1649 and this was followed by three other editions in 1659−1661, 1683 and 1693. The 1659−1661 edition was a two volume work more than twice the length of the original filled with explanations and examples provided by van Schooten and this students. One of these students, Johannes Hudde provided a convenient method for determining double roots of a polynomial, known as Hudde's rule, that had been a difficult procedure in Descartes's method of tangents. These editions established analytic geometry in the seventeenth century.\n\n\n\n\n"}
{"id": "34780256", "url": "https://en.wikipedia.org/wiki?curid=34780256", "title": "Lagrange Prize", "text": "Lagrange Prize\n\nThe Lagrange-CRT Foundation Prize is an annual International award created by the CRT Foundation with the scientific coordination of the ISI Foundation. The prize is awarded for scientific research in the field of complexity sciences, its applications and dissemination.\nThe Lagrange Prize is awarded in Turin, Italy.\n\nThe Lagrange-CRT Foundation Prize is awarded to a selected scientist (below \n50 years of age) for achievements in research on complex systems, including\ntheoretical and experimental research. In \nparticular, the prize recognizes outstanding contributions relevant to the \nprogress of complexity science.\n\nThe winner of the Lagrange-CRT Foundation Prize is chosen by the Scientific Commission in collaboration with the ISI Foundation. The prize is in the amount of €50,000. The award ceremony takes place in Turin.\n\n<br>\n\n\n\n"}
{"id": "17567590", "url": "https://en.wikipedia.org/wiki?curid=17567590", "title": "List of Hawaii state symbols", "text": "List of Hawaii state symbols\n\nThe following is a list of symbols of the U.S. state of Hawaii.\n\n\nIn addition to the state symbols, Hawaii has official colors, flowers, and lei-making materials for each of the major islands in the Hawaiian Islands archipelago.\n\n\n"}
{"id": "8244041", "url": "https://en.wikipedia.org/wiki?curid=8244041", "title": "List of Texas state symbols", "text": "List of Texas state symbols\n\nThe following is a list of symbols of the U.S. state of Texas.\n\nA pledge of allegiance to the Texas flag was established in 1933.\n\nFour ships of the United States Navy and one in the Confederate States Navy have borne the name \"Texas\":\n\n\n"}
{"id": "913612", "url": "https://en.wikipedia.org/wiki?curid=913612", "title": "List of U.S. state dances", "text": "List of U.S. state dances\n\nThis is a list of official U.S. state dances:\n\n"}
{"id": "55196915", "url": "https://en.wikipedia.org/wiki?curid=55196915", "title": "List of agricultural pest nematode species", "text": "List of agricultural pest nematode species\n\nThis article is an attempt to list all agricultural pest nematodes. Species are sorted in alphabetical order of Latin name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "927350", "url": "https://en.wikipedia.org/wiki?curid=927350", "title": "List of discredited substances", "text": "List of discredited substances\n\nThis page is a list of substances or materials generally considered discredited.\n\nA substance can be discredited in one of three ways:\n\n\nThis is not to be construed as implying that these items––are discredited. What is listed are fire, water, metal, etc. as universal principles or fundamentals.\n\n"}
{"id": "50490019", "url": "https://en.wikipedia.org/wiki?curid=50490019", "title": "List of haplotype estimation and genotype imputation software", "text": "List of haplotype estimation and genotype imputation software\n\nThis is a list of notable software for haplotype estimation and genotype imputation.\n\nAlphabetical order:\n"}
{"id": "26569604", "url": "https://en.wikipedia.org/wiki?curid=26569604", "title": "List of medical schools in Asia", "text": "List of medical schools in Asia\n\nThe following is a list of medical schools (or universities with a medical school), in Asia.\n\n\n\n\n\n\"Further Information: Medical education in Jordan.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "12069493", "url": "https://en.wikipedia.org/wiki?curid=12069493", "title": "List of rivers of Assam", "text": "List of rivers of Assam\n\nThis is a list of rivers of Assam, India.\n\n\n\n\n"}
{"id": "25432211", "url": "https://en.wikipedia.org/wiki?curid=25432211", "title": "M. Watt Espy", "text": "M. Watt Espy\n\nMajor Watt Espy, Jr. (March 2, 1933 – August 13, 2009) was a researcher and expert on capital punishment in the United States.\n\nEspy, a resident of Headland, Alabama, attended the University of Alabama where he was a member of Phi Sigma Kappa fraternity. Even in college he had garnered a reputation as an engaging speaker, serving as toastmaster for the 30th anniversary banquet of the chapter, held in 1955. He graduated in 1957.\n\nEspy was an author, with John Ortiz Smykla, of \"The Espy Files\", a database of executions carried out in the United States and preceding territories from 1608, which is the most complete source of data on the issue, identifying 15,487 people put to death.\n\nHe began his research in the 1970s when he was a salesman, working with everything from cemetery plots to security systems. While making calls he'd stop at a prison or courthouse for information. He became a full-time researcher in 1977.\n\nEspy became a death penalty opponent due to his concerns about racial bias, innocence and lack of deterrence. Espy served as a citizen witness to exactly one execution - that of John Louis Evans in the Alabama electric chair on April 22, 1983.\n\nEspy died aged 76 on August 13, 2009.\n\n"}
{"id": "13226237", "url": "https://en.wikipedia.org/wiki?curid=13226237", "title": "Mural instrument", "text": "Mural instrument\n\nA mural instrument is an angle measuring device mounted on or built into a wall. For astronomical purposes, these walls were oriented so they lie precisely on the meridian. A mural instrument that measured angles from 0 to 90 degrees was called a mural quadrant. They were utilized as astronomical devices in ancient Egypt and ancient Greece. Edmond Halley, due to the lack of an assistant and only one vertical wire in his transit, confined himself to the use of a mural quadrant built by George Graham after its erection in 1725 at the Royal Observatory, Greenwich. Bradley's first observation with that quadrant was made on 15 June 1742.\n\nMany older mural quadrants have been constructed by marking directly on the wall surfaces. More recent instruments were made with a frame that was constructed with precision and mounted permanently on the wall.\n\nThe arc is marked with divisions, almost always in degrees and fractions of a degree. In the oldest instruments, an indicator is placed at the centre of the arc. An observer can move a device with a second indicator along the arc until the line of sight from the movable device's indicator through the indicator at the centre of the arc aligns with the astronomical object. The angle is then read, yielding the elevation or altitude of the object. In smaller instruments, an alidade could be used. More modern mural instruments would use a telescope with a reticle eyepiece to observe the object.\n\nMany mural quadrants were constructed, giving the observer the ability to measure a 90° range of elevation. There were also mural sextants that read 60°.\n\nIn order to measure the position of, for example, a star, the observer needs a sidereal clock in addition to the mural instrument. With the clock measuring time, a star of interest is observed with the instrument until it crosses an indicator showing that it is transiting the meridian. At this instant, the time on the clock is recorded as well as the angular elevation of the star. This yields the position in the coordinates of the instrument. If the instrument's arc is not marked relative to the celestial equator, then the elevation is corrected for the difference, resulting in the star's declination. If the sidereal clock is precisely synchronized with the stars, the time yields the right ascension directly.\n\n"}
{"id": "32577237", "url": "https://en.wikipedia.org/wiki?curid=32577237", "title": "Musa Adyshev", "text": "Musa Adyshev\n\nAcademician Musa Mirzapayazovich Adyshev () was a Soviet and Kyrgyzstan geologist who lived and worked in Kyrgyzstan. He is best known for identification of Tien Shan black-shale province and substantiation of its stratigraphic location.\n\nAfter graduation from Central Asian State University in Tashkent in 1947 he worked in the Institute of Geology of Kyrgyz branch of Academy of Sciences of the USSR.\n\nIn 1953-1974 he was a director of the Institute of Geology, in 1957-1979 - member of Board, since 1974 - vice president, and in 1978 the president of the Academy of Sciences of Kyrgyz SSR (presently Kyrgyz Academy of Sciences).\n\nOrganizations named after academician M.Adyshev\n\nGeographic places\n"}
{"id": "7528885", "url": "https://en.wikipedia.org/wiki?curid=7528885", "title": "Outline of Big Science", "text": "Outline of Big Science\n\nThe following outline is provided as an overview of and topical guide to Big Science.\n\nBig Science – term used by scientists and historians of science to describe a series of changes in science which occurred in industrial nations during and after World War II.\n\n\n\n"}
{"id": "23210", "url": "https://en.wikipedia.org/wiki?curid=23210", "title": "Pseudorandomness", "text": "Pseudorandomness\n\nA pseudorandom process is a process that appears to be random but is not. Pseudorandom sequences typically exhibit statistical randomness while being generated by an entirely deterministic causal process. Such a process is easier to produce than a genuinely random one, and has the benefit that it can be used again and again to produce exactly the same numbers, which is useful for testing and fixing software.\n\nTo generate truly random numbers would require precise, accurate, and repeatable system measurements of absolutely non-deterministic processes. Linux uses, for example, various system timings (like user keystrokes, I/O, or least-significant digit voltage measurements) to produce a pool of random numbers. It attempts to constantly replenish the pool, depending on the level of importance, and so will issue a random number. This system is an example, and similar to those of dedicated hardware random number generators. Even with all of this work, it is not truly random.\n\nThe generation of random numbers has many uses (mostly in statistics, for random sampling, and simulation). Before modern computing, researchers requiring random numbers would either generate them through various means (dice, cards, roulette wheels, etc.) or use existing random number tables.\n\nThe first attempt to provide researchers with a ready supply of random digits was in 1927, when the Cambridge University Press published a table of 41,600 digits developed by L.H.C. Tippett. In 1947, the RAND Corporation generated numbers by the electronic simulation of a roulette wheel; the results were eventually published in 1955 as \"A Million Random Digits with 100,000 Normal Deviates\".\n\nA pseudorandom variable is a variable which is created by a deterministic algorithm, often a computer program or subroutine, which in most cases takes random bits as input. The pseudorandom string will typically be longer than the original random string, but less random (less entropic in the information theory sense). This can be useful for randomized algorithms.\n\nPseudorandom number generators are widely used in such applications as computer modeling (e.g., Markov chains), statistics, experimental design, etc.\n\nIn theoretical computer science, a distribution is pseudorandom against a class of adversaries if no adversary from the class can distinguish it from the uniform distribution with significant advantage.\nThis notion of pseudorandomness is studied in computational complexity theory and has applications to cryptography.\n\nFormally, let \"S\" and \"T\" be finite sets and let F = {\"f\": \"S\" → \"T\"} be a class of functions. A distribution D over \"S\" is ε-pseudorandom against F if for every \"f\" in F, the statistical distance between the distributions \"f\"(\"X\"), where \"X\" is sampled from D, and \"f\"(\"Y\"), where \"Y\" is sampled from the uniform distribution on \"S\", is at most ε.\n\nIn typical applications, the class F describes a model of computation with bounded resources\nand one is interested in designing distributions D with certain properties that are pseudorandom against F. The distribution D is often specified as the output of a pseudorandom generator.\n\nThough random numbers are needed in cryptography, the use of pseudorandom number generators (whether hardware or software or some combination) is insecure. When random values are required in cryptography, the goal is to make a message as hard to crack as possible, by eliminating or obscuring the parameters used to encrypt the message (the key) from the message itself or from the context in which it is carried. Pseudorandom sequences are deterministic and reproducible; all that is required in order to discover and reproduce a pseudorandom sequence is the algorithm used to generate it and the initial seed. So the entire sequence of numbers is only as powerful as the randomly chosen parts - sometimes the algorithm and the seed, but usually only the seed.\n\nThere are many examples in cryptographic history of ciphers, otherwise excellent, in which random choices were not random enough and security was lost as a direct consequence. The World War II Japanese PURPLE cipher machine used for diplomatic communications is a good example. It was consistently broken throughout World War II, mostly because the \"key values\" used were insufficiently random. They had patterns, and those patterns made any intercepted traffic readily decryptable. Had the keys (i.e. the initial settings of the stepping switches in the machine) been made unpredictably (i.e. randomly), that traffic would have been much harder to break, and perhaps even secure in practice.\n\nSince pseudorandom numbers are in fact deterministic, a given seed will always determine the same pseudorandom number. This attribute is used in security, in the form of rolling code to avoid replay attacks, in which a command would be intercepted to be used by a thief at a later time.\n\nA Monte Carlo method simulation is defined as any method that utilizes sequences of random numbers to perform the simulation. Monte Carlo simulations are applied to many topics including quantum chromodynamics, cancer radiation therapy, traffic flow, stellar evolution and VLSI design. All these simulations require the use of random numbers and therefore pseudorandom number generators, which makes creating random-like numbers very important.\n\nA simple example of how a computer would perform a Monte Carlo simulation is the calculation of π. If a square enclosed a circle and a point were randomly chosen inside the square the point would either lie inside the circle or outside it. If the process were repeated many times, the ratio of the random points that lie inside the circle to the total number of random points in the square would approximate the ratio of the area of the circle to the area of the square. From this we can estimate pi, as shown in the Python code below utilizing a SciPy package to generate pseudorandom numbers with the MT19937 algorithm. Note that this method is a computationally inefficient way to numerically approximate π.\n\n\n"}
{"id": "26092740", "url": "https://en.wikipedia.org/wiki?curid=26092740", "title": "Quadratic Lie algebra", "text": "Quadratic Lie algebra\n\nA quadratic Lie algebra is a Lie algebra together with a compatible symmetric bilinear form. Compatibility means that it is invariant under the adjoint representation. Examples of such are semisimple Lie algebras, such as su(n) and sl(n,R).\n\nA quadratic Lie algebra is a Lie algebra (g,[..]) together with a non-degenerate symmetric bilinear form formula_1 that is invariant under the adjoint action, i.e.\nwhere \"X,Y,Z\" are elements of the Lie algebra g.\nA localization/ generalization is the concept of Courant algebroid where the vector space g is replaced by (sections of) a vector bundle.\n\nAs a first example, consider \"R\" with zero-bracket and standard inner product\n\nSince the bracket is trivial the invariance is trivially fulfilled.\n\nAs a more elaborate example consider so(3), i.e. \"R\" with base \"X,Y,Z\", standard inner product, and Lie bracket\nStraightforward computation shows that the inner product is indeed preserved. A generalization is the following.\n\nA big group of examples fits into the category of semisimple Lie algebras, i.e. Lie algebras whose adjoint representation is faithful. Examples are sl(n,R) and su(n), as well as direct sums of them. Let thus g be a semi-simple Lie algebra with adjoint representation \"ad\", i.e.\nDefine now the Killing form\nDue to the Cartan criterion, the Killing form is non-degenerate if and only if the Lie algebra is semisimple.\n\nIf g is in addition a simple Lie algebra, then the Killing form is up to rescaling the only invariant symmetric bilinear form.\n"}
{"id": "41256454", "url": "https://en.wikipedia.org/wiki?curid=41256454", "title": "Rational analysis", "text": "Rational analysis\n\nRational analysis is a theoretical framework, methodology, and research program in cognitive science that has been developed by John Anderson. The goal of rational analysis as a research program is to explain the function and purpose of cognitive processes and to discover the structure of the mind. Chater and Oaksford contrast it with the mechanistic explanations of cognition offered by both computational models and neuroscience.\n\nRational analysis starts from the assumption that the mind is adapted to its environment. Rational analysis uses this assumption to investigate the structure and purpose of representations and cognitive processes by studying the structure of the environment. The methodology of rational analysis comprises six steps:\n\n\nRational analysis has been applied to memory, categorization, causal inference, problem solving, and reasoning.\n"}
{"id": "8210612", "url": "https://en.wikipedia.org/wiki?curid=8210612", "title": "Resorption", "text": "Resorption\n\nResorption is the absorption into the circulatory system of cells or tissue, usually by osteoclasts.\n\nTypes of resorption include:\n"}
{"id": "3729332", "url": "https://en.wikipedia.org/wiki?curid=3729332", "title": "Robert Fiske Griggs", "text": "Robert Fiske Griggs\n\nRobert Fiske Griggs (22 August 1881 in Brooklyn, Connecticut – 10 June 1962), was a botanist who led a 1915 National Geographic Society expedition to observe the aftermath of the Katmai volcanic eruption.\n\nIn June 1917, Griggs and the eager NGS explorers rushed to the Katmai coast with the express goal of exploring the Valley of Ten Thousand Smokes. They quickly worked their way up through the ash-filled Katmai River valley and over the pass. It was a month of terror and elation for the twelve adventurers. Through the long Alaska summer days, they took chemical and geologic samples, shot photographs, and made rough maps. Mincing their way across the crumbling, treacherous surface of the hot ash, they studied the temperatures and temperaments of the roaring fumaroles and explored the perilous margins of the pyroclastic deposits. As they explored and documented the valley, they began to build a picture of the eruption. For five years, the American public had been entranced by the exciting volcanic discoveries in Alaska. Hungry for stories to push the horrors of World War I from their minds, thousands of National Geographic subscribers were thrilled to read Griggs’ gripping articles about the adventures of his exploring parties.\n\nAs the discoveries unfolded, Griggs became increasingly zealous in his advocacy of the site. His vivid descriptions of the wonders of the Katmai country ignited the interest of what was then a budding conservation movement in the United States. The mysterious volcanic valley seemed an ideal candidate for protection. Griggs and the chiefs of the National Geographic Society campaigned persistently to preserve the area, and in 1918, President Woodrow Wilson declared of land as Katmai National Monument.\n\nGriggs, Robert F. \"The Valley of Ten Thousand Smokes: An Account of the Discovery and Exploration of the Most Wonderful Volcanic Region in the World,\" National Geographic (February 1918), 115-169.\n"}
{"id": "21686537", "url": "https://en.wikipedia.org/wiki?curid=21686537", "title": "Rosalind Franklin Award", "text": "Rosalind Franklin Award\n\nThe Royal Society Rosalind Franklin Award was established in 2003 and is awarded annually by the Royal Society to a woman for an outstanding work in any field of Science, technology, engineering, and mathematics (STEM). It is named in honour of Rosalind Franklin and initially funded by the Department of Trade and Industry (DTI) and subsequently the Department for Innovation, Universities and Skills (DIUS) as part of its efforts to promote women in STEM. Women are a significantly underrepresented group in STEM making up less than 9% of the United Kingdom's full-time and part-time Professors in Science. The award consists of a medal and a grant of £30,000, and the recipient delivers a lecture as part of the Society's public lecture series, some of which are available on YouTube.\n\n\n the Rosalind Franklin award committee (which takes the decision on the prize each year) includes:\n\n\n]\n"}
{"id": "13893964", "url": "https://en.wikipedia.org/wiki?curid=13893964", "title": "Sven Erik Jørgensen", "text": "Sven Erik Jørgensen\n\nSven Erik Jørgensen (August 29, 1934 – March 5, 2016 in Copenhagen) was an ecologist and chemist. He became professor emeritus in environmental chemistry at the University of Copenhagen. He received in 1958 a master of science in chemical engineering from the Danish Technical University, then doctor of environmental engineering (Karlsruhe University) and doctor of science in ecological modelling (Copenhagen University). He was an honourable doctor at Coimbra University, Portugal and at Dar es Salaam University, Tanzania. In 1975 he founded a journal, \"Ecological Modelling\", and in 1978 ISEM, the International Society of Ecological Modelling. He received several awards: the Ruder Boskovic Medal, the Prigogine Prize, the Pascal Medal, the Einstein professorship at the Chinese Academy of Sciences, the Santa Chiara Prize for multidisciplinary teaching, and in 2004, together with William Mitsch, the Stockholm Water Prize. He published 366 papers of which 275 were in peer-reviewed international journals, and edited or authored 76 books, of which several have been translated into other languages (Chinese, Russian, Spanish, and Portuguese). He authored a textbook in ecological modeling “Fundamentals of Ecological Modelling”, which was published as a fourth edition together with Brian Fath in 2011. It has been translated into Chinese and Russian (third edition). Recently he authored the textbook “Introduction to Systems Ecology”. It was published in English in 2012 and in Chinese in 2013. He was editor in chief of the Encyclopedia of Ecology published in 2008, and of the Encyclopedia of Environmental Management published during December 2012. He taught courses in ecological modelling in 32 countries. He was the editorial board member of 18 international journals in the fields of ecology and environmental management. He was the president of ISEM and was elected to the European Academy of Sciences, for which he was chairman of the Section for Environmental Sciences.\n\nPersonal data: married (1970), one son.\n"}
{"id": "29895551", "url": "https://en.wikipedia.org/wiki?curid=29895551", "title": "Synchronous context-free grammar", "text": "Synchronous context-free grammar\n\nSynchronous context-free grammars (SynCFG or SCFG; not to be confused with stochastic CFGs) are a type of formal grammar designed for use in transfer-based machine translation. Rules in these grammars apply to two languages at the same time, capturing grammatical structures that are each other's translations.\n\nThe theory of SynCFGs borrows from syntax-directed transduction and syntax-based machine translation, modeling the reordering of clauses that occurs when translating a sentence by correspondences between phrase-structure rules in the source and target languages. Performance of SCFG-based MT systems has been found comparable with, or even better than, state-of-the-art phrase-based machine translation systems.\nSeveral algorithms exist to perform translation using SynCFGs.\n\nRules in a SynCFG are superficially similar to CFG rules, except that they specify the structure of two phrases at the same time; one in the source language (the language being translated) and one in the target language. Numeric indices indicate correspondences between non-terminals in both constituent trees. Chiang gives the Chinese/English example:\n\nThis rule indicates that an phrase can be formed in Chinese with the structure \"yu you \", where and are variables standing in for subphrases; and that the corresponding structure in English is \"have with \" where and are independently translated to English.\n\n"}
{"id": "37669571", "url": "https://en.wikipedia.org/wiki?curid=37669571", "title": "Territorial entity", "text": "Territorial entity\n\nA territorial entity is an entity that covers a part of the surface of the Earth with specified borders.\n\n\nEstablished by a non-physical act, such as a law, order, decree, for administrative tasks. Can include political entities with their own government, but also statistical regions or reserves.\n\n\nEstablished by physical acts, e.g. settlement, see List of uninhabited regions.\n\n"}
{"id": "40327989", "url": "https://en.wikipedia.org/wiki?curid=40327989", "title": "The Helix (magazine)", "text": "The Helix (magazine)\n\nThe Helix was a bi-monthly teen science magazine published by CSIRO Publishing, as the young-adult bi-monthly magazine of the Double Helix Science Club. The magazine was established in 1986 as the newsletter for the science club. Soon afterwards, it grew into a magazine in its own right. In 1999, a spin-off science magazine for younger readers, called \"Scientriffic\", was created.\n\nThe magazine was usually 40 pages long and trimmed to quarto paper size. It typically contained articles about science and mathematics of interest to teens.\n\nThe magazine was relaunched in July 2015 as Double Helix, combining both \"Scientriffic\" and \"The Helix\" into one magazine, starting from Issue 1, with 8 issues per year.\n\nThe final editor-in-chief was Sarah Kellett. Previous editors have included Ross Kingsland, Darren Osborne, Simon Torok, Kath Kovac, Gabrielle Tramby, Maaroof Fakhri and Jasmine Fellows.\n"}
{"id": "4516886", "url": "https://en.wikipedia.org/wiki?curid=4516886", "title": "The Subatomic Monster", "text": "The Subatomic Monster\n\nThe Subatomic Monster [1985] is a collection of seventeen nonfiction science essays written by Isaac Asimov. It was the eighteenth of a series of books collecting essays from \"The Magazine of Fantasy and Science Fiction\", these being first published between June 1983 and October 1984. It was first published by Doubleday & Company in 1985.\n\n\n"}
{"id": "40671192", "url": "https://en.wikipedia.org/wiki?curid=40671192", "title": "Umoove", "text": "Umoove\n\nUmoove is a high tech startup company that has developed and patented a software-only face and eye tracking technology. The idea was first conceived as an attempt to aid people with disabilities but has since evolved. The only compatibility qualification for tablet computers and smartphones to run Umoove software is a front-facing camera. \nUmoove headquarters are in Israel on Jerusalem’s Har Hotzvim.\n\nUmoove has 15 employees and received two million dollars in financing in 2012. The company’s original founders invested around $800,000 to start the business in 2010.\nIn 2013 Umoove was named one of the top three most promising Israeli start ups by Newsgeeks magazine.\nThe company also participated in the 2013 LeWeb conference in Paris, France, where innovative technology startups are showcased.\n\nThe technology uses information extracted from previous frames, such as the angle of the user’s head to predict where to look for facial targets in the next frame. This anticipation minimizes the amount of computation needed to scan each image.\nUmoove accounts for variances in environment, lighting conditions and user hand shake/movement. The technology is designed to provide a consistent experience, whether you’re in a brightly lit area or a darkened basement, and to work fluidly between them by adapting its processing when it detects color and brightness shifts. It uses an active stabilization technique to filter out natural body movements from an unstable camera in order to minimize false-positive motion detection.\n\nRunning the Umoove software on a Samsung Galaxy S3 is said to take up only 2% CPU.\nUmoove works exclusively with software and there is no hardware add-on necessary. It can be run on any smartphone or tablet computer that has a front-facing camera. Umoove claims that even a low-quality camera on an old device will run their software flawlessly.\n\nIn January 2014 Umoove released its first game onto the app store. The Umoove Experience game lets users control where they are 'flying' in the game through simple gestures and motions with their head. The avatar will basically go toward wherever the user looks. The game was created to showcase the technology for game developers but that did not stop some from criticizing its simplicity. Umoove also announced that they raised another one million dollars and that they are opening offices in Silicon Valley, California.\n\nIn February 2014, Umoove announced that their face-tracking software development kit is available for Android developers as well as iOS.\n\nThe Umoove Experience garnered mostly positive reviews from bloggers and mainstream media with some predicting that it could be the future of mobile gaming. Mashable wrote that Umoove’s technology could be the emergence of gesture recognition technology in the mobile space, similar to Kinect with console gaming and what Leap Motion has done with desktop computers.\n\nSome, however, remain skeptical. CNET, for example, did not give the game a positive review and called the eye tracking technology ‘freaky but cool’. They also noted that pioneering technologies have been known to fall short of expectations, citing Apple Inc’s Siri as an example. The technology blog GigaOM said that the Umoove Experience is ’awesome’ and technology evangelist Robert Scoble has called Umoove \"brilliant\".\n\nIn January 2015, Umoove released uHealth, an mobile application that uses eye tracking game-like exercise to challenge the user’s ability to be attentive, continuously focus, follow commands and avoid distractions. The app is designed in the form of two games, one to improve attention and another that hones focus.\nuHealth is a training tool, not a diagnostic. Umoove has stated that they want to use their technology for diagnosing neurological disorders but this will depend on clinical tests and FDA approval. The company cites the direct relationship between eye movements and brain activity as well as various vision based therapies have been backed by many scientific studies conducted over the past decades. uHealth is the first time this type of therapy is delivered right to the end user through a simple download.\n\nIn March 2013 there were rumors on the internet that Umoove would be the functioning software embedded into the Samsung Galaxy S4, which was due to launch that month. This rumor was perpetrated by, among others, New York Times, Techcrunch and Yahoo. \nOnce Samsung launched without the Umoove technology rumors about a potential collaboration with Apple Inc hit the web. It has been said that due to the fact that Apple Inc is losing market share and stock value to Samsung they will be more aggressive and eye tracking is a logical place to make that move.\n"}
{"id": "28102077", "url": "https://en.wikipedia.org/wiki?curid=28102077", "title": "Worldchanging (book)", "text": "Worldchanging (book)\n\nWorldchanging: A User's Guide for the 21st Century is a book about environmental concerns and practical actual responses. It is a compendium of the solutions, ideas and inventions emerging today for building a sustainable, livable, prosperous future. In November 2006, Worldchanging published a survey of global innovation, with a foreword by Al Gore, design by Stefan Sagmeister and an introduction by Bruce Sterling. It has received praise, was a winner of the \"Green Prize\" for sustainability literature, and is being translated into French under the title \"Change Le Monde\", German and several other languages. Harry N. Abrams, Inc., the publisher of the hardcover edition, listed it among their 50 best selling titles in July 2008.\n\nThe book was mentioned by BusinessWeek as one of the \"Best Innovation and Design Books for 2006\".\n\nThe updated version was published April 1, 2011, called \"Worldchanging, Revised Edition: A User's Guide for the 21st Century\" . Alex Steffen is the author, Bill McKibben wrote the Introduction, and Van Jones wrote the Foreword. Sagmeister Inc. was the designer.\n\n"}
