{"id": "28913980", "url": "https://en.wikipedia.org/wiki?curid=28913980", "title": "Balch Glacier", "text": "Balch Glacier\n\nBalch Glacier () is a glacier long, on the east coast of Graham Land, flowing southeast into Mill Inlet, to the south of Gould Glacier. It was first surveyed by the Falkland Islands Dependencies Survey in 1946–47, and named \"East Balch Glacier\". With \"West Balch Glacier\" it was reported to fill a transverse depression across Graham Land, but further survey in 1957 showed that there is no close topographical alignment between the two. The name \"Balch\", for Edwin S. Balch, an American Antarctic historian, has been limited to this glacier and an entirely new name, Drummond Glacier, approved for the west glacier.\n\n"}
{"id": "2109864", "url": "https://en.wikipedia.org/wiki?curid=2109864", "title": "Bassorah Fossa", "text": "Bassorah Fossa\n\nBassorah Fossa is a trough south of Ali Baba crater on Saturn's moon Enceladus. Bassorah Fossa was first seen in \"Voyager 2\" images. It is located at 45.4° north, 6.3° west, and is 131 kilometers long.\n\nBassorah Fossa is named after the city of Basra, Iraq, from which Sindbad embarked on his third voyage in the \"Arabian Nights\".\n"}
{"id": "47603597", "url": "https://en.wikipedia.org/wiki?curid=47603597", "title": "Behavioral Strategy", "text": "Behavioral Strategy\n\nBehavioral strategy refers to the study of corporate or business strategies from a micro-foundations perspective. This area of study is emerging in the field of management and economics. It considers the role of conscious and unconscious biases of executives, managers, teams, and organizations in strategic decision making. The field is emerging following similar developments in the fields of behavioral economics and behavioral finance.\n\n"}
{"id": "19488901", "url": "https://en.wikipedia.org/wiki?curid=19488901", "title": "Civil-military operations", "text": "Civil-military operations\n\nCivil-military operations or CMO are activities of a military force to minimize civil interference on and maximize civil support for military operations. CMO is conducted in conjunction with combat operations during wartime and becomes a central part of a military campaign in counter-insurgencies. Some militaries have specialized units dedicated to conduct CMO, such as civil affairs forces or form task forces specifically for this purposes, such as a joint civil-military operations task force in the U.S. Military. Also, some militaries have staff sections dedicated to planning and coordinating CMO for their command. CMO is often called civil-military co-operation or CIMIC in NATO operations and civil-military co-ordination in UN operations.\nThe Canadian Military defines CMO as:\n\nThe U.S. Military defines CMO as:\n"}
{"id": "4555987", "url": "https://en.wikipedia.org/wiki?curid=4555987", "title": "Clausthalite", "text": "Clausthalite\n\nClausthalite is a lead selenide mineral, PbSe. It forms a solid solution series with galena PbS.\n\nIt occurs in low-sulfur hydrothermal deposits with other selenides and in mercury deposits. It is associated with tiemannite, klockmannite, berzelianite, umangite, gold, stibiopalladinite and uraninite.\n\nIt was first described in 1832 and named for the discovery locality of Clausthal-Zellerfeld in the Harz Mountains, Germany.\n\n"}
{"id": "42279243", "url": "https://en.wikipedia.org/wiki?curid=42279243", "title": "Corotation circle", "text": "Corotation circle\n\nThe corotation circle is the circle around the galactic center of a spiral galaxy, where the stars move at the same speed as the spiral arms. The radius of this circle is called corotation radius. Inside the circle the stars move faster and outside they move slower than the spiral arms.\n\nThe Sun is located near the corotation circle of the Milky Way.\n"}
{"id": "29018709", "url": "https://en.wikipedia.org/wiki?curid=29018709", "title": "Correlation coefficient", "text": "Correlation coefficient\n\nA correlation coefficient is a numerical measure of some type of correlation, meaning a statistical relationship between two variables. The variables may be two columns of a given data set of observations, often called a sample, or two components of a multivariate random variable with a known distribution.\n\nSeveral types of correlation coefficient exist, each with their own definition and own range of usability and characteristics. They all assume values in the range from −1 to +1, where +1 indicates the strongest possible agreement and −1 the strongest possible disagreement. As tools of analysis, correlation coefficients present certain problems, including the propensity of some types to be distorted by outliers and the possibility of incorrectly being used to infer a causal relationship between the variables.\n\nThe Pearson product-moment correlation coefficient, also known as \"r\", \"R\", or Pearson's \"r\", is a measure of the strength and direction of the linear relationship between two variables that is defined as the covariance of the variables divided by the product of their standard deviations. This is the best known and most commonly used type of correlation coefficient; when the term \"correlation coefficient\" is used without further qualification, it usually refers to the Pearson product-moment correlation coefficient\n\nIntraclass correlation (ICC) is a descriptive statistic that can be used when quantitative measurements are made on units that are organized into groups; it describes how strongly units in the same group resemble each other.\n\nRank correlation is a measure of the relationship between the rankings of two variables or two rankings of the same variable:\n\n"}
{"id": "990648", "url": "https://en.wikipedia.org/wiki?curid=990648", "title": "Dynamicism", "text": "Dynamicism\n\nDynamicism, also termed the \"dynamic hypothesis\" or the \"dynamic hypothesis in cognitive science\" or \"dynamic cognition\", is a new approach in cognitive science exemplified by the work of philosopher Tim van Gelder. It argues that differential equations are more suited to modeling cognition than more traditional computer models. Cf also dynamical systems theory.\n"}
{"id": "30244495", "url": "https://en.wikipedia.org/wiki?curid=30244495", "title": "Engineering apprentice", "text": "Engineering apprentice\n\nAn engineering apprenticeship in the United Kingdom is an apprenticeship in mechanical engineering or electrical engineering or aeronautical engineering to train craftsmen, technicians, senior technicians, Incorporated Engineers and Chartered Engineer for vocational oriented work and professional practice. Chartered Engineers are usually formed through a university degree programme at the Masters Engineering level and may undertake a short form of post graduate apprenticeship. A typical example is the apprenticeships formerly available at the British Thomson-Houston and English Electric companies at Rugby in England. Subjects covered included mathematics, engineering sciences, limits and fits, metallurgy, foundry technology, engineering drawing, design, materials science for engineering materials, metalworking by hand, operating machine tools, and basic features of engineering design. Also refer to apprenticeship and the UK and German section. Elite technical apprenticeships (4-6 years long) have been a decades long tradition at UK companies such as BAE Systems, Rolls-Royce Holdings, Bombardier Aerospace (Short Brothers), and Babcock International.\n"}
{"id": "37355313", "url": "https://en.wikipedia.org/wiki?curid=37355313", "title": "Enterobacteria phage Wphi", "text": "Enterobacteria phage Wphi\n\nEnterobacteria phage Wphi is a virus of the family Myoviridae, genus \"P2-like viruses\".\n\nAs a member of the group I of the Baltimore classification, \"Enterobacteria phage Wphi\" is a dsDNA viruses. All the members of family Myoviridae share a nonenveloped morphology consisting of a head and a tail separated by a neck. Its genome is linear. The propagation of the virions includes the attaching to a host cell (a bacterium, as \"Enterobacteria phage Wphi\" is a bacteriophage) and the injection of the double stranded DNA; the host transcribes and translates it to manufacture new particles. To replicate its genetic content requires host cell DNA polymerases and, hence, the process is highly dependent on the cell cycle.\n"}
{"id": "43531836", "url": "https://en.wikipedia.org/wiki?curid=43531836", "title": "Ezekiel Adebiyi", "text": "Ezekiel Adebiyi\n\nEzekiel Adebiyi is a Nigerian bioinformatics professor and research scientist. He is the current president of Nigerian Society of Bioinformatics and Computational Biology. He is also the current vice-president of African Society for Bioinformatics and Computational Biology. In 2010, he was made a professor at Covenant University, an elevation that made him the first bioinformatics professor in West Africa.\nAdebiyi was born on 28 July 1970 in Ibadan, Nigeria, although he is a native of Boluwaduro area of Osun State. He had his secondary school at United Community Secondary School, Ilorin. Between 1987 and 1991, he studied Mathematics at University of Ilorin graduating as the best graduating student. He also completed his master's degree from the same institution in 1995. His doctorate thesis was on \"Pattern Discovery in Biology and Strings Sorting: Theory and Experimentation\", which he concluded in 2002 at University of Tübingen.\n\nAdebiyi began his lecturing career immediately after graduation at University of Ilorin. In 2003, he withdrew from the school to become a visiting scientist at several research centers including San Diego Supercomputer Center, University of Montpellier and German Cancer Research Center. He continued with professional research before joining Covenant University Computer science department in 2008. He was promoted to the professorial cadre in 2010.\n\nBetween 2007 and 2011, Adebiyi was the vice president of African Society for Bioinformatics and Computational Biology. After his tenure, he became the secretary of the professional body. He is also the president of Nigeria Society of Bioinformatics and Computational Biology (NISBCB).\n\nAdebiyi is fluent in Yoruba, English and German.\n\n"}
{"id": "4675271", "url": "https://en.wikipedia.org/wiki?curid=4675271", "title": "Financial risk modeling", "text": "Financial risk modeling\n\nFinancial risk modeling is the use of formal econometric techniques to determine the aggregate risk in a financial portfolio. Risk modeling is one of many subtasks within the broader area of financial modeling.\n\nRisk modeling uses a variety of techniques including market risk, value at risk (VaR), historical simulation (HS), or extreme value theory (EVT) in order to analyze a portfolio and make forecasts of the likely losses that would be incurred for a variety of risks. Such risks are typically grouped into credit risk, liquidity risk, market risk, and operational risk categories.\n\nMany large financial intermediary firms use risk modeling to help portfolio managers assess the amount of capital reserves to maintain, and to help guide their purchases and sales of various classes of financial assets. \n\nFormal risk modeling is required under the Basel II proposal for all the major international banking institutions by the various national depository institution regulators. In the past, risk analysis was done qualitatively but now with the advent of powerful computing software, quantitative risk analysis can be done quickly and effortlessly.\n\nModeling the changes by distributions with finite variance is now known to be inappropriate. Benoît Mandelbrot found in the 1960s that changes in prices in financial markets do not follow a Gaussian distribution, but are rather modeled better by Lévy stable distributions. The scale of change, or volatility, depends on the length of the time interval to a power a bit more than 1/2. Large changes up or down, also called \"fat tails\", are more likely than what one would calculate using a Gaussian distribution with an estimated standard deviation.\n\nQuantitative risk analysis and its modeling have been under question in the light of corporate scandals in the past few years (most notably, Enron), Basel II, the revised FAS 123R and the Sarbanes-Oxley Act, and for their failure to predict the financial crash of 2008.\n\n\n\n"}
{"id": "1676066", "url": "https://en.wikipedia.org/wiki?curid=1676066", "title": "Fritz Mueller", "text": "Fritz Mueller\n\nFritz K. Mueller (1907 – 2001 Huntsville, Alabama, USA) was a German engineer.\n\nMueller was hired by Kreiselgeräte Company in 1933. He developed the PIGA accelerometer. and worked on gyroscopes for Nazi Germany's \"Kriegsmarine\". Later on, he worked on the guidance and control system for the A3 test rocket, the A5, and the A4 (V-2) ballistic missile.\n\nUnder Project Paperclip, Mueller emigrated to the United States on 16 November 1945 with the Argentina group. There, he worked on developing guidance systems for the PGM-11 Redstone, PGM-19 Jupiter, MGM-31 Pershing, and the Saturn I missiles.\nIn 1960 Mueller left NASA for private industry.\n"}
{"id": "4967481", "url": "https://en.wikipedia.org/wiki?curid=4967481", "title": "High Resolution Microwave Survey", "text": "High Resolution Microwave Survey\n\nThe High Resolution Microwave Survey was a NASA project that was to scan ten million frequencies using radio telescopes. A decade in the making, the objective was to find transmissions from alien intelligences. The primary point of observation for the project was the Arecibo Ionospheric Observatory in Puerto Rico. The project began in October 1992 with SETI researcher Jill Tarter on board. However, one year later, first-term Nevada Senator Richard Bryan succeeded in shutting down the project.\n\n\n"}
{"id": "49753467", "url": "https://en.wikipedia.org/wiki?curid=49753467", "title": "Ilirneyite", "text": "Ilirneyite\n\nIlirneyite is a rare tellurate mineral with the formula Mg[ZnMn(TeO)]•4.5HO. It was discovered at the Sentyabr'skoe deposit (of silver and gold), Western Chukotka, Russia.\n\nIlirneyite is a trivalent-manganese-analogue of zemannite. It is also a zinc- and manganese-analogue of keystoneite and kinichilite.\n\n"}
{"id": "57032460", "url": "https://en.wikipedia.org/wiki?curid=57032460", "title": "Imagining the Elephant", "text": "Imagining the Elephant\n\nImagining the Elephant: A Biography of Allan MacLeod Cormack is a 2008 biography of physicist and Nobel laureate Allan MacLeod Cormack written by biomedical engineer and author Christopher Kit Vaughan and published by Imperial College Press. The book won the University of Cape Town Book Award in 2010, an honour shared with Nobel laureate J.M. Coetzee who won the award in 1984 for Waiting for the Barbarians.\n\nCormack shared the Nobel Prize in Physiology or Medicine in 1979 with Godfrey Hounsfield “for the development of computer assisted tomography.” The book’s title is inspired by the parable of six blind men and an elephant, in which the men, by approaching the animal from different angles, imagine it to be an elephant. This is analogous to computed tomography – or CT scan – in which a series of two-dimensional X-rays taken from different angles are used to construct a three-dimensional image of the object.\n\nThe book is divided into nine chapters: (1) From John O'Groats to Jo'burg; (2) On the Slopes of Table Mountain; (3) Physics and Friends at Cambridge; (4) Return to the Fairest Cape; (5) A New Beginning in Boston; (6) Finding Radon and His Transform; (7) On the Road to Stockholm; (8) Citizen of the World; and (9) At Home in Massachusetts. In addition, there are five appendices: (A) Allan Cormack's Publications; (B) Nobel Lecture; (C) Presentation of Nobel Prize; (D) Man and Science in the 21st Century; and (E) A Teenager's Odyssey. Appendix D is an essay written by Cormack for \"The Mainichi Newspapers\", while Appendix E is a mini-biography with cartoons written for \"The Weekly Shonen Jump\", a magazine for teenagers. Neither of these essays, which were originally published in Japanese, had previously been published in English.\n\nWriting in \"The New England Journal of Medicine\", Reginald Greene commented: “Those interested in the history of science are indebted to Vaughan for producing this wonderful biography of Allan Cormack and for creating an expert and vivid description of one of the two streams of discovery that led to the invention of computed tomography.” Robin Cherry, who was one of Cormack’s students in the early 1950s, said: “A prodigious effort has gone into this book, and the result is a comprehensive work of reference which will be a valuable source for those interested in the history of science and medicine.”\n\n"}
{"id": "437476", "url": "https://en.wikipedia.org/wiki?curid=437476", "title": "International Nuclear Information System", "text": "International Nuclear Information System\n\nThe International Nuclear Information System (INIS) hosts one of the world's largest collections of published information on the peaceful uses of nuclear science and technology.\n\nINIS is based in Vienna, Austria and has been operating since 1970. INIS is operated by the IAEA (International Atomic Energy Agency) in collaboration with 127 Member States and 24 co-operating international organizations. All the content it holds is currently available free to \"all Internet users around the world\".\n\n\n"}
{"id": "2299386", "url": "https://en.wikipedia.org/wiki?curid=2299386", "title": "Lisfranc injury", "text": "Lisfranc injury\n\nLisfranc injury, also known as Lisfranc fracture, is an injury of the foot in which one or more of the metatarsal bones are displaced from the tarsus. The injury is named after Jacques Lisfranc de St. Martin (2 April 179013 May 1847), a French surgeon and gynecologist who described an amputation of the foot through the tarsometatarsal articulation, in 1815, after the War of the Sixth Coalition.\n\nIn humans, the midfoot consists of five bones that form the arches of the foot (the cuboid, navicular, and three cuneiform bones) and their articulations with the bases of the five metatarsal bones. Lisfranc injuries are caused when excessive kinetic energy is applied either directly or indirectly to the midfoot and are often seen in traffic collisions or industrial accidents.\n\nDirect Lisfranc injuries are usually caused by a crush injury, such as a heavy object falling onto the midfoot, or the foot being run over by a car or truck, or someone landing on the foot after a fall from a significant height. Indirect Lisfranc injuries are caused by a sudden rotational force on a plantar flexed (downward pointing) forefoot. Examples of this type of trauma include a rider falling from a horse but the foot remaining trapped in the stirrup, or a person falling forward after stepping into a storm drain.\n\nIn athletic trauma, Lisfranc injuries occur commonly in activities such as windsurfing, kitesurfing, wakeboarding, or snowboarding (where appliance bindings pass directly over the metatarsals). American football players occasionally acquire this injury, and it most often occurs when the athlete's foot is plantar flexed and another player lands on the heel. This can also be seen in pivoting athletic positions such as a baseball catcher or a ballerina spinning.\n\nIn a high energy injury to the midfoot, such as a fall from a height or a motor vehicle accident, the diagnosis of a Lisfranc injury should, in theory at least, pose less of a challenge. There will be deformity of the midfoot and X-ray abnormalities should be obvious. Further, the nature of the injury will create heightened clinical suspicion and there may even be disruption of the overlying skin and compromise of the blood supply. Typical X-ray findings would include a gap between the base of the first and second toes. The diagnosis becomes more challenging in the case of low energy incidents, such as might occur with a twisting injury on the racquetball court, or when an American Football lineman is forced back upon a foot that is already in a fully plantar flexed position. Then, there may only be complaint of inability to bear weight and some mild swelling of the forefoot or midfoot. Bruising of the arch has been described as diagnostic in these circumstances but may well be absent. Typically, conventional radiography of the foot is utilized with standard non-weight bearing views, supplemented by weight bearing views which may demonstrate widening of the interval between the first and second toes, if the initial views fail to show abnormality. Unfortunately, radiographs in such circumstances have a sensitivity of 50% when non-weight bearing and 85% when weight bearing, meaning that they will appear normal in 15% of cases where a Lisfranc injury actually exists. In the case of apparently normal x-rays, if clinical suspicion remains, advanced imaging such as magnetic resonance imaging (MRI) or X-ray computed tomography (CT) is a logical next step.\n\nThere are three classifications for the fracture:\n\nOptions include operative or non-operative treatment. If the dislocation is less than 2 mm, the fracture can be managed with casting for six weeks. The patient's injured limb cannot bear weight during this period. For severe Lisfranc injuries, open reduction with internal fixation (ORIF) and temporary screw or Kirschner wire (K-wire) fixation is the treatment of choice. The foot cannot be allowed to bear weight for a minimum of six weeks. Partial weight-bearing may then begin, with full weight bearing after an additional several weeks, depending on the specific injury. K-wires are typically removed after six weeks, before weight bearing, while screws are often removed after 12 weeks.\n\nWhen a Lisfranc injury is characterized by significant displacement of the tarsometatarsal joint(s), nonoperative treatment often leads to severe loss of function and long-term disability secondary to chronic pain and sometimes to a planovalgus deformity. In cases with severe pain, loss of function, or progressive deformity that has failed to respond to nonoperative treatment, mid-tarsal and tarsometatarsal arthrodesis (operative fusion of the bones) may be indicated.\n\nDuring the Napoleonic Wars, Jacques Lisfranc de St. Martin encountered a soldier who suffered from vascular compromise and secondary gangrene of the foot after a fall from a horse. Subsequently, Lisfranc performed an amputation at the level of the tarsometatarsal joints, and that area of the foot has since been eponymously referred to as the \"Lisfranc joint\". Although Lisfranc did not describe a specific mechanism of injury or classification scheme, a Lisfranc injury has come to mean a dislocation or fracture-dislocation injury at the tarsometatarsal joints.\n\n\n"}
{"id": "2680895", "url": "https://en.wikipedia.org/wiki?curid=2680895", "title": "List of compounds with carbon number 4", "text": "List of compounds with carbon number 4\n\nThis is a partial list of molecules that contain 4 carbon atoms.\n\n"}
{"id": "55185244", "url": "https://en.wikipedia.org/wiki?curid=55185244", "title": "List of earthquakes in the Caribbean", "text": "List of earthquakes in the Caribbean\n\nMajor earthquakes in the Caribbean are infrequent and are sometimes accompanied by tsunami.\n\n\nSources\n"}
{"id": "342094", "url": "https://en.wikipedia.org/wiki?curid=342094", "title": "List of gene families", "text": "List of gene families\n\nThis is a list of gene families or gene complexes, that is sets of genes which occur across a number of different species which often serve similar biological functions. These gene families typically encode functionally related proteins, and sometimes the term gene families is a shorthand for the sets of proteins that the genes encode. They may or may not be physically adjacent on the same chromosome.\n\n\n\n\n\n\n\n"}
{"id": "50688850", "url": "https://en.wikipedia.org/wiki?curid=50688850", "title": "List of incurable diseases", "text": "List of incurable diseases\n\nThis is an incomplete list of incurable diseases. It includes both physical and mental diseases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19983343", "url": "https://en.wikipedia.org/wiki?curid=19983343", "title": "List of internet service providers in India", "text": "List of internet service providers in India\n\nThis is a list of internet service providers in India. There were 302 internet service providers (ISPs) offering broadband and narrowband internet services in India as of 30 June 2018.\n\nThe following table shows the top 10 ISPs in India by total subscriber base as of 30 June 2018. Broadband is defined as \"an always-on Internet connection with download speed of 512 kbit/s or above.\" The number of internet users are 512.26 million, out of which 65.14 million are narrowband subscribers and 447.12 million are broadband subscribers.\n\nNote:\n\n\n\n\n"}
{"id": "22770838", "url": "https://en.wikipedia.org/wiki?curid=22770838", "title": "List of systems biology conferences", "text": "List of systems biology conferences\n\nSystems biology is a biological study field that focuses on the systematic study of complex interactions in biological systems, thus using a new perspective (integration instead of reduction) to study them. Particularly from year 2000 onwards, the term is used widely in the biosciences.\n\nThe field has generated interest among scientists, resulting in regular and one-time conferences and meetings. Below is a partial list.\n"}
{"id": "169823", "url": "https://en.wikipedia.org/wiki?curid=169823", "title": "List of two-dimensional geometric shapes", "text": "List of two-dimensional geometric shapes\n\nThis is a list of two-dimensional geometric shapes in Euclidean and other geometries. For mathematical objects in more dimensions, see list of mathematical shapes. For a broader scope, see list of shapes.\n\n\n\n\n\n"}
{"id": "24081543", "url": "https://en.wikipedia.org/wiki?curid=24081543", "title": "List of volcanic craters in Arizona", "text": "List of volcanic craters in Arizona\n\nThe United States National Geodetic Survey lists 28 craters in the state of Arizona.\n\n<includeonly></includeonly>\n\n\nLocated just northeast of the town of Clifton, Arizona, this dormant cinder cone volcano and crater is easily visible from the town and In the satellite view of Google Maps. The area is still geologically active, with several hot springs in the area.\n\n"}
{"id": "7119981", "url": "https://en.wikipedia.org/wiki?curid=7119981", "title": "List of volcanoes in Chile", "text": "List of volcanoes in Chile\n\nThis is a list of volcanoes in Chile. The Smithsonian Institution's Global Volcanism Program lists 105 volcanoes in Chile that have been active during the Holocene. SERNAGEOMIN lists 90 active volcanoes in Chile. \n\nThe Pacific islands of Chile are of volcanic origin. They have been formed from magma coming from three distinct hotspots, Easter, Juan Fernández and San Felix hotspots. The westernmost part of the ridges formed by these hotspots contain the most recently active volcanoes.\n\nSome volcanoes or groups of volcanoes are under surveillance of OVDAS because of their critical activity or proximity to big cities.\n\nThis list does not include Chilean claims in the Antarctic.\n\n\n"}
{"id": "25851180", "url": "https://en.wikipedia.org/wiki?curid=25851180", "title": "Matias Zaldarriaga", "text": "Matias Zaldarriaga\n\nMatias Zaldarriaga is an Argentine cosmologist.\n\nBorn in Coghlan neighbourhood, Buenos Aires, at the present time he works in the Institute for Advanced Study located in Princeton, New Jersey, United States. He is known especially for his work on the cosmic microwave background (CMB). \nTogether with Uros Seljak, he developed the CMBFAST code, the first computationally efficient method for computing the anisotropy of the CMB for an arbitrary set of cosmological parameters. In 2018, he was elected a member of the National Academy of Sciences.\n\nIn 2003, he was awarded the Helen B. Warner Prize for Astronomy by the\nAmerican Astronomical Society, and in 2005 he won the Gribov Medal of the European Physical Society. In\n2006, he was the recipient of a MacArthur Fellowship.\n\n"}
{"id": "53117142", "url": "https://en.wikipedia.org/wiki?curid=53117142", "title": "Microbes and Man", "text": "Microbes and Man\n\nMicrobes and Man is a popularising book by the English microbiologist John Postgate FRS on the role of microorganisms in human society, first published in 1969, and still in print in 2017. Critics called it a \"classic\" and \"a pleasure to read\".\n\nThe book is structured as follows:\n\n\nThe 4th edition has 32 illustrations, ranging from photographs of microscopic algae, protozoa, fungi, viruses and bacteria, to the macroscopic effects of microbes such as a sulphur-forming lake in Libya and fish killed by bacterial reduction of sulphate in water.\n\n\nThe book has been translated into nine languages: Arabic, Chinese, Czech, French, German, Japanese, Polish, Portuguese, and Spanish.\n\n\"The Guardian\" described the book as \"a passionate case for the importance of micro-organisms\".\n\nIn his textbook \"Essential Microbiology\", Stuart Hogg recommends the book to readers who want a general overview of microbes and their uses, stating \"there can be no better starting point than John Postgate's classic\".\n\n\"New Scientist\" described the book as \"a pleasure to read from first page to last. It is a literal statement. Start to read it and the first page, describing the astonishing dispersion of microbes, from the upper atmosphere to the depths of the sea, will provide any reader with enough wonder and excitement to take them through to the last page and the surface of Venus.\" The magazine commented that Postgate's \"admirable, elegantly written and painlessly informative book\" came close to losing its alliterative title, at the hands of \"militant feminists\" at Penguin Books editing the paperback version in 1986.\n\nDennis R. Schneider, reviewing the 3rd edition in 1992 for \"Cell\", described the book as having \"succinctly and carefully explained examples of how microorganisms affect our lives ... one of the classics of popular science\", standing alongside classics like Rosebury's \"Life of Man\" and De Kruif's \"Microbe Hunters\". Schneider wrote that the book's Britishness \"'colours' the text\", but Postgate's emphasis on the beneficial and not just the harmful effects of microbes was welcome and admirably explored. He noted few errors, but objected to Postgate's assertion that AIDS \"originated by transmission from a primate\", for which there was at that time no evidence. Schneider would have liked a \"better and longer\" account of molecular biology. His chief criticism, however, was that by the 1990s the book no longer had an audience, since \"the Victorial ideal of the educated middle class has vanished into the wasteland of broken families, double digit unemployment and a damaged educational system\". All the same, he found the book \"of value and beauty (except perhaps to the publisher)\".\n\nCharles W. Kim, reviewing the 3rd edition for \"The Quarterly Review of Biology\", stated that \"If the author's intent was to present the impact of the ubiquitous microorganisms on the environment and humans, he has succeeded admirably\", describing Postgate's style as \"unique\".\n\nD. Roy Cullimore, in his \"Practical Atlas for Bacterial Identification\", comments that all four editions were \"easy reading\", addressing the challenges that microbes presented to human society. He suggests that \"ideally\" all four books be read in sequence for an overiew of the development of microbiology in half a century.\n"}
{"id": "25676077", "url": "https://en.wikipedia.org/wiki?curid=25676077", "title": "Nucleonica", "text": "Nucleonica\n\nNucleonica is a nuclear science web portal. The company Nucleonica GmbH was founded by Dr. Joseph Magill in 2011 as a spin-off from the European Commission’s Joint Research Centre, Institute for Transuranium Elements. In addition to providing user friendly access to nuclear data, the main focus of Nucleonica is to provide professionals in the nuclear industry with a suite of validated scientific applications for everyday calculations. The portal is also suitable for education and training in the nuclear field.\n\nNucleonica GmbH is also responsible for the management and development of the Karlsruhe Nuclide Chart print and online versions.\n\nUsers can register for free access to Nucleonica. This free access gives the user access to most applications but is restricted to a limited number of nuclides. For full access to all nuclides and applications, the user can upgrade to Premium for which there is an annual user charge.\n"}
{"id": "51611782", "url": "https://en.wikipedia.org/wiki?curid=51611782", "title": "Open Data Indices", "text": "Open Data Indices\n\nOpen data indices are indicators which assess and evaluates the general openness of an open government data portal. Open data indices not only show how open a data portal is, but also encourage citizens and government officials alike, to participate in their local open data communities, particularly in advocating for local open data and local open data policies.\n\nThere are two mainstream methodologies, which are Global Open Data Index and Open Data Barometer. The Global Open Data Index evaluates an open data portal from 11 different aspects based on the Open Definition of open data, while the Open Data Barometer adds two more indices compared to the previous one.\n\nAccording to the service offered by Open Knowledge International, they run a measurement called \"Global Open Data Index\" which is \"an annual effort to measure the state of open government data around the world\". And they evaluate the openness of an open dataset according to the following questions:\n\nThe Open Knowledge Foundation specifically indicates that the data of an open data portal should be directly comes from the official government department or a third party with the permission of the government that they can fully represent the government. And if so, the third party should explicitly states the permission.\n\nThis question does not examines if the data can be accessed online or by public but if the data exists in any digital format.\n\nA data could be considered as publicly available when it can be accessed without any permission or password by every individual (not just government officers) and there is no restrictions for the amount of photocopies can be made if the data is in the paper form. For this question, it does not matter if the data is in paper form or digital form.\n\nThe data is available for free if the access of the data does not require any forms of charges.\n\nThe data is available online if it can be accessed through the Internet from an official source.\n\nThis question addresses whether the data is in a form that can be easily processed by the computer. File types such as XLS, CSV, JSON, XML are considered as machine-readable, while PDF, or HTML are not.\n\nIf the whole dataset can be easily downloaded, it can be considered as available in bulk.\n\nThis question addresses whether the data can be freely used, reused, and redistributed by everyone without any restrictions. A list of types of licenses that meet the requirements is listed at http://opendefinition.org/licenses/.\n\nThis question examines if the data is updated on a regular basis. It requires personal judgement with rationale.\n\nEach of these questions evaluates different aspects of a dataset, and each question is weighted differently based on the importance. There is in total 13 types of datasets. The final score is calculated according to following equation: sum of all datasets scores/1300 ( (the maximum possible score that a country can get) - sum (13 dataset)/1300 = index percentage. The Global Open Data Index ranks each country according to their percentage of openness.\n\nIn addition, the Open Data Barometer adds two more question for their evaluation of the open data portal, and they are:\n\n\n"}
{"id": "23208557", "url": "https://en.wikipedia.org/wiki?curid=23208557", "title": "Orbital replacement unit (HST)", "text": "Orbital replacement unit (HST)\n\nAn orbital replacement unit or orbital replaceable unit is a modular component of spacecraft that can be replaced upon failure either by robot or by extravehicular activity. The Hubble Space Telescope (HST) was designed with 70 such parts, including scientific instruments and limited-life items such as batteries.\n\nOn HST some parts were designed from the start as ORUs and all used captive bolts with a standard 7/16\" double-height hex head; later when it was decided to avoid returning HST to Earth for repair, more systems and modules were designated as ORUs (but used a wider variety of fasteners). HST servicing mission 3A (SM3A) replaced (or added) 15 ORUs, e.g. it replaced the DF-224 computer with the \"Advanced Computer\".\n\nThe electrical system of the International Space Station also has such subsystems that provide power generation, power distribution and energy storage.\n\n"}
{"id": "44004831", "url": "https://en.wikipedia.org/wiki?curid=44004831", "title": "Ossian Aschan", "text": "Ossian Aschan\n\nAdolf Ossian Aschan (16 May 1860, Helsinki – 25 February 1939) was a Finnish chemist and politician. He was a member of the Parliament of Finland from 1910 to 1911, representing the Swedish People's Party of Finland (SFP). He served as the professor of chemistry at the University of Helsinki from 1908 to 1927.\n\n"}
{"id": "29378249", "url": "https://en.wikipedia.org/wiki?curid=29378249", "title": "Paolo Enriques", "text": "Paolo Enriques\n\nPaolo Enriques (17 August 1878 in Leghorn – 26 December 1932 in Rome) was an Italian zoologist. \n\nEnriques taught Zoology and Comparative Anatomy at the University of Sassari (1917 to 1921), then in 1922 he became Professor of Zoology in the University of Padua University, and Director of the Institute of Zoology and Comparative Anatomy. He was primarily interested in comparative cytology, physiology and genetics.He wrote “Teoria cellulare” or, in English Cellular Theory (1911), “Eredità dell’uomo” or Inheritance in Man (1924), and “Le leggi di Mendel e i cromosomi” or Mendel’s Laws and Chromosomes (1932). He died in a car accident in Rome.\n\n"}
{"id": "533206", "url": "https://en.wikipedia.org/wiki?curid=533206", "title": "Place theory (hearing)", "text": "Place theory (hearing)\n\nPlace theory is a theory of hearing that states that our perception of sound depends on where each component frequency produces vibrations along the basilar membrane. By this theory, the pitch of a sound, such as a human voice or a musical tone, is determined by the places where the membrane vibrates, based on frequencies corresponding to the tonotopic organization of the primary auditory neurons.\n\nMore generally, schemes that base attributes of auditory perception on the neural firing rate as a function of place are known as rate–place schemes.\n\nThe main alternative to the place theory is the temporal theory, also known as timing theory. These theories are closely linked with the volley principle or volley theory, a mechanism by which groups of neurons can encode the timing of a sound waveform. In all cases, neural firing patterns in time determine the perception of pitch. The combination known as the place–volley theory uses both mechanisms in combination, primarily coding low pitches by temporal pattern and high pitches by rate–place patterns. It is now generally believed that there is good evidence for both mechanisms.\n\nThe place theory is usually attributed to Hermann Helmholtz, though it was widely believed much earlier.\n\nExperiments to distinguish between place theory and rate theory are difficult to devise, because of the strong correlation: large vibrations with low rate are produced at the apical end of the basilar membrane while large vibrations with high rate are produced at the basal end. The two can be controlled independently using cochlear implants: pulses with a range of rates can be applied via electrodes distributed along the membrane. Experiments using implant recipients showed that, at low stimulation rates, ratings of pitch on a pitch scale were proportional to the log of stimulation rate, but also decreased with distance from the round window. At higher rates, the effect of rate was weaker, but the effect of place was strong.\n"}
{"id": "3306703", "url": "https://en.wikipedia.org/wiki?curid=3306703", "title": "Reciprocal determinism", "text": "Reciprocal determinism\n\nReciprocal determinism is the theory set forth by psychologist Albert Bandura which states that a person's behavior both influences and is influenced by personal factors and the social environment. Bandura accepts the possibility that an individual's behavior may be conditioned through the use of consequences. At the same time he asserts that a person's behavior (and personal factors, such as cognitive skills or attitudes) can impact the environment. These skill sets result in an under- or overcompensated ego that, for all creative purposes, is too strong or too weak to focus on pure outcome. This is important because Bandura was able to prove the strong correlation between this with experiments.\n\nBandura was able to show this when he created the Bandura's Box experiment. As an example, Bandura's reciprocal determinism could occur when a child is acting out in school. The child doesn't like going to school; therefore, he/she acts out in class. This results in teachers and administrators of the school disliking having the child around. When confronted by the situation, the child admits he/she hates school and other peers don't like him/her. This results in the child acting inappropriately, forcing the administrators who dislike having him/her around to create a more restrictive environment for children of this stature. Each behavioral and environmental factor coincides with the child and so forth resulting in a continuous battle on all three levels.\n\nReciprocal determinism is the idea that behavior is controlled or determined by the individual, through cognitive processes, and by the environment, through external social stimulus events. The basis of reciprocal determinism should transform individual behavior by allowing subjective thought processes transparency when contrasted with cognitive, environmental, and external social stimulus events.\n\nActions do not go one way or the other, as it is affected by repercussions, meaning one's behavior is complicated and can't be thought of as individual and environmental means. Behavior consist of environmental and individual parts that interlink together to function. Many studies showed reciprocal associations between people and their environments over time.\n\nResearch conducted in this field include the study of doctor-patient relationships where one group of patients are termed 'physician-reliant' and the other group 'self-reliant'. The physician-reliant patients tend to be more passive in their decision making and rely on their physicians to make their choices for them. Self-reliant patients take a more active role in deciding which health options would better suit them.\n\nAnother relevant research is regarding the reciprocal determinism of self-efficacy and mathematical performance. It shows that reciprocal determinism may not be the appropriate model in all cultures but does take place in most. Self-efficacy is a conceptualized assessment of the person's competence to perform a specific task. Self-efficacy results from success or failures that arise in attempts to learn a task. Self-efficacy, measure by a personal confidence level before each question, and the mathematical scores were obtained in 41 countries for the study by Kitty and Trevor Williams. The reciprocal determinism of mathematics self-efficacy and achievement was found in 26 of the 30 nations. They suggest that this might be a fundamental psychological process that takes place across national boundaries.\nAccording to Albert Bandura, self-efficacy is defined as a person's belief in their capability to accomplish a certain task. Another study looked at the relationship of self-efficacy and job culture with job satisfaction among athletic trainers. The study used Bandura's triadic reciprocal causation model as a template to label job satisfaction as the behavioural factor, self-efficacy as the personal factor, and job culture as the environmental factor.\n\nTriadic reciprocal causation is a term introduced by Albert Bandura to refer to the mutual influence between three sets of factors:\n\nBehavioral genetics is a relatively new field of study attempting to make sense of both genetic and environmental contributions to individual variations in human behavior. Genes can be turned on and off. Multiple genes are factors in forming behavior traits.\n\nResearchers believe there is a genetic link to impulsive aggression through the impact of a gene on the production of an enzyme called Monoamine oxidase A (MAOA). The MAOA gene reduces the production of MAOA, leading to increased incidents of impulsive aggression. A 26-year study in New Zealand found strong correlation between experience of childhood abuse and criminal or violent behavior in males with the MAOA gene. In that study, impulsive aggression was found to be nine times more likely to manifest in males with the gene who were abused than in abused males without the gene or males with the gene who had not been abused.\n\n\n"}
{"id": "50987710", "url": "https://en.wikipedia.org/wiki?curid=50987710", "title": "Robert Edwards Carter Stearns", "text": "Robert Edwards Carter Stearns\n\nRobert Edwards Carter Stearns (1 February 1827, Boston – 27 July 1909, Los Angeles) was an American conchologist.\n\nRobert Stearns was passionate about natural history in his youth. Later he specialised in conchology, especially that of the West Coast of the United States. He was a member of the Fisheries Commission (1882–1884) and Secretary of the University of California (Berkeley) (1874–1882). He became Assistant Curator of Molluscs at the National Museum of Natural History (1885–1892). Stearns married Mary Ann Libby on 28 March 1850.They had one child, a daughter.\n\n"}
{"id": "798571", "url": "https://en.wikipedia.org/wiki?curid=798571", "title": "Rule of succession", "text": "Rule of succession\n\nIn probability theory, the rule of succession is a formula introduced in the 18th century by Pierre-Simon Laplace in the course of treating the sunrise problem.\n\nThe formula is still used, particularly to estimate underlying probabilities when there are few observations, or for events that have not been observed to occur at all in (finite) sample data. Assigning events a zero probability contravenes Cromwell's rule; such contravention can never be strictly justified in physical situations, albeit sometimes must be assumed in practice.\n\nIf we repeat an experiment that we know can result in a success or failure, \"n\" times independently, and get \"s\" successes, then what is the probability that the next repetition will succeed?\n\nMore abstractly: If \"X\", ..., \"X\" are conditionally independent random variables that each can assume the value 0 or 1, then, if we know nothing more about them,\n\nSince we have the prior knowledge that we are looking at an experiment for which both success and failure are possible, our estimate is as if we had observed one success and one failure for sure before we even started the experiments. In a sense we made \"n\" + 2 observations (known as pseudocounts) with \"s\"+1 successes. Although this may seem the simplest and most reasonable assumption, which also happens to be true, it still requires a proof. Indeed, assuming a pseudocount of one per possibility is one way to generalise the binary result, but has unexpected consequences — see Generalization to any number of possibilities, below.\n\nNevertheless, if we had not known from the start that both success and failure are possible, then we would have had to assign\n\nBut see Mathematical details, below, for an analysis of its validity. In particular it is not valid when formula_3, or formula_4.\n\nIf the number of observations increases, formula_5 and formula_6 get more and more similar, which is intuitively clear: the more data we have, the less importance should be assigned to our prior information.\n\nLaplace used the rule of succession to calculate the probability that the sun will rise tomorrow, given that it has risen every day for the past 5000 years. One obtains a very large factor of approximately 5000 × 365.25, which gives odds of 1826251:1 in favour of the sun rising tomorrow.\n\nHowever, as the mathematical details below show, the basic assumption for using the rule of succession would be that we have no prior knowledge about the question whether the sun will or will not rise tomorrow, except that it can do either. This is not the case for sunrises.\n\nLaplace knew this well, and he wrote to conclude the sunrise example: “But this number is far greater for him who, seeing in the totality of phenomena the principle regulating the days and seasons, realizes that nothing at the present moment can arrest the course of it.” Yet Laplace was ridiculed for this calculation; his opponents gave no heed to that sentence, or failed to understand its importance.\n\nIn the 1940s, Rudolf Carnap investigated a probability-based theory of inductive reasoning, and developed measures of degree of confirmation, which he considered as alternatives to Laplace's rule of succession. See also New riddle of induction#Carnap.\n\nThe proportion \"p\" is assigned a uniform distribution to describe the uncertainty about its true value. (This proportion is not random, but uncertain. We assign a probability distribution to \"p\" to express our uncertainty, not to attribute randomness to \"p\". But this amounts, mathematically, to the same thing as treating \"p as if\" it were random).\n\nLet \"X\" be 1 if we observe a \"success\" on the \"i\"th trial, otherwise 0, with probability \"p\" of success on each trial. Thus each \"X\" is 0 or 1; each \"X\" has a Bernoulli distribution. Suppose these \"X\"s are conditionally independent given \"p\".\n\nBayes' theorem says that to find the conditional probability distribution of \"p\" given the data \"X\", \"i\" = 1, ..., \"n\", one multiplies the \"prior\" (i.e., marginal) probability measure assigned to \"p\" by the likelihood function\n\nwhere \"s\" = \"x\" + ... + \"x\" is the number of \"successes\" and \"n\" is of course the number of trials, and then normalizes, to get the \"posterior\" (i.e., conditional on the data) probability distribution of \"p\". (We are using capital \"X\" to denote a random variable and lower-case \"x\" either as the dummy in the definition of a function or as the data actually observed.)\n\nThe prior probability density function that expresses total ignorance of \"p\" except for the certain knowledge that it is neither 1 nor 0 (i.e., that we know that the experiment can in fact succeed or fail) is equal to 1 for 0 < \"p\" < 1 and equal to 0 otherwise. To get the normalizing constant, we find\n\n(see beta function for more on integrals of this form).\n\nThe posterior probability density function is therefore\n\nThis is a beta distribution with expected value\n\nSince the conditional probability for success in the next experiment, given the value of \"p\", is just \"p\", the law of total probability tell us that the probability of success in the next experiment is just the expected value of \"p\". Since all of this is conditional on the observed data \"X\" for \"i\" = 1, ..., \"n\", we have\n\nThe same calculation can be performed with the prior that expresses total ignorance of \"p\", including ignorance with regards to the question whether the experiment can succeed, or can fail. This prior, except for a normalizing constant, is 1/(\"p\"(1 − \"p\")) for 0 ≤ \"p\" ≤ 1 and 0 otherwise. If the calculation above is repeated with this prior, we get\n\nThus, with the prior specifying total ignorance, the probability of success is governed by the observed frequency of success. However, the posterior distribution that led to this result is the Beta(\"s\",\"n\" − \"s\") distribution, which is not proper when \"s\" = \"n\" or \"s\" = 0 (i.e. the normalisation constant is infinite when \"s\" = 0 or \"s\" = \"n\"). This means that we cannot use this form of the posterior distribution to calculate the probability of the next observation succeeding when \"s\" = 0 or \"s\" = \"n\". This puts the information contained in the rule of succession in greater light: it can be thought of as expressing the prior assumption that if sampling was continued indefinitely, we would eventually observe at least one success, and at least one failure in the sample. The prior expressing total ignorance does not assume this knowledge.\n\nTo evaluate the \"complete ignorance\" case when \"s\" = 0 or \"s\" = \"n\" can be dealt with by first going back to the hypergeometric distribution, denoted by formula_13. This is the approach taken in Jaynes(2003). The binomial formula_14 can be derived as a limiting form, where formula_15 in such a way that their ratio formula_16 remains fixed. One can think of formula_17 as the number of successes in the total population, of size formula_18\n\nThe equivalent prior to formula_19 is formula_20, with a domain of formula_21. Working conditional to formula_18 means that estimating formula_23 is equivalent to estimating formula_17, and then dividing this estimate by formula_18. The posterior for formula_17 can be given as:\n\nAnd it can be seen that, if \"s\" = \"n\" or \"s\" = 0, then one of the factorials in the numerator cancels exactly with one in the denominator. Taking the \"s\" = 0 case, we have:\n\nAdding in the normalising constant, which is always finite (because there is no singularities in the range of the posterior, and there are a finite number of terms) gives:\n\nSo the posterior expectation for formula_16 is:\n\nAn approximate analytical expression for large \"N\" is given by first making the approximation to the product term:\n\nand then replacing the summation in the numerator with an integral\n\nThe same procedure is followed for the denominator, but the process is a bit more tricky, as the integral is harder to evaluate\n\nwhere ln is the natural logarithm plugging in these approximations into the expectation gives\n"}
{"id": "38077669", "url": "https://en.wikipedia.org/wiki?curid=38077669", "title": "Samuel Stevens (naturalist)", "text": "Samuel Stevens (naturalist)\n\nSamuel Stevens (11 March 1817 – 29 August 1899) was a natural history agent in London.\n\nStevens was a keen entomological collector for much of his life. He ran a natural history agency from his shop at 24 Bloomsbury Street, London, from 1848 until 1867. Two of his earliest, and best known, clients were Alfred Russel Wallace and Henry Walter Bates, supporting their joint collecting expedition to the Amazon and then Wallace's journeys in the 'Malay Archipelago' by buying their specimens and displaying them to learned societies.\n\nHe is remembered in the specific name of the Jamaican snail, \"Petitia stevensiana\" (now \"Fadyenia stevensiana\"), named by Edward Chitty in 1857 “in compliment to the naturalist’s universal friend, S. Stevens, Esq., Bloomsbury Street, London.”\n"}
{"id": "34494696", "url": "https://en.wikipedia.org/wiki?curid=34494696", "title": "ShelXle", "text": "ShelXle\n\nThe program ShelXle is a graphical user interface for the structure refinement program SHELXL. ShelXle combines an editor with syntax highlighting for the SHELXL-associated .ins (input) and .res (output) files with an interactive graphical display for visualization of a three-dimensional structure including the electron density (Fo) and difference density (Fo-Fc) maps.\n\nShelXle can display electron density maps like the macro molecular program Coot (program) but is more intended for smaller molecules.\nA number of excellent graphical user interfaces (GUIs) exist for small molecule crystal structure refinement with SHELX (e.g., WINGX, Olex2, XSEED, PLATON and SYSTEM-S, and the Bruker programs XP and XSHELL)\n\nShelXle is free software, distributed under the GNU LGPL. It is available from the ShelXle web site or from SourceForge http://sourceforge.net/projects/shelxle/ . Binaries are available for Windows, macOS and the Linux distributions SuSE, Debian and Ubuntu.\nThe Windows binary is distributed with the NSIS Installer.\n\n\nShelXle uses the Qt (framework). It is written entirely in C++ and does not use any Scripting language. For the refinement it calls the external binary of SHELXL which might also be SHELXH, SHELXLMP from George M. Sheldrick or XL from Bruker.\n\nSHELX is developed by George M. Sheldrick since the late 1960s. Important releases are SHELX76 and SHELX97. It is still developed but releases are usually after ten years of testing. \nAcademic users can download the SHELX programs freely after registration.\n\n"}
{"id": "4914941", "url": "https://en.wikipedia.org/wiki?curid=4914941", "title": "Sulfur assimilation", "text": "Sulfur assimilation\n\nSulfur is an essential element for growth and physiological functioning of plants. However, its content strongly varies between plant species and it ranges from 0.1 to 6% of the plants' dry weight.\n\nSulfates taken up by the roots are the major sulfur source for growth, though it has to be reduced to sulfide before it is further metabolized. Root plastids contain all sulfate reduction enzymes, but the reduction of sulfate to sulfide and its subsequent incorporation into cysteine predominantly takes place in the shoot, in the chloroplasts.\n\nCysteine is the precursor or reduced sulfur donor of most other organic sulfur compounds in plants. The predominant proportion of the organic sulfur is present in the protein fraction (up to 70% of total sulfur), as cysteine and methionine (two amino acids) residues.\n\nCysteine and methionine are highly significant in the structure, conformation and function of proteins. Plants contain a large variety of other organic sulfur compounds, as thiols (glutathione), sulfolipids and secondary sulfur compounds (alliins, glucosinolates, phytochelatins), which play an important role in physiology and protection against environmental stress and pests.\n\nSulfur compounds are also of great importance for food quality and for the production of phyto-pharmaceutics. Sulfur deficiency will result in the loss of plant production, fitness and resistance to environmental stress and pests.\n\nSulfate is taken up by the roots that have high affinity. The maximal sulfate uptake rate is generally already reached at sulfate levels of 0.1 mM and lower. The uptake of sulfate by the roots and its transport to the shoot is strictly controlled and it appears to be one of the primary regulatory sites of sulfur assimilation.\n\nSulfate is actively taken up across the plasma membrane of the root cells, subsequently loaded into the xylem vessels and transported to the shoot by the transpiration stream. The uptake and transport of sulfate is energy dependent (driven by a proton gradient generated by ATPases) through a proton/sulfate co-transport. In the shoot the sulfate is unloaded and transported to the chloroplasts where it is reduced. The remaining sulfate in plant tissue is predominantly present in the vacuole, since the concentration of sulfate in the cytoplasm is kept rather constant.\n\nDistinct sulfate transporter proteins mediate the uptake, transport and subcellular distribution of sulfate. According to their cellular and subcellular gene expression, and possible functioning the sulfate transporters gene family has been classified in up to 5 different groups. Some groups are expressed exclusively in the roots or shoots or expressed both in the roots and shoots.\n\n\nRegulation and expression of the majority of sulfate transporters are controlled by the sulfur nutritional status of the plants. Upon sulfate deprivation, the rapid decrease in root sulfate is regularly accompanied by a strongly enhanced expression of most sulfate transporter genes (up to 100-fold), accompanied by a substantially enhanced sulfate uptake capacity. The nature of these transporters is not yet fully solved, whether sulfate itself or metabolic products of the sulfur assimilation (O-acetylserine, cysteine, glutathione) act as signals in the regulation of sulfate uptake by the root and its transport to the shoot, and in the expression of the sulfate transporters involved.\n\nEven though root plastids contain all sulfate reduction enzymes, sulfate reduction predominantly takes place in the leaf chloroplasts. The reduction of sulfate to sulfide occurs in three steps. Sulfate needs to be activated to adenosine 5'-phosphosulfate (APS) prior to its reduction to sulfite.\n\nThe activation of sulfate is catalyzed by ATP sulfurylase, which affinity for sulfate is rather low (Km approximately 1 mM) and the in situ sulfate concentration in the chloroplast is most likely one of the limiting/regulatory steps in sulfur reduction. Subsequently, APS is reduced to sulfite, catalyzed by APS reductase with likely glutathione as reductant.\n\nThe latter reaction is assumed to be one of the primary regulation points in the sulfate reduction, since the activity of APS reductase is the lowest of the enzymes of the sulfate reduction pathway and it has a fast turnover rate. Sulfite is with high affinity reduced by sulfite reductase to sulfide with ferredoxin as a reductant. The remaining sulfate in plant tissue is transferred into the vacuole. The remobilization and redistribution of the vacuolar sulfate reserves appear to be rather slow and sulfur-deficient plants may still contain detectable levels of sulfate.\n\nSulfide is incorporated into cysteine, catalyzed by O-acetylserine (thiol)lyase, with O-acetylserine as substrate. The synthesis of O-acetylserine is catalyzed by serine acetyltransferase and together with O-acetylserine (thiol)lyase it is associated as enzyme complex named cysteine synthase.\n\nThe formation of cysteine is the direct coupling step between sulfur (sulfur metabolism) and nitrogen assimilation in plants. This differs from the process in yeast, where sulfide must be incorporated first in homocysteine then converted in two steps to cysteine.\n\nCysteine is sulfur donor for the synthesis of methionine, the major other sulfur-containing amino acid present in plants. This happens through the transsulfuration pathway and the methylation of homocysteine.\n\nBoth cysteine and methionine are sulfur-containing amino acids and are of great significance in the structure, conformation and function of proteins and enzymes, but high levels of these amino acids may also be present in seed storage proteins. The thiol groups of the cysteine residues in proteins can be oxidized resulting in disulfide bridges with other cysteine side chains (and form cystine) and/or linkage of polypeptides.\n\nDisulfide bridges (disulfide bonds) make an important contribution to the structure of proteins. The thiol groups are also of great importance in substrate binding of enzymes, in metal-sulfur clusters in proteins (e.g. ferredoxins) and in regulatory proteins (e.g. thioredoxins).\n\nGlutathione or its homologues, e.g. homoglutathione in Fabaceae; hydroxymethylglutathione in Poaceae are the major water-soluble non-protein thiol compounds present in plant tissue and account for 1-2% of the total sulfur. The content of glutathione in plant tissue ranges from 0.1 - 3 mM. Cysteine is the direct precursor for the synthesis of glutathione (and its homologues). First, γ-glutamylcysteine is synthesized from cysteine and glutamate catalyzed by gamma-glutamylcysteine synthetase. Second, glutathione is synthesized from γ-glutamylcysteine and glycine (in glutathione homologues, β-alanine or serine) catalyzed by glutathione synthetase. Both steps of the synthesis of glutathione are ATP dependent reactions. Glutathione is maintained in the reduced form by an NADPH-dependent glutathione reductase and the ratio of reduced glutathione (GSH) to oxidized glutathione (GSSG) generally exceeds a value of 7.\nGlutathione fulfils various roles in plant functioning. In sulfur metabolism it functions as reductant in the reduction of APS to sulfite. It is also the major transport form of reduced sulfur in plants. Roots likely largely depend for their reduced sulfur supply on shoot/root transfer of glutathione via the phloem, since the reduction of sulfur occurs predominantly in the chloroplast. Glutathione is directly involved in the reduction and assimilation of selenite into selenocysteine. Furthermore, glutathione is of great significance in the protection of plants against oxidative and environmental stress and it depresses/scavenges the formation of toxic reactive oxygen species, e.g. superoxide, hydrogen peroxide and lipid hydroperoxides. Glutathione functions as reductant in the enzymatic detoxification of reactive oxygen species in the glutathione-ascorbate cycle and as thiol buffer in the protection of proteins via direct reaction with reactive oxygen species or by the formation of mixed disulfides. The potential of glutathione as protectant is related to the pool size of glutathione, its redox state (GSH/GSSG ratio) and the activity of glutathione reductase. Glutathione is the precursor for the synthesis of phytochelatins, which are synthesized enzymatically by a constitutive phytochelatin synthase. The number of γ-glutamyl-cysteine residues in the phytochelatins may range from 2 - 5, sometimes up to 11. Despite the fact that the phytochelatins form complexes which a few heavy metals, viz. cadmium, it is assumed that these compounds play a role in heavy metal homeostasis and detoxification by buffering of the cytoplasmatic concentration of essential heavy metals. Glutathione is also involved in the detoxification of xenobiotics, compounds without direct nutritional value or significance in metabolism, which at too high levels may negatively affect plant functioning. Xenobiotics may be detoxified in conjugation reactions with glutathione catalyzed by glutathione S-transferase, which activity is constitutive; different xenobiotics may induce distinct isoforms of the enzyme. Glutathione S-transferases have great significance in herbicide detoxification and tolerance in agriculture and their induction by herbicide antidotes ('safeners') is the decisive step for the induction of herbicide tolerance in many crop plants. Under natural conditions glutathione S-transferases are assumed to have significance in the detoxification of lipid hydroperoxides, in the conjugation of endogenous metabolites, hormones and DNA degradation products, and in the transport of flavonoids.\n\nSulfolipids are sulfur containing lipids. Sulfoquinovosyl diacylglycerols are the predominant sulfolipids present in plants. In leaves its content comprises up to 3 - 6% of the total sulfur present. This sulfolipid is present in plastid membranes and likely is involved in chloroplast functioning. The route of biosynthesis and physiological function of sulfoquinovosyl diacylglycerol is still under investigation. From recent studies it is evident that sulfite it the likely sulfur precursor for the formation of the sulfoquinovose group of this lipid.\n\nBrassica species contain glucosinolates, which are sulfur-containing secondary compounds. Glucosinolates are composed of a β-thioglucose moiety, a sulfonated oxime and a side chain. The synthesis of glucosinolates starts with the oxidation of the parent amino acid to an aldoxime, followed by the addition of a thiol group (through conjugation with glutathione) to produce thiohydroximate. The transfer of a glucose and a sulfate moiety completes the formation of the glucosinolates.\n\nThe physiological significance of glucosinolates is still ambiguous, though they are considered to function as sink compounds in situations of sulfur excess. Upon tissue disruption glucosinolates are enzymatically degraded by myrosinase and may yield a variety of biologically active products such as isothiocyanates, thiocyanates, nitriles and oxazolidine-2-thiones. The glucosinolate-myrosinase system is assumed to play a role in plant-herbivore and plant-pathogen interactions.\n\nFurthermore, glucosinolates are responsible for the flavor properties of Brassicaceae and recently \nhave received attention in view of their potential anti-carcinogenic properties.\nAllium species contain γ-glutamylpeptides and alliins (S-alk(en)yl cysteine sulfoxides). The content of these sulfur-containing secondary compounds strongly depends on stage of development of the plant, temperature, water availability and the level of nitrogen and sulfur nutrition. In onion bulbs their content may account for up to 80% of the organic sulfur fraction. Less is known about the content of secondary sulfur compounds in the seedling stage of the plant.\n\nIt is assumed that alliins are predominantly synthesized in the leaves, from where they are subsequently transferred to the attached bulb scale. The biosynthetic pathways of synthesis of γ-glutamylpeptides and alliins are still ambiguous. γ-Glutamylpeptides can be formed from cysteine (via γ-glutamylcysteine or glutathione) and can be metabolized into the corresponding alliins via oxidation and subsequent hydrolyzation by γ-glutamyl transpeptidases.\n\nHowever, other possible routes of the synthesis of γ-glutamylpeptides and alliins may not be excluded. Alliins and γ-glutamylpeptides are known to have therapeutic utility and might have potential value as phytopharmaceutics. The alliins and their breakdown products (e.g. allicin) are the flavor precursors for the odor and taste of species. Flavor is only released when plant cells are disrupted and the enzyme alliinase from the vacuole is able to degrade the alliins, yielding a wide variety of volatile and non-volatile sulfur-containing compounds. The physiological function of γ-glutamylpeptides and alliins is rather unclear.\n\nThe rapid economic growth, industrialization and urbanization are associated with a strong increase in energy demand and emissions of air pollutants including sulfur dioxide (see also acid rain) and hydrogen sulfide, which may affect plant metabolism. Sulfur gases are potentially phytotoxic, however, they may also be metabolized and used as sulfur source and even be beneficial if the sulfur fertilization of the roots is not sufficient.\n\nPlant shoots form a sink for atmospheric sulfur gases, which can directly be taken up by the foliage (dry deposition). The foliar uptake of sulfur dioxide is generally directly dependent on the degree of opening of the stomates, since the internal resistance to this gas is low. Sulfur is highly soluble in the apoplastic water of the mesophyll, where it dissociates under formation of bisulfite and sulfite.\n\nSulfite may directly enter the sulfur reduction pathway and be reduced to sulfide, incorporated into cysteine, and subsequently into other sulfur compounds. Sulfite may also be oxidized to sulfate, extra- and intracellularly by peroxidases or non-enzymatically catalyzed by metal ions or superoxide radicals and subsequently reduced and assimilated again. Excessive sulfate is transferred into the vacuole; enhanced foliar sulfate levels are characteristic for exposed plants.\nThe foliar uptake of hydrogen sulfide appears to be directly dependent on the rate of its metabolism into cysteine and subsequently into other sulfur compounds. There is strong evidence that O-acetyl-serine (thiol)lyase is directly responsible for the active fixation of atmospheric hydrogen sulfide by plants.\n\nPlants are able to transfer from sulfate to foliar absorbed atmospheric sulfur as sulfur source and levels of 60 ppb or higher appear to be sufficient to cover the sulfur requirement of plants. There is an interaction between atmospheric and pedospheric sulfur utilization. For instance, hydrogen sulfide exposure may result in a decreased activity of APS reductase and a depressed sulfate uptake.\n\n\n"}
{"id": "25040181", "url": "https://en.wikipedia.org/wiki?curid=25040181", "title": "Suzanne Briet", "text": "Suzanne Briet\n\nRenée-Marie-Hélène-Suzanne Briet (; ; 1 February 1894 in Paris, France - 1989 in Boulogne, France), known as \"Madame Documentation,\" was a librarian, author, historian, poet, and visionary best known for her treatise \"Qu'est-ce que la documentation?\" (\"What is Documentation?\"), a foundational text in the modern study of information science. She is also known for her writings on the history of Ardennes and the poet Arthur Rimbaud.\n\nHer treatise \"Qu'est-ce que la documentation?\" offers a vision of documentation that moves beyond Paul Otlet's emphasis on fixed forms of documents, such as the book, toward \"an unlimited horizon of physical forms and aesthetic formats for documents and an unlimited horizon of techniques and technologies (and of 'documentary agencies' employing these) in the service of multitudes of particular cultures.\" Like many early European Documentalists, Briet embraced modernity and science. However, her work made a difference to modernism and science through the influence of French post-structuralist theorists and her strong orientation toward humanistic scholarship. She subsequently ushered in a second generation of European Documentation and introduced humanistic methods and concerns, especially semiotics and cultural studies, to information science.\n\nAlthough Briet had been highly regarded throughout much of her career—the Grand Cross of the Légion d'honneur was conferred on her in 1950 — she was largely forgotten in her later life, until her death in 1989, when scholars found a renewed interest in her ideas. Today scholars often credit Briet as a visionary, having laid the foundation for contemporary frameworks and methodologies in information science roughly 50 years earlier. \"Her modernist perspective,\" writes Michael Buckland, \"combined with semiotics, deserves attention now because it is different from, and offers an alternative to, the scientific, positivist view that has so dominated information science and which is increasingly questioned.\"\n\nSuzanne Briet was born in Paris, France on 1 February 1894, coming of age at a time of great social change and economic loss in France after World War I. Although Briet grew up in Paris, she remained attached to her birthplace and ancestral home. Briet was only 20 at the outbreak of the war. Ardennes was the pathway of German armies invading France, and during the hostilities her uncle was deported, his village was invaded, and her grandfather's house was destroyed. Historians have suggested that perhaps because of her experiences during the war, as well as her travels to England as a child, Briet took an early interest in the League of Nations, sitting in on some of the sessions held in Paris, and the founding of other international organizations. Briet's mother expected her to become a teacher. Briet's family sent her to Ecole de Sèvres, an elite women's school for training secondary school teachers, where she earned a degree in history and qualified to teach English and history. After teaching in Algeria from 1917 to 1920, Briet pursued a career in librarianship. She studied with Louis Barrau-Dihigo at the Sorbonne, who was apparently so taken with Briet's talents that \"when Briet explained that she could only participate on Saturday, he changed the time of the course to accommodate her.\" At age 30 in 1924, Briet was one of the first of three women appointed as professional librarians at the Bibliothèque Nationale.\n\nBriet's career at the Bibliothèque Nationale witnessed major changes in the library profession. Briet played a central role in the \"modern library\" movement, which eschewed elitist traditions that had dominated many libraries in favor of \"modern\" ideas of librarianship. In addition to technological innovations, Briet saw the emergence of documentation as a distinct profession with its own techniques, standards and training. Women also entered the professional classes in France in increasing numbers during World War II, growing from only 10% of the library profession in 1927 to 50% by the end of the war. Her main achievements during these years were symbolic of her interest in service and moderninzation. Between 1934 and 1954, Briet created and supervised the Salle des Catalogues et Bibliographies, making available materials throughout France that had been previously restricted to most patrons. By 1931 she co-founded (with chemist Jean Gérard) the Union Française des Organismes de Documentation, the French analogue of the American Documentation Institution, today known as the American Society for Information Science and Technology.\n\nDuring World War II, Briet witnessed many of her colleagues deported during the German occupation of Paris and others arrested as communists. Despite a climate of fear, censorship, oppression and physical hardship, Briet made sure to carry out the catalog and bibliographic services of the Bibliothèque Nationale. During the war, Briet continued her interest in Documentation, even attending a conference in Salzburg, Germany, which was organized by German Documentalists. At the end of the war, Briet took on a larger role in a growing international documentation movement. In 1950, she prepared an international survey of education for librarians and documentalists commissioned by UNESCO and was awarded the Légion d'honneur. In 1951 Briet helped establish the Institut National de Techniques de la Documentation at the Conservatoire National des Arts et Métiers. She was the founding Director of Studies and eventually the Vice-President of the International Federation for Documentation.\n\nThat same year, Briet published her treatise \"Qu'est-ce que la documentation?\", in which she outlines, in 48 pages, her philosophy of documentation, \"pushing boundaries of the field beyond texts to include any material form of evidence. ('Is a living animal a document?' she asked.)\" Her thirty-year career at the Bibliothèque Nationale often put Briet in close contact with major French thinkers of the day, including scientists, historians, linguists, and philosophers, which had a significant impact on her philosophy. Briet retired from the Bibliothèque Nationale in 1954 at age 60. She wrote her last essays on documentation in 1955. She spent her retirement concentrating on other interests, including the history of Ardennes and the poet Arthur Rimbaud. Her memoirs were published in 1979. She died in Boulogne in 1989.\n\nBriet published roughly 100 essays, books, and reports on documentation, library science, and history She took up many of the pressing issues of documentation in her day: internationalization, institutionalization, information or documentary overload, scholarly communication, science and technology studies, world peace, and international development. Briet had been deeply engaged in the documentation movement from the 1920s onward, bringing to it a deep understanding of culture and the humanities. Expanding on the techniques and technological ideas of earlier European Documentalists, such as Paul Otlet and Henri La Fontaine, \"Briet understood that technology and culture were deeply connected. She saw society and, therefore, culture, as being re-shaped by technology. The techniques of documentation in aiding and shaping intellectual work were, in her view, both a symptom of, and contributing force within the 'industrialization' of knowledge workers. We can now see, in the impact of computers and telecommunications, how right she was.\" As one scholar notes, \"Not again -- until Actor-network theory at the end of the twentieth century -- would a social network account of technical production, and specifically, documentary production, be articulated.\"\n\nBriet's body of work points to the necessity to understand cultural categories, historical lineages, and the social forces that create and sustain information, urging scholars and information professionals to explore beyond the boundaries of their cultural specialization. Paul Otlet problematized the definition of document when he opened the possibility of any object, independent of the human intention behind its creation, to be a considered a document. Briet solves this problem with her argument that a document must be defined by its intentional use as such. She gives the example of an antelope -- running wild in the plains, it is not a document, but captured and placed in a zoo, it becomes an object of study and thus a document. It operates, as Briet says it must, as “evidence in support of a fact”. An object’s treatment as evidence is contingent not only on its own properties but on its very framing as a source of information. It must be organized into a meaningful relationship with other evidence in order to have the indexical power of evidence.\n\nAccording to one scholar, \"One of Briet's most important insights was that individual documents may be interpreted in different ways by different people wishing to put them to different uses for different purposes. This variability of interpretation is characteristic of documents even at the level of individual words, and the different decisions made by different translators at the word level can have significant consequences.\" Briet developed the notion of \"indice\" (literally \"index\") as not only pointing to an object but also reflective of the networks in which that object appears as a named thing, leading to a semiotics-inspired definition of \"document.\" Furthermore, Briet argued that techniques and technologies are expressions of a networked culture. One scholar explains the idea this way: \"Information and communication technologies may introduce a 'new rhythm' to society and culture, but they themselves are a \"symptom\" of Western social development.\" Thus, technique and technology are historically specific and symptomatic of culture. Briet argues that documentation must respond by incorporating these symptoms and specificities of Western culture but also must incorporate \"Western modernity's opposite trend toward global expansion.\" Briet also saw knowledge as embedded and emergent in cultural and social production and saw modernity as the growth of networks of knowledge. Indeed, in an increasingly globalized world, Briet argued for documentation to take up the call of material necessity rather than be put into the service of culture or any one culture; that is, \"documentation marks the importance of particular, more 'localized' or specialized cultures in terms of their material needs, their specialized vocabularies, and the techniques and technologies needed to provide documentary services to these groups.\"\n\n\n\n"}
{"id": "2150134", "url": "https://en.wikipedia.org/wiki?curid=2150134", "title": "Teknisk Ukeblad", "text": "Teknisk Ukeblad\n\nTeknisk Ukeblad (TU, ) is a Norwegian engineering magazine.\n\n\"TU\" has appeared weekly since 13 April 1883 and was published by Ingeniørforlaget, now Teknisk Ukeblad Media jointly owned by three national professional associations of engineers and architects: the Norwegian Society of Engineers and Technologists (NITO, founded 1936), Tekna (founded in 1874), and the Norwegian Polytechnic Society (PF, founded 1852).\n\nOn 24 June 2010 \"TU\" had a total circulation of 302,000 weekly copies.\n\nCorresponding publications are \"Ny Teknik\" in Sweden, \"Ingeniøren\" in Denmark and \"Technisch Weekblad\" in the Netherlands.\n\n"}
{"id": "49288659", "url": "https://en.wikipedia.org/wiki?curid=49288659", "title": "The Archimedeans", "text": "The Archimedeans\n\nThe Archimedeans are the mathematical society of the University of Cambridge, founded in 1935. It currently has over 2000 active members, many of them alumni, making it one of the largest student societies in Cambridge. The society hosts regular talks at the Centre for Mathematical Sciences, including in the past by many well-known speakers in the field of mathematics. It publishes two magazines, \"Eureka\" and \"QARCH\".\n\nOne of several aims of the society, as laid down in its constitution, is to encourage co-operation between the existing mathematical societies of individual Cambridge colleges, which at present are just the Adam's society of St John's College and the Trinity Mathematical Society, but in the past have included many more.\n\nThe society is mentioned in G. H. Hardy's essay A Mathematician's Apology.\n\nThe main focus of the society's activities are the regular talks, which generally concern topics from mathematics or theoretical physics, and are accessible to students on an undergraduate level. Among the list of recent speakers are Fields medalists Michael Atiyah, Wendelin Werner and Alain Connes, as well as authors Ian Stewart and Simon Singh. Many of the speakers are international, and are hosted by The Archimedeans during their visit.\n\nAfter exams and University-wide project deadlines, the society is also known to organise social events, which have shown to be highly popular.\n\n\"Eureka\" is a mathematical journal that is published annually by The Archimedeans. It includes articles on a variety of topics in mathematics, written by students and academics from all over the world, as well as a short summary of the activities of the society, problem sets, puzzles, artwork and book reviews. The magazine has been published 60 times since 1939, and authors include many famous mathematicians and scientists such as Paul Erdős, Martin Gardner, Douglas Hofstadter, Godfrey Hardy, Béla Bollobás, John Conway, Stephen Hawking, Roger Penrose, Ian Stewart, Fields Medallist Timothy Gowers and Nobel laureate Paul Dirac.\n\nThe journal is distributed free of charge to all current members of the Archimedeans. In addition, there are many subscriptions by other students, alumni and libraries. Subscriptions to \"Eureka\" are the society's main source of income.\n\nThe Archimedeans also publish \"QARCH\", a magazine containing problem sets and solutions or partial solutions submitted by readers. It is published on an irregular basis and distributed free of charge.\n"}
{"id": "11166419", "url": "https://en.wikipedia.org/wiki?curid=11166419", "title": "The Future's So Bright, I Gotta Wear Shades", "text": "The Future's So Bright, I Gotta Wear Shades\n\n\"The Future's So Bright, I Gotta Wear Shades\" is a song by Timbuk3. It is the opening track from their debut album, \"Greetings from Timbuk3\". Released as the album's first single in 1986, it was the band's only significant mainstream hit.\n\nThe inspiration for the song, and the title specifically, came when Barbara MacDonald said to her husband singer/songwriter Pat MacDonald, \"The future is looking so bright, we'll have to wear sunglasses!\" But, while Barbara had made the comment in earnest – it was the early '80s, the two had met and married and were starting a family, their first EP was coming, their book was filling up with gigs – Pat heard the comment as an ironic quip and wrote down instead, \"The future's so bright, I gotta wear shades.\"\n\nFrom there, the lyrics to the song were born, but not the song as it ended up in the minds of popular culture. While Pat wrote a song of a young nuclear scientist and his rich future, listening audiences heard a graduation theme song.\n\nPat revealed on VH1's \"100 Greatest One-Hit Wonders of the 80s\" that the meaning of the song was widely misinterpreted as a positive perspective in regard to the near future. Pat somewhat clarified the meaning by stating that it was, contrary to popular belief, a \"grim\" outlook. While not saying so directly, he hinted at the idea that the bright future was in fact due to impending nuclear holocaust. The \"job waiting\" after graduation signified the demand for nuclear scientists to facilitate such events. Pat drew upon the multitude of past predictions which transcend several cultures that foreshadow the world ending in the 1980s, along with the nuclear tension at the height of the Cold War to compile the song.\n\nTwo verses were written more explicitly portraying the ironic intent of the song. One went:\n\nThe other referred to a supporter of Ronald Reagan as \"a flaming fascist\". However, they were omitted from the final recording because MacDonald felt they were too heavy-handed and obvious. When they performed the song on \"The Joan Rivers Show\" in 1989, the third verse they sang was similar to the former omitted verse.\n\nSimilarly, the group's EP \"Looks Like Dark to Me\" contains a slower version of the song with an additional verse, making clear the dark nature of the song's intent:\n\nThat same EP's title track also refers back to this song:\n\nThe song was the group's only major hit, reaching number 19 on the \"Billboard\" Hot 100 and number 14 on the \"Billboard\" Album Rock Tracks chart. Additionally, the song reached number 21 on the UK Singles Chart.\n\nThe former members of Timbuk 3 have refused to license the song for commercials, including a $900,000 offer from AT&T and offers from Ford, the U.S. Army, and Bausch & Lomb for their Ray-Ban sunglasses.\n\n"}
{"id": "7004633", "url": "https://en.wikipedia.org/wiki?curid=7004633", "title": "The Intelligent Man's Guide to Science", "text": "The Intelligent Man's Guide to Science\n\nThe Intelligent Man's Guide to Science is a general guide to the sciences written by Isaac Asimov. It was first published in 1960 by Basic Books. Revised versions were published as \"The New Intelligent Man's Guide to Science\" (1965), \"Asimov's Guide to Science\" (1972), and \"Asimov's New Guide to Science\" (1984). The book received positive reviews, praising it as a well-written work on science.\n\nAsimov was first contacted by Leon Svirsky of Basic Books in 1959 about the possibility of writing a book that would provide an overview of science, and the two met at Asimov's home on 13 May to discuss the details. Six days later, Asimov received a contract for the book, along with a $1500 advance. At this point in his life, it had been just over a year since Asimov had given up his teaching duties at Boston University and taken up writing full-time. He had published 11 nonfiction books, including books on chemistry, physics, astronomy, a college-level biochemistry textbook, and a collection of science essays. However, he was momentarily daunted by the prospect of writing a major book on all of science, and he delayed signing the contract until 15 July, after receiving encouragement from his friend (and future wife) Janet Jeppson.\n\nThe book's title was Svirsky's, chosen as a deliberate homage to George Bernard Shaw's \"The Intelligent Woman's Guide to Socialism and Capitalism\" (1928). Asimov feared the title would be seen as elitist and condescending, and he suggested \"Everyone's Guide to Science\" as an alternative, but Svirsky refused. Years later, when he was confronted by annoyed feminists who asked why the book was restricted to men, Asimov would claim that the \"intelligent man\" of the title referred to himself; thus anticipating the title \"Asimov's Guide to Science\" adopted for the third edition. Svirsky also wanted the book confined to scientific advances made in the 20th century. Asimov, however, preferred to approach each field in a historical manner, starting with the ancient Greeks or, at the very least, Galileo Galilei. As often happened when Asimov was given editorial directions he disagreed with, he ignored them, and wrote the book just as he wanted to. In organizing the various fields of science, Asimov chose to begin with the universe as a whole and work inward in narrowing circles until he was inside the brain at the end.\n\nAsimov began work on the book on 2 October, and found that he had no trouble with it at all, writing anywhere from 6000 to 10,000 words a day without any sense of strain. By 27 January 1958, Asimov was able to deliver the first half of the completed manuscript to Basic Books, but at a meeting a month later, Svirsky suggested cutting the book in half so it could fit in one volume. At that point, Asimov was only two chapters shy of finishing the book, but saw no reason to complete it if it would be subjected to such radical abridgment, and halted work. He resumed work after being informed on 11 March that Svirsky would not try to reduce the book by half, but would instead publish it in two volumes. Svirsky also insisted that the book include an introduction by the geneticist George Beadle. Asimov felt that his work didn't need an introduction by anyone else, and even though he found Beadle's introduction to be very elegant, he still resented its inclusion. Asimov delivered the final chapters to Basic Books on 21 April, and the appendices on 4 May.\n\nWhen he began proofing the book's galleys, Asimov was horrified to find that Svirsky still cut out some 30% of the book's material. Asimov reinserted as much information into the galley proofs as he could, but he remained unhappy with the book.\n\n\"The Intelligent Man's Guide to Science\" was first published in 1960 by Basic Books. It was published in revised editions as \"The New Intelligent Man's Guide to Science\" in 1965, \"Asimov's Guide to Science\" in 1972, and \"Asimov's New Guide to Science\" in 1984.\n\n\"The Intelligent Man's Guide to Science\" received positive reviews from the physicist Derek J. de Solla Price in \"Science\" and Floyd C. Gale in \"Galaxy Science Fiction\", and a mixed review from John Pfeiffer in \"The New York Times\".\n\nPrice considered Asimov's work a novelty in popular science writing. He credited Asimov with surveying the whole of modern science. Gale credited Asimov with writing well and making difficult concepts easy to understand. Gale considered the book well-written and credited Asimov with helping to make even difficult concepts easy to understand. Pfeiffer wrote that Asimov tried to discuss too many aspects of science in the limited space available to him and compressed material \"to a point where the result is almost a listing of developments with inadequate transitions in between\". He concluded that Asimov had \"prepared a good introduction to modern research\" that \"would have been better if he had allowed himself more space for the unique, imaginative writing of which he is so obviously capable.\"\n\n\"The Intelligent Man's Guide to Science\" was nominated for a National Book Award in the nonfiction category, losing to the journalist William L. Shirer's \"The Rise and Fall of the Third Reich\" (1960). Asimov has stated that \"The Intelligent Man's Guide to Science\" led to his recognition as a major figure in the field of science writing.\n\n\"Asimov's Guide to Science\" was reviewed by John Cheney in \"Contemporary Physics\". \"Asimov's New Guide to Science\" received positive reviews from Paul Stuewe in \"Quill & Quire\", Margrett J. McFadden in \"Voice of Youth Advocates\", and Robert H. Bell in \"Science Books & Films\", and a mixed review from E. L. Williams in \"Choice\". The book was also reviewed by Jim Pirie in \"Chemical Engineer\", and the geneticist H. Bentley Glass in \"The Quarterly Review of Biology\".\n\nStuewe considered the book well-written, and credited Asimov with covering developments in technology since the publication of \"Asimov's Guide to Science\". McFadden considered the book enjoyable to read, and praised Asimov for presenting new information \"from dinosaurs to robots, the solar system to new physics discoveries\". Bell considered the book thorough and engaging, crediting Asimov with \"encyclopedic knowledge of astronomy, geology, physics, and chemistry\" and \"considerable understanding and knowledge of organic chemistry, cellular function and theory, microbiology, the human body and its needs, evolution, and the mind\", and providing useful \"figures, sketches, and maps\". Williams complimented Asimov for his updated treatment of artificial intelligence, computers, cancer, the solar system, quasars, black holes, evolution, and the energy crisis, but considered it disappointing that there was no update on genetic engineering. Williams also commented that, \"There are fewer photographs and their quality is not as good as in the 1972 edition. The table of contents has been divided into very helpful subheadings, making it easy to use as a quick reference. The name and subject indexes are good.\"\n\n\n"}
{"id": "34029697", "url": "https://en.wikipedia.org/wiki?curid=34029697", "title": "The Relaxation Response", "text": "The Relaxation Response\n\nThe Relaxation Response is a book written in 1975 by Herbert Benson, a Harvard physician, and Miriam Z. Klipper. The response is a simple, secular version of Transcendental Meditation (\"TM\"), presented for people in the Western world.\n\nBenson writes in his book, \"We claim no innovation but simply a scientific validation of age-old wisdom\". People from the Transcendental Meditation movement, who felt they could reduce blood pressure using TM, visited Harvard Medical School in 1968, asking to be studied. The school, which at the time was studying the relationship of monkeys' behavior and blood pressure, told them \"No, thank you.\" But when they persisted, Benson told them he would study them. He met with Maharishi Mahesh Yogi first to find out if he could agree in advance to any outcome, which Mahesh did. Benson mentions in his book that independent studies were already underway by then-PhD candidate R. Keith Wallace working with Archie Wilson at the University of California, Los Angeles, but that no published studies of TM existed.\n\nThe Benson-Henry Institute at Massachusetts General Hospital teaches how to elicit the response in nine steps. Benson's website and his book describe four steps. Two of those steps are essential: a mental device (a simple word, phrase or activity to repeat to keep the mind from wandering) and a passive attitude. The goal is to activate the parasympathetic nervous system, which causes humans to relax.\n\nBenson developed the idea of the response, which counters the fight-or-flight response described during the 1920s by Walter Bradford Cannon at the Harvard Medical School. According to Benson more than 60 percent of all visits to healthcare providers are related to stress. It causes the “fight or flight” hormones, epinephrine and norepinephrine, to secrete into the bloodstream. This incites or exacerbates a number of conditions. They include hypertension, headaches, insomnia, irritable bowel syndrome and chronic low back pain, as well as heart disease, stroke and cancer.\n\nA physician with ABC News adds that the immune system works best when relaxed. He said about twenty deep breaths per day, done \"with intention\", can accomplish this.\n\nIn a 1986 US national survey, reported in \"The New York Times\", this best-seller was the number one self-help book that clinical psychologists recommended to their patients.\n\n"}
{"id": "58785", "url": "https://en.wikipedia.org/wiki?curid=58785", "title": "Timeline of gravitational physics and relativity", "text": "Timeline of gravitational physics and relativity\n\nTimeline of gravitational physics and general relativity\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "11259783", "url": "https://en.wikipedia.org/wiki?curid=11259783", "title": "UK households: a longitudinal study", "text": "UK households: a longitudinal study\n\nThe UK Household Longitudinal Study (UKHLS) is now known as Understanding Society. It is the largest panel survey in the world, supporting social and economic research. Its sample size is 40,000 households from the United Kingdom or approximately 100,000 individuals.\n\nData collection, or fieldwork, began in January 2009 and the plan is to follow and interview annually the members of the original households (and their newly formed households, if applicable). The fieldwork period is 24 months, but each person is still interviewed annually, i.e., the fieldwork for consecutive waves overlap.\n\nThe study is mainly funded by the Economic and Social Research Council, and led by the Institute for Social and Economic Research (ISER) at the University of Essex.\n\nAs a panel survey, Understanding Society is a form of longitudinal study which means that the survey consists of information about the same individuals at regular intervals and so can be used to track changes in people's lives and attitudes over time. It can also be used to measure phenomena such as poverty persistence, unemployment duration, duration of marriage or cohabitation and analyze the factors that affect these durations. The study allows for deeper analysis of a wide range of sections of the population as they respond to regional, national and international change. Understanding Society will enhance our insight into the pathways that influence peoples longer term occupational trajectories; their health and well-being, their financial circumstances and personal relationships.\n\nFrom its second wave (starting in 2010) onwards the \"Understanding Society\" incorporates the British Household Panel Survey, which has been carried out at ISER since 1991.\n\nThe \"Understanding Society\" is one of the first longitudinal surveys in UK to put ethnicity at the heart of the survey and include an \"Ethnic Minority Boost Sample\" (approximately 1000 adults from each of the five main ethnic minority groups: Indian, Pakistani, Bangladeshi, Caribbean and African plus members of other minorities as identified in the screening).\n\nThe study will capture biomedical data on 20,000 participants and place this alongside rich social histories, helping us weigh the extent to which people's environment influences their health relative to their genetic make-up.\n\nThis study also consists of a methodological survey called the \"Innovation Panel\" which is conducted in the year prior to the main survey to enable research in key methodological issues such as the quality of new questions, methods to improve response rates, mode effects. This sample consists of 1500 households.\n\nThe study hosts a \"biennial conference\"\n\nEvery wave includes the following:\n\nData for wave 6 will likely be available November 2016.\n\n\n"}
{"id": "3842913", "url": "https://en.wikipedia.org/wiki?curid=3842913", "title": "Vladimir Jurko Glaser", "text": "Vladimir Jurko Glaser\n\nVladimir Jurko Glaser (April 21, 1924 – January 22, 1984) was a Croatian theoretical physicist working on quantum field theory and the canonization of the analytic S-matrix.\n\nGlaser was born in Gorizia, Italy. He graduated physics from the University of Zagreb in 1949 and later was attending seminar of Werner Heisenberg (1951-52) at Göttingen. Based on work carried out in Göttingen under Heisenberg he received a doctorate degree from the University of Zagreb. Being a part of Heisenberg's group at Göttingen he later worked with many famous physicists such as Harry Lehmann, Wolfhart Zimmermann (on extensions of LSZ formalism) and Walter Thirring. From 1955 to 1957 he was head of the Department of Theoretical Physics at the Ruđer Bošković Institute in Zagreb. In 1957 he found a permanent employment at the Department of Theoretical Physics of CERN, Geneva. He died in Geneva.\n\nIn 1955, he published one of the first monographs on quantum electrodynamics, \"Kovarijantna kvantna elektrodinamika\" (in Croatian). With French physicists Jacques Bros and Henri Epstein he worked on setting up analyticity properties required for the use of dispersion relations in high energy collisions. Epstein, Glaser and Arthur Jaffe proved that (Wightman) quantum fields can necessarily have negative energy density values. Together with Henri Epstein, he found a new approach to renormalization theory called causal perturbation theory, where ultraviolet divergences are avoided in the calculation of Feynman diagrams by using mathematically well-defined quantities only.\n"}
