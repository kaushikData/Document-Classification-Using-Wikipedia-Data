{"id": "14346088", "url": "https://en.wikipedia.org/wiki?curid=14346088", "title": "Adduct purification", "text": "Adduct purification\n\nAdduct purification is a technique for preparing extremely pure simple organometallic compounds, which are generally unstable and hard to handle, by purifying a stable adduct with a Lewis acid and then obtaining the desired product from the pure adduct by thermal decomposition.\n\nEpichem Limited is the licensee of the major patents in this field, and uses the trademark EpiPure to refer to adduct-purified materials; Professor Anthony Jones at Liverpool University is the initiator of the field and author of many of the important papers.\n\nThe choice of Lewis acid and of reaction medium is important; the desired organometallics are almost always air- and water-sensitive. Initial work was done in ether, but this led to oxygen impurities, and so more recent work involves tertiary amines or nitrogen-substituted crown ethers. \n\n"}
{"id": "49343464", "url": "https://en.wikipedia.org/wiki?curid=49343464", "title": "Alismatid monocots", "text": "Alismatid monocots\n\nAlismatid monocots (alismatids, \"basal monocots\") is an informal name for a group of early branching (hence basal) monocots, consisting of two orders, the Acorales and Alismatales. The name has also been used to refer to the Alismatales alone. Monocots are frequently treated as three informal groupings based on their branching from ancestral monocots and shared characteristics: alismatid monocots, lilioid monocots (the five other non-commelinid monocots) and commelinid monocots. Research at the Royal Botanical Gardens, Kew is organised into two teams I: Alismatids and Lilioids and II: Commelinids. A similar approach is taken by Judd in his Plant systematics. They also organise their monocot research into two teams I: Alismatids and Lilioids, and II: Commelinids. A similar approach is taken by Judd in his \"Plant systematics\".\n\nCladogram showing the orders of monocots (Lilianae \"sensu\" Chase & Reveal) based on molecular phylogenetic evidence.\n\n"}
{"id": "8370797", "url": "https://en.wikipedia.org/wiki?curid=8370797", "title": "Archaeological Recording Kit", "text": "Archaeological Recording Kit\n\nArchaeological Recording Kit (ARK) is a web-based, open source software package for recording and disseminating archaeological data. ARK is primarily designed for recording excavations, but can also be used for archaeological surveys, palaeoenvironmental research and collections management.\n\nARK is based on the LAMP stack and MapServer, and is free software released under the GNU GPL. It was developed by L-P Archaeology, a British commercial archaeology practice.\n\nThe Fasti Online project was built using an ARK back-end, and demonstrates its usage beyond normal archaeological recording.\n\n"}
{"id": "569111", "url": "https://en.wikipedia.org/wiki?curid=569111", "title": "Basionym", "text": "Basionym\n\nIn the scientific name of organisms, basionym or basyonym means the original name on which a new name is based. The term original combination or protonym is used in the same way in zoology. Bacteriology uses a similar term, basonym, spelled without an \"i\".\n\nThe term \"basionym\" is used in botany only for the circumstances where a previous name exists with a useful description, and the \"International Code of Nomenclature for algae, fungi, and plants\" does not require a full description with the new name. A basionym must therefore be legitimate. Basionyms are regulated by the code's articles 6.10, 7.3, 41, and others.\n\nThe basionym of the name \"Picea abies\" (the Norway spruce) is \"Pinus abies\". The species was originally named \"Pinus abies\" by Carl Linnaeus. Later on, botanist Gustav Karl Wilhelm Hermann Karsten decided this species should not be grouped in the same genus (\"Pinus\") as the pines, so he transferred it to the genus \"Picea\" (the spruces). The new name \"Picea abies\" is \"combinatio nova\", a new combination (abbreviated \"comb. nov.\").\n\nIn 1964, the subfamily name Pomoideae, which had been in use for the group within family Rosaceae that have pome fruit like apples, was no longer acceptable under the code of nomenclature because it is not based on a genus name. Claude Weber did not consider the family name Malaceae to be taxonomically appropriate, so he created the name Maloideae at the rank of subfamily, referring to the original description of the family, and using the same type. This change of rank from family to subfamily is an example of \"status novus\" (abbreviated \"stat. nov.\"), also called a \"name at new rank\".\n\n"}
{"id": "28597971", "url": "https://en.wikipedia.org/wiki?curid=28597971", "title": "Bibliotheca Botanica", "text": "Bibliotheca Botanica\n\nBibliotheca Botanica (\"Bibliography of botany\", Amsterdam, 1736, Salomen Schouten; 2nd edn., 1751) is a botany book by Swedish naturalist Carl Linnaeus (1707–1778). The book was written and published in Amsterdam when Linnaeus was twenty-eight and dedicated to the botanist Johannes Burman (1707–1779). The first edition appeared in 1735 with the full title \"Bibliotheca Botanica recensens libros plus mille de plantis huc usque editos secundum systema auctorum naturale in classes, ordines, genera et species\"; it was an elaborate classification system for his catalogue of books.\n\nThe Preface, dated 8 August 1735, on pages 2–19 contains Linnaeus's extended account of botanical history in the form of a botanical analogy; in pages 2–3 Linnaeus lists previous bibliographers and then gives his account of botanical history leading to a golden age lasting from 1683 to 1703 (see also \"Incrementa Botanices\", Biuur 1753 and \"Reformatio Botanices\", Reftelius, 1762, for other historical notes by Linnaeus). The Preface mentions that \"Bibliotheca Botanica\" was the first part of a planned \"Bibliotheca medica\" (which he did not produce).\n\nA digest of \"Bibliotheca Botanica\", which elaborated on the first chapter of the \"Fundamenta Botanica\", is given in Aphorisms 5–52 of the \"Philosophia Botanica\".\n\nLinnaean authority Frans Stafleu describes the book:\n\nThe term “methodists” (methodici, equivalent to present-day systematists) was coined by Linnaeus in his \"Bibliotheca Botanica\" to denote the authors who care about the principles of classification in contrast to the collectors who are concerned primarily with the description of plants paying little or no attention to their arrangement into genera etc. For Linnaeus the important early Methodists were Italian physician and botanist Andrea Caesalpino, the English naturalist John Ray, German physician and botanist Augustus Quirinus Rivinus, and a French physician, botanist, and traveller Joseph Pitton de Tournefort.\n\nBotanical bibliography effectively began, as did bibliography in general, with the work of the sixteenth-century Swiss natural historian and polymath Conrad Gesner (1516–65). His \"Bibliotheca Universalis\", a general compendium of some 12,000 items in Latin, Greek, or Hebrew arranged by authors’ forenames, appeared in 1545 as an attempt to bring some order into the rapidly increasing range of literature consequent to the Renaissance and the introduction of printing.\n\nThe \"Bibliotheca Botanica\" was the first botanical bibliography arranged by subject. The titles were arranged hierarchically into 16 classes or chapters, each with one or more ordines or sections. Applying this \"methodus naturalis\" to books and people was a mark of his ‘scholastic’ view of the world. Most subsequent classifications of botanical literature, including geographical entities, would be more or less empirically based highlighting a recurrent conflict between essentialism, empiricism, nominalism and other doctrines in the theory and practice of any kind of classification.\n\nHeller notes the incomplete coverage of material, incorrect dating of books, and many minor errors in his book descriptions. Also, that his “natural method” of classifying books was “not very practical”.\n\nFull bibliographic details including exact dates of publication, pagination, editions, facsimiles, brief outline of contents, location of copies, secondary sources, translations, reprints, travelogues, and commentaries are given in Stafleu and Cowan's \"Taxonomic Literature\".\n\n"}
{"id": "55164426", "url": "https://en.wikipedia.org/wiki?curid=55164426", "title": "Chappuis absorption", "text": "Chappuis absorption\n\nChappuis absorption refers to the absorption of electromagnetic radiation by ozone, that is especially noticeable in the ozone layer, which absorbs a small part of sunlight in the visible part of the electromagnetic spectrum. The Chappuis absorption bands occur at wavelengths between 400 and 650 nm. Within this range are two absorption maxima of similar height at 575 and 603 nm wavelengths. Compared to the absorption of ultraviolet light by the ozone layer, known as the Hartley and Huggins absorptions, Chappuis absorption is distinctly weaker. Along with Rayleigh scattering, it contributes to the blue color of the sky, and is noticeable when the light has to travel a long path through the atmosphere. For this reason, Chappuis absorption only has a significant effect on the color of the sky at dusk, during the so-called blue hour. It is named after the French chemist James Chappuis (1854–1934), who discovered this effect.\n\nJames Chappuis was the first researcher (in 1880) to notice that light passing through ozone gas has a blue tint. He attributed this effect to absorption in the yellow, orange, and red parts of the light spectrum. The French chemist Auguste Houzeau had already shown in 1858 that the atmosphere contains traces of ozone, so Chappuis presumed that ozone could explain the blue color of the sky. He was certainly aware that this was not the only possible explanation, since the blue light that can be seen from Earth's surface is polarised. Polarization cannot be explained by light absorption by ozone, but can be explained by Rayleigh scattering, which was already known by Chappuis's time. Contemporary scientists thought that Rayleigh scattering was sufficient to explain the blue sky, and so the idea that ozone could play a role was eventually forgotten.\n\nIn the early 1950s, Edward Hulburt was conducting research on the sky at dusk, to verify theoretical predictions on the temperature and density of the upper atmosphere on the basis of scattered light measured at the Earth's surface. The basic idea was that after the Sun passes under the horizon, it continues to illuminate the upper layers of the atmosphere. Hulburt wished to relate the intensity of light reaching the Earth's surface through Rayleigh scattering to the abundance of particles at each altitude, as the sunlight passes through the atmosphere at different heights over the course of sunset. In his measurements, performed in 1952 at Sacramento Peak in New Mexico, he found that the intensity of measured light was lower by a factor of 2 to 4 than the predicted value. His predictions were based on his theory, and on measurements that were made in the upper atmosphere only a few years before by rocket flights launched not far from Sacramento Peak. The magnitude of the deviation between prediction and photometric measurements made on Sacramento Peak precluded mere measurement error. Until then, theory had predicted that the sky at the zenith during sundown should appear blue-green to grey, and the color should shift to yellow during dusk. This was obviously in conflict with daily observation that the blue color of the sky in the zenith at dusk changes only imperceptibly. As Hulburt knew about the absorption by ozone, and as the spectral range of Chappuis absorption had been more precisely measured only a few years before by the French couple Arlette and Étienne Vassy, he made an attempt to account for this effect in his calculations. This brought the measurements completely into agreement with the theoretical predictions. The results of Hulburt were repeatedly confirmed in the following years. Indeed, not all color effects at dusk in clear sky can be explained by the deeper layers. To this end it is probably necessary to account for spectral extinction by aerosols in theoretical simulations.\n\nIndependently of Hulburt, the French meteorologist Jean Dubois had proposed a few years before that Chappuis absorption had an effect on another color phenomenon of the sky at dusk. Dubois worked on the so-called \"Earth's shadow\" in his doctoral thesis in the 1940s, and he hypothesized that this effect could also be attributed to Chappuis absorption. However, this conjecture is not supported by more recent measurements.\n\nChappuis absorption is a continuum absorption in the wavelength range between 400 and 650 nm. It is caused by the photodissociation (breaking-apart) of the ozone molecule. The absorption maximum lies around 603 nm, with a cross-section of 5.23 10 cm. A second, somewhat smaller maximum at ca. 575 nm has a cross-section of 4.83 10 cm. The absorbance energy in the Chappuis bands lies between 1.8 and 3.1 eV. The measured values imply that absorption mechanism is barely temperature-dependent; the deviation accounts for less than three percent. Around its maxima, Chappuis absorption is about three orders of magnitude weaker than the absorption of ultraviolet light in the range of the Hartley bands. Indeed, the Chappuis absorption is one of the few noteworthy absorption processes within the visible spectrum in Earth's atmosphere.\n\nOverlaid on the absorption spectrum of the Chappuis bands at shorter wavelengths are partly irregular and diffuse bands caused by molecular vibrations. The irregularity of these bands implies that the ozone molecule is only for an extremely short time in an excited state before it dissociates. During this short excitation it is mostly undergoing symmetrical stretching vibrations, although with some contributions from bending vibrations. A consistent theoretical explanation of the vibration structure that is in line with the experimental data was for a long time an unsolved problem; even today, not all details of the Chappuis absorption can be explained by theory.\n\nLike when it absorbs ultraviolet light, the ozone molecule can decompose into an O molecule and an O atom during Chappuis absorption. Unlike the Hartley and Huggins absorptions, however, the decomposition products do not remain in an excited state. Dissociation in the Chappuis bands is the most important photochemical process involving ozone in the Earth's atmosphere below an altitude of 30 km. Over this altitude, it is outweighed by absorptions in the Hartley band. However, neither the Hartley nor the Chappuis absorptions cause significant loss of ozone in the stratosphere, despite the high potential photodissociation rate, because the elemental oxygen has a high probability of encountering an O molecule and recombining back into ozone.\n\n"}
{"id": "25700376", "url": "https://en.wikipedia.org/wiki?curid=25700376", "title": "Corynebacteriophage", "text": "Corynebacteriophage\n\nA corynebacteriophage is a DNA-containing bacteriophage specific for corynebacteria. It introduces toxigenicity into strains of \"Corynebacterium diphtheriae\".\n"}
{"id": "41710616", "url": "https://en.wikipedia.org/wiki?curid=41710616", "title": "Diseases from Space", "text": "Diseases from Space\n\nDiseases from Space is a book published in 1979 that authored by astronomers Fred Hoyle and Chandra Wickramasinghe, where they propose that many of the most common diseases which afflict humanity, such as influenza, the common cold and whooping cough, have their origins in extraterrestrial sources. The two authors argue the case for outer space being the main source for these pathogens- or at least their causative agents. \n\nThe claim connecting terrestrial disease and extraterrestrial pathogens was rejected by the scientific community.\n\nFred Hoyle and Chandra Wickramasinghe spent over 20 years investigating the nature and composition of interstellar dust. Though many hypotheses regarding this dust had been postulated by various astronomers since the middle of the 19th century, all were found to be wanting as and when new data on the gas and dust clouds became available. probably polymers. Chandra Wickramasinghe proposed the existence of polymeric composition based on the molecule formaldehyde (HCO).\n\nIn 1974 Wickramasinghe first proposed the hypothesis that some dust in interstellar space was largely organic (containing carbon and nitrogen), and followed this up with other research confirming the hypothesis. Wickramasinghe also proposed and confirmed the existence of polymeric compounds based on the molecule formaldehyde (HCO). Fred Hoyle and Wickramasinghe later proposed the identification of bicyclic aromatic compounds from an analysis of the ultraviolet extinction absorption at 2175A., thus demonstrating the existence of polycyclic aromatic hydrocarbon molecules in space.\n\nHoyle and Wickramasinghe went further and speculated that the overall spectroscopic data of cosmic dust and gas clouds also matched those for desiccated bacteria. This led them to conclude that diseases such as influenza and the common cold are incident from space and fall upon the Earth in what they term \"pathogenic patches.\" Hoyle and Wickramasinghe viewed the process of evolution in a manner at variance with the standard Darwinian model. They speculated that genetic material in the form of incoming pathogens from the cosmos provided the mechanism for driving the evolutionary engine. Hoyle passed away in 2001, and Wickramasinghe still advocates for these views and beliefs.\n\nThe claim connecting terrestrial disease and extraterrestrial pathogens was rejected and dismissed by the scientific community. On 24 May 2003 \"The Lancet\" journal published a letter from Wickramasinghe, jointly signed by Milton Wainwright and Jayant Narlikar, in which they speculate that the virus that causes severe acute respiratory syndrome (SARS) could be extraterrestrial in origin instead of originating from chickens. \"The Lancet\" subsequently published three responses to this letter, showing that the hypothesis was not evidence-based, and casting doubts on the quality of the experiments referenced by Wickramasinghe in his letter.\n\nFirst published in 1979 by J.M. Dent & Sons Ltd.\nPublished in 1980 by Harper & Row.\nPublished in 1981 by Sphere Books Ltd.\n\n"}
{"id": "11042797", "url": "https://en.wikipedia.org/wiki?curid=11042797", "title": "Dynamic simulation", "text": "Dynamic simulation\n\nDynamic simulation (or dynamic system simulation) is the use of a computer program to model the time varying behavior of a system. The systems are typically described by ordinary differential equations or partial differential equations. As mathematical models incorporate real-world constraints, like gear backlash and rebound from a hard stop, equations become nonlinear. This requires numerical methods to solve the equations. A numerical simulation is done by stepping through a time interval and calculating the integral of the derivatives by approximating the area under the derivative curves. Some methods use a fixed step through the interval, and others use an adaptive step that can shrink or grow automatically to maintain an acceptable error tolerance. Some methods can use different time steps in different parts of the simulation model. Industrial uses of dynamic simulation are many and range from nuclear power, steam turbines, 6 degree of freedom vehicle modeling, electric motors, econometric models, biological systems, robot arms, mass spring dampers, hydraulic systems, and drug dose migration through the human body to name a few. These models can often be run in real time to give a virtual response close to the actual system. This is useful in process control and mechatronic systems for tuning the automatic control systems before they are connected to the real system, or for human training before they control the real system.\nSimulation is also used in computer games and animation and can be accelerated by using a physics engine, the technology used in many powerful computer graphics software programs, like 3ds Max, Maya, Lightwave, and many others to simulate physical characteristics. In computer animation, things like hair, cloth, liquid, fire, and particles can be easily modeled, while the human animator animates simpler objects. Computer-based dynamic animation was first used at a very simple level in the 1989 Pixar short film \"Knick Knack\" to move the fake snow in the snowglobe and pebbles in a fish tank.\n\nThis animation was made with a software system dynamics, with a 3D modeler. The calculated values are associated with parameters of the rod and crank. In this example the crank is driving, we vary both the speed of rotation, its radius and the length of the rod, the piston follows.\n\n\n"}
{"id": "24233854", "url": "https://en.wikipedia.org/wiki?curid=24233854", "title": "Enhydro agate", "text": "Enhydro agate\n\nEnhydro agates are nodules, agates, or geodes with water trapped inside its cavity. Enhydros are closely related to fluid inclusions, but are composed of chalcedony. The formation of enhydros is still an ongoing process, with specimens dated back to the Eocene Epoch. They are commonly found in areas with volcanic rock.\n\nEnhydro agates are made up of banded microcrystalline or cryptocrystalline quartz. The agate has a hollow center, partially containing water. Enhydro agates can also contain debris or petroleum. Because the cavity is not full, the agate can produce sound from being shaken. Agates vary in size. The largest recorded agate was found in Fuxin City, China, with a diameter of 63 cm and weighing 310 kg.\n\nEnhydros are formed when water rich in silica percolates through volcanic rock, forming layers of deposited mineral. As layers build up, the mineral forms a cavity in which the water becomes trapped. The cavity is then layered with the silica-rich water, forming its shell. Unlike fluid inclusions, the chalcedony shell is porous, allowing water to enter and exit the cavity very slowly. The water inside of an enhydro agate is most times not the same water as when the formation occurred. During the formation of an enhydro agate, debris can get trapped in the cavity. Types of debris varies in every agate.\n\n"}
{"id": "4517008", "url": "https://en.wikipedia.org/wiki?curid=4517008", "title": "Fact and Fancy", "text": "Fact and Fancy\n\nFact and Fancy is a collection of seventeen scientific essays by Isaac Asimov. It was the first in a series of books collecting his essays from \"The Magazine of Fantasy and Science Fiction\", and Asimov's second book of science essays altogether (after \"Only a Trillion\"). Doubleday & Company first published it in March 1962. It was also published in paperback by Pyramid Books as part of The Worlds of Science series.\n\nThe only essay that did not appear in \"Fantasy and Science Fiction\" was \"Our Lonely Planet\", which first appeared in \"Astounding Science Fiction\".\n\n\n"}
{"id": "18571658", "url": "https://en.wikipedia.org/wiki?curid=18571658", "title": "Falsterbo Lighthouse", "text": "Falsterbo Lighthouse\n\nFalsterbo Lighthouse () . To the north-east of the lighthouse is the city of Skanör-Falsterbo, to the south-east of the lighthouse are some of the finest sandy beaches in Sweden and surrounding the lighthouse is the golf course of the Falsterbo Golf Club.\n\nThe sea route past the Falsterbo Headland has always been dangerous, because of the moving sand banks hidden under the sea. In 1230 the Dominikans from Lübeck sent a letter to the Danish king Valdemar with a request that a \"mark\" should be built to warn seafarers. There is no evidence that it was ever built. Most likely is that a prominent house at Falsterbo and The Church of Santa Maria were used as seamarks.\n\nIn 1636, a lever light known as a \"swape\" light was built nearby at Kolabacken. An iron basket full of burning coal was hoisted up and down by a balanced bar, hence the light was moving and easier to detect. The coal fire was intensely red and could not be mistaken for a star or ship lantern. The remains of the beacon are still visible as a small hillock of ashes and coal, \"Coal Hill\" (). Towards the end of the 18th century the lever light was moved to the site of the present lighthouse, closer to the new shoreline.\n\nThe lighthouse was built in 1793-96 and the \"light\" was a coal fire at the top. In 1842-43 the uppermost crenellated parts were replaced with the present lantern. Coal was replaced with rapeseed oil. The oil was very inflammable and the lighthouse keepers had to watch the lamp all night. To make a periodic light; a screen was moved around the lantern by heavy weights. Around 1850 a house for the keeper was built next to the lighthouse. At the end of the 19th century another house was built for the assistants to the lighthouse keeper.\n\nAlso when the oil was replaced with paraffin and, later gas, the screen still had to be moved around. When electric light was installed in 1935 the screen was removed and so were most of the staff. Only one lighthouse keeper remained. In 1972 the lighthouse was automated and the last keeper retired.\n\nThe lighthouse is high and broad. Nowadays it has no importance as a navigation mark and therefore the light is not very strong (ca. 4000 candela). It was totally turned off 1990-93. The interval of the light is intermittent: 4 seconds on, 1 second off, repeated.\n\nEven though the lighthouse is managing itself nowadays, there are still lots of activities around it. Falsterbo is one of twenty synoptic weather stations in Sweden still manned. Every three hours weather data (wind, temperature, air pressure, visibility, cloud cover etc.) are reported to the Swedish Meteorological and Hydrological Institute. In earlier days the weather observations were carried out by the lighthouse keepers.\n\nThe lighthouse garden is the ringing site of the Falsterbo Bird Observatory. Falsterbo is a premier site in Europe to watch autumn bird migration. Several millions of birds pass every autumn en route to wintering areas in Africa or southern Europe. Annually, about 25,000 small birds are trapped and ringed.\n\nEvery year on the last Sunday of August it is \"Lighthouse Day\". Then the lighthouse is open to the public. Visitors are shown not only the lighthouse itself but also bird ringing and the weather station.\n\n\n"}
{"id": "4421651", "url": "https://en.wikipedia.org/wiki?curid=4421651", "title": "Guido Pontecorvo", "text": "Guido Pontecorvo\n\nProf Guido Pellegrino Arrigo Pontecorvo FRS FRSE (29 November 1907 – 25 September 1999) was an Italian-born Scottish geneticist.\n\nGuido Pontecorvo was born on 29 November 1907 in Pisa into a family of wealthy Italian industrialists. He was one of eight children. He was a brother to Gillo Pontecorvo and Bruno Pontecorvo.\n\nHe was dismissed from his post in Florence in 1938, due to his Jewish heritage. He then fled to Britain in 1939 with his new wife Leonore Freyenmuth (of German descent).\n\n\nIn 1946 he was elected a Fellow of the Royal Society of Edinburgh. His proposers were Alan William Greenwood, John Walton, Hugh Donald and James E. Nichols. He was elected a Fellow of the Royal Society in 1955.\n\nHe died in Switzerland on 25 September 1999.\n\nThe Institute of Genetics building at the University of Glasgow was re-named as the Pontecorvo Building in 1995 in honour of Guido Pontecorvo and to celebrate 50 years of the institute in Glasgow. The Pontecorvo Building is part of the Anderson College complex located on Dumbarton Road in the West End of Glasgow. The building has been vacant since 2011 awaiting redevelopment. Until then it housed one of the few still operational Paternoster elevators in the UK. \n\nHe has also lent his name to the annual Pontecorvo Award, presented to the final year undergraduate student in the department with the highest grades. \n\nPast Winners:\n\n\nNames contained in brackets represent a second prize given to a student for a separate, notable achievement in their final year.\n"}
{"id": "22735861", "url": "https://en.wikipedia.org/wiki?curid=22735861", "title": "Infectious pancreatic necrosis", "text": "Infectious pancreatic necrosis\n\nInfectious pancreatic necrosis (IPN) is a severe viral disease of salmonid fish. It is caused by infectious pancreatic necrosis virus, which is a member of the Birnaviridae family. This disease mainly affects young salmonids, such as trout or salmon, of less than six months, although adult fish may carry the virus without showing symptoms. Resistance to infection develops more rapidly in warmer water. It is highly contagious and found worldwide, but some regions have managed to eradicate or greatly reduce the incidence of disease. The disease is normally spread horizontally via infected water, but spread also occurs vertically. It is not a zoonosis.\n\nA sharp rise in mortality is often seen (depending on the virulence of the disease). Other clinical signs include abdominal swelling, anorexia, abnormal swimming, darkening of the skin, and trailing of the feces from the vent. On necropsy, internal damage (viral necrosis) to the pancreas and thick mucus in the intestines often is present. Surviving fish should recover within one to two weeks.\n\nDiagnostic methods for the detection of the disease include: characteristic histological pancreatic lesion, PCR, indirect fluorescent antibody testing, ELISA, and virus culture. High virus titers can be isolated from carrier animals.\n\nCurrently, no treatment is available. However in certain territories vaccines are available for prevention of the disease e.g. Winvil 3 Micro \n\nGood husbandry measures, such as high water quality, low stocking density, and no mixing of batches, help to reduce disease incidence. To eradicate the disease, very strict protocol with regards to movement, water sources and stock replacement must be in place – and still it is difficult to achieve and comes at a high economic cost.\n\n"}
{"id": "6355328", "url": "https://en.wikipedia.org/wiki?curid=6355328", "title": "Intervention theory", "text": "Intervention theory\n\nIn social studies and social policy, intervention theory is the analysis of the decision making problems of intervening effectively in a situation in order to secure desired outcomes. Intervention theory addresses the question of when it is desirable not to intervene and when it is appropriate to do so. It also examines the effectiveness of different types of intervention. The term is used across a range of social and medical practices, including health care, child protection and law enforcement. It is also used in business studies.\n\nWithin the theory of nursing, intervention theory is included within a larger scope of practice theories. Burns and Grove point out that it directs the implementation of a specific nursing intervention and provides theoretical explanations of how and why the intervention is effective in addressing a particular patient care problem. These theories are tested through programs of research to validate the effectiveness of the intervention in addressing the problem.\n\nIn \"Intervention Theory and Method\" Chris Argyris argues that in organization development, effective intervention depends on appropriate and useful knowledge that offers a range of clearly defined choices and that the target should be for as many people as possible to be committed to the option chosen and to feel responsibility for it. Overall, interventions should generate a situation in which actors believe that they are working to internal rather than external influences on decisions.\n\n"}
{"id": "2296559", "url": "https://en.wikipedia.org/wiki?curid=2296559", "title": "J. Val Klump", "text": "J. Val Klump\n\nJeffrey Val Klump is an American limnologist. He was the first person to reach the deepest spot in Lake Superior, a depth of 1333 feet (733 feet below sea level), the second lowest point in the United States after Iliamna Lake, on July 30, 1985 while aboard the R/V Seward Johnson with the Johnson Sea Link-II submersible. Klump was also the first person to reach to the deepest point in Lake Michigan as part of the same expedition. He is currently a professor and dean at the School of Freshwater Sciences at the University of Wisconsin–Milwaukee in Milwaukee, Wisconsin.\n"}
{"id": "44400805", "url": "https://en.wikipedia.org/wiki?curid=44400805", "title": "Kosmos 382", "text": "Kosmos 382\n\nKosmos 382 was a Soyuz 7K-L1E modification of a Soyuz 7K-L1 \"Zond\" spacecraft and was successfully test launched into Low Earth Orbit on a Proton rocket designated as (Soyuz 7K-L1E No.2) on December 2, 1970. \n\nThe main purpose of the mission was to test the N1/L3 spacecraft's Block D lunar orbit insertion/descent stage by simulating the lunar orbit insertion burn, the lunar orbit circularization burn and the final lunar descent burn. Over the course of 5 days, the Block D was thus ignited three times to raise the initial ~190 km × ~300 km × 51.6° orbit to a final 2577 km × 5082 km × 55.87° orbit. The Block-D stage was fitted with cameras in the tanks to monitor the fuel and oxidizer behaviour in weightlessness and during acceleration . \n\n\n"}
{"id": "287141", "url": "https://en.wikipedia.org/wiki?curid=287141", "title": "Langdon Winner", "text": "Langdon Winner\n\nLangdon Winner (born August 7, 1944) is Thomas Phelan Chair of Humanities and Social Sciences in the Department of Science and Technology Studies at Rensselaer Polytechnic Institute, Troy, New York.\n\nLangdon Winner was born in San Luis Obispo, California on August 7, 1944. He received his B.A. in 1966, M.A. in 1967 and Ph.D. in 1973, all in political science at the University of California, Berkeley. His primary focus was political theory.\n\nHe has been a professor at Leiden, MIT, University of California, Los Angeles and at the University of California, Santa Cruz. Since 1985 he has been at the Rensselaer Polytechnic Institute; he was a visiting professor at Harvey Mudd College (2000) and Colgate University (2001). In 2010 he was a Fulbright Fellow visiting the Universidad Complutense in Madrid.\n\nWinner lives in upstate New York. He is married to Gail P. Stuart and has three children. His interests include science, technology, American popular culture, and theories of sustainability.\n\nWinner is known for his articles and books on science, technology, and society. He also spent several years as a reporter, rock music critic, and contributing editor for \"Rolling Stone\" magazine.\n\nIn 1980 Winner proposed that technologies embody social relations, i.e. power. To the question he poses \"Do Artifacts Have Politics?\", Winner identifies two ways in which artifacts can have politics. The first, involving technical arrangements and social order, concerns how the invention, design, or arrangement of artifacts or the larger system becomes a mechanism for settling the affairs of a community. This way \"transcends the simple categories of 'intended' and 'unintended' altogether\", representing \"instances in which the very process of technical development is so thoroughly biased in a particular direction that it regularly produces results heralded as wonderful breakthroughs by some social interests and crushing setbacks by others\" (Winner, p. 25-6, 1999). It implies that the process of technological development is critical in determining the politics of an artifact; hence the importance of incorporating all stakeholders in it. (Determining who the stakeholders are and how to incorporate them are other questions entirely.)\n\nThe second way in which artifacts can have politics refers to artifacts that correlate with particular kinds of political relationships, which Winner refers to as inherently political artifacts (Winner, p. 22, 1999). He distinguishes between two types of inherently political artifacts: those that require a particular sociological system and those that are strongly compatible with a particular sociological system (Winner, p. 29, 1999). A further distinction is made between conditions internal to the workings of a given technical system and those that are external to it (Winner, p. 33, 1999). This second way in which artifacts can have politics can be further articulated as consisting of four 'types' of artifacts: those requiring a particular internal sociological system, those compatible with a particular internal sociological system, those requiring a particular external sociological system, and those compatible with a particular external sociological system.\n\nCertain features of Winner's thesis have been criticized by other scholars, including Bernward Joerges. Such criticisms are often narrowly focused upon particular cases in Winner's essays, the height of the bridges built by Robert Moses on the Long Island Parkway, for example, and tend to overlook his general arguments about the interweaving of political institutions and technological devices.\n\nWinner contributed piano and backing vocals to the hoax album The Masked Marauders created by \"Rolling Stone\".\n\nOver the years one focus of Winner's criticism has been the excessive use of technologies in the classroom, both in K-12 schools and higher education. Winner's critique is well explained in his article \"Information Technology and Educational Amnesia,\" and expressed in his satirical lecture, \"The Automatic Professor Machine.\"\n\n\n\n"}
{"id": "312397", "url": "https://en.wikipedia.org/wiki?curid=312397", "title": "Law of total expectation", "text": "Law of total expectation\n\nThe proposition in probability theory known as the law of total expectation, the law of iterated expectations, the tower rule, Adam's law, and the smoothing theorem, among other names, states that if formula_1 is a random variable whose expected value formula_2 is defined, and formula_3 is any random variable on the same probability space, then\n\ni.e., the expected value of the conditional expected value of formula_1 given formula_3 is the same as the expected value of formula_1.\n\nOne special case states that if formula_8 is a finite or countable partition of the sample space, then\n\nSuppose that two factories supply light bulbs to the market. Factory formula_1<nowiki>'</nowiki>s bulbs work for an average of 5000 hours, whereas factory formula_3<nowiki>'</nowiki>s bulbs work for an average of 4000 hours. It is known that factory formula_1 supplies 60% of the total bulbs available. What is the expected length of time that a purchased bulb will work for?\n\nApplying the law of total expectation, we have:\n\nformula_13\n\nwhere\n\nThus each purchased light bulb has an expected lifetime of 4600 hours.\n\nLet the random variables formula_1 and formula_3, defined on the same probability space, assume a finite or countably infinite set of finite values. Assume that formula_25 is defined, i.e. formula_26. If formula_27 is a partition of the probability space formula_28, then\n\nProof.\nIf the series is finite, then we can switch the summations around, and the previous expression will become\n\nIf, on the other hand, the series is infinite, then its convergence cannot be conditional, due to the assumption that formula_32 The series converges absolutely if both formula_33 and formula_34 are finite, and diverges to an infinity when either formula_33 or formula_34 is infinite. In both scenarios, the above summations may be exchanged without affecting the sum.\n\nLet formula_37 be a probability space on which two sub σ-algebras formula_38 are defined. For a random variable formula_39 on such a space, the smoothing law states that if formula_25 is defined, i.e.\nformula_41, then\n\nProof. Since a conditional expectation is a Radon–Nikodym derivative, verifying the following two properties establishes the smoothing law:\n\n\nThe first of these properties holds by definition of the conditional expectation. To prove the second one, note that\n\nso the integral formula_47 is defined (not equal formula_48).\n\nThe second property thus holds since\nformula_49 implies\n\nCorollary. In the special case when formula_51 and formula_52, the smoothing law reduces to\n\nwhere formula_55 is the indicator function of the set formula_56.\n\nIf the partition formula_57 is finite, then, by linearity, the previous expression becomes\n\nand we are done.\n\nIf, however, the partition formula_59 is infinite, then we use the dominated convergence theorem to show that\n\nIndeed, for every formula_61,\n\nSince every element of the set formula_28 falls into a specific partition formula_56, it is straightforward to verify that the sequence formula_65 pointwise-converges to formula_1. By initial assumption, formula_67. Applying the dominated convergence theorem yields the desired.\n\n\n"}
{"id": "58598178", "url": "https://en.wikipedia.org/wiki?curid=58598178", "title": "Letter to Benedetto Castelli", "text": "Letter to Benedetto Castelli\n\nGalileo Galilei's Letter to Benedetto Castelli (1613) was his first statement on the authority of scripture and the Catholic Church in matters of scientific enquiry. In a series of bold and innovative arguments, he undermined the claims for Biblical authority which the opponents of Copernicus used. The letter was the subject of the first complaint about Galileo to the Inquisition in 1615.\n\nIn 1610 Galileo had published \"Sidereus Nuncius\" (\"The Starry Messenger\") which made him famous across Europe. This work prompted many debates as to whether the earth really was the centre of the universe. Galileo usually avoided referring to scripture in his arguments about the universe, while the Aristolelian scholars who opposed Copernicus cited the Bible in support of their views - for example Lodovico delle Colombe in his 1611 work \"Contra il Moto della Terra\" (\"Against the Motion of the Earth\") explicitly challenged anyone defending Copernicus to answer the charge that he was going against what the Bible taught.\n\nThis presented Galileo with a dilemma - if he did not respond, he effectively conceded that the biblical text confirmed the Aristotelian view despite the fact that the Church had no firm position on the Copernican question; on the other hand if he tried to engage in arguments based on scripture, he allowed himself to be drawn into a field where the Church regarded its authority as absolute. Indeed an earlier venture into this form of argument by Galileo had been stopped by the Church. In the manuscript for \"Letters on Sunspots\" (1613), he argued that \"flaws\" in the Sun demonstrated that the heavens were not immutable, as had previously been thought. A paragraph in which Galileo supported this by claiming that the scriptures supported the mutability of the heavens was removed by the Inquisition censors.\n\nOn 14 December 1613 Galileo's friend and former pupil Benedetto Castelli wrote to him to say that at a recent dinner in Pisa with the Grand Duke Cosimo II de' Medici a conversation had taken place in which Cosimo Boscaglia, a professor of philosophy, argued that the motion of the Earth could not be true, as it was contrary to the Bible. Castelli had disagreed with him and maintained, as Galileo held, that the Earth's motion was possible. After the dinner, Castelli had been called back by the Dower Duchess Christina of Tuscany to answer points she raised from scriptural arguments against the motion of the Earth. Castelli had responded and Boscaglia had remained silent. Castelli wished to alert Galileo to this exchange, and advised Galileo that their mutual friend Niccolò Arrighetti would come to Florence and explain matters further. This Arrighetti did.\n\nGalileo felt that it was important for him to set out an argument to show how scripture could not be used as the basis for scientific enquiry. He did so with great speed, replying with a letter to Castelli in less than a week, on 21 December 1613. His Letter to Benedetto Castelli was not published, but was circulated widely in manuscript form. As the debate about its arguments continued, Galileo thought it advisable to review and expand the arguments he had set out. This was the basis of his subsequent Letter to the Grand Duchess Christina, which expanded the eight pages of his letter to Castelli to forty pages.\n\nIn his letter to Benedetto Castelli, Galileo argues that using the Bible as evidence against the Copernican system involves three key errors. Firstly, claiming that the Bible shows the earth to be static and concluding that the earth therefore does not move is arguing from a false premise; whether the earth moves or not is a thing which must be demonstrated (or not) through scientific enquiry. Secondly, the Bible is not even a source of authority on this kind of question, but only on matters of faith - thus if the Bible happens to say something about a natural phenomenon, this is not sufficient for us to say that it is so. Thirdly, he shows by deft argument that it is open to question whether the Bible, as his opponents claimed, even contradicted Copernicus' model of the universe. Indeed, Galileo argues, a key passage in the Bible which was held by his opponents to support the view that the sun moves round the earth supports his own views much better.\n\nIn the Bible Joshua 10:12 is an account of how God commanded the Sun to stand still so that Joshua could defeat his enemies. According to those opposed to Copernicus, this showed clearly that the Sun (and not the earth) moved. Galileo argued that this passage could not be used to support the traditional earth-centred view of the universe at all. If we assume the universe to be as it was described by Claudius Ptolemy, the sun's annual motion was a slow movement towards the East, so if God had commanded it to stop, the daily movement towards the West would no longer have been counteracted and as a result the day would actually have got slightly shorter rather than longer. However if we assume the universe to be as Copernicus described it, the sun is at the centre and its rotation drives the rotation of all the planets. Thus if God had ordered the Sun to stop turning, everything would have stopped and the day would have been longer, just as the Bible described.\n\nTwo aspects of Galileo's letter are particularly worthy of note. First, his boldness in venturing into the field of exegesis, where he was bound to upset many theologians who would not welcome his contributions. Second, his rhetorically brilliant argument was based on a fundamental contradiction; he began by arguing that faith and science were distinct, and that the Bible could not be used as the basis for arguments about science; he then went on to show, according to some novel and clever arguments, that actually the Bible supported his own scientific views.\n\nHis letter argues a position on scriptural authority which is very similar in substance, if not in tone, to that set out set out by the Catholic Church itself centuries later, in Leo XIII's 1893 encyclical, \"Providentissimus Deus\". Emphasising that the Bible makes use of figurative language and is not meant to teach science, this argues:\n\n\"...here is the rule also laid down by St. Augustine, for the theologian: \"Whatever they [i.e. scientists] can really demonstrate to be true of physical nature, we [i.e. theologians] must show to be capable of reconciliation with our Scriptures; and whatever they assert in their treatises which is contrary to these Scriptures of ours, that is to Catholic faith, we must either prove it as well as we can to be entirely false, or at all events we must, without the smallest hesitation, believe it to be so.\" \n\nLikewise, Galileo accepted that the Bible was infallible in matters of doctrine, but he agreed with Cardinal Baronius's observation that it was \"intended to teach us how to go to heaven, not how the heavens go.\" He also pointed out that both St. Augustine and St. Thomas Aquinas had taught that scripture had not been written to teach a system of astronomy, citing St. Augustine’s comment that \"One does not read in the Gospel that the Lord said: I will send you the Paraclete who will teach you about the course of the sun and moon. For He willed to make them Christians, not mathematicians.\"\n\nIn late 1614 or early 1615, Niccolò Lorini obtained a copy of Galileo's letter, some parts of which he and his fellow Dominicans at the convent of San Marco in Florence adjudged to be \"suspect or rash\". He therefore forwarded it to Cardinal Paolo Emilio Sfondrati at the Congregation of the Index, together with a covering letter dated 7 February 1615, calling for the matter to be investigated.\n\nThe Holy Office examined Lorini's copy of Galileo's letter to Castelli on 25 February 1615 at the house of Robert Bellarmine. It was clear to them that Lorini's version was not the complete letter, as it was obvious that an introductory section was missing. (Lorini's version also included the phrase 'Scripture does not refrain from perverting its most important dogmas...' whereas Galileo's original had said 'Scripture accommodates itself to the capacity of uncouth and uneducated people'). For whatever reason, the Holy Office wished to make certain that it had an accurate version before proceeding with its investigation, so Cardinal Garzia Mellini, secretary of the Inquisition, wrote to the archbishop of Pisa, where Castelli taught at the university, and asked him to provide the original letter.\n\nBy now Galileo had apparently realised that some kind of investigation was underway, so he asked Castelli to return the original of the letter to him. He made a copy of it and sent this version to his friend Archbishop Piero Dini in Rome, protesting at the “wickedness and ignorance” of his enemies, and expressing concern that the Inquisition “may be in part deceived by this fraud which is going around under the cloak of zeal and charity”.\n\nThe Holy Office submitted Galileo's letter to an unnamed theological adviser for examination. That adviser's report, undated, concluded that there were three places where Galileo had used language which was offensive, but 'although this document sometimes uses words improperly, it does not deviate from the narrow path of Catholic expression.'\n\nUntil 2018 two versions existed of the Letter to Bedenetto Castelli - the Lorini version held in the Vatican archives and the Dini version. The general scholarly view was that Lorini’s version was not authentic and that Lorini had amended it to show Galileo in the worst possible light, while the Dini version was generally held to be true.\n\nIn August 2018 the original manuscript of the letter was discovered in the archives of the Royal Society. This is the letter Galileo sent to Castelli and later asked Castelli to return to him. It shows the edits Galileo made in his own handwriting as he prepared the Dini version, replacing words and phrases that the Inquisition might object to with words less likely to offend. None of the arguments were changed, but the tone of the letter was altered considerably. \n\nIt is thus certain that the Lorini version was in most respects authentic, while the Dini version was not, as Galileo claimed, a true copy of his original letter.\n\n\n"}
{"id": "9418063", "url": "https://en.wikipedia.org/wiki?curid=9418063", "title": "Lewis and Clark State Historic Site", "text": "Lewis and Clark State Historic Site\n\nThe Lewis and Clark State Historic Site opened in 2002 and is owned and operated by the Illinois Department of Natural Resources Division of Historic Preservation (formerly Illinois Historic Preservation Division). The site, located in Hartford, Illinois, commemorates Camp River Dubois, the camp of the Lewis and Clark Expedition from December 1803 to May 1804. The site is National Trail Site #1 on the Lewis and Clark National Historic Trail and is located directly off the Confluence Bike Trail, part of the Confluence Greenway. The site is at the southern end of the Meeting of the Great Rivers Scenic Route.\n\nThe Lewis and Clark State Historic Site is situated on the dry side of the Chain of Rocks Levee, approximately 1/4 mile from the Illinois shore of the Mississippi River. The site replaced Lewis and Clark State Park, which is no longer in existence.\n\nMain attractions at the site include a interpretive center and an outdoor replica of Camp River Dubois. The interpretive center contains a theater, multiple hands-on exhibits and displays, and a full-scale cutaway keelboat.\n\n"}
{"id": "27734797", "url": "https://en.wikipedia.org/wiki?curid=27734797", "title": "List of Catholic clergy scientists", "text": "List of Catholic clergy scientists\n\nThis is a list of Catholic churchmen throughout history who have made contributions to science. These churchmen-scientists include Nicolaus Copernicus, Gregor Mendel, Georges Lemaître, Albertus Magnus, Roger Bacon, Pierre Gassendi, Roger Joseph Boscovich, Marin Mersenne, Bernard Bolzano, Francesco Maria Grimaldi, Nicole Oresme, Jean Buridan, Robert Grosseteste, Christopher Clavius, Nicolas Steno, Athanasius Kircher, Giovanni Battista Riccioli, William of Ockham, and others listed below. The Catholic Church has also produced many lay scientists and mathematicians.\n\nThe Jesuits in particular have made numerous significant contributions to the development of science. For example, the Jesuits have dedicated significant study to earthquakes, and seismology has been described as \"the Jesuit science.\" The Jesuits have been described as \"the single most important contributor to experimental physics in the seventeenth century.\" According to Jonathan Wright in his book \"God's Soldiers\", by the eighteenth century the Jesuits had \"contributed to the development of pendulum clocks, pantographs, barometers, reflecting telescopes and microscopes, to scientific fields as various as magnetism, optics and electricity. They observed, in some cases before anyone else, the colored bands on Jupiter’s surface, the Andromeda nebula and Saturn’s rings. They theorized about the circulation of the blood (independently of Harvey), the theoretical possibility of flight, the way the moon effected the tides, and the wave-like nature of light.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2577081", "url": "https://en.wikipedia.org/wiki?curid=2577081", "title": "List of Jamaican flags", "text": "List of Jamaican flags\n\nThis is a list of flags used in Jamaica.\n\n"}
{"id": "2678386", "url": "https://en.wikipedia.org/wiki?curid=2678386", "title": "List of compounds with carbon number 15", "text": "List of compounds with carbon number 15\n\nThis is a partial list of molecules that contain 15 carbon atoms.\n\n"}
{"id": "13186780", "url": "https://en.wikipedia.org/wiki?curid=13186780", "title": "List of countries by natural gas proven reserves", "text": "List of countries by natural gas proven reserves\n\nThis is a list of countries by natural gas proven reserves based on The World Factbook (when no citation is given). or other authoritative third-party sources (as cited). Based on data from BP, at the end of 2009, proved gas reserves were dominated by three countries: Iran, Russia, and Qatar, which together held nearly half the world's proven reserves.\n\nThere is some disagreement on which country has the largest proven gas reserves. Sources that consider that Russia has by far the largest proven reserves include the US CIA (47.6 trillion cubic meters), the US Energy Information Administration (49 tcm), and OPEC (48.81 tcm). However, BP credits Russia with only 32.9 tcm, which would place it in second place, slightly behind Iran (33.1 to 33.8 tcm, depending on the source).\n\nDue to constant announcements of shale gas \"recoverable\" reserves, as well as drilling in Central Asia, South America and Africa, deepwater drilling, estimates are undergoing frequent updates, mostly increasing. Since 2000, some countries, notably the US and Canada, have seen large increases in proved gas reserves due to development of shale gas, but shale gas deposits in most countries are yet to be added to reserve calculations.\n\nTOTAL: 205.34 trillion cubic meters\n\n\nComparison of proven natural gas reserves from different sources (billions of cubic meters, as of 31 Dec. 2014/1 Jan. 2015) \n"}
{"id": "58469107", "url": "https://en.wikipedia.org/wiki?curid=58469107", "title": "List of heaviest extant mustelids", "text": "List of heaviest extant mustelids\n"}
{"id": "29955313", "url": "https://en.wikipedia.org/wiki?curid=29955313", "title": "List of presidential trips made by Barack Obama during 2010", "text": "List of presidential trips made by Barack Obama during 2010\n\nThis is a list of presidential trips made by Barack Obama during 2010, the second year of his presidency as the 44th President of the United States. During 2010, Obama traveled to eight different nation states internationally, in addition to many more trips made domestically within the United States.\n\nThis list excludes trips made within Washington, D.C., the U.S. federal capital in which the White House, the official residence and principal workplace of the President, is located. Additionally excluded are trips to Camp David, the country residence of the President, and to the private home of the Obama family in Kenwood, Chicago.\n"}
{"id": "49903741", "url": "https://en.wikipedia.org/wiki?curid=49903741", "title": "List of things named after Pafnuty Chebyshev", "text": "List of things named after Pafnuty Chebyshev\n\n\n\n"}
{"id": "9248684", "url": "https://en.wikipedia.org/wiki?curid=9248684", "title": "Near-field optics", "text": "Near-field optics\n\nNear-field optics is that branch of optics that considers configurations that depend on the passage of light to, from, through, or near an element with subwavelength features, and the coupling of that light to a second element located a subwavelength distance from the first. The barrier of spatial resolution imposed by the very nature of light itself in conventional optical microscopy contributed significantly to the development of near-field optical devices, most notably the near-field scanning optical microscope, or NSOM.\n\nThe limit of optical resolution in a conventional microscope, the so-called diffraction limit, is in the order of half the wavelength of the light used to image. Thus, when imaging at visible wavelengths, the smallest resolvable features are several hundred nanometers in size (although point-like sources, such as quantum dots, can be resolved quite readily). Using near-field optical techniques, researchers currently resolve features in the order of tens of nanometers in size. While other imaging techniques (e.g. atomic force microscopy and electron microscopy) can resolve features of much smaller size, the many advantages of optical microscopy make near-field optics a field of considerable interest.\n\nThe notion of developing a near-field optical device was first conceived by Edward Hutchinson Synge in 1928 but was not realized experimentally until the 1950s when several researchers demonstrated the feasibility of sub-wavelength resolution. Published images of sub-wavelength resolution appeared when Ash and Nichols examined gratings with line spacing less than one millimeter using microwaves of 3 cm wavelength. In 1982 Dieter Pohl at IBM in Zurich, Switzerland, first obtained sub-wavelength resolution at visible wavelengths using near-field optical techniques.\n\nNear and far field, the general principle in electromagnetism\n"}
{"id": "24907827", "url": "https://en.wikipedia.org/wiki?curid=24907827", "title": "Olex2", "text": "Olex2\n\nOlex and Olex2 are versatile software for crystallographic research. Olex used to be a research project developed during PhD to implement topological (as connectivity) analysis of polymeric chemical structures and still is widely used around the world. Olex2 is an open source project with the C++ code portable to Windows, Mac and Linux. Although the projects share the common name they are not related at the source code level.\n\nOlex program is designed for the analysis of extended structural networks. It only runs on Windows systems and source code is available only on request. It allows packing the structure, constructing the topological network and the evaluation of the networks Schläfli and vertex symbols and to produce raster pictures of the model visible on screen. This kind of the topological network analysis is normally done to find relevance of considered structures and possibly to predict physical properties of the investigated material.\n\nOlex2 is a relatively new, open source software with a BSD licence which provides tools from the crystallographic structure solution to the final report preparation. It is still in the stage of active development. Olex2 platform independent GUI is provided by wxWidgets. Olex2 has an extended HTML based interface, enhanced by Pillow and OpenGL graphics. Olex2 provides numerous tools for the structure analysis and publication, including Fourier maps and voids calculation and visualisation, space group determination, calculation of esd's for almost any possible geometrical parameters, CIF translation to HTML and other documents, hydrogen atom placement and many others. Olex2 provides the final picture output as raster images or PostScript, Ortep-like or POV-Ray output. The software is provided as pre-build binaries for Windows, Mac and Linux as well as in the source code form. Several build scripts (SCons, CMake and make) are provided to help with the Olex2 development - but only SCons is supported throughout and used for each release update and any problems have to be addressed to the supporters. Olex2 is now supported by OlexSys Ltd.\n\nAs a GUI Olex2 is built from two components - the Olex2 core, written in C++ and exposing underlying model to the GUI, mostly based on the Python code. This segregation allows extending Olex2 with custom scripts and exploiting its functionality by the user at various levels - miller index operations, file manipulations and many others.\n\nOlex2 provides a set of commercial extensions:\n\nhttp://www.olexsys.org\n"}
{"id": "16597114", "url": "https://en.wikipedia.org/wiki?curid=16597114", "title": "Operation Julin", "text": "Operation Julin\n\nThe United States's Julin nuclear test series was a group of 7 nuclear tests conducted in 1991–1992. These tests followed the \"Operation Sculpin\" series, and were the last before negotiations began for the Comprehensive Test Ban Treaty.\n"}
{"id": "11544358", "url": "https://en.wikipedia.org/wiki?curid=11544358", "title": "Pele (volcano)", "text": "Pele (volcano)\n\nPele is a very active volcano on the surface of Jupiter's moon Io. It is located on Io's trailing hemisphere at A large, tall volcanic plume has been observed at Pele by various spacecraft starting with \"Voyager 1\" in 1979, though it has not been persistent. The discovery of the Pele plume on March 8, 1979 confirmed the existence of active volcanism on Io. The plume is associated with a lava lake at the northern end of the mountain Danube Planum. Pele is also notable for a persistent, large red circle that surrounds the volcano resulting from sulfurous fallout from the volcanic plume.\n\nAs \"Voyager 1\" approached the Jupiter system in March 1979, it acquired numerous images of the planet and its four largest satellites, including Io. One of the most distinctive features of these distant images of Io was a large, elliptical, footprint-shaped ring on the satellite's trailing hemisphere (the side facing away from the direction of motion in a synchronously-rotating satellite like Io). During the encounter itself on March 5, 1979, \"Voyager 1\" acquired high-resolution images of the footprint-shaped region. At the center of bow tie-shaped dark region in the middle of the ring was a depression partially filled with dark material, by in size. This depression, later found to be the source of the Pele volcano, is at the northern base of a rifted mountain later named Danube Planum. With the other dramatic evidence for volcanic activity on the surface of Io from this encounter, researchers hypothesized that Pele was likely a caldera.\n\nOn March 8, 1979, three days after passing Jupiter, \"Voyager 1\" took images of Jupiter's moons to help mission controllers determine the spacecraft's exact location, a process called optical navigation. While processing images of Io to enhance the visibility of background stars, navigation engineer Linda Morabito found a tall cloud along the moon's limb. At first, she suspected the cloud to be a moon behind Io, but no suitably sized body would have been in that location. The feature was determined to be a volcanic plume tall and wide, generated by active volcanism at Pele. Based on the size of the plume observed at Pele, the ring of reddish (or dark as it appeared to Voyager's cameras, which were insensitive to red-wavelengths) material was determined to be a deposit of plume material. Following this discovery, seven other plumes were located in earlier \"Voyager\" images of Io. Thermal emission from Pele detected by the \"Voyager 1\" Infrared Interferometer Spectrometer (IRIS) detected a thermal hotspot at Pele, indicative of cooling lava, further indicating that volcanic activity at the surface was linked to the plumes observed by \"Voyager 1\".\n\nWhen \"Voyager 2\" flew through the Jupiter system in July 1979, its imaging campaign was modified to observe Io's plumes in action and to look for surface changes. Pele's plume, designated Plume 1 at the time as it was the first of Io's volcanic plumes to be discovered, was not seen by \"Voyager 2\" four months later. Surface monitoring observations revealed changes with the red ring surrounding Pele. While it was heart- or hoofprint-shaped during the \"Voyager 1\" encounter, it was now more elliptical with the notch in the southern part of the plume deposit now filled in, possibly due to changes in the distribution of plume sources within the Pele patera.\n\nFollowing the Voyager encounters, the International Astronomical Union officially named the volcano after the Hawaiian volcano goddess, Pele, in 1979.\n\n\"Galileo\" arrived at the Jupiter system in 1995 and, from 1996 to 2001, regularly monitored volcanic activity on Io through observations of Io's thermal emission at near-infrared wavelengths, imaging Io while it was in the Jupiter's shadow in order to look for thermal hotspots at visible and near-infrared wavelengths, and imaging Io during most orbit in order to detect changes in the appearance of diffuse material and lava flows on the surface. Thermal emission from Pele was detected in nearly every occasion Io's trailing hemisphere was imaged while the moon was in the shadow of Jupiter. The volcanic plume at Pele was found to be intermittent or primarily composed of gas with occasional bursts of increased dust content. It was detected only twice by \"Galileo\" in December 1996 and December 2000. In these two detections, the plume height varied from to . The plume was also detected by the Hubble Space Telescope in October 1999 while \"Galileo\" was conducting a flyby of the moon. The Hubble observations allowed for the detection of diatomic sulfur (S) for the first time on Io in Pele's plume. Subtle changes in the shape and intensity of the large red-ring plume deposit surrounding Pele were observed in daylight images of the volcano, with the most notable change seen in September 1997 when dark pyroclastic material from an eruption of Pillan Patera covered up a portion of Pele's plume deposit.\n\nDuring \"Galileo's\" encounters with Io between October 1999 and October 2001, the spacecraft observed Pele on three occasions using its camera and infrared spectrometers while the volcano was on Io's night-side. The cameras revealed a curved line of bright spots along the margin of the Pele patera (a term used for volcanic depressions on Io, akin to calderas). Within the east-west dark band along the southeastern portion of the patera, a large amount of thermal emission was observed, with temperatures and distribution consistent with a large, basaltic lava lake.\n\nThermal emission at Pele was also seen in December 2000 by the \"Cassini\" spacecraft, in December 2001 from the Keck Telescope in Hawaii, and by the \"New Horizons\" spacecraft in February 2007.\n\nPele has a volcanic crater, also known a patera, by in size, which lies at the base of the northern tip of the mountain Danube Planum. The patera has multiple floor levels, with a higher north-eastern section and a lower section that consists of an east-west-trending graben. Volcanic activity at Pele, as seen in images taken by \"Galileo\" in October 2001 while Pele was on Io's night side, appears to be limited to small thermal \"hot-spots\" along the margins of the patera and a more intense thermal emission source within a dark area in the southeast portion of the patera floor. This distribution of activity, combined with Pele's stability as a hotspot in terms of temperature and power emitted, suggests that Pele is a large, active lava lake, a combination of eruption style and intensity of activity not seen elsewhere on Io. The small hotspots seen in the Galileo data represent areas where the crust of the lava lake breaks up along the margins of the patera, allowing fresh lava to become exposed at the surface. The southeastern portion of the patera, an area of dark terrain in \"Voyager 1\" imagery, is the most active region of the Pele volcano, with the most extensive region of hot lava at Pele. This area is thought to be a vigorously overturning lava lake, suggestive of a combination of a large mass flux of lava to the lake from a magma reservoir below the surface and a large mass fraction of dissolved volatiles like sulfur dioxide and diatomic sulfur. Given Pele's brightness at near-infrared wavelengths, activity at this portion of the lava lake may also result in lava fountaining.\n\nLava temperatures measured using the near-infrared emission spectrum of thermal hotspots observed at Pele are consistent with silicate basaltic lava erupting at the lava lake. The measurements from \"Galileo\" and \"Cassini\" images of Pele suggest peak temperatures of at least 1250-1350 °C, while the near-infrared spectrometer on \"Galileo\" found peak temperatures of 1250-1280 °C. While Pele's energy output and temperature remained consistent on the timescale of months to years throughout much of the \"Galileo\" missions, measurements of Pele's brightness using \"Cassini\" data taken during an eclipse of Io by Jupiter found considerable variations on the timescale of minutes. This is consistent with variations in the distribution and size of lava fountains at Pele over that timeframe.\n\nPele's plume is the archetypal Pele-type plume: tall, producing a large reddish deposit that is concentric around the source vent. The plume is created from the degassing of sulfur (S) and sulfur dioxide (SO) from erupting lava in the Pele lava lake. The persistence of degassed sulfurous compounds to Pele's plume is likely from a stable and consistent magma supply to its lava lake, which could be the largest magma chamber of Io's volcanoes. Images of the plume taken by \"Voyager 1\" revealed a large structure without a central column like the smaller, Prometheus-type plumes, but instead having a filamentary structure. This morphology is consistent with a plume that is formed by sulfurous gases erupted skyward from the Pele lava lake, which then condense into solid S and SO when they reach the shock canopy along the outer edge of the umbrella-shaped plume. These condensed materials then are deposited onto the surface, forming a large, red, oval-shaped ring around the Pele volcano. The oval shape of the deposits, elongated in roughly the north-south direction, may be the result of an east-west, linear source region, consistent with the shape and orientation of the graben that forms the southern and more active portion of the Pele patera. Variable activity in different portions of the Pele lava lake may also result in the changes in brightness and shape of the plume deposit over time observed by various spacecraft.\n"}
{"id": "4615047", "url": "https://en.wikipedia.org/wiki?curid=4615047", "title": "Philippe Danfrie", "text": "Philippe Danfrie\n\nPhilippe Danfrie the elder (about 1532 in Cornouaille in Brittany - 1606 in Paris), was a designer and maker of mathematical instruments in metal and paper, as well as a type-cutter, engraver, minter of coins and medals, publisher and author. Much is known about Danfrie's life and activities. He is probably best known as designer of the surveying instrument known as the graphometer.\n\nDanfrie went to Paris in the 1550s and set up as an engraver of letter punches. He produced a number of books in partnership with Richard Breton in 1558–60 and later with Pierre Haman and Jean Le Royer. He also made mathematical instruments, globes and astrolabes and dies for marking bookbindings. In 1571 he cut his first dies for jetons. As Engraver-General of the French coinage from 1582, he provided the puncheons from which the dies used in every mint in France were taken. He also produced a number of medals (e.g. London, BM) commemorating the events of the first fifteen years of Henry IV’s reign.\n\nEngraver and superintendent of the Paris Mint, Philippe Danfrie was also an engineer and inventor of scientific instruments. Active in the second half of the sixteenth century, he built globes, astrolabes, and clocks. He invented a typeface that he used in his published writings. These include: \"Declaration de l'usage du Graphometre...\", printed in Paris in 1597 together with \"Traicte de l'usage du Trigometre, avec le quel on peut facilement mesurer sans subiection d'Arithmetique\".\n\nHis son Philippe Danfrie the younger (b. ?Paris, c. 1572; d. Paris, 1604) was appointed Controller-General of effigies in 1591. On his appointment it was claimed that he had demonstrated great skill in modelling portraits in wax and engraving puncheons. His most famous and only signed medal (e.g. London, BM) is cast rather than struck and celebrates the victory of Henry IV over the Duke of Savoy in 1600.\n\n"}
{"id": "261407", "url": "https://en.wikipedia.org/wiki?curid=261407", "title": "Plasma cosmology", "text": "Plasma cosmology\n\nPlasma cosmology is a non-standard cosmology whose central postulate is that the dynamics of ionized gases and plasmas play important, if not dominant, roles in the physics of the universe beyond the Solar System. In contrast, the current observations and models of cosmologists and astrophysicists explain the formation, development, and evolution of astronomical bodies and large-scale structures in the universe as influenced by gravity (including its formulation in Einstein's theory of general relativity) and baryonic physics.\n\nSome theoretical concepts about plasma cosmology originated with Hannes Alfvén, who tentatively proposed the use of plasma scaling to extrapolate the results of laboratory experiments and plasma physics observations and scale them over many orders-of-magnitude up to the largest observable objects in the universe (see box.)\n\nCosmologists and astrophysicists who have evaluated plasma cosmology have rejected it because it does not match the observations of astrophysical phenomena as well as current cosmological theory. Very few papers supporting plasma cosmology have appeared in the literature since the mid-1990s.\n\nThe term \"plasma universe\" is sometimes used as a synonym for plasma cosmology, as an alternative description of the plasma in the universe.\n\nIn the 1960s, the theory behind plasma cosmology was introduced by Alfvén, a plasma expert who won the 1970 Nobel Prize in Physics for his work on magnetohydrodynamics (MHD). In 1971, Oskar Klein, a Swedish theoretical physicist, extended the earlier proposals and developed the Alfvén–Klein model of the universe, or \"metagalaxy\", an earlier term used to refer to the empirically accessible part of the universe, rather than the entire universe including parts beyond our particle horizon. In this Alfvén–Klein cosmology, sometimes called Klein–Alfvén cosmology, the universe is made up of equal amounts of matter and antimatter with the boundaries between the regions of matter and antimatter being delineated by cosmic electromagnetic fields formed by double layers, thin regions comprising two parallel layers with opposite electrical charge. Interaction between these boundary regions would generate radiation, and this would form the plasma. Alfvén introduced the term ambiplasma for a plasma made up of matter and antimatter and the double layers are thus formed of ambiplasma. According to Alfvén, such an ambiplasma would be relatively long-lived as the component particles and antiparticles would be too hot and too low-density to annihilate each other rapidly. The double layers will act to repel clouds of opposite type, but combine clouds of the same type, creating ever-larger regions of matter and antimatter. The idea of ambiplasma was developed further into the forms of heavy ambiplasma (protons-antiprotons) and light ambiplasma (electrons-positrons).\n\nAlfvén–Klein cosmology was proposed in part to explain the observed baryon asymmetry in the universe, starting from an initial condition of exact symmetry between matter and antimatter. According to Alfvén and Klein, ambiplasma would naturally form pockets of matter and pockets of antimatter that would expand outwards as annihilation between matter and antimatter occurred in the double layer at the boundaries. They concluded that we must just happen to live in one of the pockets that was mostly baryons rather than antibaryons, explaining the baryon asymmetry. The pockets, or bubbles, of matter or antimatter would expand because of annihilations at the boundaries, which Alfvén considered as a possible explanation for the observed expansion of the universe, which would be merely a local phase of a much larger history. Alfvén postulated that the universe has always existed due to causality arguments and the rejection of \"ex nihilo\" models, such as the Big Bang, as a stealth form of creationism. The exploding double layer was also suggested by Alfvén as a possible mechanism for the generation of cosmic rays,\n\nIn 1993, theoretical cosmologist Jim Peebles criticized Alfvén–Klein cosmology, writing that \"there is no way that the results can be consistent with the isotropy of the cosmic microwave background radiation and X-ray backgrounds\". In his book he also showed that Alfvén's models do not predict Hubble's law, the abundance of light elements, or the existence of the cosmic microwave background. A further difficulty with the ambiplasma model is that matter–antimatter annihilation results in the production of high energy photons, which are not observed in the amounts predicted. While it is possible that the local \"matter-dominated\" cell is simply larger than the observable universe, this proposition does not lend itself to observational tests.\n\nHannes Alfvén from the 1960s to 1980s argued that plasma played an important if not dominant role in the universe because electromagnetic forces are far more important than gravity when acting on interplanetary and interstellar charged particles. He further hypothesized that they might promote the contraction of interstellar clouds and may even constitute the main mechanism for contraction, initiating star formation. The current standard view is that magnetic fields can hinder collapse, that large-scale Birkeland currents have not been observed, and that the length scale for charge neutrality is predicted to be far smaller than the relevant cosmological scales.\n\nIn the 1980s and 1990s, Alfvén and Anthony Peratt, a plasma physicist at Los Alamos National Laboratory, outlined a program they called the \"plasma universe\". In plasma universe proposals, various plasma physics phenomena were associated with astrophysical observations and were used to explain extant mysteries and problems outstanding in astrophysics in the 1980s and 1990s. In various venues, Peratt profiled what he characterized as an alternative viewpoint to the mainstream models applied in astrophysics and cosmology.\n\nFor example, Peratt proposed that the mainstream approach to galactic dynamics which relied on gravitational modeling of stars and gas in galaxies with the addition of dark matter was overlooking a possibly major contribution from plasma physics. He mentions laboratory experiments of Winston H. Bostick in the 1950s that created plasma discharges that looked like galaxies. Perrat conducted computer simulations of colliding plasma clouds that he reported also mimicked the shape of galaxies. Peratt proposed that galaxies formed due to plasma filaments joining in a z-pinch, the filaments starting 300,000 light years apart and carrying Birkeland currents of 10 amperes. Peratt also reported simulations he did showing emerging jets of material from the central buffer region that he compared to quasars and active galactic nuclei occurring without supermassive black holes. Peratt proposed a sequence for galaxy evolution: \"the transition of double radio galaxies to radioquasars to radioquiet QSO's to peculiar and Seyfert galaxies, finally ending in spiral galaxies\". He also reported that flat galaxy rotation curves were simulated without dark matter. At the same time Eric Lerner, an independent plasma researcher and supporter of Peratt's ideas, proposed a plasma model for quasars based on a dense plasma focus.\n\nStandard astronomical modeling and theories attempt to incorporate all known physics into descriptions and explanations of observed phenomena, with gravity playing a dominant role on the largest scales as well as in celestial mechanics and dynamics. To that end, both Keplerian orbits and Einstein's general theory of relativity are generally used as the underlying frameworks for modeling astrophysical systems and structure formation, while high-energy astronomy and particle physics in cosmology additionally appeal to electromagnetic processes including plasma physics and radiative transfer to explain relatively small scale energetic processes observed in the x-rays and gamma rays. In conventional cosmology, plasma physics is not considered to be the dominant force on most large-scale phenomena, although much of the matter in the universe is thought to be ionised or exist as plasma. (See astrophysical plasma for more.)\n\nProponents of plasma cosmology claim electrodynamics is as important as gravity in explaining the structure of the universe, and speculate that it provides an alternative explanation for the evolution of galaxies and the initial collapse of interstellar clouds. In particular plasma cosmology is claimed to provide an alternative explanation for the flat rotation curves of spiral galaxies and to do away with the need for dark matter in galaxies and with the need for supermassive black holes in galaxy centres to power quasars and active galactic nuclei. However, theoretical analysis shows that \"many scenarios for the generation of seed magnetic fields, which rely on the survival and sustainability of currents at early times [of the universe are disfavored]\", i.e. Birkeland currents of the magnitude needed (10 amps over scales of megaparsecs) for galaxy formation do not exist. Additionally, many of the issues that were mysterious in the 1980s and 1990s, including discrepancies relating to the cosmic microwave background and the nature of quasars, have been solved with more evidence that, in detail, provides a distance and time scale for the universe.\n\nSome of the places where plasma cosmology supporters are most at odds with standard explanations include the need for their models to have light element production without Big Bang nucleosynthesis, which, in the context of Alfvén–Klein cosmology, has been shown to produce excessive x-rays and gamma rays beyond that observed. Plasma cosmology proponents have made further proposals to explain light element abundances, but the attendant issues have not been fully addressed. In 1995 Eric Lerner published his alternative explanation for the cosmic microwave background radiation (CMB). He argued that his model explained the fidelity of the CMB spectrum to that of a black body and the low level of anisotropies found, even while the level of isotropy at 1:10 is not accounted for to that precision by any alternative models. Additionally, the sensitivity and resolution of the measurement of the CMB anisotropies was greatly advanced by WMAP and the Planck satellite and the statistics of the signal were so in line with the predictions of the Big Bang model, that the CMB has been heralded as a major confirmation of the Big Bang model to the detriment of alternatives. The acoustic peaks in the early universe are fit with high accuracy by the predictions of the Big Bang model, and, to date, there has never been an attempt to explain the detailed spectrum of the anisotropies within the framework of plasma cosmology or any other alternative cosmological model.\n\n\n\n"}
{"id": "39523982", "url": "https://en.wikipedia.org/wiki?curid=39523982", "title": "Prediction and Research Moored Array in the Atlantic", "text": "Prediction and Research Moored Array in the Atlantic\n\nThe Prediction and Research Moored Array in the Atlantic, also known as PIRATA, is a system of moored observation buoys in the tropical Atlantic Ocean which collect meteorological and oceanographic data. The data collected by the PIRATA array helps scientists to better understand climatic events in the Tropical Atlantic and to improve weather forecasting and climate research worldwide. Climatic and oceanic events in the tropical Atlantic, such as the Tropical Atlantic SST Dipole affect rainfall and climate in both West Africa and Northeast Brazil. The northern tropical Atlantic is also a major formation area for hurricanes affecting the West Indies and the United States. Alongside the RAMA array in the Indian Ocean and the TAO/TRITON network in the Pacific Ocean, PIRATA forms part of the worldwide system of tropical ocean observing buoys.\n\nThe project is a tripartite cooperation between Brazil, France and the United States. The principal agencies involved are NOAA in the United States, IRD and Météo-France of France plus INPE and DHN from Brazil.\n\nThe PIRATA buoy network consists of seventeen Autonomous Temperature Line Acquesition System, or ATLAS, buoys. Twelve buoys were originally deployed in 1997. Two of these buoys were decommissioned in 1999 because of vandalism by fishing craft. Three extensions of the original network have been added. Three buoys were deployed off the coast of Brazil in 2005 and four more in 2006/2007 to extend coverage to the north and the north-east. As a demonstration exercise one buoy was deployed to the south-east of the region, off the coast of Africa, between June 2006 and June 2007.\n\nIn addition to the ATLAS buoys, PIRATA has three island based meteorological stations, one at Fernando de Noronha, another on the Saint Peter and Saint Paul Archipelago and one on São Tomé. A tidal gauge is also maintained at São Tomé. Dedicated hydrographic cruises and annual buoy maintenance voyages are also undertaken under the auspices of the PIRATA project.\n\nEach ATLAS buoy measures\nIn addition one buoy has an Acoustic Doppler Current Profiler fitted alongside to measure water current velocities and four buoys are equipped to measure net heat flux.\n\nDaily mean observations from the ATLAS buoys are received in near real time via both the Argos System and Brazilian satellites. The data is processed by the TAO Project Office of NOAA and also placed on the Global Telecommunications System for real time distribution to weather centres and other users. High frequency measurements are stored on the buoys and retrieved during maintenance operations. The array provides 4,000 to 4,500 unique hourly values per month.\n"}
{"id": "9144685", "url": "https://en.wikipedia.org/wiki?curid=9144685", "title": "Principles of Geology", "text": "Principles of Geology\n\nPrinciples of Geology: being an attempt to explain the former changes of the Earth's surface, by reference to causes now in operation is a book by the Scottish geologist Charles Lyell that was first published in 3 volumes from 1830–1833. Lyell used the theory of Uniformitarianism to describe how the Earth's surface was changing over time. This theory was in direct contrast to the geological theory of Catastrophism. Many individuals believed in catastrophism to allow room for religious beliefs. For example, Noah's Flood could be described thus as a real geological event as catastrophism describes the changing of the Earth surface as one-time, violent events. Lyell challenged the believers of the catastrophic theory by studying Mount Etna in Sicily and describing the changes from one stratum to another and the fossil records within the rocks to prove that slow, gradual changes were the cause of the ever-changing Earth's surface. Lyell used geological proof to determine that the Earth was older than 6,000 years, as had been previously contested. The book shows that the processes that are occurring in the present are the same processes that occurred in the past.\n\nPublished in three volumes in 1830–33, the book established Lyell's credentials as an important geological theorist and popularized the doctrine of uniformitarianism (first suggested by James Hutton in \"Theory of the Earth\" published in 1795). The central argument in \"Principles\" was that \"the present is the key to the past\": that geological remains from the distant past could, and should, be explained by reference to geological processes now in operation and thus directly observable.\n\nThe book is notable for being one of the first to use the term \"evolution\" in the context of biological speciation.\n\nIn Lyell's work, he described the three rules he believes to cause the steady change of the Earth. The first rule is that geologic change comes from slow and continual procedures that have been happening over a long period of time. This rule is the basic ideal of Uniformitarianism and is easy to understand why this was a rule. The second rule is that all the forces that affect the geology of the Earth comes from the Earth. The third rule is that celestial cycles do not impact the patterns of Earth's geologic record. Rule two and rule three go together because Lyell thought that only forces on the Earth cause changes to Earth's geology, and nothing else.\n\nVolume 1 introduces Lyell's theory of uniformitarianism. He develops and argues that the earthly processes that we see in the present were the same processes as in the past and caused the Earth to look like it does today. Volume 2 builds off of the uniformitarianism theory in volume 1, but focuses more on the organic matter rather than the inorganic matter. This volume is what Darwin took with him on his voyage on the \"Beagle\". In the 3rd volume, Lyell identifies four periods of the Tertiary: Newer Pliocene, Older Pliocene, Miocene, and the Eocene. Lyell used deposits and fossils from these periods to argue for uniformity during the Tertiary. This also talks about the grammar or syntax of the processes that occurred in the past in today's language. \n\nLyell's interpretation of geologic change as the steady accumulation of minute changes over enormously long spans of time, a central theme in the \"Principles\", influenced the 22-year-old Charles Darwin, who was given Volume 1 of the first edition by Robert FitzRoy, captain of HMS \"Beagle\", just before they set out (December 1831) on their voyage on the \"Beagle\". On their first stop ashore at St Jago, Darwin found rock formations which -seen \"through Lyell's eyes\"- gave him a revolutionary insight into the geological history of the island, an insight he applied throughout his travels. While in South America, Darwin received Volume 2, which rejected the idea of organic evolution, proposing \"Centres of Creation\" to explain diversity and territory of species. Darwin's ideas gradually moved beyond this, but in geology he operated very much as Lyell's disciple and sent home extensive evidence and theorizing supporting Lyell's uniformitarianism, including Darwin's ideas about the formation of atolls.\n\nCharles Lyell's \"Principles of Geology\" was met with a lot of criticism when it was first published. The main argument against Lyell is that he took an \"a priori\" approach in his work. This means that Charles Lyell was pulling from a theoretical idea instead of pulling from empirical evidence to explain what was occurring in the geological world. One opponent of \"Principles of Geology\" that agreed with this point was Adam Sedgwick. This opposition from Sedgwick comes from his thinking that evidence is all that is needed to support an idea, and that the evidence of geologic events points to a catastrophic event. The criticism of Lyell and his work continued into the 20th century. These arguments agreed with the \"a priori\" argument, but continued on to say that Lyell combined the empirical evidence with the scientific explanation of geology that was accepted at the time.\n\nThe \"a priori\" argument is not the only argument that Lyell faced for his work. In 1812, Baron Georges Cuvier argued against uniformitarianism with the results of his study of the Paris Basin. Cuvier and his colleagues found long period of consistent change with intermittent patterns of sudden fossil disappearance in the geologic record for the area, which is now known as mass extinction. Cuvier explained these sudden changes in the geologic record with catastrophic forces. Lyell responded to this argument, stating that the geologic record was \"grossly imperfect\" and that observations cannot be trusted if they go against \"the plan of Nature\".\n\nIn recent years, geologists have begun to question the laws of uniformitarianism Lyell laid out. There is now clear evidence of catastrophic change caused by volcanic eruptions, large earthquakes, and asteroid impacts. Moreover, there is evidence that certain cataclysmic occurrences that left marks in the geological and fossil records may correspond to the periodicity of the solar system's 26-million-year cycle of movement around the galactic core of the Milky Way. Even if catastrophes are rare, their magnitude may affect geology more than has been appreciated under Lyell's version of uniformitarianism. \n\n\n"}
{"id": "22946554", "url": "https://en.wikipedia.org/wiki?curid=22946554", "title": "Project commissioning", "text": "Project commissioning\n\nProject commissioning is the process of assuring that all systems and components of a building or industrial plant are designed, installed, tested, operated, and maintained according to the operational requirements of the owner or final client. A commissioning process may be applied not only to new projects but also to existing units and systems subject to expansion, renovation or revamping. \n\nIn practice, the commissioning process comprises the integrated application of a set of engineering techniques and procedures to check, inspect and test every operational component of the project, from individual functions, such as instruments and equipment, up to complex amalgamations such as modules, subsystems and systems.\n\nCommissioning activities, in the broader sense, are applicable to all phases of the project, from the basic and detailed design, procurement, construction and assembly, until the final handover of the unit to the owner, including sometimes an assisted operation phase.\n\nCommissioning of large civil and industrial projects is a complex and sophisticated technical specialty which may be considered as a specific and independent . As such, it can be as important as the more traditional ones, i.e. civil, naval, chemical, mechanical, electrical, electronic, instrumentation, automation, or telecom engineering. Large projects for which this statement can be made include chemical and petrochemical plants, oil and gas platforms and pipelines, metallurgical plants, paper and cellulose plants, coal handling plants, thermoelectric and hydroelectric plants, buildings, bridges, highways, and railroads.\n\nHowever, there is currently no formal education or university degree which addresses the training or certification of a \"Project Commissioning Engineer\". Various short training courses and on-line training are available, but they are designed for qualified engineers. Commissioning for buildings is a specific discipline in itself, and there are qualifications available for this.\n\nThe International Association of Commissioning Engineers (IACE) was formed in 2015 by a group of Commissioning Professionals, and launched in January 2016. IACE is preparing accredited qualifications to fill the gap in the academic landscape for Commissioning Engineers.\n\nThe main objective of commissioning is to affect the safe and orderly handover of the unit from the constructor to the owner, guaranteeing its operability in terms of performance, reliability, safety and information traceability. Additionally, when executed in a planned and effective way, commissioning normally represents an essential factor for the fulfillment of schedule, costs, safety and quality requirements of the project. \n\nFor complex projects, the large volume and complexity of commissioning data, together with the need to guarantee adequate information traceability, normally leads to the use of powerful IT tools, known as commissioning management systems, to allow effective planning and monitoring of the commissioning activities.\n\n"}
{"id": "566027", "url": "https://en.wikipedia.org/wiki?curid=566027", "title": "Red Sky at Morning (Speth book)", "text": "Red Sky at Morning (Speth book)\n\nJames Gustave Speth authored the book Red Sky at Morning: America and the Crisis of the Global Environment, which Yale University Press published in 2004. A central premise of the book is that environmentalism, so far, has been unsuccessful in protecting the natural environment on Earth. Deprecating the past efficacy of the Natural Resources Defense Council, the White House Council on Environmental Quality, and the United Nations Development Programme — as well as the actions of the former George W. Bush administration – Speth writes (as cited in the \"TIME\" article listed in the \"References\" section): \"The climate convention is not protecting climate, the biodiversity convention is not protecting biodiversity, [and] the desertification convention is not preventing desertification.\" Potential for effective environmentalism, he says (as cited in the \"TIME\" article) now rests upon actions analogous to \"jazz\": volunteerism and improvisation. He also notes, \"Since the Montreal Protocol, [the United States] has not accorded global-scale environmental challenges the priority needed.\" (\"p.116\")\n\n\n\n"}
{"id": "50899170", "url": "https://en.wikipedia.org/wiki?curid=50899170", "title": "Santucci's Armillary Sphere", "text": "Santucci's Armillary Sphere\n\nSantucci's armillary sphere is a Ptolemaic armillary sphere at the Museo Galileo, the largest existing in the world.\n\nBegun on March 4, 1588, and completed on May 6, 1593, this large armillary sphere was built under the supervision of Antonio Santucci at the request of Ferdinand I de' Medici. The sphere represents the \"universal machine\" of the world according to the concepts developed by Aristotle and perfected by Ptolemy. The terrestrial globe is placed at the center, and it also displays territories that were still relatively little known at the time.\n\nThe device was restored in the 19th century but is now incomplete and some of its parts are mismatched. The wooden parts of the sphere are elaborately painted and covered with fine gold leaf. The sphere rests on a stand with four sirens.\n\nThis model is similar to a smaller one built by Santucci in 1582 for King Philip II of Spain, now in the Escorial Library.\n\n"}
{"id": "32909000", "url": "https://en.wikipedia.org/wiki?curid=32909000", "title": "Skin friction line", "text": "Skin friction line\n\nIn scientific visualization skin friction lines are used to visualize flows on 3D-surfaces. They are obtained by calculating the streamlines of a derived vector field on the surface, the wall shear stress. Skin friction arises from the friction of the fluid against the \"skin\" of the object that is moving through it and forms a vector at each point on the surface. A skin friction line is a curve on the surface tangent to skin friction vectors. A limit streamline is a streamline where the distance normal to the surface tends to zero. Limit streamlines and skin friction lines coincide.\n\nThe lines can be visualized by placing a viscous film on the surface.\n\nThe skin friction lines may exhibit a number of different types of singularities: attachment nodes, detachment nodes, Isotropic nodes, Saddle points, and foci.\n\n"}
{"id": "46767412", "url": "https://en.wikipedia.org/wiki?curid=46767412", "title": "Sleisenger and Fordtran's Gastrointestinal and Liver Disease", "text": "Sleisenger and Fordtran's Gastrointestinal and Liver Disease\n\nSleisenger and Fordtran's Gastrointestinal and Liver Disease is a textbook on hepatology and gastroenterology for medical students, internists, and surgeons. First published in 1978, it has undergone many revisions to reflect the rapid advances in internal medicine and is currently in its 10th edition. It has been described as \"a comprehensive and authoritative textbook of gastrointestinal diseases\", the \"standard bearer in gastrointestinal textbooks\", and \"the dominant textbook of gastroenterology\".\n"}
{"id": "8557773", "url": "https://en.wikipedia.org/wiki?curid=8557773", "title": "Stanislaus von Prowazek", "text": "Stanislaus von Prowazek\n\nStanislaus Josef Mathias von Prowazek, Edler von Lanow (12 November, 1875 Jindřichův Hradec, Bohemia – 17 February, 1915, Cottbus), born Stanislav Provázek, was a Czech zoologist and parasitologist, who along with pathologist Henrique da Rocha Lima (1879-1956) discovered the pathogen of epidemic typhus.\n\nAs a student at the University of Prague, he was influenced by the teachings of zoologist Berthold Hatschek and philosopher Ernst Mach. Other important influences to his career were immunologist Paul Ehrlich at the Institute for Experimental Therapy in Frankfurt (1901) and zoologist Richard von Hertwig at the University of Munich.\n\nWith radiologist Ludwig Halberstädter, he described the inclusion bodies (Halberstädter-Prowazek bodies) of \"Chlamydia trachomatis\", the agent that is the cause of trachoma.\n\nIn 1906 he succeeded his late friend, Fritz Schaudinn, as director of the zoological section at the Institut für Schiffs- und Tropenkrankheiten in Hamburg. In 1908 he conducted research at the Instituto Oswaldo Cruz, outside of Rio de Janeiro, and from 1910, carried out investigations of infectious diseases in Sumatra, German Samoa, Yap and Saipan.\n\nProwazek studied epidemic typhus in Serbia (1913) and Istanbul (1914). Later, while Prowazek and Rocha Lima were working in a German prison hospital, they both became infected with typhus. Prowazek died soon afterwards on February 17, 1915. Rocha Lima named the infectious agent of epidemic typhus - \"Rickettsia prowazekii\" - after his colleague.\n\n\n"}
{"id": "24374551", "url": "https://en.wikipedia.org/wiki?curid=24374551", "title": "Sumihiko Hatusima", "text": "Sumihiko Hatusima\n\nHatsushima was born in Nagasaki Prefecture, Japan in 1906.\nHis tertiary studies and early lectureship was at Kyushu Imperial University, where he was awarded a doctorate in 1942.\nHe accompanied Ryōzō Kanehira on a collecting expedition in New Guinea in 1940. Hatsushima returned to Austronesia in a collecting expedition to The Philippines in 1964.\n\nHatsushima died in 2008.\n"}
{"id": "4112587", "url": "https://en.wikipedia.org/wiki?curid=4112587", "title": "The Sleeper Awakes", "text": "The Sleeper Awakes\n\nThe Sleeper Awakes (1910) is a dystopian science fiction novel by English writer H. G. Wells, about a man who sleeps for two hundred and three years, waking up in a completely transformed London where he has become the richest man in the world. The main character awakes to see his dreams realised, and the future revealed to him in all its horrors and malformities.\n\nThe novel is a rewritten version of When the Sleeper Wakes, a story by Wells that was serialised between 1898 and 1899.\n\nThe novel was originally published, as \"When the Sleeper Wakes\", in \"The Graphic\" from 1898 to 1903 and illustrated by H. Lanos. Dissatisfied with its original form, Wells, who was an outspoken socialist and author of prophetic writings, rewrote it in 1910. \"Like most of my earlier work\", he wrote in the 1910 edition's preface, \"it was written under considerable pressure; there are marks of haste not only in the writing of the latter part, but in the very construction of the story\".\n\nThe short story \"A Story of the Days To Come\" (1897) is a forerunner of the novel, being a tale set within the same future society.\n\nGraham, an Englishman living in London in 1897 takes drugs to cure insomnia and falls into a coma. He wakes up in 2100. He later learns that he has inherited huge wealth and that his money has been put into a trust. Over the years, the trustees, the \"White Council\", have used his wealth to establish a vast political and economic world order.\n\nWhen he wakes Graham is disoriented. The people around him had not expected him to wake up, and are alarmed. Word spreads that the \"Sleeper\" has awakened. A mob gathers around the building and demands to see the fabled Sleeper. The people around Graham will not answer his questions. They place Graham under house arrest. Graham learns that he is the legal owner and master of most of the world.\n\nRebels led by Ostrog help Graham to escape. They say that the people need Graham's leadership to rise against the White Council. Unconvinced, but unwilling to remain a prisoner, Graham goes with them. Graham arrives at a massive hall where the workers have gathered to prepare for the revolution. They march against the White Council but are soon attacked by the state police. In the confusion, Graham is separated from the revolutionaries. He meets an old man who tells him the story of the Sleeper - the White Council invested his wealth to buy the industries and political entities of half the world, establishing a plutocracy and sweeping away parliament and the monarchy. The Sleeper is just a figurehead. The old man thinks that the Sleeper is a made-up figure used to brainwash the population.\n\nGraham meets Ostrog, who says that the rebels have won and that the people are demanding that the Sleeper should rule. Ostrog retains power while Graham becomes his puppet ruler. Graham gets interested in aeroplanes and learns how to fly. He sees from the air that no-one lives in the country or small towns any more, all agriculture being run like industry; and that there are now only four huge cities in Britain, all powered by huge wind-mills. His carefree life ends when Helen Wotton tells him that for the lower class the revolution has changed nothing. Graham questions Ostrog who admits that the lower classes are still dominated and exploited but defends the system. It emerges that Ostrog only wanted to take power for himself and has used the revolution and Graham to do so.\n\nOstrog admits that in other cities the workers have continued to rebel even after the fall of the White Council. Ostrog has used a black African police force to get the workers back in line. Graham is furious and demands that Ostrog keep his police out of London. Ostrog agrees. Graham decides to examine this new society for himself.\n\nGraham visits London in disguise to see how the workers live. Their lives are ghastly. Unskilled workers toil in factories, paid in food for each day's work, with no job security. They speak a dialect so strong that Graham cannot understand them. Industrial diseases are rife. Workers wear uniforms of different colour according to their trade. The family unit no longer exists and children are cared for in huge institutions. Lives are dominated by \"babble machines\" which spread news and \"pleasure cities\" where unspecified joys are available. \"Euthanasy\" is considered normal.\n\nGraham learns that Ostrog has ordered his troops to London. Graham confronts Ostrog, who tries to arrest Graham. The workers rise up again and help Graham to escape. He meets Helen and learns that it was she who told the public about Ostrog's treachery. Graham leads the liberation of London.\n\nOstrog escapes and joins his troops who are flying to London. His men still hold a few airports. The workers find anti-aircraft guns, but they need time to set them up. The revolutionaries have only one aircraft; Graham gives away all of his wealth to the rebels and proceeds to fly the one aircraft against the invaders, bringing some of them down. The revolutionaries get the anti-aircraft guns working and start to shoot down the invaders. Graham attacks Ostrog's aeroplane but fails. Graham's aeroplane crashes.\n\nThemes include socialism; the betrayal of revolution; and how an elite can manipulate a population both by oppression and impoverishment on the one hand, and by the use of technology and provision of pleasure on the other. In this respect, the book has elements explored later both in \"Nineteen Eighty-Four\" by George Orwell and \"Brave New World\" by Aldous Huxley.\n\nFloyd C. Gale of \"Galaxy Science Fiction\" said of \"The Sleeper Awakes\" despite the \"impossibly timid\" and outdated science, \"The worth of the story lies in its human values ... This is 'Young Wells' at his non-Utopian best\".\n\nAspects of the novel's storyline are similar to the plot of the Woody Allen 1973 film \"Sleeper\".\n\nPulp writer Harry Stephen Keeler took the idea further in a 1914 story called \"John Jones' Dollar\", in which a solar system's economy is built around a single silver dollar left to accumulate until the year 2921 to the \"astounding\" sum of $6.3 trillion, financing an interplanetary socialist paradise.\n\nThe \"Futurama\" episode \"A Fishful of Dollars\" is loosely based upon Wells's novel. Fry discovers that his bank account has continued to accrue interest over the course of a thousand years.\n\n"}
{"id": "58388405", "url": "https://en.wikipedia.org/wiki?curid=58388405", "title": "Timeline of eurypterid research", "text": "Timeline of eurypterid research\n\nThis timeline of eurypterid research is a chronologically ordered list of important fossil discoveries, controversies of interpretation, and taxonomic revisions of eurypterids, a group of extinct aquatic arthropods closely related to modern horseshoe crabs that lived during the Paleozoic Era.\n\nThe scientific study of eurypterids began in the early 19th century when James E. DeKay recognized a fossil that had previously been described as that of a fish as arthropod in nature. Though DeKay erronously believed the fossil to represent a crustacean and a missing link between trilobites and branchiopods, the fossil became the type species of first ever eurypterid to be scientifically described, \"Eurypterus remipes\", in 1825.\n\nAs of 2018, over 250 species of eurypterids have been described. The most recent species to be described is \"Soligorskopterus tchepeliensis\".\n\n1818\n\n\n1825\n\n\n1839\n\n\n1844\n\n\n1856\n\n1889\n\n1901\n1903\n1907\n\n1939\n\n1950\n1951\n1957\n\n1961\n1966\n\n1980\n\n2005\n2006\n2007\n\n2012\n2015\n2017\n2018\n\n"}
{"id": "6961430", "url": "https://en.wikipedia.org/wiki?curid=6961430", "title": "Total correlation", "text": "Total correlation\n\nIn probability theory and in particular in information theory, total correlation (Watanabe 1960) is one of several generalizations of the mutual information. It is also known as the \"multivariate constraint\" (Garner 1962) or \"multiinformation\" (Studený & Vejnarová 1999). It quantifies the redundancy or dependency among a set of \"n\" random variables.\n\nFor a given set of \"n\" random variables formula_1, the total correlation formula_2 is defined as the Kullback–Leibler divergence from the joint distribution formula_3 to the independent distribution of formula_4,\n\nThis divergence reduces to the simpler difference of entropies,\nwhere formula_7 is the information entropy of variable formula_8, and formula_9 is the joint entropy of the variable set formula_1. In terms of the discrete probability distributions on variables formula_11, the total correlation is given by\n\nThe total correlation is the amount of information \"shared\" among the variables in the set. The sum formula_13 represents the amount of information in bits (assuming base-2 logs) that the variables would possess if they were totally independent of one another (non-redundant), or, equivalently, the average code length to transmit the values of all variables if each variable was (optimally) coded independently. The term formula_14 is the \"actual\" amount of information that the variable set contains, or equivalently, the average code length to transmit the values of all variables if the set of variables was (optimally) coded together. The difference between\nthese terms therefore represents the absolute redundancy (in bits) present in the given\nset of variables, and thus provides a general quantitative measure of the\n\"structure\" or \"organization\" embodied in the set of variables\n(Rothstein 1952). The total correlation is also the Kullback–Leibler divergence between the actual distribution formula_15 and its maximum entropy product approximation formula_4.\n\nTotal correlation quantifies the amount of dependence among a group of variables. A near-zero total correlation indicates that the variables in the group are essentially statistically independent; they are completely unrelated, in the sense that knowing the value of one variable does not provide any clue as to the values of the other variables. On the other hand, the maximum total correlation (for a fixed set of individual entropies formula_17) is given by\n\nand occurs when one of the variables determines \"all\" of the other variables. The variables are then maximally related in the sense that knowing the value of one variable provides complete information about the values of all the other variables, and the variables can be figuratively regarded as \"cogs,\" in which the position of one cog determines the positions of all the others (Rothstein 1952).\n\nIt is important to note that the total correlation counts up \"all\" the redundancies among a set of variables, but that these redundancies may be distributed throughout the variable set in a variety of complicated ways (Garner 1962). For example, some variables in the set may be totally inter-redundant while others in the set are completely independent. Perhaps more significantly, redundancy may be carried in interactions of various degrees: A group of variables may not possess any pairwise redundancies, but may possess higher-order \"interaction\" redundancies of the kind exemplified by the parity function. The decomposition of total correlation into its constituent redundancies is explored in a number sources (Mcgill 1954, Watanabe 1960, Garner 1962, Studeny & Vejnarova 1999, Jakulin & Bratko 2003a, Jakulin & Bratko 2003b, Nemenman 2004, Margolin et al. 2008, Han 1978, Han 1980).\n\nConditional total correlation is defined analogously to the total correlation, but adding a condition to each term. Conditional total correlation is similarly defined as a Kullback-Leibler divergence between two conditional probability distributions,\n\nAnalogous to the above, conditional total correlation reduces to a difference of conditional entropies,\n\nClustering and feature selection algorithms based on total correlation have been explored by Watanabe. Alfonso et al. (2010) applied the concept of total correlation to the optimisation of water monitoring networks.\n\n\n"}
{"id": "45451212", "url": "https://en.wikipedia.org/wiki?curid=45451212", "title": "Two-factor theory of intelligence", "text": "Two-factor theory of intelligence\n\nCharles Spearman developed his two-factor theory of intelligence using factor analysis. His research not only led him to develop the concept of the \"g factor\" of general intelligence, but also the \"s\" factor of specific intellectual abilities. L. L. Thurstone, Howard Gardner, and Robert Sternberg also researched the structure of intelligence, and in analyzing their data, concluded that a single underlying factor was influencing the general intelligence of individuals. However, Spearman was criticized in 1916 by Godfrey Thomson, who claimed that the evidence was not as crucial as it seemed. Modern research is still expanding this theory by investigating Spearman's law of diminishing returns, and adding connected concepts to the research.\n\nIn 1904, Charles Spearman had developed a statistical procedure called factor analysis. In factor analysis, related variables are tested for correlation to each other, then the correlation of the related items are evaluated to find clusters or groups of the variables. Spearman tested how well people performed on various tasks relating to intelligence. Such tasks include: distinguishing pitch, perceiving weight and colors, directions, and mathematics. When analyzing the data he collected, Spearman noted that those that did well in one area also scored higher in other areas. With this data, Spearman concluded that there must be one central factor that influences our cognitive abilities. Spearman termed this general intelligence \"g\".\n\nDue to the controversy of the structure of intelligence, other psychologists also published their relevant research. Other than Charles Spearman, three others developed a hypothesis regarding the structure of intelligence. L. L. Thurstone tested subjects on 56 different abilities; from his data he established seven primary mental abilities relating to intelligence. He categorized them as: spatial ability, numerical ability, word fluency, memory, perceptual speed, verbal comprehension, and inductive reasoning. Other researchers, interested in this new research study, analyzed Thurstone's data, discovering that those that scored high in one category often did well in the others. This finding gives support that there is an underlying factor influencing them, namely \"g\".\n\nHoward Gardner suggested in his theory of multiple intelligences that intelligence is formed out of multiple abilities. He recognized eight intelligences: linguistic, musical, spatial, intrapersonal, interpersonal, logical-mathematical, bodily-kinesthetic, and naturalist. He also considered the possibility of a ninth intelligent ability, existential intelligence. Gardner proposed that individuals who excelled in one ability would lack in another. Instead, his results showed that each of his eight intelligences correlate positively with each other. After further analysis, Gardner found that logic, spatial abilities, language, and mathematics are all linked in some way, giving support for an underlying \"g\" factor that is prominent in almost all intelligence in general.\n\nRobert Sternberg agreed with Gardner that there were multiple intelligences, but he narrowed his scope to just three in his triarchic theory of intelligence: analytical, creative, and practical. He classified analytical intelligence as problem-solving skills in tests and academics. Creative intelligence is considered how people react adaptively in new situations, or create novel ideas. Practical intelligence is defined as the everyday logic used when multiple solutions or decisions are possible. When Sternberg analyzed his data the relationship between the three intelligences surprised him. The data resembled what the other psychologists had found. All three mental abilities correlated highly with one another, and evidence that one basic factor, \"g\", was the primary influence.\n\nNot all psychologists agreed with Spearman and his general intelligence. In 1916, Godfrey Thomson wrote a paper criticizing Spearman's \"g\":The object of this paper is to show that the cases brought forward by Professor Spearman in favor of the existence of General Ability are by no means \"crucial.\" They are it is true not inconsistent with the existence of such a common element but neither are they inconsistent with its non-existence. The essential point about Professor Spearman's hypothesis is the existence of this General Factor. Both he and his opponents are agreed that there are Specific Factors peculiar to individual tests, both he and his opponents agree that there are Group Factors which run through some but not all tests. The difference between them is that Professor Spearman says there is a further single factor which runs through all tests, and that by pooling a few tests the Group Factors can soon be eliminated and a point reached where all the correlations are due to the General Factor alone. (pp. 217)\n\nSpearman originally came up with the term General Intelligence, or as he called it, \"g\", to measure intelligence in his Two Theory on Intelligence. Spearman first researched in an experiment with 24 children from a small village school measuring three intellectual measures, based on teachers rankings, to address intellectual and sensory as the two different sets of measure: School Cleverness, Common Sense A and Common Sense B. His results showed the average r between intellectual and sensory measures to be +.38, School Cleverness and Commonsense to be at +0.55, and the three tasks intercorrelated at +0.25. This data was looked at other populations including high school. Spearman proposed that intellectual and sensory measure be combined as assessment of general intelligence.\n\nSpearman proposed that his two-factor theory has two components. The general intelligence, \"g\", influences the performance on all mental tasks, while another component influences abilities on a particular task. To explain the differences in performance on different tasks, Spearman hypothesized that this other component was specific to a certain aspect of intelligence. This second factor he named \"s\", for specific ability. Regarding \"g\", Spearman saw individuals as having more or less general intelligence, while \"s\" varied from person to person on a task. In 1999, behavior geneticist Robert Plomin described \"g\" by saying: \"\"g\" is one of the most reliable and valid measures in the behavioral domain... and it predicts important social outcomes such as educational and occupational levels far better than any other trait.\"\n\nTo visualize \"g\", imagine a Venn diagram with four circles overlapping. In the middle of the overlapping circles, would be \"g\", which influences all the specific intelligences, while \"s\" is represented by the four circles. Though the specific number of \"s\" factors are unknown, a few have been relatively accepted: mechanical, spatial, logical, and arithmetical.\n\nRising interest in the debate on the structure of intelligence prompted Spearman to elaborate and argue for his hypothesis. He claimed that \"g\" was not made up of one single ability, but rather two genetically influenced, unique abilities working together. He called these abilities \"eductive\" and \"reproductive\". He suggested that future understanding of the interaction between these two different abilities would drastically change how individual differences and cognition are understood in psychology, possibly creating the basis for wisdom.\n\nMany researches are currently using Spearman's form of intelligence testing in their current studies. Although not all of the studies are currently using Spearman's exact model for intelligence testing, they are adding some modern concepts to that study. Spearman described that there was a functional relationship between intelligence and Sensory Discriminatory Abilities. Recent research has determined that there is an overlap between Working Memory, General Discriminatory Abilities, and Fluid Intelligence. His work has been built on, expanded, and linked to many other factors related to intelligence.\n\nIntelligence testing measuring the \"g\" factor has been studied recently to re-explore Spearman's law of diminishing returns. This study investigates how \"g\" test scores will most likely decrease as \"g\" increases. Research has been done to investigate if \"g\" scores are made up of scores from Differential Ability Scales, \"s\" factors, and how the law of diminishing returns compare to Spearman's Law of diminishing returns. With the use of linear and nonlinear Confirmatory Factor Analysis, it is showing that the nonlinear model best described the data. The nonlinear model suggests that as \"g\" increases, the \"s\" factor lowers the overall score and inaccurately represents general intelligence.\n\nThis theory is still greatly present in today's modern psychology. Researchers are examining this theory and recreating it in modern research. The \"g\" factor is still frequently studied in current research. For example, a study could use and be compared with various other similar intelligence measures. Scales such as the Wechsler Intelligence Scale for Children has been compared with Spearman's \"g\", which shows that there has a decrease in statistic significance.\n\nResearch has been adapted to incorporate modern psychological topics into Spearman's Two Factor Theory of Intelligence. Nature versus Nurture is one topic that has been cross studied with Spearman's \"g\" factor. Research shows that although environmental factors influence the \"g\" factor differently, it has been found that it is affected if influenced early in life, rather than adulthood where there is little to no impact. Genetic influence has been documented to greatly influence \"g\" factor on intelligence.\n\n"}
{"id": "9234323", "url": "https://en.wikipedia.org/wiki?curid=9234323", "title": "Urban ecosystem", "text": "Urban ecosystem\n\nUrban ecosystems are the cities, towns, and urban strips constructed by humans.\n\nThis is the growth in the urban population and the supporting built infrastructure has affected both urban environments and also on areas which surround urban areas. These include semi or 'peri-urban' environments that fringe cities as well as agricultural and natural landscapes. Semi environments can also be called peri-urban.\n\nNowadays scientists are developing ways to measure and understand the effects of urbanization on human and environmental health.\n\nBy considering urban areas as part of a broader social-ecological system, scientists can investigate how urban landscapes function and how they affect other landscapes with which they interact. In this context, urban environments are affected by their surrounding environment but also affect that environment. Knowing this may provide clues as to which alternative development options will lead to the best overall environmental outcome.\n\nCSIRO CSE's urban ecosystem research is focused on:\n\n\n\n\n"}
{"id": "29550701", "url": "https://en.wikipedia.org/wiki?curid=29550701", "title": "Women of Outstanding Achievement Photographic Exhibition", "text": "Women of Outstanding Achievement Photographic Exhibition\n\nThe Women of Outstanding Achievement Photographic Exhibition was an annual event organised by the UKRC. It recognised women within science, engineering and technology (SET). The exhibition was created in 2006. Between six and eight women were chosen each year to be photographed by Robert Taylor. Nominations occur in the Autumn of each year and the recipients were announced at a ceremony in March of the following year.\n\nMany of the portraits from previous years are on permanent loan to institutions such as the University of Oxford, The Royal Society and The Royal Academy of Engineering. The portrait of scientist Nancy Rothwell was purchased by the National Portrait Gallery.\n\nIn March 2010 the exhibition opened at The Royal Academy of Engineering. Evan Harris, MP for the Liberal Democrats, unveiled the portraits.\n\nRecent recipients include:\n\n"}
