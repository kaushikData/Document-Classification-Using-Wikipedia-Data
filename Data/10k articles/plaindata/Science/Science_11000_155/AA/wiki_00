{"id": "58548643", "url": "https://en.wikipedia.org/wiki?curid=58548643", "title": "AKTIP (therapeutic institute)", "text": "AKTIP (therapeutic institute)\n\nAKTIP – Konzultační a terapeutický institut Praha (The Consultation and Therapeutic Institute Prague) is a private institute run by the public business company Progressive consulting, offering services in the field of psychosomatic care. The head of the institute is psychiatrist Jarmila Klímová.\n\nControversy about the methods of the institute emerges when the documentary Infiltrace: Obchod se zdravím (Infiltration: Health Business) was broadcast by Czech television on May 21, 2018.\nThe documentary revealed that the company uses unethical techniques to the detriment of its patients for its financial gain, including pseudoscience, manipulation, and incitement of fear of cancer and anxiety. AKTIP offers examinations and healing by instrumentation and homeopathic preparations of doubtful effectiveness for various somatic diseases, including thyroid disease.\nAlthough it has physicians on staff, this institution is not a legitimate provider of health services. It is not a medical facility.\n\nAKTIP is the holder of the Silver Erratic Boulder award in the category of teams for 2016.\n\"“An eventual client meets here imaginary care in a wide range of disciplines. His complex problems will be treated by a graphologist, an expert in oriental diagnostics and bioresonance therapy will also supply its part of information. If it fails, a miraculous ANESA device, known as the non-invasive AMP blood analyzer (Golden Erratic Boulder award for 2011), will come to the scene”\", said Sisyfos.\n\nIn the awards report its representatives also cite the statement of the head of AKTIP Jarmila Klímová that the human body is set for 400 years of life. \"“If we really lived only by biological hours, we could be at rest between 380 and 460 years, because this age is set up for our body ... And why do we live only 60 or 80 years? No, because the most important influence outside of ourselves, which fundamentally affects the length and quality of life, we do not accept.”\"\n\nThe practices of AKTIP was already criticized by the Czech Oncological Society in 2014.\nProcedures used by AKTIP employees have been critically evaluated by doctors (f.ex. by cardiologist Věra Adámková) The psychologist Petr Weiss described AKTIP's practices as charlatan fabrications, fraud, and money-pulling activities.\n\nAccording to the President of the Czech Medical Chamber Milan Kubka, AKTIP's position in the future could be solved by a law on healers that would force \"all such charlatan institutions\" to mandatory registration. \"“This institution only looks like a medical facility, and that's a scam from my point of view.”\"\n\nSisyfos, in response to the AKTIP case, has published an article analyzing the principles of some devices used by individual \"consultants\" of AKTIP. This article also expresses concern that AKTIP is not an exceptional case, but that it is a fairly common phenomenon among institutions that provide services of alternative medicine.\n\n"}
{"id": "277664", "url": "https://en.wikipedia.org/wiki?curid=277664", "title": "Aharonov–Bohm effect", "text": "Aharonov–Bohm effect\n\nThe Aharonov–Bohm effect, sometimes called the Ehrenberg–Siday–Aharonov–Bohm effect, is a quantum mechanical phenomenon in which an electrically charged particle is affected by an electromagnetic potential (V, A), despite being confined to a region in which both the magnetic field B and electric field E are zero. The underlying mechanism is the coupling of the electromagnetic potential with the complex phase of a charged particle's wave function, and the Aharonov–Bohm effect is accordingly illustrated by interference experiments.\n\nThe most commonly described case, sometimes called the Aharonov–Bohm solenoid effect, takes place when the wave function of a charged particle passing around a long solenoid experiences a phase shift as a result of the enclosed magnetic field, despite the magnetic field being negligible in the region through which the particle passes and the particle's wavefunction being negligible inside the solenoid. This phase shift has been observed experimentally. There are also magnetic Aharonov–Bohm effects on bound energies and scattering cross sections, but these cases have not been experimentally tested. An electric Aharonov–Bohm phenomenon was also predicted, in which a charged particle is affected by regions with different electrical potentials but zero electric field, but this has no experimental confirmation yet. A separate \"molecular\" Aharonov–Bohm effect was proposed for nuclear motion in multiply connected regions, but this has been argued to be a different kind of geometric phase as it is \"neither nonlocal nor topological\", depending only on local quantities along the nuclear path.\n\nWerner Ehrenberg (1901–1975) and Raymond E. Siday first predicted the effect in 1949. Yakir Aharonov and David Bohm published their analysis in 1959. After publication of the 1959 paper, Bohm was informed of Ehrenberg and Siday's work, which was acknowledged and credited in Bohm and Aharonov's subsequent 1961 paper. \nThe effect was confirmed experimentally, with a very large error, while Bohm was still alive. By the time the error was down to a respectable value, Bohm had died.\n\nIn the 18th and 19th centuries, physics was dominated by Newtonian dynamics, with its emphasis on forces. Electromagnetic phenomena were elucidated by a series of experiments involving the measurement of forces between charges, currents and magnets in various configurations. Eventually, a description arose according to which charges, currents and magnets acted as local sources of propagating force fields, which then acted on other charges and currents locally through the Lorentz force law. In this framework, because one of the observed properties of the electric field was that it was irrotational, and one of the observed properties of the magnetic field was that it was divergenceless, it was possible to express an electrostatic field as the gradient of a scalar potential (e.g. Coulomb's electrostatic potential, which is mathematically analogous to the classical gravitational potential) and a stationary magnetic field as the curl of a vector potential (then a new concept – the idea of a scalar potential was already well accepted by analogy with gravitational potential). The language of potentials generalised seamlessly to the fully dynamic case but, since all physical effects were describable in terms of the fields which were the derivatives of the potentials, potentials (unlike fields) were not uniquely determined by physical effects: potentials were only defined up to an arbitrary additive constant electrostatic potential and an irrotational stationary magnetic vector potential.\n\nThe Aharonov–Bohm effect is important conceptually because it bears on three issues apparent in the recasting of (Maxwell's) classical electromagnetic theory as a gauge theory, which before the advent of quantum mechanics could be argued to be a mathematical reformulation with no physical consequences. The Aharonov–Bohm thought experiments and their experimental realization imply that the issues were not just philosophical.\n\nThe three issues are: \n\nBecause of reasons like these, the Aharonov–Bohm effect was chosen by the \"New Scientist\" magazine as one of the \"seven wonders of the quantum world\".\n\nIt is generally argued that Aharonov–Bohm effect illustrates the physicality of electromagnetic potentials, \"Φ\" and A, in quantum mechanics. Classically it was possible to argue that only the electromagnetic fields are physical, while the electromagnetic potentials are purely mathematical constructs, that due to gauge freedom aren't even unique for a given electromagnetic field.\n\nHowever, Vaidman has challenged this interpretation by showing that the AB effect can be explained without the use of potentials so long as one gives a full quantum mechanical treatment to the source charges that produce the electromagnetic field. According to this view, the potential in quantum mechanics is just as physical (or non-physical) as it was classically. Aharonov, Cohen, and Rohrlich responded that the effect may be due to a local gauge potential or due to non-local gauge-invariant fields.\n\nTwo papers published in the journal in 2017 \"Physical Review A\" have demonstrated a quantum mechanical solution for the system. Their analysis shows that the phase shift can be viewed as generated by solenoid's vector potential acting on the electron or the electron's vector potential acting on the solenoid or the electron and solenoid currents acting on the quantized vector potential.\n\nSimilarly, the Aharonov–Bohm effect illustrates that the Lagrangian approach to dynamics, based on energies, is not just a computational aid to the Newtonian approach, based on forces. Thus the Aharonov–Bohm effect validates the view that forces are an incomplete way to formulate physics, and potential energies must be used instead. In fact Richard Feynman complained that he had been taught electromagnetism from the perspective of electromagnetic fields, and he wished later in life he had been taught to think in terms of the electromagnetic potential instead, as this would be more fundamental. In Feynman's path-integral view of dynamics, the potential field directly changes the phase of an electron wave function, and it is these changes in phase that lead to measurable quantities.\n\nThe Aharonov–Bohm effect shows that the local E and B fields do not contain full information about the electromagnetic field, and the electromagnetic four-potential, (\"Φ\", A), must be used instead. By Stokes' theorem, the magnitude of the Aharonov–Bohm effect can be calculated using the electromagnetic fields alone, \"or\" using the four-potential alone. But when using just the electromagnetic fields, the effect depends on the field values in a region from which the test particle is excluded. In contrast, when using just the electromagnetic four-potential, the effect only depends on the potential in the region where the test particle is allowed. Therefore, one must either abandon the principle of locality, which most physicists are reluctant to do, or accept that the electromagnetic four-potential offers a more complete description of electromagnetism than the electric and magnetic fields can. On the other hand, the AB effect is crucially quantum mechanical; quantum mechanics is well-known to feature non-local effects (albeit still disallowing superluminal communication), and Vaidman has argued that this is just a non-local quantum effect in a different form.\n\nIn classical electromagnetism the two descriptions were equivalent. With the addition of quantum theory, though, the electromagnetic potentials \"Φ\" and A are seen as being more fundamental.  \nDespite this, all observable effects end up being expressible in terms of the electromagnetic fields, E and B. This is interesting because, while you can calculate the electromagnetic field from the four-potential, due to gauge freedom the reverse is not true.\n\nThe magnetic Aharonov–Bohm effect can be seen as a result of the requirement that quantum physics be invariant with respect to the gauge choice for the electromagnetic potential, of which the magnetic vector potential formula_1 forms part.\n\nElectromagnetic theory implies that a particle with electric charge formula_2 travelling along some path formula_3 in a region with zero magnetic field formula_4, but non-zero formula_1 (by formula_6), acquires a phase shift formula_7, given in SI units by\n\nTherefore, particles, with the same start and end points, but travelling along two different routes will acquire a phase difference formula_9 determined by the magnetic flux formula_10 through the area between the paths (via Stokes' theorem and formula_11), and given by:\n\nIn quantum mechanics the same particle can travel between two points by a variety of paths. Therefore, this phase difference can be observed by placing a solenoid between the slits of a double-slit experiment (or equivalent). An ideal solenoid (i.e. infinitely long and with a perfectly uniform current distribution) encloses a magnetic field formula_4, but does not produce any magnetic field outside of its cylinder, and thus the charged particle (e.g. an electron) passing outside experiences no magnetic field formula_4. However, there is a (curl-free) vector potential formula_1 outside the solenoid with an enclosed flux, and so the relative phase of particles passing through one slit or the other is altered by whether the solenoid current is turned on or off. This corresponds to an observable shift of the interference fringes on the observation plane.\n\nThe same phase effect is responsible for the quantized-flux requirement in superconducting loops. This quantization occurs because the superconducting wave function must be single valued: its phase difference formula_9 around a closed loop must be an integer multiple of formula_17 (with the charge formula_18 for the electron Cooper pairs), and thus the flux must be a multiple of formula_19. The superconducting flux quantum was actually predicted prior to Aharonov and Bohm, by F. London in 1948 using a phenomenological model.\n\nThe first claimed experimental confirmation was by Robert G. Chambers in 1960, in an electron interferometer with a magnetic field produced by a thin iron whisker, and other early work is summarized in Olariu and Popèscu (1984). However, subsequent authors questioned the validity of several of these early results because the electrons may not have been completely shielded from the magnetic fields. An early experiment in which an unambiguous Aharonov–Bohm effect was observed by completely excluding the magnetic field from the electron path (with the help of a superconducting film) was performed by Tonomura et al. in 1986. The effect's scope and application continues to expand. Webb \"et al.\" (1985) demonstrated Aharonov–Bohm oscillations in ordinary, non-superconducting metallic rings; for a discussion, see Schwarzschild (1986) and Imry & Webb (1989). Bachtold \"et al.\" (1999) detected the effect in carbon nanotubes; for a discussion, see Kong \"et al.\" (2004).\n\nThe magnetic Aharonov–Bohm effect is also closely related to Dirac's argument that the existence of a magnetic monopole can be accommodated by the existing magnetic source-free Maxwell's equations if both electric and magnetic charges are quantized.\n\nA magnetic monopole implies a mathematical singularity in the vector potential, which can be expressed as a Dirac string of infinitesimal diameter that contains the equivalent of all of the 4π\"g\" flux from a monopole \"charge\" \"g\". The Dirac string starts from, and terminates on, a magnetic monopole. Thus, assuming the absence of an infinite-range scattering effect by this arbitrary choice of singularity, the requirement of single-valued wave functions (as above) necessitates charge-quantization. That is, formula_20 must be an integer (in cgs units) for any electric charge \"q\" and magnetic charge \"q\".\n\nLike the electromagnetic potential A the Dirac string is not gauge invariant (it moves around with fixed endpoints under a gauge transformation) and so is also not directly measurable.\n\nJust as the phase of the wave function depends upon the magnetic vector potential, it also depends upon the scalar electric potential. By constructing a situation in which the electrostatic potential varies for two paths of a particle, through regions of zero electric field, an observable Aharonov–Bohm interference phenomenon from the phase shift has been predicted; again, the absence of an electric field means that, classically, there would be no effect.\n\nFrom the Schrödinger equation, the phase of an eigenfunction with energy \"E\" goes as formula_21. The energy, however, will depend upon the electrostatic potential \"V\" for a particle with charge \"q\". In particular, for a region with constant potential \"V\" (zero field), the electric potential energy \"qV\" is simply added to \"E\", resulting in a phase shift:\n\nwhere \"t\" is the time spent in the potential.\n\nThe initial theoretical proposal for this effect suggested an experiment where charges pass through conducting cylinders along two paths, which shield the particles from external electric fields in the regions where they travel, but still allow a varying potential to be applied by charging the cylinders. This proved difficult to realize, however. Instead, a different experiment was proposed involving a ring geometry interrupted by tunnel barriers, with a bias voltage \"V\" relating the potentials of the two halves of the ring. This situation results in an Aharonov–Bohm phase shift as above, and was observed experimentally in 1998.\n\nNano rings were created by accident while intending to make quantum dots. They have interesting optical properties associated with excitons and the Aharonov–Bohm effect. Application of these rings used as light capacitors or buffers includes photonic computing and communications technology. Analysis and measurement of geometric phases in mesoscopic rings is ongoing. It is even suggested they could be used to make a form of slow glass.\n\nSeveral experiments, including some reported in 2012, show Aharonov-Bohm oscillations in charge density wave (CDW) current versus magnetic flux, of dominant period \"h\"/2\"e\" through CDW rings up to 85 µm in circumference above 77 K. This behavior is similar to that of the superconducting quantum interference devices (see SQUID).\n\nThe Aharonov–Bohm effect can be understood from the fact that one can only measure absolute values of the wave function. While this allows for measurement of phase differences through quantum interference experiments, there is no way to specify a wavefunction with constant absolute phase. In the absence of an electromagnetic field one can come close by declaring the eigenfunction of the momentum operator with zero momentum to be the function \"1\" (ignoring normalization problems) and specifying wave functions relative to this eigenfunction \"1\". In this representation the i-momentum operator is (up to a factor formula_23) the differential operator formula_24. However, by gauge invariance, it is equally valid to declare the zero momentum eigenfunction to be formula_25 at the cost of representing the i-momentum operator (up to a factor) as formula_26 i.e. with a pure gauge vector potential formula_27. There is no real asymmetry because representing the former in terms of the latter is just as messy as representing the latter in terms of the former. This means that it is physically more natural to describe wave \"functions\", in the language of differential geometry, as sections in a complex line bundle with a hermitian metric and a U(1)-connection formula_28. The curvature form of the connection, formula_29, is, up to the factor i, the Faraday tensor of the electromagnetic field strength. The Aharonov–Bohm effect is then a manifestation of the fact that a connection with zero curvature (i.e. flat), need not be trivial since it can have monodromy along a topologically nontrivial path fully contained in the zero curvature (i.e. field free) region. By definition this means that sections that are parallelly translated along a topologically non trivial path pick up a phase, so that covariant constant sections cannot be defined over the whole field free region.\n\nGiven a trivialization of the line-bundle, a non-vanishing section, the U(1)-connection is given by the 1-form corresponding to the electromagnetic four-potential \"A\" as formula_30 where \"d\" means exterior derivation on the Minkowski space. The monodromy is the holonomy of the flat connection. The holonomy of a connection, flat or non flat, around a closed loop formula_31 is formula_32 (one can show this does not depend on the trivialization but only on the connection). For a flat connection one can find a gauge transformation in any simply connected field free region(acting on wave functions and connections) that gauges away the vector potential. However, if the monodromy is nontrivial, there is no such gauge transformation for the whole outside region. In fact as a consequence of Stokes' theorem, the holonomy is determined by the magnetic flux through a surface formula_33 bounding the loop formula_31, but such a surface may exist only if formula_33 passes through a region of non trivial field:\n\nThe monodromy of the flat connection only depends on the topological type of the loop in the field free region (in fact on the loops homology class). The holonomy description is general, however, and works inside as well as outside the superconductor. Outside of the conducting tube containing the magnetic field, the field strength formula_37. In other words, outside the tube the connection is flat, and the monodromy of the loop contained in the field-free region depends only on the winding number around the tube. The monodromy of the connection for a loop going round once (winding number 1) is the phase difference of a particle interfering by propagating left and right of the superconducting tube containing the magnetic field. \nIf one wants to ignore the physics inside the superconductor and only describe the physics in the outside region, it becomes natural and mathematically convenient to describe the quantum electron by a section in a complex line bundle with an \"external\" flat connection formula_28 with monodromy\n\nrather than an external EM field formula_41. The Schrödinger equation readily generalizes to this situation by using the Laplacian of the connection for the (free) Hamiltonian\n\nEquivalently, one can work in two simply connected regions with cuts that pass from the tube towards or away from the detection screen. In each of these regions the ordinary free Schrödinger equations would have to be solved, but in passing from one region to the other, in only one of the two connected components of the intersection (effectively in only one of the slits) a monodromy factor formula_43 is picked up, which results in the shift in the interference pattern as one changes the flux.\n\nEffects with similar mathematical interpretation can be found in other fields. For example, in classical statistical physics, quantization of a molecular motor motion in a stochastic environment can be interpreted as an Aharonov–Bohm effect induced by a gauge field acting in the space of control parameters.\n\n\n"}
{"id": "33972172", "url": "https://en.wikipedia.org/wiki?curid=33972172", "title": "Arab Health", "text": "Arab Health\n\nArab Health is a healthcare conference and trade show in the Middle East. It first took place in Dubai, United Arab Emirates in 1975. Healthcare industry representatives attend from the Middle East, Asia, Europe and the US. \n\nThe congress is one of the world's largest medical conferences.\n\nArab Health is supported by the UAE Ministry of Health, the Abu Dhabi Health Authority, the Dubai Health Authority and the Dubai Healthcare City Authority.\n\nAwards are given for innovation and achievement in the industry. They include awards for radiology, surgery, laboratory work, patient-centred care, and clinicians.\n\n"}
{"id": "16489856", "url": "https://en.wikipedia.org/wiki?curid=16489856", "title": "Atkinson Glacier", "text": "Atkinson Glacier\n\nAtkinson Glacier () is a glacier between Findlay Range and Lyttelton Range, Admiralty Mountains, flowing northward into Dennistoun Glacier. Named by the New Zealand Antarctic Place-Names Committee in 1983 after William Atkinson, field assistant, New Zealand Antarctic Division, mechanic with the New Zealand Antarctic Research Program (NZARP) geological party to the area, 1981–82, led by R.H. Findlay.\n\n"}
{"id": "29181459", "url": "https://en.wikipedia.org/wiki?curid=29181459", "title": "Bedding (animals)", "text": "Bedding (animals)\n\nBedding, in ethology and animal husbandry, is material, usually organic, used by animals to support their bodies when resting or otherwise stationary. It reduces pressure on skin, heat loss, and contamination by waste produced by an animal or those it shares living space with.\n\nWood Shavings (pine, cedar, and aspen) are absorbent and have good odor control. Different textures such as fine cut, soft shreds, or thick cut are used for different animals. Wood shavings can be dusty and contain aromatic oils that can cause respiratory, gastrointestinal, urinary tract, or skin disorders and other health problems in some animals. Aspen and kiln-dried wood shavings tend to be less dusty, plus the oils are removed.\n\nHemp Bedding is extremely absorbent and thus efficient, has good odor control and minimal dust, and provides more insulation than other bedding materials. Additionally, hemp is naturally pest-repellent and horses are not tempted to eat it. Due to its low dust, hemp bedding is recommended for horses with allergy or respiratory issues. From an environmental consideration, hemp is more sustainable than wood as it requires both less time and human intervention to grow repeatedly. \n\nCorncob Bedding contains no aromatic oils or dust. Corncobs are heat dried which makes it very absorbent. When water or urine is absorbed, the corncob will start molding so daily cleaning is needed. If not properly maintained, bacterial infections are likely to occur. Corncobs are cut to little pieces making it easy to ingest. This could be dangerous if ingested by a small animal. \n\nPaper Bedding includes either recycled paper or cardboard boxes. Paper bedding is ideal for animals with allergies since it contains no oils and little dust. Unlike corncob bedding, paper bedding has no adverse effects with consumption. Paper is very absorbent, but when saturated with water or urine, a strong odor results.\n\nStraw is a soft, dry stalks containing small grains such as barley, oats, rice, rye, and wheat. Straw is easy to handle and available in most agricultural areas. When deciding to use straw, is imperative to make sure that the straw is not palatable. To do this, the seed must be checked to ensure it is not available for consumption. Straw has excellent absorbency and is unlikely to mold.\n\nHay is composed of grasses that have been cut then dried. Although hay is most commonly used for food, it also can be used for the purpose of animal bedding. This may result in an insufficient diet for the animal if it begins eating its bedding. In addition, old hay may give off dust that could result in respiratory problems. A disadvantage of using hay is that it is one of the more expensive beddings. It is quite absorbent but once it is soiled, it begins to decompose quickly producing an unpleasant odor. Because of the moisture, hay will mold quickly and could result in a bacterial infection.\n\nWood Chips are a mixture of bark, sawdust, and post peelings. Agriculturalists use wood chips as a cost efficient bedding. While its cost may be desirable, woodchips provide minimal comfort and absorbency. Wood chips generate mold and mildew, because of its highly damp environment. This causes bacterial growth and potential infection if not changed often. Wood chips are also known to stain the coat of animals from bedding in the moist chips.\n\nSawdust must be kiln‐dried to ensure cleanliness and absorbability. Sawdust ensures quick and easy cleaning, because of the soiled or wet material begins to clump together. Although the cleanup is easier, sawdust is known to create a lot of health problems such as respiratory, urinary tract, mastitis, and skin disorders.\n\nSand is ideal when looking at microbial growth. Sand is the most comfortable bedding because of its natural ability to form to the animals curvature. However, large sand particles can cause abrasions and bruising on an animal. Another downfall is sand is not very absorbent and causes a difficult clean up of soiled materials.\n\nMany animals benefit from bedding, including livestock, poultry, rodents and reptiles. Bedding, in the most simplest context, provides comfort for these animals. When constructed properly, these cushioned structures decrease irritability and rough textures that can cause pain, pressure, and stress on the subject. This also prevents bruising and sores to preserve the physical beauty of the animal.\n\nBedding also creates an environment of moisture retention. Although incapable to prevent all microbial activity, bedding absorbs a substantial amount of moisture within the animal’s living environment. Soaking up excess urine, bedding assists in keeping the cage dry until the next change. A drier cage also promotes ventilation which decreases harmful levels of environmental pathogens.\n\nThese factors are few of many. With insulation against cold weather and drafts amongst the list, odor control sustains a position as well. Assistance in decreasing and filtering dust to protect against respiratory infections ranks high. Creating an environment conducive for thermoregulation and chemical resistance, along with simple privacy, bedding is ideal for many animal types. All these factors relay a decreases in stress and pain encouraging peaceful resting opportunities that intern increase the overall life, productivity, and well being of the animal.\n\nBedding maintenance is an important part of both human and animal health, cleanliness, and well being. Storage of bedding is important to insure that the bedding does not ruin. The best place to store it is in an environment that is dry and above ground level. Frequent bedding change is important to decrease the amount of bacteria.The most bacterial contaminated area is the front of the stall. This area should receive the most attention when cleaning and changing the bedding. Bedding should not be throughout the stall, it should be clear of the feeding and watering trough. It is recommended that the bedding is more frequently changed when there are a great number of animals, since the bedding will become contaminated faster. \n\nIt is important to note that weather, frequent bedding maintenance, barn design, ventilation, and stall management influence bacteria levels. When one or several of these things change or are not met the susceptibility of illness significantly increases. These illnesses are potentially deadly to these animals and that is why the maintenance of bedding is so important.\n"}
{"id": "3219275", "url": "https://en.wikipedia.org/wiki?curid=3219275", "title": "Blind taste test", "text": "Blind taste test\n\nIn marketing, a blind taste test is often used as a tool for companies to compare their brand to another brand. For example, the Pepsi Challenge is a famous taste test that has been run by Pepsi since 1975, as a method to show their superiority to Coca-Cola. Additionally, taste tests are sometimes used as a tool by companies to develop their brand or new products.\n\nBlind taste tests are ideal for goods such as food or wine that are consumed directly. Researchers use blind taste tests to obtain information about customers' perceptions and preferences on the goods. Blind taste test can be used to: \n\nBlind taste tests require a \"blind testing\" meaning the people taking the blind taste test are unaware of the identity of the brand being tested, or if done at home this can be as simple as a blindfold over the person taking the test. This means that any bias, preconceived ideas about a particular brand or food, is eliminated. The people taking the test will also be unaware of any changes done to the product.\n\nIn the famous Pepsi Challenge, they got people to take a sip from two different unlabelled glasses, not knowing which was Coke and which was Pepsi.\n\nThere are two types of blind taste tests:\n\nTaste tests are commonly employed by the public television show \"America's Test Kitchen\" and its spin-off series \"Cook's Country\", typically administered by Jack Bishop.\n"}
{"id": "50102483", "url": "https://en.wikipedia.org/wiki?curid=50102483", "title": "Calcium fructoborate", "text": "Calcium fructoborate\n\nCalcium fructoborate is an organic compound containing boron (and fructose and calcium). Its structural formula is Ca[(CHO)B]∙4HO.\n\nIt is naturally found in some plants, and is also manufactured and promoted as a dietary supplement.\n"}
{"id": "525519", "url": "https://en.wikipedia.org/wiki?curid=525519", "title": "Chemtrail conspiracy theory", "text": "Chemtrail conspiracy theory\n\nThe chemtrail conspiracy theory is based on the erroneous belief that long-lasting condensation trails are \"chemtrails\" consisting of chemical or biological agents left in the sky by high-flying aircraft, sprayed for nefarious purposes undisclosed to the general public. Believers in this conspiracy theory say that while normal contrails dissipate relatively quickly, contrails that linger must contain additional substances. Those who subscribe to the theory speculate that the purpose of the chemical release may be solar radiation management, weather modification, psychological manipulation, human population control, or biological or chemical warfare and that the trails are causing respiratory illnesses and other health problems.\n\nThe arguments have been dismissed by the scientific community. There is no evidence that purported chemtrails differ from normal water-based contrails routinely left by high-flying aircraft under certain atmospheric conditions. Although proponents have tried to prove that chemical spraying occurs, their analyses have been flawed or based on misconceptions. Because of the persistence of the conspiracy theory and questions about government involvement, scientists and government agencies around the world have repeatedly explained that the supposed chemtrails are in fact normal contrails.\n\nThe term \"chemtrail\" is a portmanteau of the words \"chemical\" and \"trail\", just as \"contrail\" is a portmanteau of \"condensation\" and \"trail\".\n\nChemtrail conspiracy theories began to circulate after the United States Air Force (USAF) published a 1996 report about weather modification. Following the report, in the late 1990s the USAF was accused of \"spraying the U.S. population with mysterious substances\" from aircraft \"generating unusual contrail patterns.\" The theories were posted on Internet forums by people including Richard Finke and William Thomas, and were among many conspiracy theories popularized by late-night radio host Art Bell, starting in 1999. As the chemtrail conspiracy theory spread, federal officials were flooded with angry calls and letters.\n\nA multi-agency response attempting to dispel the rumors was published in 2000 by the Environmental Protection Agency (EPA), the Federal Aviation Administration (FAA), the National Aeronautics and Space Administration (NASA) and the National Oceanic and Atmospheric Administration (NOAA). Many chemtrail believers interpreted agency fact sheets as further evidence of the existence of a government cover-up. The EPA refreshed its posting in 2015.\n\nIn the early 2000s the USAF released an undated fact sheet that stated the conspiracy theories were a hoax fueled in part by citations to a 1996 strategy paper drafted within their Air University titled \"Weather as a Force Multiplier: Owning the Weather in 2025\". The paper was presented in response to a military directive to outline a future strategic weather modification system for the purpose of maintaining the United States' military dominance in the year 2025, and identified as \"fictional representations of future situations/scenarios.\" The USAF further clarified in 2005 that the paper \"does not reflect current military policy, practice, or capability,\" and that it is \"not conducting any weather modification experiments or programs and has no plans to do so in the future.\" Additionally, the USAF states that the \"'Chemtrail' hoax has been investigated and refuted by many established and accredited universities, scientific organizations, and major media publications.\"\n\nThe conspiracy theories are seldom covered by the mainstream media, and when they are, they are usually cast as an example of anti-government paranoia. For example, in 2013, when it was made public that the CIA, NASA, and NOAA intended to provide funds to the National Academy of Sciences to conduct research into methods to counteract global warming with geoengineering, an article in the \"International Business Times\" anticipated that \"the idea of any government agency looking at ways to control, or manipulate, the weather will be met with scrutiny and fears of a malign conspiracies\" [\"sic\"], and mentioned chemtrail conspiracy theories as an example.\n\nProponents of the chemtrail conspiracy theory find support for their theories in their interpretations of sky phenomena, videos posted to the internet, and reports about government programs; they also have certain beliefs about the goals of the alleged conspiracy and the effects of its alleged efforts and generally take certain actions based on those beliefs.\n\nProponents of the chemtrail conspiracy theory say that chemtrails can be distinguished from contrails by their long duration, asserting that the chemtrails are those trails left by aircraft that persist for as much as a half day or transform into cirrus-like clouds. The proponents claim that after 1995 contrails had a different chemical composition and lasted a lot longer on the sky; proponents fail to acknowledge evidence of long-lasting contrails shown in World War II era photographs.\n\nProponents characterize contrails as streams that persist for hours and that, with their criss-cross, grid-like or parallel stripe patterns, eventually blend to form large clouds. Proponents view the presence of visible color spectra in the streams, unusual concentrations of sky tracks in a single area, or lingering tracks left by unmarked or military airplanes flying at atypical altitudes or locations as markers of chemtrails.\n\nPhotographs of barrels installed in the passenger space of an aircraft for flight test purposes have been claimed to show aerosol dispersion systems. The real purpose of the barrels is to simulate the weight of passengers or cargo. The barrels are filled with water, and the water can be pumped from barrel to barrel in order to test different centers of gravity while the aircraft is in flight.\n\nJim Marrs has cited a 2007 Louisiana television station report as evidence for chemtrails. In the report the air underneath a crosshatch of supposed chemtrails was measured and apparently found to contain unsafe levels of barium: at 6.8 parts per million, three times the US nationally recommended limit. A subsequent analysis of the footage showed, however, that the equipment had been misused, and the reading exaggerated by a factor of 100—the true level of barium measured was both usual and safe.\n\nIn May 2014 a video that went viral showed a commercial passenger airplane landing on a foggy night, which was described as emitting chemtrails. \"Discovery News\" pointed out that passengers sitting behind the wings would clearly see anything being sprayed, which would defeat any intent to be secretive, and that the purported chemical emission was normal air disruption caused by the wings, visible due to the fog. In October 2014, Englishman Chris Bovey filmed a video of a plane jettisoning fuel on a flight from Buenos Aires to London, which had to dump fuel to lighten its load for an emergency landing in São Paulo. The clip went viral on Facebook, with over three million views and more than 52,000 shares, cited as evidence of chemtrails. He later disclosed that the video post was done as a prank, and consequently he was subjected to some vitriolic abuse and threats from several conspiracy believers.\n\nIn some accounts, the chemicals are described as barium and aluminum salts, polymer fibers, thorium, or silicon carbide.\n\nChemtrail believers interpret the existence of cloud seeding programs and research into climate engineering as evidence for the conspiracy.\n\nVarious versions of the chemtrail conspiracy theory have been propagated via the Internet and radio programs. There are websites dedicated to the conspiracy theory, and it is particularly favored by right-wing groups because it fits well with deep suspicion of government.\n\nA 2014 review of 20 chemtrail websites found that believers appeal to science in some of their arguments, but do not believe what academic or government-employed scientists say; scientists and federal agencies have consistently denied that chemtrails exist, explaining the sky tracks are simply persistent contrails. The review also found that believers generally hold that chemtrails are evidence of a global conspiracy; they allege various goals which include profit (for example, manipulating futures prices or making people sick to benefit drug companies), population control, or weapons testing (use of weather as a weapon, or testing bioweapons). One of these ideas is that clouds are being seeded with electrically conductive materials as part of a massive electromagnetic superweapons program based around the High Frequency Active Auroral Research Program (HAARP). Believers say chemtrails are toxic; the 2014 review found that they generally hold that every person is under attack and often express fear, anxiety, sadness and anger about this. A 2011 study of people from the US, Canada, and the UK found that 2.6% of the sample believed entirely in the conspiracy theory, and 14% believed it partially. An analysis of responses given to the 2016 Cooperative Congressional Election Study showed that 9% of the 36,000 respondents believed it was \"completely true\" that \"...the government has a secret program that uses airplanes to put harmful chemicals into the air...\" while a further 19% believed this was \"somewhat true\".\n\nChemtrail conspiracy theorists often describe their experience as being akin to a religious conversion experience. When they \"wake up\" and become \"aware\" of chemtrails, the experience motivates them to advocacy of various forms. For example, they often attend events and conferences on geoengineering, and have sent threats to academics working in the geoengineering field.\n\nIn 2001 in response to requests from constituents, US Congressman Dennis Kucinich introduced (but did not author) H.R. 2977 (107th), the \"Space Preservation Act of 2001\" that would have permanently prohibited the basing of weapons in space, listing chemtrails as one of a number of \"exotic weapons\" that would be banned. Proponents have interpreted this explicit reference to chemtrails as official government acknowledgment of their existence. Skeptics note that the bill in question also mentions \"extraterrestrial weapons\" and \"environmental, climate, or tectonic weapons.\" The bill received an unfavorable evaluation from the United States Department of Defense and died in committee, with no mention of chemtrails appearing in the text of any of the three subsequent failed attempts by Kucinich to enact a Space Preservation Act.\n\nIn 2003, in a response to a petition by concerned Canadian citizens regarding \"chemicals used in aerial sprayings are adversely affecting the health of Canadians,\" the Government House Leader responded by stating, \"There is no substantiated evidence, scientific or otherwise, to support the allegation that there is high altitude spraying conducted in Canadian airspace. The term 'chemtrails' is a popularised expression, and there is no scientific evidence to support their existence.\" The house leader went on to say that \"it is our belief that the petitioners are seeing regular airplane condensation trails, or contrails.\"\n\nIn the United Kingdom, in 2005 Elliot Morley, a Minister of State for the Department for Environment, Food and Rural Affairs was asked \"what research [the] Department has undertaken into the polluting effects of chemtrails for aircraft,\" and responded that \"the Department is not researching into chemtrails from aircraft as they are not scientifically recognised phenomena,\" and that work was being conducted to understand \"how contrails are formed and what effects they have on the atmosphere.\"\n\nSome chemtrail believers adopt the notions of Wilhelm Reich (1897–1957) who devised a \"cloudbuster\" device from pipework. Reich claimed this device would influence weather and remove harmful energy from the atmosphere. Some chemtrail believers have built cloudbusters filled with crystals and metal filings, which are pointed at the sky in an attempt to clear it of chemtrails.\n\nChemtrail believers sometimes gather samples and have them tested, rather than rely on reports from government or academic laboratories, but their experiments are usually flawed; for example collecting samples in jars with metal lids contaminates the sample and is not done in scientific testing.\n\nContrails, or condensation trails, are \"streaks of condensed water vapor created in the air by an airplane or rocket at high altitudes.\" Fossil fuel combustion (as in piston and jet engines) produces carbon dioxide and water vapor. At high altitudes the air is very cold. Hot humid air from the engine exahust mixes with the colder surrounding air, causing the water vapor to condense into droplets or ice crystals that form visible clouds. The rate at which contrails dissipate is entirely dependent on weather conditions. If the atmosphere is near saturation, the contrail may exist for some time. Conversely, if the atmosphere is dry, the contrail will dissipate quickly.\nIt is well established by atmospheric scientists that contrails can persist for hours, and that it is normal for them to spread out into cirrus sheets. The different-sized ice crystals in contrails descend at different rates, which spreads the contrail vertically. Then the differential in wind speeds between altitudes (wind shear) results in horizontal spreading of the contrail. This mechanism is similar to the formation of cirrus uncinus clouds. Contrails between can often merge into an \"almost solid\" interlaced sheet. Contrails can have a lateral spread of several kilometers, and given sufficient air traffic, it is possible for contrails to create an entirely overcast sky that increases the ice budget of individual contrails and persists for hours.\nExperts on atmospheric phenomena say that the characteristics attributed to chemtrails are simply features of contrails responding to diverse conditions in terms of sunlight, temperature, horizontal and vertical wind shear, and humidity levels present at the aircraft's altitude. In the US, the gridlike nature of the National Airspace System's flight lanes tends to cause crosshatched contrails, and in general it is hard to discern from the ground whether overlapping contrails are at similar altitudes or not. The jointly published fact sheet produced by NASA, the EPA, the FAA, and NOAA in 2000 in response to alarms over chemtrails details the science of contrail formation, and outlines both the known and potential impacts contrails have on temperature and climate. The USAF produced a fact sheet that described these contrail phenomena as observed and analyzed since at least 1953. It also rebutted chemtrail theories more directly by identifying the theories as a hoax and disproving the existence of chemtrails.\n\nPatrick Minnis, an atmospheric scientist with NASA's Langley Research Center in Hampton, Virginia, has said that logic does not dissuade most chemtrail proponents: \"If you try to pin these people down and refute things, it's, 'Well, you're just part of the conspiracy',\" he said.\n\nAnalysis of the use of commercial aircraft tracks for climate engineering has shown them to be generally unsuitable.\n\nAstronomer Bob Berman has characterized the chemtrail conspiracy theory as a classic example of failure to apply Occam's razor, writing in 2009 that instead of adopting the long-established \"simple solution\" that the trails consist of frozen water vapour, \"the conspiracy web sites think the phenomenon started only a decade ago and involves an evil scheme in which 40,000 commercial pilots and air traffic controllers are in on the plot to poison their own children.\"\n\nA 2016 survey of 77 atmospheric scientists concluded that \"76 out of 77 (98.7%) of scientists that took part in this study said there was no evidence of a [secret large-scale atmospheric program (SLAP)], and that the data cited as evidence could be explained through other factors, such as typical contrail formation and poor data sampling instructions presented on SLAP websites.\"\n\n"}
{"id": "919325", "url": "https://en.wikipedia.org/wiki?curid=919325", "title": "China University of Political Science and Law", "text": "China University of Political Science and Law\n\nChina University of Political Science and Law (CUPL; simplified Chinese: 中国政法大学; traditional Chinese: 中國政法大學; pinyin: \"Zhōngguó Zhèngfǎ Dàxué\" abbr. 法大, \"Fǎ Dà\") is a national public research university specialized in law, arts, history, philosophy, economics, management and foreign languages established in 1952 in Beijing, China.\n\nChina University of Political Science and Law is a first tier institution within the national key university Project 211 and the Project 985 Innovative Platforms for Key Disciplines as part of the national endeavor to build world-class universities. CUPL is a Chinese Ministry of Education Double First Class Discipline University with the highest status \"A+\" in legal studies. It is widely considered to be one of the best Chinese universities in legal studies. It is also one of the most competitive and selective universities to enter in China.\n\nCUPL has two campuses, one in Haidian, Beijing which is the original campus of the university, and the other locates in Changping, Beijing. The university's Haidian campus now hosts postgraduate students only while undergraduates study at a much larger campus in Changping, Beijing. In 2015, CUPL comprises 13 schools, with 15,833 students and 951 faculty members, of whom 290 are professors. CUPL maintains a broad international exchange program, with approximately 1000 foreign students from many countries.\n\nCUPL was initially established in 1952, with its official name Peking College of Political Science and Law and combined departments of law, political science, sociology and other subjects of Peking University, Yenching University, Fu Jen Catholic University and Tsinghua University. The prestigious scholar Ch'ien Tuan-Sheng (钱端升), who was educated at Harvard University and regarded as the founder of modern political science in China, was appointed as the first president. But after the outbreak of Cultural Revolution CUPL was greatly affected. CUPL had been stopped during Cultural Revolution. Ch'ien died of illness in 1990.\n\nIn 1983, under the policy of Central People's Government to develop the college quickly and make it the center of politics and law education in China, BCPSL was renamed as the China University of Political Science and Law. The Changping campus was a part of the State's Seventh Five-Year-Plan in 1985.\n\nUnder the motto of \"Cherish the Moral, Understand the Law, Know the World, Serve the Public\"; CUPL made its contribution to the development of legal education and training in China. It was the first university to establish specialties such as Legal History, Civil and Commercial Law, Economic Law, Procedure Law and Comparative Law in the PRC. It also contributed to the education and promotion of Roman law in China. With over 100,000 graduates in the past 50 years who have become the elites of law enforcement and practitioners in China, CUPL has developed a niche for the enactment and enforcement of law in China. Furthermore, it became prominent in other public affairs in China, among which members of the faculty and student body played an active role in the Tiananmen Square protests of 1989 demanding democracy and rule of law.\n\nTo face the challenge of globalization, CUPL developed joint programs with international partners. CUPL provided the first opportunity to study Chinese law in Beijing with an American Bar Association-approved program inaugurated in 1995 by the Duquesne University School of Law. In 2008, an exchange program was formed with Fordham University School of Law.\n\nCUPL also maintains relationship with the University of Exeter and the University of Oxford in England, Deakin University in Australia, the University of Pennsylvania, Georgetown University, Washington University in St. Louis, the UIUC, the University of California, Berkeley, the University of California, Davis in the United States, the University of Montreal in Canada, and National University of Singapore Faculty of Law in Singapore.\n\nCUPL also offers two LLM programs and a PhD program for international students in English. One of them is based in Changping and is with the China-EU University. This is an EU funded course, and its main focus is on educating Chinese students on EU law.\n\nThere is another LLM program at Haidian campus, in downtown Beijing, which is aimed at International students to learn about Chinese law. There is also a PhD programs in English at Haidian campus. These are flexible and taught entirely in English. They include the option to study Mandarin, and gain law work experience in Beijing and other cities in China.\n\nThese postgraduate programs are also open as a semester program to international students from any institution around the world who would like to study at CUPL for one semester.\n\nThere are Chinese language programs for international students. These are 8,000 RMB per semester, or 16,000 RMB per year. Students can also stay in accommodation on campus which is 1,000 RMB per year.\n\n\n\n"}
{"id": "312229", "url": "https://en.wikipedia.org/wiki?curid=312229", "title": "Chiral anomaly", "text": "Chiral anomaly\n\nIn physics, a chiral anomaly is the anomalous nonconservation of a chiral current. In everyday terms, it is equivalent to a sealed box that contained equal number of positive and negative charged particles, that when opened was found to have more positive than negative particles, or vice versa.\n\nSuch events are expected to be prohibited according to classical conservation laws, but we know there must be ways they can be broken, because we have evidence of charge-parity non-conservation (\"CP violation\"). It is possible that other imbalances have been caused by breaking of a \"chiral law\" of this kind. Many physicists suspect that the fact that the observable universe contains more matter than antimatter is caused by a chiral anomaly [citation needed], although this observation does not itself rigorously establish that a chiral anomaly must exist. Research into chiral breaking laws is a major endeavor in particle physics research at this time.\n\nIn some theories of fermions with chiral symmetry, the quantization may lead to the breaking of this (global) chiral symmetry. In that case, the charge associated with the chiral symmetry is not conserved. The non-conservation happens in a tunneling process from one vacuum to another. Such a process is called an instanton.\n\nIn the case of a symmetry related to the conservation of a fermionic particle number, one may understand the creation of such particles as follows. The definition of a particle is different in the two vacuum states between which the tunneling occurs; therefore a state of no particles in one vacuum corresponds to a state with some particles in the other vacuum.\n\nIn particular, there is a Dirac sea of fermions and, when such a tunneling happens, it causes the energy levels of the sea fermions to gradually shift upwards for the particles and downwards for the anti-particles, or vice versa. This means particles which once belonged to the Dirac sea become real (positive energy) particles and particle creation happens.\n\nTechnically, an anomalous symmetry is a symmetry of the action formula_1, but not of the measure and therefore \"not\" of the generating functional formula_2 of the quantized theory (ℏ is Planck's action-quantum divided by 2π).\n\nThe measure consists of a part depending on the fermion field formula_3 and a part depending on its complex conjugate formula_4. The transformations of both parts under a chiral symmetry do not cancel in general. Note that if \"ψ\" is a Dirac fermion, then the chiral symmetry can be written as formula_5 where formula_6 is some matrix acting on \"ψ\".\n\nFrom the formula for formula_7 one also sees explicitly that in the classical limit, ℏ → 0, anomalies don't come into play, since in this limit only the extrema of formula_1 remain relevant.\n\nThe anomaly is proportional to the instanton number of a gauge field to which the fermions are coupled (note that the gauge symmetry is always non-anomalous and is exactly respected, as is required by the consistency of the theory).\n\nThe chiral anomaly can be calculated exactly by one-loop Feynman diagrams, e.g. the famous \"triangle diagram\", contributing to the pion decays, formula_9 and formula_10.\n\nThe amplitude for this process can be calculated directly from the change in the measure of the fermionic fields under the chiral transformation.\n\nWess and Zumino developed a set of conditions on how the partition function ought to behave under gauge transformations called the Wess-Zumino consistency conditions.\n\nFujikawa derived this anomaly using the correspondence between functional determinants and the partition function using the Atiyah-Singer index theorem. See Fujikawa's method.\n\nThe Standard Model of electroweak interactions has all the necessary ingredients for successful baryogenesis, although these interactions have never been observed and may be insufficient to explain the total baryon number of observed universe if the initial baryon number of the universe at the time of the Big Bang is zero. Beyond the violation of charge conjugation formula_11 and CP violation formula_12 (charge+parity), baryonic charge violation appears through the Adler–Bell–Jackiw anomaly of the formula_13 group.\n\nBaryons are not conserved by the usual electroweak interactions due to quantum chiral anomaly. The classic electroweak Lagrangian conserves baryonic charge. Quarks always enter in bilinear combinations formula_14, so that a quark can disappear only in collision with an antiquark. In other words, the classical baryonic current formula_15 is conserved:\n\nHowever, quantum corrections known as the sphaleron destroy this conservation law: instead of zero in the right hand side of this equation, there is a non vanishing quantum term,\nwhere is a numerical constant vanishing for ℏ =0,\nand the gauge field strength formula_19 is given by the expression\n\nElectroweak sphalerons can only change the baryon and/or lepton number by 3 or multiples of 3 (collision of three baryons into three leptons/antileptons and vice versa).\n\nAn important fact is that the anomalous current non-conservation is proportional to the total derivative of a vector operator, formula_21 (this is non-vanishing due to instanton configurations of the gauge field, which are pure gauge at the infinity), where the anomalous current formula_22 is\nwhich is the Hodge dual of the Chern-Simons 3-form.\n\n\n\n\n"}
{"id": "2167852", "url": "https://en.wikipedia.org/wiki?curid=2167852", "title": "Cirrus uncinus cloud", "text": "Cirrus uncinus cloud\n\nCirrus uncinus is a type of cirrus cloud. The name \"cirrus uncinus\" is derived from Latin, meaning \"curly hooks\". Also known as \"mares' tails\", these clouds are generally sparse in the sky and very thin.\n\nThe clouds occur at high altitudes, at a temperature of about . They are generally seen when a warm or occluded front is approaching. They are very high in the troposphere and generally mean that precipitation, usually rain, is approaching.\n\n\n"}
{"id": "23011904", "url": "https://en.wikipedia.org/wiki?curid=23011904", "title": "Complementary cells", "text": "Complementary cells\n\nComplementary cells are a mass of cells in plants, formed from the cork cambium at the position of the lenticels. It is a group of loosely arranged cells that aid in gaseous exchange through cork.\n"}
{"id": "33644123", "url": "https://en.wikipedia.org/wiki?curid=33644123", "title": "Conceptions of Library and Information Science", "text": "Conceptions of Library and Information Science\n\nConceptions of Library and Information Science (CoLIS) is a series of conferences about historical, empirical and theoretical perspectives in Library and Information Science.\n\n"}
{"id": "50565707", "url": "https://en.wikipedia.org/wiki?curid=50565707", "title": "Continuous analytics", "text": "Continuous analytics\n\nContinuous analytics is a data science process that abandons ETLs and complex batch data pipelines in favor of cloud-native and microservices paradigms. Continuous data processing enables realtime interactions and immediate insights with fewer resources.\n\nAnalytics is the application of mathematics and statistics to big data. Data scientists write analytics programs to look for solutions to business problems, like forecasting demand or setting an optimal price.\n\nThe continuous approach runs multiple stateless engines which concurrently enrich, aggregate, infer and act on the data. Data scientists, dashboards and client apps all access the same raw or real-time data derivatives with proper identity-based security, data masking and versioning in real-time. \n\nTraditionally data scientists have not been part of IT development teams, like regular Java programmers. This is because their skills set them apart in their own department not normally related to IT, i.e., math, statistics, and data science. So it is logical to conclude that their approach to writing software code does not enjoy the same efficiencies as the traditional programming team. In particular traditional programming has adopted the Continuous Delivery approach to writing code and the agile methodology. That releases software in a continuous circle, called iterations.\n\nContinuous analytics then is the extension of the continuous delivery software development model to the big data analytics development team. The goal of the continuous analytics practitioner then is to find ways to incorporate writing analytics code and installing big data software into the agile development model of automatically running unit and functional tests and building the environment system with automated tools.\n\nTo make this work means getting data scientists to write their code in the same code repository that regular programmers use so that software can pull it from there and run it through the build process. It also means saving the configuration of the big data cluster (sets of virtual machines) in some kind of repository as well. That facilitates sending out analytics code and big data software and objects in the same automated way as the continuous integration process.\n"}
{"id": "16571351", "url": "https://en.wikipedia.org/wiki?curid=16571351", "title": "Dark fluid", "text": "Dark fluid\n\nIn astronomy and cosmology, dark fluid is an alternative theory to both dark matter and dark energy and attempts to explain both phenomena in a single framework.\n\nDark fluid proposes that dark matter and dark energy are not separate physical phenomena as previously thought, nor do they have separate origins, but that they are strongly linked together and can be considered as two facets of a single fluid. At galactic scales, the dark fluid behaves like dark matter, and at larger scales its behavior becomes similar to dark energy. Our observations within the scales of the Earth and the Solar System are currently insufficient to explain the gravitational effects observed at such larger scales.\n\nTwo major conundrums have arisen in astrophysics and cosmology in recent times, both dealing with the laws of gravity. The first was the realization that there aren't enough visible stars or gas inside galaxies to account for their high rate of rotation. The theory of dark matter was created to explain this phenomenon. It theorizes that the galaxies are spinning as fast as they are because there is more matter in those galaxies (including our own Milky Way) than can be seen by counting the mass of stars and gas alone, and that this unseen (dark) matter is invisible because it doesn't interact with the electromagnetic force from which all forms of light come.\n\nThe second conundrum came from the observations of a very specific kind of supernova, known as a Type Ia supernova used as a standard candle: when they were compared in distant vs. nearby galaxies, it was found that the distant supernova were fainter, and thus farther away than expected. This implied that the Universe was not only expanding, but accelerating its expansion. The theory of dark energy was created to explain this phenomenon.\n\nThe traditional approach to modeling the effects of gravity assumes that general relativity is as valid at cosmological scales as it is in the Solar System, where its predictions have been more accurately tested. Not changing the rules of gravity, however, implies the presence of dark matter and dark energy in parts of the Universe where the curvature of the space-time manifold is far less than that in the Solar System. It is phenomenologically possible to alter the equations of gravity in regions of low space-time curvature such that the dynamics of the space-time causes what we assign to the presence of dark matter and dark energy. Dark fluid theory hypothesizes that the dark fluid is a specific kind of fluid whose attractive and repulsive behaviors depend on the local energy density. In this theory, the dark fluid behaves like dark matter in the regions of space where the baryon density is high. The idea is that when the dark fluid is in the presence of matter, it slows down and coagulates around it; this then attracts more dark fluid to coagulate around it, thus amplifying the force of gravity near it. The effect is always present, but only becomes noticeable in the presence of a very large mass such as a galaxy. This description is similar to theories of dark matter, and a special case of the equations of dark fluid reproduces dark matter.\n\nOn the other hand, in places where there is relatively little matter, as in the voids between galactic superclusters, this theory predicts that the dark fluid relaxes and acquires a negative pressure. Thus dark fluid becomes a repulsive force, with an effect similar to that of dark energy.\n\nDark fluid goes beyond dark matter and dark energy in that it predicts a continuous range of attractive and repulsive qualities under various matter density cases. Indeed, special cases of various other gravitational theories are reproduced by dark fluid, e.g. inflation, quintessence, k-essence, f(R), Generalized Einstein-Aether f(K), MOND, TeVeS, BSTV, etc. Dark fluid theory also suggests new models, such as a certain f(K+R) model that suggests interesting corrections to MOND that depend on redshift and density.\n\nDark fluid theory is not treated like a standard fluid mechanics model, because many of the fluid mechanics equations are too difficult to solve completely. A formalized fluid mechanical approach, like the generalized Chaplygin gas model, would be an ideal method for modeling this theory, but it currently requires too many observational data points to be computationally feasible, and not enough such data points are available to cosmologists yet. A simplification step was undertaken by modeling the theory through scalar field models instead, as is done in other alternative approaches to dark energy and dark matter.\n\n"}
{"id": "1884317", "url": "https://en.wikipedia.org/wiki?curid=1884317", "title": "Dual economy", "text": "Dual economy\n\nA dual economy is the existence of two separate economic sectors within one country, divided by different levels of development, technology, and different patterns of demand. The concept was originally created by Julius Herman Boeke to describe the coexistence of modern and traditional economic sectors in a colonial economy.\n\nDual economies are common in less developed countries, where one sector is geared to local needs and another to the global export market. Dual economies may exist within the same sector, for example a modern plantation or other commercial agricultural entity operating in the midst of traditional cropping systems. Sir Arthur Lewis used the concept of a dualistic economy as the basis of his labour supply theory of rural-urban migration. Lewis distinguished between a rural low-income subsistence sector with surplus population, and an expanding urban capitalist sector (see Dual-sector model). The urban economy absorbed labour from rural areas (holding down urban wages) until the rural surplus was exhausted.\n\nA World Bank comparison of sectoral growth in Côte d'Ivoire, Ghana and Zimbabwe since 1965 provided evidence against the existence of a basic dual economy model. The research implied that a positive link existed between growth in industry and growth in agriculture. The authors argued that for maximum economic growth, policymakers should have focused on agriculture and services as well as industrial development.\n\n\n"}
{"id": "31556181", "url": "https://en.wikipedia.org/wiki?curid=31556181", "title": "Dérive (magazine)", "text": "Dérive (magazine)\n\ndérive – Zeitschrift für Stadtforschung is an Austrian science magazine on urbanism.\n\n\"dérive\" is published quarterly since 2000 by the Vienna based \"Verein für Stadtforschung\".\n\nThe journal publishes articles from a broad range of urbanism disciplines such as architecture, urban and land-use planning, art, geography, sociology, or philosophy. Articles from urban sociology include contributions from Loïc Wacquant and Saskia Sassen.\n\nDérive is a concept of psychogeography that includes unplanned journeys through urban space. The individual travels where the subtle aesthetic contours of the surrounding architecture and geography subconsciously direct them, with the ultimate goal of encountering an entirely new and authentic experience.\n\nThe magazine is part of the European network of cultural magazines Eurozine.\n\nThe journal also hosts the annual festival urbanize on urban issues.\n\n"}
{"id": "27354368", "url": "https://en.wikipedia.org/wiki?curid=27354368", "title": "Ellison–Cliffe Lecture", "text": "Ellison–Cliffe Lecture\n\nThe Ellison–Cliffe Lecture is held annually by the Royal Society of Medicine. The lectures, which commenced in 1987, are named after Dr. Percy Cliffe and his wife, Dr. Carice Ellison, who endowed the lecture to be given on a subject connected with the contribution of fundamental science to the advancement of medicine.\n\nThe Lecturer is also awarded a medal in honour of their presentation.\n\n"}
{"id": "19631083", "url": "https://en.wikipedia.org/wiki?curid=19631083", "title": "Explorer 54", "text": "Explorer 54\n\nExplorer 54, also called as AE-D (Atmospheric Explorer D), was a NASA scientific satellite belonging to series Atmosphere Explorer, being launched on October 6, 1975 from Vandenberg AFB board a Delta 2910 rocket. \n\nThe purpose of the Explorer 54 was to continue the investigation begun by Explorer 51 (AE-C) of the chemical processes and energy transfer mechanisms that control the structure and behavior of the earth's atmosphere and ionosphere in the region of high absorption of solar energy. This mission was planned to sample the high latitude regions at the same time that the Explorer 55 (AE-E) mission was sampling the equatorial and low latitude regions. The same type of spacecraft as Explorer 54 was used, and the payload consisted of the same types of instruments except for deletion of the extreme solar UV monitor and the Bennett ion mass spectrometer, which were part of the Explorer 55 payload. \n\nThe polar orbit provided the sampling of all latitudes and the perigee moved through all latitudes in 3 months and all local times in 4 months. Unfortunately, a failure in the solar power panels resulted in the termination of operations on 29 January 1976, after slightly less than 4 months of useful life. However, all the regions at the perigee altitudes were sampled during this time. The spacecraft re-entered the atmosphere about 1 month after cessation of telemetry. To continue the correlated observations with the Explorer 55 mission, Explorer 51 was reactivated on 28 February 1976, to replace Explorer 54.\n"}
{"id": "49641893", "url": "https://en.wikipedia.org/wiki?curid=49641893", "title": "Ferraioloite", "text": "Ferraioloite\n\nFerraioloite is a rare mineral with formula MgMn(FeAl)Zn(PO)(OH)(HO). It is related to the phosphate mineral falsterite. Ferraioloite was found in pegmatites of the Foote Lithium Company Mine, Cleveland County, North Carolina, US. The name honors James (Jim) A. Ferraiolo (1947–2014).\n"}
{"id": "270054", "url": "https://en.wikipedia.org/wiki?curid=270054", "title": "Formal verification", "text": "Formal verification\n\nIn the context of hardware and software systems, formal verification is the act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics.\n\nFormal verification can be helpful in proving the correctness of systems such as: cryptographic protocols, combinational circuits, digital circuits with internal memory, and software expressed as source code.\n\nThe verification of these systems is done by providing a formal proof on an abstract mathematical model of the system, the correspondence between the mathematical model and the nature of the system being otherwise known by construction. Examples of mathematical objects often used to model systems are: finite state machines, labelled transition systems, Petri nets, vector addition systems, timed automata, hybrid automata, process algebra, formal semantics of programming languages such as operational semantics, denotational semantics, axiomatic semantics and Hoare logic.\n\nOne approach and formation is model checking, which consists of a systematically exhaustive exploration of the mathematical model (this is possible for finite models, but also for some infinite models where infinite sets of states can be effectively represented finitely by using abstraction or taking advantage of symmetry). Usually this consists of exploring all states and transitions in the model, by using smart and domain-specific abstraction techniques to consider whole groups of states in a single operation and reduce computing time. Implementation techniques include state space enumeration, symbolic state space enumeration, abstract interpretation, symbolic simulation, abstraction refinement. The properties to be verified are often described in temporal logics, such as linear temporal logic (LTL), Property Specification Language (PSL), SystemVerilog Assertions (SVA), or computational tree logic (CTL). The great advantage of model checking is that it is often fully automatic; its primary disadvantage is that it does not in general scale to large systems; symbolic models are typically limited to a few hundred bits of state, while explicit state enumeration requires the state space being explored to be relatively small.\n\nAnother approach is deductive verification. It consists of generating from the system and its specifications (and possibly other annotations) a collection of mathematical \"proof obligations\", the truth of which imply conformance of the system to its specification, and discharging these obligations using either interactive theorem provers (such as HOL, ACL2, Isabelle, Coq or PVS), automatic theorem provers, or satisfiability modulo theories (SMT) solvers. This approach has the disadvantage that it typically requires the user to understand in detail why the system works correctly, and to convey this information to the verification system, either in the form of a sequence of theorems to be proved or in the form of specifications of system components (e.g. functions or procedures) and perhaps subcomponents (such as loops or data structures).\n\nFormal verification of software programs involves proving that a program satisfies a formal specification of its behavior. Subareas of formal verification include deductive verification (see above), abstract interpretation, automated theorem proving, type systems, and lightweight formal methods. A promising type-based verification approach is dependently typed programming, in which the types of functions include (at least part of) those functions' specifications, and type-checking the code establishes its correctness against those specifications. Fully featured dependently typed languages support deductive verification as a special case.\n\nAnother complementary approach is program derivation, in which efficient code is produced from functional specifications by a series of correctness-preserving steps. An example of this approach is the Bird–Meertens formalism, and this approach can be seen as another form of correctness by construction.\n\nThese techniques can be \"sound\", meaning that the verified properties can be logically deduced from the semantics, or \"unsound\", meaning that there is no such guarantee. A sound technique yields a result only once it has searched the entire space of possibilities. An example of an unsound technique is one that searches only a subset of the possibilities, for instance only integers up to a certain number, and give a \"good-enough\" result. Techniques can also be \"decidable\", meaning that their algorithmic implementations are guaranteed to terminate with an answer, or undecidable, meaning that they may never terminate. Because they are bounded, unsound techniques are often more likely to be decidable than sound ones.\n\nVerification is one aspect of testing a product's fitness for purpose. Validation is the complementary aspect. Often one refers to the overall checking process as V & V.\n\n\nThe verification process consists of static/structural and dynamic/behavioral aspects. E.g., for a software product one can inspect the source code (static) and run against specific test cases (dynamic). Validation usually can be done only dynamically, i.e., the product is tested by putting it through typical and atypical usages (\"Does it satisfactorily meet all use cases?\").\n\nProgram repair is performed with respect to an oracle, encompassing the desired functionality of the program which is used for validation of the generated fix. A simple example is a test-suite—the input/output pairs specify the functionality of the program. A variety of techniques are employed, most notably using satisfiability modulo theories (SMT) solvers, and genetic programming, using evolutionary computing to generate and evaluate possible candidates for fixes. The former method is deterministic, while the latter is randomized.\n\nProgram repair combines techniques from formal verification and program synthesis. Fault-localization techniques in formal verification are used to compute program points which might be possible bug-locations, which can be targeted by the synthesis modules. Repair systems often focus on a small pre-defined class of bugs in order to reduce the search space. Industrial use is limited owing to the computational cost of existing techniques.\n\nThe growth in complexity of designs increases the importance of formal verification techniques in the hardware industry. At present, formal verification is used by most or all leading hardware companies, but its use in the software industry is still languishing. This could be attributed to the greater need in the hardware industry, where errors have greater commercial significance. Because of the potential subtle interactions between components, it is increasingly difficult to exercise a realistic set of possibilities by simulation. Important aspects of hardware design are amenable to automated proof methods, making formal verification easier to introduce and more productive.\n\n, several operating systems have been formally verified:\nNICTA's Secure Embedded L4 microkernel, sold commercially as seL4 by OK Labs; OSEK/VDX based real-time operating system ORIENTAIS by East China Normal University; Green Hills Software's Integrity operating system; and SYSGO's PikeOS.\n\nAs of 2017, formal verification has been applied to the design of large computer networks through a mathematical model of the network, and as part of a new network technology category, intent-based networking. Network software vendors that offer formal verification solutions include Cisco Forward Networks and Veriflow Systems.\n\nThe CompCert C compiler is a formally verified C compiler implementing the majority of ISO C.\n\n"}
{"id": "21099086", "url": "https://en.wikipedia.org/wiki?curid=21099086", "title": "Global Engineering Education", "text": "Global Engineering Education\n\nGlobal Engineering Education is a field of study that focuses on the impact of globalization on the engineering industry.\n\nOver the past decade or so educators and researchers have made an effort to transform engineering education in light of global trends in the profession. In 1985, the National Research Council issued a study that spotlighted the need for universities to graduate engineers with professional skills. This message was reinforced through a 1994 joint report published by the Engineering Deans Council and ASEE that stated, “Today, engineering colleges … must educate their students to work as part of teams, communicate well, and understand the economic, social, environmental and international context of their professional activities.”\n\nGlobal competency is essential for engineers from any country who now compete in an international market for engineering know-how. No longer is cultural sensitivity needed only for product design destined for diverse markets. Increasingly, successful entry into the engineering profession requires significant intercultural skills in order to join efficient and productive collaborations with diverse engineering colleagues. Those colleagues may be encountered “virtually” at a distance, in person at an international site, or next door in the office of a multinational corporation. Outsourcing is increasing, not only for products but also for processes, including highly technical engineering work. Projects are distributed across sites and effective collaboration requires professionals who can work productively with colleagues who are very different from themselves.\n\nIEEE Education Society belongs to the IEEE (Institute of Electrical and Electronics Engineers) and its major objective is to strive a global leader in engineering education. The organization of education engineering worldwide conferences as they are the EDUCON and the FIE - Frontiers in Education Conference with an open view of all the publications included there as well the integration of engineering educator leaders along all the world in the Administration Committee allows a world integration of different views and perspectives. Newly publication IEEE-RITA (Revista Iberoamericana de Tecnologías del Aprendizaje - Revista Iberoamericana de Tecnologías de Aprendizagem - Latin-American Learning Technologies Journal) incorporates a new vision of technology and engineering education in Spanish and Portuguese languages.\n\nThese are some organizations around the world that focus on global engineering education\n\n"}
{"id": "50783328", "url": "https://en.wikipedia.org/wiki?curid=50783328", "title": "Glossary of mechanical engineering", "text": "Glossary of mechanical engineering\n\n\"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\"\n\nThis glossary of mechanical engineering terms pertains specifically to mechanical engineering and its sub-disciplines. For a broad overview of engineering, see glossary of engineering. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "37849924", "url": "https://en.wikipedia.org/wiki?curid=37849924", "title": "Hercules A", "text": "Hercules A\n\nHercules A is a bright astronomical radio source within the vicinity of the constellation Hercules corresponding to the galaxy 3C 348.\n\nDuring a survey of bright radio sources in the mid-20th century, astronomers found a very bright radio source in the constellation Hercules. The radio source is strongest in the middle range frequency and emits synchrotron radiation, suggesting the source of radio emission may be gravitational interaction. In 1959, astronomers from the Radio Astronomy Group (later the Cavendish Astrophysics Group) detected the radio source using the Cambridge Interferometer of the Cavendish Observatory in Cambridge University in United Kingdom, including it in the Third Cambridge Catalogue of Radio Sources (3C) as 3C 348, the 348th object detected by the survey.\n\nThe galaxy, 3C 348, is a supergiant elliptical galaxy. It is classified as type E3 to E4 of the updated Hubble-de Vaucouleurs extended galaxy morphological classification scheme. Little else is known about the galaxy.\n\n3C 348, the galaxy at the image center, appears to be a relatively normal elliptical galaxy in visible light. When imaged in radio waves, however, plasma jets over one million light years long appear. Detailed analyses indicate that the galaxy is actually over 1,000 times more massive (approx. 10 solar masses) than our Milky Way Galaxy, and the central black hole is nearly 1,000 times more massive (approx. 4 billion solar masses) than the black hole at our Milky Way's center, one of the largest known. The physics that creates the jets is poorly understood, with a likely energy source being matter ejected perpendicular to the accretion disc of the central black hole.\n\n"}
{"id": "45233200", "url": "https://en.wikipedia.org/wiki?curid=45233200", "title": "Huxley (Martian crater)", "text": "Huxley (Martian crater)\n\nHuxley is a crater in the Hellas quadrangle of Mars, located at 63.0°S latitude and 259.2°W longitude. It is 107.0 km in diameter and was named after Thomas Henry Huxley, and the name was approved in 1973 by the International Astronomical Union (IAU) Working Group for Planetary System Nomenclature (WGPSN).\n\nHuxley is due south of the larger Secchi, other nearby named craters include Redi to the northwest, Laylá to the northeast and the Heinlein-Weinbaum crater pair to the east-southeast, no nearby named craters are located south but another crater named Gilbert is in the southwest.\n\n"}
{"id": "9226972", "url": "https://en.wikipedia.org/wiki?curid=9226972", "title": "International Union of Nutritional Sciences", "text": "International Union of Nutritional Sciences\n\nThe International Union of Nutritional Sciences (IUNS) is an international non-governmental organization established in 1948 to devote the advancement of nutrition. Since its 1948 foundation, the membership has grown to include 82 national adhering bodies and 16 affiliations.\n\nThe Council consists of 5 Officers, the President, President-Elect, Vice-President, Secretary-General, Treasurer, Immediate Past-President, and six Council members. IUNS's current council consists of the following:\n- President: Anna Lartey Ghana;\n- Vice President: V. Prakash India;\n- President Elect: Alfredo Martinez Hernandez Spain;\n- Secretary General: Catherine Geissler UK;\n- Treasurer: Helmut Heseker Germany;\n- Member: Lynette Neufeld Canada;\n- Member: S. K. Roy Bangladesh;\n- Member: Godwin D. Ndossi Tanzania;\n- Member: Andrew Prentice UK;\n- Member: Teruo Miyazawa Japan;\n- Member: Reynaldo Martorell United States\n- Immediate Past-President: Ibrahim Elmadfa Austria;\n- Immediate Past Secretary General: Rekia Belahsen Morocco;\n\nIUNS is registered in Vienna, Austria.\n\nIUNS;\nThe Nutrition Society;\n10 Cambridge Court;\n210 Shepherds Bush Road;\nLondon;\nUK;\nW6 7NJ\n\n"}
{"id": "1018068", "url": "https://en.wikipedia.org/wiki?curid=1018068", "title": "Jan Kazimierz Danysz", "text": "Jan Kazimierz Danysz\n\nJean Danysz (11 March 1884 – 4 November 1914) born Jan Kazimierz Danysz, was a French physicist of Polish extraction. He was an assistant of Maria Skłodowska-Curie and notable in the development of beta spectrometry.\n\nDanysz made considerable advances on the magnetic deflection techniques of Baeyer, Hahn and Meitner, placing the source (he used radium) in a capillary tube under a slit, with a photographic plate in the same horizontal plane. By this means the known number of lines (later understood to be conversion lines) superimposed on the beta energy spectrum of RaB + RaC went from 9 to 27 (later work by Robinson and Rutherford found 64; 16 from RaB and 48 from RaC). He finished his doctoral thesis in 1913, and by 1914 he was considered by Rutherford as a leading researcher into beta decay, but he did no further work. He enlisted in the French army in 1914 and was killed in action near Cormicy.\n\nJ. Danysz, Le Radium 9, 1 (1912); 10, 4 (1913)\n\nDanysz, J. \"Recherches expérimentales sur les β rayons de la famille du radium\" Ann. Chim. Phys. 30 (1913) 241–320\n\n"}
{"id": "17019451", "url": "https://en.wikipedia.org/wiki?curid=17019451", "title": "Keisuke Ito", "text": "Keisuke Ito\n\nAs a doctor, Ito developed a vaccination against smallpox. He also widely studied the Japanese flora and fauna with Philipp Franz von Siebold, the author of \"Fauna Japonica\" and \"Flora Japonica\". \"Rhododendron keiskei\" has been named after him.\nHe wrote \"Taisei honzou meiso\" (Japanese:\"泰西本草名疏\") published in 1829.\n\nIto became a professor at the University of Tokyo in 1881.\n\nHe died in 1901, and he was ennobled with the title of baron (danshaku).\n"}
{"id": "8999017", "url": "https://en.wikipedia.org/wiki?curid=8999017", "title": "Laboratory specimen", "text": "Laboratory specimen\n\nIn medicine, a laboratory specimen is a biological specimen taken by sampling, that is, gathered matter of a medical patient's tissue, fluid, or other material derived from the patient used for laboratory analysis to assist differential diagnosis or staging of a disease process. Common examples include throat swabs, sputum, urine, blood, surgical drain fluids, and tissue biopsies.\n"}
{"id": "54010164", "url": "https://en.wikipedia.org/wiki?curid=54010164", "title": "Lebanese Marine and Wildlife Museum", "text": "Lebanese Marine and Wildlife Museum\n\nThe Lebanese Marine and Wildlife Museum (, al-\"matḥaf al-lubnani lil-hayat al-bahriya wa al-bariya\") is a zoological museum in Jeita, and is one of the largest such museums in the middle east. Over 90% of the specimens in the museum were collected from Lebanon by Dr. Jamal Younes (president and owner of the museum).\n\nIts main goals are to study Lebanon's ecosystem, encourage preservation, and maintain a scientific archive of Lebanon's wildlife and the Mediterranean marine life.\n\nThe Museum was first opened in Tyre in 2001 by Dr. Jamal Younes, who has been collecting and preserving specimens for more than 30 years. After the collection grew too large, there was no more space for some specimens, so it was decided to construct a new museum large enough to display all the specimens in Jeita.\n\nBorn and raised in Tyre, a peninsula known for its rich history and ties to the sea, and distinguished for having both sandy and rocky beaches, creating a unique and rich marine ecosystem.\n\nWhile studying Dentistry in Europe, he also learned taxidermy. After his graduation, he opening a clinic for orthodontics in Tyre, where he would go out diving every night, most of the times on his own. Looking to find something new to add to his collection and as time passed he amassed the biggest collection of taxidermy marine and wildlife in the Middle East, which was the basis of his Museum which today contains more than 5000 specimens.\n\nThe Lebanese Marine and Wildlife Museum has over 2000 species with over 5000 specimens on display. The museum currently contains 6 Exhibits which show off Lebanon's rich and diverse ecosystems, which are:\n\nDisplaying over 30 of Lebanon's mammals which once roamed freely the forests of Lebanon, most of which are now critically endangered, due to deforestation, indiscriminate hunting and habitat destruction.\n\nThe Museum Has over 200 species of birds, both native and migratory\n\nover 20 species of lizards and 30 species of snakes collected from across Lebanon are on display\n\nover 300 specimens of minerals and gems from around the world\n\nThe seashell collections contains over 200 seashell species from the Mediterranean and over 300 of the largest seashells from around the world.\n\nThis huge display contains over 40 species sharks, one of them is a 7m basking shark, in additions to hundreds of fish, crabs, cephalopods, crustaceans, sea turtles, dolphins and a rare monk seal.\n\nThe museum is located in Jeita along Jieta Grotto road, 800m before Jeita Grotto.\n\n"}
{"id": "56312051", "url": "https://en.wikipedia.org/wiki?curid=56312051", "title": "List of original films distributed by Netflix", "text": "List of original films distributed by Netflix\n\nNetflix is an American global on-demand Internet streaming media provider, that has distributed a number of original programs, including original series, specials (including stand-up comedy specials), miniseries, documentaries, and films. Netflix's original films also include content that was first screened in other countries for exclusive broadcast in other territories, which is branded in those regions as Netflix original content.\n\nThe following projects have all been announced as being in development, but do not have a specific release date known at this time.\n\nThese feature presentations have been acquired by Netflix for release as Netflix original films in certain territories.\n"}
{"id": "467022", "url": "https://en.wikipedia.org/wiki?curid=467022", "title": "List of protected areas of Bosnia and Herzegovina", "text": "List of protected areas of Bosnia and Herzegovina\n\nNational parks of Bosnia and Herzegovina\nNature parks of Bosnia and Herzegovina\n\n"}
{"id": "7119912", "url": "https://en.wikipedia.org/wiki?curid=7119912", "title": "List of volcanoes in Argentina", "text": "List of volcanoes in Argentina\n\nThis is a list of active and extinct volcanoes in Argentina.\n"}
{"id": "7120000", "url": "https://en.wikipedia.org/wiki?curid=7120000", "title": "List of volcanoes in El Salvador", "text": "List of volcanoes in El Salvador\n\nThis is a list of active and extinct volcanoes in El Salvador. \n\n"}
{"id": "547827", "url": "https://en.wikipedia.org/wiki?curid=547827", "title": "Loss aversion", "text": "Loss aversion\n\nIn cognitive psychology and decision theory, loss aversion refers to people's tendency to prefer avoiding losses to acquiring equivalent gains: it is better to not lose $5 than to find $5. The principle is very prominent in the domain of economics. What distinguishes loss aversion from risk aversion is that the utility of a monetary payoff depends on what was previously experienced or was expected to happen. Some studies have suggested that losses are twice as powerful, psychologically, as gains. Loss aversion was first identified by Amos Tversky and Daniel Kahneman.\n\nLoss aversion implies that one who loses $100 will lose more satisfaction than another person will gain satisfaction from a $100 windfall. In marketing, the use of trial periods and rebates tries to take advantage of the buyer's tendency to value the good more after the buyer incorporates it in the status quo. In past behavioral economics studies, users participate up until the threat of loss equals any incurred gains. Recent methods established by Botond Kőszegi and Matthew Rabin in experimental economics illustrates the role of expectation, wherein an individual's belief about an outcome can create an instance of loss aversion, whether or not a tangible change of state has occurred.\n\nNote that whether a transaction is framed as a loss or as a gain is very important to this calculation: would you rather get a $5 discount, or avoid a $5 surcharge? The same change in price framed differently has a significant effect on consumer behavior. Although traditional economists consider this \"endowment effect\" and all other effects of loss aversion to be completely irrational, that is why it is so important to the fields of marketing and behavioral finance. The effect of loss aversion in a marketing setting was demonstrated in a study of consumer reaction to price changes to insurance policies. The study found price increases had twice the effect on customer switching, compared to price decreases. Similarly, users in behavioral and experimental economics studies decided to cease participation in iterative money-making games when the threat of loss was close to the expenditure of effort, even when the user stood to further their gains.\n\nHumans may be hardwired to be loss averse due to asymmetric evolutionary pressure on losses and gains: for an organism operating close to the edge of survival, the loss of a day's food could cause death, whereas the gain of an extra day's food would not cause an extra day of life (unless the food could be easily and effectively stored).\n\nLoss aversion was first proposed as an explanation for the endowment effect—the fact that people place a higher value on a good that they own than on an identical good that they do not own—by Kahneman, Knetsch, and Thaler (1990). Loss aversion and the endowment effect lead to a violation of the Coase theorem—that \"the allocation of resources will be independent of the assignment of property rights when costless trades are possible\" (p. 1326).\n\nIn several studies, the authors demonstrated that the endowment effect could be explained by loss aversion but not five alternatives: (1) transaction costs, (2) misunderstandings, (3) habitual bargaining behaviors, (4) income effects, or (5) trophy effects. In each experiment half of the subjects were randomly assigned a good and asked for the minimum amount they would be willing to sell it for while the other half of the subjects were given nothing and asked for the maximum amount they would be willing to spend to buy the good. Since the value of the good is fixed and individual valuation of the good varies from this fixed value only due to sampling variation, the supply and demand curves should be perfect mirrors of each other and thus half the goods should be traded. The authors also ruled out the explanation that lack of experience with trading would lead to the endowment effect by conducting repeated markets.\n\nThe first two alternative explanations—that under-trading was due to transaction costs or misunderstanding—were tested by comparing goods markets to induced-value markets under the same rules. If it was possible to trade to the optimal level in induced value markets, under the same rules, there should be no difference in goods markets.\n\nThe results showed drastic differences between induced-value markets and goods markets. The median prices of buyers and sellers in induced-value markets matched almost every time leading to near perfect market efficiency, but goods markets sellers had much higher selling prices than buyers' buying prices. This effect was consistent over trials, indicating that this was not due to inexperience with the procedure or the market. Since the transaction cost that could have been due to the procedure was equal in the induced-value and goods markets, transaction costs were eliminated as an explanation for the endowment effect.\n\nThe third alternative explanation was that people have habitual bargaining behaviors, such as overstating their minimum selling price or understating their maximum bargaining price, that may spill over from strategic interactions where these behaviors are useful to the laboratory setting where they are sub-optimal. An experiment was conducted to address this by having the clearing prices selected at random. Buyers who indicated a willingness-to-pay higher than the randomly drawn price got the good, and vice versa for those who indicated a lower WTP. Likewise, sellers who indicated a lower willingness-to-accept than the randomly drawn price sold the good and vice versa. This incentive compatible value elicitation method did not eliminate the endowment effect but did rule out habitual bargaining behavior as an alternative explanation.\n\nIncome effects were ruled out by giving one third of the participants mugs, one third chocolates, and one third neither mug nor chocolate. They were then given the option of trading the mug for the chocolate or vice versa and those with neither were asked to merely choose between mug and chocolate. Thus, wealth effects were controlled for those groups who received mugs and chocolate. The results showed that 86% of those starting with mugs chose mugs, 10% of those starting with chocolates chose mugs, and 56% of those with nothing chose mugs. This ruled out income effects as an explanation for the endowment effect. Also, since all participants in the group had the same good, it could not be considered a \"trophy\", eliminating the final alternative explanation.\n\nThus, the five alternative explanations were eliminated in the following ways:\n\nRecently, studies have questioned the existence of loss aversion. In several studies examining the effect of losses in decision making under risk and uncertainty no loss aversion was found. There are several explanations for these findings: one, is that loss aversion does not exist in small payoff magnitudes; the other, is that the generality of the loss aversion pattern is lower than that thought previously. Finally, losses may have an effect on attention but not on the weighting of outcomes; as suggested, for instance, by the fact that losses lead to more autonomic arousal than gains even in the absence of loss aversion. This latter effect is sometimes known as Loss Attention. Even in a non-risky domain, prospective affective judgments about gains and losses show that loss aversion is magnitude dependent such that for low magnitudes there is no loss aversion \n\nLoss aversion may be more salient when people compete. Gill and Prowse (2012) provide experimental evidence that people are loss averse around reference points given by their expectations in a competitive environment with real effort.\n\nDavid Gal (2006) argued that many of the phenomena commonly attributed to loss aversion, including the status quo bias, the endowment effect, and the preference for safe over risky options, are more parsimoniously explained by inertia than by a loss/gain asymmetry. Gal and Rucker (2018) made similar arguments. \n\nIn 2005, experiments were conducted on the ability of capuchin monkeys to use money. After several months of training, the monkeys began showing behavior considered to reflect understanding of the concept of a medium of exchange. They exhibited the same propensity to avoid perceived losses demonstrated by human subjects and investors.\nWhile a subsequent study suggested that the 2005 results were not indicative of loss aversion because of timing differences in the presentation of gains and losses to the monkeys, a follow-up 2008 study by Laksminaryanan, Chen and Santos ruled out this alternative explanation.\n\nExpectation-based loss aversion is a phenomenon in behavioral economics. When the expectations of an individual fail to match reality, they lose an amount of utility from the lack of experiencing fulfillment of these expectations. Analytical framework by Botond Kőszegi and Matthew Rabin provides a methodology through which such behavior can be classified and even predicted. An individual's most recent expectations influences loss aversion in outcomes outside the status quo; a shopper intending to buy a pair of shoes on sale experiences loss aversion when the pair she had intended to buy is no longer available.\n\nSubsequent research performed by Johannes Abeler, Armin Falk, Lorenz Goette, and David Huffman in conjunction with the Institute of Labor Economics used the framework of Kőszegi and Rabin to prove that people experience expectation-based loss aversion at multiple thresholds. The study evinced that reference points of people causes a tendency to avoid expectations going unmet. Participants were asked to participate in an iterative money-making task given the possibilities that they would receive either an accumulated sum for each round of \"work\", or a predetermined amount of money. With a 50% chance of receiving the \"fair\" compensation, participants were more likely to quit the experiment as this amount approached the fixed payment. They chose to stop when the values were equal as no matter which random result they received, their expectations would be matched. Participants were reluctant to work for more than the fixed payment as there was an equal chance their expected compensation would not be met.\n\nLoss aversion experimentation has most recently been applied within an educational setting in an effort to improve achievement within the U.S. Recent results from Program for International Student Assessment (PISA) 2009 ranked the US ranks #31 in math and #17 in Reading. In this latest experiment, Fryer et al. posits framing merit pay in terms of a loss in order to be most effective. This study was performed in the city of Chicago Heights within nine K-8 urban schools, which included 3,200 students. 150 out of 160 eligible teachers participated and were assigned to one of four treatment groups or a control group. Teachers in the incentive groups received rewards based on their students' end of the year performance on the ThinkLink Predictive Assessment and K-2 students took the Iowa Test of Basic Skills (ITBS) in March. The control group followed the traditional merit pay process of receiving \"bonus pay\" at the end of the year based on student performance on standardized exams. However, the experimental groups received a lump sum given at beginning of the year, that would have to be paid back. The bonus was equivalent to approximately 8% of the average teacher salary in Chicago Heights, approximately $8,000.\n\nMethodology—\"Gain\" and \"Loss\" teachers received identical net payments for a given level of performance. The only difference is the timing and framing of the rewards. An advance on the payment and the re framing of the incentive as avoidance of a loss, the researchers observed treatment effects in excess of 0.20 and some as high as 0.398 standard deviations. According to the authors, 'this suggests that there may be significant potential for exploiting loss aversion in the pursuit of both optimal public policy and the pursuit of profits'.\n\nUtilizing loss aversion, specifically within the realm of education, has gotten much notoriety in blogs and mainstream media.\n\nThe Washington Post discussed merit pay in a 2012 article and specifically the study conducted by Fryer et al. The article discusses the positive results of the experiment and estimates the testing gains of those of the \"loss\" group are associated with an increase in lifetime earnings of between $37,180 and $77,740. They also comment on the fact that it didn't matter much whether the pay was tied to the performance of a given teacher or to the team to which that teacher was assigned. They state that \"a merit pay regime need not pit teachers in a given school against each other to get results\".\n\nScience Daily specifically covers the Fryer study stating that the study showed that \"students gained as much as a 10 percentile increase in their scores compared to students with similar backgrounds -- if their teacher received a bonus at the beginning of the year, with conditions attached.\" It also explains how there was no gain for students when teachers were offered the bonus at the end of the school year.\n\nThomas Amadio, superintendent of Chicago Heights Elementary School District 170, where the experiment was conducted, is quoted in this article stating \"the study shows the value of merit pay as an encouragement for better teacher performance\".\n\nEducation weekly also weighs in and discusses utilizing loss aversion within education, specifically merit pay. The article states there are \"few noteworthy limitations to the study, particularly relative to scope and sample size; further, the outcome measure was a 'low-stakes' diagnostic assessment, not the state test—it's unclear if findings would look the same if the test was used for accountability purposes. Still Fryer et al. have added an interesting tumbling element to the merit-pay routine\".\n\nThe Sun Times interviewed John List, Chairman of the University of Chicagos' department of economics. He stated \"It's a deeply ingrained behavioral trait. .. that all human beings have—this underlying phenomenon that 'I really, really dislike losses, and I will do all I can to avoid losing something'.\" The article also speaks to only one other study to enhance performance in a work environment. The only prior field study of a \"loss aversion\" payment plan, they said, \"occurred in Nanjing, China, where it improved productivity among factory workers who made and inspected DVD players and other consumer electronics\". The article also covers a reaction by Barnett Berry, president of the Center for Teaching Quality, who stated \"the study seems to suggest that districts pay 'teachers working with children and adolescents' in the same way 'Chinese factory workers' were paid for 'producing widgets'. I think this suggests a dire lack of understanding of the complexities of teaching.\"\n\nThere has also been other criticism of the notion of loss aversion as an explanation of greater effects. Indeed, all of the noted findings in education can be explained simply by the additional attention to a task when it includes losses (i.e., loss attention), independently of the weighting to gains and losses.\n\nLarry Ferlazzo in his blog questioned what kind of positive classroom culture a \"loss aversion\" strategy would create with students, and what kind of effect a similar plan with teachers would have on school culture. He states that \"the usual kind of teacher merit pay is bad enough, but a threatened 'take-away' strategy might even be more offensive\".\n\nIn earlier studies, both bidirectional mesolimbic responses of activation for gains and deactivation for losses(or vica versa) and gain or loss-specific responses have been seen. While reward anticipation is associated with ventral striatum activation, negative outcome anticipation engages the amygdala. However, only some studies have shown involvement of amygdala during negative outcome anticipation but not others which has led to some inconsistencies. It has later been proven that inconsistencies may only have been due to methodological issues including the utilisation of different tasks and stimuli, coupled with ranges of potential gains or losses sampled from either payoff matrices rather than parametric designs, and most of the data are reported in groups, therefore ignoring the variability amongst individuals. Thus later studies rather than focusing on subjects in groups, focus more on individual differences in the neural bases by jointly looking at behavioural analyses and neuroimaging.\n\nNeuroimaging studies on loss aversion involves measuring brain activity with functional magnetic resonance imaging (fMRI) to investigate whether individual variability in loss aversion were reflected in differences in brain activity through bidirectional or gain or loss specific responses, as well as multivariate source-based morphometry (SBM) to investigate a structural network of loss aversion and univariate voxel-basedmorphometry (VBM) to identify specific functional regions within this network.\n\nBrain activity in a right ventral striatum cluster increases particularly when anticipating gains. This involves the ventral caudate nucleus, pallidum, putamen, bilateral orbitofrontal cortex, superior frontal and middle gyri, posterior cingulate cortex, dorsal anterior cingulate cortex, and parts of the dorsomedial thalamus connecting to temporal and prefrontal cortex. There is a significant correlation between degree of loss aversion and strength of activity in both the frontomedial cortex and the ventral striatum. This is shown by the slope of brain activity deactivation for increasing losses being significantly greater than the slope of activation for increasing gains in the appetitive system involving the ventral striatum in the network of reward-based behavioural learning. On the other hand, when anticipating loss, the central and basal nuclei of amygdala, right posterior insula extending into the supramarginal gyrus mediate the output to other structures involved in the expression of fear and anxiety, such as the right parietal operculum and supramarginal gyrus. Consistent with gain anticipation, the slope of the activation for increasing losses was significantly greater than the slope of the deactivation for increasing gains.\n\nMultiple neural mechanisms are recruited while making choices, showing functional and structural individual variability. Biased anticipation of negative outcomes leading to loss aversion involves specific somatosensory and limbic structures. fMRI test measuring neural responses in striatal, limbic and somatosensory brain regions help track individual differences in loss aversion. Its limbic component involved the amygdala(associated with negative emotion and plays a role in the expression of fear) and putamen in the right hemisphere. The somatosensory component included the middle cingulate cortex, as well as the posterior insula and rolandic operculum bilaterally. The latter cluster partially overlaps with the right hemispheric one displaying the loss-oriented bidirectional response previously described, but, unlike that region, it mostly involved the posterior insula bilaterally. All these structures play a critical role in detecting threats and prepare the organism for appropriate action, with the connections between amygdala nuclei and the striatum controlling the avoidance of aversive events. There are functional differences between the right and left amygdala. Overall, the role of amygdala in loss anticipation suggested that loss aversion may reflect a Pavlovian conditioned approach-avoidance response. Hence, there is a direct link between individual differences in the structural properties of this network and the actual consequences of its associated behavioral defense responses.\n\nThe neural activity involved in the processing of aversive experience and stimuli is not just a result of a temporary fearful overreaction prompted by choice-related information, but rather a stable component of one's own preference function, reflecting a specific pattern of neural activity encoded in the functional and structural construction of a limbic-somatosensory neural system anticipating heightened aversive state of the brain. Even when no choice is required, individual differences in the intrinsic responsiveness of this interoceptive system reflect the impact of anticipated negative effects on evaluative processes, leading preference for avoiding losses rather than acquiring greater but riskier gains.\n\nIndividual differences in loss aversion are related to variables such as age, gender, and genetic factors affecting thalamic norepinephrine transmission, as well as neural structure and activities. Outcome anticipation and ensuing loss aversion involve multiple neural systems, showing functional and structural individual variability directly related to the actual outcomes of choices.\n\nIn a study, adolescents and adults are found to be similarly loss-averse on behavioural level but they demonstrated different underlying neural responses to the process of rejecting gambles. Although adolescents rejected the same proportion of trials as adults, adolescents displayed greater caudate and frontal pole activation than adults to achieve this. These findings suggest a difference in neural development during the avoidance of risk. It is possible that adding affectively arousing factors (e.g. peer influences) may overwhelm the reward-sensitive regions of the adolescent decision making system leading to risk-seeking behaviour. On the other hand, although men and women did not differ on their behavioural task performance, men showed greater neural activation than women in various areas during the task. Loss of striatal dopamine neurons is associated with reduced risk-taking behaviour.  Acute administration of D2 dopamine agonists may cause an increase in risky choices in humans. This suggests dopamine acting on stratum and possibly other mesolimbic structures can modulate loss aversion by reducing loss prediction signalling.\n\n\n"}
{"id": "20013029", "url": "https://en.wikipedia.org/wiki?curid=20013029", "title": "Medullary ray (botany)", "text": "Medullary ray (botany)\n\nMedullary rays are cellular structures found in some species of wood. They appear as radial planar structures, perpendicular to the growth rings, which are visible to the naked eye. In a transverse section they appear as radiating lines from the centre of the log. In an axial section they may appear as a variety of transverse markings, depending on how close the section is to the plane of the ray. In a tangential section they may be hard to see at all.\n\nThey are formed by the activity of fascicular cambium. During the process of the division of cambium, the cambium cuts out cells on both the outer and inner side. These cells are parenchymatous. Most of these cells transform into xylem and phloem. But certain cells don't transform into xylem and phloem and remain as such. These cells cut out by the cambium towards the periphery are phloem parenchyma while those towards the pith are xylem parenchyma. Both of these cells together work as secondary medullary rays.\n\nThese medullary or pith rays are essential for the radial conduction of the water, minerals and other organic substances.They transport the substances from centre to periphery\n\nThese rays are also known as vascular rays or pith rays.\n\nIn this context, the term refers to radial sheets or ribbons extending vertically through the tree across and perpendicular to the growth rings. Also called pith rays or wood rays, these formations of primarily parenchyma cells allow the radial transport of sap and are essential in the process of tylosis.\n\nIn quartersawn material, where the wood is cut into boards with the growth rings roughly perpendicular to the face of the board, the medullary rays often produce beautiful figure such as silver grain, medullary spots, pith flecks, etc.\n"}
{"id": "2881071", "url": "https://en.wikipedia.org/wiki?curid=2881071", "title": "Monandrous", "text": "Monandrous\n\nIn botanical terms, monandrous simply means to have a single stamen.\n\nA distinction between monandrous and other flowers is particularly relevant in the classification of orchids. The monandrous orchids form a clade consisting of the subfamilies Orchidoideae, Vanilloideae, and Epidendroideae. The other subfamilies, Apostasioideae and Cypripedioideae, have at least two stamens.\n\nIn animals, a monandrous system occurs when females have one mate at a time. For example, a female speckled wood butterfly will typically only mate once within her short lifetime. This is also common in certain bee species, like \"Bombus terrestris\" and \"Bombus pratorum\", where a female will only mate with one male during her nuptial flight and use the sperm reserves for the rest of her life. This is also seen in a few species of stingless bees, like \"Plebeia remota\", where the males will attempt to mate with the queen as she tries to leave the nest, but only one male will be successful in mating.\n"}
{"id": "3464108", "url": "https://en.wikipedia.org/wiki?curid=3464108", "title": "Nestedness", "text": "Nestedness\n\nNestedness is a measure of structure in an ecological system, usually applied to species-sites systems (describing the distribution of species across locations), or species-species interaction networks (describing the interactions between species, usually as bipartite networks such as hosts-parasites, plants-pollinators, etc.).\n\nA system (usually represented as a matrix) is said to be nested when the elements that have a few items in them (locations with few species, species with few interactions) have a subset of the items of elements with more items. Imagine a series of islands that are ordered by their distance from the mainland. If the mainland has all species, the first island has a subset of mainland's species, the second island has a subset of the first island's species, and so forth, then this system is perfectly nested.\n\nOne measurement unit for nestedness is a system's 'temperature' offered by Atmar and Patterson in 1993. This measures the order in which species' extinctions would occur in the system (or from the other side - the order of colonizing a system). The 'colder' the system is, the more fixed the order of extinction would be. In a warmer system, extinctions will take a more random order. Temperatures go from 0°, coldest and absolutely fixed, to 100° absolutely random.\n\nFor various reasons, the Nestedness Temperature Calculator is not mathematically satisfying (no unique solution, not conservative enough). A software (BINMATNEST) is available from the authors on request and from the Journal of Biogeography to correct these deficits In addition, ANINHADO solves problems of large matrix size and processing of a large number of randomized matrices; in addition it implements several null models to estimate the significance of nestedness.\n\nBastolla \"et al.\" introduced a simple measure of nestedness based on the number of common neighbours for each pair of nodes. They argue that this can help reduce the effective competition between nodes in certain situations. For instance, two insect species might \"help\" each other by pollinating the same subset of plants, thereby reducing the extent to which they are harmful to each other. The authors suggest that this effect is behind a correlation between nestedness and diversity in plant-pollinator ecosystems. However, Johnson \"et al.\" have shown that this measure does not, in fact, properly account for the desired effect. These authors propose a refined version of the measure, and go on to show how certain network properties affect nestedness.\n\n"}
{"id": "2348167", "url": "https://en.wikipedia.org/wiki?curid=2348167", "title": "Nomen nudum", "text": "Nomen nudum\n\nIn taxonomy (especially in zoological and botanical nomenclature), a nomen nudum (\"naked name\"; plural nomina nuda) is a \"designation\" which looks exactly like a scientific name of an organism, and may have originally been intended to be a scientific name, but fails to be one because it has not (or has not yet) been published with an adequate description (or a reference to such a description). This makes it a \"bare\" or \"naked\" name, one which cannot be accepted as it stands. A largely equivalent but much less frequently used term is nomen tantum (\"name only\").\n\nAccording to the rules of zoological nomenclature a \"nomen nudum\" is unavailable; the glossary of the \"International Code of Zoological Nomenclature\" gives this definition:\nAnd among the rules of that same Zoological Code:\n\nAccording to the rules of botanical nomenclature a \"nomen nudum\" is not validly published. The glossary of the \"International Code of Nomenclature for algae, fungi, and plants\" gives this definition:\n\n\"Nomina nuda\" that were published before 1 January 1959 can be used to establish a cultivar name. For example, \"Veronica sutherlandii\", a \"nomen nudum\", has been used as the basis for \"Hebe pinguifolia\" 'Sutherlandii'.\n\n"}
{"id": "1179553", "url": "https://en.wikipedia.org/wiki?curid=1179553", "title": "One Good Turn (book)", "text": "One Good Turn (book)\n\nOne Good Turn: A Natural History of the Screwdriver and the Screw is a book published in 2000 by Canadian architect, professor and writer Witold Rybczynski.\n\nThe idea for the book came in 1999 when an editor at \"The New York Times Magazine\" asked Rybczynski to write a short essay on the best and most useful common tool of the previous 1000 years. Rybczynski took the assignment, but as he researched the history of the items in his workshop – hammers and saws, levels and planes – he found that most dated well back into antiquity. At the point of giving up, he asked his wife for ideas. She answered: \"You always need a screwdriver for something.\"\n\nRybczynski discovered that the screwdriver is a relatively new addition to the toolbox, an invention of the Late Middle Ages in Europe and the only major mechanical device not independently invented by the Chinese. Leonardo da Vinci was there at the start, designing a number of screw-cutting machines with interchangeable gears. Nevertheless, it took generations for the screw (and with it, the screwdriver and lathe) to come into general use, and it was not until modern times that improvements such as slotted screws came into being. Rybczynski spends some time discussing the Canadian invention, the Robertson screwdriver.\n\n"}
{"id": "10613828", "url": "https://en.wikipedia.org/wiki?curid=10613828", "title": "Orion Abort Test Booster", "text": "Orion Abort Test Booster\n\nThe Orion Abort Test Booster (ATB) is a small solid rocket launcher being developed by Orbital ATK Corporation under contract by U.S. Air Force Space and Missile Systems Center, Experimental Launch & Test [SMC/LEX].\n\nIts goal will be to demonstrate and qualify the Orion Launch Abort System (LAS) that will allow the astronaut crew to safely escape in the event of an emergency during launch pad operations, through the ascent phase of the Orion vehicle.\n\nWhile the first test was at one point planned for 2009, in early 2013 a decision was made to postpone abort testing until after the 2018 uncrewed maiden flight of the SLS vehicle. The test is now baselined for December 2019.\n\nThe ATB can be considered the successor to Little Joe II used during similar Apollo abort testing at White Sands Missile Range, New Mexico. It uses a single stage Peacekeeper missile first stage motor [SR118] inside an Aero-Shell to replicate the Orion Service Module 5.5 meter diameter. Ballast plates are used to increase the weight of the vehicle to mimic ascent acceleration of the Space Launch System. The ATB also includes an Avionics cylinder assembly mounted to the forward SR118 motor skirt, and a Thrust Reaction Structure to carry the loads from the 92\" diameter motor to the larger Orion Service Module 5.5 meter diameter.\n\nThe Abort Test launch will be from Cape Canaveral SLC-46, a Navy site leased to Space Florida. The existing Mobile Access Tower will be modified with platforms designed and built by Langley Research Center, VA to surround the ATB and Launch Abort Vehicle [comprising a Separation Ring, Boilerplate Crew Module (also built at Langley Research Center), and Launch Abort System] outer mold line.\n\nThe abort separation of the Crew Module/LAS from the ATB will occur while the SR118 is still thrusting around 30,000' altitude during the transonic regime. Currently, no parachute system will be installed on the Crew Module. The purpose of the test is to collect data on the separation environment.\n\n"}
{"id": "28129447", "url": "https://en.wikipedia.org/wiki?curid=28129447", "title": "PTScientists", "text": "PTScientists\n\nPTScientists, formerly known as Part-Time Scientists, is a group of volunteer scientists and engineers based in Germany. They became the first German team to officially enter the Google Lunar X-Prize competition on June 24, 2009, which was finished without winner in March 2018. Their goal remains to land a mission on the Moon by 2019.\n\nIn March 2017, the group announced that they planned to perform the world's first private Moon landing. A landing module called \"Autonomous Landing and Navigation Module\" (ALINA) will launch on a Falcon 9 to the surface of the Moon. ALINA lander will deploy two lunar rovers in the Taurus–Littrow lunar valley, that will search for the Lunar Roving Vehicle left there by NASA astronauts Eugene Cernan and Harrison Schmitt in 1972 during the Apollo 17 mission. \n\nThe rovers are being developed by German automobile manufacturer Audi. The prototype rover is called \"Asimov Jr. R3\", while the two flight rovers are named \"Audi Lunar Quattro\". ALINA lander will also perform communications relay between the rovers and Earth, using technology based on Infineon chips, Nokia, and Vodafone's 4G LTE network.\n\nThe lander and rovers may carry commercial payloads or scientific instruments for a fee. The only confirmed payload to date is hardware for a live video broadcast.\n\nThe Part-Time Scientists team formed in June 2009, when ten teams had already entered Google Lunar X-Prize (GLXP), which had started in 2007. Later the company Part-Time Scientists GmbH (Limited) was founded.\n\nOn August 22–23, 2009, the Part-Time Scientists presented their project at the Open Doors Day of the Federal Ministry for Education and Research.\n\nOn December 28, 2009, the team presented their mission at the 26th annual Chaos Communication Congress. In a two-hour presentation, the team provided a detailed overview of all parts of the project. This was the first time the European-made private lunar rover prototype had been presented to the public.\n\nEarly 2015 the team won awards in the categories Mobility and Vision, and a total of $750,000 in the Milestone Prizes of GLXP.\n\nDuring the Advertising Festival in Cannes, on June 23, 2015, Audi was announced as a main sponsor and the rover developer. As a result of this cooperation, the two identical rovers were named \"Audi Lunar Quattro\" during the 2016 North American International Auto Show in Detroit.\n\n\"Part-Time Scientists GmbH\" is the company representing the team. The company opened offices in Berlin-Mahlsdorf in 2015. It also uses the name \"PTScientists\".\n\nPart-Time Scientists GmbH is reselling payload on the Moon mission to individuals, organizations and companies. The cost for one kilogram of payload is between €700,000 and €800,000.\nFurthermore, the know-how of the team is available as a consulting service. The European Space Agency is currently studying six private companies, including PTScientists, to work on potential ISRU payload delivery to the Moon surface by 2025.\n\nAn additional source of income are merchandising products for the Moon mission.\n\nPart-Time Scientists lists as their partners:\n\nKey partners\n\nMission scientific and academic partners\n\nMission technology partners\n\nMission supporters\n"}
{"id": "4379729", "url": "https://en.wikipedia.org/wiki?curid=4379729", "title": "Pavel Senko", "text": "Pavel Senko\n\nPavel Kononovich Senko () (October 4, 1916 – 2000) was a Soviet polar explorer, scientist, and member and leader of numerous expeditions to Arctic Ocean and Antarctica under the auspices of the Arctic and Antarctic Research Institute and Soviet Antarctic Expedition.\n\nThere is a valley at the bottom of Arctic Ocean named after him : \"Senko Valley\" from 87° 04’ N 97° 00’ W to 87° 45’ N 101° 10’ W .\nThere is a mountain named after him in Antarctica: \"Senko Mountain\" at 71° 25,2’ S 12° 46,8’ E on the Zavaritsky Ridge.\n\nHe was leader of the winter party and officer-in-charge of Mirny Station on the 9th Soviet Antarctic Expedition and lead several expeditions subsequently. His name had shown up in the 1997 edition of \"The Guinness Book of records\" as one of the four \"first people to have indisputably reached the North Pole at ground level exactly 90°00`00\" (+-300m) on April, 23, 1948\" \n\n\nCristopher Pala The oddest Place on the earth - Rediscovering the North Pole\n"}
{"id": "1854270", "url": "https://en.wikipedia.org/wiki?curid=1854270", "title": "Rainout", "text": "Rainout\n\nA rainout is the process of precipitation causing the removal of radioactive particles from the atmosphere onto the ground, creating nuclear fallout by rain. The rainclouds of the rainout are often formed by the particles of a nuclear explosion itself and because of this, the decontamination of rainout is more difficult than a \"dry\" fallout.\n\nA rainout could occur in the vicinity of ground zero or the contamination could be carried aloft before deposition depending on the current atmospheric conditions and how the explosion occurred. The explosion, or burst, can be air, surface, subsurface, or seawater. An air burst will produce less fallout than a comparable explosion near the ground due to less particulate being contaminated. Detonations at the surface will tend to produce more fallout material. In case of water surface bursts, the particles tend to be rather lighter and smaller, producing less local fallout but extending over a greater area. The particles contain mostly sea salts with some water; these can have a cloud seeding effect causing local rainout and areas of high local fallout. Fallout from a seawater burst is difficult to remove once it has soaked into porous surfaces because the fission products are present as metallic ions which become chemically bonded to many surfaces. For subsurface bursts, there is an additional phenomenon present called \"base surge\". The base surge is a cloud that rolls outward from the bottom of the subsiding column, which is caused by an excessive density of dust or water droplets in the air. This surge is made up of small solid particles, but it still behaves like a fluid. A soil earth medium favors base surge formation in an underground burst. Although the base surge typically contains only about 10% of the total bomb debris in a subsurface burst, it can create larger radiation doses than fallout near the detonation, because it arrives sooner than fallout, before much radioactive decay has occurred. For underwater bursts, the visible surge is, in effect, a cloud of liquid (usually water) droplets with the property of flowing almost as if it were a homogeneous fluid. After the water evaporates, an invisible base surge of small radioactive particles may persist.\n\nMeteorogically, snow and rain will accelerate local fallout. Under special meteorological conditions, such as a local rain shower that originates above the radioactive cloud, limited areas of heavy contamination just downwind of a nuclear blast may be formed.\nRain on an area contaminated by a surface burst changes the pattern of radioactive intensities by washing off higher elevations, buildings, equipment, and vegetation. This reduces intensities in some areas and possibly increases intensities in drainage systems; on low ground; and in flat, poorly drained areas.\n"}
{"id": "53698085", "url": "https://en.wikipedia.org/wiki?curid=53698085", "title": "The Visualization Handbook", "text": "The Visualization Handbook\n\nThe Visualization Handbook is a textbook by Charles D. Hansen and Christopher R. Johnson that serves as a survey of the field of scientific visualization by presenting the basic concepts and algorithms in addition to a current review of visualization research topics and tools. It is commonly used as a textbook for scientific visualization graduate courses. It is also commonly cited as a reference for scientific visualization and computer graphics in published papers, with almost 500 citations documented on Google Scholar.\n\n\n"}
{"id": "28477492", "url": "https://en.wikipedia.org/wiki?curid=28477492", "title": "VVC weather station", "text": "VVC weather station\n\nThe VDNKh weather station is the principal weather station in Moscow, Russia. It opened in 1948 on the grounds of the All-Russia Exhibition Centre. Temperature and precipitation readings at VVC weather station form the official weather reports and historical statistics.\n\nThe station's World meteorological organization classification index is 27612.\n\n"}
{"id": "17582548", "url": "https://en.wikipedia.org/wiki?curid=17582548", "title": "Victor Hayward", "text": "Victor Hayward\n\nVictor George Hayward AM (1887–1916) was a London-born accounts clerk whose taste for adventure took him to Antarctica as a member of Sir Ernest Shackleton’s Imperial Trans-Antarctic Expedition, 1914–17. He had previously spent time working on a ranch in northern Canada and this experience, combined with his \"do-anything\" attitude, was sufficient for him to be engaged by Shackleton as a general assistant to the Ross Sea party, a support group with a mission to lay depots for the main cross-continental party.\n\nHayward quickly proved himself to be hard-working and resourceful. He was one of the ten members of the shore party that was marooned when the Ross Sea party’s expedition ship \"Aurora\" broke from its McMurdo Sound moorings during a storm and was unable to return. In difficult circumstances he played a full part in the efforts of the stranded group to fulfil its mission, despite its shortages of food, proper clothing, and equipment. During the main depot-laying journey on the Great Ice Barrier in 1915–16 Hayward was one of the six who marched to the Beardmore Glacier to lay the last of the required chain of depots. On the return leg the party was struck with scurvy, which caused the death of Arnold Spencer-Smith. Although suffering badly himself, Hayward helped bring the rest of the party off the Barrier to the relative safety of the Hut Point shelter.\n\nHayward disappeared on 8 May 1916 while walking across the frozen surface of McMurdo Sound in the hopes of reaching the expedition’s base at Cape Evans. His body was never found. Seven years later Hayward was posthumously awarded the Albert Medal for his efforts to save the lives of his stricken companions on the Barrier journey.\n\nVictor George Hayward was born 23 October 1887 at 5 Manor Park Road, Harlesden, North London. He was the 12th of 14 children of Francis Checkley Hayward and Mary Jane Fairchild. His father became a senior executive of the London and North Western Railway. Hayward was educated at an Essex boarding school, and after attending a London business college was employed as an accounts clerk in the City. From an early age he had been fascinated by stories of adventure, a particular favourite, acquired as a Sunday School prize, being R M Ballantyne’s \"The World of Ice, or Adventures in the Polar Regions\". Unprepared for a life of \"bourgeois complacency\", Hayward took leave from his employers to spend seven months working on a ranch in northern Canada. On his return he found settling back into an office routine difficult, and applied to Shackleton’s office for a position on the newly announced Trans-Antarctic expedition. His offer to \"do anything\" secured him his place in the Ross Sea party.\n\nThe Ross Sea party’s expedition ship \"Aurora\" arrived in Antarctica in January 1915. During the short season before the onset of winter Hayward volunteered for everything—helping Ernest Joyce with the dogs, assisting the motor tractor party and man-hauling depot supplies on the Barrier. When the shore party was marooned after \"Aurora\" was blown from her moorings, Hayward became an accomplished seal hunter, helping to boost the party’s depleted stocks of food. Described as a \"quiet, brawny man\", Hayward’s closest bond was formed with the party’s leader, Aeneas Mackintosh, with whom he shared a common yearning for home and loved ones.\n\nThe main depot-laying journey of the expedition began in September 1915. Although privately recording that he thought the programme to be followed \"impossible\", Hayward threw his physical might into the job. He was one of the party of six that undertook the longest stage of the depot-laying journey, from 80°S to the Beardmore Glacier. This journey was completed on 26 January 1916. On the return journey Mackintosh and parson/photographer Arnold Spencer-Smith fell victims to scurvy and had to be carried on the sledge, drawn by Joyce, Ernest Wild, Dick Richards and Hayward. When the food situation became acute Hayward, by now showing scurvy symptoms himself, nevertheless went forward with Joyce and Richards to obtain life-saving food and fuel for the rest of the party. Eventually, after suffering both physical and mental breakdown, Hayward was lashed to the sledge and on 18 March was hauled to the shelter at Hut Point where, since the doorway was iced up, he was hoisted in through the window. Spencer-Smith died and was buried in the ice; Mackintosh barely survived. Joyce had expected Hayward to die, but he too survived.\n\nHelped by a diet of seal—plentifully available around Hut Point—the party slowly recovered. Mackintosh was anxious that as soon as possible they make the final stage of the return journey—the across the frozen surface of McMurdo Sound to the base at Cape Evans. The ice would not be safe until winter set in, in June or July, but Mackintosh and Hayward grew impatient. In early May they had recovered sufficiently to begin testing the ice. On 8 May Mackintosh announced that he and Hayward intended to walk across to Cape Evans, and against the urgent pleadings of Joyce, Richards and Wild, they set out at 1:00 pm., carrying only light supplies. Two hours later a blizzard swept over the Sound and they were lost from view. They did not arrive at Cape Evans, and no trace of their bodies was ever found, nor was the nature of their fate established. They may have fallen through the ice, or been carried out to sea when the ice broke up. If by chance they had managed to reach the temporary safety of land they would have been cut off, without hope either of returning to Hut Point or reaching Cape Evans, and would have perished from hypothermia.\n\nAfter the remainder of the Hut Point party had crossed to Cape Evans in mid-July a series of searches instituted by Joyce failed to establish the fate of Mackintosh and Hayward. Further searches took place when \"Aurora\" finally returned to relieve the party, in January 1917. A memorial cross was erected to Mackintosh and Hayward at Wind Vane Hill, a weather observation post near Cape Evans.\n\nSeven years after their struggle on the ice, in belated recognition, on 4 July 1923 Joyce, Richards, Wild and Hayward (the last two posthumously, Wild having died on active service in 1918) were awarded the Albert Medal, in recognition of their efforts to save the lives of Mackintosh and Spencer-Smith on the Barrier.\n\n"}
{"id": "20826071", "url": "https://en.wikipedia.org/wiki?curid=20826071", "title": "Volumetric flux", "text": "Volumetric flux\n\nIn fluid dynamics, the volumetric flux is the rate of volume flow across a unit area (m·s·m). \nVolumetric flux = liters/(second*area). The density of a particular property in a fluid's volume, multiplied with the volumetric flux of the fluid, thus defines the advective flux of that property. The volumetric flux through a porous medium is often modelled using Darcy's law.\n\nVolumetric flux is not to be confused with volumetric flow rate, which is the volume of fluid that passes through a \"given\" surface per unit of time (as opposed to a \"unit\" surface).\n"}
{"id": "8137454", "url": "https://en.wikipedia.org/wiki?curid=8137454", "title": "Weizmann Women &amp; Science Award", "text": "Weizmann Women &amp; Science Award\n\nThe Weizmann Women & Science Award is a biennial award established in 1994 to honor an outstanding woman scientist in the United States who has made significant contributions to the scientific community. The objective of the award, which includes a $25,000 research grant to the recipient, is to promote women in science, and to provide a strong role model to motivate and encourage the next generation of young women scientists.\n\nThe award was originally given by the American Committee for the Weizmann Institute of Science (ACWIS) and now it is awarded by the Weizmann Institute and the award ceremony takes place at the Weizmann Institute. \nThe Weizmann Institute is a center of basic interdisciplinary scientific research and graduate study, addressing crucial problems in technology, medicine and health, energy, agriculture and the environment.\n\n\n\n"}
{"id": "38608010", "url": "https://en.wikipedia.org/wiki?curid=38608010", "title": "Western Livestock Journal", "text": "Western Livestock Journal\n\nThe Western Livestock Journal is a weekly livestock industry newspaper. Originally called the \"Farm and Ranch Market Journal\", it was started by Nelson R. Crow in 1922 and is published by Crow Publications.\n\nThe \"Farm and Ranch Market Journal\" became \"Western Livestock Journal\" in the early 1930s. In 1952, Nelson purchased \"Livestock Magazine\" from the Biggs family in Denver. The two weeklies were combined in the ’70s to create one national edition of \"Western Livestock Journal\" and the monthly magazine was renamed \"Livestock Magazine\", and split into three editorial editions. \"Livestock Magazine\" ceased publication in the early ’80s.\n\nCrow Publications publishes the \"Northwest American Bull Guide\" magazine (yearly), the \"Commercial Cattle Issue\" (yearly), the \"Properties Ranch & Home magazine\" (quarterly), as well as the \"Western Livestock Journal\" (weekly). Crow Publications/\"Western Livestock Journal\" is located in Greenwood Village (CO, United States).\n\n"}
