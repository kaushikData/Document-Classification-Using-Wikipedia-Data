{"id": "20650676", "url": "https://en.wikipedia.org/wiki?curid=20650676", "title": "ASC-15", "text": "ASC-15\n\nThe ASC-15 (Advance System Controller Model 15) was a digital computer developed by International Business Machines (IBM) for use on the Titan II intercontinental ballistic missile (ICBM). It was subsequently modified and used on the Titan III and Saturn I Block II launch vehicles.\n\nIts principal function on these rockets was to make navigation calculations using data from inertial sensor systems. It also performed readiness checks before launch. It was a digital serial processor using fixed-point data with 27-bit words. The storage was a drum memory. Electronic circuits were welded encapsulated modules, consisting of discrete resistors, transistors, capacitors, and other components welded together and encapsulated in a foam material. It was manufactured in the IBM plant at Owego, NY.\n\nThe first inertial guidance system for the Titan II was built by AC Spark Plug, and included an inertial measurement unit based in designs from Draper Labs at MIT and the ASC-15 computer designed and built by IBM in Owego, NY. The first Titan II missile carrying this system was launched 16 March 1962. Acquiring spares for this system became difficult, and the Air Force decided to replace it with a new system. The AC Spark Plug system, including the ASC-15, was replaced by the Delco Electronics Universal Space Guidance System (USGS) on operational Titan II missiles starting in January 1978. The guidance computer in the USGS was the Magic 352, made by Delco.\n\nThe ASC-15 was built on an aluminum frame about 1.5x1.5x1 feet. The sides, top and bottom were covered by pieces of laminated plastic, covered with gold-plated aluminum foil. These covers were slightly convex and ribbed for stiffness. Inside the covers were fifty-two logic sticks, each containing four welded encapsulated modules. These surrounded a bell frame housing a drum memory. See Figure 2.\n\nThe drum was a thin-walled stainless steel cylinder 3 inches long and 4.5 inches in diameter covered with a magnetic nickel-cobalt alloy. It was driven by a synchronous motor at 6,000 rpm. The drum had 70 tracks, of which 58 were used and 12 were spare. These tracks were used as follows:\nThe capacity of a track was 1,728 bits. Instruction words were 9-bits long, and data was stored in 27-bit words.\n\nCoincident with 58 tracks were 67 read heads and 13 write heads. While the drum was spinning at 6,000 rpm, the heads floated above the surface of the drum on a thin layer of air. When the drum was spinning up or slowing down, the heads were raised off the drum by camshafts rotated by a chain that was driven by a motor on top of the drum housing, to avoid scoring the magnetic surface. See Figure 3.\n\nThe Titan III was a space launch vehicle based on the Titan II ICBM. The ASC-15 was kept as the vehicle guidance computer, but the drum was lengthened slightly to provide 78 usable tracks, an increase of 20 over the drum used in the Titan II. The memory held 9,792 instructions (51 tracks) and 1,152 constants (18 tracks). The speed was the same as for the Titan II: 100 revolutions/second × 64 words/revolution × 27 bits/word = 172.8 kilobits/second. The time for an addition operation was 156 µs; for a multiplication, 1,875 µs; and for a division, 7,968 µs.\n\nNo guidance computer was used for Saturn I Block I (missions SA-1, 2, 3 and 4). The guidance system for SA-2 is shown in Figure 4. The pitch program was provided by a cam device located in the Servo Loop Amplifier Box. The sequence of events was controlled by a program device that was also used on Jupiter missiles. This was a 6-track tape recorder that sent pulses to a set of relays (the flight sequencer) to activate and deactivate various circuits in a precisely timed sequence.\n\nThe ASC-15 was first flown on SA-5, the first Saturn I Block II vehicle and the first to achieve orbit. It was a passenger on this mission, not guiding the vehicle but generating test data for later evaluation. The active guidance system on SA-5 was similar to that of earlier flights. The passenger system was the ASC-15 and the ST-124 inertial platform. Guidance was open loop; that is guidance commands were functions only of time. SA-5 also saw the introduction of the Instrument Unit.\n\nOn SA-6, while open loop ST-90S guidance was used for the first stage (S-I), after separation the ST-124 and ASC-15 used path adaptive guidance (closed loop) to control the second stage (S-IV). The SA-6 guidance system is shown in Figure 5. The effectiveness of the path adaptive guidance was demonstrated inadvertently when premature shutdown of S-IV engine number eight had virtually no effect on the vehicle trajectory.\n\nThe arrangement of the ST-90S and ST-124 systems (including the ASC-15 guidance computer) on SA-6 is shown in Figure 6. This is version 1 of the Instrument Unit, which flew on SA-5, 6, and 7.\n\nOn SA-7 the ST-124 system guided the firing of both stages. The guidance and control system for SA-7 is shown in Figure 7. The digital computer is the ASC-15. It replaced both the cam device that contained the S-I tilt program for earlier missions. and the program device that controlled the sequence of events on those missions.\n\nThe next mission flown after SA-7 was SA-9. It carried a new version of the Instrument Unit, one that was unpressurized and shorter than version 1. Version 2 flew on the remaining Saturn I missions (SA-8, 9, and 10), and is shown under construction at MSFC in Figure 8. Figure 9 is a blowup of this image, showing the dummy ASC-15 and dummy ST-124.\n\n\n"}
{"id": "8099403", "url": "https://en.wikipedia.org/wiki?curid=8099403", "title": "Acidophile (histology)", "text": "Acidophile (histology)\n\nAcidophile (or acidophil, or, as an adjectival form, acidophilic) is a term used by histologists to describe a particular staining pattern of cells and tissues when using haematoxylin and eosin stains. Specifically, the name refers to structures which \"love\" acid, and take it up readily. More specifically, acidophilia can be described by cationic groups of most often proteins in the cell readily reacting with acidic stains.\n\nIt describes the microscopic appearance of cells and tissues, as seen through a microscope, after a histological section has been stained with an acidic dye. The most common such dye is eosin, which stains acidophilic substances red and is the source of the related term eosinophilic. Note that a single cell can have both acidophilic substances/organelles and basophilic substances/organelles, albeit some have historically had so much of one stain that the cell itself is called a eosinophil.\n\n"}
{"id": "7778309", "url": "https://en.wikipedia.org/wiki?curid=7778309", "title": "Ahlfeldite", "text": "Ahlfeldite\n\nAhlfeldite ((Ni,Co)<nowiki>[</nowiki>SeO]·2HO) is a mineral of secondary origin. It's named after Friedrich Ahlfeld (1892–1982), a German-Bolivian mining engineer and geologist. Its type locality is Virgen de Surumi mine, Pakajake Canyon, Chayanta Province, Potosí Department, Bolivia.\n"}
{"id": "41713978", "url": "https://en.wikipedia.org/wiki?curid=41713978", "title": "Anchialus Glacier", "text": "Anchialus Glacier\n\nAnchialus Glacier (, 'Lednik Anchialus' \\'led-nik an-hi-'a-lo\\) is the 8.5 km long and 3.4 km wide glacier in Sostra Heights on the east side of northern Sentinel Range in Ellsworth Mountains, Antarctica. It is situated north of lower Embree Glacier, east of Sabazios Glacier, south of lower Newcomer Glacier and northwest of Vit Ice Piedmont. The glacier drains the northeast slopes of Mount Malone and the west slopes of Bracken Peak, flows northwards and joins Newcomer Glacier east of Mount Lanning.\n\nThe glacier is named after the ancient town of Anchialus in Southeastern Bulgaria.\n\nAnchialus Glacier is centred at . US mapping in 1961.\n\n\n\n"}
{"id": "13501877", "url": "https://en.wikipedia.org/wiki?curid=13501877", "title": "Ashing", "text": "Ashing\n\nAshing is a test to deduce the amount of ash forming material present in a petroleum product so as to decide its use in certain applications. Ash-forming materials are considered to be undesirable impurities or contaminants.\n\nThe specimen is placed in a suitable vessel, evaporating dish or crucible and ignited. It is allowed to burn until only ash and carbon remains. The carbonaceous residue is reduced to ash by heating in a muffle furnace at about 775C, cooled and weighed.\n\nAshing is also performed prior to chemical analysis by inductively coupled plasma emission spectrometry. Another application is the detection of asbestos content in certain products.\n\n"}
{"id": "4305644", "url": "https://en.wikipedia.org/wiki?curid=4305644", "title": "Basis set superposition error", "text": "Basis set superposition error\n\nIn quantum chemistry, calculations using finite basis sets are susceptible to basis set superposition error (BSSE). As the atoms of interacting molecules (or of different parts of the same molecule - intramolecular BSSE) approach one another, their basis functions overlap. Each monomer \"borrows\" functions from other nearby components, effectively increasing its basis set and improving the calculation of derived properties such as energy. If the total energy is minimised as a function of the system geometry, the short-range energies from the mixed basis sets must be compared with the long-range energies from the unmixed sets, and this mismatch introduces an error. \n\nOther than using infinite basis sets, two methods exist to eliminate the BSSE. In the \"chemical Hamiltonian approach\" (CHA), basis set mixing is prevented \"a priori\", by replacing the conventional Hamiltonian with one in which all the projector-containing terms that would allow mixing have been removed. In the \"counterpoise method\" (CP), the BSSE is calculated by re-performing \"all\" the calculations using the mixed basis sets, and the error is then subtracted \"a posteriori\" from the uncorrected energy. (The mixed basis sets are realised by introducing \"ghost orbitals\", basis set functions which have no electrons or protons.) Though conceptually very different, the two methods tend to give similar results. It has however been shown that the error is often larger when using the CP method since the central atoms in the system have much greater freedom to mix with all of the available functions compared to the outer atoms. Whereas in the CHA model, those orbitals have no greater intrinsic freedom and therefore the correction treats all fragments equally. The errors inherent in either BSSE correction disappear more rapidly than the total value of BSSE in larger basis sets.\n\n"}
{"id": "57011047", "url": "https://en.wikipedia.org/wiki?curid=57011047", "title": "Bellilinea", "text": "Bellilinea\n\nBellilinea is a thermophilic bacteria genus from the family of Anaerolineaceae with one known species (\"Bellilinea caldifistulae\"). \"Bellilinea caldifistulae\" has been isolated from thermophilic digester sludge from Niigata in Japan.\n"}
{"id": "6018334", "url": "https://en.wikipedia.org/wiki?curid=6018334", "title": "Bending moment", "text": "Bending moment\n\nA bending moment is the reaction induced in a structural element when an external force or moment is applied to the element causing the element to bend. The most common or simplest structural element subjected to bending moments is the beam. The diagram shows a beam which is simply supported at both ends. Simply supported means that each end of the beam can rotate; therefore each end support has no bending moment. The ends can only react to the shear loads. Other beams can have both ends fixed; therefore each end support has both bending moment and shear reaction loads. Beams can also have one end fixed and one end simply supported. The simplest type of beam is the cantilever, which is fixed at one end and is free at the other end (neither simple or fixed). In reality, beam supports are usually neither absolutely fixed nor absolutely rotating freely.\n\nThe internal reaction loads in a cross-section of the structural element can be resolved into a resultant force and a resultant couple. For equilibrium, the moment created by external forces (and external moments) must be balanced by the couple induced by the internal loads. The resultant internal couple is called the bending moment while the resultant internal force is called the shear force (if it is transverse to the plane of element) or the normal force (if it is along the plane of the element).\n\nThe bending moment at a section through a structural element may be defined as \"the sum of the moments about that section of all external forces acting to one side of that section\". The forces and moments on either side of the section must be equal in order to counteract each other and maintain a state of equilibrium so the same bending moment will result from summing the moments, regardless of which side of the section is selected. If clockwise bending moments are taken as negative, then a negative bending moment within an element will cause \"sagging\", and a positive moment will cause \"hogging\". It is therefore clear that a point of zero bending moment within a beam is a point of contraflexure—that is the point of transition from hogging to sagging or vice versa.\n\nMoments and torques are measured as a force multiplied by a distance so they have as unit newton-metres (N·m), or pound-foot (lbf·ft). The concept of bending moment is very important in engineering (particularly in civil and mechanical engineering) and physics.\n\nTensile and compressive stresses increase proportionally with bending moment, but are also dependent on the second moment of area of the cross-section of a beam (that is, the shape of the cross-section, such as a circle, square or I-beam being common structural shapes). Failure in bending will occur when the bending moment is sufficient to induce tensile stresses greater than the yield stress of the material throughout the entire cross-section. In structural analysis, this bending failure is called a plastic hinge, since the full load carrying ability of the structural element is not reached until the full cross-section is past the yield stress. It is possible that failure of a structural element in shear may occur before failure in bending, however the mechanics of failure in shear and in bending are different.\n\nMoments are calculated by multiplying the external vector forces (loads or reactions) by the vector distance at which they are applied. When analysing an entire element, it is sensible to calculate moments at both ends of the element, at the beginning, centre and end of any uniformly distributed loads, and directly underneath any point loads. Of course any \"pin-joints\" within a structure allow free rotation, and so zero moment occurs at these points as there is no way of transmitting turning forces from one side to the other.\n\nIt is more common to use the convention that a clockwise bending moment to the left of the point under consideration is taken as positive. This then corresponds to the second derivative of a function which, when positive, indicates a curvature that is 'lower at the centre' i.e. sagging. When defining moments and curvatures in this way calculus can be more readily used to find slopes and deflections.\n\nCritical values within the beam are most commonly annotated using a bending moment diagram, where negative moments are plotted to scale above a horizontal line and positive below. Bending moment varies linearly over unloaded sections, and parabolically over uniformly loaded sections.\n\nEngineering descriptions of the computation of bending moments can be confusing because of unexplained sign conventions and implicit assumptions. The descriptions below use vector mechanics to compute moments of force and bending moments in an attempt to explain, from first principles, why particular sign conventions are chosen.\n\nAn important part of determining bending moments in practical problems is the computation of moments of force.\nLet formula_1 be a force vector acting at a point A in a body. The moment of this force about a reference point (O) is defined as \nwhere formula_3 is the moment vector and formula_4 is the position vector from the reference point (O) to the point of application of the force (A). The formula_5 symbol indicates the vector cross product. For many problems, it is more convenient to compute the moment of force about an axis that passes through the reference point O. If the unit vector along the axis is formula_6, the moment of force about the axis is defined as\nwhere formula_8 indicates the vector dot product.\n\nThe adjacent figure shows a beam that is acted upon by a force formula_9. If the coordinate system is defined by the three unit vectors formula_10, we have the following \nTherefore,\nThe moment about the axis formula_13 is then\n\nThe negative value suggests that a moment that tends to rotate a body clockwise around an axis should have a negative sign. However, the actual sign depends on the choice of the three axes formula_10. For instance, if we choose another right handed coordinate system with formula_16, we have\nThen,\nFor this new choice of axes, a positive moment tends to rotate body clockwise around an axis.\n\nIn a rigid body or in an unconstrained deformable body, the application of a moment of force causes a pure rotation. But if a deformable body is constrained, it develops internal forces in response to the external force so that equilibrium is maintained. An example is shown in the figure below. These internal forces will cause local deformations in the body.\n\nFor equilibrium, the sum of the internal force vectors is equal to the applied external force and the sum of the moment vectors created by the internal forces is equal to the moment of the external force. The internal force and moment vectors are oriented in such a way that the total force (internal + external) and moment (external + internal) of the system is zero. The internal moment vector is called the bending moment.\n\nThough bending moments have been used to determine the stress states in arbitrary shaped structures, the physical interpretation of the computed stresses is problematic. However, physical interpretations of bending moments in beams and plates have a straightforward interpretation as the stress resultants in a cross-section of the structural element. For example, in a beam in the figure, the bending moment vector due to stresses in the cross-section \"A\" perpendicular to the \"x\"-axis is given by\nExpanding this expression we have,\nWe define the bending moment components as \nThe internal moments are computed about an origin that is at the neutral axis of the beam or plate and the integration is through the thickness (formula_22)\n\nIn the beam shown in the adjacent figure, the external forces are the applied force at point A (formula_23) and the reactions at the two support points O and B (formula_24 and formula_25). The reactions can be computed using balances of forces and moments about point A, i.e.,\nIf formula_27 is the length of the beam, we have\nIf we solve for the reactions we have\nLooking at the free body diagram of the part of the beam to the left of point X, the total moment of the external forces about the point X is\nIf we compute the cross products, we have\nFor this situation, the only non-zero component of the bending moment is\nFor the sum of the moments at X about the axis formula_13 to be zero, we require\nAt formula_35, we have formula_36.\n\nIn the above discussion, it is implicitly assumed that the bending moment is positive when the top of the beam is compressed. That can be seen if we consider a linear distribution of stress in the beam and find the resulting bending moment. Let the top of the beam be in compression with a stress formula_37 and let the bottom of the beam have a stress formula_38. Then the stress distribution in the beam is formula_39. The bending moment due to these stresses is\nwhere formula_41 is the area moment of inertia of the cross-section of the beam. Therefore the bending moment is positive when the top of the beam is in compression.\n\nMany authors follow a different convention in which the stress resultant formula_42 is defined as\nIn that case, positive bending moments imply that the top of the beam is in tension. Of course, the definition of top depends on the coordinate system being used. In the examples above, the top is the location with the largest formula_44-coordinate.\n\n\n"}
{"id": "4483284", "url": "https://en.wikipedia.org/wiki?curid=4483284", "title": "Body memory", "text": "Body memory\n\nBody memory (BM) is a hypothesis that the body itself is capable of storing memories, as opposed to only the brain. While experiments have demonstrated the possibility of cellular memory there are currently no known means by which tissues other than the brain would be capable of storing memories.\n\nModern usage of BM tends to frame it exclusively in the context of traumatic memory and ways in which the body responds to recall of a memory. In this regard, it has become relevant in treatment for PTSD.\n\nPeter Levine calls BM \"implicit memory\" or more specifically procedural memory, things that the body is capable of doing automatically and not in one's consciousness. He clarifies 3 types of BM and frames his work in terms of traumatic memory consequence and resolution:\n\nNicola Diamond elaborates on the opinion of philosopher Merleau-Ponty and asserts that BM is formed by doing. Whether practicing a bodily activity or forming a reaction to a traumatic memory.\n\nEdward Casey speaks of BM as, \"memory intrinsic to the body, how we remember by and through the body\", rather than what is remembered about the body.\n\nThomas Fuchs defines 6 different types of BM: procedural, situational, intercorporeal, incorporative, pain, and traumatic memory. He notes that they are not strictly separable from one another but \"derived from different dimensions of bodily experience. Michelle Summa further refines this definition as an implicit memory. A pre-thematic, operative consciousness of the past expressed through the body.\n\nAntonio Damasio calls these reactions to memories \"somatic markers\" or emotions that are expressed primarily as physical feelings.\n\nThese memories are often associated with phantom pain in a part or parts of the body – the body appearing to remember the past trauma. The idea of body memory is a belief frequently associated with the idea of repressed memories, in which memories of incest or sexual abuse can be retained and recovered through physical sensations. It may also be associated with phantom limb sensation but this is less common.\n\nIn 1993, Susan E. Smith, presented a paper relating the idea of \"Survivor Psychology\" at a false memory syndrome conference, stated about BM that, \"body memories are thought to literally be emotional, kinesthetic, or chemical recordings stored at the cellular level and retrievable by returning to or recreating the chemical, emotional, or kinesthetic conditions under which the memory recordings are filed. She went on in the abstract of the paper, \"one of the most commonly used theories to support the ideology of repressed memories or incest and sexual abuse amnesia is body memories.\" and \"The belief in these pseudoscientific concepts appears to be related to scientific illiteracy, gullibility, and a lack of critical thinking skills and reasoning abilities in both the mental health community and in society at large\"\n\nA 2017 systematic review of cross-disciplinary research in body memory found that the available data neither largely support or refute the claim that memories are stored outside of the brain and more research is needed.\n\nIn the \"Encyclopedia of Phenomenology\" Embree notes that, \"To posit body memory is to open up a Pandora's Box\", and links the idea to physical associations of memory rather than as a memory stored in a bodily manner.\n\nCellular memory (CM) is a parallel hypothesis to BM positing that memories can be stored outside the brain in all cells. The idea that non-brain tissues can have memories is believed by some who have received organ transplants, though this is considered impossible. The author said the stories are intriguing though and may lead to some serious scientific investigation in the future. In his book \"TransplantNation\" Douglas Vincent suggests that atypical newfound memories, thoughts, emotions and preferences after an organ transplant are more suggestive of immunosuppressant drugs and the stress of surgery on perception than of legitimate memory transference. In other words, \"as imaginary as a bad trip on LSD or other psychotropic drug.\"\n\nBiologists at Tufts University have been able to train flatworms despite the loss of the brain and head. This may show memory stored in other parts of the body in some animals. A worm reduced to 1/279th of the original can be regrown within a few weeks and be trained much quicker to head towards light and open space for food, an unnatural behavior for a flatworm. With each head removed training times appear reduced. This may just be a sign of epigenetics showing the appearance of memory.\n\nHowever, in the 1950s and 1960s James McConnell flatworm experiments measured how long it took to learn a maze. McConnell trained some to move around a maze and then chopped them up and fed them to untrained worms. The untrained group learned faster compared to a control that had not been fed trained worms. McConnell believed the experiment indicated cellular memory. The training involved stressing the worms with electric shock. This kind of stress releases persistent hormones and shows no evidence for memory transfer. Similar experiments with mice being trained and being fed to untrained mice showed improved learning. It was not a memory that was transferred but hormone enriched tissue.\n\nIn epigenetics there are various mechanisms for cells to pass on \"memories\" of stressors to their progeny. Strategies include Msn2 nucleo-cytoplasmic shuttling, changes in chromatin, partitioning of anti-stress factors, and damaged macromolecules between mother and daughter cells.\n\nIn adaptive immunity there is a functional CM that enables he immune system to learn to react to pathogens through mechanisms such as, cytoxic memory mediation in bone marrow, innate immune memory in stromal cells, fungal mediation of innate and inherited immunological response, and T and B-cell immune training. In this regard CM is essential for vaccine and immunity research.\n\n"}
{"id": "32753396", "url": "https://en.wikipedia.org/wiki?curid=32753396", "title": "BugGuide", "text": "BugGuide\n\nBugGuide (or BugGuide.net) is a website and online community of naturalists, both amateur and professional, who share observations of insects, spiders, and other related creatures. The website consists of informational guide pages and many thousands of photographs of arthropods from the United States and Canada which are used for identification and research. The non-commercial site is hosted by the Iowa State University Department of Entomology. BugGuide was conceived by photographer Troy Bartlett in 2003 and since 2006 has been maintained by Dr. John VanDyk, Adjunct Assistant Professor of Entomology and Senior Systems Analyst at Iowa State University. The website has been recognized for helping change public perception of insects.\n\nAccording to VanDyk, BugGuide had over 809 million hits in 2010, averaging approximately 26 hits per second. He also stated that in early 2011 the site consisted of almost 34,000 written pages representing about 23 percent of the estimated insect species in North America. In April 2012 the guide surpassed 500,000 photos. By October 2014, BugGuide had 30,774 species pages and 48,572 total pages, with over 808,718 images submitted by more than 27,846 contributors. On 22 September 2014, BugGuide surpassed 1,000,000 pages (most of which are photographs).\n\nThe photographs posted have contributed to or resulted in several scientific publications. A large proportion of images featured in an atlas of vespid wasps are credited to contributors to BugGuide. BugGuide photographs have detected new state records of invasive pest ants and beetles.\n\nGeologist and moth collector Richard Wilson said of the site, \"The BugGuide site is very useful for anyone finding an insect and it is very interactive on getting it identified if a picture can be taken.\"\n\nAccording to gardening author Margaret Roach, \"The site is where naturalists of all levels share photos of 'insects, spiders and their kin' to foster enthusiasm and expand the knowledge base about these often-overlooked (and as BugGuide points out, 'oft-maligned') creatures.\"\n\nAccording to the site itself, BugGuide.net has been responsible for the identification of 11 new, previously undescribed species as of mid-2014. In addition, 12 species new to the Western Hemisphere were first identified via the site; another seven new to North America; and numerous new country records (primarily the United States) and state/county sightings.\n\n"}
{"id": "50585194", "url": "https://en.wikipedia.org/wiki?curid=50585194", "title": "Charles Edward Foister", "text": "Charles Edward Foister\n\nDr Charles Edward Foister FRSE (1903–1989) was a British botanist and plant pathologist. He was Director of Scottish Agricultural Scientific Services in Edinburgh from 1957. He specialised in lichens and fungi.\n\nHe was born in Cambridge in England on 17 August 1903, the son of Frederick W Foister and his wife Esther Elizabeth Smith. He was educated locally and won a place at Cambridge University graduating BA in 1925. He continued as a postgraduate taking a Diploma in Agricultural Science (1927). He later received a doctorate (PhD) from Edinburgh University.\n\nHe was employed as a plant pathologist in eastern Edinburgh for all of his working life. He became the official plant pathologist for Britain in 1938. He was an active member of the Botanical Society of Edinburgh.\n\nIn 1954 he was elected a Fellow of the Royal Society of Edinburgh. His proposers were Sir William Wright Smith, Stephen J Watson, Malcolm Wilson and Alexander Nelson.\n\nHe died at Colchester in Essex on 23 July 1989.\n\nHe never married and was presumed homosexual.\n\n"}
{"id": "23478375", "url": "https://en.wikipedia.org/wiki?curid=23478375", "title": "Clinical professor", "text": "Clinical professor\n\nClinical professor, also known as professor of practice, is an academic appointment made to a member of a profession who is associated with a university and engages in practical instruction of professional students. Titles in this category may include Clinical Instructor, Assistant Clinical Professor, Associate Clinical Professor, and Clinical Professor. Clinical professorship generally does not offer a \"tenure track\", but can be either full or part-time, and is typically noted for its emphasis on practical skills training as opposed to theoretical matters. Thus, most members of such faculty are expected to have considerable practical experience in their respective fields of expertise; unlike with most other faculty, this is deemed at least as important as educational credentials. For administrative purposes, some universities classify such a designation as equivalent to \"adjunct professor\". Clinical professors may be salaried or may teach as a volunteer.\n\nIn the field of medicine, full clinical professors generally work full time at the medical college and its affiliated hospitals, while assistant clinical professors are part time with their full practice elsewhere, with little involvement in medical education; the assistant clinical professor position may be almost entirely honorary. In Canada, doctors who teach are called \"preceptors\".\n\n"}
{"id": "8435386", "url": "https://en.wikipedia.org/wiki?curid=8435386", "title": "Cosmopolitan democracy", "text": "Cosmopolitan democracy\n\nCosmopolitan democracy is a political theory which explores the application of norms and values of democracy at the transnational and global sphere. It argues that global governance of the people, by the people, for the people is possible and needed. Writers advocating cosmopolitan democracy include Immanuel Kant, David Held, Daniele Archibugi, Richard Falk, and Mary Kaldor.\nIn the cosmopolitan democracy model, decisions are made by those affected, avoiding a single hierarchical form of authority. According to the nature of the issues at stake, democratic practice should be reinvented to take into account the will of stakeholders. This can be done either through direct participation or through elected representatives. The model advocated by cosmopolitan democrats is confederal and decentralized—global governance without world government—unlike those models of global governance supported by classic World Federalism thinkers, such as Albert Einstein.\n\nThe victory of Western liberal states ending the Cold War inspired the hope that international relations could be guided by the ideals of democracy and the rule of law. In the early 1990s, a group of thinkers developed the political project of cosmopolitan democracy with the aim of providing intellectual arguments in favour of an expansion of democracy, both within states and at the global level. While some significant successes have been achieved in terms of democratization within states, much less has been attained in democratizing the global system.\n\nIn different forms, the necessity to expand democratic procedures beyond the nation-state has been supported by political philosopher Jürgen Habermas, and sociologist Ulrich Beck.\n\nCriticisms of cosmopolitan democracy have come from realist, marxist, communitarian and multicultural perspectives. Democratic theorist Robert Dahl has expressed his doubts about the possibility of expanding democracy in international organizations to any significant degree, as he believes that democracy diminishes with size. Opponents of Dahl's approach point to the fact that bigger countries are not necessarily less democratic. For example, there is no correlation between voters' turnout and population size; in fact it is smallest in countries with fewer than 100,000 citizens. \n\nThe idea of cosmopolitan democracy has been advocated with reference to the reform of international organizations. This includes the institution of the International Criminal Court, a directly elected World Parliament or world assembly of governments, and more widely the democratization of international organizations. Supporters of cosmopolitan democracy have been sceptical about the effectiveness of military interventions, even when they are apparently motivated by humanitarian intentions. They have instead suggested popular diplomacy and arms control.\n\n\n"}
{"id": "2435562", "url": "https://en.wikipedia.org/wiki?curid=2435562", "title": "Cultural cringe", "text": "Cultural cringe\n\nCultural cringe, in cultural studies and social anthropology, is an internalized inferiority complex that causes people in a country to dismiss their own culture as inferior to the cultures of other countries. It is closely related to the concept of colonial mentality and is often linked with the display of anti-intellectual attitudes towards thinkers, scientists, and artists who originate from a colonial or post-colonial nation. It can also be manifested in individuals in the form of cultural alienation. \n\nIn 1894, Australian bush poet Henry Lawson wrote in his preface to his \"Short Stories in Prose and Verse\":\n\nThe term \"cultural cringe\" was coined in Australia after the Second World War by the Melbourne critic and social commentator A. A. Phillips, and defined in an influential and highly controversial 1950 essay of the same name. It explored ingrained feelings of inferiority that local intellectuals struggled against, and which were most clearly pronounced in the Australian theatre, music, art and letters. The implications of these insights potentially applied to all former colonial nations, and the essay is now recognised as a cornerstone in the development of post-colonial theory in Australia. In essence, Phillips pointed out that the public widely assumed that anything produced by local dramatists, actors, musicians, artists and writers was necessarily deficient when compared against the works of their British and European counterparts. In the words of the poet Chris Wallace-Crabbe (quoted by Peter Conrad), Australia was being made to rhyme with failure. The only ways local arts professionals could build themselves up in public esteem was either to follow overseas fashions, or, more often, to spend a period of time working in Britain.\n\nAs Lawson continued in his 1894 preface: \"The same paltry spirit tried to dispose of the greatest of modern short-story writers as 'The Californian Dickens', but America wasn't built that way – neither was Bret Harte!\" The cultural cringe of Australians and the cultural swagger of Americans reflects deep contrasts between the American and the Australian experiences of extricating themselves from English apron-strings. Dealing specifically with Australia, Phillips pointed out that \"sport\" has been the only field in which ordinary people accepted that their nation was able to perform and excel internationally. Indeed, while they prided themselves on the qualities of locally produced athletes and sportsmen, whom they invariably considered first rate, Australians behaved as if in more intellectual pursuits the nation generated only second-rate talent. Some commentators believe that cultural cringe contribute to the perceived anti-intellectualism that has underpinned public life in Australia.\n\nThe cultural cringe is tightly connected with \"cultural alienation\", that is, the process of devaluing or abandoning one's own culture or cultural background. A person who is culturally alienated places little value on their own or host culture, and instead hungers for that of a – sometimes imposed – colonising nation. The post-colonial theorists Bill Ashcroft, Gareth Griffiths and Helen Tiffin link alienation with a sense of dislocation or displacement some peoples (especially those from immigrant cultures) will feel when they look to a distant nation for their values. Culturally alienated societies often exhibit a weak sense of cultural self-identity and place little worth on themselves. It has been argued that the most common manifestation of this alienation among peoples from post-colonial nations at present is an appetite for all things American, from television and music, to clothing, slang, even names. However, the popularity of American culture across both formerly colonized and colonizer countries possibly negate this argument. Culturally alienated individuals will also exhibit little knowledge or interest in the history of their host society, placing no real value on such matters.\n\nThe issue of cultural alienation has led the Australian sociologists Brian Head and James Walter to interpret the cultural cringe as the belief that one's own country occupies a \"subordinate cultural place on the periphery\" and that \"intellectual standards are set and innovations occur elsewhere.\" As a consequence, a person who holds this belief is inclined to devalue their own country's cultural, academic and artistic life, and to venerate the \"superior\" culture of another (colonising) country.\n\nA more sophisticated approach to the issues raised by the cultural cringe, as felt by artistic practitioners in former colonies around the world, was developed and advanced by the Australian art historian Terry Smith in his essay 'The Provincialism Problem'.\n\nThe term cultural cringe is most commonly used in Australia, where it is believed by some to be a fact of Australian cultural life.\nIn \"Another Look at the Cultural Cringe\", the Australian academic Leonard John Hume examined the idea of cultural cringe as an oversimplification of the complexities of Australian history and culture. His controversial essay argues that \"The cultural cringe ... did not exist, but it was needed, and so it was invented.\"\n\nThe cultural cringe can be expressed in the almost obsessive curiosity of Australians to know what foreigners think of Australia and its culture.\n\nSome commentators claim the cultural cringe particularly affects local television programming in Australia, which is heavily influenced by imported shows, mainly of American and British origin. The Federal government has legislated to keep a quota of Australian content (Australian Content Standard and Television Program Standard 23).\n\nSome argue that a form of cultural cringe resulted in anti-heritage attitudes which led to the demolition of many world class pre-war buildings in Melbourne, Brisbane and Sydney, destroying some of the world's best examples of Victorian architecture. Modernism was promoted to many Australians as casting off imperial Europe to rebuild a new independent identity, and the existing pre-war architecture, which was a feature of Australian cities, was denigrated. This resulted in many calls to demolish the Royal Exhibition Building, labelled the derogatory term \"white elephant\". It was not until Queen Elizabeth II granted the building Royal status that Australians began to recognise its value. The building became the first in Australia to be given World Heritage status. This reaction against the cultural cringe continues in some fields such as architecture, where local architects are shunned for using introduced styles.\n\nIt has also been claimed that cultural cringe has led to federal government information technology contracts going to large foreign multinationals, rather than domestic IT companies.\n\nAnother manifestation of cultural cringe is the \"convict stain\". For several generations following the cessation of penal transportation, many Australians felt a sense of shame about being descended from convicts, and many did not even attempt to investigate their families' origins for fear of finding convicts in their lineage. In recent decades community attitudes have changed, and many Australians with convict ancestors are now more comfortable investigating and discussing their past, wearing their forebears status almost as a badge of pride. Colloquially, attempts by non-Australians to negatively connote convict pasts are laughed off by Australians, who are now more inclined to associate criminal forebears as evidence for the possession of more positively perceived Australian attributes such as anti-authoritarianism.\n\nIn Brazil, the phrase \"complexo de vira-lata\" (the \"Mongrel complex\") denotes the same as cultural cringe. It was allegedly coined by playwright and journalist, Nelson Rodrigues, in the 1950s. The term is often evoked to criticize some attitude by any Brazilian toward a foreign culture or politics deemed as submissive and self-dismissive.\n\nMany cultural commentators in Canada have suggested that a similar process also operates in that country. The specific phrase \"cultural cringe\" is not widely used to label the phenomenon in Canada, although it has been used in isolated instances; more typically, Canadian cultural commentators speak of a \"Canadian inferiority complex\" or label specific instances of the phenomenon with satirical terms such as \"beaver hour\".\n\nPrior to the 1970s, Canadian radio stations gave almost no airtime to Canadian music, and apart from CBC Television, Canadian television stations spent very little money on Canadian-produced programming; in response the Canadian Radio-television and Telecommunications Commission (CRTC) developed Canadian content requirements for radio and broadcasters. Whether these 'Cancon' rules are necessary remains controversial.\n\nNew Zealanders are said to suffer from a cultural cringe, which has been wearing off in recent years. The New Zealand English accent is said to have been influenced by a cultural cringe since the 1900s but it too is lessening in recent years. It appears that the attitude has lessened markedly in the 2000s following the success of the award-winning \"Lord of the Rings\" movie trilogy which showcased much New Zealand scenery and film-making talent and boosted international awareness of New Zealand. The TV show \"Flight of the Conchords\", which screened on HBO in the United States from 2007–2009, furthered American and international awareness of New Zealand, as have motion pictures such as \"The World's Fastest Indian\".\n\nScottish First Minister Jack McConnell claimed a \"Scottish cringe\" in relation to Scotland's disdain for free enterprise.\n\n\n"}
{"id": "59227670", "url": "https://en.wikipedia.org/wiki?curid=59227670", "title": "Darlene Lim", "text": "Darlene Lim\n\nDarlene Sze Shien Lim is a NASA geobiologist and exobiologist preparing astronauts for space colonization. She is considered a leading expert in Mars analog missions, having presented them publicly for NPR, academic societies, and print media.\nLim is a first-generation Canadian; her parents emigrated from Singapore when she was a child. She grew up \"...spending time in the Canadian Rockies, and watching [Jacques Cousteau] on TV.\" She studied biology while an undergraduate at Queen's University, and credits Professor John Smol with igniting her interest in limnology, the study of lakes and ponds. Lim completed a doctoral degree in geology from the University of Toronto, studying limnology. She was already working on NASA-sponsored projects - such as the Haughton Mars Project, which involved studying arctic craters as Mars simulated environments - during her graduate studies. She became a postdoctoral researcher with Dr. Chris McKay at NASA's Ames Research Center in 2004, and later became a staff scientist and project leader.\nIn popular media\n\nAs part of her activities with SHAD, Lim gave \"...a dozen radio interviews\" with CBC throughout her undergraduate and Ph.D. Given her career was inspired in part by Jacques Cousteau, a fitting panel appearance with Jean-Michel Cousteau occurred in 2003 at Idea City. From 2008-2009, Lim's work was part of IGLO, a traveling exhibit about exploring other planets. She was the lead guest on the NASA Ames podcast in 2016. A profile of her Mars-simulation colony in Hawai'i (BASALT) appeared in the Chicago Tribune. Lim appeared on the SAGANet \"Ask an Astrobiologist!\" streaming program in 2017. She appeared twice on the \"Science Friday\" radio hour in 2018, contributing a 25-minute segment about undersea volcanic exploration tuned into by over 1 million listeners. Lim participated in the Frontiers for Life in Space panel at the MIT Media Lab. She was a judge in the HP \"Home Mars\" VR competition in 2018. \n\nVolunteer service\n\nLim serves on the Ocean Exploration Advisory Board of NOAA. She served as a Scientist-in-Residence at the government of Canada's \"Marsville\" program from 2000 to 2002. From 2009-2015, she served as Co-Chair of the Mars Exploration Program Analysis Group Goal IV (Prepare for Human Exploration). Lim founded the Haven House Family Shelter STEM Explorer's Speaker series, to educate homeless children in San Francisco. \n\nLim has taken a decidedly nontraditional path, choosing government labs and public-facing space research over a traditional academic career. Lim has explored extreme environments worldwide, from Hawai'i and Florida to the Arctic and Antarctic. Lim worked on data analysis for the \"Nautilus\" project, an initiative of Robert Ballard's to explore and map the deep ocean. In 2000, Lim was an inaugural crew member in the first-ever Mars simulated colony in the Arctic (FMARS). She established the Pavilion Lake Research Project in 2004, studying microbial geologic formations underwater. In a 2017 interview, newly-minted astronaut Zena Cardman specifically calls out LIm as the reason she became interested in NASA exobiology projects. Lim is the Principal Investigator of SUBSEA, a biogeochemical analogue study for life on other planets. This work hypothesizes future human exploration of Europa and Enceladus, two Solar System moons with potentially habitable environments. Lim and colleague Jennifer Heldmann scouted extreme environments in Iceland in anticipation of a new mission kick-off in 2019.\n\n"}
{"id": "7927191", "url": "https://en.wikipedia.org/wiki?curid=7927191", "title": "Dimensional metrology", "text": "Dimensional metrology\n\nDimensional metrology is the science of calibrating and using physical measurement equipment to quantify the physical size of or distance from any given object.\n\nEarly Mesopotamian and Egyptian metrologists created a set of measurement standards based on body measures such as fingers, palms, hands, feet, Cubits, and paces and agricultural measures such as feet, yards, paces, fathoms, rods, cords, perch, stadia, miles and degrees of the Earth's circumference. Early Egyptian rulers based on units of fingers, palms and feet based on inscription grids that incorporated standards of measure as canons of proportion were made commensurate with Mesopotamian standards based on fingers, hands and feet so that four palms or three hands equaled one foot and ten hands equaled one meter. These standards which were used to measure and define property such as buildings and fields were adopted by the Greeks, Romans and Persians as legal standards and became the basis of European standards of measure. They were also used to relate length to area with units such as the khet, setat and aroura, area to volume with units such as the artaba and space to time with units such as the Egyptian minute of march, the itrw which recorded an hours travel on a river, and the days sail. Specialized units for carpenters, masons and other craftsmen such as the remen were worked into a system of unit fractions that allowed calculations utilizing analytic geometry. Carpenters and surveyors were some of the first dimensional inspectors.\n\nModern measurement equipment include hand tools, CMMs (coordinate-measuring machines), machine vision systems, laser trackers, and optical comparators. For hand tools, see gauge. A CMM is based on CNC technology to automate measurement of Cartesian coordinates using a touch probe, contact scanning probe, or non-contact sensor. Optical comparators are used when physically touching the part is undesirable. Instruments can now build 3D models of a part and its internal passages using x-ray technology or 3D laser scanners.\n\nMeasurements are often expressed as a size relative to a theoretically perfect part that has geometry defined in a print or computer model. A print is a blueprint illustrating the defined geometry of a part and its features. Each feature can have a size, a distance from other features, and a tolerance set on each element. The international language used to describe physical parts is called GD&T. Prints can be hand drawn or automatically generated by a computer CAD model. Computer controlled measurement systems can measure a part relative to a CAD model without the need for a print.\n\nIndustrial metrology is common in manufacturing quality control systems. The prints and CAD models described above are usually made by a mechanical engineer.\n\n\n"}
{"id": "3420840", "url": "https://en.wikipedia.org/wiki?curid=3420840", "title": "Emanoil Bacaloglu", "text": "Emanoil Bacaloglu\n\nEmanoil Bacaloglu (; 11 April 1830 – 30 August 1891) was a Wallachian and Romanian mathematician, physicist and chemist and a scubadiver, medic, shoe salesman, a vegetarian,policeman and fireman.\n\nBorn in Bucharest and of Greek origin, he studied physics and mathematics in Paris and Leipzig, later becoming a professor at the University of Bucharest and, in 1879, a member of the Romanian Academy. Considered to be the founder of many scientific and technological fields in Romania (and aiding in the creation of the Romanian Athenaeum), Bacaloglu was also an accomplished scientist. He helped create Romanian-language terminology in his fields and was one of the principal founders of the Society of Physical Sciences in 1890.\n\nHe was also a participant in the 1848 Wallachian revolution.\n\nHe is known for the \"Bacaloglu pseudosphere\". This is a surface of revolution for which the \"Bacaloglu curvature\" is constant.\n\n\n\n"}
{"id": "40542571", "url": "https://en.wikipedia.org/wiki?curid=40542571", "title": "Equine herpesvirus 9", "text": "Equine herpesvirus 9\n\nEquine herpesvirus 9 (EHV-9) is a virus of the family Herpesviridae that isolated from a case of epizootic encephalitis in a herd of Thomson's gazelle (Gazella thomsoni) in 1993. Fatal encephalitis was reported from Thomson's gazelle, Giraffe, and Polar bear in natural infections. The virus was reported in an aborted Persian onager and a polar bear.\n"}
{"id": "263902", "url": "https://en.wikipedia.org/wiki?curid=263902", "title": "Evanescent field", "text": "Evanescent field\n\nIn electromagnetics, an evanescent field, or evanescent wave, is an oscillating electric and/or magnetic field that does not propagate as an electromagnetic wave but whose energy is spatially concentrated in the vicinity of the source (oscillating charges and currents). Even when there in fact is an electromagnetic wave produced (e.g., by a transmitting antenna) one can still identify as an evanescent field the component of the electric or magnetic field that cannot be attributed to the propagating wave observed at a distance of many wavelengths (such as the far field of a transmitting antenna).\n\nA hallmark of an evanescent field is that there is no net energy flow in that region. Since the net flow of electromagnetic energy is given by the average Poynting vector, that means that the Poynting vector in these regions, as averaged over a complete oscillation cycle, is zero.\n\nIn many cases one cannot simply say that a field is or is not evanescent. For instance, in the above illustration energy is indeed transmitted in the horizontal direction. The field strength drops off exponentially away from the surface, leaving it concentrated in a region very close to the interface, for which reason this is referred to as a surface wave. However, there is \"no\" propagation of energy \"away\" from (or toward) the surface (in the z direction), so that one could properly describe the field as being \"evanescent in the z direction.\" This is one illustration of the inexactness of the term. In most cases where they exist, evanescent fields are simply thought of and referred to as electric or magnetic fields, without the evanescent property (zero average Poynting vector in one or all directions) ever being pointed out. The term is especially applied to differentiate a field or solution from cases where one normally expects a propagating wave.\n\nEveryday electronic devices and electrical appliances are surrounded by large fields which have this property. Their operation involves alternating voltages (producing an electric field between them) and alternating currents (producing a magnetic field around them). The term \"evanescent\" is never heard in this ordinary context. Rather, there may be concern with inadvertent production of a propagating electromagnetic wave and thus discussion of reducing radiation losses (since the propagating wave steals power from the circuitry) or interference. On the other hand, \"evanescent field\" is used in various contexts where there \"is\" a propagating (even if confined) electromagnetic wave involved, to describe accompanying electromagnetic components which do not have that property. Or in some cases where there would \"normally\" be an electromagnetic wave (such as light refracted at the interface between glass and air) the term is invoked to describe the field when that wave is suppressed (such as with light in glass incident on an air interface beyond the critical angle).\n\nAlthough all electromagnetic fields are classically governed according to Maxwell's equations, different technologies or problems have certain types of expected solutions, and when the primary solutions involve wave propagation the term \"evanescent\" is frequently applied to field components or solutions which do not share that property. For instance, the propagation constant of a hollow metal waveguide is a strong function of frequency (a so-called dispersion relation). Below a certain frequency (the cut-off frequency) the propagation constant becomes an imaginary number. A solution to the wave equation having an imaginary wavenumber does \"not\" propagate as a wave but falls off exponentially, so the field excited at that lower frequency is considered evanescent. It can also be simply said that propagation is \"disallowed\" for that frequency. The formal solution to the wave equation can describe modes having an identical form, but the change of the propagation constant from real to imaginary as the frequency drops below the cut-off frequency totally changes the physical nature of the result. One can describe the solution then as a \"cut-off mode\" or an \"evanescent mode\"; while a different author will just state that no such mode exists. Since the evanescent field corresponding to the mode was computed as a solution to the wave equation, it is often discussed as being an \"evanescent wave\" even though its properties (such as not carrying energy) are inconsistent with the definition of wave.\n\nAlthough this article concentrates on electromagnetics, the term \"evanescent\" is used similarly in fields such as acoustics and quantum mechanics where the wave equation arises from the physics involved. In these cases, solutions to the wave equation resulting in imaginary propagation constants are likewise termed \"evanescent\" and have the essential property that no net energy is transmitted even though there is a non-zero field.\n\nIn optics and acoustics, evanescent waves are formed when waves traveling in a medium undergo total internal reflection at its boundary because they strike it at an angle greater than the so-called \"critical angle\". The physical explanation for the existence of the evanescent wave is that the electric and magnetic fields (or pressure gradients, in the case of acoustical waves) cannot be discontinuous at a boundary, as would be the case if there was no evanescent wave field. In quantum mechanics, the physical explanation is exactly analogous—the Schrödinger wave-function representing particle motion normal to the boundary cannot be discontinuous at the boundary.\n\nElectromagnetic evanescent waves have been used to exert optical radiation pressure on small particles to trap them for experimentation, or to cool them to very low temperatures, and to illuminate very small objects such as biological cells or single protein and DNA molecules for microscopy (as in the total internal reflection fluorescence microscope). The evanescent wave from an optical fiber can be used in a gas sensor, and evanescent waves figure in the infrared spectroscopy technique known as attenuated total reflectance.\n\nIn electrical engineering, evanescent waves are found in the near-field region within one third of a wavelength of any radio antenna. During normal operation, an antenna emits electromagnetic fields into the surrounding nearfield region, and a portion of the field energy is reabsorbed, while the remainder is radiated as EM waves.\n\nRecently, a graphene-based Bragg grating (one-dimensional photonic crystal) has been fabricated and demonstrated its competence for excitation of surface electromagnetic waves in the periodic structure using a prism coupling technique.\n\nIn quantum mechanics, the evanescent-wave solutions of the Schrödinger equation give rise to the phenomenon of wave-mechanical tunneling.\n\nIn microscopy, systems that capture the information contained in evanescent waves can be used to create super-resolution images. Matter radiates both propagating and evanescent electromagnetic waves. Conventional optical systems capture only the information in the propagating waves and hence are subject to the diffraction limit. Systems that capture the information contained in evanescent waves, such as the superlens and near field scanning optical microscopy, can overcome the diffraction limit; however these systems are then limited by the system's ability to accurately capture the evanescent waves. The limitation on their resolution is given by\n\nwhere formula_2 is the maximum wave vector that can be resolved, formula_3 is the distance between the object and the sensor, and formula_4 is a measure of the quality of the sensor.\n\nMore generally, practical applications of evanescent waves can be classified as (1) those in which the energy associated with the wave is used to excite some other phenomenon within the region of space where the original traveling wave becomes evanescent (for example, as in the total internal reflection fluorescence microscope) or (2) those in which the evanescent wave couples two media in which traveling waves are allowed, and hence permits the transfer of energy or a particle between the media (depending on the wave equation in use), even though no traveling-wave solutions are allowed in the region of space between the two media. An example of this is so-called \"wave-mechanical tunnelling\", and is known generally as \"evanescent wave coupling\".\n\nFor example, consider total internal reflection in two dimensions, with the interface between the media lying on the x axis, the normal along y, and the polarization along z. One might naively expect that for angles leading to total internal reflection, the solution would consist of an incident wave and a reflected wave, with no transmitted wave at all, but there is no such solution that obeys Maxwell's equations. Maxwell's equations in a dielectric medium impose a boundary condition of continuity for the components of the fields \"E, H, D\", and \"B\". For the polarization considered in this example, the conditions on \"E\" and \"B\" are satisfied if the reflected wave has the same amplitude as the incident one, because these components of the incident and reflected waves superimpose destructively. Their \"H\" components, however, superimpose constructively, so there can be no solution without a non-vanishing transmitted wave. The transmitted wave cannot, however, be a sinusoidal wave, since it would then transport energy away from the boundary, but since the incident and reflected waves have equal energy, this would violate conservation of energy. We therefore conclude that the transmitted wave must be a non-vanishing solution to Maxwell's equations that is not a traveling wave, and the only such solutions in a dielectric are those that decay exponentially: evanescent waves.\n\nMathematically, evanescent waves can be characterized by a wave vector where one or more of the vector's components has an imaginary value. Because the vector has imaginary components, it may have a magnitude that is less than its real components. If the angle of incidence exceeds the critical angle, then the wave vector of the transmitted wave has the form\nwhich represents an evanescent wave because the \"y\" component is imaginary. (Here α and β are real and \"i\" represents the imaginary unit.)\n\nFor example, if the polarization is perpendicular to the plane of incidence, then the electric field of any of the waves (incident, reflected, or transmitted) can be expressed as\n\nwhere formula_7 is the unit vector in the \"z\" direction.\n\nSubstituting the evanescent form of the wave vector k (as given above), we find for the transmitted wave:\n\nwhere α is the attenuation constant and β is the phase constant.\n\nEspecially in optics, \"evanescent-wave coupling\" refers to the coupling between two waves due to physical overlap of what would otherwise be described as the evanescent fields corresponding to the propagating waves.\n\nOne classical example is frustrated total internal reflection in which the evanescent field very close (see graph) to the surface of a dense medium at which a wave normally undergoes total internal reflection overlaps another dense medium in the vicinity. This disrupts the totality of the reflection, diverting some power into the second medium.\n\nCoupling between two optical waveguides may be effected by placing the fiber cores close together so that the evanescent field generated by one element excites a wave in the other fiber. This is used to produce fiber optic splitters and in fiber tapping. At radio (and even optical) frequencies, such a device is called a directional coupler. The device is usually called a power divider in the case of microwave transmission and modulation.\nEvanescent-wave coupling is synonymous with near field interaction in electromagnetic field theory. Depending on the nature of the source element, the evanescent field involved is either predominantly electric (capacitive) or magnetic (inductive), unlike (propagating) waves in the far field where these components are connected (identical phase, in the ratio of the impedance of free space). The evanescent wave coupling takes place in the non-radiative field near each medium and as such is always associated with matter; i.e., with the induced currents and charges within a partially reflecting surface. Other commonplace examples are the coupling between the primary and secondary coils of a transformer, or between the two plates of a capacitor. In quantum mechanics the wave function interaction may be discussed in terms of particles and described as quantum tunneling.\n\nEvanescent wave coupling is commonly used in photonic and nanophotonic devices as waveguide sensors or couplers (see e.g., prism coupler).\n\nEvanescent wave coupling is used to excite, for example, dielectric microsphere resonators.\n\nA typical application is resonant energy transfer, useful, for instance, for charging electronic gadgets without wires. A particular implementation of this is WiTricity; the same idea is also used in some Tesla coils.\n\nEvanescent coupling, as near field interaction, is one of the concerns in electromagnetic compatibility.\n\nCoupling of optical fibers without loss for fiber tapping.\n\nEvanescent wave coupling plays a major role in the theoretical explanation of extraordinary optical transmission.\n\nEvanescent wave coupling is used in powering devices wirelessly.\n\nA total internal reflection fluorescence microscope uses the evanescent wave produced by total internal reflection to excite fluorophores close to a surface. This is useful when surface properties of biological samples need to be studied.\n\n"}
{"id": "51524515", "url": "https://en.wikipedia.org/wiki?curid=51524515", "title": "Fizzy extraction", "text": "Fizzy extraction\n\nFizzy extraction is an extraction technique developed by Paweł Urban's group for analysis of semivolatile species dissolved in liquid matrices. It is used to extract semivolatile compounds from liquid samples. Unlike in sparging methods, such as \"purge-closed loop\", effervescence plays the key role in fizzy extraction. A liquid sample is subjected to a small overpressure (ca. 150 kPa) of a carrier gas (e.g. CO) under stirring. An abrupt decompression of the extraction chamber leads to effervescence. The resulting bubbles with volatiles/semivolatiles are liberated into the headspace of the extraction chamber within few seconds, and transferred to an online detector (e.g. mass spectrometer). The advantages of this extraction are speed, simplicity, and operation at low temperature.\n\n"}
{"id": "460086", "url": "https://en.wikipedia.org/wiki?curid=460086", "title": "Foot-and-mouth disease virus", "text": "Foot-and-mouth disease virus\n\nThe foot-and-mouth disease virus (FMDV) is the pathogen that causes foot-and-mouth disease. It is a picornavirus, the prototypical member of the genus \"Aphthovirus\". The disease, which causes vesicles (blisters) in the mouth and feet of bovids, suids, ovids, caprids and other cloven-hoofed animals is highly infectious and a major plague of animal farming.\n\nThe virus particle (25-30 nm) has an icosahedral capsid made of protein, without envelope, containing a single strand of ribonucleic acid (RNA) containing a positive encoding of its genome. When the virus comes in contact with the membrane of a host cell, it binds to a receptor site and triggers a folding-in of the membrane. Once the virus is inside the host cell, the capsid dissolves, and the RNA gets replicated, and translated into viral proteins by the cell's ribosomes using a cap-independent mechanism driven by the internal ribosome entry site element.\n\nThe synthesis of viral proteins include 2A 'cleavage' during translation. They include proteases that inhibit the synthesis of normal cell proteins, and other proteins that interact with different components of the host cell. The infected cell ends up producing large quantities of viral RNA and capsid proteins, which are assembled to form new viruses. After assembly, the host cell lyses (bursts) and releases the new viruses.\n\nThe foot-and-mouth disease virus occurs in seven major serotypes: O, A, C, SAT-1, SAT-2, SAT-3, and Asia-1. These serotypes show some regionality, and the O serotype is most common.\n"}
{"id": "28197922", "url": "https://en.wikipedia.org/wiki?curid=28197922", "title": "Gas slug", "text": "Gas slug\n\nA gas slug is a conglomerate of high pressure gas bubbles that forms within certain volcanoes, the agittation of which is a key driving factor in Strombolian eruptions. They start out as small bubbles of gas inside of volcanic magma. These accumulate into one large bubble, which starts to rise through the lava plume. Once this accumulated slug reaches the top of the column and comes in contact with air, it bursts with a loud pop because of the lower air pressure, throwing magma into the air in the typical lava arc of a Strombolian eruption. This type of eruption is episodic, non-damaging to its source vent, and one of the slowest forms of activity, with the ability to sustain itself for thousands of years. Although the effect of gas slugs in lava is well understood, how they form is not well understood, but recent research suggests that they can form as deep as under the surface.\n"}
{"id": "15843784", "url": "https://en.wikipedia.org/wiki?curid=15843784", "title": "Hoelite", "text": "Hoelite\n\nHoelite is a mineral, discovered in 1922 at Mt. Pyramide, Spitsbergen, Norway and named after Norwegian geologist Adolf Hoel (1879–1964). Its chemical formula is CHO (9,10-anthraquinone).\n\nIt is a very rare organic mineral which occurs in coal fire environments in association with sal ammoniac and native sulfur.\n"}
{"id": "35528199", "url": "https://en.wikipedia.org/wiki?curid=35528199", "title": "ISCB Africa ASBCB Conference on Bioinformatics", "text": "ISCB Africa ASBCB Conference on Bioinformatics\n\nThe ISCB Africa ASBCB Conference on Bioinformatics is a biennial academic conference on the subjects of bioinformatics and computational biology, organized by the African Society for Bioinformatics and Computational Biology (ASBCB). The conference was first held in 2007 as the \"ASBCB Conference on the Bioinformatics of African Pathogens, Hosts and Vectors\". Since 2009, the conference has been jointly organized with the International Society for Computational Biology (ISCB) and held in different locations within Africa. Although having an evident African focus, the meeting is intended to be a truly international event, encompassing scientists and students from leading institutions in the US, Latin America, Europe and Africa. Holding this event in Africa, ISCB and ASBCB intend to promote local efforts for cooperation and dissemination of leading research techniques to combat major African diseases.\n\nThe meeting usually consists of a 3-day conference followed by practical workshops. The main 3-day meeting includes keynote presentations by up to 6 invited speakers from around\nthe world, including Africa. Session Chairs introduce Keynote Speakers with an overview of the session, highlighting the most significant challenges and the current state of the art in the\nfield before the keynote speakers launch their presentations. Highly accomplished researchers, primarily but not exclusively from non-African countries, present during the post conference tutorial workshops.\n\n\nSince 2009, the ISCB Africa ASBCB Conference has been partnering with the Genes, Infection and Evolution journal to publish top papers presented at the conference.\n\n"}
{"id": "38448365", "url": "https://en.wikipedia.org/wiki?curid=38448365", "title": "Index of physics articles (L)", "text": "Index of physics articles (L)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "38448486", "url": "https://en.wikipedia.org/wiki?curid=38448486", "title": "Index of physics articles (U)", "text": "Index of physics articles (U)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "1789863", "url": "https://en.wikipedia.org/wiki?curid=1789863", "title": "Interactional justice", "text": "Interactional justice\n\nInteractional justice is defined by sociologist John R. Schermerhorn as the \"...degree to which the people affected by decision are treated by dignity and respect.\" ( John R. Schermerhorn, Organizational behavior) The theory focuses on the interpersonal treatment people receive when procedures are implemented.\n\nInteractional justice, a subcomponent of organizational justice, has come to be seen as consisting of two specific types of interpersonal treatment (e.g. Greenberg, 1990a, 1993b). The first labeled interpersonal justice, reflects the degree to which people are treated with politeness, dignity, and respect by authorities or third parties involved in executing procedures or determining outcomes. The second, labeled informational justice, focuses on the explanations provided to people that convey information about why procedures were used in a certain way or why outcomes were distributed in a certain fashion. Where more adequacy of explanation is prevalent, the perceived level of informational justice is higher. (Sam Fricchione 2006).\n\nIt is important that a high degree of interactional justice exists in a subordinate/supervisor relationship in order to reduce the likelihood of counterproductive work behavior. If a subordinate perceives that interactional injustice exists, then the subordinate will hold feelings of resentment toward either the supervisor or institution and will therefore seek to \"even the score\". A victim of interaction injustice will have increased expressions of hostility toward the offender which can manifest in actions of counterproductive work behavior and reduce the effectiveness of organizational communication.\n\nAbuse directed toward a subordinate from a supervisor often stems from displaced aggression. In this case, the individual (supervisor) is unwilling to retaliate against the direct source of mistreatment and will therefore abuse a less threatening target such as a subordinate since the subordinate is incapable of retaliation. Thus, interactional injustice can essentially trickle-down from the top of an organization to the bottom due to displaced aggression that exists in the top ranks of the hierarchy.\n\n"}
{"id": "38087043", "url": "https://en.wikipedia.org/wiki?curid=38087043", "title": "Kyoto Prize in Advanced Technology", "text": "Kyoto Prize in Advanced Technology\n\nThe Kyoto Prize in Advanced Technology is awarded once a year by the Inamori Foundation. The Prizeis one of three Kyoto Prize categories; the others are the Kyoto Prize in Basic Sciences and the Kyoto Prize in Arts and Philosophy. The first Kyoto Prize in Advanced Technology was awarded to Rudolf E. Kálmán, the \"creator of modern control and system theory\". The Prize is widely regarded as the most prestigious award available in fields which are traditionally not honored with a Nobel Prize. \n\nThe Kyoto Prize in Advanced Technology is awarded on a rotating basis to researchers in the following four fields: \n\n"}
{"id": "57936674", "url": "https://en.wikipedia.org/wiki?curid=57936674", "title": "Lewis's triviality result", "text": "Lewis's triviality result\n\nIn the mathematical theory of probability, David Lewis's triviality result is a theorem about the impossibility of systematically equating the conditional probability formula_1 with the probability of a so-called conditional event, formula_2.\n\nThe statement \"The probability that if formula_3, then formula_4, is 20%\" means (put intuitively) that event formula_4 may be expected to occur in 20% of the outcomes where event formula_3 occurs. The standard formal expression of this is formula_7, where the conditional probability formula_1 equals, by definition, formula_9.\n\nBeginning in the 1960s, several philosophical logicians—most notably Ernest Adams and Robert Stalnaker—floated the idea that one might also write formula_10, where formula_2 is the conditional event \"If formula_3, then formula_4\". That is, given events formula_3 and formula_4, one might suppose there is an event, formula_2, such that formula_17 could be counted on to equal formula_1, so long as formula_19.\n\nPart of the appeal of this move would be the possibility of embedding conditional expressions within more complex constructions. One could write, say, formula_20, to express someone's high subjective degree of confidence (\"75% sure\") that either formula_3, or else if formula_4, then formula_23. Compound expressions containing conditional expressions might also be useful in the programming of automated decision-making systems.\n\nHow might such a convention be combined with standard probability theory? The most direct extension of the standard theory would be to treat formula_2 as an event like any other, i.e., as a set of outcomes. Adding formula_2 to the familiar Venn- or Euler diagram of formula_3 and formula_4 would then result in something like Fig. 1, where formula_28 are probabilities allocated to the eight respective regions, such that formula_29. \n\nFor formula_17 to equal formula_1 requires that formula_32, i.e., that the probability inside the formula_2 region equal the formula_34 region's proportional share of the probability inside the formula_3 region. In general the equality will of course not be true, so that making it reliably true requires a new constraint on probability functions: in addition to satisfying Kolmogorov's probability axioms, they must also satisfy a new constraint, namely that formula_36 for any events formula_3 and formula_4 such that formula_19.\n\nFiring what van Fraassen (1976) called \"a veritable bombshell\" at the above proposal, Lewis (1976) pointed out a seemingly fatal problem: assuming a nontrivial set of events, the new, restricted class of formula_40-functions will not be closed under conditioning, the operation that turns probability function formula_40 into new function formula_42, predicated on event formula_23's occurrence. That is, if formula_36, it will not in general be true that formula_45 as long as formula_46. This implies that if rationality requires having a well-behaved probability function, then a fully rational person (or computing system) would become irrational simply in virtue of learning that arbitrary event formula_23 had occurred.\n\nLewis's proof is as follows. Let a set of events be non-trivial if it contains two possible events, formula_3 and formula_4, that are mutually exclusive but do not together exhaust all possibilities, so that formula_19, formula_51, formula_52, and formula_53. The existence of two such events implies the existence of the event formula_54, as well, and, if conditional events are admitted, the event formula_55. The proof derives a contradiction from the assumption that such a minimally non-trivial set of events exists.\n\nA graphical version of the proof starts with Fig. 2, where the formula_3 and formula_4 from Fig. 1 are now disjoint and formula_2 has been replaced by formula_55. By the assumption that formula_3 and formula_4 are possible, formula_88 and formula_89. By the assumption that together formula_3 and formula_4 do not together exhaust all possibilities, formula_92. And by the new constraint on probability functions, formula_93 formula_94, which means that\n\nConditioning on an event involves zeroing out the probabilities outside the event's region and increasing the probabilities inside the region by a common scale factor. Here, conditioning on formula_3 will zero out formula_97 and formula_98 and scale up formula_99 and formula_100, to formula_101 and formula_102, respectively, and so\n\nConditioning instead on formula_58 will zero out formula_99 and formula_100 and scale up formula_97 and formula_98, and so\n\nFrom (2), it follows that formula_112, and since formula_101 is the scaled-up value of formula_99, it must also be that formula_115. Similarly, from (3), formula_116. But then (1) reduces to formula_117, which implies that formula_118, which contradicts the stipulation that formula_92.\n\nIn a follow-up article (1986), Lewis noted that the triviality proof can proceed by conditioning not on formula_3 and formula_58 but instead, by turns, on each of a finite set of mutually exclusive and jointly exhaustive events formula_122 He also gave a variant of the proof that involved not total conditioning, in which the probability of either formula_3 or formula_58 is set to 1, but partial conditioning (i.e., Jeffrey conditioning), by which probability is incrementally shifted from formula_58 to formula_3.\n\nSeparately, Hájek (1989) pointed out that even without conditioning, if the number of outcomes is large but finite, then in general formula_127, being a ratio of two outputs of the formula_40-function, will take on more values than any single output of the function can. So, for instance, if in Fig. 1 formula_129are all multiples of 0.01 (as would be the case if there were exactly 100 equiprobable outcomes), then formula_17 must be a multiple of 0.01, as well, but formula_9 need not be. That being the case, formula_17 cannot reliably be made to equal formula_133.\n\nHájek also argued (1994) that the condition formula_36 caused acceptable formula_40-functions to be implausibly sparse and isolated from one another. One way to put the point: standardly, any weighted average of two probability function is itself a probability function, so that between any two formula_40-functions there will be a continuum of weighted-average formula_40-functions along which one of the original formula_40-functions gradually transforms into the other. But these continua disappear if the added formula_36 condition is imposed. Now an average of two acceptable formula_40-functions will in general not be an acceptable formula_40-function.\n\nAssuming that formula_36 holds for a minimally nontrivial set of events and for any formula_40-function leads to a contradiction. Thus formula_36 can hold for any formula_40-function only for trivial sets of events—that is the triviality result. However, the proof relies on background assumptions that may be challenged. It may be proposed, for instance, that the referent event of an expression like “formula_2” is not fixed for a given formula_3 and formula_4, but instead changes as the probability function changes. Or it may be proposed that conditioning on formula_23 should follow a rule other than formula_150.\n\nBut the most common response, among proponents of the formula_36 condition, has been to explore ways to model conditional events as something other than subsets of a universe set of outcomes. Even before Lewis published his result, Schay (1968) had modeled conditional events as ordered \"pairs\" of sets of outcomes. With that approach and others in the same spirit, conditional events and their associated combination and complementation operations do not constitute the usual algebra of sets of standard probability theory, but rather a more exotic type of structure, known as a conditional event algebra.\n\n"}
{"id": "144221", "url": "https://en.wikipedia.org/wiki?curid=144221", "title": "List of UNIVAC products", "text": "List of UNIVAC products\n\nThis is a list of UNIVAC products. It ends in 1986, the year that Sperry Corporation merged with Burroughs Corporation to form Unisys.\n\n\n\n\n\n\n\n\n\n\nThese machines implemented a variant of the IBM System/360 architecture\n\n\n\n\n\n\n\n\n"}
{"id": "9788916", "url": "https://en.wikipedia.org/wiki?curid=9788916", "title": "List of celebrity inventors", "text": "List of celebrity inventors\n\nThe following is a list of celebrity inventors and their patents. (For the purposes of this article, an inventor is a person who has been granted a patent.) After Google released a patent search online in December 2006, a website called \"Ironic Sans\", made the public aware of a number of celebrity patents found through the new patent search engine.\n\nAdditional lists of inventors can be found at List of inventors. See also .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22327569", "url": "https://en.wikipedia.org/wiki?curid=22327569", "title": "List of dental colleges in India", "text": "List of dental colleges in India\n\nThis is a list of dental colleges in India.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "40688661", "url": "https://en.wikipedia.org/wiki?curid=40688661", "title": "List of institutes and centers of the National Institutes of Health", "text": "List of institutes and centers of the National Institutes of Health\n\nThe National Institutes of Health (NIH) is an agency of the United States Department of Health and Human Services and is the primary agency of the United States government responsible for biomedical and health-related research. It comprises 27 separate institutes and centers (ICs) that carry out its mission in different areas of biomedical research. It also includes the Office of the Director, which sets policies and coordinates activities of the 27 ICs.\n\nIn addition to being divided by research area, NIH has many operating groups called centers operating across all of the Institutes.\n\nThe Office of the Director is the central office at NIH. The OD is responsible for setting policy for NIH and for planning, managing, and coordinating the programs and activities of all the NIH components. Program offices in the Office of the Director are responsible for stimulating specific areas of research throughout NIH and for planning and supporting research and related activities. Current program areas are: minority health, women's health, AIDS research, disease prevention, and behavioral and social sciences research. In July 2009, President Barack Obama nominated Dr. Francis S. Collins, M.D., PhD, to be the Director of the NIH. On August 7, 2009, the US Senate confirmed Dr. Collins by unanimous vote.\n\nProgram offices within the Office of the Director fund research through the institutes:\n"}
{"id": "3678732", "url": "https://en.wikipedia.org/wiki?curid=3678732", "title": "List of people with brain tumors", "text": "List of people with brain tumors\n\nA brain tumor is an abnormal growth of cells within the brain or inside the skull, and can be cancerous (malignant) or non-cancerous (benign). Just over half of all primary brain tumors are malignant; the rest are benign, though they may still be life-threatening. In the United States in 2000, survivors of benign primary brain tumors outnumbered those who had cancerous primary brain tumors by approximately 4:1. Metastatic brain cancer is over six times more common than primary brain cancer, as it occurs in about 10–30% of all people with cancer.\n\nThis is a list of notable people who have had a primary or metastatic brain tumor (either benign or malignant) at some time in their lives, as confirmed by public information. Tumor type and survival duration are listed where the information is known. Blank spaces in these columns appear where precise information has not been released to the public. Medicine does not designate most long-term survivors as cured.\n\nThe National Cancer Institute estimated 22,070 new cases of primary brain cancer and 12,920 deaths due to the illness in the United State in 2009. The age-adjusted incidence rate is 6.4 per 100,000 per year, and the death rate is 4.3 per 100,000 per year. The lifetime risk of developing brain cancer for someone born today is 0.60%. Only around a third of those diagnosed with brain cancer survive for five years after diagnosis. These high overall mortality rates are a result of the prevalence of aggressive types, such as glioblastoma multiforme. Nearly 14% of new brain tumor diagnoses occur in persons under 20 years of age.\n"}
{"id": "7119963", "url": "https://en.wikipedia.org/wiki?curid=7119963", "title": "List of volcanoes in Bolivia", "text": "List of volcanoes in Bolivia\n\nThe country of Bolivia hosts numerous active and extinct volcanoes across its territory. The active volcanoes are in western Bolivia making up the Cordillera Occidental, the western limit of the Altiplano plateau. Many of the active volcanoes are international mountains shared with Chile. All Cenozoic volcanoes of Bolivia are part of the Central Volcanic Zone (CVZ) of the Andean Volcanic Belt that results due to processes involved in the subduction of Nazca Plate under the South American Plate. The Central Volcanic Zone is a major upper Cenozoic volcanic province.\nApart from Andean volcanoes the geology of Bolivia host the remnants of ancient volcanoes around the Precambrian Guaporé Shield in the eastern part of the country.\n\n"}
{"id": "340091", "url": "https://en.wikipedia.org/wiki?curid=340091", "title": "M8 motorway (Scotland)", "text": "M8 motorway (Scotland)\n\nThe M8 is the busiest motorway in Scotland and one of the busiest in the United Kingdom. It connects the country's two largest cities, Glasgow and Edinburgh, and serves other large communities including Airdrie, Coatbridge, Greenock, Livingston and Paisley. The motorway is long. A major construction project to build the final section between Newhouse and Baillieston was completed on 30 April 2017. The motorway has one service station, named Heart of Scotland Services, previously Harthill due to its proximity to the village.\n\nWith the advent of motorway-building in the United Kingdom in the late 1950s, the M8 was planned as one of a core of new motorways, designed to replace the A8 road as a high-capacity alternative for intercity travel. The motorway was constructed piecemeal in several stages bypassing towns, beginning in 1965 with the opening by Minister of State for Scotland George Willis of the bypass of Harthill. In 1968 the Renfrew Bypass was opened as the A8(M), becoming part of the M8 when the motorway to the west was connected. The Glasgow inner city section was constructed between 1968 and 1972, using a scheme outlined in the Bruce Report, which was published as the Second World War was closing, and which set out a series of initiatives to regenerate the city. Bruce's scheme evolved into what would become the Glasgow Inner Ring Road, a motorway \"box\" which would encircle the city centre, connected to the Renfrew Bypass at its south western corner, and the Monkland Motorway (built over the former route of the Monkland Canal) towards Edinburgh at its north eastern corner. Together, these three sections of motorway make up the present day M8.\n\nMost of the motorway's length was complete by 1980. Since then, the only additions have been a new interchange with the M80 motorway in 1992, a eastern extension from Newbridge to the then-new Edinburgh City Bypass in 1995, and the new junction on the approach to the Kingston Bridge in Glasgow connecting to the new M74 extension in 2011. As part of the Scottish Government's 'M8 M73 M74 Motorway Improvements programme', on which construction began in early 2015, the remaining unfinished section between Baillieston (J8) and Newhouse (J6) was built, alongside other major improvements enhancing connectivity to the local road network, M73, and M74. The new section was fully opened on 30 April 2017.\n\nFrom the Edinburgh City Bypass, the road runs west to junction with the M9 motorway (for the Forth Road Bridge), bypassing to the north of Livingston and south of Bathgate. It continues across Scotland's Central Belt. The next section – originally designated the Monkland Motorway – begins on the boundary of the City of Glasgow at the M73 motorway junction (the main interchange for all routes south via the M74 motorway) before passing through the districts of Barlanark, Riddrie, Dennistoun and Townhead (following the route of the abandoned Monkland Canal) on the way directly into the city centre. The central section – the uncompleted Glasgow Inner Ring Road – contains numerous junctions serving local communities including Cowcaddens, Garnethill, Kelvingrove and Anderston. It then crosses the River Clyde on the Kingston Bridge, runs west through Kinning Park, Bellahouston and Hillington before leaving Glasgow. Continuing west, it bypasses Renfrew and Paisley (carrying traffic directly over what was the main runway at Renfrew Airport, closed in 1966) before serving Glasgow International Airport, running to the south of Erskine, and terminating at Langbank, around east of Greenock.\n\nThe M8 nominally comprises sections of the international E-road network, namely E05 (Langbank-Baillieston) and E16 (Baillieston-Edinburgh), although neither is signposted – no such roads are in the United Kingdom.\n\nThe M8, more explicitly the Glasgow section, is unusual amongst UK motorways (and more similar to many US Interstates) in that it directly serves (and bisects) a large urban area, whereas most other motorways bypass such conurbations. The central Glasgow section is elevated above much of the surrounding area on a concrete viaduct, including several incomplete constructions such as several pedestrian overpasses and adjoining arterial roads, and has slip roads that enter and exit from the passing (right-hand) lane. It contains one of the busiest river crossings in Europe at the Kingston Bridge. The speed limit there is reduced to 50 mph (80 km/h) as a result.\n\nThere were successive failed attempts to build the southern flank of the Glasgow Inner Ring Road as defined by the Bruce Report. This section of road, which is an extension of the M74, differs in route from the original Bruce proposals of the late 1940s, and was intended to funnel long distance traffic from the north and south which is bound for the southern Clyde Coast and allow it to bypass the urban section of the M8. Following many years of intensive political discussion and legal battles, construction of the M74 Completion scheme began in 2008, and it opened in June 2011. Early indications are that the new road has been successful in reducing traffic levels on the urban section of the M8.\n\nThe root cause of the high congestion on the urban section of the M8 is that traffic from the M73 and M80 is forced onto the eastern section of M8 which within converges from five lanes to two on the Kingston Bridge approaches. The result of this is often long periods of traffic congestion. Prior to the construction of the M74 extension, incentives were undertaken in an attempt to minimise delays on this section; these include restricting exits around the Kingston Bridge, a ramp metering programme, and expanded use of electronic signing above and beside the motorway as part of the CITRAC (\"Centrally Integrated TRAffic Control\") system.\n\nThe M8 represents a significant barrier to wildlife expansion, particularly the reintroduced beaver, from the north of Scotland into the Scottish Lowlands, Southern Uplands and Dumfries and Galloway.\n\n\n"}
{"id": "11102737", "url": "https://en.wikipedia.org/wiki?curid=11102737", "title": "Master of Chemistry", "text": "Master of Chemistry\n\nA Master of Chemistry (or M.Chem) degree is a specific master's degree for courses in the field of Chemistry.\n\nIn the UK, the M.Chem degree is an undergraduate award, available after pursuing a four-year course of study at a university. It is classed as a level 7 qualification in the National Qualifications Framework.\n\nIn Scotland the M.Chem degree is a 5-year course.\n\nIn terms of course structure, M.Chem degrees have the same content that is usually seen in other degree programmes, i.e. lectures, laboratory work, coursework and exams each year. There are also usually one or more substantial projects undertaken in the fourth year, which may well have research elements. At the end of the second or third years, there is usually a threshold of academic performance in examinations to be reached to allow progression into the final year. Final results are awarded on the standard British undergraduate degree classification scale.\n"}
{"id": "27599234", "url": "https://en.wikipedia.org/wiki?curid=27599234", "title": "Millennium Prize Problems", "text": "Millennium Prize Problems\n\nThe Millennium Prize Problems are seven problems in mathematics that were stated by the Clay Mathematics Institute on May 24, 2000. The problems are the Birch and Swinnerton-Dyer conjecture, Hodge conjecture, Navier–Stokes existence and smoothness, P versus NP problem, Poincaré conjecture, Riemann hypothesis, and Yang–Mills existence and mass gap. A correct solution to any of the problems results in a 1 million prize being awarded by the institute to the discoverer(s).\n\nTo date, the only Millennium Prize problem to have been solved is the Poincaré conjecture, which was solved by the Russian mathematician Grigori Perelman in 2003.\n\nIn dimension 2, a sphere is characterized by the fact that it is the only closed and simply-connected surface. The Poincaré conjecture states that this is also true in dimension 3. It is central to the more general problem of classifying all 3-manifolds. The precise formulation of the conjecture states: \n\nA proof of this conjecture was given by Grigori Perelman in 2003, based on work by Richard Hamilton; its review was completed in August 2006, and Perelman was selected to receive the Fields Medal for his solution, but he declined the award. Perelman was officially awarded the Millennium Prize on March 18, 2010, but he also declined that award and the associated prize money from the Clay Mathematics Institute. The Interfax news agency quoted Perelman as saying he believed the prize was unfair. Perelman told Interfax he considered his contribution to solving the Poincaré conjecture no greater than that of Hamilton.\n\nThe question is whether or not, for all problems for which an algorithm can \"verify\" a given solution quickly (that is, in polynomial time), an algorithm can also \"find\" that solution quickly. Since the former describes the class of problems termed NP, while the latter describes P, the question is equivalent to asking whether all problems in NP are also in P. This is generally considered one of the most important open questions in mathematics and theoretical computer science as it has far-reaching consequences to other problems in mathematics, and to biology, philosophy and cryptography (see P versus NP problem proof consequences). A common example of an NP problem not known to be in P is the Boolean satisfiability problem.\n\nMost mathematicians and computer scientists expect that P ≠ NP; however, it remains to be proven.\n\nThe official statement of the problem was given by Stephen Cook.\n\nThe Hodge conjecture is that for projective algebraic varieties, Hodge cycles are rational linear combinations of algebraic cycles.\n\nThe official statement of the problem was given by Pierre Deligne.\n\nThe Riemann hypothesis is that all nontrivial zeros of the analytical continuation of the Riemann zeta function have a real part of /. A proof or disproof of this would have far-reaching implications in number theory, especially for the distribution of prime numbers. This was Hilbert's eighth problem, and is still considered an important open problem a century later.\n\nThe official statement of the problem was given by Enrico Bombieri.\n\nIn physics, classical Yang–Mills theory is a generalization of the Maxwell theory of electromagnetism where the \"chromo\"-electromagnetic field itself carries charges. As a classical field theory it has solutions which travel at the speed of light so that its quantum version should describe massless particles (gluons). However, the postulated phenomenon of color confinement permits only bound states of gluons, forming massive particles. This is the mass gap. Another aspect of confinement is asymptotic freedom which makes it conceivable that quantum Yang-Mills theory exists without restriction to low energy scales. The problem is to establish rigorously the existence of the quantum Yang–Mills theory and a mass gap.\n\nThe official statement of the problem was given by Arthur Jaffe and Edward Witten.\n\nThe Navier–Stokes equations describe the motion of fluids, and are one of the pillars of fluid mechanics. However, theoretical understanding of their solutions is incomplete. In particular, solutions of the Navier–Stokes equations often include turbulence, for which general solutions remains one of the greatest unsolved problems in physics, despite its immense importance in science and engineering.\n\nEven basic properties of the solutions to Navier–Stokes have never been proven. For the three-dimensional system of equations, and given some initial conditions, mathematicians have not yet proved that smooth solutions always exist, or that if they do exist, they have bounded energy per unit mass. This is called the \"Navier–Stokes existence and smoothness\" problem.\n\nThe problem is to make progress towards a mathematical theory that will give insight into these equations, by proving either that smooth, globally defined solutions exist that meet certain conditions, or that they do not always exist and the equations break down. The official statement of the problem was given by Charles Fefferman.\n\nThe Birch and Swinnerton-Dyer conjecture deals with certain types of equations: those defining elliptic curves over the rational numbers. The conjecture is that there is a simple way to tell whether such equations have a finite or infinite number of rational solutions. Hilbert's tenth problem dealt with a more general type of equation, and in that case it was proven that there is no way to decide whether a given equation even has any solutions.\n\nThe official statement of the problem was given by Andrew Wiles.\n\n\n\n"}
{"id": "49681", "url": "https://en.wikipedia.org/wiki?curid=49681", "title": "Ontology (information science)", "text": "Ontology (information science)\n\nIn computer science and information science, an ontology encompasses a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities that substantiate one, many, or all domains.\n\nEvery field creates ontologies to limit complexity and organize information into data and knowledge. As new ontologies are made, their use hopefully improves problem solving within that domain. Translating research papers within every field is a problem made easier when experts from different countries maintain a controlled vocabulary of jargon between each of their languages.\n\nSince Google started an initiative called Knowledge Graph, a substantial amount of research has gone on using the phrase \"knowledge graph\" as a generalized term. Although there is no clear definition for the term knowledge graph, it is sometimes used as synonym for ontology. One common interpretation is that a knowledge graph represents a collection of interlinked descriptions of entities – real-world objects, events, situations or abstract concepts. Unlike ontologies, knowledge graphs, such as Google's Knowledge Graph and DBpedia, often contain large volumes of factual information with less formal semantics. In some contexts, the term \"knowledge graph\" is used to refer to any knowledge base that is represented as a graph.\n\nThe compound word \"ontology\" combines \"onto-\", from the Greek ὄν, \"on\" (gen. ὄντος, \"ontos\"), i.e. \"being; that which is\", which is the present participle of the verb εἰμί, \"eimí\", i.e. \"to be, I am\", and -λογία, \"-logia\", i.e. \"logical discourse\", see classical compounds for this type of word formation.\n\nWhile the etymology is Greek, the oldest extant record of the word itself, the New Latin form \"ontologia\", appeared in 1606 in the work \"Ogdoas Scholastica\" by Jacob Lorhard (\"Lorhardus\") and in 1613 in the \"Lexicon philosophicum\" by Rudolf Göckel (\"Goclenius\").\n\nThe first occurrence in English of \"ontology\" as recorded by the \"OED\" (\"Oxford English Dictionary\", online edition, 2008) came in \"Archeologia Philosophica Nova\" or \"New Principles of Philosophy\" by Gideon Harvey.\n\nWhat ontologies in both information science and philosophy have in common is the attempt to represent entities, ideas, and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of ontology engineering (e.g., Quine and Kripke in philosophy, Sowa and Guarino in computer science), and debates concerning to what extent normative ontology is possible (e.g., foundationalism and coherentism in philosophy, BFO and Cyc in artificial intelligence). Applied ontology is considered a spiritual successor to prior work in philosophy, however many current efforts are more concerned with establishing controlled vocabularies of narrow domains than first principles, the existence of fixed essences, or whether enduring objects (e.g., perdurantism and endurantism) may be ontologically more primary than processes.\n\nEvery field uses ontological assumptions to frame explicit theories, research, and applications. For instance, the definition and ontology of economics is a primacy concern in Marxist economics, but also in other subfields of economics. An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what capital assets are at risk and by how much (see risk management).\n\nArtificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing within machine translation and knowledge representation, but ontology editors are being used often in a range of fields like education without the intent to contribute to AI.\n\nOntologies arise out of the branch of philosophy known as metaphysics, which deals with questions like \"what exists?\" and \"what is the nature of reality?\" One of five traditional branches of philosophy, metaphysics is concerned with exploring existence through properties, entities, and relations such as those between particulars and universals, intrinsic and extrinsic properties, or essence and existence. Metaphysics has been an ongoing topic of discussion since recorded history.\n\nSince the mid-1970s, researchers in the field of artificial intelligence (AI) have recognized that knowledge engineering is the key to building large and powerful AI systems. AI researchers argued that they could create new ontologies as computational models that enable certain kinds of automated reasoning, which was only marginally successful. In the 1980s, the AI community began to use the term \"ontology\" to refer to both a theory of a modeled world and a component of knowledge-based systems. Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.\n\nIn the early 1990s, the widely cited Web page and paper \"Toward Principles for the Design of Ontologies Used for Knowledge Sharing\" by Tom Gruber is credited with a deliberate definition of \"ontology\" as a technical term in computer science. Gruber introduced the term as \"a specification of a conceptualization\": An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.\n\nAttempting to distance ontologies from taxonomies and similar efforts in knowledge modeling that rely on classes and inheritance, Gruber stated (1993): Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to conservative definitions — that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world. To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.\n\nAs refinement of Gruber's definition Feilmayr and Wöß (2016) stated: \"An ontology is a formal, explicit specification of a shared conceptualization that is characterized by high semantic expressiveness required for increased complexity.\"\n\nContemporary ontologies share many structural similarities, regardless of the language in which they are expressed. Most ontologies describe individuals (instances), classes (concepts), attributes, and relations. In this section each of these components is discussed in turn.\n\nCommon components of ontologies include:\n\nOntologies are commonly encoded using ontology languages.\n\nA domain ontology (or domain-specific ontology) represents concepts which belong to a part of the world, such as biology or politics. Each domain ontology typically models domain specific definitions of terms. For example, the word \"card\" has many different meanings. An ontology about the domain of poker would model the \"playing card\" meaning of the word, while an ontology about the domain of computer hardware would model the \"punched card\" and \"video card\" meanings.\n\nSince domain ontologies are written by different people, they represent concepts in very specific and unique ways, and are often incompatible within the same project. As systems that rely on domain ontologies expand, they often need to merge domain ontologies by hand-tuning each entity or using a combination of software merging and hand-tuning. This presents a challenge to the ontology designer. Different ontologies in the same domain arise due to different languages, different intended usage of the ontologies, and different perceptions of the domain (based on cultural background, education, ideology, etc.).\n\nAt present, merging ontologies that are not developed from a common upper ontology is a largely manual process and therefore time-consuming and expensive. Domain ontologies that use the same upper ontology to provide a set of basic elements with which to specify the meanings of the domain ontology entities can be merged with less effort. There are studies on generalized techniques for merging ontologies, but this area of research is still ongoing, and it's a recent event to see the issue sidestepped by having multiple domain ontologies using the same upper ontology like the OBO Foundry.\n\nAn upper ontology (or foundation ontology) is a model of the common relations and objects that are generally applicable across a wide range of domain ontologies. It usually employs a core glossary that contains the terms and associated object descriptions as they are used in various relevant domain ontologies.\n\nStandardized upper ontologies available for use include BFO, BORO method, Dublin Core, GFO, OpenCyc/ResearchCyc, SUMO, UMBEL, the Unified Foundational Ontology (UFO), and DOLCE. WordNet has been considered an upper ontology by some and has been used as a linguistic tool for learning domain ontologies.\n\nThe Gellish ontology is an example of a combination of an upper and a domain ontology.\n\nA survey of ontology visualization methods is presented by Katifori et al. An updated survey of ontology visualization methods and tools was published by Dudás et al. The most established ontology visualization methods, namely indented tree and graph visualization are evaluated by Fu et al. A visual language for ontologies represented in OWL is specified by the \"Visual Notation for OWL Ontologies (VOWL)\".\n\nOntology engineering (also called ontology building) is a set of tasks related to the development of ontologies for a particular domain. It is a subfield of knowledge engineering that studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tools and languages that support them.\n\nOntology engineering aims to make explicit the knowledge contained in software applications, and organizational procedures for a particular domain. Ontology engineering offers a direction for overcoming semantic obstacles, such as those related to the definitions of business terms and software classes. Known challenges with ontology engineering include:\n\nOntology editors are applications designed to assist in the creation or manipulation of ontologies. It's common for ontology editors to use one or more ontology languages.\n\nAspects of ontology editors include: the visual navigation possibilities within the knowledge model, inference engines and information extraction, support for modules, import & export foreign knowledge representation languages for ontology matching, and the support of meta-ontologies such as OWL-S, Dublin Core, etc.\n\nOntology learning is the automatic or semi-automatic creation of ontologies, including extracting a domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time consuming, there is great motivation to automate the process. Information extraction and text mining have been explored to automatically link ontologies to documents, for example in the context of the BioCreative challenges.\n\nAn ontology language is a formal language used to encode an ontology. There are a number of such languages for ontologies, both proprietary and standards-based:\n\n\nThe W3C Linking Open Data community project coordinates attempts to converge different ontologies into worldwide Semantic Web.\n\nThe development of ontologies has led to the emergence of services providing lists or directories of ontologies called ontology libraries.\n\nThe following are libraries of human-selected ontologies.\n\nThe following are both directories and search engines.\n\nIn general, ontologies can be used beneficially in several fields.\n\n\n\n\n"}
{"id": "33180836", "url": "https://en.wikipedia.org/wiki?curid=33180836", "title": "Orbis Latinus", "text": "Orbis Latinus\n\nOrbis Latinus, originally by Dr. J. G. Th. Graesse, is a Latin-German dictionary of Latin place names. Most recently updated in 1972, it is the most comprehensive modern reference work of Latin toponymy, covering antiquity to modern times.\n\nJohann Georg Theodor Graesse (1814–1885), a librarian, art historian, and literary scholar, published the first edition of \"Orbis Latinus\" in 1861. Although this first edition already listed a considerable number of names from around the world, it contained large gaps, especially in its coverage of more obscure locations. There followed a 1909 edition, almost doubled in size, under the direction of the University of Breslau Professor Frederick Benedict (b. 1850), who evaluated previously published sources and historical works far more systematically. Benedict increased the keywords of \"Orbis\", especially in regard to central Europe and the Middle Ages, but largely ignored areas outside of Europe, and did not consider material beyond the early modern period. Nevertheless, this work was re-released in a third edition, substantially unchanged, in 1922.\n\nIn the decades after World War II, Helmut Plechl (b. 1920), together with Sophie-Charlotte Plechl, began work on the fourth edition of \"Orbis Latinus\", adding many names of monasteries, mountains, and bodies of water, and taking into account material dating from antiquity to the Latin literature of the nineteenth century. This was first published in a one-volume manual edition in 1971, followed by a large three-volume edition in 1972. Due to the political instability of the period, the political boundaries of Germany as they existed in 1937 were used for the work.\n\nThe Bavarian State Library made images of the three-volume 1972 edition available online in July 2010, and has also made a full-text searchable version available.\n\n"}
{"id": "2003525", "url": "https://en.wikipedia.org/wiki?curid=2003525", "title": "Painite", "text": "Painite\n\nPainite is a very rare borate mineral. It was first found in Myanmar by British mineralogist and gem dealer Arthur C.D. Pain in the 1950s. When it was confirmed as a new mineral species, the mineral was named after him.\n\nThe chemical makeup of painite contains calcium, zirconium, boron, aluminium and oxygen (CaZrAlO(BO)). The mineral also contains trace amounts of chromium and vanadium, which are responsible for Painite's typically orange-red to brownish-red color, similar to topaz. The crystals are naturally hexagonal in shape, and, until late 2004, only two had been cut into faceted gemstones.\n\nExtensive exploration in the Mogok region has identified several new painite occurrences that have been vigorously explored resulting in several thousand new painite specimens.\n"}
{"id": "33258408", "url": "https://en.wikipedia.org/wiki?curid=33258408", "title": "Problems in General Physics", "text": "Problems in General Physics\n\nProblems in General Physics is a book written by Igor Irodov and first published in 1981. It is a collection of about 2000 physics problems in mechanics, thermodynamics, molecular physics, oscillations, and electromagnetism. In India, the book is widely used by high school students preparing for engineering entrance examinations such as the Joint Entrance Examination. Each problem in the book is really unique, especially mechanics problems, and the questions help students understand the basic laws of physics. \n"}
{"id": "59053678", "url": "https://en.wikipedia.org/wiki?curid=59053678", "title": "Rhodopirellula", "text": "Rhodopirellula\n\nRhodopirellula is a marine genus of bacteria from the family of Planctomycetaceae. \n"}
{"id": "48944968", "url": "https://en.wikipedia.org/wiki?curid=48944968", "title": "Schiaparelli EDM lander", "text": "Schiaparelli EDM lander\n\n\"Schiaparelli\" EDM lander was the Entry, Descent and Landing Demonstrator Module (EDM) of the ExoMars programme—a joint mission of the European Space Agency (ESA) and the Russian space agency Roscosmos. It was built in Italy and was intended to test technology for future soft landings on the surface of Mars. It also had a limited but focused science payload that would have measured atmospheric electricity on Mars and local meteorological conditions.\n\nLaunched together with the ExoMars Trace Gas Orbiter (TGO) on 14 March 2016, \"Schiaparelli\" attempted a landing on 19 October 2016. Telemetry signals from \"Schiaparelli\", monitored in real time by the Giant Metrewave Radio Telescope in India (and confirmed by Mars Express), were lost about one minute from the surface during the final landing stages. On 21 October 2016, NASA released an image by the \"Mars Reconnaissance Orbiter\" showing what appears to be the lander's crash site. The telemetry data accumulated by the TGO and ESA's \"Mars Express\" orbiters are being used to investigate the failure modes of the landing technology employed.\n\nThe \"Schiaparelli\" Entry, Descent, and Landing Demonstrator module is named for Giovanni Schiaparelli (1835–1910), an astronomer active in the 19th century who made Mars observations. In particular, he recorded features he called \"canali\" in his native Italian. His observations of what translates as channels in English inspired many. The dark streaks on Mars are an albedo feature which is related to dust distribution; these albedo features on Mars slowly change over time, and in the last few decades have been monitored by Mars orbiters. Schiaparelli is famous for making hand-drawn maps of Mars during its 1877 oppositions with Earth with an optical refracting telescope. He was also the first astronomer to determine the relationship between comet debris and yearly meteor showers.\n\nOther things named for Schiaparelli include the main-belt asteroid 4062 Schiaparelli, named on 15 September 1989 (), the lunar crater Schiaparelli, the Martian crater Schiaparelli, Schiaparelli Dorsum on Mercury, and the 2016 ExoMars EDM lander.\n\nThe mission was named in November 2013; previously it was known as the Exomars Entry, descent and landing Demonstrator Module, or ExoMars EDM for short. Another name was ExoMars \"static lander\", however some designs for what was the static lander are quite different due to various stages of design and program restructuring. Another name, especially for both orbiter and lander together is \"ExoMars 2016\".\n\nThe EDM traces itself back to the ESA Aurora programme, which has the goal of human exploration of space, and thus producing missions that are building blocks to support this goal. ExoMars originated out of this, and provides context for understanding the EDM. Schiaparelli forms an important \"block\" of learning how to land heavy payloads on Mars, which is vital to future manned missions. Another \"block\" is the ExoMars rover, which is intended to demonstrate among other things the ability to traverse several km/miles on the surface of Mars. The Aurora program is focused on two types of the mission, one are larger flagship spacecraft and the other are smaller missions specifically meant to offload risk from the larger missions. There are also various science goals:\n\nAn important date in its development was 2005, when the ESA council approved 650 million Euros for a Mars rover and static lander. At this time the idea was for a single launch bringing both a Mars Exploration Rover class rover and instrumented static lander to Mars with a simpler cruise stage;in this case the static lander both landed the rover and performed its own studies. However to accomplish its mission goals within the constraints of a Soyuz launcher, the rover was budgeted for 6 kg at one point. This led to the search for bigger rockets; the Ariane V, Atlas V, and Proton were evaluated. As heavier launchers were considered, heavier rovers from 180 even up to 600 kg were considered, and eventually the idea of test lander to offload risk from the rover lander was taken seriously and fit well with a two launch setup that allowed for a heavier orbiter and a heavier rover. Another factor was if the demonstrator should wait in Mars orbit for the Global dust storm to start. Early in the development, the idea was for the lander to be carried by a dedicated cruise stage called the \"Carrier Module\". Eventually, the Trace Gas Orbiter mission was merged into ExoMars becoming the mother-ship for the EDM.\n\nAn older iteration of the static lander was planned to carry a group of eleven instruments collectively called the \"Humboldt payload\", that would be dedicated to investigate the geophysics of the deep interior. But a payload confirmation review in the first quarter of 2009 resulted in a severe de-scope of the lander's instruments, and the Humboldt suite was cancelled. Notional instruments in the Humboldt payload included a sub-surface radar, meteorological instruments, and the geophysical instruments.\n\nThe data obtained from \"Schiaparelli\" are expected to provide ESA and Roscosmos with the technology for landing on the surface of Mars with a controlled soft landing, key technologies for the 2020 ExoMars rover mission.\n\nThe descent module \"Schiaparelli\" and orbiter completed testing and were integrated to a Proton-M rocket at the Baikonur cosmodrome in Baikonur in mid-January 2016. TGO and EDM arrived at Baikonur in December 2015. In February the spacecraft was mounted to the Briz-M upper stage, and in early March that was attached to the Proton rocket.\n\nThe launch occurred at 09:31 GMT (15:31 local time) on 14 March 2016. Four rocket burns occurred in the following 10 hours before the descent module and orbiter were released. A signal from the orbiter was received at 21:29 GMT that day, confirming that the launch was successful and the spacecraft was functioning properly. Shortly after separation from the probes, the Briz-M upper booster stage exploded a few kilometres away, without damaging the orbiter or lander.\nAfter its launch, the Trace Gas Orbiter and EDM traveled together coasting through space towards Mars. During this time the EDM was powered from an umbilical power line to the TGO, thus preserving the EDM's limited internal batteries. On 14 October 2016, the TGO did a final adjustment to its trajectory before the separation of Schiaparelli. The launch mass of the two spacecraft together is 4332 kg including the 600 kg \"Schiaparelli\" module. This was the heaviest spacecraft yet sent to Mars. The journey from Earth to Mars in 2016 took about 7 months.\n\nOn 16 October 2016, the TGO and EDM separated, the orbiter heading for Mars orbit insertion and the EDM for Mars atmospheric entry. Prior to the separation, the EDM was spun up 2.5 RPM (see also spin stabilization) and then released at a velocity of about 1 km/h relative to TGO. The EDM was designed to go into a lower-power hibernation mode for about 3 days while it traveled solo to Mars. The EDM came out of hibernation about an hour and a half prior to reaching the Martian atmosphere. Meanwhile, after the separation, the TGO adjusted its trajectory for its Mars orbit insertion and by 19 October 2016 performed a 139-minute rocket engine burn to enter Mars orbit. On the same day, the \"Schiaparelli\" module arrived at Mars traveling at and engaged in its prime task of entry, descent, and landing. It used a heat shield, parachute and retrorockets to slow its descent. The TGO entered Mars' orbit and it will undergo several months of aerobraking to adjust its speed and orbit, with science activities beginning in late 2017. The TGO will continue serving as a relay satellite for future Mars landing missions until 2022.\n\nThe landing site chosen was Meridiani Planum, a Martian plain prized by Mars landers for its flat terrain and low elevation that gives a spacecraft time and distance to slow down before reaching the ground. The EDM cannot avoid obstacles during its descent, so it was important to pick a large flat area with a minimum of obstacles. The landing ellipse is about 100 km long by 15 km wide, centered at 6° West and 2° South running East-West, with the eastern edge including the \"Opportunity\" rover landing site, and near Endeavour crater where it was still operating when the EDM was launched and when it attempted to land. The \"Opportunity\" rover (MER-B) landing site is called the \"Challenger Memorial Station\". It was also thought that the EDM would have a chance of arriving when Mars experienced its global dust storms, and thus gain knowledge about the atmosphere under these less common conditions. The site is also known to be scientifically interesting; The \"Opportunity\" rover discovered a type of iron mineral that forms in the presence of water, so it is theorized there was a significant amount of water there in the past.\n\nThe landing was planned to take place on Meridiani Planum during the dust storm season, which would have provided a chance to characterise a dust-loaded atmosphere during entry and descent, measure the dust's static electricity charge —typically produced by friction— and to conduct surface measurements associated with a dust-rich environment.\n\nGlobal dust storms have occurred at least nine times since 1924 including 1977, 1982, 1994, 2001 and 2007; the 2007 dust storms nearly ended the functioning of the solar-powered U.S. Mars Exploration Rovers \"Spirit\" and \"Opportunity\". Global dust storms obscured Mars when the \"Mariner 9\" orbiter arrived there in 1971, and it took several weeks for the dust to settle down and allow for clear imaging of the surface of Mars. It was predicted that Mars global dust storms are likely to occur in the fall of 2016, but they had not started when the EDM attempted its landing. The Mars Global dust storms hit in the summer of 2018, choking off the light to the solar powered Opportunity rover which was still operating nearby to the Schiaparelli landing site.\n\nThe \"Schiaparelli\" lander separated from the TGO orbiter on 16 October 2016, three days before arrival at Mars, and entered the atmosphere at on 19 October 2016. (see also Mars atmospheric entry) When the lander disconnected from the orbiter, it switched to internal battery power and used a low-power hibernation mode while it coasted for three days just before entering the Martian atmosphere. \"Schiaparelli\" came out of hibernation several hours before its entry, at a speed of and an altitude of above the surface of Mars. The heat shield was used during the plunge into the atmosphere to decelerate the lander to by the time it reached altitude. During entry the COMARS+ instrumentation the EDM operated to collect data on how heat and air flow around the entry capsule.\n\nAfter slowing its initial entry through the atmosphere, the module deployed a parachute and was to complete its landing on retrorockets by using a closed-loop guidance, navigation and control system based on a Doppler radar altimeter sensor, and on-board inertial measurement units. Throughout the descent, various sensors recorded a number of atmospheric parameters and lander performance. The plan was that at in altitude the front heat shield would be jettisoned and the radar altimeter turned on, then at altitude above Mars the rear heat cover and parachute would be jettisoned.\n\nThe final stages of the landing were to be performed using pulse-firing liquid-fuel engines or retrorockets. About two metres above ground, the engines were designed to turn off and let the platform land on a crushable structure, designed to deform and absorb the final touchdown impact. On final landing it was designed to endure rocks about one foot high, and it was hoped, but not guaranteed, that no out-sized boulders or craters would be encountered. On final contact, the lander was designed to handle slopes of up to 19 degrees and rocks up to in height.\n\nThe \"Opportunity\" rover was operating in the region and the two teams worked together to attempt to image the EDM on its descent, which, depending on conditions, might have been possible especially if the EDM \"went long\" in its landing ellipse. However, the rover's cameras had no view of the lander during its descent. It was the first time a surface probe attempted to image the landing of another vehicle from the surface of Mars. (Other spacecraft have imaged each other, especially orbiters viewing ones on the ground, and in 2005 \"Mars Global Surveyor\" imaged \"Mars Express\" in orbit around Mars.)\n\nEDL summary (as planned):\n\nContact was lost with the module 50 seconds before the planned touch-down. By 21 October 2016, after studying the data, ESA said it was likely that things went wrong when the parachute released early, the engines then turned on but then turned off after too short of time.\n\nThe \"Schiaparelli\" lander attempted an automated landing on 19 October 2016, but the signal was unexpectedly lost a short time before the planned landing time. ESA's \"Mars Express\" and NASA's \"Mars Reconnaissance Orbiter\" (MRO) and MAVEN continued listening for the lander's signal to no avail.\n\n\"Schiaparelli\" transmitted about 600 megabytes of telemetry during its landing attempt, and detailed analysis found that its atmospheric entry occurred normally, with the parachute deploying at and , and its heat shield releasing at . However, the lander's inertial measurement unit, which measures rotation, became saturated (unable to take higher readings) for about one second. This saturation, coupled with data from the navigation computer, generated an altitude reading that was negative, or below ground level. This caused the premature release of the parachute and back shell. The braking thrusters then fired for about three seconds rather than the expected 30 seconds, followed by the activation of ground systems as if the vehicle had already landed. In reality, it was still at an altitude of . The lander continued transmitting for 19 seconds after the thrusters cut off; the loss of signal occurred 50 seconds before it was supposed to land. \"Schiaparelli\" impacted the Martian surface at , near terminal velocity.\n\nA day after the attempted landing, the Context Camera of NASA's MRO identified new ground markings due to the lander's impact and parachute. The crash site is about 54 km (~33.5 miles) from where the active NASA Mars rover \"Opportunity\" was at the time of the landing. On 27 October 2016, ESA released high resolution images of the crash site taken by the MRO HiRISE camera on 25 October 2016. The front heatshield, module impact site, and the rear heat-shield and parachute are identified. It is thought that the crater is about half a meter (yard) deep and it may be possible to further study this crater at a later time. On a related note, an artificially made crater was actually the goal of the \"THOR\" mission proposed under the Mars Scout program that produced Phoenix and MAVEN, the goal was sub-surface exavation. That mission was passed over, but another orbiter was able to discover naturally occurring fresh impact craters, and ice was found in them.\n\nAlthough the lander crashed, ESA officials declared \"Schiaparelli\" a success because it had fulfilled its primary function of testing the landing system for the ExoMars 2020 surface platform and returning telemetry data during its descent. By 20 October, the bulk of the descent data had been returned to Earth and was being analysed. Unlike the \"Beagle 2\" lander, which was not heard from again after being released from Mars Express in 2003, the Exomars module transmitted during descent so data collected and transmitted on the way down was not lost if the spacecraft was destroyed on impact.\n\nAn investigation that concluded in May 2017 revealed that, at that time, the lander deployed its parachute and then began spinning unexpectedly fast. This superfast rotation briefly saturated \"Schiaparelli\" spin-measuring instrument, which resulted in a large attitude-estimation error by the guidance, navigation and control-system software. This resulted in the computer calculating that it was below ground level, triggering the early release of the parachute and backshell, a brief firing of the thrusters for only 3 seconds instead of 30 seconds, and the activation of the on-ground system as if \"Schiaparelli\" had landed.\n\nImages of module's crash site suggested that a fuel tank may have exploded asymmetrically in the impact. It is reported that the lander impacted the surface at about . Additional imaging of the site by November further confirmed the identity of the spacecraft's parts. The additional imaging was in colour and it was noted that parachute was slightly shifted.\n\nBy taking more images using a technique called super-resolution reconstruction (SRR) the resolution can be improved, and this was done for the formerly lost Beagle 2 probe. Two other benefits to more images is that is easier to discern between image noise such as cosmic ray hits and real objects, and among bright spots high albedo objects versus momentary specular reflections. Finally, with multiple images over time, movement and changes, such as the wind blowing a parachute can be observed.\n\nThe primary mission goal was to test the landing systems, including the parachute, Doppler radar altimeter, hydrazine thrusters, etc. The secondary mission goal was scientific. The lander was to measure the wind speed and direction, humidity, pressure and surface temperature, and determine the transparency of the atmosphere. The surface science payload was called DREAMS, and was designed to conduct meteorological data for a few days after landing, as well as measure the first measurements of atmospheric static electricity on Mars.\n\nA descent camera (DECA) was included in the payload. Its captured images were to be transmitted after landing. AMELIA, COMARS+, and DECA collected data during the entry, descent, and landing for about six minutes. Much of this data was transmitted while it was descending. Although EDL portion was designed to last literally a few minutes, and the surface observations at most a few days, one instrument, INRRI, was a passive laser retro-reflector that could be used as long as possible, even decades later, for laser range-finding of the lander.\n\nINRRI was mounted to the top (zenith) side of the lander, to enable spacecraft above to target it. It weighed about 25 grams on Earth, and was contributed by the Italian Space Agency (ASI). The design used a cube corner reflector to return incoming laser light. The cubes are made of fused silica which are mounted to an aluminum support structure. INRRI was also mounted to the InSight Mars lander.\n\n\nThe lander's scientific payload for the surface was the meteorological DREAMS (Dust Characterization, Risk Assessment, and Environment Analyser on the Martian Surface) package, consisting of a suite of sensors to measure the wind speed and direction (MetWind), humidity (MetHumi), pressure (MetBaro), surface temperature (MarsTem), the transparency of the atmosphere (Solar Irradiance Sensor - SIS), and atmospheric electrification (Atmospheric Relaxation and Electric-field Sensor - Micro-ARES). The institutions that contributed to the DREAMS science payload include INAF and CISAS from Italy, LATMOS from France, ESTEC from the Netherlands, FMI from Finland, and INTA from Spain.\n\nThe DREAMS payload was intended to function for 2 to 8 Mars days as an environmental station for the duration of the surface mission after landing. The planned lander arrival was made to coincide with the Mars global dust storm season and collect data on a dust-loaded Mars atmosphere. DREAMS had been hoped to provide new insights into the role of electric forces on dust lifting, the mechanism that initiates dust storms. In addition, the MetHumi sensor was intended to complement MicroARES measurements with critical data about humidity, to enable scientists to better understand the dust electrification process.\n\nAtmospheric electricity on Mars is still unmeasured, and its possible role in dust storms and atmospheric chemistry remains unknown. It has been speculated that atmospheric static electricity may have played a role in the inconclusive results from the \"Viking\" lander life experiments, which were positive for metabolizing microbial life, but no organic compounds were detected by the mass spectrometer. The two favored possible explanations are reactions with hydrogen peroxide () or ozone () created by ultraviolet light or atmospheric electrical processes during dust storms.\n\nDREAMS-P was a pressure sensor and DREAMS-H was for humidity;the sensors feed a single data-handling circuit board.\n\nIn addition to the surface payload, a camera called DECA (Descent Camera) on the lander operated during the descent. It was intended to deliver additional context information and exact location data in the form of images. DECA is a reflight of the Visual Monitoring Camera (VMC) of the Planck and Herschel mission.\n\nAnother surface experiment that was focused on dust was the Materials Adherence Experiment on the Mars Pathfinder lander, about twenty years prior to ExoMars.\n\nThe Descent Camera (DECA) was intended to capture about 15 downward-looking views as it approached the surface of Mars. It was to begin acquiring images after the lower heat shield was ejected. This camera had a 60 degree field of view to capture greyscale images, to support technical knowledge of the descent. DECA was a flight spare of the visual monitoring camera of the Herschel Space Observatory and Plank mission that were launched together. The camera dimensions are squared, with a mass of . The DECA descent camera data were stored during descent and not meant to be relayed to Earth until after landing, so these images were lost in the crash. The purpose of this transfer delay was to protect the spacecraft and data from electrostatic discharges. DECA was designed and built in Belgium by \"Optique et Instruments de Précision\" (OIP).\n\nThe main goals for DECA included:\n\nBecause the \"Schiarapelli\" demonstrator lander transmitted \"during\" its descent, a great deal of telemetry was successfully returned. About 600 megabytes of data, amounting to about 80% of telemetry, were relayed to Earth and are being used to investigate the failure modes of the landing technology employed.\n\nNote about masses: on the Mars surface the gravity is less than on Earth, so the weight is 37% of the Earth weight.\n\nAt one point, Roscosmos offered to contribute a 100 watt radioisotope thermoelectric generator (RTG) power source for the EDM lander to allow it to monitor the local surface environment for a full Martian year, but because of complex Russian export control procedures, it later opted for the use of a non-rechargeable electric battery with enough power for 2 to 8 sols. Solar panels were also considered when a longer mission (1–2 months) supported by a heavier, more complex, lander was under consideration. By the 2010s the focus was on executing a short-lived (a few days surface time) technology demonstration, with an emphasis on landing systems.\n\n\"Schiaparelli\" had a UHF radio to communicate with Mars orbiters. The lander had two antennae, one on the back shell and one on the lander. When the back shell is ejected, it can transmit from the spiral antenna on body of the lander. The ExoMars TGO could also communicate with it using the UHF system. When an orbiter can communicate with the lander depends on where it is in its orbit, and not all orbiters could record or talk with lander because the globe of Mars blocks the line of sight to the lander. The ExoMars TGO could also communicate with it using the UHF system. The EDM \"woke up\" from hibernation about 90 minutes prior to landing, and transmitted continuously for 15 minutes prior to landing.\n\nDuring its landing, the EDM signal was monitored at Mars by the \"Mars Express\" orbiter, and remotely by the Giant Metrewave Radio Telescope in Pune, India. \"Mars Express\" also communicates with other landers and rovers using its Melacom communication system. The \"Mars Reconnaissance Orbiter\" (MRO) overflew the landing two hours after landing, and was available to check for signals from \"Schiaparelli\". The ExoMars TGO could also communicate with it using the UHF system.\n\nThe communication system standard at Mars is the Electra radio, in use since the arrival of the \"Mars Reconnaissance Orbiter\" in 2006. Prior to this, several orbiters used a first generation UHF relay system, including \"Mars Global Surveyor\", \"Mars Odyssey\", and \"Mars Express\". Using orbiters to relay data from Mars landers and rovers is noted for its energy efficiency.\n\nOn 19 October 2016 it took 9 minutes and 47 seconds for a radio transmission to travel at roughly the speed of light from Mars to Earth. So even though the radio array at Pune listened in \"real time\", the entire EDL sequence, which would take about 6 minutes, had already occurred even as it was being recorded as starting to enter the atmosphere. There is a tiny bit of variation because the speed of light is slowed down by the air of Mars and Earth (see Refractive index), and another factor is Time dilation, because the probe existed at a significantly different velocity and in a different gravitational field the radio station back on Earth (though relatively small).\n\nThe \"Schiaparelli\" lander has two main computers, one is called the Central Terminal & Power Unit (CTPU) and housed in a warm box on top, and the other computer is called the Remote Terminal & Power Unit (RTPU) and is on the underside of the lander. Overall, the CTPU handles surface operations and the RTPU handles entry and descent, and is actually destroyed on final landing with surface because it is on the underside. When the Trace Gas Orbiter and Entry Demonstrator Module are connected, the RTPU handles the interface and sends power from the orbiter to the module. When it disconnects from the orbiter, then it must run off its internal batteries. The CTPU uses a LEON central processor, and also has RAM, PROM, and a timer. The CTPU also handles data sent to the UHF radio communication system. When the lander disconnects from the orbiter, it spends most of its time in a low-power hibernation mode while it coasts through space before entering the Martian atmosphere. The lander must coast through space for about 3 days by itself before landing, meanwhile the orbiter has to do a Mars orbit insertion. The DECA descent camera data is not downloaded to the computer for relay to Earth until after landing, and it is not transmitted during descent.\n\nA disk-band-gap parachute was deployed by a pyrotechnic mortar. It was tested at full scale in the largest wind tunnel in the world as part of its development. A sub-scale parachute was tested in Earth's atmosphere was conducted in 2011; it was ascended by balloon to 24.5 kilometers altitude and then released, and the pyrotechnic deployment systems was tested after a period of free-fall. On 19 October 2016 the parachute was successfully deployed on Mars.\n\n\"Schiaparelli\" module has 3 sets of three thrusters, nine total, that operate starting at about 1 km (half a mile) up in pulse mode, slowing the spacecraft from . Each of the nine engines is a CHT-400 rocket engine that can produce 400 Newtons of thrust. These rocket engines are fueled by three spherical 17.5 liter tanks holding hydrazine propellant. The tanks hold about 15–16 kilograms of hydrazine (about 34 pounds, 2.4 stones) of fuel per tank, or 46 kg overall (101 pounds or 7.24 stones). The propellant is pressurized by helium, held in a single tank containing 15.6 liters at a pressure of 170 bar (2465 psi). The thrusters shut down 1–2 meters/yards from the surface, after which the crumple zone underneath the lander handles the final stop. Data from a timer, doppler radar, and inertial measurement unit are merged in the lander's computers to control the operation of the thrusters.\n\nA possible \"shutdown\" moment for the next ExoMars mission was the ESA ministerial meeting in December 2016 which considered certain issues including €300 million of ExoMars funding and lessons learned from the ExoMars 2016 missions so far. One concern is the \"Schiapraelli\" module crash, as this landing system is being produced in near duplication for the ExoMars 2020 mission consisting of the ExoMars rover delivered by the instrumented ExoMars 2020 surface platform. The ExoMars team has been praised for \"putting a brave face\" on what happened and being positive about the EDM's very credible return on its prime mission: data about entry, descent, and landing, despite the difficulties on final landing. Also, there was the successful insertion of the TGO into Mars orbit with its large science payload. Another positive was the development of the demonstrator module as part of the overall grand plan for ExoMars, which meant that the landing technologies underwent a real-world test before carrying more valuable cargo. Just as the EDM itself was tested on Earth to gain knowledge about how it would perform on Mars, the EDM is also a test for future missions. Study of what happened is critical, as significant breakthroughs in understanding can impact the lessons learned from a mission, which in turn effects public opinion, technology, future mission design, and even the feelings of everyone involved. For example, \"Beagle 2\" Mars lander was suspected to have undergone a high velocity impact with Mars in 2003, but when it was found on Mars intact with its panels partly deployed the EDL design was validatedbut only after more than a decade. The lead developer did suffer heavy criticism and even ridicule for this failure, eventually dying from a brain hemorrhage in 2014, just a year before his spacecraft was found intact. A preliminary report on the malfunction was presented at the December 2016 ESA ministerial meeting. By December the outcome was known: ExoMars would go on being financially supported by the ESA. €436 million ($464 million) was authorized to finish the mission.\n\n\n"}
{"id": "4709501", "url": "https://en.wikipedia.org/wiki?curid=4709501", "title": "Space weathering", "text": "Space weathering\n\nSpace weathering is the type of weathering that occurs to any object exposed to the harsh environment of outer space. Bodies without atmospheres (including the Moon, Mercury, the asteroids, comets, and most of the moons of other planets) take on many weathering processes:\nSpace weathering is important because these processes affect the physical and optical properties of the surface of many planetary bodies. Therefore, it is critical to understand the effects of space weathering in order to properly interpret remotely sensed data.\n\nMuch of our knowledge of the space weathering process comes from studies of the lunar samples returned by the Apollo program, particularly the lunar soils (or regolith). The constant flux of high energy particles and micrometeorites, along with larger meteorites, act to comminute, melt, sputter and vaporize components of the lunar soil.\n\nThe first products of space weathering that were recognized in lunar soils were \"agglutinates\". These are created when micrometeorites melt a small amount of material, which incorporates surrounding glass and mineral fragments into a glass-welded aggregate ranging in size from a few micrometers to a few millimeters. Agglutinates are very common in lunar soil, accounting for as much as 60 to 70% of mature soils. These complex and irregularly-shaped particles appear black to the human eye, largely due to the presence of nanophase iron.\n\nSpace weathering also produces surface-correlated products on individual soil grains, such as glass splashes; implanted hydrogen, helium and other gases; solar flare tracks; and accreted components, including nanophase iron. It wasn't until the 1990s that improved instruments, in particular transmission electron microscopes, and techniques allowed for the discovery of very thin (60-200 nm) patinas, or rims, which develop on individual lunar soil grains as a result of the redepositing of vapor from nearby micrometeorite impacts and the redeposition of material sputtered from nearby grains.\n\nThese weathering processes have large effects on the spectral properties of lunar soil, particularly in the ultraviolet, visible, and near infrared (UV/Vis/NIR) wavelengths. These spectral changes have largely been attributed to the inclusions of \"nanophase iron\" which is a ubiquitous component of both agglutinates and soil rims. These very small (one to a few hundred nanometers in diameter) blebs of metallic iron are created when iron-bearing minerals (e.g. olivine and pyroxene) are vaporized and the iron is liberated and redeposited in its native form.\n\nOn the Moon, the spectral effects of space weathering are threefold: as the lunar surface matures it becomes darker (the albedo is reduced), redder (reflectance increases with increasing wavelength), and the depth of its diagnostic absorption bands are reduced These effects are largely due to the presence of nanophase iron in both the agglutinates and in the accreted rims on individual grains. The darkening effects of space weathering are readily seen by studying lunar craters. Young, fresh craters have bright ray systems, because they have exposed fresh, unweathered material, but over time those rays disappear as the weathering process darkens the material.\n\nSpace weathering is also thought to occur on asteroids, though the environment is quite different from the Moon. Impacts in the asteroid belt are slower, and therefore create less melt and vapor. Also, fewer solar wind particles reach the asteroid belt. And finally, the higher rate of impactors and lower gravity of the smaller bodies means that there is more overturn and the surface exposure ages should be younger than the lunar surface. Therefore, space weathering should occur more slowly and to a lesser degree on the surfaces of asteroids.\n\nHowever, we do see evidence for asteroidal space weathering. For years there had been a so-called \"conundrum\" in the planetary science community because, in general, the spectra of asteroids do not match the spectra of our collection of meteorites. Particularly, the spectra of S-type asteroids, did not match the spectra of the most abundant type of meteorites, ordinary chondrites (OCs). The asteroid spectra tended to be redder with a steep curvature in the visible wavelengths. However, Binzel et al. have identified near Earth asteroids with spectral properties covering the range from S-type to spectra similar to those of OC meteorites, suggesting an ongoing process is occurring that can alter the spectra of OC material to look like S-type asteroids. There is also evidence of regolith alteration from Galileo's flybys of Gaspra and Ida showing spectral differences at fresh craters. With time, the spectra of Ida and Gaspra appear to redden and lose spectral contrast. Evidence from NEAR Shoemaker's x-ray measurements of Eros indicate an ordinary chondrite composition despite a red-sloped, S-type spectrum, again suggesting that some process has altered the optical properties of the surface. \nResults from the Hayabusa spacecraft at the asteroid Itokawa, also ordinary chondrite in composition, shows spectral evidence of space weathering. In addition, definitive evidence of space weathering alteration has been identified in the grains of soil returned by the Hayabusa spacecraft. Because Itokawa is so small (550 m diameter), it was thought that the low gravity would not allow for the development of a mature regolith, however, preliminary examination of the returned samples reveals the presence of nanophase iron and other space weathering effects on several grains. In addition, there is evidence that weathering patinas can and do develop on rock surfaces on the asteroid. Such coatings are likely similar to the patinas found on lunar rocks.\n\nThere is evidence to suggest most of the color change due to weathering occurs rapidly, in the first hundred thousands years, limiting the usefulness of spectral measurement for determining the age of asteroids.\n\nThe environment at Mercury also differs substantially from the Moon. For one thing, it is significantly hotter in the day (diurnal surface temperature ~100 °C for the Moon, ~425 °C on Mercury) and colder at night, which may alter the products of space weathering. In addition, because of its location in the Solar System, Mercury is also subjected to a slightly larger flux of micrometeorites that impact at much higher velocities than the Moon. These factors combine to make Mercury much more efficient than the Moon at creating both melt and vapor. Per unit area, impacts on Mercury are expected to produce 13.5x the melt and 19.5x the vapor than is produced on the Moon. Agglutinitic glass-like deposits and vapor-deposited coatings should be created significantly faster and more efficiently on Mercury than on the Moon.\n\nThe UV/Vis spectrum of Mercury, as observed telescopically from Earth, is roughly linear, with a red slope. There are no absorption bands related to Fe-bearing minerals, such as pyroxene. This means that either there is no iron on the surface of Mercury, or else the iron in the Fe-bearing minerals has been weathered to nanophase iron. A weathered surface would then explain the reddened slope.\n"}
{"id": "32422953", "url": "https://en.wikipedia.org/wiki?curid=32422953", "title": "Strategic Studies Quarterly", "text": "Strategic Studies Quarterly\n\nStrategic Studies Quarterly is a quarterly peer-reviewed academic journal sponsored by the United States Air Force covering issues related to national and international security. Published by Air University Press, the \"SSQ\" explores strategic issues of current and continuing interest to the United States Department of Defense and US international partners. New editions are released on the first day of March, June, September, and December. In early 2017, the journal launched an online news talk show, \"Issues and Answers,\" with interviews and discussion panels of United States Department of Defense subject matter experts and outside academics.\n\n"}
{"id": "30762208", "url": "https://en.wikipedia.org/wiki?curid=30762208", "title": "Three-torus model of the universe", "text": "Three-torus model of the universe\n\nThe three-torus model is a cosmological model proposed in 1984 by Alexei Starobinsky and Yakov Borisovich Zel'dovich at the Landau Institute in Moscow. The theory describes the shape of the universe (topology) as a three-dimensional torus. It is also informally known as the doughnut theory.\n\nThe cosmic microwave background (CMB) was discovered by Bell Labs in 1964. Greater understanding of the universe's CMB provided greater understanding of the universe's topology. In order to understand these CMB results, NASA supported development of two exploratory satellites, the Cosmic Background Explorer (COBE) in 1989 and the Wilkinson Microwave Anisotropy Probe (WMAP) in 2001.\n\n"}
{"id": "1538121", "url": "https://en.wikipedia.org/wiki?curid=1538121", "title": "Tourbillon", "text": "Tourbillon\n\nIn horology, a tourbillon (; \"whirlwind\") is an addition to the mechanics of a watch escapement. Developed around 1795 and patented by the French-Swiss watchmaker Abraham-Louis Breguet on June 26, 1801, a tourbillon aims to counter the effects of gravity by mounting the escapement and balance wheel in a rotating cage, to negate the effect of gravity when the timepiece (thus the escapement) is stuck in a certain position. By continuously rotating the entire balance wheel/escapement assembly at a slow rate (typically about one revolution per minute), the tourbillon averages out positional errors.\n\nOriginally an attempt to improve accuracy, tourbillons are still included in some expensive modern watches as a novelty and demonstration of watchmaking virtuosity. The mechanism is usually exposed on the watch's face to show it off.\n\nGravity directly affects the most delicate parts of the escapement, namely the pallet fork, balance wheel and hairspring. Most important is the hairspring, which functions as the timing regulator for the escapement and is thus the part most sensitive to exterior effects, such as magnetism, shocks, temperature, as well as inner effects such as pinning positions (inner collet), terminal curve, and heavy points on the balance wheel. \n\nMany inventions have been developed to counteract these problems. Temperature and magnetism problems have been eliminated with new materials. Shocks have much less effect today than at Breguet's time thanks to stronger and more resilient materials. The oscillator still gets disturbed at the moment of the shock, but the hairspring is not as easily deformed from shocks as before, and re-stabilizes itself quickly after such an event.\n\nGravity comes into play on the remaining effects. One of them is easily taken away, namely heavy points on the balance wheel. This leaves pinning point and terminal curve. Both of these add a lot of variation to the regulation of a watch; assembly, regulation adjustments by the watchmaker, positioning in the watch, and later position changes by the owner. As the balance wheel goes from one extreme position to the other in its swing back and forth, the hairspring's coils extend and contract a great deal, leading to problems that are extremely hard to counteract. Some have tried using hairsprings that are cylindrical or even spherical instead of flat as is prominent today. Some variations of Breguet's overcoil have been developed to counteract the effects of the terminal curve. As for the pinning point, Grossmann, Berthoud, Breguet, Caspari and Leroy tried many different possibilities, but not much improvement was gained.\n\nThe biggest obstacle to regulating a watch, even today, is getting a constant impulse from the escapement regardless of its spatial orientation. This has been made much easier with accurate timing instruments which give instantaneous results, whereas in Breguet's time all that watchmakers had was another watch to regulate from. So, results were not very exact and it could take weeks to get them. Effects of gravity on an escapement can have quite significant effects with slight variations of position. Even if a pocket watch was kept most of the time in a breast pocket, the exact position could still vary over 45°. Watchmakers can regulate a watch in up to eight positions: dial up, crown down, dial down, crown left, crown up, crown right, half-way position crown up, and half-way position crown down. A tourbillon quite neatly reduces this problem; it only needs to be regulated for three positions; the two horizontal positions, dial up & down, and one vertical position.\n\nEven with new materials and improved theories, it is impossible to regulate a mechanical watch so it keeps the same time in all positions. A tourbillon offers watchmakers the possibility of higher accuracy than conventional movements, although poising the balance well and ensuring that the balance spring expands and contracts symmetrically can achieve nearly the same result. A tourbillon typically makes one complete revolution per minute. This improves timekeeping in the four vertical positions because even if a watch is stationary in a random vertical position, the tourbillon makes the escapement turn around its own axis, effectively cancelling the effect of gravity by turning the balance through all possible vertical positions during its rotation. A normal tourbillon has no effect in horizontal positions, since the horizontal balance is not affected by gravity as it turns.\n\nA tourbillon has no effect on the change in rate that accompanies a change in attitude from dial horizontal (dial up or down) to dial vertical (pendant up, down, left or right). The change in rate between horizontal and vertical is much greater than rate changes between different vertical positions. Breguet designed the tourbillon for pocket watches that are kept in a vertical position in the waistcoat pocket, and can be maintained in this vertical position overnight by hanging on a suitable stand. In this attitude a tourbillon is effective. However, the attitude of a wristwatch changes frequently from vertical to horizontal depending on the wearer's hand movement. The effect of a tourbillon is small compared to the change in rate resulting from changes from vertical to horizontal, and vice versa. Inclined tourbillons such as those made by Greubel Forsey are an improvement in this regard.\n\nMechanical watches are now purchased by those who value craftsmanship and aesthetics over very accurate timing. Most tourbillons use standard Swiss lever escapements, but some have a detent escapement.\n\nThe tourbillon is considered to be one of the most challenging of watch mechanisms to make (although technically not a complication itself) and is valued for its engineering and design principles. The first production tourbillon mechanism was produced by Breguet for Napoleon in one of his carriage clocks (travel clocks of the time were of considerable weight, typically weighing almost 200 pounds).\n\nAnthony Randall invented the double-axis tourbillon in January 1977 and subsequently patented it. The first working example was later constructed by Richard Good in 1978. In 1980 Anthony Randall made a double-axis tourbillon in a carriage clock, which was located in the (now closed) Time Museum in Rockford, Illinois, US, and was included in their \"Catalogue of Chronometers\".\n\nIn 2003, inspired by this invention, the young German watchmaker Thomas Prescher developed for the Thomas Prescher Haute Horlogerie the first flying double-axis tourbillon in a pocket watch and, in 2004, the first flying double-axis tourbillon with constant force in the carriage in a wristwatch. Shown at the Baselworld 2003 and 2004 in Basel, Switzerland.\n\nA characteristic of this tourbillon is that it turns around two axes, both of which rotate once per minute. The whole tourbillon is powered by a special constant-force mechanism, called a \"remontoire\". Thomas Prescher invented the constant-force mechanism to equalize the effects of a wound and unwound mainspring, friction, and gravitation. Thereby even force is always supplied to the oscillation regulating system of the double-axis tourbillon. The device incorporates a modified system after a design by Henri Jeanneret.\n\nRobert Greubel and Stephen Forsey launched the brand Greubel Forsey in 2004 with the introduction of their Double Tourbillon 30° (DT30). Both men had been working together since 1992 at Renaud & Papi, where they developed complicated watch movements. The Double Tourbillon 30° features one tourbillon carriage rotating once per minute and inclined at 30°, inside another carriage which rotates once every four minutes. In 2005, Greubel Forsey presented their Quadruple Tourbillon à Différentiel (QDT), using two double-tourbillons working independently. A spherical differential connects the four rotating carriages, distributing torque between two wheels rotating at different speeds.\n\nIn 2004, Thomas Prescher developed the first triple-axis tourbillon for the Thomas Prescher Haute Horlogerie with constant force in the carriage in a wristwatch. Presented at the Baselworld 2004 in Basel, Switzerland, in a set of three watches including a single-axis, a double-axis and a triple-axis tourbillon.\n\nThe world's unique tri-axial tourbillon movement for wristwatch with traditional jewel bearings only was invented by the independent watchmaker Aaron Becsei, from Bexei Watches, in 2007. The Primus wristwatch was presented at the Baselworld 2008 in Basel, Switzerland. In the three axis tourbillon movement, the 3rd (external) cage has a unique form which provides the possibility of using jewel bearings everywhere, instead of ball-bearings. This is a unique solution at this size and level of complication. There are a few wrist and pocket watches that include the Triple Axis or Tri-Axial Tourbillon escapements. Examples of companies and watchmakers that include this mechanism are Vianney Halter in his \"Deep Space\" watch, Thomas Prescher, Aaron Becsei, Girard-Perregaux with the \"Tri-Axial Tourbillon\" and Jaeger Le-Coultre with the \"Gyrotourbillon\".\n\nRather than being supported by a bridge, or cock, at both the top and bottom, the flying tourbillon is cantilevered, being only supported from one side. The first flying tourbillon was designed by Alfred Helwig, instructor at the German School of Watchmaking, in 1920.\n\nIn 1993, Kiu Tai-Yu, a Chinese watchmaker residing in Hong Kong, created a semi-flying tourbillon with only an abbreviated carriage for the escapement wheel and pallet fork, the upper pivot of the balance wheel being supported in a sapphire bridge.\n\nJaeger-LeCoultre's first wristwatch tourbillon was introduced in 1993 (though JLC had produced tourbillons prior to that, including the famous observatory competition caliber 170) and in 2004 the company introduced the Gyrotourbillon I. Gyrotourbillon I is a double-axis tourbillon with a perpetual calendar and equation of time, and since then, Jaeger-LeCoultre has gone on to produce several variations on the multi-axis tourbillon theme. In general, however, these have been fairly thick watches (Gyrotourbillon I is 16mm thick) but with the Reverso Tribute Gyrotourbillon, JLC has produced a thinner and much more wearable version of its multi-axis tourbillon. At 51.1mm x 31mm x 12.4mm it's still not a small watch, but it is very comfortable on the wrist and certainly easier to wear than some of its predecessors.\n\nIn modern mechanical watch designs, a tourbillon is not required to produce a highly accurate timepiece; there is even debate amongst horologists as to whether tourbillons ever improved the accuracy of mechanical time pieces, even when they were first introduced, or whether the time pieces of the day were inherently inaccurate due to design and manufacturing techniques. Nevertheless, the tourbillon is one of the most valued features of collectors' watches and premium timepieces, possibly for the same reason that mechanical watches fetch a much higher price than similar quartz watches that are much more accurate. High-quality tourbillon wristwatches, usually made by the Swiss luxury watch industry, are very expensive, and typically retail for tens of thousands of dollars or euros, with much higher prices in the hundreds of thousands of dollars or euros being common. A recent renaissance of interest in tourbillons has been met by the industry with increased availability of time pieces bearing the feature, with the result that prices for basic tourbillon models have receded somewhat in recent years (whereas previously they were very rare, in either antiques or new merchandise); however, any time piece that has a tourbillon will cost a great deal more than an equivalent piece without the feature.\n\nModern implementations typically allow the tourbillon to be seen through a window in the watch face. In addition to enhancing the charm of the piece, the tourbillon can act as a second hand for some watches as it generally rotates once per minute. However some tourbillons spin faster (Greubel Forsey's 24-second tourbillon for example). There are many watches that feature the oscillating balance wheel visible through the watch dial that are not tourbillons. This feature is often referred to as \"open heart\" though it is often called a tourbillon by unscrupulous dealers.\n\nSeveral Chinese manufacturers now produce a variety of tourbillon movements. These movements are bought as ébauches by some foreign manufacturers and incorporated into watches that meet the requirements of the Federation of the Swiss Watch Industry to be sold as Swiss Made, requiring 60% of the value to have been made in Switzerland. The availability of cheap tourbillons has led industry spectators to worry that another quartz crisis may occur, where the Swiss watch industry will not be able to adapt quickly to cheaper complicated mechanical watches produced in other countries. In 2016, TAG Heuer began offering the Carrera Heuer-02T tourbillon at a suggested retail price of 14,900 CHF (~15,000 USD), significantly lower than the 100,000 CHF or more charged by other established Swiss watch brands. \n\n"}
{"id": "1686347", "url": "https://en.wikipedia.org/wiki?curid=1686347", "title": "Wilhelm Heinrich Waagen", "text": "Wilhelm Heinrich Waagen\n\nWilhelm Heinrich Waagen (23 June 1841 – 24 March 1900) was a geologist and paleontologist. He was born in Munich and died in Vienna.\n\nHe received a Doctor of Philosophy degree at the University of Munich, where he published an elaborate work on geology that was crowned by the university. In 1866 he became an instructor in palaeontology at the University of Munich and at the same time taught Princess Theresa and Prince Arnulf of Bavaria. Although an excellent teacher, and especially competent in practical work, Waagen, who was a most loyal Catholic, had little prospect of obtaining a professorship at the University of Munich. Consequently, in 1870, he accepted the offer of a position as assistant in the geological survey of India.\n\nIn 1875, he returned permanently to Europe because of the severity of the Indian climate. In 1877 he became an instructor at the University of Vienna in 1877 and lectured with great success on the geology of India. In 1879 Waagen went to the German Polytechnic of Prague as professor of geology and mineralogy; in 1890 he was professor of palaeontology at the University of Vienna; in 1886 he had declined a call to the school of mines at Berlin.\n\nHe was named councillor of the board of mines (Oberbergart) and in 1893 was made a corresponding member of the Academy of Sciences.\n\nWaagen's writings before his trip to India treat especially the German Jura and its fossils. He did work of permanent value in the geological investigation of India (the Salt Range) by the scientific presentation of rich palaeontological material. Wilhelm Waagen did a pioneer work in the Salt Range. He established a refined lithostratigraphy of the Early Triassic series (Mianwali Formation) that still holds today. This allows to replace most of the ammonoids he described in their original stratigraphic position, an accomplishment rarely achieved by paleontologists in the late 19th Century. He first realized how important the Early Triassic ammonoid succession of the Salt Range was for the construction of the Triassic time scale.\n\nIn 1869, after an exhaustive study of ammonites, Waagen advocated the theory of evolution or mutation for certain series of fossils. As a young man he had taken an active part in the Catholic life of Munich, and two years before his death he wrote a treatise on the first chapter of Genesis that showed both the geologist and the Christian.\n\nWaagen was one of the editors of the periodical \"\"Geognostische-paläontologische Beiträge\" (Munich), and during the years 1894-1900 editor of the \"Beiträge zur Paläontologie Oesterreich-Ungarns und des Orients\" (Vienna); after the death of Joachim Barrande (1883) he edited several volumes of Barrande's work \"Système silurien\". \nWaagen's most important works were: \nHe wrote in English: \"Jurassic Fauna of Kutch\" (1873-6); \"Productus Limestone\" (1879–91); \"Fossils from the Ceratite Formation\" (1892).\n"}
